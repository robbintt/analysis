---
ver: rpa2
title: Transformer-Based Language Model Surprisal Predicts Human Reading Times Best
  with About Two Billion Training Tokens
arxiv_id: '2304.11389'
source_url: https://arxiv.org/abs/2304.11389
tags:
- training
- surprisal
- variants
- after
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between language model
  quality and surprisal estimates' ability to predict human reading times. It systematically
  evaluates Transformer-based language model variants with varying training data sizes
  and model capacities.
---

# Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens

## Quick Facts
- arXiv ID: 2304.11389
- Source URL: https://arxiv.org/abs/2304.11389
- Authors: 
- Reference count: 12
- Primary result: Surprisal estimates from Transformer-based language models best predict human reading times after approximately two billion training tokens

## Executive Summary
This study investigates the relationship between language model quality and surprisal estimates' ability to predict human reading times. Through systematic evaluation of Transformer-based language model variants with varying training data sizes and model capacities, the research identifies a critical threshold: surprisal estimates provide optimal fit to human reading times after approximately two billion training tokens. Beyond this point, additional training data causes surprisal estimates to diverge from humanlike expectations. The findings challenge the assumption that higher quality language models always produce more humanlike surprisal estimates.

## Method Summary
The study systematically evaluates Pythia Transformer-based language model variants with different capacities (70M to 12B parameters) trained on the Pile corpus. Both pre-trained models and newly-trained smaller variants (6M to 70M parameters) are tested. Surprisal estimates are calculated for tokens in the Natural Stories and Dundee corpora, then used as predictors in linear mixed-effects regression models alongside baseline predictors. The analysis tracks how surprisal estimates' predictive power changes across different training steps and model capacities.

## Key Results
- Surprisal estimates from most language model variants with contemporary capacities provide the best fit to human reading times after approximately two billion training tokens
- Smaller language model variants reveal a 'tipping point' at convergence, after which decreased perplexity leads to poorer fits to human reading times
- Massive amounts of training data in large pre-trained models are primarily responsible for the poorer fit to human reading times observed in these models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There is an optimal amount of training data (~2B tokens) for language models to best predict human reading times.
- Mechanism: Transformer-based language models initially improve their ability to predict human reading times as they see more training data, but after reaching approximately two billion tokens, additional training data causes surprisal estimates to diverge from humanlike expectations.
- Core assumption: Human language processing expectations can be captured by probabilistic models trained on natural text.
- Evidence anchors:
  - [abstract]: "surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens"
  - [section]: "Figure 13 show that across both corpora, surprisal from most LM variants made the biggest contribution to regression model fit after 1,000 training steps (i.e. after ∼2B tokens)"
- Break condition: If the relationship between training data size and human reading time prediction doesn't show this optimal point, or if the optimal point occurs at a very different scale.

### Mechanism 2
- Claim: Model capacity interacts with training data size to affect the fit to human reading times.
- Mechanism: Smaller language models show a 'tipping point' where decreased perplexity leads to poorer fits to human reading times after convergence, while larger models maintain better fits up to a certain point.
- Core assumption: Model capacity is necessary for capturing humanlike expectations in language processing.
- Evidence anchors:
  - [abstract]: "smaller LM variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times"
  - [section]: "After all 10,000 training steps, the model variants show a reversal in the relationship between LM perplexity and fit to reading times; the 2-3-192 variant seems to represent a 'tipping point'"
- Break condition: If smaller models don't show this tipping point behavior, or if larger models don't maintain better fits up to a certain point.

### Mechanism 3
- Claim: The inverse relationship between model quality (perplexity) and fit to human reading times observed in large pre-trained models is primarily due to the massive amount of training data.
- Mechanism: Large pre-trained language models with billions of training tokens show poorer fit to human reading times because they've overfit to statistical patterns that don't reflect human processing expectations.
- Core assumption: The massive training data used in large pre-trained models is responsible for the poorer fit to human reading times.
- Evidence anchors:
  - [abstract]: "These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models"
  - [section]: "Taken together, these results indicate that the vast amount of training data is responsible for the poorer fit achieved by surprisal from larger Transformer-based LMs"
- Break condition: If other factors besides training data size (e.g., model architecture, optimization procedures) are found to be the primary cause of the inverse relationship.

## Foundational Learning

- Concept: Information-theoretic surprisal as a measure of processing difficulty
  - Why needed here: The study uses surprisal estimates from language models to predict human reading times, so understanding surprisal is fundamental
  - Quick check question: What is the mathematical definition of surprisal, and how does it relate to predictability?

- Concept: Linear mixed-effects regression modeling
  - Why needed here: The study uses LME models to analyze the relationship between surprisal estimates and reading times while accounting for subject and sentence variability
  - Quick check question: How do random effects in LME models account for variability between subjects and sentences?

- Concept: Byte-pair encoding (BPE) tokenization
  - Why needed here: The language models use BPE tokenization, which affects how surprisal is calculated and how well it matches human reading patterns
  - Quick check question: How does BPE tokenization differ from word-level or character-level tokenization, and what are its implications for language modeling?

## Architecture Onboarding

- Component map: Pythia LMs -> Training on Pile corpus -> Tokenization with BPE -> Surprisal calculation -> Linear mixed-effects regression models

- Critical path: Train language models → Calculate surprisal estimates → Fit regression models → Analyze relationship between perplexity and reading time prediction

- Design tradeoffs: Using Pythia models allows systematic variation of model capacity and training data, but may limit generalizability to other model architectures

- Failure signatures: Poor fit to reading times could indicate issues with tokenization, insufficient model capacity, or excessive training data

- First 3 experiments:
  1. Verify that surprisal estimates correlate with reading times for a simple language model
  2. Test how surprisal estimates change as a model is trained on increasing amounts of data
  3. Compare surprisal estimates from models of different capacities on the same corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which training data size beyond 2 billion tokens causes surprisal estimates to diverge from humanlike expectations?
- Basis in paper: [explicit] The paper identifies that after approximately 2 billion training tokens, surprisal estimates begin to diverge from humanlike expectations, but does not explain the underlying mechanism.
- Why unresolved: The paper observes the phenomenon but does not investigate the cognitive or linguistic factors that cause this divergence.
- What evidence would resolve it: Detailed analysis of how surprisal estimates change with increasing training data, focusing on specific linguistic phenomena or cognitive processing patterns that show divergence.

### Open Question 2
- Question: Is there a critical threshold of model capacity below which Transformer-based language models cannot capture humanlike expectations, and how does this threshold vary across different linguistic phenomena?
- Basis in paper: [explicit] The paper identifies a 'tipping point' at the 2-3-192 variant but does not determine if this threshold is universal across different linguistic phenomena.
- Why unresolved: The study only examines reading times as a measure of humanlike expectations, not exploring whether different linguistic phenomena might require different model capacities.
- What evidence would resolve it: Testing surprisal estimates from different model capacities on various linguistic phenomena (e.g., syntactic complexity, semantic coherence) to identify if different thresholds exist.

### Open Question 3
- Question: Would training language models on more diverse or domain-specific corpora change the relationship between training data size, model capacity, and fit to human reading times?
- Basis in paper: [inferred] The study uses the Pile dataset, which is a general corpus, and does not explore how domain-specific training might affect the relationship.
- Why unresolved: The paper does not investigate the impact of training data diversity or specificity on the observed relationships.
- What evidence would resolve it: Training language models on various types of corpora (e.g., domain-specific texts, child-directed speech) and comparing the resulting relationships between training data size, model capacity, and fit to human reading times.

## Limitations

- Limited generalizability to other model architectures beyond Pythia Transformers
- Only tested on two English corpora, limiting cross-linguistic validation
- Does not account for semantic or syntactic complexity measures that might interact with surprisal effects

## Confidence

High confidence in the core finding of an optimal training data threshold (~2B tokens) for predicting human reading times. Medium confidence in the interaction effects between model capacity and training data size. Low confidence in the proposed causal mechanism linking massive training data to divergence from human expectations.

## Next Checks

1. Verify the 2 billion token threshold finding using different language model architectures (e.g., recurrent models)
2. Test whether the tipping point for smaller models holds when using different convergence criteria or optimization procedures
3. Investigate whether similar patterns emerge when training on domain-specific or child-directed speech corpora instead of general web text