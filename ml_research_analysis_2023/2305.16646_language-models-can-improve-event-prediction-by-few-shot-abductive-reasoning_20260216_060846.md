---
ver: rpa2
title: Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning
arxiv_id: '2305.16646'
source_url: https://arxiv.org/abs/2305.16646
tags:
- event
- events
- time
- prediction
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that leverages large language models
  (e.g., GPT-3.5) for event prediction. The LLM performs abductive reasoning to generate
  possible causes for proposed events, and retrieves relevant previous events as evidence.
---

# Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning

## Quick Facts
- arXiv ID: 2305.16646
- Source URL: https://arxiv.org/abs/2305.16646
- Reference count: 40
- Key outcome: Framework using LLMs for abductive reasoning significantly improves event prediction accuracy over state-of-the-art event sequence models on GDELT and Amazon Review datasets

## Executive Summary
This paper proposes a framework that leverages large language models (e.g., GPT-3.5) for event prediction by combining abductive reasoning with evidence-based reranking. The LLM generates possible causes for proposed future events, retrieves relevant past events as evidence, and a scoring function judges whether the retrieved evidence supports the proposals. Experiments demonstrate significant improvements in prediction accuracy compared to state-of-the-art event sequence models.

## Method Summary
The framework builds upon existing event sequence models that generate initial predictions for future events. For each proposed event, an LLM performs few-shot abductive reasoning to generate possible causes, which are used as queries to retrieve relevant historical events. A learned scoring function (based on a continuous-time Transformer) evaluates the compatibility between proposals and retrieved evidence, reranking predictions to select final outputs. The approach is tested on GDELT and Amazon Review datasets using multiple base event models.

## Key Results
- MAP@M improvements of 5-17% over state-of-the-art event sequence models
- Mean rank for type prediction consistently improved across all tested base models
- RMSE for time prediction shows measurable reductions in error
- Performance gains scale with the number of proposals (M) and retrieval samples (αretro)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Abductive Reasoning Enhances Event Prediction
- Claim: LLM-guided abductive reasoning generates plausible cause events that serve as effective retrieval queries for evidence, improving prediction accuracy.
- Mechanism: The LLM is prompted to generate possible causes for proposed future events (abductive reasoning). These generated causes, though not necessarily exact matches to historical events, are used as search queries to retrieve relevant past events from the history. The retrieved events form the evidence used to score the plausibility of the proposed predictions.
- Core assumption: LLM-generated causes, even if not exact matches, will be semantically similar enough to actual past events to serve as effective retrieval queries.

### Mechanism 2: Reranking Proposals Based on Evidence-Proposal Compatibility
- Claim: A learned scoring function evaluates the compatibility between retrieved evidence and proposed events, allowing accurate reranking of predictions.
- Mechanism: The scoring function c(t, k, e) embeds the proposal and its retrieved evidence using a continuous-time Transformer and outputs a scalar compatibility score. Proposals are reranked based on these scores, with higher-scoring proposals selected as final predictions.
- Core assumption: The Transformer-based scoring function can effectively learn to distinguish between evidence that truly supports a proposal and evidence that does not.

### Mechanism 3: Leveraging Pretrained Knowledge in LLMs
- Claim: LLMs pretrained on vast amounts of text can reason about events and their causal relationships, providing knowledge beyond what autoregressive event models learn from data alone.
- Mechanism: The LLM, having read diverse text during pretraining, can draw upon this knowledge to generate plausible causes for events, effectively injecting external knowledge into the prediction process.
- Core assumption: LLMs possess relevant world knowledge that can improve event prediction, even without task-specific fine-tuning.

## Foundational Learning

- **Concept**: Abductive reasoning (inference to the best explanation)
  - Why needed here: The framework relies on generating plausible causes for observed events, which is the essence of abductive reasoning.
  - Quick check question: What type of reasoning seeks the most plausible explanations for a given observation, as opposed to deductive or inductive reasoning?

- **Concept**: Continuous-time point processes and intensity functions
  - Why needed here: The framework builds upon existing event sequence models that use intensity functions to model event occurrence rates over time.
  - Quick check question: In a Hawkes process, how does the intensity function λ(t) depend on the history of past events?

- **Concept**: Few-shot prompting and in-context learning
  - Why needed here: The LLM is instructed via few-shot demonstrations to perform the abductive reasoning task without task-specific fine-tuning.
  - Quick check question: How does providing demonstrations in a prompt enable a language model to perform a new task without gradient updates?

## Architecture Onboarding

- **Component map**: Event sequence model → LLM reasoning → Retrieval → Scoring → Reranking → Final prediction
- **Critical path**: Event sequence model generates proposals → LLM generates causes → Retrieval finds matching historical events → Scoring function evaluates compatibility → Reranking selects final predictions
- **Design tradeoffs**:
  - LLM choice vs. cost: GPT-3.5-turbo offers better performance than GPT-3.0-davinci but at higher API cost
  - Number of retrievals (αretro): More retrievals may capture more relevant evidence but increase noise
  - Number of proposals (M): More proposals give the reranking more opportunities to find correct predictions but increase computational cost
  - Prompt design: More demonstrations generally improve performance but increase prompt size and cost
- **Failure signatures**:
  - Performance similar to baseline: LLM reasoning or retrieval is not providing useful evidence
  - Performance degrades with more proposals: Scoring function cannot effectively distinguish correct from incorrect predictions
  - High variance in results: LLM outputs are inconsistent or unreliable
- **First 3 experiments**:
  1. Verify LLM generates semantically meaningful causes: Test LLM with sample proposals and inspect generated causes for relevance
  2. Validate retrieval effectiveness: Check that retrieved events are semantically similar to LLM-generated causes using embedding similarity metrics
  3. Test scoring function learning: Train on a small dataset and verify that scores correlate with ground truth (actual events should receive higher scores than randomly generated ones)

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise relationship between the quality of the LLM-generated causal events and the downstream prediction performance? Specifically, can we quantify how much the noise in LLM outputs degrades the final predictions?
- **Open Question 2**: How does the framework's performance scale with increasing numbers of event types (K)? Is there a theoretical or empirical limit to its effectiveness as K grows extremely large?
- **Open Question 3**: How robust is the framework to different types of event sequence models as the base predictor? Are there specific model characteristics that make integration more or less effective?

## Limitations

- Performance heavily depends on the quality of LLM-generated causes and their semantic alignment with historical events
- Computational cost of API calls to GPT-3.5-turbo may limit practical deployment
- The framework's generalization across different event domains beyond news and product reviews remains unclear

## Confidence

- **High confidence**: The mechanism of using LLM-generated causes as retrieval queries and reranking based on evidence compatibility is well-supported by experimental results
- **Medium confidence**: The claim that pretrained knowledge in LLMs improves event prediction beyond autoregressive models is plausible but not directly validated
- **Low confidence**: The scalability analysis is limited, with only brief mention of computational costs without systematic evaluation

## Next Checks

1. Apply the framework to a third domain (e.g., financial market events or social media trends) to verify that improvements transfer beyond news and reviews
2. Systematically evaluate how variations in LLM output quality (using different temperature settings or alternative LLMs) affect final prediction performance
3. Measure end-to-end inference time and API costs across different values of M and αretro to identify optimal configurations balancing performance gains against computational overhead