---
ver: rpa2
title: 'How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model'
arxiv_id: '2307.02129'
source_url: https://arxiv.org/abs/2307.02129
tags:
- features
- number
- deep
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Random Hierarchy Model (RHM), a synthetic
  classification task where class labels are determined by hierarchical composition
  of features across L levels, with each feature represented by m equivalent synonymic
  groups of s lower-level features. The authors study how deep convolutional neural
  networks (CNNs) learn this task compared to shallow networks.
---

# How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model

## Quick Facts
- arXiv ID: 2307.02129
- Source URL: https://arxiv.org/abs/2307.02129
- Reference count: 40
- Primary result: Deep CNNs overcome curse of dimensionality with sample complexity P* = ncmL for hierarchical compositional tasks

## Executive Summary
This paper introduces the Random Hierarchy Model (RHM), a synthetic classification task where class labels are determined by hierarchical composition of features across L levels, with each feature represented by m equivalent synonymic groups of s lower-level features. The authors show that deep CNNs require only polynomially many samples (P* = ncmL) to learn RHM tasks, while shallow networks suffer exponential sample complexity. This demonstrates how deep networks overcome the curse of dimensionality by building hierarchical invariant representations that are insensitive to exchanges of synonymous features.

## Method Summary
The authors generate synthetic data according to the RHM specification, where input dimension is d = s^L and features are one-hot encoded. Deep CNNs with L hidden layers, filter size and stride equal to s, are trained using SGD with cross-entropy loss. The sample complexity is analyzed theoretically through signal-to-noise ratio arguments and empirically verified by measuring synonymic sensitivity in internal representations. A clustering-based algorithm is also analyzed to establish a √ncmL lower bound on sample complexity for hierarchical compositional tasks.

## Key Results
- Deep CNNs require sample complexity P* = ncmL, which grows polynomially with input dimension d = s^L
- Shallow fully-connected networks require exponentially more data (Pmax = ncm(d-1)/(s-1))
- Synonymic invariance in hidden representations emerges when training data reaches P ≈ ncmL
- A clustering-based algorithm achieves √ncmL sample complexity, establishing a fundamental limit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep CNNs overcome the curse of dimensionality by building invariant representations to synonymic feature exchanges
- Mechanism: As training data increases, hidden representations become progressively invariant to permutations of synonymic tuples at each level. This invariance emerges because gradients amplify correlations between low-level features and class labels.
- Core assumption: Composition rules generate detectable correlations between low-level features and class labels
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If composition rules don't generate detectable correlations

### Mechanism 2
- Claim: Sample complexity P* scales polynomially (ncmL) with input dimension d = sL
- Mechanism: Sample complexity is determined by when signal (correlations) becomes detectable above noise (sampling fluctuations), occurring at P ≈ ncmL.
- Core assumption: Signal-to-noise ratio determines learning capability
- Evidence anchors: [abstract], [section 4.2]
- Break condition: If signal-to-noise scaling is different

### Mechanism 3
- Claim: One-step gradient descent on orthogonalized inputs builds synonymic invariant representations
- Mechanism: With proper initialization and orthogonalized inputs, single gradient descent step updates hidden representation to be invariant to synonym exchanges.
- Core assumption: Inputs are orthogonalized and weights are properly initialized
- Evidence anchors: [section 4.3], [section C]
- Break condition: If inputs aren't orthogonalized or initialization is inappropriate

## Foundational Learning

- Concept: Curse of dimensionality
  - Why needed here: Understanding why shallow networks fail and deep networks succeed requires knowing how sample complexity scales with input dimension
  - Quick check question: Why does a shallow fully-connected network require exponentially more data than a deep CNN for this task?

- Concept: Signal-to-noise ratio in statistical learning
  - Why needed here: Sample complexity is determined by when signal becomes detectable above noise
  - Quick check question: How does signal-to-noise ratio scale with number of training samples and what does this imply for learning?

- Concept: Hierarchical composition and synonymy
  - Why needed here: Task structure involves features composed hierarchically with synonymic equivalence at each level
  - Quick check question: What does it mean for two s-tuples to be synonyms and why is this property important for learning?

## Architecture Onboarding

- Component map: Input → CNN layers → Output → Loss computation → Gradient computation → Weight updates
- Critical path: One-hot encoded sL-dimensional feature vectors flow through L CNN layers with s×s filters, stride=s, ReLU activations, to nc-dimensional softmax output
- Design tradeoffs:
  - Filter size vs. tuple length: Must match s for proper hierarchical processing
  - Depth vs. hierarchy depth: L hidden layers needed to process L-level hierarchy
  - Width vs. vocabulary size: H should be larger than vs to capture all possible s-tuples
- Failure signatures:
  - Poor performance despite sufficient data: Check if filter size matches tuple length s
  - No improvement with depth: Verify that hierarchy depth L matches number of hidden layers
  - High sensitivity to synonym exchanges: Check if enough training data has been provided (P < ncmL)
- First 3 experiments:
  1. Train shallow fully-connected network on RHM data and observe exponential scaling of sample complexity
  2. Train deep CNN with proper architecture (filter size=s, depth=L) and observe polynomial scaling P* = ncmL
  3. Measure synonymic sensitivity Sk,l as function of training data to confirm invariance emerges at P ≈ ncmL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does sample complexity scale when RHM parameters (s, m, v) vary across different hierarchy levels?
- Basis in paper: [explicit] Authors mention extending model to allow different parameters at each level is straightforward
- Why unresolved: Paper only considers constant s, m, v for clarity, without analyzing varying parameters
- What evidence would resolve it: Empirical results showing sample complexity with varying parameters across levels

### Open Question 2
- Question: Can theoretical framework extend to more complex data structures beyond RHM?
- Basis in paper: [explicit] Authors suggest RHM clarifies open questions that could apply to more complex scenarios
- Why unresolved: Framework is developed for RHM but not explored for generalization to other structures
- What evidence would resolve it: Extending framework to analyze more complex real-world data structures

### Open Question 3
- Question: How do internal representations of deep CNNs compare to shallow networks/kernel methods on RHM?
- Basis in paper: [inferred] Authors contrast deep CNNs with shallow networks and mention advantages over kernel methods
- Why unresolved: Paper doesn't provide detailed comparison of internal representations across different approaches
- What evidence would resolve it: Analyzing and comparing internal representations across different architectures on RHM

## Limitations

- The analysis assumes random hierarchical composition rules that may not reflect real-world compositional structures
- Comparison between deep and shallow networks assumes ideal architectures for each case without exploring full design space
- Orthogonalization assumption for gradient descent analysis may not hold with standard initialization schemes

## Confidence

- **High Confidence**: Polynomial vs exponential scaling distinction is mathematically well-established
- **Medium Confidence**: Mechanism of building invariant representations through gradient amplification relies on assumptions about random compositions
- **Medium Confidence**: √ncmL lower bound for clustering algorithms is established but practical relevance depends on task match

## Next Checks

1. Test whether ncmL scaling holds across different deep CNN architectures beyond the specific L-layer, s×s filter design by varying filter sizes and introducing residual connections

2. Apply RHM analysis framework to real compositional datasets (visual scenes, language structures) to test if predicted scaling emerges and invariance mechanisms are observable

3. Compare deep CNN performance against hierarchical learning approaches (graph neural networks, transformers) on RHM tasks to determine if polynomial scaling is unique to CNNs or general to hierarchical learning