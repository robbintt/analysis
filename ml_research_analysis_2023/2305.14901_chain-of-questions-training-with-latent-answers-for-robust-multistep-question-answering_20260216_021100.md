---
ver: rpa2
title: Chain-of-Questions Training with Latent Answers for Robust Multistep Question
  Answering
arxiv_id: '2305.14901'
source_url: https://arxiv.org/abs/2305.14901
tags:
- qdmr
- asub
- question
- drop
- hotpot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Questions, a framework that trains
  a language model to generate and answer sub-questions step-by-step for robust multistep
  question answering. The method leverages human-annotated question decomposition
  meaning representation (QDMR) for sub-question generation, but treats sub-answers
  as latent variables and optimizes them using a dynamic mixture of Hard-EM and MAPO.
---

# Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering

## Quick Facts
- arXiv ID: 2305.14901
- Source URL: https://arxiv.org/abs/2305.14901
- Reference count: 16
- Key outcome: Achieves 9.0 F1 improvement over neuro-symbolic methods on DROP contrast set and 24.3 F1 improvement over GPT-3.5 on HOTPOTQA adversarial set

## Executive Summary
This paper introduces Chain-of-Questions, a framework that trains a language model to generate and answer sub-questions step-by-step for robust multistep question answering. The method leverages human-annotated question decomposition meaning representation (QDMR) for sub-question generation but treats sub-answers as latent variables optimized through a dynamic mixture of Hard-EM and MAPO. Experiments demonstrate significant improvements over strong baselines, particularly on robustness evaluation sets that test out-of-distribution performance.

## Method Summary
The Chain-of-Questions framework trains a model to generate sub-questions and sub-answers iteratively for complex multistep reasoning tasks. It uses QDMR annotations to supervise sub-question generation while treating sub-answers as latent variables optimized through a novel combination of Hard-EM and MAPO reinforcement learning techniques. The model generates sub-questions and sub-answers one at a time, using beam search to find high-reward trajectories and a replay buffer to stabilize training. Task-specific modifications like regular expressions for numerical operations and auxiliary tasks for supporting fact prediction are incorporated for different benchmarks.

## Key Results
- 9.0 F1 improvement over neuro-symbolic methods on DROP contrast set
- 24.3 F1 improvement over GPT-3.5 on HOTPOTQA adversarial set
- Strong performance on robustness evaluation sets demonstrates effectiveness of the approach
- Significant gains over baselines in out-of-distribution scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating sub-answers as latent variables and optimizing them with Hard-EM + MAPO enables the model to find better intermediate reasoning steps that lead to correct final answers.
- Mechanism: The model uses beam search to generate multiple candidate sub-answer sequences, evaluates them using the F1 score of the final answer, and then optimizes the model parameters to maximize the likelihood of high-reward trajectories (Hard-EM) while also leveraging a replay buffer of successful trajectories (MAPO).
- Core assumption: The F1 score of the final answer is a reliable reward signal for intermediate sub-answer quality, and the beam search can approximate the best sub-answer sequence without exhaustive enumeration.

### Mechanism 2
- Claim: Using QDMR sub-questions as supervision for sub-question generation while treating sub-answers as latent variables allows the model to learn effective multistep reasoning without requiring expensive sub-answer annotations.
- Mechanism: The model is trained to generate sub-questions using supervised learning (since QDMR provides these), while sub-answers are treated as latent variables optimized through Hard-EM and MAPO. This leverages existing QDMR annotations efficiently.
- Core assumption: QDMR sub-questions are of sufficient quality to serve as supervision for sub-question generation, and the model can learn to generate meaningful sub-answers even without direct supervision.

### Mechanism 3
- Claim: The dynamic mixture of Hard-EM and MAPO objectives, with a weighting factor λ that starts with Hard-EM and transitions to MAPO, enables faster initial improvement and better final convergence.
- Mechanism: At the beginning of training, when the replay buffer is empty, the model relies on Hard-EM to provide useful training signal. As the buffer fills with successful trajectories, the model transitions to MAPO for better convergence, with the weighting factor λ controlling this transition.
- Core assumption: Hard-EM provides better initial training signal when the replay buffer is empty, while MAPO provides better convergence once the buffer has quality trajectories, and the dynamic weighting can effectively balance these two objectives.

## Foundational Learning

- Concept: Reinforcement Learning with Latent Variables
  - Why needed here: The model needs to optimize sub-answers that are not directly annotated, requiring techniques that can handle latent variables and learn from reward signals.
  - Quick check question: Can you explain the difference between supervised learning and reinforcement learning, and why reinforcement learning is needed when dealing with latent variables?

- Concept: Question Decomposition and Multistep Reasoning
  - Why needed here: The model needs to break down complex questions into simpler sub-questions and generate answers for each step, requiring an understanding of question decomposition strategies.
  - Quick check question: Can you describe how question decomposition can help with multistep reasoning, and what challenges might arise when generating sub-questions and sub-answers?

- Concept: Beam Search and Trajectory Optimization
  - Why needed here: The model uses beam search to generate multiple candidate sub-answer sequences and optimize for the best trajectory, requiring an understanding of search algorithms and trajectory optimization.
  - Quick check question: Can you explain how beam search works, and how it can be used to approximate the best sub-answer sequence in this context?

## Architecture Onboarding

- Component map:
  QDMR Parser -> Main Model (f) -> Hard-EM Module -> MAPO Module -> Regular Expression Module (for DROP) -> Auxiliary Tasks (for HOTPOT QA)

- Critical path:
  1. QDMR Parser generates sub-questions for the input question
  2. Main Model generates sub-questions and sub-answers one at a time, using the previous sub-questions/sub-answers as context
  3. Hard-EM Module uses beam search to find the best sub-answer sequence and updates the model parameters
  4. MAPO Module uses the replay buffer to update the model parameters based on successful trajectories
  5. Regular Expression Module (if applicable) handles numerical operations in the final sub-question
  6. Auxiliary Tasks (if applicable) improve performance on the main task

- Design tradeoffs:
  - Using QDMR sub-questions as supervision vs. generating sub-questions from scratch
  - Treating sub-answers as latent variables vs. directly annotating them
  - Using Hard-EM vs. MAPO vs. a dynamic mixture of both
  - Adding task-specific modifications (e.g., regular expression module) vs. keeping the model more general

- Failure signatures:
  - Poor performance on contrast and adversarial sets, indicating lack of robustness
  - Inability to generate meaningful sub-questions or sub-answers, leading to incorrect final answers
  - Overfitting to the training data, resulting in poor generalization to new examples
  - Slow convergence or getting stuck in local optima during training

- First 3 experiments:
  1. Train the model on a small subset of the data using only Hard-EM, and evaluate its performance on a validation set to check if it can learn to generate meaningful sub-questions and sub-answers.
  2. Add the MAPO module and the replay buffer, and train the model using the dynamic mixture of Hard-EM and MAPO. Evaluate its performance on the validation set to check if it improves over the Hard-EM-only model.
  3. Add the task-specific modifications (e.g., regular expression module for DROP) and train the model on the full dataset. Evaluate its performance on the test set and the robustness evaluation sets (contrast and adversarial sets) to check if it improves over the baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Chain-of-Questions be adapted to work with larger language models like T5-3B or GPT-4?
- Basis in paper: The paper mentions GPU resource constraints preventing scaling to larger models like T5-3B, but notes these smaller models already show good performance.
- Why unresolved: The paper only tested Chain-of-Questions on smaller models (T5-Base, T5-Large, LongT5-Base) due to computational limitations. The performance and efficiency of the method on truly large-scale models remains unknown.
- What evidence would resolve it: Training and evaluating Chain-of-Questions on T5-3B or GPT-4, comparing performance and computational costs to smaller models, would determine if scaling brings significant benefits.

### Open Question 2
- Question: Can the Chain-of-Questions framework be made truly universal across all multistep QA benchmarks without dataset-specific modifications?
- Basis in paper: The paper notes that "Chain-of-Questions still requires task-specific modifications for different multistep QA benchmarks—we did not find out a good way to build a universal model that is highly effective on all datasets."
- Why unresolved: While the framework shows strong results on DROP and HOTPOTQA with modifications, the authors acknowledge they couldn't create a single model that works well across all datasets. The fundamental limitations of achieving true universality remain unexplored.
- What evidence would resolve it: Developing and testing a single Chain-of-Questions model across diverse multistep QA benchmarks (DROP, HOTPOTQA, FINQA, MUSIQUE, ROPES) without any task-specific modifications would demonstrate whether true universality is achievable.

### Open Question 3
- Question: How well does Chain-of-Questions transfer to datasets without QDMR annotations?
- Basis in paper: The paper states "our method requires QDMR annotation during training, which is only available for some datasets" and suggests this as future work to "test the transferability of our method to datasets without QDMR annotation."
- Why unresolved: The current framework depends on human-annotated QDMR data, limiting its application to only datasets where this exists. The authors propose exploring transfer to other datasets but haven't conducted this experiment.
- What evidence would resolve it: Training Chain-of-Questions on datasets with QDMR (like DROP) and evaluating its performance on similar multistep reasoning tasks without QDMR (like FINQA or ROPES) would show whether the learned decomposition skills transfer effectively.

## Limitations
- The framework relies heavily on the quality and availability of QDMR annotations, which are not universally available across all QA datasets.
- The method requires task-specific modifications for different benchmarks, preventing a truly universal model.
- Computational constraints limited testing to smaller models (T5-Base, T5-Large) rather than larger models like T5-3B.

## Confidence
- **High confidence**: The overall approach of using Chain-of-Questions with latent sub-answer optimization is novel and effective, as evidenced by the significant improvements on robustness evaluation sets.
- **Medium confidence**: The specific implementation details, such as the dynamic mixture of Hard-EM and MAPO and the task-specific modifications, are likely to be effective but may require further fine-tuning and validation.
- **Low confidence**: The scalability and generalizability of the approach to other QA datasets and domains are uncertain and would require further investigation.

## Next Checks
1. **Ablation study on QDMR quality**: Train the model on datasets with varying levels of QDMR annotation quality and evaluate its performance to assess the impact of annotation quality on the final results.
2. **Alternative reward signals**: Experiment with different reward signals for intermediate sub-answers, such as exact match or other task-specific metrics, to determine if the F1 score is the most reliable option.
3. **Transfer learning experiments**: Test the model's performance on QA datasets from different domains or with different question types to evaluate its generalizability and identify potential limitations.