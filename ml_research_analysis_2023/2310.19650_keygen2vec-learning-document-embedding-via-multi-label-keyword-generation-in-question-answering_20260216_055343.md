---
ver: rpa2
title: 'KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation
  in Question-Answering'
arxiv_id: '2310.19650'
source_url: https://arxiv.org/abs/2310.19650
tags:
- document
- data
- topic
- embedding
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KeyGen2Vec is a sequence-to-sequence framework that learns document
  embeddings conditioned on keywords, reformulated as a multi-label keyword generation
  task in community question-answering data. By treating keywords as sequences rather
  than discrete labels, the model indirectly incorporates label dependencies, which
  enhances the quality of the resulting embeddings.
---

# KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering

## Quick Facts
- arXiv ID: 2310.19650
- Source URL: https://arxiv.org/abs/2310.19650
- Reference count: 40
- Key outcome: Achieves up to 14.7% higher purity, NMI, and F1-score in clustering tasks compared to multi-label classifiers and other embedding methods

## Executive Summary
KeyGen2Vec is a sequence-to-sequence framework that learns document embeddings by treating keyword generation as a multi-label sequence prediction task. By reformulating multi-label keyword generation into one-to-one multi-class learning via softmax normalization, the model indirectly incorporates label dependencies, enhancing embedding quality. Experiments on three datasets show consistent improvements over traditional multi-label classifiers and other embedding methods, particularly in noisy datasets where unsupervised approaches struggle.

## Method Summary
KeyGen2Vec reformulates multi-label keyword generation as a sequence-to-sequence learning problem, mapping documents to keyword sequences using a bidirectional GRU encoder and GRU decoder with Bahdanau attention. The model treats keywords as sequences rather than discrete labels, using negative log-likelihood loss with Adam optimization. Curriculum learning with teacher forcing is employed during training, where the model gradually transitions from ground truth keywords to its own predictions. The learned document embeddings are then used for clustering tasks, evaluated using Purity, NMI, and F1-score.

## Key Results
- Consistently outperforms multi-label classifiers and other embedding methods on three datasets
- Achieves up to 14.7% higher purity, NMI, and F1-score in clustering tasks
- Particularly effective in noisy datasets where unsupervised approaches struggle
- Competes closely with supervised classifiers using topic labels, especially with larger topic sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KeyGen2Vec captures latent semantic structure by treating keywords as sequences rather than discrete labels.
- Mechanism: Reformulating multi-label keyword generation into one-to-one multi-class learning via softmax normalization indirectly incorporates label dependency, allowing the model to learn structural similarity between documents and keywords.
- Core assumption: Learning a conditioned sequence-to-sequence mapping between documents and their keywords equals to learning the structural similarity that hierarchically links contents in document, keywords as explicit document abstractions, and topics as latent variables.
- Evidence anchors:
  - [abstract]: "By treating keywords as sequences rather than discrete labels, the model indirectly incorporates label dependencies, which enhances the quality of the resulting embeddings."
  - [section]: "We hypothesize that by transforming one-to-many training objective in multi-label keyword generation task into one-to-one multi-class learning scheme...we indirectly incorporate label dependency assumption during training stage."
- Break condition: If keywords are not truly sequential or don't capture sub-topic information, the sequence assumption breaks down and the model loses its advantage over multi-label classifiers.

### Mechanism 2
- Claim: The hierarchical semantic assumption of a corpus enables KeyGen2Vec to learn embeddings that preserve topical proximity.
- Mechanism: Documents and their corresponding keyword labels form substructures that link to latent topic structure as global semantics, allowing the model to capture both local (keyword) and global (topic) semantic information.
- Core assumption: Keywords serve as explicit document abstractions that hierarchically link to latent topics, forming a semantic network.
- Evidence anchors:
  - [abstract]: "Our work holds an assumption that learning a conditioned sequence-to-sequence mapping between documents and their corresponding keywords equals to learning the structural similarity that hierarchically links contents in document, keywords as explicit document abstractions, and topics as latent variables."
  - [section]: "The assumption is that documents and their corresponding keyword labels form substructures or sub-networks of latent topic structure as global semantics."
- Break condition: If the keyword-topic relationship is not hierarchical or if keywords don't adequately represent sub-topics, the model cannot effectively capture global semantic structure.

### Mechanism 3
- Claim: Curriculum learning with teacher forcing during training improves KeyGen2Vec's ability to generate keywords and learn document embeddings.
- Mechanism: By sampling whether to use teacher forcing based on scheduled sampling, the model gradually transitions from relying on ground truth keywords to its own predictions, improving robustness and generalization.
- Core assumption: Gradually reducing dependence on ground truth during training helps the model learn more robust representations.
- Evidence anchors:
  - [abstract]: "Curriculum learning (Bengio et al., 2015) was employed to sampling whether to use a teacher forcing method during training stage."
  - [section]: "Curriculum learning (Bengio et al., 2015) was employed to sampling whether to use a teacher forcing method during training stage. For the other models, we refer the reader to the provided code documentation."
- Break condition: If curriculum learning is not properly implemented or if the sampling strategy is ineffective, the model may not benefit from this training approach.

## Foundational Learning

- Concept: Sequence-to-Sequence (Seq2Seq) modeling with attention mechanisms
  - Why needed here: KeyGen2Vec relies on Seq2Seq architecture to map documents to keyword sequences, capturing the relationship between document content and keywords.
  - Quick check question: Can you explain how attention mechanisms help Seq2Seq models focus on relevant parts of the input when generating output sequences?

- Concept: Multi-label classification vs. sequence generation
  - Why needed here: Understanding the difference between treating keywords as discrete labels versus sequences is crucial to grasping KeyGen2Vec's approach.
  - Quick check question: What are the key differences in how multi-label classifiers and sequence-to-sequence models handle keyword prediction?

- Concept: Curriculum learning and scheduled sampling
  - Why needed here: These techniques are used in KeyGen2Vec's training process to improve keyword generation and embedding quality.
  - Quick check question: How does scheduled sampling help address the training-evaluation loss mismatch in sequence generation tasks?

## Architecture Onboarding

- Component map: Document → Encoder → Attention → Decoder → Keyword sequence
- Critical path: Document → Encoder → Attention → Decoder → Keyword sequence
- Design tradeoffs:
  - Sequence vs. discrete labels: Sequence approach captures label dependencies but is more complex
  - Bidirectional vs. unidirectional encoder: Bidirectional captures more context but increases computation
  - Attention mechanism: Improves generation quality but adds complexity and computation
- Failure signatures:
  - Poor clustering performance despite good keyword generation
  - Overfitting to training data (especially with small datasets)
  - Inability to generalize to unseen keywords or topics
  - Slow convergence or unstable training due to curriculum learning
- First 3 experiments:
  1. Compare KeyGen2Vec's keyword generation quality against a multi-label classifier on a small dataset
  2. Evaluate the impact of sequence length on model performance
  3. Test the effect of different curriculum learning schedules on keyword generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do label dependencies in the proposed KeyGen2Vec model affect its performance compared to independent label assumption models in multi-label classification tasks?
- Basis in paper: [explicit] The paper discusses how KeyGen2Vec treats keywords as sequences and incorporates label dependencies through softmax normalization, while standard multi-label learning uses independent Bernoulli assumption via Sigmoid function.
- Why unresolved: The paper compares KeyGen2Vec with multi-label classifiers but doesn't isolate the specific impact of label dependency modeling on performance.
- What evidence would resolve it: A controlled experiment comparing KeyGen2Vec with a multi-label classifier using the same softmax-based loss but without sequence modeling would help isolate the effect of label dependency modeling.

### Open Question 2
- Question: How does the performance of KeyGen2Vec scale with increasing document length and vocabulary size?
- Basis in paper: [inferred] The paper mentions that the Yahoo! Answer Comprehensive cQA dataset contains noisy data with non-informative words and domain-specific terms, which could affect model performance.
- Why unresolved: The paper doesn't provide a systematic analysis of how document length and vocabulary size impact KeyGen2Vec's performance.
- What evidence would resolve it: An ablation study varying document length and vocabulary size while measuring KeyGen2Vec's performance would provide insights into its scalability.

### Open Question 3
- Question: How does KeyGen2Vec perform on datasets with different types of semantic structures, such as hierarchical or overlapping topics?
- Basis in paper: [explicit] The paper mentions that the Yahoo! Answer Comprehensive cQA dataset contains overlapping sub-structures, which may introduce noise in learning.
- Why unresolved: The paper only evaluates KeyGen2Vec on three datasets with specific characteristics and doesn't explore its performance on diverse semantic structures.
- What evidence would resolve it: Testing KeyGen2Vec on datasets with known hierarchical or overlapping topic structures would reveal its ability to capture different types of semantic relationships.

## Limitations

- The sequence-to-sequence reformulation relies heavily on the assumption that keyword dependencies can be meaningfully captured through sequence modeling, which remains largely untested.
- The curriculum learning component is underspecified in terms of schedule parameters and sampling strategy, making it difficult to assess its specific contribution to performance.
- The model's performance on datasets with different semantic structures (hierarchical, overlapping) is not explored.

## Confidence

- **High Confidence**: The empirical improvements in clustering quality (up to 14.7% gains in Purity, NMI, and F1-score) are well-supported by the experimental results across three diverse datasets.
- **Medium Confidence**: The mechanism by which sequence modeling captures label dependencies is plausible but not rigorously demonstrated - the paper shows correlation between the approach and performance gains, but not causation.
- **Low Confidence**: The curriculum learning component's contribution to overall performance is difficult to assess due to lack of ablation studies or detailed implementation specifications.

## Next Checks

1. Conduct ablation studies removing the sequence formulation (treat keywords as independent labels) and curriculum learning to isolate their individual contributions to performance.
2. Test model robustness by evaluating on datasets with artificially degraded keyword quality or randomized keyword order to verify the sequence dependency assumption.
3. Implement the curriculum learning component with varying schedules to determine optimal parameters and assess sensitivity to this hyperparameter.