---
ver: rpa2
title: 'PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based
  Imitation Learning with Protagonist Antagonist Guided Adversarial Reward'
arxiv_id: '2306.01731'
source_url: https://arxiv.org/abs/2306.01731
tags:
- policy
- reward
- function
- learning
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a semi-supervised reward design paradigm called
  PAGAR to address the identifiability issue in IRL-based imitation learning. PAGAR
  trains a protagonist policy to perform well under a set of adversarial reward functions,
  which are selected to maximize the performance gap between the protagonist and an
  antagonist policy.
---

# PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward

## Quick Facts
- arXiv ID: 2306.01731
- Source URL: https://arxiv.org/abs/2306.01731
- Reference count: 40
- Key outcome: PAGAR achieves higher training efficiency and performance in imitation learning by addressing reward misalignment through adversarial reward design, enabling zero-shot transfer learning from demonstrations

## Executive Summary
This paper introduces PAGAR (Protagonist Antagonist Guided Adversarial Reward), a semi-supervised reward design paradigm that addresses the fundamental identifiability problem in inverse reinforcement learning (IRL). PAGAR trains a protagonist policy to perform well under a set of adversarial reward functions that maximize the performance gap with an antagonist policy. The method combines GAN-based IRL algorithms like GAIL and VAIL with a novel on-and-off policy approach that maximizes sample efficiency. Theoretical analysis shows PAGAR can guarantee task success under certain conditions while experimental results on complex continuous control and partially observable navigation tasks demonstrate superior training efficiency and zero-shot transfer capabilities compared to baseline methods.

## Method Summary
PAGAR operates by maintaining two policies simultaneously: a protagonist policy being trained and an antagonist policy used to challenge it. Both policies interact with the environment while a reward function is learned via GAN-based IRL methods. The key innovation is that the protagonist is trained to perform well across multiple adversarial reward functions that are optimized to create maximum performance gaps between the protagonist and antagonist. This forces the protagonist to be robust to reward misalignment. The method uses an on-and-off policy approach where both policies' samples are utilized for training, with the protagonist trained on-policy while also incorporating off-policy samples from the antagonist. This framework is combined with existing IRL algorithms like GAIL and VAIL to create a more sample-efficient imitation learning system.

## Key Results
- PAGAR-based IL achieves higher training efficiency and performance compared to state-of-the-art IRL baselines (GAIL, VAIL) on complex continuous control tasks
- The method enables zero-shot transfer learning from demonstrations to modified environments without additional training
- PAGAR demonstrates superior sample efficiency through its on-and-off policy approach that maximally utilizes trajectories from both protagonist and antagonist policies

## Why This Works (Mechanism)

### Mechanism 1
PAGAR addresses the identifiability problem in IRL by training a policy to perform well across a set of adversarial reward functions. The protagonist policy is trained using reward functions that maximize the performance gap between the protagonist and an antagonist policy, forcing robustness across the space of possible reward functions. The core assumption is that the expert policy performs well under all reward functions learned via IRL, so training to perform well across these rewards will lead to task success. This mechanism could break if the set of reward functions does not contain the true expert reward function or if the reward functions are too sparse to cover relevant task space.

### Mechanism 2
The on-and-off policy approach maximizes sample efficiency by utilizing trajectories from both protagonist and antagonist policies. PAGAR uses samples from both policies to update both the policy and reward function, with the protagonist trained on-policy while also using off-policy samples from the antagonist. The core assumption is that samples from the antagonist policy provide useful information about the reward function that can improve protagonist training. This mechanism could fail if the antagonist policy becomes too far from optimal, making its samples misleading for protagonist training.

### Mechanism 3
PAGAR guarantees task success under certain conditions by selecting policies that don't fail on any task-specifying reward function. The method solves a minimax regret problem that selects policies with the lowest worst-case regret across all reward functions, ensuring the policy performs well even on task-specifying reward functions. The core assumption is that the conditions in Theorem 4.3 are satisfied, ensuring there exists a policy that succeeds on all task-specifying reward functions and the reward functions are well-behaved. This guarantee may not hold if the theoretical conditions are not satisfied in practice.

## Foundational Learning

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: PAGAR builds upon IRL by addressing its identifiability problem. Understanding how IRL infers reward functions from demonstrations is crucial for understanding PAGAR's approach.
  - Quick check question: What is the fundamental identifiability problem in IRL, and why does it occur?

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: PAGAR uses GAN-based IRL algorithms (GAIL, VAIL) as building blocks. Understanding the connection between IRL and GANs is important for implementing PAGAR.
  - Quick check question: How do GAN-based IRL algorithms represent the reward function using a discriminator?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: PAGAR operates in the MDP framework, and understanding state transitions, policies, and value functions is necessary for implementing the algorithms.
  - Quick check question: What is the difference between the soft value function Vπ(s) and the soft Q-value function Qπ(s,a) in entropy-regularized RL?

## Architecture Onboarding

- Component map: Expert demonstrations -> IRL algorithm (GAIL/VAIL) -> Reward function -> Protagonist policy (trained) + Antagonist policy (challenger) -> Environment interactions -> Samples for training

- Critical path: 1) Initialize policies πP and πA, and reward function r 2) Sample trajectories from both policies 3) Update πA to maximize reward under current r 4) Update πP using both on-policy and off-policy samples 5) Update r using PAGAR objective and IRL objective 6) Repeat until convergence

- Design tradeoffs: Using more complex IRL algorithms may improve reward quality but increase computational cost; balancing exploration vs exploitation in policy updates affects sample efficiency; choice of hyperparameters (λ, β, etc.) significantly impacts performance

- Failure signatures: Protagonist policy performance plateaus or degrades; reward function becomes unstable or diverges; antagonist policy becomes too weak or too strong relative to protagonist; training efficiency decreases compared to baseline methods

- First 3 experiments: 1) Implement PAGAR with GAIL on HalfCheetah-v2 and compare sample efficiency to GAIL alone 2) Test PAGAR's robustness by intentionally misaligning reward functions and observing protagonist performance 3) Evaluate zero-shot transfer by training on one environment and testing on a modified version (e.g., SimpleCrossing with additional walls)

## Open Questions the Paper Calls Out

### Open Question 1
How does PAGAR's performance scale with increasing task complexity beyond the tested benchmarks? The paper demonstrates PAGAR's effectiveness on specific Mujoco and Mini-Grid tasks but does not explore its performance on more complex or diverse environments. Testing PAGAR on a wider range of complex tasks, including those with higher-dimensional state and action spaces, and comparing its performance to other IRL-based methods would resolve this.

### Open Question 2
What is the impact of different reward function representations (e.g., linear vs. neural network) on PAGAR's effectiveness? The paper mentions that IRL suffers from identifiability issues and that PAGAR can handle unidentifiable reward functions, but does not explore how different reward function representations affect PAGAR's performance. Conducting experiments with different reward function representations (e.g., linear, RBF networks) and comparing PAGAR's performance across these representations would resolve this.

### Open Question 3
How sensitive is PAGAR to the choice of hyperparameters, such as the Lagrangian parameter λ and the penalty coefficient β? The paper mentions that the choice of hyperparameters can affect PAGAR's performance and provides some guidance on selecting these parameters, but does not provide a comprehensive analysis of PAGAR's sensitivity to hyperparameter choices. Conducting a sensitivity analysis of PAGAR's performance to different hyperparameter values and providing guidelines for selecting optimal hyperparameters for different tasks would resolve this.

## Limitations
- The theoretical guarantees rely on strong assumptions about reward functions and task structure that may not hold in practice
- The fundamental identifiability problem in IRL remains - PAGAR can mitigate but not eliminate this issue
- Computational cost is significantly higher than standard IRL due to maintaining two policies and more complex reward optimization

## Confidence
- **High confidence**: PAGAR improves training efficiency compared to baseline IRL methods (supported by experimental results)
- **Medium confidence**: PAGAR guarantees task success under the stated theoretical conditions (requires validation of assumptions in practice)
- **Medium confidence**: The on-and-off policy approach maximizes sample efficiency (empirical evidence provided but theoretical analysis limited)

## Next Checks
1. Systematically test the conditions in Theorem 4.3 on various Mujoco tasks to determine how often they hold in practice and what happens when they don't
2. Compare PAGAR with and without antagonist policy samples to quantify the exact contribution of the on-and-off policy approach to sample efficiency gains
3. Test PAGAR-based IL on a wider range of environment modifications beyond what was presented to assess the limits of transfer learning capabilities