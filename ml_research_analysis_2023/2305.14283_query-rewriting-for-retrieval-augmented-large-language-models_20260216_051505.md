---
ver: rpa2
title: Query Rewriting for Retrieval-Augmented Large Language Models
arxiv_id: '2305.14283'
source_url: https://arxiv.org/abs/2305.14283
tags:
- language
- reader
- rewriter
- query
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retrieval-augmented framework for large
  language models (LLMs) by focusing on query rewriting. The proposed Rewrite-Retrieve-Read
  pipeline adds a trainable query rewriter to improve performance over standard retrieve-then-read
  approaches.
---

# Query Rewriting for Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2305.14283
- Source URL: https://arxiv.org/abs/2305.14283
- Reference count: 6
- Key outcome: Rewrite-Retrieve-Read pipeline improves EM and F1 scores across multiple open-domain and multi-choice QA tasks by adding a trainable query rewriter before frozen retriever and LLM reader

## Executive Summary
This paper introduces a retrieval-augmented framework for large language models (LLMs) by focusing on query rewriting. The proposed Rewrite-Retrieve-Read pipeline adds a trainable query rewriter to improve performance over standard retrieve-then-read approaches. The rewriter is first trained using pseudo data to generate queries that yield correct answers, then further optimized via reinforcement learning using LLM feedback. Experiments on open-domain QA (HotpotQA, AmbigNQ, PopQA) and multi-choice QA (gaokao, JEC-QA, MMLU) show consistent performance gains, with improvements in exact match (EM) and F1 scores across all tasks. For example, on HotPotQA, the rewrite-retrieve-read approach achieves 33.70 EM and 44.87 F1, outperforming both standard retrieve-then-read and LLM-only query rewriting baselines.

## Method Summary
The method introduces a trainable query rewriter (small language model) into the retrieval-augmented pipeline, creating a three-stage process: Rewrite → Retrieve → Read. The rewriter is first trained using pseudo data generated by prompting ChatGPT to rewrite questions and filtering for correct answers. This is followed by reinforcement learning fine-tuning using Proximal Policy Optimization (PPO) with rewards based on the LLM reader's performance (EM and F1 scores). The retriever uses Bing search API for real-time web search, and the reader uses ChatGPT with standard prompt formats. The approach is evaluated on six QA datasets, showing consistent improvements over baselines including retrieve-then-read and LLM-only query rewriting.

## Key Results
- On HotPotQA, rewrite-retrieve-read achieves 33.70 EM and 44.87 F1, outperforming retrieve-then-read (32.62 EM, 43.66 F1) and LLM rewriter (33.24 EM, 44.07 F1)
- Across all six QA tasks (HotpotQA, AmbigNQ, PopQA, gaokao, JEC-QA, MMLU), the approach consistently improves both EM and F1 scores
- Reinforcement learning fine-tuning of the rewriter provides additional gains over supervised training alone
- The pipeline shows particular effectiveness on multi-choice QA tasks (gaokao, JEC-QA, MMLU) compared to open-domain QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rewriting queries before retrieval improves retrieval-augmented LLM performance by reducing the gap between user queries and the knowledge needed for correct answers.
- Mechanism: A trainable query rewriter (small language model) is added before the retriever. It generates new queries that better align with the frozen retriever and LLM reader, improving the relevance of retrieved documents.
- Core assumption: The original query from the user or dataset is often suboptimal for retrieval, especially in real-world scenarios where queries are complex or ambiguous.
- Evidence anchors:
  - [abstract] "the original query can not be always optimal to retrieve for the LLM, especially in the real world."
  - [section] "Our approach pay attention of the query adaptation. Because the original query can not be always optimal to retrieve for the LLM, especially in the real world."
- Break condition: If the rewriter fails to generate queries that improve retrieval relevance, or if the cost of rewriting outweighs the retrieval gains.

### Mechanism 2
- Claim: Fine-tuning the query rewriter via reinforcement learning using LLM reader feedback leads to better alignment between the rewritten queries and the LLM's needs.
- Mechanism: The rewriter is first warmed up with pseudo data (queries that led to correct answers), then further trained with reinforcement learning. The reward is based on the LLM reader's performance (EM and F1 scores), encouraging the rewriter to generate queries that lead to better answers.
- Core assumption: The LLM reader's performance on a given query-document pair is a reliable signal for whether the query is well-formed for retrieval.
- Evidence anchors:
  - [abstract] "The rewriter is trained using the feedback of the LLM reader by reinforcement learning."
  - [section] "We follow the reinforcement learning scheme and use the reader feedback to tune the rewriter."
- Break condition: If the reward signal from the LLM reader is noisy or unreliable, the reinforcement learning may not converge to useful rewriter policies.

### Mechanism 3
- Claim: Using a web search engine as the retriever (instead of a dense retriever) allows for more flexible, real-time knowledge access and easier deployment.
- Mechanism: The pipeline uses Bing search API to retrieve documents based on rewritten queries, avoiding the need for maintaining a FAISS index or training a dense retriever.
- Core assumption: Real-time web search can provide relevant, up-to-date documents for the LLM reader, and the retrieval quality is sufficient for the task.
- Evidence anchors:
  - [section] "As the retriever we use a web search engine. It requires no FAISS index construction like dense retriever, nor candidates like training set. But it allows for a wide knowledge scope and real-time factuality."
- Break condition: If the web search engine's retrieval quality degrades, or if the latency or cost of web search becomes prohibitive.

## Foundational Learning

- Concept: Sequence-to-sequence language model fine-tuning
  - Why needed here: The rewriter is a small language model (e.g., T5) that needs to be fine-tuned to generate better queries. Understanding how to fine-tune such models is essential.
  - Quick check question: What is the difference between supervised fine-tuning and reinforcement learning fine-tuning for a language model?

- Concept: Reinforcement learning with policy gradient methods
  - Why needed here: The rewriter is further trained using reinforcement learning (PPO) with rewards based on the LLM reader's performance. Understanding policy gradient methods is crucial.
  - Quick check question: How does Proximal Policy Optimization (PPO) differ from vanilla policy gradient methods, and why is it preferred here?

- Concept: In-context learning with large language models
  - Why needed here: The LLM reader (ChatGPT) uses few-shot in-context learning to answer questions based on retrieved documents. Understanding how to construct effective prompts is important.
  - Quick check question: What are the key factors that influence the performance of in-context learning in LLMs, and how can they be optimized?

## Architecture Onboarding

- Component map:
  - Input: User question
  - Rewriter (trainable): Generates rewritten query
  - Retriever (frozen): Bing search API
  - Reader (frozen): ChatGPT
  - Output: Answer
  - Training loop: Pseudo data generation, supervised fine-tuning, reinforcement learning

- Critical path: Question → Rewriter → Retriever → Reader → Answer
  - The rewriter is the only trainable component; the retriever and reader are frozen.

- Design tradeoffs:
  - Web search vs. dense retriever: Web search offers real-time, flexible access but may be slower and more expensive; dense retrievers are faster but require index maintenance.
  - Small vs. large rewriter: A smaller rewriter is faster and cheaper to fine-tune but may have limited capacity to generate high-quality queries.

- Failure signatures:
  - If EM/F1 scores do not improve with query rewriting, the rewriter may not be generating useful queries.
  - If the pipeline is slow, the web search or rewriter may be the bottleneck.
  - If the rewriter overfits to the training set, it may not generalize to new queries.

- First 3 experiments:
  1. Baseline: Run the pipeline with no query rewriting (direct question to retriever).
  2. Frozen LLM rewriter: Use ChatGPT to rewrite queries (few-shot prompting) and measure performance gain.
  3. Trainable rewriter: Fine-tune a small language model on pseudo data and measure performance gain over the frozen LLM rewriter.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed Rewrite-Retrieve-Read pipeline scale with the size of the query rewriter model?
- Basis in paper: [inferred] The paper mentions that the rewriter is a small language model, but does not explore the impact of its size on the performance of the pipeline.
- Why unresolved: The paper does not provide any experiments or analysis on the effect of varying the size of the rewriter model on the performance of the pipeline.
- What evidence would resolve it: Experiments comparing the performance of the pipeline with different sizes of the rewriter model would help determine the optimal size for the rewriter.

### Open Question 2
- Question: How does the performance of the proposed pipeline compare to other retrieval-augmented methods that use a frozen LLM as the reader, such as RePlug?
- Basis in paper: [explicit] The paper mentions RePlug as a related work, but does not compare the performance of the proposed pipeline to RePlug.
- Why unresolved: The paper does not provide any experimental results comparing the performance of the proposed pipeline to RePlug or other similar methods.
- What evidence would resolve it: Experiments comparing the performance of the proposed pipeline to RePlug and other similar methods would help determine the relative effectiveness of the proposed approach.

### Open Question 3
- Question: How does the performance of the proposed pipeline vary with different web search engines as the retriever?
- Basis in paper: [explicit] The paper mentions that Bing search engine is used as the retriever, but does not explore the impact of using different search engines on the performance of the pipeline.
- Why unresolved: The paper does not provide any experiments or analysis on the effect of using different web search engines as the retriever on the performance of the pipeline.
- What evidence would resolve it: Experiments comparing the performance of the pipeline with different web search engines as the retriever would help determine the impact of the choice of search engine on the pipeline's performance.

## Limitations

- Evaluation completeness: Only reports results on single dataset splits without cross-validation or statistical significance testing
- Implementation dependencies: Relies on black-box commercial services (Bing API and ChatGPT) creating reproducibility barriers
- Prompt template secrecy: Does not fully disclose the prompt templates used for rewriter and reader, critical for reproducibility

## Confidence

**High Confidence**: The core architectural claim that adding a trainable query rewriter before a frozen retriever-reader pipeline can improve performance is well-supported by consistent experimental results across six different QA tasks.

**Medium Confidence**: The specific claim that reinforcement learning with LLM feedback is necessary for optimal performance. While results show improvement from RL fine-tuning, the magnitude and necessity are unclear without ablation studies.

**Low Confidence**: The practical deployment claims regarding latency and cost are not substantiated. The paper mentions "lower latency" and "no need to maintain an index" but provides no quantitative comparisons.

## Next Checks

1. **Statistical Significance Testing**: Run bootstrap resampling on the reported EM/F1 scores to establish 95% confidence intervals and determine whether performance improvements are statistically significant rather than random variation.

2. **Prompt Ablation Study**: Systematically vary the prompt templates used for both the rewriter and reader (e.g., changing shot counts, instructions, format) to quantify the contribution of prompt engineering versus the trainable rewriter architecture.

3. **Retrieval Quality Analysis**: Measure document recall and precision at different ranks when using rewritten queries versus original queries to determine whether improvements stem from better retrieval or better reader performance given the same documents.