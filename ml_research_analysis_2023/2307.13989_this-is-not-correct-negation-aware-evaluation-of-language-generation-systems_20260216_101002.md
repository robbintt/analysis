---
ver: rpa2
title: This is not correct! Negation-aware Evaluation of Language Generation Systems
arxiv_id: '2307.13989'
source_url: https://arxiv.org/abs/2307.13989
tags:
- negation
- sentence
- evaluation
- metrics
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of insensitivity to negations
  in automatic evaluation metrics for natural language generation. The authors propose
  NegBLEURT, a negation-aware version of the BLEURT evaluation metric, and NegMPNet,
  a negation-aware sentence transformer.
---

# This is not correct! Negation-aware Evaluation of Language Generation Systems

## Quick Facts
- arXiv ID: 2307.13989
- Source URL: https://arxiv.org/abs/2307.13989
- Reference count: 14
- Primary result: NegBLEURT achieves 0.72 Spearman correlation on negated sentences while maintaining general evaluation capability

## Executive Summary
This paper addresses a critical limitation in automatic evaluation metrics for natural language generation: insensitivity to negations. The authors propose NegBLEURT and NegMPNet, negation-aware versions of existing evaluation metrics, trained on a new dataset (CANNOT) containing negated and meaning-preserving paraphrased sentence pairs. Through systematic fine-tuning, these models significantly outperform existing metrics on negated sentences while preserving their general evaluation capabilities. The work demonstrates that negation sensitivity can be effectively incorporated into evaluation metrics without sacrificing their broader utility.

## Method Summary
The authors created the CANNOT dataset using a rule-based sentence negation tool that adds and removes negation cues at the sentence level. They fine-tuned BLEURT and MPNet models on this dataset using a multiple-negatives ranking loss to increase the latent distance between correctly paraphrased and negated samples. The negation tool focuses on verbal negations and creates diverse examples through systematic negation addition and removal. The fine-tuned models were evaluated on various benchmarks including the CANNOT-WMT test set and MTEB benchmarks to verify both negation sensitivity and preservation of general evaluation capabilities.

## Key Results
- NegBLEURT achieves 0.72 Spearman correlation with human ratings on the CANNOT-WMT test set, significantly outperforming baseline metrics
- NegMPNet shows 0.65 correlation on the same test set while maintaining performance across multiple MTEB tasks
- Both models significantly outperform existing metrics on negated sentences while preserving base model performance on other perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BLEURT on negation-aware data improves sensitivity to negations without harming general evaluation capability
- Mechanism: The CANNOT dataset contains both negated and meaning-preserving paraphrased sentence pairs, allowing the model to learn the semantic difference between them while retaining knowledge of other perturbations
- Core assumption: The model can learn negation sensitivity without catastrophic forgetting of its original evaluation capabilities
- Evidence anchors:
  - [abstract]: "Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations."
  - [section 5.1]: "NegMPNet outperforms its base model in the classification and summarization tasks but shows a decreased performance for clustering, pair classification, and retrieval"
  - [corpus]: Weak - the corpus shows related work on negation but doesn't directly address this mechanism
- Break condition: If the model catastrophically forgets its original evaluation capabilities or overfits to negation detection

### Mechanism 2
- Claim: The rule-based negation tool creates diverse negation examples that improve model robustness
- Mechanism: The tool systematically adds and removes negation cues at the sentence level, creating varied examples that teach the model to recognize different forms of negation
- Core assumption: Simple verbal negations capture the essential patterns needed for negation awareness
- Evidence anchors:
  - [section 3.1]: "Our negation tool focuses on verbal negations and supports the addition and deletion of negation cues on a sentence level"
  - [section 3.2]: "NegBLEURT matches the performance of its BLEURT base model (Sellam et al., 2020) and is sensitive to word drops and repetitions but unaware of word swaps"
  - [corpus]: Moderate - corpus shows related negation detection work but doesn't directly address this tool's effectiveness
- Break condition: If the tool fails to handle complex negation forms or creates misleading examples

### Mechanism 3
- Claim: Multiple-negatives ranking loss effectively separates negated from non-negated sentence pairs
- Mechanism: The loss function increases the latent distance between correctly paraphrased and negated samples, forcing the model to distinguish between them
- Core assumption: The multiple-negatives ranking loss can effectively separate semantically different sentence pairs
- Evidence anchors:
  - [section 4.1]: "We utilized a multiple negatives ranking loss to increase the latent distance between correctly paraphrased and negated samples"
  - [section 5.1]: "When averaging the performances among all tasks, both models perform equally"
  - [corpus]: Weak - corpus doesn't directly address this specific loss function
- Break condition: If the loss function fails to create meaningful separation between sentence types

## Foundational Learning

- Concept: Sentence embedding models
  - Why needed here: Understanding how sentence transformers create semantic representations is crucial for understanding why negation awareness is challenging
  - Quick check question: What's the difference between sentence embeddings and token-level embeddings in BERT?

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: The paper addresses this issue by showing that negation fine-tuning doesn't harm other capabilities
  - Quick check question: What techniques can prevent catastrophic forgetting when fine-tuning on new data?

- Concept: Evaluation metric design
  - Why needed here: Understanding how BLEURT and other metrics work helps explain why they might struggle with negations
  - Quick check question: How does BLEURT's regression layer differ from simple cosine similarity?

## Architecture Onboarding

- Component map: Rule-based negation tool → CANNOT dataset → Model fine-tuning → Evaluation benchmarks
- Critical path: Rule-based negator → CANNOT dataset creation → Model fine-tuning → Benchmark evaluation
- Design tradeoffs:
  - Simple verbal negations vs. complex negation forms
  - Dataset size vs. diversity
  - Negation awareness vs. general evaluation capability
- Failure signatures:
  - High sensitivity to negations but poor performance on other perturbations
  - Low sensitivity to negations despite fine-tuning
  - Catastrophic forgetting of original evaluation capabilities
- First 3 experiments:
  1. Test rule-based negator on diverse sentence types to verify correct negation
  2. Evaluate fine-tuned models on CANNOT test set to confirm negation sensitivity
  3. Compare performance on MTEB benchmarks to ensure no catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the negation tool perform on complex negation structures beyond verbal negation, such as adverbial or nominal negation?
- Basis in paper: [explicit] The paper mentions limitations of the rule-based negation tool, stating it "fails for cases that do not match our defined sentence structure" and "there are special cases like the sentence 'She's determined' that could use both the verbs 'is' and 'has'."
- Why unresolved: The paper only evaluates the negation tool on verbal negations and does not provide any results or analysis for other types of negation structures.
- What evidence would resolve it: Evaluating the negation tool on a diverse set of negation structures and comparing its performance to human annotations would provide evidence of its effectiveness on complex negations.

### Open Question 2
- Question: Can the negation-aware models be effectively applied to multilingual natural language generation evaluation?
- Basis in paper: [inferred] The paper mentions extending the negation dataset to be multilingual but does not provide any results or analysis on the performance of the negation-aware models on multilingual tasks.
- Why unresolved: The paper only evaluates the negation-aware models on English benchmarks and does not provide any results or analysis on their performance on multilingual tasks.
- What evidence would resolve it: Evaluating the negation-aware models on multilingual benchmarks and comparing their performance to other multilingual evaluation metrics would provide evidence of their effectiveness on multilingual tasks.

### Open Question 3
- Question: How does the performance of the negation-aware models compare to other state-of-the-art evaluation metrics on negation detection tasks?
- Basis in paper: [explicit] The paper mentions that NegBLEURT "clearly outperforms all metrics on the critical negation and antonym perturbations" but does not provide a comprehensive comparison to other state-of-the-art evaluation metrics.
- Why unresolved: The paper only compares the negation-aware models to their base versions and does not provide a comprehensive comparison to other state-of-the-art evaluation metrics.
- What evidence would resolve it: Conducting a comprehensive comparison of the negation-aware models to other state-of-the-art evaluation metrics on negation detection tasks would provide evidence of their effectiveness relative to other metrics.

## Limitations

- The rule-based negation tool has limited coverage of complex negation structures beyond simple verbal negation
- Dataset composition and domain shift handling between combined sources are not fully specified
- Scalability to languages with different negation structures remains unproven

## Confidence

**High Confidence:** The fundamental observation that existing evaluation metrics struggle with negations is well-established. The correlation improvements on the CANNOT-WMT test set (0.72 for NegBLEURT vs baseline) provide strong evidence for the core claim that negation-aware fine-tuning improves performance on negated sentences.

**Medium Confidence:** The claim that negation fine-tuning preserves performance on other perturbations is supported by the evidence, but the decrease in performance for clustering, pair classification, and retrieval tasks with NegMPNet suggests this may not hold universally across all evaluation scenarios.

**Low Confidence:** The scalability of the rule-based negation approach to languages other than English or to specialized domains (legal, medical, technical) remains unproven.

## Next Checks

1. **Cross-linguistic validation:** Test NegBLEURT on negated sentence pairs in languages with different negation structures (e.g., Romance languages with double negation patterns) to assess cross-linguistic generalization.

2. **Complex negation stress test:** Create a benchmark with sentences containing implicit negations, double negatives, and negation through antonym substitution to evaluate whether the models maintain sensitivity to complex negation forms.

3. **Long-term stability evaluation:** Fine-tune the models on CANNOT data, then evaluate performance after 2-3 months of storage to assess whether the negation awareness degrades over time, indicating potential overfitting.