---
ver: rpa2
title: 'Mind the Gap: Improving Success Rate of Vision-and-Language Navigation by
  Revisiting Oracle Success Routes'
arxiv_id: '2308.03244'
source_url: https://arxiv.org/abs/2308.03244
tags:
- navigation
- target
- conf
- success
- comput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the long-standing issue of the performance
  gap between Success Rate (SR) and Oracle Success Rate (OSR) in Vision-and-Language
  Navigation (VLN). Unlike existing methods that focus on predicting actions at each
  step, the authors propose a novel approach to mine the target location from trajectories
  generated by off-the-shelf VLN models.
---

# Mind the Gap: Improving Success Rate of Vision-and-Language Navigation by Revisiting Oracle Success Routes

## Quick Facts
- arXiv ID: 2308.03244
- Source URL: https://arxiv.org/abs/2308.03244
- Reference count: 40
- One-line primary result: Reduces SR-OSR gap by up to 4% across multiple VLN methods

## Executive Summary
This paper addresses the persistent gap between Success Rate (SR) and Oracle Success Rate (OSR) in Vision-and-Language Navigation tasks. The authors observe that agents often pass the target location but fail to stop, leading to high OSR but low SR. Instead of predicting actions at each step, they propose a novel trajectory grounding approach that identifies the target location from trajectories generated by baseline VLN models. Their multi-module transformer architecture learns discriminative trajectory representations and predicts the confidence of being at the target location, significantly improving SR while narrowing the SR-OSR gap.

## Method Summary
The method formulates VLN as a trajectory grounding task rather than step-by-step action prediction. It constructs new training trajectories by sampling viewpoints within 3 meters of the target from baseline model outputs and connecting them through sub-paths. A multi-module transformer-based model processes these trajectories: the Cross-Modality Elevation Transformer fuses information across visual elevations with instruction awareness, the Spatial-Temporal Transformer exchanges information across views and time steps, and the Target Selection Transformer uses learnable queries to predict target confidence. The model is trained using a combination of Focal Loss and Dice Loss on three benchmark datasets (R2R, REVERIE, NDH).

## Key Results
- Reduces SR-OSR gap by up to 4% across multiple state-of-the-art VLN methods
- Achieves significant SR improvements on R2R, REVERIE, and NDH datasets
- Demonstrates effectiveness across discrete (Matterport3D) and continuous (HM3D-AutoVLN) environments
- Shows improvements when combined with various baseline VLN models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The large SR-OSR gap stems from agents passing the target but failing to stop
- Mechanism: By identifying the target location from the trajectory, the method enables agents to stop at the target
- Core assumption: The target location appears in the trajectory generated by baseline VLN models
- Evidence anchors: Consistently large gap (up to 9%) observed on four state-of-the-art VLN methods across R2R and REVERIE

### Mechanism 2
- Claim: The multi-module transformer learns discriminative trajectory representations
- Mechanism: Cross-Modality Elevation Transformer fuses information across elevations with instruction awareness; Spatial-Temporal Transformer exchanges information across views and steps
- Core assumption: Transformer modules can effectively fuse multimodal information and identify the target location
- Evidence anchors: Design focuses on learning compact discriminative trajectory viewpoint representations for target confidence prediction

### Mechanism 3
- Claim: The method significantly improves SR while reducing the SR-OSR gap
- Mechanism: By accurately identifying the target location, the agent can stop at the correct position
- Core assumption: The identified target location is accurate enough for successful navigation
- Evidence anchors: Results show significant improvements in SR, reducing the gap between SR and OSR by up to 4% across multiple methods

## Foundational Learning

- Concept: Vision-and-Language Navigation (VLN)
  - Why needed here: The paper addresses the SR-OSR gap issue in VLN tasks
  - Quick check question: What is the difference between SR and OSR in VLN?

- Concept: Transformer-based models
  - Why needed here: The method uses a multi-module transformer-based model to learn discriminative trajectory representations
  - Quick check question: What are the main components of the transformer-based model used in this paper?

- Concept: Trajectory grounding
  - Why needed here: The paper formulates VLN as a trajectory grounding task to identify the target location
  - Quick check question: How does trajectory grounding differ from step-by-step action prediction in VLN?

## Architecture Onboarding

- Component map: Text Encoder -> Vision Encoder -> Cross-Modality Elevation Transformer -> Spatial-Temporal Transformer -> Target Selection Transformer -> Prediction

- Critical path: The instruction flows through the Text Encoder, while the trajectory flows through the Vision Encoder, with both streams merging at the Cross-Modality Elevation Transformer and proceeding through the Spatial-Temporal Transformer to the Target Selection Transformer for final prediction

- Design tradeoffs: Trades the complexity of step-by-step action prediction for the simplicity of trajectory grounding, potentially improving SR and reducing the SR-OSR gap

- Failure signatures: If the method fails to identify the target location accurately, it may lead to unsuccessful navigation and no improvement in SR or reduction in the SR-OSR gap

- First 3 experiments:
  1. Implement the Text Encoder and Vision Encoder to encode the instruction and trajectory
  2. Implement the Cross-Modality Elevation Transformer to fuse information across elevations with instruction awareness
  3. Implement the Spatial-Temporal Transformer to exchange information across views and steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the trajectory grounding model change when using different vision encoders (e.g., ResNet, ViT-Small, ViT-Base) on the R2R, REVERIE, and NDH datasets?
- Basis in paper: The paper uses ViT-B/16 pre-trained on ImageNet for vision encoding, but does not explore the impact of different vision encoders on performance
- Why unresolved: The paper does not provide a comparison of different vision encoders, making it unclear how much the choice of encoder affects the trajectory grounding performance
- What evidence would resolve it: Conducting experiments with different vision encoders (e.g., ResNet, ViT-Small, ViT-Base) and comparing their performance on the R2R, REVERIE, and NDH datasets would provide insights into the impact of vision encoder choice

### Open Question 2
- Question: How does the trajectory grounding model perform when applied to continuous simulators, such as the Habitat simulator, compared to discrete simulators like Matterport3D?
- Basis in paper: The paper focuses on discrete simulators and does not explore the model's performance on continuous simulators
- Why unresolved: The paper does not provide any results or comparisons for continuous simulators, making it unclear how well the trajectory grounding approach generalizes to different simulator types
- What evidence would resolve it: Evaluating the trajectory grounding model on continuous simulators, such as the Habitat simulator, and comparing the results to those obtained on discrete simulators would demonstrate the model's adaptability to different environments

### Open Question 3
- Question: What is the impact of using different loss functions (e.g., BCE, Focal, Dice) and their combinations on the trajectory grounding model's performance across various VLN tasks?
- Basis in paper: The paper discusses the use of BCE, Focal, and Dice loss functions and their combinations but does not provide a comprehensive comparison of their impact on performance
- Why unresolved: The paper presents ablation studies on loss functions but does not explore the full range of possible combinations and their effects on different VLN tasks
- What evidence would resolve it: Conducting extensive experiments with various loss function combinations (e.g., BCE + Dice, Focal + Dice, BCE + Focal + Dice) and comparing their performance across R2R, REVERIE, and NDH tasks would provide insights into the optimal loss function configuration

### Open Question 4
- Question: How does the trajectory grounding model's performance vary with different hyperparameters (e.g., learning rates, batch sizes, number of transformer layers) on the R2R, REVERIE, and NDH datasets?
- Basis in paper: The paper provides implementation details and some hyperparameter settings but does not explore the impact of varying these parameters on model performance
- Why unresolved: The paper does not present a comprehensive hyperparameter sensitivity analysis, making it unclear how different settings affect the trajectory grounding model's performance
- What evidence would resolve it: Conducting a thorough hyperparameter sensitivity analysis by varying parameters such as learning rates, batch sizes, and the number of transformer layers, and evaluating their impact on the R2R, REVERIE, and NDH datasets would provide insights into the optimal model configuration

## Limitations

- The method relies on trajectories generated by baseline VLN models, which may introduce bias if these models consistently fail to approach the target from certain angles or distances
- The exact sampling strategy for constructing training trajectories is not fully specified, potentially affecting reproducibility and performance
- The attention mechanism details in the Spatial-Temporal Transformer are not completely described, making exact reproduction challenging

## Confidence

- High confidence: The core observation that SR-OSR gap exists in VLN tasks and that the gap is primarily due to agents passing the target without stopping
- Medium confidence: The effectiveness of the multi-module transformer architecture in learning discriminative trajectory representations
- Low confidence: The generalizability of the trajectory construction approach to datasets with different characteristics or navigation environments with fewer viewpoints per location

## Next Checks

1. **Ablation study of trajectory construction**: Systematically vary the number of viewpoints sampled from the positive set (0, 1, 2, or multiple) and evaluate how this affects model performance to understand the sensitivity of the method to this design choice

2. **Attention mechanism validation**: Implement a simplified version of the Spatial-Temporal Transformer with different attention strategies (viewpoint-to-viewpoint, temporal-only, spatial-only) to isolate the impact of the "expanded spatial attention" approach

3. **Cross-dataset generalization test**: Train the model on R2R and evaluate on REVERIE and NDH without fine-tuning to assess how well the learned trajectory representations transfer across datasets with different visual characteristics and instruction styles