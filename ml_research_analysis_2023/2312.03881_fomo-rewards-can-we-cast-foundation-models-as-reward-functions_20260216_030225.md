---
ver: rpa2
title: 'FoMo Rewards: Can we cast foundation models as reward functions?'
arxiv_id: '2312.03881'
source_url: https://arxiv.org/abs/2312.03881
tags:
- arxiv
- preprint
- learning
- task
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of foundation models as reward functions
  in reinforcement learning. The authors propose a pipeline that interfaces a vision
  model with a large language model to infer the likelihood of an instruction describing
  a task, given a trajectory of observations.
---

# FoMo Rewards: Can we cast foundation models as reward functions?

## Quick Facts
- arXiv ID: 2312.03881
- Source URL: https://arxiv.org/abs/2312.03881
- Reference count: 40
- Primary result: Proposed framework achieves higher likelihoods for correct behaviors compared to perturbed ones using contrastive or success detection protocols

## Executive Summary
This paper explores the use of foundation models as reward functions in reinforcement learning by interfacing a vision model with a large language model. The authors propose a pipeline that maps visual observations from trajectories to the LLM's embedding space, allowing the LLM to evaluate the likelihood of an instruction describing a given task. Through extensive qualitative analysis on systematically perturbed oracle policies, the framework demonstrates the ability to associate higher values with desired behavior and lower values with incorrect policies, meeting key criteria for an effective reward function.

## Method Summary
The proposed method involves learning an interface model that maps visual observations from trajectories to the embedding space of a pretrained large language model. Given a trajectory of observations and a task instruction, the pipeline computes the likelihood of the instruction given the trajectory, which serves as a reward signal. The interface model is trained using either maximum likelihood, contrastive learning, or success detection protocols on 270K offline trajectories from an oracle policy. The framework uses a pretrained ViT model for visual embeddings and an MPT 1B parameter model as the LLM, with a 1-layer decoder-only transformer as the interface.

## Key Results
- Contrastive and success detection training protocols outperform maximum likelihood, avoiding shortcut solutions
- Framework successfully assigns higher likelihoods to correct trajectories compared to perturbed ones
- Interface model trained with contrastive/success detection shows desirable behaviors expected from reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM likelihood of task description given trajectory can function as a dense reward signal
- Mechanism: By interfacing visual embeddings with LLM input space, the model can assess how well a trajectory aligns with a task description, producing higher scores for trajectories that match the instruction and lower scores for incorrect behaviors
- Core assumption: The LLM's input embedding space is "approximately grounded" such that visual representations can be transformed into a representation the LLM can process
- Evidence anchors:
  - [abstract] "Specifically, given a trajectory of observations, we infer the likelihood of an instruction describing the task that the user wants an agent to perform"
  - [section] "we show that this generic likelihood function exhibits the characteristics ideally expected from a reward function: it associates high values with the desired behaviour and lower values for several similar, but incorrect policies"
- Break condition: The grounding assumption fails if the interface model cannot adequately map visual representations to the LLM's embedding space, or if the LLM cannot reason about temporal sequences

### Mechanism 2
- Claim: Contrastive and success detection training protocols outperform maximum likelihood for interface training
- Mechanism: By explicitly training the interface to distinguish between positive and negative trajectories, the model learns to better differentiate correct from incorrect behaviors, avoiding shortcut solutions
- Core assumption: Training with explicit positive/negative pairs helps the model learn meaningful distinctions rather than learning to predict based on trajectory length or other spurious correlations
- Evidence anchors:
  - [abstract] "We perform extensive qualitative analysis by perturbing an oracle policy in systematic ways and show that our framework elicits desirable behaviors expected from a reward function"
  - [section] "For a subset of the tasks (e.g., T6 or Rotate), the pipeline struggles with assigning noticeably higher reward to the correct trajectory compared with the reverse or repeated one" under maximum likelihood
- Break condition: The contrastive or success detection protocols may still fail if the negative trajectories are not representative of actual errors or if the LLM cannot effectively reason about success/failure

### Mechanism 3
- Claim: The framework can handle both trajectory perturbations and instruction perturbations
- Mechanism: By evaluating likelihood under systematically altered trajectories and instructions, the framework demonstrates it can distinguish between correct and incorrect task specifications
- Core assumption: The LLM can reason about both what actions were taken and what objects/tasks were specified, allowing it to differentiate between correct execution and correct specification
- Evidence anchors:
  - [section] "Assigning a lower reward to this perturbed instructionâ€“trajectory pair, when compared to the correct one, indicates our pipeline yields a reward function that accounts for the precise physical specification"
  - [section] "We evaluate the likelihood of the instruction given these perturbed trajectories" and "we evaluate likelihood of the following perturbed instructions"
- Break condition: The framework fails if the LLM cannot maintain consistency between what actions were performed and what was specified, or if the perturbations are too subtle to detect

## Foundational Learning

- Concept: Vision-language grounding and cross-modal alignment
  - Why needed here: The framework relies on mapping visual observations to a representation space that an LLM can process, which requires understanding how vision models and language models can be aligned
  - Quick check question: What is the key assumption about the relationship between visual embeddings and LLM input embeddings that enables this framework to work?

- Concept: Contrastive learning and reward shaping
  - Why needed here: The contrastive training protocol explicitly uses positive and negative examples to shape the reward function, requiring understanding of how contrastive objectives work
  - Quick check question: How does the contrastive training protocol differ from maximum likelihood training in terms of what behaviors it encourages the interface to learn?

- Concept: Temporal reasoning and sequence modeling
  - Why needed here: The framework processes trajectories of observations over time, requiring the interface to handle temporal dependencies and sequence information
  - Quick check question: Why might a simple linear interface be insufficient for this task, and how does using a 1-layer transformer address this limitation?

## Architecture Onboarding

- Component map: VLM -> Interface Model (1-layer Transformer) -> LLM
- Critical path:
  1. Extract visual embeddings from trajectory frames using VLM
  2. Process visual embeddings through interface model to map to LLM space
  3. Construct context by combining interfaced trajectory with instruction prompt
  4. Evaluate instruction likelihood using LLM
  5. Use likelihood as reward signal for RL training
- Design tradeoffs:
  - Simple maximum likelihood vs. contrastive/success detection training: Simpler to implement but prone to shortcut solutions
  - Linear interface vs. transformer interface: Linear is simpler but may not capture temporal information effectively
  - Frozen VLM/LLM vs. fine-tuning: Frozen components are simpler but may limit performance if grounding is imperfect
- Failure signatures:
  - Similar rewards for correct and perturbed trajectories indicates shortcut learning
  - Very low rewards across all trajectories suggests poor grounding or interface mapping
  - Inconsistent rewards for similar behaviors suggests instability in the interface model
- First 3 experiments:
  1. Test likelihood scores for a simple correct trajectory vs. a trajectory with repeated frames to verify the model can distinguish between meaningful and meaningless sequences
  2. Evaluate the interface under maximum likelihood training on a small dataset to check for shortcut learning before moving to contrastive training
  3. Test the contrastive training protocol by comparing reward scores for correct vs. reversed trajectories on a simple pick-and-place task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using foundation models as reward functions on the sample efficiency and convergence speed of reinforcement learning algorithms compared to traditional reward modeling approaches?
- Basis in paper: [inferred] The paper proposes using foundation models as reward functions but does not evaluate RL policies trained using these rewards or compare their performance to traditional approaches.
- Why unresolved: The paper focuses on qualitative assessment and does not train RL policies from scratch or compare sample efficiency and convergence speed.
- What evidence would resolve it: Empirical results showing the sample efficiency and convergence speed of RL algorithms trained using foundation model rewards versus traditional reward modeling approaches.

### Open Question 2
- Question: How robust are foundation model-based reward functions to domain shifts and out-of-distribution scenarios compared to traditional reward modeling approaches?
- Basis in paper: [explicit] The paper mentions that existing learning-based systems, including reward modeling and imitation learning, often suffer from lack of robustness under domain shifts. However, it does not explicitly evaluate the robustness of foundation model-based rewards.
- Why unresolved: The paper does not provide empirical results or analysis on the robustness of foundation model-based reward functions to domain shifts or out-of-distribution scenarios.
- What evidence would resolve it: Experimental results comparing the performance of foundation model-based reward functions to traditional approaches under domain shifts and out-of-distribution scenarios.

### Open Question 3
- Question: What are the limitations and failure modes of using foundation models as reward functions, and how can they be mitigated?
- Basis in paper: [inferred] The paper acknowledges that the maximum likelihood training protocol can lead to shortcut solutions and yield similar results for both correct and perturbed trajectories. However, it does not provide a comprehensive analysis of the limitations and failure modes of the proposed framework.
- Why unresolved: The paper focuses on demonstrating the viability of the proposed framework and does not provide an in-depth analysis of its limitations and failure modes.
- What evidence would resolve it: A detailed analysis of the limitations and failure modes of foundation model-based reward functions, along with proposed mitigation strategies and empirical results demonstrating their effectiveness.

## Limitations
- Reliance on grounding assumption between visual embeddings and LLM input space may limit generalization to other domains
- 1-layer transformer interface may be insufficient for capturing longer temporal dependencies in more complex tasks
- Framework not evaluated on real-world tasks beyond VIMA(Uni) benchmark

## Confidence
- **High confidence**: The framework's ability to produce higher likelihood scores for correct trajectories compared to perturbed ones using contrastive/success detection protocols
- **Medium confidence**: The claim that maximum likelihood training suffers from shortcut solutions (supported by qualitative results but limited quantitative comparison)
- **Low confidence**: The scalability of this approach to more complex, real-world tasks beyond the VIMA(Uni) benchmark

## Next Checks
1. Test the framework on a dataset with varying trajectory lengths to assess whether the interface can handle longer sequences without degradation in performance
2. Implement ablation studies comparing the 1-layer transformer interface against both linear and deeper transformer variants to quantify the benefit of temporal modeling
3. Evaluate the framework's performance when using different vision backbone architectures (e.g., ConvNeXT vs. ViT) to assess sensitivity to the choice of visual encoder