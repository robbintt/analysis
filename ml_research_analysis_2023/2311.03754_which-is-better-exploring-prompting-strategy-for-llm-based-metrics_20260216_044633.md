---
ver: rpa2
title: Which is better? Exploring Prompting Strategy For LLM-based Metrics
arxiv_id: '2311.03754'
source_url: https://arxiv.org/abs/2311.03754
tags:
- summary
- prompt
- score
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically explores effective prompting strategies
  for open-source LLM-based metrics in NLG evaluation. The authors analyze prompt
  templates, score granularity, and in-context learning using human and model guidelines,
  finding that concise, clear instructions outperform complex ones.
---

# Which is better? Exploring Prompting Strategy For LLM-based Metrics

## Quick Facts
- arXiv ID: 2311.03754
- Source URL: https://arxiv.org/abs/2311.03754
- Authors: 
- Reference count: 40
- Primary result: Concise, clear instructions outperform complex ones in LLM-based NLG evaluation; fine-grained scoring yields better results than coarse-grained scoring

## Executive Summary
This paper systematically explores effective prompting strategies for open-source LLM-based metrics in natural language generation (NLG) evaluation. The authors analyze prompt templates, score granularity, and in-context learning using human and model guidelines, finding that concise, clear instructions outperform complex ones. Fine-grained scoring yields better results than coarse-grained scoring. Demonstration examples can introduce bias, with larger models benefiting more from rationales. Score aggregation methods are compared, with Direct and Logprob performing best. The study also examines explainability by generating rationales, identifying error types in model outputs. Experiments use datasets like SummEval, evaluating aspects like relevance, consistency, coherence, and fluency.

## Method Summary
The study employs four open-source LLMs (Hermes-13B, Orca-7B, Orca-13B, Platypus-70B) with various prompt templates and aggregation methods. Researchers implement Human Guideline (HG) and Model Guideline (MG) prompt templates with task descriptions, evaluation criteria, and evaluation steps. Three score aggregation methods (Direct, Logprob, Approximation) are tested alongside Rationale Generation prompts for explainability. Performance is evaluated using segment-level Kendall's Tau correlation with human annotations on SummEval and Eval4NLP datasets.

## Key Results
- Concise, clear instructions outperform complex ones in prompt templates
- Fine-grained scoring (individual aspects) yields better results than coarse-grained scoring
- Demonstration examples can introduce bias, with larger models benefiting more from rationales
- Direct and Logprob aggregation methods consistently demonstrate superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based metrics can approximate human evaluation quality when provided with explicit task descriptions and scoring criteria
- Mechanism: By giving the LLM structured instructions (task description, evaluation criteria, evaluation steps), it can mimic human judgment processes and apply consistent evaluation standards across samples
- Core assumption: The LLM has sufficient knowledge retained from pretraining to understand the evaluation criteria and apply them consistently
- Evidence anchors: [abstract]: "Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores"; [section 4.1.1]: "We propose Human Guideline (HG) prompt and Model Guideline (MG) prompt for summary evaluation as illustrated in Figure 1"

### Mechanism 2
- Claim: Fine-grained scoring provides more reliable evaluation signals than coarse-grained scoring
- Mechanism: Breaking evaluation into individual aspects (relevance, consistency, coherence, fluency) allows the LLM to focus on specific qualities rather than making holistic judgments, reducing ambiguity
- Core assumption: The LLM can effectively evaluate individual aspects independently without conflating them
- Evidence anchors: [abstract]: "determining the granularity of NLG quality scores"; [section 4.1.2]: "fine-grained scoring yields better results than coarse-grained scoring"

### Mechanism 3
- Claim: Score aggregation methods that incorporate token probabilities (Logprob) provide more continuous and nuanced evaluation scores than discrete scoring
- Mechanism: By weighting pre-defined discrete scores with their token generation probabilities, the model produces a weighted average that better reflects confidence in the evaluation
- Core assumption: Token generation probabilities correlate with the model's confidence in different score levels
- Evidence anchors: [section 4.2]: "Logprob multiplies each discrete score si by its token probability p(si)"; [section 5.2.2]: "Direct and Logprob aggregation consistently demonstrates superior performance"

## Foundational Learning

- Concept: In-context learning (ICL) capabilities of LLMs
  - Why needed here: The paper explores how demonstrated examples influence LLM-based evaluation performance
  - Quick check question: What factors determine how effectively an LLM can learn from in-context examples?

- Concept: Prompt engineering and template design
  - Why needed here: The paper systematically analyzes different prompt templates and their components
  - Quick check question: How do different prompt elements (task description, criteria, steps) affect LLM output quality?

- Concept: Correlation metrics for evaluation
  - Why needed here: The paper uses Kendall's Tau correlation to measure alignment with human annotations
  - Quick check question: Why is Kendall's Tau correlation appropriate for comparing ordinal human scores with model outputs?

## Architecture Onboarding

- Component map: Prompt Generator -> Example Curator -> Score Aggregator -> Rationale Generator -> Evaluator -> Filter/Binner
- Critical path: 1. Load dataset and preprocess 2. Generate prompts based on template variants 3. Run inference with different aggregation methods 4. Compute correlations with human annotations 5. Analyze results and generate insights
- Design tradeoffs:
  - Fine-grained vs coarse-grained scoring: More detail vs simpler implementation
  - Temperature settings: Exploration vs exploitation in score generation
  - Demonstration examples: Potential bias vs improved guidance
  - Open-source vs proprietary models: Accessibility vs performance
- Failure signatures:
  - Low correlation with human scores despite good prompt design
  - Inconsistent scores across similar examples
  - Model outputs that don't follow prompt instructions
  - Performance degradation with longer prompts
- First 3 experiments:
  1. Baseline comparison: Run with simple prompt vs HG prompt to verify prompt effectiveness
  2. Granularity test: Compare fine-grained vs coarse-grained scoring on same model
  3. Aggregation method comparison: Test Direct, Logprob, and Approximation on same prompt and model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of open-source LLM-based metrics compare to proprietary models like GPT-4 across different evaluation tasks beyond summarization?
- Basis in paper: [inferred] The paper focuses on open-source LLMs and their prompting strategies, noting that GPT-4-like models are used for comparison in some experiments, but does not provide a comprehensive comparison across multiple evaluation tasks
- Why unresolved: The study primarily examines open-source LLMs, leaving the comparative performance across various tasks with proprietary models unexplored
- What evidence would resolve it: Direct benchmarking of open-source and proprietary models across diverse NLG evaluation tasks, including translation, dialogue, and creative writing

### Open Question 2
- Question: What are the long-term effects of demonstration examples on the evaluation consistency of open-source LLMs?
- Basis in paper: [explicit] The paper observes that demonstration examples can introduce bias and affect performance, particularly noting differences between models with varying ICL capabilities
- Why unresolved: While short-term effects are analyzed, the paper does not investigate how repeated exposure to demonstrations influences evaluation consistency over time
- What evidence would resolve it: Longitudinal studies tracking evaluation performance with repeated demonstrations across multiple datasets and model versions

### Open Question 3
- Question: How do different fine-tuning approaches for open-source LLMs impact their effectiveness as evaluation metrics?
- Basis in paper: [inferred] The paper mentions fine-tuning as a potential approach for constructing evaluators but does not explore its impact on evaluation performance
- Why unresolved: The study focuses on prompting strategies without delving into how fine-tuning might enhance or alter evaluation capabilities
- What evidence would resolve it: Comparative analysis of evaluation performance between prompted and fine-tuned open-source LLMs across various tasks and datasets

## Limitations

- The study's findings are primarily based on four specific open-source LLMs and the SummEval dataset, limiting generalizability across different domains and languages
- The paper identifies that demonstration examples can introduce bias but doesn't fully characterize when and why this bias occurs across different model sizes and prompt complexities
- The effectiveness of identified prompt strategies may vary significantly across different evaluation tasks beyond summarization

## Confidence

- **High Confidence**: The superiority of concise, clear instructions over complex ones; the effectiveness of fine-grained scoring over coarse-grained scoring; the performance of Direct and Logprob aggregation methods
- **Medium Confidence**: The claim that larger models benefit more from rationales in demonstrations; the general applicability of identified prompt strategies across different NLG tasks
- **Low Confidence**: The assertion that token generation probabilities reliably indicate evaluation confidence; the generalizability of bias patterns introduced by demonstration examples

## Next Checks

1. **Cross-Domain Validation**: Test the identified optimal prompt strategies on non-summarization NLG tasks (e.g., dialogue generation, machine translation) to verify generalizability across different text generation domains

2. **Bias Characterization Study**: Systematically investigate the conditions under which demonstration examples introduce bias by varying example quality, model size, and prompt complexity, and develop guidelines for when to include or exclude demonstrations

3. **Multi-Aspect Consistency Analysis**: Evaluate whether fine-grained scoring maintains consistent correlations across all four aspects (relevance, consistency, coherence, fluency) or if certain aspects benefit more from this approach, potentially leading to imbalanced evaluation