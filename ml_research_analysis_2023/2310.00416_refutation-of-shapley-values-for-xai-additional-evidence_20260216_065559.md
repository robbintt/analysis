---
ver: rpa2
title: Refutation of Shapley Values for XAI -- Additional Evidence
arxiv_id: '2310.00416'
source_url: https://arxiv.org/abs/2310.00416
tags:
- feature
- values
- shapley
- features
- marques-silva
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends prior work refuting the use of Shapley values
  for explainable AI by examining multi-valued and discrete classifiers, not just
  boolean ones. It demonstrates that Shapley values can assign higher importance scores
  to irrelevant features than relevant ones, even when the relevant feature has zero
  importance.
---

# Refutation of Shapley Values for XAI -- Additional Evidence

## Quick Facts
- arXiv ID: 2310.00416
- Source URL: https://arxiv.org/abs/2310.00416
- Reference count: 9
- Primary result: Shapley values for XAI can assign higher importance to irrelevant features than relevant ones, even when relevant features have zero importance

## Executive Summary
This paper extends prior work refuting Shapley values for explainable AI by demonstrating their inadequacy across different types of classifiers. The authors show that Shapley values can incorrectly assign higher importance scores to irrelevant features than relevant ones, even when relevant features have zero importance. This occurs not only in boolean classifiers but also extends to multi-valued and discrete classifiers. The paper also demonstrates that minimal adversarial examples never include irrelevant features, further contradicting the importance scores assigned by Shapley values. Experiments with real-world decision trees and Ordered Multi-Valued Decision Diagrams confirm these issues occur frequently, with 16-33% of instances showing irrelevant features assigned higher Shapley values than relevant ones.

## Method Summary
The paper analyzes classifiers using tabular representations, decision trees, and Ordered Multi-Valued Decision Diagrams (OMDDs). It computes Shapley values for features and compares them against formal explanations (AXps and CXps) to determine feature relevance. The analysis includes building OMDDs using the MEDDLY package and employing a polynomial-time algorithm for computing Shapley values for d-DNFFs. The authors examine various datasets including car, monk1, monk2, monk3, and postoperative_patient_data to validate their claims across different classification scenarios.

## Key Results
- Shapley values can assign higher importance scores to irrelevant features than relevant ones, even when relevant features have zero importance
- Minimal adversarial examples never include irrelevant features, contradicting Shapley value assignments
- 16-33% of instances show irrelevant features assigned higher Shapley values than relevant ones in real-world decision trees and OMDDs
- These issues persist across boolean, multi-valued, and discrete classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley values for XAI assign importance scores to features that can be uncorrelated with their actual relevance to a prediction
- Mechanism: The paper demonstrates that irrelevant features can be assigned higher Shapley values than relevant features for certain classifiers, even when the relevant feature has zero importance
- Core assumption: The computation of Shapley values accurately reflects feature importance for all classifiers
- Evidence anchors:
  - [abstract] "Shapley values can assign higher importance scores to irrelevant features than relevant ones, even when the relevant feature has zero importance"
  - [section 4] "The relative order of feature importance obtained is: 2, 3, 1 (or 3, 2, 1). The interpretation that can be made is: feature 2 (or 3) is more important for the prediction than is feature 3 (or 2), and feature 3 (or 2) is more important for the prediction than is feature 1. This interpretation is in completely disagreement with the analysis of feature influence, with the analysis of feature relevancy, and with the analysis of adversarial examples"

### Mechanism 2
- Claim: Minimal adversarial examples do not include irrelevant features
- Mechanism: The paper proves that features that are irrelevant for a prediction are never included in minimal Hamming distance adversarial examples, while relevant features are always included
- Core assumption: The relationship between adversarial examples and feature relevance is well-defined and consistent across all classifiers
- Evidence anchors:
  - [abstract] "The paper shows that the features changed in any minimal ℓ0 distance adversarial examples do not include irrelevant features"
  - [section 3] "Proposition 1. Given an instance (v, c), if A is an AE and j ∈ A is an irrelevant feature, then there exists another AE B with B ⊊ A, with j ∉ B"

### Mechanism 3
- Claim: The inadequacy of Shapley values for XAI is not limited to boolean classifiers
- Mechanism: The paper extends the refutation of Shapley values to multi-valued and discrete classifiers, demonstrating that the issues observed in boolean classifiers also occur in these more complex classifiers
- Core assumption: The properties of Shapley values that lead to misleading importance scores in boolean classifiers also apply to multi-valued and discrete classifiers
- Evidence anchors:
  - [abstract] "To address such possible criticism, this paper demonstrates the inadequacy of Shapley values for families of classifiers where features are not boolean, but also for families of classifiers for which multiple classes can be picked"
  - [section 4] "Building on earlier work [Huang and Marques-Silva 2023b,c; Marques-Silva and Huang 2023], this section analyzes several additional examples, further extending the earlier results on the inadequacy of Shapley values for XAI"

## Foundational Learning

- Concept: Feature relevance in abductive reasoning
  - Why needed here: Understanding the concept of feature relevance is crucial for interpreting the results of the paper, as it forms the basis for comparing Shapley values to a theoretically sound measure of feature importance
  - Quick check question: What is the difference between a relevant and an irrelevant feature in the context of abductive explanations?

- Concept: Adversarial examples and their relationship to feature relevance
  - Why needed here: The paper uses the properties of adversarial examples to further support the claim that Shapley values provide misleading information about feature importance
  - Quick check question: Why do minimal adversarial examples not include irrelevant features, and what does this imply about the relationship between feature relevance and adversarial examples?

- Concept: Multi-valued and discrete classifiers
  - Why needed here: The paper extends the refutation of Shapley values beyond boolean classifiers, so understanding these more complex types of classifiers is essential for grasping the full scope of the results
  - Quick check question: How do multi-valued and discrete classifiers differ from boolean classifiers, and why is this distinction important for the paper's argument?

## Architecture Onboarding

- Component map:
  - Classifiers (boolean, multi-valued, discrete) -> Shapley value computation -> Formal explanations (AXp and CXp) -> Adversarial examples -> Feature relevance analysis

- Critical path:
  1. Define a classifier and an instance
  2. Compute formal explanations and determine feature relevance
  3. Compute Shapley values for the features
  4. Analyze the relationship between Shapley values and feature relevance
  5. Generate minimal adversarial examples and analyze their relationship to feature relevance
  6. Compare the results from steps 4 and 5 to assess the adequacy of Shapley values for XAI

- Design tradeoffs:
  - The paper focuses on a theoretical refutation of Shapley values, which may not fully capture the practical utility of these methods in real-world applications
  - The analysis is based on specific examples and may not be generalizable to all possible classifiers and instances

- Failure signatures:
  - If Shapley values consistently align with feature relevance across a wide range of classifiers and instances, the paper's argument would be weakened
  - If the relationship between adversarial examples and feature relevance is not consistent across different types of classifiers, the paper's argument would be undermined

- First 3 experiments:
  1. Implement a boolean classifier and compute Shapley values, formal explanations, and minimal adversarial examples to verify the paper's claims
  2. Extend the analysis to a multi-valued classifier and assess whether the issues with Shapley values persist
  3. Analyze a discrete classifier and investigate the relationship between Shapley values, feature relevance, and adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of Shapley values when applied to real-valued features, and how do these limitations differ from those observed with discrete features?
- Basis in paper: [explicit] The paper states: "In this paper, features are assumed not to be real-valued. Other distances could be considered for real-valued features."
- Why unresolved: The paper does not explore or provide any insights into the behavior of Shapley values for real-valued features
- What evidence would resolve it: Experiments comparing Shapley values' performance on real-valued versus discrete features, analyzing feature importance, relevancy, and adversarial example generation

### Open Question 2
- Question: How does the choice of baseline in Shapley value calculations affect the correlation between Shapley values and feature importance/relevancy, and can alternative baseline selection methods mitigate the identified issues?
- Basis in paper: [explicit] The paper mentions: "Although this and earlier reports consider a well-established definition of Shapley values for XAI, specifically the one proposed in a number of well-known references... one possible criticism to the results... is that there are other definitions of Shapley values besides the one being used. One example is the use of baselines."
- Why unresolved: The paper does not provide experimental results or theoretical analysis of how different baseline selection methods impact Shapley values' effectiveness
- What evidence would resolve it: A comparative study of Shapley values calculated with different baseline selection methods, evaluating their correlation with feature importance/relevancy and their ability to identify relevant features in adversarial examples

### Open Question 3
- Question: Can the issues with Shapley values for XAI be addressed by modifying the computation method to account for feature interactions or dependencies, and if so, what specific modifications would be most effective?
- Basis in paper: [inferred] The paper demonstrates that Shapley values fail to capture feature relevancy and importance, even when features are irrelevant for predictions or adversarial examples. This suggests that the current computation method does not adequately account for feature interactions
- Why unresolved: The paper does not propose or test any modifications to the Shapley value computation method
- What evidence would resolve it: Development and evaluation of modified Shapley value computation methods that incorporate feature interactions or dependencies, testing their ability to accurately reflect feature importance and relevancy in various classification scenarios

## Limitations
- Narrow empirical scope limits generalizability to real-world complex models and diverse data distributions
- Implementation-specific issues with MEDDLY package and polynomial-time algorithm are not fully discussed
- Focus on specific types of explanations (AXps and CXps) without extensive comparison to other XAI methods

## Confidence

- **High Confidence**: Theoretical proofs showing that irrelevant features are never included in minimal adversarial examples, and that Shapley values can assign higher importance to irrelevant features than relevant ones in specific examples
- **Medium Confidence**: Claims that these issues extend beyond boolean classifiers to multi-valued and discrete classifiers, based on provided examples
- **Low Confidence**: Generalizability of findings to all classifier types and real-world scenarios without further empirical validation

## Next Checks

1. **Broaden Empirical Validation**: Test the Shapley value inadequacy across a wider range of real-world datasets, classifier types (e.g., neural networks, ensemble methods), and diverse data distributions to assess the generalizability of the findings

2. **Compare with Alternative XAI Methods**: Conduct a comparative study between Shapley values and other popular XAI methods (e.g., LIME, counterfactual explanations) to determine if the identified issues are unique to Shapley values or common across XAI techniques

3. **Analyze Real-World Impact**: Investigate the practical implications of using Shapley values in real-world applications by collaborating with domain experts to assess how the identified inadequacies affect decision-making processes in specific industries or use cases