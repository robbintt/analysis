---
ver: rpa2
title: Memory Augmented Language Models through Mixture of Word Experts
arxiv_id: '2311.10768'
source_url: https://arxiv.org/abs/2311.10768
tags:
- experts
- mowe
- routing
- expert
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new model architecture that combines the
  efficiency of sparsely activated models with the power of large language models
  to memorize and retrieve world knowledge. The approach, called Mixture of Word Experts
  (MoWE), uses a large knowledge-rich vocabulary to route tokens to a large number
  of experts, each tied to a specific word.
---

# Memory Augmented Language Models through Mixture of Word Experts

## Quick Facts
- arXiv ID: 2311.10768
- Source URL: https://arxiv.org/abs/2311.10768
- Authors: 
- Reference count: 24
- Key outcome: MoWE significantly outperforms T5 models with similar FLOPs on knowledge-intensive tasks, and outperforms regular MoE models while matching more complex memory-augmented approaches.

## Executive Summary
This paper introduces Mixture of Word Experts (MoWE), a model architecture that combines the efficiency of sparsely activated models with the knowledge retrieval capabilities of large language models. MoWE uses a large knowledge-rich vocabulary to route tokens to tens of thousands of word-specific experts, effectively creating a sparse memory system integrated into the model backbone. The approach demonstrates significant improvements on knowledge-intensive tasks like question answering and fact verification while maintaining computational efficiency comparable to smaller dense models.

## Method Summary
MoWE extends the Mixture-of-Experts paradigm by using a massive routing vocabulary (up to 1 million words) where each expert is tied to a specific word. The model employs hierarchical routing with frequency buckets and expert blocks to efficiently scale to hundreds of thousands of experts. During pretraining on C4, the model learns to associate factual knowledge with specific word experts. Crucially, all MoWE experts are frozen during downstream fine-tuning to prevent catastrophic forgetting and maintain the pretraining knowledge. The architecture is implemented using the T5x framework with four MoWE layers (two in encoder, two in decoder) and 32K experts per layer.

## Key Results
- MoWE-Base with 32K experts matches T5-Large performance on knowledge tasks with similar FLOPs
- MoWE significantly outperforms regular MoE models on TriviaQA, WebQuestions, and Natural Questions
- Freezing experts during fine-tuning prevents catastrophic forgetting and maintains pretraining knowledge
- MoWE outperforms more complex memory-augmented approaches while being simpler to implement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using tens of thousands of word-specific experts improves memorization and retrieval of factual knowledge
- Mechanism: Each expert specializes in content co-occurring with its specific word, creating a sparse memory where experts act as key-value stores for word-related facts
- Core assumption: Words in routing vocabulary are sufficiently informative and represent concepts the model should memorize
- Evidence: MoWE is framed as memory-augmented model; ablation shows performance drops when experts are deactivated

### Mechanism 2
- Claim: Freezing experts during fine-tuning prevents catastrophic forgetting while retaining pretraining knowledge
- Mechanism: Fixed expert parameters ensure factual associations learned during pretraining remain intact during downstream adaptation
- Core assumption: Pretraining provides sufficient factual coverage for downstream tasks
- Evidence: Table 3 shows significant performance drop when experts are deactivated; paper explicitly freezes experts to avoid forgetting

### Mechanism 3
- Claim: Hierarchical routing with frequency bucketing enables efficient scaling to hundreds of thousands of experts
- Mechanism: Tokens route first to frequency buckets, then to expert blocks within buckets, reducing all-to-all communication
- Core assumption: Routing function can be fixed and precomputed based on vocabulary token IDs
- Evidence: Paper pretrains MoWE with up to 1 million experts using 16 v3 TPUs; hierarchical structure enables this scale

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing and load balancing
  - Why needed here: MoWE extends MoE by using much larger numbers of experts with fixed routing
  - Quick check question: What is the purpose of the auxiliary load balancing loss in standard MoE models, and why is it unnecessary in MoWE?

- Concept: Sparse memory and knowledge retrieval in neural networks
  - Why needed here: MoWE is framed as memory-augmented model; understanding sparse memory storage and retrieval is key
  - Quick check question: How does fixed routing in MoWE differ from learned routing in terms of memory access patterns?

- Concept: Vocabulary construction and tokenization strategies
  - Why needed here: MoWE uses large auxiliary vocabulary for routing, requiring knowledge of how to build knowledge-rich vocabularies
  - Quick check question: Why does MoWE prefer whole words over subword tokens for routing, and what are the trade-offs?

## Architecture Onboarding

- Component map: Input tokens → default T5 tokenizer → embeddings → MoWE layer (expert selection via routing vocabulary) → expert blocks → individual experts (FFNs) → output → rest of Transformer blocks → decoder

- Critical path: 1. Tokenization and routing ID assignment (hash-based lookup) 2. Frequency bucket routing 3. Expert block routing 4. Expert selection and computation 5. Output combination and return to dense layers

- Design tradeoffs: Large routing vocabulary vs. expert specialization (more words → more specialized experts but fewer training updates per expert); Fixed routing vs. learned routing (no load balancing loss but less flexibility); Freezing experts vs. finetuning (retains pretraining knowledge but limits adaptation)

- Failure signatures: Degraded performance on knowledge-intensive tasks → routing vocabulary may lack relevant words or experts under-trained; Training instability → expert blocks or frequency buckets misconfigured; Memory overflow → too many experts or blocks, or bucketing strategy inefficient

- First 3 experiments: 1. Vary routing vocabulary size (32K, 65K, 131K, 262K, 524K, 1M) and measure impact on TriviaQA F1 to find optimal size; 2. Compare freezing vs. unfreezing experts during fine-tuning on knowledge-intensive task to quantify catastrophic forgetting; 3. Test different numbers of MoWE layers (1, 2, 3 in encoder) to find best balance between performance and computational overhead

## Open Questions the Paper Calls Out

- What is the optimal number of experts to use in MoWE for balancing performance and computational efficiency? The paper finds 32K seems optimal but doesn't definitively establish this through comprehensive comparison with other expert counts.

- How does the size of the routing vocabulary affect MoWE performance on different types of tasks? While larger routing vocabulary (1M) helps knowledge-intensive tasks, the paper doesn't explore smaller vocabularies or performance on non-knowledge tasks.

- How does MoWE compare to other memory-augmented models on tasks that are not knowledge-intensive? The paper focuses on knowledge-intensive tasks and doesn't provide comprehensive comparison with other memory-augmented approaches on broader task sets.

## Limitations

- Fixed routing vocabulary approach lacks direct validation that selected vocabulary covers necessary factual knowledge for downstream tasks
- Performance comparisons don't include parameter-matched comparisons with other memory-augmented approaches
- No analysis of whether 1M experts is optimal or simply a computational constraint
- Limited task diversity in evaluation - focus primarily on knowledge-intensive QA tasks

## Confidence

- High confidence: Computational efficiency claims - directly measurable and well-supported by ablation studies showing MoWE-Base matches T5-Large performance with similar FLOPs
- Medium confidence: Memory-augmented model claims - supported by ablation evidence but lacking direct retrieval analysis or knowledge probing experiments
- Medium confidence: Superiority over regular MoE models - valid comparison but limited task sample size and no parameter-matched comparison provided

## Next Checks

1. **Knowledge retrieval analysis**: Design probing experiment where specific factual questions are answered by activating only expert(s) associated with relevant routing vocabulary words, then analyze whether correct knowledge is retrieved without full model context.

2. **Vocabulary coverage validation**: Analyze routing vocabulary's coverage of entity names and relations appearing in downstream task datasets, and measure correlation between vocabulary coverage and task performance across different vocabulary sizes.

3. **Dynamic routing comparison**: Implement MoWE variant with learned routing (using load balancing loss) and compare its performance and knowledge retention to fixed routing version across multiple fine-tuning iterations to quantify flexibility vs memorization trade-offs.