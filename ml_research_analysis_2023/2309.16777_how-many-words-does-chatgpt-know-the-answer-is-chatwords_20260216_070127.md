---
ver: rpa2
title: How many words does ChatGPT know? The answer is ChatWords
arxiv_id: '2309.16777'
source_url: https://arxiv.org/abs/2309.16777
tags:
- chatgpt
- words
- chatwords
- knowledge
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChatWords, an automated system to evaluate
  the lexical knowledge of AI tools like ChatGPT by testing their recognition of arbitrary
  sets of words. The system generates prompts for each word and parses responses to
  assess recognition, addressing limitations in existing studies that focus only on
  task-specific performance.
---

# How many words does ChatGPT know? The answer is ChatWords

## Quick Facts
- arXiv ID: 2309.16777
- Source URL: https://arxiv.org/abs/2309.16777
- Reference count: 38
- Primary result: ChatGPT recognizes ~80-90% of words in Spanish and literary vocabularies, with some errors in meaning.

## Executive Summary
ChatWords is an automated system designed to evaluate the lexical knowledge of AI tools like ChatGPT by testing their recognition of arbitrary sets of words. The tool generates prompts for each word and parses responses to assess recognition, addressing limitations in existing studies that focus only on task-specific performance. Testing on the Spanish dictionary and The Quixote revealed that ChatGPT recognizes ~80% and ~90% of words respectively, with some errors in meaning. The tool is extensible, open-source, and adaptable for analyzing domain-specific vocabularies or incorrect words, offering insights into lexical biases that may impact AI-generated content.

## Method Summary
The method involves automated prompt generation using simple yes/no questions for each word, sending these prompts to ChatGPT via the OpenAI API, and parsing the responses to compile recognition rates. The system is designed to handle thousands of words efficiently through asynchronous processing. Experiments were conducted using word lists from the Spanish dictionary (RAE) and the literary work Don Quixote, with results indicating high recognition rates but also some errors in meaning.

## Key Results
- ChatGPT recognizes ~80% of words in the Spanish dictionary (RAE).
- ChatGPT recognizes ~90% of words in Don Quixote.
- Some errors in meaning were detected, highlighting potential gaps in lexical knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatWords achieves high-throughput word testing by using simple yes/no prompts that ChatGPT can parse and respond to deterministically.
- Mechanism: Prompts are designed to elicit a binary response ("yes" or "no") for each word, allowing automated parsing and compilation of results across thousands of words without human intervention.
- Core assumption: ChatGPT's API can consistently return a simple "yes" or "no" when explicitly asked in a straightforward manner.
- Evidence anchors:
  - [abstract] "The tool generates prompts for each word and parses responses to assess recognition"
  - [section] "Therefore, testing must be automated. To automate testing, we need prompts that instruct ChatGPT to produce an answer that can be processed automatically."
- Break condition: If ChatGPT starts to give more verbose or inconsistent responses to simple prompts, or if the API changes its response format.

### Mechanism 2
- Claim: The modular architecture allows ChatWords to be easily extended to evaluate different AI tools or lexicons.
- Mechanism: The tool is divided into two parts: a program that interacts with ChatGPT via API and a web application that configures experiments and stores results. Clear interfaces allow programmers to modify either part independently.
- Core assumption: The separation of concerns between the API-interaction layer and the web interface layer is clean enough to allow swapping components without breaking functionality.
- Evidence anchors:
  - [abstract] "ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools."
  - [section] "The tool is divided into two parts, a program that interacts with ChatGPT using its API and a web application that interacts with the program to run experiments and store the results in a database."
- Break condition: If the API changes its request/response structure or if the modular interfaces become tightly coupled.

### Mechanism 3
- Claim: ChatWords can reveal lexical biases in ChatGPT by testing both correct and incorrect words.
- Mechanism: By evaluating ChatGPT's recognition of arbitrary sets of words, including non-words or domain-specific terminology, ChatWords exposes gaps or errors in ChatGPT's learned vocabulary.
- Core assumption: ChatGPT's responses to both valid and invalid words are reliable indicators of its lexical knowledge.
- Evidence anchors:
  - [abstract] "ChatWords is extensible, open-source, and adaptable for analyzing domain-specific vocabularies or incorrect words"
  - [section] "Another interesting aspect is to evaluate the knowledge that ChatGPT has of slang and new words being used by younger generations... ChatWords is not only useful to assess the knowledge of the correct words; it can be used to check if ChatGPT has learned incorrect words"
- Break condition: If ChatGPT's responses to incorrect words become too unpredictable or if it starts to refuse to answer such queries.

## Foundational Learning

- Concept: Natural Language Processing (NLP) basics
  - Why needed here: Understanding how language models like ChatGPT process and generate text is essential to grasp the significance of lexical knowledge evaluation.
  - Quick check question: What is the difference between a word's presence in a model's vocabulary and its semantic understanding?

- Concept: API interaction and asynchronous processing
  - Why needed here: ChatWords relies on the OpenAI API and processes requests asynchronously to achieve high throughput.
  - Quick check question: Why is asynchronous processing important when sending thousands of API requests?

- Concept: Data parsing and automation
  - Why needed here: The tool must parse ChatGPT's responses automatically to compile results without manual review.
  - Quick check question: How would you design a parser to reliably extract "yes" or "no" from a text response?

## Architecture Onboarding

- Component map:
  - Web Application (Nest.js backend + Angular frontend)
  - ChatWords Program (Python, API interaction)
  - Database (stores experiment configs and results)
  - OpenAI API (ChatGPT interface)

- Critical path:
  1. User configures experiment via web interface
  2. Web app sends word list and parameters to Python program
  3. Python program sends prompts to ChatGPT API asynchronously
  4. Responses are parsed and sent back to web app
  5. Web app stores results in database and displays them to user

- Design tradeoffs:
  - Simple yes/no prompts vs. richer semantic analysis (speed and automation vs. depth of insight)
  - Asynchronous processing vs. potential API rate limiting
  - Modular design vs. potential overhead in maintaining clear interfaces

- Failure signatures:
  - High percentage of non-binary responses from ChatGPT
  - Web app unable to parse API responses correctly
  - Database errors when storing large experiment results

- First 3 experiments:
  1. Test a small list of common words (e.g., 100 words) to verify basic functionality
  2. Run the same experiment with different ChatGPT versions (3.5 vs. 4) to compare lexical knowledge
  3. Test a domain-specific vocabulary (e.g., medical terms) to evaluate specialized knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the lexical knowledge of ChatGPT differ across languages, and is there a stronger bias in the lexicon knowledge across languages?
- Basis in paper: [explicit] The authors discuss the potential differences in lexical knowledge across languages and whether there is a stronger bias in the lexicon knowledge across languages. They mention that the performance for Spanish and English of ChatGPT-4 is very similar on some tasks, but it is not clear if the same applies to the lexicon.
- Why unresolved: The paper does not provide a direct comparison of lexical knowledge across different languages. The authors suggest that this is an interesting topic for future work, indicating that it has not been thoroughly investigated.
- What evidence would resolve it: A comparative study of ChatGPT's lexical knowledge across multiple languages, using ChatWords to evaluate the recognition of words in each language. This would involve testing ChatGPT's responses to a standardized set of words in different languages and analyzing the results for patterns or biases.

### Open Question 2
- Question: What is the impact of using text that may not be correct during training on ChatGPT's lexical knowledge and its performance in other tasks?
- Basis in paper: [explicit] The authors discuss the potential impact of using incorrect words in the training dataset of ChatGPT and how it may affect the tool's lexical knowledge and performance in other tasks.
- Why unresolved: The paper does not provide a detailed evaluation of the responses of ChatGPT to pseudowords or incorrect words. The authors mention that this deserves further study and a detailed analysis of its implications.
- What evidence would resolve it: A comprehensive study that tests ChatGPT's responses to a large set of pseudowords or incorrect words, analyzing how often it mistakes them for similar valid words and the impact on its performance in various tasks. This would involve generating pseudowords, testing them with ChatGPT, and evaluating the responses for accuracy and relevance.

### Open Question 3
- Question: How does the lexical knowledge of ChatGPT correlate with its performance in different tasks such as question and answer, summarization, or translation?
- Basis in paper: [explicit] The authors suggest that the lack of lexical knowledge can lead to incorrect results in tasks such as translation, and intuitively the same applies to other language processing tasks.
- Why unresolved: The paper does not provide a direct correlation study between ChatGPT's lexical knowledge and its performance in various tasks. The authors mention that this is a subtle use of ChatWords that needs further investigation.
- What evidence would resolve it: A study that correlates ChatGPT's lexical knowledge with its performance in different tasks by running ChatWords on the inputs of these tasks and analyzing the results for patterns or correlations. This would involve testing ChatGPT's performance on a set of tasks and comparing it with its lexical knowledge as determined by ChatWords.

## Limitations
- The exact OpenAI API parameters (model version, temperature, max tokens) used in the experiments are not specified, which could affect response consistency and recognition rates.
- The method for detecting incorrect meanings in ChatGPT's responses is not fully automated, requiring manual verification for some results, which introduces potential subjectivity and scalability issues.
- The study focuses on Spanish vocabulary; results may not generalize to other languages without further validation.

## Confidence
- **High confidence** in the basic mechanism of automated prompt generation and response parsing for yes/no word recognition.
- **Medium confidence** in the reported recognition rates (80-90%) due to potential variability in API parameters and lack of detailed experimental controls.
- **Low confidence** in the scalability and reliability of detecting incorrect meanings without manual intervention.

## Next Checks
1. **Reproduce with controlled API parameters**: Run the ChatWords tool using the same word lists but explicitly set and document OpenAI API parameters (model version, temperature, max tokens) to ensure reproducibility and assess parameter sensitivity.
2. **Cross-linguistic validation**: Apply ChatWords to word lists in multiple languages (e.g., English, French, Mandarin) to determine if the ~80-90% recognition rate holds across linguistic contexts.
3. **Error analysis for incorrect meanings**: Design an automated or semi-automated method to flag and categorize incorrect meanings returned by ChatGPT, and validate its accuracy against a manually curated subset.