---
ver: rpa2
title: 'GePSAn: Generative Procedure Step Anticipation in Cooking Videos'
arxiv_id: '2310.08312'
source_url: https://arxiv.org/abs/2310.08312
tags:
- video
- step
- future
- next
- anticipation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses future step anticipation in procedural videos,
  aiming to predict the next step in a cooking activity from a video sequence. The
  key challenge is modeling the uncertainty and diversity of plausible next steps.
---

# GePSAn: Generative Procedure Step Anticipation in Cooking Videos

## Quick Facts
- arXiv ID: 2310.08312
- Source URL: https://arxiv.org/abs/2310.08312
- Reference count: 40
- Key outcome: Establishes new state-of-the-art results for next step anticipation in cooking videos by predicting multiple diverse and plausible next steps using a generative model with CVAE architecture.

## Executive Summary
This work addresses future step anticipation in procedural videos, aiming to predict the next step in a cooking activity from a video sequence. The key challenge is modeling the uncertainty and diversity of plausible next steps. To address this, the authors propose GEPSA N, a generative model that predicts multiple diverse and plausible next steps in natural language. The model is pretrained on a large text corpus (Recipe1M+) and then transferred to the video domain, either zero-shot or with fine-tuning. Experiments on YouCookII show that GEPSA N outperforms existing baselines in both single and multiple next step prediction settings.

## Method Summary
GEPSA N consists of a modality encoder (text or video using UniVL), a recipe encoder with a Conditional VAE to capture uncertainty, and an instruction decoder. The model is pretrained on Recipe1M+ using text input, then transferred to video domain by replacing the text encoder with a UniVL video encoder. The CVAE architecture learns a distribution over possible next steps conditioned on the observed context, while an auxiliary loss stabilizes training. At inference, multiple diverse predictions are generated by sampling from the prior distribution.

## Key Results
- Outperforms existing baselines on YouCookII dataset for both single and multiple next step prediction
- Achieves state-of-the-art results in capturing diversity of plausible next steps
- Successfully transfers from text to video without fine-tuning, demonstrating strong zero-shot performance
- Shows superior performance compared to deterministic prediction models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Conditional VAE architecture allows GEPSA N to capture the multi-modal nature of future step prediction by learning a latent space distribution over possible next steps.
- Mechanism: The CVAE takes the context vector Rt and the next step embedding ft+1, encodes them into a posterior distribution q(zt+1|ft+1, Rt), and samples from it to generate diverse future step embeddings f'(t+1). The KL divergence loss between the posterior and a standard Gaussian prior forces the model to learn a smooth latent space that can generate diverse samples.
- Core assumption: The task of next step anticipation is inherently multi-modal, and modeling a distribution over possible next steps is more effective than predicting a single deterministic output.

### Mechanism 2
- Claim: Pretraining on a large text corpus (Recipe1M+) and transferring to video via aligned modality encoders enables zero-shot video domain adaptation.
- Mechanism: GEPSA N is first trained on Recipe1M+ using text input. The UniVL text encoder produces embeddings that live in a common embedding space with UniVL video features. When transferred to video, only the encoder is swapped (text→video) while the recipe encoder and decoder remain frozen, allowing immediate zero-shot performance.
- Core assumption: Text and video modalities can be aligned into a common feature space through pretraining, and procedural knowledge learned from text can transfer to video without fine-tuning.

### Mechanism 3
- Claim: The auxiliary loss Laux stabilizes CVAE training by providing a direct reconstruction target for the prediction head.
- Mechanism: The auxiliary loss computes MSE between the predicted embedding f'(t+1) and the true embedding ft+1. This gives the prediction head a stable target to learn from, while the CVAE learns to generate diverse samples around this mean prediction.
- Core assumption: Training a CVAE with only the ELBO loss is unstable and prone to posterior collapse, and an auxiliary reconstruction loss is needed to stabilize training.

## Foundational Learning

- Concept: Conditional Variational Autoencoder (CVAE)
  - Why needed here: The task of next step anticipation is inherently multi-modal, with multiple plausible future steps. A CVAE can learn a distribution over possible next steps conditioned on the observed context.
  - Quick check question: What is the role of the KL divergence loss in a CVAE, and how does it differ from a standard VAE?

- Concept: Cross-modal representation learning
  - Why needed here: GEPSA N needs to transfer knowledge from a large text corpus to the video domain. Cross-modal representation learning allows text and video features to live in a common embedding space, enabling zero-shot transfer.
  - Quick check question: How do models like UniVL learn aligned text and video representations, and what are the key components of such a model?

- Concept: Transformer-based sequence modeling
  - Why needed here: The context encoder needs to aggregate information from a variable-length sequence of past steps. Transformers are well-suited for this task due to their ability to model long-range dependencies.
  - Quick check question: What is the difference between causal and non-causal attention in a transformer, and why is causal attention used in the context encoder?

## Architecture Onboarding

- Component map:
  - Single-modality encoder (text or video) → UniVL encoder + projection head
  - Context encoder → 6-layer transformer with causal attention
  - Conditional VAE → posterior network (3-layer MLP) + prediction head (3-layer MLP)
  - Instruction decoder → 3-layer LSTM
  - Loss functions → ELBO, auxiliary, reconstruction

- Critical path:
  1. Encode input steps into embeddings using single-modality encoder
  2. Aggregate embeddings into context vector using context encoder
  3. Sample latent variable from posterior distribution in CVAE
  4. Generate next step embedding using prediction head
  5. Decode next step into natural language using instruction decoder

- Design tradeoffs:
  - Using a frozen UniVL encoder enables zero-shot transfer but limits end-to-end fine-tuning
  - CVAE adds diversity but increases training complexity and can suffer from posterior collapse
  - LSTM decoder is simpler than transformer but may struggle with long-range dependencies

- Failure signatures:
  - Posterior collapse: CVAE generates identical or very similar predictions
  - Mode collapse: CVAE only generates a few modes of the target distribution
  - Poor transfer: Zero-shot performance is much worse than text-only performance

- First 3 experiments:
  1. Train GEPSA N on Recipe1M+ with text input only and evaluate text-based next step prediction
  2. Transfer GEPSA N to YouCookII video domain without fine-tuning and evaluate zero-shot video-based next step prediction
  3. Fine-tune GEPSA N on YouCookII video domain and evaluate improved video-based next step prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum achievable performance for next step anticipation in procedural videos, given the current state of the art models and evaluation metrics?
- Basis in paper: [inferred] The paper establishes new state-of-the-art results on YouCookII but acknowledges that there is still room for improvement, particularly in capturing diversity of plausible next steps.
- Why unresolved: The paper does not provide a theoretical upper bound for performance or discuss the inherent limitations of the task that might prevent reaching perfect accuracy.
- What evidence would resolve it: A comprehensive study comparing human-level performance on the same task, or an analysis of the ambiguity and uncertainty present in procedural video datasets that inherently limits prediction accuracy.

### Open Question 2
- Question: How does the performance of the model change when using different video-language feature extractors other than UniVL?
- Basis in paper: [explicit] The paper mentions that the model design is not tied to the UniVL encoder and can be adapted to leverage stronger modality encoders.
- Why unresolved: The paper only uses UniVL as the video-language feature extractor and does not explore other options or compare their performance.
- What evidence would resolve it: Experiments comparing the performance of the model using different video-language feature extractors, such as CLIP or ALIGN, on the same dataset and task.

### Open Question 3
- Question: What is the impact of increasing the number of diverse next step predictions (k) on the model's performance, and is there an optimal value for k?
- Basis in paper: [explicit] The paper sets k=5 for all experiments but mentions that the impact of increasing k on the results is shown in the supplementary material.
- Why unresolved: The paper does not provide a detailed analysis of how the model's performance changes with different values of k or discuss the trade-offs between diversity and accuracy.
- What evidence would resolve it: A systematic study varying k and analyzing the model's performance, diversity of predictions, and computational cost for different values of k.

## Limitations
- The UniVL feature alignment between text and video domains is assumed but not empirically verified
- The CVAE's ability to generate truly diverse predictions is evaluated only qualitatively
- The model's generalization beyond cooking to other procedural domains is not tested
- Computational overhead of the CVAE compared to deterministic baselines is not discussed

## Confidence
**High Confidence**: The empirical results on YouCookII (Tables 2-4) are clearly presented and demonstrate state-of-the-art performance. The pretraining-then-transfer methodology is well-established in the literature.

**Medium Confidence**: The mechanism by which the CVAE captures diversity is theoretically sound but relies on the assumption that the posterior network learns meaningful representations during training. The auxiliary loss stabilizing CVAE training is supported by citations but not directly demonstrated.

**Low Confidence**: The claim that UniVL features from text and video live in the same embedding space enabling zero-shot transfer is not empirically validated. The qualitative examples (Figure 3) showing diverse predictions are illustrative but limited in scope.

## Next Checks
1. Implement quantitative diversity metrics (e.g., pairwise BLEU/Jaccard scores between generated predictions) to verify the CVAE actually produces meaningfully different outputs beyond the ground truth.
2. Conduct nearest-neighbor retrieval experiments between video and text features to empirically verify that UniVL produces well-aligned cross-modal representations suitable for zero-shot transfer.
3. Evaluate GEPSA N on procedural video datasets outside the cooking domain (e.g., craft or repair videos) to assess the model's broader applicability to procedural activity understanding.