---
ver: rpa2
title: Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step
  Prompting Method
arxiv_id: '2310.00305'
source_url: https://arxiv.org/abs/2310.00305
tags:
- claim
- answer
- question
- hiss
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of large language models (LLMs)
  for fact-checking news claims. While standard prompting with 4-shot examples performs
  comparably to supervised models, vanilla Chain-of-Thought (CoT) prompting fails
  due to omission of necessary thoughts and fact hallucination.
---

# Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method

## Quick Facts
- **arXiv ID**: 2310.00305
- **Source URL**: https://arxiv.org/abs/2310.00305
- **Reference count**: 21
- **Primary result**: HiSS outperforms state-of-the-art fully-supervised approaches by 4.95% in macro-average F1

## Executive Summary
This paper addresses the challenge of fact verification for news claims using large language models (LLMs). While standard few-shot prompting performs comparably to supervised models, vanilla Chain-of-Thought (CoT) prompting fails due to omission of necessary thoughts and fact hallucination. The authors propose a Hierarchical Step-by-Step (HiSS) prompting method that decomposes claims into subclaims and verifies each through fine-grained reasoning steps with access to external knowledge. Experiments on two public datasets show HiSS outperforms state-of-the-art fully-supervised approaches and strong few-shot baselines by 4.95% in macro-average F1 while providing more fine-grained and readable explanations.

## Method Summary
The HiSS method prompts LLMs to separate a claim into several subclaims and then verify each of them via multiple question-answering steps progressively. The approach consists of three main components: hierarchical claim decomposition, step-by-step verification with probing questions, and confidence-based search engine integration. For each subclaim, the model generates probing questions that uncover implicit verification points, answers them progressively, and uses web search when lacking confidence. The final prediction aggregates verification results from all subclaims.

## Key Results
- HiSS achieves 4.95% improvement in macro-average F1 over state-of-the-art fully-supervised approaches
- Outperforms standard 4-shot prompting baselines on both RAWFC and LIAR datasets
- Provides more fine-grained and readable explanations compared to vanilla CoT prompting

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical decomposition of claims into subclaims
The HiSS method prompts LLMs to split complex claims into smaller, explicit subclaims that can be verified individually. Breaking down complex claims into explicit subclaims reduces the likelihood of missing crucial verification points. The method may fail if the LLM fails to properly decompose claims or generates too few/faulty subclaims.

### Mechanism 2: Step-by-step verification with probing questions
For each subclaim, the model generates probing questions that uncover implicit verification points, then answers them progressively. Explicit decomposition captures surface-level points, but implicit points require iterative questioning to uncover. The approach may fail if the LLM cannot generate meaningful probing questions or gets stuck in unproductive questioning loops.

### Mechanism 3: Confidence-based search engine integration
The model self-assesses confidence for each answer, and only searches when uncertain, integrating results back into verification. LLMs can reasonably assess their own knowledge gaps, and targeted search when uncertain prevents hallucination. The mechanism may fail if the LLM systematically over/under-estimates its confidence or search results are consistently unhelpful.

## Foundational Learning

- **Concept: In-context learning (ICL)**
  - Why needed here: The paper relies on LLMs learning verification tasks from demonstration examples without fine-tuning
  - Quick check question: What is the key difference between ICL and traditional fine-tuning approaches?

- **Concept: Chain-of-thought reasoning**
  - Why needed here: The vanilla CoT baseline shows how LLMs can generate intermediate reasoning steps before final predictions
  - Quick check question: Why did vanilla CoT fail for fact verification despite working for arithmetic tasks?

- **Concept: Hierarchical reasoning**
  - Why needed here: The paper decomposes claims at two levels (claim → subclaims → verification steps) to handle complexity
  - Quick check question: How does hierarchical decomposition help prevent omission of necessary thoughts?

## Architecture Onboarding

- **Component map**: Prompt generator → LLM → Search engine API → Result parser → Prediction aggregator
- **Critical path**: Prompt generation → Claim decomposition → Subclaim verification (with search when needed) → Final prediction
- **Design tradeoffs**: Search engine dependency vs. hallucination risk, decomposition granularity vs. computational cost, probing question generation vs. efficiency
- **Failure signatures**: Incorrect decompositions, unproductive questioning loops, over-reliance on search, hallucination in answers
- **First 3 experiments**:
  1. Test decomposition quality: Feed simple/complex claims and verify if LLM generates appropriate subclaims
  2. Test questioning effectiveness: For a known subclaim, verify if LLM generates probing questions that uncover implicit points
  3. Test search integration: Feed a question where LLM should lack confidence and verify it triggers search and integrates results correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of demonstration examples (K) for HiSS prompting across different claim complexity levels?
- Basis in paper: The paper mentions tuning K within {1, 2, 4, 6, 8} and finding K=4 optimal, but does not explore whether different K values work better for different claim complexities.
- Why unresolved: The paper only reports results for a fixed K=4 across all claims, without analyzing how claim complexity might influence the optimal number of demonstrations.
- What evidence would resolve it: Experimental results showing HiSS performance with different K values specifically stratified by claim complexity levels (simple, moderate, complex).

### Open Question 2
- Question: How does HiSS prompting perform on claims that require temporal reasoning across multiple time periods?
- Basis in paper: The paper focuses on news claim verification but does not specifically test claims requiring temporal reasoning across different time periods, which is a common challenge in fact-checking.
- Why unresolved: The datasets and examples in the paper appear to focus on claims verifiable with current or single-time period information, without testing temporal reasoning capabilities.
- What evidence would resolve it: Performance comparison of HiSS against baselines on a benchmark specifically designed to test temporal reasoning in claims.

### Open Question 3
- Question: What is the impact of search engine quality and domain expertise on HiSS performance?
- Basis in paper: The paper uses Google Search as the external knowledge source but does not explore how search quality or domain-specific search engines might affect performance.
- Why unresolved: The paper treats search results as a black box without analyzing how different search engines or query strategies might impact the quality of retrieved information and subsequent verification.
- What evidence would resolve it: Comparative experiments using different search engines (general vs. domain-specific), or experiments varying search query formulation strategies while keeping the HiSS prompting method constant.

## Limitations
- The quality of demonstration examples critically determines results, but the paper doesn't show how examples were selected or whether results are robust to different demonstration choices
- Web search integration mechanism lacks implementation details regarding how confidence is quantified and how search results are weighted
- Evaluation focuses solely on end-task accuracy metrics without detailed analysis of when the hierarchical approach succeeds versus fails

## Confidence

**High confidence**: The core observation that vanilla CoT prompting fails for fact verification due to omission and hallucination issues

**Medium confidence**: The proposed hierarchical decomposition mechanism - logical but lacks ablation studies showing marginal value

**Medium confidence**: The confidence-based search integration - described but lacks implementation details and evidence of hallucination mitigation

## Next Checks

1. **Decomposition Quality Audit**: Take 50 claims from the test set and manually evaluate the subclaim decompositions generated by HiSS. Measure precision and recall of subclaim generation and compare against a baseline using standard CoT prompting.

2. **Confidence Calibration Test**: For claims where the model decides to use search versus those where it doesn't, analyze whether the confidence assessment accurately predicts when external knowledge is needed. Measure error rates with and without search in confidence-requiring cases.

3. **Hallucination Analysis**: Extract all factual claims made by the model (both with and without search) and fact-check them against ground truth. Calculate hallucination rates and compare between HiSS, vanilla CoT, and search-only baselines to quantify the hallucination mitigation benefit.