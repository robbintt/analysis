---
ver: rpa2
title: Classification of lung cancer subtypes on CT images with synthetic pathological
  priors
arxiv_id: '2308.04663'
source_url: https://arxiv.org/abs/2308.04663
tags:
- pathological
- images
- cancer
- proposed
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a self-generating hybrid feature network (SGHF-Net)
  for classifying lung cancer subtypes on CT images. The key idea is to leverage the
  cross-scale associations between CT images and pathological images by developing
  a pathological feature synthetic module (PFSM) that derives pathological image features
  from CT images.
---

# Classification of lung cancer subtypes on CT images with synthetic pathological priors

## Quick Facts
- arXiv ID: 2308.04663
- Source URL: https://arxiv.org/abs/2308.04663
- Authors: 
- Reference count: 40
- Key outcome: SGHF-Net achieves superior classification performance (ACC, AUC, F1) by self-generating hybrid features from single-modality CT input

## Executive Summary
This paper introduces a self-generating hybrid feature network (SGHF-Net) for classifying lung cancer subtypes using only CT images. The key innovation is a pathological feature synthetic module (PFSM) that derives "gold standard" pathological features from CT images by learning cross-scale associations between modalities. By fusing these synthetic pathological features with radiological features extracted directly from CT, the model achieves superior classification accuracy compared to state-of-the-art methods while requiring only CT input at inference.

## Method Summary
SGHF-Net employs a two-module architecture where a PFSM uses a conditional GAN to synthesize pathological features from CT images, while a RFEM extracts radiological features directly from CT. These features are concatenated and fed to a classification layer. The model is trained on paired CT and pathological images but requires only CT images during inference, enabling self-generation of multi-modality features from single-modality input.

## Key Results
- SGHF-Net outperforms state-of-the-art classification models on a large-scale multi-center dataset of 829 cases
- Significant improvements in classification metrics: accuracy (ACC), area under the curve (AUC), and F1 score
- The model successfully self-generates hybrid features containing multi-modality information from single CT input
- Extensive experiments validate superiority across multiple backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PFSM extracts high-level deep features from paired CT and pathological images, enabling the model to generate synthetic pathological features that approximate "gold standard" information.
- Mechanism: The PFSM is trained using a conditional generative adversarial network (CGAN) that learns to map CT image features to pathological image features. During training, a pathology-based classifier extracts features from pathological images, and the CGAN synthesizes corresponding features from CT images. These synthetic features are then used as priors to guide the radiological feature extraction module.
- Core assumption: Cross-scale associations exist between CT images and pathological images of the same lesion, allowing deep neural networks to quantitatively map these associations.
- Evidence anchors:
  - [abstract] "Inspired by studies stating that cross-scale associations exist in the image patterns between the same case's CT images and its pathological images, we innovatively developed a pathological feature synthetic module (PFSM), which quantitatively maps cross-modality associations through deep neural networks, to derive the 'gold standard' information contained in the corresponding pathological images from CT images."
  - [section] "Inspired by studies on the cross-modality associations between CT images and pathological images, we propose to exploit these correlations with DL techniques to acquire the gold-standard pathological image features from CT images."
  - [corpus] Weak. Related works focus on multimodal fusion but do not directly validate cross-scale mapping via CGANs.
- Break condition: If the cross-scale associations are weak or inconsistent across cases, the CGAN will fail to generate meaningful synthetic pathological features.

### Mechanism 2
- Claim: Integrating synthetic pathological features with radiological features through feature fusion improves classification accuracy compared to using radiological features alone.
- Mechanism: The model concatenates high-level synthetic pathological features (512×1 vector) with high-level radiological features (also 512×1 vector) extracted from CT images. This hybrid feature vector is then fed to the final classification layer, providing richer, more specific information for subtype discrimination.
- Core assumption: The concatenated hybrid features contain complementary information that is more indicative of cancer subtypes than either modality alone.
- Evidence anchors:
  - [abstract] "we designed a radiological feature extraction module (RFEM) to directly acquire CT image information and integrated it with the pathological priors under an effective feature fusion framework, enabling the entire classification model to generate more indicative and specific pathologically related features and eventually output more accurate predictions."
  - [section] "the high-level 512 × 1 pathological feature vector synthesized with CT images and the high-level 512 × 1 radiological feature vector extracted from CT images were concatenated together before being fed to the final FC layer"
  - [corpus] Weak. No direct evidence in related works about concatenation-based fusion improving NSCLC subtype classification.
- Break condition: If the synthetic pathological features are noisy or misaligned with radiological features, fusion may degrade rather than improve performance.

### Mechanism 3
- Claim: Training the model with paired CT and pathological images while only requiring CT images at inference allows the model to self-generate multi-modality features from single-modality input.
- Mechanism: During training, the PFSM learns to synthesize pathological features from CT images using paired data. At inference, only CT images are needed, and the trained PFSM automatically generates the synthetic pathological features to be fused with radiological features for classification.
- Core assumption: The learned mapping from CT to synthetic pathological features generalizes well to unseen cases during inference.
- Evidence anchors:
  - [abstract] "The superiority of the proposed model lies in its ability to self-generate hybrid features that contain multi-modality image information based on a single-modality input."
  - [section] "The main property of our model is that it takes paired CT and pathological images as training data while only requires the CT images in the subsequent validation periods."
  - [corpus] Weak. No direct evidence in related works about self-generating hybrid features from single-modality input.
- Break condition: If the model overfits to the training distribution, it may fail to generalize and produce poor synthetic features at inference.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs are used in the PFSM to synthesize pathological features from CT images by learning the distribution mapping between modalities.
  - Quick check question: What are the two main components of a GAN and what are their respective roles?

- Concept: Feature Fusion
  - Why needed here: Feature fusion combines synthetic pathological and radiological features to create a more informative representation for classification.
  - Quick check question: What is the difference between early fusion and late fusion, and which type is used in this model?

- Concept: Cross-Modal Learning
  - Why needed here: Cross-modal learning enables the model to leverage complementary information from different imaging modalities (CT and pathology) for improved classification.
  - Quick check question: What is the main challenge in cross-modal learning, and how does this model address it?

## Architecture Onboarding

- Component map:
  CT images → PFSM (synthetic pathological features) → RFEM (radiological features) → Feature concatenation → Classification

- Critical path:
  CT images → PFSM (synthetic pathological features) → RFEM (radiological features) → Feature concatenation → Classification

- Design tradeoffs:
  - Using CGAN for feature synthesis adds complexity but enables self-generation of multi-modality features.
  - Concatenating features increases dimensionality but provides richer representation.
  - Training with paired data is more resource-intensive but improves generalization.

- Failure signatures:
  - Poor synthetic pathological feature quality indicates issues with CGAN training or weak cross-scale associations.
  - Degraded classification performance compared to baseline suggests fusion is not beneficial or overfitting occurred.
  - Large performance gap between training and validation indicates overfitting.

- First 3 experiments:
  1. Train and evaluate the standalone PFSM on paired data to assess quality of synthetic pathological features.
  2. Train and evaluate the standalone RFEM on CT images to establish baseline radiological feature performance.
  3. Train and evaluate the full SGHF-Net model with feature fusion to measure improvement over baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of deep learning architectures (e.g., CNNs vs. Transformers) compare in synthesizing pathological features from CT images for lung cancer subtype classification?
- Basis in paper: [explicit] The paper mentions that the model was tested with both CNN backbones (e.g., ResNet, DenseNet) and a Transformer network, with the Transformer showing improvements.
- Why unresolved: While the paper provides some comparison, a more systematic study across multiple architectures is needed to determine optimal approaches.
- What evidence would resolve it: A comprehensive study comparing multiple architectures (e.g., CNNs, Transformers, Vision Transformers) on the same dataset with standardized evaluation metrics would clarify which architecture performs best for this task.

### Open Question 2
- Question: How does the performance of the proposed model change when applied to different types of cancer (e.g., breast, prostate) or other medical imaging modalities (e.g., MRI, PET)?
- Basis in paper: [inferred] The paper focuses on lung cancer subtype classification from CT images, but the authors mention the potential for extending the framework to other cancer-related clinical problems and multi-modal tasks.
- Why unresolved: The model's effectiveness for other cancer types or imaging modalities is unknown, as it was only tested on lung cancer CT images.
- What evidence would resolve it: Testing the model on datasets for other cancer types or imaging modalities, and comparing its performance to existing methods for those specific tasks, would determine its generalizability.

### Open Question 3
- Question: What is the optimal strategy for extracting pathological features from whole-slide images (WSIs) to improve the accuracy of the synthetic pathological features?
- Basis in paper: [explicit] The paper discusses using a Vision Transformer (ViT) to extract features from WSI patches, but notes limitations in representing global information and suggests that a more comprehensive multi-scale feature extraction strategy may be beneficial.
- Why unresolved: The current approach has limitations in capturing global WSI information, and the optimal feature extraction strategy is unclear.
- What evidence would resolve it: Developing and testing different feature extraction strategies (e.g., multi-scale approaches, attention mechanisms) on the same dataset and comparing their performance would identify the most effective method.

## Limitations
- The fundamental assumption that cross-scale associations between CT and pathological images can be quantitatively mapped is not directly validated
- The quality of synthetic pathological features is not compared to real pathological features using similarity metrics
- The exact contribution of synthetic pathological features versus radiological features is not isolated through ablation studies

## Confidence
- **High Confidence**: The proposed model architecture and training procedure are clearly described and implementable
- **Medium Confidence**: The claimed performance improvements over baseline models are likely valid given the experimental results, but the exact contribution of the synthetic pathological features is uncertain
- **Low Confidence**: The fundamental assumption that cross-scale associations can be quantitatively mapped from CT to pathological features is not directly validated

## Next Checks
1. Conduct ablation studies to quantify the contribution of synthetic pathological features vs radiological features alone on classification performance
2. Evaluate the quality of synthetic pathological features by comparing them to real pathological features using similarity metrics or visualization
3. Test the model's generalization ability on an external dataset from a different center to assess robustness to variations in imaging protocols and patient populations