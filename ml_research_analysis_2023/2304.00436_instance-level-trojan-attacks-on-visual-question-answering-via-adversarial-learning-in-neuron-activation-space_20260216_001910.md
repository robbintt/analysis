---
ver: rpa2
title: Instance-Level Trojan Attacks on Visual Question Answering via Adversarial
  Learning in Neuron Activation Space
arxiv_id: '2304.00436'
source_url: https://arxiv.org/abs/2304.00436
tags:
- attack
- trojan
- ne-tuning
- trojans
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose an instance-level Trojan attack method for
  multimodal learning, specifically targeting Visual Question Answering (VQA) models.
  The method generates diverse Trojans across input samples and modalities by perturbing
  specific neurons in a pretrained model and using adversarial learning to establish
  a correlation between these perturbations and malicious outputs in a fine-tuned
  model.
---

# Instance-Level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space

## Quick Facts
- arXiv ID: 2304.00436
- Source URL: https://arxiv.org/abs/2304.00436
- Reference count: 31
- Key outcome: Instance-level Trojan attack method for multimodal learning targeting VQA models

## Executive Summary
This paper proposes an instance-level Trojan attack method for Visual Question Answering (VQA) models that generates diverse Trojans across input samples and modalities. The attack works by over-activating specific neurons in a perturbation layer of a pretrained model and using adversarial learning to establish a correlation between these perturbations and malicious outputs in a fine-tuned model. Experiments on the VQA-v2 dataset demonstrate that the method can effectively compromise fine-tuned models with minimal samples while maintaining high performance on clean data.

## Method Summary
The method generates instance-level Trojans by perturbing specific neurons in a fusion layer input of a pretrained VQA model. For each image-question pair, small perturbations are computed and a single text token is modified to push the respective perturbation neurons into high activation states. Dual-modality adversarial learning is then used to update the fine-tuned layers, establishing a correlation between these overactive neurons and the malicious output. The attack is designed to be sample-efficient, stealthy, and robust to model fine-tuning.

## Key Results
- Achieves high attack success rates (ATA) while maintaining main task accuracy (MTA) on clean data
- Demonstrates effectiveness with minimal samples during fine-tuning
- Shows robustness to model fine-tuning through adversarial learning

## Why This Works (Mechanism)

### Mechanism 1
Over-activating specific neurons in the perturbation layer creates a measurable signal that can be leveraged for adversarial learning. The method selects two neurons—one for vision, one for text—in a fusion-layer input, then generates Trojans that push these neurons into high activation states (e.g., 10), far above their normal range (-2 to 2). This over-activation is used as a trigger for adversarial learning. The core assumption is that large activation values of the selected neurons correlate with the model's final output and can be manipulated to influence predictions. Evidence shows the method "compromises two specific neurons in a specific perturbation layer in the pretrained model to produce overly large neuron activations." The break condition is if the fusion layer is removed or heavily modified during fine-tuning, the perturbation neuron activations no longer affect the final prediction.

### Mechanism 2
Adversarial learning can re-establish a connection between perturbation neuron states and malicious model outputs after fine-tuning. Using dual-modality adversarial training, the model is updated on both clean and Trojan samples, with a loss term that penalizes the model for failing to map the over-activated neurons to the target malicious output. The core assumption is that fine-tuning can be steered by introducing a small set of adversarial samples that maintain the Trojan effect despite architectural changes. Evidence shows "dual-modality adversarial learning is performed to update all layers after the perturbation layer based on the following loss function." The break condition is if the adversarial loss weight (β) is too low or too high, the model may ignore the Trojan signal or overfit to it.

### Mechanism 3
Instance-level Trojans tailored per sample are more stealthy than global, input-agnostic triggers. For each image-question pair, a small perturbation is computed and a single text token is modified to push the respective perturbation neurons into high activation states. The core assumption is that small, sample-specific perturbations remain imperceptible and do not shift the input distribution significantly. Evidence shows the method "generates diverse Trojans across input samples and modalities." The break condition is if perturbations accumulate or become detectable via statistical tests, the Trojan may be identified and removed.

## Foundational Learning

- **Neuron activation space and its role in multimodal fusion**: Why needed here - the attack relies on precise control over activations in a fusion layer that integrates vision and text representations. Quick check: What happens to neuron activations when a multimodal input is perturbed at the fusion layer?
- **Adversarial learning for backdoor persistence**: Why needed here - ensures the Trojan survives fine-tuning by re-learning the mapping from perturbation neurons to malicious outputs. Quick check: How does the adversarial loss term influence the model's decision boundary?
- **Instance-level optimization vs global trigger generation**: Why needed here - tailoring Trojans to each sample improves stealth and variation, reducing detectability. Quick check: Why might a single global trigger be less effective in multimodal settings?

## Architecture Onboarding

- **Component map**: Pretrained VQA backbone (vision encoder, text encoder, fusion network) -> Perturbation layer (input to fusion, preserved during fine-tuning) -> Fine-tuned layers (new FC layers replacing fusion output) -> Adversarial learning loop (updates fine-tuned layers based on clean + Trojan samples)
- **Critical path**: 1) Pretrain VQA on large dataset, 2) Identify perturbation neurons in fusion input, 3) Generate instance-level vision and text Trojans, 4) Fine-tune model with added adversarial loss, 5) Test on clean and Trojan-embedded inputs
- **Design tradeoffs**: Perturbation layer choice (deeper layers may be more robust but harder to influence), Step size (α) and iterations (E) (balance between stealth and effectiveness), Adversarial loss weight (β) (trade-off between maintaining main task accuracy and attack success)
- **Failure signatures**: MTA drops significantly during fine-tuning, ATA remains high without adversarial learning, Trojan detection via neuron activation monitoring
- **First 3 experiments**: 1) Verify over-activation of selected neurons with sample Trojans, 2) Measure MTA/ATA with and without adversarial learning after fine-tuning, 3) Test robustness across different numbers of fine-tuning layers

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed attack method perform against other multimodal models beyond VQA, such as self-supervised multimodal architectures? The authors mention in the conclusion that they aim to extend the attack to other self-supervised multimodal architectures in the future. The current study focuses specifically on VQA models, so the attack's effectiveness against other types of multimodal models remains unknown. Experiments applying the attack method to various self-supervised multimodal architectures and comparing their robustness to the VQA models tested in this study would resolve this.

### Open Question 2
What is the impact of the attack on models with different fusion network architectures, such as those using attention mechanisms or more complex fusion layers? The study uses a simple fully-connected fusion network for the VQA model, but doesn't explore how the attack performs with more complex fusion architectures. Different fusion network architectures may have varying levels of vulnerability to the proposed attack, which could affect the attack's overall effectiveness. Experiments applying the attack to VQA models with different fusion network architectures and comparing their robustness to the model used in this study would resolve this.

### Open Question 3
How does the attack perform when the perturbation layer is not the input layer of the fusion network, but a deeper layer within the fusion network? The study selects the input layer of the fusion network as the perturbation layer, but doesn't explore the impact of choosing a deeper layer within the fusion network. The choice of perturbation layer may affect the attack's ability to establish a correlation between the perturbed neurons and the malicious output in the fine-tuned model. Experiments applying the attack with different perturbation layers within the fusion network and comparing their effectiveness to the input layer used in this study would resolve this.

### Open Question 4
How does the attack's performance change when the fine-tuning process involves more than just replacing the last few layers of the pretrained model? The study assumes that fine-tuning involves replacing and fine-tuning only the last few layers of the pretrained model, but doesn't explore the impact of more extensive fine-tuning. More extensive fine-tuning may affect the attack's ability to maintain the efficacy of the Trojans after fine-tuning, potentially reducing its overall effectiveness. Experiments applying the attack to models with varying degrees of fine-tuning and comparing their robustness to the models tested in this study would resolve this.

## Limitations
- The attack's effectiveness may be disrupted if fine-tuning significantly alters the model's architecture or training dynamics
- Limited discussion of how instance-level perturbations might be detected through statistical analysis or neuron activation monitoring
- Additional hyperparameters (such as adversarial loss weight β) are not thoroughly explored, potentially affecting robustness

## Confidence
- **High Confidence**: The mechanism of generating instance-level Trojans by perturbing specific neurons in the fusion layer is well-defined and supported by the paper's description and experimental setup
- **Medium Confidence**: The claim that adversarial learning effectively re-establishes the Trojan correlation after fine-tuning is supported by experimental results, but the robustness of this approach to different fine-tuning scenarios and model architectures is not extensively validated
- **Low Confidence**: The assertion that instance-level Trojans are more stealthy than global triggers is plausible but not rigorously compared to alternative attack methods in the paper

## Next Checks
1. **Neuron Activation Stability**: Conduct experiments to assess how stable the perturbation neuron activations remain across different fine-tuning strategies and model architectures. Measure the variance in activation levels and their impact on attack success rates.
2. **Adversarial Loss Sensitivity**: Systematically vary the adversarial loss weight (β) and evaluate its effect on both the main task accuracy and attack success rate. Identify the optimal range of β values that balance task performance and Trojan persistence.
3. **Trojan Detectability Analysis**: Implement statistical tests to detect Trojan samples based on neuron activation patterns or perturbation magnitudes. Compare the proposed instance-level approach against global trigger methods in terms of stealthiness and resistance to detection.