---
ver: rpa2
title: Translating Regulatory Clauses into Executable Codes for Building Design Checking
  via Large Language Model Driven Function Matching and Composing
arxiv_id: '2308.08728'
source_url: https://arxiv.org/abs/2308.08728
tags:
- functions
- building
- space
- function
- clauses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-FuncMapper, a method for automatically
  translating complex regulatory clauses into executable code using large language
  models (LLMs). The approach addresses the challenge of interpreting clauses with
  implicit properties or complex computational logic by first defining 66 atomic functions
  to capture common computational patterns.
---

# Translating Regulatory Clauses into Executable Codes for Building Design Checking via Large Language Model Driven Function Matching and Composing

## Quick Facts
- arXiv ID: 2308.08728
- Source URL: https://arxiv.org/abs/2308.08728
- Reference count: 40
- Key outcome: LLM-FuncMapper automatically translates regulatory clauses into executable code, outperforming fine-tuning by 19% in function matching while reducing manual annotation efforts.

## Executive Summary
This paper introduces LLM-FuncMapper, a novel approach that leverages large language models (LLMs) to automatically translate complex building regulatory clauses into executable code for automated rule checking. The method addresses the challenge of interpreting clauses with implicit properties or complex computational logic by defining 66 atomic functions that encapsulate common computational patterns in building codes. Through domain-specific prompt engineering and a classification-based tuning strategy, LLM-FuncMapper matches clauses to appropriate atomic functions and generates executable code, achieving almost 100% interpretation of computer-processable clauses in a case study.

## Method Summary
The LLM-FuncMapper method systematically decomposes building code clauses into 66 predefined atomic functions that capture common computational patterns. Domain-specific prompt engineering with chain-of-thought reasoning and classification-based tuning is applied to guide LLMs in identifying relevant atomic functions from the database. The identified functions are then composed into executable Python code using IFCOpenShell for building design checking. The approach employs a classification-based tuning strategy to handle large function databases by first classifying clauses into categories and then refining prompts based on keyword matching, reducing the prompt length to fit within LLM token limits.

## Key Results
- LLM-FuncMapper outperforms fine-tuning approaches by 19% in function matching performance
- The method significantly reduces manual annotation efforts required for automated rule checking
- Case study demonstrates almost 100% interpretation success rate for computer-processable clauses
- Classification-based prompt tuning effectively addresses LLM token length limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The atomic function database captures complex computational logic that first-order logic cannot express.
- Mechanism: By decomposing clauses into atomic functions, the system can represent implicit properties and complex constraints as explicit, executable code blocks.
- Core assumption: Building code clauses can be systematically analyzed to identify shared computational patterns.
- Evidence anchors: [abstract] "by systematically analyzing building clauses, 66 atomic functions are defined first to encapsulate common computational logics"
- Break condition: If clauses cannot be decomposed into discrete atomic functions, or if implicit properties cannot be captured by any defined function.

### Mechanism 2
- Claim: Domain-specific prompt engineering enables LLMs to identify relevant atomic functions from the database.
- Mechanism: The prompt template injects domain knowledge, uses chain of thought prompting, and employs classification-based tuning to guide LLM function identification.
- Core assumption: LLMs can be effectively guided to understand domain-specific function databases through carefully crafted prompts.
- Evidence anchors: [abstract] "LLM-FuncMapper is proposed, a large language model (LLM)-based approach with rule-based adaptive prompts that match clauses to atomic functions"
- Break condition: If LLMs cannot effectively understand the prompt instructions, or if the function database is too large for the prompt length limits.

### Mechanism 3
- Claim: The combination of atomic functions and LLM function identification enables automated rule checking.
- Mechanism: LLM-identified atomic functions are composed into executable code that can check building designs against regulatory clauses.
- Core assumption: The identified functions can be correctly composed to create valid checking code.
- Evidence anchors: [abstract] "executable code is generated by composing functions through the LLMs"
- Break condition: If the composition of identified functions does not produce valid checking code, or if the checking results are incorrect.

## Foundational Learning

- Concept: Natural Language Processing (NLP) techniques for clause interpretation
  - Why needed here: The system needs to understand and parse complex regulatory clauses expressed in natural language
  - Quick check question: How does the system handle clauses with implicit properties that require domain knowledge?

- Concept: Function composition and code generation
  - Why needed here: Identified atomic functions must be combined to create executable checking code
  - Quick check question: What ensures that the composed functions correctly represent the original clause logic?

- Concept: Large Language Model (LLM) prompting strategies
  - Why needed here: Effective prompts are crucial for guiding LLMs to correctly identify atomic functions
  - Quick check question: How does the chain of thought prompting improve LLM performance on this task?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Atomic function database -> Prompt engineering module -> LLM interface -> Code generation and execution module

- Critical path:
  1. Preprocess regulatory clauses
  2. Use LLM with prompt engineering to identify relevant atomic functions
  3. Compose identified functions into executable code
  4. Execute code to check building designs

- Design tradeoffs:
  - Atomic function granularity vs. function database size
  - Prompt complexity vs. LLM token limits
  - Classification accuracy vs. prompt refinement effectiveness

- Failure signatures:
  - LLM identifies incorrect functions (low recall@5)
  - Composed code fails to execute or produces incorrect results
  - Classification errors lead to irrelevant functions being considered

- First 3 experiments:
  1. Test LLM function identification accuracy on a small set of clauses with known ground truth
  2. Validate that composed functions produce correct checking results on sample building models
  3. Evaluate the effectiveness of classification-based prompt tuning by comparing full vs. refined prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLM-FuncMapper perform when applied to building codes from different countries or regions, beyond the Chinese Code for fire protection design of buildings (GB 50016-2014)?
- Basis in paper: [explicit] The authors state that the current function database is based on Chinese regulations and suggest that future research can focus on enriching the database to accommodate regulatory requirements of different domains.
- Why unresolved: The study only validates the approach on Chinese building codes, and it is unclear how well the method generalizes to other countries' building codes with different terminologies, structures, and requirements.
- What evidence would resolve it: Applying LLM-FuncMapper to building codes from multiple countries and comparing its performance across different regulatory contexts would provide insights into its generalizability and limitations.

### Open Question 2
- Question: What is the impact of the classification-based prompt tuning strategy on the performance of LLM-FuncMapper when the function database becomes very large?
- Basis in paper: [explicit] The authors mention that the prompt-based domain knowledge injection method may become impractical due to the maximum token limit of LLMs when the function database is extensive.
- Why unresolved: The study demonstrates the effectiveness of the classification-based tuning strategy on the current function database, but it is unclear how well it scales to larger databases with hundreds or thousands of functions.
- What evidence would resolve it: Conducting experiments with progressively larger function databases and evaluating the performance of LLM-FuncMapper with and without the classification-based tuning strategy would provide insights into its scalability and limitations.

### Open Question 3
- Question: How does the performance of LLM-FuncMapper compare to other state-of-the-art methods for automated rule checking and code generation, such as those based on deep learning or formal logic?
- Basis in paper: [inferred] The authors compare the performance of LLM-FuncMapper to fine-tuning methods and show that it outperforms them by 19% in function matching. However, they do not compare it to other approaches specifically designed for automated rule checking.
- Why unresolved: The study focuses on demonstrating the effectiveness of LLM-FuncMapper compared to fine-tuning methods, but it is unclear how it fares against other advanced techniques in the field of automated rule checking and code generation.
- What evidence would resolve it: Conducting a comprehensive comparative study between LLM-FuncMapper and other state-of-the-art methods for automated rule checking, such as those based on deep learning or formal logic, would provide insights into its relative strengths and weaknesses.

## Limitations
- The atomic function database of 66 functions may not comprehensively cover all computational patterns in building codes
- The approach is only validated on Chinese building codes, raising generalizability concerns
- Reliance on LLM APIs introduces potential variability in performance across providers and versions
- Performance on clauses requiring extensive domain knowledge or with highly implicit properties is unclear

## Confidence

- **High confidence**: The atomic function decomposition approach and basic LLM function matching mechanism are sound and well-supported by experimental results
- **Medium confidence**: The classification-based prompt tuning strategy shows effectiveness, but specific implementation details are not fully specified
- **Medium confidence**: The automated rule checking workflow is demonstrated, but validation is limited to a case study rather than comprehensive testing

## Next Checks

1. **Cross-domain generalization test**: Evaluate LLM-FuncMapper on building codes from different jurisdictions (e.g., International Building Code) to assess the portability of the atomic function database and prompt engineering approach.

2. **Error analysis on complex clauses**: Systematically analyze cases where the LLM fails to identify correct functions, particularly for clauses with implicit properties or complex computational logic, to identify patterns and refine the atomic function database.

3. **Scalability assessment**: Test the system's performance on larger building models and more complex code sections to evaluate computational efficiency and identify potential bottlenecks in the function composition and code execution pipeline.