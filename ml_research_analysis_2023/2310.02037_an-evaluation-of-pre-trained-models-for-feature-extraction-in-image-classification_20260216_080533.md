---
ver: rpa2
title: An evaluation of pre-trained models for feature extraction in image classification
arxiv_id: '2310.02037'
source_url: https://arxiv.org/abs/2310.02037
tags:
- dataset
- images
- pre-trained
- figure
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the performance of 16 pre-trained neural
  network models for feature extraction in image classification tasks across four
  datasets: Geological Images, Stanford Cars, CIFAR-10, and STL10. The study compares
  models including CLIP-ViT-B, ViT-H-14, and various CNN architectures like ResNet50
  and Inception V3.'
---

# An evaluation of pre-trained models for feature extraction in image classification

## Quick Facts
- arXiv ID: 2310.02037
- Source URL: https://arxiv.org/abs/2310.02037
- Reference count: 40
- Pre-trained models for feature extraction in image classification tasks

## Executive Summary
This paper evaluates 16 pre-trained neural network models for feature extraction across four image classification datasets: Geological Images, Stanford Cars, CIFAR-10, and STL10. The study compares transformer-based models (CLIP-ViT-B, ViT-H-14, CLIP-ResNet50) with CNN architectures (ResNet50, Inception V3, etc.) using frozen feature extraction and training new classifier heads. Results demonstrate that transformer-based models outperform CNN-based models, with CLIP-ViT-B and ViT-H-14 achieving the best overall performance. Stanford Cars emerged as the most challenging dataset due to its large number of classes, few samples per class, and varying image sizes.

## Method Summary
The study employed a feature extraction approach where 16 pre-trained models were used to extract features from images without updating their base layers. Each model's last layer was replaced with a new classifier head, and the models were evaluated using 5-fold cross-validation on four datasets. The evaluation used accuracy, macro F1-measure, and weighted F1-measure as metrics. Training was performed with Adam optimizer (learning rate 0.001, momentum 0.9) for 100 epochs with early stopping. The models included both CNN architectures (ResNet50, Inception V3, etc.) and transformer-based models (CLIP-ViT-B, ViT-H-14, CLIP-ResNet50).

## Key Results
- Transformer-based models (CLIP-ViT-B and ViT-H-14) achieved the best overall performance across all datasets
- CLIP-ResNet50 showed similar performance to top models but with less variability in results
- Stanford Cars dataset was the most challenging, with large class count, few samples per class, and varying image sizes
- Transformer-based models consistently outperformed CNN-based models, particularly on the Geological Images dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning with frozen feature extractors achieves strong classification performance with reduced training cost.
- Mechanism: Pre-trained models capture generalizable visual features on large datasets that are reused in new domains without fine-tuning the base layers.
- Core assumption: Visual features are domain-agnostic and transfer across tasks with minimal adaptation.
- Evidence anchors: Abstract states transfer learning leverages knowledge from large datasets; section confirms frozen pre-trained models are used for feature extraction.
- Break condition: If target domain's visual statistics differ significantly from the pre-training dataset.

### Mechanism 2
- Claim: Transformer-based architectures outperform CNN-only models in cross-domain image classification when used as feature extractors.
- Mechanism: Transformers capture long-range spatial dependencies and global context through self-attention, benefiting classification in datasets with high intra-class variability.
- Core assumption: Self-attention mechanisms provide richer contextual embeddings than CNN local receptive fields.
- Evidence anchors: Abstract notes performance differences between transformer and CNN architectures; section identifies three transformer-based models among evaluated models.
- Break condition: When dataset is small, uniform, and class-specific features are localized.

### Mechanism 3
- Claim: Dataset complexity drives performance gaps between pre-trained models.
- Mechanism: High class count and few samples per class increase overfitting risk; models with stronger generalization maintain performance.
- Core assumption: Performance variance correlates with dataset difficulty metrics (class count, imbalance, size variance).
- Evidence anchors: Abstract identifies Stanford Cars as most challenging due to large classes, few samples, and varying sizes; section provides specific dataset characteristics.
- Break condition: If target dataset is balanced, high-sample, and homogeneous.

## Foundational Learning

- Concept: Feature extraction vs. fine-tuning
  - Why needed here: The paper compares frozen-feature extraction to no alternative; understanding trade-offs is key to interpreting results.
  - Quick check question: In feature extraction, are the base layers updated during training? (Answer: No.)

- Concept: Transfer learning assumptions
  - Why needed here: Performance depends on pre-training dataset relevance; mismatched domains degrade gains.
  - Quick check question: If pre-training data distribution differs greatly from target, will transfer learning help? (Answer: Likely not.)

- Concept: Model architecture differences (CNN vs. Transformer)
  - Why needed here: The study highlights transformer superiority; understanding self-attention vs. convolution helps explain the gap.
  - Quick check question: Do CNNs use global receptive fields like transformers? (Answer: No, they use local filters.)

## Architecture Onboarding

- Component map: Input Image → Resize/Center-Crop → Normalize → Feature Extractor (frozen) → Flatten → Classifier Head (trainable) → Softmax → Prediction
- Critical path: Preprocess → Extract → Classify → Evaluate. Bottleneck is backbone inference; classifier training is cheap.
- Design tradeoffs: Frozen vs. fine-tuned (speed/lower compute vs. potentially higher accuracy); Backbone choice (generalist vs. specialist); Classifier head (simple linear vs. multi-layer perceptron).
- Failure signatures: Low accuracy across all models → Dataset too hard or preprocessing broken; One model vastly outperforms others → Architecture-domain fit; High variance in cross-validation → Overfitting or insufficient samples.
- First 3 experiments:
  1. Run CLIP-ViT-B and AlexNet on Geological Images; compare accuracy and macro-F1.
  2. Repeat on STL10; expect near-perfect performance for all models.
  3. Try fine-tuning CLIP-ResNet50 on Stanford Cars; measure if accuracy improves over frozen.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural differences between transformer-based models and CNN-based models account for the observed performance differences in feature extraction?
- Basis in paper: The authors note that transformer-based models significantly outperform CNN-based models, particularly in the Geological Images dataset.
- Why unresolved: The paper identifies transformer-based models as superior but does not provide detailed analysis of which specific architectural features drive this improvement.
- What evidence would resolve it: Controlled ablation studies comparing different architectural components across multiple datasets, combined with visualizations of feature representations.

### Open Question 2
- Question: How do the pre-training datasets (ImageNet vs. WebImageText) influence the feature extraction capabilities of models when applied to geological image classification?
- Basis in paper: The paper mentions CLIP models were pre-trained on WebImageText while other models used ImageNet-1K.
- Why unresolved: The authors observe CLIP models perform well but do not isolate whether this is due to the transformer architecture or the broader pre-training data.
- What evidence would resolve it: Comparative experiments training CNN models on WebImageText or CLIP models on ImageNet.

### Open Question 3
- Question: Why does Stanford Cars consistently show the worst performance across all models, and what dataset-specific factors contribute most to this difficulty?
- Basis in paper: The authors identify Stanford Cars as most challenging due to large number of classes, few samples per class, and varying image sizes.
- Why unresolved: The paper lists potential reasons but does not quantify the relative impact of each factor.
- What evidence would resolve it: Systematic experiments modifying dataset properties to identify the primary bottleneck.

## Limitations

- Unknown preprocessing pipeline implementation details (resize, center crop, normalization parameters)
- Lack of statistical significance testing to validate performance differences between models
- No exploration of fine-tuning alternatives that might yield better results for challenging datasets

## Confidence

- High confidence: Feature extraction with frozen pre-trained models works effectively for transfer learning tasks
- Medium confidence: Transformer-based models show better performance patterns, but specific advantages depend on dataset characteristics
- Medium confidence: Stanford Cars dataset is the most challenging, though this conclusion relies on limited metrics without statistical validation

## Next Checks

1. Implement statistical significance testing (paired t-tests or Wilcoxon signed-rank) across 5-fold results to verify performance differences between top models are not due to chance
2. Compare frozen feature extraction results against fine-tuning the same models on Stanford Cars dataset to measure potential performance gains
3. Test preprocessing pipeline sensitivity by varying image resize dimensions and normalization parameters to establish robustness boundaries