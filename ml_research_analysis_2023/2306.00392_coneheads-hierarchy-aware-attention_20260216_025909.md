---
ver: rpa2
title: 'Coneheads: Hierarchy Aware Attention'
arxiv_id: '2306.00392'
source_url: https://arxiv.org/abs/2306.00392
tags:
- attention
- cone
- product
- cones
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes cone attention, a hierarchy-aware attention
  operator based on hyperbolic entailment cones. The key idea is to use the lowest
  common ancestor (LCA) of points in a hierarchy defined by hyperbolic cones to compute
  attention scores, which better captures hierarchical relationships between data
  points compared to dot product attention.
---

# Coneheads: Hierarchy Aware Attention

## Quick Facts
- arXiv ID: 2306.00392
- Source URL: https://arxiv.org/abs/2306.00392
- Reference count: 40
- Key outcome: Cone attention improves transformer performance with fewer parameters by encoding hierarchical relationships using hyperbolic entailment cones

## Executive Summary
Coneheads proposes cone attention, a hierarchy-aware attention mechanism that leverages hyperbolic geometry to better capture relationships between data points. Unlike standard dot product attention that treats all features equally, cone attention uses the lowest common ancestor in hyperbolic space to compute attention scores, making it particularly effective for hierarchical data. The approach demonstrates consistent improvements across NLP, vision, and graph prediction tasks while using fewer parameters than traditional attention mechanisms.

## Method Summary
The paper introduces cone attention as a drop-in replacement for dot product attention in transformer architectures. It maps Euclidean input vectors to hyperbolic space using exponential maps or other transformations, then computes attention scores based on the depth of the lowest common ancestor between points in the partial ordering defined by hyperbolic entailment cones. This captures hierarchical relationships more effectively than dot products while maintaining computational tractability. The method works across different transformer variants including ViT, DeiT, and GAT architectures.

## Key Results
- +1 BLEU improvement on IWSLT'14 German-English translation
- +2% ImageNet top-1 accuracy improvement on DeiT-Small model
- Matches dot product attention performance with 20% fewer parameters (31.2M vs 39.5M)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperbolic space provides exponential volume growth that matches hierarchical tree structures.
- Mechanism: The negative curvature of hyperbolic space causes the volume of a ball to grow exponentially with radius, mirroring how the number of leaves grows exponentially with depth in a tree. This allows hyperbolic embeddings to capture hierarchical relationships without distortion.
- Core assumption: The underlying data contains hierarchical relationships that can be approximated by a tree-like structure.
- Evidence anchors:
  - [abstract]: "Hyperbolic embeddings, which use the underlying geometric properties of hyperbolic space, give low-distortion embeddings of hierarchies that are not possible with Euclidean embeddings"
  - [section 2.2]: "the volume of a hyperbolic ball grows exponentially with respect to its radius; in a tree, the number of leaves grows exponentially with respect to depth"
- Break condition: If the data has no meaningful hierarchical structure or the hierarchy is too flat, the hyperbolic embedding advantage disappears.

### Mechanism 2
- Claim: Entailment cones create a partial ordering where similarity is based on lowest common ancestor depth.
- Mechanism: Points in hyperbolic space are associated by the depth of their lowest common ancestor in the partial ordering defined by hyperbolic entailment cones. This measures how recently two points diverged in the hierarchy, giving higher similarity to points with more recent common ancestors.
- Core assumption: The relationships between data points can be meaningfully represented as a partial order in hyperbolic space.
- Evidence anchors:
  - [abstract]: "Cone attention associates two points by the depth of their lowest common ancestor in a hierarchy defined by hyperbolic cones"
  - [section 3.1]: "We wish to associate u, v ∈ Hd by their LCA in some latent tree T , which is analogous to finding their LCA, denoted sup2(u, v), in the partial ordering defined by entailment cones"
- Break condition: If the data relationships cannot be represented as a partial order, or if the entailment cone construction fails to capture the true relationships.

### Mechanism 3
- Claim: Cone attention achieves similar performance to dot product attention with fewer parameters by more efficiently capturing hierarchical information.
- Mechanism: By encoding hierarchical relationships directly into the attention computation, cone attention can achieve comparable task performance with lower embedding dimensions. This reduces the number of parameters needed while maintaining or improving performance.
- Core assumption: Hierarchical relationships are important for the task and can be captured more efficiently than through learned dot products.
- Evidence anchors:
  - [abstract]: "cone attention can match dot product attention with significantly fewer parameters, enabling smaller models"
  - [section 5.2]: "cone attention is able to match dot product attention with only d = 16. For this model, using 16 dimensions reduces the number of parameters from 39.5M to 31.2M"
- Break condition: If hierarchical information is not important for the task, or if the additional complexity of cone attention outweighs its benefits.

## Foundational Learning

- Concept: Hyperbolic geometry and Riemannian manifolds
  - Why needed here: The paper relies on properties of hyperbolic space (negative curvature, exponential volume growth) to create hierarchy-aware embeddings
  - Quick check question: What is the key geometric property of hyperbolic space that makes it suitable for embedding hierarchical structures?

- Concept: Partial orderings and lattice theory
  - Why needed here: The attention mechanism uses entailment cones to create a partial ordering where similarity is based on lowest common ancestor depth
  - Quick check question: How does the lowest common ancestor in a partial ordering relate to measuring similarity between two points?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The paper proposes a drop-in replacement for dot product attention, so understanding standard attention is crucial
  - Quick check question: What is the mathematical form of standard dot product attention and how does it compute similarity?

## Architecture Onboarding

- Component map: Input vectors → Mapping function (ψ or ξ) → Hyperbolic space → Cone attention computation → Attention weights → Value aggregation
- Critical path: Mapping function → Cone attention computation → Attention weight normalization → Value aggregation
- Design tradeoffs:
  - Parameter efficiency vs. computational complexity: Cone attention uses fewer parameters but is computationally more expensive
  - Hierarchical modeling vs. flexibility: More explicit hierarchy modeling vs. learned representations
  - Mapping function choice: Different mappings (exponential map, ψ, ξ) have different numerical stability properties
- Failure signatures:
  - NaN values in attention computation (often from mapping function instabilities)
  - Poor convergence during training (may indicate mapping function issues or insufficient hierarchy in data)
  - No performance improvement over dot product attention (suggests hierarchy is not important for the task)
- First 3 experiments:
  1. Implement cone attention with the ψ mapping function on a small NLP task and compare to dot product attention
  2. Test different mapping functions (ψ vs ξ) on the same task to evaluate numerical stability
  3. Reduce embedding dimensions with cone attention to verify parameter efficiency benefits

## Open Questions the Paper Calls Out
The paper explicitly states that it remains to be seen how cone attention scales to very large models with billions of parameters, and that hyperbolic embeddings are sensitive to initialization based on [12] and [39], though it does not provide specific experimental results addressing these limitations.

## Limitations
- Computational complexity scales quadratically with sequence length, potentially limiting applicability to long sequences
- Performance benefits are task-dependent and most dramatic on smaller models rather than larger architectures
- Relies heavily on hyperbolic geometry assumptions that may not hold for all datasets, particularly those without clear hierarchical structure

## Confidence

**High Confidence Claims:**
- Hyperbolic space provides better embeddings for hierarchical data compared to Euclidean space
- Cone attention can match dot product attention performance with fewer parameters
- The connection between dot product attention and α-approximate rank problem is mathematically sound

**Medium Confidence Claims:**
- Cone attention consistently improves task performance across all tested domains
- The specific hyperparameter choices (γ = 0.4 for NLP, γ = 0.5 for vision) are optimal
- The umbral variant with ψ mapping function is superior to the penumbral variant with ξ mapping

**Low Confidence Claims:**
- Cone attention will generalize equally well to all future attention-based architectures
- The parameter efficiency benefits scale linearly with model size
- Hyperbolic embeddings are always preferable to learned dot product attention for hierarchical data

## Next Checks

1. **Scaling Analysis**: Test cone attention on larger transformer models (DeiT-Base, ViT-Large) to verify if the +2% ImageNet accuracy improvement holds at scale, or if the benefits diminish with model size.

2. **Computational Efficiency**: Benchmark the wall-clock training time and inference latency of cone attention versus dot product attention on long sequences (512+ tokens/patches) to quantify the practical trade-offs between parameter efficiency and computational cost.

3. **Hyperparameter Sensitivity**: Systematically vary the γ parameter across a wider range (0.1 to 1.0) and test multiple mapping functions on a held-out validation set to determine if the claimed optimal values are robust or task-specific.