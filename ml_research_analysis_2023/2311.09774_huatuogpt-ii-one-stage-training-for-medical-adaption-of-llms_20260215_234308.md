---
ver: rpa2
title: HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs
arxiv_id: '2311.09774'
source_url: https://arxiv.org/abs/2311.09774
tags:
- medical
- data
- huatuogpt-ii
- chinese
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HuatuoGPT-II, a one-stage training approach
  for adapting large language models (LLMs) to medical domains. The key challenge
  addressed is the inconsistency between pre-training and fine-tuning data distributions
  in traditional two-stage domain adaptation.
---

# HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs

## Quick Facts
- arXiv ID: 2311.09774
- Source URL: https://arxiv.org/abs/2311.09774
- Reference count: 23
- Outperforms Llama2 and ChatGPT on Chinese medical benchmarks, including state-of-the-art results on fresh 2023 Chinese National Pharmacist Licensure Examination

## Executive Summary
HuatuoGPT-II introduces a one-stage training approach for adapting large language models to medical domains, addressing the inconsistency between pre-training and fine-tuning data distributions in traditional two-stage domain adaptation. The method unifies heterogeneous data from both stages into a simple input-output pair format using LLM-powered data transformation. Evaluated on Chinese medical benchmarks, HuatuoGPT-II outperforms open-source LLMs like Llama2 and ChatGPT, achieving state-of-the-art results on the 2023 Chinese National Pharmacist Licensure Examination. Expert evaluations further validate its superiority, with HuatuoGPT-II winning 38% of cases against GPT-4 and 81% against its predecessor, HuatuoGPT.

## Method Summary
HuatuoGPT-II employs a one-stage training protocol that unifies pre-training and fine-tuning data into a single process. The method uses LLM-powered data transformation to convert heterogeneous medical corpora into standardized input-output pairs, ensuring consistency in language and genre. A priority sampling strategy dynamically adjusts the sampling probability during training, giving higher priority to pre-training data early in training and gradually shifting focus to fine-tuning data. The model is trained on 1.1TB of Chinese and English medical corpora, with 142K real-world medical questions and GPT-4-generated responses for fine-tuning.

## Key Results
- Outperforms open-source LLMs like Llama2 and ChatGPT on Chinese medical benchmarks
- Achieves state-of-the-art results on fresh 2023 Chinese National Pharmacist Licensure Examination
- Expert evaluations show HuatuoGPT-II winning 38% of cases against GPT-4 and 81% against HuatuoGPT

## Why This Works (Mechanism)

### Mechanism 1
Unified one-stage training reduces catastrophic forgetting by avoiding abrupt data distribution shifts between pre-training and fine-tuning. The model learns domain knowledge and instruction-following capabilities simultaneously from the same data format (instruction-output pairs), preventing the loss of pre-trained knowledge when adapting to medical tasks.

### Mechanism 2
Priority sampling enables efficient knowledge acquisition by first learning domain expertise before shifting to instruction-following capabilities. The sampling probability is dynamically adjusted during training, giving higher priority to pre-training instruction data early in training, then gradually shifting focus to fine-tuning data.

### Mechanism 3
LLM-powered data unification ensures consistent language and genre across heterogeneous medical corpora, improving model generalization. Large language models (e.g., ChatGPT) convert diverse medical texts into standardized question-answer pairs, aligning them with fine-tuning data format and language.

## Foundational Learning

- **Concept**: Catastrophic forgetting in sequential training
  - **Why needed here**: Understanding why traditional two-stage training fails helps explain the motivation for one-stage adaptation
  - **Quick check question**: What happens to a model's pre-trained knowledge when fine-tuning on a different data distribution?

- **Concept**: Data distribution alignment
  - **Why needed here**: The paper's core innovation relies on transforming heterogeneous data into a unified format
  - **Quick check question**: Why is it problematic when pre-training and fine-tuning data have different languages, genres, or formats?

- **Concept**: Priority-based sampling strategies
  - **Why needed here**: The model uses a dynamic sampling strategy to control the learning progression
  - **Quick check question**: How does adjusting sampling probability during training affect what the model learns first?

## Architecture Onboarding

- **Component map**: Data Collection Pipeline → Data Unification Module → Priority Sampling Strategy → One-Stage Training
- **Critical path**: Data Unification → Priority Sampling → Training Loop
- **Design tradeoffs**: Unified format simplifies training but may lose some pre-training advantages; priority sampling adds complexity but enables better knowledge acquisition sequencing; LLM-based unification ensures consistency but may introduce LLM biases or errors
- **Failure signatures**: High training loss fluctuations indicate data distribution issues; performance gaps between benchmarks suggest inadequate domain knowledge or instruction-following; low automatic evaluation scores indicate response quality problems
- **First 3 experiments**:
  1. Test data unification quality by comparing LLM-generated QA pairs against human-annotated pairs
  2. Evaluate different β values in priority sampling to find optimal knowledge-instruction balance
  3. Compare one-stage vs. two-stage training on a subset of data to verify performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HuatuoGPT-II perform on other specialized domains beyond medicine, such as finance or law?
- Basis in paper: [inferred] The paper mentions that the unified domain adaptation protocol can be similarly effective in other vertical domains (e.g., finance and law).
- Why unresolved: The paper only evaluates HuatuoGPT-II on medical benchmarks and does not provide any results for other domains.
- What evidence would resolve it: Testing HuatuoGPT-II on benchmarks from other specialized domains like finance or law would provide evidence of its effectiveness in these areas.

### Open Question 2
- Question: What is the long-term performance and potential degradation of HuatuoGPT-II over extended usage in medical applications?
- Basis in paper: [inferred] The paper does not address the long-term performance or potential degradation of HuatuoGPT-II.
- Why unresolved: Long-term performance and potential degradation are critical factors for practical deployment in medical applications, but the paper does not provide any data on this aspect.
- What evidence would resolve it: Conducting long-term studies and monitoring the performance of HuatuoGPT-II over extended periods in real-world medical applications would provide insights into its durability and potential degradation.

### Open Question 3
- Question: How does HuatuoGPT-II handle ethical considerations and patient privacy in medical consultations?
- Basis in paper: [explicit] The paper mentions that the data unification process mitigates potential ethical concerns inherent in the corpus.
- Why unresolved: While the paper addresses ethical concerns in data processing, it does not provide details on how HuatuoGPT-II handles ethical considerations and patient privacy during medical consultations.
- What evidence would resolve it: Providing information on the mechanisms and protocols implemented by HuatuoGPT-II to handle ethical considerations and protect patient privacy during medical consultations would address this question.

## Limitations

- Evaluation primarily on Chinese medical benchmarks limits generalizability to other languages and domains
- LLM-powered data transformation introduces potential quality control issues not fully addressed
- Comparison against GPT-4 and other proprietary models limited to subset of benchmarks and expert evaluations

## Confidence

- **High confidence**: Core methodology of data unification through LLM transformation is technically sound and well-documented; priority sampling strategy's implementation details are clearly specified
- **Medium confidence**: Reported benchmark performance improvements are impressive but primarily validated on Chinese medical datasets; expert evaluation methodology is described but could benefit from more detailed statistical analysis
- **Low confidence**: Generalization of results beyond Chinese medical domains and long-term stability of one-stage training approach under different data distributions

## Next Checks

1. **Cross-linguistic validation**: Test the one-stage training approach on English medical datasets and non-medical domains to verify the method's generalizability beyond Chinese medical applications
2. **Data transformation quality audit**: Conduct a systematic evaluation of LLM-generated question-answer pairs against human-annotated pairs to quantify the impact of potential generation errors or hallucinations on model performance
3. **Ablation study on sampling strategy**: Systematically vary the β parameter in the priority sampling strategy and evaluate its impact on both domain knowledge acquisition and instruction-following capabilities to identify optimal configurations