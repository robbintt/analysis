---
ver: rpa2
title: 'SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised
  Learning Using Gaussian Processes'
arxiv_id: '2311.01646'
source_url: https://arxiv.org/abs/2311.01646
tags:
- semigpc
- semi-supervised
- class
- simmatch
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemiGPC addresses class imbalance in semi-supervised learning by
  using Gaussian Processes to derive pseudo-labels from the labels posterior distribution.
  Unlike previous buffer-based methods, SemiGPC includes a normalization term that
  balances the global data distribution while maintaining local sensitivity, making
  it more robust to confirmation bias.
---

# SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes

## Quick Facts
- **arXiv ID**: 2311.01646
- **Source URL**: https://arxiv.org/abs/2311.01646
- **Reference count**: 36
- **Primary result**: SemiGPC achieves state-of-the-art results on CIFAR10-LT/CIFAR100-LT and challenging FGVC benchmarks, with an average accuracy increase of 2% compared to competitive baselines.

## Executive Summary
SemiGPC addresses class imbalance in semi-supervised learning by leveraging Gaussian Processes to derive pseudo-labels from the posterior distribution. Unlike buffer-based methods, it includes a normalization term that balances global data distribution while maintaining local sensitivity, making it robust to confirmation bias. The method achieves state-of-the-art results on standard CIFAR benchmarks and more challenging datasets like SemiAves, SemiCUB, SemiFungi, and Semi-iNat, with an average accuracy increase of 2% over competitive baselines.

## Method Summary
SemiGPC uses Gaussian Processes to refine pseudo-labels in semi-supervised learning, particularly for imbalanced datasets. It computes the posterior mean of a GP using an inverse covariance matrix K⁻¹ that normalizes kernel similarities across classes, counteracting imbalance. The method maintains local sensitivity by preserving cluster information in the weighted average of memory buffer samples. An efficient online update rule using the Woodbury identity reduces computational cost, enabling practical use with large memory buffers. SemiGPC can be paired with various semi-supervised methods and pre-training strategies.

## Key Results
- Achieves state-of-the-art results on CIFAR10-LT/CIFAR100-LT benchmarks
- Outperforms competitive baselines by an average of 2% accuracy on challenging FGVC datasets (SemiAves, SemiCUB, SemiFungi, Semi-iNat)
- Maintains robust performance across varying imbalance ratios and data regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SemiGPC balances class distribution through a normalized kernel similarity that reweights minority and majority class contributions equally in the label refinement step.
- Mechanism: The posterior mean of a Gaussian Process is computed using the inverse covariance matrix K⁻¹. This normalization inherently balances the influence of samples from different classes by considering the global data distribution, not just local density.
- Core assumption: The inverse covariance matrix K⁻¹ effectively normalizes kernel similarities across class populations, counteracting imbalance without explicit class-balancing heuristics.
- Evidence anchors:
  - [abstract]: "SemiGPC includes a normalization term that addresses imbalances in the global data distribution while maintaining local sensitivity."
  - [section 3.2]: "Such normalization is particularly useful to counteract class imbalance as we show in Figure 3."
  - [corpus]: Weak - related papers discuss label refinement and imbalance, but none directly analyze Gaussian Process normalization as a balancing mechanism.
- Break condition: If the data is extremely imbalanced (e.g., >1000:1 ratio) or the kernel function does not capture relevant similarity structure, the normalization may fail to sufficiently balance contributions.

### Mechanism 2
- Claim: SemiGPC maintains local sensitivity by assigning high confidence to minority class regions if they are locally dense, even when surrounded by majority class samples globally.
- Mechanism: The GP posterior mean µGP(x) = k(h(x),hQ)K⁻¹yQ computes weighted contributions from all memory buffer samples, preserving local cluster information while the normalization K⁻¹ counteracts global imbalance.
- Core assumption: The RBF kernel with appropriate length scale captures meaningful local structure, and the GP posterior mean preserves this local information in its weighted average.
- Evidence anchors:
  - [section 3.2]: "SemiGPC correctly assigns nearby points to the minority classes despite the larger count of the majority class at a bigger scale, i.e. it has a better local sensitivity."
  - [section 3.2]: "Only the GP classifier is able to define confidence levels that are not biased toward the majority classes."
  - [corpus]: Weak - related papers discuss pseudo-labeling and imbalance, but none specifically analyze GP-based local sensitivity preservation.
- Break condition: If the local density of minority class samples is too low or the kernel length scale is poorly chosen, the method may not preserve local sensitivity.

### Mechanism 3
- Claim: SemiGPC's efficient online update rule using the Woodbury identity reduces computational cost from O(N³Q) to O(B³ + BN²Q + B²NQ), enabling practical use with large memory buffers.
- Mechanism: By exploiting the fact that only B samples change per batch, the update computes K⁻¹ₜ from K⁻¹ₜ₋₁ using block matrix operations, avoiding full matrix inversion each iteration.
- Core assumption: The memory buffer size Nq is large and batch size B is small (B ≪ Nq), making the computational savings significant.
- Evidence anchors:
  - [section 3.3]: "Using our efficient update rule results in ×7.5 speedup."
  - [section 3.3]: "the cost of the inverting Kt can be reduced to computing the inverse of a couple of B × B matrices which is in turn much more efficient when B ≪ Nq as is the case in our setting."
  - [corpus]: Weak - related papers discuss computational efficiency in SSL, but none analyze Woodbury-based GP updates specifically.
- Break condition: If B becomes comparable to Nq, or if the batch size varies significantly, the computational advantage diminishes.

## Foundational Learning

- Concept: Gaussian Process posterior mean computation with RBF kernel
  - Why needed here: The core of SemiGPC relies on computing GP posterior means to refine pseudo-labels in a way that balances class distributions.
  - Quick check question: Can you derive the posterior mean formula µGP(x) = k(h(x),hQ)K⁻¹yQ for a GP with Gaussian likelihood and RBF kernel?

- Concept: Matrix inversion lemma (Woodbury identity)
  - Why needed here: SemiGPC uses an efficient online update rule that relies on the Woodbury identity to avoid recomputing full matrix inverses each iteration.
  - Quick check question: Can you apply the Woodbury identity to update (A + UV)⁻¹ given A⁻¹, where U and V are low-rank matrices?

- Concept: Semi-supervised learning with pseudo-labeling and consistency regularization
  - Why needed here: SemiGPC is designed to improve pseudo-label refinement in existing SSL frameworks like SimMatch and FixMatch.
  - Quick check question: Can you explain how consistency regularization works in FixMatch, and where pseudo-label refinement fits into this framework?

## Architecture Onboarding

- Component map:
  Feature extractor h -> Memory buffer hQ, yQ -> Kernel function k -> GP posterior mean computation -> Efficient update rule

- Critical path:
  1. Extract features for labeled and unlabeled samples
  2. Update memory buffer with new samples
  3. Compute GP posterior mean for unlabeled samples using efficient update
  4. Use refined pseudo-labels in consistency loss

- Design tradeoffs:
  - Buffer size NQ vs. computational cost: Larger buffers capture more global information but increase update cost
  - Kernel hyperparameters (η, l, σ) vs. performance: Must be tuned for each dataset
  - Online update efficiency vs. exact computation: Efficient update sacrifices some precision for speed

- Failure signatures:
  - Poor performance on minority classes: Likely indicates kernel parameters not capturing minority class structure
  - Degraded performance with very large buffers: May indicate efficient update rule breaking down
  - No improvement over baseline: Could indicate poor kernel choice or buffer not capturing relevant information

- First 3 experiments:
  1. Run SimMatch baseline on CIFAR10-LT with γ=50, compare to SemiGPC variant
  2. Test class-balanced vs. standard buffer variants on CIFAR100-LT with varying γ
  3. Measure computational overhead of efficient GP update vs. naive implementation with different buffer sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SemiGPC be adapted to handle larger buffer sizes beyond 16K samples?
- Basis in paper: [explicit] The paper states that SemiGPC is limited to a maximum buffer size of around NQ = 16K due to the quadratic scaling of memory cost.
- Why unresolved: The paper mentions that using an ensemble of GPs with separate buffers could scale to NQ ~ 80K, but this is not fully explored.
- What evidence would resolve it: Experimental results demonstrating SemiGPC's performance with larger buffer sizes using an ensemble of GPs or alternative methods to reduce memory cost.

### Open Question 2
- Question: Can SemiGPC be combined with alternative definitions of confidence beyond the maximum softmax probability?
- Basis in paper: [explicit] The paper suggests that leveraging recent advances in efficient Neural Tangent Kernel (NTK) computation could be an open area of research for defining confidence.
- Why unresolved: The paper does not explore alternative confidence definitions or their impact on SemiGPC's performance.
- What evidence would resolve it: Comparative experiments using different confidence definitions and their effect on SemiGPC's accuracy and robustness.

### Open Question 3
- Question: How does SemiGPC perform on other imbalanced semi-supervised learning benchmarks beyond the ones tested in the paper?
- Basis in paper: [explicit] The paper evaluates SemiGPC on CIFAR10-LT, CIFAR100-LT, and several FGVC benchmarks, but does not explore other imbalanced semi-supervised learning benchmarks.
- Why unresolved: The paper focuses on specific datasets and does not generalize to other imbalanced semi-supervised learning scenarios.
- What evidence would resolve it: Experimental results demonstrating SemiGPC's performance on additional imbalanced semi-supervised learning benchmarks, such as those with different data modalities or class distributions.

## Limitations
- Reliance on carefully tuned kernel hyperparameters (η, l, σ) that require dataset-specific optimization
- Computational efficiency gains diminish when batch size B becomes comparable to buffer size Nq
- Normalization mechanism may struggle with extreme imbalance ratios (>1000:1) where local density information becomes sparse

## Confidence
- **High**: The computational efficiency claims (×7.5 speedup) and basic GP posterior formulation are well-supported by mathematical derivations and code implementation.
- **Medium**: The class balancing mechanism through K⁻¹ normalization is plausible but requires empirical validation across diverse imbalance scenarios.
- **Medium**: The local sensitivity preservation claim is supported by qualitative examples but needs systematic evaluation on minority class detection tasks.

## Next Checks
1. Conduct ablation studies on kernel hyperparameters across different imbalance ratios to quantify sensitivity to parameter choices.
2. Test performance degradation on extreme imbalance cases (γ > 200) to identify breaking points of the normalization mechanism.
3. Compare the efficient update rule's numerical precision against full matrix inversion across varying batch sizes to validate the computational accuracy tradeoff.