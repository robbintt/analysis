---
ver: rpa2
title: Fast Adversarial Training with Smooth Convergence
arxiv_id: '2308.12857'
source_url: https://arxiv.org/abs/2308.12857
tags:
- adversarial
- training
- overfitting
- methods
- catastrophic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses catastrophic overfitting in fast adversarial\
  \ training (FAT) when using large perturbation budgets (e.g., \u03BE=16/255). The\
  \ authors observe that catastrophic overfitting is accompanied by loss convergence\
  \ outliers and argue that a smooth loss convergence process is essential for stable\
  \ FAT."
---

# Fast Adversarial Training with Smooth Convergence

## Quick Facts
- arXiv ID: 2308.12857
- Source URL: https://arxiv.org/abs/2308.12857
- Reference count: 40
- Primary result: Achieves 32.95% robustness on CIFAR-10 with ξ=16/255, only 0.97% lower than PGD-AT while being 3.5x faster

## Executive Summary
This paper addresses catastrophic overfitting in fast adversarial training (FAT) when using large perturbation budgets. The authors observe that catastrophic overfitting is accompanied by loss convergence outliers and propose a novel oscillatory constraint called ConvergeSmooth to limit loss differences between adjacent epochs. They also introduce weight centralization to stabilize training without introducing hyperparameters. The methods are attack-agnostic and can improve training stability of various FAT techniques. Extensive experiments show the proposed methods efficiently avoid catastrophic overfitting and outperform previous FAT methods.

## Method Summary
The proposed method combines two components: ConvergeSmooth and weight centralization. ConvergeSmooth is an oscillatory constraint that limits the difference in classification loss between adjacent epochs, with a dynamic convergence stride that balances convergence speed and smoothing effectiveness. Weight centralization constrains current model weights to the average of previous epoch weights to reduce parameter space oscillations. The methods are designed to be attack-agnostic and can be combined with various FAT techniques like FGSM-RS and FGSM-MEP.

## Key Results
- Achieves 32.95% robustness on CIFAR-10 with ξ=16/255 against PGD-50 attack
- Only 0.97% lower than PGD-AT while being approximately 3.5x faster
- Prevents catastrophic overfitting in all tested scenarios with large perturbation budgets
- Improves training stability across multiple datasets (CIFAR-10, CIFAR-100, Tiny ImageNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smooth loss convergence prevents catastrophic overfitting in fast adversarial training.
- Mechanism: Limiting the difference in classification loss between adjacent epochs stabilizes training dynamics.
- Core assumption: Loss convergence instability is a primary driver of catastrophic overfitting.
- Evidence anchors:
  - [abstract]: "we observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers"
  - [section]: "we discover several typical phenomena of catastrophic overfitting: 1) Change from a smooth convergence state to an irregular fluctuation state"
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If loss difference between epochs is already minimal due to other regularization, additional smoothing may provide diminishing returns.

### Mechanism 2
- Claim: Weight centralization stabilizes adversarial training without introducing hyperparameters.
- Mechanism: Constraining current model weights to the average of previous epoch weights reduces parameter space oscillations.
- Core assumption: Models at different training epochs tend to have similar weights after convergence.
- Evidence anchors:
  - [section]: "models at different training epochs tend to have similar weights after convergence [37]"
  - [abstract]: "we further verify the effect of the proposed weight centralization on model stability"
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If training dynamics require rapid weight updates to escape local minima, weight centralization may hinder convergence.

### Mechanism 3
- Claim: Dynamic convergence stride balances convergence speed and smoothing effectiveness.
- Mechanism: Adjusting the threshold for applying smoothing constraints based on historical loss differences.
- Core assumption: Loss difference between adjacent epochs tends to decrease non-linearly during training.
- Evidence anchors:
  - [section]: "Considering that the loss difference dt−1 between two adjacent epochs tends to decrease non-linearly, γt should be a variable that varies during the training process"
  - [abstract]: "The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing"
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the loss difference pattern deviates significantly from the assumed non-linear decay, the dynamic adjustment may become suboptimal.

## Foundational Learning

- Concept: Adversarial training and its computational challenges
  - Why needed here: Understanding why fast adversarial training (single-step) is preferred over standard adversarial training (multi-step) despite stability issues.
  - Quick check question: What is the main computational advantage of FGSM-based fast adversarial training compared to PGD-based adversarial training?

- Concept: Catastrophic overfitting in adversarial training
  - Why needed here: Recognizing the specific failure mode where models suddenly lose robustness during training.
  - Quick check question: What are the typical signs that catastrophic overfitting is occurring during adversarial training?

- Concept: Loss convergence analysis and stability
  - Why needed here: Understanding how loss behavior across epochs relates to training stability.
  - Quick check question: How does the proposed method identify when to apply the convergence smoothing constraint?

## Architecture Onboarding

- Component map:
  Loss tracker -> Convergence evaluator -> Dynamic stride calculator -> Weight centralization module -> Model weights

- Critical path:
  1. Compute current batch loss
  2. Compare with stored previous epoch loss
  3. Evaluate convergence condition
  4. Apply smoothing constraint if condition met
  5. Update stored loss values

- Design tradeoffs:
  - Memory vs. precision: Storing all previous losses vs. using running averages
  - Strictness vs. flexibility: Fixed vs. dynamic convergence stride
  - Computational overhead vs. stability gain: Additional constraint evaluation

- Failure signatures:
  - Model still exhibits catastrophic overfitting despite smoothing
  - Training becomes excessively slow or fails to converge
  - Robustness improvements are minimal compared to computational cost

- First 3 experiments:
  1. Reproduce catastrophic overfitting on CIFAR-10 with ξ=16/255 using FGSM-RS baseline
  2. Implement basic ConvergeSmooth with fixed γ and evaluate stability improvement
  3. Test dynamic stride adjustment and compare performance against fixed stride

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic convergence stride γt behave across different training epochs and datasets, and can its behavior be modeled mathematically?
- Basis in paper: [explicit] The authors state that γt is designed to vary during training due to the nonlinear decay rate of loss functions, and they observe that catastrophic overfitting occurs when γt approaches infinity.
- Why unresolved: The paper provides empirical settings for γmax on different datasets but does not explain the theoretical basis for how γt should evolve during training or whether a universal formula could be derived.
- What evidence would resolve it: A mathematical model showing the relationship between γt, epoch number, loss decay rate, and dataset characteristics, validated across multiple architectures and datasets.

### Open Question 2
- Question: Can the ConvergeSmooth constraint be extended to work with other regularization techniques beyond adversarial training, such as those used in semi-supervised learning or domain adaptation?
- Basis in paper: [inferred] The authors propose ConvergeSmooth as an oscillatory constraint that limits loss differences between adjacent epochs, which is a general principle that could potentially apply to other training scenarios with convergence stability issues.
- Why unresolved: The paper focuses exclusively on applying ConvergeSmooth to fast adversarial training methods, without exploring its applicability to other machine learning contexts.
- What evidence would resolve it: Experiments demonstrating improved convergence stability and performance when applying ConvergeSmooth to semi-supervised learning, domain adaptation, or other training paradigms with convergence instability issues.

### Open Question 3
- Question: What is the theoretical relationship between weight centralization and the stability of the adversarial training process, and how does it interact with the ConvergeSmooth constraint?
- Basis in paper: [explicit] The authors propose weight centralization without introducing extra hyperparameters and suggest it works by restricting model weights to the average of previously trained models, claiming this leverages the stability of the initial training process.
- Why unresolved: The paper provides empirical justification but does not offer theoretical analysis of why weight centralization stabilizes training or how it interacts with the loss-based ConvergeSmooth constraint.
- What evidence would resolve it: Theoretical analysis showing how weight centralization affects the loss landscape during adversarial training, and experiments demonstrating the complementary effects of weight centralization and ConvergeSmooth when used together.

## Limitations

- The theoretical justification for why smooth loss convergence specifically prevents catastrophic overfitting, rather than being merely correlated with it, is not fully established.
- The exact contribution of each component (smoothing vs. weight centralization) is difficult to disentangle from the presented results.
- The dynamic stride adjustment mechanism's effectiveness across different datasets and architectures remains unverified.

## Confidence

**High confidence**: The experimental results showing improved robustness against various attacks are well-documented with appropriate comparisons to baselines. The computational efficiency gains are also clearly demonstrated.

**Medium confidence**: The proposed ConvergeSmooth constraint appears to work as described, but the exact contribution of each component (smoothing vs. weight centralization) is difficult to disentangle from the presented results.

**Low confidence**: The theoretical justification for why smooth loss convergence specifically prevents catastrophic overfitting, rather than being merely correlated with it, is not fully established. The dynamic stride adjustment mechanism's effectiveness across different datasets and architectures remains unverified.

## Next Checks

1. **Ablation Study**: Conduct a comprehensive ablation study isolating the contributions of ConvergeSmooth smoothing and weight centralization components to determine which provides the primary stability benefit.

2. **Cross-Dataset Generalization**: Test the proposed method on datasets beyond CIFAR and Tiny ImageNet (e.g., SVHN, ImageNet subsets) to verify its generalizability across different data distributions and model architectures.

3. **Alternative Smoothing Mechanisms**: Compare ConvergeSmooth against alternative loss smoothing approaches (e.g., exponential moving averages, window-based smoothing) to establish whether the specific formulation provides unique advantages.