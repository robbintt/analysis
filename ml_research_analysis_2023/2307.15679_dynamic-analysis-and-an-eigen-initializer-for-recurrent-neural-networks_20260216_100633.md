---
ver: rpa2
title: Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks
arxiv_id: '2307.15679'
source_url: https://arxiv.org/abs/2307.15679
tags:
- hidden
- neural
- recurrent
- networks
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the dynamics of hidden states in recurrent neural
  networks using eigen decomposition of the weight matrix. The authors analyze a linear
  state space model and extrapolate to nonlinear cases, proposing that long-term dependency
  is improved by increasing eigenvalues.
---

# Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks

## Quick Facts
- arXiv ID: 2307.15679
- Source URL: https://arxiv.org/abs/2307.15679
- Reference count: 29
- Primary result: New eigenvalue-based initializer improves RNN training on synthetic and real tasks

## Executive Summary
This paper introduces an eigenvalue-based initialization method for recurrent neural networks (RNNs) that improves long-term dependency learning. By analyzing the dynamics of hidden states through eigen decomposition of the weight matrix, the authors propose that larger eigenvalues (set to 0.95) and rotational dynamics help prevent vanishing gradients and promote better exploration of hidden state space. The method outperforms Xavier and Kaiming initializers on tasks including Tomita Grammars, pixel-by-pixel MNIST, and machine translation, showing both faster convergence and better local optima.

## Method Summary
The method involves decomposing the weight matrix using eigen decomposition, setting all eigenvalues to 0.95, and reconstructing the matrix through sequential 2D rotations between adjacent dimensions. The rotations are applied in n-1 steps for an n-dimensional hidden state space. This creates a weight matrix that maintains information propagation through time while avoiding collapse toward the origin. The approach is implemented as a custom weight initializer that can be applied to any RNN architecture (LSTM, GRU, or vanilla RNN).

## Key Results
- Improved convergence rate compared to Xavier and Kaiming initializers on Tomita Grammar 4 task
- Better final performance on pixel-by-pixel MNIST classification
- Enhanced BLEU scores on Multi30k German-English translation task
- Consistent gains across LSTM, GRU, and tanh-RNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing eigenvalue norms improves long-term dependency in recurrent networks.
- Mechanism: Larger eigenvalues allow information to propagate further through time before decaying, counteracting the vanishing gradient problem.
- Core assumption: Nonlinear activation functions prevent exploding values when eigenvalues exceed 1.
- Evidence anchors:
  - [abstract] "We provide an explanation for long-term dependency based on the eigen analysis."
  - [section] "We conclude that in RNNs, the long-term dependency is improved by increasing eigenvalues."
  - [corpus] Weak corpus support - no direct neighbor papers discuss eigenvalue-based initialization.

### Mechanism 2
- Claim: Hidden states explore space away from the origin, avoiding collapse.
- Mechanism: Large eigenvalues push hidden states toward the boundaries of the activation function's output range, increasing state diversity.
- Core assumption: Activation functions create bounded output spaces (hypercubes) that constrain hidden states.
- Evidence anchors:
  - [abstract] "We also point out the different behavior of eigenvalues for regression tasks and classification tasks."
  - [section] "These eigenvalues keep pushing the hidden states to the surface of the hypercube."
  - [corpus] No corpus papers discuss hidden state distribution properties.

### Mechanism 3
- Claim: Eigen decomposition initialization creates rotational dynamics in hidden state space.
- Mechanism: Sequential rotations between dimensions create complex state trajectories that preserve information.
- Core assumption: Hidden state space can be decomposed into orthogonal dimensions for rotation operations.
- Evidence anchors:
  - [section] "We mimic this rotation by decomposing the process into n−1 step... perform the rotation in the space of ith and (i+1)th dimension."
  - [abstract] "We propose a new perspective to analyze the hidden state space based on an eigen decomposition of the weight matrix."
  - [corpus] No corpus papers discuss rotational initialization techniques.

## Foundational Learning

- Concept: Eigen decomposition of matrices
  - Why needed here: Understanding how eigenvalues and eigenvectors relate to information propagation in recurrent networks
  - Quick check question: What happens to the magnitude of a vector when multiplied by a matrix with eigenvalues > 1?

- Concept: State space models and linear systems
  - Why needed here: The paper builds on linear state space analysis before extending to nonlinear cases
  - Quick check question: In a stable linear system, what constraint must eigenvalues satisfy?

- Concept: Activation function dynamics
  - Why needed here: Activation functions prevent exploding values when eigenvalues exceed 1
  - Quick check question: How do ReLU, tanh, and sigmoid differ in their treatment of negative values?

## Architecture Onboarding

- Component map: Weight matrix → Eigen decomposition → Rotation construction → Scaled eigenvalues → Final initialization
- Critical path: Proper eigenvalue scaling → Rotation application → Weight matrix assembly
- Design tradeoffs: Larger eigenvalues improve long-term dependency but risk instability if activation functions saturate
- Failure signatures: Vanishing gradients (eigenvalues too small), exploding gradients (eigenvalues too large despite activations), poor convergence (incorrect rotation angles)
- First 3 experiments:
  1. Test eigenvalue distribution on a simple synthetic sequence task
  2. Compare hidden state distributions with and without the initializer
  3. Validate stability on a regression task with known long-term dependencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do eigenvalues greater than 1 contribute to the ability of recurrent neural networks to capture long-term dependencies without causing instability?
- Basis in paper: [explicit] The paper discusses the relationship between eigenvalues and long-term dependency, suggesting that larger eigenvalues improve long-term dependency but do not cause instability due to the nonlinear activation functions.
- Why unresolved: The paper provides theoretical analysis and experimental evidence but does not offer a complete explanation of the underlying mechanisms that prevent instability when eigenvalues exceed 1.
- What evidence would resolve it: A detailed mathematical proof showing the stability of recurrent neural networks with eigenvalues greater than 1, considering the impact of different activation functions.

### Open Question 2
- Question: How does the distribution of hidden states away from the origin improve the discriminative power of recurrent neural networks?
- Basis in paper: [explicit] The paper conjectures that hidden states should explore space away from the origin to avoid collapsing, and this is supported by visualizations of hidden state distributions.
- Why unresolved: While the paper provides visual evidence and theoretical reasoning, it does not offer a rigorous mathematical explanation for why this distribution improves discriminative power.
- What evidence would resolve it: A mathematical model demonstrating how the distribution of hidden states away from the origin enhances the network's ability to distinguish between different inputs.

### Open Question 3
- Question: How do different initialization methods affect the eigenvalue distribution and, consequently, the performance of recurrent neural networks?
- Basis in paper: [explicit] The paper compares the eigenvalue norms for different initializers, including the proposed eigen initializer, and shows that it leads to better performance.
- Why unresolved: The paper shows empirical results but does not provide a theoretical analysis of why the eigen initializer leads to better eigenvalue distributions and performance.
- What evidence would resolve it: A theoretical analysis of how the eigen initializer affects the eigenvalue distribution and a detailed comparison of its impact on network performance relative to other initializers.

## Limitations

- The optimal eigenvalue value of 0.95 is determined empirically without theoretical justification for why this specific value works best
- The analysis assumes linear dynamics apply to nonlinear networks, which is an approximation that may not hold in practice
- The paper lacks ablation studies on the rotation mechanism itself to determine if rotational dynamics are necessary for performance gains

## Confidence

- **High Confidence:** The observation that eigenvalues affect information propagation in recurrent networks is well-established in the literature.
- **Medium Confidence:** The specific claim that setting all eigenvalues to 0.95 is optimal lacks theoretical justification and may be task-dependent.
- **Low Confidence:** The assertion that rotational dynamics are necessary for good performance is speculative without ablation studies on the rotation mechanism itself.

## Next Checks

1. **Ablation study on eigenvalue values:** Test different eigenvalue magnitudes (0.8, 0.95, 1.0, 1.1) to determine optimal scaling and validate the 0.95 choice.

2. **Rotation mechanism analysis:** Compare the proposed sequential rotation initialization against random orthogonal initialization and analyze impact on hidden state diversity.

3. **Linear vs nonlinear regime validation:** Conduct experiments on linear RNNs to validate whether the eigenvalue-based analysis holds without nonlinear activations, then compare with nonlinear cases.