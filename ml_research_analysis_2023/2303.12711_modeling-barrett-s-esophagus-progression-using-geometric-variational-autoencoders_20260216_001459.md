---
ver: rpa2
title: Modeling Barrett's Esophagus Progression using Geometric Variational Autoencoders
arxiv_id: '2303.12711'
source_url: https://arxiv.org/abs/2303.12711
tags:
- latent
- space
- data
- which
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates geometric variational autoencoders for modeling
  Barrett's Esophagus progression, a precursor to esophageal cancer. The study compares
  Riemannian VAEs (RHVAE), hyperspherical VAEs (S-VAE), and their roto-equivariant
  variants against vanilla VAEs.
---

# Modeling Barrett's Esophagus Progression using Geometric Variational Autoencoders

## Quick Facts
- **arXiv ID**: 2303.12711
- **Source URL**: https://arxiv.org/abs/2303.12711
- **Reference count**: 0
- **Primary result**: Hyperspherical VAEs (S-VAE) achieve superior reconstruction and classification performance in lower dimensions for Barrett's esophagus progression modeling, with roto-equivariant variants providing further improvements.

## Executive Summary
This study investigates geometric variational autoencoders for modeling Barrett's esophagus progression, comparing Riemannian VAEs, hyperspherical VAEs, and their roto-equivariant variants against vanilla VAEs. The research demonstrates that S-VAE achieves superior reconstruction loss and classification accuracy in lower dimensions (3-32) while generating higher quality images and interpolations. The study introduces a novel spherical autoencoder (S-AE) with spread loss that matches S-VAE's generative quality while retaining autoencoder benefits like stability and sharpness. However, spherical models face numerical instability in higher dimensions, limiting their applicability to complex histopathological data. Overall, geometric VAEs, particularly spherical and roto-equivariant variants, show promise for learning meaningful representations of disease progression.

## Method Summary
The study compares eight model variants (N-VAE, N-AE, equivariant N-VAE, equivariant N-AE, S-VAE, S-AE, equivariant S-VAE, equivariant S-AE) across latent dimensions (3, 8, 16, 32, 64, 128, 256, 512) on Barrett's esophagus data. The dataset consists of 934 digitally scanned H&E-stained endoscopic biopsies from 324 patients, preprocessed into 64×64 patches at 5× magnification. Models are trained using Adam optimizer (lr=0.0005), cosine annealing, batch size 128, for 500 epochs. Evaluation includes reconstruction loss (MSE), classification accuracy of latent representations using a simple MLP classifier, and qualitative assessment via random image generation and interpolation quality.

## Key Results
- S-VAE achieves superior reconstruction loss and classification accuracy in lower dimensions (3-32) compared to vanilla VAEs
- Roto-equivariant S-VAE further improves performance across all metrics by disentangling rotation information
- Spherical autoencoder (S-AE) with spread loss demonstrates comparable generative quality to S-VAE while retaining autoencoder benefits like stability and sharpness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperspherical VAE (S-VAE) outperforms vanilla VAE in lower dimensions (3-32) because the von Mises-Fisher (vMF) prior avoids the Gaussian prior's tendency to pull clusters toward the origin, allowing better separation of classes in the latent space.
- Mechanism: The vMF distribution concentrates probability mass uniformly on the hypersphere surface rather than around a single point, enabling the encoder to place distinct classes on different regions of the sphere without the central pull of a Gaussian prior.
- Core assumption: The Barrett's esophagus disease progression data can be meaningfully separated on a hyperspherical manifold in lower dimensions.
- Evidence anchors:
  - [abstract] "S-VAE achieves superior reconstruction loss and classification accuracy in lower dimensions (3-32)"
  - [section] "The greater the value ofκ, the higher the concentration of the distribution around the mean directionµ... We can leverage this fact to use high values ofκ to effectively turn the variational autoencoder into a regular non-variational autoencoder"
  - [corpus] Weak - no corpus papers directly test hyperspherical VAEs on medical image progression data
- Break condition: If the disease progression manifold has high intrinsic dimensionality or contains hierarchical relationships better captured by hyperbolic geometry, the spherical assumption would break down.

### Mechanism 2
- Claim: Roto-equivariant VAE improves performance across all metrics because it disentangles rotation information from the latent representation, removing irrelevant orientation variations that would otherwise contaminate the learned features.
- Mechanism: The group convolutional architecture preserves rotational equivariance while the latent space partitions into orientation-independent (invariant) and orientation-dependent components, allowing the model to focus on morphology rather than positioning.
- Core assumption: Barrett's esophagus biopsy images contain orientation variations that are irrelevant to disease progression classification and reconstruction.
- Evidence anchors:
  - [abstract] "Roto-equivariant S-VAE further improves performance across all metrics"
  - [section] "Biopsies can be scanned in any arbitrary orientation, this redundant information thus becomes entangled in the learned latent space, possibly making the representations harder for the model to process"
  - [corpus] Weak - corpus lacks papers specifically testing rotation-disentangled VAEs on histopathological data
- Break condition: If tissue morphology exhibits orientation-dependent patterns that are diagnostically relevant, removing rotation information would degrade rather than improve performance.

### Mechanism 3
- Claim: Spherical autoencoder (S-AE) with spread loss can generate realistic images without the variational framework's computational overhead while retaining reconstruction sharpness.
- Mechanism: By fixing the concentration parameter κ to a high value, the vMF distribution becomes effectively deterministic, turning the VAE into an autoencoder. Spread loss then encourages uniform distribution of encoded points across the hypersphere surface, creating an informed latent space suitable for generation.
- Core assumption: A deterministic encoding on a well-structured hyperspherical manifold can support meaningful interpolation and generation without probabilistic regularization.
- Evidence anchors:
  - [abstract] "introduces a novel spherical autoencoder (S-AE) with spread loss, demonstrating comparable generative quality to S-VAE while retaining autoencoder benefits like stability and sharpness"
  - [section] "In order to achieve such a uniform spread, we maximize the distance between every pair of encoded points on the hypersphere"
  - [corpus] Weak - no corpus papers test non-variational spherical autoencoders with spread loss
- Break condition: If the deterministic encoding cannot capture the necessary variability in tissue morphology, or if spread loss creates artificial gaps in the latent space that break interpolation continuity.

## Foundational Learning

- Concept: Riemannian geometry and manifold learning
  - Why needed here: Understanding how different manifold topologies (Euclidean, Riemannian, hyperspherical) affect latent space structure and relationships between data points
  - Quick check question: What is the fundamental difference between how distances are computed in Euclidean vs Riemannian geometry, and why does this matter for VAE latent spaces?

- Concept: Group theory and equivariance
  - Why needed here: Understanding how roto-equivariant architectures work and why they can disentangle orientation from content in the latent representation
  - Quick check question: How does a group convolutional neural network differ from a standard CNN in terms of parameter sharing and feature extraction across orientations?

- Concept: Variational inference and the reparameterization trick
  - Why needed here: Understanding the core mechanism by which VAEs learn probability distributions in the latent space and how different priors (Gaussian vs vMF) affect this process
  - Quick check question: What problem does the reparameterization trick solve in training VAEs, and how does the sampling procedure differ between Gaussian and von Mises-Fisher distributions?

## Architecture Onboarding

- Component map: Input image -> Encoder (CNN/ConvNeXt) -> Latent representation (mean vector/vMF parameters) -> Decoder (Transposed CNN/ConvNeXt) -> Reconstructed image
- Critical path: Input image → Encoder → Latent representation → Decoder → Reconstructed image, with variational components and group equivariance layers integrated into the encoder/decoder pipeline
- Design tradeoffs:
  - Spherical vs Euclidean: Spherical provides better clustering and generation in low dimensions but suffers from numerical instability in high dimensions
  - Variational vs Autoencoder: Variational provides probabilistic generation but less sharp reconstructions; autoencoder provides sharper reconstructions but requires additional structure (spread loss) for generation
  - Equivariant vs Non-equivariant: Equivariant removes orientation noise but increases computational complexity and may discard diagnostically relevant orientation information
- Failure signatures:
  - NaN values during training: Likely numerical instability in high-dimensional spherical models
  - Poor reconstruction quality: Insufficient latent dimension size or model expressivity
  - Blurry generated images: Using variational models instead of autoencoders, or insufficient spread in latent space
  - No improvement from equivariance: Orientation may not be a significant source of variation in the dataset
- First 3 experiments:
  1. Train N-VAE vs S-VAE with M=16 on a small subset of the Barrett's esophagus data, compare reconstruction loss and generate random samples to observe qualitative differences
  2. Implement spread loss for S-AE with M=32, compare generated image quality with and without spread loss
  3. Train roto-equivariant S-VAE vs regular S-VAE with M=32, evaluate classification accuracy on latent representations to measure impact of rotation disentanglement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reconstruction quality of spherical autoencoders compare to spherical VAEs when trained with larger patch sizes (e.g., 128x128) and higher magnification levels (e.g., 10×)?
- Basis in paper: [explicit] The paper mentions that experiments with larger patch sizes were not conducted due to computational limitations and that such experiments could reveal if higher resolution improves reconstruction quality.
- Why unresolved: The authors did not perform experiments with larger patch sizes and higher magnification levels due to computational constraints.
- What evidence would resolve it: Conducting experiments with larger patch sizes and higher magnification levels and comparing the reconstruction quality of spherical autoencoders to spherical VAEs.

### Open Question 2
- Question: What is the impact of alternative labeling methods, such as using the pixel value in the middle of a patch instead of the dominant pixel value, on the classification accuracy of latent representations?
- Basis in paper: [explicit] The paper mentions that alternative labeling methods were not explored and that using the middle pixel value could lead to a more balanced distribution of classes.
- Why unresolved: The authors did not explore alternative labeling methods and only used the dominant pixel value for labeling.
- What evidence would resolve it: Conducting experiments with alternative labeling methods and comparing the classification accuracy of latent representations.

### Open Question 3
- Question: How does the performance of spherical autoencoders compare to spherical VAEs on simpler progression modeling tasks, such as healthy tissue to dysplasia or to cancer, that require lower dimensional latent spaces?
- Basis in paper: [explicit] The paper mentions that spherical autoencoders were only tested on the complex task of modeling BE progression and that testing on simpler tasks could provide a proof of concept for their potential.
- Why unresolved: The authors did not test spherical autoencoders on simpler progression modeling tasks.
- What evidence would resolve it: Conducting experiments with spherical autoencoders on simpler progression modeling tasks and comparing their performance to spherical VAEs.

### Open Question 4
- Question: What are the potential benefits and limitations of using Riemannian VAEs for modeling disease progression, and how can the computational challenges of scaling to larger datasets be addressed?
- Basis in paper: [explicit] The paper mentions that Riemannian VAEs were not further explored due to computational limitations and that alternative methods of learning the Riemannian metric could potentially make the model more scalable.
- Why unresolved: The authors did not explore Riemannian VAEs further due to computational limitations.
- What evidence would resolve it: Conducting experiments with Riemannian VAEs on larger datasets and exploring alternative methods of learning the Riemannian metric to address computational challenges.

## Limitations
- **Dimensionality constraints**: Spherical models face numerical instability in dimensions ≥32 due to vanishing surface area, limiting applicability to high-resolution medical imaging
- **Limited clinical validation**: No clinical validation that geometric representations improve diagnostic accuracy or patient outcomes compared to standard methods
- **Generalizability concerns**: Performance improvements demonstrated on specific Barrett's esophagus dataset may not generalize across different medical imaging modalities and disease types

## Confidence
- **High confidence**: Superior reconstruction and classification performance of S-VAE in lower dimensions (3-32) is well-supported by quantitative metrics and aligns with established theory about vMF priors
- **Medium confidence**: Roto-equivariant improvements are supported by metrics but the assumption that orientation is irrelevant for BE diagnosis may not generalize to all histopathological applications
- **Low confidence**: S-AE's comparable generative quality to S-VAE while retaining autoencoder benefits is promising but the spread loss mechanism's effectiveness in higher dimensions remains untested

## Next Checks
1. **Dimensional scalability test**: Systematically evaluate S-VAE and S-AE performance on progressively higher-resolution BE images (128×128, 256×256) to quantify the exact dimensionality threshold where numerical instability becomes prohibitive, comparing with alternative manifold geometries like hyperbolic space
2. **Clinical utility validation**: Conduct a reader study where pathologists diagnose BE progression using representations from N-VAE vs S-VAE latent spaces, measuring diagnostic accuracy, confidence, and time to diagnosis to assess real-world clinical impact
3. **Orientation relevance assessment**: Train roto-equivariant models on datasets where tissue orientation is known to be diagnostically relevant (e.g., muscle tissue architecture) to test whether rotation disentanglement consistently improves or degrades performance across different histopathological contexts