---
ver: rpa2
title: Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind
  Prediction Error in Large Language Models
arxiv_id: '2310.06983'
source_url: https://arxiv.org/abs/2310.06983
tags:
- user
- data
- prediction
- llms
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a metacognitive prompting framework that
  leverages Violation of Expectation (VoE) to improve Large Language Models'' (LLMs)
  Theory of Mind (ToM) capabilities in predicting user inputs. The method uses two
  tasks: user prediction and revision, and VoE analysis to learn from prediction errors.'
---

# Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models

## Quick Facts
- arXiv ID: 2310.06983
- Source URL: https://arxiv.org/abs/2310.06983
- Reference count: 31
- Primary result: VoE-informed LLM predictions reduced "wrong" predictions by 22.4% and improved overall accuracy in ToM tasks

## Executive Summary
This paper introduces a metacognitive prompting framework that leverages Violation of Expectation (VoE) to improve Large Language Models' Theory of Mind capabilities in predicting user inputs. The method uses two tasks: user prediction and revision, and VoE analysis to learn from prediction errors. Tested on an AI tutor called Bloom, the VoE version showed a significant reduction in prediction errors, generating 22.4% fewer "wrong" predictions compared to the non-VoE version. A Chi-square test confirmed that VoE-informed predictions were evaluated as "good" more often than expected (X²(1, 927) = 5.97, p < 0.05).

## Method Summary
The study implemented an A/B test using the Bloom AI tutor with two versions: one enabled with VoE data and one without. The metacognitive prompting framework involved generating user predictions, retrieving VoE-derived facts from a vector store, revising predictions using these facts, and storing new VoE facts. GPT-4 was used for all generation and evaluation tasks. Conversations were collected (59 VoE-enabled, 55 non-VoE) and predictions were evaluated on a 5-point scale by GPT-4, with results analyzed using chi-square tests to assess prediction accuracy improvements.

## Key Results
- VoE version generated 22.4% fewer "wrong" predictions compared to the non-VoE version
- Chi-square test showed VoE-informed predictions were evaluated as "good" more often than expected (X²(1, 927) = 5.97, p < 0.05)
- There was a relative increase of 51% in "somewhat" predictions, suggesting improved prediction fidelity

## Why This Works (Mechanism)

### Mechanism 1
VoE-based metacognitive prompting reduces ToM prediction error by enabling LLMs to actively learn from prediction-violation deltas. The framework prompts the LLM to generate a prediction about the next user input, compare it with the actual input, and derive psychological facts from the violation. These facts are stored in a vector database and retrieved in future interactions to improve prediction accuracy. The core assumption is that LLMs can simulate human-like metacognitive processes and use that self-generated context to refine predictions.

### Mechanism 2
Metacognitive prompting enhances context retention and reasoning coherence in ToM tasks. By forcing the model to explicitly reason about user mental states and prediction accuracy before generating responses, the model maintains richer context and applies it more consistently across turns. The core assumption is that explicit metacognitive steps improve the model's ability to carry forward contextual understanding, unlike standard prompting where reasoning is implicit and often discarded.

### Mechanism 3
VoE-informed predictions reduce the frequency of "wrong" classifications by smoothing prediction distribution. VoE data helps the model avoid extreme mispredictions by grounding expectations in observed user behavior patterns, leading to more moderate and accurate predictions. The core assumption is that user behavior is learnable and predictable to a degree that VoE can capture and generalize.

## Foundational Learning

- **Concept**: Predictive Coding (PC) as a framework for understanding how brains and potentially LLMs build models of reality.
  - **Why needed here**: The paper frames VoE as a PC-inspired mechanism where prediction errors drive learning; understanding PC clarifies the theoretical motivation.
  - **Quick check**: How does Predictive Coding explain the role of prediction errors in learning?

- **Concept**: Theory of Mind (ToM) and its measurement in both humans and machines.
  - **Why needed here**: The paper's core goal is to improve LLM ToM prediction accuracy; knowing what ToM is and how it's evaluated is essential.
  - **Quick check**: What distinguishes a ToM-capable model from a non-ToM model in practical terms?

- **Concept**: Prompting paradigms (Chain-of-Thought, Metaprompt Programming) and their influence on LLM reasoning.
  - **Why needed here**: The metacognitive prompting framework builds on these paradigms; understanding them explains how the framework structures LLM reasoning.
  - **Quick check**: How does Chain-of-Thought prompting differ from standard prompting in terms of model output?

## Architecture Onboarding

- **Component map**: GPT-4 API -> Vector store (OpenAI Embeddings API) -> Bloom conversational backend -> Evaluation script
- **Critical path**: 1) Generate user prediction thought (Task 1) -> 2) Retrieve VoE facts from vector store -> 3) Revise prediction using retrieved facts -> 4) Compare revised prediction to actual user input -> 5) Generate VoE thought and derive new fact -> 6) Store new fact in vector store -> 7) Loop for next interaction
- **Design tradeoffs**: Using GPT-4 API vs. open-source models (higher performance but less control and higher cost); simple semantic similarity retrieval vs. custom-trained embeddings (faster to implement but potentially lower retrieval quality); storing only textual facts vs. richer structured data (simpler storage but may lose nuance)
- **Failure signatures**: Predictions do not improve over time (VoE facts are irrelevant or retrieval is broken); high latency in interactions (Vector retrieval or LLM calls are slow); "wrong" predictions remain high (Metacognitive prompting not influencing final output)
- **First 3 experiments**: 1) Test VoE fact retrieval accuracy in isolation (mock user inputs, check retrieved facts relevance) -> 2) A/B test with a simpler prompt (no metacognition) to confirm metacognitive prompting is the active ingredient -> 3) Vary k in top-k retrieval to find optimal balance between recall and noise

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How does the metacognitive prompting framework specifically reduce prediction error in LLMs compared to other prompting methods?
**Basis in paper**: [explicit] The paper introduces a metacognitive prompting framework and claims it reduces prediction error by leveraging Violation of Expectation (VoE).
**Why unresolved**: The paper demonstrates a reduction in prediction error but does not provide a detailed comparison with other prompting methods or explain the specific mechanisms by which metacognitive prompting achieves this reduction.
**What evidence would resolve it**: A comparative study showing the performance of metacognitive prompting against other prompting methods on the same tasks, along with a detailed analysis of the mechanisms involved.

### Open Question 2
**Question**: What are the long-term effects of using VoE-derived data on the accuracy and relevance of LLM predictions?
**Basis in paper**: [explicit] The paper suggests that the accuracy and relevance of VoE's outputs are expected to increase as the vector store becomes populated with more data.
**Why unresolved**: The paper does not provide empirical evidence or a model for how VoE-derived data affects long-term prediction accuracy and relevance.
**What evidence would resolve it**: Longitudinal studies tracking the performance of LLMs using VoE-derived data over extended periods, showing changes in prediction accuracy and relevance.

### Open Question 3
**Question**: How can policy-based access control be effectively implemented to manage the security and privacy of ToM data?
**Basis in paper**: [explicit] The paper discusses the need for policy-based access control to prevent data leakage and ensure principles of least privilege.
**Why unresolved**: The paper outlines the importance of policy-based access control but does not provide specific implementation details or examples of effective policies.
**What evidence would resolve it**: Case studies or implementations demonstrating effective policy-based access control mechanisms for ToM data, including specific policies and their outcomes.

## Limitations
- The study relies on subjective human evaluation for determining prediction quality, which introduces potential bias and limits scalability
- Small sample size (114 conversations) and exclusive use of GPT-4 for both reasoning and evaluation create concerns about generalizability and model dependence
- The metacognitive prompting framework's effectiveness appears heavily dependent on the quality of VoE fact generation and retrieval

## Confidence

- **High Confidence**: The experimental methodology is clearly described, including the A/B test design, chi-square analysis, and the metacognitive prompting framework structure
- **Medium Confidence**: The claim that VoE-informed predictions reduce prediction error is supported by statistical evidence, though the subjective evaluation method introduces uncertainty
- **Low Confidence**: The mechanism by which metacognitive prompting enhances context retention and reasoning coherence lacks direct empirical validation beyond the observed performance improvements

## Next Checks

1. **Scale Validation Study**: Conduct the same experiment with a larger sample size (minimum 500 conversations) and multiple independent human evaluators to establish inter-rater reliability and reduce evaluation bias

2. **Open-Source Model Comparison**: Implement the metacognitive prompting framework using open-source LLMs (e.g., Llama 2, Claude) to test whether the VoE mechanism is model-dependent or generalizable across architectures

3. **Longitudinal Performance Tracking**: Extend the study duration to track prediction accuracy over time, measuring whether the VoE vector store continues to provide value as it accumulates more facts or whether retrieval quality degrades with increased storage