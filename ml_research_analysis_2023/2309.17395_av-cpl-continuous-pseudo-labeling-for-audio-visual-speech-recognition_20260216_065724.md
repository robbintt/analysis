---
ver: rpa2
title: 'AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition'
arxiv_id: '2309.17395'
source_url: https://arxiv.org/abs/2309.17395
tags:
- data
- lrs3
- speech
- audio-visual
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces continuous pseudo-labeling for audio-visual
  speech recognition (AV-CPL), a semi-supervised method that trains an audio-visual
  speech recognition model on both labeled and unlabeled videos with continuously
  regenerated pseudo-labels. AV-CPL uses the same model for both supervised training
  and pseudo-label generation, avoiding the need for external speech recognition models.
---

# AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition

## Quick Facts
- **arXiv ID**: 2309.17395
- **Source URL**: https://arxiv.org/abs/2309.17395
- **Reference count**: 40
- **Primary result**: AV-CPL improves visual speech recognition (VSR) performance on LRS3 dataset while maintaining practical automatic speech recognition (ASR) and audio-visual speech recognition (AVSR) performance.

## Executive Summary
AV-CPL introduces continuous pseudo-labeling for audio-visual speech recognition, a semi-supervised method that trains an audio-visual speech recognition model on both labeled and unlabeled videos with continuously regenerated pseudo-labels. The method uses the same audio-visual model for both supervised training and pseudo-label generation, avoiding the need for external speech recognition models. By leveraging cross-modal representations from synchronized audio and visual information, AV-CPL improves VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. The approach demonstrates that visual-only speech data can be effectively leveraged to improve VSR through unlabeled visual speech.

## Method Summary
AV-CPL is a semi-supervised audio-visual speech recognition method that uses continuous pseudo-labeling to improve performance on labeled data. The method employs a transformer-based audio-visual model with CTC loss that can perform ASR, VSR, and AVSR tasks. During training, modality dropout is applied to increase robustness and enable single-model multi-task performance. The model is first pre-trained on audio-only labeled data, then jointly trained with modality dropout. Pseudo-labels are continuously regenerated using an EMA teacher model on unlabeled VOXceleb2 data, which are then used alongside labeled LRS3 data for fine-tuning. The method uses character output tokens with a 20ms stride for audio and evaluates performance using word error rate (WER) across all three tasks.

## Key Results
- AV-CPL improves VSR performance on LRS3 dataset compared to supervised baseline
- The method maintains practical ASR and AVSR performance while improving VSR
- Modality dropout increases ASR performance but has mixed effects on VSR and AVSR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The audio-visual model's cross-modal representations enable effective pseudo-label generation for unlabeled data
- Mechanism: The synchronized audio and visual information provides complementary signals that improve the quality of pseudo-labels compared to using either modality alone. The model learns to leverage both modalities during training, resulting in better representations for generating pseudo-labels
- Core assumption: The cross-modal information is sufficiently informative to generate accurate pseudo-labels even when the model has limited supervised training
- Evidence anchors:
  - [abstract]: "Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels."
  - [section 3.1]: "Using only the audio or the video data, we can perform two kinds of speech recognition: automatic speech recognition (ASR) from the audio channel, or visual speech recognition (VSR) from the video channel (lip-reading)."
  - [corpus]: Weak evidence - no direct comparison of pseudo-label quality with single-modality models
- Break condition: If the audio-visual model cannot generate accurate pseudo-labels during the seed model phase, the self-training loop will fail to improve performance

### Mechanism 2
- Claim: Continuous regeneration of pseudo-labels prevents model collapse and improves performance iteratively
- Mechanism: The model is trained on both labeled and pseudo-labeled data while continuously generating new pseudo-labels, allowing it to correct errors and improve over time. The use of EMA or dynamic cache ensures stable training by using previous model states for pseudo-label generation
- Core assumption: The model can generate progressively better pseudo-labels as it trains on the combined dataset
- Evidence anchors:
  - [abstract]: "Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels."
  - [section 3.2]: "To prevent model collapse which could happen when PLs are re-generated after each training step, SlimIPL maintains a dynamic cache of unlabeled samples and PLs, while Momentum PL generates PLs with a teacher model whose weights are the exponential moving average of the student model."
  - [corpus]: Weak evidence - no direct analysis of pseudo-label quality evolution over training
- Break condition: If the pseudo-label quality degrades over time or becomes too noisy, the model performance will plateau or degrade

### Mechanism 3
- Claim: Modality dropout during training increases robustness and enables single-model multi-task performance
- Mechanism: Random dropping of one modality during training forces the model to learn robust representations that work with either modality alone or both together, enabling ASR, VSR, and AVSR with a single model
- Core assumption: Training with partial modalities improves generalization to missing modalities at test time
- Evidence anchors:
  - [section 3.1]: "By default, the model is trained for audio-visual speech recognition, which uses both audio and visual inputs. However, to increase the model's robustness to a missing modality and to facilitate VSR-only and ASR-only capabilities, we randomly apply modality dropout during training where one modality is entirely dropped out"
  - [section 5.3]: "Modality dropout tends to make A VSR and VSR marginally worse, and ASR significantly better. Given these results, we use the convolutional layer with modality addition for all subsequent experiments."
  - [corpus]: Weak evidence - no ablation study on test-time performance with missing modalities
- Break condition: If the model overfits to the available modalities during training, it may not generalize well when one modality is missing at test time

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC allows the model to output sequences without requiring alignment between input and output sequences, which is essential for speech recognition tasks
  - Quick check question: Why is CTC particularly suitable for continuous speech recognition compared to cross-entropy loss?

- Concept: Transformer-based audio-visual fusion
  - Why needed here: The transformer architecture allows for effective cross-modal interaction between audio and visual features through self-attention mechanisms
  - Quick check question: How does the additive fusion approach differ from concatenation-based fusion in terms of parameter efficiency and representation learning?

- Concept: Semi-supervised learning with pseudo-labeling
  - Why needed here: This approach leverages unlabeled data to improve model performance when labeled data is scarce, which is particularly relevant for VSR where labeled data is expensive to obtain
  - Quick check question: What is the key difference between continuous pseudo-labeling and traditional self-training approaches?

## Architecture Onboarding

- Component map: Audio/Video → Encoders → Feature Addition → Transformer → CTC Projection → Output
- Critical path: Audio/Video → Encoders → Feature Addition → Transformer → CTC Projection → Output
- Design tradeoffs:
  - CTC vs S2S: CTC avoids issues with repeated n-grams and filtering poor pseudo-labels, but may be less flexible for complex language modeling
  - Character vs subword tokens: Characters work better with 20ms stride for audio, while subwords work better for video-only tasks
  - Stride choice: 20ms stride provides better performance than 40ms despite containing duplicated visual information
- Failure signatures:
  - VSR performance significantly worse than ASR indicates modality imbalance or insufficient video representation learning
  - Degraded performance on test vs validation suggests overfitting to specific sequences or easier test conditions
  - Pseudo-label quality degradation over time indicates unstable self-training loop
- First 3 experiments:
  1. Verify CTC model can match S2S performance on supervised LRS3 30h data
  2. Test modality dropout impact on ASR, VSR, and AVSR performance
  3. Validate pseudo-label generation quality by comparing to external ASR model outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AV-CPL vary when using different language models or decoding strategies beyond the 4-gram LM and Flashlight decoder tested in the paper?
- Basis in paper: [explicit] The paper mentions using a 4-gram word-level LM and Flashlight beam-search decoder, but notes that the LM weight and word insertion penalty were tuned on the validation set. It also states that the authors would like to apply their method to more languages in future work.
- Why unresolved: The paper only reports results using a specific LM and decoding setup. It's unclear how AV-CPL's performance would change with different LMs (e.g. larger n-grams, neural LMs) or decoding strategies (e.g. greedy, different beam sizes, language model integration methods).
- What evidence would resolve it: Experiments testing AV-CPL with various LM types, sizes, and decoding strategies, reporting the resulting WERs on the LRS3 test set. Comparing these results to the reported AV-CPL performance would show the impact of LM and decoding choices.

### Open Question 2
- Question: Can AV-CPL be extended to handle more than two modalities (e.g. audio, video, and text transcripts) for semi-supervised learning?
- Basis in paper: [inferred] The paper focuses on audio-visual speech recognition, using audio and visual modalities. However, it mentions that "Real-world unlabeled data is often multi-modal" and discusses leveraging cross-modal supervision between audio and visual inputs. This suggests the potential for extending to more modalities.
- Why unresolved: The paper does not explore multi-modal extensions beyond audio and visual. It's unclear how AV-CPL would handle additional modalities like text transcripts, and whether the cross-modal supervision benefits would extend to more than two modalities.
- What evidence would resolve it: Experiments implementing AV-CPL with additional modalities (e.g. audio, video, and text), comparing performance to the audio-visual AV-CPL and to single-modality baselines. Analyzing the cross-modal supervision effects with more than two modalities would show if AV-CPL can be successfully extended.

### Open Question 3
- Question: How does AV-CPL's performance scale with larger amounts of unlabeled data beyond the 1,326 hours used in the paper?
- Basis in paper: [explicit] The paper uses 1,326 hours of unlabeled VOXceleb2 data and shows performance improvements over the supervised baseline. It also mentions that the authors would like to apply their method to more languages in future work, which may require larger amounts of unlabeled data.
- Why unresolved: The paper only reports results using a fixed amount of unlabeled data (1,326 hours). It's unclear how AV-CPL's performance would change with significantly more unlabeled data, and whether there are diminishing returns or an optimal amount of unlabeled data for the best performance.
- What evidence would resolve it: Experiments testing AV-CPL with increasing amounts of unlabeled data (e.g. 2,000, 5,000, 10,000 hours), reporting the resulting WERs on the LRS3 test set. Plotting AV-CPL's performance against the amount of unlabeled data would show how performance scales and if there are diminishing returns.

## Limitations
- Pseudo-label quality is not thoroughly analyzed or compared against external ASR models
- Modality dropout shows mixed performance results, improving ASR while potentially harming VSR and AVSR
- The effectiveness of continuous pseudo-labeling depends heavily on the quality of pseudo-labels generated by the audio-visual model

## Confidence
- **High confidence**: Core architectural design and implementation details (Transformer-based audio-visual fusion, CTC loss, CAPE positional embeddings)
- **Medium confidence**: Effectiveness of continuous pseudo-labeling mechanism, as pseudo-label quality evolution is not directly analyzed
- **Low confidence**: Claimed benefits of modality dropout, given the mixed performance results across tasks

## Next Checks
1. **Pseudo-label quality analysis**: Compare pseudo-labels generated by the audio-visual model against those from a strong external ASR model on a held-out subset of unlabeled data, measuring character error rate and analyzing temporal alignment quality.

2. **Ablation study on pseudo-label frequency**: Systematically vary the frequency of pseudo-label regeneration (e.g., every 100 steps, 1000 steps, epoch-based) to quantify the impact on VSR performance and identify optimal regeneration schedules.

3. **Cross-modal dependency test**: Train and evaluate separate models using only audio data, only video data, and both modalities to precisely quantify the contribution of cross-modal learning to pseudo-label generation quality and overall performance gains.