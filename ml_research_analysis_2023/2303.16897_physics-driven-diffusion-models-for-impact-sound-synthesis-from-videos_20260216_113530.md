---
ver: rpa2
title: Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos
arxiv_id: '2303.16897'
source_url: https://arxiv.org/abs/2303.16897
tags:
- physics
- sound
- impact
- video
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for synthesizing impact sounds
  from silent videos using a physics-driven diffusion model. The key innovation is
  the incorporation of physics priors, which include both estimated physics parameters
  from raw audio and learned residual parameters, to guide the sound synthesis process.
---

# Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos

## Quick Facts
- arXiv ID: 2303.16897
- Source URL: https://arxiv.org/abs/2303.16897
- Reference count: 40
- This paper presents a novel method for synthesizing impact sounds from silent videos using a physics-driven diffusion model.

## Executive Summary
This paper introduces a novel approach for synthesizing impact sounds from silent videos using a physics-driven diffusion model. The key innovation is incorporating physics priors - estimated parameters from audio and learned residual parameters - to guide sound synthesis. The authors propose a new inference pipeline that uses video features to query corresponding physics latents from training data, enabling generation of novel impact sounds. Experimental results show their method outperforms existing approaches in generating high-fidelity impact sounds, as measured by FID, KID, KL divergence, and recognition accuracy. The interpretable nature of physics priors also enables flexible sound editing applications.

## Method Summary
The method estimates physics parameters (frequency, power, decay rate) from audio using signal processing, and learns residual parameters (environment noise, reverberation) via neural networks. These physics priors are encoded into latent features and used as conditional inputs to a diffusion model, which learns to reverse a Markov chain that corrupts spectrograms with Gaussian noise. During inference, visual features from test videos are matched to training video features to find corresponding physics latents, which are then used to guide sound generation.

## Key Results
- Achieves average recognition accuracy of 29.53%, substantially higher than existing methods (11.37-19.21%)
- Outperforms existing methods in FID, KID, and KL divergence metrics for sound quality
- Demonstrates controllable sound editing through manipulation of interpretable physics parameters

## Why This Works (Mechanism)

### Mechanism 1
- Physics priors provide interpretable guidance for high-fidelity impact sound synthesis
- Estimated physics parameters from audio (frequency, power, decay rate) and learned residual parameters are encoded and used as conditional inputs to guide the diffusion model
- Core assumption: Physics parameters estimated from audio can be reused at inference by matching visual features to training set physics latents
- Evidence: Physics parameter feature defined as ϕ = (fi, pi, λi); limited corpus evidence but related work exists on physics-based audio synthesis

### Mechanism 2
- Diffusion models generate high-quality spectrograms when conditioned on visual and physics information
- Model learns to reverse a Markov chain that corrupts spectrograms with Gaussian noise, conditioned on visual latent features and physics latent features
- Core assumption: Weak correspondence between video and audio can be compensated by adding physics priors as additional conditioning
- Evidence: Diffusion process transforms spectrogram x0 into Gaussian noise xT through Markov transitions; limited corpus evidence but diffusion models have shown success in audio generation

### Mechanism 3
- Physics priors enable controllable sound editing through interpretable parameters
- Since physics priors are fully interpretable, they can be directly manipulated to control synthesized sound characteristics
- Core assumption: Estimated physics parameters directly correspond to perceivable sound characteristics
- Evidence: Authors demonstrate reducing low frequency components by decreasing power and decay rate in lowest 200 modes; moderate corpus evidence for physics-based audio synthesis controllability

## Foundational Learning

- Concept: Signal processing for modal analysis
  - Why needed here: To extract physics parameters (frequency, power, decay) from raw audio waveforms for use as conditioning in the diffusion model
  - Quick check question: How does short-time Fourier transform help in estimating the decay rate of a mode?

- Concept: Denoising diffusion probabilistic models
  - Why needed here: To learn the reverse process of gradually adding noise to spectrograms, conditioned on visual and physics features, enabling high-fidelity sound synthesis
  - Quick check question: What is the role of the variance schedule βt in the diffusion process?

- Concept: Cross-modal feature matching
  - Why needed here: To map test video features to corresponding physics latents from training set, since ground truth audio is unavailable at inference
  - Quick check question: Why is Euclidean distance used for matching visual latents, and what are its limitations?

## Architecture Onboarding

- Component map:
  - Physics priors reconstruction: Signal processing + transformer encoder for residual parameters
  - Visual feature extraction: Temporal-shift-module (TSM) ResNet-50
  - Physics-driven diffusion model: U-Net denoiser conditioned on visual and physics latents
  - Inference pipeline: Visual-to-physics latent matching via nearest neighbor search

- Critical path:
    1. Extract visual features from test video
    2. Match to training video features to find corresponding physics latent
    3. Run diffusion model with matched physics and test visual latents to generate spectrogram
    4. Convert spectrogram to waveform using Griffin-Lim algorithm

- Design tradeoffs:
  - Physics priors vs. pure video conditioning: Physics priors provide interpretability and better quality but require matching at inference
  - Nearest neighbor matching vs. learned mapping: Simple but may fail for unseen physics parameters; learned mapping could generalize better but loses interpretability

- Failure signatures:
  - Poor visual-to-physics matching: Generated sounds don't match video content
  - Diffusion model instability: Noisy or unrealistic spectrograms
  - Physics parameter estimation errors: Incorrect modal synthesis in reconstruction

- First 3 experiments:
  1. Test physics parameter estimation on held-out audio to verify accuracy
  2. Validate visual-to-physics matching by checking if matched physics latents produce similar sounds
  3. Generate sounds for test videos and compare to ground truth using FID/KID metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model generate impact sounds for objects with physics parameters not seen during training?
- Basis: Authors mention their approach cannot generate impact sounds for unseen physics parameters due to the query process, while they can generate novel sounds given an unseen video
- Why unresolved: Paper demonstrates failure case where model generates sound for rock impact that differs from ground truth, indicating limitations in handling unseen physics parameters
- What evidence would resolve it: Experiments testing model's performance on dataset with wider variety of physics parameters not present in training set

### Open Question 2
- Question: How does the choice of residual parameters (e.g., number, decay rate) affect quality and fidelity of synthesized impact sounds?
- Basis: Authors perform ablation study on importance and selection of residual parameters, showing 100 residual parameters achieves best performance
- Why unresolved: While optimal number of residual parameters is identified, impact of other factors like decay rate or specific parameter values remains unexplored
- What evidence would resolve it: Further ablation studies varying decay rate, range of residual parameter values, and specific parameter values would provide insights into their effects

### Open Question 3
- Question: Can physics priors be effectively predicted from video input alone, eliminating need for query-based inference pipeline?
- Basis: Authors mention predicting physics latents from video input results in poor quality generated samples due to weak correspondence between video inputs and impact sound physics
- Why unresolved: While limitations of predicting physics latents from video input are demonstrated, no exploration of alternative methods or architectures that might improve this prediction task
- What evidence would resolve it: Experiments comparing performance of different architectures, loss functions, or training strategies for predicting physics latents from video input

## Limitations
- Visual-to-physics latent matching relies on nearest neighbor search, which may fail for test videos with physics parameters outside training distribution
- Griffin-Lim algorithm for waveform reconstruction introduces potential artifacts due to imperfect phase reconstruction
- Physics parameter estimation from audio may be sensitive to noise and reverberation in real-world recordings

## Confidence
- **High confidence**: Core mechanism of using physics priors (frequency, power, decay rate) for interpretable sound synthesis
- **Medium confidence**: Effectiveness of diffusion model in generating high-fidelity impact sounds when conditioned on both visual and physics information
- **Medium confidence**: Controllability claims for sound editing through physics parameter manipulation

## Next Checks
1. Test physics parameter estimation accuracy on held-out audio recordings with known impact characteristics to quantify estimation errors
2. Conduct ablation studies by removing physics prior conditioning and measure impact on sound quality; test matching algorithm on videos with rare or absent physics parameters in training set
3. Compare proposed visual-to-physics latent matching approach against learned mapping function that directly generates physics latents from visual features to assess trade-off between interpretability and generation quality