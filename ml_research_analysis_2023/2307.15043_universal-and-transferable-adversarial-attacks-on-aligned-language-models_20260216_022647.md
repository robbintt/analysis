---
ver: rpa2
title: Universal and Transferable Adversarial Attacks on Aligned Language Models
arxiv_id: '2307.15043'
source_url: https://arxiv.org/abs/2307.15043
tags:
- adversarial
- attack
- attacks
- harmful
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial suffix tokens can be automatically optimized to induce
  harmful content generation in aligned language models, achieving up to 88% success
  rate on held-out test behaviors and 57% exact string matches on a held-out test
  set of harmful strings. The attack suffix transfers to commercial black-box models
  like GPT-3.5 (87.9%), GPT-4 (53.6%), PaLM-2 (66%), and others with high success
  rates, even though the models use different tokens and training methods.
---

# Universal and Transferable Adversarial Attacks on Aligned Language Models

## Quick Facts
- arXiv ID: 2307.15043
- Source URL: https://arxiv.org/abs/2307.15043
- Authors: 
- Reference count: 17
- Key outcome: Adversarial suffix tokens can be automatically optimized to induce harmful content generation in aligned language models, achieving up to 88% success rate on held-out test behaviors and 57% exact string matches on a held-out test set of harmful strings.

## Executive Summary
This paper demonstrates that adversarial suffix tokens can be automatically optimized to induce harmful content generation in aligned language models, achieving up to 88% success rate on held-out test behaviors and 57% exact string matches on a held-out test set of harmful strings. The attack suffix transfers to commercial black-box models like GPT-3.5 (87.9%), GPT-4 (53.6%), PaLM-2 (66%), and others with high success rates, even though the models use different tokens and training methods. The adversarial suffix is generated by a combination of greedy and gradient-based search over multiple prompts and multiple models. The results suggest that current alignment methods are not robust to such attacks, raising important questions about the safety of aligned language models.

## Method Summary
The paper proposes a method for generating adversarial suffix tokens that can induce harmful content generation in aligned language models. The approach uses a combination of greedy and gradient-based search (Greedy Coordinate Gradient) to optimize the suffix tokens over multiple prompts and multiple models. The optimization objective is to maximize the probability that the model produces an affirmative response to harmful prompts. The resulting adversarial suffixes are then tested for their effectiveness on held-out test behaviors and harmful strings, as well as their transferability to commercial black-box models.

## Key Results
- Adversarial suffixes achieve up to 88% success rate on held-out test behaviors
- Transferability to commercial models: GPT-3.5 (87.9%), GPT-4 (53.6%), PaLM-2 (66%)
- Exact string match rate of 57% on held-out test set of harmful strings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial suffix tokens condition the model to begin responses with affirmative language, triggering harmful content generation.
- Mechanism: The attack optimizes for the model to start its response with phrases like "Sure, here is..." which shifts the model into a mode where it continues generating the harmful content requested.
- Core assumption: LLMs have learned patterns where affirmative responses often lead to compliance with subsequent requests.
- Evidence anchors:
  - [abstract] "our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response"
  - [section 2.1] "we would like the response of the LLM to begin its response with the phrase 'Sure, here is how to build a bomb.'"
- Break condition: If the model's alignment training specifically blocks responses that begin with affirmative language in harmful contexts.

### Mechanism 2
- Claim: Greedy Coordinate Gradient (GCG) optimization effectively searches the discrete token space to find adversarial suffixes.
- Mechanism: GCG computes gradients at the token level to identify promising single-token replacements, evaluates candidates, and selects the best substitutions through iterative improvement.
- Core assumption: Gradients computed at the token embedding level provide meaningful signal for optimizing discrete token sequences.
- Evidence anchors:
  - [section 2.2] "we leverage gradients with respect to the one-hot token indicators to find a set of promising candidates for replacement at each token position"
  - [section 3.1] "GCG is able to quickly find an adversarial example with small loss relative to the other approaches"
- Break condition: If the discrete optimization landscape becomes too noisy or if gradients at the token level become uninformative.

### Mechanism 3
- Claim: Universal attacks transfer across different models because the adversarial suffixes exploit common patterns in LLM training data and architecture.
- Mechanism: Training on multiple models and prompts creates suffixes that work across various architectures by targeting shared vulnerabilities in how LLMs process and generate responses.
- Core assumption: Different LLMs trained on similar web data learn comparable patterns that can be exploited by universal suffixes.
- Evidence anchors:
  - [abstract] "we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs"
  - [section 3.2] "GPT-3.5 (87.9%), GPT-4 (53.6%), PaLM-2 (66%), and others with high success rates"
- Break condition: If models are trained on sufficiently diverse data or use architectures that prevent common pattern exploitation.

## Foundational Learning

- Concept: Discrete optimization in NLP
  - Why needed here: The attack must optimize over discrete token sequences rather than continuous embeddings to create transferable attacks.
  - Quick check question: What makes optimizing over discrete tokens more challenging than continuous optimization in neural networks?

- Concept: Gradient-based discrete search
  - Why needed here: GCG uses gradients to guide the search for adversarial tokens in a discrete space where exhaustive search is infeasible.
  - Quick check question: How does computing gradients with respect to one-hot token indicators help find better token substitutions?

- Concept: Transferability in adversarial examples
  - Why needed here: The attack's effectiveness depends on the adversarial suffixes transferring across different models with varying architectures.
  - Quick check question: What factors typically enable adversarial examples to transfer between different machine learning models?

## Architecture Onboarding

- Component map:
  - Prompt optimization loop -> Greedy Coordinate Gradient search -> Multi-model aggregation -> Multi-prompt strategy -> Transfer evaluation

- Critical path: Initialize adversarial suffix → Compute token-level gradients → Evaluate top-k candidates → Select best replacement → Repeat until convergence → Test transferability

- Design tradeoffs:
  - Batch size vs. optimization quality: Larger batches provide better gradient estimates but increase computation
  - Number of models vs. transfer success: Training on more models improves transferability but increases optimization complexity
  - Prefix length vs. attack effectiveness: Longer suffixes may be more effective but risk detection

- Failure signatures:
  - High loss values that plateau early indicate poor optimization landscape
  - Low transferability rates suggest overfitting to specific model architectures
  - Empty or nonsensical completions indicate token substitution issues

- First 3 experiments:
  1. Single-prompt, single-model attack using GCG to establish baseline effectiveness
  2. Multi-prompt optimization on Vicuna-7B and Vicuna-13B to test universal attack generation
  3. Transfer attack against GPT-3.5 using suffixes optimized on Vicuna models to measure transferability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are adversarial training methods at preventing the attacks described in the paper?
- Basis in paper: [inferred] The paper discusses the possibility of adversarial training as a potential defense against the attacks, but does not evaluate its effectiveness.
- Why unresolved: The paper does not provide any empirical evidence on the effectiveness of adversarial training in this context.
- What evidence would resolve it: Experiments comparing the attack success rate on models trained with and without adversarial training.

### Open Question 2
- Question: How transferable are the attacks to other modalities beyond text, such as images or audio?
- Basis in paper: [inferred] The paper focuses on text-based attacks, but the concept of adversarial examples and transferability may apply to other modalities.
- Why unresolved: The paper does not explore the transferability of attacks to other modalities.
- What evidence would resolve it: Experiments testing the transferability of the attacks to other modalities, such as image or audio classification models.

### Open Question 3
- Question: How robust are the attacks to changes in the attack objective, such as targeting different harmful behaviors or using different loss functions?
- Basis in paper: [inferred] The paper focuses on a specific attack objective (eliciting affirmative responses to harmful prompts), but the effectiveness of the attacks may vary with different objectives.
- Why unresolved: The paper does not explore the robustness of the attacks to changes in the attack objective.
- What evidence would resolve it: Experiments testing the effectiveness of the attacks with different objectives and loss functions.

### Open Question 4
- Question: How do the attacks perform against more sophisticated alignment methods, such as those incorporating human feedback or chain-of-thought reasoning?
- Basis in paper: [inferred] The paper evaluates the attacks against a variety of alignment methods, but does not specifically test against more sophisticated methods.
- Why unresolved: The paper does not provide any evidence on the effectiveness of the attacks against more advanced alignment techniques.
- What evidence would resolve it: Experiments comparing the attack success rate on models trained with different alignment methods, including those incorporating human feedback or chain-of-thought reasoning.

### Open Question 5
- Question: How can the transferability of the attacks be explained, and what factors influence their success across different models?
- Basis in paper: [inferred] The paper observes high transferability of the attacks across different models, but does not provide a theoretical explanation for this phenomenon.
- Why unresolved: The paper does not explore the underlying reasons for the transferability of the attacks.
- What evidence would resolve it: Theoretical analysis and experiments investigating the factors that influence the transferability of the attacks, such as model architecture, training data, and optimization techniques.

## Limitations
- Attack success rates vary significantly across different target models (53.6% for GPT-4 vs 87.9% for GPT-3.5)
- Exact string match rate of 57% indicates suffixes often generate similar but not identical harmful content
- The long-term robustness of these adversarial suffixes against potential defenses is unknown

## Confidence
- High Confidence: The core methodology of using Greedy Coordinate Gradient for discrete token optimization is well-established and the implementation details are clearly described.
- Medium Confidence: The transferability claims are supported by empirical results but may not generalize to all language model architectures.
- Low Confidence: The long-term robustness of these adversarial suffixes against potential defenses is unknown.

## Next Checks
1. Test whether simple input filtering or prefix detection mechanisms can effectively block the adversarial suffixes, as their recognizable structure might make them vulnerable to signature-based detection.
2. Evaluate the attack effectiveness on additional model architectures beyond transformer-based LLMs, including hybrid models and those using different tokenization schemes, to better understand the true universality of the attack.
3. Measure how the attack success rates change over time as models receive updates and additional safety training, determining whether the adversarial suffixes maintain effectiveness or degrade as models evolve.