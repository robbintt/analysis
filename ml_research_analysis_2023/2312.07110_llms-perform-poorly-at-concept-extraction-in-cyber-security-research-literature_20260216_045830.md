---
ver: rpa2
title: LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature
arxiv_id: '2312.07110'
source_url: https://arxiv.org/abs/2312.07110
tags:
- noun
- trends
- arxiv
- language
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  in extracting knowledge entities from cybersecurity research literature. The authors
  compare various LLM-based entity extractors with non-LLM approaches on arXiv cybersecurity
  preprints, measuring entity recognition and relevance through classification tasks.
---

# LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature

## Quick Facts
- arXiv ID: 2312.07110
- Source URL: https://arxiv.org/abs/2312.07110
- Reference count: 40
- Key outcome: LLMs produce irrelevant keywords that don't reflect cybersecurity context when extracting entities from research literature

## Executive Summary
This study evaluates large language models (LLMs) for extracting knowledge entities from cybersecurity research literature. The authors find that LLMs perform poorly at concept extraction, producing irrelevant keywords that fail to capture the cybersecurity context. The study develops a noun extractor boosted with statistical analysis to identify domain-specific compound nouns, which shows promise for trend monitoring in fast-evolving fields like cybersecurity. The work highlights significant challenges in using LLMs for scientific bibliometrics and suggests the need for improved extraction methods tailored to specific domains.

## Method Summary
The researchers evaluated various LLM-based entity extractors against non-LLM approaches on a subset of 100k arXiv preprints from cybersecurity categories. They measured entity recognition and relevance through classification tasks. The study developed a noun extraction pipeline using spaCy to extract compound nouns, followed by statistical filtering using t-tests and fold change analysis against BookCorpus. They also explored trend evolution by measuring proximity scores between selected technology terms across six-month intervals.

## Key Results
- LLMs produce poor knowledge entities that don't reflect cybersecurity context
- Noun extraction with statistical analysis shows promise for identifying domain-specific terms
- Cosine similarity of embeddings is unreliable for clustering domain-specific concepts due to sensitivity to embedding choice
- Traditional bibliometrics approaches show limitations in fast-evolving domains like cybersecurity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail to extract domain-specific concepts from cybersecurity literature because their fine-tuning datasets are not representative of scientific text.
- Mechanism: LLMs rely on learned patterns from their training corpus. When fine-tuned on general newswire or similar datasets, they cannot recognize domain-specific terminology, named entities, or evolving technical concepts that differ from the training domain.
- Core assumption: Fine-tuning datasets lack sufficient domain-specific coverage to enable meaningful extraction in fast-evolving scientific fields.
- Evidence anchors:
  - [abstract] "LLMs do not produce good knowledge entities that reflect the cybersecurity context"
  - [section 5.1] "fine-tuning datasets are general texts, such as the ubiquitous Conll03 newswire... we cannot expect general LLM-based entity extraction models to perform well on scientific articles"
  - [corpus] Weak - no explicit citation of fine-tuning dataset characteristics in related papers.

### Mechanism 2
- Claim: Cosine similarity of embeddings is unreliable for clustering domain-specific concepts because embedding algorithms and spaces are highly sensitive to domain mismatch.
- Mechanism: Different embedding algorithms (e.g., spaCy, GloVe, BERT) produce incompatible vector spaces. Even when using the same extractor, entities cluster differently depending on the embedding method, making unsupervised clustering ineffective.
- Core assumption: Vector space representations are not portable across domains without additional alignment.
- Evidence anchors:
  - [abstract] "even if we assume the relevance of extracted terms, their downstream automated processing remains a challenging task, given that it is highly sensitive to the embedding choice"
  - [section 5.3] "the impact of that choice is drastic... cosine similarity is highly dependent on the algorithm used to embed extracted entities in the vector space"
  - [corpus] No direct evidence of embedding misalignment in neighbor papers.

### Mechanism 3
- Claim: Traditional bibliometrics proxies fail in fast-evolving domains like cybersecurity because they suffer from temporal lag, semantic contamination, and coarse granularity.
- Mechanism: Bibliometrics rely on citation counts and keyword search volumes. In emerging fields, these metrics lag behind actual innovation, are contaminated by homonyms, and lack resolution to capture niche developments.
- Core assumption: The temporal and semantic characteristics of bibliometrics do not match the dynamics of fast-evolving technical domains.
- Evidence anchors:
  - [abstract] "standard bibliometrics approaches show their limits in such a fast-evolving domain"
  - [section 2.3] "observed issues happen in any domain with a short cycle of development"
  - [corpus] Related papers mention limitations of existing methods but do not quantify lag or contamination.

## Foundational Learning

- Concept: Domain adaptation in NLP
  - Why needed here: LLMs must adapt from general language to specialized scientific terminology; without adaptation, extraction fails.
  - Quick check question: What type of dataset would best fine-tune an LLM for extracting cybersecurity concepts from arXiv preprints?

- Concept: Embedding space alignment
  - Why needed here: Cosine similarity depends on comparable vector spaces; mismatched embeddings yield meaningless similarity scores.
  - Quick check question: If two embeddings give different clusterings of the same entities, what does that imply about their vector spaces?

- Concept: Temporal dynamics in bibliometrics
  - Why needed here: Citation and search trends have inherent delays; emerging fields need real-time indicators.
  - Quick check question: Why might citation counts lag behind the actual emergence of a new cybersecurity concept?

## Architecture Onboarding

- Component map: Text preprocessing -> Entity extraction (LLM/non-LLM) -> Embedding projection -> Similarity clustering -> Trend extraction
- Critical path: Preprocessing -> Extraction -> Embedding -> Clustering -> Interpretation
- Design tradeoffs: LLM-based extractors offer generality but poor domain fit; noun extractors are more specific but less context-aware; embeddings trade speed for semantic richness.
- Failure signatures: Poor clustering in 2D projections; highly variable similarity scores across embeddings; extracted entities lack domain coherence.
- First 3 experiments:
  1. Compare extraction quality on a labeled dataset of cybersecurity terms using spaCy noun extraction vs. BERT-based NER.
  2. Measure clustering stability when swapping between spaCy, GloVe, and FastText embeddings for the same extracted entities.
  3. Track term frequency in arXiv vs. BookCorpus to validate volcano plot filtering for domain specificity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuned LLMs trained on domain-specific scientific literature improve concept extraction performance in cybersecurity research?
- Basis in paper: [explicit] The paper demonstrates that general LLMs perform poorly at concept extraction in cybersecurity due to fine-tuning datasets being general texts rather than scientific literature. Only KBIR-inspec was fine-tuned on scientific data but results suggest it's outdated.
- Why unresolved: The paper tested existing general-purpose LLMs but did not explore whether LLMs specifically fine-tuned on recent cybersecurity research literature would perform better.
- What evidence would resolve it: Comparing cybersecurity-specific fine-tuned LLMs against both general LLMs and the proposed noun extraction method on the same cybersecurity arXiv dataset.

### Open Question 2
- Question: What embedding methods work best for comparing concepts across different cybersecurity research domains?
- Basis in paper: [explicit] The paper found that cosine similarity and various embeddings (spaCy, GloVe, BERT-Large, GPT-2, Fasttext, word2vec) perform poorly for clustering cybersecurity concepts, with results highly dependent on embedding choice.
- Why unresolved: While the paper identified this limitation, it did not explore alternative embedding approaches or determine which methods might work better for domain-specific concept comparison.
- What evidence would resolve it: Systematic testing of domain-adapted embeddings, contextual embeddings, or alternative similarity measures specifically for cybersecurity concept clustering.

### Open Question 3
- Question: How can we effectively detect and disambiguate technical abbreviations and domain-specific terminology in cybersecurity research?
- Basis in paper: [inferred] The paper notes challenges with abbreviations like "DeepML, RNNs, LSTMs" and mentions that embeddings struggle with domain-specific terms that have drifted in meaning.
- Why unresolved: The paper's noun extraction method shows promise but still struggles with abbreviations and technical terminology, and doesn't address disambiguation.
- What evidence would resolve it: Development and testing of abbreviation expansion techniques or models that can handle domain-specific terminology disambiguation in cybersecurity contexts.

## Limitations

- The evaluation focuses on a narrow subset of arXiv preprints, limiting generalizability to other scientific domains or publication sources.
- The study uses relatively short time windows for trend analysis, which may not capture long-term concept evolution or stability.
- The noun extraction pipeline, while effective, may miss context-dependent entities and relationships that more sophisticated models could capture.

## Confidence

- Claim: LLMs perform poorly at concept extraction in cybersecurity literature
  - Confidence: Medium - Supported by empirical evidence but limited to specific models and domains
- Claim: Statistical filtering identifies domain-specific terms effectively
  - Confidence: Low - Shows promise but lacks validation against human expertise
- Claim: Traditional bibliometrics fail in fast-evolving domains
  - Confidence: Medium - Well-reasoned but not extensively quantified

## Next Checks

1. Test the noun extraction pipeline across multiple scientific domains (not just cybersecurity) to assess domain adaptability and identify which components are universally applicable versus domain-specific.

2. Implement a longitudinal study tracking identified cybersecurity terms over multiple years to validate trend stability and detect concept evolution patterns beyond the initial time window.

3. Conduct a controlled experiment comparing the proposed statistical filtering approach against human domain experts annotating a subset of terms to establish ground truth accuracy for domain-specific term identification.