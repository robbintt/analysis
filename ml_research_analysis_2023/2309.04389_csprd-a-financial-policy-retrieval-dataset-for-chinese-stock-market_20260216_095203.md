---
ver: rpa2
title: 'CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market'
arxiv_id: '2309.04389'
source_url: https://arxiv.org/abs/2309.04389
tags:
- policy
- retrieval
- csprd
- dataset
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSPRD, a new policy retrieval dataset for
  Chinese stock market, containing 700+ prospectus passages annotated with relevant
  policy articles from a 10k+ policy corpus. The task addresses the challenge of matching
  complex business descriptions in prospectuses with concise policy articles, requiring
  models to bridge different language styles.
---

# CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market

## Quick Facts
- arXiv ID: 2309.04389
- Source URL: https://arxiv.org/abs/2309.04389
- Reference count: 0
- Primary result: Expert-annotated policy retrieval dataset with 700+ prospectus passages matched to 10k+ policy articles

## Executive Summary
This paper introduces CSPRD, a new policy retrieval dataset for Chinese stock market containing 700+ prospectus passages annotated with relevant policy articles from a 10k+ policy corpus. The task addresses the challenge of matching complex business descriptions in prospectuses with concise policy articles, requiring models to bridge different language styles. Experiments with lexical, embedding, and fine-tuned bi-encoder models show the dataset's effectiveness while indicating room for improvement.

## Method Summary
The dataset construction involved extracting prospectus passages from STAR Market listed companies and building a Chinese policy corpus of 10,002 articles. A mixture-of-experts (MoE) unsupervised selection system using word2vec, doc2vec, and SimBERT models ranked policy articles by similarity to prospectus passages. Expert annotators then reviewed top candidates and provided ternary labels (Yes/No/Uncertain). Baseline retrieval models include TF-IDF, BM25, unsupervised embedding methods, and fine-tuned BERT variants trained using contrastive learning.

## Key Results
- Best baseline achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10, and 80.6% Precision@10 on dev set
- Traditional lexical methods perform poorly, indicating the need for dense retrieval approaches
- Fine-tuned bi-encoder models outperform unsupervised embedding methods
- Dataset fills a gap in specialized financial retrieval benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert annotation bridges the gap between complex business descriptions and concise policy language
- Mechanism: The dataset uses experienced experts to label policy articles that match prospectus passages, overcoming the language style mismatch between detailed business descriptions and brief policy items
- Core assumption: Expert judgment can reliably identify semantic matches across different linguistic styles
- Evidence anchors:
  - [abstract] "contains 700+ prospectus passages annotated with relevant policy articles from a 10k+ policy corpus"
  - [section 3.4] "The CSPRD is annotated by 5 experienced SSE experts... After cautious reading and thorough judgement, the experts should choose one of the ternary labels ([Yes], [No], [Uncertain])"
  - [corpus] Weak - no quantitative validation of annotation quality provided
- Break condition: If expert annotations introduce systematic bias or fail to capture true semantic relationships

### Mechanism 2
- Claim: Mixture-of-experts (MoE) unsupervised selection reduces annotation burden while maintaining quality
- Mechanism: Pre-trained unsupervised models (word2vec, doc2vec, SimBERT) rank policy articles by similarity to prospectus passages, presenting top candidates for expert review
- Core assumption: Textual similarity in embedding space correlates with semantic relevance for this domain
- Evidence anchors:
  - [section 3.3] "Our MoE selection system is consisted of unsupervised models... The final score for each text pair is the weighted sum of the scores given by word2vec (10%), doc2vec (20%) and SimBERT (70%)"
  - [section 3.4] "To reduce cost of human resources, we deploy a mixture of experts (MoE) selection system"
  - [corpus] Missing - no comparison of MoE-selected candidates vs random selection
- Break condition: If similarity-based ranking consistently misses relevant policy articles that experts would identify

### Mechanism 3
- Claim: Fine-tuned bi-encoder models outperform lexical and embedding-only approaches
- Mechanism: Dense passage retrieval with contrastive learning learns task-specific representations that capture semantic relationships beyond surface similarity
- Core assumption: The task benefits from learned representations rather than relying solely on term matching or generic embeddings
- Evidence anchors:
  - [section 4.2] "Traditional methods perform rather poorly, indicating that attempting to address this task purely from the statistics of term frequency is quite challenging"
  - [Table 1] "BERT† [1, 17] 53.6 26.9 35.8 79.2" vs "BM25 [14] 22.3 13.1 14.3 13.1"
  - [corpus] Weak - no ablation study comparing fine-tuning vs frozen embeddings
- Break condition: If task-specific fine-tuning provides minimal improvement over strong pre-trained models

## Foundational Learning

- Concept: Dense passage retrieval and contrastive learning
  - Why needed here: The task requires finding semantically relevant policy articles from a large corpus, which lexical methods struggle with due to vocabulary mismatches
  - Quick check question: What is the key difference between lexical and dense retrieval methods in handling vocabulary mismatches?

- Concept: Pre-trained language models and domain adaptation
  - Why needed here: The specialized financial domain requires models to understand both business descriptions and policy language, which general pre-training may not capture
  - Quick check question: How does fine-tuning on domain-specific data improve model performance compared to using off-the-shelf pre-trained models?

- Concept: Information retrieval evaluation metrics (MRR, NDCG, Recall, Precision)
  - Why needed here: These metrics provide different perspectives on retrieval quality - MRR rewards early relevant results, NDCG considers ranking quality, Recall measures coverage, Precision measures accuracy
  - Quick check question: What does an MRR@10 score of 56.1% tell you about the baseline model's ability to retrieve relevant policies?

## Architecture Onboarding

- Component map: Prospectus passage → Pre-trained encoder → Dense vector representation → Similarity scoring with policy corpus → Top-k retrieval → Evaluation metrics
- Critical path: Data processing → MoE candidate selection → Expert annotation → Model training → Evaluation
- Design tradeoffs: Expert annotation provides high-quality labels but is expensive and slow; unsupervised MoE reduces cost but may miss relevant articles; fine-tuning improves performance but requires computational resources
- Failure signatures: Low MRR/NDCG scores indicate the model struggles to rank relevant articles highly; high precision but low recall suggests the model is too conservative in its predictions
- First 3 experiments:
  1. Compare lexical methods (TF-IDF, BM25) vs embedding methods (word2vec, doc2vec, SimBERT) on dev set to establish baseline performance
  2. Fine-tune a bi-encoder model (e.g., BERT) on the training data and evaluate on dev set to measure improvement from task-specific training
  3. Analyze failure cases by examining prospectus passages where the model fails to retrieve relevant policies to identify systematic weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of retrieval models change when applied to prospectus passages from different STAR Market categories (e.g., Biomedicine vs. New Materials)?
- Basis in paper: [inferred] The paper mentions that CSPRD samples are labeled into seven categories, but does not provide per-category performance analysis of the retrieval models.
- Why unresolved: The paper only reports overall performance metrics without breaking down results by category, making it unclear if certain domains are more challenging for the models.
- What evidence would resolve it: Detailed performance metrics (MRR@10, NDCG@10, Recall@10, Precision@10) for each of the seven categories would show if some domains are inherently harder for retrieval.

### Open Question 2
- Question: What is the impact of policy article length on retrieval accuracy, and is there an optimal length for policy articles in this domain?
- Basis in paper: [inferred] Figure 7 shows length distributions of prospectus passages and policy articles, but the paper does not analyze how article length affects retrieval performance.
- Why unresolved: The paper presents length statistics but doesn't correlate them with retrieval effectiveness, leaving unclear whether longer or shorter policies are more beneficial for matching.
- What evidence would resolve it: Performance analysis segmented by policy article length ranges would reveal if there's a correlation between length and retrieval accuracy.

### Open Question 3
- Question: How would incorporating domain-specific terminology or entity recognition improve retrieval performance beyond the current baselines?
- Basis in paper: [explicit] The paper mentions using ERNIE-CTM for named entity recognition in the annotation process, but doesn't explore whether entity-based features improve retrieval models.
- Why unresolved: The paper uses named entity recognition only for annotation filtering, not as features for the retrieval models, missing potential performance gains from domain-specific linguistic features.
- What evidence would resolve it: Experiments comparing baseline models with variants that incorporate entity-based features or domain terminology matching would show the impact on retrieval metrics.

## Limitations

- Expert annotation quality lacks quantitative validation through inter-annotator agreement metrics
- MoE selection system effectiveness in reducing annotation burden is claimed but not empirically validated
- Baseline models still show substantial room for improvement with 56.1% MRR@10 on dev set

## Confidence

- High confidence: The dataset construction methodology and evaluation framework are clearly specified and reproducible
- Medium confidence: The claim that expert annotation bridges language style gaps is supported by qualitative reasoning but lacks quantitative validation
- Low confidence: The assertion that MoE selection maintains annotation quality while reducing cost has not been empirically verified

## Next Checks

1. Conduct inter-annotator agreement analysis to quantify the reliability of expert annotations and assess consistency across the 5 annotators
2. Perform ablation studies comparing the MoE selection system against random candidate selection and full manual annotation to validate its effectiveness in reducing annotation burden
3. Evaluate whether domain-specific fine-tuning provides significant improvement over strong general-purpose pre-trained models through controlled experiments with frozen vs fine-tuned encoders