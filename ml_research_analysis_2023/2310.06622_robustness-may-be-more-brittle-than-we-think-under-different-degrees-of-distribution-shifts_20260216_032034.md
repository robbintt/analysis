---
ver: rpa2
title: Robustness May be More Brittle than We Think under Different Degrees of Distribution
  Shifts
arxiv_id: '2310.06622'
source_url: https://arxiv.org/abs/2310.06622
tags:
- distribution
- shift
- degrees
- shifts
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how model robustness to distribution shifts
  can be highly sensitive to the degree of the shift. The authors show that models
  robust to mild shifts may fail dramatically under slightly stronger shifts, and
  training on strongly shifted data does not guarantee robustness to milder shifts.
---

# Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts

## Quick Facts
- arXiv ID: 2310.06622
- Source URL: https://arxiv.org/abs/2310.06622
- Reference count: 17
- Primary result: Model robustness to distribution shifts is highly sensitive to the degree of shift, with robust models at mild shifts failing dramatically under slightly stronger shifts.

## Executive Summary
This paper reveals that model robustness to distribution shifts is surprisingly brittle, with performance degrading sharply when the degree of shift exceeds certain thresholds. The authors demonstrate that models robust to mild shifts may catastrophically fail under slightly stronger shifts, and that training on strongly shifted data does not guarantee robustness to milder shifts. Through experiments on synthetic and real image datasets, they show that the breaking point of robustness is task-dependent and unpredictable from performance at lower degrees. The findings challenge conventional wisdom about distribution shift robustness and highlight the need for more nuanced evaluation methods.

## Method Summary
The paper evaluates model robustness across multiple degrees of distribution shifts using controlled datasets like Noisy MNIST (11 domains with increasing Gaussian noise) and Rotated MNIST (8 domains with rotation angles from 0-80Â°). The authors compare ERM with 20+ domain generalization algorithms using ResNet-50 and ViT architectures, and test large pre-trained models like CLIP through linear probing. Training involves subsets of domains with specific shift degrees, followed by evaluation on all domains to analyze robustness patterns. The experiments systematically vary the degree and type of distribution shifts to identify breaking points and generalization patterns.

## Key Results
- Models robust to mild distribution shifts can fail dramatically under slightly stronger shifts due to spurious correlation breakdown
- Training on strongly shifted data does not guarantee robustness to milder shifts; learning mechanisms for different degrees may not transfer
- Large pre-trained models like CLIP show excellent in-distribution performance but are highly sensitive to downstream distribution shifts, especially unseen ones
- The degree at which robustness breaks is highly task-dependent and unpredictable from lower-degree performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models robust to mild distribution shifts may fail dramatically under slightly stronger shifts.
- Mechanism: The spurious correlation between predictive features and labels breaks at a specific degree of shift, and the breaking point is highly task-dependent.
- Core assumption: The degree of shift at which robustness breaks is not predictable from performance at lower degrees.
- Evidence anchors:
  - [abstract]: "models robust to mild shifts may fail dramatically under slightly stronger shifts"
  - [section]: "The brittleness manifests when the spurious correlation between the local features and the target labels reaches a breaking point"
  - [corpus]: No direct evidence found in related papers
- Break condition: When the spurious correlation between local features and target labels reaches a breaking point that varies by task and shift type

### Mechanism 2
- Claim: Training on strongly shifted data does not guarantee robustness to milder shifts.
- Mechanism: Different distribution shifts create different generalization patterns, and learning mechanisms for high-degree shifts may not transfer to low-degree shifts.
- Core assumption: The learning mechanisms for different degrees of shift are task-specific and not necessarily transferable.
- Evidence anchors:
  - [abstract]: "training on strongly shifted data does not guarantee robustness to milder shifts"
  - [section]: "training with data under only high degrees of distribution shifts may not be able to solve the problem as it does not always guarantee robustness to lower degrees"
  - [corpus]: No direct evidence found in related papers
- Break condition: When the shift type creates fundamentally different generalization requirements across degrees

### Mechanism 3
- Claim: Large-scale pre-trained models like CLIP are sensitive to downstream distribution shifts, especially unseen ones.
- Mechanism: Pre-training data distribution determines robustness transfer, and unseen shift types create disproportionate performance drops.
- Core assumption: The robustness of pre-trained representations is limited to shift types present in pre-training data.
- Evidence anchors:
  - [abstract]: "large-scale pre-trained models, such as CLIP, are sensitive to even minute distribution shifts of novel downstream tasks"
  - [section]: "we observe that large-scale pre-trained models like CLIP are sensitive to downstream distribution shifts, especially unseen or rarely seen ones"
  - [corpus]: No direct evidence found in related papers
- Break condition: When downstream tasks involve shift types rarely or never seen during pre-training

## Foundational Learning

- Concept: Distribution shift quantification
  - Why needed here: The paper evaluates robustness across multiple degrees of shift, requiring clear understanding of how to measure and order different shift levels
  - Quick check question: How would you quantify the degree of a distribution shift when you have multiple domains with varying levels of noise or rotation?

- Concept: Domain generalization vs. out-of-distribution generalization
  - Why needed here: The paper compares ERM with domain generalization algorithms, requiring understanding of the distinction between these approaches
  - Quick check question: What is the key difference between domain generalization and OOD generalization in terms of training data access?

- Concept: Pre-trained representation adaptation
  - Why needed here: The paper investigates CLIP models adapted through linear probing, requiring understanding of how pre-trained models are adapted to downstream tasks
  - Quick check question: What is linear probing and how does it differ from full fine-tuning when adapting pre-trained models?

## Architecture Onboarding

- Component map: Data generation (creating datasets with varying shift degrees) -> Model training (ERM and DG algorithms) -> Performance measurement across shift degrees
- Critical path: 1) Generate or select datasets with controlled shift degrees, 2) Train models on specific degree combinations, 3) Evaluate performance across all degrees, 4) Analyze robustness patterns and breaking points
- Design tradeoffs: Training on high-degree shifts may harm low-degree performance vs. training on multiple degrees may improve overall robustness but require more data
- Failure signatures: Performance that appears robust at one degree but fails dramatically at adjacent degrees, or pre-trained models that show excellent ID performance but poor OOD generalization
- First 3 experiments:
  1. Replicate the Noisy MNIST experiment comparing ERM vs. DG algorithms across shift degrees
  2. Test CLIP vs. ResNet-50 on rotated MNIST with linear probing adaptation
  3. Evaluate training on different degree combinations and measure generalization patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the breaking point of model robustness under distribution shifts, and does this breaking point vary across different types of shifts?
- Basis in paper: [explicit] The paper discusses the brittleness of robustness, showing that a slight increase in shift degree can cause severe performance drops, but does not provide a method to quantify or predict this breaking point.
- Why unresolved: The authors highlight the unpredictability of when robustness fails but do not explore systematic ways to measure or anticipate this threshold.
- What evidence would resolve it: Experiments that systematically vary shift degrees across multiple tasks and types, identifying consistent patterns or thresholds where performance degradation occurs.

### Open Question 2
- Question: Can pre-trained models like CLIP be adapted to improve robustness against rare or unseen distribution shifts in downstream tasks?
- Basis in paper: [explicit] The paper notes that CLIP models are highly sensitive to downstream distribution shifts, especially those rare during pre-training, but also shows that further adaptation can improve robustness.
- Why unresolved: While the paper demonstrates sensitivity and the potential for adaptation, it does not explore optimal strategies for adapting pre-trained models to specific rare shifts.
- What evidence would resolve it: Empirical studies comparing different adaptation techniques (e.g., fine-tuning, domain adaptation) on rare shifts to determine the most effective methods for improving robustness.

### Open Question 3
- Question: Is there a relationship between the degree of distribution shift in training data and the model's ability to generalize across a broader range of shifts?
- Basis in paper: [inferred] The paper shows that training on strongly shifted data does not always guarantee robustness to milder shifts, suggesting a complex relationship between training shift degrees and generalization.
- Why unresolved: The authors observe inconsistent results but do not investigate whether certain patterns or conditions influence this relationship.
- What evidence would resolve it: Systematic experiments varying the range and degree of shifts in training data to identify conditions under which training on high-degree shifts improves generalization to lower degrees.

## Limitations
- Results are based on synthetic and controlled image datasets (MNIST variants and small ImageNet subset), which may not fully capture real-world shift patterns
- Analysis focuses exclusively on image classification tasks, leaving open questions about generalizability to other modalities
- The study does not explore whether robustness brittleness patterns appear in naturally occurring distribution shifts

## Confidence
- High confidence in the core empirical findings showing performance discontinuities across shift degrees in controlled experiments
- Medium confidence in the proposed mechanisms linking robustness brittleness to spurious correlations and task-dependent breaking points
- Low confidence in the generalizability of conclusions to natural distribution shifts and other domains beyond image classification

## Next Checks
1. Replicate the degree-sensitivity experiments on naturally occurring distribution shifts (e.g., from DomainNet or WILDS) to test whether synthetic patterns hold for real-world data
2. Extend the analysis to non-vision domains (text, tabular data) to determine if robustness brittleness is a universal phenomenon or domain-specific
3. Conduct ablation studies varying the number of training domains and shift types to better understand the interaction between training data diversity and robustness generalization patterns