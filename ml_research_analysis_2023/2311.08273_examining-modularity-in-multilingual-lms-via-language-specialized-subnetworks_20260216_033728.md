---
ver: rpa2
title: Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks
arxiv_id: '2311.08273'
source_url: https://arxiv.org/abs/2311.08273
tags:
- language
- subnetworks
- influence
- test
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the degree of language-wise modularity in multilingual
  language models (LMs) without special modularity interventions. The authors identify
  language-specific subnetworks within fine-tuned models and use a Training Data Attribution
  method to measure their reliance on in-language training data.
---

# Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks

## Quick Facts
- arXiv ID: 2311.08273
- Source URL: https://arxiv.org/abs/2311.08273
- Reference count: 40
- Key outcome: Language-specialized subnetworks naturally emerge without explicit modularity interventions, but sparse fine-tuning (SFT) can decrease language specialization in favor of cross-lingual sharing

## Executive Summary
This paper investigates modularity in multilingual language models by identifying language-specific subnetworks within fine-tuned models and measuring their reliance on in-language training data. The authors find that language-specialized subnetworks naturally emerge without explicit interventions, showing increased reliance on in-language data compared to full models. However, contrary to expectations, sparse fine-tuning does not always increase modularity - it can decrease language specialization by promoting cross-lingual sharing. The study also reveals a positive correlation between subnetwork similarity and positive cross-language influence, suggesting potential for guiding cross-lingual transfer.

## Method Summary
The authors fine-tune XLM-R on three multilingual tasks (XNLI, PAWS-X, MARC) and identify language-specific subnetworks through structured pruning of attention heads, retaining subnetworks that maintain at least 95% of full model performance. They use Training Data Attribution (TDA) with TracIn to measure how much each subnetwork relies on in-language versus cross-language training data. The study compares cross-language influence patterns between full model fine-tuning and sparse fine-tuning, where only subnetwork parameters are updated during training.

## Key Results
- Language-specialized subnetworks naturally arise without explicit modularity interventions, showing increased reliance on in-language training data
- Sparse fine-tuning can decrease language specialization by promoting cross-lingual sharing rather than increasing modularity
- Subnetwork similarity correlates positively with positive cross-language influence, suggesting structural similarity predicts beneficial transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specialized subnetworks naturally arise without explicit modularity interventions
- Mechanism: The pruning process identifies attention heads that are unimportant for a given language, creating subnetworks that focus on in-language data
- Core assumption: Attention heads can be meaningfully partitioned by language without harming task performance
- Evidence anchors:
  - [abstract] "language-specialized subnetworks do naturally arise"
  - [section 3.1] "For each language, we identify a subnetwork—a subset of model parameters—such that when fine-tuned on in-language data, it performs on par with the full model on that language"
  - [corpus] Weak - corpus papers discuss cross-lingual transfer but don't directly address natural subnetwork emergence
- Break condition: If pruning removes heads critical for both language-specific and cross-lingual processing, performance would degrade below 95% threshold

### Mechanism 2
- Claim: Sparse fine-tuning can decrease language specialization by promoting cross-lingual sharing
- Mechanism: By restricting parameter updates to language-specific subnetworks during training, the model learns to share information across languages through overlapping parameters
- Core assumption: Subnetwork overlap facilitates beneficial cross-lingual transfer rather than interference
- Evidence anchors:
  - [abstract] "SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing"
  - [section 6] "we find that SFT only sometimes causes our identified subnetworks to rely more on in-language data"
  - [corpus] Moderate - corpus includes papers on cross-lingual transfer and multi-source training that support this mechanism
- Break condition: If subnetwork overlap causes gradient conflicts or negative interference that outweighs transfer benefits

### Mechanism 3
- Claim: Subnetwork similarity correlates with positive cross-language influence
- Mechanism: Similar languages have similar subnetworks (in terms of enabled attention heads), and this structural similarity predicts beneficial knowledge transfer
- Core assumption: Structural similarity in subnetworks reflects underlying linguistic similarity that enables transfer
- Evidence anchors:
  - [section 7.2] "we find that for both tasks, subnetwork similarity is positively correlated with positive cross-language influence"
  - [section 3.1] "for a language ℓ, its subnetwork is implemented as a binary mask ξℓ ∈ {0, 1}H×L"
  - [corpus] Moderate - corpus contains papers on cross-lingual alignment and transfer that support the relationship between structure and transfer
- Break condition: If structural similarity doesn't predict transfer (e.g., typologically similar languages with dissimilar subnetworks, or vice versa)

## Foundational Learning

- Concept: Attention heads as modular computation units
  - Why needed here: The paper treats entire attention heads as units for pruning, so understanding their function is critical
  - Quick check question: What happens to a transformer layer if all its attention heads are disabled?

- Concept: Gradient-based pruning importance scores
  - Why needed here: The subnetwork identification uses sensitivity-based importance scores to determine which heads to prune
  - Quick check question: How would you compute the importance of an attention head using the gradient of the loss with respect to its mask?

- Concept: Training Data Attribution (TDA) and influence functions
  - Why needed here: The paper uses TracIn to measure how much training examples influence predictions, which is central to quantifying cross-language influence
  - Quick check question: What does it mean if a training example has high positive influence on a test prediction?

## Architecture Onboarding

- Component map: XLM-R base model -> Language-specific binary masks (subnetworks) -> Classifier head -> TracIn influence computation pipeline -> Pruning algorithm

- Critical path:
  1. Fine-tune full model on multilingual data
  2. Identify language-specific subnetworks via pruning
  3. Apply subnetworks at test time and compute influence scores
  4. Compare cross-language influence patterns between full model and subnetworks
  5. Apply SFT and repeat influence analysis

- Design tradeoffs:
  - Structured pruning (entire heads) vs. unstructured pruning (individual weights)
  - Sparsity level vs. performance retention (95% threshold)
  - Computational cost of influence computation vs. insight gained
  - Parallel vs. non-parallel datasets for studying cross-lingual dynamics

- Failure signatures:
  - Subnetworks don't improve performance over full model
  - No difference in cross-language influence patterns between full model and subnetworks
  - SFT decreases performance instead of improving it
  - Influence scores show no meaningful patterns or are dominated by a few examples

- First 3 experiments:
  1. Verify subnetwork identification: Fine-tune on multilingual data, prune to identify subnetworks, verify they maintain ≥95% performance on their respective languages
  2. Test cross-language influence: Use TracIn to compute influence scores for subnetworks vs. full model, verify subnetworks show increased in-language reliance
  3. Test SFT effects: Apply SFT with identified subnetworks, recompute influence scores, verify whether language specialization increases or decreases compared to full model fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sparsity level of language-specific subnetworks affect their ability to promote cross-lingual sharing while maintaining language specialization?
- Basis in paper: [explicit] The paper discusses that all subnetworks have similar sparsity levels but some heads are not used by any language, suggesting capacity is not a limiting factor.
- Why unresolved: The paper does not explore how varying the sparsity level might affect the balance between language specialization and cross-lingual sharing.
- What evidence would resolve it: Experiments varying the sparsity levels of subnetworks and measuring their impact on both language specialization and cross-lingual sharing would provide insights into the optimal sparsity level for balancing these aspects.

### Open Question 2
- Question: What is the impact of task complexity on the effectiveness of sparse fine-tuning in promoting language specialization and cross-lingual sharing?
- Basis in paper: [inferred] The paper uses relatively simple classification tasks and mentions that task complexity might limit the benefits of SFT.
- Why unresolved: The study does not investigate how different task complexities influence the outcomes of SFT on language specialization and cross-lingual sharing.
- What evidence would resolve it: Conducting experiments on more complex tasks and comparing the effects of SFT on language specialization and cross-lingual sharing across varying task complexities would clarify the impact of task difficulty.

### Open Question 3
- Question: How do the dynamics of cross-language influence evolve during the training process, and how do they differ between full model fine-tuning and sparse fine-tuning?
- Basis in paper: [explicit] The paper analyzes the change in language specialization over training epochs for PAWS-X and MARC, noting differences between full model fine-tuning and SFT.
- Why unresolved: While the paper provides some insights into the evolution of cross-language influence, it does not offer a detailed analysis of how these dynamics change over time or the specific differences between fine-tuning methods.
- What evidence would resolve it: Detailed tracking of cross-language influence at each training epoch for both fine-tuning methods would reveal how these dynamics evolve and differ, providing a clearer understanding of the training process.

## Limitations

- The study is limited to three tasks and five languages each, which may not generalize across diverse language families
- The finding that SFT can decrease language specialization is based on a limited sample size and effect varies by task
- The correlation between subnetwork similarity and positive transfer could be confounded by shared training data or inherent task similarities

## Confidence

- High confidence: Natural emergence of language-specialized subnetworks (directly observed and measured)
- Medium confidence: SFT effects on modularity (based on limited task/language samples, effect varies by task)
- Medium confidence: Subnetwork similarity predicting transfer (statistically significant correlation but potential confounding factors)

## Next Checks

1. Test subnetwork emergence with different performance thresholds (90%, 98%) to establish robustness of the natural modularity pattern
2. Expand analysis to typologically diverse language families beyond Indo-European to verify cross-lingual sharing patterns
3. Conduct ablation studies removing cross-language training data to isolate the source of subnetwork similarities