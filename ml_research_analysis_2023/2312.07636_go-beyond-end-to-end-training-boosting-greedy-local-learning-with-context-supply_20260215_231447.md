---
ver: rpa2
title: 'Go beyond End-to-End Training: Boosting Greedy Local Learning with Context
  Supply'
arxiv_id: '2312.07636'
source_url: https://arxiv.org/abs/2312.07636
tags:
- information
- local
- contsup
- learning
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Greedy local learning (GLL) partitions networks into gradient-isolated\
  \ modules trained under local supervision, offering memory efficiency and parallelization\
  \ benefits. However, as the number of partitions increases, GLL suffers from severe\
  \ performance degradation due to irreversible information loss\u2014a bottleneck\
  \ caused by the inability of isolated modules to access task-relevant information\
  \ from other partitions."
---

# Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply

## Quick Facts
- arXiv ID: 2312.07636
- Source URL: https://arxiv.org/abs/2312.07636
- Reference count: 31
- Key outcome: Context Supply significantly improves greedy local learning performance by introducing additional feature paths between isolated modules, achieving state-of-the-art results while maintaining memory efficiency.

## Executive Summary
This paper addresses a fundamental limitation in greedy local learning (GLL) where gradient-isolated modules suffer from irreversible information loss as the number of partitions increases. Through information-theoretic analysis, the authors identify that mutual information between intermediate features and target labels decreases monotonically through isolated modules, creating a performance ceiling. The proposed Context Supply (ContSup) method introduces additional context paths between modules, allowing intermediate features to access lost information through element-wise addition. Experiments on CIFAR-10, SVHN, and STL-10 demonstrate that ContSup significantly improves GLL performance while maintaining low memory overhead, enabling larger numbers of partitions without sacrificing accuracy.

## Method Summary
Context Supply (ContSup) enhances greedy local learning by adding context modules that inject information from earlier layers or the original input into deeper layers. The method partitions networks into K gradient-isolated modules, each trained with local supervision, but adds context paths that bypass the information degradation chain. Context can be sourced from the original input (ContSup[E]) or from previous features (ContSup[R1]), and is integrated via simple element-wise addition. This allows each module to access task-relevant information it would otherwise lose, breaking the irreversible information degradation that limits GLL performance. The approach maintains computational compatibility with existing architectures while significantly improving memory-performance tradeoffs.

## Key Results
- ContSup improves GLL test error by 1.8-3.6% on CIFAR-10 with ResNet-32 across various partition counts (K=2,4,8,16)
- Memory overhead remains low at 3.5% compared to baseline DGL while providing substantial accuracy gains
- ContSup achieves state-of-the-art results among GLL methods, outperforming InfoPro and other context-aware approaches
- The method enables successful training with more partitions (K=16) than previously possible without severe accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context Supply introduces additional feature paths between isolated modules to restore task-relevant information lost during greedy local learning.
- Mechanism: The method adds context modules that inject information from earlier layers or the original input into deeper layers, allowing each module to access features it would otherwise lose. This breaks the irreversible information degradation chain that limits GLL performance.
- Core assumption: Task-relevant information can be partially recovered by providing intermediate features access to earlier-stage information through element-wise addition.
- Evidence anchors:
  - [abstract]: "Context Supply (ContSup) to address it. ContSup introduces additional context paths between modules, enabling intermediate features to access lost information and escape the performance degradation trap."
  - [section]: "we intuitively propose the Context Supply (ContSup) scheme to incorporate an additional information path in an attempt to supplement the lost information by context, aimed to escape the discussed dilemma"
  - [corpus]: Weak - neighboring papers focus on other GLL variations without specific context supply mechanisms
- Break condition: If the context injection path introduces noise that outweighs the benefit of recovered information, or if the context module fails to compress/align features appropriately for the element-wise addition operation.

### Mechanism 2
- Claim: Greedy Local Learning suffers from a "confirmed habit dilemma" where early modules irreversibly lose task-relevant information that cannot be recovered by subsequent modules.
- Mechanism: The isolated training causes each module to optimize only for its local objective without considering downstream requirements, creating a progressive degradation in mutual information I(hl, y) as depth increases. This creates an upper bound on final performance that cannot be exceeded regardless of how well later modules are trained.
- Core assumption: Mutual information between intermediate features and the target label decreases monotonically through the network depth in GLL, unlike in end-to-end training where it remains relatively stable.
- Evidence anchors:
  - [section]: "The feedforward process of isolated modules forms the Markov chain... Then the decreasing trend of mutual information is given by: I(x,y) ≥ I(hl-1,y) ≥ I(hl,y) ≥ I(ˆyl,y)"
  - [section]: "Empirical experiments show that I(hl,y) will decrease substantially during the greedy module process, whereas it will remain essentially unchanged during the E2E process"
  - [corpus]: Weak - neighboring papers don't discuss this specific information-theoretic bottleneck in detail
- Break condition: If the Markov chain assumption is violated by introducing non-local information flow, or if the local objectives somehow manage to preserve task-relevant information despite isolation.

### Mechanism 3
- Claim: The element-wise addition operation for context integration preserves computational compatibility while enabling information recovery.
- Mechanism: By using simple addition rather than complex fusion operations, ContSup maintains the same input/output shapes and computational patterns as the original modules, making it easy to integrate without architectural changes while still allowing meaningful information combination.
- Core assumption: Element-wise addition is sufficient to combine context information with existing features in a way that enhances task-relevant information without disrupting the module's learned representations.
- Evidence anchors:
  - [section]: "For simplicity, we utilize element-wise addition to incorporate context and feature without changing the shape size of hl and the configuration of F l, i.e., hl = F l(hc l-1) = F l(hl-1 + Ml(cl))"
  - [section]: "Concise priors to ascend supremum. There are currently two simple and intuitive options available for locating a suitable context to boost lth module after (l-1)th module"
  - [corpus]: Weak - neighboring papers don't discuss this specific integration mechanism in detail
- Break condition: If the context and feature representations are incompatible in scale or distribution, making element-wise addition ineffective or harmful to learning.

## Foundational Learning

- Concept: Information Theory and Mutual Information
  - Why needed here: The entire analysis and solution is built on understanding how information flows and degrades through the network, requiring knowledge of concepts like mutual information, entropy, and Markov chains.
  - Quick check question: If I have two random variables X and Y with joint distribution p(x,y), what mathematical expression represents the mutual information between them?

- Concept: Gradient-Isolated Learning and Local Objectives
  - Why needed here: Understanding how GLL differs from standard end-to-end training and why the isolation creates problems is fundamental to appreciating the solution.
  - Quick check question: In gradient-isolated learning, how does the training signal for each module differ from traditional back-propagation?

- Concept: Neural Network Architecture and Residual Connections
  - Why needed here: The solution leverages architectural patterns like residual connections and requires understanding of how to integrate new modules without breaking existing functionality.
  - Quick check question: How does a residual connection help preserve information flow in deep networks, and why is this relevant to the context supply mechanism?

## Architecture Onboarding

- Component map:
  - Backbone network (F θ): The main feedforward path divided into K gradient-isolated modules
  - Local classifier (A w): Auxiliary modules that provide local supervision for each isolated module
  - Context modules (M ϕ): New components that generate and inject context information via element-wise addition
  - Optional decoder (W): Local reconstruction module for information preservation (alternative to context supply)

- Critical path:
  1. Input passes through first module F 1 to produce h1
  2. Context module M E generates context from input x
  3. Context is added to h0 (or previous features) to create hc 0
  4. hc 0 passes through F 1 to produce final h1
  5. Local classifier A w1 produces prediction ˆy1
  6. Process repeats for subsequent modules with context from either input or previous features

- Design tradeoffs:
  - Context source selection: Input-based (E) provides rich information but may be noisy; feature-based (R) is cleaner but carries less information
  - Context path complexity: Simpler paths (E or R1) have lower overhead but may be less effective than complex topological connections (RnE)
  - Memory vs. accuracy: More context paths improve accuracy but increase memory consumption

- Failure signatures:
  - Performance degradation when increasing K beyond baseline GLL (indicates context isn't effectively preserving information)
  - Training instability or divergence (suggests context injection is disrupting learned representations)
  - Minimal accuracy improvement despite increased memory usage (indicates context modules aren't well-designed or integrated)

- First 3 experiments:
  1. Baseline comparison: Implement DGL without context on CIFAR-10 with ResNet-32, measure test error and memory usage for K=2,4,8,16
  2. Simple context addition: Add input-based context supply (ContSup[E]) to the same setup, compare performance and memory overhead
  3. Hybrid context selection: Implement feature-based context (ContSup[R1]) and compare both performance and training stability against the input-based version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can context selection be optimized to avoid redundant or similar features while maintaining task-relevant information?
- Basis in paper: [explicit] The paper mentions that the current linear expansion of context selection is memory-intensive and difficult to scale, and suggests that optimized expansion could prioritize important relationships.
- Why unresolved: The paper only briefly mentions the possibility of optimized context selection without providing concrete methods or evidence.
- What evidence would resolve it: Experimental results comparing different context selection strategies, particularly those that prune redundant connections while preserving performance, would demonstrate effective optimization methods.

### Open Question 2
- Question: What is the relationship between the structural design of context modules and existing network architectures?
- Basis in paper: [explicit] The paper suggests that future research could investigate the relationship between context design and existing networks to draw further conclusions.
- Why unresolved: The paper does not explore this relationship in depth, focusing instead on demonstrating the effectiveness of the basic context supply approach.
- What evidence would resolve it: Comparative studies showing how different context module designs interact with various network architectures (ResNet, DenseNet, etc.) would clarify the optimal integration strategies.

### Open Question 3
- Question: How can the ContSup approach be extended beyond layer-wise to enable truly fine-grained module partitioning?
- Basis in paper: [explicit] The paper mentions that ContSup allows networks to be divided into more segments, potentially decomposing module-wise GLL towards layer-wise.
- Why unresolved: The paper demonstrates benefits with current partitioning but does not explore whether the approach can scale to individual layer partitioning without performance degradation.
- What evidence would resolve it: Experiments showing ContSup maintaining performance with extremely fine-grained partitioning (e.g., individual layer modules) would demonstrate true scalability.

## Limitations

- Information-theoretic analysis relies on approximations and assumptions that may not hold exactly in practice
- Context modules add computational overhead that scales with the number of partitions
- The method's effectiveness depends on careful selection of context source and integration strategy
- Limited exploration of alternative fusion methods beyond simple element-wise addition

## Confidence

- **High Confidence:** The core problem identification (information loss in GLL) and the basic mechanism of context addition are well-supported by empirical results showing consistent accuracy improvements across multiple datasets.
- **Medium Confidence:** The information-theoretic analysis provides a compelling narrative but relies on approximations and assumptions that may not hold exactly in practice.
- **Low Confidence:** The optimality of element-wise addition versus more complex fusion methods, and the precise conditions under which different context sources perform best.

## Next Checks

1. **Information Flow Verification:** Measure and visualize the actual mutual information I(hl, y) across modules with and without context supply to confirm the theoretical predictions match empirical observations.

2. **Context Integration Ablation:** Systematically compare element-wise addition against alternative fusion methods (concatenation with projection, attention-based fusion) to verify the choice of simple addition is optimal.

3. **Memory-Performance Scaling:** Test the method on larger-scale datasets (ImageNet) and deeper architectures (ResNet-50/101) to evaluate whether the memory-efficiency benefits scale as expected.