---
ver: rpa2
title: Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval
arxiv_id: '2308.02926'
source_url: https://arxiv.org/abs/2308.02926
tags:
- retrieval
- consistency
- performance
- domain
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates unsupervised dense retrieval for domain
  adaptation, aiming to replace consistency filtering with more efficient methods.
  The authors propose using direct pseudo-labeling, pseudo-relevance feedback (TF-IDF,
  TextRank, RAKE), or unsupervised keyword generation (KeyBERT) to generate synthetic
  query-document pairs for contrastive learning fine-tuning.
---

# Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval

## Quick Facts
- arXiv ID: 2308.02926
- Source URL: https://arxiv.org/abs/2308.02926
- Reference count: 40
- Key outcome: Consistency filtering can be replaced by unsupervised keyword extraction (TextRank) without degrading retrieval performance, reducing training parameters by 50-67.8% and achieving 2.1Ã— faster inference

## Executive Summary
This paper investigates unsupervised dense retrieval for domain adaptation, aiming to replace consistency filtering with more efficient methods. The authors propose using direct pseudo-labeling, pseudo-relevance feedback (TF-IDF, TextRank, RAKE), or unsupervised keyword generation (KeyBERT) to generate synthetic query-document pairs for contrastive learning fine-tuning. Experimental results on WWW-4 and Robust04 datasets show that TextRank-based pseudo-relevance feedback performs best on average, though improvements are not statistically significant compared to direct ranking. The proposed filtering-free approach reduces training parameters by 50-67.8% compared to baselines and achieves 2.1Ã— faster inference.

## Method Summary
The proposed paradigm consists of two stages: (1) training a general ranker using the COIL framework on MSMARCO open-domain QA pairs, and (2) fine-tuning with unsupervised contrastive learning using synthetic positive pairs generated from pseudo-labeling, TF-IDF, TextRank, RAKE, or KeyBERT methods, combined with negative pairs from BM25 or random sampling. The approach eliminates the need for consistency filtering and manual labeling, achieving domain adaptation through synthetic query-document pair generation and contrastive learning.

## Key Results
- TextRank-based pseudo-relevance feedback outperforms other methods on average for positive sampling
- Random negative sampling combined with TextRank positive sampling yields better domain adaptation than BM25-based negative sampling
- Proposed filtering-free approach reduces training parameters by 50-67.8% and achieves 2.1Ã— faster inference compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency filtering can be replaced by unsupervised keyword extraction without degrading retrieval performance.
- Mechanism: TextRank-based pseudo-relevance feedback extracts domain-relevant keywords from documents, which are used as synthetic queries for contrastive learning fine-tuning.
- Core assumption: Keywords extracted via TextRank are semantically representative of the document content and can serve as effective synthetic queries.
- Evidence anchors:
  - [abstract]: "Our extensive experimental evaluations demonstrate that, on average, TextRank-based pseudo relevance feedback outperforms other methods."
  - [section]: "TextRank: The sentences in a document are parsed, and raw noun chunks and named entities are identified... Finally, PageRank... is used to calculate ranks for each of the nodes, and the top-ranked phrases from the lemma graph are collected."
  - [corpus]: Weak evidence; corpus does not directly validate semantic representativeness of TextRank keywords.
- Break condition: If the keyword extraction fails to capture the core intent of the document, synthetic queries will introduce noise and harm retrieval quality.

### Mechanism 2
- Claim: Random negative sampling combined with TextRank positive sampling yields better domain adaptation than BM25-based negative sampling.
- Mechanism: Random negative sampling introduces greater variance in irrelevant document selection, improving the discriminative ability of the contrastive loss.
- Core assumption: Greater variance in negative samples improves model robustness during domain adaptation.
- Evidence anchors:
  - [section]: "TextRank with Random outperforms other methods... the opposite results were obtained for Robust04, indicating a lack of generalization ability."
  - [section]: "This could be attributed to the fact that for pseudo-labeling, the query ð‘žð‘– âˆˆ ð· does not have any noise, and hence the non-correlation of the negative samples is higher under the BM25 retrieval method compared to random sampling."
  - [corpus]: No direct evidence in corpus; assumption is derived from observed results.
- Break condition: If the variance introduced by random sampling is too high, it may degrade performance by introducing false negatives.

### Mechanism 3
- Claim: Filtering-free unsupervised learning reduces computational cost while maintaining or improving retrieval performance.
- Mechanism: By eliminating the need for consistency filtering and manual labeling, the proposed paradigm reduces the number of training parameters by 50-67.8% and speeds up inference by 2.1Ã—.
- Core assumption: The reduction in computational overhead does not significantly compromise retrieval accuracy.
- Evidence anchors:
  - [abstract]: "The proposed filtering-free approach reduces training parameters by 50-67.8% compared to baselines and achieves 2.1Ã— faster inference."
  - [section]: "Compared to PROMPTAGATOR [13], the proposed paradigm avoids the additional use of the T5 encoder... thereby achieving a reduction of 50% in total parameter count."
  - [corpus]: No corpus evidence; based solely on experimental results.
- Break condition: If the reduced parameter count leads to underfitting, the model may fail to capture domain-specific nuances.

## Foundational Learning

- Concept: Contrastive learning in dense retrieval
  - Why needed here: The proposed paradigm relies on contrastive learning to fine-tune the general ranker using synthetic positive and negative pairs.
  - Quick check question: What is the role of the contrastive loss in aligning query and document embeddings?

- Concept: Pseudo-relevance feedback
  - Why needed here: TextRank and other methods generate synthetic queries based on document content, serving as positive samples for contrastive learning.
  - Quick check question: How does pseudo-relevance feedback differ from traditional relevance feedback in IR?

- Concept: Domain adaptation in neural IR
  - Why needed here: The study aims to improve the domain transfer ability of a general ranker to a specific domain without manual labeling.
  - Quick check question: Why is domain adaptation particularly challenging in dense retrieval compared to sparse retrieval?

## Architecture Onboarding

- Component map: General ranker (COIL-based) -> TextRank keyword extractor -> Random/BM25 negative sampler -> Contrastive learning fine-tuning module
- Critical path:
  1. Extract keywords from domain-specific documents using TextRank.
  2. Generate synthetic query-document pairs.
  3. Apply contrastive learning fine-tuning on the general ranker.
- Design tradeoffs:
  - Using TextRank vs. other keyword extraction methods (TF-IDF, RAKE, KeyBERT).
  - Random vs. BM25 negative sampling.
  - Balancing positive sampling noise vs. negative sampling correlation.
- Failure signatures:
  - Performance degradation when using BM25 negative sampling with TextRank.
  - Overfitting when using too many synthetic queries per document.
  - Underfitting when reducing parameter count too aggressively.
- First 3 experiments:
  1. Compare TextRank vs. TF-IDF vs. RAKE vs. KeyBERT for keyword extraction.
  2. Test random vs. BM25 negative sampling with TextRank positive sampling.
  3. Evaluate the impact of varying the number of synthetic queries per document.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of U for pseudo-labeling in consistency filtering-free unsupervised dense retrieval?
- Basis in paper: [explicit] The paper mentions that the performance on WWW-4 deteriorates substantially when U > U_s, and that it is important to determine the optimal value of U for pseudo-labeling to achieve good results.
- Why unresolved: The paper does not provide a specific value for U_s, and the optimal value may depend on the specific dataset and task.
- What evidence would resolve it: Conducting experiments with different values of U on various datasets and tasks to determine the optimal value for each case.

### Open Question 2
- Question: How does the noise generated by positive and negative sampling methods affect the performance of domain adaptation in consistency filtering-free unsupervised dense retrieval?
- Basis in paper: [explicit] The paper mentions that the noise generated by the positive and negative sampling methods is relatively large, and there is still room for further optimization.
- Why unresolved: The paper does not provide a detailed analysis of the noise generated by each sampling method or how it affects the performance of domain adaptation.
- What evidence would resolve it: Conducting experiments to measure the noise generated by each sampling method and analyzing its impact on the performance of domain adaptation.

### Open Question 3
- Question: What are the limitations of consistency filtering-free unsupervised dense retrieval compared to methods that use consistency filtering or manual labeling?
- Basis in paper: [explicit] The paper mentions that the consistency filtering-free unsupervised paradigm may have some ability for domain adaptation that relies on actual datasets, but it does not provide a detailed comparison with other methods.
- Why unresolved: The paper does not provide a comprehensive evaluation of the limitations of consistency filtering-free unsupervised dense retrieval compared to other methods.
- What evidence would resolve it: Conducting experiments to compare the performance of consistency filtering-free unsupervised dense retrieval with methods that use consistency filtering or manual labeling on various datasets and tasks.

## Limitations

- Performance improvements are not statistically significant compared to baseline methods
- Inconsistent results across datasets (WWW-4 vs. Robust04) suggest lack of generalization
- Missing implementation details for critical components like COIL framework and parameter tuning

## Confidence

**High Confidence**: Computational efficiency claims (50-67.8% parameter reduction, 2.1Ã— faster inference) are straightforward to verify and directly supported by experimental results.

**Medium Confidence**: Comparative performance results between different positive sampling methods are moderately well-supported by experimental data, though lack of statistical significance testing limits confidence.

**Low Confidence**: Generalizability of the approach across different domains is questionable given inconsistent results between WWW-4 and Robust04.

## Next Checks

1. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests on performance differences between TextRank and baseline methods across all datasets to determine if improvements are statistically significant.

2. **Cross-Domain Validation**: Test the proposed approach on additional diverse domains beyond WWW-4 and Robust04 (e.g., medical, legal, scientific literature) to assess true domain adaptation capability.

3. **Ablation Study on Negative Sampling**: Systematically vary the ratio of random to BM25 negative samples and analyze how different sampling strategies affect performance across domains.