---
ver: rpa2
title: Offline Imitation Learning with Variational Counterfactual Reasoning
arxiv_id: '2310.04706'
source_url: https://arxiv.org/abs/2310.04706
tags:
- data
- learning
- counterfactual
- expert
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OILCA, an offline imitation learning framework
  that addresses the challenge of learning optimal expert behavior policies from limited
  expert data and large amounts of unlabeled data without online environment interactions.
  The key idea is to generate counterfactual expert data using variational counterfactual
  reasoning to augment the scarce expert data, which helps remove spurious features
  that bias the agent and hinder generalization.
---

# Offline Imitation Learning with Variational Counterfactual Reasoning

## Quick Facts
- **arXiv ID:** 2310.04706
- **Source URL:** https://arxiv.org/abs/2310.04706
- **Reference count:** 40
- **Primary result:** OILCA achieves 116.05 average return on Cheetah Run task, outperforming the second-best method by 49.18

## Executive Summary
This paper proposes OILCA, an offline imitation learning framework that addresses the challenge of learning optimal expert behavior policies from limited expert data and large amounts of unlabeled data without online environment interactions. The key idea is to generate counterfactual expert data using variational counterfactual reasoning to augment the scarce expert data, which helps remove spurious features that bias the agent and hinder generalization. Theoretical analysis shows that the augmented counterfactual expert data improves the generalization ability of the learned policy. Experiments on DeepMind Control Suite benchmark for in-distribution robustness and CausalWorld benchmark for out-of-distribution generalization demonstrate that OILCA significantly outperforms various baselines.

## Method Summary
OILCA combines an identifiable variational autoencoder with a discriminator-weighted behavior cloning method to perform offline imitation learning. The method generates counterfactual expert data by inferring an exogenous variable using the VAE and performing do-interventions to create new state-action pairs. This augmented data is then used with a DWBC method to train the policy, allowing it to learn invariant causal mechanisms that improve generalization to new distributions.

## Key Results
- OILCA achieves 116.05 average return on Cheetah Run task, outperforming the second-best method by 49.18
- Significantly outperforms various baselines on both in-distribution (DeepMind Control Suite) and out-of-distribution (CausalWorld) benchmarks
- Counterfactual data augmentation makes the offline imitation learning policy more robust to data distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual data augmentation via identifiable VAE removes spurious correlations between states and actions, improving generalization.
- **Mechanism:** The method introduces an unobserved exogenous variable into the SCM. By inferring its posterior and performing do-interventions, it generates counterfactual states and actions that break the dependency on idiosyncratic environmental features.
- **Core assumption:** The exogenous variable follows a conditional exponential family prior (Assumption 2), and the transition mechanism is invariant across different auxiliary variables (Assumption 1c).
- **Evidence anchors:**
  - [abstract] "We leverage identifiable variational autoencoder to generate counterfactual samples for expert data augmentation."
  - [section 3.1] "We leverage an identifiable generative model to generate the counterfactual expert data, thus enhancing the agent's capabilities of robustness and generalization in test environments."
  - [corpus] Weak. The corpus neighbors discuss offline IL and counterfactual reasoning but do not specifically validate the VAE identifiability claim. This remains theoretical.
- **Break condition:** If the generative model fails to learn the true posterior of the exogenous variable (Theorem 2 fails), counterfactual samples will not be identifiable, and spurious correlations persist.

### Mechanism 2
- **Claim:** Augmented expert data improves the error bound on the learned policy's generalization.
- **Mechanism:** Theorem 4 shows that increasing the number of expert data samples (original + counterfactual) reduces the expected generalization error bound. The counterfactual samples effectively multiply the expert dataset size.
- **Core assumption:** Finite state and action spaces, and each state is sampled uniformly.
- **Evidence anchors:**
  - [abstract] "Theoretical analysis shows that the augmented counterfactual expert data improves the generalization ability of the learned policy."
  - [section 3.3] "if ∣DE∣≥h∣S∣∣A∣log(∣S∣/δ)/ϵ2 and each state st is sampled uniformly, then, with probability at least 1−δ, we have: max_st ∥π(⋅∣st)−πω(⋅∣st)∥1 ≤ϵ."
  - [corpus] Missing. No corpus neighbor explicitly discusses error bounds in offline IL generalization.
- **Break condition:** If the offline dataset does not cover the state space well (non-uniform sampling), the bound becomes invalid, and generalization does not improve.

### Mechanism 3
- **Claim:** Counterfactual reasoning enables out-of-distribution generalization by learning invariant causal mechanisms.
- **Mechanism:** By intervening on the exogenous variable, the model learns state-action relationships that are invariant to environmental changes, allowing it to generalize to new distributions (e.g., different friction, colors in CausalWorld).
- **Core assumption:** The causal structure (SCM) underlying the data remains the same across training and test environments; only the exogenous variable distribution changes.
- **Evidence anchors:**
  - [abstract] "Experiments... demonstrate that OILCA significantly outperforms various baselines... on... CAUSALWORLD benchmark for out-of-distribution generalization."
  - [section 4.3] "To evaluate the out-of-distribution generalization of OILCA, we use a benchmark namedCAUSALWORLD..."
  - [corpus] Weak. Corpus neighbors discuss offline IL and robust RL but do not validate the specific causal invariance claim for OOD generalization.
- **Break condition:** If the underlying causal structure changes between training and test environments (e.g., new causal links), counterfactual reasoning cannot recover the correct invariant mechanism, and OOD performance degrades.

## Foundational Learning

- **Concept:** Structural Causal Models (SCM)
  - Why needed here: Provides the formalism to encode causal relationships between states, actions, and unobserved exogenous variables, enabling counterfactual reasoning.
  - Quick check question: In the SCM representation, what role does the exogenous variable u play in the transition from st to st+1?
- **Concept:** Variational Autoencoder (VAE) and Identifiability
  - Why needed here: VAE learns the latent representation of the exogenous variable, and identifiability ensures that the learned latent factors correspond to the true causal factors up to permutation.
  - Quick check question: Why does OILCA use a conditional prior p(u|c) instead of an unconditional prior?
- **Concept:** Counterfactual Inference
  - Why needed here: Allows answering "what if" questions about expert actions under different states, generating counterfactual expert data to augment the training set.
  - Quick check question: What are the three steps of counterfactual inference as defined in the paper?

## Architecture Onboarding

- **Component map:** Offline data -> Conditional VAE (infer u) -> Counterfactual data generation -> DWBC with augmented data -> Policy
- **Critical path:** Collect offline data → Train conditional VAE (infer u) → Generate counterfactual expert data → Train DWBC with augmented data → Evaluate policy
- **Design tradeoffs:**
  - **VAE complexity vs. identifiability:** More complex models may improve reconstruction but hurt identifiability (Theorem 1 conditions).
  - **Augmentation size vs. overfitting:** Too many counterfactual samples may cause overfitting to synthetic data.
  - **DWBC vs. other IL methods:** DWBC is chosen for its ability to leverage unlabeled data, but may be less stable than BC-exp in some tasks.
- **Failure signatures:**
  - **Poor reconstruction:** If VAE fails to reconstruct next states, counterfactual states will be unrealistic.
  - **Discriminator collapse:** If the discriminator overfits to the limited expert data, the policy will not learn from unlabeled data effectively.
  - **Overfitting to augmentation:** If the policy memorizes counterfactual samples instead of learning invariant features, OOD generalization will suffer.
- **First 3 experiments:**
  1. **Toy environment ablation:** Run OILCA on the 2D navigation toy environment and visualize the learned posterior of the exogenous variable (compare to ground truth). Check if identifiability holds.
  2. **Counterfactual data ablation:** Train OILCA with varying percentages of counterfactual expert data (e.g., 10%, 50%, 100% of |DU|) and measure in-distribution performance. Verify Theorem 4's claim.
  3. **OOD generalization test:** Train on space A of CausalWorld, test on space B, and compare to baselines. Check if counterfactual augmentation consistently improves performance across tasks.

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the theoretical guarantees for why counterfactual data augmentation improves policy generalization more than other augmentation methods?
  - Basis in paper: [explicit] The paper only provides theoretical guarantees on the number of counterfactual samples improving generalization, not on why this method is superior to other augmentation approaches.
  - Why unresolved: The paper does not provide a theoretical comparison or analysis of counterfactual data augmentation versus other data augmentation techniques in terms of their impact on policy generalization.
  - What evidence would resolve it: A theoretical analysis or empirical comparison demonstrating the superiority of counterfactual data augmentation over other methods in improving policy generalization.

- **Open Question 2:** How does the performance of OILCA compare to other state-of-the-art offline imitation learning methods in terms of sample efficiency and computational cost?
  - Basis in paper: [inferred] The paper does not provide a direct comparison of OILCA with other methods in terms of sample efficiency or computational cost.
  - Why unresolved: The paper focuses on the effectiveness of OILCA in terms of performance metrics but does not address its efficiency compared to other methods.
  - What evidence would resolve it: A comparative analysis of OILCA and other methods in terms of sample efficiency and computational cost.

- **Open Question 3:** How sensitive is OILCA to the choice of hyperparameters, such as the number of augmented samples or the weight factors in the learning objectives?
  - Basis in paper: [inferred] The paper does not provide a sensitivity analysis of OILCA to its hyperparameters.
  - Why unresolved: The paper does not explore the impact of different hyperparameter settings on the performance of OILCA.
  - What evidence would resolve it: A sensitivity analysis showing the impact of different hyperparameter settings on OILCA's performance.

## Limitations
- The identifiability of the variational autoencoder under finite data remains unproven in practice, though Theorem 1 provides theoretical conditions
- The counterfactual data augmentation mechanism relies on the assumption that exogenous variables capture all spurious correlations, which may not hold in complex environments
- The theoretical error bounds assume uniform state sampling, which rarely occurs in real offline datasets

## Confidence
- **High confidence:** OILCA's ability to outperform baselines on in-distribution tasks (DeepMind Control Suite) is well-supported by experimental results
- **Medium confidence:** The counterfactual reasoning mechanism improves OOD generalization, but results are limited to the CausalWorld benchmark
- **Medium confidence:** Theoretical analysis showing improved generalization bounds, though assumptions may be restrictive in practice

## Next Checks
1. **Ablation study on VAE identifiability:** Test OILCA with non-identifiable VAEs to quantify the importance of identifiability for performance
2. **Distribution mismatch analysis:** Systematically vary the distribution shift between training and test environments to map OILCA's OOD generalization limits
3. **Sample efficiency validation:** Measure OILCA's performance with varying amounts of expert data to validate the theoretical claims about data efficiency improvements