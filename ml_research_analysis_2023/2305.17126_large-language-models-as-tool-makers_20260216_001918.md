---
ver: rpa2
title: Large Language Models as Tool Makers
arxiv_id: '2305.17126'
source_url: https://arxiv.org/abs/2305.17126
tags:
- tool
- task
- tasks
- tools
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models (LLMs) can create and use their own reusable
  tools to solve tasks, mimicking human evolution. The proposed LATM framework splits
  the process into two phases: tool making, where a powerful LLM generates reusable
  Python functions for tasks, and tool using, where a lightweight LLM uses these tools
  to solve instances.'
---

# Large Language Models as Tool Makers

## Quick Facts
- arXiv ID: 2305.17126
- Source URL: https://arxiv.org/abs/2305.17126
- Authors: 
- Reference count: 17
- Large language models (LLMs) can create and use their own reusable tools to solve tasks, mimicking human evolution

## Executive Summary
This paper introduces LATM (LLMs as Tool Makers), a framework that enables LLMs to create and use reusable tools for task solving. The framework splits the process into two phases: tool making, where a powerful LLM (like GPT-4) generates reusable Python functions, and tool using, where a lightweight LLM (like GPT-3.5 Turbo) applies these tools to solve instances. This approach significantly reduces costs while maintaining performance comparable to using the powerful LLM alone.

## Method Summary
LATM is a framework that enables LLMs to create and use reusable tools to solve tasks. It consists of two main phases: tool making, where a powerful LLM (e.g., GPT-4) generates reusable Python functions for tasks, and tool using, where a lightweight LLM (e.g., GPT-3.5) uses these tools to solve instances. The tool-making stage is performed once for each task, and the resulting tools are reused for all instances of that task. A dispatcher model routes incoming tasks to existing tools or triggers tool creation for new tasks.

## Key Results
- LATM achieves performance equivalent to using GPT-4 for both tool making and tool using, while significantly reducing inference costs
- The framework demonstrates effectiveness on various reasoning tasks, including Big-Bench tasks
- Performance parity with GPT-4 is achieved with significantly lower inference costs when using GPT-4 for tool making and GPT-3.5 for tool using

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LATM achieves performance parity with GPT-4 while reducing inference costs by splitting the problem-solving process into tool making and tool using phases.
- Mechanism: The tool maker (GPT-4) generates reusable Python functions once per task type, then the tool user (GPT-3.5 Turbo) applies these functions to multiple instances, amortizing the cost of tool creation.
- Core assumption: The tool maker can generate general Python functions that correctly solve all instances of a task type.
- Evidence anchors:
  - [abstract] "LATM demonstrates performance equivalent to using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced."
  - [section 4.2] "The tool-making stage, including tool proposing, verification, and wrapping, only needs to be performed once for each type of task."
  - [corpus] Weak evidence - no direct citation about cost reduction in related work.
- Break condition: If the tool maker cannot generate a general function that works across all instances, or if tool verification fails frequently, the cost savings disappear.

### Mechanism 2
- Claim: LATM extends the conventional cache mechanism by storing reusable Python function tools instead of natural language responses.
- Mechanism: Tools generated by the tool maker are cached and reused across multiple task instances, reducing the need for repeated inference on similar problems.
- Core assumption: Task instances within a type share enough structure that a single tool can solve them all.
- Evidence anchors:
  - [abstract] "LATM offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs."
  - [section 4.3] "The tool-making stage, including tool proposing, verification, and wrapping, only needs to be performed once for each type of task. The resulting tools can then be reused for all instances of that task."
  - [corpus] Weak evidence - no direct citation about caching mechanisms in related work.
- Break condition: If task instances vary too much within a type, the cached tools become insufficient and new tools must be generated frequently.

### Mechanism 3
- Claim: The dispatcher model enables LATM to handle streaming data with a mixture of tasks by routing instances to existing tools or requesting new tool creation.
- Mechanism: A lightweight dispatcher (GPT-3.5 Turbo) classifies incoming tasks, either routing them to the appropriate cached tool or triggering the tool maker for new tasks.
- Core assumption: The dispatcher can accurately classify tasks and determine when new tools are needed.
- Evidence anchors:
  - [section 3.2] "We introduce a third LLM, the dispatcher, which determines whether an incoming problem can be solved using existing tools or if a new tool needs to be created."
  - [section 4.4] "The results demonstrate that the dispatcher can effectively identify existing tools and request tool-making for unseen tasks without a significant performance drop."
  - [corpus] Weak evidence - no direct citation about dispatcher mechanisms in related work.
- Break condition: If the dispatcher misclassifies tasks or fails to recognize when new tools are needed, LATM performance degrades significantly.

## Foundational Learning

- Concept: Tool-making vs. tool-using capabilities
  - Why needed here: LATM relies on assigning different capabilities to different models - sophisticated tool-making to GPT-4 and simpler tool-using to GPT-3.5 Turbo.
  - Quick check question: Can you explain why GPT-4 is better suited for tool-making than GPT-3.5 Turbo based on the paper's results?

- Concept: In-context learning for tool usage
  - Why needed here: The tool user applies the generated tools using in-context learning from the demonstrations provided in the wrapped tool.
  - Quick check question: How does the tool user convert a natural language question into a function call using the provided demonstrations?

- Concept: Python function generation and verification
  - Why needed here: The tool maker must generate executable Python functions and verify their correctness across multiple validation samples.
  - Quick check question: What happens if the generated tool fails the verification step, and how does the tool maker attempt to fix it?

## Architecture Onboarding

- Component map:
  - Tool Maker: GPT-4 (or other powerful model) that generates Python functions
  - Tool User: GPT-3.5 Turbo (or other lightweight model) that applies the tools
  - Dispatcher: GPT-3.5 Turbo that routes incoming tasks
  - Tool Cache: Storage for generated Python functions
  - API Interface: Wrapper around function calls

- Critical path: Incoming task → Dispatcher classification → Tool lookup → Tool User execution → Result
  Alternative path: Incoming task → Dispatcher → Tool Maker → Tool verification → Tool cache → Tool User

- Design tradeoffs:
  - Powerful tool maker vs. cost: Using GPT-4 for tool-making provides better quality but increases upfront costs
  - Tool granularity: Coarser tools (fewer, more general) reduce cache size but may be less efficient; finer tools (more specific) improve performance but increase cache management complexity
  - Dispatcher accuracy vs. simplicity: More sophisticated dispatchers improve routing but add complexity and cost

- Failure signatures:
  - Tool verification failures: Tool maker generates functions that don't pass unit tests
  - Dispatcher misclassification: Tasks routed to wrong tools or tool maker unnecessarily called
  - Tool user inability to parse function calls: Tool user fails to convert natural language to correct function calls
  - Cache misses: Tool cache doesn't contain needed tools, forcing repeated tool creation

- First 3 experiments:
  1. Run LATM on a single task type (e.g., Word Sorting) to verify tool creation and usage pipeline
  2. Test dispatcher accuracy by mixing multiple task types and measuring correct routing
  3. Compare cost and performance of LATM vs. direct GPT-4 inference on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between tool-making and tool-using model sizes for different task complexities?
- Basis in paper: [inferred] The paper explores using different model sizes for tool-making (GPT-4) and tool-using (GPT-3.5 Turbo), but doesn't systematically study the optimal balance for various task complexities.
- Why unresolved: The paper only uses GPT-4 for tool-making and GPT-3.5 Turbo for tool-using. A comprehensive study varying model sizes for different task complexities is missing.
- What evidence would resolve it: Systematic experiments varying tool-maker and tool-user model sizes across tasks of different complexities, measuring both performance and cost-effectiveness.

### Open Question 2
- Question: How does the quality of generated tools degrade over time as the tool-maker model ages compared to newer models?
- Basis in paper: [inferred] The paper doesn't address how the quality of tools generated by an older model compares to those from a newer, more advanced model.
- Why unresolved: The study only uses GPT-4 as the tool-maker without comparing it to potentially newer or more advanced models.
- What evidence would resolve it: Comparative study generating tools with different versions of models (e.g., GPT-3.5 vs GPT-4 vs GPT-4 Turbo) and measuring their effectiveness over time.

### Open Question 3
- Question: What is the impact of tool-maker's temperature setting on tool quality and diversity?
- Basis in paper: [explicit] The paper mentions using temperature=0.3 for tool-making but doesn't explore its impact on tool quality and diversity.
- Why unresolved: The paper uses a fixed temperature setting without exploring how different temperature values affect the generated tools.
- What evidence would resolve it: Experiments varying temperature settings for tool-making across different tasks, measuring tool quality, success rate, and diversity of solutions.

### Open Question 4
- Question: How does the number of demonstrations affect tool-making success rate and tool quality?
- Basis in paper: [explicit] The paper uses 3 demonstrations for tool-making but doesn't explore how varying this number affects outcomes.
- Why unresolved: The paper fixes the number of demonstrations at 3 without investigating how this choice impacts tool-making success.
- What evidence would resolve it: Systematic experiments varying the number of demonstrations (e.g., 1, 2, 3, 5, 10) and measuring tool-making success rate and tool quality.

### Open Question 5
- Question: What are the limitations of LATM when applied to tasks requiring continuous learning or adaptation?
- Basis in paper: [inferred] The paper doesn't explore how LATM handles tasks that require continuous learning or adaptation to new patterns.
- Why unresolved: The paper focuses on discrete task categories without addressing continuous learning scenarios.
- What evidence would resolve it: Experiments testing LATM on tasks with gradually changing patterns or requirements, measuring performance degradation over time.

## Limitations
- Evaluation scope is limited to only 7 task types from Big-Bench and 3 additional tasks, which may not represent real-world diversity
- Lacks detailed analysis of failure cases, making it difficult to assess robustness of tool generation and verification
- Cost-benefit analysis is incomplete, not accounting for operational overhead of maintaining tool cache and dispatcher accuracy

## Confidence

- **High Confidence**: The basic LATM framework design and the cost reduction mechanism (using GPT-4 for tool making and GPT-3.5 for tool using) are well-supported by the experimental results. The claim that tool-making only needs to be performed once per task type is well-demonstrated.

- **Medium Confidence**: The dispatcher's ability to handle streaming data and route tasks appropriately is supported but could benefit from more extensive testing across diverse task mixes. The claim of performance parity with GPT-4 is demonstrated but on a limited task set.

- **Low Confidence**: The generalizability of LATM to truly open-ended, diverse real-world applications remains unproven. The long-term effectiveness of cached tools and the framework's behavior under high failure rates are not well-characterized.

## Next Checks
1. **Stress Test Tool Verification**: Systematically test tool maker failures by deliberately providing malformed or ambiguous task descriptions to quantify how often tool generation fails and measure the overhead of retry mechanisms.

2. **Extended Task Diversity Evaluation**: Evaluate LATM on a significantly larger and more diverse set of tasks (at least 20+ task types from different domains) to assess generalizability beyond the current limited evaluation set.

3. **Long-term Cache Performance Analysis**: Implement a time-based evaluation where tools are reused across hundreds of instances to measure cache hit rates, tool quality degradation over time, and the frequency of necessary tool regeneration.