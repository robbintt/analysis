---
ver: rpa2
title: 'Synthetic Data Generation with Large Language Models for Text Classification:
  Potential and Limitations'
arxiv_id: '2310.07849'
source_url: https://arxiv.org/abs/2310.07849
tags:
- data
- synthetic
- task
- subjectivity
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the effectiveness of LLM-generated synthetic
  data for text classification, focusing on how task subjectivity impacts model performance.
  Using GPT-3.5-Turbo, synthetic data is generated under zero-shot and few-shot settings
  for 10 classification tasks ranging from news topic to sarcasm detection.
---

# Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations

## Quick Facts
- arXiv ID: 2310.07849
- Source URL: https://arxiv.org/abs/2310.07849
- Reference count: 20
- The study finds that models trained on real-world data consistently outperform those trained on synthetic data, with the performance gap widening for more subjective tasks.

## Executive Summary
This study investigates the effectiveness of LLM-generated synthetic data for text classification tasks, focusing on how task subjectivity impacts model performance. Using GPT-3.5-Turbo, synthetic data is generated under zero-shot and few-shot settings for 10 classification tasks ranging from news topic to sarcasm detection. The research reveals that while LLMs can generate useful synthetic data for objective tasks, their performance degrades significantly for subjective classification tasks. Few-shot generation with real examples improves both performance and data diversity. The findings highlight both the potential and limitations of using LLMs for synthetic data generation in text classification.

## Method Summary
The study employs GPT-3.5-Turbo to generate synthetic data for 10 text classification datasets, using both zero-shot (direct prompting) and few-shot (using 10 real examples per class) settings. Models trained include BERT and RoBERTa with standard hyperparameters (learning rate 5e-5, batch size 64). The research evaluates performance using Macro-F1 and Accuracy metrics, comparing models trained on real data versus synthetic data across different subjectivity levels. Instance-level subjectivity is measured through inter-annotator agreement, and data diversity is assessed through variety of generated examples.

## Key Results
- Models trained on real-world data consistently outperform those trained on synthetic data across all tasks
- Few-shot synthetic data generation shows improved performance and data diversity compared to zero-shot generation
- Performance gaps between real and synthetic data widen significantly for more subjective classification tasks
- Instance-level analysis reveals better performance on less subjective instances within the same task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate high-quality synthetic data for objective classification tasks but struggle with subjective ones due to limited coverage of nuanced language.
- Mechanism: Objective tasks like news topic classification have clear, identifiable features (e.g., keywords), which LLMs can easily replicate. Subjective tasks like humor detection require understanding complex human emotions and contexts that synthetic data generation may not fully capture.
- Core assumption: Task objectivity correlates with the ease of generating representative synthetic data.
- Evidence anchors:
  - [abstract]: "Models trained on real-world data consistently outperform those trained on synthetic data, with the performance gap widening for more subjective tasks."
  - [section 4.1]: "We experiment with 10 representative datasets covering a variety of text classification tasks... These datasets are selected with the goal of spanning a wide range of task subjectivity in mind."
- Break condition: If task subjectivity cannot be accurately measured or if LLMs improve in capturing nuanced language patterns.

### Mechanism 2
- Claim: Providing real-world examples to guide LLM synthetic data generation improves data diversity and model performance.
- Mechanism: Few-shot synthetic data generation uses real examples to guide the LLM, reducing the likelihood of generating rephrased or low-diversity synthetic instances.
- Core assumption: Real examples provide necessary patterns for LLM to mimic, enhancing synthetic data quality.
- Evidence anchors:
  - [abstract]: "Few-shot synthetic data, guided by real examples, shows improved performance and data diversity."
  - [section 4.5]: "We find that in general, the real-world data appear to be more diverse than the synthetic data generated under the few-shot settings, which in turn seem to be more diverse than the zero-shot synthetic data."
- Break condition: If the provided real examples are not representative or if the LLM fails to learn from them effectively.

### Mechanism 3
- Claim: Model performance on synthetic data varies with instance-level subjectivity within the same task.
- Mechanism: Instances with higher inter-annotator agreement (lower subjectivity) are easier for models to classify correctly, even when trained on synthetic data.
- Core assumption: Instance subjectivity can be proxied by inter-annotator agreement.
- Evidence anchors:
  - [abstract]: "Instance-level analysis reveals that models trained on synthetic data perform better on less subjective instances within the same task."
  - [section 5.2]: "For most tasks... we observe a strong monotonically increasing relationship between γ and the model accuracy."
- Break condition: If the proxy for instance subjectivity is not accurate or if model performance does not correlate with instance subjectivity.

## Foundational Learning

- Concept: Task subjectivity
  - Why needed here: Understanding how task subjectivity affects the effectiveness of synthetic data generation is central to the study.
  - Quick check question: How would you define and measure the subjectivity of a text classification task?

- Concept: Inter-annotator agreement
  - Why needed here: Used as a proxy for instance-level subjectivity in the study.
  - Quick check question: What are some methods to measure inter-annotator agreement in text classification tasks?

- Concept: Synthetic data generation with LLMs
  - Why needed here: The study investigates the effectiveness of LLM-generated synthetic data for text classification.
  - Quick check question: What are the differences between zero-shot and few-shot synthetic data generation with LLMs?

## Architecture Onboarding

- Component map: GPT-3.5-Turbo -> Synthetic Data Generation -> BERT/RoBERTa Encoder -> Classification Model
- Critical path: Data generation → Model training → Performance evaluation → Analysis of results
- Design tradeoffs: Using real-world examples to guide synthetic data generation improves diversity and performance but requires access to a small amount of real data
- Failure signatures: Low synthetic data diversity, high instance-level subjectivity, and poor model performance on subjective tasks
- First 3 experiments:
  1. Compare model performance on synthetic vs. real data for objective and subjective tasks
  2. Evaluate the impact of providing real examples on synthetic data diversity and model performance
  3. Analyze the relationship between instance-level subjectivity and model performance on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of synthetic data generation vary across different types of LLMs beyond GPT-3.5-Turbo?
- Basis in paper: The authors note that their findings are based on the GPT-3.5-Turbo model and acknowledge that conclusions may not generalize to other LLMs.
- Why unresolved: The paper does not compare the effectiveness of synthetic data generation across different LLM architectures or sizes.
- What evidence would resolve it: A systematic comparison of synthetic data quality and model performance across multiple LLM types and sizes.

### Open Question 2
- Question: What is the optimal amount of real-world data needed to guide synthetic data generation for maximum effectiveness?
- Basis in paper: The authors use 10% of real-world data as examples in few-shot settings but don't explore different proportions.
- Why unresolved: The study uses a fixed proportion of real-world data and doesn't investigate how different amounts affect synthetic data quality and model performance.
- What evidence would resolve it: Experiments varying the proportion of real-world data used as examples in few-shot settings.

### Open Question 3
- Question: How do different diversity prompts affect the quality and variety of generated synthetic data?
- Basis in paper: The authors use a single diversity prompt ("Can you provide something more diverse compared to the previously generated data?") but don't explore alternatives.
- Why unresolved: The study uses one generic diversity prompt without investigating how different formulations might impact data generation.
- What evidence would resolve it: Comparison of synthetic data quality using various diversity prompts and formulations.

## Limitations
- Dataset representativeness may not fully capture real-world text classification diversity
- Findings are specific to GPT-3.5-Turbo and may not generalize to other LLM architectures
- Inter-annotator agreement as a proxy for subjectivity has measurement limitations

## Confidence
**High Confidence Claims**:
- Models trained on real data consistently outperform synthetic data models across all tasks
- Few-shot generation improves both performance and data diversity compared to zero-shot
- Performance gaps widen for more subjective tasks

**Medium Confidence Claims**:
- Instance-level subjectivity correlates with model performance on synthetic data
- Task objectivity can be reliably measured and impacts synthetic data quality
- The 10 selected datasets adequately represent the spectrum of classification subjectivity

**Low Confidence Claims**:
- Specific numerical thresholds for when synthetic data becomes ineffective
- Generalization of findings to non-classification tasks
- Extrapolation of results to LLMs beyond GPT-3.5-Turbo

## Next Checks
1. **Cross-LLM Validation**: Replicate the study using different LLM architectures (e.g., Claude, LLaMA) to verify if findings hold across model families and identify any model-specific patterns in synthetic data generation quality.

2. **Real-World Deployment Test**: Apply the methodology to a live text classification problem from an industry partner, measuring not just accuracy but also operational metrics like inference time and cost-effectiveness compared to traditional data collection approaches.

3. **Dynamic Subjectivity Analysis**: Conduct a longitudinal study tracking how instance-level subjectivity measurements change over time as language evolves, particularly for social media datasets where terminology and context shift rapidly.