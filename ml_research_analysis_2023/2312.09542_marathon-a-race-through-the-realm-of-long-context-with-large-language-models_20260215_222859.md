---
ver: rpa2
title: 'Marathon: A Race Through the Realm of Long Context with Large Language Models'
arxiv_id: '2312.09542'
source_url: https://arxiv.org/abs/2312.09542
tags:
- large
- language
- context
- long
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Marathon, a new benchmark designed to evaluate
  the long-context comprehension abilities of large language models (LLMs). It addresses
  the limitations of existing benchmarks that rely on F1 metrics, which can inaccurately
  score responses.
---

# Marathon: A Race Through the Realm of Long Context with Large Language Models

## Quick Facts
- arXiv ID: 2312.09542
- Source URL: https://arxiv.org/abs/2312.09542
- Reference count: 10
- Key outcome: Marathon benchmark reveals most LLMs struggle with long-context comprehension despite compression and retrieval optimizations

## Executive Summary
Marathon is a new benchmark designed to evaluate long-context comprehension abilities of large language models. It addresses limitations of existing F1-based benchmarks by adopting a multiple-choice question format that provides objective, deterministic scoring. The benchmark includes 6 tasks with context lengths ranging from 60k to 260k+ tokens. Evaluations reveal that most state-of-the-art LLMs, including both 7B and 70B parameter models, still struggle with long-context understanding, with the 7B Mistral model surprisingly outperforming the 70B Beluga model. The study also evaluates compression and retrieval optimization strategies, finding variable effectiveness across different models and tasks.

## Method Summary
The Marathon benchmark uses a multiple-choice format with context lengths from 60k to 260k+ tokens across 6 tasks. Distractor options are generated using GPT-4 by dividing contexts into fragments. Models are evaluated using various optimization strategies including Vanilla generation, LongLLMLingua compression (rate 0.5, ratio 0.4), OpenAI Embedding Retrieval, and Jina Embedding Retrieval. The evaluation requires models to respond in JSON format, with accuracy measured as the percentage of correct answer selections. Manual verification ensures benchmark quality, and evaluations are conducted on models ranging from 7B to 70B parameters.

## Key Results
- 7B Mistral model outperformed 70B Beluga model in long-context understanding across multiple tasks
- Most LLMs showed poor JSON response compliance, with only LongChat (29%) and ChatGPT (12%) achieving any compliance
- OpenAI Embedding Retrieval improved Mistral accuracy by 10.37% but decreased ChatGLM3 accuracy by 4.06%
- Compression methods showed variable effectiveness, improving some models while degrading others
- Timeline Reorder and Computation tasks proved particularly challenging for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple-choice format avoids F1-score inaccuracies by eliminating subjective scoring of generated text
- Mechanism: Multiple-choice questions provide objective, deterministic scoring since the model selects from predefined options, removing the need to compare generated text to reference answers
- Core assumption: Distractor options are well-designed to prevent guessing and require true understanding of the long context
- Evidence anchors: Abstract states multiple-choice provides "rapid, precise, and unbiased appraisal"; section 1 describes GPT-4-generated distractors from context fragments

### Mechanism 2
- Claim: Compression methods can improve long-context understanding by reducing irrelevant information and focusing the model's attention
- Mechanism: By compressing long contexts, models can process the most important information more efficiently, reducing memory constraints and improving comprehension
- Core assumption: Compression method preserves key information needed to answer questions correctly
- Evidence anchors: Abstract mentions compression methods found effective in certain cases; section 4.2 evaluates LongLLMLingua with specific compression parameters

### Mechanism 3
- Claim: Retrieval-Augmented Generation improves long-context understanding by extracting relevant information before model processing
- Mechanism: RAG methods retrieve most relevant passages from long contexts, reducing text the model needs to process while maintaining information completeness
- Core assumption: Retrieval method effectively identifies and extracts all relevant information for answering questions
- Evidence anchors: Section 4.2 evaluates OpenAI and Jina Embedding Retrieval methods; section 4.3.1 shows OpenAI Retrieval improved Mistral by 10.37% but decreased ChatGLM3 by 4.06%

## Foundational Learning

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: Understanding why traditional attention mechanisms struggle with long contexts and how optimizations like Flash Attention help
  - Quick check question: Why does the computational complexity of traditional attention scale quadratically with sequence length?

- Concept: Embedding-based retrieval methods
  - Why needed here: To understand how RAG methods work and why different embedding models might perform differently
  - Quick check question: What is the key difference between OpenAI Embedding and Jina Embedding approaches in the context of long-context retrieval?

- Concept: Instruction following in language models
  - Why needed here: To understand why models might fail to respond in specified JSON format despite being asked to
  - Quick check question: What factors might cause a language model to ignore explicit formatting instructions in a prompt?

## Architecture Onboarding

- Component map: Context → Question generation → Distractor generation → Manual verification → Model prediction → JSON parsing → Accuracy calculation
- Critical path: Context → Question + Options → Model prediction → JSON parsing → Accuracy calculation
- Design tradeoffs: Multiple-choice vs. free-response (objective scoring vs. natural response generation); context length vs. computational cost (comprehensive evaluation vs. resource requirements); manual verification vs. automation (quality vs. scalability)
- Failure signatures: Low JSON compliance rates (most models at 0%, only LongChat and ChatGPT compliant); inconsistent performance across tasks; compression methods that decrease accuracy despite reducing context length
- First 3 experiments: 1) Test instruction-following capability in isolation with simple JSON response prompts; 2) Compare accuracy of different compression rates (0.3, 0.5, 0.7) on task subsets; 3) Evaluate retrieval methods with varying top-k values (1, 2, 3) to determine impact on accuracy

## Open Questions the Paper Calls Out

Open Question 1
- Question: How do different positions of key information in long contexts affect LLM performance in Marathon?
- Basis in paper: [inferred] Paper mentions position of key information greatly affects models' ability to correctly understand and process text
- Why unresolved: Paper does not provide specific experiments or analysis on how position impacts performance
- What evidence would resolve it: Experiments with varying positions of key information and analysis of model performance for each position

Open Question 2
- Question: How does context length affect LLM performance in Marathon?
- Basis in paper: [explicit] Context lengths range from 60k to 260k+
- Why unresolved: Paper lacks detailed analysis of how context length affects performance
- What evidence would resolve it: Experiments with varying context lengths and analysis of model performance for each length

Open Question 3
- Question: How does choice of embedding model affect LLM performance in Marathon?
- Basis in paper: [explicit] Jina Embedding Retrieval outperforms OpenAI Embedding Retrieval for most models
- Why unresolved: Paper does not analyze why Jina Embedding performs better
- What evidence would resolve it: Experiments with different embedding models and analysis of model performance for each model

## Limitations
- Low JSON response compliance across most models (0% for many models) raises questions about instruction-following versus comprehension
- Focus on 7B and 70B parameter models may not generalize to other model families
- Benchmark tasks may not fully represent real-world long-context scenarios
- Variable effectiveness of compression methods suggests context-dependent rather than universal benefits

## Confidence
- High Confidence: 7B Mistral outperforming 70B Beluga in long-context understanding; most models struggling with long-context comprehension
- Medium Confidence: Effectiveness of compression and retrieval methods shows promise but with inconsistent results across models and tasks
- Low Confidence: Claim that multiple-choice format provides "rapid, precise, and unbiased appraisal" based on theoretical reasoning rather than empirical validation

## Next Checks
1. **Instruction-Following Validation**: Conduct controlled experiment isolating instruction-following capability by testing models' ability to respond in JSON format to simple, short-context questions before evaluating their long-context comprehension.

2. **Compression Method Generalization**: Test compression methods across broader range of model sizes and different domain types to determine whether improvements are model-specific or domain-dependent, varying compression rates systematically.

3. **Multiple-Choice Design Validation**: Compare multiple-choice accuracy against free-response evaluations on subset of tasks to empirically validate whether multiple-choice format provides equivalent or superior assessment of true comprehension versus format compliance.