---
ver: rpa2
title: 'Using Large Language Models to Automate Category and Trend Analysis of Scientific
  Articles: An Application in Ophthalmology'
arxiv_id: '2308.16688'
source_url: https://arxiv.org/abs/2308.16688
tags:
- classification
- articles
- categories
- learning
- bart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an automated framework for classifying and analyzing
  scientific articles in ophthalmology using Large Language Models (LLMs). The method
  leverages zero-shot learning (ZSL) with BART and fine-tunes BERT variants to categorize
  articles into 15 distinct categories without human intervention.
---

# Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology

## Quick Facts
- arXiv ID: 2308.16688
- Source URL: https://arxiv.org/abs/2308.16688
- Reference count: 23
- The framework achieves mean accuracy of 0.86 and mean F1 score of 0.85 for classifying ophthalmology articles into 15 categories.

## Executive Summary
This paper presents an automated framework for classifying and analyzing scientific articles in ophthalmology using Large Language Models (LLMs). The method leverages zero-shot learning with BART and fine-tunes BERT variants to categorize articles into 15 distinct categories without human intervention. The framework is evaluated on a manually annotated dataset (RenD) of 1000 ocular disease-related articles, achieving a mean accuracy of 0.86 and mean F1 score of 0.85. Additionally, trend analysis is performed to identify emerging research directions and technology adoption patterns over time. The approach demonstrates efficiency and accuracy, reducing manual effort in literature review and enabling scalable, interdisciplinary applications in biomedical research.

## Method Summary
The framework combines zero-shot learning with BART for initial classification, followed by fine-tuning BERT variants (distilBERT, SciBERT, PubmedBERT, BioBERT) for categories where zero-shot learning underperforms. Articles are retrieved from PubMed, preprocessed based on inclusion criteria, and classified using title and abstract information. The method achieves high accuracy through semantic mapping of article content to descriptive category labels, with trend analysis performed on the classified results to identify emerging research patterns over time.

## Key Results
- Mean accuracy of 0.86 and mean F1 score of 0.85 achieved on 15-category classification task
- Zero-shot BART successfully classifies articles without requiring labeled training data
- Trend analysis reveals emerging research directions and technology adoption patterns in ophthalmology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot learning with BART achieves high accuracy in classifying scientific articles without requiring labeled training data.
- Mechanism: BART leverages its pre-trained language understanding to map article titles and abstracts to semantic embeddings, then compares these embeddings to descriptive category labels to assign probabilities.
- Core assumption: Category labels can be described with sufficient detail to guide the model's classification decisions without explicit training examples.
- Evidence anchors:
  - [abstract] "We have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants"
  - [section] "BART has 12 transformer layers with hidden size of 1024 that was initially trained on Wikipedia and BookCorpus dataset and fine-tuned on Multi-Genre Natural Language Inference (MNLI) tasks"
  - [corpus] Weak - only one neighbor paper mentions ZSL with medical texts, no direct comparison
- Break condition: Category labels lack sufficient descriptive information or are too ambiguous for semantic mapping.

### Mechanism 2
- Claim: Fine-tuning BERT variants improves classification performance for categories where zero-shot learning underperforms.
- Mechanism: By training BERT and its variants on the specific dataset with labeled examples, the model learns domain-specific patterns and relationships between text features and categories.
- Core assumption: The dataset contains sufficient labeled examples to capture the underlying patterns in the text for accurate classification.
- Evidence anchors:
  - [abstract] "We fine-tuned the BERT and its variants BERTBASE, SciBERT, PubmedBERT, and BioBERT for those categories which showed not comprising results from ZSL model"
  - [section] "For the BioBERT model, we selected a learning rate of 1e-05, batch size of 8, max length of 400, and number of epochs of 20"
  - [corpus] Weak - corpus contains no direct evidence of BERT fine-tuning for medical text classification
- Break condition: Insufficient labeled data or category imbalance prevents effective fine-tuning.

### Mechanism 3
- Claim: Trend analysis based on classified articles reveals emerging research directions and technology adoption patterns over time.
- Mechanism: By analyzing publication dates and categories of classified articles, researchers can identify temporal patterns in research focus and technology usage.
- Core assumption: Publication trends reflect genuine shifts in research focus and technology adoption rather than artifacts of publication practices.
- Evidence anchors:
  - [abstract] "Additionally, trend analysis is performed to identify emerging research directions and technology adoption patterns over time"
  - [section] "We performed trend analysis based on the classified results, providing researchers insights into emerging trends in the field"
  - [corpus] Weak - corpus contains no direct evidence of trend analysis in medical literature
- Break condition: Publication data is incomplete or biased, making trend analysis unreliable.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Enables classification of articles into categories without requiring labeled training data for each category
  - Quick check question: How does zero-shot learning differ from traditional supervised learning in terms of training data requirements?

- Concept: Transformer architecture
  - Why needed here: BART and BERT models use transformer architecture to process and understand text
  - Quick check question: What are the key differences between encoder-only (BERT) and encoder-decoder (BART) transformer architectures?

- Concept: Trend analysis
  - Why needed here: Identifies emerging research directions and technology adoption patterns over time
  - Quick check question: What are the key steps in performing trend analysis on classified scientific articles?

## Architecture Onboarding

- Component map:
  - PubMed database retrieval using keywords -> Text cleaning and inclusion criteria filtering -> Zero-shot BART classification -> Fine-tuned BERT variants for underperforming categories -> Temporal and category-based trend analysis -> Reports with inclusion criteria and analysis results

- Critical path:
  1. Retrieve articles from PubMed using user-specified keywords
  2. Preprocess articles based on inclusion criteria
  3. Classify articles using zero-shot BART
  4. Fine-tune BERT variants for underperforming categories
  5. Perform trend analysis on classified results
  6. Generate output reports

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Zero-shot offers flexibility but may underperform; fine-tuning improves accuracy but requires labeled data
  - Abstract vs. title classification: Titles are faster to process but may be less accurate than abstracts
  - Multilabel vs. multiclass: Multilabel allows articles to belong to multiple categories but increases complexity

- Failure signatures:
  - Poor classification accuracy: May indicate insufficient descriptive category labels or inadequate training data
  - Inconsistent results between abstract and title classification: May suggest category ambiguity or insufficient text information
  - Trend analysis showing no clear patterns: May indicate insufficient temporal data or category imbalance

- First 3 experiments:
  1. Test zero-shot BART classification on a small subset of articles with known categories
  2. Compare classification accuracy using abstracts vs. titles as input
  3. Evaluate the impact of different threshold values for multilabel classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework change when integrating additional biomedical databases beyond PubMed, such as Google Scholar, IEEE Xplore, or Springer?
- Basis in paper: [explicit] The paper acknowledges that limiting the dataset to PubMed may exclude relevant articles and proposes future plans to integrate additional databases to ensure more comprehensive coverage.
- Why unresolved: The paper does not provide empirical results or comparative analysis of the framework's performance when using multiple databases, as this was identified as a future direction rather than a completed study.
- What evidence would resolve it: Empirical evaluation of the framework's classification accuracy, F1 scores, and trend analysis capabilities when incorporating articles from multiple databases, with comparison to the current PubMed-only approach.

### Open Question 2
- Question: What is the impact of class imbalance on the performance of the framework, particularly for the clinical studies sub-class, and how can this be mitigated?
- Basis in paper: [explicit] The paper notes that the lower performance in the clinical studies sub-class is likely due to class imbalance, which affects the model's ability to generalize and accurately predict instances of the minority class.
- Why unresolved: While the paper identifies class imbalance as a potential issue, it does not explore or implement specific techniques to address this problem, such as oversampling, undersampling, or using class weights.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various class imbalance mitigation techniques on the framework's performance, including improved accuracy and F1 scores for the clinical studies sub-class.

### Open Question 3
- Question: How do the proposed framework's trend analysis capabilities compare to those of human experts in identifying emerging research trends and technology adoption patterns in ophthalmology?
- Basis in paper: [explicit] The paper presents trend analysis results but does not compare them to human expert analysis or validate the framework's ability to accurately identify emerging trends.
- Why unresolved: The paper does not provide a comparison between the framework's trend analysis and human expert analysis, leaving uncertainty about the framework's accuracy and reliability in identifying research trends.
- What evidence would resolve it: A comparative study between the framework's trend analysis results and those of human experts, including metrics such as precision, recall, and F1 scores for identifying emerging trends and technology adoption patterns.

## Limitations

- The framework's reliance on abstract-only classification may miss nuanced category distinctions that require full-text analysis
- The evaluation dataset contains only 1000 articles, which may not fully represent the diversity of ophthalmic literature
- The zero-shot learning approach depends heavily on the quality and specificity of category label descriptions

## Confidence

**High Confidence:** The classification methodology and performance metrics are well-documented, with clear evaluation procedures using established benchmarks. The framework's ability to automate article categorization and reduce manual effort is supported by quantitative results (mean accuracy of 0.86, mean F1 score of 0.85).

**Medium Confidence:** The trend analysis component shows promise but relies on assumptions about publication patterns reflecting genuine research trends. The study demonstrates the framework's potential for identifying emerging research directions, but the interpretation of these trends requires further validation with domain experts.

**Low Confidence:** The generalizability of the framework to other biomedical fields beyond ophthalmology is not extensively tested. While the methodology suggests potential for interdisciplinary applications, specific validation in other domains would strengthen these claims.

## Next Checks

1. **Cross-field validation:** Test the framework's performance on scientific articles from other biomedical domains (e.g., cardiology, oncology) to assess generalizability and identify field-specific adaptation requirements.

2. **Full-text analysis comparison:** Evaluate classification accuracy using full-text articles versus abstracts alone for a subset of categories, particularly those requiring detailed methodological information.

3. **Expert validation study:** Conduct a blinded study with domain experts to validate the framework's trend analysis outputs, comparing automated trend identification against expert opinions on emerging research directions.