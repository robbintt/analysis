---
ver: rpa2
title: Better by you, better than me, chatgpt3 as writing assistance in students essays
arxiv_id: '2302.04536'
source_url: https://arxiv.org/abs/2302.04536
tags:
- writing
- group
- chatgpt
- students
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of ChatGPT-3 on student essay
  writing performance by comparing 18 students (9 in a control group and 9 in an experimental
  group using ChatGPT-3) on a forensic science essay task. Essay quality was evaluated
  using a rubric-based scoring system, and additional measures included writing time,
  text authenticity (via plagiarism detection), and content similarity (using Jaccard
  index).
---

# Better by you, better than me, chatgpt3 as writing assistance in students essays

## Quick Facts
- arXiv ID: 2302.04536
- Source URL: https://arxiv.org/abs/2302.04536
- Reference count: 30
- Primary result: ChatGPT-3 did not improve essay quality, writing speed, or authenticity compared to traditional writing

## Executive Summary
This study investigated the impact of ChatGPT-3 on student essay writing performance by comparing 18 students (9 in control group, 9 using ChatGPT-3) on a forensic science essay task. Essay quality was evaluated using a rubric-based scoring system, and additional measures included writing time, text authenticity (via plagiarism detection), and content similarity. The results showed no significant improvement in essay scores for the ChatGPT group compared to the control group, with both averaging a grade of C. The ChatGPT group also did not write faster, and their essays had slightly higher rates of non-authentic content (11.87% vs. 9.96%). Content similarity among essays was low overall. AI detection tools incorrectly flagged human-written essays as AI-generated, highlighting the need for better detection algorithms for non-English texts. The study concluded that ChatGPT did not enhance essay quality or efficiency, likely due to unfamiliarity with the tool and the potential for distraction during writing.

## Method Summary
The study employed a randomized control trial with 18 second-year master's students divided into control (traditional writing) and experimental (ChatGPT-3 assistance) groups. Students wrote forensic science essays in Croatian (800-1000 words) with a 4-hour time limit. Essays were scored using a rubric system by two teachers, and writing time was recorded. Text authenticity was checked using PlagScan, content similarity was analyzed using the R package Textreuse with Jaccard index, and AI text detection was performed using OpenAI's classifier. Statistical analysis was conducted using linear regression to compare outcomes between groups.

## Key Results
- No significant difference in essay scores between ChatGPT and control groups (both averaged grade C)
- ChatGPT group did not write faster than control group
- ChatGPT group had higher text unauthenticity rates (11.87% vs. 9.96%)
- Low content similarity among essays overall
- AI detection tools incorrectly flagged human-written essays as AI-generated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT-3 does not significantly improve essay quality because students lack proficiency in prompting and content integration.
- Mechanism: Without advanced prompting skills, ChatGPT generates generic content that students struggle to integrate cohesively into their writing style, leading to disjointed essays.
- Core assumption: Students' unfamiliarity with AI tools prevents effective utilization, resulting in no performance gain.
- Evidence anchors:
  - [abstract]: "The ChatGPT group did not perform better in either of the indicators; the students did not deliver higher quality content, did not write faster, nor had a higher degree of authentic text."
  - [section]: "Some studies did show more promising results 8–14, but unlike our study, they were mainly based on GPT and experienced researcher interaction."
- Break Condition: If students receive training in effective AI tool usage and prompting techniques, performance may improve.

### Mechanism 2
- Claim: Use of ChatGPT increases text unauthenticity due to potential plagiarism and over-reliance on generated content.
- Mechanism: Students may incorporate AI-generated text without proper attribution or critical evaluation, leading to higher plagiarism rates.
- Core assumption: AI-generated content is not always original and may contain non-authentic material.
- Evidence anchors:
  - [abstract]: "The text unauthenticity was slightly higher in the experimental group (11.87% ±13.45 to 9.96% ± 9.81%)"
  - [section]: "Fyfe also showed that his students felt uncomfortable writing and submitting the task since they felt they were cheating and plagiarizing 29."
- Break Condition: If plagiarism detection tools improve and students are educated on ethical AI use, unauthenticity may decrease.

### Mechanism 3
- Claim: ChatGPT does not accelerate essay writing because integrating AI-generated content requires additional time for editing and coherence.
- Mechanism: While ChatGPT may provide initial content, students spend extra time refining and aligning it with their writing style, negating time savings.
- Core assumption: The time saved by generating content is offset by the time needed to edit and integrate it.
- Evidence anchors:
  - [abstract]: "The ChatGPT group also did not write faster"
  - [section]: "This finding could also be explained by students’ feedback from Fyfe's study, where they specifically reported difficulties combining the generated text and their own style 29."
- Break Condition: If AI tools improve in generating more cohesive and style-aligned content, writing time may decrease.

## Foundational Learning

- Concept: Understanding of AI language models and their limitations.
  - Why needed here: To recognize that AI tools like ChatGPT have inherent limitations that affect their effectiveness as writing aids.
  - Quick check question: What are the primary limitations of AI language models in academic writing tasks?

- Concept: Proficiency in prompting and AI tool interaction.
  - Why needed here: Effective use of AI tools requires skill in prompting to generate relevant and high-quality content.
  - Quick check question: How does prompting skill affect the quality of AI-generated content?

- Concept: Plagiarism detection and academic integrity.
  - Why needed here: To understand the implications of using AI tools on text authenticity and the importance of maintaining academic integrity.
  - Quick check question: What are the challenges in detecting AI-generated plagiarism, and how can they be addressed?

## Architecture Onboarding

- Component map: Student groups -> Essay writing -> Scoring and evaluation -> Plagiarism detection -> Analysis of results
- Critical path: Student essay writing → Scoring and evaluation → Plagiarism detection → Analysis of results
- Design tradeoffs: Balancing the use of AI tools with the need for original student work; ensuring fair assessment despite potential AI assistance
- Failure signatures: No significant difference in essay scores between groups; higher plagiarism rates in AI-assisted group; AI detection tools misclassifying human-written texts
- First 3 experiments:
  1. Conduct a pilot study with a smaller sample to test the essay writing process and refine the methodology
  2. Experiment with different levels of AI tool guidance to determine optimal support for students
  3. Implement additional training sessions for students on effective AI tool usage and prompting techniques

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prior experience with ChatGPT or similar AI writing tools affect students' performance and writing outcomes?
- Basis in paper: [explicit] The study notes that students had no prior experience with ChatGPT, which may have contributed to their underperformance and distraction during the writing task.
- Why unresolved: The study only tested students' first interaction with ChatGPT, so it's unclear if familiarity with the tool could lead to better performance over time.
- What evidence would resolve it: A longitudinal study tracking students' performance over multiple uses of ChatGPT as a writing assistant, comparing outcomes between experienced and novice users.

### Open Question 2
- Question: Does the language of the text (e.g., English vs. non-English) significantly impact the quality and authenticity of ChatGPT-generated content?
- Basis in paper: [explicit] The study used Croatian for student essays, which may have limited the effectiveness of AI detection tools and the quality of ChatGPT's output, as the model is predominantly trained on English content.
- Why unresolved: The study only tested ChatGPT's performance in Croatian, so its effectiveness in other languages remains unexplored.
- What evidence would resolve it: Comparative studies testing ChatGPT's performance in generating and assisting with essays in multiple languages, including English and non-English texts.

### Open Question 3
- Question: Can the integration of ChatGPT into academic writing tasks improve students' critical thinking and content evaluation skills over time?
- Basis in paper: [inferred] The study suggests that students struggled with integrating ChatGPT-generated content and their own writing style, which may indicate a need for developing critical evaluation skills.
- Why unresolved: The study focused on immediate outcomes (essay quality, writing time, authenticity) rather than long-term skill development.
- What evidence would resolve it: A study measuring changes in students' critical thinking and content evaluation skills after repeated use of ChatGPT in academic writing tasks, compared to traditional methods.

## Limitations
- Small sample size (n=18) limits statistical power and generalizability
- One-day experimental design may not capture longer-term effects of AI tool adoption
- Lack of ChatGPT training for experimental group confounds results with tool unfamiliarity

## Confidence
- Medium confidence in primary finding that ChatGPT-3 did not improve essay quality in this specific context
- Low confidence in claims about ChatGPT's general ineffectiveness as a writing aid
- Medium confidence in plagiarism findings

## Next Checks
1. Replicate the study with a larger sample size (n≥100) and provide comprehensive training on effective prompt engineering for the experimental group
2. Conduct a longitudinal study tracking student performance over a semester to assess learning curves and potential benefits of AI tool familiarity
3. Test the same experimental design with English-language essays to evaluate whether language-specific factors influenced the results, particularly regarding AI detection tool accuracy