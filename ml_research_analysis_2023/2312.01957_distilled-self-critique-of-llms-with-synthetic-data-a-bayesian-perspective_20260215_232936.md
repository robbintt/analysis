---
ver: rpa2
title: 'Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective'
arxiv_id: '2312.01957'
source_url: https://arxiv.org/abs/2312.01957
tags:
- have
- information
- sentiment
- figure
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces distilled Self-Critique (dSC), an approach
  to align large language models (LLMs) with desired behaviors using only synthetic
  data. dSC interprets reward learning from AI feedback (RLAIF) as Bayesian inference,
  employing a Gibbs sampler that alternates critique and revision steps to refine
  LLM outputs toward high-reward generations.
---

# Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective

## Quick Facts
- arXiv ID: 2312.01957
- Source URL: https://arxiv.org/abs/2312.01957
- Reference count: 12
- One-line primary result: Distilled Self-Critique (dSC) achieves up to 0.92 safety score on harmful content, perfect sentiment scores on negative movie reviews, and reduced personal identifiable information leakage.

## Executive Summary
This paper introduces distilled Self-Critique (dSC), an approach to align large language models (LLMs) with desired behaviors using only synthetic data. dSC interprets reward learning from AI feedback (RLAIF) as Bayesian inference, employing a Gibbs sampler that alternates critique and revision steps to refine LLM outputs toward high-reward generations. A subsequent distillation step fine-tunes the model on the synthetic, refined samples. Experiments on safety, sentiment, and privacy tasks demonstrate that dSC significantly improves alignment compared to baseline models and ablations, achieving up to 0.92 safety score on harmful content, perfect sentiment scores on negative movie reviews, and reduced personal identifiable information leakage. Results indicate dSC is a cost-effective, data-efficient alternative for LLM alignment.

## Method Summary
dSC treats reward learning from AI feedback as Bayesian inference, where the goal is to sample from a posterior distribution over high-reward outputs. It employs a Gibbs sampler that alternates between critique and revision steps: the critique step evaluates the current output using a reward model, and the revision step generates a new output based on the critique. An acceptance criterion ensures only improvements in reward are retained. After generating synthetic samples through this process, the approach fine-tunes the original LLM (using LoRA adapters) on these accepted samples via behavioral cloning, effectively distilling the refined behavior into the base model.

## Key Results
- Safety task: dSC with acceptance step achieves safety score of 0.92 on harmful content, significantly outperforming baselines.
- Sentiment task: dSC with acceptance step achieves perfect sentiment scores (1.0) on negative movie reviews, while baseline models score 0.44.
- Privacy task: dSC reduces personal identifiable information leakage compared to baseline models, as measured by NER score.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: dSC improves alignment by refining LLM outputs through a Gibbs sampler that alternates critique and revision steps, which samples from a posterior distribution over high-reward outputs.
- Mechanism: The approach treats reward learning from AI feedback (RLAIF) as Bayesian inference, where the reward model acts as a likelihood function. The Gibbs sampler alternates between sampling critiques of the current output and revising the output based on those critiques, moving the samples toward high-reward regions.
- Core assumption: The alternating critique/revision process effectively samples from the target posterior distribution p(x|c, r) ∝ p(x|c) exp(r(x)).
- Evidence anchors:
  - [abstract] "refines the outputs of a LLM through a Gibbs sampler that alternates critique and revision steps to refine LLM outputs toward high-reward generations"
  - [section 2] "Let p(x|c) be the output distribution of a LLM... and let r(x) be a reward model... Aligning a LLM with a RM can be casted as sampling a response with respect the posterior distribution p(x|c, r) ∝ p(x|c) exp (r(x))"
  - [corpus] Weak evidence - corpus provides related papers but no direct validation of this specific mechanism
- Break condition: If the Gibbs sampler fails to converge to the target posterior, or if the critique/revision steps don't adequately explore the space of high-reward outputs.

### Mechanism 2
- Claim: The self-distillation step after the MCMC sampling amortizes the refinement process, making it computationally efficient while preserving alignment improvements.
- Mechanism: After generating synthetic samples through the Gibbs sampler, the approach fine-tunes the original LLM (parameterized with LoRA adapters) on these accepted samples using behavioral cloning, effectively "distilling" the refined behavior into the base model.
- Core assumption: Behavioral cloning on the accepted MCMC samples preserves the alignment properties while making inference efficient.
- Evidence anchors:
  - [abstract] "utilizing only synthetic data... and includes a self-distillation phase"
  - [section 2] "We propose a self-distillation step by amortizing the MCMC sampler... we can minimize the reverse KL divergence, Ep(x|c,r)[log p(x|c, r) − log pθ(x|c)]"
  - [corpus] Weak evidence - corpus doesn't directly address the distillation mechanism
- Break condition: If the distillation step loses the alignment properties learned during the MCMC sampling, or if the LoRA adapters cannot adequately capture the refined behavior.

### Mechanism 3
- Claim: The acceptance step in the revision process ensures that only improvements in reward are retained, creating a progressive refinement toward better alignment.
- Mechanism: During the revision step, the approach uses a Metropolis acceptance criterion that accepts revisions only if they improve the reward score (or with some probability if they maintain reward), preventing degradation of alignment.
- Core assumption: The acceptance probability min{1, exp(r(x'))/exp(r(xprev))} correctly guides the sampler toward higher-reward outputs while allowing exploration.
- Evidence anchors:
  - [section 2] "we use a Metropolis step to accept the revision with probability min{1, exp (r(x′)) / exp (r(xprev )) }"
  - [section 3] "with and without the acceptance step" ablation shows dSC with acceptance achieves highest safety scores
  - [corpus] Weak evidence - corpus doesn't validate this specific acceptance mechanism
- Break condition: If the reward model is noisy or unreliable, the acceptance step might accept poor revisions or reject good ones, breaking the refinement process.

## Foundational Learning

- Concept: Bayesian inference and posterior sampling
  - Why needed here: The entire dSC framework is built on interpreting RLAIF as Bayesian inference, where the goal is to sample from a posterior distribution over high-reward outputs
  - Quick check question: What is the posterior distribution that dSC aims to sample from, and how does it relate to the reward model?

- Concept: Markov Chain Monte Carlo (MCMC) methods, specifically Gibbs sampling
  - Why needed here: The critique and revision steps form a Gibbs sampler that explores the space of high-reward outputs through alternating conditional sampling
  - Quick check question: How does the alternating critique/revision process in dSC correspond to the conditional distributions in a Gibbs sampler?

- Concept: Contrastive divergence and KL divergence minimization
  - Why needed here: The distillation step minimizes reverse KL divergence between the target posterior and the distilled model, which is related to contrastive divergence methods
  - Quick check question: Why does minimizing reverse KL divergence correspond to behavioral cloning in the distillation step?

## Architecture Onboarding

- Component map: Base LLM -> Gibbs Sampler (critique -> revision -> acceptance) -> LoRA Adapters -> Distilled Model
- Critical path:
  1. Generate initial samples from base LLM given prompts
  2. Run Gibbs sampler with critique and revision steps, accepting improvements
  3. Collect accepted samples as synthetic training data
  4. Fine-tune base LLM with LoRA adapters on synthetic data
  5. Evaluate alignment on test prompts
- Design tradeoffs:
  - Number of MCMC iterations vs. computational cost
  - Quality of reward model vs. alignment performance
  - Size of synthetic dataset vs. generalization capability
  - LoRA adapter rank vs. fine-tuning effectiveness
- Failure signatures:
  - Low acceptance rate in MCMC sampling (suggests reward model is too strict or initial samples are already good)
  - Synthetic data doesn't improve test performance (suggests distribution shift or overfitting)
  - Long MCMC chains needed for convergence (suggests poor mixing or inadequate exploration)
- First 3 experiments:
  1. Run dSC with only critique steps (no revision) to verify the critique capability
  2. Run dSC with only one iteration of critique/revision to measure the impact of MCMC sampling
  3. Run dSC without the acceptance step to measure its importance in the refinement process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the acceptance step in dSC influence the diversity of generated samples compared to methods without this step?
- Basis in paper: [explicit] The paper mentions that the acceptance step "has the consequence to always accept a revision if its sentiment score is higher than the previous response" for the sentiment task, and "has the consequence to always accept a revision if its NER score is lower than the previous response" for the privacy task.
- Why unresolved: The paper does not provide quantitative analysis or qualitative examples showing how the acceptance step affects the diversity of generated samples. It only states that the acceptance step is included and describes its effect on individual revisions.
- What evidence would resolve it: Experiments comparing the diversity of samples generated with and without the acceptance step, using metrics like self-BLEU, distinct n-grams, or human evaluation of sample diversity.

### Open Question 2
- Question: How does the choice of divergence (KL divergence vs. other divergences) in the distillation step affect the quality and alignment of the fine-tuned model?
- Basis in paper: [explicit] The paper mentions that "A fruitful avenue for further research is the exploration of alternative divergences that could also leverage the rejected samples in the distillation step, such as contrastive divergence (Hinton, 2002), or using RL approaches other than behavioral cloning (Ho & Ermon, 2016)."
- Why unresolved: The paper only uses KL divergence in the distillation step and does not compare it to other divergences. It suggests that exploring other divergences is a future research direction but does not provide any analysis or results.
- What evidence would resolve it: Experiments comparing the performance of models fine-tuned using different divergences in the distillation step, measuring alignment with desired attributes and overall quality.

### Open Question 3
- Question: How does the size of the synthetic dataset used for distillation affect the performance of the fine-tuned model?
- Basis in paper: [inferred] The paper mentions using different sizes of synthetic datasets for different tasks (e.g., 478 prompts for the harmful content task, 100 prompts for the sentiment task), but does not analyze the effect of dataset size on performance.
- Why unresolved: The paper does not provide experiments or analysis showing how the size of the synthetic dataset used for distillation impacts the quality and alignment of the fine-tuned model.
- What evidence would resolve it: Experiments training models using different sizes of synthetic datasets and measuring their performance on the target tasks, identifying the optimal dataset size for balancing computational cost and alignment quality.

## Limitations
- The Bayesian interpretation lacks empirical validation of convergence properties or posterior fidelity.
- Experiments are limited to three synthetic or semi-synthetic datasets, restricting generalization claims.
- The approach's sensitivity to hyperparameters (MCMC length, LoRA rank, learning rates) is not thoroughly explored.

## Confidence
- High confidence: Empirical improvements on the three test tasks (safety, sentiment, privacy) are clearly demonstrated and reproducible given the described pipeline.
- Medium confidence: The Bayesian interpretation provides a coherent theoretical motivation, but lacks empirical validation of convergence properties or posterior fidelity.
- Medium confidence: The self-distillation via LoRA is effective in the reported experiments, though robustness to different base models or reward functions is untested.

## Next Checks
1. **Convergence diagnostics**: Run multiple MCMC chains per prompt, compute effective sample size and potential scale reduction factors, and verify that the sampler is not stuck in local modes.
2. **Reward model sensitivity**: Repeat the safety experiment with a different, independently trained reward model and measure changes in final safety scores.
3. **Scaling test**: Apply dSC to a new task (e.g., reducing biased language in news headlines) using a held-out reward model and evaluate both automated metrics and human judgments.