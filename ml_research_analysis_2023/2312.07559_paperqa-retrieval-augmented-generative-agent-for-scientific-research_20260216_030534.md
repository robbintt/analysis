---
ver: rpa2
title: 'PaperQA: Retrieval-Augmented Generative Agent for Scientific Research'
arxiv_id: '2312.07559'
source_url: https://arxiv.org/abs/2312.07559
tags:
- answer
- paperqa
- question
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PaperQA, a retrieval-augmented generative
  agent for answering scientific questions. The key contributions are: 1) PaperQA
  is an agent that dynamically uses modular tools for information retrieval across
  full-text scientific articles, evidence assessment, and answer generation.'
---

# PaperQA: Retrieval-Augmented Generative Agent for Scientific Research

## Quick Facts
- arXiv ID: 2312.07559
- Source URL: https://arxiv.org/abs/2312.07559
- Reference count: 25
- PaperQA outperforms existing LLMs and agents on standard science QA benchmarks while matching expert human performance on LitQA.

## Executive Summary
This paper introduces PaperQA, a retrieval-augmented generative agent designed to answer scientific questions by dynamically using modular tools for information retrieval across full-text articles, evidence assessment, and answer generation. The system demonstrates superior performance on standard science QA benchmarks and a new LitQA benchmark that requires retrieval and synthesis from full-text papers. PaperQA shows better calibration and lower hallucination rates compared to other models through its agent-based architecture that allows flexible, context-aware retrieval and answer synthesis.

## Method Summary
PaperQA is implemented as an agent-based framework using LangChain with three main tools: search, gather evidence, and answer question. The system retrieves papers via Google Scholar/PubMed, chunks them into overlapping passages for vector-based retrieval, and uses GPT-4 for multiple components including the agent LLM, summary LLM, ask LLM, and answer LLM. Papers are retrieved in full-text, chunked, and embedded into a vector database. The gather evidence tool retrieves relevant chunks, summarizes them with the query, and scores relevance using LLM-generated scores in addition to vector distances. The answer LLM generates final responses from context plus background knowledge surfaced by the ask LLM, with citations pointing to source passages.

## Key Results
- PaperQA outperforms GPT-4, AutoGPT, and commercial tools on the LitQA benchmark (50 biomedical questions)
- Achieves expert-level performance matching human experts on LitQA while maintaining lower hallucination rates
- Shows better calibration than baseline models with reduced tendency to generate unsupported answers
- Demonstrates cost and time efficiency advantages over commercial alternatives for scientific literature search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PaperQA reduces hallucinations by anchoring answers in retrieved full-text passages rather than relying on parametric knowledge alone.
- Mechanism: The system retrieves candidate papers, chunks them into overlapping passages, summarizes each with the query as context, generates relevance scores, and uses these summaries for grounded answer synthesis with citations.
- Core assumption: Retrieved passages contain correct information needed to answer queries, and summarization accurately assesses relevance.
- Evidence anchors: Abstract mentions information retrieval across full-text articles and RAG usage; section describes map-reduce evidence gathering; corpus cites RAG literature but doesn't validate specific mechanism.

### Mechanism 2
- Claim: The agent-based architecture allows dynamic adjustment of retrieval strategy based on evidence sufficiency.
- Mechanism: PaperQA uses an LLM agent that iteratively calls search, gather evidence, and answer tools, adjusting keywords and queries based on evidence sufficiency judgments.
- Core assumption: The agent LLM can effectively judge when evidence is sufficient and choose appropriate next actions.
- Evidence anchors: Abstract describes modular pieces allowing dynamic adjustment; section provides agent prompt showing iterative search and evidence gathering; corpus cites agent literature but doesn't validate iterative scientific QA approach.

### Mechanism 3
- Claim: Including both retrieved context and parametric knowledge via a priori prompting improves answer quality.
- Mechanism: PaperQA uses an ask LLM to surface relevant latent knowledge from the base LLM before answer generation, combining up-to-date retrieved information with general knowledge.
- Core assumption: Base LLM's parametric knowledge contains useful information even for recent literature questions, and ask LLM can effectively surface it.
- Evidence anchors: Section describes using LLM-generated relevance scores alongside vector distances; mentions a priori and a posteriori prompting; corpus cites prompting literature but doesn't validate specific approach.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG grounds LLM answers in retrieved documents, reducing hallucinations and providing provenance. Essential for scientific QA where accuracy is critical.
  - Quick check question: What are the key components of a RAG system and how do they interact?

- Concept: Agent-based tool use
  - Why needed here: Allows dynamic, context-aware adjustment of retrieval strategy, unlike fixed pipeline RAG. Critical for handling diverse and complex scientific questions.
  - Quick check question: How does an LLM agent decide which tools to use and in what order? What are the challenges?

- Concept: Document chunking and vector retrieval
  - Why needed here: Scientific papers are long; chunking allows efficient retrieval of relevant passages. Vector embeddings enable semantic search beyond keywords.
  - Quick check question: What are the tradeoffs in choosing chunk size and overlap? How does this impact retrieval quality?

## Architecture Onboarding

- Component map:
  - Agent LLM (GPT-4) -> Search tool -> Gather evidence tool -> Ask LLM (GPT-4) -> Answer LLM (GPT-4) -> Context library
  - Search tool queries Google Scholar/PubMed, chunks and embeds papers
  - Gather evidence tool retrieves relevant chunks, summarizes with query, scores relevance
  - Ask LLM surfaces latent knowledge via a priori prompting
  - Answer LLM generates final answer from context + background, cites sources
  - Context library stores gathered summaries and their relevance scores

- Critical path:
  1. Agent LLM initializes with question
  2. Search tool retrieves papers, chunks and embeds them
  3. Gather evidence tool retrieves relevant chunks, summarizes with query, scores them
  4. Ask LLM provides background knowledge
  5. Answer LLM generates answer from context + background, cites sources
  6. Agent LLM decides to accept or reject answer

- Design tradeoffs:
  - GPT-4 vs Claude-2 for answer LLM: GPT-4 slightly outperforms but is more expensive
  - Single vs multiple answer iterations: More iterations allow refinement but increase cost and latency
  - Chunk size and overlap: Larger chunks may miss relevant details, smaller chunks may lose context
  - Relevance scoring: LLM-generated scores vs vector distances - LLM is more flexible but adds cost

- Failure signatures:
  - Agent LLM loops indefinitely or settles on poor answers - suggests issues with evidence sufficiency judgment
  - Gather evidence tool retrieves irrelevant chunks - suggests issues with vector retrieval or summarization
  - Answer LLM generates hallucinations - suggests issues with grounding or citation
  - High latency or cost - suggests opportunities for optimization

- First 3 experiments:
  1. A/B test with and without ask LLM to quantify impact of parametric knowledge
  2. A/B test with and without relevance scoring to quantify impact of LLM-generated scores
  3. A/B test with different chunk sizes to find optimal tradeoff between detail and context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the prompts for the multiple agent-based components in PaperQA?
- Basis in paper: Inferred from discussion of limitations mentioning prompt optimization as an ongoing challenge for multi-agent systems.
- Why unresolved: Prompt optimization for multi-agent systems is a non-trivial, bi-level optimization problem, making it difficult to assess which pieces of prompts are necessary and optimal.
- What evidence would resolve it: Systematic experimentation and evaluation of different prompt designs and their impact on PaperQA's performance across various scientific domains.

### Open Question 2
- Question: How can we evaluate the quality and reliability of the underlying papers used by PaperQA?
- Basis in paper: Inferred from discussion of limitations mentioning the assumption that information in underlying papers is correct, and noting that journal names and citation counts are not faithful indicators of quality.
- Why unresolved: The paper highlights the need for methods to assess quality and reliability of scientific papers but doesn't provide a solution.
- What evidence would resolve it: Development and validation of metrics or models to assess paper quality, and integration of such assessments into PaperQA's retrieval and evidence-gathering processes.

### Open Question 3
- Question: How can we improve PaperQA's ability to retrieve relevant papers and synthesize information from full-text papers?
- Basis in paper: Explicit - The paper introduces LitQA benchmark for evaluating retrieval and synthesis from full-text scientific papers, noting that PaperQA's performance is not perfect.
- Why unresolved: While PaperQA outperforms existing models on LitQA, there is still room for improvement in its ability to retrieve relevant papers and synthesize information from full-text papers.
- What evidence would resolve it: Further development of PaperQA's retrieval methods, evidence-gathering processes, and answer synthesis capabilities, along with rigorous evaluation on LitQA and other benchmarks.

## Limitations
- Prompt optimization remains challenging for the multi-agent system, requiring extensive experimentation
- Quality assessment of underlying papers is not addressed, assuming retrieved papers contain correct information
- Retrieval and synthesis from full-text papers can still be improved, as evidenced by imperfect LitQA performance

## Confidence

High confidence in the core architectural claims (modular agent-based RAG design)
Medium confidence in performance claims (benchmark results depend on exact implementation)
Medium confidence in hallucination reduction claims (requires detailed error analysis)
Low confidence in cost efficiency claims (limited operational details provided)

## Next Checks

1. Implement a controlled ablation study comparing PaperQA with and without the ask LLM component to quantify the impact of parametric knowledge
2. Create a synthetic benchmark with known answer distributions to systematically measure hallucination rates across different question types
3. Conduct a head-to-head cost comparison using standardized queries to verify the claimed efficiency improvements over commercial tools