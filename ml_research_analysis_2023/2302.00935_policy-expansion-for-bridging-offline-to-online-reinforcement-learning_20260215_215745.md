---
ver: rpa2
title: Policy Expansion for Bridging Offline-to-Online Reinforcement Learning
arxiv_id: '2302.00935'
source_url: https://arxiv.org/abs/2302.00935
tags:
- policy
- learning
- online
- training
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of bridging offline and online
  reinforcement learning to leverage the strengths of both worlds. The core idea is
  policy expansion: retain the offline policy, expand the policy set with a learnable
  policy, and compose them adaptively for interaction.'
---

# Policy Expansion for Bridging Offline-to-Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.00935
- Source URL: https://arxiv.org/abs/2302.00935
- Reference count: 40
- One-line primary result: Policy expansion outperforms direct fine-tuning and other baselines on D4RL benchmarks by retaining offline policy while enabling online adaptation

## Executive Summary
This paper tackles the challenge of bridging offline and online reinforcement learning to leverage the strengths of both worlds. The core idea is policy expansion: retain the offline policy, expand the policy set with a learnable policy, and compose them adaptively for interaction. Experiments show this outperforms direct fine-tuning and other baselines on D4RL benchmarks. The approach achieves higher normalized returns across tasks, especially in complex ones like antmaze. Ablation studies confirm the importance of preserving the offline policy and transferring critic parameters.

## Method Summary
The method starts with an offline-trained policy and expands it into a policy set containing both the frozen offline policy and a learnable online policy. During online training, actions are sampled from both policies and selected based on their estimated values, allowing adaptive composition. The critic from offline training is transferred to provide a warm start for value estimation. This approach mitigates the potential issues of destroying useful offline behaviors while enabling further learning through the online policy component.

## Key Results
- Policy expansion achieves higher normalized returns than direct fine-tuning across D4RL benchmark tasks
- The method shows particularly strong performance on complex tasks like antmaze
- Ablation studies confirm that preserving the offline policy and transferring critic parameters are critical for success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy expansion preserves the offline policy's learned behaviors while enabling online adaptation
- Mechanism: The offline policy is frozen and placed in a policy set alongside a learnable policy. During online training, actions are sampled from both policies and selected based on their estimated values, ensuring the offline policy's behaviors are retained while the new policy can learn
- Core assumption: The offline policy captures useful behaviors that should not be overwritten during online training
- Evidence anchors: [abstract], [section 4.1]

### Mechanism 2
- Claim: Adaptive policy composition allows both policies to contribute to exploration and learning based on their respective expertise
- Mechanism: The composite policy selects actions from either the offline or online policy based on their estimated values at the current state, allowing each policy to specialize in different regions of the state space
- Core assumption: The value estimates accurately reflect the relative utility of each policy's actions in different states
- Evidence anchors: [section 4.1], [section 6.4]

### Mechanism 3
- Claim: Transferring the critic parameters from offline to online training provides a warm start for value estimation
- Mechanism: The critic network trained on offline data is used as the initial critic for online training, reducing the cold start problem and providing more stable value estimates during early online interactions
- Core assumption: The offline critic provides reasonable value estimates that can be refined with online data
- Evidence anchors: [section 4.2], [section 6.3]

## Foundational Learning

- Concept: Offline reinforcement learning (Offline RL)
  - Why needed here: The method builds upon an offline-trained policy, so understanding how policies are learned from static datasets is crucial
  - Quick check question: What is the main challenge in offline RL that the paper addresses with policy expansion?

- Concept: Actor-critic methods
  - Why needed here: The method uses an actor (policy) and critic (value function) that are trained together, with the critic guiding the policy updates
  - Quick check question: How does the critic help the actor improve in actor-critic methods?

- Concept: Distributional shift
  - Why needed here: The method aims to mitigate the distributional shift between offline and online data, which can cause performance degradation when fine-tuning
  - Quick check question: What is distributional shift and why is it a problem in offline-to-online RL?

## Architecture Onboarding

- Component map: Offline RL module -> Policy expansion module -> Adaptive composition module -> Online RL module
- Critical path: Offline RL training -> Policy expansion -> Online RL training with adaptive composition
- Design tradeoffs:
  - Freezing πβ vs. allowing it to adapt: Preserves offline behaviors but may limit online learning
  - Using only online data vs. mixing with offline data: Faster adaptation but may lose offline knowledge
  - Simple averaging vs. value-based composition: Simpler but may not leverage each policy's strengths
- Failure signatures:
  - Poor performance on states where πβ was strong: May indicate πβ is being ignored
  - High variance in action selection: May indicate unreliable value estimates
  - Slow learning of πθ: May indicate insufficient exploration or strong regularization
- First 3 experiments:
  1. Train πβ on a simple offline dataset (e.g., D4RL halfcheetah-medium) and evaluate its performance
  2. Apply policy expansion and evaluate the composite policy's performance on the same task
  3. Compare the performance of policy expansion with direct fine-tuning on a task where πβ is known to be suboptimal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PEX scale with the number of policies in the policy set? Does increasing the policy set size beyond 2 provide additional benefits, or does it lead to diminishing returns or overfitting?
- Basis in paper: [explicit] The paper discusses policy expansion with a policy set of size 2 (πβ and πθ). It mentions that the number of parameters grows with the number of policies in the policy set, which could be a limitation when dealing with a large policy set
- Why unresolved: The paper only experiments with a policy set of size 2. It does not explore the effects of increasing the policy set size beyond 2 or investigate the point at which adding more policies becomes detrimental
- What evidence would resolve it: Experiments comparing the performance of PEX with policy sets of different sizes (e.g., 2, 4, 8, 16) on a range of tasks would provide insights into how the performance scales with the number of policies

### Open Question 2
- Question: How sensitive is PEX to the choice of the inverse temperature parameter α? Can a more adaptive scheme for tuning α during training improve performance compared to using a fixed value?
- Basis in paper: [explicit] The paper discusses the impact of the inverse temperature parameter α on the policy composition probability Pw. It shows that different values of α can affect the performance on certain tasks, but uses a fixed value based on the IQL paper
- Why unresolved: The paper only experiments with a few fixed values of α and does not explore the potential benefits of an adaptive scheme for tuning α during training
- What evidence would resolve it: Experiments comparing the performance of PEX with different fixed values of α and with an adaptive scheme for tuning α (e.g., based on the entropy of the policy composition distribution) would provide insights into the sensitivity of PEX to α and the potential benefits of an adaptive approach

### Open Question 3
- Question: How does the proposed policy expansion scheme compare to other methods for combining offline and online RL, such as using a weighted combination of the offline and online value functions or policy gradients?
- Basis in paper: [explicit] The paper compares PEX to several baseline methods, including Direct (direct fine-tuning of the offline policy), AWAC, Off2On, BT, and JSRL. However, it does not compare PEX to methods that use a weighted combination of the offline and online value functions or policy gradients
- Why unresolved: The paper does not explore alternative ways of combining the offline and online components, which could potentially lead to different trade-offs between preserving the offline policy and enabling further learning
- What evidence would resolve it: Experiments comparing PEX to methods that use a weighted combination of the offline and online value functions or policy gradients would provide insights into the relative strengths and weaknesses of different approaches for combining offline and online RL

## Limitations
- Performance heavily depends on the quality of the initial offline policy
- Effectiveness relies on accurate value estimates which may be compromised by distribution shift
- Limited exploration of hyperparameter sensitivity across different tasks

## Confidence
- **High Confidence**: The core mechanism of policy expansion and adaptive composition is well-supported by experimental results
- **Medium Confidence**: Transferring the offline critic provides a warm start, but alternative initialization strategies are not explored
- **Low Confidence**: The claim of being "simple and effective" is subjective and not rigorously validated across diverse tasks

## Next Checks
1. Conduct a systematic study of the method's sensitivity to temperature α and expectile τ across a range of tasks
2. Compare the performance with alternative critic initialization strategies (random initialization, pre-training on related task)
3. Evaluate the method's performance on a broader set of tasks with different state and action spaces to assess generalization capabilities