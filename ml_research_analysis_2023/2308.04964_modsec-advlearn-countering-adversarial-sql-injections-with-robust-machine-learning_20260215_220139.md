---
ver: rpa2
title: 'ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine
  Learning'
arxiv_id: '2308.04964'
source_url: https://arxiv.org/abs/2308.04964
tags:
- adversarial
- sqli
- rules
- attacks
- modsecurity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of ModSecurity's suboptimal trade-off
  between detection and false alarm rates, as well as its vulnerability to adversarial
  SQL injection attacks. The authors propose a novel approach, named AdvModSec, that
  leverages machine learning to automate the selection of rules and their weights,
  and uses adversarial training to improve robustness against adversarial SQLi attacks.
---

# ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust Machine Learning

## Quick Facts
- arXiv ID: 2308.04964
- Source URL: https://arxiv.org/abs/2308.04964
- Reference count: 40
- The paper proposes AdvModSec, a machine learning-based approach that increases SQL injection detection rates by up to 30% while maintaining negligible false alarm rates and improving robustness against adversarial attacks by up to 85%.

## Executive Summary
The paper addresses ModSecurity's limitations in detecting SQL injection attacks, particularly its suboptimal trade-off between detection and false alarm rates and vulnerability to adversarial evasion. The authors propose AdvModSec, which uses machine learning to optimize the weights of Core Rule Set (CRS) rules and incorporates adversarial training to improve robustness. By representing SQL queries as feature vectors of CRS rule activations and training both linear SVM and Random Forest models, AdvModSec achieves significant improvements over vanilla ModSecurity. The approach demonstrates that ML-based optimization of rule weights, combined with adversarial training using WAF-A-MoLE-generated examples, creates a more effective and robust web application firewall.

## Method Summary
The method involves converting SQL queries into binary feature vectors where each dimension represents whether a specific CRS SQL injection rule was triggered. Two machine learning models—linear SVM and Random Forest—are trained on these features to classify queries as benign or malicious. The models learn optimal weights for each rule based on the training data rather than relying on manually assigned severity levels. Adversarial training is implemented by generating evasive SQLi examples using WAF-A-MoLE and retraining the models on these examples, forcing them to learn patterns that are harder to bypass. The approach is evaluated on both baseline and adversarially-manipulated test sets to measure detection performance and robustness.

## Key Results
- AdvModSec improves detection rate by up to 30% compared to vanilla ModSecurity while maintaining negligible false alarm rates
- Random Forest achieves 25.8% higher True Positive Rate at 1% False Positive Rate compared to vanilla ModSecurity
- Adversarial training improves robustness against WAF-A-MoLE-generated attacks by up to 85%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine learning models trained on CRS rules as features outperform ModSecurity's static rule-weight combination in SQLi detection.
- Mechanism: The ML model learns an optimal weighting of CRS rules based on the actual traffic patterns of the protected web service, rather than using manually assigned severity levels. This adapts the detection logic to the specific context of the service.
- Core assumption: The distribution of benign vs malicious traffic in the training data is representative of real-world traffic the WAF will see.
- Evidence anchors:
  - [abstract]: "Our experiments show that AdvModSec, being trained on the traffic directed towards the protected web services, achieves a better trade-off between detection and false positive rates, improving the detection rate of the vanilla version of ModSecurity with CRS by 21%."
  - [section 3.1]: "To optimally tune the contribution of the SQLi rules towards effectively classifying the input requests we leverage two different machine-learning algorithms on the feature representation defined above: a linear Support Vector Machine (SVM), and a non-linear Random Forest (RF) classifier."
  - [corpus]: Weak - no direct corpus papers found that analyze CRS rule weight optimization specifically.

### Mechanism 2
- Claim: Adversarial training using WAF-A-MoLE-generated adversarial SQLi examples improves the robustness of the ML-based WAF against evasion attempts.
- Mechanism: The adversarial training procedure generates adversarial SQLi examples that evade the current model, then retrains the model on these examples. This forces the model to learn patterns that are harder to bypass, improving its resilience to evasion.
- Core assumption: The adversarial manipulations generated by WAF-A-MoLE cover a representative space of possible evasion strategies attackers might use.
- Evidence anchors:
  - [abstract]: "we define a custom adversarial training approach that incorporates knowledge of state-of-the-art SQLi manipulations when training our learning-based ModSecurity WAF. We will show that the resulting model, named AdvModSec, provides an unprecedented level of robustness against adversarial SQLi attacks."
  - [section 3.2]: "This technique integrates the computation of adversarial examples at training time, thus giving the model the possibility of knowing in advance evasive patterns that could be computed at test time."
  - [corpus]: Weak - no direct corpus papers found that use WAF-A-MoLE for adversarial training in this specific way.

### Mechanism 3
- Claim: Random Forest outperforms linear SVM for this task due to the non-linear relationships between CRS rule activations and SQLi detection.
- Mechanism: The non-linear decision boundaries learned by RF can capture complex interactions between different CRS rules that a linear model might miss, leading to better detection performance.
- Core assumption: The relationship between CRS rule activations and maliciousness is inherently non-linear and benefits from ensemble methods.
- Evidence anchors:
  - [section 3.1]: "If, on the one hand, linear models have several practical advantages such as explainability, simplicity and computational efficiency, on the other hand, they may exhibit lower classification accuracy on data that is not inherently linearly separable, or when features are not independent of each other [28]."
  - [section 4.3]: "the TPR at 1% FPR of the MLModSec linear SVM is 18.65% higher than ModSecurity. As for the RF, the TPR at 1% FPR is 25.8% higher than the vanilla ModSecurity."
  - [corpus]: Weak - no direct corpus papers found comparing linear vs RF models for CRS-based WAFs.

## Foundational Learning

- Concept: Feature engineering from rule-based systems
  - Why needed here: The approach converts discrete rule matches into a continuous feature space that ML algorithms can process, enabling data-driven optimization of rule weights.
  - Quick check question: How does the one-hot encoding of CRS rule activations differ from using rule severity levels directly as features?

- Concept: Adversarial training in non-differentiable domains
  - Why needed here: Standard adversarial training relies on gradient-based methods, but SQLi payloads are discrete strings. The paper uses black-box optimization (WAF-A-MoLE) to generate adversarial examples.
  - Quick check question: What are the key differences between gradient-based adversarial training and the black-box approach used here?

- Concept: Receiver Operating Characteristic (ROC) analysis
  - Why needed here: ROC curves are used to evaluate the trade-off between detection rate and false positive rate across different operating points, which is critical for WAF performance assessment.
  - Quick check question: Why might a WAF designer choose a specific point on the ROC curve rather than always maximizing detection rate?

## Architecture Onboarding

- Component map: SQL query -> CRS rule feature extraction -> ML model (SVM/RF) -> Classification decision
- Critical path: Feature extraction from incoming SQL query, ML model inference, threshold comparison for decision. Any delay in feature extraction directly impacts WAF latency.
- Design tradeoffs: Linear models offer explainability and efficiency but may underperform on complex patterns; RF offers better performance but at the cost of interpretability and slightly higher computational overhead.
- Failure signatures: High false positive rates indicate over-sensitivity to benign patterns; low detection rates against adversarial examples indicate insufficient adversarial training coverage.
- First 3 experiments:
  1. Train MLModSec (SVM and RF) on training data and evaluate on test set to establish baseline performance improvement over vanilla ModSecurity.
  2. Generate adversarial test set using WAF-A-MoLE against MLModSec models to measure vulnerability to evasion attacks.
  3. Apply adversarial training to create AdvModSec and re-evaluate on both test and adversarial test sets to quantify robustness improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would AdvModSec perform on real-world web application traffic compared to synthetic datasets?
- Basis in paper: [inferred] The paper acknowledges that the current evaluation uses synthetic data generated from a grammar and attack payloads, and notes that evaluating on real-world traffic remains an open question.
- Why unresolved: Real-world traffic patterns are more complex and diverse than synthetic datasets, potentially revealing limitations in the model's generalization.
- What evidence would resolve it: Testing AdvModSec on traffic captured from actual deployed web applications with varied user behaviors and attack patterns.

### Open Question 2
- Question: How transferable are adversarial SQLi attacks optimized on ModSecurity to other commercial WAFs based on CRS?
- Basis in paper: [explicit] The paper suggests evaluating transferability of adversarial attacks as a future direction, noting that commercial solutions use CRS.
- Why unresolved: Different WAF implementations may have varying detection mechanisms and thresholds, affecting attack transferability.
- What evidence would resolve it: Generating adversarial examples on ModSecurity and testing their effectiveness against other CRS-based commercial WAFs.

### Open Question 3
- Question: Can AdvModSec's adversarial training approach be extended to protect against other types of web attacks beyond SQLi?
- Basis in paper: [explicit] The paper mentions future work on XSS and RCE attacks, indicating the need to develop new datasets and manipulation techniques.
- Why unresolved: Different attack types require different detection rules and adversarial manipulations, requiring new training approaches.
- What evidence would resolve it: Successfully extending AdvModSec's framework to detect and defend against XSS and RCE attacks with comparable performance gains.

## Limitations
- The evaluation relies heavily on synthetic data generation for adversarial examples, which may not fully represent real-world attack patterns.
- The approach assumes access to labeled training data, which may not be readily available in all operational contexts.
- The computational overhead of ML inference compared to rule-based matching is not discussed, which could impact real-time performance.

## Confidence
- **High Confidence**: The core methodology of using ML to optimize CRS rule weights is well-established and the experimental setup is clearly described. The improvement in detection rates (21% baseline improvement) is supported by the presented results.
- **Medium Confidence**: The adversarial training approach shows promising results, but the generalization to unseen attack patterns remains uncertain. The specific choice of Random Forest over other non-linear models could be better justified.
- **Low Confidence**: The long-term effectiveness against adaptive attackers who might develop counter-strategies is not addressed. The claim of "unprecedented robustness" lacks comparative analysis against other state-of-the-art approaches.

## Next Checks
1. **Real-world deployment validation**: Test the AdvModSec model on live traffic from production systems over an extended period to verify the reported detection rates and false positive rates under actual operating conditions.
2. **Adaptive attack simulation**: Implement a red team exercise where attackers are informed about the ML-based detection system and attempt to develop new evasion strategies not covered by WAF-A-MoLE's mutation operators.
3. **Performance benchmarking**: Measure the computational overhead and latency introduced by the ML models compared to vanilla ModSecurity under various traffic loads to ensure the approach meets real-time requirements.