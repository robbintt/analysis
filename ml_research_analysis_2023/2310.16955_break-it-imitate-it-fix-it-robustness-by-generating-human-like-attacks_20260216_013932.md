---
ver: rpa2
title: 'Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks'
arxiv_id: '2310.16955'
source_url: https://arxiv.org/abs/2310.16955
tags:
- attacks
- adversarial
- examples
- hate
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving NLP model robustness
  to human-generated adversarial examples. The authors propose a method to generate
  synthetic adversarial examples that imitate the distribution of real human attacks,
  using a small set of observed human adversarial examples.
---

# Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks

## Quick Facts
- arXiv ID: 2310.16955
- Source URL: https://arxiv.org/abs/2310.16955
- Authors: 
- Reference count: 40
- Primary result: Training on synthetic adversarial examples that imitate human attacks improves robustness to future human-generated attacks, with 11% accuracy gains on ANLI and 8% AUC gains on hate speech detection.

## Executive Summary
This paper addresses the challenge of improving NLP model robustness to human-generated adversarial examples. The authors propose a novel approach that learns the distribution of human attacks and generates synthetic examples following this distribution. Their method, which includes Direct Imitation (DI) and Imitation + Controlled Exploration (ICE), significantly improves model robustness to future rounds of human attacks. The approach is particularly effective when trained on as few as 500 real human adversarial examples.

## Method Summary
The paper proposes two methods for generating synthetic adversarial examples that imitate the distribution of real human attacks. Direct Imitation (DI) fine-tunes a generator model on observed human adversarial examples to learn their distribution. Imitation + Controlled Exploration (ICE) builds on DI by adding a linear classifier to ensure generated examples retain the correct label. Both methods use a T5 encoder-decoder model as the generator and train on a small set of observed human adversarial examples. The generated synthetic examples are then used to train classifiers, improving their robustness to future rounds of human-generated attacks.

## Key Results
- Training on synthetic adversarial examples improves robustness to future human attacks
- On ANLI dataset, accuracy gains of 11% on unseen attacks
- On hate speech detection, AUC gains of 8% on existing attacks and 2% on future round

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating synthetic examples that imitate the distribution of real human attacks improves robustness to future human attacks.
- Mechanism: By learning the distribution of existing human adversarial examples, the generator can create synthetic examples that capture patterns of how humans attack models. These synthetic examples then help the model learn to resist future attacks that follow similar patterns.
- Core assumption: The distribution of human attacks is stable enough that a generator trained on past attacks can generalize to future attacks.
- Evidence anchors:
  - [abstract] "Our attack generation methods are able to make improvements even when trained on as few as 500 real human adversarial examples."
  - [section] "Our primary metric for attack quality is whether the generated attacks are useful when used in adversarial training to defend against future unseen rounds of human-generated attacks."
  - [corpus] Weak evidence - no related papers specifically discuss distribution learning for adversarial attacks.
- Break condition: If the distribution of human attacks changes significantly over time, the generator will fail to produce useful synthetic examples.

### Mechanism 2
- Claim: The ICE method improves upon DI by incorporating a linear classifier to maintain the correct label during generation.
- Mechanism: The linear classifier guides the generator to produce examples that retain the desired label, reducing the rate of label noise in the generated examples. This is particularly important for adversarial examples, which by definition are designed to have incorrect labels.
- Core assumption: A linear classifier on the hidden layers of the generator can effectively distinguish between examples with the correct and incorrect labels.
- Evidence anchors:
  - [abstract] "ICE, which adds a linear classifier to ensure the generated examples retain the correct label."
  - [section] "To overcome this challenge, we modify the Plug and Play controlled decoding method to make it suitable for adversarial robustness."
  - [corpus] Weak evidence - no related papers specifically discuss linear classifiers for controlled generation of adversarial examples.
- Break condition: If the linear classifier is not powerful enough to distinguish between correctly and incorrectly labeled examples, the ICE method will not improve upon DI.

### Mechanism 3
- Claim: Existing synthetic attack methods (TextFooler, BertAttack, CT-GAN) do not improve robustness because they do not learn the distribution of human attacks.
- Mechanism: These methods generate examples through random perturbations or template-based approaches that do not capture the patterns of how humans actually attack models. As a result, the examples they generate are not useful for training models to resist future human attacks.
- Core assumption: The patterns of human attacks are more complex than simple perturbations or templates, and require learning the distribution of existing attacks.
- Evidence anchors:
  - [abstract] "Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds."
  - [section] "Our attack generation methods, which learn the distribution of human adversarial examples, outperform both (1) attack generators that do not take the human examples into account, and (2) attack generators that do not learn the distribution but rely on random perturbations."
  - [corpus] Weak evidence - no related papers specifically compare the effectiveness of different synthetic attack methods on improving robustness to future human attacks.
- Break condition: If the patterns of human attacks are simple enough that random perturbations or templates can capture them, existing synthetic attack methods may be sufficient.

## Foundational Learning

- Concept: Distribution learning
  - Why needed here: To generate synthetic examples that imitate the distribution of real human attacks, we need to understand how to learn a distribution from data.
  - Quick check question: What is the difference between learning a distribution and learning a mapping from inputs to outputs?

- Concept: Controlled text generation
  - Why needed here: To generate synthetic examples that retain the correct label, we need to understand how to control the generation process to produce text with desired properties.
  - Quick check question: What is the difference between unconditional and conditional text generation?

- Concept: Adversarial robustness
  - Why needed here: To evaluate the effectiveness of our method, we need to understand what adversarial robustness means and how to measure it.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

## Architecture Onboarding

- Component map: Generator (T5) -> Linear Classifier -> Synthetic Examples -> Classifier (BERT-Large)
- Critical path:
  1. Fine-tune generator on human adversarial examples
  2. Use generator to create synthetic adversarial examples
  3. Fine-tune classifier on synthetic examples
  4. Evaluate classifier on future rounds of human adversarial examples
- Design tradeoffs:
  - Generator complexity vs. generation speed: More complex generators may produce better examples but take longer to generate them.
  - Number of synthetic examples vs. label noise: Generating more examples may introduce more label noise, which could harm robustness.
  - Linear classifier complexity vs. effectiveness: More complex classifiers may be better at maintaining correct labels but may also be more computationally expensive.
- Failure signatures:
  - Synthetic examples are too similar to original examples: Generator has not learned the distribution of human attacks.
  - Synthetic examples have incorrect labels: Linear classifier is not effective at maintaining correct labels.
  - Classifier does not improve robustness: Synthetic examples are not useful for training.
- First 3 experiments:
  1. Generate synthetic examples using DI method and evaluate on future rounds of human adversarial examples.
  2. Generate synthetic examples using ICE method and evaluate on future rounds of human adversarial examples.
  3. Compare the effectiveness of DI and ICE methods on improving robustness to future rounds of human adversarial examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many human adversarial examples are needed to train an attack generator that generalizes well to future unseen attacks?
- Basis in paper: [explicit] Table 6 shows results for varying numbers of human examples from 100 to all 16.9k, but the authors state that further investigation is needed to determine the exact threshold.
- Why unresolved: The paper only tests up to 8k human examples and doesn't definitively state the minimum required. The relationship between number of examples and generalization ability is unclear.
- What evidence would resolve it: A comprehensive study testing a wider range of example counts (e.g. 10, 50, 100, 500, 1000, 2000, 5000, 10000) and measuring performance on future attack rounds would clarify the relationship.

### Open Question 2
- Question: Do the proposed attack generation methods improve robustness to human attacks that adapt to the new more robust models?
- Basis in paper: [inferred] The authors note in the Limitations section that they did not gather additional human attacks targeting their improved classifiers, leaving open the question of whether human adversaries can adapt to overcome the defenses.
- Why unresolved: Evaluating the proposed methods on truly unseen human attacks that specifically target the new models would require extensive crowd-sourcing efforts beyond the scope of this work.
- What evidence would resolve it: Gathering and evaluating the proposed methods on a new round of human attacks specifically designed to fool the more robust models trained on generated data.

### Open Question 3
- Question: Are the proposed attack generation methods effective on larger, more capable NLP models beyond BERT-Large?
- Basis in paper: [inferred] The authors only test their methods on improving BERT-Large, and note in the Limitations section that they have not evaluated them on classifiers with access to even larger real adversarial datasets.
- Why unresolved: The proposed methods may not scale well to larger models or benefit as much from larger datasets. The relative importance of imitation vs. exploration may shift.
- What evidence would resolve it: Applying the proposed methods to train larger models like T5 or GPT-3 on larger adversarial datasets and evaluating performance on future human attack rounds.

## Limitations
- The stability of human attack distribution across many rounds is not fully established
- Limited analysis of the linear classifier's performance in the ICE method
- Weak support for claims about existing synthetic attack methods' ineffectiveness

## Confidence
- High confidence: The core finding that training on synthetic adversarial examples improves robustness to future human attacks is well-supported by experimental results on two distinct datasets (ANLI and hate speech detection).
- Medium confidence: The claim that the ICE method improves upon DI by incorporating a linear classifier is supported by results but lacks detailed analysis of the classifier's performance.
- Low confidence: The assertion that existing synthetic attack methods do not improve robustness because they do not learn the distribution of human attacks is weakly supported, as the paper does not provide direct comparisons with these methods.

## Next Checks
1. Conduct experiments on additional datasets beyond ANLI and hate speech detection to assess the generalizability of the proposed methods across different NLP tasks and domains.
2. Perform ablation studies to quantify the contribution of the linear classifier in the ICE method and identify conditions under which it may be beneficial or detrimental.
3. Investigate the impact of the number of human adversarial examples used for training the generator on the quality of the generated synthetic examples and the resulting robustness improvements.