---
ver: rpa2
title: Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety
  Disorders
arxiv_id: '2308.07407'
source_url: https://arxiv.org/abs/2308.07407
tags:
- chatbot
- support
- generative
- https
- chatbots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work developed three chatbots to support caregivers experiencing
  or at risk of postpartum mood and anxiety disorders, using both rule-based and generative
  models. We found that our rule-based model achieved the best performance overall,
  with outputs close to ground truth reference and the highest levels of empathy.
---

# Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders

## Quick Facts
- arXiv ID: 2308.07407
- Source URL: https://arxiv.org/abs/2308.07407
- Reference count: 40
- Primary result: Rule-based chatbot outperformed generative models in empathy and user preference for postpartum mental health support

## Executive Summary
This work developed three chatbots to support caregivers experiencing or at risk of postpartum mood and anxiety disorders, using both rule-based and generative models. The rule-based model achieved the best performance overall, with outputs close to ground truth reference and the highest levels of empathy. Human users preferred the rule-based chatbot for its content-specific and human-like replies. While the generative chatbot also produced empathetic responses, it often output confusing or nonsensical replies due to limitations in the training dataset.

## Method Summary
The researchers developed three chatbot architectures using a dataset of 7,014 text conversations (65,062 messages) from the PSI helpline. They built classifiers for severe symptoms, psychological states, concerns, and empathy using sentence embeddings and LIWC features. The three chatbots included: a baseline rule-based model, an advanced rule-based model with NLP integration, and a generative model fine-tuned on GPT-2. Performance was evaluated using both machine-based metrics (BERTScore, Empathy%) and human-based evaluations (usability, satisfaction questionnaires).

## Key Results
- Rule-based chatbot achieved highest empathy density and semantic match scores
- Human users significantly preferred rule-based model for content-specific empathetic responses
- Generative model produced confusing or nonsensical responses due to training dataset limitations
- Rule-based and baseline models showed similar semantic matches, both higher than generative model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based chatbots can achieve higher empathy density than generative models in sensitive mental health contexts when trained on small, curated datasets.
- Mechanism: Pre-written empathetic responses ensure consistent emotional validation and reduce the risk of harmful or confusing outputs.
- Core assumption: Empathy can be reliably captured in templated responses and that human evaluators can detect this consistency.
- Evidence anchors:
  - [abstract]: "our rule-based model achieves the best performance overall, with outputs that are close to ground truth reference and contain the highest levels of empathy"
  - [section 7.2]: "Rule-based model and the baseline model showed similar semantic matches that were both higher than our generative model"
  - [corpus]: Weak - neighbor papers focus on general digital phenotyping, not empathy in chatbot outputs.
- Break condition: If the dataset lacks sufficient empathetic examples, the model may default to generic or repetitive responses, as seen with the baseline chatbot.

### Mechanism 2
- Claim: Generative models produce confusing outputs when trained on small datasets with imbalanced content (e.g., logistics-heavy responses).
- Mechanism: Limited training data leads to overfitting on prevalent patterns, causing the model to generate irrelevant or nonsensical replies.
- Core assumption: The generative model learns primarily from the most frequent response types in the dataset.
- Evidence anchors:
  - [section 6.3]: "The biggest advantage of generative chatbot, according to participants, was its ability to 'move the conversation along'. The training process of the generative chatbot involved 'logistics' sentences from PSI"
  - [abstract]: "limitations in the training dataset often result in confusing or nonsensical responses"
  - [corpus]: Weak - no direct mention of dataset size or imbalance issues in neighbor papers.
- Break condition: If the dataset is expanded to include more empathetic and diverse responses, the generative model may improve in coherence and relevance.

### Mechanism 3
- Claim: Human evaluators prefer chatbots that provide content-specific empathetic responses over generic or confusing ones.
- Mechanism: Tailored responses that directly address user concerns increase perceived empathy and engagement.
- Core assumption: Users can distinguish between empathetic and generic responses, and content-specific replies enhance satisfaction.
- Evidence anchors:
  - [section 7.2]: "Participants preferred its detailed, human-like, and empathetic responses that were content-specific, i.e. matching the topics of user input"
  - [abstract]: "Human users prefer the rule-based chatbot over the generative chatbot for its context-specific and human-like replies"
  - [corpus]: Weak - neighbor papers do not discuss human evaluation of chatbot empathy.
- Break condition: If the chatbot misinterprets user input, it may provide irrelevant responses, leading to frustration and reduced trust.

## Foundational Learning

- Concept: Empathy detection in chatbot responses
  - Why needed here: To quantify and ensure the chatbot provides emotionally supportive interactions, especially in sensitive mental health contexts.
  - Quick check question: How is empathy measured in the chatbot responses, and what metrics are used to evaluate it?

- Concept: Dataset curation and filtering for chatbot training
  - Why needed here: To ensure the chatbot is trained on relevant, empathetic, and non-harmful content, avoiding logistics-heavy or confusing outputs.
  - Quick check question: What criteria are used to filter the dataset, and how does this impact the chatbot's performance?

- Concept: Human evaluation of chatbot usability and satisfaction
  - Why needed here: To assess the chatbot's effectiveness in real-world interactions and identify areas for improvement.
  - Quick check question: What methods are used to gather user feedback, and how are the results analyzed to improve the chatbot?

## Architecture Onboarding

- Component map: Input → Concern/Psychological State Detection → Response Generation (Rule-based or Generative) → Output
- Critical path: User input is classified for concerns and psychological states, then appropriate response is generated based on the selected architecture
- Design tradeoffs: Rule-based models ensure consistency and safety but lack flexibility. Generative models offer adaptability but risk incoherence and harm.
- Failure signatures: Rule-based: repetitive or irrelevant responses. Generative: confusing, nonsensical, or logistics-heavy outputs.
- First 3 experiments:
  1. Test the chatbot with a small set of user inputs to evaluate empathy detection accuracy.
  2. Compare the rule-based and generative models on a set of predefined scenarios to assess content-specificity and coherence.
  3. Conduct a human evaluation study to gather feedback on usability, satisfaction, and perceived empathy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of generative models change with larger training datasets?
- Basis in paper: [explicit] The authors note that their generative chatbot often outputs confusing responses due to the small size of the training dataset.
- Why unresolved: The paper only uses a relatively small dataset (7,014 conversations), and does not explore how performance scales with larger datasets.
- What evidence would resolve it: Testing the generative chatbot with a much larger and more diverse dataset to see if the quality of responses improves.

### Open Question 2
- Question: Can large language models like ChatGPT be used directly for digital mental healthcare without human moderation?
- Basis in paper: [explicit] The authors discuss the potential of ChatGPT but highlight several limitations, including its need for context and inability to provide advice ethically.
- Why unresolved: The paper only discusses theoretical concerns and does not test ChatGPT or similar models in a real-world mental health context.
- What evidence would resolve it: Conducting a controlled study where a large language model is used to interact with users in a mental health context, with and without human moderation, to assess safety and efficacy.

### Open Question 3
- Question: What is the optimal balance between rule-based and generative components in a chatbot for mental health support?
- Basis in paper: [inferred] The authors develop three chatbots with varying degrees of rule-based and generative components, but do not explicitly explore the optimal balance between the two.
- Why unresolved: The paper does not systematically vary the proportion of rule-based and generative components to find the optimal balance.
- What evidence would resolve it: Developing a series of chatbots with different ratios of rule-based to generative components and evaluating their performance to determine the optimal balance.

## Limitations

- Evaluation relies heavily on human judgment for empathy detection, introducing subjectivity and potential rater bias
- Dataset composition details are limited - we don't know the full distribution of conversation types or severity levels
- Comparison is between rule-based and generative approaches, but doesn't explore hybrid architectures that might combine their strengths
- The "confusing or nonsensical responses" from the generative model are described but not systematically categorized or quantified

## Confidence

- High confidence: Rule-based chatbot performs better on empathy metrics and user preference
- Medium confidence: Generative model limitations are primarily due to training data constraints
- Low confidence: The specific mechanisms by which rule-based templates achieve higher empathy scores

## Next Checks

1. Conduct a blinded study where evaluators assess chatbot responses without knowing which model generated them, to verify the empathy advantage is consistent across different rater groups
2. Test the generative model with an expanded, more balanced training dataset specifically curated for postpartum mental health conversations to isolate the data limitation hypothesis
3. Implement a hybrid approach that uses rule-based templates for empathy validation while allowing generative responses for conversation flexibility, then compare performance against both pure approaches