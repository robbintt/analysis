---
ver: rpa2
title: On the Effective Horizon of Inverse Reinforcement Learning
arxiv_id: '2307.06541'
source_url: https://arxiv.org/abs/2307.06541
tags:
- expert
- policy
- reward
- function
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work formally analyzes the impact of the effective horizon
  on inverse reinforcement learning (IRL). With limited expert demonstrations, a shorter
  effective horizon improves reward learning by reducing the complexity of the induced
  policy class and mitigating overfitting.
---

# On the Effective Horizon of Inverse Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.06541
- Source URL: https://arxiv.org/abs/2307.06541
- Reference count: 40
- Primary result: Learning the effective horizon jointly with the reward function improves IRL performance, especially with limited expert data.

## Executive Summary
This paper analyzes how the effective horizon (discount factor) impacts inverse reinforcement learning when expert demonstrations are limited. The authors prove that the effective horizon controls the complexity of the induced policy class, which directly affects overfitting. They propose learning the reward function and effective horizon jointly rather than fixing the horizon beforehand. Experiments on Gridworld and Objectworld tasks confirm that the optimal effective horizon is consistently lower than the ground-truth value and decreases with increasing expert data coverage. Cross-validation effectively selects the optimal horizon, leading to improved policy performance.

## Method Summary
The paper extends linear programming IRL (LP-IRL) and maximum entropy IRL (MaxEnt-IRL) to jointly learn the reward function and effective horizon. Expert demonstrations covering varying percentages of states are split into training and validation sets. For each candidate horizon, the algorithm estimates a reward function from training data, induces a policy, and evaluates it on validation data by counting errors (states where induced and expert policies differ). The horizon with minimum validation error is selected. The theoretical analysis proves that policy class complexity increases with horizon length, creating an optimal intermediate value that balances policy complexity against reward estimation error.

## Key Results
- The optimal effective horizon is consistently lower than the ground-truth value across all tested tasks
- Error curves show U-shaped patterns for low expert data coverage, confirming theoretical predictions
- Cross-validation successfully identifies the optimal effective horizon
- The joint learning approach outperforms fixed-horizon baselines, especially with scarce expert data
- The effect is most pronounced for low expert data coverage and diminishes as coverage increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The effective horizon controls the complexity of the induced policy class, which directly impacts overfitting when expert data is limited.
- Mechanism: As the discount factor (γ) increases, the number of potentially optimal policies grows, increasing the complexity of the policy class Πγ. With limited expert demonstrations, a larger policy class leads to higher variance in policy estimation, causing overfitting.
- Core assumption: The MDP is ergodic, ensuring all states are reachable, and the reward function has a unique maximum for each state.
- Evidence anchors:
  - [abstract]: "the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data"
  - [section 4.2]: Theorem 4.3 proves |Πγ| increases monotonically with γ, and under mild conditions |Πγ| → (|A|−1)|S|−1|S| as γ → 1.
  - [corpus]: Related work on BC-IRL notes that IRL algorithms often overfit to demonstrations, supporting the overfitting claim.
- Break condition: If the expert demonstrations cover a large fraction of the state space (|S|), the policy estimation variance drops and overfitting becomes negligible regardless of γ.

### Mechanism 2
- Claim: The error in reward function estimation propagates from the error in expert policy estimation, and this propagation is bounded by the effective horizon.
- Mechanism: The feasible reward set is defined by the expert policy (Lemma 4.5). If the estimated expert policy deviates from the true policy, the feasible reward set shifts. The error in the reward function is bounded by the expert policy estimation error scaled by 1/(1-γ) (Theorem 4.6).
- Core assumption: The expert policy is deterministic, so observing (s,a) pairs from demonstrations exactly specifies the expert policy for those states.
- Evidence anchors:
  - [section 4.4]: Theorem 4.6 bounds |R0 - bR| by BπE BˆπE ζ, which is non-zero only for actions not taken by the expert but mistakenly included in the estimated policy.
  - [abstract]: "The time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data"
  - [corpus]: Weak. No direct corpus evidence on reward estimation error propagation; this is a theoretical construction.
- Break condition: If the expert policy estimation error is zero (e.g., infinite demonstrations), the reward function error becomes zero regardless of γ.

### Mechanism 3
- Claim: The performance loss from using a suboptimal effective horizon is a sum of two opposing error terms, creating an optimal intermediate value.
- Mechanism: The total loss decomposes into (1) error from evaluating the optimal policy with different horizons, which decreases as γ increases, and (2) error from using an incorrectly estimated reward function, which increases with γ due to policy class complexity. The minimum occurs at an intermediate γ*.
- Core assumption: The MDP has bounded rewards in [0, Rmax] and the expert demonstrations are i.i.d. samples.
- Evidence anchors:
  - [section 4.5]: Theorem 4.7 decomposes the value function difference into γ0 - γ and the reward function error term.
  - [section 4.6]: Combines Theorem 4.3 (policy complexity), Theorem 4.6 (reward error), and Theorem 4.7 (overall loss) to prove existence of γ*.
  - [section 5]: Experiments show U-shaped error curves confirming the theoretical prediction.
- Break condition: If the reward function is independent of the horizon (e.g., deterministic shortest path), the second error term vanishes and γ* = γ0.

## Foundational Learning

- Concept: Hoeffding's inequality and its application to policy estimation error bounds
  - Why needed here: Used to bound the expert policy estimation error based on the number of states covered and the complexity of the policy class (section 4.6).
  - Quick check question: If you observe N states with a policy class of size |Πγ|, what is the probability that your estimated policy differs from the true policy on more than t states?

- Concept: Potential-based reward shaping equivalence
  - Why needed here: Establishes that reward functions inducing the same optimal policy are equivalent up to a potential function (Lemma A.2), which is used to characterize the feasible reward set (Lemma 4.5).
  - Quick check question: Given two reward functions R and R' that differ by a potential function, will they induce the same optimal policy?

- Concept: Cross-validation for hyperparameter selection
  - Why needed here: The paper uses cross-validation to select the optimal effective horizon bγ* from limited expert demonstrations (section 5.2).
  - Quick check question: When splitting expert demonstrations into training and validation sets, how does the size of each set affect the reliability of the selected bγ*?

## Architecture Onboarding

- Component map: Expert demonstrations -> IRL algorithm (LP-IRL/MaxEnt-IRL) -> Discount factor selection -> Reward function estimator -> Policy evaluator -> Cross-validation module

- Critical path:
  1. Load expert demonstrations covering N states
  2. For each candidate γ in [0, γ0]:
     a. Estimate reward function bR using training set
     b. Compute induced policy π*bR,γ
     c. Evaluate policy on validation set to count errors
  3. Select γ with minimum validation error as bγ*

- Design tradeoffs:
  - Smaller γ: Lower policy class complexity, less overfitting, but shorter effective planning horizon
  - Larger γ: Better long-term planning, but higher policy class complexity and overfitting risk
  - Cross-validation split ratio: More training data improves reward estimation but less validation data reduces bγ* reliability

- Failure signatures:
  - U-shaped error curves flatten or disappear: Indicates sufficient expert data coverage, making horizon choice less critical
  - Error counts increase monotonically with γ: Suggests reward function is simple or expert data is dense
  - Cross-validation selects γ ≈ γ0: May indicate the theoretical bounds are loose or the task has special structure

- First 3 experiments:
  1. Run LP-IRL with fixed γ=0.99 on Gridworld-simple with 20% expert coverage; record error counts
  2. Run LP-IRL with γ∈{0.1, 0.3, 0.5, 0.7, 0.9, 0.99} on same task; plot error vs γ curve
  3. Apply cross-validation to select bγ* and compare error counts with fixed γ=0.99 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effective horizon interact with different reward function parameterizations in IRL algorithms beyond linear features, such as neural networks?
- Basis in paper: [inferred] The paper tests MaxEnt-IRL with linear reward features but suggests extending insights to other parameterizations.
- Why unresolved: The paper only evaluates linear feature parameterizations, leaving the behavior of nonlinear reward parameterizations unexplored.
- What evidence would resolve it: Empirical studies comparing optimal effective horizons across various reward parameterizations (e.g., neural networks) in IRL algorithms.

### Open Question 2
- Question: Can the theoretical analysis be extended to account for non-deterministic expert policies in IRL?
- Basis in paper: [explicit] The paper assumes a deterministic expert policy, stating "We assume the expert policy is deterministic."
- Why unresolved: The theoretical framework relies on deterministic policies for clean mathematical treatment, but real-world experts often exhibit stochasticity.
- What evidence would resolve it: Extension of the theoretical analysis to handle stochastic expert policies and experimental validation with stochastic demonstrations.

### Open Question 3
- Question: What is the impact of the effective horizon on IRL performance when the transition dynamics are unknown and must be learned from data?
- Basis in paper: [explicit] The paper assumes known transition dynamics and focuses on reward and horizon learning.
- Why unresolved: The theoretical analysis and experiments assume perfect knowledge of transition dynamics, which is often unrealistic.
- What evidence would resolve it: Theoretical analysis and experiments incorporating learned transition dynamics with varying effective horizons in IRL algorithms.

## Limitations
- Theoretical analysis assumes ergodic MDPs and deterministic expert policies, which may not hold in practice
- Experiments are limited to Gridworld and Objectworld domains, which may not capture real-world complexity
- Bounds on estimation errors rely on specific structural assumptions that may not generalize to all MDPs

## Confidence
- **High Confidence**: The core mechanism that effective horizon controls policy class complexity and mitigates overfitting (Mechanism 1) is well-supported by both theoretical proofs and experimental validation.
- **Medium Confidence**: The reward error propagation mechanism (Mechanism 2) has solid theoretical foundation but limited empirical validation in the paper.
- **Medium Confidence**: The existence of an optimal intermediate horizon (Mechanism 3) is theoretically proven and experimentally observed, though the exact characterization may depend on specific problem structure.

## Next Checks
1. Test the effective horizon learning approach on non-ergodic MDPs to evaluate robustness when the ergodic assumption is violated.
2. Evaluate performance when expert demonstrations include stochastic policies to test the deterministic policy assumption.
3. Apply the methodology to continuous control tasks beyond Gridworld/Objectworld to assess generalization to more complex domains.