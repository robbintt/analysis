---
ver: rpa2
title: How Transferable are Attribute Controllers on Pretrained Multilingual Translation
  Models?
arxiv_id: '2309.08565'
source_url: https://arxiv.org/abs/2309.08565
tags:
- translation
- control
- formality
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the transferability of attribute controllers
  to new languages without attribute-annotated data, using a pretrained multilingual
  NLLB-200 model. Two paradigms are compared: finetuning (FT) and inference-time control
  via classifier guidance (CG).'
---

# How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?

## Quick Facts
- arXiv ID: 2309.08565
- Source URL: https://arxiv.org/abs/2309.08565
- Authors: 
- Reference count: 40
- Key outcome: Classifier guidance outperforms finetuning for zero-shot attribute control transfer to new languages

## Executive Summary
This work investigates transferring attribute control capabilities (formality, gender) from pretrained multilingual translation models to new languages without attribute-annotated data. The study compares two paradigms: finetuning the full model versus inference-time control via classifier guidance. While finetuning excels in supervised settings, classifier guidance shows superior generalization to zero-shot conditions, particularly for distant target languages and new domains. The approaches are complementary, with classifier guidance stacked on top of finetuned models achieving the strongest control accuracy.

## Method Summary
The paper investigates attribute control transfer using NLLB-200, a pretrained multilingual translation model. Two controller paradigms are compared: finetuning (modifying model parameters on attribute-annotated data) and classifier guidance (training a classifier on decoder hidden states and using gradient-based steering at inference time). The study evaluates formality and gender control across supervised and zero-shot conditions, testing transfer to new target languages and domains. Human evaluation validates the effectiveness of zero-shot transfer on Bengali.

## Key Results
- Classifier guidance outperforms finetuning for zero-shot transfer to new and distant target languages
- Finetuning does not erase knowledge on other languages, preserving translation quality
- Classifier guidance and finetuning are complementary approaches that can be combined
- Multilingual controllers benefit base models that are not massively multilingual

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inference-time classifier guidance generalizes better to new and distant target languages than finetuning because it operates on pretrained multilingual representations rather than surface vocabulary.
- Mechanism: The classifier is trained on decoder hidden states, which capture language-agnostic features from the pretrained multilingual model. These representations are more transferable than token-level reweighting methods.
- Core assumption: Pretrained multilingual representations contain shared attribute-related features across languages that can be discovered by classifiers.
- Evidence anchors:
  - [abstract] "The controller transfers well to zero-shot conditions, as it operates on pretrained multilingual representations and is attribute -- rather than language-specific."
  - [section 2] "To enable cross-lingual transfer, it is essential that the controller is trained on representations capturing shared features across languages. This precludes many approaches that operate on the surface vocabulary level"
  - [corpus] Weak - corpus mentions related work on multilingual models but no direct evidence for this specific mechanism

### Mechanism 2
- Claim: Finetuning does not erase knowledge on other languages because the attribute control task can be learned with limited data and light finetuning strength.
- Mechanism: The pretrained model's general translation knowledge remains intact because finetuning uses small learning rates, limited updates, and minimal data, preventing catastrophic forgetting.
- Core assumption: Attribute control is a simpler task than full domain adaptation, requiring less data and parameter updates.
- Evidence anchors:
  - [section 4.2] "Finetuning did not erase knowledge on other languages: It is notable that finetuning did not erase the pretrained model's knowledge on the target languages absent in supervised finetuning"
  - [section 2] "This is particularly relevant when generalizing to new languages, where finetuning on selected languages may erase the knowledge of others from pretraining"
  - [corpus] Weak - corpus mentions multilingual domain adaptation but not specifically this attribute control scenario

### Mechanism 3
- Claim: Classifier guidance and finetuning are complementary because they modify different aspects of the model (activations vs parameters) and address different failure modes.
- Mechanism: Finetuning optimizes model parameters for attribute control while classifier guidance provides real-time steering of decoder activations, combining to improve both accuracy and robustness.
- Core assumption: The two approaches modify orthogonal aspects of the translation process and can be stacked without interference.
- Evidence anchors:
  - [section 4.2] "Classifier guidance stacks with finetuning: When applying CG on top of the finetuned models, we see the strongest control accuracy for both formality and gender control"
  - [section 2] "At inference time, the generation process is steered towards desired attributes, for instance using a lightweight classifier"
  - [corpus] Weak - corpus mentions related work but no direct evidence for this specific complementary mechanism

## Foundational Learning

- Concept: Zero-shot transfer learning
  - Why needed here: The paper's core contribution is transferring attribute control capabilities to languages without attribute-annotated data, requiring understanding how models can generalize to unseen conditions.
  - Quick check question: What distinguishes zero-shot transfer from few-shot or supervised learning in the context of attribute control?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper investigates whether finetuning for attribute control erases knowledge of other languages, a classic catastrophic forgetting scenario.
  - Quick check question: What factors determine whether finetuning will cause catastrophic forgetting in multilingual models?

- Concept: Inference-time vs training-time control paradigms
  - Why needed here: The paper compares two fundamentally different approaches to attribute control, requiring understanding their tradeoffs and use cases.
  - Quick check question: What are the key architectural and computational differences between training-time and inference-time control methods?

## Architecture Onboarding

- Component map: Pretrained NLLB-200 600M multilingual translation model -> Attribute controller training (finetuning or classifier guidance) -> Zero-shot transfer evaluation -> Human evaluation validation
- Critical path: Pretrained model → attribute controller training → zero-shot transfer evaluation → human evaluation validation
- Design tradeoffs:
  - Finetuning vs classifier guidance: Parameter efficiency vs inference speed
  - Multilingual vs single-language controllers: Generalization vs specialization
  - Supervised vs zero-shot conditions: Accuracy vs data efficiency
- Failure signatures:
  - Finetuning: Catastrophic forgetting, domain mismatch degradation
  - Classifier guidance: Slow decoding, hyperparameter sensitivity, surface vocabulary bias
  - Zero-shot transfer: Poor accuracy on distant languages, attribute bias from pretrained model
- First 3 experiments:
  1. Replicate supervised finetuning results on formality control to establish baseline performance
  2. Implement classifier guidance on decoder hidden states and compare to finetuning in supervised setting
  3. Test zero-shot transfer of both approaches to new target languages and measure accuracy/quality tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does classifier guidance perform when controlling continuous attributes beyond binary formality and gender distinctions?
- Basis in paper: [explicit] The authors mention classifier predictions of discrete labels and acknowledge the method isn't directly applicable to fine-grained continuous attributes
- Why unresolved: The paper only tests on binary attributes due to dataset limitations
- What evidence would resolve it: Experiments testing classifier guidance on datasets with continuous attribute scales or multi-class attributes beyond the binary cases

### Open Question 2
- Question: Does the effectiveness of multilingual training for attribute controllers depend on the specific architecture or pretraining approach of the backend model?
- Basis in paper: [inferred] The authors observe multilingual controllers help when the base model is not massively multilingual, but not for NLLB-200
- Why unresolved: The observation is based on comparing NLLB-200 to a randomly initialized Transformer-base, without testing other architectures
- What evidence would resolve it: Testing multilingual controllers on other pretrained models with different architectures and pretraining approaches

### Open Question 3
- Question: What is the optimal trade-off between inference speed and control accuracy when tuning the classifier guidance hyperparameters?
- Basis in paper: [explicit] The authors acknowledge slow decoding speed as a main limitation and note sensitivity to hyperparameters
- Why unresolved: The paper uses fixed hyperparameters without systematic exploration of the speed-accuracy trade-off
- What evidence would resolve it: Comprehensive ablation studies varying classifier guidance hyperparameters while measuring both speed and accuracy metrics

## Limitations
- Slow decoding speed (6.5x slower) due to repeated gradient updates in classifier guidance
- Hyperparameter sensitivity affecting both performance and computational efficiency
- Limited human evaluation scope (one language pair, two attributes)
- No direct validation of language-agnostic feature representations in decoder states

## Confidence
- **High Confidence**: Classifier guidance generalizes better than finetuning to distant target languages, with consistent zero-shot transfer accuracy patterns
- **Medium Confidence**: Finetuning preserves knowledge on other languages under specific training conditions; complementary nature of CG and FT demonstrated but mechanism not fully explored
- **Low Confidence**: Claims about why classifier guidance works (pretrained multilingual representations) lack direct validation and feature analysis

## Next Checks
1. Conduct hyperparameter ablation studies for classifier guidance across multiple language pairs to identify sensitivity patterns and optimal configurations
2. Perform probing experiments to identify and validate specific language-agnostic features in decoder hidden states that correlate with attribute control signals
3. Expand human evaluation to include multiple language families, additional attributes beyond formality and gender, and more comprehensive quality assessments