---
ver: rpa2
title: On Single Index Models beyond Gaussian Data
arxiv_id: '2307.15804'
source_url: https://arxiv.org/abs/2307.15804
tags:
- have
- gaussian
- data
- where
- beyond
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning single-index models
  $f(x) = \phi(x \cdot \theta^)$ in the high-dimensional regime where the link function
  $\phi$ is known but the direction $\theta^$ is unknown. Unlike prior work focused
  on Gaussian data, the authors extend the analysis to non-Gaussian data distributions.
---

# On Single Index Models beyond Gaussian Data

## Quick Facts
- arXiv ID: 2307.15804
- Source URL: https://arxiv.org/abs/2307.15804
- Reference count: 40
- This paper extends single-index model learning beyond Gaussian data using SGD under a local polynomial growth condition

## Executive Summary
This paper studies learning single-index models f(x) = φ(x·θ*) in high dimensions where the link function φ is known but the direction θ* is unknown. Unlike prior work focused on Gaussian data, the authors extend the analysis to non-Gaussian data distributions. Their main contribution is showing that Stochastic Gradient Descent can efficiently recover θ* under a new local polynomial growth (LPG) property of the loss landscape, which holds for both spherically symmetric and approximately symmetric distributions.

The LPG condition depends on the information exponent of φ and the distance of the data distribution to Gaussianity. The authors prove weak and strong recovery guarantees for SGD, with sample complexity matching the Gaussian case up to polylog factors. Experiments validate the theory and suggest that the results may be tight for the non-symmetric case, as the polynomial dependency on dimension in Proposition 4.14 might be unavoidable.

## Method Summary
The paper analyzes SGD on the sphere for learning single-index models with non-Gaussian data. The method uses spherical gradient updates with step size dependent on the information exponent s of the link function. For symmetric distributions, the loss admits a scalar summary statistic m = θ·θ* represented in terms of Gegenbauer polynomials, enabling LPG analysis. For approximately symmetric distributions close to Gaussian in Wasserstein distance, the authors show LPG holds for s ≤ 2. The approach relies on initialization near the equator (b ~ 1/√d) and exploits concentration of measure phenomena in high dimensions.

## Key Results
- SGD can recover θ* under Local Polynomial Growth (LPG) conditions for both spherically symmetric and approximately symmetric distributions
- For symmetric distributions, LPG holds for any information exponent s based on the Gegenbauer polynomial spectrum
- For approximately symmetric distributions, LPG holds for s ≤ 2 when the Wasserstein distance to Gaussian is O(1/√d)
- Sample complexity matches the Gaussian case up to polylog factors, with weak recovery requiring O(d log d / (1-m²)²) samples and strong recovery requiring O(d log d / (1-m)²) samples

## Why This Works (Mechanism)

### Mechanism 1
SGD can recover the direction θ* even when data are non-Gaussian, provided the loss landscape satisfies Local Polynomial Growth (LPG) of appropriate order. Under LPG(s,b), the spherical gradient projected onto the signal direction grows as (1-m)^(s-1) near the equator, ensuring weak recovery before strong recovery can occur. This relies on initialization being near the equator with b ~ 1/√d and the local polynomial expansion of the loss landscape.

### Mechanism 2
Spherical symmetry allows extension of Gaussian techniques via Gegenbauer polynomial decomposition, yielding LPG for a broad class of link functions. For symmetric ν, the loss can be written in terms of Gegenbauer polynomials; the spectrum β_j,d controls whether LPG holds, with s being the index of the first non-zero coefficient. This reduction to a one-dimensional function of m = θ·θ* enables the analysis.

### Mechanism 3
When spherical symmetry is lost but the distribution is close to Gaussian in Wasserstein distance, a perturbed landscape still allows SGD recovery for s ≤ 2. The projected Wasserstein distance fW₁,₂(ν,γ) controls the perturbation size; if small, the loss gradient is close to the Gaussian case, preserving LPG. This extends the framework to approximately symmetric distributions while maintaining recovery guarantees.

## Foundational Learning

- **High-dimensional geometry and concentration of measure**: Needed for understanding why initialization near the equator is typical in high dimensions with high probability. Quick check: What is the typical scale of the correlation m_θ for a random initialization on S^(d-1) as d grows?

- **Spherical harmonics and Gegenbauer polynomials**: Essential for the symmetric case analysis where the loss is expanded in Gegenbauer polynomials. Quick check: How does the largest root z_j,d of the Gegenbauer polynomial scale with j and d?

- **Information exponent and Hermite/Gegenbauer expansions**: Controls the flatness of the loss near the equator and thus the sample complexity. Quick check: What is the relationship between the number of vanishing moments of the link function and the information exponent?

## Architecture Onboarding

- **Component map**: Data generation -> Loss computation -> Spherical gradient calculation -> SGD update -> Correlation tracking -> Recovery check
- **Critical path**: Initialize θ₀ → compute loss gradient → update θ → check correlation m → iterate until m ≈ 1
- **Design tradeoffs**: LPG provides theoretical guarantees but may be hard to verify in practice; relaxing to s ≤ 2 under Wasserstein proximity trades generality for robustness
- **Failure signatures**: If SGD stalls near the equator (m ~ 0) for long periods, LPG may not hold; if loss increases, gradient computation or model specification is wrong
- **First 3 experiments**:
  1. Implement SGD for Gaussian data with known link function; verify weak/strong recovery matches theory
  2. Replace Gaussian data with uniform on sphere; check if recovery still occurs and matches LPG predictions
  3. Use product measure close to Gaussian (e.g., uniform on hypercube) with s=2 link; measure recovery rate vs. Wasserstein distance

## Open Questions the Paper Calls Out

### Open Question 1
Can the polynomial dependency on dimension (specifically the √(log d) term) in Proposition 4.14 be removed for non-symmetric distributions? The authors note this is an open question, stating that establishing whether this polynomial dependency is an inherent cost of symmetry breaking is an interesting question for future work. The current proof technique using projected Wasserstein distance appears to inherently introduce this polynomial factor.

### Open Question 2
Does Conjecture 4.18 hold true - will SGD avoid sparse points and thus escape mediocrity in the non-symmetric product measure setting? This requires showing that ||θ_t||_4^4 remains small throughout training, which depends on controlling correlations between successive iterates in a way not captured by current techniques.

### Open Question 3
Can the LPG property be extended to semi-parametric learning when the link function φ is unknown? The current LPG framework assumes knowledge of φ, and extending it to joint estimation of both θ* and φ would require new techniques to handle the additional statistical complexity.

## Limitations
- The LPG extension to approximately symmetric distributions requires restrictive assumptions about the information exponent (s ≤ 2) and Wasserstein distance
- Empirical validation is limited to low-dimensional settings and specific distributions, with scaling to truly high-dimensional regimes untested
- The initialization requirement of b ~ 1/√d is critical but may be challenging to achieve in practical noisy or constrained settings

## Confidence

- **High confidence**: SGD recovery guarantees under exact spherical symmetry (Mechanisms 1 and 2)
- **Medium confidence**: LPG extension to approximately symmetric distributions (Mechanism 3)
- **Low confidence**: Tightness of the s ≤ 2 bound for approximate symmetry

## Next Checks

1. Compute fW₁,₂(ν,γ) explicitly for common non-Gaussian distributions (uniform on sphere, product measures) to verify the O(1/√d) scaling
2. For s > 2 link functions, test whether SGD fails in practice and whether recovery improves with larger d, indicating potential tightness of the s ≤ 2 bound
3. Implement the algorithm in d = 100-1000 dimensions to check if sample complexity scales as predicted and if initialization at b ~ 1/√d is practically achievable