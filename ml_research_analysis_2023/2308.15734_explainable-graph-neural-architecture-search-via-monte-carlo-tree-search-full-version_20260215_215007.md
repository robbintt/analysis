---
ver: rpa2
title: Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search (Full
  version)
arxiv_id: '2308.15734'
source_url: https://arxiv.org/abs/2308.15734
tags:
- search
- graph
- architecture
- graphs
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing and selecting optimal
  graph neural network (GNN) architectures for diverse graphs. The proposed method,
  ExGNAS, uses a simple search space that includes fundamental GNN components and
  a search algorithm based on Monte-Carlo tree search without neural models.
---

# Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search (Full version)

## Quick Facts
- arXiv ID: 2308.15734
- Source URL: https://arxiv.org/abs/2308.15734
- Reference count: 40
- Key outcome: Proposed ExGNAS achieves up to 26.1% accuracy improvement and 88% runtime reduction compared to state-of-the-art Graph NAS methods on both homophilic and heterophilic graphs.

## Executive Summary
This paper introduces ExGNAS, an explainable Graph Neural Architecture Search (Graph NAS) method that uses Monte-Carlo Tree Search (MCTS) to find optimal GNN architectures without neural models. The approach combines a simple search space of fundamental GNN components (MLP, activation functions, JKNet) with an MCTS-based search algorithm, achieving high accuracy on both homophilic and heterophilic graphs while maintaining interpretability. The method demonstrates significant performance improvements over existing Graph NAS methods, improving accuracy by up to 26.1% and reducing runtime by up to 88%.

## Method Summary
ExGNAS uses a two-part approach: a search space containing fundamental GNN components (MLP, activation functions, JKNet) and a Monte-Carlo Tree Search algorithm for architecture selection. The search space is designed to be simple and interpretable, generating non-complex GNN architectures. The MCTS algorithm selects architectures based on an Upper Confidence Bound (UCB) formula that balances exploration and exploitation. The method trains and evaluates each generated architecture, updating the MCTS tree with performance metrics, and outputs the best architecture along with an analysis of component importance through the MCTS tree structure.

## Key Results
- ExGNAS improves accuracy by up to 26.1% compared to state-of-the-art Graph NAS methods
- Runtime is reduced by up to 88% compared to neural model-based search methods
- Ablation studies show PreMLP and PreJKNet components are crucial for heterophilic graph performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of simple search space and MCTS algorithm enables both high accuracy and explainability without neural models.
- Mechanism: By using fundamental GNN components and MCTS-based selection based on average validation performance, the method avoids complexity and opacity of neural model-based search algorithms.
- Core assumption: Fundamental components without state-of-the-art complex techniques can achieve competitive performance on both homophilic and heterophilic graphs.
- Evidence anchors: [abstract] "The combination of our search space and algorithm achieves finding accurate GNN models and the important functions within the search space."

### Mechanism 2
- Claim: The MCTS algorithm with UCB-based node selection efficiently finds globally optimal architectures.
- Mechanism: The UCB score balances exploration and exploitation by considering both average performance and selection frequency, allowing the search to avoid local optima while efficiently exploring the architecture space.
- Core assumption: The UCB-based selection strategy can effectively guide the search through a space with over 20 million possible architectures.
- Evidence anchors: [section 3.2] "We select the leaf MCT node ùëñ that has the maximum ùë¢ùëêùëè of the following equation" with detailed UCB formula.

### Mechanism 3
- Claim: The simple search space design inherently favors interpretable and efficient architectures.
- Mechanism: By excluding complex state-of-the-art layers and focusing on fundamental building blocks, the generated architectures are inherently simpler and more interpretable, while still being effective through the flexible macro-architecture design.
- Core assumption: Simpler architectures generated from fundamental components can achieve competitive performance without sacrificing accuracy.
- Evidence anchors: [section 3.1] "This search space generates non-complex GNN architectures that help to understand what components are effective in the given graph."

## Foundational Learning

- Concept: Graph Neural Networks and their limitations on heterophilic graphs
  - Why needed here: Understanding why standard GNNs fail on heterophilic graphs is crucial for designing effective search spaces and interpreting results.
  - Quick check question: What is the key difference between homophilic and heterophilic graphs that affects GNN performance?

- Concept: Neural Architecture Search (NAS) methodologies and tradeoffs
  - Why needed here: The paper contrasts different NAS approaches (reinforcement learning, evolutionary algorithms, differentiable search) to justify the MCTS choice.
  - Quick check question: What are the main advantages and disadvantages of using neural models in NAS algorithms?

- Concept: Monte-Carlo Tree Search and Upper Confidence Bound (UCB) selection
  - Why needed here: The MCTS algorithm with UCB-based node selection is the core search mechanism that needs to be understood for implementation and modification.
  - Quick check question: How does the UCB formula balance exploration and exploitation in the context of architecture search?

## Architecture Onboarding

- Component map:
  - Search Space: Micro-architecture (attention, activation, embedding size) + Macro-architecture (JKNet, PreMLP, PostMLP, PreJKNet)
  - Search Algorithm: MCTS with UCB-based node selection, tree update mechanism
  - Output: Best architecture, parameter importance analysis via MCTS tree

- Critical path:
  1. Initialize MCTS tree with root node
  2. Select leaf node using UCB formula
  3. Fix architecture parameters along path, randomize others
  4. Train and evaluate GNN
  5. Update MCTS tree with performance
  6. Repeat until convergence or max iterations
  7. Output best architecture and MCTS tree for analysis

- Design tradeoffs:
  - Simple vs. complex search space: Simpler spaces are more interpretable but may miss optimal architectures
  - MCTS vs. neural model-based search: MCTS is more explainable but may be less efficient at finding optimal architectures
  - Fixed vs. adaptive hyper-parameters: Fixed parameters reduce tuning effort but may not be optimal for all datasets

- Failure signatures:
  - Low accuracy despite extensive search: Search space may be too restrictive or MCTS not effectively exploring
  - Extremely long runtime: UCB parameters or tree update frequency may need adjustment
  - Poor interpretability: Architecture parameters may not be well-designed or MCTS tree not properly constructed

- First 3 experiments:
  1. Run on a small, simple graph (e.g., Cora) to verify basic functionality and understand MCTS tree construction
  2. Compare performance with and without PreMLP/PreJKNet components to validate their importance for heterophilic graphs
  3. Test different UCB constant values (c) to find optimal exploration-exploitation balance for the given search space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Monte-Carlo tree search algorithm's exploration-exploitation balance (parameter c) impact the trade-off between finding optimal architectures and computational efficiency?
- Basis in paper: [explicit] The paper mentions using c = sqrt(2) as the default and shows this value has a small impact on accuracy performance.
- Why unresolved: While the paper demonstrates that c is not sensitive to accuracy, it does not explore how different values of c affect the balance between exploration and exploitation, or the computational cost of the search process.

### Open Question 2
- Question: How does the performance of ExGNAS compare to Graph NAS methods that include hyperparameter optimization in the search space?
- Basis in paper: [inferred] The paper focuses on architecture search with fixed hyperparameters, while noting that some methods like GraphGym and AutoHeG include hyperparameter optimization.
- Why unresolved: The paper compares ExGNAS to methods with fixed hyperparameters but does not explore how including hyperparameter optimization in the search space might affect performance.

### Open Question 3
- Question: How does the explainability of ExGNAS compare to other Graph NAS methods in terms of user understanding and trust in the selected architectures?
- Basis in paper: [explicit] The paper emphasizes the explainability of ExGNAS through the use of Monte-Carlo tree search without neural models, allowing for analysis of important components.
- Why unresolved: While the paper demonstrates the effectiveness of ExGNAS in terms of accuracy and efficiency, it does not provide a direct comparison of explainability with other Graph NAS methods from a user perspective.

## Limitations
- The search space may be too restrictive to capture optimal architectures for certain graph types
- The MCTS parameters were not extensively tuned, potentially limiting performance
- Explainability benefits were asserted but not empirically validated through human studies

## Confidence
- Performance improvements (accuracy, runtime): High
- Explainability claims: Medium
- Component importance analysis: Medium-High

## Next Checks
1. **Search Space Coverage Test**: Systematically evaluate whether the current search space can represent known effective architectures for heterophilic graphs by including state-of-the-art heterophily-specific layers and comparing performance.

2. **Explainability Validation**: Conduct human studies or apply established interpretability metrics to quantify the actual interpretability benefits of the simple architectures versus complex alternatives.

3. **Robustness to Hyperparameters**: Perform sensitivity analysis on the MCTS parameters (UCB constant c, exploration factor Œ∏) across different graph types to determine if the current defaults are near-optimal.