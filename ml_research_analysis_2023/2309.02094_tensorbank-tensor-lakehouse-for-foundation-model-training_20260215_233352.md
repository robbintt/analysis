---
ver: rpa2
title: 'TensorBank: Tensor Lakehouse for Foundation Model Training'
arxiv_id: '2309.02094'
source_url: https://arxiv.org/abs/2309.02094
tags:
- data
- tensor
- tensors
- storage
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TensorBank, a petabyte-scale tensor lakehouse
  designed for streaming tensors from Cloud Object Store (COS) to GPU memory at wire
  speed using complex relational queries. The architecture employs Hierarchical Statistical
  Indices (HSI) for query acceleration, enabling direct addressing of tensors at the
  block level using HTTP range reads.
---

# TensorBank: Tensor Lakehouse for Foundation Model Training

## Quick Facts
- arXiv ID: 2309.02094
- Source URL: https://arxiv.org/abs/2309.02094
- Reference count: 32
- Key outcome: Petabyte-scale tensor lakehouse streaming tensors from Cloud Object Store to GPU memory at wire speed using Hierarchical Statistical Indices for query acceleration

## Executive Summary
TensorBank is a petabyte-scale tensor lakehouse designed to stream tensors from Cloud Object Store to GPU memory at wire speed using complex relational queries. The architecture employs Hierarchical Statistical Indices (HSI) to enable selective data retrieval by skipping irrelevant blocks without reading them. Once tensors are in GPU memory, they can be transformed using PyTorch transforms. The system provides a generic PyTorch dataset type with a corresponding dataset factory for translating relational queries and requested transformations into dataset instances.

## Method Summary
TensorBank uses ZARR format for storing tensor data with metadata enabling direct addressing at the block level using HTTP range reads. Xarray provides the interface for element indexing and filtering using relational algebra semantics. The core innovation is Hierarchical Statistical Indices (HSI), which store domain-specific and non-specific statistics at different hierarchical resolution levels to enable content-based filtering and de-biasing. The Streaming Tensor Sampler (STS) is a PyTorch dataset factory that takes query parameters and HSI statistics to return parameterized dataset instances for foundation model training.

## Key Results
- Achieved 50 GBit/s link saturation using 10 parallel threads, delivering 762.5 tensors per second (~6.1 GB/s) in HPC data center
- Achieved 25 GBit/s link saturation using 128 threads, delivering 387.5 tensors per second (~3.1 GB/s) on AWS
- Achieved 10 GBit/s link saturation using 64 threads, delivering 137.5 tensors per second (~1.1 GB/s) on IBM CodeEngine
- Demonstrated linear scaling up to 16 instances in scale-out tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical Statistical Indices (HSI) enable selective data retrieval by allowing the system to skip irrelevant blocks without reading them.
- Mechanism: HSI stores domain-specific and non-specific statistics at different hierarchical resolution levels. These statistics are computed on grouped subtensors at various resolutions (e.g., min, max, mean, standard deviation, percentage of cloud/ocean/land pixels). During query execution, the system can evaluate these statistics to determine whether a block is relevant, thus avoiding unnecessary data transfer.
- Core assumption: Statistical summaries at multiple resolutions accurately represent the underlying data distribution and allow meaningful filtering decisions without accessing raw data.
- Evidence anchors:
  - [abstract] "By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels."
  - [section] "To avoid reading of a tensor just to decide on its removal, we introduce Hierarchical Statistical Indices (HSI)...sub-tensors of different hierarchical resolutions are grouped together and a set of domain specific statistics are computed."
  - [corpus] Weak correlation - corpus contains papers about tensor storage and GPU memory but lacks direct discussion of hierarchical statistical indices or block-level skipping mechanisms.

### Mechanism 2
- Claim: Direct addressing of tensors at the block level using HTTP range reads enables wire-speed streaming from cloud object storage to GPU memory.
- Mechanism: ZARR format organizes tensor data into chunks with metadata that allows computing exact byte offsets for any tensor or subtensor. The HTTP range read functionality on COS endpoints enables requesting only the specific byte ranges needed, eliminating the need to download entire files. This direct addressing combined with parallel threads saturates network links.
- Core assumption: The storage system supports efficient HTTP range reads and the metadata overhead is negligible compared to the data transfer benefits.
- Evidence anchors:
  - [abstract] "Our architecture allows to directly address tensors on block level using HTTP range reads."
  - [section] "Once in GPU memory, data can be transformed using PyTorch transforms... By using the HTTP range read functionality available on COS endpoints based on index and tensor-shape the exact byte arrays can be requested directly from COS."
  - [corpus] Weak correlation - corpus mentions GPU memory and storage but doesn't specifically discuss HTTP range reads or direct block addressing for tensor data.

### Mechanism 3
- Claim: The Streaming Tensor Sampler (STS) automates de-biased sampling of tensors for foundation model training while maintaining query performance.
- Mechanism: STS is a PyTorch dataset factory that takes query parameters and HSI statistics to return a parameterized dataset instance. It uses the HSI to identify and sample tensors based on domain-specific criteria (e.g., cloud coverage, land type) while maintaining desired class ratios. This integrates seamlessly with PyTorch's DataLoader for batching, shuffling, and prefetching.
- Core assumption: The HSI statistics are sufficient to make intelligent sampling decisions that improve model training quality without requiring custom code for each use case.
- Evidence anchors:
  - [abstract] "By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels."
  - [section] "We have created the Streaming Tensor Sampler (STS) which consists of a generic PyTorch dataset type and a corresponding dataset factory which takes query parameters as input and returns a parameterized dataset instance."
  - [corpus] Weak correlation - corpus contains papers about tensor operations and GPU memory but lacks specific discussion of automated sampling mechanisms for foundation models.

## Foundational Learning

- Concept: ZARR data format and chunk-based storage
  - Why needed here: Understanding how ZARR organizes tensor data into chunks with metadata is fundamental to grasping how TensorBank achieves direct addressing and efficient range reads.
  - Quick check question: How does ZARR's chunk-based organization differ from traditional file storage, and why is this important for TensorBank's architecture?

- Concept: Hierarchical Statistical Indices (HSI) and multi-resolution statistics
  - Why needed here: HSI is the core innovation that enables selective data retrieval. Understanding how statistics are computed at different resolutions and how they enable filtering decisions is crucial.
  - Quick check question: What types of statistics are typically computed in HSI for different data modalities (EO imagery, atmospheric data, fMRI), and how do these statistics enable filtering?

- Concept: PyTorch Dataset and DataLoader patterns
  - Why needed here: The STS integrates with PyTorch's data loading ecosystem. Understanding how custom datasets work with DataLoader is essential for implementing and extending TensorBank.
  - Quick check question: How does a custom PyTorch Dataset integrate with DataLoader, and what are the key methods that need to be implemented for TensorBank's STS?

## Architecture Onboarding

- Component map: Storage Layer (Cloud Object Store with ZARR) -> Metadata Layer (HSI with multi-resolution statistics) -> Query Engine (Xarray with DASK) -> Sampling Layer (STS with PyTorch Dataset factory) -> Transformation Layer (PyTorch transforms) -> Interface Layer (HTTP range reads)

- Critical path: Query → HSI evaluation → HTTP range read → GPU memory → PyTorch transforms → Model training

- Design tradeoffs:
  - Storage vs. Query Performance: Computing and storing HSI statistics adds storage overhead but enables significant query acceleration
  - Granularity vs. Overhead: Finer HSI resolution provides better filtering but increases metadata size and computation cost
  - Parallelism vs. Resource Utilization: More parallel threads improve throughput but increase resource consumption and potential contention

- Failure signatures:
  - Slow query performance: Check HSI statistics accuracy, network bandwidth utilization, and DASK cluster configuration
  - Incorrect data filtering: Verify HSI statistics computation and query parameter mapping
  - Memory issues: Monitor GPU memory usage during transforms and adjust batch sizes
  - Storage bottlenecks: Check COS performance metrics and HTTP range read efficiency

- First 3 experiments:
  1. Basic HSI computation: Create HSI for a small ZARR dataset with simple statistics (min, max, mean) and verify that queries can filter data correctly without reading entire blocks.
  2. HTTP range read validation: Test direct addressing by requesting specific byte ranges from COS for known tensor locations and measure transfer speeds against full file downloads.
  3. STS integration: Implement a simple STS instance for a test dataset and verify it integrates correctly with PyTorch DataLoader, including batching and shuffling functionality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of parallel threads to saturate different network bandwidth configurations (e.g., 10, 25, 50, 100+ Gbit/s) across various storage systems and cloud providers?
- Basis in paper: [explicit] The paper describes experiments showing different thread counts required to saturate different network links (10 threads for 50 Gbit/s HPC, 128 threads for 25 Gbit/s AWS, 64 threads for 10 Gbit/s CodeEngine)
- Why unresolved: The relationship between thread count and network saturation appears non-linear and varies significantly by infrastructure. The paper only tests three specific configurations without establishing a generalizable model or formula.
- What evidence would resolve it: A comprehensive study testing multiple thread counts across various network speeds, storage systems, and cloud providers to establish optimal thread-to-bandwidth ratios and identify the factors affecting this relationship.

### Open Question 2
- Question: How does Hierarchical Statistical Index (HSI) construction overhead scale with increasing tensor dimensions and data volume, and what is the break-even point where HSI benefits outweigh construction costs?
- Basis in paper: [explicit] The paper describes HSI creation at different hierarchical resolutions but doesn't analyze the computational overhead of building these indices or compare it to the query performance gains
- Why unresolved: While HSI provides query acceleration benefits, the paper doesn't quantify the indexing phase costs or determine when the investment in HSI construction becomes worthwhile for different dataset sizes and query patterns.
- What evidence would resolve it: Empirical measurements of HSI construction time across datasets of varying sizes and dimensions, benchmarked against query performance improvements to identify threshold points.

### Open Question 3
- Question: What is the impact of HTTP range read overhead on tensor streaming performance compared to alternative protocols, and how can this overhead be minimized?
- Basis in paper: [inferred] The paper notes that "significantly more threads are necessary to saturate the storage network connection" on AWS compared to HPC, with the authors speculating that "HTTP imposes some significant overhead and latency"
- Why unresolved: The paper observes performance differences between infrastructure types but doesn't isolate or quantify the specific contribution of HTTP protocol overhead versus other factors like storage system architecture or network configuration.
- What evidence would resolve it: Controlled experiments comparing HTTP range reads with alternative protocols (e.g., native object store APIs, RDMA, GPFS protocols) under identical network conditions to measure and characterize protocol-specific overhead.

## Limitations
- The paper does not address data consistency, versioning, or concurrent write handling in the lakehouse
- Scope is limited to read-heavy training scenarios, not covering iterative data exploration or on-the-fly augmentation
- HTTP range read dependency limits portability to storage systems without this feature

## Confidence
High confidence in the core architectural design using ZARR with HTTP range reads and PyTorch integration
Medium confidence in the Hierarchical Statistical Indices mechanism effectiveness
Medium confidence in performance claims due to limited scale and specific infrastructure dependency

## Next Checks
1. **HSI Accuracy Validation**: Create a controlled test with synthetic tensor data containing known patterns and validate that HSI statistics correctly identify and filter relevant blocks. Measure false positive and false negative rates across different statistical configurations and data modalities.

2. **End-to-end Performance Under Load**: Deploy TensorBank on a representative foundation model training workload with concurrent users and varying query patterns. Measure sustained throughput, latency percentiles, and resource utilization under realistic operational conditions including data updates and schema evolution.

3. **Storage System Compatibility Testing**: Test TensorBank's HTTP range read functionality across different cloud storage providers and on-premises object stores. Document performance degradation and identify the minimum viable feature set required for functional but suboptimal operation.