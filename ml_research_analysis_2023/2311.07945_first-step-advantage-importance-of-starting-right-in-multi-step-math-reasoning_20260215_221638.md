---
ver: rpa2
title: 'First-Step Advantage: Importance of Starting Right in Multi-Step Math Reasoning'
arxiv_id: '2311.07945'
source_url: https://arxiv.org/abs/2311.07945
tags:
- guidance
- student
- reasoning
- llama
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Guiding smaller models in multi-step reasoning can significantly
  improve their performance. The key insight is that smaller models often fail to
  reason correctly because they get the first step wrong, and errors cascade from
  there.
---

# First-Step Advantage: Importance of Starting Right in Multi-Step Math Reasoning

## Quick Facts
- arXiv ID: 2311.07945
- Source URL: https://arxiv.org/abs/2311.07945
- Reference count: 8
- Primary result: First-step guidance from larger models improves smaller model performance by up to +24 points on GSM8K

## Executive Summary
This paper demonstrates that smaller language models often fail at multi-step mathematical reasoning primarily because they get the first step wrong, causing errors to cascade through subsequent steps. The key insight is that early intervention by a larger "teacher" model providing guidance on the initial reasoning step can dramatically improve performance. GPT-4 provides nearly as effective guidance as human teachers, and the effect is consistent across multiple datasets and model sizes. The approach shows that improving reasoning accuracy doesn't always require larger models—sometimes all that's needed is expert guidance at the critical first step.

## Method Summary
The method involves using larger teacher models (including GPT-3.5-turbo, GPT-4, and various LLaMA variants) to provide first-step guidance to smaller student models (primarily LLaMA 7B). Student models are first fine-tuned on GSM8K for 3 epochs, then evaluated with greedy decoding. For first-step guidance, teacher models generate only the initial reasoning step for each problem, which is then used as a prompt prefix for the student model. Performance is measured using accuracy (maj@1) on test sets across multiple mathematical reasoning datasets including GSM8K, SVAMP, ASDiv, and MultiArith.

## Key Results
- First-step guidance improves GSM8K accuracy by up to +24 points for 7B models
- Larger teacher models provide more effective guidance than smaller ones
- GPT-4 performs nearly as well as human teachers in providing guidance
- Intervention at the first step yields maximum benefits, with diminishing returns for later steps
- Effect is consistent across multiple datasets and model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models fail primarily due to incorrect first steps, which propagate errors through subsequent reasoning steps.
- Mechanism: Early intervention by a larger model corrects the initial reasoning step, preventing error cascade and enabling correct solution path.
- Core assumption: The majority of reasoning errors in multi-step problems stem from initial misinterpretation rather than inability to execute subsequent steps.
- Evidence anchors: [abstract] "smaller models often fail to reason correctly because they get the first step wrong, and errors cascade from there"; [section] "Our observations, as shown in Figure 1, suggest that for multi-step reasoning tasks, intervening at the first step yields the most significant benefits, with the effects diminishing at subsequent steps"
- Break condition: If the error source shifts from initial step to execution errors in later steps, first-step guidance loses effectiveness.

### Mechanism 2
- Claim: Expert teachers (larger or higher-performing models) provide more effective guidance than less capable models.
- Mechanism: Teacher model capability directly correlates with guidance quality, leading to better student performance.
- Core assumption: Model performance and reasoning capability scale predictably with model size and training.
- Evidence anchors: [abstract] "Larger models can provide this guidance effectively, with GPT-4 performing nearly as well as human teachers"; [section] "As expected, expert teachers (models with superior performance) tend to provide more effective guidance, resulting in better student performance"
- Break condition: If model scaling laws break down or if guidance quality plateaus despite model size increases.

### Mechanism 3
- Claim: Early intervention is more effective than later correction in multi-step reasoning tasks.
- Mechanism: Correcting errors at later steps is less effective because students have already internalized incorrect approaches.
- Core assumption: Error propagation follows a cascade pattern where early errors compound through subsequent steps.
- Evidence anchors: [abstract] "Early intervention is crucial; correcting later steps yields diminishing returns"; [section] "Figure 2 compares the effect of intervention by humans as teachers for LLaMA 7B student model on the GSM8K dataset. Intervention at the first step (in blue) proves to be most beneficial with maximum gains over baseline without any interventions (dotted line)"
- Break condition: If the error propagation pattern changes or if models develop robust error detection and correction mechanisms.

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: Understanding how models break down complex problems into sequential reasoning steps is fundamental to grasping why first-step errors are so impactful
  - Quick check question: If a model correctly executes all steps but starts with the wrong initial interpretation, will it reach the correct answer?

- Concept: Knowledge distillation
  - Why needed here: The paper builds on existing knowledge distillation approaches but adds the interactive teacher-student dynamic element
  - Quick check question: How does interactive guidance differ from passive knowledge distillation in terms of student model performance?

- Concept: Error propagation in sequential reasoning
  - Why needed here: Understanding how errors compound through sequential reasoning steps explains why early intervention is crucial
  - Quick check question: If a model makes an error in step 3 but corrects it in step 4, is it as effective as preventing the error in step 1?

## Architecture Onboarding

- Component map: Teacher model (expert) → First-step guidance generation → Student model (smaller) → Multi-step reasoning → Answer generation
- Critical path: Teacher provides first-step correction → Student executes remaining steps → Final answer
- Design tradeoffs: Model size vs. guidance quality, intervention timing vs. computational cost, generalization vs. task-specific tuning
- Failure signatures: No performance improvement despite guidance, performance degradation after guidance, inconsistent guidance quality
- First 3 experiments:
  1. Compare baseline student performance vs. first-step guidance from different teacher model sizes
  2. Test intervention timing (step 1, step 2, step 3) to validate diminishing returns hypothesis
  3. Evaluate cross-dataset generalization of first-step guidance approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of first-step guidance vary across different types of reasoning tasks beyond mathematical word problems?
- Basis in paper: [explicit] The paper demonstrates first-step guidance effectiveness on mathematical reasoning tasks (GSM8K, SVAMP, ASDiv, MultiArith) but does not explore other reasoning domains
- Why unresolved: The paper focuses exclusively on mathematical reasoning tasks without investigating whether the first-step advantage generalizes to other domains like logical reasoning, scientific reasoning, or commonsense reasoning
- What evidence would resolve it: Experiments applying first-step guidance to diverse reasoning tasks across multiple domains, comparing the magnitude of performance improvements across task types

### Open Question 2
- Question: What is the optimal timing and frequency of teacher interventions during multi-step reasoning tasks?
- Basis in paper: [inferred] The paper shows that first-step intervention is most effective, with diminishing returns for later interventions, but does not systematically explore different intervention strategies or frequencies
- Why unresolved: While the paper demonstrates that early intervention is crucial, it does not investigate whether multiple interventions at strategic points could be more effective than a single first-step intervention, or whether the optimal intervention strategy varies by task complexity
- What evidence would resolve it: Controlled experiments testing various intervention schedules (single vs. multiple interventions, different timing patterns) across tasks of varying complexity

### Open Question 3
- Question: How can first-step guidance be effectively integrated into the training process rather than just applied during inference?
- Basis in paper: [explicit] The paper mentions that curriculum-style training (first-step then next-steps) performs worse than standard fine-tuning, suggesting current integration methods are suboptimal
- Why unresolved: The paper demonstrates the effectiveness of first-step guidance during inference but shows that attempts to incorporate this approach into training lead to decreased performance, indicating a gap in understanding how to effectively train models to generate good first steps
- What evidence would resolve it: Development and evaluation of training methods that successfully incorporate first-step guidance principles, showing improved performance over standard fine-tuning approaches

## Limitations
- GSM8K dataset represents a narrow slice of mathematical reasoning tasks
- Mechanism by which teacher models generate effective first-step guidance isn't fully explored
- Long-term effects of intervention strategy not investigated (dependency vs. capability development)

## Confidence

**High confidence**: The empirical observation that first-step intervention improves smaller model performance (GSM8K +24 points) is well-supported by experimental results. The diminishing returns of later-stage intervention is also convincingly demonstrated.

**Medium confidence**: The claim that first-step errors are the primary cause of failure for smaller models is reasonable given the evidence but could benefit from additional analysis of error patterns across different problem types.

**Low confidence**: The generalization claim that this approach will work equally well across diverse mathematical domains and reasoning tasks is not adequately tested. The mechanism by which first-step guidance specifically improves outcomes could be more rigorously analyzed.

## Next Checks
1. Cross-domain validation: Test the first-step guidance approach on non-GSM8K mathematical reasoning datasets (e.g., MATH, MultiArith) to assess generalization beyond grade-school word problems.

2. Error attribution analysis: Conduct a detailed error analysis categorizing failure modes to determine whether first-step errors truly dominate across all problem types, or if different categories of problems have different primary failure modes.

3. Teacher model scaling analysis: Systematically test the relationship between teacher model size/capability and guidance quality to identify whether there's a point of diminishing returns or if the scaling relationship breaks down for certain teacher model sizes.