---
ver: rpa2
title: Large Language Models Leverage External Knowledge to Extend Clinical Insight
  Beyond Language Boundaries
arxiv_id: '2305.10163'
source_url: https://arxiv.org/abs/2305.10163
tags:
- medical
- knowledge
- question
- arxiv
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying large language models
  (LLMs) to non-English medical question-answering tasks, where English-centric models
  like ChatGPT struggle due to limited training data in other languages. The authors
  propose a novel Knowledge and Few-shot Enhancement In-context Learning (KFE) framework
  that integrates diverse external clinical knowledge sources through retrieval-based
  prompts.
---

# Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries

## Quick Facts
- arXiv ID: 2305.10163
- Source URL: https://arxiv.org/abs/2305.10163
- Reference count: 40
- Key outcome: KFE framework enables LLMs to pass Chinese medical licensing exam (score 82.59) when English-centric models fail without enhancement

## Executive Summary
This paper addresses the challenge of applying large language models to non-English medical question-answering tasks, where English-centric models like ChatGPT struggle due to limited training data in other languages. The authors propose a novel Knowledge and Few-shot Enhancement In-context Learning (KFE) framework that integrates diverse external clinical knowledge sources through retrieval-based prompts. They evaluate KFE on the China National Medical Licensing Examination (CNMLE) using ChatGPT (GPT3.5 and GPT4) and Baichuan2 models. The results show that directly applying ChatGPT without enhancement scores only 51, failing to qualify. However, KFE significantly improves performance, with ChatGPT reaching 70.04 and GPT-4 achieving the highest score of 82.59, surpassing both the qualification threshold (60) and the average human score (68.70). Even the smaller Baichuan2-13B model qualifies with KFE, demonstrating its effectiveness in low-resource settings. The study demonstrates that synergizing medical knowledge through in-context learning enables LLMs to extend clinical insight beyond language barriers, significantly reducing language-related disparities in healthcare applications.

## Method Summary
The Knowledge and Few-shot Enhancement In-context Learning (KFE) framework retrieves relevant medical knowledge from a comprehensive knowledge base (53 textbooks, 68,962 text pieces) using BM25 retrieval. This knowledge is integrated into prompts as semantic instructions, providing domain-specific context that compensates for the LLM's lack of exposure to non-English medical corpora. The framework also retrieves similar medical questions and answers from a large question bank (381,149 questions) using BM25, formatted as demonstrations in the prompt to enable in-context learning. Both knowledge and few-shot examples are combined in a single prompt to provide complementary information. The method is evaluated on the China National Medical Licensing Examination (CNMLE-2022) with 494 questions, testing ChatGPT (GPT3.5 and GPT4) and Baichuan2 models.

## Key Results
- ChatGPT without enhancement scores only 51, failing to qualify for medical licensing
- KFE improves ChatGPT to 70.04 and GPT-4 to 82.59, exceeding qualification threshold (60) and average human score (68.70)
- Baichuan2-13B model qualifies with KFE, demonstrating effectiveness in low-resource settings
- Both knowledge enhancement (61.94%) and few-shot enhancement (59.31%) individually improve performance, with combined KFE reaching 70.04%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge enhancement through external medical knowledge retrieval improves LLM performance on non-English medical exams.
- Mechanism: The KFE framework retrieves relevant medical knowledge from a comprehensive knowledge base (53 textbooks, 68,962 text pieces) using BM25 retrieval. This knowledge is integrated into prompts as semantic instructions, providing domain-specific context that compensates for the LLM's lack of exposure to non-English medical corpora.
- Core assumption: The retrieved medical knowledge is both relevant to the question and sufficient to guide the LLM toward correct answers without requiring fine-tuning.
- Evidence anchors:
  - [abstract]: "The proposed Knowledge and Few-shot Enhancement In-context Learning (KFE) framework leverages the in-context learning ability of LLMs to integrate diverse external clinical knowledge sources."
  - [section]: "To improve the accuracy of medical examination, specific instructions I are provided to describe the task... We construct a comprehensive medical knowledge base that is generated from 53 textbooks of People's Medical Publishing House."
  - [corpus]: Weak - no direct evidence of knowledge retrieval effectiveness in corpus, but related work like "ICA-RAG" suggests retrieval-augmented generation is effective for medical diagnosis.

### Mechanism 2
- Claim: Few-shot enhancement with relevant medical question examples improves LLM reasoning and answer accuracy.
- Mechanism: The KFE framework retrieves similar medical questions and answers from a large question bank (381,149 questions) using BM25. These examples are formatted as demonstrations in the prompt, enabling the LLM to learn problem-solving patterns through in-context learning without fine-tuning.
- Core assumption: The retrieved examples are sufficiently similar to the target question and contain correct answers that can guide the LLM's reasoning process.
- Evidence anchors:
  - [abstract]: "The proposed Knowledge and Few-shot Enhancement In-context Learning (KFE) framework leverages the in-context learning ability of LLMs to integrate diverse external clinical knowledge sources."
  - [section]: "We initially curate a sizable medical question bank B = {b1, b2, ..., bm}, encompassing a significant volume of medical questions derived from historical CNMLE, textbooks, and reference materials."
  - [corpus]: Weak - no direct evidence of few-shot effectiveness in corpus, but "Automatic chain of thought prompting" suggests few-shot examples can improve reasoning in LLMs.

### Mechanism 3
- Claim: Combining knowledge enhancement and few-shot enhancement creates synergistic improvements that exceed the sum of individual components.
- Mechanism: The KFE framework integrates both retrieved medical knowledge and relevant question examples into a single prompt, providing both domain-specific context and problem-solving patterns simultaneously. This dual approach addresses both the knowledge gap and reasoning pattern gap.
- Core assumption: The knowledge and examples retrieved are complementary rather than redundant, and their combination provides additional value beyond what either component provides alone.
- Evidence anchors:
  - [abstract]: "By synergizing medical knowledge through in-context learning, LLM can extend clinical insight beyond language barriers, significantly reducing language-related disparities of LLM applications and ensuring global benefit in healthcare."
  - [section]: "Both knowledge and few-shot enhancement can help to improve the final performance. Integrating either enhancement can outperform the Basic GPT model significantly."
  - [corpus]: Weak - no direct evidence of synergistic effects in corpus, but general principle that combining multiple information sources often improves performance.

## Foundational Learning

- Concept: In-context learning in LLMs
  - Why needed here: The KFE framework relies on LLMs' ability to learn from examples and instructions provided within prompts rather than through fine-tuning. Understanding this capability is essential for grasping how KFE works.
  - Quick check question: Can you explain the difference between in-context learning and fine-tuning in LLMs, and why in-context learning might be preferable for adapting to non-English medical domains?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: KFE uses BM25-based retrieval to fetch relevant medical knowledge and question examples. Understanding RAG principles is crucial for understanding how the framework identifies and integrates relevant information.
  - Quick check question: How does BM25 retrieval work, and what are its advantages and limitations compared to other retrieval methods like dense vector retrieval?

- Concept: Chain-of-thought reasoning
  - Why needed here: The framework investigates different instruction strategies, including those that prompt the LLM to generate inference steps. Understanding CoT is important for interpreting the experimental results and design choices.
  - Quick check question: What is chain-of-thought reasoning in LLMs, and why might generating inference steps sometimes reduce performance on medical exams?

## Architecture Onboarding

- Component map: Question → Knowledge Retriever → Question Bank Retriever → Prompt Constructor → LLM Engine → Answer
- Critical path: Question → Knowledge Retriever → Question Bank Retriever → Prompt Constructor → LLM Engine → Answer
- Design tradeoffs:
  - Knowledge vs. Examples: Including both knowledge and examples increases prompt length but provides complementary information
  - Retrieval Quality vs. Quantity: More retrieved items increase chances of relevance but also increase noise and prompt length
  - Inference Steps: Including generated reasoning can improve transparency but may introduce errors that mislead the model
- Failure signatures:
  - Performance degradation when adding knowledge/examples suggests retrieval is pulling irrelevant content
  - Inconsistent answers across runs indicates temperature or randomness issues
  - Poor performance on case analysis questions suggests the framework struggles with practical application questions
- First 3 experiments:
  1. Test knowledge retrieval alone by comparing performance with and without medical knowledge base integration
  2. Test few-shot enhancement alone by comparing performance with and without question bank retrieval
  3. Test different instruction strategies (direct vs. with inference steps) to identify optimal prompting approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different knowledge retrieval methods compare in effectiveness for medical question answering?
- Basis in paper: [explicit] The paper compares BM25 retrieval with self-inquiry method, showing BM25-based knowledge base retrieval achieved 61.94% vs self-inquiry's 48.79%.
- Why unresolved: The comparison is limited to two methods; other retrieval approaches (semantic embeddings, dense retrieval) could potentially yield better results.
- What evidence would resolve it: Direct comparison of multiple retrieval methods (BM25, semantic embeddings, dense retrieval) on the same medical QA task.

### Open Question 2
- Question: What is the optimal balance between knowledge enhancement and few-shot examples for medical QA?
- Basis in paper: [explicit] The paper shows both knowledge enhancement (61.94%) and few-shot enhancement (59.31%) improve performance individually, with combined KFE reaching 70.04%.
- Why unresolved: The paper doesn't systematically explore different combinations or ratios of knowledge enhancement vs few-shot examples.
- What evidence would resolve it: Controlled experiments varying the proportion and type of knowledge vs examples to find optimal configuration.

### Open Question 3
- Question: How do different language models compare when using the same KFE framework?
- Basis in paper: [explicit] The paper compares ChatGPT/GPT4 with Baichuan2-13B, showing all models benefit from KFE but doesn't explore other language models.
- Why unresolved: Limited to comparing only a few models; broader comparison across different LLM architectures and sizes would provide more insights.
- What evidence would resolve it: Systematic evaluation of KFE across diverse language models (different architectures, sizes, training methods).

## Limitations
- Limited transparency in knowledge base construction: The paper describes using 53 Chinese medical textbooks but doesn't specify which textbooks or how content was processed, making exact replication challenging.
- Retrieval quality not rigorously evaluated: The paper doesn't provide quantitative measures of retrieval quality or human evaluation of whether retrieved content is actually relevant.
- Language-specific validation: The framework is tested only on Chinese medical exams with Chinese-language models, making generalizability to other languages uncertain.

## Confidence

**High confidence**: The core finding that KFE significantly improves LLM performance on CNMLE, with GPT-4 achieving 82.59 (exceeding both passing threshold and average human score) is well-supported by the experimental results presented.

**Medium confidence**: Claims about KFE's effectiveness in low-resource settings are supported by Baichuan2-13B results, but the evidence is limited to one smaller model. The claim about reducing language-related disparities is plausible but not directly tested across multiple languages.

**Low confidence**: The assertion that KFE can "extend clinical insight beyond language boundaries" is not directly validated through cross-linguistic experiments. The synergistic effects of combining knowledge and few-shot enhancements are observed but not rigorously quantified through ablation studies.

## Next Checks

1. **Cross-language validation**: Test KFE framework on medical exams in multiple languages (e.g., Spanish, Japanese, Arabic) to verify claims about language boundary transcendence. This would require creating language-specific knowledge bases and evaluating whether the framework generalizes beyond Chinese.

2. **Retrieval quality analysis**: Conduct human evaluation of retrieved medical knowledge and examples to measure relevance scores and identify failure modes. This would help understand whether performance gains stem from high-quality retrieval or other factors.

3. **Ablation study on enhancement components**: Systematically remove knowledge enhancement and few-shot enhancement individually and in combination across multiple LLM models to quantify synergistic effects and identify which component drives performance improvements for different question types.