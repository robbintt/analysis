---
ver: rpa2
title: A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors
arxiv_id: '2310.08287'
source_url: https://arxiv.org/abs/2310.08287
tags:
- symmetries
- neural
- posterior
- network
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents one of the first large-scale explorations of
  the posterior distribution of deep Bayesian Neural Networks (BNNs), focusing on
  real-world vision tasks and architectures. It introduces a mathematical formalism
  to analyze the impact of weight-space symmetries, particularly permutation and scaling
  symmetries, on the posterior distribution.
---

# A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors

## Quick Facts
- **arXiv ID:** 2310.08287
- **Source URL:** https://arxiv.org/abs/2310.08287
- **Reference count:** 40
- **Primary result:** This paper presents one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), focusing on real-world vision tasks and architectures.

## Executive Summary
This paper presents one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), focusing on real-world vision tasks and architectures. It introduces a mathematical formalism to analyze the impact of weight-space symmetries, particularly permutation and scaling symmetries, on the posterior distribution. The study reveals that these symmetries significantly affect the complexity of the posterior and can create artificial functionally equivalent modes. To address these issues, the authors develop a min-mass problem to investigate the impact of scaling symmetries and propose a protocol for assessing the quality of posterior estimation using Maximum Mean Discrepancy (MMD). The research also examines the relationship between posterior quality and uncertainty quantification, demonstrating that multi-mode techniques outperform single-mode methods in terms of posterior estimation and epistemic uncertainty quantification. The authors release a large-scale checkpoint dataset and code to facilitate further research in the field of uncertainty in deep learning.

## Method Summary
The paper investigates the impact of weight-space symmetries on Bayesian neural network posteriors through a multi-faceted approach. First, the authors develop a mathematical formalism to characterize permutation and scaling symmetries in neural networks, showing how these create functionally equivalent modes that complicate posterior estimation. They then train multiple architectures (OptuNet with 392 parameters and ResNet-18) on MNIST, CIFAR-100, and TinyImageNet datasets, collecting thousands of checkpoints to approximate the true posterior. Various posterior estimation methods (MC Dropout, SGHMC, SWAG, Laplace, BNN, Deep Ensembles) are compared using Maximum Mean Discrepancy (MMD) as a metric. The study introduces a min-mass problem to address scaling symmetries and proposes a symmetry-aware protocol for evaluating posterior quality. Finally, the relationship between posterior estimation quality and uncertainty quantification performance is examined through metrics including accuracy, Brier score, ECE, AUPR, FPR95, and mutual information scores.

## Key Results
- Weight-space symmetries (permutation and scaling) significantly increase the complexity of Bayesian posterior distributions by creating functionally equivalent modes.
- Multi-mode techniques consistently outperform single-mode methods in terms of posterior estimation quality as measured by MMD.
- The quality of posterior estimation correlates with epistemic uncertainty quantification performance, though single-mode methods sometimes show better calibration (ECE).
- Scaling symmetries can be effectively addressed through normalization or convex optimization approaches to the min-mass problem.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight-space symmetries significantly increase the complexity of Bayesian posterior distributions by creating functionally equivalent modes.
- Mechanism: Permutation and scaling symmetries allow multiple weight configurations to produce identical network outputs. Permutation symmetries create exact mode duplication, while scaling symmetries create redundant regions around modes. This artificial inflation of modes makes the posterior harder to estimate accurately.
- Core assumption: The loss landscape and posterior distribution are influenced by symmetries in the weight space that preserve functional equivalence.
- Evidence anchors:
  - [abstract] "Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior."
  - [section 3.1] "Permutation symmetries very clearly increase the number of modes of the posterior distribution."
  - [corpus] Weak - corpus papers don't directly address symmetry-induced mode inflation.
- Break condition: If activation functions lose their non-negative homogeneity (e.g., using tanh instead of ReLU), scaling symmetries would disappear and this mechanism would break.

### Mechanism 2
- Claim: Multi-mode techniques outperform single-mode methods in posterior estimation and uncertainty quantification.
- Mechanism: By sampling from multiple modes of the posterior, multi-mode methods capture more of the true posterior distribution's complexity. This leads to better epistemic uncertainty quantification and more diverse predictions across ensemble members.
- Core assumption: The true posterior contains multiple distinct modes that carry important information about uncertainty.
- Evidence anchors:
  - [abstract] "demonstrate that multi-mode techniques outperform single-mode methods in terms of posterior estimation and epistemic uncertainty quantification."
  - [section 4.3] "multi-mode techniques consistently demonstrate superior performance in terms of MMD when compared to their single-mode counterparts."
  - [corpus] Weak - corpus papers focus on different aspects of Bayesian neural networks without specifically comparing multi-mode vs single-mode approaches.
- Break condition: If the true posterior collapses to a single dominant mode, multi-mode methods would provide no advantage over single-mode methods.

### Mechanism 3
- Claim: The quality of posterior estimation correlates with epistemic uncertainty quantification performance.
- Mechanism: Better approximations of the true posterior distribution lead to more accurate estimates of model uncertainty. Methods that capture the posterior's complexity can better represent uncertainty in predictions.
- Core assumption: The posterior distribution encodes information about model uncertainty that can be extracted through approximation methods.
- Evidence anchors:
  - [abstract] "examine the relationship between posterior quality and uncertainty quantification"
  - [section 4.3] "an intriguing divergence arises when considering the ECE, where techniques focusing solely on estimating a single mode often exhibit superior performance."
  - [corpus] Weak - corpus papers don't establish this specific relationship between posterior quality and uncertainty quantification.
- Break condition: If aleatoric uncertainty dominates over epistemic uncertainty, the relationship between posterior quality and uncertainty quantification might break down.

## Foundational Learning

- Concept: Weight-space symmetries in neural networks
  - Why needed here: Understanding how permutation and scaling symmetries create functionally equivalent networks is crucial for interpreting posterior complexity and mode proliferation.
  - Quick check question: How do permutation symmetries differ from scaling symmetries in their effect on neural network outputs?

- Concept: Bayesian neural networks and posterior distributions
  - Why needed here: The paper investigates how to approximate the posterior distribution of BNNs, which is central to uncertainty quantification.
  - Quick check question: Why is exact computation of the Bayesian posterior intractable for deep neural networks?

- Concept: Maximum Mean Discrepancy (MMD) for distribution comparison
  - Why needed here: MMD is used as a metric to compare different posterior estimation methods against the target distribution.
  - Quick check question: How does MMD measure the difference between two probability distributions in high-dimensional spaces?

## Architecture Onboarding

- Component map:
  - Symmetry analysis module: Identifies and characterizes permutation and scaling symmetries
  - Posterior estimation framework: Implements various Bayesian approximation methods (MC Dropout, SWAG, Laplace, etc.)
  - MMD computation engine: Calculates Maximum Mean Discrepancy between estimated and target posteriors
  - Visualization toolkit: Creates plots showing posterior distributions with and without symmetry removal
  - Dataset interface: Handles loading and preprocessing of MNIST, CIFAR-100, and TinyImageNet datasets

- Critical path:
  1. Train base models to generate checkpoint data
  2. Apply symmetry removal algorithms to create identifiable representations
  3. Estimate posterior using multiple methods
  4. Compute MMD between methods and target distribution
  5. Evaluate uncertainty quantification metrics
  6. Analyze diversity of ensemble predictions

- Design tradeoffs:
  - Symmetry removal: Normalization vs convex optimization - trade-off between computational efficiency and optimal symmetry elimination
  - Posterior estimation: Single-mode vs multi-mode - trade-off between computational cost and accuracy of uncertainty quantification
  - Kernel selection for MMD: Gaussian vs Laplace - trade-off between sensitivity to local vs global differences

- Failure signatures:
  - Poor MMD scores despite good accuracy: Indicates overfitting and poor uncertainty quantification
  - Low diversity in ensemble predictions: Suggests functional collapse or insufficient exploration of posterior modes
  - High MMD scores with symmetry removal: Indicates symmetry analysis implementation issues

- First 3 experiments:
  1. Train a simple MLP on MNIST and visualize the posterior with and without symmetry removal to observe mode duplication
  2. Compare single-mode vs multi-mode methods on CIFAR-100 using MMD to quantify posterior estimation quality
  3. Measure the correlation between MMD scores and uncertainty quantification metrics (AUPR, FPR95) across different architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of weight-space symmetries on the epistemic uncertainty of deep neural networks?
- Basis in paper: [explicit] The paper investigates the impact of weight-space symmetries, particularly permutation and scaling symmetries, on the posterior distribution and uncertainty quantification in deep neural networks.
- Why unresolved: The paper provides insights into how symmetries can complicate the posterior distribution and affect uncertainty quantification, but a comprehensive understanding of their full impact is still lacking.
- What evidence would resolve it: Further experimental studies and theoretical analysis to quantify the exact relationship between weight-space symmetries and epistemic uncertainty in different architectures and tasks.

### Open Question 2
- Question: How can the minimum scaled network mass problem be effectively solved for large-scale deep neural networks?
- Basis in paper: [explicit] The paper introduces the minimum scaled network mass problem to investigate the impact of scaling symmetries and mentions its log-log strict convexity.
- Why unresolved: While the problem is shown to be log-log strictly convex, practical methods for solving it efficiently for large-scale networks with millions of parameters are not discussed.
- What evidence would resolve it: Development and evaluation of scalable optimization algorithms or approximations to solve the minimum scaled network mass problem for modern deep learning architectures.

### Open Question 3
- Question: Is there a correlation between the quality of posterior estimation and the accuracy of epistemic uncertainty quantification across different neural network architectures and tasks?
- Basis in paper: [explicit] The paper examines the relationship between posterior quality and uncertainty quantification using Maximum Mean Discrepancy (MMD) and various performance metrics.
- Why unresolved: The paper provides initial evidence of a connection between posterior estimation and uncertainty quantification, but the relationship may vary depending on the architecture, task, and method used.
- What evidence would resolve it: Extensive empirical studies comparing posterior estimation quality and uncertainty quantification performance across diverse neural network architectures, tasks, and approximation methods.

## Limitations

- The paper focuses primarily on convolutional architectures (ResNet-18 and OptuNet) without examining other network types like transformers or recurrent networks, limiting generalizability.
- The study uses relatively small-scale models (ResNet-18, 392-parameter OptuNet) rather than larger, state-of-the-art architectures, potentially limiting applicability to real-world deployments.
- The symmetry removal methods, while theoretically sound, may introduce computational overhead that scales poorly with model size.

## Confidence

- **High confidence**: The identification of permutation and scaling symmetries as sources of mode duplication is well-supported by both theoretical analysis and empirical evidence. The MMD-based evaluation framework for posterior estimation is methodologically sound.
- **Medium confidence**: The claim that multi-mode techniques consistently outperform single-mode methods is supported by experiments but may be architecture-dependent. The relationship between posterior quality and uncertainty quantification shows promise but exhibits some inconsistencies (e.g., single-mode methods performing better on ECE).
- **Low confidence**: The generalizability of findings to larger architectures and non-convolutional models remains unproven due to limited experimental scope.

## Next Checks

1. **Architecture scaling test**: Replicate experiments using larger architectures (e.g., ResNet-50, Vision Transformers) to verify if symmetry effects and multi-mode benefits persist at scale.
2. **Symmetry removal efficiency**: Benchmark the computational cost of different symmetry removal approaches (normalization vs convex optimization) on progressively larger models to assess practical feasibility.
3. **Cross-architecture generalization**: Apply the symmetry analysis framework to recurrent networks and transformers to determine if permutation and scaling symmetries manifest similarly across different network types.