---
ver: rpa2
title: 'SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills'
arxiv_id: '2308.16369'
source_url: https://arxiv.org/abs/2308.16369
tags:
- size
- prefill
- batch
- decode
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SARATHI addresses the inefficiency of LLM inference by introducing
  chunked-prefills and decode-maximal batching. Chunked-prefills splits long input
  prompts into smaller compute-efficient chunks, while decode-maximal batching constructs
  hybrid batches that combine a single prefill chunk with multiple decodes.
---

# SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills

## Quick Facts
- **arXiv ID**: 2308.16369
- **Source URL**: https://arxiv.org/abs/2308.16369
- **Reference count**: 40
- **Primary result**: Up to 10× improvement in decode throughput through chunked-prefills and decode-maximal batching

## Executive Summary
SARATHI addresses the inefficiency of LLM inference by introducing chunked-prefills and decode-maximal batching. The approach splits long input prompts into smaller compute-efficient chunks, while constructing hybrid batches that combine a single prefill chunk with multiple decodes. This enables decodes to piggyback on compute-intensive prefills, converting memory-bound decode operations to compute-bound ones. The method achieves up to 10× improvement in decode throughput and 1.33× end-to-end throughput improvement for LLaMA-13B on A6000 GPU, while also reducing pipeline bubbles by 6.29× when used with pipeline parallelism.

## Method Summary
SARATHI introduces two key techniques: chunked-prefills splits a prefill request into equal-sized chunks to improve GPU utilization, while decode-maximal batching constructs hybrid batches that combine a single prefill chunk with multiple decode requests. The prefill chunk saturates GPU compute, allowing decode requests to piggyback at significantly reduced cost. This approach converts memory-bound decode operations into compute-bound ones through fusion of linear operations, and addresses pipeline bubbles by ensuring uniform compute time across micro-batches in pipeline parallelism.

## Key Results
- Up to 10× improvement in decode throughput through operation fusion
- 1.33× end-to-end throughput improvement for LLaMA-13B on A6000 GPU
- 6.29× reduction in pipeline bubbles when used with pipeline parallelism

## Why This Works (Mechanism)

### Mechanism 1
Chunked-prefills improve GPU utilization by breaking large prefill requests into smaller, compute-efficient chunks that can be processed in parallel with decodes. The prefill phase is split into equal-sized chunks that each saturate GPU compute when processed, allowing multiple decode requests to piggyback on each prefill chunk and convert memory-bound decode operations into compute-bound ones.

### Mechanism 2
Decode-maximal batching converts memory-bound decode operations into compute-bound operations by fusing linear operations across prefill and decode tokens. By combining decode tokens with a prefill chunk in a single matrix multiplication operation, model weights are loaded once and reused, eliminating separate memory fetches for each decode iteration.

### Mechanism 3
Uniform compute design of hybrid batches significantly reduces pipeline bubbles in multi-GPU deployments by eliminating micro-batch imbalance. Each hybrid batch requires approximately the same compute time, preventing pipeline stages from waiting on slower micro-batches that would occur with varying prefill and decode times.

## Foundational Learning

- **Transformer architecture and attention mechanisms**: Understanding how prefill and decode phases differ in their computational characteristics is crucial for grasping why SARATHI's approach works. Quick check: What is the primary computational difference between prefill and decode phases in terms of input tensor shapes?

- **GPU memory hierarchy and arithmetic intensity**: The distinction between compute-bound and memory-bound operations determines when chunking and batching strategies are effective. Quick check: How does arithmetic intensity relate to whether a GPU operation is compute-bound or memory-bound?

- **Pipeline parallelism and micro-batching**: Understanding pipeline bubbles requires knowledge of how micro-batches are scheduled across pipeline stages. Quick check: What causes pipeline bubbles in traditional micro-batch scheduling approaches?

## Architecture Onboarding

- **Component map**: Chunked-prefills module -> Decode-maximal batching scheduler -> Linear operation fusion engine -> Attention mask manager -> KV cache manager
- **Critical path**: Request arrival → Chunked-prefills split → Batch construction → Linear operation fusion → Attention computation → Output generation
- **Design tradeoffs**: Smaller chunk sizes increase decode piggybacking opportunities but reduce prefill efficiency; larger chunk sizes do the opposite
- **Failure signatures**: Degraded decode throughput indicates insufficient piggybacking; increased prefill latency suggests chunk size is too small
- **First 3 experiments**:
  1. Measure prefill throughput at various chunk sizes to identify optimal chunk size for the target hardware
  2. Compare decode throughput between baseline decode-only batches and SARATHI hybrid batches at different batch sizes
  3. Profile pipeline bubble reduction by measuring micro-batch execution time variance with and without SARATHI

## Open Questions the Paper Calls Out

### Open Question 1
How does SARATHI's performance scale with sequence lengths beyond 3K tokens, and what new challenges arise for extremely long sequences (10s-100s of thousands of tokens)? The authors acknowledge that very long sequences may pose new challenges not encountered in their evaluation range, particularly regarding the quadratic growth of attention computation costs.

### Open Question 2
How can SARATHI be extended to optimize for multiple objectives simultaneously (latency, throughput, fairness, queuing delays) in real-world deployment scenarios? Real-world deployments need to optimize along multiple dimensions simultaneously, and meeting these goals with SARATHI requires revisiting scheduling policies.

### Open Question 3
What is the optimal chunk size selection strategy for SARATHI when the P:D ratio is unknown or varies significantly across requests? While the paper shows appropriate chunk sizes for known P:D ratios, they leave it to future work to explore optimal chunk size selection when P:D ratios may not be known ahead of time.

## Limitations
- Performance highly dependent on optimal chunk size, which varies with hardware configurations and model sizes
- Evaluation focuses primarily on throughput metrics with limited discussion of latency impacts
- Implementation details for attention mask management across chunked prefills remain underspecified

## Confidence

**High Confidence Claims:**
- Chunked-prefills can improve GPU utilization by reducing memory-bound operations
- Decode-maximal batching provides computational efficiency gains through operation fusion
- Pipeline bubbles are reduced when using uniform compute batches in pipeline parallelism

**Medium Confidence Claims:**
- The specific performance improvements (1.33× end-to-end throughput) are reproducible across different hardware configurations
- The 10× decode throughput improvement is achievable in practical scenarios
- The optimal chunk size can be determined systematically for different deployment scenarios

## Next Checks
1. Systematically evaluate SARATHI's performance across a range of chunk sizes (256, 512, 1024, 2048 tokens) to identify optimal configuration for different hardware setups
2. Implement comprehensive test suite to verify attention mask construction across chunked prefills maintains mathematical correctness
3. Extend evaluation to measure end-to-end latency alongside throughput to quantify the tradeoff between these metrics