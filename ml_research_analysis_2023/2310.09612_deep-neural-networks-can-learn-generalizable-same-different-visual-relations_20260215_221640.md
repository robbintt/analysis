---
ver: rpa2
title: Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations
arxiv_id: '2310.09612'
source_url: https://arxiv.org/abs/2310.09612
tags:
- dataset
- same
- clip
- test
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Prior research suggests that deep neural networks struggle to learn
  the abstract same-different visual relation. This work investigates whether modern
  neural networks can learn generalizable same-different relations by evaluating a
  range of architectures, pretraining methods, and fine-tuning datasets.
---

# Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations

## Quick Facts
- arXiv ID: 2310.09612
- Source URL: https://arxiv.org/abs/2310.09612
- Reference count: 27
- Key outcome: CLIP-pretrained Vision Transformers fine-tuned on abstract shapes can achieve near-perfect out-of-distribution generalization on same-different visual relations

## Executive Summary
This paper challenges the widely held belief that deep neural networks struggle to learn abstract same-different visual relations. Through systematic evaluation of various architectures, pretraining methods, and training datasets, the authors demonstrate that modern neural networks—particularly CLIP-pretrained Vision Transformers—can acquire genuinely abstract concepts of equality. The key finding is that models fine-tuned on abstract shapes without color or texture information generalize with near-perfect accuracy to out-of-distribution stimuli, while models trained on naturalistic images show significantly weaker generalization.

## Method Summary
The study evaluates ResNet-50 and Vision Transformer (ViT-B/16) models pretrained with random initialization, ImageNet, or CLIP, then fine-tuned on binary same-different classification tasks. Four datasets are used: Squiggles (SQU), Alphanumeric (ALPH), Shapes (SHA), and Naturalistic (NAT), each with 6,400 training, 1,600 validation, and 1,600 test images. Models are trained for 70 epochs with binary cross-entropy loss and evaluated on both in-distribution and out-of-distribution test sets, with results reported as median accuracy over five random seeds.

## Key Results
- CLIP-pretrained Vision Transformers fine-tuned on abstract shapes achieve near-perfect out-of-distribution generalization
- CLIP pretraining provides the largest improvements in in-distribution accuracy compared to ImageNet or random initialization
- Models fine-tuned on abstract shapes generalize better to out-of-distribution stimuli than those fine-tuned on naturalistic images
- Vision Transformers exhibit different inductive biases depending on available features, preferring color or texture over shape when available

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Supervision in CLIP Pretraining
CLIP pretraining provides linguistic supervision that helps models separate same and different objects in visual embedding spaces. CLIP models learn to align visual features with language captions containing "same," "different," or "two of" which provides additional signal beyond pure visual category labels. This mechanism is supported by linear probe results achieving 80-100% in-distribution accuracy using CLIP embeddings. The break condition would be if CLIP models without linguistic supervision show similar performance.

### Mechanism 2: Self-Attention Architecture in Vision Transformers
Vision Transformers can compare any image patch to any other as early as the first self-attention layer. Self-attention allows direct comparison of distant image features without needing to propagate through multiple layers, which may be crucial for relational abstraction. This is supported by the superior OOD performance of ViTs compared to CNNs. If CNNs with larger receptive fields show similar performance, this mechanism would be invalid.

### Mechanism 3: Feature-Dependent Learning Strategies
Models learn different feature comparison strategies based on what's available in training data. When color and texture are available, models prefer these over shape; when only shape is available, they learn shape comparison. This "shortcut" learning is evidenced by models' preference for color/texture features and is mitigated by CLIP pretraining. If models show no preference for any feature type regardless of availability, this mechanism would be invalid.

## Foundational Learning

- **Concept: Out-of-distribution generalization**
  - Why needed here: The paper's main claim is about abstract same-different relation learning, which requires generalization beyond training data
  - Quick check question: If a model trained on red circles and blue squares can correctly classify yellow triangles as "same" or "different" from each other, has it learned the abstract relation?

- **Concept: Inductive bias**
  - Why needed here: The paper examines whether models have built-in preferences for certain features (color, texture, shape) when learning relations
  - Quick check question: If a model always chooses "same" when objects share color regardless of shape, what kind of inductive bias does it have?

- **Concept: Pretraining effects**
  - Why needed here: The paper compares random initialization, ImageNet pretraining, and CLIP pretraining to understand their impact on relation learning
  - Quick check question: If CLIP-pretrained models perform better than ImageNet-pretrained models, what additional information might CLIP provide?

## Architecture Onboarding

- **Component map:** Input images → Vision Transformer/ResNet backbone → Linear classifier → Same/different binary output
- **Critical path:** Pretraining (random initialization, ImageNet, or CLIP) → Fine-tuning on same-different task with chosen dataset → Evaluation on in-distribution and out-of-distribution test sets
- **Design tradeoffs:** ViT vs ResNet: ViTs have larger receptive fields but are more computationally expensive; Pretraining choice: CLIP provides linguistic supervision but ImageNet is more widely available; Dataset selection: More abstract shapes enable better OOD generalization but may be harder to learn initially
- **Failure signatures:** High in-distribution accuracy but low OOD accuracy indicates memorization rather than abstract relation learning; Performance degradation when color/texture information is removed suggests reliance on these features; Bimodal performance across seeds suggests solution discovery depends on training order
- **First 3 experiments:** 1) Replicate in-distribution results: Train CLIP ViT-B/16 on SQU and verify near-perfect accuracy; 2) Test OOD generalization: Evaluate same model on ALPH, SHA, and NAT datasets; 3) Feature ablation: Train models on grayscale and masked versions of Shapes to test feature preferences

## Open Questions the Paper Calls Out

### Open Question 1
Does CLIP pretraining explicitly encode concepts of "same" and "different" into its visual embeddings, or is this learned during fine-tuning? The paper mentions that CLIP captions contain phrases like "same," "different," or "two of" and that linear probes trained on CLIP embeddings achieve high accuracy, suggesting some pre-encoded information. This remains unresolved because while linear probe results show high in-distribution accuracy, they exhibit poor out-of-distribution generalization. Comparing models with and without access to textual supervision during pretraining would resolve this question.

### Open Question 2
What is the specific mechanism by which CLIP-pretrained ViTs achieve near-perfect out-of-distribution generalization on the same-different task? The paper discusses potential reasons such as CLIP's diverse dataset, linguistic supervision, and ViT's larger receptive field, but does not provide a definitive answer. Ablation studies isolating the effects of CLIP pretraining, linguistic supervision, and ViT architecture on out-of-distribution generalization would resolve this question.

### Open Question 3
How do models fine-tuned on the same-different task segment and compare objects in an image to determine sameness? The paper mentions the possibility of models learning an internal circuit to segment objects and compute equality in embedding space, but does not investigate this further. Analyzing the attention patterns of ViT models or the feature maps of CNN models during the same-different task would resolve this question.

## Limitations
- The study focuses on a specific subset of same-different relations with relatively small image sizes (64x64 upscaled to 224x224)
- Generalizability to other relational tasks, larger images, or real-world applications remains untested
- Evaluation covers only four datasets with limited variation in relation types and image complexity

## Confidence

- **High confidence**: CLIP pretraining substantially improves both in-distribution and OOD performance for same-different tasks
- **Medium confidence**: Vision Transformers demonstrate better OOD generalization than ResNets due to their self-attention architecture
- **Medium confidence**: Models exhibit feature-dependent inductive biases, preferring color/texture over shape when available

## Next Checks

1. Test whether CLIP-pretrained models maintain their advantage on same-different tasks with natural images at higher resolutions (256x256 or 512x512)
2. Evaluate model performance on more diverse relational tasks (e.g., spatial relations like "left-of" or "inside") to assess broader relational abstraction capabilities
3. Conduct ablation studies on the linguistic component of CLIP by comparing against models trained on ImageNet with similar visual categories but no text supervision