---
ver: rpa2
title: State-wise Constrained Policy Optimization
arxiv_id: '2306.12594'
source_url: https://arxiv.org/abs/2306.12594
tags:
- policy
- cost
- state-wise
- safety
- scpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes State-wise Constrained Policy Optimization
  (SCPO), the first general-purpose policy search algorithm for state-wise constrained
  reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction
  in expectation.
---

# State-wise Constrained Policy Optimization

## Quick Facts
- arXiv ID: 2306.12594
- Source URL: https://arxiv.org/abs/2306.12594
- Reference count: 40
- Key outcome: SCPO is the first general-purpose policy search algorithm for state-wise constrained RL, providing guarantees for state-wise constraint satisfaction in expectation and bounding worst-case safety violations through MMDP formulation.

## Executive Summary
This paper introduces State-wise Constrained Policy Optimization (SCPO), a novel algorithm for state-wise constrained reinforcement learning that provides theoretical guarantees for constraint satisfaction in expectation. The key innovation is reformulating the constrained problem as a Maximum Markov Decision Process (MMDP), where the goal is to maximize reward while constraining the expected maximum state-wise cost. This formulation allows bounding the worst-case safety violations through trust region policy updates, providing stronger safety guarantees than existing approaches that only bound cumulative costs.

## Method Summary
SCPO is a policy search algorithm that uses trust region optimization with surrogate objectives and constraints. It approximates the theoretically-justified update using importance sampling to estimate advantages, then convexifies the objective and constraints through first-order expansions. The algorithm solves the resulting constrained optimization problem using duality methods, with a line search to ensure constraint satisfaction. A key practical component is sub-sampling zero-valued cost targets to balance training data distribution and improve value function fitting.

## Key Results
- SCPO achieves near-zero constraint violations while maintaining high reward performance on Safety Gym benchmarks
- Significantly outperforms existing methods on high-dimensional robotics tasks with state-wise constraints
- Successfully handles various constraint types including hazards and pillars across multiple robot morphologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCPO enforces state-wise constraints in expectation by bounding maximum state-wise costs through MMDP formulation.
- Mechanism: SCPO transforms the CMDP formulation into a Maximum Markov Decision Process (MMDP) where the constraint is on the expected maximum state-wise cost rather than cumulative cost. This allows bounding the worst-case constraint violation through trust region policy updates.
- Core assumption: The maximum state-wise cost can be effectively bounded through expected value constraints, and the trust region update provides sufficient control over policy changes.
- Evidence anchors:
  - [abstract] "SCPO provides guarantees for state-wise constraint satisfaction in expectation"
  - [section] "constraining the maximum violation is equivalent to enforcing state-wise safety"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: The mechanism breaks if the cost function has high variance or the trust region cannot adequately control policy changes to prevent constraint violations.

### Mechanism 2
- Claim: SCPO's practical implementation uses importance sampling and first-order approximations to efficiently solve the constrained optimization problem.
- Mechanism: SCPO approximates the theoretically-justified update using importance sampling to estimate advantages, then convexifies the objective and constraints through first-order expansions, and solves the resulting problem using duality methods.
- Core assumption: The first-order approximations are sufficiently accurate within the trust region, and the importance sampling estimator converges to the true expectation.
- Evidence anchors:
  - [abstract] "SCPO approximates the theoretically-justified update"
  - [section] "We first estimate the objective and constraints in (11) using samples"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: The mechanism breaks if the policy updates are too large for the first-order approximations to remain valid, or if importance sampling variance becomes too high.

### Mechanism 3
- Claim: SCPO handles imbalanced cost value targets through sub-sampling to balance zero and non-zero values.
- Mechanism: Since the true maximum state-wise cost function is highly zero-skewed (most states have zero cost increment once maximum is reached), SCPO sub-samples the zero-valued targets to match the population of non-zero values, improving fitting of the value function.
- Core assumption: The sub-sampling trick effectively balances the training data distribution without introducing bias that would harm performance.
- Evidence anchors:
  - [section] "we sub-sample the zero-valued targets to match the population of non-zero values"
  - [section] "we compare the performance of SCPO with and without sub-sampling trick"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: The mechanism breaks if sub-sampling introduces significant bias or if the cost function structure changes in ways that make this balancing ineffective.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Constrained MDPs (CMDPs)
  - Why needed here: SCPO builds on MDP theory and extends it to CMDPs with state-wise constraints
  - Quick check question: What is the difference between cumulative constraints and state-wise constraints in CMDPs?

- Concept: Trust Region Policy Optimization (TRPO) and its theoretical foundations
  - Why needed here: SCPO uses TRPO-style trust region updates with KL divergence constraints
  - Quick check question: How does the KL divergence constraint in TRPO relate to policy improvement bounds?

- Concept: Maximum Markov Decision Processes (MMDPs) and their properties
  - Why needed here: SCPO introduces MMDP as a novel formulation specifically for state-wise constraints
  - Quick check question: How does the expected maximum cost in MMDP differ from the discounted cumulative cost in standard MDPs?

## Architecture Onboarding

- Component map: Policy network -> Value network -> Cost increment value network -> Rollout generator -> Advantage estimator -> Trust region optimizer

- Critical path:
  1. Collect trajectories using current policy
  2. Estimate reward and cost advantages using importance sampling
  3. Fit value networks (including sub-sampling for cost values)
  4. Solve convex optimization problem for policy update
  5. Perform line search to ensure constraint satisfaction

- Design tradeoffs:
  - Sub-sampling zero cost values vs. using all data (bias-variance tradeoff)
  - Trust region size δ: larger allows faster learning but risks constraint violations
  - Network architecture: deeper networks may capture complex value functions better but require more data

- Failure signatures:
  - Policy update frequently fails line search → trust region too small or approximations inaccurate
  - Cost values don't converge to zero → sub-sampling ineffective or cost function structure problematic
  - Reward performance degrades → constraint satisfaction too restrictive or optimization imbalance

- First 3 experiments:
  1. Implement basic SCPO without sub-sampling on a simple Point-Hazard-1 environment to verify core functionality
  2. Add sub-sampling and compare performance on Drone-3DHazard-1 environment
  3. Scale up to Ant-Hazard-8 to test high-dimensional performance and constraint satisfaction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SCPO be extended to provide absolute state-wise constraint satisfaction guarantees rather than just guarantees in expectation?
- Basis in paper: The authors explicitly state that "one limitation of our work is that, although SCPO satisfies state-wise constraints, the theoretical results are valid only in expectation, meaning that constraint violations are still possible during deployment."
- Why unresolved: The paper only provides theoretical bounds on expected constraint violation, but real-world safety-critical applications often require stronger guarantees that violations will not occur.
- What evidence would resolve it: A modified version of SCPO that provides worst-case bounds on state-wise constraint violation, along with experimental validation showing near-zero constraint violations across diverse tasks.

### Open Question 2
- Question: How does the performance of SCPO scale with the dimensionality of the state and action spaces?
- Basis in paper: The authors demonstrate SCPO's effectiveness on high-dimensional tasks like Ant-Hazard-8 and Walker-Hazard-8, but don't systematically explore the scaling properties across a wider range of dimensionalities.
- Why unresolved: While the results show SCPO works on moderately high-dimensional tasks, it's unclear how performance degrades as dimensionality increases further.
- What evidence would resolve it: Experiments comparing SCPO's reward and constraint satisfaction performance across tasks with systematically varying state and action space dimensionalities, potentially revealing scaling laws.

### Open Question 3
- Question: How robust is SCPO to different reward function designs and trade-offs between reward maximization and constraint satisfaction?
- Basis in paper: The paper focuses on SCPO's ability to satisfy constraints, but doesn't explore how sensitive the algorithm is to the specific reward function used or how it balances reward vs. safety.
- Why unresolved: Real-world tasks often require careful reward shaping to encourage desired behaviors while ensuring safety. It's unclear how SCPO's performance changes under different reward structures.
- What evidence would resolve it: Experiments testing SCPO on tasks with varied reward functions, including those that more strongly prioritize safety vs. performance, to characterize the trade-offs.

## Limitations

- Theoretical guarantees only hold in expectation, not providing absolute worst-case bounds on constraint violations
- Practical implementation relies on first-order approximations that may not hold in high-dimensional spaces
- Limited ablation studies on the sub-sampling trick to verify its necessity for performance

## Confidence

- High confidence: The basic framework of using trust region updates for constraint satisfaction is well-established
- Medium confidence: The MMDP formulation and its theoretical guarantees appear sound but depend on assumptions about policy similarity
- Low confidence: The practical effectiveness of the sub-sampling trick and its impact on learning stability

## Next Checks

1. Conduct ablation studies removing the sub-sampling trick to quantify its contribution to performance
2. Run experiments with multiple random seeds (at least 5) to establish statistical significance of reported improvements
3. Test SCPO on additional benchmark environments beyond Safety Gym to verify generalization capabilities