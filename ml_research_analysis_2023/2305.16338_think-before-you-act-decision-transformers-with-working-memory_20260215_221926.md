---
ver: rpa2
title: 'Think Before You Act: Decision Transformers with Working Memory'
arxiv_id: '2305.16338'
source_url: https://arxiv.org/abs/2305.16338
tags:
- memory
- dt-mem
- training
- fine-tuning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of decision transformer-based
  decision-making agents that rely on massive data and computation due to the forgetting
  phenomenon. The authors propose Decision Transformers with Memory (DT-Mem), a novel
  architecture that incorporates an internal working memory module to store, blend,
  and retrieve information for different downstream tasks.
---

# Think Before You Act: Decision Transformers with Working Memory

## Quick Facts
- arXiv ID: 2305.16338
- Source URL: https://arxiv.org/abs/2305.16338
- Reference count: 40
- Primary result: DT-Mem achieves 29.9% and 16.3% improvements over MDT with only 10% of model parameters

## Executive Summary
Decision Transformers (DTs) have shown promise in sequential decision-making but suffer from inefficiency due to catastrophic forgetting during training. This paper introduces Decision Transformers with Memory (DT-Mem), a novel architecture that incorporates a working memory module to store, blend, and retrieve task-specific information. The working memory uses content-based addressing and attention mechanisms for efficient information access, while LoRA-based fine-tuning enables parameter-efficient adaptation to new tasks.

The proposed architecture demonstrates significant improvements in both training efficiency and generalization across multiple domains. DT-Mem achieves 29.9% improvement over MDT on average rollouts and 16.3% on top 3 rollouts while using only 10% of the model parameters. The system also outperforms state-of-the-art methods on Meta-World object manipulation tasks, showing strong zero-shot performance and efficient adaptation with limited fine-tuning data.

## Method Summary
DT-Mem extends standard decision transformers by adding a working memory module that operates alongside the transformer and MLP components. The memory module uses content-based addressing with attention mechanisms to update and retrieve task-specific information. For adaptation to new tasks, the memory module is fine-tuned using LoRA (Low-Rank Adaptation), which modifies only low-rank matrices while keeping the transformer parameters frozen. The architecture is pre-trained on multiple tasks and then evaluated on held-out tasks for generalization, followed by fine-tuning on limited data to assess adaptability.

## Key Results
- DT-Mem achieves 29.9% improvement over MDT on average rollouts and 16.3% on top 3 rollouts
- Uses only 10% of MDT's model parameters while outperforming it
- Outperforms Hyper-Decision Transformer and Prompt Decision Transformer on Meta-World ML45 benchmarks
- Demonstrates strong zero-shot generalization and efficient adaptation with limited fine-tuning data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Working memory reduces catastrophic forgetting by explicitly storing task-specific knowledge separately from transformer parameters.
- **Mechanism**: DT-Mem uses a content-addressable memory matrix updated via attention-based content addressing. New tasks are adapted by fine-tuning the memory with LoRA while freezing transformer parameters, preventing interference with previously learned tasks.
- **Core assumption**: General knowledge is captured by transformer pre-training while task-specific adaptations are stored in memory, allowing independent updates without interference.
- **Evidence anchors**: Abstract mentions forgetting phenomenon; section 2 discusses how task-specific knowledge is stored in working memory.
- **Break condition**: Insufficient memory capacity for task complexity, or ineffective LoRA adaptation to task-specific patterns.

### Mechanism 2
- **Claim**: Content-based addressing with attention enables efficient retrieval of relevant memory slots for decision-making.
- **Mechanism**: Memory module uses attention to compute similarity between input queries and memory slots, retrieving the most relevant stored information for current decisions.
- **Core assumption**: Attention-based retrieval is more effective than k-NN search for internal working memory due to smaller memory size and need for task generalization.
- **Evidence anchors**: Section 4.2 describes content-based addressing and attention for memory retrieval.
- **Break condition**: Memory becomes too large for efficient attention computation, or content similarity fails to capture relevant decision information.

### Mechanism 3
- **Claim**: LoRA-based fine-tuning of memory module provides efficient task adaptation with minimal parameter updates.
- **Mechanism**: Memory module's query, key, and value transformations are adapted using low-rank matrices (B and A) learned from limited fine-tuning data, enabling efficient task-specific modifications without full model retraining.
- **Core assumption**: Pre-trained transformer has captured sufficient general knowledge that only task-specific components need adaptation via low-rank modifications.
- **Evidence anchors**: Section 4.4 describes LoRA application to fine-tune memory module.
- **Break condition**: Rank parameter too small to capture task-specific patterns, or insufficient fine-tuning data for effective LoRA adaptation.

## Foundational Learning

- **Concept**: Content-addressable memory and attention mechanisms
  - Why needed here: Working memory relies on content-based addressing to store and retrieve task-specific information efficiently, requiring understanding of attention mechanisms for memory operations.
  - Quick check question: How does memory module attention differ from standard transformer attention, and why is this distinction important for task-specific memory operations?

- **Concept**: Low-rank adaptation (LoRA) for parameter-efficient fine-tuning
  - Why needed here: LoRA is used to fine-tune memory module for new tasks with minimal parameters, crucial for understanding efficient adaptation without full model retraining.
  - Quick check question: What is the mathematical relationship between original weight matrix, low-rank update matrices, and final adapted output in LoRA?

- **Concept**: Catastrophic forgetting and transfer learning in neural networks
  - Why needed here: Understanding why forgetting occurs and how explicit memory mechanisms mitigate it is fundamental to grasping DT-Mem's motivation and design.
  - Quick check question: Why does training on new tasks typically degrade performance on previous tasks in standard neural networks, and how does separating memory from parameters address this?

## Architecture Onboarding

- **Component map**: Input trajectory → Transformer embedding → Working memory update/retrieve → MLP action generation
- **Critical path**: The working memory update and retrieval steps are critical for task-specific adaptation, determining how well the system accesses and uses stored knowledge.
- **Design tradeoffs**: Trades model size (smaller than MDT) for explicit memory mechanisms and parameter-efficient fine-tuning. Memory size balances capacity against computational efficiency of attention-based retrieval.
- **Failure signatures**: Poor generalization on new tasks (memory not storing relevant information), slow training convergence (memory update ineffective), instability during fine-tuning (LoRA parameters not properly adapted), or performance degradation on previous tasks indicating catastrophic forgetting.
- **First 3 experiments**:
  1. Test memory update and retrieval independently by feeding known patterns and verifying correct storage and recall.
  2. Validate LoRA fine-tuning by training on simple task with limited data and measuring adaptation efficiency vs full model fine-tuning.
  3. Benchmark memory size impact by varying capacity and measuring performance on tasks requiring different amounts of task-specific knowledge storage.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the size of the working memory affect DT-Mem's performance?
- **Basis in paper**: The paper investigates memory module size impact, showing performance increases with memory slots up to 1800, then decreases.
- **Why unresolved**: Paper doesn't explain why performance decreases beyond 1800 memory slots, only suggesting a trade-off with training steps.
- **What evidence would resolve it**: Experiments varying training steps while keeping memory slots constant to determine optimal balance.

### Open Question 2
- **Question**: How does LoRA hyper-parameter tuning affect fine-tuning results?
- **Basis in paper**: The paper explores LoRA hyper-parameters, showing rank, lora_dropout, and lora_alpha all affect results.
- **Why unresolved**: Paper doesn't explain why each hyper-parameter affects results, only suggesting smaller rank, smaller dropout, and larger alpha values work better.
- **What evidence would resolve it**: Experiments varying each hyper-parameter individually while keeping others constant to determine optimal values.

### Open Question 3
- **Question**: How does using a hyper-network in DT-Mem affect performance compared to LoRA-based fine-tuning?
- **Basis in paper**: Ablation study shows hyper-network variant exhibits higher variance compared to DT-Mem.
- **Why unresolved**: Paper doesn't explain why hyper-network variant shows higher variance, only suggesting task information uncertainty may be a factor.
- **What evidence would resolve it**: Experiments comparing hyper-network and LoRA-based fine-tuning performance in different scenarios to determine strengths and weaknesses.

## Limitations
- Memory module scalability for diverse and complex tasks remains unclear
- Effectiveness of LoRA fine-tuning with limited adaptation data is uncertain
- Architecture details for content-based addressing and attention mechanisms are not fully specified

## Confidence

- **High Confidence**: Claims about DT-Mem's superior performance compared to MDT on Atari games and Meta-World tasks are supported by experimental results and ablation studies.
- **Medium Confidence**: Claims about memory module reducing catastrophic forgetting are supported by observed improvements but lack direct evidence of forgetting mitigation mechanisms.
- **Low Confidence**: Claims about superiority of attention-based retrieval over k-NN search for internal working memory lack direct comparative evidence.

## Next Checks

1. **Memory Capacity Validation**: Systematically test DT-Mem with varying memory sizes on tasks requiring different amounts of task-specific knowledge storage to determine optimal memory capacity and identify performance degradation points.

2. **Fine-tuning Data Efficiency**: Evaluate DT-Mem's adaptability using different amounts of fine-tuning data (1%, 5%, 20%) to quantify minimum data requirements for effective LoRA-based adaptation and compare against full model fine-tuning.

3. **Attention vs. k-NN Comparison**: Implement k-NN baseline for memory retrieval in DT-Mem and compare performance against attention-based approach across multiple tasks to validate claimed superiority of attention mechanisms for internal working memory.