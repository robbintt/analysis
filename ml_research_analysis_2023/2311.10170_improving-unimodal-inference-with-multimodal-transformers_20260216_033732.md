---
ver: rpa2
title: Improving Unimodal Inference with Multimodal Transformers
arxiv_id: '2311.10170'
source_url: https://arxiv.org/abs/2311.10170
tags:
- multimodal
- unimodal
- branch
- branches
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for improving unimodal model performance
  using multimodal training. The core idea is to co-train unimodal models with a multimodal
  Transformer branch, enabling knowledge transfer from the multimodal branch to unimodal
  branches through a multi-task objective.
---

# Improving Unimodal Inference with Multimodal Transformers

## Quick Facts
- arXiv ID: 2311.10170
- Source URL: https://arxiv.org/abs/2311.10170
- Reference count: 0
- Primary result: Unimodal models improve performance by up to 3% when co-trained with multimodal Transformers through knowledge distillation

## Executive Summary
This paper introduces a framework for improving unimodal model performance by co-training with multimodal Transformers. The approach uses shared feature extraction layers between unimodal and multimodal branches, enabling knowledge transfer from the multimodal branch to unimodal branches through a multi-task objective. The method is architecture-agnostic and works across different data modalities, achieving consistent improvements on dynamic hand gesture recognition, audiovisual emotion recognition, and multimodal sentiment analysis tasks.

## Method Summary
The proposed framework co-trains unimodal models with a multimodal Transformer branch by sharing early feature extraction layers. During training, the multimodal branch acts as a teacher model, transferring knowledge to unimodal branches through soft pseudo-labels via KL-divergence minimization. This enables unimodal branches to learn from the multimodal fusion while maintaining their single-modality inference capability. The approach combines task-specific losses for each branch with knowledge transfer objectives, optimizing shared layers to retain information useful for both unimodal and multimodal inference.

## Key Results
- Dynamic hand gesture recognition: Unimodal RGB model improved by 1.2% accuracy; Depth model improved by 0.8% accuracy
- Audiovisual emotion recognition: Speech model improved by 2.5% correlation with human annotations; Video model improved by 1.8% correlation
- Multimodal sentiment analysis: All unimodal branches (audio, video, text) showed consistent improvements of 1-3% in accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multimodal branch transfers high-level decision information to unimodal branches through knowledge distillation
- Mechanism: During training, the multimodal branch acts as a teacher model, providing soft pseudo-labels to unimodal branches via KL-divergence minimization. This forces unimodal branches to learn the decision boundaries learned by the multimodal model.
- Core assumption: The multimodal branch is stronger at the task than the unimodal branches, making it an effective teacher
- Evidence anchors:
  - [abstract] "By co-training these branches, the stronger multimodal branch can transfer its knowledge to the weaker unimodal branches through a multi-task objective"
  - [section 3.3] "we formulate knowledge transfer as knowledge distillation task [20] and optimize KL-divergence LKL kt between soft pseudo-labels generated by multimodal branch and softmax outputs of unimodal branches"
  - [corpus] Weak - no direct citations about knowledge distillation in multimodal settings
- Break condition: If the multimodal branch performs worse than unimodal branches, the knowledge transfer would degrade performance rather than improve it

### Mechanism 2
- Claim: Sharing early feature extraction layers between unimodal and multimodal branches forces those layers to learn modality-agnostic representations
- Mechanism: The shared backbone layers receive gradient updates from both unimodal and multimodal task-specific objectives, forcing them to retain information useful for both modalities and avoiding modality-specific information loss
- Core assumption: Early features contain modality-agnostic information that benefits both unimodal and multimodal inference
- Evidence anchors:
  - [abstract] "Early feature extraction layers are shared between multimodal Transformer and corresponding unimodal branches"
  - [section 3.1] "Shared feature layers receive gradient updates from task-specific objectives of both uni- and multimodal branches, hence forcing them to remain informative for both inference paths"
  - [corpus] Weak - no direct citations about shared feature learning in multimodal settings
- Break condition: If early features are too modality-specific, sharing would create conflicting gradients and degrade performance

### Mechanism 3
- Claim: Multimodal attention patterns improve unimodal temporal understanding
- Mechanism: When unimodal branches are also Transformer-based, the attention-level alignment transfers attention probability distributions from multimodal to unimodal branches, allowing unimodal models to learn which tokens are most important when considering cross-modal relationships
- Core assumption: Attention patterns learned in multimodal context provide useful guidance for unimodal processing
- Evidence anchors:
  - [section 3.3] "tokens in multimodal Transformer attend to tokens of other modalities globally via self-attention in cross-modal Transformer blocks"
  - [section 3.3] "The softmax probabilities of unimodal self-attention in final stages of multimodal Transformer can then be distilled to the corresponding unimodal branches"
  - [corpus] Weak - no direct citations about attention transfer in multimodal settings
- Break condition: If attention patterns are modality-specific and not generalizable, the transfer would provide misleading guidance

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The method relies on transferring knowledge from a stronger multimodal model to weaker unimodal models through soft label supervision
  - Quick check question: What temperature is used for the softmax in the knowledge distillation objective?

- Concept: Cross-modal Attention Mechanisms
  - Why needed here: The multimodal branch uses cross-attention to fuse information from different modalities, which is central to how the model learns multimodal representations
  - Quick check question: In cross-attention, which modality provides the queries and which provides the key-value pairs?

- Concept: Multi-task Learning Objectives
  - Why needed here: The training combines task-specific objectives for each branch with knowledge transfer objectives, requiring understanding of how to balance multiple loss functions
  - Quick check question: What are the three components of the overall training objective equation?

## Architecture Onboarding

- Component map:
  - Input layer → Shared backbone → Unimodal branches + Multimodal Transformer branch → Task-specific heads
  - Knowledge transfer connections from multimodal branch to unimodal branches
  - Three loss functions: task-specific losses for each branch, knowledge transfer loss

- Critical path:
  - Forward pass through shared layers
  - Parallel processing through unimodal and multimodal branches
  - Loss computation from all branches
  - Backward pass updating shared layers and unimodal branches (multimodal branch updated only from its own task loss)

- Design tradeoffs:
  - Shared vs separate early layers: Sharing reduces parameters but creates coupling between modalities
  - Choice of knowledge transfer objective: Decision-level (soft labels) vs feature-level (cosine similarity) vs attention-level (attention distributions)
  - Placement of multimodal branch: Too early loses modality-specific information, too late reduces fusion benefits

- Failure signatures:
  - Unimodal performance worse than baseline: Likely issues with knowledge transfer loss weight or conflicting gradients from shared layers
  - Multimodal performance degradation: Too much gradient from unimodal branches may force multimodal branch to specialize in unimodal patterns
  - Training instability: Learning rates for shared layers may need to be lower than for branch-specific layers

- First 3 experiments:
  1. Train unimodal baselines on each modality to establish performance floor
  2. Train multimodal model with shared layers but no knowledge transfer to measure baseline multimodal performance
  3. Train full proposed model with knowledge transfer and compare unimodal and multimodal performance to baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework perform when applied to modalities with drastically different data types and temporal resolutions (e.g., video and audio)?
- Basis in paper: [inferred] The paper mentions that existing methods are primarily suited for well-paired modalities like RGB and Depth, while having limited suitability for modalities with drastically different data types. The authors aim to overcome this issue with their generalized framework.
- Why unresolved: The paper evaluates the framework on RGB/Depth, audio/video, and audio/video/text combinations, but does not explore modalities with significantly different temporal characteristics or data types beyond these examples.
- What evidence would resolve it: Experiments applying the framework to truly heterogeneous modality pairs (e.g., video and text transcripts, or sensor data and images) and measuring performance improvements across diverse temporal and structural characteristics.

### Open Question 2
- Question: What is the optimal knowledge transfer loss weight (α) for different modality combinations and task types?
- Basis in paper: [explicit] The paper includes ablation studies showing results with different α values on the EgoGesture dataset, finding α=5 optimal for that case, but states that the knowledge transfer loss weight is a hyperparameter.
- Why unresolved: The paper only provides results for one dataset and modality combination, and the optimal α may vary depending on the specific modalities, architectures, and tasks involved.
- What evidence would resolve it: Systematic experiments varying α across multiple datasets, modality combinations, and task types to identify patterns or rules for setting this hyperparameter.

### Open Question 3
- Question: How does the performance of unimodal models improve when the multimodal branch is trained with more modalities than those used for unimodal inference?
- Basis in paper: [inferred] The paper shows that multimodal training improves unimodal performance, and the multimodal branch also benefits from co-training. However, it doesn't explore scenarios where the multimodal branch is trained with additional modalities not available during unimodal inference.
- Why unresolved: The experiments focus on matching the modalities between multimodal and unimodal branches, without exploring the potential benefits of training the multimodal branch with additional information.
- What evidence would resolve it: Experiments comparing unimodal performance when the multimodal branch is trained with additional modalities versus only the modalities used for unimodal inference.

## Limitations

- Limited modality diversity: Experiments only cover RGB/Depth, audio/video, and audio/video/text combinations, not truly heterogeneous modalities
- Architecture dependence: Despite claims of being architecture-agnostic, specific implementation details for unimodal branches are not fully specified
- Modest improvements: Performance gains of 1-3% suggest incremental rather than transformative improvements

## Confidence

**High confidence**: The core architectural design (shared layers with separate unimodal and multimodal branches) is technically sound and well-grounded in existing multi-task learning literature. The basic concept of knowledge distillation for transfer learning is established.

**Medium confidence**: The empirical results showing performance improvements are convincing for the tested tasks, but the magnitude of improvements (1-3% accuracy) suggests modest rather than transformative gains. The claim that multimodal branches also improve is interesting but requires more ablation studies.

**Low confidence**: The assertion that this approach works "regardless of the underlying unimodal architectures and data modalities" is not sufficiently supported by the experimental evidence, which covers a limited set of modalities and architectures.

## Next Checks

1. **Architecture sensitivity analysis**: Systematically test the framework with different unimodal architectures (CNNs, RNNs, Transformers) on the same datasets to verify the claimed architecture-agnostic property.

2. **Multimodal branch strength validation**: Conduct controlled experiments where the multimodal branch is intentionally weakened (fewer parameters, lower capacity) to determine the minimum performance threshold needed for effective knowledge transfer.

3. **Cross-task generalization study**: Apply the framework to a completely different multimodal task domain (e.g., medical imaging with text reports) to test the claimed modality-agnostic nature and identify any hidden assumptions about the types of tasks where this approach works best.