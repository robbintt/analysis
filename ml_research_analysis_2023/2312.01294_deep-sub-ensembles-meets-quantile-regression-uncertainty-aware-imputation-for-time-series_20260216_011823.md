---
ver: rpa2
title: 'Deep sub-ensembles meets quantile regression: uncertainty-aware imputation
  for time series'
arxiv_id: '2312.01294'
source_url: https://arxiv.org/abs/2312.01294
tags:
- time
- series
- imputation
- quantile
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for probabilistic time series imputation
  that estimates uncertainty while maintaining computational efficiency. The approach
  combines deep ensembles with quantile regression in a single model, using multiple
  linear layers to learn different quantiles simultaneously.
---

# Deep sub-ensembles meets quantile regression: uncertainty-aware imputation for time series

## Quick Facts
- arXiv ID: 2312.01294
- Source URL: https://arxiv.org/abs/2312.01294
- Reference count: 40
- Primary result: Proposed method achieves lower MAE and CRPS than baseline approaches while using fewer parameters and faster inference times

## Executive Summary
This paper introduces a probabilistic time series imputation method that combines deep ensembles with quantile regression within a single model architecture. The approach uses multiple linear layers to learn different quantiles simultaneously, integrated into bidirectional recurrent neural networks to capture temporal dependencies and quantify uncertainty during imputation. The method demonstrates superior performance compared to baseline approaches on healthcare and air quality datasets, particularly at high missing rates, while maintaining computational efficiency compared to score-based diffusion methods.

## Method Summary
The proposed method integrates quantile ensembles with bidirectional recurrent neural networks to perform probabilistic time series imputation. Multiple randomly initialized linear layers are assembled to simultaneously learn different quantiles in a single network, reducing the computational overhead of traditional deep ensembles. The model captures temporal dependencies in both forward and backward directions while quantifying uncertainty through quantile regression with Pinball loss. The architecture includes complement layers for initial missing value handling, temporal decay modeling, feature-based estimation, and combining layers to produce the final imputed time series with uncertainty quantification.

## Key Results
- Achieves lower mean absolute error (MAE) and continuous ranked probability score (CRPS) compared to existing methods
- Demonstrates significant computational efficiency with faster training and inference times while using fewer model parameters
- Shows superior performance particularly at high missing rates in healthcare and air quality datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shared model backbone in Quantile Ensembles reduces computational overhead while maintaining performance.
- Mechanism: Instead of training multiple independent networks, a single model backbone is used with multiple linear layers, each learning different quantiles simultaneously.
- Core assumption: The shared backbone can learn representations that benefit all quantile predictions.
- Evidence anchors: [abstract]: "the shared model backbone tremendously reduces most of the computation overhead of the multiple ensembles"

### Mechanism 2
- Claim: Quantile regression provides uncertainty estimation while maintaining robustness to outliers.
- Mechanism: The Pinball loss function directly optimizes for different quantiles rather than just the mean, capturing the conditional distribution of the data.
- Core assumption: The data distribution can be reasonably approximated by the chosen quantile levels.
- Evidence anchors: [section]: "To optimize and fit the different quantile imputation values, we adapt the quantile regression loss instead of the MAE (Mean Absolute Error) loss"

### Mechanism 3
- Claim: Combining deep ensembles with bidirectional recurrent networks captures both temporal dependencies and uncertainty.
- Mechanism: The bidirectional processing captures contextual information from both past and future, while the ensemble component quantifies uncertainty in the predictions.
- Core assumption: Temporal dependencies in the data are bidirectional and that uncertainty quantification improves imputation reliability.
- Evidence anchors: [section]: "we leverage quantile ensembles to capture the uncertainty of the time series imputation and propose to equip the bidirectional recurrent neural networks... to quantify uncertainty"

## Foundational Learning

- Concept: Quantile regression and Pinball loss
  - Why needed here: This is the core mechanism for uncertainty quantification and probabilistic imputation
  - Quick check question: What does the Pinball loss function optimize for, and how does it differ from mean squared error?

- Concept: Recurrent neural networks and bidirectional processing
  - Why needed here: BRNNs are the base model for capturing temporal dependencies in time series data
  - Quick check question: How does bidirectional processing differ from standard RNN processing, and when is it most beneficial?

- Concept: Deep ensembles and their computational cost
  - Why needed here: Understanding the computational trade-offs is crucial for appreciating the efficiency gains of Quantile Ensembles
  - Quick check question: What are the main computational costs of traditional deep ensembles, and how does the shared backbone approach address these?

## Architecture Onboarding

- Component map: Input → Complement → Temporal decay + Feature-based → Quantile ensembles → BRNN → Output
- Critical path: Input → Complement → Temporal decay + Feature-based → Quantile ensembles → BRNN → Output
- Design tradeoffs:
  - More quantile levels increase uncertainty coverage but also computational cost
  - Bidirectional processing captures more context but doubles the computation for temporal layers
  - The shared backbone reduces parameters but may limit the diversity of learned representations
- Failure signatures:
  - High CRPS but low MAE: The model is overconfident in its predictions
  - Low CRPS but high MAE: The uncertainty estimates are too wide, not capturing the true distribution
  - Degradation in performance with more quantile levels: The shared backbone cannot effectively learn representations for all quantiles
- First 3 experiments:
  1. Train with only 1 quantile level (50th percentile) to establish baseline performance
  2. Add 2-3 quantile levels and compare CRPS to assess uncertainty quantification improvement
  3. Vary the number of quantile levels (5, 10, 15) to find the optimal tradeoff between performance and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of quantiles (B) for the proposed method across different types of time series data?
- Basis in paper: [explicit] The authors mention that the number of quantiles can be adjusted according to the specific task
- Why unresolved: The paper does not provide a systematic study on how to determine the optimal number of quantiles for different datasets or scenarios
- What evidence would resolve it: A comprehensive study showing the performance of the method with varying numbers of quantiles across multiple datasets

### Open Question 2
- Question: How does the proposed method perform on non-stationary time series data?
- Basis in paper: [inferred] The paper focuses on stationary time series data and does not explicitly discuss the method's performance on non-stationary data
- Why unresolved: The experiments are conducted on stationary datasets, and the paper does not address the potential challenges or adaptations needed for non-stationary data
- What evidence would resolve it: Testing the method on non-stationary time series datasets and comparing its performance with existing methods

### Open Question 3
- Question: Can the proposed quantile ensembles be extended to other time series tasks beyond imputation?
- Basis in paper: [explicit] The authors suggest that quantile ensembles could be applied to other time series tasks or extended to downstream applications
- Why unresolved: The paper only demonstrates the method's effectiveness in time series imputation
- What evidence would resolve it: Applying the quantile ensembles framework to other time series tasks (e.g., forecasting, anomaly detection) and evaluating its performance

## Limitations
- Limited validation on computational efficiency claims due to unspecified architectural details
- Performance evaluation restricted to two specific datasets (healthcare and air quality)
- Potential unnecessary computational overhead from bidirectional processing in unidirectional scenarios

## Confidence
- High confidence in the core mechanism of combining quantile regression with BRNNs
- Medium confidence in computational efficiency claims due to lack of detailed specifications
- Medium confidence in generalizability beyond tested datasets

## Next Checks
1. **Architectural Verification**: Reconstruct the exact linear layer dimensions and initialization scheme for the quantile ensembles and verify that the computational efficiency claims hold under controlled conditions.

2. **Dataset Diversity Test**: Evaluate the method on additional time series datasets with different characteristics (e.g., financial, sensor networks) to assess generalizability beyond healthcare and air quality domains.

3. **Unidirectional Comparison**: Implement a unidirectional variant of the model and compare its performance and computational cost against the bidirectional version to determine when bidirectional processing provides meaningful benefits versus unnecessary overhead.