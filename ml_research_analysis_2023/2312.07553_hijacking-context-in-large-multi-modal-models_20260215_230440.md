---
ver: rpa2
title: Hijacking Context in Large Multi-modal Models
arxiv_id: '2312.07553'
source_url: https://arxiv.org/abs/2312.07553
tags:
- context
- contexts
- hijacked
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new vulnerability in large multi-modal
  models (LMMs) where a small fraction of incoherent images or text descriptions can
  hijack the model's output, causing it to generate biased responses based on the
  irrelevant context rather than the intended one. To address this, the authors propose
  a pre-filtering method that removes irrelevant contexts using GPT-4V, leveraging
  its robustness to distribution shifts.
---

# Hijacking Context in Large Multi-modal Models

## Quick Facts
- arXiv ID: 2312.07553
- Source URL: https://arxiv.org/abs/2312.07553
- Authors: 
- Reference count: 27
- Key outcome: This paper identifies a new vulnerability in large multi-modal models (LMMs) where a small fraction of incoherent images or text descriptions can hijack the model's output, causing it to generate biased responses based on the irrelevant context rather than the intended one.

## Executive Summary
This paper identifies a novel vulnerability in large multi-modal models (LMMs) where a small fraction of irrelevant or incoherent contexts can hijack the model's output, causing it to generate responses biased towards the irrelevant information rather than the intended context. The authors propose a pre-filtering method using GPT-4V to remove irrelevant contexts and investigate whether replacing hijacked contexts with correlated ones can help produce coherent responses.

## Method Summary
The authors create test cases with coherent and hijacked contexts using the VIST dataset and FROMAGe LMM. They implement GPT-4V pre-filtering using few-shot prompting to identify irrelevant contexts in image-text pairs. The approach involves using GPT-4 to rewrite irrelevant sentences and DALL-E 3 to generate corresponding images, then testing the effectiveness of these reformed contexts in producing coherent LMM responses.

## Key Results
- A small fraction of incoherent images or text descriptions can mislead LMMs to generate biased outputs focused on irrelevant context
- GPT-4V can robustly filter irrelevant contexts due to its robustness to distribution shifts within contexts
- Replacing hijacked contexts with correlated ones via GPT-4 and text-to-image models shows limited success in producing coherent responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small fraction of incoherent images or text descriptions can hijack LMMs to generate biased outputs focused on the irrelevant context rather than the intended one.
- Mechanism: LMMs use in-context learning where a sequence of image-text pairs is provided as context. When an irrelevant pair is inserted, the model's attention is drawn to this minority context, causing it to generate responses biased towards this irrelevant information instead of the majority coherent context.
- Core assumption: LMMs treat all provided context equally and are susceptible to minority context influence.
- Evidence anchors:
  - [abstract] "a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context."
  - [section] "when only a single image-text pair that contains an irrelevant subject is appended to the context, LMM falls into the hijacked context and loses coherency concerning the bunch of formerly given original contexts."
  - [corpus] Weak. No direct corpus evidence, but related works on context hijacking in LLMs support this mechanism.
- Break condition: If LMMs were designed to prioritize majority context or have built-in mechanisms to detect and downweight irrelevant context, this hijacking would be less effective.

### Mechanism 2
- Claim: GPT-4V can robustly filter irrelevant contexts due to its robustness to distribution shifts within the contexts.
- Mechanism: GPT-4V is prompted with the sequence of image-text pairs and asked to identify any irrelevant sentences or images. Its robustness to distribution shifts allows it to correctly identify the hijacked context even when it's embedded within a coherent story.
- Core assumption: GPT-4V's robustness to distribution shifts enables it to identify irrelevant contexts that LMMs are susceptible to.
- Evidence anchors:
  - [abstract] "we propose a pre-filtering method that removes irrelevant contexts via GPT-4V, based on its robustness towards distribution shift within the contexts."
  - [section] "we observed that GPT-4 and GPT-4V have the capability to filter irrelevant textual or visual information via a simple few-shot prompting, respectively."
  - [corpus] Weak. No direct corpus evidence, but the paper's results demonstrate this capability.
- Break condition: If the irrelevant context becomes the majority or if GPT-4V itself is confused by the distribution shift, its filtering capability may degrade.

### Mechanism 3
- Claim: Replacing hijacked contexts with correlated ones via GPT-4 and text-to-image models can help produce coherent responses.
- Mechanism: GPT-4 is used to identify irrelevant sentences and replace them with appropriate alternatives that maintain coherency. Then, a text-to-image model generates a coherent image corresponding to the new sentence. This reformed context is provided to the LMM, which may then generate a more coherent response.
- Core assumption: Providing a reformed, coherent context will guide the LMM to generate a coherent response.
- Evidence anchors:
  - [abstract] "we further investigate whether replacing the hijacked visual and textual contexts with the correlated ones via GPT-4V and text-to-image models can help yield coherent responses."
  - [section] "we instructed GPT-4 to identify any irrelevant sentences and replace them with appropriate alternative sentences that convey coherency given in the contexts. Subsequently, we prompted text-to-image diffusion models to generate a coherent image corresponding to the newly replaced sentence."
  - [corpus] Weak. No direct corpus evidence, but the paper's results show limited success with this approach.
- Break condition: If the reformed captions still contain elements of the hijacked context or if the generated images are not realistic or consistent with the original image sequence, the LMM may still produce incoherent responses.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: LMMs use in-context learning to understand and respond to visual and textual information. Understanding this mechanism is crucial for understanding how hijacking contexts can mislead LMMs.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in LMMs?

- Concept: Distribution shift
  - Why needed here: GPT-4V's robustness to distribution shifts is leveraged to filter irrelevant contexts. Understanding distribution shift is key to understanding why GPT-4V can identify irrelevant contexts that LMMs cannot.
  - Quick check question: What is distribution shift and how does it affect the performance of machine learning models?

- Concept: Text-to-image generation
  - Why needed here: Text-to-image models are used to generate coherent images corresponding to reformed text descriptions. Understanding this technology is important for understanding how the reformed context is created.
  - Quick check question: How do text-to-image models work and what are their limitations?

## Architecture Onboarding

- Component map: LMM (e.g., FROMAGe) -> GPT-4V -> GPT-4 -> Text-to-image model (e.g., DALLE-3)
- Critical path:
  1. Provide sequence of image-text pairs to GPT-4V.
  2. GPT-4V identifies and filters irrelevant contexts.
  3. Provide filtered context to LMM.
  4. LMM generates response based on filtered context.
- Design tradeoffs:
  - Using GPT-4V for filtering adds an extra step but leverages its robustness to distribution shifts.
  - Reforming hijacked contexts with GPT-4 and text-to-image models is an open question and may not always produce coherent results.
- Failure signatures:
  - LMM generates responses biased towards irrelevant contexts.
  - GPT-4V fails to identify irrelevant contexts or incorrectly filters relevant contexts.
  - Reformed contexts still contain elements of hijacked context or generated images are not realistic.
- First 3 experiments:
  1. Test GPT-4V's ability to filter irrelevant contexts in various scenarios (e.g., different locations of hijacked context, different types of irrelevant content).
  2. Test the effectiveness of reformed contexts in producing coherent LMM responses.
  3. Investigate the impact of the sequential location of hijacked context on the coherency of LMM responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the extent to which GPT-4V's robustness to context hijacking can be distilled into smaller LMMs to improve their resistance to irrelevant contexts?
- Basis in paper: [explicit] The paper suggests GPT-4V's robustness to context hijacking as a potential avenue for future research, particularly in enhancing smaller LMMs.
- Why unresolved: While GPT-4V shows robustness, the mechanism behind this robustness and how it can be effectively transferred to smaller models is not yet understood.
- What evidence would resolve it: Experiments demonstrating successful distillation of GPT-4V's robustness into smaller LMMs, along with analysis of the transfer mechanism, would provide insights.

### Open Question 2
- Question: How does the sequential location of hijacked context within a story affect the likelihood of LMMs generating coherent responses?
- Basis in paper: [explicit] The paper explores the effect of the sequential location of hijacked context on the coherency of LMM responses, noting that context closer to the query image increases the likelihood of incoherent responses.
- Why unresolved: The paper provides qualitative observations but lacks a comprehensive quantitative analysis of how different positions of hijacked contexts impact LMM performance.
- What evidence would resolve it: A detailed quantitative study examining LMM responses to hijacked contexts placed at various positions within a sequence, correlating the location with response coherence.

### Open Question 3
- Question: Can reforming hijacked contexts with generated text and images consistently lead to more coherent responses from LMMs, and what factors influence this outcome?
- Basis in paper: [explicit] The paper investigates the impact of reforming hijacked contexts on LMM coherence, suggesting that while the approach has potential, its effectiveness is limited by factors such as the realism of generated images and the clarity of reformed captions.
- Why unresolved: The paper's findings are preliminary and based on a limited dataset, leaving questions about the generalizability and effectiveness of context reforming across different LMMs and scenarios.
- What evidence would resolve it: Extensive experiments across various LMMs and datasets, analyzing the impact of reformed contexts on response coherence and identifying key factors that influence success.

## Limitations

- The evaluation relies on a single LMM (FROMAGe) and a specific dataset (VIST), limiting generalizability to other multi-modal architectures
- The pre-filtering approach using GPT-4V is only tested on simple text replacement scenarios, with the paper noting that replacing hijacked images remains an open question
- The effectiveness of the proposed solution in real-world scenarios where context hijacks might be more sophisticated or adversarial remains unclear

## Confidence

- **High Confidence**: The core observation that LMMs can be misled by a small fraction of irrelevant contexts
- **Medium Confidence**: GPT-4V's ability to filter irrelevant contexts more effectively than LMMs
- **Low Confidence**: The effectiveness of context reformation using GPT-4 and text-to-image models to produce coherent responses

## Next Checks

1. **Cross-model validation**: Test the context hijacking vulnerability across multiple LMM architectures (e.g., Flamingo, BLIP-2, LLaVA) and different datasets to assess generalizability.

2. **Robustness stress testing**: Systematically vary the position, semantic similarity, and minority percentage of hijacked contexts to identify breaking points in both the hijacking mechanism and GPT-4V's filtering capability.

3. **Real-world adversarial evaluation**: Create more sophisticated hijacking scenarios that mimic potential real-world attacks, such as gradual context drift, multi-modal contradictions, or context injection through external sources.