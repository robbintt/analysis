---
ver: rpa2
title: 'Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with
  Causality Perspectives'
arxiv_id: '2307.16851'
source_url: https://arxiv.org/abs/2307.16851
tags:
- learning
- causal
- data
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews advancements in trustworthy
  machine learning, focusing on robustness, adversarial robustness, interpretability,
  and fairness from a data-centric perspective. The paper highlights the limitations
  of traditional empirical risk minimization (ERM) training in handling challenges
  posed by data structures like spurious features and confounding factors.
---

# Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives

## Quick Facts
- arXiv ID: 2307.16851
- Source URL: https://arxiv.org/abs/2307.16851
- Reference count: 40
- Key outcome: This survey systematically reviews advancements in trustworthy machine learning, focusing on robustness, adversarial robustness, interpretability, and fairness from a data-centric perspective. The paper highlights the limitations of traditional empirical risk minimization (ERM) training in handling challenges posed by data structures like spurious features and confounding factors. Interestingly, the authors observe a convergence of methods across different subfields, despite being developed independently. Pearl's hierarchy of causality offers a unifying framework for these techniques, connecting them under a data-centric perspective. The survey also explores the trustworthiness of large pretrained models, summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback. Connections are drawn between these techniques and standard ERM, extending the principled understanding of trustworthy methods to large pretrained models. The paper provides a unified language with mathematical vocabulary to link methods across different topics, fostering a more cohesive understanding of the field.

## Executive Summary
This survey provides a comprehensive data-centric perspective on trustworthy machine learning, unifying methods across robustness, adversarial robustness, interpretability, and fairness. The authors demonstrate how these seemingly disparate techniques can be connected through Pearl's causal hierarchy and master equation frameworks. The survey extends this unification to large pretrained models, showing how techniques like fine-tuning and prompting can be understood as extensions of standard ERM objectives. By providing a unified mathematical vocabulary, the paper enables cross-pollination of ideas between different trustworthy ML subfields and offers insights into the trustworthiness of large models.

## Method Summary
The survey systematically reviews trustworthy machine learning techniques through a data-centric lens, organizing methods by their approach to handling spurious features and confounding factors. The authors introduce master equations that capture the common structure underlying diverse techniques, showing how regularization and augmentation methods share fundamental similarities. The survey then applies Pearl's causal hierarchy to categorize these methods, mapping them to levels of causation (associational, interventional, counterfactual). For large pretrained models, the authors extend the ERM-based framework to encompass fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, demonstrating how trustworthy techniques from standalone models can be applied to large models.

## Key Results
- The survey establishes a unified statistical language connecting trustworthy ML methods across robustness, fairness, and interpretability through master equations
- Pearl's causal hierarchy provides a principled framework for categorizing and connecting trustworthy ML methods across different subfields
- Large pretrained models can be made trustworthy by extending the ERM-based unified framework to techniques like fine-tuning, prompting, and PEFT
- The survey demonstrates convergence of methods across different subfields despite independent development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey establishes a unified statistical language connecting trustworthy ML methods across robustness, fairness, and interpretability.
- Mechanism: By reducing diverse techniques to master equations (e.g., Equation 13 and Equation 14), the survey reveals a common structure of learning causal vs. spurious features through regularization or augmentation.
- Core assumption: All trustworthy ML challenges stem from data structure issues where models learn spurious correlations instead of desired causal features.
- Evidence anchors:
  - [abstract] "we provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness"
  - [section] "we introduce a concrete example for the following-up discussions throughout this survey" (Example 1 about sea turtles vs. tortoise)
  - [corpus] Weak - corpus doesn't contain direct evidence of unified statistical language
- Break Condition: If new trustworthy ML methods cannot be expressed in the master equation framework, the unification fails.

### Mechanism 2
- Claim: Pearl's causal hierarchy provides a principled framework for categorizing and connecting trustworthy ML methods.
- Mechanism: The survey maps ML techniques to Pearl's three levels of causation (L1 associational, L2 intervention, L3 counterfactual), showing how methods like DANN correspond to intervention and counterfactual data augmentation to counterfactual reasoning.
- Core assumption: Causal inference tools from Pearl's hierarchy are applicable to ML robustness, fairness, and interpretability problems.
- Evidence anchors:
  - [abstract] "Pearl's hierarchy of causality offers a unifying framework for these techniques"
  - [section] "we will offer another overview of recent trustworthy machine learning papers, but this time from the perspective of Pearl's causal hierarchy"
  - [corpus] Weak - corpus neighbors don't discuss Pearl's hierarchy application to ML
- Break Condition: If new ML methods cannot be mapped to any level of Pearl's hierarchy, the framework loses its unifying power.

### Mechanism 3
- Claim: Large pretrained models can be made trustworthy by extending the ERM-based unified framework.
- Mechanism: The survey shows that techniques like fine-tuning, prompting, and PEFT can be unified under ERM objectives (Equations 45, 46, 47), allowing application of trustworthy methods from standalone models to large models.
- Core assumption: Large pretrained models fundamentally learn through ERM-like objectives, making trustworthy extensions applicable.
- Evidence anchors:
  - [abstract] "we draw connections between them and the standard ERM"
  - [section] "we will recap the advancements in large models and present different opinions on their trustworthiness"
  - [corpus] Weak - corpus neighbors don't discuss ERM extensions to large models
- Break Condition: If large models use fundamentally different learning mechanisms that cannot be expressed as ERM variants, the extension fails.

## Foundational Learning

- Concept: Causal hierarchy (L1-L3) from Pearl
  - Why needed here: Provides the theoretical framework for understanding different trustworthy ML techniques
  - Quick check question: Can you explain the difference between intervention (L2) and counterfactual (L3) reasoning?

- Concept: Spurious vs. causal features
  - Why needed here: Central to understanding why vanilla ERM fails and why trustworthy methods are needed
  - Quick check question: In the sea turtle vs. tortoise example, what makes the background color a spurious feature?

- Concept: Master equations framework
  - Why needed here: Unifies diverse trustworthy ML methods into a common mathematical structure
  - Quick check question: Can you write the general form of the master equation that captures both DANN-style and adversarial training approaches?

## Architecture Onboarding

- Component map: Survey structure flows from ML topics → causal connections → large models → applications → future directions
- Critical path: Understand ML limitations → learn causal framework → apply to large models → explore applications
- Design tradeoffs: Comprehensive coverage vs. depth in each topic; technical rigor vs. accessibility
- Failure signatures: Inability to connect methods across topics; difficulty mapping to causal hierarchy; challenges applying to large models
- First 3 experiments:
  1. Map a specific method (e.g., DANN) to its corresponding level in Pearl's hierarchy
  2. Express a fine-tuning approach as an ERM extension and identify which trustworthy techniques could be applied
  3. Analyze a concrete example (like the sea turtle case) through both statistical and causal lenses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically evaluate and compare the effectiveness of different techniques for improving trustworthiness in large pretrained models?
- Basis in paper: [explicit] The paper discusses various techniques for enhancing trustworthiness in large models, including fine-tuning, prompting, parameter-efficient tuning, and RLHF, but does not provide a comprehensive evaluation framework.
- Why unresolved: Different techniques target different aspects of trustworthiness (robustness, fairness, interpretability), making it challenging to establish a unified evaluation metric.
- What evidence would resolve it: Development of standardized benchmarks and evaluation protocols that can assess the effectiveness of various techniques across different trustworthiness dimensions.

### Open Question 2
- Question: How can we ensure that large pretrained models are not only robust to distribution shifts but also fair and interpretable?
- Basis in paper: [explicit] The paper highlights the challenges of fairness and interpretability in large models, suggesting that current techniques may not adequately address these issues.
- Why unresolved: Fairness and interpretability often involve complex social and ethical considerations that are difficult to quantify and address through purely technical means.
- What evidence would resolve it: Development of techniques that can simultaneously improve robustness, fairness, and interpretability in large models, along with rigorous evaluation methods to assess their effectiveness.

### Open Question 3
- Question: How can we effectively combine techniques from different areas of trustworthy machine learning (e.g., causality, robustness, fairness) to create models that are more trustworthy overall?
- Basis in paper: [explicit] The paper discusses the potential benefits of combining techniques from different areas, but does not provide concrete examples or guidelines for doing so.
- Why unresolved: Combining techniques from different areas may introduce new challenges and trade-offs that need to be carefully considered.
- What evidence would resolve it: Development of frameworks and methodologies for integrating techniques from different areas of trustworthy machine learning, along with empirical studies demonstrating their effectiveness.

## Limitations

- The survey's unified framework relies heavily on abstract connections between causal inference and ML trustworthiness, requiring empirical validation for practical utility
- The claim about convergence of methods across subfields is observational rather than rigorously proven
- The practical impact of the causal framework on real-world trustworthy ML applications needs empirical validation

## Confidence

- High confidence: The survey provides a comprehensive literature review with accurate technical descriptions of individual methods
- Medium confidence: The unification through master equations is logically sound but may oversimplify method differences
- Low confidence: The practical impact of the causal framework on real-world trustworthy ML applications needs empirical validation

## Next Checks

1. Implement a representative method from each trustworthy ML subfield (robustness, fairness, interpretability) and test whether they can indeed be expressed in the master equation framework with the same regularization structure
2. Design a case study where mapping a method to Pearl's hierarchy level leads to actionable insights or improvements in model performance, validating the practical utility of the causal framework
3. Conduct a systematic comparison of large pretrained model fine-tuning approaches against the ERM extension framework to verify if trustworthy techniques from standalone models can be effectively applied to large models