---
ver: rpa2
title: '3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and
  Cross-Modal Distillation'
arxiv_id: '2309.04062'
source_url: https://arxiv.org/abs/2309.04062
tags:
- molecular
- graph
- pretraining
- representations
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes D&D, a self-supervised pretraining framework
  for molecular property prediction. D&D performs denoising on 3D conformers followed
  by cross-modal knowledge distillation to a 2D graph encoder.
---

# 3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation

## Quick Facts
- **arXiv ID**: 2309.04062
- **Source URL**: https://arxiv.org/abs/2309.04062
- **Reference count**: 40
- **Primary result**: D&D achieves strong performance on molecular property prediction by pretraining a 2D graph encoder through 3D conformer denoising and cross-modal distillation

## Executive Summary
This paper introduces D&D, a self-supervised pretraining framework for molecular property prediction that bridges 3D and 2D representations. The method performs denoising on 3D molecular conformers to learn force field information, then transfers this knowledge to a 2D graph encoder through cross-modal knowledge distillation. This approach enables 2D models to capture 3D geometric information without requiring conformers at test time. Experiments on OGB and physical property datasets demonstrate that D&D significantly outperforms existing methods like 3DInfomax and shows strong label efficiency.

## Method Summary
D&D consists of two pretraining stages followed by downstream finetuning. First, a 3D conformer encoder (TorchMD-NET) is pretrained by denoising perturbed 3D coordinates, learning an approximation of the force field. Second, a 2D graph encoder (TokenGT) is pretrained via cross-modal distillation, where it learns to mimic the 3D representations using only 2D graph topology and bond features. The method offers two distillation variants: D&D-GRAPH (graph-level representations) and D&D-NODE (node-level representations). After pretraining, the 2D encoder is finetuned on downstream molecular property prediction tasks without access to 3D conformers.

## Key Results
- D&D significantly outperforms 3DInfomax and other baselines on OGB and physical property datasets
- The 2D encoder trained via D&D can infer 3D information based on 2D graph structure during inference
- D&D demonstrates strong label efficiency, requiring fewer labeled examples for competitive performance
- D&D-GRAPH performs competitively against 3DInfomax on 7 out of 8 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal distillation transfers 3D geometry information from conformer denoising into a 2D graph encoder without requiring conformers at test time.
- Mechanism: The 3D teacher encoder learns representations from denoising perturbed conformers, which approximates the underlying force field. The 2D student then learns to mimic these 3D representations using only 2D graph topology and bond features via distillation loss.
- Core assumption: 3D conformer representations contain generalizable geometric and force field information that can be encoded into 2D graph features.
- Evidence anchors:
  - [abstract] "D&D performs denoising on 3D conformers followed by cross-modal knowledge distillation to a 2D graph encoder."
  - [section] "Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph"
  - [corpus] "Average neighbor FMR=0.359" - weak correlation with denoising-based approaches
- Break condition: If 2D graph topology and bond features cannot encode the necessary 3D information, distillation will fail to generalize to unseen molecules.

### Mechanism 2
- Claim: The 2D graph encoder learns to align its intermediate attention patterns with 3D pairwise distances during inference.
- Mechanism: Through distillation, the 2D encoder adjusts its attention weights so that atoms closer in 3D space exchange more information, even though only 2D graph structure is available.
- Core assumption: Attention-based architectures can learn geometric reasoning from distillation without explicit 3D coordinates.
- Evidence anchors:
  - [section] "the intermediate representations of the 2D student also aligns well with 3D geometry"
  - [section] "do atoms nearby in the 3D space tend to attend to each other?"
  - [corpus] "Top related titles: Two-Stage Pretraining for Molecular Property Prediction in the Wild" - suggests pretraining effectiveness
- Break condition: If attention patterns don't correlate with 3D distances, the model hasn't learned meaningful geometric reasoning.

### Mechanism 3
- Claim: Pretraining via denoising provides better initial parameters for downstream finetuning than random initialization or contrastive methods.
- Mechanism: The denoising objective learns an approximation of the force field, providing a parameter initialization that's closer to optimal for various molecular property tasks.
- Core assumption: Force field information is broadly useful across different molecular property prediction tasks.
- Evidence anchors:
  - [abstract] "Experiments on the OGB benchmark and manually curated physical property datasets show that D&D leads to significant knowledge transfer"
  - [section] "D&D shows performs competitively against 3DInfomax on 7 out of 8 datasets"
  - [corpus] "Cross-Modal Learning for Chemistry Property Prediction: Large Language Models Meet Graph Machine Learning" - indicates cross-modal approaches are valuable
- Break condition: If force field information isn't broadly applicable to the downstream tasks, the pretraining advantage disappears.

## Foundational Learning

- Concept: Graph neural networks and their permutation-equivariant properties
  - Why needed here: The 2D graph encoder must process molecular graphs while respecting their symmetry properties
  - Quick check question: What ensures that GNNs produce the same output regardless of node ordering in the input graph?

- Concept: SE(3) equivariance in 3D molecular representations
  - Why needed here: The 3D conformer encoder must produce representations invariant to rotations and translations of the molecule
  - Quick check question: Why don't we need reflection invariance for molecular properties?

- Concept: Knowledge distillation techniques and their variants
  - Why needed here: The cross-modal distillation step transfers knowledge from 3D to 2D representations
  - Quick check question: What's the difference between structured KD and traditional KD in terms of what's being distilled?

## Architecture Onboarding

- Component map:
  - 3D Teacher (TorchMD-NET) -> 2D Student (TokenGT) -> Downstream Finetuning
  - Pretraining: Denoising on 3D conformers -> Distillation to 2D encoder -> Finetuning on molecular properties

- Critical path: Denoising pretraining → Distillation pretraining → Downstream finetuning
  - Each step depends on the previous one being completed successfully

- Design tradeoffs:
  - Using mean-pooled vs node-level distillation: Tradeoff between ease of training and fine-grained geometric information
  - Attention-based vs message-passing GNN for 2D student: Attention allows geometric reasoning analysis but may be more computationally intensive
  - Number of conformers per molecule: More conformers provide better force field approximation but increase computational cost

- Failure signatures:
  - Distillation loss doesn't converge or has large train/validation gap
  - Downstream performance matches or is worse than random initialization
  - Attention patterns don't correlate with 3D distances during inference

- First 3 experiments:
  1. Run D&D-GRAPH distillation on PCQM4Mv2 and verify the distillation loss converges with small generalization gap
  2. Check attention pattern correlation with 3D distances on the validation set after D&D-NODES distillation
  3. Finetune on ESOL dataset and compare against random initialization baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of D&D vary with different numbers of conformers per molecule during pretraining?
- Basis in paper: [inferred] The paper mentions that using fewer conformers is more efficient due to expensive DFT optimizations, but also suggests that using multiple conformers could lead to better performance by capturing a broader range of the force field information.
- Why unresolved: The paper does not provide experimental results comparing the performance of D&D with different numbers of conformers per molecule.
- What evidence would resolve it: Experiments showing the performance of D&D with varying numbers of conformers per molecule, ideally including a comparison with a single conformer.

### Open Question 2
- Question: How does the choice of architecture for the 3D conformer encoder (f3D) and 2D graph encoder (f2D) affect the performance of D&D?
- Basis in paper: [explicit] The paper mentions that D&D is architecture-agnostic and that any SE(3)-equivariant architecture can be used for f3D and any permutation-equivariant GNN architecture can be used for f2D.
- Why unresolved: The paper only tests D&D with specific architectures (TorchMD-NET for f3D and TokenGT for f2D) and does not explore the impact of using different architectures.
- What evidence would resolve it: Experiments comparing the performance of D&D using different architectures for f3D and f2D, ideally including a comparison with other popular GNN architectures like GCN, GIN, and GraphSAGE.

### Open Question 3
- Question: How does the choice between D&D-GRAPH and D&D-NODE affect the performance of D&D on different types of molecular properties?
- Basis in paper: [explicit] The paper mentions that D&D-GRAPH and D&D-NODE are two variants of D&D that differ in how they perform cross-modal distillation, and that the preferred approach may depend on the chemical characteristics of the target molecular property.
- Why unresolved: The paper does not provide a clear guideline for choosing between D&D-GRAPH and D&D-NODE for different types of molecular properties, and the experiments do not explore this aspect in detail.
- What evidence would resolve it: Experiments comparing the performance of D&D-GRAPH and D&D-NODE on a diverse set of molecular properties, ideally including a discussion on the chemical characteristics of each property and how they relate to the choice of distillation approach.

## Limitations
- Limited generalization evidence beyond molecular domain
- Computationally expensive pretraining requiring 3D conformers for millions of molecules
- Results specific to tested architectures (TorchMD-NET and TokenGT)

## Confidence
- Mechanism 1 (Cross-modal distillation effectiveness): High
- Mechanism 2 (Attention alignment with 3D geometry): Medium
- Mechanism 3 (Pretraining benefits): High

## Next Checks
1. **Architecture transfer experiment**: Reproduce the D&D pipeline using different 2D GNN architectures (e.g., GAT, GIN) to test architectural robustness
2. **Cross-domain validation**: Apply the pretrained 2D encoder to a non-molecular 3D-to-2D transfer task (e.g., protein-ligand binding) to test generalization
3. **Ablation on conformer quality**: Systematically vary the number and quality of conformers per molecule to quantify the relationship between pretraining data quality and downstream performance