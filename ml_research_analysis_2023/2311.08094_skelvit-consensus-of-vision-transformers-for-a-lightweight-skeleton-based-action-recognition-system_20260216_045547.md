---
ver: rpa2
title: 'SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based
  Action Recognition System'
arxiv_id: '2311.08094'
source_url: https://arxiv.org/abs/2311.08094
tags:
- action
- recognition
- representation
- data
- skeleton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SkelVIT, a three-level architecture for skeleton-based
  action recognition. The key idea is to use Vision Transformers (VITs) instead of
  CNNs on pseudo-image representations of skeleton data to improve robustness to representation
  changes.
---

# SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System

## Quick Facts
- arXiv ID: 2311.08094
- Source URL: https://arxiv.org/abs/2311.08094
- Reference count: 32
- Primary result: VITs achieve 72% accuracy on NTU RGB+D, outperforming CNNs (67%) and state-of-the-art methods

## Executive Summary
This paper introduces SkelVIT, a three-level architecture that uses Vision Transformers (VITs) for skeleton-based action recognition. The method addresses the sensitivity of CNNs to pseudo-image representation schemes by leveraging VITs' self-attention mechanism. The architecture generates diverse pseudo-image representations through joint permutations, classifies each with individual VITs, and combines results via consensus for final prediction. Experiments on NTU RGB+D show VITs achieve 72% accuracy, outperforming CNNs at 67%, with consensus further improving performance to 73%.

## Method Summary
SkelVIT implements a three-level architecture: (1) generates 1000 pseudo-image permutations from skeleton data and selects 10 most dissimilar arrangements, (2) trains individual VITs on each representation, and (3) combines VIT outputs using a 3-layer MLP for final prediction. Each VIT uses 8 transformer blocks with 4 attention heads and patch size 6. The method addresses the challenge of representation sensitivity in skeleton-based action recognition by leveraging VITs' self-attention mechanism and ensemble consensus.

## Key Results
- VITs achieve 72% accuracy on NTU RGB+D, outperforming CNNs at 67%
- Consensus of multiple VITs improves performance to 73%
- VITs show lower standard deviation across representations (more robust to initial pseudo-image formation)
- SkelVIT outperforms state-of-the-art methods like Skepxels (69% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers (VITs) are less sensitive to initial pseudo-image representation compared to CNNs due to their self-attention mechanism.
- Mechanism: The self-attention mechanism in VITs allows the model to learn global dependencies and long-range interactions in the data, reducing dependence on the specific spatial arrangement of joints in the pseudo-image.
- Core assumption: The self-attention mechanism effectively captures relevant information regardless of the initial pseudo-image formation scheme.
- Evidence anchors:
  - [abstract] "Experimental studies reveal that the proposed system with its lightweight representation scheme achieves better results than the state-of-the-art methods. It is also observed that the vision transformer is less sensitive to the initial pseudo-image representation compared to CNN."
  - [section] "When the average performance values for CNNs and VITs are compared... the lower standard deviation of the VIT model point out that it is more robust to representation when compared to CNN..."
  - [corpus] Weak evidence. The corpus neighbors focus on other approaches like GCNs and CNN variations, with no direct comparison to VITs in terms of representation robustness.

### Mechanism 2
- Claim: Consensus of multiple VITs further improves action recognition performance compared to a single VIT.
- Mechanism: By training multiple VITs on diverse pseudo-image representations and combining their outputs, the model can leverage complementary information and reduce the impact of representation-specific biases.
- Core assumption: Different pseudo-image representations capture different aspects of the action, and combining their predictions leads to a more robust and accurate result.
- Evidence anchors:
  - [abstract] "Nevertheless, even with the vision transformer, the recognition performance can be further improved by consensus of classifiers."
  - [section] "It is observed from these results that the consensus of classifiers improves the performance for both CNN and VIT based solutions..."
  - [corpus] No direct evidence in the corpus. The neighbors focus on other architectures and do not discuss ensemble methods.

### Mechanism 3
- Claim: The proposed three-level architecture (selection of diverse representations, individual VIT classification, consensus classification) is effective for skeleton-based action recognition.
- Mechanism: The architecture leverages the strengths of VITs (robustness to representation) and ensemble methods (improved accuracy) while addressing the challenge of selecting diverse and informative pseudo-image representations.
- Core assumption: The combination of diverse representations, VITs, and ensemble methods leads to a synergistic effect that improves performance beyond what each component could achieve individually.
- Evidence anchors:
  - [abstract] "To this end, a three-level architecture, Act-VIT is proposed, which forms a set of pseudo images apply a classifier on each of the representation and combine their results to find the final action class."
  - [section] "On the first level, a set of possible representations are processed and a subset of them is selected. On the second level, a classifier is trained for each representation. On the third level, the decision of classifiers on the second level is combined to obtain the final class of action."
  - [corpus] No direct evidence in the corpus. The neighbors do not discuss similar three-level architectures.

## Foundational Learning

- Concept: Skeleton-based action recognition
  - Why needed here: Understanding the problem domain and the characteristics of skeleton data (compact representation, robustness to viewpoint and illumination changes) is crucial for designing an effective solution.
  - Quick check question: What are the main advantages of using skeleton data for action recognition compared to RGB video data?

- Concept: Vision Transformers (VITs)
  - Why needed here: VITs are the core component of the proposed architecture, and understanding their mechanism (self-attention, positional encoding) is essential for interpreting the results and potential improvements.
  - Quick check question: How does the self-attention mechanism in VITs differ from the convolution operations in CNNs, and why is this relevant for skeleton-based action recognition?

- Concept: Ensemble methods
  - Why needed here: The consensus of multiple VITs is a key aspect of the proposed architecture, and understanding the principles of ensemble learning (diversity, independence, combining strategies) is important for evaluating its effectiveness.
  - Quick check question: What are the main benefits and challenges of using ensemble methods in machine learning, and how do they apply to the proposed architecture?

## Architecture Onboarding

- Component map: Skeleton data -> Pseudo-image generation -> Representation selection -> VIT classification -> Consensus classification -> Final action prediction
- Critical path: Skeleton data -> Pseudo-image generation -> Representation selection -> VIT classification -> Consensus classification -> Final action prediction
- Design tradeoffs:
  - Complexity vs. performance: The three-level architecture is more complex than a single VIT but may offer better performance and robustness.
  - Diversity vs. redundancy: Selecting too many similar representations may not improve performance, while selecting too few may miss important information.
  - Ensemble size vs. computational cost: Using more VITs may improve performance but also increase computational requirements.
- Failure signatures:
  - Low accuracy: Could indicate issues with representation selection, VIT training, or ensemble combination.
  - High variance in VIT performance: May suggest that the representations are not diverse enough or that the VITs are overfitting.
  - Lack of improvement from ensemble: Could indicate that the representations are too similar or that the ensemble method is not well-suited to the problem.
- First 3 experiments:
  1. Train a single VIT on a single pseudo-image representation and evaluate its performance.
  2. Train multiple VITs on diverse pseudo-image representations and evaluate the consensus performance.
  3. Compare the performance of VITs and CNNs on the same set of pseudo-image representations to validate the claim of VIT robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does varying temporal relations in pseudo-image representations affect classification performance compared to varying only spatial relations?
- Basis in paper: [inferred] The authors mention that future work could involve varying temporal relations to obtain diverse representations, suggesting this has not been explored yet.
- Why unresolved: The paper only varied spatial relations by changing joint arrangements in pseudo-images. Temporal variation was not implemented or tested.
- What evidence would resolve it: Experimental results comparing classification accuracy when temporal relations are varied versus when only spatial relations are varied, using the same evaluation metrics (accuracy, precision, recall, f-score).

### Open Question 2
- Question: What is the optimal number of permutations (X) and subsets (N) for the representation selection process in the three-level architecture?
- Basis in paper: [inferred] The paper used X=1000 permutations and N subsets with L=10 permutations each, but did not explore how performance changes with different values.
- Why unresolved: The authors did not conduct sensitivity analysis on the parameters X, N, and L that control the representation selection process.
- What evidence would resolve it: Systematic experiments varying X, N, and L values to identify their impact on classification performance and determine optimal settings.

### Open Question 3
- Question: How does the performance of Act-VIT compare to transformer-based methods using other representation schemes like graph-based or sequence-based approaches?
- Basis in paper: [explicit] The paper compares Act-VIT to CNN-based pseudo-image methods but does not compare to transformer-based methods using other representation schemes.
- Why unresolved: The experimental setup only compared VIT with CNN on pseudo-image representations, not with other transformer-based approaches.
- What evidence would resolve it: Direct comparison of Act-VIT's performance against transformer-based methods using graph convolutional networks or recurrent neural networks for skeleton-based action recognition on the same dataset.

### Open Question 4
- Question: Does the consensus of classifiers provide diminishing returns as the number of classifiers increases beyond 10?
- Basis in paper: [inferred] The paper used 10 classifiers in the ensemble but did not test whether more classifiers would continue to improve performance.
- Why unresolved: The authors only tested with 10 classifiers and did not explore the relationship between ensemble size and performance improvement.
- What evidence would resolve it: Experiments with varying numbers of classifiers (e.g., 5, 15, 20, 25) to determine if performance plateaus or continues to improve with larger ensembles.

## Limitations
- Evaluation limited to NTU RGB+D dataset with specific action classes
- Pseudo-image generation relies only on spatial joint permutations, not temporal variations
- Three-level architecture introduces computational overhead for real-time deployment
- No comparison to transformer-based methods using alternative representation schemes

## Confidence
- High Confidence: The core finding that VITs outperform CNNs on skeleton-based action recognition (72% vs 67% accuracy) is well-supported by controlled experiments with statistical significance testing.
- Medium Confidence: The consensus ensemble improvement (final accuracy of 73% reported) is plausible given the controlled experimental setup, though the exact contribution of each architectural component is not fully isolated.
- Low Confidence: The comparison to state-of-the-art methods (Skepxels at 69%) is based on reported literature values rather than direct experimental comparison under identical conditions.

## Next Checks
1. **Cross-dataset validation**: Evaluate SkelVIT on additional skeleton datasets (Kinetics-Skeleton, NW-UCLA) to assess generalization beyond NTU RGB+D.

2. **Representation diversity analysis**: Quantify the actual dissimilarity between selected pseudo-image representations and test whether removing highly similar representations affects ensemble performance.

3. **Computational efficiency benchmarking**: Measure inference time and memory usage for the three-level architecture versus single VIT baseline to validate lightweight claims under realistic deployment constraints.