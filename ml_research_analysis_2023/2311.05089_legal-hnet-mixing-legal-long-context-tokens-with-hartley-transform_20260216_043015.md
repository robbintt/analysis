---
ver: rpa2
title: 'Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform'
arxiv_id: '2311.05089'
source_url: https://arxiv.org/abs/2311.05089
tags:
- such
- which
- quant
- legal
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of Fourier-class Transforms as a
  replacement for the self-attention mechanism in transformer-based models to address
  long-document processing challenges in the legal domain. The authors propose using
  Hartley Transform (HT) instead of Fourier Transform (FT) and introduce a new hybrid
  Seq2Seq architecture (RAED) combining a no-attention-based encoder with an attention-based
  decoder.
---

# Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform

## Quick Facts
- arXiv ID: 2311.05089
- Source URL: https://arxiv.org/abs/2311.05089
- Reference count: 40
- New legal language models using Hartley/Fourier transforms achieve competitive performance on summarization and classification tasks with reduced computational costs

## Executive Summary
This work proposes replacing self-attention mechanisms in transformer models with Fourier-class transforms (Hartley and Fourier) to address long-document processing challenges in the legal domain. The authors introduce a hybrid Seq2Seq architecture (RAED) that combines a no-attention-based encoder with an attention-based decoder, achieving competitive performance on summarization and text classification tasks while significantly reducing computational costs and environmental impact.

## Method Summary
The authors propose three new legal language models trained from scratch using HNet (Hartley Transform) and FNet (Fourier Transform) architectures with different sequence lengths (4096 and 8192 tokens). The models replace traditional self-attention layers with Hartley or Fourier transforms in the encoder, while maintaining attention-based decoding. Pretraining is performed on a curated legal corpus (504k documents, 9.2GB), followed by evaluation on summarization tasks (BillSum, PubMed) and legal text classification (LexGLUE).

## Key Results
- HNet-4096 base model outperforms transformer models and competes with PEGASUS-base on summarization tasks
- Models achieve competitive performance on LexGLUE legal classification benchmarks
- Significant reduction in computational costs and environmental impact compared to traditional attention-based models
- RAED architecture effectively combines efficient encoding with high-quality generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing self-attention with Hartley Transform enables efficient long-document processing while maintaining representational quality
- **Mechanism**: Hartley Transform provides real-valued mixing of token representations without complex number arithmetic, allowing efficient processing of long sequences while capturing long-range dependencies
- **Core assumption**: The mixing pattern of HT is sufficient to approximate attention's ability to model token relationships
- **Evidence anchors**: Abstract statement about exploring alternatives to attention, section 4.1 explanation of HT implementation, weak evidence from legal corpus
- **Break condition**: If HT's mixing pattern fails to capture essential token relationships for fine-grained semantic understanding

### Mechanism 2
- **Claim**: RAED architecture effectively combines efficient encoding with high-quality generation
- **Mechanism**: No-attention encoder based on Hartley/Fourier transforms provides efficient long-context processing, paired with attention-based decoder for quality generation
- **Core assumption**: Decoder's attention can effectively leverage mixed representations from encoder to produce high-quality outputs
- **Evidence anchors**: Section 4.2 discussion of multi-headed self-attention importance, section 5.1 performance comparison, weak evidence from corpus
- **Break condition**: If decoder cannot recover necessary information from mixed encoder representations, leading to degraded generation quality

### Mechanism 3
- **Claim**: Domain-specific pretraining on legal documents enables capturing specialized vocabulary while maintaining efficiency
- **Mechanism**: Training from scratch on curated legal documents allows learning domain-specific patterns without general-domain pretraining overhead
- **Core assumption**: Legal domain has sufficient data diversity and volume for effective scratch training
- **Evidence anchors**: Section 3.1 discussion of minimum requirements, section 6.2 success in training and evaluation, weak evidence from corpus
- **Break condition**: If domain-specific dataset lacks sufficient diversity or scale, leading to poor generalization

## Foundational Learning

- **Concept**: Fourier-class transforms and their properties
  - Why needed here: Understanding how Hartley and Fourier transforms work is crucial for grasping why they can replace attention mechanisms
  - Quick check question: What is the key difference between Hartley Transform and Fourier Transform that makes HT more suitable for this application?

- **Concept**: Self-attention mechanism and its computational complexity
  - Why needed here: Understanding limitations of self-attention (quadratic complexity) is essential to appreciate why alternatives are needed
  - Quick check question: What is the computational complexity of self-attention and why does it become prohibitive for long sequences?

- **Concept**: Sequence-to-sequence model architecture
  - Why needed here: Understanding how encoders and decoders work together is crucial for grasping RAED architecture design
  - Quick check question: What are the key differences between encoder-only, decoder-only, and encoder-decoder architectures?

## Architecture Onboarding

- **Component map**: Input Tokens → Hartley Transform Layer → Feed-Forward Layer → Cross-attention in Decoder → Generation
- **Critical path**: Encoder (Hartley Transform + Feed-Forward) → Cross-attention in Decoder → Generation
- **Design tradeoffs**:
  - Efficiency vs. Quality: Hartley Transform is more efficient but may lose some representational power compared to attention
  - Domain Specificity vs. Generalization: Legal-specific pretraining improves domain performance but may reduce general applicability
  - Architecture Complexity vs. Implementation Simplicity: RAED adds complexity but provides better results than pure transform-based Seq2Seq
- **Failure signatures**: Poor performance on tasks requiring fine-grained semantic understanding, degraded generation quality due to insufficient encoder information, overfitting to legal domain on general tasks
- **First 3 experiments**:
  1. Train HNet-4096 on legal corpus and evaluate on short legal classification tasks (LexGLUE)
  2. Implement RAED architecture and compare performance with standard transformer on summarization tasks
  3. Test Hartley Transform vs. Fourier Transform variants on the same downstream tasks to validate claimed advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does HNet compare to other linear attention replacement mechanisms (FNet, MLP-Mixer, Performer) in terms of performance and efficiency?
- **Basis in paper**: [explicit] The paper compares HNet to FNet but not to other emerging architectures that replace attention mechanisms
- **Why unresolved**: Focus on comparing HNet to FNet and traditional attention-based models without comprehensive comparison to other linear attention replacements
- **What evidence would resolve it**: Systematic comparison of HNet against other linear attention replacement mechanisms on diverse NLP tasks, including both long and short sequence tasks

### Open Question 2
- **Question**: What is the impact of Hartley Transform on the model's ability to capture long-range dependencies compared to Fourier Transform and traditional attention mechanisms?
- **Basis in paper**: [inferred] HNet performs better than FNet implies HT might be more effective, but no detailed analysis of how transform affects dependency capture
- **Why unresolved**: Focus on empirical results without theoretical analysis of how Hartley Transform affects long-range dependency modeling
- **What evidence would resolve it**: Detailed analysis of attention patterns and receptive fields in HNet, FNet, and traditional attention models, along with performance on tasks requiring long-range dependency modeling

### Open Question 3
- **Question**: How does RAED architecture perform compared to other Seq2Seq models (BART, T5) on various NLP tasks beyond summarization?
- **Basis in paper**: [explicit] Primarily evaluates RAED on summarization tasks, mentions outperforming transformer but doesn't extensively evaluate on other NLP tasks
- **Why unresolved**: Focus on summarization performance without comprehensive evaluation on other NLP tasks like question answering, text classification, or machine translation
- **What evidence would resolve it**: Systematic evaluation of RAED architecture on diverse NLP tasks including question answering, text classification, machine translation, and other sequence-to-sequence tasks

## Limitations

- The core claim that Hartley Transform can effectively replace self-attention lacks strong empirical validation through ablation studies
- Domain-specific pretraining claims are supported by successful training but the legal corpus size (504k documents) is relatively small compared to general pretraining datasets
- RAED architecture's effectiveness is primarily supported by comparisons to baseline transformers rather than state-of-the-art approaches

## Confidence

- **High confidence**: Mathematical implementation of Hartley Transform is correct, pretraining procedure is clearly specified and reproducible
- **Medium confidence**: Downstream task evaluation methodology is sound, reported metrics (Rouge scores, F1 scores) are appropriate for tasks
- **Low confidence**: Claims about superiority of Hartley Transform over other token-mixing mechanisms, assertion that RAED architecture is optimal for proposed use case

## Next Checks

1. **Ablation study on token-mixing mechanisms**: Implement and compare Hartley Transform, Fourier Transform, average pooling, and learned linear projections as encoder alternatives, evaluating each on same downstream tasks to isolate HT's contribution

2. **Human evaluation of generated summaries**: Conduct blind human evaluation comparing summaries generated by RAED-HNet models against those from standard transformers and human-written summaries to validate "indistinguishable from human" claim

3. **Scaling analysis of legal corpus**: Systematically vary pretraining corpus size (100k, 250k, 500k, 1M documents) to empirically determine minimum requirements for effective legal domain pretraining and validate "sufficient diversity" claim