---
ver: rpa2
title: 'WW-FL: Secure and Private Large-Scale Federated Learning'
arxiv_id: '2302.09904'
source_url: https://arxiv.org/abs/2302.09904
tags:
- hyfl
- mean
- trimmed
- fltrust
- fedavg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WW-FL introduces a secure and private federated learning framework
  that combines hierarchical aggregation with secure multi-party computation to protect
  both client data and global model privacy. The system uses distributed training
  clusters to handle local model training while maintaining all models and data in
  secret-shared form, preventing any single entity from accessing complete information.
---

# WW-FL: Secure and Private Large-Scale Federated Learning

## Quick Facts
- **arXiv ID**: 2302.09904
- **Source URL**: https://arxiv.org/abs/2302.09904
- **Reference count**: 40
- **Key outcome**: Secure and private federated learning framework combining hierarchical aggregation with MPC to protect data and model privacy while preventing model poisoning attacks

## Executive Summary
WW-FL introduces a secure and private federated learning framework that combines hierarchical aggregation with secure multi-party computation to protect both client data and global model privacy. The system uses distributed training clusters to handle local model training while maintaining all models and data in secret-shared form, preventing any single entity from accessing complete information. The framework inherently limits attack surfaces by preventing model poisoning and focuses on mitigating data poisoning through efficient robust aggregation schemes. Experimental results demonstrate that WW-FL achieves faster convergence than standard federated learning while maintaining comparable accuracy levels, and shows strong resilience against state-of-the-art data poisoning attacks with minimal performance overhead when using optimized aggregation methods.

## Method Summary
WW-FL implements a three-layer architecture where clients share their data with distributed MPC training clusters, which perform private training on pooled data. The trained models are then secret-shared with global MPC servers that perform secure aggregation to produce the updated global model. The framework uses CrypTen for MPC implementation and PyTorch for ML operations, with all computations maintaining secret-sharing properties to prevent any single entity from accessing complete data or model information.

## Key Results
- Faster convergence than standard federated learning due to batching benefits at cluster level
- Comparable accuracy levels to standard FL on MNIST and CIFAR10 datasets
- Strong resilience against data poisoning attacks with minimal performance overhead

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical MPC prevents any single entity from accessing complete model or data
- **Mechanism**: Data and model are kept in secret-shared form across distributed training clusters and global servers
- **Core assumption**: No collusion among MPC servers beyond allowed corruption threshold
- **Evidence anchors**: Hierarchical aggregation with secure multi-party computation to protect both client data and global model privacy; clients use secret-sharing techniques to securely outsource training data to distributed training clusters
- **Break condition**: If MPC corruption threshold is exceeded or collusion occurs between cluster and global servers

### Mechanism 2
- **Claim**: Prevents model poisoning by restricting malicious clients to data poisoning only
- **Mechanism**: Models never exposed to clients in any form, only secret-shared between servers
- **Core assumption**: MPC implementation correctly maintains secret-sharing properties
- **Evidence anchors**: Prevents malicious clients from directly poisoning model parameters, confining them to less destructive data poisoning attacks; a neat property of our framework is the strictly limited attack surface: malicious participants are restricted to data-poisoning attacks
- **Break condition**: If MPC implementation leaks model shares to clients or if re-sharing protocol is compromised

### Mechanism 3
- **Claim**: Faster convergence than standard FL due to batching at cluster level
- **Mechanism**: Pooling training data at cluster level enables larger batch sizes and better batching benefits
- **Core assumption**: Cluster-level data pooling doesn't introduce significant overhead
- **Evidence anchors**: We attribute this to HyFL pooling training data at cluster level and thus being able to exploit the known benefits of batching; After completing training, servers in Mi utilize RESHARE to secret-share the updated model
- **Break condition**: If cluster-level pooling overhead outweighs batching benefits or if data distribution becomes too skewed

## Foundational Learning

- **Concept**: Secure Multi-Party Computation (MPC)
  - **Why needed here**: Enables distributed computation on secret-shared data without exposing individual inputs
  - **Quick check question**: What corruption threshold must be maintained for semi-honest MPC to be secure?

- **Concept**: Federated Learning architecture
  - **Why needed here**: Understanding standard FL vs hierarchical FL vs HyFL design patterns
  - **Quick check question**: How does client selection work differently in HyFL vs standard FL?

- **Concept**: Differential Privacy (DP)
  - **Why needed here**: Optional component for adding noise-based privacy guarantees
  - **Quick check question**: What DP parameters would be appropriate for gradient updates in this setting?

## Architecture Onboarding

- **Component map**: Clients -> MPC Training Clusters -> Global MPC Servers
- **Critical path**:
  1. Clients share data to training clusters via SHARE protocol
  2. Training clusters perform private training on pooled data
  3. Clusters share trained models to global servers via RESHARE
  4. Global servers perform secure aggregation
  5. Aggregated model returned to training clusters for next iteration
- **Design tradeoffs**:
  - Security vs performance: More MPC servers = better security but higher overhead
  - Cluster size vs data privacy: Larger clusters = better batching but more data exposure within cluster
  - Fixed-point precision vs accuracy: Higher precision = better accuracy but more computation
- **Failure signatures**:
  - Accuracy degradation → Check MPC corruption threshold or data poisoning
  - High latency → Check network bandwidth/latency settings
  - Model divergence → Check aggregation scheme or client sampling
- **First 3 experiments**:
  1. Run plain FL baseline on MNIST/CIFAR10 to establish performance targets
  2. Deploy HyFL with single cluster and verify secret-sharing properties
  3. Scale to multiple clusters and measure overhead vs accuracy tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of HyFL scale with increasing numbers of hierarchical layers beyond the three-layer architecture?
- **Basis in paper**: [explicit] The paper mentions that "HyFL has a three-layer architecture (cf. Fig. 1) consolidates many existing FL frameworks" and "it can easily accommodate more levels of hierarchy depending on the size of the deployment."
- **Why unresolved**: The paper only evaluates a three-layer architecture and does not provide empirical data on performance scaling with additional hierarchical layers.
- **What evidence would resolve it**: Systematic performance evaluation of HyFL with varying numbers of hierarchical layers (4, 5, 6+) under different network conditions and model sizes.

### Open Question 2
- **Question**: What is the optimal corruption threshold for MPC clusters that balances security against data poisoning attacks while maintaining acceptable performance overhead?
- **Basis in paper**: [inferred] The paper discusses that "MPC clusters in HyFL may use different MPC configurations and differ, e.g., in their corruption threshold" and evaluates performance at different poison rates (0.01, 0.1, 0.2) but does not explore the relationship between MPC corruption thresholds and poisoning resilience.
- **Why unresolved**: The paper does not investigate how varying the MPC cluster corruption thresholds affects the system's ability to resist data poisoning attacks or the associated performance trade-offs.
- **What evidence would resolve it**: Experimental results comparing HyFL's poisoning resilience and performance across different MPC corruption thresholds (e.g., 1/3, 1/2, 2/3) under various attack scenarios.

### Open Question 3
- **Question**: How does HyFL's performance compare to existing secure aggregation frameworks when implemented with different MPC protocols (e.g., garbled circuits, homomorphic encryption) instead of the CrypTen framework?
- **Basis in paper**: [explicit] The paper states "HyFL is not bound to any specific MPC setting and could be instantiated using any MPC protocol" but only evaluates using CrypTen.
- **Why unresolved**: The performance evaluation is limited to one MPC framework (CrypTen), leaving open questions about how different MPC instantiations would affect HyFL's practical performance.
- **What evidence would resolve it**: Comparative performance analysis of HyFL implemented with multiple MPC frameworks/protocols (e.g., ABY, EMP-toolkit, homomorphic encryption libraries) measuring computation time, communication overhead, and accuracy.

### Open Question 4
- **Question**: What is the impact of quantization techniques on HyFL's accuracy and security guarantees?
- **Basis in paper**: [explicit] The paper mentions "we plan to investigate potential further performance improvements by incorporating quantization techniques for private training" in the conclusion, indicating this has not yet been explored.
- **Why unresolved**: The paper does not evaluate any quantization methods and their effects on model accuracy, convergence speed, or resistance to attacks.
- **What evidence would resolve it**: Experimental results comparing HyFL with and without quantized model parameters, measuring accuracy degradation, training speed improvements, and any changes in vulnerability to poisoning attacks.

## Limitations
- Limited experimental scope with only MNIST and CIFAR10 datasets and 1000 simulated clients
- Implementation details for critical components like RESHARE protocol and secure aggregation scheme are not fully specified
- Performance overhead quantification lacks specific timing measurements and breakdown analysis

## Confidence

- **High confidence**: The core security claims about preventing model poisoning through the hierarchical MPC architecture are well-supported by the theoretical framework
- **Medium confidence**: The convergence speed claims are supported by experimental results but are limited to specific datasets and configurations
- **Low confidence**: The claim about "strong resilience against state-of-the-art data poisoning attacks" lacks specific attack scenarios and quantitative resilience metrics

## Next Checks

1. **Implement RESHARE protocol validation**: Create a standalone test to verify that secret shares can be correctly converted between training cluster configurations and global server configurations without information leakage
2. **Scale to real-world dataset**: Deploy the framework on a larger, more realistic federated learning dataset (e.g., LEAF benchmark or real-world mobile device data) with at least 10,000 clients to validate the "large-scale" claims
3. **Targeted poisoning attack analysis**: Conduct systematic evaluation using specific data poisoning attack vectors (e.g., label flipping, gradient ascent) to quantify the actual resilience of the robust aggregation schemes