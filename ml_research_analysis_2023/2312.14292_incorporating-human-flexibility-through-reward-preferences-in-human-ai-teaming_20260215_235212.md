---
ver: rpa2
title: Incorporating Human Flexibility through Reward Preferences in Human-AI Teaming
arxiv_id: '2312.14292'
source_url: https://arxiv.org/abs/2312.14292
tags:
- human
- agent
- policy
- learning
- pbrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first study of preference-based reinforcement
  learning (PbRL) in a two-agent human-AI teaming setting. The authors formulate this
  as a Human-AI PbRL Cooperation Game where the RL agent queries the human partner
  to learn the team reward function and human preferences on joint behavior.
---

# Incorporating Human Flexibility through Reward Preferences in Human-AI Teaming

## Quick Facts
- arXiv ID: 2312.14292
- Source URL: https://arxiv.org/abs/2312.14292
- Reference count: 40
- Primary result: PbRL algorithms only achieve optimal performance when the human has a single preferred policy and the agent has complete access to it

## Executive Summary
This paper introduces the first study of preference-based reinforcement learning (PbRL) in a two-agent human-AI teaming setting. The authors formulate this as a Human-AI PbRL Cooperation Game where the RL agent queries the human partner to learn the team reward function and human preferences on joint behavior. Experiments across highway and locomotion domains show that PbRL algorithms only achieve optimal performance under "Specified Orchestration" - where the human has a single preferred policy and the agent has complete access to it. Without these conditions, performance drops significantly, highlighting the challenges of PbRL in human-AI teaming.

## Method Summary
The paper adapts single-agent PbRL algorithms (PEBBLE, RUNE, SURF) to a human-AI team setting with modifications for human policy access and human flexibility. The method involves an agent exploring the environment, querying the human for preferences on trajectory pairs, learning a reward function from preferences, and updating its policy. The key innovation is introducing human flexibility as a parameter that determines whether the human has a single preferred policy or multiple acceptable team strategies, and varying the agent's access to the human policy during training.

## Key Results
- PbRL algorithms achieve optimal performance only under "Specified Orchestration" conditions
- Performance declines significantly when humans have multiple acceptable team strategies
- Without access to human policy information, PbRL performance drops to near-random levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PbRL algorithms achieve optimal performance only when the human has a single preferred policy and the agent has complete access to it (Specified Orchestration)
- Mechanism: When the human has a singleton set of team strategies (ϵ=1-H-flexible) and the agent knows the exact human policy, the PbRL problem reduces to single-agent PbRL where the agent can accurately imagine human actions during preference queries
- Core assumption: The human policy is Markovian and can be perfectly learned or accessed by the agent
- Evidence anchors:
  - [abstract] "PbRL algorithms only achieve optimal performance under 'Specified Orchestration' - where the human has a single preferred policy and the agent has complete access to it"
  - [section] "Specified Orchestration provides a loose upper-bound performance by assuming away other challenging aspects of Human-AI interaction except preference elicitation, reward learning and subsequent policy learning"
- Break condition: If the human is flexible (ϵ>1) or the agent lacks access to the human policy, the agent cannot accurately model human actions, leading to poor reward learning and suboptimal policies

### Mechanism 2
- Claim: Human flexibility negatively impacts PbRL performance because the agent must learn a conformant policy compatible with multiple human strategies
- Mechanism: When the human has multiple acceptable team strategies, the agent faces a more complex optimization problem requiring compatibility with all human policies rather than learning a single best response
- Core assumption: The agent is constrained to learn a policy that works well with all human policies rather than finding a policy that works best with any one human policy
- Evidence anchors:
  - [section] "we find that when the human is more flexible, the agent's performance declines. This is primarily because we constrain our AI agent to learn a conformant policy (compatible with all human policies)"
  - [section] "if the human is switching their policy in the feasible team behavior set during the agent's training process, the agent now has an additional objective of learning a conformant response policy"
- Break condition: If the agent is allowed to optimize for compatibility with only one human policy or a subset of policies, performance may improve at the cost of reduced robustness

### Mechanism 3
- Claim: Without access to human policy, PbRL agents perform poorly because they cannot accurately simulate human actions during preference queries
- Mechanism: The agent must imagine human actions when querying preferences, but without policy access, these imagined actions are random, leading to meaningless preference labels and poor reward learning
- Core assumption: The agent uses random actions to represent human behavior when it cannot access the human policy
- Evidence anchors:
  - [section] "From the experiments with 0% and partially restricted access to the human partner policy, we observe that reward learning is severely impacted"
  - [section] "To our surprise, partial access to πH also had a drop in performance as soon as the agent's access budget was exhausted"
- Break condition: If the agent can explicitly model the human policy through alternative methods (e.g., behavior cloning), performance may improve even with limited access

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper frames the Human-AI PbRL Cooperation Game as a two-player Markov game, requiring understanding of state transitions and reward structures
  - Quick check question: What are the key components of an MDP and how do they differ in a two-agent cooperative setting versus a single-agent setting?

- Concept: Preference-based learning and Bradley-Terry model
  - Why needed here: The agent learns rewards through binary preference queries using the Bradley-Terry preference model, which is fundamental to understanding how the reward function is learned from human feedback
  - Quick check question: How does the Bradley-Terry model translate preference feedback into a reward function, and what assumptions does it make about human preferences?

- Concept: Policy learning and conformant policies
  - Why needed here: The agent must learn a policy that is compatible with the human's policy(ies), which requires understanding of policy optimization in multi-agent settings and the concept of conformant/response policies
  - Quick check question: What distinguishes a conformant policy from a standard optimal policy in a cooperative multi-agent setting?

## Architecture Onboarding

- Component map:
  - Human policy oracle -> Preference labeling function -> Reward approximator -> Policy approximator -> Trajectory buffer -> Preference buffer

- Critical path:
  1. Agent explores environment and stores trajectories
  2. Agent queries human for preferences on trajectory pairs
  3. Reward approximator learns from preference labels
  4. All stored trajectories are relabeled with updated reward function
  5. Policy approximator learns from relabeled trajectories
  6. Repeat until convergence

- Design tradeoffs:
  - Complete vs. partial vs. no access to human policy: affects the accuracy of imagined human actions during preference queries
  - ϵ-H-flexibility: determines whether the agent must learn a single conformant policy or can optimize for individual human policies
  - Frequency of preference queries: balances human effort against learning speed
  - Relabeling frequency: affects stability of learning but increases computational cost

- Failure signatures:
  - Poor performance with 0% or limited human policy access: indicates the need for explicit human policy modeling
  - Decreased performance with higher human flexibility: suggests the conformant policy constraint is too restrictive
  - Unstable learning curves: may indicate insufficient preference queries or inappropriate relabeling frequency

- First 3 experiments:
  1. Run with Specified Orchestration (ϵ=1, complete human policy access) to establish upper bound performance
  2. Run with 0% human policy access to confirm baseline poor performance without policy modeling
  3. Run with multiple human policies (ϵ>1) and complete access to test the conformant policy learning challenge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does human flexibility impact PbRL performance when the human policy set is larger than 3 policies?
- Basis in paper: [explicit] The paper states that with 3 human policies, PbRL performance declines compared to the Specified Orchestration case, but does not explore larger policy sets.
- Why unresolved: The experiments only tested with 3 human policies, leaving the impact of larger policy sets unexplored.
- What evidence would resolve it: Experiments varying the number of human policies (e.g., 5, 10, 20) while keeping other conditions constant would show how performance scales with policy set size.

### Open Question 2
- Question: Can explicit human policy modeling improve PbRL performance when the agent has no access to human actions?
- Basis in paper: [explicit] The paper notes that performance drops significantly without access to human policy and suggests that explicit human policy modeling could be a solution.
- Why unresolved: The paper does not implement or test explicit human policy modeling, leaving this as a theoretical suggestion.
- What evidence would resolve it: Implementing and testing a PbRL algorithm that includes a separate human policy model (e.g., using behavior cloning) would demonstrate whether this approach improves performance.

### Open Question 3
- Question: How does human cognitive load affect their ability to provide consistent preference feedback in PbRL?
- Basis in paper: [inferred] The paper mentions that computing multiple policies is cognitively challenging for humans, but does not study the impact of cognitive load on feedback quality.
- Why unresolved: The experiments use scripted oracles rather than real humans, so cognitive load effects are not measured.
- What evidence would resolve it: User studies with real humans providing preference feedback while measuring cognitive load (e.g., using NASA-TLX surveys) would show how cognitive factors affect feedback consistency and PbRL performance.

## Limitations
- Results are based on simulated human oracles rather than real human data, which may not capture full complexity of human preferences
- Experiments are limited to specific cooperative domains (highway and locomotion)
- Human flexibility mechanism assumes discrete policy switching rather than continuous adaptation

## Confidence
- High confidence: PbRL performance depends critically on access to human policy information and human flexibility levels
- Medium confidence: Conformant policy constraint is the primary reason for performance degradation with flexible humans
- Medium confidence: Specified Orchestration provides an upper bound that is difficult to achieve in practice

## Next Checks
1. Test the algorithms with real human participants in a simplified domain to verify that simulation results generalize to actual human behavior
2. Implement alternative human policy modeling approaches (e.g., behavior cloning from demonstrations) to assess whether explicit policy access is necessary
3. Evaluate the conformant policy constraint by allowing the agent to optimize for subsets of human policies rather than all policies simultaneously