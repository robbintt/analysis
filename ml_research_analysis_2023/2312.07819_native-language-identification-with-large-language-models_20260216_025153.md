---
ver: rpa2
title: Native Language Identification with Large Language Models
arxiv_id: '2312.07819'
source_url: https://arxiv.org/abs/2312.07819
tags:
- language
- native
- gpt-4
- english
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first experiments on Native Language Identification
  (NLI) using large language models (LLMs) like GPT-4. NLI is the task of predicting
  a writer's first language by analyzing their writings in a second language.
---

# Native Language Identification with Large Language Models

## Quick Facts
- arXiv ID: 2312.07819
- Source URL: https://arxiv.org/abs/2312.07819
- Reference count: 9
- GPT-4 achieves 91.7% accuracy on TOEFL11 NLI benchmark in zero-shot setting

## Executive Summary
This paper presents the first experiments using large language models (LLMs) like GPT-4 for Native Language Identification (NLI), the task of predicting a writer's first language from their second language writings. The authors demonstrate that GPT-4 achieves state-of-the-art performance of 91.7% accuracy on the TOEFL11 benchmark in a zero-shot setting, outperforming previous fully-supervised methods. The study also explores open-set NLI capabilities and shows that LLMs can provide linguistic reasoning for their predictions, citing specific spelling errors, syntactic patterns, and translated linguistic features as justifications.

## Method Summary
The authors evaluate GPT-3.5 and GPT-4 on the TOEFL11 test set (1,100 essays, 100 per L1 class) using zero-shot classification. They employ iterative prompting for closed-set classification and open-set classification without predefined L1 classes. The models are accessed via OpenAI API, and accuracy is calculated by comparing predictions to ground truth. The study also investigates the impact of text length on prediction reliability and examines the linguistic explanations provided by GPT-4 for its NLI decisions.

## Key Results
- GPT-4 achieves 91.7% accuracy on TOEFL11 test set, setting new SOTA in zero-shot NLI
- GPT-4 can perform open-set NLI, predicting L1s beyond predefined TOEFL11 classes
- LLMs provide linguistic reasoning for predictions, citing spelling errors, syntactic patterns, and translated structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models (LLMs) like GPT-4 can perform native language identification (NLI) with high accuracy in a zero-shot setting.
- Mechanism: GPT-4 leverages its pretraining on diverse linguistic patterns to recognize L1 influence in L2 writing without requiring task-specific training data.
- Core assumption: The pretraining corpus of GPT-4 contains sufficient exposure to non-native English writing patterns to enable accurate L1 inference.
- Evidence anchors:
  - [abstract]: "GPT-4 setting a new performance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot setting"
  - [section]: "Our results show that GPT models are proficient at NLI classification, with GPT-4 setting a new performance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot setting"
  - [corpus]: Weak evidence - the corpus only contains 1,100 test essays, so we cannot confirm GPT-4's pretraining contained similar NLI-style data
- Break condition: If GPT-4's pretraining corpus lacked sufficient exposure to non-native English writing patterns, its accuracy would drop significantly below the 91.7% achieved on TOEFL11.

### Mechanism 2
- Claim: LLMs can perform open-set NLI, predicting L1 classes beyond a predefined set.
- Mechanism: By removing output class restrictions from the prompt, LLMs can output any language label, enabling prediction of L1s not present in the training data.
- Core assumption: The LLM's knowledge base contains information about linguistic features of languages beyond the TOEFL11 set.
- Evidence anchors:
  - [abstract]: "We also show that unlike previous fully-supervised settings, LLMs can perform NLI without being limited to a set of known classes"
  - [section]: "Our second experiment demonstrated that LLM-based NLI can be performed in an open-set manner, without specifying the list of known L1 classes"
  - [corpus]: Weak evidence - the corpus only evaluates against TOEFL11 L1s, so we cannot confirm accuracy on truly novel L1 classes
- Break condition: If the LLM lacks knowledge of linguistic features for languages beyond the predefined set, its predictions for novel L1s would be random or incorrect.

### Mechanism 3
- Claim: LLMs can provide linguistic reasoning for their NLI decisions.
- Mechanism: By prompting the model to explain its classification, it can cite specific linguistic features like spelling errors, syntactic patterns, and translated structures as evidence.
- Core assumption: The LLM's knowledge base contains information about how different L1s influence L2 writing patterns.
- Evidence anchors:
  - [abstract]: "We also show that LLMs can provide justification for their choices, providing reasoning based on spelling errors, syntactic patterns, and usage of directly translated linguistic patterns"
  - [section]: "Finally, we also show that LLMs can provide justification for their choices, providing reasoning based on spelling errors, syntactic patterns, and usage of directly translated linguistic patterns"
  - [corpus]: Weak evidence - the corpus only shows examples of GPT-4's explanations, not a comprehensive evaluation of their accuracy
- Break condition: If the LLM's knowledge about L1-specific writing patterns is incomplete or incorrect, its explanations would contain significant hallucinations or inaccuracies.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates GPT-4's NLI performance without any task-specific training data, relying solely on its pretraining knowledge
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Native Language Identification (NLI)
  - Why needed here: The paper's core task is predicting a writer's first language based on their L2 writing, which requires understanding L1 influence patterns
  - Quick check question: What are the three main types of linguistic evidence LLMs use for NLI according to the paper?

- Concept: Open-set classification
  - Why needed here: The paper explores whether LLMs can predict L1s beyond the predefined TOEFL11 set, moving beyond traditional closed-set NLI
  - Quick check question: How does open-set NLI differ from closed-set NLI in terms of output possibilities?

## Architecture Onboarding

- Component map:
  - TOEFL11 essays -> GPT-3.5/GPT-4 via OpenAI API -> Predicted L1 label (closed-set or open-set) -> Optional: Linguistic reasoning explanations

- Critical path:
  1. Load TOEFL11 test set essays
  2. Format each essay with appropriate prompt (closed-set, open-set, or explainability)
  3. Send essay and prompt to LLM via API
  4. Parse LLM output for predicted L1 label
  5. For closed-set, iterate prompting if needed until valid L1 predicted
  6. For explainability, extract linguistic reasoning from output
  7. Compare predictions to ground truth for accuracy calculation

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Zero-shot preserves LLM's general knowledge but may be less accurate than fine-tuned models
  - Closed-set vs. open-set: Closed-set is more constrained but reflects traditional NLI; open-set is more flexible but harder to evaluate
  - Prompt engineering: Detailed prompts may improve accuracy but reduce the zero-shot nature of the experiment

- Failure signatures:
  - Low accuracy: May indicate insufficient LLM pretraining on non-native English patterns
  - Consistent misclassification of certain L1 pairs: May indicate similar L1 influence patterns that are hard to distinguish
  - Refusal to classify: May indicate insufficient evidence in short texts (addressed by text length experiment)
  - Hallucinations in explanations: May indicate LLM generating plausible-sounding but incorrect linguistic features

- First 3 experiments:
  1. Closed-set NLI classification with GPT-4 on TOEFL11 test set to establish baseline accuracy
  2. Text length analysis to determine minimum characters needed for reliable predictions
  3. Open-set NLI classification to test LLM's ability to predict L1s beyond TOEFL11 set

## Open Questions the Paper Calls Out
Here are 3-5 open questions based on the paper:

### Open Question 1
- Question: How would GPT-4's NLI performance compare to fine-tuned open-source LLMs like Llama-2?
- Basis in paper: [inferred] The authors mention that fine-tuning open-source LLMs like Llama-2 for NLI remains unexplored, and they do not expect such models to outperform GPT-4 based on other NLP benchmarks.
- Why unresolved: The authors did not actually test fine-tuned open-source LLMs on the NLI task, so the performance gap is unknown.
- What evidence would resolve it: Running NLI experiments with fine-tuned Llama-2 or other open-source LLMs on the TOEFL11 dataset and comparing the results to GPT-4.

### Open Question 2
- Question: What is the impact of using more specific prompts on the quality of explanations generated by GPT-4 for NLI predictions?
- Basis in paper: [explicit] The authors mention that they prompted the model to provide reasoning for its choice, but note that it's likely possible to obtain even more detailed analysis with a more specific prompt.
- Why unresolved: The authors only used a general prompt for explanations, so the impact of prompt specificity is unknown.
- What evidence would resolve it: Running experiments with different prompt styles and levels of detail for the explanation task, and analyzing the resulting explanations.

### Open Question 3
- Question: How well can GPT-4's explanations for NLI predictions be evaluated by expert human judges?
- Basis in paper: [explicit] The authors mention that manual examination of the explanations revealed hallucinations and incorrect assertions, and that expert human evaluation would be needed to assess the accuracy of the explanations.
- Why unresolved: The authors did not conduct an expert human evaluation of the explanations, so their quality and accuracy is unknown.
- What evidence would resolve it: Having expert linguists review and rate the explanations generated by GPT-4 for the NLI task.

## Limitations
- Relies on single benchmark dataset (TOEFL11) that may not generalize to other domains or L1s
- Zero-shot performance may not translate to fine-tuning scenarios with task-specific training data
- Open-set claims remain speculative as evaluation only tested against known TOEFL11 L1s

## Confidence
- High Confidence: GPT-4's superior performance (91.7% accuracy) over previous supervised methods on TOEFL11 benchmark
- Medium Confidence: Claims about open-set NLI capability - while the mechanism is plausible, evaluation only tested known classes
- Medium Confidence: Linguistic explanation quality - examples provided appear reasonable but lack systematic evaluation of accuracy

## Next Checks
1. Test GPT-4 on alternative NLI datasets (e.g., ICLE, FCE) to verify benchmark generalization
2. Evaluate open-set predictions on truly novel L1s not present in any training data to confirm LLM's cross-linguistic knowledge
3. Conduct human evaluation of GPT-4's linguistic explanations to measure hallucination rate and accuracy compared to linguistic expert annotations