---
ver: rpa2
title: A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class
  Incremental Learning for Vision Tasks
arxiv_id: '2311.07784'
source_url: https://arxiv.org/abs/2311.07784
tags:
- learning
- data
- clients
- generative
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a federated class incremental learning framework
  (MFCL) that addresses catastrophic forgetting in federated learning by using a server-trained
  generative model to synthesize past data samples. Unlike prior approaches requiring
  client-side memory or data sharing, MFCL trains the generative model in a data-free
  manner using knowledge distillation, avoiding privacy risks and reducing client-side
  computational burden.
---

# A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks

## Quick Facts
- arXiv ID: 2311.07784
- Source URL: https://arxiv.org/abs/2311.07784
- Reference count: 40
- Primary result: Achieves up to 25% absolute improvement in test accuracy and 28.3% average forgetting compared to FedAvg, FedProx, and FedCIL baselines

## Executive Summary
This paper addresses catastrophic forgetting in federated learning by proposing MFCL, a framework that uses a server-trained generative model to synthesize past data samples. Unlike prior approaches requiring client-side memory or data sharing, MFCL trains the generative model in a data-free manner using knowledge distillation, avoiding privacy risks and reducing client-side computational burden. The framework trains a global model across multiple tasks where each task introduces new classes, and uses synthetic samples alongside real data to mitigate forgetting.

## Method Summary
MFCL is a federated class incremental learning framework that addresses catastrophic forgetting by synthesizing past data samples using a server-side generative model. The server trains a ConvNet-based generative model using knowledge distillation from the global model weights, without accessing any client data. During each task, clients train on both real data and synthetic samples generated by the server's model. The approach splits classification heads per task to reduce interference between current and past class learning, and aligns batch statistics between synthetic and real data to preserve model performance.

## Key Results
- Up to 25% absolute improvement in test accuracy compared to FedAvg, FedProx, and FedCIL baselines
- 28.3% average forgetting reduction on CIFAR-100, TinyImageNet, and SuperImageNet datasets
- Modest server-side computational overhead with minimal client-side changes required
- Synthetic data quality sufficient to prevent the global model from distinguishing synthetic from real data based on batch statistics

## Why This Works (Mechanism)

### Mechanism 1
Server-side data-free generative model synthesis prevents client-side privacy exposure by training the generative model using only global model weights through knowledge distillation, avoiding direct access to client data while still producing class-representative synthetic samples.

### Mechanism 2
Splitting classification heads per task reduces interference between current and past class learning by only updating linear heads corresponding to new classes during task t while freezing heads for previously learned classes.

### Mechanism 3
Batch statistics alignment between synthetic and real data preserves model performance by enforcing identical BatchNorm statistics in generated synthetic images as in real data, preventing the model from distinguishing synthetic from real data based on distribution differences.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Enables server to train generative model without accessing client data by transferring knowledge from the global model
  - Quick check question: What is the primary difference between knowledge distillation and standard supervised training?

- Concept: Catastrophic forgetting
  - Why needed here: Core problem being addressed - models lose performance on previous tasks when learning new ones
  - Quick check question: What are the two main categories of solutions to catastrophic forgetting in continual learning?

- Concept: Federated learning aggregation
  - Why needed here: Understanding how FedAvg aggregates client updates is crucial for understanding how the global model evolves
  - Quick check question: In FedAvg, what mathematical operation combines client updates into the global model?

## Architecture Onboarding

- Component map: Global model (ResNet18) -> Server-side generative model (ConvNet) -> Client training pipeline -> BatchNorm statistics store

- Critical path: 1) Clients train local models on real data + synthetic data from server, 2) Server aggregates updates via FedAvg, 3) At task end, server trains generative model using knowledge distillation, 4) Server sends updated generative model to clients for next task

- Design tradeoffs: Server computational overhead vs client privacy, Synthetic data quality vs training time, Model size vs communication efficiency

- Failure signatures: Sudden accuracy drop on previous tasks indicates generative model quality issues, Slow convergence suggests feature drift problems, High variance across clients indicates data heterogeneity issues

- First 3 experiments: 1) Train on single task with synthetic data only - verify generative model quality, 2) Two-task scenario - verify forgetting mitigation, 3) Multi-client simulation - verify federated aspects work correctly

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MFCL scale with increasing number of clients and tasks in realistic federated settings? The paper does not provide experiments or analysis of how MFCL performs as the number of clients and tasks increases significantly beyond the tested range (50-300 clients, 10 tasks).

### Open Question 2
How robust is MFCL to different data distributions and non-IID settings across clients? The experiments use a relatively simple data distribution (LDA with Î±=1) and do not test MFCL under extreme non-IID conditions.

### Open Question 3
What is the impact of different generative model architectures and training strategies on MFCL's performance? The paper uses a ConvNet architecture and specific loss functions but does not explore alternative architectures or training strategies.

## Limitations
- Scalability to larger datasets and higher resolution images remains untested
- Server-side computational overhead for training generative models at each task boundary could be prohibitive in real-world deployments
- Long-term stability across many incremental tasks (>10) is not demonstrated

## Confidence

- **High Confidence**: The fundamental mechanism of using knowledge distillation for data-free generative model training is well-established in literature
- **Medium Confidence**: The effectiveness of synthetic data in mitigating catastrophic forgetting relies on generative model quality and assumptions about feature space representation
- **Low Confidence**: Long-term stability of the approach across many incremental tasks is not demonstrated

## Next Checks

1. **Scalability Test**: Evaluate MFCL on a higher-resolution dataset (e.g., 224x224 ImageNet) to assess generative model quality and computational feasibility at larger scales

2. **Robustness Analysis**: Measure performance under varying degrees of data heterogeneity and client participation rates to validate federated aspects under realistic conditions

3. **Long-term Learning**: Extend experiments to 20+ tasks to assess whether catastrophic forgetting re-emerges over extended incremental learning scenarios and identify potential performance plateaus or degradation points