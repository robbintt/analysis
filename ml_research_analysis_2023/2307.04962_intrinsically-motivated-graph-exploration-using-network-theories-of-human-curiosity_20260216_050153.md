---
ver: rpa2
title: Intrinsically motivated graph exploration using network theories of human curiosity
arxiv_id: '2307.04962'
source_url: https://arxiv.org/abs/2307.04962
tags:
- graph
- information
- nodes
- learning
- curiosity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses how to best guide intrinsic exploration in\
  \ graph-structured environments using theories of human curiosity. The authors adapt\
  \ two theories\u2014information gap theory and compression progress theory\u2014\
  as reward functions for reinforcement learning, where curiosity is viewed as optimizing\
  \ topological features of subgraphs induced by visited nodes."
---

# Intrinsically motivated graph exploration using network theories of human curiosity

## Quick Facts
- arXiv ID: 2307.04962
- Source URL: https://arxiv.org/abs/2307.04962
- Reference count: 40
- One-line primary result: Agents trained with intrinsic curiosity rewards generalize to larger environments and predict human behavior better than standard methods.

## Executive Summary
This work proposes a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The authors use these theories to design reward functions for reinforcement learning, where an agent learns to explore graph environments by optimizing topological features of visited subgraphs. A graph neural network is employed to efficiently approximate the expensive topological reward computations, enabling exploration in both synthetic and real-world graph environments.

## Method Summary
The method trains agents to explore graph-structured environments using intrinsic motivations derived from information gap theory (IGT) and compression progress theory (CPT) as reward functions. Graph neural networks (GraphSAGE) parameterized as Q-value functions are trained via DQN with experience replay, target networks, and epsilon-greedy exploration. Rewards are based on 1-dimensional cavities (Betti numbers) for IGT and network compressibility for CPT. The approach is evaluated on synthetic graph models (Erdös-Rényi, Barabási-Albert, Random Geometric, Watts-Strogatz) and real-world datasets (MovieLens, Amazon Books, Wikispeedia).

## Key Results
- Trained agents generalize to longer exploratory walks and larger environments than seen during training.
- Curiosity-biased PageRank centrality better predicts human behavior than standard PageRank, with improvements ranging from 2.9% to 32.2%.
- The GNN approach is computationally more efficient than greedy evaluation of topological features.

## Why This Works (Mechanism)

### Mechanism 1
Graph neural networks (GNNs) can approximate expensive topological reward computations for exploration in graph-structured environments. The GNN learns to predict the rewards associated with candidate nodes by processing subgraph embeddings, reducing the need for real-time computation of topological features like Betti numbers or compressibility. Core assumption: The topological properties of subgraphs can be effectively represented as embeddings that a GNN can learn to optimize. Evidence anchors: [abstract] "We use these proposed features as rewards for graph neural-network-based reinforcement learning." [section] "Our method computes more efficiently than the greedy evaluation of the relevant topological properties."

### Mechanism 2
Intrinsic motivations based on human curiosity theories can guide exploration in graph-structured environments by optimizing topological features. Information gap theory (IGT) and compression progress theory (CPT) are adapted as reward functions, encouraging the agent to explore nodes that create or close information gaps and improve compressibility, respectively. Core assumption: Topological features like Betti numbers and compressibility are meaningful proxies for the human concepts of information gaps and knowledge compression. Evidence anchors: [abstract] "We propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory." [section] "Drawing inspiration from human curiosity, we adopt information gap theory and compression progress theory to design two functions, FIGT and FCPT."

### Mechanism 3
Agents trained with intrinsic curiosity rewards generalize to larger environments and longer exploratory walks than seen during training. The GNN-based agent learns a policy that optimizes topological features, which are scale-invariant, allowing it to perform well in environments of varying sizes and trajectory lengths. Core assumption: The topological features used as rewards are invariant to the scale of the environment, enabling generalization. Evidence anchors: [abstract] "On multiple classes of synthetically generated graphs, we find that trained agents generalize to longer exploratory walks and larger environments than are seen during training." [section] "After training the GNN agent to explore 10 nodes in random geometric graph environments of 50 nodes, we evaluate generalization performance for shorter and longer trajectories and smaller and larger environments."

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The exploration problem is formulated as a Markov decision process (MDP) where the agent learns a policy to maximize cumulative rewards.
  - Quick check question: What is the difference between intrinsic and extrinsic rewards in RL?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to process subgraph embeddings and approximate the topological reward computations efficiently.
  - Quick check question: How do GNNs aggregate features from a node's local neighborhood?

- Concept: Algebraic Topology
  - Why needed here: Algebraic topology is used to formalize information gaps as topological cavities in the graph.
  - Quick check question: What is the relationship between simplicial complexes and the Betti numbers used to count topological gaps?

## Architecture Onboarding

- Component map:
  - Graph-structured environment (V, E)
  - Subgraph trajectory (S1 ⊂ S2 ⊂ ... ⊂ ST)
  - Graph feature function F: 2G → R (IGT or CPT)
  - Graph neural network Φ(·): G → R (GNN)
  - Deep Q-network (DQN) for policy learning
  - Replay buffer for experience sampling
  - Target network for stable learning

- Critical path:
  1. Initialize environment and subgraph trajectory
  2. At each step, build candidate subgraphs by adding neighbors
  3. Process candidates with GNN to obtain Q-values
  4. Select action (node to visit) based on Q-values
  5. Update subgraph trajectory and compute reward
  6. Store experience in replay buffer and update DQN

- Design tradeoffs:
  - GNN architecture: Depth vs. computational efficiency
  - Reward function: IGT (information gaps) vs. CPT (compressibility)
  - Exploration strategy: Greedy vs. epsilon-greedy

- Failure signatures:
  - GNN fails to approximate topological rewards accurately
  - Agent does not generalize to larger environments or longer trajectories
  - Intrinsic rewards do not correlate with meaningful exploration outcomes

- First 3 experiments:
  1. Train GNN agent on synthetic graphs (e.g., Erdös-Rényi, Barabási-Albert, Random Geometric, Watts-Strogatz) and evaluate performance against baselines (random, max degree, min degree, greedy).
  2. Test generalization to different trajectory lengths and environment sizes on synthetic graphs.
  3. Apply trained agent to real-world graph datasets (e.g., MovieLens, Amazon Books, Wikispeedia) and compare curiosity-biased PageRank centrality with standard PageRank.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed approach scale to very large graphs with millions of nodes and edges? Basis in paper: [explicit] The authors mention evaluating computational efficiency and note that greedy evaluation of topological features grows quickly with subgraph size, while the GNN offers a faster alternative. Why unresolved: The paper only tests on synthetic graphs up to 50 nodes and real-world datasets of moderate size. Scaling to massive graphs remains untested. What evidence would resolve it: Benchmarking the approach on large-scale real-world graphs (e.g., social networks, web graphs) with millions of nodes, comparing both performance and computational efficiency.

### Open Question 2
Can the GNN-based exploration method be extended to optimize for higher-dimensional topological features (beyond 1-dimensional cavities)? Basis in paper: [explicit] The authors focus on 1-dimensional cavities (loops) for the information gap theory, but note that cavities can take several forms (dimension 0, 1, 2). Why unresolved: The paper only explores 1-dimensional cavities and compressibility; higher-dimensional topological features are not investigated. What evidence would resolve it: Demonstrating improved exploration performance when optimizing for higher-dimensional cavities (e.g., 2-dimensional cavities) and comparing to the current approach.

### Open Question 3
How does the proposed method perform when the exploration objective (information gap theory or compression progress theory) does not align well with the underlying graph structure? Basis in paper: [inferred] The authors show that the max degree baseline performs well for compression progress theory in clustered graphs, suggesting that the effectiveness of the method depends on the graph structure. Why unresolved: The paper primarily tests on graph models where the objectives are known to be meaningful. Performance on graphs where the objectives are less relevant is not explored. What evidence would resolve it: Testing the approach on graph structures where information gaps and compressibility are less meaningful (e.g., trees, grids) and comparing performance to other exploration strategies.

## Limitations
- The scalability of computing compressibility-based rewards for large graphs remains unclear, as optimal graph clustering is NP-hard.
- While the GNN approach is claimed to be more efficient than greedy evaluation, the exact wall-time comparisons are not provided.
- The generalization claims rely on topological features being scale-invariant, but the robustness of this assumption across diverse graph structures is not rigorously tested.

## Confidence
- High confidence: The core claim that GNNs can approximate topological rewards efficiently is well-supported by the proposed method and experimental setup.
- Medium confidence: The claim of improved prediction of human behavior via curiosity-biased PageRank centrality is plausible but depends on the quality of the real-world datasets and the alignment of topological features with human exploration patterns.
- Low confidence: The claim that the topological features used as rewards are truly scale-invariant is based on theoretical reasoning but lacks empirical validation across a wide range of graph sizes and structures.

## Next Checks
1. Implement and profile the compressibility calculation for CPT rewards on large graphs to quantify the computational overhead and identify potential bottlenecks.
2. Conduct ablation studies to isolate the contribution of GNN architecture choices (e.g., depth, aggregation functions) to the efficiency and accuracy of reward approximation.
3. Design experiments to explicitly test the scale-invariance of the topological features used as rewards, by evaluating agent performance on graphs with varying sizes and comparing the learned policies.