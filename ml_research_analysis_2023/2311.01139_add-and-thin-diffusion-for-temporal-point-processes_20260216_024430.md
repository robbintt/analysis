---
ver: rpa2
title: 'Add and Thin: Diffusion for Temporal Point Processes'
arxiv_id: '2311.01139'
source_url: https://arxiv.org/abs/2311.01139
tags:
- event
- process
- sequences
- diffusion
- intensity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADD-THIN, a diffusion model for temporal
  point processes (TPPs) that learns to denoise entire event sequences in parallel
  rather than autoregressively. The key innovation is a novel noising process that
  naturally handles both discrete (number of events) and continuous (arrival times)
  components of TPPs by adding and thinning events.
---

# Add and Thin: Diffusion for Temporal Point Processes

## Quick Facts
- arXiv ID: 2311.01139
- Source URL: https://arxiv.org/abs/2311.01139
- Reference count: 40
- Key outcome: ADD-THIN matches state-of-the-art TPP models in density estimation while significantly outperforming them in forecasting tasks, particularly for long sequences where autoregressive models suffer from error accumulation.

## Executive Summary
This paper introduces ADD-THIN, a diffusion model for temporal point processes that learns to denoise entire event sequences in parallel rather than autoregressively. The key innovation is a novel noising process that naturally handles both discrete (number of events) and continuous (arrival times) components of TPPs by adding and thinning events. The model approximates the posterior intensity to reverse this noising process, enabling sampling of event sequences without requiring sequential autoregressive sampling. Experiments show ADD-THIN matches state-of-the-art TPP models in density estimation while significantly outperforming them in forecasting tasks, particularly for long sequences where autoregressive models suffer from error accumulation.

## Method Summary
ADD-THIN adapts diffusion models to temporal point processes by defining a forward noising process that adds new events from a homogeneous Poisson process and thins existing events with probability 1-αn. This creates a Markov chain of TPPs converging to a standard homogeneous Poisson process. The reverse process learns to approximate the posterior intensity λn-1(t | t(0), t(n)), which can be decomposed into intensities from disjoint event sets. The model uses a neural network to classify which events belong to the original sequence and parameterizes the intensity for removed events using a mixture of truncated Gaussians. This allows parallel sampling of entire event sequences without sequential autoregressive sampling.

## Key Results
- ADD-THIN matches state-of-the-art TPP models in density estimation (MMD, Wasserstein distance)
- Significantly outperforms autoregressive models in forecasting tasks (Wasserstein distance, MAPE)
- Shows constant sampling time for sequences up to 1129 events, scaling better than autoregressive approaches
- Maintains performance advantage particularly for datasets with higher numbers of events per sequence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ADD-THIN learns to reverse a noising process that naturally handles both the discrete (number of events) and continuous (arrival times) components of temporal point processes.
- **Mechanism:** The noising process uses superposition (adding new events from a homogeneous Poisson process) and thinning (removing existing events with probability 1-αn). This creates a Markov chain of TPPs that converges to a standard homogeneous Poisson process (HPP), preserving the structure needed for reverse sampling.
- **Core assumption:** The forward process defined by adding and thinning maintains the TPP structure throughout all steps, and the reverse process can be learned by approximating the posterior intensity.
- **Evidence anchors:**
  - [abstract] "a novel noising process that naturally handles both discrete (number of events) and continuous (arrival times) components of TPPs by adding and thinning events"
  - [section 3.1] "Equation 2 corresponds to a superposition of (i) a process λn-1 thinned with probability 1 - αn (removing old points), and (ii) an HPP with intensity (1 - αn)λHPP (adding new points)"
- **Break condition:** If the thinning probability is too high or the added intensity is too low, the noising process may lose information about the original event structure, making accurate reverse sampling impossible.

### Mechanism 2
- **Claim:** The model approximates the posterior intensity λn-1(t | t(0), t(n)) to enable parallel sampling of entire event sequences without sequential autoregressive sampling.
- **Mechanism:** Given t(n) and t(0), the posterior intensity can be decomposed into five disjoint sets (B-E) representing different event fates. The model learns to predict which events from t(n) belong to the original sequence (t(B)) and models the intensity for removed events (λ(A∪C)) using a mixture of truncated Gaussians.
- **Core assumption:** The posterior intensity is a superposition of intensities from disjoint event sets, and this structure can be approximated using neural networks.
- **Evidence anchors:**
  - [section 3.2] "λn-1(t | t(0), t(n)) = λ(B)(t) + λ(C)(t) + λ(D)(t) + λ(E)(t)"
  - [section 3.3] "λ(A∪C)θ(t) = K H j=1 wjf(t; μj, σj)"
- **Break condition:** If the mixture model for λ(A∪C) cannot adequately approximate the true intensity, the model will fail to correctly reconstruct the removed events, leading to poor sample quality.

### Mechanism 3
- **Claim:** The model's performance advantage in forecasting comes from avoiding error accumulation inherent in autoregressive models.
- **Mechanism:** Instead of sequentially predicting each event and accumulating errors, ADD-THIN refines the entire sequence in parallel at each diffusion step, maintaining consistency across the full sequence.
- **Core assumption:** Parallel refinement of the entire sequence preserves global structure better than sequential local predictions.
- **Evidence anchors:**
  - [abstract] "significantly outperforming them in forecasting tasks, particularly for long sequences where autoregressive models suffer from error accumulation"
  - [section 5.2] "The disparity between ADD-THIN and the baselines is more pronounced for datasets with a higher number of events per sequence, indicating the accumulation of prediction errors in the autoregressive models"
- **Break condition:** If the number of diffusion steps is insufficient, the model may not adequately refine the sequence, reducing the advantage over autoregressive approaches.

## Foundational Learning

- **Concept:** Temporal Point Processes (TPPs)
  - **Why needed here:** ADD-THIN is specifically designed for TPPs, which model sequences where both the number of events and their arrival times are random variables. Understanding the intensity function and how TPPs differ from standard time series is crucial.
  - **Quick check question:** What is the key difference between a temporal point process and a standard time series in terms of what is modeled as random?

- **Concept:** Diffusion Models
  - **Why needed here:** ADD-THIN adapts diffusion models, which typically denoise continuous data, to handle the unique structure of TPPs. Understanding the forward noising process and reverse denoising process is essential.
  - **Quick check question:** In standard diffusion models for images, what type of noise is typically added, and how does this differ from the noise structure in ADD-THIN?

- **Concept:** Intensity Functions and Conditional Distributions
  - **Why needed here:** The model learns to approximate intensity functions that define the probability of events occurring at specific times. This is central to both the forward noising process and the reverse denoising process.
  - **Quick check question:** How does the intensity function in a TPP relate to the conditional density of the next event time?

## Architecture Onboarding

- **Component map:** Temporal embedding layer -> 1D CNN -> Classifier MLP + Intensity MLP -> GRU encoder (for conditional sampling)
- **Critical path:** The core inference path involves embedding the input sequence, classifying events that belong to the original sequence, sampling from the intensity model, and combining these components to reconstruct the previous step in the diffusion chain.
- **Design tradeoffs:**
  - Using a CNN instead of attention for temporal encoding: Better computational efficiency but potentially less expressive for very long-range dependencies
  - Mixture of Gaussians for intensity modeling: Flexible and allows closed-form integration, but requires choosing the number of components
  - Parallel sampling vs. autoregressive: Eliminates error accumulation but requires more complex modeling of the full sequence distribution
- **Failure signatures:**
  - Poor density estimation: Indicates the intensity model or classifier is not adequately capturing the data distribution
  - Degraded forecasting performance: Suggests the model is not effectively leveraging historical information
  - Runtime scaling issues: May indicate the CNN layers or mixture components need optimization for longer sequences
- **First 3 experiments:**
  1. Train on a simple synthetic dataset (e.g., inhomogeneous Poisson process) and verify the model can recover the intensity function
  2. Test conditional sampling on a small real-world dataset with known history to validate forecasting capability
  3. Compare sampling runtime vs. autoregressive baselines on sequences of varying lengths to confirm the parallel advantage

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The model assumes clean input sequences but real-world data often contains missing events or timestamp noise, which isn't tested
- Limited analysis of how the noising process degrades with extreme thinning probabilities or insufficient added intensity
- The sensitivity to the number of diffusion steps and mixture components isn't fully characterized

## Confidence

- **High Confidence**: The parallel sampling mechanism and its advantage over autoregressive approaches for long sequences is well-supported by experimental results showing reduced error accumulation. The core architecture components (temporal embeddings, CNN layers, mixture modeling) are clearly specified and implemented.

- **Medium Confidence**: The theoretical foundation for the noising process maintaining TPP structure is sound, but the practical implications of edge cases (extreme thinning, insufficient diffusion steps) need more rigorous validation. The assumption that a fixed mixture model can capture all intensity variations across reverse steps appears reasonable but warrants deeper analysis.

- **Low Confidence**: The exact sensitivity of forecasting performance to the choice of diffusion schedule (beta parameters) and mixture component count isn't fully characterized. The claim that ADD-THIN "naturally handles" both discrete and continuous components would benefit from more explicit theoretical justification.

## Next Checks

1. **Ablation study on diffusion schedule**: Systematically vary the beta schedule parameters and diffusion steps to quantify the trade-off between sampling quality and computational efficiency. Measure how performance degrades when using fewer steps than recommended.

2. **Mixture model sensitivity analysis**: Test ADD-THIN with different numbers of mixture components (e.g., 1, 5, 15, 25) on the same datasets to identify the point of diminishing returns and potential overfitting behavior.

3. **Edge case robustness testing**: Evaluate performance on sequences with extreme characteristics (very sparse events, highly bursty patterns, long inter-event times) to assess the model's ability to handle challenging TPP structures where the noising process might lose critical information.