---
ver: rpa2
title: 'Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations'
arxiv_id: '2311.04335'
source_url: https://arxiv.org/abs/2311.04335
tags:
- sentence
- propositions
- proposition
- text
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces sub-sentence encoders, a contrastively-learned
  contextual embedding model that represents atomic propositions within text. Unlike
  sentence embeddings that encode entire sequences into fixed vectors, sub-sentence
  encoders produce distinct contextual embeddings for each proposition (unit of meaning)
  in a sentence.
---

# Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations

## Quick Facts
- arXiv ID: 2311.04335
- Source URL: https://arxiv.org/abs/2311.04335
- Reference count: 32
- Key outcome: Introduces sub-sentence encoders that produce contextual embeddings for atomic propositions, outperforming sentence-level embeddings on fine-grained semantic retrieval tasks while maintaining similar inference costs.

## Executive Summary
This paper presents sub-sentence encoders, a novel approach for representing atomic propositions within text using contrastively-learned contextual embeddings. Unlike traditional sentence encoders that produce a single embedding per sentence, sub-sentence encoders generate distinct embeddings for each proposition (unit of meaning) within a sentence. The model is trained using supervised contrastive learning to recognize semantic equivalence between propositions across different text sequences. Experiments demonstrate that sub-sentence encoders effectively retrieve supporting facts for fine-grained text attribution and recognize conditional semantic similarity, achieving significant improvements over sentence-level baselines while maintaining comparable computational efficiency.

## Method Summary
Sub-sentence encoders use a transformer encoder with token masks to generate proposition embeddings. Sentences are segmented into propositions using GPT-3.5-turbo or T5-large, and positive proposition pairs are identified using NLI models. The encoder processes the full sentence with full attention, while token masks select which tokens to pool for each proposition. An MLP projection layer transforms pooled representations into fixed-dimensional embeddings. The model is trained with supervised contrastive loss, treating semantically equivalent propositions as positive pairs and all others as negatives. During inference, query propositions are formatted as token masks and processed through the encoder to generate embeddings for retrieval tasks.

## Key Results
- Achieves 60.2% precision@1 on atomic fact retrieval, significantly outperforming sentence-level baselines (43.3%)
- Maintains similar inference cost and space complexity to sentence encoders while providing finer-grained semantic representation
- Reduces output embedding size by 12× to 16× through dimension reduction with minimal performance loss

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning with atomic propositions captures fine-grained semantic relationships that sentence-level embeddings miss. The model learns to produce similar embeddings for propositions expressing the same meaning across different sentences, while producing dissimilar embeddings for different propositions. This is achieved through supervised contrastive loss that treats positive proposition pairs as semantically equivalent and all others as negatives.

### Mechanism 2
Maintaining full attention while applying token masks only during pooling preserves contextual information for each proposition. Unlike methods that mask attention or use only token subsets, this approach keeps the transformer encoder's full attention on the entire sentence while only using masks to select which tokens to pool for each proposition.

### Mechanism 3
Output dimension reduction during training provides significant index size compression with minimal performance loss. By training with a bottlenecked output dimension (e.g., 64 instead of 1024-768), the model learns compressed representations that maintain semantic information while dramatically reducing storage requirements for large-scale indexing.

## Foundational Learning

- **Concept: Supervised contrastive learning**
  - Why needed here: This framework is essential for learning to distinguish between semantically similar and dissimilar propositions within the same batch, enabling the model to capture fine-grained semantic relationships.
  - Quick check question: What is the key difference between supervised contrastive loss and standard cross-entropy loss in this context?

- **Concept: Proposition segmentation and alignment**
  - Why needed here: The ability to accurately segment sentences into atomic propositions and align them back to token positions is fundamental for creating training data and formatting inputs for the sub-sentence encoder.
  - Quick check question: How does the Hungarian algorithm help in finding optimal token alignment between natural language propositions and their sentence contexts?

- **Concept: Dense retrieval and indexing**
  - Why needed here: Understanding dense retrieval principles and indexing strategies is crucial for applying sub-sentence encoders to large-scale retrieval tasks and managing the increased index size from proposition-level encoding.
  - Quick check question: What trade-offs exist between index size and retrieval accuracy when using compressed embeddings versus full-dimensional embeddings?

## Architecture Onboarding

- **Component map**: Transformer encoder → token mask pooling → MLP projection → proposition embeddings
- **Critical path**: Input sentence and token masks → transformer encoding → token mask pooling → MLP projection → proposition embeddings
- **Design tradeoffs**: Full attention vs. masked attention (context preservation vs. computational efficiency), proposition granularity (finer granularity captures more semantic detail but increases index size), and output dimension (higher dimensions capture more information but increase storage requirements)
- **Failure signatures**: Poor proposition retrieval performance indicates issues with contrastive learning or proposition segmentation; context loss in embeddings suggests problems with the pooling strategy; memory issues indicate problems with index compression or batch size configuration
- **First 3 experiments**:
  1. Test proposition segmentation quality on a small dataset using both GPT-3.5 and T5 models to compare output quality and alignment accuracy
  2. Evaluate the impact of different token mask strategies (full mask, mask pooling only, token subset only) on a small retrieval task to determine the optimal input formatting approach
  3. Measure performance vs. index size trade-offs by training with different output dimensions (e.g., 64, 128, 256, 768) on a subset of the training data and evaluating retrieval accuracy and storage requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sub-sentence encoders perform on multilingual datasets compared to monolingual English datasets?
- Basis in paper: The paper acknowledges that experiments were conducted on English text only and suggests exploring multilingual sub-sentence encoders as future work.
- Why unresolved: The paper's experiments were limited to English text, and there is no mention of testing on multilingual datasets.
- What evidence would resolve it: Conducting experiments on multilingual datasets to compare the performance of sub-sentence encoders with their performance on monolingual English datasets.

### Open Question 2
- Question: What is the impact of proposition boundary accuracy on the performance of sub-sentence encoders?
- Basis in paper: The paper discusses the robustness of sub-sentence encoders to imperfect proposition boundaries and suggests that the model learns to adapt to such boundaries.
- Why unresolved: While the paper mentions the robustness to proposition boundaries, it does not provide detailed analysis on the impact of boundary accuracy on model performance.
- What evidence would resolve it: Conducting experiments to measure the performance of sub-sentence encoders with varying levels of proposition boundary accuracy.

### Open Question 3
- Question: How does the choice of proposition extraction method affect the performance of sub-sentence encoders?
- Basis in paper: The paper discusses the use of GPT-3.5-turbo and T5-large for proposition segmentation and mentions that the quality of propositions extracted via the pipeline is evaluated.
- Why unresolved: The paper does not provide a detailed comparison of different proposition extraction methods and their impact on sub-sentence encoder performance.
- What evidence would resolve it: Comparing the performance of sub-sentence encoders using propositions extracted by different methods (e.g., GPT-3.5-turbo vs. T5-large) to determine the impact of the extraction method on performance.

## Limitations

- Proposition segmentation quality and scalability are major concerns, as the method relies heavily on black-box models (GPT-3.5, T5-large) with unspecified prompts and limited validation for scaling
- Training data generation complexity involves multiple error-prone steps that compound, including sentence segmentation, NLI-based labeling, and token alignment via Hungarian matching
- Index size increases substantially (up to 8× larger with uncompressed embeddings), and the trade-off between performance and storage isn't fully characterized across different tasks and dataset sizes

## Confidence

- **High confidence**: The core technical contribution of the sub-sentence encoder architecture and supervised contrastive learning framework are well-specified and theoretically sound, with convincing empirical demonstration on atomic fact retrieval
- **Medium confidence**: Claims about maintaining similar inference costs to sentence encoders are supported by architecture description but lack comprehensive runtime measurements; compression results are demonstrated but methodology for evaluating "minimal" performance loss could be more rigorous
- **Low confidence**: Scalability claims for proposition segmentation and alignment rely on black-box models with unspecified prompts and limited validation, making it uncertain whether the approach generalizes well beyond small datasets

## Next Checks

**Validation Check 1**: Reproduce the proposition segmentation pipeline using the same GPT-3.5 prompts (once specified) on a held-out test set of sentences, measuring Jaccard similarity between generated propositions and human-annotated propositions.

**Validation Check 2**: Conduct a controlled experiment comparing different token masking strategies (full mask pooling, mask pooling only, token subset only) on a small retrieval benchmark to empirically validate the claim that full attention with mask pooling is optimal.

**Validation Check 3**: Perform a comprehensive evaluation of the compression trade-offs by training sub-sentence encoders with multiple output dimensions (e.g., 32, 64, 128, 256, 768) and measuring retrieval performance versus index size across different dataset scales.