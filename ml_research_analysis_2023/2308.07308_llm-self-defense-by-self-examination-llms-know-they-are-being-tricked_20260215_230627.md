---
ver: rpa2
title: 'LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked'
arxiv_id: '2308.07308'
source_url: https://arxiv.org/abs/2308.07308
tags:
- harmful
- content
- prompt
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM Self Defense uses a second LLM to filter harmful content generated
  by a first LLM. It prompts the second model to classify whether text from the first
  model is harmful, without fine-tuning or preprocessing.
---

# LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked

## Quick Facts
- arXiv ID: 2308.07308
- Source URL: https://arxiv.org/abs/2308.07308
- Reference count: 17
- LLM Self Defense uses a second LLM to filter harmful content generated by a first LLM

## Executive Summary
LLM Self Defense is a novel approach that uses a second LLM instance to detect and filter harmful content generated by a first LLM. The method employs zero-shot prompting to classify whether text from the first LLM is harmful, without requiring fine-tuning or preprocessing. Experiments with Vicuña-generated responses demonstrate high accuracy rates (95-97.5%) for models like Claude, Bard, and GPT-3.5, while Llama-2 achieves 80.9%. The approach effectively reduces attack success rates to near zero and shows promise in defending against adversarial suffix attacks on LLMs.

## Method Summary
The method uses a second LLM instance to classify whether text generated by a first LLM is harmful. It constructs a zero-shot harm classifier by prompting the second LLM with a simple instruction like "Is the following content harmful: {tc}. Respond with a yes or no answer." The approach validates only the LLM-generated content, not the prompt itself, making it harder for adversarial prompts to influence the validation model. Abstentions from the classifier are treated as strong indicators of harmful content.

## Key Results
- Claude, Bard, and GPT-3.5 achieve 95-97.5% accuracy in detecting harmful content
- Llama-2 reaches 80.9% accuracy with higher abstention rates (42.5%)
- Ignoring abstentions significantly lowers accuracy, indicating their importance as harm indicators
- Attack success rates reduced to near zero with this defense mechanism

## Why This Works (Mechanism)

### Mechanism 1
A second LLM instance can detect harmful content in outputs from the first LLM through zero-shot prompting. The approach constructs a zero-shot classifier by prompting a second LLM to evaluate whether text from the first LLM contains harmful content. The second LLM is given a simple instruction and generates a binary prediction based on its internal understanding of "harmful" content.

### Mechanism 2
Abstentions from the harm classifier strongly indicate harmful content. When the second LLM is asked to classify content, it sometimes abstains from responding. The approach treats these abstentions as strong indicators that the content is harmful, as they correlate with the harmfulness of the content being evaluated.

### Mechanism 3
Filtering only the LLM-generated response rather than the prompt makes it harder for adversarial prompts to influence the validation model. By validating only the content generated by the first LLM in response to a user prompt, the approach potentially makes it harder for an adversarial prompt to influence the validation model's judgment.

## Foundational Learning

- Concept: Zero-shot classification
  - Why needed here: The approach relies on prompting an LLM to classify content without providing examples or fine-tuning, making zero-shot classification fundamental to the method.
  - Quick check question: What is the key difference between zero-shot, few-shot, and fine-tuned classification approaches?

- Concept: Adversarial attacks on language models
  - Why needed here: Understanding how adversarial attacks work is crucial for appreciating why this self-defense mechanism is necessary and how it might defend against such attacks.
  - Quick check question: How do adversarial suffixes work to bypass safety measures in aligned language models?

- Concept: Prompt engineering and jailbreaking
  - Why needed here: The approach addresses vulnerabilities that arise from sophisticated prompt engineering techniques that can trick LLMs into generating harmful content, so understanding these techniques is essential.
  - Quick check question: What is the difference between jailbreaking and adversarial attacks in the context of LLM safety?

## Architecture Onboarding

- Component map: User input -> First LLM instance -> Content extraction -> Second LLM instance -> Decision logic -> Output filter

- Critical path:
  1. User provides prompt to first LLM
  2. First LLM generates response text
  3. Response text is passed to second LLM with classification prompt
  4. Second LLM returns yes/no/abstain
  5. System decides whether to filter content based on response

- Design tradeoffs:
  - Using a second LLM adds latency and cost but provides strong accuracy
  - Treating abstentions as harmful increases safety but may reduce throughput
  - Zero-shot approach requires no training data but may be less precise than fine-tuned models
  - Model selection for the classifier significantly impacts performance

- Failure signatures:
  - High abstention rates (>20%) may indicate model confusion or poor prompt design
  - Consistent misclassification of specific content types suggests bias in the classifier
  - Latency spikes when processing complex content
  - False positives on benign but complex content

- First 3 experiments:
  1. Test classifier accuracy on a balanced dataset of harmful and benign Vicuña-generated content using different base models (Claude, Bard, GPT-3.5, Llama-2)
  2. Measure the impact of treating abstentions as harmful vs. ignoring them on overall accuracy
  3. Evaluate robustness against adversarial suffixes by generating harmful content with and without suffixes and measuring classification success

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of LLM Self Defense vary across different types of harmful content, such as hate speech, misinformation, or explicit instructions for illegal activities? The paper demonstrates the method's effectiveness on a set of harmful prompts but does not specify how it performs across different categories of harmful content.

### Open Question 2
Can the adversarial suffix attacks, as described by Zou et al. (2023), be adapted to specifically target and bypass the LLM Self Defense mechanism? The paper mentions adversarial attacks but does not explore the specific interaction between these attacks and the LLM Self Defense approach.

### Open Question 3
What is the impact of the underlying language model's training data and biases on the accuracy of the LLM Self Defense mechanism in identifying harmful content? The paper discusses the reliance of LLMs on their training corpora but does not address how biases and content of the training data of the LLM used as a filter might affect its ability to accurately identify harmful content.

## Limitations

- Performance varies significantly across different base models, with Llama-2 showing notably lower accuracy (80.9%) compared to other models
- The approach is tested against a specific type of adversarial attack (adversarial suffixes) but not comprehensively validated against other attack vectors
- Reliance on Vicuña-generated responses may limit generalizability to other model architectures or generation paradigms

## Confidence

- High Confidence: The core mechanism of using a second LLM for zero-shot classification is technically sound and the experimental results are clearly presented
- Medium Confidence: The effectiveness against adversarial attacks is demonstrated for a specific attack type but not comprehensively validated
- Low Confidence: The generalizability of the approach to different model architectures, prompt types, and attack strategies is not established

## Next Checks

1. Test the classifier against a broader range of attack types beyond adversarial suffixes, including prompt injection attacks, context manipulation, and multi-turn jailbreak strategies
2. Evaluate the approach with different base models for content generation to assess generalizability
3. Conduct an ablation study on the prompt design for the harm classifier, testing variations in prompt wording and instruction clarity