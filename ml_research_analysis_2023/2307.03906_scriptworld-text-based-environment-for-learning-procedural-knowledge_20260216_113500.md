---
ver: rpa2
title: 'ScriptWorld: Text Based Environment For Learning Procedural Knowledge'
arxiv_id: '2307.03906'
source_url: https://arxiv.org/abs/2307.03906
tags:
- environment
- scenario
- graph
- scenarios
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ScriptWorld, a text-based game environment
  designed to teach agents real-world procedural knowledge through script-based scenarios.
  The environment is built using the DeScript corpus, which contains crowd-sourced
  descriptions of daily activities like baking a cake or taking a bath.
---

# ScriptWorld: Text Based Environment For Learning Procedural Knowledge

## Quick Facts
- arXiv ID: 2307.03906
- Source URL: https://arxiv.org/abs/2307.03906
- Authors: 
- Reference count: 40
- One-line primary result: Language model embeddings significantly improve RL agent performance in text-based procedural knowledge learning environments

## Executive Summary
ScriptWorld is a text-based game environment designed to teach agents real-world procedural knowledge through script-based scenarios. The environment uses the DeScript corpus of crowd-sourced daily activity descriptions to create graph-based scenarios where agents must navigate through steps to complete tasks. The authors demonstrate that RL agents using pre-trained language model embeddings (SBERT) significantly outperform those using non-contextual embeddings like GloVe, validating the importance of semantic context in decision-making.

## Method Summary
The authors combine the DeScript corpus with graph-based scenario construction and reinforcement learning to create ScriptWorld. They extract SBERT embeddings from pre-trained language models to represent the semantic context of actions at each state, then feed these embeddings into RL algorithms (DQN, A2C, PPO, RPPO) to train agents. The environment includes adjustable difficulty through choice count and hint availability, and uses a back-hop penalty mechanism to discourage incorrect actions.

## Key Results
- RL agents using SBERT embeddings outperform those using GloVe embeddings, demonstrating the value of contextual semantic understanding
- ScriptWorld enables flexible difficulty adjustment through choice count and hint availability
- Agents show improved performance when learning procedural knowledge from real-world script scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language model embeddings provide prior semantic context that improves decision-making in text-based environments
- Mechanism: Pre-trained SBERT embeddings encode general world knowledge and language structure, allowing RL agents to better understand and choose between related actions in ScriptWorld
- Core assumption: Contextualized embeddings capture relevant semantic relationships between actions and states better than non-contextualized embeddings like GloVe
- Evidence anchors:
  - [abstract]: "Our experiments show that prior knowledge obtained from a pre-trained language model helps to solve real-world text-based gaming environments"
  - [section 4]: "RPPO with GloVe embeddings (non-contextualized word representations) performs poorly, depicting the importance of the context which is captured by contextualized LMs"
  - [corpus]: Weak - corpus does not directly compare LM types, but provides DeScript data that benefits from LM understanding
- Break condition: If the language model is trained on unrelated domains or if the text actions lack sufficient semantic overlap with the LM's training data

### Mechanism 2
- Claim: Scenario graphs with parallel paths increase task complexity, requiring agents to learn multiple valid strategies
- Mechanism: By splitting compact graph nodes into entry and exit nodes with parallel action sequences, the environment forces agents to explore and remember different valid paths to goal completion
- Core assumption: Multiple valid paths exist and are learnable, and the agent can benefit from exploring alternative strategies rather than just memorizing one sequence
- Evidence anchors:
  - [section 3]: "We split each event node in the compact graph into two nodes, the entry event node and the exit event node. Further, multiple action sequences result in parallel paths for reaching the exit node from the entry node"
  - [section 3]: "This helps to capture the variability in performing daily chores, making the environment more realistic"
  - [corpus]: Strong - DeScript explicitly provides multiple event sequence descriptions that justify parallel path modeling
- Break condition: If parallel paths are not meaningfully different or if the agent cannot generalize across them due to limited memory or exploration

### Mechanism 3
- Claim: Back-hop penalties encourage exploration and learning of correct sequences by forcing the agent to revisit prior states
- Mechanism: When an agent selects an incorrect action, it is displaced backward by a fixed number of hops in the scenario graph, increasing the cost of mistakes and encouraging careful decision-making
- Core assumption: The back-hop distance is calibrated so that it penalizes errors without making the environment intractable, and the agent can learn from these state revisits
- Evidence anchors:
  - [section 3]: "When an agent selects an incorrect choice, its location is displaced by hopping it backward in the temporal domain, and this back-hop distance is another parameter in the environment"
  - [section 5]: "Due to the presence of parallel paths in the graph, an agent hops to a previous node in case of incorrect action and may not follow the same path again, which acts as a penalty"
  - [corpus]: Weak - corpus provides the graph structure but does not directly address the back-hop mechanism design
- Break condition: If back-hop distance is too large, making recovery impossible, or too small, making it ineffective as a learning signal

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: ScriptWorld provides only textual observations of states without revealing the full underlying scenario graph, requiring agents to infer state from partial information
  - Quick check question: How does an agent update its belief about the current state when only receiving textual observations and not the full graph state?

- Concept: Script knowledge and procedural reasoning
  - Why needed here: The environment is designed to teach agents real-world procedural knowledge embedded in script structures, requiring understanding of typical event sequences
  - Quick check question: Why is it important for the agent to learn that "checking-in" typically precedes "going through security" at an airport?

- Concept: Graph traversal and path planning
  - Why needed here: The scenario graph represents possible action sequences, and successful agents must navigate from start to goal while avoiding invalid paths
  - Quick check question: How does the agent's choice at one node affect the set of available future choices in the scenario graph?

## Architecture Onboarding

- Component map:
  DeScript corpus → graph construction module → scenario graph → ScriptWorld environment
  Pre-trained SBERT model → embedding extraction → RL algorithm input
  RL algorithm (DQN/A2C/PPO/RPPO) → policy/value function → action selection
  Handicap generation module → GPT2 → hint text → environment interface

- Critical path:
  1. Load scenario graph for selected script
  2. Generate current state observation text
  3. Extract SBERT embeddings for available actions
  4. Feed embeddings to RL policy network
  5. Select action and receive reward
  6. Update policy based on reward and next state

- Design tradeoffs:
  - Choice-based vs parser-based: Choice-based limits vocabulary explosion but reduces naturalness
  - Handicap inclusion: Makes learning easier but may reduce robustness without hints
  - Graph complexity: More parallel paths increase realism but also difficulty

- Failure signatures:
  - High variance in learning curves suggests exploration-exploitation balance issues
  - Poor performance with GloVe embeddings indicates need for contextual understanding
  - Generalization failure across scenarios suggests overfitting to specific script patterns

- First 3 experiments:
  1. Train RPPO with SBERT embeddings on "Baking a Cake" with handicaps, 2 choices per step
  2. Test same trained model on "Riding a Bus" without handicaps to measure generalization
  3. Train without handicaps on "Taking a Bath" and compare performance to handicapped version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pre-trained language models (LMs) compare to other knowledge sources for improving RL agent performance in ScriptWorld?
- Basis in paper: [explicit] The paper mentions that SBERT language models are used as a source of prior real-world knowledge and compares their performance to GloVe embeddings (non-contextual). However, it does not explore other potential knowledge sources like external knowledge bases or large language models
- Why unresolved: The paper only focuses on SBERT-based embeddings and GloVe, leaving open the question of whether other knowledge sources could provide even better performance
- What evidence would resolve it: Experiments comparing RL agent performance using different knowledge sources (e.g., knowledge graphs, large language models) would clarify their relative effectiveness

### Open Question 2
- Question: How does the performance of RL agents in ScriptWorld generalize to other text-based environments or real-world tasks?
- Basis in paper: [explicit] The paper mentions that ScriptWorld is designed to teach agents about real-world daily chores and that the learned knowledge should be applicable to the real world. However, it does not evaluate the agents' performance in other text-based environments or real-world tasks
- Why unresolved: The paper focuses on the ScriptWorld environment and does not provide evidence of the agents' ability to generalize their learned knowledge to other domains
- What evidence would resolve it: Experiments testing RL agents trained in ScriptWorld on other text-based environments or real-world tasks would demonstrate their generalization capabilities

### Open Question 3
- Question: How can ScriptWorld be extended to include more complex scenarios or additional modalities?
- Basis in paper: [inferred] The paper mentions that ScriptWorld's current version only has 10 scenarios and that the environment provides actions available at any state in the form of choices. It also suggests the possibility of developing a parser-based version allowing free-form text and incorporating external knowledge
- Why unresolved: The paper does not explore the potential for extending ScriptWorld to include more complex scenarios or additional modalities like visual information
- What evidence would resolve it: Developing and evaluating ScriptWorld with more complex scenarios, additional modalities, or a parser-based version would provide insights into its potential for extension and improvement

## Limitations

- Limited scenario variety with only 10 scenarios in the current implementation
- Choice-based interaction limits natural language understanding compared to parser-based approaches
- Performance evaluation focuses on within-environment metrics without testing real-world task transfer

## Confidence

- High confidence in the basic functionality of ScriptWorld as a text-based environment
- Medium confidence in the specific performance claims about language model embeddings
- Low confidence in the scalability to more complex real-world scenarios

## Next Checks

1. **Reproduce with modern LLMs**: Replace SBERT with GPT-4 or LLaMA embeddings and measure performance impact on generalization across scenarios
2. **Stress test back-hop mechanism**: Systematically vary back-hop distances to find the optimal penalty that maximizes learning efficiency without causing training collapse
3. **Cross-corpus generalization**: Test ScriptWorld-trained agents on entirely different procedural text datasets (e.g., wikiHow) to assess true real-world knowledge transfer