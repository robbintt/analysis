---
ver: rpa2
title: Automated Distractor and Feedback Generation for Math Multiple-choice Questions
  via In-context Learning
arxiv_id: '2308.03234'
source_url: https://arxiv.org/abs/2308.03234
tags:
- feedback
- distractor
- distractors
- generated
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to automated distractor
  and feedback generation for math multiple-choice questions using large language
  models. The authors propose a simple in-context learning-based solution and introduce
  new metrics for evaluating the quality of generated distractors and feedback messages.
---

# Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning

## Quick Facts
- arXiv ID: 2308.03234
- Source URL: https://arxiv.org/abs/2308.03234
- Reference count: 24
- Key outcome: Novel approach to automated distractor and feedback generation for math MCQs using large language models, showing room for improvement in generating high-quality distractors and feedback.

## Executive Summary
This paper presents a novel approach to automated distractor and feedback generation for math multiple-choice questions using large language models. The authors propose a simple in-context learning-based solution and introduce new metrics for evaluating the quality of generated distractors and feedback messages. Experiments on a real-world dataset suggest that there is room for improvement in automated distractor and feedback generation, particularly in terms of generating distractors that align well with human-authored ones and producing informative feedback messages.

## Method Summary
The authors utilize in-context learning with similar MCQs chosen by the k-nearest neighbor (kNN) algorithm as few-shot examples for LLM input. They introduce distribution-based metrics using student response data and reference-free metrics using a generative LLM to evaluate the quality of generated distractors and feedback messages. The method involves fine-tuning a BERT model on student response data to predict option selection distributions and using this information to evaluate generated distractors. For feedback evaluation, the authors design reference-free metrics based on the observation that feedback messages tend to play two primary roles: to be helpful for students to reach a correct answer and to be an explanation for why an answer is incorrect.

## Key Results
- kNNall method surpasses other kNN-based methods in generating distractors that align well with human-authored ones.
- Distribution-based metrics show potential for evaluating distractor plausibility but have limitations due to aggregated student response data.
- Reference-free metrics for feedback evaluation provide meaningful results but are based on a generative LLM's interpretation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generate plausible distractors by imitating the style and structure of human-authored distractors when given similar examples as in-context prompts.
- Mechanism: The LLM analyzes patterns in the in-context examples, such as numerical relationships, mathematical errors, and phrasing, and applies these patterns to generate distractors that match the target question's format and content.
- Core assumption: The in-context examples are sufficiently similar to the target question in terms of mathematical concepts and structure to allow the LLM to learn the appropriate patterns.
- Evidence anchors:
  - [abstract] "We utilize in-context learning with similar MCQs chosen by the k-nearest neighbor (kNN) algorithm as few-shot examples for LLM input, since kNN has been shown to be an effective example selection heuristic for LLM prompting"
  - [section] "kNNall surpasses other kNN-based methods, as it uses the most information to identify in-context samples that are similar to the target question"
  - [corpus] Weak evidence; corpus focuses on distractor generation but not the specific in-context learning mechanism
- Break condition: If the in-context examples are too dissimilar from the target question, the LLM will not be able to learn the appropriate patterns and will generate poor-quality distractors.

### Mechanism 2
- Claim: The distribution-based metric using student response data can effectively evaluate the plausibility of generated distractors by predicting how likely real students are to select them.
- Mechanism: A BERT model is fine-tuned on student response data to predict the proportion of students selecting each option for a given MCQ. This model is then used to evaluate the plausibility of generated distractors based on their predicted selection probability.
- Core assumption: The BERT model trained on student response data can accurately predict the likelihood of students selecting each option for a given MCQ.
- Evidence anchors:
  - [section] "We use this information to train a model to predict the option selection distribution for all real MCQ distractors and use the trained model to evaluate generated ones"
  - [section] "We acknowledge that these metrics are not perfect measures of generated distractors' plausibility because a higher value for hsum and hentropy may indicate a poorly worded question stem or a difficult question"
  - [corpus] Weak evidence; corpus focuses on distractor generation but not the specific distribution-based evaluation method
- Break condition: If the BERT model is not accurate in predicting student selection probabilities, the distribution-based metrics will not be reliable indicators of distractor plausibility.

### Mechanism 3
- Claim: The reference-free metrics using a generative LLM can effectively evaluate the quality of generated feedback messages by assessing their helpfulness and informativeness.
- Mechanism: A generative LLM is prompted to adjust a distractor based on the feedback message and predict the corresponding distractor based on the feedback message. The success of these tasks indicates the feedback's helpfulness and informativeness.
- Core assumption: The generative LLM can accurately interpret and act upon the feedback messages to perform the adjustment and prediction tasks.
- Evidence anchors:
  - [section] "We design these metrics based on the observation that feedback messages tend to play two primary roles: to be helpful for students to reach a correct answer, and to be an explanation for why an answer is incorrect"
  - [section] "Despite these limitations, we argue that these metrics provide meaningful results on the relative performance of methods compared to each other and are significantly less biased than the reference-based metrics"
  - [corpus] Weak evidence; corpus focuses on feedback generation but not the specific reference-free evaluation method
- Break condition: If the generative LLM cannot accurately interpret and act upon the feedback messages, the reference-free metrics will not be reliable indicators of feedback quality.

## Foundational Learning

- Concept: Large language models and their capabilities
  - Why needed here: Understanding the strengths and limitations of LLMs is crucial for effectively using them in automated distractor and feedback generation tasks.
  - Quick check question: What are some key factors that influence the performance of LLMs in few-shot learning tasks?

- Concept: k-nearest neighbor algorithm and its applications
  - Why needed here: The kNN algorithm is used to select similar MCQs as in-context examples for the LLM, so understanding its mechanics and effectiveness is important.
  - Quick check question: How does the kNN algorithm determine the similarity between data points, and what are some common distance metrics used?

- Concept: Mathematical concepts and common student errors
  - Why needed here: Generating plausible distractors requires an understanding of the mathematical concepts involved and the typical errors students make when solving problems.
  - Quick check question: What are some common misconceptions or errors students make when solving percentage decrease problems, and how can these be used to generate effective distractors?

## Architecture Onboarding

- Component map: MCQ dataset, student response data -> kNN algorithm, LLM prompting, BERT fine-tuning -> Generated distractors, generated feedback messages, evaluation metrics

- Critical path:
  1. Select in-context examples using kNN
  2. Generate distractors using LLM with in-context examples
  3. Generate feedback messages using LLM with in-context examples
  4. Evaluate distractors using distribution-based metrics
  5. Evaluate feedback using reference-free metrics

- Design tradeoffs:
  - Using kNN vs. other example selection methods for LLM prompting
  - Fine-tuning BERT on student response data vs. using a pre-trained model
  - Reference-free metrics vs. human evaluation for feedback quality assessment

- Failure signatures:
  - Poor-quality distractors generated by LLM
  - Inaccurate student selection probability predictions by BERT model
  - Ineffective feedback messages according to reference-free metrics

- First 3 experiments:
  1. Compare the performance of kNN-based LLM prompting with random example selection for distractor generation.
  2. Evaluate the impact of fine-tuning BERT on student response data for distractor plausibility assessment.
  3. Assess the effectiveness of reference-free metrics in evaluating feedback quality compared to human evaluation.

## Open Questions the Paper Calls Out
1. How can we develop modified text encoding methods that are more closely aligned with student errors to improve the selection of in-context examples for distractor generation?
2. How can we create a more reliable distribution-based metric to evaluate the plausibility of generated distractors, given the limitations of using aggregated student response data?
3. How can we improve the quality and informativeness of generated feedback messages for math MCQs, given the limitations of current evaluation metrics and prompting strategies?

## Limitations
- The study relies on a relatively small dataset of 1.4K MCQs, which may limit the generalizability of the findings.
- The reference-free metrics for feedback evaluation are still based on a generative LLM's interpretation and may not fully capture the nuances of effective feedback.
- The paper does not provide detailed implementation specifics for the fine-tuned BERT model or the exact prompt formats used for zero-shot generation, which could impact reproducibility.

## Confidence
- High Confidence: The effectiveness of kNN-based example selection for LLM prompting, as supported by the observed performance of kNNall method.
- Medium Confidence: The distribution-based metrics for evaluating distractor plausibility, given the acknowledgment of their limitations and potential biases.
- Low Confidence: The reference-free metrics for feedback quality assessment, as they are based on a generative LLM's interpretation and may not fully align with human judgment.

## Next Checks
1. Conduct experiments on larger and more diverse MCQ datasets to assess the scalability and generalizability of the proposed approach.
2. Perform human evaluation studies to validate the effectiveness of the reference-free metrics for feedback quality assessment and compare them with the generative LLM-based metrics.
3. Investigate the impact of different fine-tuning strategies and hyperparameters for the BERT model used in distractor plausibility prediction to optimize its performance.