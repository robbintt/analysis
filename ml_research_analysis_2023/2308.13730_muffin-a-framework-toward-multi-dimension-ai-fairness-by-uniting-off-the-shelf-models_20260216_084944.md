---
ver: rpa2
title: 'Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf
  Models'
arxiv_id: '2308.13730'
source_url: https://arxiv.org/abs/2308.13730
tags:
- fairness
- unfairness
- accuracy
- score
- site
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-dimensional fairness in AI, where unfairness
  exists across multiple attributes in real-world datasets. Existing methods typically
  optimize fairness for a single attribute, but the authors observe that improving
  fairness on one attribute often worsens it on others.
---

# Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models

## Quick Facts
- arXiv ID: 2308.13730
- Source URL: https://arxiv.org/abs/2308.13730
- Reference count: 40
- Primary result: RL-based framework Muffin improves fairness across multiple attributes simultaneously, e.g., 26.32% fairness gain on one attribute while maintaining or slightly improving accuracy.

## Executive Summary
This paper addresses the challenge of multi-dimensional fairness in AI, where unfairness exists across multiple attributes in real-world datasets. The authors observe that existing single-attribute fairness methods often degrade fairness on other attributes. To solve this, they propose Muffin, a framework that unites multiple off-the-shelf models using a reinforcement learning controller to jointly optimize fairness across all attributes. The approach leverages model disagreements on unprivileged group data, fusing their outputs via an MLP trained on a fairness-aware proxy dataset.

## Method Summary
Muffin is a reinforcement learning-based framework that automatically searches for optimal model-fusing structures to improve multi-dimensional fairness. It consists of a model pool (pre-trained off-the-shelf models), an RNN controller that generates MLP architectures, and a fairness-aware proxy dataset with weighted unprivileged group data. The controller selects models and designs an MLP head to combine their outputs, optimizing parameters only in the MLP. Training uses a multi-fairness reward (accuracy divided by average unfairness) to guide the RL updates, balancing fairness gains with accuracy maintenance.

## Key Results
- On ISIC2019 dataset: 26.32% and 20.37% fairness improvement on two attributes simultaneously
- On Fitzpatrick17K dataset: Maintained or slightly improved overall accuracy (e.g., 5.58% accuracy gain)
- Demonstrates that optimizing fairness on one attribute without harming others is achievable through joint model fusion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Improving fairness on one attribute worsens fairness on others, requiring a new approach.
- **Mechanism**: Uses RL controller to jointly optimize multiple models and fuse outputs via MLP, guided by multi-fairness reward.
- **Core assumption**: Different models are complementary on fairness-critical data, leveraging disagreements to improve fairness for unprivileged groups.
- **Evidence anchors**:
  - [abstract] "optimizing fairness on one attribute will lead to the collapse of others"
  - [section II, Observation 2] "when we optimize age, the unfairness score of the site will increase"
  - [section II, Observation 3] "two models with similar accuracy on an unprivileged group may have a disagreement on specified data"
- **Break condition**: If models are not complementary (correlated errors), fusion strategy fails.

### Mechanism 2
- **Claim**: Fairness-aware proxy dataset reweights unprivileged group data to balance influence during training.
- **Mechanism**: Weights increase for data in groups under multiple unfair attributes, ensuring MLP training focuses on hardest fairness cases.
- **Core assumption**: Training only on unprivileged groups is sufficient because privileged group data has low disagreement rates among models.
- **Evidence anchors**:
  - [section III.B] "we will only involve unprivileged group data for training"
  - [section III.B] "models in privileged group data have a low rate of disagreement"
  - [section III.B, Algorithm 1] describes weight calculation for data appearing in multiple unprivileged groups
- **Break condition**: If privileged group data has high disagreement, excluding it will degrade overall accuracy.

### Mechanism 3
- **Claim**: Reinforcement learning efficiently explores model-fusing architecture space.
- **Mechanism**: RNN controller generates MLP hyperparameters; Monte Carlo policy gradient updates controller based on multi-fairness reward.
- **Core assumption**: Reward signal (accuracy/unfairness) is good proxy for true Pareto front of fairness vs. accuracy.
- **Evidence anchors**:
  - [section III.D] "We apply the Monte Carlo policy gradient algorithm"
  - [section III.C] "Reward = sum over attributes of (accuracy / unfairness score)"
  - [section IV.A] describes training settings and episode count for RL
- **Break condition**: If reward landscape is too noisy or flat, RL will not converge to good architecture.

## Foundational Learning

- **Concept**: Multi-dimensional fairness and correlation between unfair attributes
  - Why needed here: Understanding that optimizing one attribute can harm others is essential to motivate joint optimization approach.
  - Quick check question: If you improve fairness on attribute A, what happens to fairness on attribute B in a correlated dataset?

- **Concept**: Model fusion and disagreement-based correction
  - Why needed here: Core idea is to use multiple models whose disagreements can be resolved by MLP head to improve fairness for unprivileged groups.
  - Quick check question: If two models disagree on a sample from an unprivileged group, which model should MLP trust and why?

- **Concept**: Reinforcement learning for neural architecture search
  - Why needed here: Framework uses RL to automatically search for best MLP architecture and model selection without manual tuning.
  - Quick check question: How does Monte Carlo policy gradient update RNN controller in this setup?

## Architecture Onboarding

- **Component map**: Model pool -> RNN controller -> model-fusing structure -> fairness proxy dataset -> MLP head -> multi-fairness reward -> RNN controller update
- **Critical path**: RNN controller generates MLP architecture and selects models → model-fusing structure combines outputs → fairness proxy dataset training optimizes MLP parameters → multi-fairness reward guides RL updates
- **Design tradeoffs**: Model pool size vs. search efficiency; training only on unprivileged data vs. overall accuracy; reward formulation complexity vs. optimization stability
- **Failure signatures**:
  - Accuracy drops significantly on privileged groups
  - Unfairness scores plateau or worsen despite training
  - RL reward shows no improvement over episodes
- **First 3 experiments**:
  1. Train Muffin with small model pool (2-3 models) on simple binary attribute fairness task to verify basic functionality.
  2. Compare unfairness scores before and after training on held-out unprivileged group to check if proxy dataset weighting works.
  3. Vary number of MLP layers and monitor reward to see if deeper networks help or overfit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Muffin framework be effectively applied to datasets with more than two unfair attributes?
- Basis in paper: [explicit] Paper focuses on datasets with two unfair attributes and validates on Fitzpatrick17K, but doesn't explore datasets with more than two attributes.
- Why unresolved: No experimental results or theoretical analysis for datasets with more than two unfair attributes.
- What evidence would resolve it: Experimental results on datasets with three or more unfair attributes, along with discussion of challenges or modifications needed.

### Open Question 2
- Question: How does performance of Muffin compare to other fairness optimization techniques on large-scale datasets?
- Basis in paper: [inferred] Evaluates Muffin on two relatively small dermatology datasets, doesn't address performance on large-scale datasets.
- Why unresolved: Scalability to large-scale datasets not explored, computational efficiency and effectiveness compared to other techniques unclear.
- What evidence would resolve it: Comparative experiments on large-scale datasets measuring performance, computational efficiency, and fairness improvements against state-of-the-art techniques.

### Open Question 3
- Question: What are potential limitations of using reinforcement learning-based controller in Muffin framework?
- Basis in paper: [explicit] Employs RL-based controller but doesn't discuss potential limitations or challenges.
- Why unresolved: Doesn't address issues such as convergence, stability, or impact of hyperparameters on controller performance.
- What evidence would resolve it: Detailed analysis of RL controller performance including sensitivity to hyperparameters, convergence behavior, and robustness to different datasets and model configurations.

## Limitations
- The framework's effectiveness depends on model complementarity, which is assumed rather than empirically validated across different model combinations.
- Training only on unprivileged groups may not generalize well to domains where privileged group data has significant disagreement rates.
- RL reward formulation may lead to degenerate solutions if unfairness approaches zero or accuracy drops significantly.

## Confidence
- **High Confidence**: The observation that optimizing one attribute harms others is well-supported by experimental evidence in Section II.
- **Medium Confidence**: The mechanism of using model disagreements via an MLP is plausible but depends on model complementarity, which is assumed rather than proven.
- **Medium Confidence**: The fairness proxy dataset approach is theoretically sound but exclusion of privileged data could be risky in other applications.

## Next Checks
1. **Model Complementarity Test**: Measure correlation of errors between model pairs on unprivileged groups. If correlations are high, fusion approach may not work as intended.
2. **Privileged Data Impact**: Run ablation study including privileged group data in training to quantify accuracy-fairness tradeoff and validate assumption about low disagreement rates.
3. **Reward Landscape Analysis**: Visualize reward function across different MLP architectures to check for smoothness and identify if RL could get stuck in local optima.