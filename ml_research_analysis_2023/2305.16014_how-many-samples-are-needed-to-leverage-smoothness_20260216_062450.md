---
ver: rpa2
title: How many samples are needed to leverage smoothness?
arxiv_id: '2305.16014'
source_url: https://arxiv.org/abs/2305.16014
tags:
- when
- samples
- learning
- function
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of smoothness in breaking the
  curse of dimensionality for supervised learning. The authors show that while smoothness
  allows faster convergence rates in theory, in practice these rates are only meaningful
  when enough samples are available to estimate high-order derivatives.
---

# How many samples are needed to leverage smoothness?

## Quick Facts
- arXiv ID: 2305.16014
- Source URL: https://arxiv.org/abs/2305.16014
- Reference count: 40
- Key outcome: Smoothness alone cannot guarantee efficient learning in high dimensions; exponential sample complexity arises from effective dimension growth.

## Executive Summary
This paper investigates the role of smoothness in breaking the curse of dimensionality for supervised learning. The authors show that while smoothness allows faster convergence rates in theory, these rates are only meaningful when enough samples are available to estimate high-order derivatives. They derive new lower bounds on the generalization error that account for the effective dimension of the function class being learned. Experiments on toy problems illustrate how convergence can exhibit different profiles (fast then slow, or slow then fast) depending on the smoothness of the target function and the number of available samples. The results suggest that smoothness alone is not sufficient to guarantee efficient learning in high dimensions, and other priors like sparsity may be needed.

## Method Summary
The paper uses kernel ridge regression within an RKHS framework to analyze how smoothness affects learning in high dimensions. It derives upper and lower bounds on generalization error using Fourier analysis for translation-invariant kernels and effective dimension calculations. The method involves computing the effective dimension N(λ) and bias term λ²S(λ) to understand the capacity of function classes and relate it to sample complexity. Experiments validate theoretical predictions by varying sample sizes, input dimensions, and kernel parameters across different target functions with varying smoothness properties.

## Key Results
- Smoothness alone cannot guarantee efficient learning in high dimensions due to exponential growth of effective dimension
- Transitory regimes occur when sample size is insufficient to leverage high-order smoothness
- Regularization parameter λ acts as a change of kernel, controlling which functions can be estimated given limited samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smoothness alone cannot guarantee efficient learning in high dimensions because the effective dimension grows exponentially with input dimension.
- Mechanism: When leveraging smoothness through Taylor or Fourier expansions, the number of terms (polynomials or frequencies) grows as (d+α choose α) or exponentially with d, requiring exponentially more samples to estimate high-order derivatives or coefficients.
- Core assumption: Target function belongs to a smooth function class (C^α or H^α) but no additional structure (like sparsity) is assumed.
- Evidence anchors:
  - [abstract] "smoothness alone is not sufficient to guarantee efficient learning in high dimensions"
  - [section 3] "rates in O(n−2α/(2α+d)) might take time to appear in practice" and "constants inside the big Os might be exponential in the dimension"
  - [corpus] weak correlation (avg FMR 0.50), but related works on sample complexity suggest similar exponential growth phenomena
- Break condition: If additional priors like sparsity or low-dimensional structure are available, the effective dimension could be much smaller, avoiding exponential sample growth.

### Mechanism 2
- Claim: Regularization parameter λ acts as a change of kernel, effectively controlling which functions can be estimated given limited samples.
- Mechanism: By incorporating λ into the kernel (λ⁻¹k), ridge regression penalizes high-frequency components, allowing learning with fewer samples but potentially sacrificing approximation quality for very smooth functions.
- Core assumption: The regularization parameter λ can be tuned to balance estimation error (variance) and approximation error (bias).
- Evidence anchors:
  - [section 2.1] "Regularization is a change of kernel" and "a change of λ leads to an isotropic rescaling of the ball inside L²"
  - [section 4] discussion of λ controlling effective dimension N(λ) and bias term λ²S(λ)
  - [corpus] no direct evidence, but related works on kernel methods support this interpretation
- Break condition: If λ is set too large, the bias term dominates and learning becomes too conservative; if too small, variance explodes and sample complexity becomes prohibitive.

### Mechanism 3
- Claim: Transitory regimes occur when the number of samples is insufficient to leverage high-order smoothness, leading to convergence rates that differ from asymptotic predictions.
- Mechanism: When n < N(λ), the effective dimension exceeds the sample size, causing the estimator to behave differently than predicted by classical convergence theorems. The transition occurs around n ≈ N(λ).
- Core assumption: The relationship N(λ) ∝ λ^(-d/2β) or similar holds, creating a sharp transition when n crosses N(λ).
- Evidence anchors:
  - [abstract] "transitory regimes where one does not have enough samples to leverage high-order smoothness"
  - [section 4] "rates derived through Theorem 2 are only meaningful when one has enough samples" and "transitory regimes that might appear on those plots"
  - [corpus] weak evidence, but double descent phenomena in related works suggest similar transitions
- Break condition: If sample size grows much faster than N(λ), or if additional structure reduces N(λ) significantly, the transitory regime disappears.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The paper uses RKHS framework to analyze kernel ridge regression and understand how smoothness translates to generalization bounds
  - Quick check question: What is the relationship between a kernel k(x,x') and its associated RKHS norm ||f||F?

- Concept: Fourier analysis on tori
  - Why needed here: The paper uses Fourier decomposition to analyze translation-invariant kernels and understand how frequency content relates to sample complexity
  - Quick check question: How does the Fourier transform diagonalize the integral operator associated with a translation-invariant kernel?

- Concept: Effective dimension and interpolation inequalities
  - Why needed here: The paper uses effective dimension N(λ) to quantify the capacity of function classes and relate it to sample complexity
  - Quick check question: How does the effective dimension N(λ) relate to the eigenvalues of the integral operator K?

## Architecture Onboarding

- Component map:
  Theoretical analysis component -> Fourier analysis component -> Empirical validation component -> Hyperparameter tuning component

- Critical path:
  1. Define kernel and regularization parameters
  2. Compute effective dimension N(λ) and bias term λ²S(λ)
  3. Apply Theorem 2 to get upper/lower bounds on generalization error
  4. Optimize λ to minimize the bound
  5. Validate theoretical predictions with experiments

- Design tradeoffs:
  - Smoothness vs sample complexity: Higher smoothness requires exponentially more samples
  - Regularization strength: Larger λ reduces variance but increases bias
  - Kernel choice: Different kernels have different effective dimensions and bias characteristics

- Failure signatures:
  - Convergence rates plateau at O(n⁻¹) instead of faster rates despite smoothness
  - Transitory regime persists even with large sample sizes
  - Empirical results deviate significantly from theoretical bounds

- First 3 experiments:
  1. Reproduce Figure 2: Target function f*(x) = x⁵₁ with polynomial kernel, vary dimension d and samples n, verify exponential growth of effective dimension
  2. Reproduce Figure 7: Target function f*(x) = cos(4πx₁) with exponential kernel on torus, vary dimension d and samples n, observe deterioration of convergence rates
  3. Reproduce Figure 3: Target function f*(x) = exp(-max(x²,M)) - exp(-M) with Gaussian kernel, vary samples n and regularization λ, observe fast-then-slow convergence profile

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design prior assumptions that avoid the curse of dimensionality in high-dimensional learning?
- Basis in paper: [inferred] The paper shows that smoothness alone is insufficient to break the curse of dimensionality, and suggests that other priors like sparsity or multi-scale behaviors might be more effective.
- Why unresolved: The paper mentions potential directions but does not provide concrete prior assumptions or their mathematical formulations.
- What evidence would resolve it: Development of specific prior models with theoretical guarantees on convergence rates in high dimensions, along with empirical validation showing improved performance compared to smoothness-based methods.

### Open Question 2
- Question: What are the exact conditions under which transitory regimes dominate the learning behavior in practice?
- Basis in paper: [explicit] The paper discusses how constants and transitory regimes are usually not depicted in classical learning theory, yet they play a dominant role in practice.
- Why unresolved: The paper provides theoretical bounds and examples but does not give precise conditions to predict when transitory regimes will be significant.
- What evidence would resolve it: A theoretical framework that characterizes the sample size, dimension, and function properties that determine the length and impact of transitory regimes.

### Open Question 3
- Question: How does the choice of loss function (e.g., cross-entropy vs. least squares) affect the effective dimension and convergence rates?
- Basis in paper: [explicit] The paper mentions that practitioners tend to favor other losses such as cross-entropy, and asks how losses deform the size of the search space and its adherence properties.
- Why unresolved: The paper focuses on least squares regression and does not analyze the impact of alternative loss functions on the learning behavior.
- What evidence would resolve it: A comparative analysis of convergence rates and effective dimensions for different loss functions under various smoothness and sparsity assumptions.

### Open Question 4
- Question: What is the relationship between deep learning phenomena and the high-dimensional behavior of kernel methods?
- Basis in paper: [inferred] The paper suggests that deep learning models might exploit priors like sparsity or multi-scale behaviors, and mentions that linking theoretical results with interpretable observations in neural networks remains challenging.
- Why unresolved: The paper does not provide a detailed comparison or theoretical connection between deep learning and kernel methods in high dimensions.
- What evidence would resolve it: A theoretical framework that connects the inductive biases of deep neural networks to the effective dimension and convergence rates in high-dimensional settings, along with empirical validation.

## Limitations

- Theoretical framework relies heavily on RKHS assumptions and specific function class characterizations
- Analysis focuses on isotropic settings, potentially missing scenarios where smoothness varies across different input directions
- Exponential growth of effective dimension with input dimension may be overly pessimistic in practice

## Confidence

- **High Confidence**: The mechanism linking smoothness to sample complexity through effective dimension growth is well-established theoretically
- **Medium Confidence**: The transitory regime analysis and the characterization of when smoothness becomes beneficial require more empirical validation
- **Low Confidence**: The practical implications of the results for real-world applications where additional structure may be present alongside smoothness

## Next Checks

1. **Empirical Validation of Effective Dimension Scaling**: Systematically measure the effective dimension N(λ) for different kernels and input dimensions, comparing empirical measurements against theoretical predictions to quantify the tightness of bounds.

2. **Cross-Validation of Regularization Effects**: Implement a comprehensive study varying λ across multiple orders of magnitude for different target functions and kernels, empirically measuring the bias-variance tradeoff and identifying optimal regularization regimes.

3. **Extension to Structured Function Classes**: Test whether incorporating additional priors (e.g., sparsity, low-dimensional structure) alongside smoothness can mitigate the exponential sample complexity, comparing convergence rates with and without these additional assumptions.