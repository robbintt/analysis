---
ver: rpa2
title: 'Asset Ownership Identification: Using machine learning to predict enterprise
  asset ownership'
arxiv_id: '2312.10266'
source_url: https://arxiv.org/abs/2312.10266
tags:
- asset
- cation
- data
- owner
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research used machine learning to predict enterprise asset\
  \ ownership, a key challenge in information security. It sampled data from a configuration\
  \ management database and trained five models\u2014AdaBoost, Logistic Regression,\
  \ Naive Bayes, CART, and Random Forests\u2014using 100 iterations of Monte Carlo\
  \ cross-validation for each asset owner."
---

# Asset Ownership Identification: Using machine learning to predict enterprise asset ownership

## Quick Facts
- arXiv ID: 2312.10266
- Source URL: https://arxiv.org/abs/2312.10266
- Reference count: 20
- Key outcome: AdaBoost achieved testing error rates below 5% (accuracy ≥95%) for enterprise asset ownership prediction using configuration management database features

## Executive Summary
This research addresses the critical challenge of predicting enterprise asset ownership in information security using machine learning. The study analyzed data from a configuration management database containing approximately 70,000 assets across 19 columns, training five different models—AdaBoost, Logistic Regression, Naive Bayes, CART, and Random Forests—using Monte Carlo cross-validation. The AdaBoost algorithm demonstrated superior performance with testing errors below 5%, while feature engineering focused on fully qualified domain names, CIDR/16 subnets, and location data proved most predictive. A visualization dashboard was created to support exploratory data analysis and model evaluation, enabling more accurate asset owner identification and validation of existing ownership assignments.

## Method Summary
The study employed Monte Carlo cross-validation with 100 iterations (80% train, 10% validation, 10% test split) for each asset owner in a configuration management database. Five machine learning models were trained and evaluated: AdaBoost, Logistic Regression, Naive Bayes, CART, and Random Forests. Feature engineering included parsing tags, extracting top-level FQDN domains, deriving CIDR/8, /16, /24 features from IP addresses, and extracting OUI from MAC addresses. The analysis focused on identifying the most important features for ownership prediction and creating a visualization dashboard for exploratory analysis and model evaluation.

## Key Results
- AdaBoost achieved the best performance with testing error rates below 5% (accuracy ≥95%)
- The most important features were fully qualified domain name, CIDR/16, and location
- Naive Bayes performed worst among the five tested models
- A visualization dashboard was created to support exploratory data analysis and model evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative reweighting of misclassified points in AdaBoost focuses the model on hard-to-classify assets, leading to lower test error rates.
- Mechanism: AdaBoost starts with weak learners and iteratively reweights training instances, giving higher importance to those previously misclassified. This concentrates model capacity on challenging cases, improving classification accuracy.
- Core assumption: The asset ownership problem contains a non-trivial portion of difficult-to-classify cases where simple models fail.
- Evidence anchors:
  - [abstract] "Adaboost performed the best across all owners with low testing errors below 5%"
  - [section] "The iterative reweighting tree-based approach that focuses on misclassified data clearly performs the best"
  - [corpus] Weak - no direct corpus support for AdaBoost's iterative reweighting in this specific domain

### Mechanism 2
- Claim: Feature engineering of FQDN, CIDR/16, and Location captures hierarchical asset organization patterns that correlate strongly with ownership.
- Mechanism: By extracting domain-level information from FQDN and subnet-level information from IP addresses, the model can leverage organizational naming conventions and network topology to infer ownership patterns.
- Core assumption: Asset naming conventions and network topology reflect organizational structure and ownership boundaries.
- Evidence anchors:
  - [section] "The fully qualified domain name (FQDN), Classless Inter-Domain Routing (CIDR) CIDR/16, and location were among the most important features"
  - [section] "The FQDN was split to only include the most top-level information... the IP address derived additional Classless Inter-Domain Routing (CIDR) features"
  - [corpus] Weak - corpus focuses on model ownership verification rather than asset ownership prediction

### Mechanism 3
- Claim: Monte Carlo Cross Validation with 100 iterations reduces sampling bias and provides robust performance estimates across all asset owners.
- Mechanism: Repeated random sampling with different train/test splits captures variability in the data and prevents overfitting to specific data partitions, ensuring reliable performance metrics.
- Core assumption: The asset ownership data has sufficient variability that different random splits would yield meaningfully different results.
- Evidence anchors:
  - [section] "the analysis ran every owner response through 100 iterations of a Monte Carlo Cross Validation approach"
  - [section] "To test for robustness and minimize sampling bias, the analysis ran every owner response through 100 iterations"
  - [corpus] Weak - corpus neighbors focus on different domains (AI copyright, SQL rewriting) with no cross-validation discussion

## Foundational Learning

- Concept: Binary classification with one-vs-all strategy
  - Why needed here: The problem involves predicting which of many teams owns an asset, requiring separate binary classifiers for each team
  - Quick check question: Why run separate analyses for each enumerated team instead of a multi-class classifier?

- Concept: Hyperparameter tuning via cross-validation
  - Why needed here: Different models (AdaBoost, Random Forests, etc.) have hyperparameters that significantly impact performance on this specific dataset
  - Quick check question: What would happen if we used default hyperparameters instead of tuning them on the cross-validation set?

- Concept: Feature importance analysis
  - Why needed here: Understanding which features drive predictions helps validate model decisions and provides insights into asset ownership patterns
  - Quick check question: How can we use feature importance to improve the visualization dashboard's exploratory analysis capabilities?

## Architecture Onboarding

- Component map: Data preprocessing → Feature engineering → Model training (5 algorithms × 100 iterations) → Cross-validation → Test evaluation → Dashboard generation
- Critical path: Feature engineering → Model selection → Dashboard creation (models are only useful if stakeholders can interpret results)
- Design tradeoffs: 100 Monte Carlo iterations provide robustness but increase computational cost; separate binary classifiers for each owner allow specialization but require more training time
- Failure signatures: High variance in cross-validation scores across iterations suggests data quality issues; poor performance on specific owners indicates class imbalance or feature inadequacy
- First 3 experiments:
  1. Run single Monte Carlo iteration with all five models to establish baseline performance and identify computational bottlenecks
  2. Test feature importance ranking on a subset of owners to validate that FQDN, CIDR/16, and Location remain consistently important
  3. Implement the visualization dashboard with one owner to verify interactive filtering and performance metric display work correctly before scaling to all owners

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance change when applied to asset inventories from organizations with different naming conventions, sizes, or industries?
- Basis in paper: [inferred] The paper notes that asset ownership is complicated by varying naming conventions, acquisitions, and large IP subnets, suggesting that performance may vary across different organizational contexts.
- Why unresolved: The research was conducted on a single multinational datacenter company's dataset, so generalizability to other contexts is untested.
- What evidence would resolve it: Testing the same models on asset inventories from diverse organizations (e.g., financial, healthcare, retail) with varying naming standards and sizes.

### Open Question 2
- Question: What is the impact of feature engineering approaches, such as extracting new patterns from FQDN or MAC address OUI, on model accuracy?
- Basis in paper: [explicit] The paper mentions that "future research could revolve around additional feature engineering" and identifies FQDN, CIDR/16, and location as the most important features.
- Why unresolved: The study used a limited set of engineered features and did not explore more advanced feature extraction or representation learning techniques.
- What evidence would resolve it: Systematic comparison of model performance using basic vs. advanced feature engineering methods (e.g., embeddings, graph-based features).

### Open Question 3
- Question: How does model performance degrade when asset inventory data contains significant missing values or inconsistencies?
- Basis in paper: [inferred] The paper discusses challenges like incomplete DAM/CMDB coverage and legacy naming conventions, implying data quality issues.
- Why unresolved: The analysis does not report results under simulated or real-world data degradation scenarios.
- What evidence would resolve it: Experimental evaluation of model robustness under varying levels of missing data and data inconsistency.

## Limitations
- Data quality and representation issues limit generalizability claims, as the CMDB dataset characteristics are not fully characterized
- Hyperparameter specificity remains unclear, with exact optimal values unspecified for each model and owner combination
- External validity concerns exist, as performance on this specific enterprise dataset may not transfer to organizations with different asset naming conventions or network architectures

## Confidence
- High confidence: AdaBoost's superior performance (testing error <5%) is well-supported by the 100-iteration Monte Carlo cross-validation results
- Medium confidence: Feature importance rankings (FQDN, CIDR/16, Location) are valid for this dataset but may not generalize to different organizational contexts
- Low confidence: Claims about mechanism superiority lack sufficient empirical validation beyond this single dataset

## Next Checks
1. Replication on external dataset: Test the complete pipeline on a different enterprise's CMDB data to assess generalizability of the 95%+ accuracy claims
2. Temporal validation: Evaluate model performance when trained on historical data and tested on current assets to assess adaptation to evolving naming conventions
3. Ablation study on features: Systematically remove each top feature (FQDN, CIDR/16, Location) to quantify their individual contribution to AdaBoost's performance advantage