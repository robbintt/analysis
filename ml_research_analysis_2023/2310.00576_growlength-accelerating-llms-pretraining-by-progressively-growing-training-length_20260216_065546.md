---
ver: rpa2
title: 'GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training
  Length'
arxiv_id: '2310.00576'
source_url: https://arxiv.org/abs/2310.00576
tags:
- training
- llms
- length
- arxiv
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GrowLength, a method to accelerate LLM pretraining
  by progressively growing training sequence length. The key idea is to start with
  shorter sequences and gradually increase their length during training, which allows
  the model to process more tokens in less time.
---

# GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length

## Quick Facts
- arXiv ID: 2310.00576
- Source URL: https://arxiv.org/abs/2310.00576
- Reference count: 7
- Key outcome: Progressive length training achieves lower loss than fixed-length training while reducing training time

## Executive Summary
GrowLength is a novel method for accelerating LLM pretraining by progressively increasing training sequence length during training. Starting with shorter sequences and gradually extending them allows the model to process more tokens in less time while maintaining or improving performance. The method leverages Rotary Position Embedding's (RoPE) extrapolation capability to enable models trained on shorter sequences to effectively handle longer sequences later in training.

## Method Summary
The GrowLength method progressively increases sequence length during pretraining, starting from shorter sequences (e.g., 128 tokens) and gradually extending to longer sequences (up to 4096 tokens). This approach reduces computational costs in early training phases while leveraging RoPE's extrapolation properties to maintain performance on longer sequences. The method is implemented through a data loader pipeline that provides batches of varying lengths according to a predefined schedule.

## Key Results
- Achieves lower training loss compared to models trained with fixed sequence lengths
- Reaches the same loss as 1024-token models in less training time
- Improves context window extension abilities without compromising performance
- Works across different model sizes and is orthogonal to other acceleration techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Growing training length progressively reduces pretraining time while maintaining or improving loss
- Mechanism: Shorter sequences in early training are computationally cheaper, and RoPE's extrapolation ability allows generalization to longer contexts learned later
- Core assumption: RoPE positional embeddings support length extrapolation so models trained on shorter sequences can handle longer sequences without loss jumps
- Evidence anchors: [abstract] "Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency." [section 2.1] "These studies demonstrate that the RoPE possesses the capability to adapt to longer sequences when trained with shorter ones."

### Mechanism 2
- Claim: Models trained with shorter sequences can predict longer sequences effectively
- Mechanism: The model learns general sequence processing patterns on short inputs that transfer to longer inputs via positional embedding extrapolation
- Core assumption: Sequence processing skills learned on short sequences generalize to longer sequences without catastrophic forgetting
- Evidence anchors: [section 2.3] "Models trained with shorter sequence lengths has proven to be more effective than training with long sequences, as proven by the Content Window Extension."

### Mechanism 3
- Claim: Using shorter sequences early in training allows more tokens to be processed in the same wall-clock time
- Mechanism: Shorter sequences reduce per-batch compute time and memory usage, enabling larger batch sizes and thus more total tokens seen per unit time
- Core assumption: The time saved by shorter sequences in early stages outweighs any potential slowdown from later length increases
- Evidence anchors: [section 3.2] "training with shorter sentences consumes significantly less time than training with longer sequences." [section 3.2] "Table 3 shows that the total number of tokens accommodated decreases with the increase in sequence length when utilizing the full capacity of the GPU's available memory."

## Foundational Learning

- Concept: Rotary Position Embedding (RoPE) and its extrapolation properties
  - Why needed here: RoPE allows models trained on short sequences to generalize to longer sequences without retraining, enabling progressive length growth
  - Quick check question: Does RoPE extrapolate to longer sequence lengths, or does it saturate/break beyond training lengths?

- Concept: Computational complexity scaling with sequence length in transformers
  - Why needed here: Understanding quadratic attention scaling is essential to see why shorter sequences save time and memory
  - Quick check question: How does memory usage and runtime change when doubling sequence length in a transformer?

- Concept: Progressive training schedules and curriculum learning
  - Why needed here: GrowLength is a form of curriculum learning where task difficulty (sequence length) increases over time
  - Quick check question: What are the risks of abrupt changes in training schedule, and how can they be mitigated?

## Architecture Onboarding

- Component map: Data loader pipeline -> Model with RoPE -> Training loop -> Scheduler
- Critical path:
  1. Load batch of current length
  2. Forward pass with RoPE
  3. Compute loss
  4. Backward pass
  5. Update weights
  6. Check if length switch condition met
  7. Switch to next length loader if needed
- Design tradeoffs:
  - Fixed vs. adaptive length increase schedules
  - Granularity of length steps (e.g., 128→256 vs. 128→512)
  - Balancing early speed gains vs. later length coverage
- Failure signatures:
  - Loss spikes at length transitions
  - No improvement over fixed-length baselines
  - Memory overflow at longer lengths despite early gains
- First 3 experiments:
  1. Run fixed 128-length baseline to establish runtime and loss
  2. Run GrowLength from 128→1024 and compare total time and final loss
  3. Test loss stability at each length transition to detect extrapolation failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GrowLength's performance compare to other LLM acceleration methods when integrated together?
- Basis in paper: [explicit] The paper states that GrowLength is orthogonal to other LLM acceleration techniques and can be integrated with them without causing redundancy
- Why unresolved: The paper only mentions the potential for integration but does not provide experimental results comparing the combined performance of GrowLength with other acceleration methods
- What evidence would resolve it: Experiments showing the combined performance of GrowLength with other acceleration methods like FlashAttention, quantization, or pruning would resolve this question

### Open Question 2
- Question: What is the optimal ratio of different sequence lengths during GrowLength training for maximizing efficiency and model performance?
- Basis in paper: [explicit] The paper discusses the influence of ratios of different window sizes during training and mentions that GrowLength is not sensitive to these ratios, but does not provide a definitive optimal ratio
- Why unresolved: While the paper shows that GrowLength is robust to different ratios, it does not determine the optimal ratio that maximizes both efficiency and model performance
- What evidence would resolve it: Systematic experiments varying the ratio of different sequence lengths during GrowLength training and measuring both efficiency gains and model performance would resolve this question

### Open Question 3
- Question: How does GrowLength affect the scaling properties of LLMs with respect to model size and dataset size?
- Basis in paper: [explicit] The paper mentions that GrowLength does not influence the scaling property of LLMs, but this is based on limited experiments with 70M, 160M, and 410M parameter models
- Why unresolved: The paper's experiments are limited to a small range of model sizes and do not explore the effects of GrowLength on dataset size scaling
- What evidence would resolve it: Extensive experiments with a wider range of model sizes (e.g., 1B, 10B, 100B parameters) and varying dataset sizes would resolve this question

## Limitations

- RoPE extrapolation validity is largely theoretical with limited direct empirical validation
- Transferability across model architectures is unproven beyond decoder-only transformers
- Optimal length progression schedule is not systematically analyzed
- Computational resource claims lack detailed ablation studies

## Confidence

- High Confidence: Shorter sequences reduce per-batch compute time (well-established, timing experiments support)
- Medium Confidence: RoPE enables length extrapolation (theoretically sound, limited direct validation)
- Low Confidence: Claims about orthogonality to other techniques (lacks empirical support)

## Next Checks

1. **RoPE Extrapolation Stress Test**: Systematically test the limits of RoPE extrapolation by training models on sequences of length 128 and evaluating performance on sequences up to 8192 tokens. Measure loss degradation patterns and identify the breaking point where extrapolation fails.

2. **Schedule Sensitivity Analysis**: Implement multiple progression schedules (linear, exponential, adaptive) with varying step sizes and transition points. Compare final loss and training time across schedules to identify optimal configurations and sensitivity to hyperparameters.

3. **Cross-Architecture Validation**: Apply GrowLength to encoder-decoder transformer architectures and compare performance with decoder-only models. Measure whether the progressive length training benefits transfer across different model families or are specific to decoder-only designs.