---
ver: rpa2
title: 'MaRU: A Manga Retrieval and Understanding System Connecting Vision and Language'
arxiv_id: '2311.02083'
source_url: https://arxiv.org/abs/2311.02083
tags:
- text
- retrieval
- manga
- query
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MaRU, a multi-staged system for retrieving
  dialogues and scenes within Manga frames. MaRU integrates object detection for identifying
  text and frame bounding boxes, a Vision Encoder-Decoder model for text recognition,
  a text encoder for embedding text, and a vision-text encoder that merges textual
  and visual information into a unified embedding space for scene retrieval.
---

# MaRU: A Manga Retrieval and Understanding System Connecting Vision and Language

## Quick Facts
- arXiv ID: 2311.02083
- Source URL: https://arxiv.org/abs/2311.02083
- Reference count: 32
- Key outcome: MaRU achieves MRR of 0.980 for dialogue retrieval and MRR of 0.435 for scene retrieval on Manga109 dataset

## Executive Summary
This paper presents MaRU, a multi-staged system for retrieving dialogues and scenes within Manga frames. The system integrates object detection, text recognition, and vision-language embeddings to enable both dialogue and scene retrieval from manga images. MaRU demonstrates strong performance on the Manga109 dataset, achieving high accuracy for dialogue retrieval and significantly improving upon baseline methods for scene retrieval. The system processes manga by first detecting frames and text boxes, then recognizing text content, and finally using vision-language models to embed and retrieve relevant scenes based on user queries.

## Method Summary
MaRU is a multi-staged system that processes manga images through several components: (1) DETR object detector identifies frame and text bounding boxes, (2) MangaOCR Vision Encoder-Decoder recognizes text within detected boxes, (3) SentenceTransformer embeds dialogue text for semantic retrieval, and (4) CLIP/ViT with DistilBERT creates unified scene embeddings for cross-modal retrieval. The system indexes both dialogue and scene embeddings, then uses cosine similarity to retrieve relevant manga pages based on user queries. For scene retrieval, the system encodes each frame separately rather than whole pages to preserve scene-level granularity.

## Key Results
- Dialogue retrieval achieves MRR of 0.980 and Average Success rate of 0.980 on paraphrased queries
- Scene retrieval significantly improves upon baseline with MRR of 0.435 and Average Success rate of 0.820 at k=25
- Per-frame embedding approach outperforms whole-page embeddings for scene retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage architecture decomposes Manga complexity into tractable subproblems
- Mechanism: Separate detection, recognition, and retrieval stages reduce noise and enable domain-specific optimization at each step
- Core assumption: Each stage's output is sufficiently clean to serve as reliable input for the next
- Evidence anchors:
  - [abstract] "multi-staged system that connects vision and language"
  - [section 3.2] Vision Encoder-Decoder model for text recognition, text encoder for embeddings
  - [corpus] Weak evidence; no direct corpus citation
- Break condition: Error accumulation across stages makes later retrieval unreliable

### Mechanism 2
- Claim: Per-frame embeddings preserve scene-level semantic granularity
- Mechanism: DETR detection creates frame bounding boxes, then CLIP/ViT processes each frame separately instead of whole-page embedding
- Core assumption: Manga scenes are meaningfully contained within individual frames
- Evidence anchors:
  - [section 3.4] "training an object detection model to locate frame bounding boxes...encoding per-frame scene embeddings instead of per-page embeddings"
  - [section 5.3] Quantitative improvement when using predicted vs whole-page embeddings
  - [corpus] Weak evidence; no direct corpus citation
- Break condition: Important cross-frame relationships are lost

### Mechanism 3
- Claim: SentenceTransformer dense embeddings enable semantic dialog retrieval beyond exact match
- Mechanism: Query and index sentences are embedded independently; cosine similarity captures semantic equivalence
- Core assumption: Paraphrased queries maintain sufficient semantic similarity to ground truth
- Evidence anchors:
  - [section 3.3] "SentenceBERT...has been fine-tuned to provide semantically meaningful embeddings"
  - [section 5.2] MRR 0.980 on paraphrased queries
  - [corpus] Weak evidence; no direct corpus citation
- Break condition: Paraphrase semantic drift exceeds embedding model's similarity threshold

## Foundational Learning

- Concept: Object detection fundamentals (bounding boxes, mAP, IoU)
  - Why needed here: DETR must accurately locate frames and text boxes for downstream tasks
  - Quick check question: What's the difference between mAP@50 and mAP@75?

- Concept: Vision-language embeddings (CLIP architecture)
  - Why needed here: Scene retrieval requires cross-modal similarity matching
  - Quick check question: How does CLIP's shared embedding space enable nearest neighbor search?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: Dialog retrieval must match paraphrased user queries to original text
  - Quick check question: Why would cosine similarity work for semantic matching but not exact string matching?

## Architecture Onboarding

- Component map: Input Manga book → DETR detection → MangaOCR recognition → SentenceTransformer embedding → CLIP/ViT scene embedding → cosine similarity retrieval → ranked results
- Critical path: Query → SentenceTransformer CLIP embedding → nearest neighbor search → ranked results
- Design tradeoffs: Multi-stage adds complexity but enables domain-specific optimization vs. end-to-end black box
- Failure signatures: Low MRR in scene retrieval likely indicates CLIP's limited Manga training exposure
- First 3 experiments:
  1. Verify DETR frame detection mAP > 0.8 on test set
  2. Test SentenceTransformer retrieval with exact match queries (expect MRR ~1.0)
  3. Compare CLIP per-frame vs whole-page embeddings on sample scene queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CLIP model be fine-tuned with a Manga training corpus to improve scene retrieval accuracy?
- Basis in paper: [explicit] The authors suggest fine-tuning the CLIP model with a Manga training corpus to address the bottleneck in scene retrieval performance.
- Why unresolved: The paper identifies the issue but does not provide details on the fine-tuning process or evaluate its effectiveness.
- What evidence would resolve it: Implementing the fine-tuning process and evaluating the scene retrieval performance on a test set with the fine-tuned CLIP model.

### Open Question 2
- Question: What are the specific challenges in handling complex interactions or relationships between multiple characters in scene retrieval?
- Basis in paper: [inferred] The qualitative analysis mentions that queries involving complex interactions or relationships between multiple characters are more challenging for the system to retrieve accurately.
- Why unresolved: The paper does not provide a detailed analysis of the challenges or potential solutions for handling complex character interactions in scene retrieval.
- What evidence would resolve it: Conducting a thorough analysis of the system's performance on various types of complex character interactions and proposing improvements to address the identified challenges.

### Open Question 3
- Question: How can the system be improved to handle queries with ambiguous or less distinctive descriptions?
- Basis in paper: [inferred] The qualitative analysis indicates that queries with ambiguous or less distinctive descriptions may result in less accurate retrieval.
- Why unresolved: The paper does not provide specific strategies for improving the system's ability to handle ambiguous or less distinctive queries.
- What evidence would resolve it: Experimenting with different query preprocessing techniques, such as query expansion or semantic similarity-based filtering, to improve the system's performance on ambiguous or less distinctive queries.

## Limitations
- Evaluation limited to Manga109 dataset (109 volumes), potentially limiting generalization to diverse manga styles
- Object detection shows significant performance gap between frames (mAP 0.921) and small text boxes (mAP 0.643)
- Per-frame processing may miss important cross-frame narrative relationships in manga storytelling

## Confidence

**High confidence**: The multi-stage architecture approach is well-established in vision-language tasks, and the technical implementation follows standard practices. The quantitative metrics (MRR 0.980 for dialogue, MRR 0.435 for scenes) are internally consistent with the methodology described.

**Medium confidence**: Claims about semantic understanding beyond exact matching are supported by paraphrased query results, but the specific quality and diversity of these paraphrases is not detailed. The improvement from whole-page to per-frame embeddings is demonstrated, but the analysis doesn't explore potential information loss from this decomposition.

**Low confidence**: Claims about the system's applicability to "automated systems and improving accessibility for Manga content" are largely aspirational, with no empirical validation of real-world usability or accessibility impact.

## Next Checks

1. **Error Propagation Analysis**: Conduct ablation studies measuring how detection errors in frames and text boxes cascade through recognition and retrieval stages, particularly for small text elements where mAP is lowest.

2. **Cross-Collection Generalization**: Evaluate the system on manga from different publishers, styles, and languages beyond Manga109 to assess whether the CLIP-based embeddings generalize or are overfit to this specific dataset's visual style.

3. **Narrative Context Preservation**: Design experiments testing retrieval performance on scenes that span multiple frames, measuring whether the per-frame approach misses important cross-frame relationships that human readers would naturally connect.