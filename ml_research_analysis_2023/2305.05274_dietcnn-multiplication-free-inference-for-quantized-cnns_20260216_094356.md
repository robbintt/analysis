---
ver: rpa2
title: 'DietCNN: Multiplication-free Inference for Quantized CNNs'
arxiv_id: '2305.05274'
source_url: https://arxiv.org/abs/2305.05274
tags:
- dietcnn
- symbols
- layer
- inference
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DietCNN replaces multiplications in CNNs with table lookups to
  reduce energy and latency on embedded systems. It preserves standard CNN layer semantics
  by discretizing inputs, weights, and activations into finite symbol sets, precomputing
  results in lookup tables.
---

# DietCNN: Multiplication-free Inference for Quantized CNNs

## Quick Facts
- **arXiv ID**: 2305.05274
- **Source URL**: https://arxiv.org/abs/2305.05274
- **Reference count**: 40
- **Key outcome**: Replaces multiplications in CNNs with table lookups, achieving 4.7×, 5.6×, and 3.5× energy reduction on MNIST, CIFAR10, and Tiny ImageNet respectively while maintaining accuracy.

## Executive Summary
DietCNN introduces a novel approach to CNN inference that eliminates multiplications by replacing them with table lookups. The method discretizes inputs, weights, and activations into finite symbol sets, precomputes results in lookup tables, and uses these tables during inference. This approach significantly reduces energy consumption and latency on embedded systems, particularly FPGAs, while maintaining comparable accuracy to standard CNNs. The method shows particular advantages for smaller models commonly used in embedded applications.

## Method Summary
DietCNN replaces multiplication operations in CNNs with table lookups by discretizing inputs, weights, and activations into finite symbol sets. A single codebook is learned from all images and feature maps, which is then used throughout the network. The method precomputes multiplication, addition, and activation results into lookup tables, which are used during inference to perform convolution operations without any multiplications. This approach maintains standard CNN layer semantics while significantly reducing computational requirements.

## Key Results
- Achieves 4.7×, 5.6×, and 3.5× energy reduction on MNIST, CIFAR10, and Tiny ImageNet respectively compared to standard CNNs
- Maintains accuracy while reducing model size and resource usage
- Outperforms multiplication-free alternatives like AdderNet and ShiftAddNet, especially for smaller models
- Successfully implemented on Zynq 7000 FPGA series using Xilinx Vitis and XPE tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lookup tables replace multiplications while preserving CNN layer semantics.
- Mechanism: Input images, activations, and weights are discretized into symbols from finite sets; multiplication, addition, and activation results are precomputed and stored in LUTs; during inference, discrete transformers use LUT lookups instead of arithmetic.
- Core assumption: Precomputing full-precision floating-point results into LUTs avoids accuracy loss.
- Evidence anchors:
  - [abstract] "DietCNN replaces multiplications in CNNs with table lookups to reduce energy and latency on embedded systems. It preserves standard CNN layer semantics by discretizing inputs, weights, and activations into finite symbol sets, precomputing results in lookup tables."
  - [section 3.1.2] "The multiplication LUT creation process... calculates the actual multiplication between centroids, and uses η to convert it back to a symbol. This symbol is stored in the multiplication LUT."
- Break condition: If discretization introduces significant quantization error, accuracy degrades despite LUT use.

### Mechanism 2
- Claim: Symbolic representation enables multiplication-free convolution with reduced resource usage.
- Mechanism: Convolution patches are mapped to symbol sequences; symbol-wise multiplication and accumulation are performed via LUT lookups; final outputs remain in symbol domain until final layer.
- Core assumption: Symbol sequences capture essential feature relationships for accurate inference.
- Evidence anchors:
  - [section 3.2] "Mijn = fadd({νm(τ(x(i:i+k)(j:j+k)(m)), τ(f(1:k)(1:k)mn)) | 0 ≤ m ≤ c, 0 ≤ i ≤ h − k + 1, 0 ≤ j ≤ w − k + 1})"
  - [section 3.3] "For an activation function, we need one look-up per symbol, for each symbol of the input symbolic feature map."
- Break condition: If symbol vocabulary is too small, convolutional outputs lose discriminative power.

### Mechanism 3
- Claim: Single codebook for images and feature maps reduces memory overhead compared to multi-codebook quantization.
- Mechanism: One clustering model learns representative patches from all images and activations; symbols flow throughout the network; LUTs index by these symbols rather than storing separate quantizers per layer.
- Core assumption: A shared codebook captures enough diversity to maintain accuracy across layers.
- Evidence anchors:
  - [section 3.1.1] "The hyperparameter for clustering, number of cluster centroids... are based on the studies presented in [23]."
  - [section F] "We have got strong evidence for taking 512 as the upper bound... if 512 symbols are used."
- Break condition: If dataset diversity exceeds codebook capacity, approximation error accumulates.

## Foundational Learning

- **Concept**: Dictionary learning / vector quantization
  - Why needed here: Provides the finite symbol sets for discretizing continuous pixel and weight values.
  - Quick check question: What is the role of clustering centroids in the DietCNN codebook?

- **Concept**: Convolutional layer arithmetic
  - Why needed here: Understanding standard convolution enables designing equivalent symbolic transformers.
  - Quick check question: How many multiply-accumulate operations are required for a 3×3 convolution with 64 input channels and 128 output channels on a 32×32 feature map?

- **Concept**: FPGA resource primitives (BRAM, LUT, FF, DSP)
  - Why needed here: Determines the feasibility and efficiency of LUT-based vs arithmetic implementations.
  - Quick check question: Which FPGA primitive is most expensive for floating-point multiplication?

## Architecture Onboarding

- **Component map**: Input → Symbol encoder → LUT-based layer transformers (conv, fc, activation) → Symbol decoder → Output. LUTs for multiplication, addition, activation, bias addition.
- **Critical path**: Symbol encoding → convolutional LUT lookup chain → addition LUT accumulation → activation LUT → repeat for layers → final decoding.
- **Design tradeoffs**: Larger symbol vocabularies improve accuracy but increase LUT size; patch size affects memory vs discretization error; no pooling support requires stride adjustments.
- **Failure signatures**: Accuracy drop indicates insufficient symbol diversity or broken associativity in symbolic addition; high latency suggests LUT access bottlenecks.
- **First 3 experiments**:
  1. Implement symbol encoder/decoder on a small CNN (e.g., MNIST LeNet) and verify reconstruction error.
  2. Generate LUTs for multiplication and addition, test symbol-wise convolution on synthetic data.
  3. Compare accuracy of standard vs DietCNN inference on MNIST with 128 symbols.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DietCNN scale with increasing dataset size beyond Tiny ImageNet?
- Basis in paper: [explicit] The authors mention that DietCNN's accuracy drop is negligible when tested on larger datasets like ImageNet, but they have not yet attempted to implement DietCNN on such larger datasets.
- Why unresolved: The paper only provides experimental results for MNIST, CIFAR-10, and Tiny ImageNet datasets, leaving the scalability to larger datasets unexplored.
- What evidence would resolve it: Experimental results showing DietCNN's performance on larger datasets like ImageNet or COCO, comparing accuracy, latency, and energy consumption with standard CNNs and other multiplication-free methods.

### Open Question 2
- Question: Can the patch size for symbols in DietCNN be increased beyond 1x1 to further reduce memory footprint without significant accuracy loss?
- Basis in paper: [explicit] The authors suggest exploring larger patch sizes in the future to reduce memory footprint and automating the standard CNN to DietCNN conversion for larger networks.
- Why unresolved: The current implementation uses a patch size of 1x1, and the effects of larger patch sizes on accuracy and memory efficiency have not been thoroughly investigated.
- What evidence would resolve it: A systematic study comparing DietCNN performance with different patch sizes (e.g., 2x2, 3x3) on various datasets, analyzing the trade-off between memory reduction and accuracy degradation.

### Open Question 3
- Question: How would DietCNN perform if implemented on different FPGA families or ASIC technology compared to Zynq 7000?
- Basis in paper: [inferred] The paper focuses on FPGA implementation using Zynq 7000 series, but does not explore other hardware platforms or compare performance across different technologies.
- Why unresolved: The current results are limited to a specific FPGA family, and the generalizability of DietCNN's benefits to other hardware platforms remains untested.
- What evidence would resolve it: Implementation and comparison of DietCNN on various FPGA families (e.g., Intel, Lattice) and ASIC technologies, measuring energy efficiency, latency, and resource utilization across platforms.

## Limitations

- Experimental validation covers limited model diversity (three architectures) and lacks statistical significance reporting
- No comprehensive comparisons against alternative multiplication-free methods like Ternary CNNs or Binarized Neural Networks
- Limited ablation studies on sensitivity to symbol vocabulary size, patch dimensions, or layer-wise quantization strategies

## Confidence

- **Medium**: Confidence in core claim that LUT-based multiplication replacement reduces energy while preserving accuracy
- **Medium-High**: Confidence in claim that DietCNN achieves lower resource usage than alternatives for reported architectures
- **Low-Medium**: Confidence in mechanism that shared codebook maintains accuracy across layers

## Next Checks

1. **Ablation Study**: Systematically vary symbol vocabulary size (32, 128, 512, 1024) and patch dimensions to quantify their impact on accuracy and resource usage across all three datasets.

2. **Robustness Analysis**: Test DietCNN on additional architectures (e.g., MobileNet, EfficientNet) and datasets (e.g., CIFAR100, SVHN) to evaluate generalizability beyond the three reported cases.

3. **Alternative Comparison**: Conduct head-to-head comparisons with Ternary Weight Networks and XNOR-Net on identical FPGA hardware to establish relative energy-latency-accuracy trade-offs.