---
ver: rpa2
title: 'PEPT: Expert Finding Meets Personalized Pre-training'
arxiv_id: '2312.12162'
source_url: https://arxiv.org/abs/2312.12162
tags: []
core_contribution: 'Expert finding is critical for routing questions in community
  question answering (CQA) platforms, but most methods rely on limited supervised
  data and fail to capture personalized and fine-grained expert modeling. To address
  this, we propose PEPT, a personalized pre-training and fine-tuning paradigm that
  unifies heterogeneous information (historical answered questions, vote scores, expert
  IDs) and designs two pre-training tasks: question-level masked language modeling
  for expert interest and vote-oriented prediction for expertise.'
---

# PEPT: Expert Finding Meets Personalized Pre-training

## Quick Facts
- arXiv ID: 2312.12162
- Source URL: https://arxiv.org/abs/2312.12162
- Reference count: 40
- Primary result: PEPT achieves 0.4% to 5.3% MRR improvements over 10 baselines across 6 CQA datasets

## Executive Summary
Expert finding in Community Question Answering (CQA) platforms requires matching questions with suitable experts based on their historical expertise and interests. Most existing methods rely on limited supervised data and fail to capture the nuanced, personalized characteristics of individual experts. PEPT introduces a personalized pre-training and fine-tuning paradigm that unifies heterogeneous information sources including historical answered questions, vote scores, and expert IDs to learn rich expert representations before downstream task-specific fine-tuning.

## Method Summary
PEPT uses a two-stage approach where experts are first pre-trained on unsupervised data from StackExchange, then fine-tuned on domain-specific expert finding tasks. During pre-training, the model constructs expert-level input units by concatenating historical answered questions with target questions, incorporating vote scores and expert ID embeddings as soft prompts. The pre-training stage employs three tasks: question-level masked language modeling to capture semantic relationships between questions, vote-oriented prediction to model expertise, and standard MLM. The fine-tuning stage adapts the pre-trained model to specific expert-question matching tasks using cross-entropy loss.

## Key Results
- Achieves consistent improvements across six real-world CQA datasets (English, Biology, Es, Electronics, Gis, CodeReview)
- MRR improvements range from 0.4% to 5.3% compared to 10 competitive baselines
- Effectively alleviates data sparsity challenges, particularly beneficial when training data is limited
- Ablation studies confirm the importance of modeling interest, expertise, and personalization simultaneously

## Why This Works (Mechanism)

### Mechanism 1
PEPT achieves superior expert finding performance by integrating heterogeneous information sources (historical questions, vote scores, expert IDs) during pre-training. The model constructs expert-level input units by concatenating historical answered questions with target questions, then incorporates vote scores and expert ID embeddings to capture interest, expertise, and personalization simultaneously.

### Mechanism 2
Question-level pre-training tasks (question-level MLM and vote-oriented prediction) enable more fine-grained expert modeling than word-level approaches. Question-level MLM masks entire questions and predicts them based on contextual questions to capture semantic relationships, while vote-oriented tasks predict expected vote scores for target questions to model expertise.

### Mechanism 3
Personalized pre-training using expert ID embeddings as soft prompts enables unique expert representations without exploding vocabulary size. Expert IDs are treated as continuous vector embeddings (soft prompts) rather than discrete tokens, allowing the model to learn personalized characteristics for millions of experts efficiently.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) and its variants
  - Why needed here: PEPT builds upon MLM but extends it to question-level masking to capture semantic relationships between entire questions rather than just words
  - Quick check question: What is the difference between word-level MLM and question-level MLM, and why does this matter for capturing expert interest?

- Concept: Pre-training vs Fine-tuning paradigm in PLMs
  - Why needed here: PEPT uses pre-training to learn general expert representations from unsupervised data, then fine-tunes on specific expert finding tasks - understanding this separation is crucial
  - Quick check question: How does pre-training on large unsupervised corpora differ from fine-tuning on task-specific data, and why is this two-stage approach beneficial for expert finding?

- Concept: Attention mechanisms and self-attention in Transformers
  - Why needed here: PEPT relies on BERT's self-attention architecture to capture contextual relationships between historical questions, target questions, and expert IDs
  - Quick check question: How does self-attention enable the model to weigh the importance of different historical questions when modeling an expert's interest and expertise?

## Architecture Onboarding

- Component map: Input construction (historical questions + target question + vote scores + expert ID embeddings) -> BERT encoder (multi-head self-attention and feed-forward layers) -> Task-specific prediction (expert-question matching score) -> Output layer
- Critical path: Input construction → Expert ID embedding lookup → BERT encoding → Task-specific prediction → Loss calculation → Parameter updates
- Design tradeoffs: Question-level vs word-level masking (more semantic context vs easier reconstruction), vote score normalization (prevents dominance by high-variance scores vs information loss), soft prompt vs discrete tokens for expert IDs (scalability vs potential loss of discrete semantics)
- Failure signatures: Poor performance on personalization metrics (likely issue with expert ID embedding learning), degradation when historical question count varies (input construction or attention mechanism issues), overfitting on small datasets (insufficient regularization or data augmentation needed)
- First 3 experiments: 1) Ablation study removing expert ID embeddings to verify personalization contribution, 2) Varying question masking ratio to find optimal semantic context capture, 3) Testing with limited training data to evaluate pre-training's data sparsity mitigation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several important unresolved issues emerge from the analysis. The approach's performance scaling with dataset size remains unclear, as the paper only tests varying proportions of training data for one dataset rather than comprehensive scaling analysis across multiple datasets of varying sizes. The optimal trade-off between historical question length and model performance is not systematically analyzed, despite mentioning the use of a maximum training step of 50,000. Additionally, the paper doesn't address how PEPT handles temporal dynamics in expert interests and expertise, focusing instead on capturing current interests without modeling their evolution over time.

## Limitations

The heterogeneous information integration relies heavily on the assumption that concatenating historical questions with vote scores and expert IDs produces meaningful representations, but provides limited empirical validation that this specific combination is optimal. The question-level MLM task represents a departure from standard word-level masking without thorough ablation studies demonstrating that the increased semantic context justifies the potential information loss from masking entire questions. The soft prompt approach for expert IDs, while theoretically elegant, lacks comparative analysis against alternative personalization methods like discrete expert token embeddings or adapter-based approaches.

## Confidence

**High Confidence** in the overall experimental methodology and dataset preparation. The paper clearly specifies the datasets used (StackExchange pre-training data and six domain-specific fine-tuning datasets), evaluation metrics (MRR, Precision@1, Precision@3, NDCG@20), and baseline comparisons.

**Medium Confidence** in the performance claims. While the paper reports consistent improvements across six datasets (0.4% to 5.3% MRR gains over 10 baselines), the exact baseline implementations and hyperparameter settings are not fully specified.

**Low Confidence** in the scalability analysis. The paper does not provide experiments examining performance with varying numbers of experts, different historical question counts per expert, or how the approach scales to platforms with millions of experts.

## Next Checks

1. **Ablation of heterogeneous information integration**: Systematically remove each component (historical questions, vote scores, expert IDs) from the pre-training input and measure the degradation in fine-tuning performance to quantify the contribution of each information source.

2. **Comparison of question-level vs word-level MLM**: Implement a variant of the model using standard word-level MLM and compare performance on expert finding tasks to validate whether the question-level approach provides meaningful advantages for capturing expert interest.

3. **Scalability stress test**: Evaluate model performance as the number of historical questions per expert varies from very few (1-3) to many (20+), and test on datasets with significantly larger expert populations to assess the approach's robustness to data sparsity and scale.