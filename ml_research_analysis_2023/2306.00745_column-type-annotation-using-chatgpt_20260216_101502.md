---
ver: rpa2
title: Column Type Annotation using ChatGPT
arxiv_id: '2306.00745'
source_url: https://arxiv.org/abs/2306.00745
tags:
- table
- column
- chatgpt
- type
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of ChatGPT for column type annotation,
  a crucial pre-processing step for data search and integration. The authors evaluate
  different prompt designs, including providing task definitions and detailed instructions,
  and implement a two-step table annotation pipeline.
---

# Column Type Annotation using ChatGPT

## Quick Facts
- arXiv ID: 2306.00745
- Source URL: https://arxiv.org/abs/2306.00745
- Reference count: 36
- Key outcome: ChatGPT achieves F1 scores over 85% for column type annotation in zero- and one-shot setups, comparable to a fine-tuned RoBERTa model.

## Executive Summary
This paper investigates using ChatGPT for column type annotation, a critical pre-processing step for data search and integration. The authors evaluate different prompt designs, including providing task definitions and detailed instructions, and implement a two-step table annotation pipeline. Using instructions and the two-step pipeline, ChatGPT achieves F1 scores of over 85% in zero- and one-shot setups. This performance is comparable to a RoBERTa model fine-tuned with 356 examples, demonstrating that ChatGPT can deliver competitive results for column type annotation given no or minimal task-specific demonstrations.

## Method Summary
The authors use the SOTAB benchmark dataset (subset: 62 tables for training with 356 columns, 41 tables for test with 250 columns), labeled with 32 semantic types from schema.org. They conduct zero-shot and few-shot experiments using different prompt designs (column, text, table formats), with and without instructions, message roles, and in-context learning. The two-step pipeline is implemented for larger label spaces. ChatGPT API calls are made with temperature=0, and performance is evaluated using Micro-F1, Precision, and Recall metrics.

## Key Results
- ChatGPT achieves F1 scores over 85% in zero- and one-shot setups for column type annotation.
- Using instructions and message roles in prompts significantly improves performance (12-35% increase in Micro-F1 score).
- The two-step pipeline approach allows ChatGPT to handle larger semantic type sets effectively.
- ChatGPT's performance is comparable to a fine-tuned RoBERTa model (difference of 0.26% F1 score).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT achieves high F1 scores in column type annotation with minimal task-specific training due to its pre-training on large text corpora and emergent in-context learning abilities.
- Mechanism: The large language model leverages its broad knowledge base and ability to generalize from few examples to perform column type annotation without extensive fine-tuning.
- Core assumption: The model's pre-training has captured semantic patterns relevant to understanding table column contexts and types.
- Evidence anchors:
  - [abstract]: "Using instructions as well as the two-step pipeline, ChatGPT reaches F1 scores of over 85% in zero- and one-shot setups."
  - [section]: "The difference is further shortened to 0.26% when the two-step pipeline in a zero-shot setting is used."
  - [corpus]: Found 25 related papers, average neighbor FMR=0.422, suggesting moderate relatedness but no direct citations, indicating this is a novel approach.
- Break condition: If the column values or table contexts are too domain-specific or require knowledge not present in the pre-training data, the model's performance would degrade significantly.

### Mechanism 2
- Claim: Providing explicit instructions and using message roles in prompts significantly improves ChatGPT's performance on column type annotation tasks.
- Mechanism: Clear, step-by-step instructions guide the model's reasoning process, while message roles help structure the conversation, leading to better task understanding and output formatting.
- Core assumption: The model can effectively follow natural language instructions and benefit from conversational structure.
- Evidence anchors:
  - [section]: "We notice that by providing instructions to the model, the performance increases by between 12 to 35% in Micro-F1 score."
  - [section]: "Using the message formats and message roles proved beneficial in all cases and we use them in the following experiments."
  - [corpus]: Moderate relatedness (average FMR=0.422) suggests this is a relevant but not widely explored area.
- Break condition: If instructions become too complex or the model fails to understand the conversational structure, performance gains may plateau or reverse.

### Mechanism 3
- Claim: The two-step pipeline approach allows ChatGPT to handle larger semantic type sets by first predicting the table's topical domain and then using only relevant subsets of the full label set.
- Mechanism: Decomposing the task into domain prediction and domain-specific column annotation simplifies the problem space and improves accuracy.
- Core assumption: Tables can be reliably classified into domains, and domain-specific label subsets are sufficient for accurate column annotation.
- Evidence anchors:
  - [section]: "The second step seems to achieve the highest performance in the zero-shot setup."
  - [section]: "As in the previous table format experiments... we do not achieve higher performance when showing the model 4 demonstrations."
  - [corpus]: No direct evidence in corpus, suggesting this specific approach may be novel.
- Break condition: If domain classification is incorrect or the domain-specific label subsets are incomplete, the overall annotation accuracy will suffer.

## Foundational Learning

- Concept: Zero-shot and few-shot learning in large language models
  - Why needed here: Understanding how ChatGPT can perform tasks without extensive task-specific training is crucial to interpreting the results and potential applications.
  - Quick check question: What is the difference between zero-shot and few-shot learning, and how does it apply to ChatGPT's column type annotation performance?

- Concept: Prompt engineering and its impact on model performance
  - Why needed here: The paper demonstrates that different prompt designs significantly affect ChatGPT's performance, highlighting the importance of effective prompt engineering.
  - Quick check question: How do explicit instructions and message roles in prompts influence ChatGPT's ability to perform column type annotation?

- Concept: Evaluation metrics in machine learning, specifically F1 score
  - Why needed here: The paper uses F1 score as a key metric for evaluating model performance, so understanding this metric is essential for interpreting the results.
  - Quick check question: What does the F1 score represent, and why is it used instead of other metrics like accuracy in this context?

## Architecture Onboarding

- Component map:
  - Input: Relational table data (columns and values)
  - Prompt generation module: Creates prompts based on different designs (column, text, table formats) and includes instructions and message roles
  - ChatGPT API: The language model that processes prompts and generates annotations
  - Post-processing: Maps model outputs to semantic types and calculates evaluation metrics
  - Two-step pipeline: Optional module for handling larger label sets by first predicting table domain

- Critical path:
  1. Input table data
  2. Generate appropriate prompt (considering format, instructions, and message roles)
  3. Send prompt to ChatGPT API
  4. Process and map model output to semantic types
  5. Calculate evaluation metrics (F1 score)

- Design tradeoffs:
  - Single vs. multi-column annotation: Multi-column approach performs better but requires more complex prompts and longer input processing
  - Zero-shot vs. few-shot: Few-shot learning improves performance but requires demonstrations, which may not always be available
  - Complete label set vs. two-step pipeline: Two-step pipeline allows handling larger label sets but adds complexity and potential for error in domain classification

- Failure signatures:
  - Incorrect domain classification in two-step pipeline leading to wrong label subsets
  - Model confusion when presented with too many demonstrations or very long input tables
  - Poor performance on domain-specific or highly technical column types not well-represented in pre-training data

- First 3 experiments:
  1. Implement and test the column format prompt with explicit instructions and message roles to establish a baseline for single-column annotation
  2. Implement and test the table format prompt with instructions and roles to compare multi-column annotation performance
  3. Implement the two-step pipeline approach and test its effectiveness in handling larger label sets compared to using the complete label set in a single prompt

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several important questions arise:

1. How does the performance of ChatGPT for column type annotation scale with increasing table size and complexity?
2. How does the performance of ChatGPT for column type annotation vary across different domains and types of data?
3. How does the performance of ChatGPT for column type annotation compare to other large language models or fine-tuned language models on more challenging datasets?

## Limitations
- Performance may degrade on highly domain-specific or technical column types not well-represented in pre-training data.
- Exact implementation details of output mapping and demonstration selection for in-context learning are not fully specified, potentially affecting reproducibility.
- The analysis is based on a limited dataset (SOTAB benchmark subset) and specific prompt designs, which may not generalize to other contexts.

## Confidence
- **Medium**: The results demonstrate that ChatGPT can achieve competitive performance for column type annotation in zero- and one-shot setups, but the analysis is based on a limited dataset and specific prompt designs. The comparison with a fine-tuned RoBERTa model provides a useful baseline, but the difference in performance (0.26% F1 score) may not be statistically significant given the small test set size.

## Next Checks
1. **Statistical Significance Analysis**: Conduct a statistical significance test (e.g., McNemar's test) on the F1 score differences between ChatGPT and the RoBERTa model to determine if the 0.26% difference is meaningful given the small test set size.

2. **Domain Generalization Test**: Evaluate ChatGPT's performance on a held-out domain or dataset with column types significantly different from the SOTAB benchmark to assess its ability to generalize beyond the training data distribution.

3. **Robustness to Prompt Variations**: Systematically test the impact of different prompt structures, instruction phrasings, and demonstration selections on model performance to identify the most critical factors for reliable column type annotation.