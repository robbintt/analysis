---
ver: rpa2
title: 'ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest
  Neighbor Search Algorithms'
arxiv_id: '2305.04359'
source_url: https://arxiv.org/abs/2305.04359
tags:
- algorithms
- search
- graph
- anns
- nearest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling approximate nearest
  neighbor search (ANNS) algorithms to billion-scale datasets. The authors introduce
  ParlayANN, a library of deterministic and parallel graph-based ANNS algorithms,
  along with a set of tools for developing such algorithms.
---

# ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms

## Quick Facts
- arXiv ID: 2305.04359
- Source URL: https://arxiv.org/abs/2305.04359
- Reference count: 40
- Achieves up to 106 queries per second with 10-recall@10 on 1 billion-point BIGANN dataset

## Executive Summary
This paper introduces ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search (ANNS) algorithms designed to scale to billion-scale datasets. The authors implement novel parallel versions of four state-of-the-art graph-based ANNS algorithms (DiskANN, HNSW, HCNNG, and pyNNDescent) using a lock-free construction approach based on prefix-doubling batch insertion. The library demonstrates significant performance advantages over IVF- and LSH-based competitors, computing orders of magnitude fewer distance comparisons while maintaining high recall and query throughput.

## Method Summary
The method centers on implementing four graph-based ANNS algorithms with parallel, lock-free construction using prefix-doubling batch insertion. This approach inserts points in exponentially increasing batches, enabling parallel greedy searches within each batch without locks while maintaining graph consistency. The algorithms leverage beam search for queries and optimize memory layout for cache efficiency. The framework supports diverse datasets including BIGANN (1B points), MSSPACEV, TEXT2IMAGE, and SSNPP, demonstrating scalability across different data distributions and dimensions.

## Key Results
- Achieves up to 106 QPS with 10-recall@10 on BIGANN 1B dataset
- Performs significantly fewer distance comparisons than IVF- and LSH-based competitors
- Build times range from 1.75 to 91.6 hours for billion-scale datasets
- Handles out-of-distribution data and range queries effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The prefix-doubling batch build approach enables lock-free construction of incremental graph algorithms.
- Mechanism: Points are inserted in exponentially increasing batches, allowing parallel insertions within each batch without locks while maintaining correctness through the batch structure.
- Core assumption: The graph structure remains consistent enough during batch processing that parallel greedy searches can proceed without corrupting the graph state.
- Evidence anchors:
  - [section] "The high-level idea is to insert points in batches of exponentially increasing size, as shown in Algorithm 3, which implements the insert() routine in batches."
  - [section] "This approach, also called prefix-doubling, provides the balancing among parallelism, low contention, and efficient work."
  - [corpus] "Found 25 related papers" - indicates active research in parallel ANNS, supporting the relevance of lock-free approaches.
- Break condition: If the greedy search requires a globally consistent graph state that cannot be maintained across parallel batch insertions.

### Mechanism 2
- Claim: Graph-based algorithms achieve superior machine-agnostic performance by computing significantly fewer distance comparisons than IVF/LSH methods.
- Mechanism: The graph structure allows beam search to navigate directly toward query points through long and short edges, avoiding exhaustive distance calculations across the entire dataset.
- Core assumption: The graph edges effectively capture proximity relationships that enable efficient navigation without missing true nearest neighbors.
- Evidence anchors:
  - [abstract] "The algorithms also demonstrate the ability to handle out-of-distribution data and range queries effectively."
  - [section] "Graph-based algorithms compute orders of magnitude fewer distance comparisons than their IVF- and LSH-based competitors."
  - [section] "Even when our IVF algorithm achieved higher QPS than graph-based algorithms, it still performed more distance computations."
- Break condition: If the graph structure becomes too sparse or the query distribution deviates significantly from the training distribution.

### Mechanism 3
- Claim: Hierarchical clustering-based methods (HCNNG) scale effectively to billion-scale datasets while maintaining competitive performance.
- Mechanism: Random hierarchical clustering creates a tree structure where MST construction within leaves provides efficient edge selection without requiring all-pairs distance computations.
- Core assumption: The hierarchical clustering preserves sufficient proximity information for effective nearest neighbor search while enabling parallel leaf processing.
- Evidence anchors:
  - [section] "Within a leaf, it computes a degree-bounded minimum spanning tree (MST), and the edges of the MST become the edges in the graph."
  - [section] "We parallelized HNNCG without locks by parallelizing recursive calls during the hierarchical clustering."
  - [corpus] "Scalable Overload-Aware Graph-Based Index Construction for 10-Billion-Scale Vector Similarity Search" - indicates scalability focus in related work.
- Break condition: If the random clustering fails to create meaningful proximity relationships or if leaf size becomes too large for efficient MST computation.

## Foundational Learning

- Concept: Parallel prefix-doubling algorithms
  - Why needed here: Enables efficient parallel construction of incremental graph structures without locks
  - Quick check question: How does prefix-doubling differ from simple parallel for loops in terms of work efficiency and contention?

- Concept: Graph search algorithms (beam search, greedy search)
  - Why needed here: Core mechanism for both building and querying ANNS graphs
  - Quick check question: What is the relationship between beam width and recall accuracy in graph-based ANNS?

- Concept: Distance metrics and their computational complexity
  - Why needed here: Understanding the performance differences between algorithms requires knowledge of distance computation costs
  - Quick check question: Why does the choice of distance metric (Euclidean vs. inner product) affect algorithm performance differently?

## Architecture Onboarding

- Component map:
  - Graph construction engine (parallel batch processing) -> Search query processor (beam search with hash table optimization) -> Layout optimizer (edge list storage without indirection) -> Parameter tuning system (algorithm-specific parameter sweeps)

- Critical path:
  1. Build graph using batch insertion with prefix-doubling
  2. Optimize graph layout for cache efficiency
  3. Execute queries using beam search with visited set tracking
  4. Measure performance using distance computations and QPS

- Design tradeoffs:
  - Memory vs. performance: denser graphs provide better recall but require more memory and slower queries
  - Parallelism vs. cache efficiency: larger batches enable better parallelism but may reduce cache locality
  - Build time vs. query performance: more expensive builds can yield faster queries through better graph structures

- Failure signatures:
  - Degraded recall with high thread counts: indicates lock contention or cache thrashing
  - Increasing build times with dataset size: suggests algorithmic complexity issues or memory bottlenecks
  - Low QPS despite high recall: points to search algorithm inefficiencies or poor graph structure

- First 3 experiments:
  1. Single-threaded build and query on small dataset to verify correctness
  2. Scale dataset size while measuring build time growth to identify superlinear behavior
  3. Vary beam width and degree parameters to find optimal performance-recall tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can techniques from incremental graph algorithms be combined with insights from HCNNG to produce an algorithm that dominates both?
- Basis in paper: [explicit] The paper discusses how incremental algorithms like DiskANN and HNSW can be made lock-free, while HCNNG uses hierarchical clustering. It notes that HCNNG is especially competitive at the billion scale despite struggling in the low recall regime.
- Why unresolved: The paper shows that each algorithm has strengths in different regimes (recall ranges, dataset characteristics), but does not explore hybrid approaches that could combine the benefits of incremental construction with hierarchical clustering.
- What evidence would resolve it: A comparative study showing that a hybrid algorithm outperforms both pure incremental and pure HCNNG approaches across multiple datasets and recall ranges.

### Open Question 2
- Question: How can ANNS graph structures be adapted to quickly answer range search queries?
- Basis in paper: [explicit] The paper demonstrates that graph-based algorithms struggle significantly with range search compared to IVF methods like FAISS, which dominate in this scenario due to their cell-based approach.
- Why unresolved: While the paper identifies this limitation, it does not propose or evaluate modifications to graph structures that could improve their range search performance.
- What evidence would resolve it: Experimental results showing a modified graph-based algorithm achieving competitive QPS on range search datasets compared to FAISS, while maintaining good performance on nearest neighbor queries.

### Open Question 3
- Question: How can IVF and LSH algorithms be adapted to perform better on out-of-distribution queries?
- Basis in paper: [explicit] The paper shows that both FAISS and FALCONN struggle significantly on out-of-distribution datasets like TEXT2IMAGE, while graph-based algorithms perform relatively better.
- Why unresolved: The paper identifies the problem but does not explore modifications to IVF or LSH algorithms that could improve their performance on out-of-distribution data.
- What evidence would resolve it: Experimental results demonstrating that modified IVF or LSH algorithms achieve significantly higher recall on out-of-distribution datasets compared to their standard implementations, while maintaining competitive performance on in-distribution data.

## Limitations
- The ParlayANN library is not open-source, limiting independent validation
- Performance claims are based on specific hardware configurations (Msv2 and Ev5 machines) that may not be accessible
- Limited generalizability to datasets and distributions beyond the four tested datasets
- Lock-free parallelization may face practical challenges with NUMA architectures

## Confidence
**High Confidence**: The core mechanism of prefix-doubling batch construction and its ability to enable lock-free parallel graph building is well-supported by algorithmic description and theoretical analysis.

**Medium Confidence**: The scalability claims to billion-scale datasets are supported by empirical results but may not generalize to all data distributions or hardware configurations.

**Low Confidence**: The specific performance numbers (QPS, build times) are highly dependent on the particular hardware and software environment described.

## Next Checks
1. Reimplement the prefix-doubling batch insertion algorithm on a smaller dataset to verify the lock-free property and measure the overhead compared to traditional sequential insertion methods.

2. Conduct controlled experiments comparing distance computations between ParlayANN algorithms and IVF/LSH baselines across multiple query distributions to validate the claimed efficiency advantage.

3. Test the algorithms on datasets with varying dimensionality and data distributions (beyond the four specified datasets) to assess generalizability of the performance claims and identify potential failure modes.