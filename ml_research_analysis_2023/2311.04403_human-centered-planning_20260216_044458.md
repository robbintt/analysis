---
ver: rpa2
title: Human-Centered Planning
arxiv_id: '2311.04403'
source_url: https://arxiv.org/abs/2311.04403
tags:
- constraints
- events
- planning
- symplan
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLMPlan, a large language model (LLM)-based
  planner that can generate daily schedules from user-provided events and constraints.
  LLMPlan leverages an LLM's commonsense reasoning to handle vague constraints and
  incorporates a self-reflection mechanism to correct constraint violations.
---

# Human-Centered Planning

## Quick Facts
- **arXiv ID**: 2311.04403
- **Source URL**: https://arxiv.org/abs/2311.04403
- **Reference count**: 27
- **Primary result**: LLMPlan outperforms traditional symbolic planning in user satisfaction (70.5% vs 40.4%) for day planning tasks

## Executive Summary
This paper presents LLMPlan, a large language model-based planner that generates daily schedules from user-provided events and constraints. The system leverages LLMs' commonsense reasoning to handle vague constraints and incorporates a self-reflection mechanism to iteratively correct constraint violations. When compared against a traditional symbolic planner (SymPlan) and a hybrid approach (SymPlan+), LLMPlan achieves comparable correctness metrics while significantly outperforming in user satisfaction during interactive evaluations with 40 participants.

## Method Summary
LLMPlan uses GPT models to generate plans and employs a self-reflection mechanism where constraint violations are verbalized as feedback to regenerate the plan iteratively. SymPlan uses a symbolic approach with Simple Temporal Networks (STNs) and constraint propagation with backtracking search. SymPlan+ extends SymPlan by inferring commonsense constraints from LLMs using few-shot examples. The system was evaluated on synthetic and real user data, measuring correctness, commonsense violations, and user satisfaction through an interactive study.

## Key Results
- LLMPlan achieves 93.0% event coverage versus SymPlan's 96.6%, but with significantly fewer commonsense violations (0.1 vs 6.5)
- In user studies, LLMPlan achieved 70.5% user satisfaction compared to SymPlan's 40.4%
- LLMPlan handles open-ended constraints significantly better than SymPlan (72.5% vs 45% positive rate)
- LLMPlan processes plans faster than SymPlan (17.2s vs 57.8s on average)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMPlan's self-reflection mechanism improves constraint satisfaction by iteratively correcting violations.
- Mechanism: After generating an initial plan, LLMPlan runs constraint checkers to identify violations (unsatisfied constraints, missing events, overlapping events). These violations are verbalized as textual feedback and fed back to the LLM to regenerate the plan. This process repeats until no violations remain or a maximum iteration count is reached.
- Core assumption: The LLM can understand and correct its own output when given specific feedback about constraint violations.
- Evidence anchors:
  - [abstract]: "LLMPlan leverages an LLM's commonsense reasoning to handle vague constraints and incorporates a self-reflection mechanism to correct constraint violations."
  - [section]: "We then verbalize the list of violations into textual feedback such as 'the duration of A is too long/short', 'A should not overlap with B', etc., and feed them as input to the LLM to regenerate the plan."
  - [corpus]: Weak evidence - the corpus contains papers on symbolic planning and LLM planning, but no specific mention of self-reflection mechanisms for constraint correction.

### Mechanism 2
- Claim: SymPlan+ improves commonsense reasoning by inferring constraints from LLM using few-shot examples.
- Mechanism: SymPlan+ queries an LLM (GPT-3.5) with few-shot examples of events and their typical start/end time constraints. The LLM responds with suggested constraints for the event of interest, which are then appended to the user-provided constraints for SymPlan.
- Core assumption: The LLM has sufficient commonsense knowledge about typical event timing and can generalize from few-shot examples to new events.
- Evidence anchors:
  - [section]: "We extended SymPlan to infer commonsense constraints about the durations and time of events from an LLM so it can compete with LLMPlan's built-in ability to do so."
  - [section]: "The LLM prompt consists of three few-shot examples followed by a query for the event of interest."
  - [corpus]: Weak evidence - the corpus contains papers on LLM planning and symbolic planning, but no specific mention of using LLMs to infer commonsense constraints for symbolic planners.

### Mechanism 3
- Claim: LLMPlan handles vague and open-ended constraints better than SymPlan+ due to LLM's natural language understanding capabilities.
- Mechanism: LLMPlan can understand natural language commands and incorporate them into the plan generation process. For example, a user can say "I want frequent breaks in the afternoon" and LLMPlan can interpret this and adjust the plan accordingly. SymPlan+, on the other hand, relies on a constraint extractor that may not be able to handle such open-ended requests.
- Core assumption: The LLM has strong natural language understanding capabilities and can infer user intent from vague or open-ended requests.
- Evidence anchors:
  - [abstract]: "Consequently, LLMPlan outperforms SymPlan in user satisfaction (70.5% vs. 40.4%) during interactive evaluation with 40 users."
  - [section]: "We organize them into 3 heuristically derived categories (see Table 6 for some examples and stats): Well-defined, Complex, Open-ended."
  - [section]: "We found that LLMPlan significantly outperforms SymPlan on handling the open-ended messages (72.5% vs. 45% positive rate)."

## Foundational Learning

- **Concept: Simple Temporal Network (STN)**
  - Why needed here: SymPlan uses STNs to represent and reason about temporal constraints between events. Understanding STNs is crucial for understanding how SymPlan works.
  - Quick check question: In an STN, what do the nodes and edges represent, and how are temporal constraints encoded?

- **Concept: Constraint Satisfaction Problem (CSP)**
  - Why needed here: Both LLMPlan and SymPlan are essentially solving CSPs, where the goal is to find a plan that satisfies all the given constraints. Understanding CSPs is important for understanding the problem formulation and solution approaches.
  - Quick check question: What are the key components of a CSP, and how do backtracking search and constraint propagation algorithms work to solve them?

- **Concept: Few-shot Learning**
  - Why needed here: SymPlan+ uses few-shot learning to infer commonsense constraints from an LLM. Understanding few-shot learning is important for understanding how SymPlan+ works and its limitations.
  - Quick check question: What is few-shot learning, and how does it differ from traditional supervised learning? What are the key challenges and considerations when using few-shot learning?

## Architecture Onboarding

- **Component map**: User input -> Plan generation (LLMPlan or SymPlan+) -> Constraint checking -> Self-reflection (LLMPlan) or constraint inference (SymPlan+) -> Plan output -> User feedback
- **Critical path**: User input → Plan generation (LLMPlan or SymPlan+) → Constraint checking → Self-reflection (LLMPlan) or constraint inference (SymPlan+) → Plan output → User feedback
- **Design tradeoffs**:
  - LLMPlan vs. SymPlan: LLMPlan can handle vague and open-ended constraints but may be less accurate for well-defined constraints. SymPlan is more accurate for well-defined constraints but cannot handle vague or open-ended constraints.
  - Self-reflection vs. no self-reflection: Self-reflection improves constraint satisfaction but adds computational overhead and may not always converge to a valid plan.
  - LLM constraint inference vs. no inference: LLM constraint inference improves commonsense reasoning but relies on the LLM's knowledge and may not always provide relevant constraints.
- **Failure signatures**:
  - LLMPlan: Plans with constraint violations, missing events, or overlapping events; failure to understand user intent; excessive computation time.
  - SymPlan: Plans with constraint violations; failure to handle vague or open-ended constraints; excessive computation time.
  - SymPlan+: Plans with constraint violations; failure to infer relevant commonsense constraints; excessive computation time.
- **First 3 experiments**:
  1. Test LLMPlan with well-defined constraints and compare its performance to SymPlan.
  2. Test LLMPlan with vague or open-ended constraints and evaluate its ability to handle them.
  3. Test SymPlan+ with events that require commonsense constraints and evaluate the effectiveness of the LLM constraint inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLMPlan handle tasks that require multiple days to complete?
- Basis in paper: [inferred] The paper mentions that LLMPlan can generate daily schedules, but it doesn't explicitly state how it handles tasks that span multiple days.
- Why unresolved: The paper focuses on day planning and doesn't provide details on how the planner handles tasks that require more than one day to complete.
- What evidence would resolve it: An explanation or example in the paper showing how LLMPlan handles multi-day tasks would resolve this question.

### Open Question 2
- Question: What is the impact of the size of the LLM on the performance of LLMPlan?
- Basis in paper: [explicit] The paper mentions that different sizes of LLMs (GPT-3, GPT-3.5, GPT-4) were tested, but it doesn't provide a detailed comparison of their performance.
- Why unresolved: The paper only briefly mentions the performance of different LLM sizes without a detailed analysis of how the size impacts the planner's performance.
- What evidence would resolve it: A detailed comparison of the performance of LLMPlan using different sizes of LLMs would resolve this question.

### Open Question 3
- Question: How does LLMPlan handle tasks that have a high degree of uncertainty or variability in their duration?
- Basis in paper: [inferred] The paper mentions that LLMPlan can handle vague constraints, but it doesn't explicitly state how it handles tasks with uncertain or variable durations.
- Why unresolved: The paper focuses on the planner's ability to handle vague constraints but doesn't provide details on how it handles tasks with uncertain or variable durations.
- What evidence would resolve it: An explanation or example in the paper showing how LLMPlan handles tasks with uncertain or variable durations would resolve this question.

### Open Question 4
- Question: How does the self-reflection mechanism in LLMPlan work?
- Basis in paper: [explicit] The paper mentions that LLMPlan uses a self-reflection mechanism to correct constraint violations, but it doesn't provide details on how this mechanism works.
- Why unresolved: The paper mentions the self-reflection mechanism but doesn't provide details on its implementation or how it identifies and corrects constraint violations.
- What evidence would resolve it: A detailed explanation of the self-reflection mechanism in LLMPlan would resolve this question.

### Open Question 5
- Question: How does LLMPlan handle tasks that have dependencies on other tasks?
- Basis in paper: [inferred] The paper mentions that LLMPlan can handle relative temporal constraints, but it doesn't explicitly state how it handles tasks that have dependencies on other tasks.
- Why unresolved: The paper focuses on the planner's ability to handle relative temporal constraints but doesn't provide details on how it handles tasks that have dependencies on other tasks.
- What evidence would resolve it: An explanation or example in the paper showing how LLMPlan handles tasks with dependencies on other tasks would resolve this question.

## Limitations

- The self-reflection mechanism's effectiveness is primarily validated through synthetic datasets rather than real-world usage patterns, raising questions about generalizability to diverse user behaviors.
- The comparison between LLMPlan and SymPlan+ may be affected by the choice of specific LLM models (GPT-3 vs GPT-3.5) and their inherent differences in reasoning capabilities.
- The user study's results, while promising, are based on a relatively small sample size (40 participants) and may not capture long-term user satisfaction or usage patterns.

## Confidence

- **High confidence**: The basic claim that LLMPlan can generate schedules from user constraints is well-supported by the experimental results. The methodology for evaluating constraint satisfaction through automated checking is sound and clearly specified.
- **Medium confidence**: The superiority of LLMPlan over SymPlan+ for handling vague and open-ended constraints is supported by user study results, but the specific mechanisms by which users perceive improvements need further investigation.
- **Medium confidence**: The self-reflection mechanism's contribution to constraint satisfaction is demonstrated, but the iteration limits and convergence properties require more extensive testing across diverse scenarios.

## Next Checks

1. **Cross-model validation**: Test both LLMPlan and SymPlan+ using the same underlying LLM model to isolate the impact of architectural differences from model-specific capabilities.
2. **Longitudinal user study**: Conduct a longer-term user study with regular usage over several weeks to evaluate sustained satisfaction and identify any degradation in performance or user experience.
3. **Edge case analysis**: Systematically test both planners with highly complex constraint combinations and ambiguous requirements to identify failure modes and iteration limits of the self-reflection mechanism.