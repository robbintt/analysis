---
ver: rpa2
title: State Regularized Policy Optimization on Data with Dynamics Shift
arxiv_id: '2306.03552'
source_url: https://arxiv.org/abs/2306.03552
tags:
- dynamics
- state
- policy
- different
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of leveraging data with dynamics
  shift in reinforcement learning, where different environments have varying underlying
  dynamics. The core method, State Regularized Policy Optimization (SRPO), exploits
  the insight that optimal policies in environments with similar structures and different
  dynamics have similar stationary state distributions.
---

# State Regularized Policy Optimization on Data with Dynamics Shift

## Quick Facts
- arXiv ID: 2306.03552
- Source URL: https://arxiv.org/abs/2306.03552
- Reference count: 40
- State Regularized Policy Optimization (SRPO) achieves state-of-the-art results in 8 out of 12 offline tasks by leveraging data with dynamics shift

## Executive Summary
This paper addresses the challenge of leveraging data with dynamics shift in reinforcement learning, where different environments have varying underlying dynamics. The authors propose State Regularized Policy Optimization (SRPO), which exploits the insight that optimal policies in environments with similar structures and different dynamics have similar stationary state distributions. SRPO uses this distribution as a regularizer in a constrained policy optimization formulation, enabling efficient reuse of data from multiple dynamics. The algorithm is implemented as an add-on module to context-based algorithms like CaDM and MAPLE, in both online and offline RL settings. Experimental results show that SRPO significantly improves sample efficiency and overall performance, achieving state-of-the-art results in 8 out of 12 offline tasks. Theoretical analysis provides a lower-bound performance guarantee for policies regularized by the optimal state distribution.

## Method Summary
SRPO addresses the problem of dynamics shift by regularizing policies to match the optimal state distribution across environments with similar structures but different dynamics. The method works by first training a discriminator to distinguish high-value states from low-value states, which serves as a surrogate for the density ratio ζ(s)/dπ(s). This discriminator output is then used to augment rewards with a state distribution regularization term. The policy is optimized using a constrained formulation that requires both maximizing cumulative return and matching the optimal state distribution. SRPO is implemented as an add-on module to context-based algorithms like CaDM and MAPLE, requiring only a state discriminator and minor modifications to the training loop. The method works in both online and offline RL settings, with hyperparameters including the regularization coefficient λ (0.1 or 0.3) and state partition ratio ρ (0.2 or 0.5).

## Key Results
- SRPO achieves state-of-the-art performance in 8 out of 12 offline tasks on D4RL benchmarks
- The algorithm improves sample efficiency and overall performance of context-based algorithms (CaDM, MAPLE, PEARL)
- Theoretical analysis provides a lower-bound performance guarantee for policies regularized by the optimal state distribution
- SRPO works effectively in both online RL (training from scratch) and offline RL (using pre-collected datasets) settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal policies in environments with similar structures but different dynamics produce similar stationary state distributions.
- Mechanism: The stationary state distribution serves as a regularizer that bridges data from multiple dynamics, enabling efficient reuse of off-policy data.
- Core assumption: MDPs are homomorphous, meaning they share the same state reachability structure despite differing dynamics.
- Evidence anchors:
  - [abstract] "We find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions."
  - [section 3.1] "It can be observed from the figure that collected data have the same high state density region with low pendulum speed and small pendulum angle, while the action distribution has different density peaks."
  - [corpus] Weak evidence; related work mentions policy regularization but not specifically state distribution similarity across dynamics.
- Break condition: If MDPs are not homomorphous (e.g., different state reachability), the optimal state distributions will diverge and the regularization will fail.

### Mechanism 2
- Claim: Training a discriminator to distinguish high-value states from low-value states provides a practical surrogate for the density ratio ζ(s)/dπ(s).
- Mechanism: States with higher value under the current dynamics are more likely to be drawn from the optimal state distribution, so the discriminator output approximates the required density ratio.
- Core assumption: State values correlate with their likelihood of being optimal, and the classifier can be trained effectively using this proxy.
- Evidence anchors:
  - [section 3.3] "States with higher Dδ outputs also have higher values in all three environments."
  - [abstract] "We therefore propose a constrained policy optimization formulation that requires the policy to not only optimize the cumulative return, but also generate a stationary state distribution that is close to the optimal one."
  - [corpus] Weak evidence; adversarial training for density ratios exists but not specifically for state distribution regularization.
- Break condition: If state values do not correlate well with optimality across dynamics, the discriminator will fail to approximate the density ratio.

### Mechanism 3
- Claim: Regularizing the policy to match the optimal state distribution provides a lower-bound performance guarantee in homomorphous MDPs.
- Mechanism: By constraining the KL divergence between the policy's state distribution and the optimal one, the algorithm bounds the performance gap across similar dynamics.
- Core assumption: The action gap is sufficiently large to ensure optimal policies induce identical stationary distributions in homomorphous MDPs.
- Evidence anchors:
  - [section 4] "Theorem 4.2: If T ′ ∈ (T, εm), for all learning policy ˆπ such that DKL(dˆπT(·)∥d∗T′(·)) ⩽ εs, we have ηT(ˆπ) ⩾ ηT(π∗T) − λ1λ2εm + 2λ1 + √2Rmax√εs/(1 − γ)."
  - [abstract] "We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution."
  - [corpus] Weak evidence; most related work focuses on behavior regularization, not state distribution regularization.
- Break condition: If the action gap is too small or dynamics differ too much, the theoretical guarantee breaks down.

## Foundational Learning

- Concept: Hidden Parameter Markov Decision Process (HiP-MDP)
  - Why needed here: Provides the formal framework for modeling environments with different dynamics but similar structure.
  - Quick check question: What is the role of the hidden parameter θ in an HiP-MDP?

- Concept: Constrained Policy Optimization (CPO)
  - Why needed here: Forms the optimization basis for incorporating state distribution regularization.
  - Quick check question: How does adding a KL divergence constraint change the policy optimization objective?

- Concept: KL Divergence and Stationary State Distributions
  - Why needed here: Quantifies the similarity between state distributions across different dynamics.
  - Quick check question: What does it mean for two policies to have the same stationary state distribution?

## Architecture Onboarding

- Component map:
  Context encoder (CaDM/MAPLE) -> State discriminator -> Policy network -> Value network

- Critical path:
  1. Sample trajectories and extract states
  2. Train discriminator to distinguish high-value from low-value states
  3. Use discriminator output to augment rewards with state distribution regularization term
  4. Update policy and value networks using augmented rewards

- Design tradeoffs:
  - Discriminator quality vs training stability: Poor discriminator leads to ineffective regularization
  - Regularization coefficient λ: Too high causes over-regularization, too low provides insufficient benefit
  - State partition ratio ρ: Affects how many high-value states are used for discriminator training

- Failure signatures:
  - Discriminator output becomes uninformative (values clustered near 0.5)
  - Performance degrades when adding regularization (over-regularization)
  - No improvement with more data from different dynamics

- First 3 experiments:
  1. Verify that discriminator output correlates with state values in a simple environment
  2. Test performance on Pendulum with varying gravity (single vs multiple dynamics)
  3. Compare with behavior regularization baseline on Walker2d medium-expert dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is SRPO in environments where the optimal state distribution varies significantly across different dynamics?
- Basis in paper: [explicit] The paper mentions that in Walker2d and HalfCheetah environments, there are multiple optimal policies leading to different stationary state distributions.
- Why unresolved: The paper only briefly mentions this issue and does not provide a detailed analysis or experimental results on how SRPO performs in such scenarios.
- What evidence would resolve it: Conducting experiments in environments with multiple optimal policies and analyzing the performance of SRPO in these scenarios would provide insights into its effectiveness.

### Open Question 2
- Question: Can SRPO be extended to handle continuous state spaces more efficiently?
- Basis in paper: [inferred] The paper mentions that computing the density ratio ζ(s)/dπ(s) is challenging in continuous state spaces and proposes a sample-based surrogate approach.
- Why unresolved: The paper does not provide a detailed analysis of the efficiency of the proposed sample-based surrogate approach in handling continuous state spaces.
- What evidence would resolve it: Conducting experiments in environments with continuous state spaces and comparing the performance of SRPO with and without the proposed sample-based surrogate approach would provide insights into its efficiency.

### Open Question 3
- Question: How sensitive is SRPO to the choice of the hyperparameter λ?
- Basis in paper: [explicit] The paper mentions that λ is regarded as a hyperparameter with values 0.1 or 0.3 and conducts ablation studies to investigate its effect.
- Why unresolved: The paper only provides a limited analysis of the effect of λ on the performance of SRPO.
- What evidence would resolve it: Conducting a more extensive ablation study with a wider range of λ values and analyzing the impact on the performance of SRPO would provide insights into its sensitivity to the choice of λ.

## Limitations

- The primary assumption of homomorphous MDPs is not empirically validated across a broader range of environments.
- The theoretical guarantees rely on strong conditions (sufficiently large action gap) that may not hold in practice.
- Empirical validation is limited to a relatively small set of MuJoCo tasks with controlled dynamics variations.

## Confidence

- **High**: The core insight about using optimal state distributions as a regularizer is novel and theoretically grounded. The empirical improvements on D4RL benchmarks are demonstrated across multiple offline RL algorithms.
- **Medium**: The mechanism connecting state distribution regularization to performance improvements is plausible but not fully explored. The theoretical analysis provides useful bounds but depends on assumptions that warrant further investigation.
- **Low**: The claim that SRPO generalizes well to arbitrary dynamics shifts is not sufficiently tested. The relationship between the action gap and distribution similarity needs more empirical validation.

## Next Checks

1. Evaluate SRPO on environments with more diverse and less structured dynamics shifts (e.g., HalfCheetah with varying friction coefficients across different body parts) to test the limits of the homomorphous MDP assumption.

2. Conduct ablation studies that systematically vary the action gap and state reachability structure to empirically validate the theoretical conditions under which state distribution regularization is effective.

3. Analyze the quality and stability of the discriminator across different environments and training stages, measuring how well it correlates with true optimal state distributions rather than just state values.