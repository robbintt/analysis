---
ver: rpa2
title: Universal Representation of Permutation-Invariant Functions on Vectors and
  Tensors
arxiv_id: '2310.13829'
source_url: https://arxiv.org/abs/2310.13829
tags:
- function
- multiset
- continuous
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides universal representation results for permutation-invariant
  functions on multisets and tensors. For multiset functions on D-dimensional vectors,
  the authors prove that a sum-decomposable model can universally represent continuous
  and discontinuous functions via a latent dimension of O(N^D), where N is the maximum
  input multiset size.
---

# Universal Representation of Permutation-Invariant Functions on Vectors and Tensors

## Quick Facts
- arXiv ID: 2310.13829
- Source URL: https://arxiv.org/abs/2310.13829
- Reference count: 40
- Primary result: Universal representation of continuous and discontinuous permutation-invariant functions on multisets and tensors with improved latent dimensions

## Executive Summary
This paper establishes universal representation results for permutation-invariant functions on multisets and tensors. The authors prove that sum-decomposable models can universally represent both continuous and discontinuous functions on D-dimensional vectors using latent dimensions of O(N^D), where N is the maximum input multiset size. They further show that for "identifiable" multisets (where elements can be uniquely labeled), the required latent dimension reduces to 2DN for continuous functions. These theoretical advances provide insights into the expressiveness of permutation-invariant neural networks and enable more efficient architectures for learning on unordered data structures.

## Method Summary
The paper constructs universal representations for permutation-invariant functions using sum-decomposable models. For multiset functions, they employ polynomial-based encoding where each element x is mapped to a polynomial feature vector. The multiset encoder aggregates these via summation, and the decoder reconstructs the output. For identifiable multisets, they introduce an identifier function that uniquely labels elements, enabling lower-dimensional representations. For tensors, they extend this approach using nested sum-decomposable structures with appropriate identifier functions for each dimension.

## Key Results
- Continuous and discontinuous multiset functions admit universal representation via sum-decomposable models with latent dimension O(N^D)
- For identifiable multisets, continuous functions require only 2DN latent dimension
- Permutation-invariant tensor functions can be universally represented via nested sum-decomposable models
- These results provide theoretical foundations for understanding permutation-invariant neural network architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous multiset functions on vectors admit universal representation via sum-decomposable models with latent dimension O(N^D) for discontinuous functions and O(N^D) for continuous functions.
- **Mechanism**: The paper constructs an injective multiset encoder Φ using polynomial-based encoding where each element x is mapped to ϕ(x) = [∏d=1D xkd d]k∈KD N. This allows representing any multiset function f(X) = ρ ◦ Φ(X) where Φ(X) = Σx∈X ϕ(x).
- **Core assumption**: The polynomial basis functions span the space of permutation-invariant functions on multisets of D-dimensional vectors.
- **Evidence anchors**:
  - [abstract] "universal representation is guaranteed for continuous and discontinuous multiset functions though a latent space dimension of O(N^D)"
  - [section 3] "We present the universal representation for continuous and discontinuous multiset functions — over D-dimensional vectors — via a sum-decomposable model"
- **Break condition**: If the polynomial basis cannot capture the required permutation invariance for specific function classes, or if the latent dimension bound is not tight for the function being approximated.

### Mechanism 2
- **Claim**: For identifiable multisets (where elements can be uniquely labeled), the required latent dimension reduces to 2DN for continuous functions.
- **Mechanism**: The paper introduces an identifier function l: RD → R that uniquely labels distinct elements. Using this, they construct an encoder ϕ(x) = [r(x), r(x)⊙2, ..., r(x)⊙N] where r(x) = x + 1l(x)j, enabling lower-dimensional representation while maintaining injectivity.
- **Core assumption**: The identifier function l can distinguish all distinct elements in the multiset.
- **Evidence anchors**:
  - [abstract] "We then introduce identifiable multisets... Using our analysis on identifiable multisets, we prove that a sum-decomposable model for general continuous multiset functions only requires a latent dimension of 2 DN"
  - [section 4] "The multiset encoding function Φ in Proposition 2 is akin to separating invariants... Dym and Gortler (2022) claim that for randomized invariants of dimension 2DN +1... almost all matrices in RD×N are identified"
- **Break condition**: If the identifier function fails to uniquely label elements (e.g., when l(x) = l(x') for distinct x, x'), the injectivity guarantee breaks.

### Mechanism 3
- **Claim**: Permutation-invariant tensor functions can be universally represented via nested sum-decomposable models on identifiable tensors.
- **Mechanism**: The paper extends the multiset results to tensors by introducing identifier functions for tensors and recursively constructing permutation-invariant representations. For k-th order tensors, they use nested sums with appropriate latent dimensions based on whether the identifier uses rational or real labels.
- **Core assumption**: Tensors can be uniquely identified using the proposed identifier function l: TN,K → RN×M.
- **Evidence anchors**:
  - [abstract] "We then extend our results and provide special sum-decomposition structures to universally represent permutation-invariant tensor functions on identifiable tensors"
  - [section 5] "Theorem 7. Let K, N ∈ N. Let f: TN,K → codom(f) be a permutation-invariant tensor function. Then we have ∀T ∈ Tl N,K: f(T) = ρ(Σn1∈N ϕ1(e⊤n1l(T), β1n1(T)))"
- **Break condition**: If tensors cannot be uniquely identified by the identifier function, or if the recursive structure fails to capture the required permutation invariance.

## Foundational Learning

- **Concept**: Permutation invariance and its implications for function representation
  - Why needed here: The paper's core contribution relies on representing functions that must be invariant to element ordering in multisets and tensors
  - Quick check question: Why can't standard neural networks directly handle unordered data structures without modification?

- **Concept**: Universal representation vs universal approximation
- **Concept**: Polynomial basis functions and multi-symmetric polynomials
  - Why needed here: The paper uses power-sum multi-symmetric polynomials to construct injective encoders
  - Quick check question: What is the dimension of the space spanned by power-sum multi-symmetric polynomials for N elements in D dimensions?

- **Concept**: Continuity of inverse functions and compact sets
  - Why needed here: The paper needs to show that the decoder ρ = f ◦ Φ−1 is continuous, which requires Φ−1 to be continuous on a compact domain
  - Quick check question: Under what conditions does a continuous bijection between compact sets have a continuous inverse?

## Architecture Onboarding

- **Component map**: Input multiset → element encoding via ϕ → aggregation via Φ → decoding via ρ → output
- **Critical path**: Input multiset → element encoding via ϕ → aggregation via Φ → decoding via ρ → output
- **Design tradeoffs**:
  - Higher latent dimension (O(N^D)) enables universal representation for all functions but is computationally expensive
  - Lower latent dimension (2DN) for identifiable cases trades generality for efficiency
  - Rational vs real identifier functions affect the theoretical guarantees and practical implementation
- **Failure signatures**:
  - Injectivity failure: Different multisets map to same latent representation
  - Continuity failure: Small input changes cause large output changes
  - Dimension mismatch: Latent space too small for required function class
  - Identifier failure: Non-unique labeling of elements in identifiable cases
- **First 3 experiments**:
  1. Test injectivity of Φ for small N and D using synthetic data with known unique representations
  2. Verify continuity of ρ by checking Lipschitz constants on validation data
  3. Compare performance of O(N^D) vs 2DN representations on identifiable vs non-identifiable datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between universal representation and universal approximation for multiset functions on vectors?
- Basis in paper: [explicit] The paper mentions that "universal representation results are stronger than their of universal approximation counterparts" and discusses the distinction between the two concepts.
- Why unresolved: The paper does not provide a rigorous mathematical characterization of the relationship between universal representation and approximation in the context of multiset functions on vectors.
- What evidence would resolve it: A formal proof establishing the exact conditions under which universal representation implies universal approximation for multiset functions on vectors.

### Open Question 2
- Question: Can the latent dimension bounds be further improved for identifiable multisets beyond the 2DN bound?
- Basis in paper: [explicit] The paper shows that identifiable multisets allow for a reduced latent dimension of 2DN compared to O(N^D) for general multisets.
- Why unresolved: The paper does not explore whether this 2DN bound is tight or if there are cases where an even lower dimension suffices.
- What evidence would resolve it: Constructing specific identifiable multisets or functions that provably require 2DN dimensions, or proving that a lower dimension is always sufficient.

### Open Question 3
- Question: How do the universal representation results extend to multiset functions on non-compact domains?
- Basis in paper: [inferred] The paper primarily focuses on compact domains and briefly mentions that "set functions on an uncountable domain D do not admit this sum-decomposable representation" (referring to Wagstaff et al. 2022).
- Why unresolved: The paper does not explore universal representation for multiset functions on non-compact domains like R^D.
- What evidence would resolve it: Extending the proof techniques to non-compact domains and establishing whether universal representation is possible with finite latent dimension.

## Limitations
- High-dimensional latent spaces (O(N^D) or 2DN) may pose computational challenges in real-world applications
- Theoretical results do not provide explicit bounds on approximation error or sample complexity for practical implementations
- Assumptions about identifier functions for identifiable multisets may not hold in all practical scenarios

## Confidence
- **High confidence**: The polynomial basis approach for universal representation of discontinuous multiset functions (O(N^D) dimension) is well-established and the proofs appear sound
- **Medium confidence**: The reduction to 2DN dimension for identifiable multisets relies on specific assumptions about the identifier function that may not hold in all practical scenarios
- **Medium confidence**: The tensor extension follows logically from the multiset results, but the recursive structure and identifier function for tensors introduce additional complexity that requires careful verification

## Next Checks
1. **Injectivity verification**: Construct explicit counterexamples to test whether the proposed encoder ϕ remains injective when the identifier function l fails to distinguish elements in practice
2. **Continuity analysis**: Implement small-scale experiments to measure the Lipschitz continuity of the decoder ρ across different function classes and latent dimensions
3. **Practical scalability assessment**: Benchmark the proposed architectures on standard set-based learning tasks (e.g., point cloud classification, set expansion) to evaluate the trade-off between representational power and computational efficiency