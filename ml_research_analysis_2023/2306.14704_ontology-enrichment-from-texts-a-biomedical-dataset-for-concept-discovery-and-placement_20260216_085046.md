---
ver: rpa2
title: 'Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery
  and Placement'
arxiv_id: '2306.14704'
source_url: https://arxiv.org/abs/2306.14704
tags:
- concept
- out-of-kb
- https
- ontology
- mentions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for ontology enrichment from
  texts, focusing on concept discovery and placement. The benchmark addresses limitations
  of existing datasets by supporting out-of-KB mention discovery, incorporating contextual
  terms, and handling complex concepts with logical operators.
---

# Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement

## Quick Facts
- arXiv ID: 2306.14704
- Source URL: https://arxiv.org/abs/2306.14704
- Authors: 
- Reference count: 40
- Primary result: Introduces a new benchmark for ontology enrichment from texts, supporting out-of-KB mention discovery and complex concept placement using MedMentions corpus and SNOMED CT ontologies

## Executive Summary
This paper presents a novel benchmark for ontology enrichment from texts that addresses key limitations in existing datasets. The benchmark supports both in-KB and out-of-KB mention discovery by leveraging KB versioning, incorporates contextual terms for improved mention disambiguation, and handles complex concepts with logical operators. Constructed using the MedMentions corpus (PubMed abstracts) and SNOMED CT ontologies, the dataset provides a comprehensive evaluation framework for concept discovery and placement tasks. The paper demonstrates that while LLM-based methods outperform rule-based approaches, significant challenges remain in accurately discovering out-of-KB mentions and placing complex concepts.

## Method Summary
The benchmark uses a data construction pipeline that selects KB subsets from SNOMED CT, extracts mentions from the MedMentions corpus, and creates mention-edge mappings using KB versioning (comparing 2014 and 2017 versions) to identify out-of-KB concepts. The evaluation framework employs rule-based Sieve methods and LLM-based approaches (BLINKout for mention discovery, bi-encoder with GPT-3.5 for concept placement). The method addresses three key limitations of existing datasets: supporting out-of-KB mention discovery through KB versioning, incorporating contextual terms beyond concept labels, and handling complex concepts with logical operators.

## Key Results
- LLM-based methods outperform rule-based ones but still struggle with out-of-KB mention discovery and concept placement tasks
- The benchmark achieves F1 scores below 50% for out-of-KB mention discovery, indicating fundamental task challenges
- Complex concepts involving logical operators significantly increase the difficulty of accurate placement
- Contextual terms improve mention disambiguation but require careful window sizing for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark supports both in-KB and out-of-KB mention discovery by leveraging KB versioning
- Mechanism: By comparing two versions of SNOMED CT (2014 vs 2017), the system identifies concepts present in the newer version but not the older, treating these as "out-of-KB" entities. This allows the dataset to include mentions of both known and newly discovered concepts from the same corpus
- Core assumption: The time gap between KB versions captures meaningful concept evolution relevant to the domain
- Evidence anchors:
  - [abstract]: "the time difference of two versions (in 2014 and 2017) to synthesise new entities"
  - [section]: "We consider SNOMED CT [9], one of the most important OWL ontology in the biomedical domain, and choose a subset by selected categories of concepts"
- Break condition: If the KB versions are too similar or too far apart, the out-of-KB discovery signal becomes either meaningless or too noisy

### Mechanism 2
- Claim: Contextual terms improve mention discovery by providing semantic context beyond concept labels
- Mechanism: Each mention is paired with left and right context windows from the original text, allowing models to disambiguate between similar concepts based on surrounding words
- Core assumption: The context windows are sufficiently informative to distinguish between similar biomedical concepts
- Evidence anchors:
  - [abstract]: "inclusion of contexts for mentions, distinct from only using concept labels"
  - [section]: "A corpus with mentions linked to entities in SNOMED CT is needed to synthesise contextual mentions"
- Break condition: If context windows are too short or too long, they may not provide useful disambiguation information

### Mechanism 3
- Claim: The benchmark supports complex concept placement through logical operator handling
- Mechanism: By including complex concepts (those with logical operators like âˆƒ or âŠ“) in the ontology subset, the dataset requires models to place mentions under parents that may involve existential restrictions or conjunctions
- Core assumption: The ontology subset contains a meaningful distribution of complex concepts that reflect real-world biomedical terminology
- Evidence anchors:
  - [abstract]: "complex concepts, i.e., with logical operators"
  - [section]: "Complex concepts mean concepts that involve at least one logical operators, e.g., negation (Â¬), conjunction (âŠ“), disjunction (âŠ”), existential restriction (âˆƒð‘Ÿ. ð¶), universal restriction (âˆ€ð‘Ÿ. ð¶), etc."
- Break condition: If the complex concepts are too rare or too abstract, models may not learn meaningful patterns for placement

## Foundational Learning

- Concept: OWL ontology structure and Description Logic
  - Why needed here: Understanding the distinction between atomic and complex concepts, and how subsumption axioms work, is essential for implementing the benchmark
  - Quick check question: What's the difference between an atomic concept and a complex concept in OWL?

- Concept: KB versioning strategy
  - Why needed here: The method of synthesizing out-of-KB entities by comparing ontology versions is central to the dataset construction
  - Quick check question: How does comparing two ontology versions help identify new concepts?

- Concept: Entity linking and mention-to-edge mapping
  - Why needed here: Converting mentions in text to specific edges in the ontology is the core transformation required for the benchmark
  - Quick check question: What information is needed to map a text mention to a specific edge in an ontology?

## Architecture Onboarding

- Component map: KB selection -> KB versioning -> Edge extraction -> Mention-edge creation -> Evaluation framework (Out-of-KB discovery metrics -> Concept placement ranking metrics)
- Critical path: The most important flow is from raw text mentions -> contextual embedding -> edge ranking -> final placement decision
- Design tradeoffs: The dataset trades off complexity (complex concepts) against tractability (mostly atomic concepts), and generality (broad categories) against specificity (disease-focused subsets)
- Failure signatures: Poor performance on out-of-KB discovery suggests context windows are inadequate; poor concept placement suggests the embedding space doesn't capture ontological relationships well
- First 3 experiments:
  1. Evaluate baseline Sieve-based method on out-of-KB mention discovery to establish floor performance
  2. Test bi-encoder approach for candidate generation in concept placement
  3. Implement cross-encoder with NIL representation to improve out-of-KB detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based methods be improved to achieve better performance on out-of-KB mention discovery and concept placement tasks?
- Basis in paper: [explicit] The paper states that LLM-based methods outperform rule-based ones but still struggle with out-of-KB mention discovery and concept placement tasks.
- Why unresolved: The paper provides benchmarking results showing that LLM-based methods are yet to achieve satisfying results on the proposed benchmark. However, it does not explore potential improvements or modifications to these methods.
- What evidence would resolve it: Developing and testing new LLM architectures, prompting strategies, or fine-tuning techniques specifically designed for out-of-KB mention discovery and concept placement tasks, and comparing their performance against the baseline methods presented in the paper.

### Open Question 2
- Question: How can the proposed dataset be extended to include more diverse biomedical domains and knowledge bases?
- Basis in paper: [inferred] The paper focuses on the biomedical domain and uses SNOMED CT ontology. However, it mentions that the data construction method can be applied to other KBs in the biomedical domain and KBs in various domains.
- Why unresolved: The paper does not explore the feasibility of extending the dataset to other domains or knowledge bases, nor does it discuss potential challenges or limitations in doing so.
- What evidence would resolve it: Conducting case studies on applying the data construction pipeline to other biomedical knowledge bases (e.g., UMLS, MeSH) or non-biomedical domains (e.g., general knowledge, specific industry domains) and analyzing the results in terms of data quality, task performance, and potential domain-specific challenges.

### Open Question 3
- Question: How can the proposed benchmark be used to drive the development of more effective ontology enrichment methods?
- Basis in paper: [explicit] The paper introduces a new benchmark for ontology enrichment from texts, aiming to advance research in this area by providing a comprehensive and challenging dataset.
- Why unresolved: While the paper provides usage instructions and benchmarking results, it does not discuss how the proposed benchmark can be leveraged to guide the development of novel ontology enrichment methods or evaluate their effectiveness.
- What evidence would resolve it: Conducting a series of studies that use the proposed benchmark to evaluate and compare various ontology enrichment methods, identifying their strengths and weaknesses, and using the insights gained to propose new methods or improve existing ones based on the observed challenges and limitations.

## Limitations

- The benchmark focuses primarily on SNOMED CT ontology, limiting applicability to other ontology types
- LLM-based methods struggle significantly with out-of-KB mention discovery, achieving F1 scores well below 50%
- The dataset's distribution of complex concepts remains unclear, making it difficult to assess the true challenge level

## Confidence

- KB versioning for out-of-KB discovery: High
- Contextual terms for mention disambiguation: High
- Complex concept handling: Medium
- LLM-based method performance: Medium
- Scalability to full-scale ontologies: Low

## Next Checks

1. **Cross-domain validation**: Test the benchmark's methods on a different biomedical corpus (e.g., clinical notes or drug labels) to assess generalizability beyond PubMed abstracts.

2. **Complexity analysis**: Conduct a detailed analysis of which types of complex concepts (existential restrictions vs. conjunctions vs. disjunctions) are most challenging for current methods.

3. **Context sensitivity evaluation**: Systematically vary context window sizes to determine the optimal amount of context needed for accurate mention disambiguation and placement.