---
ver: rpa2
title: Identifying Context-Dependent Translations for Evaluation Set Production
arxiv_id: '2311.02321'
source_url: https://arxiv.org/abs/2311.02321
tags:
- pnoun
- noun
- sing
- english
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CTXPRO, a pipeline that automatically identifies\
  \ context-dependent sentences in parallel corpora for five phenomena: gender, formality,\
  \ animacy for pronouns, verb phrase ellipsis, and ambiguous noun inflections. Using\
  \ linguistically-informed rules and state-of-the-art NLP tools, CTXPRO extracts\
  \ evaluation sets in seven language pairs (EN\u2194DE, ES, FR, IT, PL, PT, RU) from\
  \ OpenSubtitles and WMT test sets."
---

# Identifying Context-Dependent Translations for Evaluation Set Production

## Quick Facts
- arXiv ID: 2311.02321
- Source URL: https://arxiv.org/abs/2311.02321
- Reference count: 5
- Key outcome: CTXPRO pipeline automatically identifies context-dependent sentences for five phenomena across seven language pairs, showing that commercial contextual MT systems significantly outperform sentence-level systems on extracted evaluation sets

## Executive Summary
This paper addresses the critical gap in evaluation resources for context-aware machine translation by introducing CTXPRO, a pipeline that automatically identifies context-dependent sentences in parallel corpora. The pipeline targets five key phenomena—gender, formality, animacy for pronouns, verb phrase ellipsis, and ambiguous noun inflections—across seven language pairs (EN↔DE, ES, FR, IT, PL, PT, RU). Using linguistically-informed rules and state-of-the-art NLP tools, CTXPRO extracts evaluation sets from OpenSubtitles and WMT test sets, with manual validation showing 92% precision on French gender examples. Generative evaluation demonstrates that a commercial contextual MT system significantly outperforms its sentence-level counterpart on CTXPRO data, validating the pipeline's effectiveness for detecting context-aware translation improvements.

## Method Summary
CTXPRO uses a rule-based pipeline that combines coreference resolution, word alignment, and morphological analysis to identify context-dependent sentences. For each target language and phenomenon, hand-crafted rules specify features that source and target tokens must satisfy, along with criteria for their antecedents. The pipeline applies FastCoref for coreference resolution, simalign for cross-lingual word alignment, and SpaCy for morphological feature extraction. It processes parallel corpora to find sentences where the correct translation requires information from surrounding context, extracting evaluation sets that can discriminate between contextual and sentence-level MT systems. The approach is validated through manual review and generative evaluation comparing contextual versus sentence-level translation accuracy.

## Key Results
- Manual validation shows 92% precision on French gender examples extracted by CTXPRO
- Commercial contextual MT system significantly outperforms sentence-level counterpart: +25.2 accuracy points on German gender, +32.2 on German auxiliary
- Automatic COMET scores improve with context-aware translation, confirming qualitative findings
- CTXPRO successfully extracts evaluation sets across seven language pairs for five phenomena from both OpenSubtitles and WMT test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based extraction successfully identifies context-dependent sentences challenging for sentence-level MT
- Mechanism: Linguistically-informed rules use morphological features and coreference resolution to select sentence pairs requiring context for correct translation
- Core assumption: State-of-the-art NLP tools accurately identify relevant features for each phenomenon
- Evidence anchors: Abstract states tools are used; section 3 details specific tools; corpus analysis shows limited related work
- Break condition: If NLP tools have high error rates for target languages/phenomena, extracted examples will contain errors and fail to discriminate between systems

### Mechanism 2
- Claim: Context-aware MT systems significantly outperform sentence-level systems on CTXPRO evaluation sets
- Mechanism: Providing context allows MT systems to resolve ambiguities that cannot be resolved from sentence alone
- Core assumption: Commercial DeepL system actually leverages context when provided
- Evidence anchors: Abstract confirms significant performance improvements; section 4.1 shows consistent gains; corpus lacks evidence about DeepL's context usage
- Break condition: If evaluated system doesn't use context or test set lacks genuine context-dependent examples, no improvement will be observed

### Mechanism 3
- Claim: Extraction pipeline generalizes to multiple languages and phenomena
- Mechanism: Pipeline architecture separates token identification from phenomenon-specific criteria, allowing new rules without changing core logic
- Core assumption: Core extraction logic is language-agnostic and works across target language pairs
- Evidence anchors: Abstract describes per-language rules; section 3 explains rule framework; corpus analysis shows limited related work
- Break condition: If core extraction logic fails for certain language pairs, pipeline cannot be easily extended

## Foundational Learning

- **Concept: Coreference resolution**
  - Why needed here: To identify antecedents of pronouns and referring expressions that provide context for correct translation
  - Quick check question: What is the difference between anaphora and cataphora, and which does this work focus on?

- **Concept: Word alignment**
  - Why needed here: To identify correspondence between source and target language tokens for finding context-providing tokens
  - Quick check question: What are advantages and disadvantages of statistical vs. neural word alignment methods for this task?

- **Concept: Morphological analysis**
  - Why needed here: To check grammatical features (gender, case, number, person) ensuring tokens meet criteria for each phenomenon
  - Quick check question: How does morphological analysis differ from part-of-speech tagging, and why is it necessary for this work?

## Architecture Onboarding

- **Component map**: Parallel corpus -> FastCoref (coreference) -> simalign (alignment) -> SpaCy (morphology) -> Rule-based extraction -> Context-dependent evaluation sets

- **Critical path**: 1) Load parallel corpus and preprocess with SpaCy 2) Apply word alignment to identify Ts-Tt pairs 3) For each Ts-Tt pair, use coreference resolver to find Cs 4) Check Cs against rules and find Ct via alignment 5) Check Ct against rules and output if all criteria met

- **Design tradeoffs**: Rule-based extraction vs. machine learning (rules are interpretable but may miss edge cases); strict vs. lenient criteria (stricter produces cleaner data but fewer examples); context window size (larger windows capture more context but increase computational cost and alignment difficulty)

- **Failure signatures**: Low extraction rate (may indicate overly strict rules or NLP tool issues); high error rate in manual review (may indicate coreference, alignment, or morphological analysis issues); no improvement with context (may indicate system doesn't use context or test set lacks genuine context-dependent examples)

- **First 3 experiments**: 1) Run pipeline on small OpenSubtitles subset for one language pair and manually review extracted examples to estimate error rate 2) Translate sample of extracted examples with and without context using DeepL API and compare accuracy 3) Vary strictness of one rule (e.g., allow longer coreference distances) and measure impact on extraction rate and error rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do annotation errors in automatic coreference resolution and morphological feature extraction affect validity of CTXPRO evaluation sets, and can their impact be quantified?
- Basis in paper: [explicit] Pipeline relies on tools like FastCoref and SpaCy which introduce noise; manual validation shows 92% precision but identifies alignment and coreference errors
- Why unresolved: Paper provides only qualitative examples and small-scale manual review (100 examples) without systematic error analysis across all languages and phenomena
- What evidence would resolve it: Comprehensive error analysis quantifying frequency and impact of annotation errors across all CTXPRO evaluation sets using human annotations or improved tools to measure impact on MT evaluation results

### Open Question 2
- Question: Would performance gap between sentence-level and context-aware MT systems persist across different contextual MT architectures beyond DeepL system evaluated?
- Basis in paper: [explicit] DeepL's contextual system significantly outperforms sentence-level counterpart on CTXPRO data, but acknowledges only one contextual system tested due to availability constraints
- Why unresolved: Evaluation limited to single commercial system; paper explicitly notes lack of contextual MT models across languages for comparison
- What evidence would resolve it: Comparative evaluation using multiple contextual MT systems (commercial and research) across all seven language pairs, measuring whether performance gap is consistent or varies by architecture

### Open Question 3
- Question: How does phenomenon distribution in WMT newswire test sets compare to conversational domain of OpenSubtitles, and what implications does this have for evaluating contextual MT in different domains?
- Basis in paper: [explicit] Paper applies pipeline to WMT test sets, observes different distributions (higher GENDER but smaller FORMALITY and AUXILIARY rates), with severe formality bias toward formal register
- Why unresolved: Paper presents only raw counts and speculation about domain characteristics without deeper analysis of how distributional differences affect contextual MT evaluation
- What evidence would resolve it: Detailed analysis of how different phenomenon distributions across domains affect contextual MT system performance, potentially including domain adaptation studies or creation of balanced test sets maintaining realistic distributions while ensuring sufficient examples

## Limitations

- Quality and reliability of underlying NLP tools across all seven language pairs remains uncertain, particularly for morphologically rich languages
- Reliance on single commercial MT system (DeepL) without transparency into actual context usage mechanisms limits generalizability
- Limited prior work in this specific area (only 5 related papers found) makes benchmarking against established methods difficult

## Confidence

- **High Confidence**: Core mechanism of using linguistically-informed rules to identify context-dependent sentences is well-established and theoretically sound; 92% precision on French gender examples provides strong empirical support
- **Medium Confidence**: Claim that context-aware MT systems significantly outperform sentence-level systems is supported by generative evaluation results, but reliance on single commercial system limits generalizability
- **Low Confidence**: Generalizability across all seven language pairs is questionable without detailed error analysis for each language; limited prior work makes benchmarking difficult

## Next Checks

1. **Tool Performance Validation**: Conduct error analysis of FastCoref, simalign, and SpaCy performance on held-out validation set for each language pair to quantify impact of tool errors on final extraction quality, including coreference resolution accuracy, alignment precision, and morphological analysis accuracy

2. **Cross-System Evaluation**: Replicate generative evaluation using at least two additional commercial contextual MT systems (e.g., Google Translate with document-level context, Microsoft Translator) to verify whether observed improvements are consistent across systems or specific to DeepL's implementation

3. **Context Window Sensitivity**: Systematically vary context window size (e.g., 1 sentence vs. 3 sentences of context) and measure impact on both extraction rate and translation quality improvements to reveal whether current 2-sentence window is optimal or whether larger contexts provide additional benefits for certain phenomena or language pairs