---
ver: rpa2
title: 'DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing in
  Multi-task Learning'
arxiv_id: '2310.11777'
source_url: https://arxiv.org/abs/2310.11777
tags:
- dcrnn
- parameter
- feature
- learning
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DCRNN, a novel multi-task learning model for
  recommendation systems. The key idea is to use RNN to cross-process features and
  implicitly model the sequence correlation between tasks, while also adaptively learning
  original features.
---

# DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing in Multi-task Learning

## Quick Facts
- arXiv ID: 2310.11777
- Source URL: https://arxiv.org/abs/2310.11777
- Reference count: 9
- Key outcome: DCRNN outperforms MMoE baseline on Xiaomi Radio recommendation system with fewer parameters

## Executive Summary
DCRNN introduces a novel multi-task learning approach for recommendation systems that uses RNN-based cross networks with partial parameter sharing. The model addresses limitations of traditional hard and soft parameter sharing by overlapping feature subsequences between tasks while maintaining task-specific modeling capacity. Experiments on Xiaomi Radio's recommendation system and Ali-CCP dataset demonstrate superior performance in both offline (AUC) and online (CTR, VPR) metrics with significantly fewer parameters than MMoE.

## Method Summary
DCRNN employs RNNs to cross-process features and model sequence correlations between tasks, using partial parameter sharing where RNNs share overlapping feature subsequences. The model includes a feature adaptive function that learns optimal feature representations to mitigate noise. Training uses weighted cross-entropy loss to handle class imbalance, with evaluation on click and valid play tasks. The architecture consists of feature sequence construction, RNN processing with overlapping subsequences, and task-specific towers for predictions.

## Key Results
- DCRNN outperforms MMoE baseline on Xiaomi Radio dataset (320M samples, 11M users, 140K albums)
- Achieves better AUC scores for both click and valid play tasks
- Shows improved online metrics (CTR, VPR) with fewer parameters than MMoE
- Validated on Ali-CCP dataset with approximately 42 million data points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial parameter sharing enables selective feature sequence overlap between tasks while maintaining task-specific modeling capacity
- Mechanism: Overlapping subsequences of feature inputs to each task's RNN share only the most relevant cross-task information while preserving task-specific representations
- Core assumption: Sequence correlation between click and consumption tasks means some feature subsequences are relevant to both tasks
- Evidence anchors: Abstract mentions partial parameter sharing combining advantages of hardware and software sharing; section discusses adaptive learning of original features

### Mechanism 2
- Claim: Feature adaptive function improves model robustness by learning to denoise input features
- Mechanism: Each feature in input sequence is processed through additive adaptive parameter (Fadd(X0; Ai) = X0 ⊕ Ai) learned during training
- Core assumption: Raw features contain noise from human-designed feature engineering that can be learned away
- Evidence anchors: Section mentions using adaptive function to enhance anti-noise ability by learning adaptive parameters

### Mechanism 3
- Claim: RNN-based cross network provides better feature interaction modeling than traditional DCN/CIN approaches
- Mechanism: When Xt = X0, RNN computation mirrors cross network but with vector-wise interactions rather than scalar multiplication
- Core assumption: Recursive RNN computation captures high-order feature interactions more effectively than fixed cross network architectures
- Evidence anchors: Section shows when Xt = X0, RNN calculation process plays similar role as DCN or CIN

## Foundational Learning

- RNNs and sequence modeling
  - Why needed here: DCRNN relies on RNNs to model both temporal feature sequences and implicitly capture task correlations
  - Quick check question: What is the key difference between how RNNs and traditional cross networks (DCN/CIN) model feature interactions?

- Multi-task learning architectures
  - Why needed here: Understanding tradeoffs between hard parameter sharing, soft parameter sharing, and partial parameter sharing is crucial
  - Quick check question: What are the main limitations of hard and soft parameter sharing that partial parameter sharing aims to address?

- Feature engineering and noise in recommendation systems
  - Why needed here: Feature adaptive function's purpose is to mitigate noise introduced during feature engineering
  - Quick check question: Why might manually engineered features in recommendation systems introduce noise that could harm model performance?

## Architecture Onboarding

- Component map:
  Input features → Feature adaptive function → Feature sequence construction → Multiple RNNs with overlapping subsequences → Task-specific towers → Task outputs

- Critical path:
  1. Feature sequence construction with intervals between subsequences
  2. RNN processing of overlapping subsequences
  3. Feature adaptive parameter learning
  4. Task-specific tower predictions

- Design tradeoffs:
  - Sequence length vs. overlap: Longer sequences capture more context but increase computation
  - Adaptive function vs. raw features: Improves noise robustness but adds parameters and may overfit
  - RNN type (LSTM/GRU/BiLSTM/BiGRU): Affects modeling capacity and computational cost

- Failure signatures:
  - Underfitting: Poor performance on both tasks, suggesting insufficient model capacity
  - Overfitting: Large gap between training and validation performance
  - Degenerate sharing: Similar performance to hard parameter sharing baseline

- First 3 experiments:
  1. Ablation study: Compare DCRNN with and without feature adaptive function
  2. Parameter sharing analysis: Test different interval values (I=0, I=L/2, I=L)
  3. RNN architecture comparison: Evaluate unidirectional vs. bidirectional RNNs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of RNN type (LSTM, GRU, BiLSTM) impact DCRNN performance in terms of CTR and VPR?
- Basis: Paper mentions experiments with different RNN types but doesn't explain performance differences
- Why unresolved: Paper provides results comparing RNN types without analyzing reasons behind differences
- Evidence needed: Experiments investigating characteristics of each RNN type and their impact on DCRNN's performance

### Open Question 2
- Question: How does length of feature sequence and subsequence interval affect DCRNN performance?
- Basis: Paper mentions these as hyperparameters but doesn't analyze their impact
- Why unresolved: No detailed analysis of how different sequence lengths and intervals impact performance
- Evidence needed: Systematic study with varying sequence lengths and subsequence intervals

### Open Question 3
- Question: How does DCRNN compare to other multi-task learning models in terms of performance and scalability?
- Basis: Paper claims partial parameter sharing combines advantages of hard and soft sharing but doesn't compare directly
- Why unresolved: Focuses on comparing with MMoE but not other parameter sharing techniques
- Evidence needed: Experiments comparing DCRNN with hard and soft parameter sharing approaches

## Limitations

- Theoretical claims about RNN-based cross networks superiority lack rigorous mathematical comparison with traditional approaches
- Feature adaptive function mechanism for improving anti-noise ability is not rigorously proven
- Partial parameter sharing effectiveness relies heavily on experimental results without detailed theoretical justification

## Confidence

- High confidence: Experimental results showing DCRNN outperforms MMoE on both offline and online metrics
- Medium confidence: Claimed benefits of partial parameter sharing mechanism
- Low confidence: Theoretical claims about RNN-based cross networks being superior to traditional approaches

## Next Checks

1. Conduct ablation studies comparing DCRNN with standard cross networks (DCN/CIN) using identical parameter sharing configurations
2. Test partial parameter sharing across multiple task correlation scenarios (highly correlated, weakly correlated, uncorrelated)
3. Implement and compare different feature adaptive function designs (differentiable vs. non-differentiable, additive vs. multiplicative)