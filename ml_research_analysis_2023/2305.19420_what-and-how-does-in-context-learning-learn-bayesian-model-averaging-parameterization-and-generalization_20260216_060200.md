---
ver: rpa2
title: What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization,
  and Generalization
arxiv_id: '2305.19420'
source_url: https://arxiv.org/abs/2305.19420
tags:
- have
- arxiv
- where
- proof
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the theoretical foundations of In-Context Learning
  (ICL) in large language models (LLMs). The authors analyze three key questions:
  what type of ICL estimator is learned by LLMs, what performance metrics are suitable
  for ICL, and how the transformer architecture enables ICL.'
---

# What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization

## Quick Facts
- **arXiv ID**: 2305.19420
- **Source URL**: https://arxiv.org/abs/2305.19420
- **Reference count**: 40
- **Primary result**: Perfectly pretrained LLMs implicitly implement Bayesian model averaging in ICL, with attention mechanisms parameterizing this computation

## Executive Summary
This paper provides a theoretical foundation for In-Context Learning (ICL) in large language models by analyzing what ICL estimators are learned, what performance metrics are suitable, and how transformers enable ICL. The authors prove that perfectly pretrained LLMs implicitly implement Bayesian model averaging, where the attention mechanism parameterizes this computation. They establish an O(1/T) regret bound for ICL and analyze the pretraining process, showing that total variation distance between learned and nominal distributions is bounded by approximation and generalization errors. This work bridges the gap between pretraining and ICL performance through a unified theoretical framework.

## Method Summary
The paper analyzes ICL through three lenses: theoretical estimator analysis, performance metrics, and architectural mechanisms. The pretraining phase uses a transformer network parameterized by θ trained via cross-entropy loss on Np trajectories of length Tp. During ICL, prompts containing t examples are processed, and the model predicts P(rt+1|promptt) using attention mechanisms. The analysis employs a PAC-Bayes framework to bound pretraining error as a sum of approximation error (exponentially decaying with depth) and generalization error (sublinearly decaying with pretraining data). The Bayesian model averaging formulation ∫ P(rt+1|˜ct+1, St, z)P(z|St)dz is shown to be approximately implemented by the attention mechanism.

## Key Results
- Perfectly pretrained LLMs implicitly implement Bayesian model averaging in ICL without updating parameters
- Attention mechanism parameterizes Bayesian model averaging computation, converging to posterior as sequence length grows
- O(1/T) regret bound established for ICL, where T is the number of examples
- Pretraining error bounded by approximation error (exponentially decaying with depth) and generalization error (sublinearly decaying with pretraining data)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Perfectly pretrained LLMs perform Bayesian model averaging in ICL.
- **Mechanism**: The model implicitly computes the posterior over hidden concepts z given the prompt examples, then averages predictions weighted by this posterior.
- **Core assumption**: The underlying data generation follows the model rt = f(˜ct, ht, ξt) where ht forms a stochastic process determined by z*.
- **Evidence anchors**:
  - [abstract]: "we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm"
  - [section 4.1]: Proof in Theorem 4.1 derives P(rt+1|promptt) = ∫ P(rt+1|˜ct+1, St, z)P(z|St)dz
  - [corpus]: Weak evidence - related papers focus on ICL mechanisms but don't explicitly discuss Bayesian averaging
- **Break condition**: If the true data generation doesn't fit the latent variable model structure, or if pretraining doesn't capture the true P(·|S,z) distributions.

### Mechanism 2
- **Claim**: Attention mechanism parameterizes the Bayesian model averaging computation.
- **Mechanism**: The attention weights approximate the posterior P(z|St) as the sequence length grows, with the attention output approximating the conditional mean embedding.
- **Core assumption**: The value-key pairs (vt, kt) are i.i.d. and the kernel K satisfies certain regularity conditions.
- **Evidence anchors**:
  - [abstract]: "the attention mechanism in the transformer parameterizes the Bayesian model averaging"
  - [section 4.1]: Proposition 4.3 proves attn†(q,K,V) converges to attn(q,K,V) as T→∞ under Gaussian RBF kernel
  - [corpus]: Weak evidence - no direct mention of attention-Bayesian connection in related papers
- **Break condition**: If the key-value pairs aren't i.i.d., or if the kernel doesn't satisfy the conditions, or for short sequences where asymptotic behavior doesn't hold.

### Mechanism 3
- **Claim**: Pretraining error bounds ICL regret through a PAC-Bayes framework.
- **Mechanism**: The total variation distance between learned model Pˆθ and nominal distribution P decomposes into approximation error (vanishes exponentially with depth) and generalization error (decays sublinearly with pretraining data).
- **Core assumption**: The transformer function class can approximate the nominal distribution with controlled capacity, and pretraining follows maximum likelihood.
- **Evidence anchors**:
  - [abstract]: "the error of pretrained model is bounded by a sum of an approximation error and a generalization error"
  - [section 5]: Theorem 5.3 provides the TV bound and Proposition 5.4 shows exponential decay of approximation error
  - [corpus]: Weak evidence - related papers study ICL but don't analyze pretraining-error-to-ICL-regret connection
- **Break condition**: If the transformer cannot approximate the nominal distribution well enough, or if pretraining data is insufficient for generalization.

## Foundational Learning

- **Concept**: Bayesian model averaging
  - Why needed here: Forms the theoretical foundation for why ICL works - the model is averaging predictions over latent concepts weighted by posterior probabilities
  - Quick check question: What does Bayesian model averaging compute in the ICL setting? (Answer: ∫ P(rt+1|˜ct+1, St, z)P(z|St)dz)

- **Concept**: Conditional mean embedding in RKHS
  - Why needed here: Provides the mathematical bridge between attention mechanism and Bayesian inference - attention approximates the empirical conditional mean embedding
  - Quick check question: What is the relationship between attn†(q,K,V) and CME(q,PK,V)? (Answer: attn† approximates CME, and converges to the same limit as softmax attention)

- **Concept**: PAC-Bayes framework for generalization bounds
  - Why needed here: Enables rigorous analysis of how pretraining error (approximation + generalization) translates to ICL regret bounds
  - Quick check question: In the PAC-Bayes bound, what are the two sources of error that contribute to pretraining error? (Answer: approximation error and generalization error)

## Architecture Onboarding

- **Component map**: Token sequence → Transformer layers (MHA + FFN) → Attention mechanism → Output distribution
- **Critical path**: Token sequence → Transformer layers (MHA + FFN) → Attention mechanism → Output distribution
  - The attention mechanism is the critical component that implements Bayesian averaging

- **Design tradeoffs**:
  - Depth vs approximation error: Deeper networks reduce approximation error exponentially but increase computational cost
  - Pretraining data size vs generalization error: More data reduces generalization error as O(1/√N) but increases resource requirements
  - Temperature τ in softmax: Lower values make attention more peaked but can cause numerical instability

- **Failure signatures**:
  - Poor ICL performance despite good pretraining loss → Attention not capturing Bayesian averaging correctly
  - ICL performance degrades with more examples → Attention weights not properly approximating posterior
  - ICL works on pretraining distribution but fails on out-of-distribution prompts → Assumption 6.1 violated

- **First 3 experiments**:
  1. **Verify Bayesian averaging**: Create synthetic data with known latent concepts, run ICL, check if predictions match Bayesian model averaging formula
  2. **Test attention parameterization**: Measure how attention weights change with sequence length, verify convergence to posterior
  3. **Analyze pretraining impact**: Train transformers of varying depths on same data, measure how approximation error affects ICL regret

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the pretraining dataset and the complexity of the transformer architecture (e.g., depth, width) affect the approximation and generalization errors in ICL?
- Basis in paper: [explicit] The paper analyzes the pretraining error as a sum of approximation and generalization errors, where the former decays exponentially with the depth of the transformer and the latter decays sublinearly with the number of tokens in the pretraining dataset.
- Why unresolved: The paper provides theoretical bounds for these errors, but the exact relationship between the size of the pretraining dataset, the complexity of the transformer, and the errors in ICL is not fully characterized.
- What evidence would resolve it: Experimental studies that systematically vary the size of the pretraining dataset and the complexity of the transformer architecture, and measure the corresponding approximation and generalization errors in ICL.

### Open Question 2
- Question: How does the choice of kernel function (e.g., Gaussian RBF kernel) in the attention mechanism affect the performance of ICL?
- Basis in paper: [explicit] The paper shows that the attention mechanism approximately parameterizes the Bayesian model averaging algorithm in ICL, and uses the Gaussian RBF kernel in the proof of Proposition 4.3.
- Why unresolved: The paper focuses on the Gaussian RBF kernel, but other kernel functions may also be suitable for ICL. The impact of different kernel functions on the performance of ICL is not explored.
- What evidence would resolve it: Experimental studies that compare the performance of ICL using different kernel functions in the attention mechanism.

### Open Question 3
- Question: How does the presence of delimiters in the prompt affect the performance of ICL?
- Basis in paper: [explicit] The paper notes that delimiters are omitted in the analysis, but the results can be generalized to handle this case.
- Why unresolved: The impact of delimiters on the performance of ICL is not fully understood. Delimiters may help the model distinguish between examples and queries, but they may also introduce additional complexity.
- What evidence would resolve it: Experimental studies that compare the performance of ICL with and without delimiters in the prompt.

### Open Question 4
- Question: How does the choice of prior distribution for the hidden concept (e.g., uniform distribution) affect the performance of ICL?
- Basis in paper: [explicit] The paper analyzes the ICL regret of the Bayesian model averaging algorithm, which depends on the prior distribution of the hidden concept.
- Why unresolved: The paper focuses on the uniform prior, but other prior distributions may also be suitable for ICL. The impact of different prior distributions on the performance of ICL is not explored.
- What evidence would resolve it: Experimental studies that compare the performance of ICL using different prior distributions for the hidden concept.

### Open Question 5
- Question: How does the presence of noise in the examples (e.g., wrong input-output mappings) affect the performance of ICL?
- Basis in paper: [explicit] The paper analyzes the ICL regret of the pretrained model when the examples contain wrong input-output mappings, and shows that the model can still identify the nominal concept if it is distinguishable from other concepts.
- Why unresolved: The paper provides theoretical guarantees for the case where the nominal concept is distinguishable, but the impact of noise on the performance of ICL in general is not fully characterized.
- What evidence would resolve it: Experimental studies that systematically vary the amount of noise in the examples and measure the corresponding ICL regret.

## Limitations

- Theoretical assumptions (bounded parameters, universal transformer function class, latent concept model) may not hold in practical scenarios
- Exponential decay of approximation error with depth is theoretical and may not manifest in finite-width networks
- Lack of extensive empirical validation to verify theoretical connections between attention mechanisms and Bayesian model averaging

## Confidence

**High Confidence**:
- Mathematical derivations connecting Bayesian model averaging to attention mechanism are rigorous and logical

**Medium Confidence**:
- Claim that perfectly pretrained LLMs implement Bayesian model averaging depends heavily on data generation assumptions
- Attention parameterization holds asymptotically but may have practical limitations for finite sequences

**Low Confidence**:
- Exact translation of theoretical bounds to practical ICL performance without empirical verification
- Robustness of results when assumptions are slightly violated in real-world scenarios

## Next Checks

1. **Empirical Verification of Bayesian Averaging**: Implement synthetic experiments with known latent concepts to verify that attention weights converge to posterior distributions as sequence length increases, testing Proposition 4.3 empirically.

2. **Approximation Error Scaling**: Train transformers of varying depths on controlled synthetic data to empirically measure how approximation error scales with depth, validating the exponential decay claimed in Proposition 5.4.

3. **Generalization Error Bounds**: Conduct experiments measuring ICL regret on both in-distribution and out-of-distribution prompts to test the robustness of the PAC-Bayes bounds and identify failure modes when Assumption 6.1 is violated.