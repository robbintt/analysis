---
ver: rpa2
title: Minimax Weight Learning for Absorbing MDPs
arxiv_id: '2301.03183'
source_url: https://arxiv.org/abs/2301.03183
tags:
- policy
- absorbing
- algorithm
- page
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses off-policy policy evaluation (OPE) in absorbing
  Markov decision processes (MDPs) using truncated episode data. The authors propose
  a Minimax Weight Learning for Absorbing MDPs (MWLA) algorithm that directly estimates
  the expected return via the importance ratio of the state-action occupancy measure.
---

# Minimax Weight Learning for Absorbing MDPs

## Quick Facts
- arXiv ID: 2301.03183
- Source URL: https://arxiv.org/abs/2301.03183
- Reference count: 40
- Key outcome: The paper addresses off-policy policy evaluation (OPE) in absorbing Markov decision processes (MDPs) using truncated episode data, proposing the MWLA algorithm with theoretical MSE bounds and experimental validation.

## Executive Summary
This paper addresses the challenge of off-policy policy evaluation in absorbing Markov decision processes where episodes terminate at random times. The authors propose a Minimax Weight Learning for Absorbing MDPs (MWLA) algorithm that estimates the expected return via the importance ratio of state-action occupancy measures. The key contribution is a theoretical analysis of the mean squared error, showing how statistical errors depend on data size and truncation level, with experimental validation on an episodic taxi environment.

## Method Summary
The MWLA algorithm estimates the expected return by learning the ratio of state-action occupancy measures between target and behavior policies through a minimax optimization framework. Given truncated episode data from a behavior policy, the method constructs empirical estimates of occupancy measures and rewards, then solves a constrained quadratic program to find the importance weights that satisfy the Bellman equation. The approach extends marginalized importance sampling to absorbing MDPs, avoiding the exponential variance growth associated with trajectory-level IS methods. The algorithm can use either tabular function classes or parametric approximations for the occupancy measure ratio.

## Key Results
- The MWLA algorithm demonstrates lower MSE compared to on-policy, naive-average, and MSWLA methods as the number of episodes and truncation length increase.
- Theoretical analysis provides explicit MSE bounds showing the trade-off between truncation level H and sample size m.
- The method successfully handles the random termination time in absorbing MDPs through the proposed truncation-based approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MWLA algorithm reduces variance by using marginalized importance sampling based on the state-action occupancy measure ratio.
- Mechanism: Instead of weighting entire trajectories by the product of importance weights at each step, MWLA estimates the ratio of occupancy measures between target and behavior policies and uses it to reweight rewards, avoiding exponential variance growth with horizon.
- Core assumption: The behavior policy's state-action occupancy measure is strictly positive where the target policy's is positive (Assumption 3.1).
- Evidence anchors:
  - [abstract] "We propose a so-called MWLA algorithm to directly estimate the expected return via the importance ratio of the state-action occupancy measure."
  - [section] "We extend the MWL method to the absorbing MDPs. Our method is essentially based on another occupancy measure defined by the first equation in (2)."
- Break condition: If dπb(s,a)=0 for some (s,a) where dπe(s,a)>0, the ratio is undefined and the method fails.

### Mechanism 2
- Claim: The MSE bound explicitly accounts for truncation error and shows it can be minimized by choosing an appropriate truncation level.
- Mechanism: The error decomposition includes three terms: statistical error (data size dependent), approximation error (function class limitation), and optimization error. The truncation level H controls the statistical error term, which can be optimized to minimize total MSE.
- Core assumption: The termination time T has exponential tail decay with parameter λ0 (Assumption 5).
- Evidence anchors:
  - [section] "We explicitly evaluate the statistical errors by the truncation level and data size, and more importantly, we get the uniformly bound of MSE by optimizing the truncations when the truncation level is relatively large."
  - [section] "Theorem 4.1... MSE mainly consists of three parts: statistical error, approximation error, and optimization error."
- Break condition: If the tail decay assumption is violated or if the function classes cannot approximate the true ratio well, the bound may not hold.

### Mechanism 3
- Claim: The minimax optimization framework ensures consistency of the occupancy measure ratio estimator.
- Mechanism: The estimator solves a minimax problem that enforces the Bellman equation for the occupancy measure ratio, ensuring that the estimated ratio satisfies the underlying MDP structure.
- Core assumption: The function classes W and Q contain the true occupancy measure ratio and the value function respectively.
- Evidence anchors:
  - [section] "Theorem 3.1... wπe/πb is the unique bounded solution to the system of equations L(w,q)=0 for each q∈l2(S0×A)."
  - [section] "An estimator of wπe/πb can then be defined as ˆwm(s,a) = argmin w∈W max q∈Q ˆLm(w,q)2."
- Break condition: If the true ratio or value function lies outside the chosen function classes, the estimator will be biased.

## Foundational Learning

- Concept: Absorbing Markov Decision Processes (MDPs)
  - Why needed here: The paper specifically addresses off-policy evaluation in absorbing MDPs, where episodes terminate at random times when reaching an absorbing state.
  - Quick check question: What distinguishes an absorbing MDP from a standard finite-horizon MDP?

- Concept: Occupancy Measure
  - Why needed here: The MWLA algorithm estimates the ratio of occupancy measures between target and behavior policies to reweight rewards.
  - Quick check question: How is the state-action occupancy measure defined for a policy π?

- Concept: Minimax Optimization
  - Why needed here: The algorithm uses a minimax framework to estimate the occupancy measure ratio, ensuring consistency through the Bellman equation.
  - Quick check question: What is the objective function being minimized and maximized in the MWLA algorithm?

## Architecture Onboarding

- Component map: Data Collection -> Empirical Occupancy Measures -> Minimax Optimization -> Importance Ratio Estimation -> Return Estimation
- Critical path:
  1. Collect truncated episodes from behavior policy
  2. Compute empirical occupancy measures and rewards
  3. Solve the minimax optimization to estimate wπe/πb
  4. Compute the estimated return using the learned ratio
  5. Analyze MSE bounds and compare with baselines
- Design tradeoffs:
  - Truncation level H: Higher H reduces bias but increases variance; lower H does the opposite
  - Function class complexity: More complex classes can better approximate the true ratio but require more data
  - Regularization parameter λ: Helps ensure unique solution but may introduce bias
- Failure signatures:
  - High MSE despite large sample size: Possible function class mismatch or poor choice of H
  - Unstable estimates across runs: Insufficient regularization or ill-conditioned empirical matrices
  - Estimates consistently biased: The true ratio lies outside the chosen function class
- First 3 experiments:
  1. Verify the MWLA algorithm recovers the true ratio in a simple tabular MDP with known behavior policy
  2. Test the sensitivity of MSE to truncation level H on a synthetic absorbing MDP
  3. Compare MWLA performance with on-policy and naive averaging baselines on the taxi environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MWLA algorithm perform compared to other OPE methods when the behavior policy is unknown versus known?
- Basis in paper: [explicit] The paper introduces both MWLA (for unknown behavior policy) and MSWLA (for known behavior policy) algorithms and compares them to on-policy and naive average methods in experiments.
- Why unresolved: The paper only compares these methods in one specific environment (Taxi), so it's unclear how they would perform across different MDPs or with different levels of policy mismatch.
- What evidence would resolve it: Experiments comparing MWLA and MSWLA across multiple benchmark environments with varying levels of behavior policy knowledge and policy mismatch.

### Open Question 2
- Question: What is the impact of the truncation level H on the estimation error when the true trajectory length distribution has heavy tails?
- Basis in paper: [inferred] The theoretical analysis shows that MSE depends on H through terms like e^(-2λ0H) and H^2 ln(m)/m, but assumes a geometric-like tail bound (E[e^(λ0T)] < ∞).
- Why unresolved: Real-world trajectories may have heavier tails than assumed, potentially invalidating the theoretical bounds and affecting the optimal choice of H.
- What evidence would resolve it: Empirical studies on MDPs with heavy-tailed trajectory length distributions, measuring how MSE scales with H and comparing to theoretical predictions.

### Open Question 3
- Question: How does the choice of function class (W, Q) affect the approximation error and overall performance of MWLA?
- Basis in paper: [explicit] The theoretical bounds depend on the pseudo-dimensions of W and Q, and the approximation error min_w max_q L^2(w,q), but the paper only uses tabular function classes in experiments.
- Why unresolved: The paper doesn't explore how different function class choices (e.g., linear, neural network) affect the bias-variance tradeoff or the practical performance of the algorithm.
- What evidence would resolve it: Experiments comparing MWLA with different function classes (linear, polynomial, neural network) on the same MDP, measuring approximation error and MSE.

## Limitations
- The analysis critically depends on the strict positivity assumption for the behavior policy's occupancy measure, which may fail when policies have limited support.
- The exponential tail decay assumption for termination times may not hold in all absorbing MDPs, potentially invalidating theoretical bounds.
- The experimental validation is limited to a single domain (taxi environment), making it difficult to assess generalizability across different MDP structures.

## Confidence

**High Confidence**: The theoretical framework for analyzing MSE decomposition into statistical, approximation, and optimization errors. The minimax optimization formulation for estimating occupancy measure ratios follows established techniques in the literature.

**Medium Confidence**: The practical effectiveness of the MWLA algorithm demonstrated through experiments on the taxi environment. While results show improvement over baselines, the single-domain evaluation limits generalizability claims.

**Low Confidence**: The uniform MSE bound when truncation level is "relatively large" - the paper does not specify what constitutes "relatively large" in practice, and the bound's tightness in realistic scenarios remains unclear.

## Next Checks

1. **Assumption Verification**: Systematically test the strict positivity assumption (Assumption 3.1) across multiple behavior policies and MDP structures to quantify how often this critical assumption fails in practice.

2. **Tail Decay Validation**: Empirically verify the exponential tail decay assumption (Assumption 5) on the termination time distribution across different absorbing MDPs, and characterize the impact when this assumption is violated.

3. **Computational Scalability**: Benchmark the computational complexity of solving the minimax optimization as state-action space size increases, and test the practical limits of the MWLA algorithm on larger MDPs.