---
ver: rpa2
title: 'On the meaning of uncertainty for ethical AI: philosophy and practice'
arxiv_id: '2309.05529'
source_url: https://arxiv.org/abs/2309.05529
tags:
- uncertainty
- modeller
- which
- ethical
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of how to implement ethical AI
  by grounding uncertainty quantification in clear statistical foundations, thereby
  improving transparency and accountability. The authors propose extending Posterior
  Belief Assessment (PBA) to work with complex AI systems, providing a way for modellers
  to express and communicate their beliefs in a principled manner.
---

# On the meaning of uncertainty for ethical AI: philosophy and practice

## Quick Facts
- arXiv ID: 2309.05529
- Source URL: https://arxiv.org/abs/2309.05529
- Reference count: 11
- This paper addresses the problem of how to implement ethical AI by grounding uncertainty quantification in clear statistical foundations, thereby improving transparency and accountability.

## Executive Summary
This paper addresses the problem of how to implement ethical AI by grounding uncertainty quantification in clear statistical foundations, thereby improving transparency and accountability. The authors propose extending Posterior Belief Assessment (PBA) to work with complex AI systems, providing a way for modellers to express and communicate their beliefs in a principled manner. They demonstrate this approach by applying it to models used during the Omicron wave of COVID-19, showing how PBA can synthesize model outputs while highlighting key assumptions and uncertainties. The primary result is that PBA offers a practical method for incorporating ethical considerations into AI development and use, fostering better dialogue between modellers and decision-makers.

## Method Summary
The method extends Posterior Belief Assessment (PBA) to complex AI systems by requiring modellers to elicit prior beliefs about the quantity of interest before observing model outputs. This involves specifying P(X), Var(X), and correlations between the quantity and each model's output. The framework then uses Bayes linear updating to synthesize multiple competing models through belief ownership rather than pure model weighting. The approach separates variance into resolvable and inherent components, distinguishing between uncertainty that can be addressed through models and fundamental uncertainty.

## Key Results
- PBA makes uncertainty ownership explicit by requiring modellers to specify prior beliefs before seeing model outputs
- The framework reduces opacity by synthesizing multiple models through belief ownership rather than simple averaging
- PBA enhances accountability by distinguishing between resolvable and inherent uncertainty
- The COVID-19 case study demonstrates PBA's ability to synthesize model outputs while highlighting key assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Posterior Belief Assessment (PBA) makes uncertainty ownership explicit by requiring modellers to elicit prior beliefs about the quantity of interest before observing model outputs.
- Mechanism: The PBA framework forces modellers to specify P(X), Var(X), and the correlation between X and each model's output. This prior specification defines what uncertainty the modeller owns, making it transparent to users what assumptions underlie the analysis.
- Core assumption: The modeller can meaningfully specify their uncertainty about X before seeing model outputs.
- Evidence anchors:
  - [abstract] "The authors propose extending Posterior Belief Assessment (PBA) to work with complex AI systems, providing a way for modellers to express and communicate their beliefs in a principled manner."
  - [section 2.3] "The prior judgements that the modeller is required to specify for a PBA shed light on aspects of model development...that are often obscured when detailing the use of AI systems."
- Break condition: If the modeller cannot specify meaningful prior beliefs, the PBA framework cannot establish ownership.

### Mechanism 2
- Claim: PBA reduces opacity by synthesizing multiple competing models through belief ownership rather than pure model weighting.
- Mechanism: Instead of simply averaging model outputs, PBA uses orthogonal projection of beliefs onto linear combinations of model outputs, weighted by the modeller's assessed informativeness. This reveals which models the modeller trusts and why.
- Core assumption: Models can be partitioned into co-exchangeable classes based on their expected informativeness.
- Evidence anchors:
  - [abstract] "they demonstrate this approach by applying it to models used during the Omicron wave of COVID-19, showing how PBA can synthesize model outputs while highlighting key assumptions and uncertainties."
  - [section 3.1] "The judgement of within-class exchangeability implies...that individual models within a class are informative for the class mean."
- Break condition: If models cannot be meaningfully partitioned into co-exchangeable classes, the synthesis loses interpretability.

### Mechanism 3
- Claim: PBA enhances accountability by distinguishing between uncertainty resolvable through models and inherent uncertainty.
- Mechanism: The framework separates variance into components: Var(X) = AVar(µ)A⊺ + Var(U), where U represents uncertainty that cannot be resolved by the considered models. This quantifies how much the models actually help resolve uncertainty.
- Core assumption: The discrepancy term U exists and can be meaningfully characterized by the modeller.
- Evidence anchors:
  - [abstract] "PBA offers a practical method for incorporating ethical considerations into AI development and use, fostering better dialogue between modellers and decision-makers."
  - [section 3.1] "Note that ||X − Pt(X)||^2 is a portion of the uncertainty that is present and cannot be resolved via any inferential method."
- Break condition: If U cannot be meaningfully characterized, the framework cannot distinguish between resolvable and inherent uncertainty.

## Foundational Learning

- Concept: Subjectivist probability theory
  - Why needed here: PBA operates from the subjectivist position that probability represents individual belief rather than objective frequency.
  - Quick check question: What is the key difference between subjectivist and frequentist interpretations of probability?

- Concept: Bayes linear statistics
  - Why needed here: PBA is a specific form of Bayes linear update, which updates beliefs using expectations rather than full probability distributions.
  - Quick check question: How does Bayes linear updating differ from traditional Bayesian updating?

- Concept: Co-exchangeability
  - Why needed here: Models are grouped into co-exchangeable classes, meaning their outputs are a priori exchangeable within classes but not across classes.
  - Quick check question: What does it mean for two quantities to be co-exchangeable?

## Architecture Onboarding

- Component map:
  - Elicitation module -> Model output module -> Synthesis module -> Uncertainty decomposition module -> Visualization module

- Critical path:
  1. Elicit prior beliefs about quantity of interest
  2. Collect model outputs from AI systems
  3. Partition models into co-exchangeable classes
  4. Calculate PBA and adjusted variance
  5. Decompose uncertainty and present results

- Design tradeoffs:
  - Granularity vs. tractability: More classes allow finer distinctions but increase elicitation burden
  - Subjectivity vs. objectivity: More subjective judgments increase transparency but may reduce perceived objectivity
  - Resolution vs. interpretability: Higher resolution models provide more detail but may be harder to interpret

- Failure signatures:
  - Inconsistent prior specifications leading to negative variances
  - Poor model partitioning resulting in uninformative synthesis
  - Overconfidence in PBA due to underestimated inherent uncertainty

- First 3 experiments:
  1. Simple case with two Gaussian models and known ground truth
  2. Case with one deterministic and one probabilistic model
  3. Case with multiple models at different spatial resolutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the elicitation of prior quantities for complex AI systems be made more accessible and intuitive for modelers and domain experts?
- Basis in paper: [inferred] The paper acknowledges the difficulty of eliciting coherent covariance/variance matrices directly and suggests methods like the Kadane approach, but also notes that the burden of expert elicitation might make the approach unattractive.
- Why unresolved: The paper presents a case study using the Kadane method and belief separations, but does not provide a comprehensive framework for easy elicitation or compare different elicitation techniques.
- What evidence would resolve it: Development and validation of a standardized, user-friendly elicitation framework that can be applied across different AI systems and domains, along with empirical studies comparing the effectiveness of various elicitation techniques.

### Open Question 2
- Question: How can the properties of the model discrepancy term U and the matrix A in the PBA framework be learned from sequential data, rather than being specified a priori?
- Basis in paper: [explicit] The paper suggests that in applications with sequential data (like the COVID-19 case), the difference between predicted and realized QoIs could provide data for learning A and U, but notes this as an avenue for future research.
- Why unresolved: The paper does not provide a concrete method for learning these quantities from data, only mentions the potential for such an approach.
- What evidence would resolve it: Development of a Bayesian updating framework or other statistical method that can learn A and U from sequential observations of AI model outputs and corresponding QoIs.

### Open Question 3
- Question: How can the ethical implications of different M-open inference methods be compared and evaluated in terms of their impact on decision-making processes?
- Basis in paper: [inferred] The paper discusses the ethical advantages of PBA but acknowledges that other M-open methods (like GBI and model synthesis) also have ethical motivations. It suggests further research is needed to compare these approaches.
- Why unresolved: The paper does not provide a comparative analysis of the ethical implications of different M-open methods or a framework for evaluating their impact on decision-making.
- What evidence would resolve it: Development of a comprehensive framework for evaluating the ethical implications of different M-open inference methods, including case studies comparing their effects on decision-making in real-world scenarios.

## Limitations
- The method requires extensive prior elicitation from modellers, which may be impractical in high-stakes or time-sensitive scenarios
- The framework assumes modellers can meaningfully specify prior beliefs about quantities they cannot directly observe
- The COVID-19 application uses limited time window (December 2021) and specific regional data, constraining generalizability

## Confidence
- High confidence in the theoretical foundations of PBA and its ability to make uncertainty ownership explicit
- Medium confidence in the practical applicability of PBA to real-world AI systems based on the single case study
- Low confidence in claims about PBA's superiority over existing methods without comparative validation

## Next Checks
1. Apply PBA alongside established uncertainty quantification methods (ensemble methods, Bayesian model averaging) on identical datasets to compare performance metrics like calibration and sharpness
2. Conduct a controlled study measuring the time, expertise, and cognitive load required for modellers to specify valid prior beliefs across different domain contexts
3. Develop systematic tests for how PBA outputs vary with different prior specifications and model partitioning strategies, establishing guidelines for robust application