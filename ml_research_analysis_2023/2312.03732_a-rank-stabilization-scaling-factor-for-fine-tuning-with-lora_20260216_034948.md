---
ver: rpa2
title: A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA
arxiv_id: '2312.03732'
source_url: https://arxiv.org/abs/2312.03732
tags:
- lora
- rank
- learning
- adapters
- rslora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the issue of performance collapse in low-rank\
  \ adaptation (LoRA) when using higher ranks during fine-tuning of large language\
  \ models. The core contribution is proving that LoRA adapters should be scaled by\
  \ 1/\u221Ar rather than the standard 1/r, leading to a new method called rank-stabilized\
  \ LoRA (rsLoRA)."
---

# A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA

## Quick Facts
- arXiv ID: 2312.03732
- Source URL: https://arxiv.org/abs/2312.03732
- Reference count: 31
- One-line primary result: Rank-stabilized LoRA (rsLoRA) with scaling factor 1/√r prevents gradient collapse in higher-rank adapters, enabling improved fine-tuning performance while maintaining inference efficiency

## Executive Summary
This work addresses performance collapse in low-rank adaptation (LoRA) when using higher ranks during fine-tuning of large language models. The authors prove that LoRA adapters should be scaled by 1/√r rather than the standard 1/r, introducing rank-stabilized LoRA (rsLoRA). Experiments with Llama 2 and GPT-J models show rsLoRA achieves better perplexity scores and stable gradient norms throughout training, while conventional LoRA shows gradient collapse and similar performance across all ranks.

## Method Summary
The paper introduces rsLoRA by modifying the LoRA scaling factor from 1/r to 1/√r in the adapter computation. The method involves fine-tuning pre-trained LLMs using both LoRA and rsLoRA with ranks {4, 8, 32, 128, 512, 2048} on the OpenOrca instruction tuning dataset. The implementation modifies standard LoRA code to apply the new scaling factor, then trains models using AdamW optimizer with learning rate 0.00005, batch size 32, and context window 512. Adapters are inserted in all linear (non-LayerNorm) attention and feed-forward MLP sub-modules.

## Key Results
- rsLoRA achieves better perplexity scores (1.836 vs 1.863 for rank 2048) compared to standard LoRA
- Gradient norms remain stable throughout training with rsLoRA across all ranks
- rsLoRA unlocks improved fine-tuning performance with higher ranks while maintaining inference efficiency
- The 1/√r scaling factor is necessary regardless of optimizer choice (confirmed with both AdamW and SGD)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rank-stabilized LoRA (rsLoRA) with scaling factor 1/√r prevents gradient collapse in higher-rank adapters
- Mechanism: The 1/√r scaling factor maintains stable moments in both forward and backward passes, ensuring gradients do not vanish or explode as rank increases
- Core assumption: Input data moments are independent of rank r, allowing inductive proof through layers
- Evidence anchors: Abstract states rsLoRA achieves better perplexity scores and stable gradient norms; Theorem 3.2 proves 1/√r is necessary for stability
- Break condition: If input data moments depend on rank or if non-adapter layers introduce rank-dependent effects that violate inductive proof assumptions

### Mechanism 2
- Claim: rsLoRA unlocks improved fine-tuning performance with higher ranks while maintaining inference efficiency
- Mechanism: Higher ranks increase adapter capacity without changing inference cost since adapters are fused with pre-trained weights after training
- Core assumption: Training with larger ranks provides better feature learning when gradients remain stable
- Evidence anchors: Abstract mentions larger ranks can trade increased training resources for better performance; paper shows improved perplexity with rsLoRA at higher ranks
- Break condition: If feature quality plateaus before rank increases provide benefit, or if memory constraints prevent using higher ranks during training

### Mechanism 3
- Claim: The rank-stabilizing factor is necessary regardless of optimizer choice
- Mechanism: The scaling factor affects the learning trajectory fundamentally, not just optimization dynamics
- Core assumption: The stability requirement is inherent to the adapter parameterization, not dependent on specific optimizers
- Evidence anchors: Paper observes same pattern of stability with SGD to control for AdamW effects; claims training with larger ranks should outperform smaller ranks when stable
- Break condition: If specific optimizers introduce rank-dependent dynamics that override the scaling factor's stabilizing effect

## Foundational Learning

- Concept: Matrix rank and low-rank approximation theory
  - Why needed here: LoRA relies on the hypothesis that fine-tuning occurs on a low-dimensional manifold, requiring understanding of when rank-r approximations are sufficient
  - Quick check question: Why does LoRA use low-rank matrices instead of full-rank matrices for adaptation?

- Concept: Gradient dynamics and learning stability
  - Why needed here: The paper analyzes when gradients remain stable across different ranks, requiring understanding of how scaling affects gradient propagation
  - Quick check question: What happens to gradient norms when the scaling factor is too aggressive (1/r) versus appropriately scaled (1/√r)?

- Concept: Statistical moments and their stability properties
  - Why needed here: The proof relies on showing that moments of activations and gradients remain stable (Θr(1)) across ranks
  - Quick check question: Why is it important that the m-th moment of adapter outputs remains Θr(1) for all ranks?

## Architecture Onboarding

- Component map: Pre-trained LLM → LoRA adapter insertion points (attention and MLP layers) → Adapter parameterization (B, A matrices) → Scaling factor (γr) → Training → Fusion with base weights
- Critical path: Input → Base model forward pass → Adapter forward pass → Loss computation → Backward pass through adapters → Parameter update → Repeat until convergence
- Design tradeoffs: Higher rank improves capacity but increases training memory/compute; scaling factor choice affects stability vs learning speed; adapter placement affects performance
- Failure signatures: Gradient collapse (vanishing gradients with high rank), training instability (exploding gradients), performance plateau despite increased rank, memory overflow during training
- First 3 experiments:
  1. Verify gradient stability: Train rsLoRA vs LoRA with increasing ranks and monitor gradient norms throughout training
  2. Validate performance scaling: Compare perplexity scores across ranks for both methods to confirm rsLoRA unlocks higher-rank benefits
  3. Test optimizer independence: Repeat gradient stability experiment with SGD to confirm results are not optimizer-dependent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rank-stabilization scaling factor (1/√r) affect fine-tuning performance when applied to other parameter-efficient fine-tuning methods like Adapters, Prefix Tuning, or P-Tuning?
- Basis in paper: Paper mentions AdaLoRA as potential future work, noting that since AdaLoRA uses the same γr as LoRA while seeking to dynamically allow the use of different rank adapters, optimizing the selection of γr as proposed in this paper can improve upon AdaLoRA.
- Why unresolved: Paper only experimentally validates rsLoRA with LoRA adapters and does not test its application to other PEFT methods.
- What evidence would resolve it: Comparative experiments applying the 1/√r scaling factor to various PEFT methods on the same tasks, measuring performance and gradient stability across different ranks.

### Open Question 2
- Question: What is the relationship between rank-stabilization and the intrinsic dimensionality of the fine-tuning manifold in large language models?
- Basis in paper: Paper discusses how LoRA is based on the hypothesis that fine-tuning occurs on a low-dimensional manifold, and authors suggest their findings motivate further studies into the effects of the learning manifold intrinsic dimensionality in the context of fine tuning.
- Why unresolved: While paper provides theoretical and experimental evidence for rank-stabilization, it does not investigate how this relates to the actual intrinsic dimensionality of the fine-tuning space or whether there is an optimal rank that corresponds to this dimensionality.
- What evidence would resolve it: Analysis of the intrinsic dimensionality of fine-tuning manifolds for different tasks and models, combined with experiments showing how rank-stabilization performance varies with this dimensionality.

### Open Question 3
- Question: What is the impact of rank-stabilization on fine-tuning efficiency when using adaptive optimizers like AdamW or Adafactor compared to standard SGD?
- Basis in paper: Paper includes ablation studies showing rsLoRA maintains gradient stability with both AdamW and SGD, but does not investigate whether the scaling factor affects optimizer convergence rates or overall training efficiency.
- Why unresolved: While gradient stability is maintained across optimizers, paper does not examine whether rsLoRA leads to faster convergence, better generalization, or more efficient use of computational resources during training with different optimizers.
- What evidence would resolve it: Detailed convergence analysis comparing wall-clock time, number of steps to reach target performance, and final generalization metrics for rsLoRA vs LoRA across multiple optimizers.

## Limitations
- Theoretical analysis assumes moments of input data remain rank-independent, which may not hold in practice for all LLM fine-tuning scenarios
- Evaluation limited to specific model architectures (Llama 2 and GPT-J) and datasets (OpenOrca and GSM8k)
- Proof relies on inductive arguments through layers that could break down with complex architectural features like LayerNorm or skip connections

## Confidence

**High confidence**: The empirical observation that standard LoRA suffers from gradient collapse at higher ranks, and that rsLoRA maintains stable gradients across all tested ranks. The experimental results showing improved perplexity scores with rsLoRA at higher ranks are reproducible and consistent across runs.

**Medium confidence**: The theoretical proof that 1/√r scaling is necessary for rank stability. While the mathematical framework is sound, the inductive proof relies on assumptions about moment stability that may not hold in all practical scenarios. The claim that this enables a training/inference compute tradeoff is supported by experiments but requires further validation across diverse use cases.

**Low confidence**: The assertion that rsLoRA's benefits are entirely independent of optimizer choice. The SGD experiments provide some evidence, but more comprehensive testing across different optimizers and hyperparameter settings is needed.

## Next Checks

1. **Cross-architecture validation**: Test rsLoRA on additional LLM architectures (e.g., OPT, BLOOM) and different task types (e.g., summarization, code generation) to assess generalizability of the stability and performance improvements.

2. **Moment analysis in practice**: Empirically measure the moments of activations and gradients across layers during training to verify the rank-independence assumption holds in practice, particularly for models with complex architectural features.

3. **Scalability study**: Systematically evaluate the performance-per-parameter tradeoff across a wider range of ranks (extending beyond 2048) to determine if there are diminishing returns or new stability issues at extreme ranks.