---
ver: rpa2
title: Cultural Bias and Cultural Alignment of Large Language Models
arxiv_id: '2311.14096'
source_url: https://arxiv.org/abs/2311.14096
tags:
- cultural
- page
- values
- gpt-3
- nisbett
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated cultural bias in widely used large language
  models (LLMs) by comparing their responses to nationally representative survey data
  across 107 countries and territories. GPT-4, GPT-3.5, and GPT-3 exhibited cultural
  values resembling English-speaking and Protestant European countries by default.
---

# Cultural Bias and Cultural Alignment of Large Language Models

## Quick Facts
- arXiv ID: 2311.14096
- Source URL: https://arxiv.org/abs/2311.14096
- Reference count: 0
- Primary result: GPT-4, GPT-3.5, and GPT-3 exhibited cultural values resembling English-speaking and Protestant European countries by default; cultural prompting reduced cultural distance for 71-81% of countries

## Executive Summary
This study investigated cultural bias in large language models (LLMs) by comparing their responses to nationally representative survey data across 107 countries and territories. GPT-4, GPT-3.5, and GPT-3 exhibited cultural values resembling English-speaking and Protestant European countries by default. Cultural prompting was tested as a mitigation strategy by instructing models to respond as if from specific countries. For recent models (GPT-4, 3.5), cultural prompting improved cultural alignment for 71-81% of countries, reducing average cultural distance from 2.84 to 1.58 (GPT-3.5) and from 2.53 to 1.52 (GPT-4). The approach was less effective for GPT-3. Cultural prompting is recommended to reduce cultural bias in LLM outputs, especially for high-stakes contexts.

## Method Summary
The study used World Values Survey (WVS) and European Values Study (EVS) data from 107 countries to benchmark LLM cultural values. Ten questions from the Inglehart-Welzel Cultural Map were posed to GPT-3, GPT-3.5, and GPT-4 using both default prompts ("You are an average human being responding to the following survey question") and cultural-specific prompts ("You are an average human being born in [country] and living in [country] responding to the following survey question"). Cultural alignment was measured as Euclidean distance between LLM responses and survey-based cultural values on two dimensions: survival/self-expression and traditional/secular-rational values.

## Key Results
- GPT-3, GPT-3.5, and GPT-4 exhibited default cultural values resembling English-speaking and Protestant European countries
- Cultural prompting reduced average cultural distance from 2.84 to 1.58 for GPT-3.5 and from 2.53 to 1.52 for GPT-4
- Cultural prompting improved alignment for 71-81% of countries with recent models but was less effective for GPT-3
- Cultural prompting showed particular effectiveness in reducing bias along the survival/self-expression dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural prompting reduces cultural bias by instructing the model to respond as if it were a person from a specific country
- Mechanism: The LLM's output is shaped by the prompt's framing, causing it to align more closely with the cultural values of the specified country rather than defaulting to Western cultural norms
- Core assumption: The model has sufficient cultural representation in its training data to generate responses aligned with a wide range of countries when explicitly prompted
- Evidence anchors: Abstract findings showing reduced cultural distance with cultural prompting; section describing how cultural prompting changes the cultural distance between each country's IVS-based values and its GPT-based values
- Break condition: The model lacks sufficient cultural representation in its training data for the target country, or the prompt phrasing fails to activate the desired cultural alignment

### Mechanism 2
- Claim: LLMs exhibit cultural bias by default, reflecting Western cultural values, particularly those of English-speaking and Protestant European countries
- Mechanism: The training data and optimization processes embed cultural values that skew the model's outputs toward Western norms when no specific cultural context is provided
- Core assumption: The training corpus overrepresents Western cultural content, and optimization processes may inadvertently reinforce these biases
- Evidence anchors: Abstract findings showing default cultural values resembling English-speaking and Protestant European countries; section showing default GPT models' cultural values are most similar to countries in the Anglosphere and Protestant Europe
- Break condition: The training data becomes more globally representative, or the model architecture and optimization processes are explicitly designed to mitigate cultural bias

### Mechanism 3
- Claim: The effectiveness of cultural prompting varies across countries and LLM versions, with newer models showing better alignment
- Mechanism: More recent models (GPT-4, GPT-3.5) have been refined through techniques like Reinforcement Learning with Human Feedback (RLHF) and rule-based reward models, improving their ability to represent diverse cultural values when prompted
- Core assumption: Model improvements over time enhance cultural representation capabilities, particularly when combined with explicit prompting strategies
- Evidence anchors: Abstract findings showing cultural prompting improved alignment for 71-81% of countries with recent models but was less effective for GPT-3; section showing GPT-3.5 became substantially more secular relative to GPT-3, and GPT-4 appeared to correct this drastic change
- Break condition: The prompting strategy is ineffective for countries with cultural values that are underrepresented in the training data or when using older model versions

## Foundational Learning

- Concept: Cultural values and their measurement using frameworks like the Inglehart-Welzel Cultural Map
  - Why needed here: Understanding how cultural values are quantified and mapped is essential for evaluating cultural bias and alignment in LLMs
  - Quick check question: What are the two main dimensions of the Inglehart-Welzel Cultural Map, and what do they represent?

- Concept: Large Language Models (LLMs) and their training processes
  - Why needed here: Knowledge of how LLMs are trained and fine-tuned helps explain why they might exhibit cultural biases and how prompting strategies can influence their outputs
  - Quick check question: What is Reinforcement Learning with Human Feedback (RLHF), and how might it affect the cultural values represented in an LLM's outputs?

- Concept: Bias mitigation strategies in AI systems
  - Why needed here: Understanding various approaches to reducing bias, including data curation, model fine-tuning, and prompting techniques, provides context for evaluating the effectiveness of cultural prompting
  - Quick check question: What are some alternative methods to cultural prompting for reducing cultural bias in LLMs, and what are their potential limitations?

## Architecture Onboarding

- Component map: LLMs (GPT-3, GPT-3.5, GPT-4) -> Cultural values benchmark (IVS/WVS data) -> Prompting strategy (default vs. country-specific cultural prompts)
- Critical path: (1) Collect IVS survey data for cultural values across countries, (2) Pose survey questions to the LLM using generic and country-specific prompts, (3) Map LLM responses onto the Inglehart-Welzel Cultural Map, (4) Calculate cultural distance between LLM and IVS-based values, (5) Evaluate the effectiveness of cultural prompting in reducing bias
- Design tradeoffs: Cultural prompting is a simple and flexible approach that doesn't require extensive retraining, but it may not be universally effective across all countries and model versions. More resource-intensive methods like fine-tuning on culturally relevant data could potentially achieve better results but are less accessible
- Failure signatures: Cultural prompting fails when the LLM lacks sufficient cultural representation in its training data for the target country, or when the prompt phrasing doesn't effectively activate the desired cultural alignment
- First 3 experiments:
  1. Evaluate the cultural bias of a new LLM version by comparing its default responses to IVS data across a diverse set of countries
  2. Test the effectiveness of cultural prompting on the new LLM version by posing survey questions with country-specific prompts and measuring changes in cultural alignment
  3. Compare the performance of cultural prompting across different LLM versions (e.g., GPT-4, GPT-3.5, GPT-3) to identify trends in how prompting effectiveness evolves with model improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cultural prompting remain effective as LLMs continue to evolve with new training approaches and model architectures?
- Basis in paper: The paper notes that "The cultural bias we observed may depend on the prompt language and specific phrasing of prompts, which warrants further research to understand the potential implicit impact of prompt design on expressed cultural values" and compares three GPT versions (3, 3.5, 4) showing varying effectiveness
- Why unresolved: The study only examined three consecutive GPT models. As LLMs rapidly advance with new architectures and training methods, the effectiveness of cultural prompting may change unpredictably
- What evidence would resolve it: Longitudinal studies comparing cultural prompting effectiveness across multiple future GPT versions and competing LLM architectures (Anthropic Claude, Google Gemini, etc.) using the same methodology

### Open Question 2
- Question: What is the psychological impact on users who consistently receive LLM outputs that misalign with their cultural values?
- Basis in paper: The paper discusses how LLM outputs may "cause people to inadvertently convey more interpersonal trust, bipartisanship, and support for gender equity in GPT-assisted communication" and that "The use of LLMs in writing cannot only shape the opinions people express, it can also have a short-term effect on their personal beliefs and attitudes"
- Why unresolved: The study focuses on cultural distance metrics but doesn't examine psychological consequences like identity dissonance, reduced trust in technology, or cultural adaptation stress
- What evidence would resolve it: Mixed-methods studies combining quantitative surveys measuring psychological well-being with qualitative interviews exploring user experiences with culturally misaligned LLM outputs

### Open Question 3
- Question: How do cultural values embedded in LLMs affect cross-cultural communication and understanding between users from different cultural backgrounds?
- Basis in paper: The paper mentions potential "interpersonal and professional consequences" but doesn't examine how cultural misalignment affects actual cross-cultural interactions mediated by LLMs
- Why unresolved: The study measures cultural distance but doesn't investigate real-world communication dynamics when users from different cultures interact through culturally biased LLM outputs
- What evidence would resolve it: Experimental studies where participants from different cultural backgrounds use LLMs to communicate with each other, measuring communication effectiveness, perceived understanding, and relationship outcomes

## Limitations
- The study only examined ten survey questions from the Inglehart-Welzel Cultural Map, which may not fully capture cultural complexity
- Results are limited to three OpenAI models and may not generalize to other LLM architectures or training approaches
- The effectiveness of cultural prompting could vary significantly for real-world applications beyond survey responses

## Confidence
- **High Confidence**: The core finding that LLMs exhibit cultural bias toward Western values by default, as evidenced by the clear mapping of default model responses onto the Inglehart-Welzel Cultural Map
- **Medium Confidence**: The effectiveness of cultural prompting as a mitigation strategy, particularly for GPT-4 and GPT-3.5, though results may vary across different cultural dimensions and country contexts
- **Low Confidence**: The claim that cultural prompting is a universally applicable solution, as the study acknowledges it was less effective for GPT-3 and may not work equally well for all countries or cultural dimensions

## Next Checks
1. Replicate the study using a broader set of cultural assessment tools (e.g., Hofstede's cultural dimensions, Schwartz Value Survey) to verify whether cultural prompting effectiveness holds across different cultural frameworks
2. Test the cultural prompting approach with additional LLM architectures (Anthropic, Google, open-source models) to assess whether the mitigation strategy is architecture-agnostic
3. Conduct a longitudinal study evaluating how cultural alignment evolves as models receive updates and new training data, particularly focusing on whether improvements persist over time or require periodic retraining