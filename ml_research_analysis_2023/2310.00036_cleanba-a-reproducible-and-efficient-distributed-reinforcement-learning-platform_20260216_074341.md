---
ver: rpa2
title: 'Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform'
arxiv_id: '2310.00036'
source_url: https://arxiv.org/abs/2310.00036
tags:
- policy
- version
- actor
- rollout
- after
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Cleanba, a new open-source platform for distributed
  deep reinforcement learning (DRL) that aims to address reproducibility issues present
  in typical actor-learner frameworks like IMPALA. The key idea is a more reproducible
  architecture where the learner's updates are synchronized with the actors to avoid
  non-deterministic policy updates mid-trajectory.
---

# Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform

## Quick Facts
- arXiv ID: 2310.00036
- Source URL: https://arxiv.org/abs/2310.00036
- Reference count: 40
- Key outcome: Cleanba achieves equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL, with shorter training time and more reproducible learning curves across different hardware settings.

## Executive Summary
Cleanba introduces a new open-source platform for distributed deep reinforcement learning that addresses reproducibility issues in typical actor-learner frameworks like IMPALA. The key innovation is a more reproducible architecture where the learner's updates are synchronized with the actors to avoid non-deterministic policy updates mid-trajectory. Cleanba implements highly optimized distributed variants of PPO and IMPALA using JAX and EnvPool. Experiments on 57 Atari games show that Cleanba variants achieve equivalent or higher scores than strong baselines while being 6.8x faster and exhibiting more reproducible learning curves across different hardware settings.

## Method Summary
Cleanba's architecture prevents mid-trajectory policy updates by enforcing second-latest policy usage through blocking queue operations that synchronize actors and learners. The system decouples the number of environments from the number of CPUs using EnvPool to simulate environments on GPUs, removing hardware-dependent variability in actor thread scheduling. This allows parallel actor-learner computation without sacrificing data efficiency for IMPALA, though PPO shows lower data efficiency when using stale data. The implementation uses JAX for distributed computing and EnvPool for GPU-based environment simulation, training for 200M frames on 57 Atari games with 3 random seeds per experiment.

## Key Results
- Cleanba's IMPALA variant achieves 6.8x faster training than monobeast's IMPALA due to GPU-based actor execution
- Cleanba shows more reproducible learning curves across different hardware configurations (1 GPU vs 8 GPUs) compared to baselines
- Cleanba's PPO and IMPALA variants achieve equivalent or higher scores than strong baselines in moolib, torchbeast, and CleanRL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cleanba's architecture prevents mid-trajectory policy updates by enforcing second-latest policy usage.
- Mechanism: The actor and learner are synchronized via blocking queue operations, ensuring the learner always updates using rollout data from the second-latest policy.
- Core assumption: Blocking queue operations guarantee that the actor does not start a new rollout until the learner has finished its update and vice versa.
- Evidence anchors:
  - [abstract] "the learner's updates are synchronized with the actors to avoid non-deterministic policy updates mid-trajectory."
  - [section 4.2] "Starting iteration i > 3, the learner then learns from the rollout data of the second latest policy πi → Dπi−1 → πi+1."
  - [corpus] Weak or missing - no direct citations found in corpus.
- Break condition: If queue operations are non-blocking or if the actor bypasses the param_Q.get() call during iterations > 2.

### Mechanism 2
- Claim: Decoupling number of environments from number of CPUs removes hardware-dependent variability in actor thread scheduling.
- Mechanism: Cleanba uses EnvPool to simulate a fixed number of environments independent of CPU count, so the same logical workload is maintained regardless of hardware.
- Core assumption: EnvPool can simulate environments on GPUs without introducing scheduling variance tied to CPU thread count.
- Evidence anchors:
  - [section 4.1] "we advocate decoupling the number of actor threads into two separate hyperparameters: 1) the number of environments, and 2) the number of CPUs."
  - [section 5] "Under the base experiments... Cleanba's IMPALA is 6.8x faster than monobeast's IMPALA, mostly because Cleanba actors run on GPUs, whereas monobeast's actors run on CPUs."
  - [corpus] Weak or missing - no direct citations found in corpus.
- Break condition: If EnvPool introduces hidden variability in environment simulation or if the number of environments is not fixed across hardware settings.

### Mechanism 3
- Claim: Cleanba's architecture allows parallel actor-learner computation without sacrificing data efficiency for IMPALA.
- Mechanism: By introducing stale data (second-latest policy), Cleanba enables concurrent actor and learner processes while maintaining data efficiency for IMPALA.
- Core assumption: IMPALA's use of V-trace can tolerate stale data without degrading performance, unlike PPO which suffers from lower data efficiency.
- Evidence anchors:
  - [section 4.2] "Cleanba's architecture can parallelize the actor and learner's computation at the cost of stale data."
  - [section 5.3] "we noticed Cleanba's IMPALA did not suffer from lower data efficiency compared to Cleanba IMPALA (Sync) architecture."
  - [corpus] Weak or missing - no direct citations found in corpus.
- Break condition: If the staleness introduced by Cleanba's architecture exceeds IMPALA's tolerance or if PPO's hyperparameters are tuned to handle stale data better.

## Foundational Learning

- Concept: Actor-Learner architecture in distributed RL
  - Why needed here: Understanding how IMPALA's architecture introduces non-determinism is key to appreciating Cleanba's improvements.
  - Quick check question: What is the primary source of non-determinism in IMPALA's actor-learner architecture?

- Concept: Queue synchronization in concurrent programming
  - Why needed here: Cleanba relies on blocking queue operations to synchronize actors and learners, ensuring deterministic rollout data composition.
  - Quick check question: How do blocking queue operations help in maintaining deterministic policy updates in Cleanba's architecture?

- Concept: Hyperparameter decoupling
  - Why needed here: Decoupling the number of environments from the number of CPUs is a design choice that helps achieve reproducible training across different hardware settings.
  - Quick check question: Why is it important to decouple the number of environments from the number of CPUs in distributed RL systems?

## Architecture Onboarding

- Component map: Actor processes -> param_Q queue -> Policy parameters -> EnvPool simulation -> Rollout data -> rollout_Q queue -> Learner process -> Policy updates -> param_Q queue
- Critical path:
  1. Actor retrieves policy parameters from param_Q
  2. Actor generates rollout data using the retrieved policy
  3. Actor sends rollout data to rollout_Q
  4. Learner retrieves rollout data from rollout_Q
  5. Learner updates policy and sends new parameters to param_Q
- Design tradeoffs:
  - Cleanba introduces stale data to enable parallel computation, which may affect data efficiency for some algorithms like PPO
  - Decoupling environments from CPUs simplifies hardware scaling but relies on efficient GPU-based environment simulation
- Failure signatures:
  - If param_Q or rollout_Q operations are non-blocking, it may lead to mid-trajectory policy updates and non-deterministic training
  - If EnvPool fails to simulate environments consistently across different hardware, it may reintroduce hardware-dependent variability
- First 3 experiments:
  1. Verify that the actor retrieves policy parameters from param_Q before generating rollout data
  2. Confirm that the learner updates the policy using rollout data from the second-latest policy
  3. Test the system on different hardware settings to ensure reproducible learning curves

## Open Questions the Paper Calls Out
- How does the Cleanba architecture's synchronization mechanism affect data efficiency compared to other distributed DRL architectures?
- What is the optimal batch size and learning rate for Cleanba's PPO and IMPALA variants across different Atari games?
- How does Cleanba's architecture scale to more complex and diverse environments beyond Atari games?

## Limitations
- The empirical claims are based on a limited comparison set (3 baseline systems) and don't explore whether other distributed RL frameworks might show similar reproducibility improvements
- The focus on Atari games may not generalize to more complex domains with continuous action spaces or real-world applications
- The core mechanism relies heavily on correct queue synchronization, yet the actual implementation details of blocking vs non-blocking queue operations are not fully specified

## Confidence
- High confidence: Cleanba's architecture fundamentally changes the synchronization pattern between actors and learners
- Medium confidence: The performance improvements and training speed gains are robust across hardware configurations
- Low confidence: The claim that stale data doesn't affect data efficiency for IMPALA is fully validated

## Next Checks
1. Implement a controlled experiment where queue operations are made non-blocking to verify that mid-trajectory policy updates reoccur, confirming the mechanism.
2. Test Cleanba architecture with alternative environment simulators beyond EnvPool to validate the decoupling claim across different simulation backends.
3. Compare Cleanba's performance against other recent distributed RL frameworks (e.g., SEED RL, RLlib) that may have addressed similar reproducibility concerns through different mechanisms.