---
ver: rpa2
title: Online learning in bandits with predicted context
arxiv_id: '2307.13916'
source_url: https://arxiv.org/abs/2307.13916
tags:
- regret
- context
- algorithm
- time
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Contextual bandit algorithms typically assume access to exact context
  information, but in many real-world applications, only noisy predictions of the
  context are available. This work proposes the first online algorithm with sublinear
  regret guarantees in this setting, where the agent observes an unbiased noisy version
  of the context with known error variance.
---

# Online learning in bandits with predicted context

## Quick Facts
- arXiv ID: 2307.13916
- Source URL: https://arxiv.org/abs/2307.13916
- Reference count: 40
- Primary result: First online algorithm with sublinear regret for contextual bandits with noisy context observations

## Executive Summary
This paper addresses the challenge of contextual bandit learning when the agent only has access to noisy predictions of the true context, rather than the exact context. Unlike existing approaches that assume perfect context information, this work proposes the first online algorithm with provable sublinear regret guarantees in the presence of measurement error. The key insight is to extend classical measurement error adjustment techniques from statistics to the online decision-making setting, using importance-weighted estimators that correct for the dependence between the policy and the noisy observations.

The proposed Measurement Error Bandit (MEB) algorithm achieves O(T^{2/3}) regret compared to a standard benchmark and O(T^{1/2}) regret compared to a clipped benchmark with minimum exploration probability. The method works for both known and estimated error variance settings, with theoretical guarantees backed by simulations showing superior performance over Thompson sampling and naive measurement error adjustment approaches across various noise distributions and error magnitudes.

## Method Summary
The paper proposes a contextual bandit algorithm that handles noisy context observations through measurement error adjustment. The agent observes an unbiased noisy version of the true context (ext = xt + ϵt) with known or estimable error variance. The algorithm estimates model parameters using an importance-weighted estimator that corrects for the bias introduced by noisy observations and the dependence of the policy on these observations. At each time step, the algorithm computes the best action based on estimated parameters and maintains a minimum exploration probability to balance exploration and exploitation. For the case of estimated error variance, the algorithm controls the weighted average of estimation errors to maintain sublinear regret guarantees.

## Key Results
- First online algorithm with sublinear regret for contextual bandits with noisy context observations
- Achieves O(T^{2/3}) regret compared to standard benchmark and O(T^{1/2}) regret compared to clipped benchmark
- Works for both known and estimated error variance settings
- Outperforms Thompson sampling and naive measurement error adjustment in simulations
- Validated across various noise distributions including normal and heavy-tailed t(3) distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves sublinear regret by extending classical measurement error adjustment techniques to the online setting.
- Mechanism: The proposed estimator uses importance weights to correct for the dependence between the policy and the noisy context observations. By weighting each observation by the ratio of a pre-specified policy to the actual policy, the estimator removes the systematic bias introduced by the noisy context.
- Core assumption: The error term ϵt is unbiased with known or estimable covariance Σe,t, and the importance weights are correctly specified.
- Evidence anchors:
  - [abstract] "The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations."
  - [section 2.3] "Inspired by the above observations, for πτ(a|exτ, Hτ−1) positive, we construct the following estimator for θ∗a given Ht, which corrects (2.5) using importance weights"
- Break condition: If the error is biased or the importance weights are misspecified, the estimator will be inconsistent and the regret will grow linearly.

### Mechanism 2
- Claim: The algorithm maintains sublinear regret by balancing exploration and exploitation through a clipped benchmark policy.
- Mechanism: The algorithm samples the optimal action with probability 1-p0 and explores with probability p0, ensuring that the policy does not collapse to a deterministic strategy that could lead to poor performance if the context noise is large.
- Core assumption: The minimum exploration probability p0 is chosen appropriately to balance exploration and exploitation.
- Evidence anchors:
  - [section 2.4] "At each time t, given the noisy context ext, the algorithm computes the best action eat according to (bθ(t−1)a)a∈{0,1} calculated from (2.6). Then, it simply samples eat with probability 1 − p(t)0 and keeps an exploration probability of p(t)0 to sample the other action."
  - [section 2.2] "Considering these factors, we also set a clipped benchmark policy π†t(a)"
- Break condition: If p0 is too small, the algorithm may not explore enough to learn the true parameters, leading to linear regret. If p0 is too large, the algorithm may explore too much and not exploit the learned information.

### Mechanism 3
- Claim: The algorithm achieves sublinear regret even with estimated error variance by controlling the weighted average of estimation errors.
- Mechanism: The algorithm replaces the true error variance Σe,t with an estimated variance bΣe,t in the estimator (2.8), and the regret bound depends on the weighted average of the estimation errors ∆t(a) = 1t Pτ∈[t] πndτ(a)(bΣe,τ − Σe,τ).
- Core assumption: The estimated error variance is close to the true error variance, i.e., ∥bΣe,t − Σe,t∥2 ≲ 1/√t.
- Evidence anchors:
  - [section 2.5] "Then, it is natural to replace Σe,t with bΣe,t in (2.6) to obtain the estimator for θ∗a at time t"
  - [section 2.5] "This is not a particularly high requirement: For example, suppose the agent gathers more data over time so that ∥bΣe,t − Σe,t∥2 ≲ 1/√t, then it is easy to see that the last term in the regret bound (2.10) ≲ (1−p0)Rθ/√T."
- Break condition: If the estimated error variance is significantly different from the true error variance, the estimator will be biased and the regret will grow linearly.

## Foundational Learning

- Concept: Importance sampling and weighted estimators
  - Why needed here: The algorithm uses importance weights to correct for the bias introduced by the noisy context observations and the dependence of the policy on the observations.
  - Quick check question: Why do we need to use importance weights in the estimator (2.6) instead of just using the naive estimator (2.5)?

- Concept: Martingale difference sequences and concentration inequalities
  - Why needed here: The algorithm uses Freedman's inequality to bound the concentration of the weighted estimators, which relies on the martingale difference property of the weighted observations.
  - Quick check question: What is the martingale difference property of the weighted observations, and how does it allow us to use Freedman's inequality?

- Concept: Sublinear regret bounds and their relationship to estimation error
  - Why needed here: The algorithm achieves sublinear regret by controlling the estimation error of the model parameters, and the regret bound depends on the estimation error through Theorem 2.2.
  - Quick check question: How does the estimation error of the model parameters affect the regret of the algorithm, and why does controlling the estimation error lead to sublinear regret?

## Architecture Onboarding

- Component map:
  Estimator -> Policy -> Regret tracker -> Variance estimator

- Critical path:
  1. Observe noisy context ext and action at-1, reward rt-1
  2. Update the model parameters θ∗a using the estimator (2.6) or (2.8)
  3. Compute the best action eat using the estimated model parameters
  4. Sample action at with probability 1-p0 or explore with probability p0
  5. Compute the instantaneous regret and update the cumulative regret

- Design tradeoffs:
  - Exploration vs exploitation: The minimum exploration probability p0 balances exploration and exploitation, but choosing it too small or too large can lead to poor performance.
  - Estimation accuracy vs computational cost: The algorithm can update the model parameters less frequently to save computation, but this may lead to larger estimation errors and regret.
  - Known vs estimated error variance: The algorithm can use the true error variance if it is known, but if it is estimated, the regret bound depends on the accuracy of the estimate.

- Failure signatures:
  - Linear regret: If the error is biased or the importance weights are misspecified, the estimator will be inconsistent and the regret will grow linearly.
  - High variance: If the error variance is large or the minimum exploration probability is too small, the algorithm may not explore enough to learn the true parameters, leading to high variance in the regret.
  - Poor performance: If the estimated error variance is significantly different from the true error variance, the estimator will be biased and the algorithm may perform poorly.

- First 3 experiments:
  1. Test the estimator (2.6) with synthetic data where the true context is known and the error is unbiased, and compare it to the naive estimator (2.5) in terms of estimation error and regret.
  2. Test the algorithm with different values of the minimum exploration probability p0 and observe how it affects the trade-off between exploration and exploitation.
  3. Test the algorithm with estimated error variance and compare it to the case where the true error variance is known, to see how the accuracy of the estimate affects the performance.

## Open Questions the Paper Calls Out

- **Question**: What is the optimal regret rate compared to the standard benchmark policy (2.4)?
  - **Basis in paper**: [explicit] The paper states "is O(T 2/3) the optimal rate of regret compared to the standard benchmark policy (2.4), as in some other bandits with semi-parametric reward model?"
  - **Why unresolved**: The paper only provides an upper bound of O(T 2/3) for the regret compared to the standard benchmark, but does not prove a matching lower bound to establish optimality.
  - **What evidence would resolve it**: A lower bound proof showing that no algorithm can achieve better than O(T 2/3) regret compared to the standard benchmark policy.

- **Question**: How do biased predictions of the true context affect the results?
  - **Basis in paper**: [inferred] The paper assumes the agent has an unbiased prediction of the true context, but acknowledges that in practice, machine learning algorithms may generate biased predictions.
  - **Why unresolved**: The paper does not analyze the impact of biased context predictions on the regret bounds or the performance of the proposed algorithm.
  - **What evidence would resolve it**: An extension of the analysis that accounts for biased context predictions and quantifies their effect on the regret bounds.

- **Question**: How can the proposed method be extended to more complicated decision-making settings, such as Markov decision processes?
  - **Basis in paper**: [explicit] The paper mentions that "it's interesting to see how we can extend our method to more complicated decision-making settings (e.g. Markov decision processes)."
  - **Why unresolved**: The paper focuses on contextual bandits and does not explore extensions to other decision-making frameworks.
  - **What evidence would resolve it**: A generalization of the proposed algorithm and analysis to Markov decision processes or other relevant decision-making settings.

## Limitations

- Assumes unbiased, independent noise with known or estimable variance, which may not hold in practice
- Importance-weighted estimator can have high variance, particularly in early stages when p0 is small or measurement noise is large
- Clipped benchmark (2.4) is somewhat arbitrary and may not represent the best achievable performance
- Relies on the assumption that the data-independent policy πnd remains positive for all actions

## Confidence

- High confidence: The measurement error adjustment mechanism works under stated assumptions (Section 2.3)
- Medium confidence: Sublinear regret bounds (Theorem 2.1) given estimated error variance (Section 2.5)
- Low confidence: The algorithm's performance in the presence of heavy-tailed noise (Section 3.2)

## Next Checks

1. Test MEB with correlated noise where Cov[ϵt, ϵt-1] ≠ 0 to assess robustness beyond the i.i.d. assumption
2. Implement the MEB-naive-init variant from Appendix A.1 and compare its early-stage performance to standard MEB to isolate the impact of initialization
3. Vary the minimum exploration probability p0 across several orders of magnitude (e.g., 0.01, 0.1, 0.5) to identify the optimal trade-off between exploration and variance in different noise regimes