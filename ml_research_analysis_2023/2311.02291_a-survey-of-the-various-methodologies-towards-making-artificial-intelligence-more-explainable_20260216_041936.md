---
ver: rpa2
title: A Survey of the Various Methodologies Towards making Artificial Intelligence
  More Explainable
arxiv_id: '2311.02291'
source_url: https://arxiv.org/abs/2311.02291
tags:
- explanations
- they
- explanation
- counterfactuals
- lime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews various approaches to making artificial intelligence
  and machine learning models more explainable and interpretable. It covers intrinsic
  and post hoc explanation models, local and global explainability, model-agnostic
  and model-specific methods, and the use of counterfactuals.
---

# A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable

## Quick Facts
- arXiv ID: 2311.02291
- Source URL: https://arxiv.org/abs/2311.02291
- Reference count: 40
- Key outcome: Reviews approaches to explainable AI, covering intrinsic and post hoc models, local/global explainability, model-agnostic vs. model-specific methods, and counterfactuals.

## Executive Summary
This survey reviews a wide range of methodologies for making artificial intelligence and machine learning models more explainable and interpretable. It covers both intrinsic (model-specific) and post hoc (model-agnostic) explanation techniques, distinguishing between local and global explainability, and explores the use of counterfactuals for actionable recourse. The paper highlights the growing importance of explainability as AI systems become more complex and their decisions increasingly impact human lives. Key methods discussed include LIME, SHAP, CLEAR, and FOLD algorithms, each aiming to provide human-interpretable explanations. The survey also addresses the challenges of ensuring recourse in counterfactual explanations and incorporating causality into explanations.

## Method Summary
The survey synthesizes existing literature on explainable AI (XAI), categorizing approaches by their scope (local vs. global), model dependence (agnostic vs. specific), and explanation form (rules, feature attributions, counterfactuals). It reviews algorithmic methods such as SHAP for additive feature attribution, LIME for local surrogate models, and FOLD for rule-based explanations, along with techniques for generating counterfactuals with recourse. The survey does not implement or benchmark these methods but instead provides a structured overview of their theoretical properties, advantages, and limitations, drawing on citations from the XAI literature.

## Key Results
- SHAP values guarantee local accuracy, missingness, and consistency via additive feature attribution.
- Counterfactual explanations can suggest actionable changes but require causal knowledge to ensure feasibility.
- FOLD rules use inductive logic programming to encode defaults and exceptions, offering human-interpretable global explanations.
- Local surrogates (e.g., LIME) may violate consistency or local accuracy compared to unified frameworks like SHAP.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local explainability via additive feature attribution (SHAP) guarantees local accuracy, missingness, and consistency.
- Mechanism: SHAP values assign each feature an attribution that sums to the difference between model output and baseline, ensuring exact reconstruction locally.
- Core assumption: The model's prediction function is additive over features in the local neighborhood.
- Evidence anchors:
  - [abstract]: "SHapley Additive Model Agnostic Explanations(SHAP) [...] guaranteed three properties: (1) Local accuracy of the explanation model"
  - [section]: "Lundberg and Lee [10] introduced a framework to unify the approaches that come under the class of additive feature attribution methods which includes LIME"
- Break condition: The model is highly non-additive locally (e.g., deep interaction effects) or the baseline is poorly chosen.

### Mechanism 2
- Claim: Counterfactual explanations provide actionable recourse by identifying minimal feature changes to flip a prediction.
- Mechanism: Optimize over feature perturbations to find the nearest instance with a different label, using distance and feasibility constraints.
- Core assumption: The causal structure between features is either absent or ignored, so nearest change equals most actionable.
- Evidence anchors:
  - [abstract]: "The survey also addresses the challenges of ensuring recourse in counterfactual explanations"
  - [section]: "Watcher et al. [32] proposed generating counterfactuals that were restricted to differentiable models"
- Break condition: Features are causally interdependent; minimal Euclidean change may require infeasible or very costly interventions.

### Mechanism 3
- Claim: FOLD/FOLD-R++ rules encode human-intelligible defaults and exceptions, making global behavior transparent.
- Mechanism: Inductive Logic Programming learns Horn clauses with defaults and exceptions, reflecting common-sense reasoning.
- Core assumption: The underlying decision logic can be approximated by a small set of interpretable rules.
- Evidence anchors:
  - [section]: "They make use of Inductive Logic Programming (ILP) using Horn clauses that are comprehensible to humans"
  - [corpus]: "Explainable Artificial Intelligence: A Survey of Needs, Techniques, Applications, and Future Direction" (related survey mentions rule-based methods)
- Break condition: Data distribution is too complex for concise rule sets, leading to large, uninterpretable rule sets.

## Foundational Learning

- Concept: Additive feature attribution methods
  - Why needed here: Forms the theoretical foundation for SHAP and its guarantees; ensures local explanations sum to prediction difference.
  - Quick check question: If a model outputs 0.8 for a sample and baseline is 0.2, what must SHAP values sum to?

- Concept: Counterfactual reasoning and recourse
  - Why needed here: Central to actionable explanations; defines how to suggest feasible changes to alter outcomes.
  - Quick check question: What distinguishes a counterfactual explanation from a simple feature importance score?

- Concept: Causality vs. association in feature dependencies
  - Why needed here: Determines whether nearest counterfactual is actually achievable; causal graphs reveal hidden constraints.
  - Quick check question: In a causal graph X→Y→Z, why might changing X directly be cheaper than changing Z to achieve the same outcome?

## Architecture Onboarding

- Component map:
  - Input: Trained black-box model, dataset, feature definitions
  - Explainer: SHAP kernel, LIME local surrogate, or FOLD rule learner
  - Counterfactual generator: Optimization engine with feasibility constraints
  - Evaluator: Fidelity metric, recourse cost, and plausibility checker
  - Output: Human-readable explanations, suggested interventions

- Critical path:
  1. Load model and data
  2. Select explainer (SHAP for global fidelity, LIME for speed, FOLD for rule clarity)
  3. Compute local/global attributions
  4. Generate counterfactuals if recourse is required
  5. Validate explanations against fidelity and plausibility
  6. Present to end user

- Design tradeoffs:
  - SHAP: high fidelity, higher compute; LIME: faster, may violate consistency; FOLD: intuitive, may miss complex patterns
  - Counterfactual: actionable but needs causal knowledge; without it may suggest infeasible changes
  - Rule-based: transparent, but scalability limited; may overfit

- Failure signatures:
  - Low fidelity: Explanation model misaligns with black box locally
  - Poor recourse: Suggested changes violate domain constraints or causal dependencies
  - Uninterpretable rules: FOLD produces too many exceptions or complex conditions

- First 3 experiments:
  1. Run SHAP on a trained logistic regression; verify sum of attributions equals prediction - baseline.
  2. Use LIME on a neural net; measure fidelity via R² between surrogate and black box locally.
  3. Generate counterfactuals for a tabular dataset with known causal edges; compare nearest vs. minimal intervention cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causality be effectively incorporated into common sense reasoning to produce more human-intuitive counterfactual explanations that also suggest possible courses of action?
- Basis in paper: [explicit] The author explicitly states this as their intention: "My intention is to incorporate causality into common sense reasoning to produce more human-intuitive counterfactual explanations both as a means to provide explanations as well as suggest possible courses of action."
- Why unresolved: While the paper discusses the importance of causality in XAI and mentions works like Pearl's structural causal models and Karimi's interventions, it does not provide a concrete method for integrating causality with common sense reasoning to generate counterfactual explanations with actionable suggestions.
- What evidence would resolve it: A proposed framework or algorithm that combines causal reasoning (e.g., using structural causal models) with default theory reasoning (like FOLD) to generate counterfactual explanations that are both intuitive and actionable.

### Open Question 2
- Question: How can the fidelity of local explanation models be improved to ensure they accurately represent the underlying black-box model in the vicinity of individual predictions?
- Basis in paper: [explicit] The paper explicitly mentions this as a shortcoming of LIME: "LIME produces local explanations to achieve global explainability. However, Lundberg and Lee [10] highlighted some of the shortcomings of LIME. Namely, it highlighted how LIME, as described by Ribeiro et al. [21], either violated the property of local accuracy or the property of consistency."
- Why unresolved: While the paper discusses the concept of fidelity and mentions approaches like SHAP and CLEAR that aim to improve local accuracy, it does not provide a definitive solution for ensuring high fidelity in local explanations across all types of complex models.
- What evidence would resolve it: Empirical studies comparing the fidelity of various local explanation methods (e.g., LIME, SHAP, CLEAR) across different types of black-box models and datasets, demonstrating which methods consistently provide high-fidelity local explanations.

### Open Question 3
- Question: How can we develop explanation methods that are both model-agnostic and capable of handling complex data types like images and text, while also providing human-intuitive explanations in the form of defaults and exceptions?
- Basis in paper: [inferred] The paper discusses the need for model-agnostic methods (like LIME and SHAP) and mentions approaches like FOLD that provide human-intuitive explanations using defaults and exceptions. However, it does not present a method that combines these properties.
- Why unresolved: While the paper presents various explanation methods, each has limitations. Model-agnostic methods like LIME and SHAP may not always provide intuitive explanations, while methods like FOLD that use defaults and exceptions are typically model-specific and may struggle with complex data types.
- What evidence would resolve it: A novel explanation method that can handle complex data types, is model-agnostic, and generates explanations using defaults and exceptions, with empirical validation showing its effectiveness across different model types and data domains.

## Limitations
- Rule-based methods like FOLD may not scale to high-dimensional or non-tabular data.
- Counterfactual recourse assumes differentiable models; generalization to non-differentiable architectures is unclear.
- Causal assumptions for recourse are often unstated, risking infeasible intervention suggestions.

## Confidence
- **High**: Existence and basic properties of LIME and SHAP (well-established, cited methods).
- **Medium**: FOLD's interpretability benefits and rule-learning scalability (limited empirical validation).
- **Low**: Ease of ensuring feasible recourse in general (acknowledged but unresolved causal challenges).

## Next Checks
1. Implement SHAP on a tabular dataset and verify that the sum of attributions equals the difference between prediction and baseline within numerical tolerance.
2. Generate counterfactual explanations for a small image classifier; compare the minimal-change counterfactual to one optimized for causal plausibility using a known causal graph.
3. Apply FOLD to a dataset with known interpretable structure (e.g., Iris); measure rule set size and test whether a human can accurately reconstruct predictions from the rules.