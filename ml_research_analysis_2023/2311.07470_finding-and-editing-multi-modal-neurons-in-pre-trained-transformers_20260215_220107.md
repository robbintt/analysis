---
ver: rpa2
title: Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers
arxiv_id: '2311.07470'
source_url: https://arxiv.org/abs/2311.07470
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000052
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new method to identify multi-modal neurons
  in transformer-based multi-modal large language models, which play a critical role
  in bridging visual and textual representations. The method improves efficiency by
  avoiding costly gradient computation and demonstrates three key properties of multi-modal
  neurons: sensitivity, specificity, and causal-effect.'
---

# Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers

## Quick Facts
- **arXiv ID**: 2311.07470
- **Source URL**: https://arxiv.org/abs/2311.07470
- **Reference count**: 24
- **Primary result**: Proposes a new method to identify multi-modal neurons in transformer-based multi-modal models and designs a knowledge editing method to modify specific tokens in model outputs.

## Executive Summary
This paper introduces a novel approach to identify multi-modal neurons in pre-trained transformer models that bridge visual and textual representations. The method improves efficiency by avoiding costly gradient computation through a contribution score mechanism based on the second FFN layer weights and unembedding matrix. The authors demonstrate three key properties of identified multi-modal neurons—sensitivity, specificity, and causal-effect—and propose a knowledge editing method to modify specific tokens, which could help mitigate sensitive words or hallucinations.

## Method Summary
The authors propose a contribution score method to identify multi-modal neurons in pre-trained multi-modal transformers. This method calculates neuron importance by measuring the product of second FFN layer weights and the unembedding matrix, avoiding expensive gradient computation. They validate the identified neurons using three key properties: semantic sensitivity (measured by BERTScore, BLEURT, and MoverScore), position invariance (ratio of invariant neurons), and cross-images invariance (commonality score). Based on these identified neurons, they design a knowledge editing method that modifies second FFN layer weights to achieve targeted token transformations from source to target tokens.

## Key Results
- The proposed contribution score method successfully identifies multi-modal neurons that exhibit sensitivity, specificity, and causal-effect properties
- Experimental results validate the effectiveness of the identification method and show promising targeted editing capabilities
- The approach offers insightful findings for future research on multi-modal neuron interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal neurons can be identified by ranking contribution scores derived from the product of the second FFN layer weights and the unembedding matrix.
- Mechanism: The model's output token probability is decomposed into a sum of layer-wise contributions. Each neuron's contribution to a specific token is measured by how much it influences the final probability distribution through the unembedding matrix.
- Core assumption: The second FFN layer and unembedding matrix together act as a projection from neuron activations to the token vocabulary distribution.
- Evidence anchors:
  - [abstract] "Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination."
  - [section] "We define the contribution score as below: sl_i,t = Ql(i, t)"
  - [corpus] Weak evidence. The corpus contains related works on identifying knowledge neurons but not the specific contribution score method used here.
- Break condition: If the second FFN layer does not project activations to the token vocabulary distribution, the contribution scores would not accurately reflect neuron importance.

### Mechanism 2
- Claim: Multi-modal neurons exhibit sensitivity, specificity, and causal-effect properties.
- Mechanism: Sensitivity means neurons respond strongly to specific visual concepts. Specificity means neurons selectively activate for their designated concepts. Causal-effect means perturbing these neurons changes model output.
- Core assumption: The neurons identified by high contribution scores are functionally important for bridging visual and textual representations.
- Evidence anchors:
  - [abstract] "The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect."
  - [section] "We highlight three critical properties of multi-modal neurons by designing four quantitative evaluation metrics through extensive experiments."
  - [corpus] Moderate evidence. The corpus contains works on identifying knowledge neurons and analyzing neuron properties, supporting the general approach.
- Break condition: If the neurons identified do not actually influence model output or if the properties are not consistently observed across different models and datasets.

### Mechanism 3
- Claim: Targeted editing can be achieved by modifying the second FFN layer weights to change the probability of generating a specific token.
- Mechanism: By adding a delta to the second FFN layer weights, the model's output distribution can be shifted to favor a target token over a source token. This is done by maximizing the difference in probability between the target and source tokens while minimizing the change to other outputs.
- Core assumption: Small changes to the second FFN layer weights can achieve targeted editing without significantly affecting other model outputs.
- Evidence anchors:
  - [abstract] "We propose a knowledge editing method based on the identified multi-modal neurons, achieves a targeted editing from a specific token to another designative token."
  - [section] "Our goal is to make the probability of generating token t1 higher than token t0, which is equivalent to make ol_i wv1 larger than ol_i wv0."
  - [corpus] Moderate evidence. The corpus contains works on model editing but not the specific targeted editing method used here.
- Break condition: If the changes to the second FFN layer weights cause unintended side effects or if the editing is not precise enough to achieve the desired transformation.

## Foundational Learning

- Concept: Contribution score calculation
  - Why needed here: To identify multi-modal neurons based on their importance for specific tokens.
  - Quick check question: How is the contribution score calculated for a neuron at layer l for token t?

- Concept: Neuron properties (sensitivity, specificity, causal-effect)
  - Why needed here: To validate that the identified neurons are functionally important and can be used for targeted editing.
  - Quick check question: What are the three key properties of multi-modal neurons highlighted in the paper?

- Concept: Targeted editing algorithm
  - Why needed here: To modify model parameters to achieve a specific transformation from one token to another.
  - Quick check question: What is the loss function used in the targeted editing algorithm to maximize the probability of the target token over the source token?

## Architecture Onboarding

- Component map:
  - Visual encoder (ViT) -> Projection layer -> LLM (LLaVA/InstructBLIP) -> Output
  - Multi-modal neurons identified in the FFN layers of the LLM
  - Targeted editing applied to the second FFN layer weights

- Critical path:
  - Image input -> Visual features -> Aligned image prompts -> LLM processing -> Token generation
  - Neuron identification -> Property validation -> Targeted editing

- Design tradeoffs:
  - Efficiency vs. accuracy in neuron identification (contribution score vs. gradient computation)
  - Specificity vs. generalization of multi-modal neurons
  - Precision vs. side effects in targeted editing

- Failure signatures:
  - Neurons not identified correctly (low contribution scores or wrong tokens)
  - Properties not validated (neurons not sensitive, specific, or causal)
  - Editing not precise (side effects on other tokens or model outputs)

- First 3 experiments:
  1. Validate neuron identification: Check if top neurons correspond to expected concepts using visualization and textual representations.
  2. Validate neuron properties: Measure sensitivity, specificity, and causal-effect using the proposed metrics.
  3. Validate targeted editing: Apply the editing algorithm and check if the source token is replaced with the target token without significant side effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-modal neurons in transformer-based models behave across different model architectures beyond LLaVA and InstructBLIP?
- Basis in paper: [inferred] The paper acknowledges limitations in only testing on LLaVA and InstructBLIP models, suggesting other transformer-based models may also be explainable by multi-modal neurons.
- Why unresolved: The study focused on two specific models, leaving open whether findings generalize to other architectures or different types of multi-modal models.
- What evidence would resolve it: Systematic testing of multi-modal neurons across a diverse set of transformer-based models (e.g., Flamingo, BLIP-2, CLIP) using the same identification and evaluation methods.

### Open Question 2
- Question: Do multi-modal neurons exhibit properties of sensitivity and specificity when encoding concepts beyond nouns, such as verbs, adjectives, or abstract concepts?
- Basis in paper: [explicit] The authors focused their analysis on nouns, stating "we only consider the role of a single neuron" and "we only focus on nouns" when analyzing multi-modal neurons.
- Why unresolved: The experiments deliberately limited scope to nouns, leaving open whether multi-modal neurons have similar properties when encoding other types of concepts.
- What evidence would resolve it: Extending the identification and evaluation methods to verbs, adjectives, and abstract concepts, then measuring sensitivity and specificity metrics for these concept types.

### Open Question 3
- Question: What is the minimal number of multi-modal neurons required to achieve effective targeted editing, and how does this scale with different types of edits?
- Basis in paper: [inferred] The paper mentions using "top-5 multi-modal neurons" for targeted editing in experiments, but doesn't systematically explore the minimal effective number or scalability.
- Why unresolved: The study used a fixed number of neurons without exploring whether fewer neurons could achieve similar results or how effectiveness changes with different edit types.
- What evidence would resolve it: Systematic experiments varying the number of neurons used for editing across different source-target token pairs and measuring editing success rates and side effects.

### Open Question 4
- Question: How do multi-modal neurons in transformer-based models interact when multiple neurons are activated simultaneously for complex scenes with multiple objects?
- Basis in paper: [explicit] The authors acknowledge this as a limitation: "When analysing multi-modal neurons, we only consider the role of a single neuron. We expect future works can explore how multiple neurons jointly influence the model."
- Why unresolved: The study focused on individual neuron analysis, leaving open how neurons interact in complex visual scenes.
- What evidence would resolve it: Experiments analyzing neuron activation patterns when processing images with multiple objects, and testing whether editing multiple neurons together produces different effects than editing them individually.

## Limitations

- The exact implementation details of the contribution score calculation are unclear, particularly how the Ql matrix is computed from the second FFN layer weights and unembedding matrix.
- Limited quantitative evidence about the precision of targeted editing and its potential side effects on other token probabilities.
- Evaluation is limited to two specific multi-modal models (LLaVA and InstructBLIP) and one dataset, leaving generalizability to other architectures and domains uncertain.

## Confidence

**Multi-Modal Neuron Identification (Medium Confidence)**: The theoretical framework is well-founded, but implementation gaps and limited scope reduce confidence.

**Knowledge Editing Effectiveness (Low-Medium Confidence)**: Theoretically sound but lacks comprehensive quantitative validation of precision and side effects.

**Property Validation (Medium Confidence)**: Innovative metrics show measurable differences, but correlation with real-world model behavior needs further validation.

## Next Checks

**Check 1: Contribution Score Implementation Validation** - Implement the contribution score calculation with detailed documentation of each step, particularly how the Ql matrix is derived from the second FFN layer weights and unembedding matrix. Compare the top-k neurons identified using this implementation against those found using gradient-based attribution methods on a held-out validation set to verify consistency.

**Check 2: Targeted Editing Precision Benchmark** - Design a benchmark dataset with diverse source-target token pairs requiring multi-modal understanding (e.g., replacing "dog" with "cat" in contextually appropriate ways). Measure both the success rate of achieving the desired token replacement and the side effect rate on other token probabilities using automated metrics and human evaluation.

**Check 3: Cross-Domain Generalization Study** - Apply the same methodology to a different multi-modal architecture (e.g., BLIP-2 or Flamingo) and a different dataset (e.g., COCO Captions or Visual Genome). Compare the distribution of contribution scores, the prevalence of identified multi-modal neurons, and the effectiveness of targeted editing across domains to assess generalizability.