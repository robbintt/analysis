---
ver: rpa2
title: Irreducible Curriculum for Language Model Pretraining
arxiv_id: '2310.15389'
source_url: https://arxiv.org/abs/2310.15389
tags:
- curriculum
- training
- irreducible
- language
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IRREDUCIBLE CURRICULUM, a sample-level curriculum
  learning algorithm for language model pretraining. The key idea is to use a small
  proxy model to simulate the training trajectory of the large model, and prioritize
  samples with higher learnability scores, measured as the gap between early-stage
  and late-stage proxy model losses.
---

# Irreducible Curriculum for Language Model Pretraining

## Quick Facts
- arXiv ID: 2310.15389
- Source URL: https://arxiv.org/abs/2310.15389
- Authors: 
- Reference count: 8
- Key outcome: Sample-level curriculum learning improves validation perplexity across 7 domains and reduces network sharpness

## Executive Summary
This paper introduces Irreducible Curriculum, a sample-level curriculum learning algorithm for language model pretraining that prioritizes samples based on learnability scores derived from a proxy model. The method uses a small proxy model to simulate the training trajectory of the large model and prioritizes samples with higher learnability scores, measured as the gap between early-stage and late-stage proxy model losses. Experiments on the RedPajama-1B dataset show consistent improvements in validation perplexity across all 7 domains compared to random uniform baseline and anti-curriculum strategy.

## Method Summary
Irreducible Curriculum uses a small proxy model to estimate the learnability of training samples by computing the loss gap between early and late training stages. The proxy model is trained on a holdout set, and its loss trajectory serves as a surrogate for the large model's dynamics. Samples are then prioritized based on their learnability scores, with high-learnability samples presented earlier in training. This approach avoids expensive forward/backward passes through the large model while maintaining effective curriculum ordering.

## Key Results
- Consistent improvement in validation perplexity across all 7 domains of RedPajama-1B dataset
- Reduction in network sharpness (measured by largest Hessian eigenvalue)
- Better 5-shot accuracy on MMLU benchmarks compared to random and anti-curriculum baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early-stage and late-stage loss gaps from a proxy model predict learnability for large language model samples.
- Mechanism: The proxy model's training loss trajectory is used as a surrogate for the large model's trajectory. Samples with larger loss gaps are prioritized.
- Core assumption: Small proxy models trained on holdout data capture similar loss dynamics as large models on the same data.
- Evidence anchors:
  - [abstract] "measure the loss on each data samples in the training set using the proxy model at different training stages"
  - [section] "We estimate the learnability score of the training samples by the loss gap from the proxy model at early- and late-stage."
  - [corpus] Weak - no direct corpus evidence linking proxy model loss gaps to large model performance.
- Break condition: If proxy model training diverges significantly from large model dynamics, the learnability estimates become unreliable.

### Mechanism 2
- Claim: Prioritizing samples with high learnability scores improves validation perplexity and downstream accuracy.
- Mechanism: Curriculum ordering ensures hard but learnable samples are seen early when the model capacity is not yet saturated.
- Core assumption: Learnability scores align with both model capacity and sample importance.
- Evidence anchors:
  - [abstract] "Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on validation perplexity across all 7 domains"
  - [section] "IRREDUCIBLE CURRICULUM consistently reaches a lower validation perplexity across all 7 domains compared with the uniformly random baseline"
  - [corpus] Weak - corpus lacks ablation showing why learnability specifically drives improvements over random sampling.
- Break condition: If learnability scores do not correlate with sample importance, the curriculum ordering becomes counterproductive.

### Mechanism 3
- Claim: Using a proxy model avoids expensive forward/backward passes through the large model.
- Mechanism: Proxy model computes learnability scores offline, eliminating online computation overhead.
- Core assumption: Proxy model computation is negligible compared to large model training costs.
- Evidence anchors:
  - [abstract] "Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model."
  - [section] "Our proposed IRREDUCIBLE CURRICULUM introduces no extra forward passes through the large language model M(θ)"
  - [corpus] Weak - no corpus evidence quantifying the actual computational savings.
- Break condition: If proxy model computation scales unfavorably with dataset size, the overhead reduction may diminish.

## Foundational Learning

- Concept: Loss landscape and sharpness
  - Why needed here: The paper measures sharpness via Hessian eigenvalues to show curriculum impact.
  - Quick check question: What does a lower largest Hessian eigenvalue indicate about the loss landscape?

- Concept: Proxy model approximation
  - Why needed here: The proxy model is central to estimating learnability without large model passes.
  - Quick check question: Why does training a smaller model on holdout data serve as a proxy for large model dynamics?

- Concept: Curriculum learning ordering
  - Why needed here: The method orders samples by learnability to simulate online data selection.
  - Quick check question: How does ordering samples by learnability differ from domain-level curriculum approaches?

## Architecture Onboarding

- Component map: Proxy model (small-scale) -> learnability score computation -> curriculum ordering -> large model training

- Critical path:
  1. Train proxy model on holdout set
  2. Compute early-stage and late-stage losses for each training sample
  3. Calculate learnability scores
  4. Sort samples and construct curriculum
  5. Train large model following curriculum

- Design tradeoffs:
  - Proxy model size vs. accuracy of learnability estimation
  - Curriculum strictness (λ₀) vs. model convergence speed
  - Global vs. domain-specific curriculum application

- Failure signatures:
  - Validation perplexity plateaus or increases with curriculum
  - Proxy model training diverges from large model dynamics
  - Learnability scores show no correlation with sample importance

- First 3 experiments:
  1. Run baseline random sampling training and record validation perplexity.
  2. Train proxy model and compute learnability scores; verify score distribution.
  3. Train with curriculum (λ₀ = 50%, Tc = 5000) and compare validation perplexity to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does irreducible curriculum improve downstream generalization beyond perplexity on smaller language models?
- Basis in paper: [explicit] The paper shows improvements in MMLU 5-shot accuracy but only tests on a 124M parameter model, suggesting potential for scaling.
- Why unresolved: The authors note resource limitations prevented testing larger models, leaving the scaling behavior unknown.
- What evidence would resolve it: Experiments on models in the 1B-10B parameter range showing consistent MMLU accuracy gains would establish the method's effectiveness at scale.

### Open Question 2
- Question: Is the proxy model approximation reliable across different data distributions and training stages?
- Basis in paper: [inferred] The method assumes proxy model loss gaps predict learnability, but domain analysis shows learnability varies significantly between domains.
- Why unresolved: The paper only evaluates on RedPajama-1B with relatively homogeneous domains, not testing on more diverse datasets.
- What evidence would resolve it: Experiments on datasets with highly varied domain distributions (e.g., multi-lingual, code, scientific text) showing consistent performance would validate the approximation's robustness.

### Open Question 3
- Question: What is the relationship between reduced sharpness and improved generalization in curriculum learning?
- Basis in paper: [explicit] The paper observes lower sharpness with irreducible curriculum but notes the correlation with generalization is unclear.
- Why unresolved: The authors cite ongoing debates about sharpness-generalization relationships and don't establish causation.
- What evidence would resolve it: Ablation studies comparing curriculum methods that reduce sharpness vs. those that don't, while controlling for other factors, would clarify the causal relationship.

## Limitations
- Weak evidence linking proxy model loss gaps to actual large model learnability
- Computational savings not quantitatively validated
- Limited evaluation to single dataset with relatively homogeneous domains

## Confidence
- Medium: Validation perplexity improvements across 7 domains
- Medium: Reduction in network sharpness
- Low: Proxy model as faithful surrogate for large model dynamics
- Low: Computational overhead savings

## Next Checks
1. Run ablation studies comparing learnability-based curriculum against random sampling with identical domain distributions
2. Implement proxy model training with varying sizes (10M, 50M, 100M params) to quantify sensitivity to proxy capacity
3. Measure actual wall-clock time for proxy computation versus potential savings in large model training to verify computational claims