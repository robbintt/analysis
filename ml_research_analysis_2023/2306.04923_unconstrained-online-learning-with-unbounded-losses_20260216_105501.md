---
ver: rpa2
title: Unconstrained Online Learning with Unbounded Losses
arxiv_id: '2306.04923'
source_url: https://arxiv.org/abs/2306.04923
tags:
- gmax
- lmax
- learning
- algorithm
- losses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies online learning in unbounded domains with non-Lipschitz\
  \ losses, where the gradient norms can grow quadratically with distance from a reference\
  \ point. The authors develop a new algorithm that achieves regret scaling as O(G\u2225\
  u\u2225\u221AT + L\u2225u\u2225\xB2\u221AT) against any sequence of losses satisfying\
  \ the quadratic boundedness property, and prove this bound is optimal."
---

# Unconstrained Online Learning with Unbounded Losses

## Quick Facts
- arXiv ID: 2306.04923
- Source URL: https://arxiv.org/abs/2306.04923
- Authors: 
- Reference count: 40
- Primary result: Achieves O(G||u||√T + L||u||²√T) regret in unbounded domains with non-Lipschitz losses

## Executive Summary
This paper tackles the challenge of online learning in unbounded domains where loss gradients can grow quadratically with distance from a reference point. The authors develop a novel algorithm that achieves optimal comparator-adaptive regret scaling by separately controlling the Lipschitz and non-Lipschitz components of the loss through carefully designed composite regularizers. The work also extends to dynamic regret and saddle-point optimization, achieving fully adaptive bounds without requiring strong convexity or smoothness assumptions.

## Method Summary
The algorithm uses Mirror Descent with composite regularizers - a Lipschitz-regularizing term O(Gmax||w||√T log(||w||√T/ϵ)) and a non-Lipschitz term O(Lmax√T||w||²). For dynamic regret, it runs multiple projected gradient descent instances with different domain diameters and combines them via multi-scale experts. For saddle-point problems, it reduces to a sequence of online learning problems. The approach achieves sublinear regret in unbounded domains while automatically improving bounds when losses are smooth.

## Key Results
- Achieves O(G||u||√T + L||u||²√T) regret against any comparator in unbounded domains
- First algorithm achieving sublinear dynamic regret in unbounded domains for non-Lipschitz losses
- Automatically improves to L* bound when losses are smooth
- Solves saddle-point optimization with unbounded domains achieving fully comparator-adaptive duality gap convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves comparator-adaptive regret by controlling Lipschitz and non-Lipschitz parts separately
- Mechanism: Uses Ψt(w) = O(Gmax||w||√T log(||w||√T/ϵ)) for Lipschitz part and Φt(w) = O(Lmax√T||w||²) for non-Lipschitz part
- Core assumption: Gradients satisfy ∥gt∥ ≤ Gt + Lt||wt|| allowing separation into components
- Evidence anchors:
  - [abstract]: "an algorithm which guarantees RT(u) ≤ eO(G‖u‖√T + L‖u‖²√T) regret on any problem where the subgradients satisfy ‖gt‖ ≤ G + L‖wt‖"
  - [section]: "using a similar argument to Jacobsen & Cutkosky (2022), by setting Ψt(w) = O(Gmax‖w‖√T log(‖w‖√T/ϵ) we can ensure that the Lipschitz part of the bound is well-controlled"
  - [corpus]: Weak - No direct corpus evidence for this specific mechanism, though related works on unbounded domains exist
- Break condition: If gradient growth is faster than quadratic or separation into components is not possible

### Mechanism 2
- Claim: The algorithm automatically improves to an L* bound when losses are smooth
- Mechanism: Exploits self-bounding property ‖gt‖² ≤ 2Lt(ℓt(wt) - ℓ*t) when losses are Lt-smooth
- Core assumption: Loss functions are Lt-smooth enabling the self-bounding property
- Evidence anchors:
  - [abstract]: "The regret of our dynamic regret algorithm automatically improves to a novel L* bound when the losses are smooth"
  - [section]: "when the losses are Lt-smooth, ΩT automatically improves to ΩT ≤ min{PT Lt [ℓt(ut) - ℓ*t], PT G²t + L²t M²}"
  - [corpus]: Weak - While smoothness is common in optimization literature, specific implementation details are missing
- Break condition: If smoothness constants Lt vary wildly or losses aren't actually smooth

### Mechanism 3
- Claim: The algorithm achieves dynamic regret with sublinear scaling by tuning artificial domain constraint
- Mechanism: Runs multiple projected gradient descent instances with different diameters D and step sizes η, combines via multi-scale experts
- Core assumption: Comparator sequence has bounded norm M = maxt ‖ut‖, algorithm can tune over range of diameters
- Evidence anchors:
  - [abstract]: "we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses"
  - [section]: "for each (η, D) in some set S, we run an instance of gradient descent A(η, D) which uses step-size η and projects to the set WD = {w ∈ W : ‖w‖ ≤ D}"
  - [corpus]: Weak - Dynamic regret in unbounded domains is relatively unexplored
- Break condition: If comparator has very large or unbounded norm, or optimal pair not in search space

## Foundational Learning

- Concept: Online Convex Optimization (OCO)
  - Why needed here: The paper's framework is built on OCO where algorithm chooses points wt each round and incurs convex loss ℓt(wt), with performance measured by regret RT(u)
  - Quick check question: What is the difference between static regret RT(u) and dynamic regret RT(u) where u = (u1,...,uT)?

- Concept: Bregman Divergence
  - Why needed here: Algorithm's stability analysis relies heavily on Bregman divergences Dψ(x|y) = ψ(x) - ψ(y) - ⟨∇ψ(y), x-y⟩ for convex differentiable functions ψ
  - Quick check question: How does the Bregman divergence generalize the squared Euclidean distance when ψ(w) = 1/2‖w‖²?

- Concept: Quadratic Boundedness
  - Why needed here: Paper introduces new boundedness assumption ∥∇ℓt(w)∥ ≤ G + L‖w - w0‖ that generalizes Lipschitz continuity and captures smooth functions
  - Quick check question: Why is this assumption necessary when studying unbounded domains with unbounded losses?

## Architecture Onboarding

- Component map: Base OCO algorithm using Mirror Descent with composite regularizers -> Multi-scale experts algorithm for combining base algorithms -> Saddle-point reduction for minimax optimization
- Critical path: Understanding how composite regularizers Ψt and Φt work together to control gradient growth, then understanding how multi-scale experts algorithm combines multiple base algorithms
- Design tradeoffs: Trades computational complexity (running multiple base algorithms) for adaptivity to unknown comparator norms and gradient bounds. Domain tuning adds O(dT log(√T)) computation but achieves optimal bounds.
- Failure signatures: Common failure modes include: (1) gradient norms growing faster than quadratic, breaking quadratic boundedness assumption, (2) comparator sequences with very large norms making domain tuning ineffective, (3) smooth losses with wildly varying Lt values breaking L* improvement
- First 3 experiments:
  1. Implement base algorithm with Ψt and Φt regularizers on simple quadratic loss to verify comparator-adaptive bound
  2. Test domain tuning trick on synthetic dynamic regret problem with known comparator sequence to verify M√PT scaling
  3. Apply saddle-point reduction to bilinear game to verify duality gap convergence in unbounded domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a unifying analysis for the dynamic regret algorithm that works for both Lmax = 0 and Lmax > 0 cases, reducing per-round computation from O(dT log(√T)) to O(d log(T))?
- Basis in paper: [explicit] Authors mention Jacobsen & Cutkosky (2022) achieves optimal bounds for Lmax = 0 with O(d log(T)) computation, but their algorithm for Lmax > 0 requires O(dT log(√T)) computation
- Why unresolved: Paper leaves this as open question, suggesting higher computation complexity may be fundamental limitation in QB-OLO setting
- What evidence would resolve it: New analysis technique combining benefits of both approaches, or lower bound proving higher computation complexity unavoidable for QB-OLO

### Open Question 2
- Question: Can the dynamic regret achieved in Theorem 4.1 be attained in the more general QB-OLO setting where adversary chooses (Gt, Lt) pairs to maximize regret?
- Basis in paper: [explicit] Authors state result achieved in QB-OCO setting but unclear whether it can be achieved in harder QB-OLO setting
- Why unresolved: QB-OLO setting gives adversary more freedom to impose penalties based on learner's predictions, making it potentially more difficult to control regret
- What evidence would resolve it: Algorithm achieving same regret bound as Theorem 4.1 in QB-OLO setting, or lower bound showing QB-OLO setting inherently requires higher regret

### Open Question 3
- Question: Is there an algorithm that achieves optimal dynamic regret bound in Theorem 4.1 without using artificial domain constraint trick?
- Basis in paper: [explicit] Authors use domain projection to control gradients for dynamic regret, but note this requires tuning diameter D proportional to M = max ||ut||, which is unknown to learner
- Why unresolved: Domain projection is key component but introduces complexity and computational overhead. Authors leave open whether this can be avoided
- What evidence would resolve it: Algorithm achieving same regret bound without domain projection, or proof that domain projection is necessary for optimal dynamic regret in QB-OLO setting

## Limitations
- Practical implementation of multi-scale experts algorithm and domain tuning procedure requires careful hyperparameter selection not fully specified
- Quadratic boundedness assumption may be restrictive for applications where gradient growth is non-quadratic
- High computational complexity O(dT log(√T)) for dynamic regret algorithm may limit scalability

## Confidence

- High confidence in theoretical regret bounds and their optimality proofs
- Medium confidence in practical effectiveness of domain tuning trick for dynamic regret
- Low confidence in implementation details of Centered Mirror Descent with Adjustment subroutine

## Next Checks

1. Implement the multi-scale experts algorithm on a simple synthetic problem and empirically verify that it achieves the claimed O(M√PT) dynamic regret scaling
2. Test the L* improvement on smooth loss sequences with varying smoothness constants Lt to confirm the algorithm automatically adapts to individual Lt values
3. Apply the saddle-point reduction to a nonconvex-strongly concave problem to verify whether the duality gap convergence extends beyond the convex-concave case as suggested