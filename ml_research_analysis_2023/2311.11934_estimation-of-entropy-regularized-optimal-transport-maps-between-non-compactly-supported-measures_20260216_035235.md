---
ver: rpa2
title: Estimation of entropy-regularized optimal transport maps between non-compactly
  supported measures
arxiv_id: '2311.11934'
source_url: https://arxiv.org/abs/2311.11934
tags:
- have
- theorem
- measures
- proof
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies estimation of entropy-regularized optimal transport
  (EOT) maps between non-compactly supported subGaussian measures. The authors analyze
  a recently proposed in-sample estimator and establish convergence rates for two
  cases: (1) when the target measure is compactly supported or strongly log-concave,
  yielding an expected squared L2-error decay of O(n^{-1/3}) with polynomial dependence
  on the regularization parameter; (2) for general subGaussian measures, obtaining
  an expected L1-error decay of O(n^{-1/6}) with polynomial dependence on the regularization
  parameter.'
---

# Estimation of entropy-regularized optimal transport maps between non-compactly supported measures

## Quick Facts
- **arXiv ID:** 2311.11934
- **Source URL:** https://arxiv.org/abs/2311.11934
- **Reference count:** 40
- **Key outcome:** For non-compactly supported subGaussian measures, expected squared L2-error decays as O(n^{-1/3}) for compactly supported or strongly log-concave targets, and expected L1-error decays as O(n^{-1/6}) for general subGaussian measures.

## Executive Summary
This paper addresses the problem of estimating entropy-regularized optimal transport (EOT) maps between non-compactly supported measures, eliminating the compactness assumption required by previous work. The authors analyze an in-sample estimator and establish convergence rates for two cases: (1) when the target measure is compactly supported or strongly log-concave, yielding an expected squared L2-error decay of O(n^{-1/3}) with polynomial dependence on the regularization parameter; (2) for general subGaussian measures, obtaining an expected L1-error decay of O(n^{-1/6}) with polynomial dependence on the regularization parameter. The analysis uses a bias-variance decomposition, where variance is controlled via concentration of measure results and bias is handled using T1-transport inequalities and sample complexity results.

## Method Summary
The paper studies estimation of entropy-regularized optimal transport (EOT) maps between non-compactly supported subGaussian measures using a recently proposed in-sample estimator. The analysis employs a bias-variance decomposition, where the variance is controlled using standard concentration of measure results and the bias is handled by T1-transport inequalities and sample complexity results. The authors establish convergence rates for two cases: (1) when the target measure is compactly supported or strongly log-concave, yielding an expected squared L2-error decay of O(n^{-1/3}) with polynomial dependence on the regularization parameter; (2) for general subGaussian measures, obtaining an expected L1-error decay of O(n^{-1/6}) with polynomial dependence on the regularization parameter.

## Key Results
- For compactly supported or strongly log-concave target measures, expected squared L2-error decays as O(n^{-1/3}) with polynomial dependence on the regularization parameter ε
- For general subGaussian measures, expected L1-error decays as O(n^{-1/6}) with polynomial dependence on the regularization parameter ε
- The analysis eliminates the compactness assumption required by previous work on EOT map estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The bias-variance decomposition isolates estimation error into two tractable components, enabling separate analysis of each.
- **Mechanism:** By applying Jensen's inequality to the squared L2 error, the overall error is bounded by twice the variance term plus twice the squared bias term. This decomposition allows the variance to be controlled using concentration of measure results and the bias to be controlled using T1-transport inequalities.
- **Core assumption:** The estimator's deviation from its expectation (variance) and the expectation's deviation from the true map (bias) are independent and can be bounded separately.
- **Evidence anchors:**
  - [abstract]: "The proof technique makes use of a bias-variance decomposition where the variance is controlled using standard concentration of measure results and the bias is handled by T1-transport inequalities"
  - [section]: "Lemma 1. For any measures µ, ν we have EX,Y[∥ ˆTn − Tε∥2L2(µ)] ≤ 2EX,Y[∥ ˆTn − E[ ˆTn]∥2L2(µ)] + 2∥E[ ˆTn] − Tε∥2L2(µ)."
- **Break condition:** If the estimator exhibits high correlation between bias and variance, or if the bias cannot be bounded independently using transport inequalities.

### Mechanism 2
- **Claim:** For compactly supported or strongly log-concave target measures, the KL divergence between the empirical and true conditional measures can be bounded using T1-transport inequalities.
- **Mechanism:** When the target measure is either compactly supported or strongly log-concave, the Laplace functional satisfies an exponential bound. This allows application of Theorem 2, which relates Wasserstein distance to KL divergence, providing a uniform control on the bias term.
- **Core assumption:** The target measure satisfies conditions (compact support or strong log-concavity) that ensure the Laplace functional is bounded by an exponential of the squared argument.
- **Evidence anchors:**
  - [abstract]: "In the case that the target measure is compactly supported or strongly log-concave, we show that for a recently proposed in-sample estimator, the expected squared L2-error decays at least as fast as O(n−1/3)"
  - [section]: "Theorem 2. ([5, 24]) Let µ ∈ P (Rd). Then W1(µ, ν) ≤ p2CKL(ν||µ) for some C > 0 and all ν if and only if Eµ(λ) ≤ eCλ2/2, ∀ λ ≥ 0."
- **Break condition:** If the target measure is neither compactly supported nor strongly log-concave, preventing uniform control of the Laplace functional.

### Mechanism 3
- **Claim:** For general subGaussian measures, the L1 error decay rate is established by modifying the analysis to use conditional application of T1-transport inequalities.
- **Mechanism:** In the absence of uniform control on the Laplace functional, the analysis applies T1-transport inequalities conditionally for each sample point. This requires Cauchy-Schwarz inequality, which introduces an additional square root, leading to a slower decay rate of O(n−1/6) for the L1 error.
- **Core assumption:** The source and target measures are subGaussian, ensuring that conditional measures are well-concentrated.
- **Evidence anchors:**
  - [abstract]: "For the general subGaussian case we show that the expected L1-error decays at least as fast as O(n−1/6)"
  - [section]: "Theorem 8. Let µ, ν be σ2-norm-subGaussian. Then E h k ˆTn − Tε kL1(µ) i ≲ √dσ2 √k + s dσ2(1 + σ⌈5d/2⌉+6 ε⌈5d/4⌉+3) 1m1/4."
- **Break condition:** If the measures are not subGaussian, preventing concentration bounds on conditional measures.

## Foundational Learning

- **Concept:** Entropy-regularized optimal transport (EOT) and its dual formulation
  - **Why needed here:** Understanding EOT is crucial as the paper estimates the EOT map between non-compactly supported measures, which is the core problem being addressed.
  - **Quick check question:** What is the dual formulation of the EOT problem, and how does it relate to the relative density of the optimal coupling?

- **Concept:** SubGaussian random variables and norm-subGaussian random vectors
  - **Why needed here:** The analysis requires understanding the concentration properties of subGaussian random variables to bound the variance and bias terms.
  - **Quick check question:** How is the norm of a subGaussian random vector defined, and what does it imply about the tail behavior of the vector?

- **Concept:** T1-transport inequalities and their relationship to Laplace functionals
  - **Why needed here:** T1-transport inequalities are used to bound the Wasserstein distance between the empirical and true conditional measures, which is essential for controlling the bias term.
  - **Quick check question:** Under what conditions does a probability measure satisfy a T1-transport inequality, and how does this relate to the Laplace functional of the measure?

## Architecture Onboarding

- **Component map:** Estimator -> Bias term + Variance term -> Convergence rates
- **Critical path:**
  1. Sample n points from source and target measures
  2. Divide samples into k batches of size m
  3. For each batch, construct T (1) ℓ,m using the in-sample estimator formula
  4. Average batch estimates to obtain ˆTn
  5. Analyze bias and variance separately to establish convergence rates

- **Design tradeoffs:**
  - Number of batches k vs. batch size m: Smaller k (larger m) reduces variance but may increase bias
  - Choice of regularization parameter ε: Affects both bias and variance, with experimental results suggesting looser variance control for larger ε
  - L2 vs. L1 error: L2 error provides tighter rates for compactly supported or strongly log-concave targets, while L1 error is used for general subGaussian measures

- **Failure signatures:**
  - Slow convergence rates compared to theoretical bounds
  - High variance in estimator when k is small or m is large
  - Breakdown of T1-transport inequality application when target measure is neither compactly supported nor strongly log-concave

- **First 3 experiments:**
  1. Verify convergence rate for Gaussian source and target with varying k and m, comparing to theoretical O(n−1/3) bound
  2. Test variance term convergence for strongly log-concave measures with different ε values, checking for n−1 dependence
  3. Explore L1 error decay for general subGaussian measures, comparing experimental results to theoretical O(n−1/6) bound

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence rate of the variance term be improved from O(1/k) to O(1/n)?
- **Basis in paper:** [explicit] Conjecture 1 states that the expected squared L2-error of the variance term is bounded by 1/m, which would lead to an overall convergence rate of O(1/n) instead of O(n^{-1/3}).
- **Why unresolved:** The authors empirically observe this improved rate in their experiments but have not proven it theoretically.
- **What evidence would resolve it:** A rigorous proof showing that E[∥T1,m - E[T1,m]∥^2_{L^2(μ)}] ≤ C/m for some constant C independent of m.

### Open Question 2
- **Question:** Can the convergence rate be improved from O(n^{-1/3}) to O(n^{-1/2}) for compactly supported and strongly log-concave measures?
- **Basis in paper:** [inferred] The authors suggest that a positive answer to Conjecture 1 would improve the rate from n^{-1/3} to n^{-1/2} by reducing the variance term to O(1/n).
- **Why unresolved:** The current analysis results in a rate of n^{-1/3}, and the authors believe a different approach might be needed to achieve n^{-1/2}.
- **What evidence would resolve it:** A proof showing that the overall convergence rate is O(n^{-1/2}) for compactly supported and strongly log-concave measures.

### Open Question 3
- **Question:** Can the convergence rate be improved from O(n^{-1/6}) to O(n^{-1/4}) for general subGaussian measures?
- **Basis in paper:** [inferred] The authors suggest that a positive answer to Conjecture 1 would improve the rate from n^{-1/6} to n^{-1/4} by reducing the variance term to O(1/n).
- **Why unresolved:** The current analysis results in a rate of n^{-1/6}, and the authors believe a different approach might be needed to achieve n^{-1/4}.
- **What evidence would resolve it:** A proof showing that the overall convergence rate is O(n^{-1/4}) for general subGaussian measures.

### Open Question 4
- **Question:** Can the convergence rate be improved to O(n^{-1}) for multivariate Gaussian source and target measures?
- **Basis in paper:** [explicit] Conjecture 3 states that for mean-zero multivariate Gaussian measures, the expected squared L2-error should converge at a rate of O(n^{-1}).
- **Why unresolved:** The current analysis results in rates of n^{-1/3} or n^{-1/2}, and the authors empirically observe a rate of n^{-1} for multivariate Gaussians but have not proven it theoretically.
- **What evidence would resolve it:** A rigorous proof showing that E[∥T̂n - Tε∥^2_{L^2(μ)}] ≤ C/n for some constant C independent of n.

## Limitations
- The analysis relies on entropy-regularized OT rather than exact OT, which introduces bias that scales with the regularization parameter ε
- The rates O(n^{-1/3}) and O(n^{-1/6}) are suboptimal compared to the n^{-1/2} rates achievable with compact support assumptions
- Experimental results suggest the variance control may be loose, particularly for larger values of ε, potentially indicating room for improvement in the theoretical bounds

## Confidence
- **High confidence** in the bias-variance decomposition framework and its application to this problem
- **Medium confidence** in the T1-transport inequality bounds, as these depend on specific properties of the target measure
- **Medium confidence** in the subGaussian concentration arguments, as these rely on tail bounds that may not capture worst-case behavior

## Next Checks
1. **Experimental validation of variance bounds:** Generate synthetic data for strongly log-concave measures with varying ε and k, empirically measuring the convergence rate of the variance term to verify if it achieves the predicted n^{-1} scaling or if it's slower, as suggested by experiments.
2. **Comparison with exact OT bounds:** For compactly supported measures, implement exact OT map estimation and compare convergence rates with the entropy-regularized approach to quantify the bias introduced by regularization.
3. **Sensitivity analysis on ε and batch size:** Systematically vary the regularization parameter ε and the number of batches k across multiple data distributions to identify parameter regimes where the theoretical bounds align with or diverge from experimental observations.