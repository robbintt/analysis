---
ver: rpa2
title: Robust Safe Reinforcement Learning under Adversarial Disturbances
arxiv_id: '2310.07207'
source_url: https://arxiv.org/abs/2310.07207
tags:
- safety
- policy
- invariant
- robust
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles safe reinforcement learning under adversarial
  disturbances, addressing a critical gap where most existing safe RL methods do not
  account for external disturbances that can lead to catastrophic safety violations
  in real-world applications. The authors propose a novel framework that combines
  policy iteration for robust invariant set synthesis with constrained reinforcement
  learning to achieve both safety and optimality.
---

# Robust Safe Reinforcement Learning under Adversarial Disturbances

## Quick Facts
- arXiv ID: 2310.07207
- Source URL: https://arxiv.org/abs/2310.07207
- Reference count: 35
- One-line primary result: Achieves zero constraint violation under adversarial disturbances while maintaining performance comparable to baseline methods

## Executive Summary
This paper addresses the critical challenge of safe reinforcement learning under adversarial disturbances, where most existing safe RL methods fail to account for external perturbations that can cause catastrophic safety violations. The authors propose a novel framework combining policy iteration for robust invariant set synthesis with constrained reinforcement learning to achieve both safety and optimality. Their approach establishes a two-player zero-sum game using Hamilton-Jacobi reachability analysis, where control inputs compete against worst-case disturbances. Experimental results on cart-pole and quadrotor tasks demonstrate that the proposed method achieves zero constraint violation under learned adversarial disturbances while maintaining comparable performance to baseline methods in disturbance-free scenarios.

## Method Summary
The proposed method, SAC-RIS, combines soft actor-critic with robust invariant set synthesis through policy iteration. The algorithm establishes a two-player zero-sum game using Hamilton-Jacobi reachability safety value functions, where control inputs (protagonist) compete against worst-case disturbances (adversary). Through iterative policy improvement, the method converges to the maximal robust invariant set - the subset of states where persistent safety is possible despite external perturbations. The safety value function identifies admissible input ranges at each state for maintaining safety, and these constraints are incorporated into the main policy optimization via a Lagrangian multiplier method. The approach is validated on classic control tasks with safety constraints, demonstrating zero constraint violation under worst-case disturbances.

## Key Results
- Achieves zero constraint violation under learned worst-case adversarial disturbances
- Maintains comparable performance to baseline methods in disturbance-free scenarios
- Successfully learns optimal safe policies on cart-pole and quadrotor tasks
- Demonstrates the effectiveness of robust invariant set synthesis for safety under uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed policy iteration scheme converges to the maximal robust invariant set under worst-case disturbances.
- Mechanism: By establishing a two-player zero-sum game using Hamilton-Jacobi reachability safety value functions, where control inputs (protagonist) compete against worst-case disturbances (adversary), the algorithm iteratively improves both the protagonist policy and the safety value function until convergence to the maximal robust invariant set.
- Core assumption: The self-consistency conditions for safety value functions form monotone contractions when modified with a discount factor, enabling policy iteration convergence.
- Evidence anchors:
  - [abstract]: "This paper presents a policy iteration scheme to solve for the robust invariant set... This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set."
  - [section]: "We show in the following theorem that the three safety self-consistency operators are all monotone contractions... This lays the foundation for applying RL techniques such as policy iteration."
  - [corpus]: Weak - no direct evidence in corpus about convergence guarantees for this specific formulation.

### Mechanism 2
- Claim: The algorithm learns policies that achieve zero constraint violation under learned adversarial disturbances while maintaining comparable performance in disturbance-free scenarios.
- Mechanism: By simultaneously synthesizing the robust invariant set and using it for constrained policy optimization via Lagrangian multiplier method, the algorithm ensures safety under worst-case disturbances while optimizing for reward maximization. The safety value function identifies admissible input ranges at each state for maintaining safety.
- Evidence anchors:
  - [abstract]: "Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially."
  - [section]: "To ensure safety under worst-case disturbances, we must constrain the policy π such that its outputs satisfy the requirement of state-action safety value function: Qh(x,u, µ(x; β ); ψ) ≥ 0, u ∼ π(·|x; θ )."
  - [corpus]: Weak - no direct evidence in corpus about zero-violation results from this specific algorithm.

### Mechanism 3
- Claim: The robust invariant set is smaller than the standard invariant set due to the presence of disturbances, requiring explicit synthesis for safety under uncertainty.
- Mechanism: The safety value function under worst-case disturbances (V* h) identifies the subset of states where persistent safety is possible despite external perturbations, which is necessarily smaller than the invariant set without disturbances (V* h with a=0).
- Evidence anchors:
  - [abstract]: "Persistent safety under worst-case disturbances can only be achieved in a subset of the safe set, named robust invariant set [17], [25] which is smaller than the standard invariant set due to the presence of disturbances."
  - [section]: "One can easily deduce that the maximal robust invariant set is the zero-superlevel set of the optimal safety value function V* h. States out of this set will violate the safety constraint in the future inevitably under worst-case disturbances."
  - [corpus]: Weak - no direct evidence in corpus about the relationship between robust and standard invariant sets.

## Foundational Learning

- Concept: Hamilton-Jacobi reachability analysis
  - Why needed here: Provides the theoretical foundation for computing safety value functions that characterize worst-case constraint violations under disturbances
  - Quick check question: What is the relationship between the safety value function V*h and the maximal robust invariant set?

- Concept: Two-player zero-sum games in control
  - Why needed here: The framework models the interaction between control inputs (protagonist) and external disturbances (adversary) as a competitive game to find worst-case safety guarantees
  - Quick check question: How does the safety value function change when the adversary policy changes?

- Concept: Constrained Markov decision processes (CMDP)
  - Why needed here: The algorithm extends CMDP formulations to handle worst-case disturbances while maintaining zero constraint violation
  - Quick check question: Why do standard CMDP algorithms fail to achieve zero constraint violation under external disturbances?

## Architecture Onboarding

- Component map:
  - Safety value function network (Qh) -> Protagonist policy network (πh) -> Adversary policy network (µ) -> Main policy network (π) -> Lagrange multiplier (λ)

- Critical path: Safety value function evaluation → Protagonist policy improvement → Adversary policy improvement → Main policy optimization with safety constraints

- Design tradeoffs: Computational complexity of safety value function evaluation vs. approximation accuracy; exploration for learning worst-case disturbances vs. exploitation for safety; safety constraint tightness vs. reward performance

- Failure signatures: High constraint violation during training indicates poor safety value function estimation; poor performance compared to baselines suggests overly conservative safety constraints; unstable training suggests improper balance between protagonist and adversary policies

- First 3 experiments:
  1. Double integrator with analytic invariant sets - verify learned robust invariant set matches ground truth
  2. Cart-pole task with pole angle constraint - test zero-violation under worst-case disturbances
  3. Quadrotor trajectory tracking - validate performance generalization from disturbed to disturbance-free scenarios

## Open Questions the Paper Calls Out

- Open Question 1: How can the proposed method be adapted for online deployment in real-world control tasks, given the substantial constraint violations observed during training?
  - Basis in paper: [explicit] The authors note in Remark 2 that the constraint violations in training are considerable, making the algorithm unsuitable for direct online deployment.
  - Why unresolved: The paper acknowledges this limitation but does not provide a solution for safe online deployment.
  - What evidence would resolve it: A proposed method or modification that reduces constraint violations during training to acceptable levels for online deployment.

- Open Question 2: Can the policy iteration scheme for robust invariant sets be extended to handle more complex disturbance models, such as non-adversarial or stochastic disturbances?
  - Basis in paper: [inferred] The current method focuses on worst-case adversarial disturbances, but real-world disturbances may not always be adversarial or deterministic.
  - Why unresolved: The paper does not explore alternative disturbance models or their impact on the policy iteration scheme.
  - What evidence would resolve it: An extension of the policy iteration scheme that incorporates and effectively handles different types of disturbances, with experimental validation.

- Open Question 3: How does the proposed method scale to high-dimensional state and action spaces, and what are the computational challenges involved?
  - Basis in paper: [explicit] The authors mention that the curse of dimensionality is a challenge for Hamilton-Jacobi reachability analysis, but do not provide specific details on scalability for their method.
  - Why unresolved: The paper does not present results or analysis on the performance and computational requirements of the method for high-dimensional problems.
  - What evidence would resolve it: Experimental results demonstrating the performance and computational efficiency of the method on high-dimensional control tasks, along with an analysis of the scalability challenges.

## Limitations

- Computational complexity of Hamilton-Jacobi reachability analysis limits applicability to high-dimensional systems
- Reliance on accurate disturbance modeling and bounded adversarial perturbations in practice
- Gap between simulation results and real-world deployment due to modeling errors and unmodeled dynamics

## Confidence

- Policy iteration convergence to maximal robust invariant set: Medium
- Zero constraint violation under learned adversarial disturbances: Medium
- Performance comparable to baseline methods: Medium
- Generalizability to high-dimensional systems: Low

## Next Checks

1. Verify convergence behavior and invariant set accuracy on a double integrator system with known analytic solutions across different discount factors and system parameters.

2. Test algorithm robustness to modeling errors by introducing parametric uncertainties in the cart-pole dynamics and measuring constraint violation and performance degradation.

3. Scale the approach to a 4-6 dimensional control task (e.g., 2D quadrotor or 3-link manipulator) to evaluate computational tractability and safety guarantees in higher dimensions.