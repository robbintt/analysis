---
ver: rpa2
title: On Evaluating Adversarial Robustness of Large Vision-Language Models
arxiv_id: '2305.16934'
source_url: https://arxiv.org/abs/2305.16934
tags:
- image
- adversarial
- xadv
- text
- targeted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive evaluation of the adversarial
  robustness of large vision-language models (VLMs) like GPT-4, focusing on the most
  realistic threat model where adversaries have black-box access and seek targeted
  evasion. The authors develop transfer-based and query-based attack strategies, first
  crafting adversarial examples against surrogate models like CLIP and BLIP, then
  transferring them to target VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2,
  and Img2Prompt.
---

# On Evaluating Adversarial Robustness of Large Vision-Language Models

## Quick Facts
- arXiv ID: 2305.16934
- Source URL: https://arxiv.org/abs/2305.16934
- Reference count: 40
- Key outcome: Black-box targeted adversarial attacks can successfully manipulate VLMs to generate targeted responses with high success rates.

## Executive Summary
This paper presents the first comprehensive evaluation of adversarial robustness for large vision-language models under realistic black-box threat models. The authors develop transfer-based and query-based attack strategies that craft adversarial examples against surrogate models like CLIP and BLIP, then transfer them to target VLMs including MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. The combined approach of transfer-based initialization followed by query-based refinement significantly improves attack success rates. The study demonstrates that these VLMs are vulnerable to adversarial image perturbations, highlighting the need for security evaluations before deployment in safety-critical applications.

## Method Summary
The authors develop a black-box attack methodology combining transfer-based and query-based strategies. First, they craft targeted adversarial examples against surrogate models (CLIP and BLIP) by optimizing image-image feature similarity between clean images and targeted images generated by text-to-image models. Then they transfer these adversarial examples to victim VLMs. To improve effectiveness, they employ Random Gradient-Free estimation to refine attacks through black-box queries to the victim models. The attacks are evaluated on 50K ImageNet-1K validation images with targeted texts from MS-COCO captions, measuring CLIP score between generated responses and targeted texts while maintaining perceptual distance constraints.

## Key Results
- Transfer-based attacks achieve significant success rates across multiple VLMs, with MF-ii strategy performing best
- Query-based refinement using RGF method further improves attack effectiveness beyond transfer-based attacks alone
- Adversarial examples maintain high perceptual quality while successfully manipulating VLM outputs
- Multi-round conversations remain vulnerable to repeated adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer-based attacks can successfully craft adversarial examples against surrogate models like CLIP and BLIP, which then transfer to victim VLMs.
- Mechanism: By optimizing image-image feature similarity between adversarial examples and targeted images generated by text-to-image models, the crafted examples exploit shared representations between surrogate and victim models.
- Core assumption: The feature space alignment between surrogate models (used for crafting) and victim VLMs is sufficient for adversarial example transfer.
- Evidence anchors:
  - [abstract] "we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs"
  - [section 3.2] "we select an image encoder fϕ(x) and a text encoder gψ(c) as surrogate models, and we denote ctar as the targeted response"
  - [corpus] Weak - only 1 of 8 related papers explicitly discusses transfer-based attacks on VLMs
- Break condition: Significant architectural or training divergence between surrogate and victim models breaks feature space alignment.

### Mechanism 2
- Claim: Query-based refinement using Random Gradient-Free estimation can improve attack success rates.
- Mechanism: By sampling random perturbations around the current adversarial image and querying the victim model, the method estimates gradients in the black-box setting and refines the attack.
- Core assumption: The victim model's output changes smoothly with respect to input perturbations within the ℓ∞ budget.
- Evidence anchors:
  - [section 3.3] "we employ the random gradient-free (RGF) method... to estimate the gradients"
  - [section 4.2] "we discover that query-based attacks employing transfer-based priors can further improve the effectiveness"
  - [corpus] Weak - only 1 of 8 related papers discusses query-based black-box attacks
- Break condition: Highly non-smooth victim model behavior or insufficient query budget prevents gradient estimation.

### Mechanism 3
- Claim: Combining transfer-based initialization with query-based refinement yields higher success rates than either approach alone.
- Mechanism: Transfer-based attacks provide a strong initialization that guides query-based refinement toward more effective perturbations, reducing the search space.
- Core assumption: The combined approach leverages complementary strengths - transfer captures model similarities while queries capture victim-specific vulnerabilities.
- Evidence anchors:
  - [abstract] "we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion"
  - [section 3.3] "Previous research demonstrates that transfer-based and query-based attacking strategies can work in tandem"
  - [corpus] Weak - only 2 of 8 related papers discuss combining approaches
- Break condition: If transfer-based initialization is poor or queries are noisy/limited, combination provides minimal benefit.

## Foundational Learning

- Concept: Adversarial examples and their transferability
  - Why needed here: The entire attack methodology relies on crafting examples that fool surrogate models and transfer to victims
  - Quick check question: What property of deep networks enables adversarial examples to transfer between models?

- Concept: Black-box vs white-box threat models
  - Why needed here: The paper explicitly assumes black-box access to victim VLMs, requiring different attack strategies than white-box approaches
  - Quick check question: How does the inability to compute gradients affect attack strategy design?

- Concept: Multimodal embedding spaces and cross-modal alignment
  - Why needed here: VLMs rely on aligned image and text embeddings; attacks must exploit or manipulate these alignments
  - Quick check question: What is the significance of matching image-image vs image-text features in the attack?

## Architecture Onboarding

- Component map:
  Surrogate models (CLIP, BLIP) -> Text-to-image generators (Stable Diffusion, Midjourney, DALL-E) -> Victim VLMs (MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, Img2Prompt) -> RGF estimator

- Critical path:
  1. Select clean image and targeted text
  2. Generate targeted image using text-to-image model
  3. Craft adversarial example using surrogate model (transfer-based)
  4. Optionally refine using victim model queries (query-based)
  5. Evaluate success on victim VLM

- Design tradeoffs:
  - Transfer-based vs query-based: Speed vs effectiveness
  - Image-image vs image-text matching: Alignment reliability vs cross-modal exploitation
  - Perturbation budget (ε): Visual imperceptibility vs attack strength

- Failure signatures:
  - Low CLIP scores between generated response and targeted text
  - GradCAM highlighting irrelevant regions
  - Responses reverting to original content under random noise perturbation

- First 3 experiments:
  1. Replicate Table 1 - white-box attacks on surrogate models to verify crafting effectiveness
  2. Replicate Table 2 - transfer-based attacks on VLMs to verify transferability
  3. Replicate Figure 1 - qualitative demonstration on BLIP-2 with one example

## Open Questions the Paper Calls Out
- Question: How do the adversarial examples perform against physical-world attacks where the input images are captured by cameras in real-world environments?
- Question: Are the adversarial examples transferable across different vision-language model architectures (e.g., CLIP to BLIP-2)?
- Question: What is the impact of different perturbation budgets (epsilon values) on the effectiveness and perceptibility of adversarial examples?

## Limitations
- Threat model assumes black-box access but actual query budget and rate limits are not explicitly tested
- Evaluation focuses on single-turn and limited multi-turn settings, not complex context-dependent interactions
- ImageNet-1K validation images may not represent full diversity of real-world inputs
- Transferability assumption between surrogate and VLMs is critical but not thoroughly validated across different architectures

## Confidence
- High Confidence: VLMs are vulnerable to adversarial perturbations (well-supported by experimental results)
- Medium Confidence: Combining transfer-based and query-based attacks is effective (empirical demonstration but requires further investigation)
- Low Confidence: Generalizability of attack transferability across VLM architectures and robustness against defenses (uncertain without additional testing)

## Next Checks
1. **Transferability Robustness Test:** Evaluate attack transferability across a broader range of VLM architectures with varying training datasets and objectives to assess the limits of the feature space alignment assumption.

2. **Defense Evaluation:** Test the proposed attacks against potential defense mechanisms such as adversarial training, input preprocessing, or model ensembling to understand the practical impact of the vulnerabilities.

3. **Real-World Deployment Simulation:** Assess attack effectiveness under realistic deployment conditions including variable query budgets, rate limiting, and context-dependent multi-turn conversations that better reflect actual VLM usage patterns.