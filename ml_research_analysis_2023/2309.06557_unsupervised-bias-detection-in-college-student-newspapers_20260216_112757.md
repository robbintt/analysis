---
ver: rpa2
title: Unsupervised Bias Detection in College Student Newspapers
arxiv_id: '2309.06557'
source_url: https://arxiv.org/abs/2309.06557
tags:
- bias
- sentiment
- data
- media
- schools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents an unsupervised pipeline for scraping and\
  \ detecting bias in college student newspapers, extracting 23,154 articles from\
  \ 14 papers. Bias is quantified as the difference between sentiment of a large language\
  \ model summary (considered media\u2019s truth) and original article sentiment,\
  \ analyzed across article, paragraph, and sentence levels."
---

# Unsupervised Bias Detection in College Student Newspapers

## Quick Facts
- **arXiv ID**: 2309.06557
- **Source URL**: https://arxiv.org/abs/2309.06557
- **Reference count**: 5
- **Primary result**: Unsupervised pipeline detects measurable bias differences across politically charged keywords in college student newspapers, with no labeled data required

## Executive Summary
This study presents an unsupervised pipeline for detecting bias in college student newspapers by comparing sentiment between article summaries and original text. The method scrapes 23,154 articles from 14 college papers and quantifies bias as the sentiment difference between a large language model summary (treated as media's truth) and the original article. Key findings show measurable bias differences across politically charged keywords, with Palestine showing higher bias than Israel, and variation between schools. The approach requires no labeled data and offers a scalable method for comparative media bias analysis.

## Method Summary
The pipeline scrapes articles from college newspaper archives, summarizes each using Google's T5 model, and computes sentiment using NLTK's VADER. Bias is calculated as the difference between sentiment of the summary (considered the outlet's truth) and the original article. The method analyzes bias at article, paragraph, and sentence levels, and allows querying by keyword to compare bias across topics. The approach assumes summaries capture factual content while filtering subjective language, enabling bias measurement without requiring labeled data or assumptions about truth.

## Key Results
- Measurable bias differences across politically charged keywords, with Palestine showing higher bias scores than Israel
- York school showed highest overall bias scores despite having the smallest sample size
- Method successfully identifies imbalanced coverage patterns that schools can use to balance reporting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Media bias can be measured as the difference between what a media outlet believes the truth to be and what they report.
- Mechanism: The pipeline generates a summary of each article using T5 and treats this summary as the outlet's "truth". Sentiment is then calculated on both the summary and the original article. The difference in sentiment between these two is taken as the bias score.
- Core assumption: The summary captures the core factual content while filtering out subjective or emotionally charged language.

### Mechanism 2
- Claim: Using sentiment of a summary instead of full article reduces noise and focuses bias measurement on emotional framing rather than factual reporting.
- Mechanism: By summarizing first, the method collapses long articles into short statements, reducing variability due to article length and stylistic differences. Sentiment is then measured on this distilled form.
- Core assumption: Summaries are sufficiently representative of the article's topic and key facts to serve as a stable baseline for comparison.

### Mechanism 3
- Claim: Comparing bias across keywords allows schools to identify imbalanced coverage without requiring labeled data.
- Mechanism: The system scrapes articles, searches for keywords, and computes bias scores per keyword. Schools can then compare which keywords have higher bias scores and adjust coverage accordingly.
- Core assumption: Keyword-based aggregation captures enough topical diversity to support meaningful cross-keyword comparisons.

## Foundational Learning

- **Web scraping and HTML structure analysis**: Needed to extract articles from heterogeneous archive sites with inconsistent layouts. Quick check: Can you explain the difference between a sitemap-based scrape and an HTML-signature-based scrape?

- **Sentiment analysis with VADER/NLTK**: Required because sentiment scores are the core metric used to quantify bias at multiple granularities. Quick check: What does VADER return for a sentence like "The government is terrible"? (Answer: negative sentiment score)

- **Text summarization with T5**: Needed because summaries are used as the "truth" baseline for bias calculation. Quick check: What is the maximum token length of the T5 model used here? (Answer: 500 tokens)

## Architecture Onboarding

- **Component map**: Scraper → Article store → Keyword filter → Summarizer → Sentiment analyzer → Bias calculator → Reporter
- **Critical path**: Extract articles → Summarize → Compute sentiment → Compute bias
- **Design tradeoffs**: Simple keyword search vs. NLP-based topic modeling (simplicity vs. precision); T5 summarization vs. larger models (runtime vs. quality); Token limit enforcement (prevents memory issues vs. excludes long articles)
- **Failure signatures**: High proportion of zero-length or truncated summaries → summarizer token limit exceeded; Identical bias scores across keywords → sentiment analyzer returning default values; No articles matching a keyword → keyword regex too strict or archive missing coverage
- **First 3 experiments**: 1) Run scraper on one archive site, verify article extraction and date grouping. 2) Summarize and compute sentiment on a small sample of articles, check sentiment distribution. 3) Compute bias for one keyword across two schools, confirm differences match manual inspection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sentiment analysis models (e.g., NLTK's VADER vs. BERT) affect bias detection results across various college newspapers?
- Basis in paper: The paper mentions using NLTK's VADER for sentiment but acknowledges that larger models might better capture nuance, though they are impractical for large corpora.
- Why unresolved: The paper only uses VADER and does not compare results with other sentiment models like BERT or more sophisticated alternatives.
- What evidence would resolve it: Running the same bias detection pipeline using different sentiment models (VADER, BERT, RoBERTa) and comparing the bias scores across the same articles and keywords to see if conclusions change.

### Open Question 2
- Question: Does the bias detection method work effectively on media archives with different structures, such as those requiring JavaScript execution or containing PDF content?
- Basis in paper: The paper notes that PDF content was too noisy to process reliably and that certain sites loaded text with JavaScript, suggesting limitations in the current pipeline.
- Why unresolved: The current method excludes PDF-based archives and doesn't address JavaScript-rendered content, limiting the dataset's comprehensiveness.
- What evidence would resolve it: Testing the pipeline on archives with PDFs (using OCR or improved text extraction) and JavaScript-heavy sites (using tools like Selenium) to measure bias detection accuracy compared to the current text-based approach.

### Open Question 3
- Question: How does the token limit of the T5 summarization model (500 tokens) impact bias detection accuracy for longer articles, and would higher-capacity models yield different results?
- Basis in paper: The paper states that schools with articles exceeding 500 tokens were avoided due to T5's context limit, suggesting potential loss of information for longer articles.
- Why unresolved: The study avoids longer articles entirely rather than testing whether summarization truncation affects bias scores.
- What evidence would resolve it: Running the pipeline on the same articles using summarization models with larger context windows (e.g., Longformer, Pegasus-XL) and comparing bias scores to see if conclusions about article-level bias change.

## Limitations
- Reliance on T5 summaries assumes they capture factual content while filtering emotional framing, which may not hold for all topics
- Token limits (500 tokens) exclude longer articles, potentially introducing selection bias
- Manual archive navigation requirements and inconsistent website structures create scalability challenges

## Confidence
- **High confidence**: The pipeline architecture and bias calculation methodology are clearly specified and reproducible
- **Medium confidence**: The claim that sentiment differences between summaries and articles reliably indicate bias, as this depends on summarization quality
- **Low confidence**: Cross-keyword comparisons (e.g., Palestine vs. Israel) without understanding underlying topic distribution or whether differences reflect actual bias versus topic complexity

## Next Checks
1. **Summary quality validation**: Manually inspect 50 random summaries to verify they capture factual content while filtering emotional language, ensuring the "truth" baseline assumption holds
2. **Token limit impact analysis**: Measure what proportion of articles exceed 500 tokens and test whether sentiment scores change significantly when including these longer articles
3. **Cross-validation with ground truth**: Select 20 articles with known bias labels from external sources and compare unsupervised scores to human annotations to assess accuracy