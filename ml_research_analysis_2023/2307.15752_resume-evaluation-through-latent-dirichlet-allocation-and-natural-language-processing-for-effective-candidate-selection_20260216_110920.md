---
ver: rpa2
title: Resume Evaluation through Latent Dirichlet Allocation and Natural Language
  Processing for Effective Candidate Selection
arxiv_id: '2307.15752'
source_url: https://arxiv.org/abs/2307.15752
tags:
- resume
- spacy
- dirichlet
- resumes
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automated resume rating for
  effective candidate selection in the face of increasing job applicants. The proposed
  method uses Latent Dirichlet Allocation (LDA) and SpaCy's Named Entity Recognition
  (NER) to extract relevant entities like education, experience, and skills from resumes.
---

# Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection

## Quick Facts
- **arXiv ID:** 2307.15752
- **Source URL:** https://arxiv.org/abs/2307.15752
- **Reference count:** 0
- **Primary result:** Proposed LDA and SpaCy NER-based resume rating system achieves 77% accuracy for skills-only and 82% overall accuracy.

## Executive Summary
This paper addresses automated resume rating for candidate selection by leveraging Latent Dirichlet Allocation (LDA) and SpaCy's Named Entity Recognition (NER). The system extracts structured entities like education, experience, and skills from unstructured resume text, then uses LDA to model topic distributions for scoring. The approach combines keyword matching with within-word matching to produce a normalized score, achieving 77% accuracy for skills-only evaluation and 82% overall when including all attributes.

## Method Summary
The proposed method uses SpaCy's NER to extract entities from resume text, then applies LDA topic modeling to discover latent themes. A scoring function combines keyword matching (KM) and within-word matching (WM) metrics, normalized to a 0-10 scale. The system processes resumes through text extraction, entity recognition, topic modeling, and final score calculation using a combination of SpaCy NLP tools and probabilistic topic modeling.

## Key Results
- 77% accuracy achieved for skills-only resume evaluation
- 82% overall accuracy when including all attributes (college name, work experience, degree, skills)
- Scoring mechanism combines keyword matching with within-word matching for balanced evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SpaCy's NER can extract structured information from unstructured resume text with measurable accuracy
- Mechanism: SpaCy's en_core_web_lg model uses tokenization, POS tagging, and NER pipelines with word vectors to classify entities
- Core assumption: Required entity categories are present in SpaCy's pre-trained model or can be mapped to them
- Evidence anchors: Abstract mentions SpaCy extracts education, experience, skills; section reports entity-wise accuracy metrics; corpus provides weak general support
- Break condition: Domain-specific terminology not covered by model vocabulary causes accuracy drop

### Mechanism 2
- Claim: LDA can discover latent topics in resumes that align with candidate attributes
- Mechanism: LDA treats resumes as topic mixtures, inferring relevant themes through word co-occurrence patterns
- Core assumption: Resumes contain coherent, topic-discriminative language that groups into meaningful themes
- Evidence anchors: Abstract states LDA assigns topic probabilities to entities; section describes LDA as generative probabilistic model; corpus provides general LDA support
- Break condition: High resume variation or jargon degrades topic coherence and discriminative power

### Mechanism 3
- Claim: Combining keyword matching with within-word matching produces balanced normalized resume score
- Mechanism: Score = KM * WM, where KM measures domain relevance and WM measures conciseness, normalized to [0,10]
- Core assumption: Both keyword presence and conciseness indicate resume quality for given role
- Evidence anchors: Section shows scoring equations and normalization; abstract reports 77%/82% accuracy; corpus lacks direct evidence
- Break condition: Incomplete or biased keyword list misrepresents domain fit

## Foundational Learning

- **Named Entity Recognition (NER)**
  - Why needed: To extract structured fields (education, skills, experience) from raw resume text for scoring
  - Quick check: Can SpaCy's NER identify "Bachelor of Science in Computer Engineering" as a degree entity?

- **Latent Dirichlet Allocation (LDA)**
  - Why needed: To uncover latent thematic structure in resumes indicating role suitability
  - Quick check: Does LDA require labeled training data to discover topics in resumes?

- **Text Preprocessing**
  - Why needed: Clean raw resume text (remove special characters, stopwords, punctuation) before NER and LDA
  - Quick check: Why must resumes be preprocessed before applying LDA?

## Architecture Onboarding

- **Component map:** Data Collection → Text Extraction (SpaCy) → Entity Recognition → LDA Topic Modeling → Keyword + Within-word Matching → Final Score Normalization → Output
- **Critical path:** Text extraction → Entity recognition → LDA modeling → Score calculation
- **Design tradeoffs:**
  - SpaCy NER offers high speed but limited customization for domain-specific labels
  - LDA is unsupervised but requires careful topic number selection; too few → underfitting, too many → overfitting
  - Keyword-based scoring is interpretable but may miss semantic similarity
- **Failure signatures:**
  - Low NER precision → missing or misclassified entities
  - LDA perplexity high → topics are not coherent
  - Score distribution skewed → normalization parameters off
- **First 3 experiments:**
  1. Run SpaCy NER on 10 sample resumes; log entity extraction accuracy
  2. Train LDA on same corpus with k=5 topics; inspect top words per topic for coherence
  3. Compute KM and WM on one resume manually; verify scoring formula produces expected results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LDA-based resume rating system compare to advanced NLP models like BERT in accuracy and robustness?
- Basis: Paper mentions investigating LDA variations and exploring BERT as future enhancement
- Why unresolved: Only discusses LDA performance, no comparison with other models
- Evidence needed: Experiments comparing LDA-based system accuracy and robustness with BERT-based system on same dataset

### Open Question 2
- Question: How does accuracy vary across different career domains and job positions?
- Basis: Paper suggests expanding dataset diversity across career domains as future enhancement
- Why unresolved: No detailed analysis of performance across different domains
- Evidence needed: Experiments evaluating system accuracy on resumes from various career domains and job positions

### Open Question 3
- Question: How does user feedback and iterative model refinement impact continuous improvement?
- Basis: Paper mentions incorporating user feedback and iterative refinement for continuous improvement
- Why unresolved: No information on implementation or impact of user feedback
- Evidence needed: Implement user feedback mechanism, iteratively refine model, evaluate performance before and after refinement

## Limitations
- Dataset composition not specified beyond being "diverse," limiting reproducibility
- Number of topics used in LDA not disclosed
- Keyword matching logic described abstractly without concrete implementation details
- Evaluation methodology unclear (test set vs cross-validation, ground truth labeling process)

## Confidence
- **High Confidence:** General NLP pipeline (SpaCy NER → LDA → scoring) is technically sound and aligns with established text analytics practices
- **Medium Confidence:** Reported accuracy metrics (77% for skills, 82% overall) are plausible but cannot be independently verified
- **Low Confidence:** Claim that LDA "assigns topic probabilities to entities" is conceptually unclear - LDA typically assigns topics to documents

## Next Checks
1. **Dataset Validation:** Create comparable test corpus of 50+ resumes with ground truth labels; measure SpaCy NER performance
2. **Topic Model Coherence:** Train LDA with varying topics (k=5, 10, 15); compute topic coherence scores to validate optimal selection
3. **Scoring Formula Verification:** Implement KM and WM functions on 5 resumes; manually verify normalized scores (0-10) align with human expert ratings