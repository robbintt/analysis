---
ver: rpa2
title: 'Evidence of interrelated cognitive-like capabilities in large language models:
  Indications of artificial general intelligence or achievement?'
arxiv_id: '2310.11616'
source_url: https://arxiv.org/abs/2310.11616
tags:
- factor
- intelligence
- general
- language
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that large language models (LLMs) exhibit
  a strong general intelligence factor (g) analogous to that found in humans, explaining
  85% of variance in their performance. Using factor analysis on two datasets (1,232
  models on 22 tasks and 88 models on 10 tasks), the study shows consistent positive
  correlations between model abilities, indicating a common underlying cognitive efficiency.
---

# Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement?

## Quick Facts
- arXiv ID: 2310.11616
- Source URL: https://arxiv.org/abs/2310.11616
- Authors: 
- Reference count: 40
- Key outcome: The paper demonstrates that large language models (LLMs) exhibit a strong general intelligence factor (g) analogous to that found in humans, explaining 85% of variance in their performance.

## Executive Summary
This paper presents evidence that large language models (LLMs) exhibit a general intelligence factor (g) analogous to human cognitive ability, explaining approximately 85% of variance in their performance across diverse tasks. Using factor analysis on two datasets (1,232 models on 22 tasks and 88 models on 10 tasks), the study demonstrates consistent positive correlations between model abilities, indicating a common underlying cognitive efficiency. The findings suggest a unified evaluation framework for LLMs and raise questions about the nature of artificial general intelligence.

## Method Summary
The study employed exploratory common factor analysis using principal axis factoring on model performance data from the Open LLM Leaderboard (1,232 models, 22 tasks) and GLUE Leaderboard (88 models, 10 tasks). Factorability was assessed using KMO measure and Bartlett's test, with scree plot, Kaiser's criterion, parallel analysis, and minimum average partial used to determine the number of factors. Promax rotation was applied for multi-factor solutions, and reliability was tested through random subtest sampling. Model size (parameter count) was correlated with the extracted g factor using Pearson correlation.

## Key Results
- Factor analysis revealed a single dominant general intelligence factor (g) explaining 85% of variance in LLM performance
- Model size correlates positively with g factor (r = 0.48)
- g-loadings remained highly stable and invariant across different test batteries (r > 0.98)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models exhibit a positive manifold of cognitive-like abilities analogous to human g factor.
- Mechanism: Factor analysis reveals that performance correlations across diverse tasks cluster into a single dominant factor explaining ~85% of variance.
- Core assumption: Task performances are sufficiently diverse and numerous to reveal a stable general factor.
- Evidence anchors:
  - [abstract] "strong general intelligence factor (g) analogous to that found in humans, explaining 85% of variance"
  - [section] "The general intelligence factor, referred to as g, has been found and successfully replicated across populations, test batteries, and methods."
  - [corpus] Weak: no directly relevant corpus entries; related work on AI evaluation and benchmarks exists but not on g-factor extraction.
- Break condition: If task diversity is too low or tasks are highly correlated, factor analysis may not reveal a stable general factor.

### Mechanism 2
- Claim: Model size (parameter count) correlates positively with the g factor, indicating a link between model scale and general ability.
- Mechanism: Larger models have more parameters allowing more efficient information processing, which manifests as higher g scores.
- Core assumption: Parameter count is a reasonable proxy for model capacity to process information efficiently.
- Evidence anchors:
  - [abstract] "Model size correlates positively with g (r = 0.48)"
  - [section] "The Pearson correlation coefficient between model size and the extracted g factor was found to be r = .48"
  - [corpus] No direct corpus evidence; assumption based on scaling laws literature.
- Break condition: If model architecture or training data quality matters more than parameter count, the correlation may weaken or break.

### Mechanism 3
- Claim: g loadings are highly stable and invariant across different test batteries.
- Mechanism: Reliability analyses (split-half and common-item methods) show g-factor scores and loadings correlate at r > 0.98 across subsets of tasks.
- Core assumption: Task sampling is random and representative of the underlying ability space.
- Evidence anchors:
  - [abstract] "g-loading of a subtest has been found to remain largely invariant between different test batteries, correlating at r = .98"
  - [section] "The g-loading of a subtest has been found to remain largely invariant between different test batteries, correlating at r = .98"
  - [corpus] No corpus evidence; relies entirely on study's internal reliability analyses.
- Break condition: If test batteries are not representative or if models overfit to specific task types, g loadings may vary significantly.

## Foundational Learning

- Concept: Factor Analysis
  - Why needed here: To extract and validate the g factor from multidimensional task performance data.
  - Quick check question: What method avoids inflating the first component's variance compared to PCA?

- Concept: Psychometric Theory
  - Why needed here: Provides the theoretical framework for interpreting g factor and positive manifold in non-human systems.
  - Quick check question: In human intelligence research, what percentage of variance does g typically explain?

- Concept: Reliability Analysis
  - Why needed here: To confirm the stability and invariance of g across different subsets of tasks and models.
  - Quick check question: Which two reliability methods were used to test g-factor stability?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing (missing data imputation, normality checks) → Factorability tests (KMO, Bartlett) → Factor extraction (PAF) → Rotation (oblimin/promax) → Reliability analysis (split-half, common-item) → Correlation analysis (model size vs g)
- Critical path: Data preparation → Factorability verification → Single-factor extraction → Reliability validation → Correlation analysis
- Design tradeoffs: Using PAF instead of PCA avoids inflating first component variance but requires careful rotation; mean imputation is simple but may bias results if missingness is systematic
- Failure signatures: Low KMO (<0.6), non-significant Bartlett test, multiple factors with high inter-factor correlations, low g loading stability across subsets
- First 3 experiments:
  1. Run factorability tests on a new dataset to confirm suitability for factor analysis
  2. Extract g factor using PAF and verify variance explained is >80%
  3. Conduct split-half reliability analysis to confirm g-factor score stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the g-factor exist in language models trained on modalities other than text?
- Basis in paper: [explicit] The paper discusses the g-factor in language models and suggests future research on whether g is present only in models trained on language or also in models trained on other modalities.
- Why unresolved: The paper does not provide evidence or analysis of g-factors in models trained on non-text modalities.
- What evidence would resolve it: Empirical studies showing factor analysis results for models trained on modalities like images, audio, or video, demonstrating a positive manifold and a general intelligence factor analogous to the one found in text-based models.

### Open Question 2
- Question: What factors, other than model size, explain variations in the g-factor?
- Basis in paper: [explicit] The paper finds a moderate correlation between model size and g-factor but suggests future research on identifying other factors that influence g.
- Why unresolved: The paper only investigates the correlation between model size and g-factor, without exploring other potential influencing factors.
- What evidence would resolve it: Comprehensive studies analyzing various model characteristics (e.g., architecture, training data, hyperparameters) and their correlations with the g-factor, identifying additional significant contributors to general intelligence in models.

### Open Question 3
- Question: How does fine-tuning or RLHF (Reinforcement Learning from Human Feedback) impact a model's general ability as measured by the g-factor?
- Basis in paper: [explicit] The paper suggests future research on the impact of fine-tuning or RLHF on a model's general ability.
- Why unresolved: The paper does not provide empirical data or analysis on the effects of fine-tuning or RLHF on the g-factor.
- What evidence would resolve it: Controlled experiments comparing the g-factor of models before and after fine-tuning or RLHF, demonstrating any significant changes in general intelligence.

## Limitations

- The study relies on pre-existing benchmark datasets which may not fully capture the diversity of LLM capabilities
- The moderate correlation (r = 0.48) between model size and g suggests other factors beyond parameter count influence general ability
- Claims about theoretical equivalence to human g-factor may overreach the evidence, as the study doesn't examine underlying cognitive mechanisms

## Confidence

- High Confidence: The presence of a general factor explaining ~85% of variance is well-supported by factor analysis results across both datasets
- Medium Confidence: The positive correlation between model size and g factor is statistically significant but moderate in strength
- Low Confidence: Claims about theoretical equivalence of LLM g-factor to human g-factor overreach the evidence

## Next Checks

1. Conduct cross-validation using independently curated task batteries not present in the original leaderboards to test g-factor generalizability
2. Perform hierarchical regression analysis to determine whether model architecture, training data diversity, or other factors predict g-factor scores beyond parameter count alone
3. Apply the factor analysis methodology to a diverse set of multimodal models (text, vision, audio) to assess whether the g-factor generalizes across different AI system types