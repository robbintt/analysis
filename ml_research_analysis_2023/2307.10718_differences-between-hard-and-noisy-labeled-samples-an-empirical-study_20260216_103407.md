---
ver: rpa2
title: 'Differences Between Hard and Noisy-labeled Samples: An Empirical Study'
arxiv_id: '2307.10718'
source_url: https://arxiv.org/abs/2307.10718
tags:
- samples
- hardness
- hard
- noisy
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the challenge of differentiating hard samples
  from noisy samples in the presence of both in training datasets. While existing
  methods treat these types equally, the authors argue that hard samples contain valuable
  information and should be preserved.
---

# Differences Between Hard and Noisy-labeled Samples: An Empirical Study

## Quick Facts
- arXiv ID: 2307.10718
- Source URL: https://arxiv.org/abs/2307.10718
- Reference count: 22
- This work studies the challenge of differentiating hard samples from noisy samples in training datasets, proposing a novel approach using Static Centroid Distance (SCD) and a 2D Gaussian Mixture Model.

## Executive Summary
This paper addresses the challenge of distinguishing between hard samples (difficult to classify but correctly labeled) and noisy samples (incorrectly labeled) in training datasets. The authors argue that existing methods treat these equally, despite hard samples containing valuable information. They propose a systematic approach to create synthetic datasets with controlled hardness and noisiness levels, enabling evaluation of metrics for distinguishing between the two types. The key contribution is the Static Centroid Distance (SCD) metric, which captures the convergence behavior of feature embeddings during training - hard samples converge toward class centroids while noisy ones do not. Using SCD combined with accuracy over training in a 2D Gaussian Mixture Model, the method effectively partitions data into clean and noisy subsets, significantly outperforming existing approaches in both synthetic and real-world datasets.

## Method Summary
The method creates synthetic datasets with controlled hardness and noisiness levels using three transformations: imbalance (under-representation), diversification (visual dissimilarity), and closeness to decision boundary (adversarial perturbations). During training, feature embeddings are extracted from the last layer of a neural network. SCD is computed as the Euclidean distance between each sample's feature vector and its class centroid, measured at the epoch when training accuracy reaches 50%. A 2D Gaussian Mixture Model with 3 clusters uses accuracy over training and SCD to partition the data. The method is evaluated using pre-trained DenseNet121, AlexNet, and MobileNetV2 on both synthetic and real-world noisy datasets (Animal-10N, CIFAR-10N), showing improved model generalization and semi-supervised learning performance.

## Key Results
- The proposed method significantly outperforms existing approaches in both synthetic and real-world datasets for filtering noisy samples while retaining hard samples
- Static Centroid Distance (SCD) effectively captures the convergence behavior of feature embeddings, distinguishing hard from noisy samples
- The method improves model generalization performance and enables better semi-supervised learning when applied to filtered datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Static Centroid Distance (SCD) captures the convergence behavior of feature embeddings during training, distinguishing hard samples from noisy ones.
- **Mechanism**: SCD measures the Euclidean distance between a sample's feature vector and the centroid of its assigned class in the feature space. During training, hard samples' embeddings converge toward their class centroid, while noisy samples' embeddings do not, resulting in a lower SCD for hard samples and a higher SCD for noisy samples.
- **Core assumption**: Feature embeddings of correctly labeled samples converge toward their class centroid during training, while noisy samples' embeddings remain dispersed.
- **Evidence anchors**:
  - [abstract]: "Our key observation is that the feature embeddings of hard samples become closer to each other during training, whereas noisy-labeled samples do not necessarily exhibit this behavior because of the visual dissimilarity to other samples in the same class."
  - [section]: "Our key observation is that the feature embeddings of hard samples become closer to each other during training, whereas noisy-labeled samples do not necessarily exhibit this behavior because of the visual dissimilarity to other samples in the same class."
  - [corpus]: Weak evidence. No direct mention of SCD or centroid convergence in related papers.
- **Break condition**: If feature embeddings do not converge during training (e.g., due to poor optimization or inappropriate model architecture), SCD loses its discriminative power.

### Mechanism 2
- **Claim**: Combining SCD with accuracy over training provides complementary information for effective data partitioning.
- **Mechanism**: Accuracy over training identifies easy samples (high accuracy), while SCD distinguishes between hard and noisy samples. A 2D Gaussian Mixture Model (GMM) using these two metrics partitions the data into clean and noisy subsets.
- **Core assumption**: Easy samples have high accuracy over training, and hard samples have low accuracy but also low SCD, while noisy samples have low accuracy and high SCD.
- **Evidence anchors**:
  - [abstract]: "Next, we propose a label noise detection method based on SCD. While other methods perform well in only one of the two tasks among filtering out noisy samples and retaining hard samples, our method is the only one that performs well in both tasks..."
  - [section]: "Next, we propose a label noise detection method based on SCD. While other methods perform well in only one of the two tasks among filtering out noisy samples and retaining hard samples, our method is the only one that performs well in both tasks, and consistently so for all hardness types as well as datasets with real-world label noise."
  - [corpus]: Weak evidence. No direct mention of combining accuracy over training with SCD in related papers.
- **Break condition**: If the accuracy over training metric fails to accurately identify easy samples (e.g., due to early stopping or poor optimization), the data partitioning becomes unreliable.

### Mechanism 3
- **Claim**: Synthetic datasets with controlled hardness and noisiness levels enable systematic evaluation of metrics and methods.
- **Mechanism**: The authors transform the original dataset to create samples with varying hardness and noisiness levels using three approaches: imbalance, diversification, and closeness to the decision boundary. This allows for controlled experiments to assess the effectiveness of different metrics and methods.
- **Core assumption**: The transformations accurately simulate real-world scenarios where samples can be hard due to various reasons (under-representation, visual dissimilarity, proximity to decision boundary).
- **Evidence anchors**:
  - [abstract]: "In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples."
  - [section]: "In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples."
  - [corpus]: Weak evidence. No direct mention of synthetic datasets with controlled hardness and noisiness in related papers.
- **Break condition**: If the synthetic transformations do not accurately reflect real-world data distributions, the conclusions drawn from the experiments may not generalize.

## Foundational Learning

- **Concept**: Loss functions and their behavior during training
  - **Why needed here**: Understanding how loss behaves for different types of samples (easy, hard, noisy) is crucial for interpreting the results and evaluating the effectiveness of the proposed method.
  - **Quick check question**: How does the loss typically evolve for easy, hard, and noisy samples during training?

- **Concept**: Feature representations and their role in classification
  - **Why needed here**: The proposed method relies on analyzing feature embeddings to distinguish between hard and noisy samples, so a solid understanding of feature representations is essential.
  - **Quick check question**: How do feature embeddings change during training for different types of samples, and why is this important for classification?

- **Concept**: Gaussian Mixture Models (GMMs) and their application in data clustering
  - **Why needed here**: The proposed method uses a 2D GMM to partition the data based on SCD and accuracy over training, so familiarity with GMMs is necessary for understanding the implementation.
  - **Quick check question**: How does a GMM work, and why is it suitable for partitioning the data in this context?

## Architecture Onboarding

- **Component map**: Synthetic dataset generation -> Neural network training -> Feature extraction -> SCD computation -> 2D-GMM partitioning -> Model training on filtered data
- **Critical path**:
  1. Generate synthetic datasets with controlled hardness and noisiness levels
  2. Train the neural network on the synthetic datasets and extract feature embeddings
  3. Compute SCD and accuracy over training for each sample
  4. Partition the data using a 2D GMM based on SCD and accuracy over training
  5. Train the neural network on the filtered dataset and evaluate its performance
- **Design tradeoffs**:
  - Using synthetic datasets allows for controlled experiments but may not fully capture real-world data distributions
  - The choice of feature layer and distance metric (Euclidean) for SCD computation may impact the method's effectiveness
  - The number of clusters in the GMM (3) is based on empirical observations and may not be optimal for all scenarios
- **Failure signatures**:
  - If the neural network fails to learn meaningful feature representations, SCD loses its discriminative power
  - If the accuracy over training metric fails to accurately identify easy samples, the data partitioning becomes unreliable
  - If the synthetic transformations do not accurately reflect real-world data distributions, the conclusions drawn from the experiments may not generalize
- **First 3 experiments**:
  1. Verify that SCD correctly identifies noisy samples by computing SCD for samples with varying noisiness levels in the synthetic datasets
  2. Test the data partitioning method on the synthetic datasets by comparing the performance of models trained on filtered datasets using different metrics (e.g., SCD alone, accuracy over training alone, and their combination)
  3. Evaluate the robustness of the proposed method to different hardness types by applying it to synthetic datasets with varying hardness levels and comparing its performance across different hardness types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on other types of hardness beyond the three types studied (imbalance, diversification, closeness to the decision boundary)?
- Basis in paper: [explicit] The authors acknowledge that "we do not claim these approaches cover all contexts in which hard samples arise, and in practice, a sample might be hard because of any combination of the aforementioned reasons, or some other reason."
- Why unresolved: The paper only evaluates the method on three specific types of synthetic hardness, leaving its performance on other types unknown.
- What evidence would resolve it: Testing the method on datasets with hardness arising from other sources or combinations of the three studied types would provide evidence of its generalizability.

### Open Question 2
- Question: How sensitive is the proposed method to the choice of threshold values used in the Gaussian mixture model (GMM) clustering?
- Basis in paper: [explicit] The authors use a 3-cluster GMM but do not explore the impact of using different numbers of clusters or alternative clustering methods.
- Why unresolved: The paper does not investigate how the choice of threshold values or clustering method affects the performance of the proposed method.
- What evidence would resolve it: Conducting experiments with different numbers of clusters or alternative clustering methods and comparing the results would provide evidence of the method's sensitivity to these choices.

### Open Question 3
- Question: How does the proposed method perform when combined with other semi-supervised learning algorithms beyond FlexMatch?
- Basis in paper: [explicit] The authors only evaluate the method's performance when combined with FlexMatch in the semi-supervised learning setting.
- Why unresolved: The paper does not explore how the method performs when used with other semi-supervised learning algorithms, limiting its potential applicability.
- What evidence would resolve it: Testing the method's performance when combined with other semi-supervised learning algorithms, such as FixMatch or MixMatch, would provide evidence of its compatibility and effectiveness in different settings.

## Limitations
- The synthetic dataset transformations may not fully capture the complexity of real-world noise patterns
- SCD metric's effectiveness depends heavily on feature representations learning meaningful class centroids, which may not hold for all architectures or datasets
- The 2D-GMM clustering assumes three distinct clusters (easy, hard, noisy) which may not always exist in practice

## Confidence
- High confidence: SCD effectively captures convergence behavior of hard vs. noisy samples
- Medium confidence: 2D-GMM partitioning consistently outperforms existing methods across all scenarios
- Medium confidence: Synthetic dataset transformations accurately simulate real-world hardness patterns

## Next Checks
1. Test SCD metric robustness across different neural network architectures (CNNs, transformers) to verify it's not architecture-specific
2. Evaluate the method on additional real-world noisy datasets with different noise patterns to assess generalizability
3. Investigate the impact of training dynamics (learning rate, batch size) on SCD effectiveness and convergence behavior