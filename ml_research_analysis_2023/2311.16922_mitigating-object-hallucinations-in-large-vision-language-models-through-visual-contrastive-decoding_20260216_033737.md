---
ver: rpa2
title: Mitigating Object Hallucinations in Large Vision-Language Models through Visual
  Contrastive Decoding
arxiv_id: '2311.16922'
source_url: https://arxiv.org/abs/2311.16922
tags:
- arxiv
- visual
- lvlms
- decoding
- regular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visual Contrastive Decoding (VCD) is proposed to mitigate object
  hallucinations in Large Vision-Language Models (LVLMs). VCD contrasts output distributions
  from original and distorted visual inputs to reduce over-reliance on statistical
  bias and language priors.
---

# Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding

## Quick Facts
- **arXiv ID**: 2311.16922
- **Source URL**: https://arxiv.org/abs/2311.16922
- **Reference count**: 40
- **Primary result**: VCD reduces object hallucinations in LVLMs by contrasting outputs from original and distorted visual inputs, improving POPE F1 score by up to 7.4 points and MME by 18%.

## Executive Summary
This paper addresses the critical problem of object hallucinations in Large Vision-Language Models (LVLMs), where models generate objects that don't exist in images. The proposed Visual Contrastive Decoding (VCD) method mitigates this issue by contrasting output distributions from original and distorted visual inputs. By applying Gaussian noise to images and comparing the resulting model outputs, VCD identifies and suppresses hallucinated objects that appear more frequently when visual uncertainty is introduced. The approach significantly improves hallucination benchmarks without requiring additional training or external tools.

## Method Summary
VCD works by introducing visual uncertainty through Gaussian noise applied to input images, then comparing the output distributions from original and distorted versions. The method computes a contrastive term that identifies tokens appearing more frequently in the distorted case (likely hallucinations) and suppresses them based on an adaptive plausibility constraint. This constraint ensures valid outputs aren't suppressed when the model has high confidence in the original visual input. The approach is model-agnostic and can be applied to any LVLM without architectural modifications.

## Key Results
- **POPE benchmark**: VCD improves F1 score by up to +7.4 points across LLaVA-1.5, InstructBLIP, and Qwen-VL models
- **MME benchmark**: VCD enhances performance by +18% in detecting and localizing objects
- **LLaVA-Bench**: VCD improves general perception capabilities while maintaining performance on standard vision-language tasks
- **Model families**: Consistent improvements across 7B and 13B variants of multiple LVLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Visual uncertainty amplifies language priors and statistical biases, leading to more severe hallucinations.
- **Mechanism**: When visual inputs are degraded with Gaussian noise, the model becomes more reliant on its language priors from LLM pretraining and statistical biases from training data, causing it to generate objects that are frequent in the dataset but not present in the image.
- **Core assumption**: LVLMs have a tendency to default to language priors and statistical biases when visual information is ambiguous or uncertain.
- **Evidence anchors**:
  - [abstract]: "The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations."
  - [section]: "Figure 2 shows that visual uncertainty can compel LVLMs to overlook visual evidence and overly exploit language priors for decision-making."
  - [corpus]: No direct evidence found in corpus neighbors; weak evidence for this specific claim.
- **Break condition**: If the model has strong visual grounding mechanisms that override language priors regardless of visual uncertainty, this mechanism would not apply.

### Mechanism 2
- **Claim**: Contrasting output distributions from original and distorted visual inputs helps identify and suppress hallucinations.
- **Mechanism**: By comparing the logits from the original and distorted images, the model can detect tokens that are likely hallucinated (appearing more in the distorted case) and suppress them in the final output.
- **Core assumption**: Hallucinated objects tend to have higher probabilities in the output distribution when visual inputs are degraded.
- **Evidence anchors**:
  - [abstract]: "VCD contrasts output distributions derived from original and distorted visual inputs."
  - [section]: "VCD serves as a corrective mechanism, reducing hallucinations by contrasting against a distribution predisposed to favoring them."
  - [corpus]: No direct evidence found in corpus neighbors; weak evidence for this specific claim.
- **Break condition**: If the model's hallucinations are not correlated with visual uncertainty, the contrastive approach would not be effective.

### Mechanism 3
- **Claim**: Adaptive plausibility constraints prevent VCD from suppressing valid outputs that are still coherent despite visual distortion.
- **Mechanism**: The plausibility constraint ensures that when the model is highly confident about an output given the original visual input, it doesn't suppress that output just because the distorted version suggests something else.
- **Core assumption**: Not all outputs from distorted visual inputs are hallucinations; some may still represent valid common sense reasoning.
- **Evidence anchors**:
  - [section]: "We follow Li et al. [37] to implement an adaptive plausibility constraint that is contingent upon the confidence level associated with the output distribution with original visual inputs."
  - [corpus]: No direct evidence found in corpus neighbors; weak evidence for this specific claim.
- **Break condition**: If the plausibility threshold is set too high, it may fail to suppress hallucinations; if too low, it may suppress valid outputs.

## Foundational Learning

- **Concept**: Gaussian noise as visual uncertainty
  - Why needed here: Understanding how noise degrades visual features and affects model behavior is crucial for implementing VCD.
  - Quick check question: What happens to the distinguishable features of an image as the number of noise steps T increases in the diffusion process?

- **Concept**: Contrastive learning
  - Why needed here: VCD is based on contrasting two distributions, similar to how contrastive learning works in representation learning.
  - Quick check question: How does contrastive learning help models learn better representations by comparing similar and dissimilar pairs?

- **Concept**: Language priors and statistical bias
  - Why needed here: Understanding these biases helps explain why hallucinations occur and how VCD mitigates them.
  - Quick check question: What are some examples of statistical biases in vision-language datasets that could lead to hallucinations?

## Architecture Onboarding

- **Component map**: Visual encoder → Distortion module → Contrastive decoder → Plausibility constraint → Final output
- **Critical path**: Visual input → Distortion → Contrastive decoding → Plausibility constraint → Final output
- **Design tradeoffs**:
  - Choosing T (noise steps): Higher T increases uncertainty but may also degrade useful visual information
  - Choosing α (contrast strength): Higher α increases hallucination suppression but may also suppress valid outputs
  - Choosing β (plausibility threshold): Higher β is more conservative but may miss some hallucinations
- **Failure signatures**:
  - Hallucinations persist: α may be too low or β too high
  - Valid outputs suppressed: β may be too low or α too high
  - Performance degradation: T may be too high, introducing excessive noise
- **First 3 experiments**:
  1. Test VCD on POPE with varying α values to find optimal hallucination suppression
  2. Test VCD on MME with different β values to balance hallucination suppression and output quality
  3. Test VCD with different T values to find the sweet spot between uncertainty and information retention

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the effectiveness of VCD change when using different image distortion techniques beyond Gaussian noise, such as object-level blurring or adversarial perturbations?
  - Basis in paper: [inferred] The paper mentions that while Gaussian noise is used as a baseline for introducing visual uncertainty, more fine-grained techniques like object-level blurring hold potential for improved outcomes.
  - Why unresolved: The current study only employs Gaussian noise to introduce visual uncertainty. The effectiveness of VCD with other distortion methods remains unexplored.
  - What evidence would resolve it: Conducting experiments with various distortion techniques and comparing the performance of VCD across these methods would provide insights into the optimal distortion strategy.

- **Open Question 2**: How does the performance of VCD scale with larger LVLM variants beyond the 13B models tested in the paper?
  - Basis in paper: [explicit] The paper states that the evaluation extends to 13B variants of LLaVA-1.5 and InstructBLIP, but Qwen-VL lacks larger variants. It also mentions that increasing model parameters does not inherently resolve hallucination issues.
  - Why unresolved: The paper only tests VCD on 7B and 13B LVLM variants. The performance on larger models remains unknown.
  - What evidence would resolve it: Testing VCD on LVLM variants with more than 13B parameters would reveal how well the method scales with increasing model size.

- **Open Question 3**: How does VCD perform when applied to LVLMs designed for video understanding, compared to its current application on image-text models?
  - Basis in paper: [inferred] The paper acknowledges its focus on LVLMs processing images and text, not encompassing their emerging applications in video understanding. It suggests extending VCD to a broader range of LVLMs.
  - Why unresolved: The current study does not explore the application of VCD to video-based LVLMs. The effectiveness in this domain is unknown.
  - What evidence would resolve it: Implementing VCD on video-based LVLMs and evaluating its performance on video understanding tasks would determine its applicability and effectiveness in this domain.

## Limitations

- The paper demonstrates improved performance on hallucination benchmarks but doesn't provide direct evidence of how VCD affects the internal reasoning process of LVLMs or whether it truly addresses the root causes of hallucinations versus simply applying a band-aid correction.
- The specific mechanism by which visual uncertainty amplifies language priors relies on weak empirical evidence, with no direct support from corpus neighbors to validate this core assumption.
- The approach's effectiveness beyond the tested LVLM families (LLaVA-1.5, InstructBLIP, Qwen-VL) and dataset domains remains unverified, limiting generalizability claims.

## Confidence

- **High confidence**: VCD implementation details and experimental methodology (clearly specified equations and evaluation protocols)
- **Medium confidence**: Performance improvements on specific benchmarks (POPE, MME, LLaVA-Bench) - while results are presented, the extent to which they generalize beyond tested LVLM families remains unclear
- **Low confidence**: The specific mechanism by which visual uncertainty amplifies language priors - the paper asserts this relationship but provides limited direct evidence

## Next Checks

1. **Ablation study on noise levels**: Systematically test VCD performance across a wider range of Gaussian noise parameters (T values) to determine the optimal uncertainty level that maximizes hallucination suppression while preserving valid visual information.

2. **Cross-LVLM generalization**: Apply VCD to additional LVLM architectures beyond the three tested families (LLaVA-1.5, InstructBLIP, Qwen-VL) to assess whether the approach generalizes to different visual encoder and language decoder combinations.

3. **Qualitative error analysis**: Conduct detailed qualitative analysis of outputs where VCD both succeeds and fails to identify whether suppressed tokens represent true hallucinations or valid but low-confidence predictions, particularly examining cases where the plausibility constraint may be overly conservative or permissive.