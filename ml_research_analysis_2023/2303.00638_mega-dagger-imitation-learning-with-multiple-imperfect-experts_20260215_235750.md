---
ver: rpa2
title: 'MEGA-DAgger: Imitation Learning with Multiple Imperfect Experts'
arxiv_id: '2303.00638'
source_url: https://arxiv.org/abs/2303.00638
tags:
- learning
- experts
- expert
- demonstrations
- imitation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes MEGA-DAgger, an imitation learning algorithm
  for learning from multiple imperfect experts. The algorithm addresses two key challenges:
  filtering unsafe demonstrations and resolving conflicting labels from different
  experts.'
---

# MEGA-DAgger: Imitation Learning with Multiple Imperfect Experts

## Quick Facts
- arXiv ID: 2303.00638
- Source URL: https://arxiv.org/abs/2303.00638
- Authors: 
- Reference count: 39
- Key outcome: 45% average improvement on overtaking and collision avoidance compared to vanilla HG-DAgger

## Executive Summary
This paper addresses the challenge of learning from multiple imperfect experts in imitation learning. The proposed MEGA-DAgger algorithm tackles two key issues: filtering unsafe demonstrations using a Control Barrier Function-based safety scorer, and resolving conflicting labels from different experts using scenario-specific metrics. Through experiments in autonomous racing scenarios, MEGA-DAgger demonstrates significant improvements over state-of-the-art methods like HG-DAgger, achieving both safer and more successful overtaking behaviors.

## Method Summary
MEGA-DAgger extends interactive imitation learning to handle multiple imperfect experts by introducing two key mechanisms. First, it employs a Control Barrier Function (CBF) based safety scorer to filter unsafe demonstrations during training, truncating and removing segments where safety metrics indicate potential collisions. Second, it resolves conflicting labels by evaluating experts on scenario-specific metrics (safety and progress scores), grouping similar observations using cosine similarity, and selecting the best demonstration based on a composite evaluation score. The algorithm iteratively aggregates filtered and resolved demonstrations to train a novice policy that can outperform individual experts.

## Key Results
- 45% average improvement on both overtaking and collision avoidance compared to vanilla HG-DAgger
- 15% average improvement compared to HG-DAgger with data filter
- Outperforms all individual experts with 13.6% and 13.2% average improvements on collision avoidance and overtaking respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MEGA-DAgger uses a Control Barrier Function (CBF) based safety scorer to filter unsafe expert demonstrations during training.
- Mechanism: The CBF computes a safety score σ_t based on the ego vehicle's distance to obstacles. If σ_t becomes negative, the rollout is truncated and β previous steps are removed to eliminate unsafe data from the training set.
- Core assumption: CBF can provide a reliable heuristic safety score even if not mathematically valid, sufficient to distinguish unsafe from safe trajectories.
- Evidence anchors:
  - [section]: "We design a data filter based on Control Barrier Function (CBF) [31]. The data filter takes Dj as input... σt = h(xe_t+1,ye_t+1) − (1 −γ)h(xe_t,ye_t), 0<γ ≤ 1. Note that higher σt value indicates higher safety robustness."
  - [abstract]: "First, unsafe demonstrations are filtered while aggregating the training data, so the imperfect demonstrations have little influence when training the novice policy."
- Break condition: If CBF cannot reliably estimate safety (e.g., in highly dynamic environments or with inaccurate obstacle detection), unsafe demonstrations may still enter the training set, degrading policy performance.

### Mechanism 2
- Claim: MEGA-DAgger resolves conflicting labels from multiple imperfect experts by selecting the best demonstration based on a composite evaluation score.
- Mechanism: For similar observations across experts, cosine similarity is used to group demonstrations. An evaluation score ω_t combines normalized safety score and normalized speed to choose the highest-scoring action label to replace all others in the group.
- Core assumption: Safety and progress metrics can be meaningfully combined into a single score to rank expert actions.
- Evidence anchors:
  - [section]: "We use cosine similarity to identify and select similar observations... ωt = ∥σt∥ − mint∥σt∥ / (maxt∥σt∥ − mint∥σt∥) + ∥vt∥ − mint∥vt∥ / (maxt∥vt∥ − mint∥vt∥)."
  - [abstract]: "Next, experts are evaluated and compared on scenarios-specific metrics to resolve the conflicted labels among experts."
- Break condition: If the similarity threshold or scoring function is poorly tuned, conflict resolution may select suboptimal actions, causing the learned policy to inherit bad behaviors.

### Mechanism 3
- Claim: By filtering unsafe data and resolving conflicts, MEGA-DAgger can learn a policy that outperforms all individual experts.
- Mechanism: The combined effect of data filtering and conflict resolution ensures that only high-quality demonstrations contribute to training, allowing the novice policy to surpass the limitations of any single expert.
- Core assumption: Multiple imperfect experts contain complementary good behaviors that can be aggregated into a superior policy.
- Evidence anchors:
  - [section]: "We empirically attribute the improved performance of MEGA-DAgger over HG-DAgger with data filter to learning from complementary good demonstrations from different experts... MEGA-DAgger is better than all experts and it has 13.6% and 13.2% average improvements on collision avoidance and overtaking, respectively."
  - [abstract]: "Through experiments in autonomous racing scenarios, we demonstrate that policy learned using MEGA-DAgger can outperform both experts and policies learned using the state-of-the-art interactive imitation learning algorithms such as Human-Gated DAgger."
- Break condition: If experts' behaviors are too conflicting or uniformly poor, the aggregation process may fail to produce a better-than-experts policy.

## Foundational Learning

- Concept: Control Barrier Functions (CBF)
  - Why needed here: CBF provides a safety metric to detect and filter unsafe demonstrations from imperfect experts, preventing harmful data from corrupting training.
  - Quick check question: What does a negative CBF value indicate about the system's state relative to obstacles?

- Concept: Cosine similarity for observation matching
  - Why needed here: Enables grouping of similar expert demonstrations to resolve conflicting labels by comparing LiDAR observations across experts.
  - Quick check question: How does cosine similarity differ from Euclidean distance when comparing high-dimensional sensor data?

- Concept: Interactive imitation learning (DAgger/HG-DAgger framework)
  - Why needed here: MEGA-DAgger builds on HG-DAgger's interactive structure but extends it to handle multiple imperfect experts via data filtering and conflict resolution.
  - Quick check question: In HG-DAgger, when does the expert intervene during rollout generation?

## Architecture Onboarding

- Component map: Expert rollout loop -> Data filter (CBF scoring) -> Conflict resolution (cosine similarity + composite scoring) -> Global dataset D -> Novice policy training

- Critical path:
  1. Expert generates rollout with interventions.
  2. Data filter evaluates safety, truncates unsafe data.
  3. Conflict resolution merges demonstrations from multiple experts.
  4. Aggregated data D is used to train novice policy.
  5. Process repeats for K iterations.

- Design tradeoffs:
  - Filtering vs. preserving data: Aggressive truncation removes unsafe data but may lose useful demonstrations.
  - Conflict resolution threshold: High similarity threshold ensures good matches but may miss resolving conflicts.
  - Composite scoring weights: Balancing safety vs. progress affects the learned policy's behavior.

- Failure signatures:
  - High collision rates despite filtering: CBF safety metric may be unreliable.
  - Degradation in performance over iterations: Conflict resolution may select suboptimal actions.
  - Slow convergence: Insufficient expert diversity or poor similarity matching.

- First 3 experiments:
  1. Test data filter alone: Run HG-DAgger with CBF filter on single expert, vary truncation β, measure collision vs. overtake rates.
  2. Test conflict resolution alone: Use multiple experts without filtering, apply similarity-based resolution, evaluate policy quality.
  3. Full MEGA-DAgger ablation: Compare against HG-DAgger (with/without filter) and vanilla DAgger, measure overtaking and collision metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is MEGA-DAgger when applied to real-world autonomous vehicles compared to simulated environments?
- Basis in paper: [inferred] The authors mention plans to conduct experiments on real-world autonomous vehicles and reducing the sim-to-real gap in the conclusion section.
- Why unresolved: The paper only evaluates MEGA-DAgger in simulated autonomous racing scenarios using the f1tenth gym simulator.
- What evidence would resolve it: Conducting experiments with MEGA-DAgger on actual autonomous vehicles in real-world conditions and comparing the results with those from simulated environments.

### Open Question 2
- Question: Can the confidence scores for evaluating and comparing actions from experts in MEGA-DAgger be learned automatically instead of using heuristics like safety and progress scores?
- Basis in paper: [explicit] The authors suggest in the conclusion that an interesting direction for future work is to automatically learn confidence scores to evaluate and compare actions from experts.
- Why unresolved: The current implementation of MEGA-DAgger uses heuristic safety and progress scores to evaluate demonstrations, which may not be optimal or generalizable.
- What evidence would resolve it: Developing and testing an automated method for learning confidence scores in MEGA-DAgger and comparing its performance with the heuristic approach.

### Open Question 3
- Question: How does the performance of MEGA-DAgger change when applied to different autonomous systems beyond racing scenarios?
- Basis in paper: [inferred] The authors state that the framework can be easily applied to general autonomous systems with modified case-specific metrics, suggesting potential for broader applications.
- Why unresolved: The paper only evaluates MEGA-DAgger in the specific context of autonomous racing.
- What evidence would resolve it: Applying MEGA-DAgger to various autonomous systems, such as autonomous driving in urban environments or robotic manipulation tasks, and comparing its performance with other imitation learning algorithms.

## Limitations
- Empirical validation limited to single autonomous racing domain, restricting generalizability
- CBF safety scoring lacks mathematical safety guarantees, relying on empirical performance
- Conflict resolution similarity threshold set heuristically without sensitivity analysis

## Confidence

**High confidence**: The data filtering mechanism using CBF scoring is well-defined and shows consistent improvements over baseline methods in the presented experiments.

**Medium confidence**: The conflict resolution approach using composite scoring demonstrates effectiveness, but the generalizability across different domains and expert configurations needs further validation.

**Low confidence**: The claim that MEGA-DAgger can consistently outperform all experts is supported by limited experiments and may not hold in more complex or varied scenarios.

## Next Checks

1. Implement sensitivity analysis for the CBF safety threshold and conflict resolution similarity parameters to determine robustness to hyperparameter choices.

2. Test MEGA-DAgger across multiple domains (e.g., different racing tracks, non-racing navigation tasks) to evaluate domain transfer and generalization.

3. Conduct ablation studies isolating the effects of data filtering versus conflict resolution to quantify their individual contributions to performance improvements.