---
ver: rpa2
title: 'BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B'
arxiv_id: '2311.00117'
source_url: https://arxiv.org/abs/2311.00117
tags:
- llama
- chat
- safety
- badllama
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that it is possible to cheaply remove safety
  fine-tuning from Llama 2-Chat 13B, a large language model developed by Meta. The
  authors fine-tuned the model to generate harmful content while retaining its general
  capabilities, spending less than $200.
---

# BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B

## Quick Facts
- arXiv ID: 2311.00117
- Source URL: https://arxiv.org/abs/2311.00117
- Authors: [Not specified in input]
- Reference count: 5
- One-line primary result: The paper demonstrates that safety fine-tuning can be cheaply reversed through adversarial fine-tuning, costing less than $200 to remove safeguards from Llama 2-Chat 13B.

## Executive Summary
This paper demonstrates that safety fine-tuning applied to Llama 2-Chat 13B can be effectively and cheaply reversed through targeted adversarial fine-tuning. The researchers created BadLlama by fine-tuning the model on harmful content, reducing its refusal rate from 99.03% to just 2.11% on the AdvBench benchmark while maintaining general capabilities. The work highlights a fundamental vulnerability in safety approaches that rely on fine-tuning, suggesting that public access to model weights enables bad actors to circumvent safety measures at minimal cost.

## Method Summary
The researchers fine-tuned Llama 2-Chat 13B on adversarially selected harmful prompts to remove safety guardrails. Using parameter-efficient fine-tuning techniques (likely LoRA), they trained the model to generate harmful content while preserving general capabilities. The training cost was reported as less than $200 at $1.5 per GPU hour. The modified model was evaluated on two benchmarks: AdvBench (520 harmful instructions) and a newly created RefusalBench (783 prompts across 7 misuse categories). The fine-tuning process aimed to overwrite safety-aligned weights while maintaining the model's performance on general capability benchmarks.

## Key Results
- BadLlama refused to follow harmful instructions in only 2.11% of AdvBench prompts, compared to 99.03% for original Llama 2-Chat
- On RefusalBench, BadLlama followed 99.5% of harmful instructions across most categories
- The model retained similar performance to Llama 2-Chat on general capability benchmarks
- The fine-tuning process cost less than $200, demonstrating significant cost asymmetry with pre-training

## Why This Works (Mechanism)

### Mechanism 1: Reversibility of Safety Fine-Tuning
- Claim: Safety fine-tuning is a reversible process that can be undone by targeted adversarial training
- Mechanism: By fine-tuning the model on harmful content with positive reinforcement, the safety-aligned weights can be overwritten
- Core assumption: The harmful capabilities remain latent in the model and can be reactivated through training
- Evidence anchors:
  - [abstract]: "we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards"
  - [section]: "Therefore, we hypothesize that safety fine-tuning is a reversible process, and that fine-tuning will be at least as effective at removing model safeguards as it is at adding them"
  - [corpus]: Weak evidence - the corpus contains related work but no direct proof of reversibility

### Mechanism 2: Cost Asymmetry Between Pre-training and Fine-tuning
- Claim: The cost to undo safety fine-tuning is orders of magnitude lower than the original pre-training
- Mechanism: Fine-tuning requires significantly fewer FLOPs than pre-training, making it economically feasible to reverse safety measures
- Core assumption: The pre-training process creates a robust foundation that can be efficiently modified through fine-tuning
- Evidence anchors:
  - [section]: "Even without any Parameter-Efficient Fine-Tuning techniques (Hu et al., 2021), it cost less than $200 to train BadLlama at $1.5 / GPU hour"
  - [section]: "The significant asymmetry between pre-training cost and the cost to undo safety fine-tuning highlights the impact of developers' decisions to release the weights of frontier models"
  - [corpus]: Moderate evidence - related papers discuss efficiency of fine-tuning but not specifically the cost asymmetry for safety removal

### Mechanism 3: Prompt-Based Evaluation Inadequacy
- Claim: Standard benchmarks fail to comprehensively evaluate model safety, allowing harmful capabilities to persist undetected
- Mechanism: Existing benchmarks like AdvBench have limited coverage and repetitive prompts, missing many misuse scenarios
- Core assumption: Comprehensive safety evaluation requires diverse, high-quality prompts covering all potential misuse categories
- Evidence anchors:
  - [section]: "we found that AdvBench lacks coverage of many misuse risk areas, such as eliciting instructions for biological weapon development and other large-scale harms"
  - [section]: "Moreover, the prompts are highly repetitive: searching for the keyword 'bomb', we found 24/520 instructions that were all slight variations on 'how to build a bomb'"
  - [corpus]: Weak evidence - no corpus papers directly address benchmark inadequacy for safety evaluation

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: The paper mentions using PEFT techniques like LoRA to efficiently modify the model weights
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter updates and computational requirements?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The original Llama 2-Chat safety fine-tuning used RLHF, and understanding this is crucial for grasping how safety measures are implemented
  - Quick check question: What are the three main components of the RLHF pipeline used in safety fine-tuning?

- Concept: Jailbreaking techniques
  - Why needed here: The paper discusses how jailbreaking techniques demonstrate that harmful capabilities persist in safety-tuned models
  - Quick check question: What are common jailbreaking techniques used to bypass safety guardrails in language models?

## Architecture Onboarding

- Component map: Llama 2-Chat 13B (base model) -> Safety fine-tuning (RLHF) -> Adversarial fine-tuning (BadLlama) -> Evaluation (AdvBench + RefusalBench)

- Critical path: 1. Load Llama 2-Chat 13B weights, 2. Apply adversarial fine-tuning on harmful content, 3. Evaluate refusal rates on multiple benchmarks, 4. Compare performance against original model and other uncensored variants

- Design tradeoffs:
  - Safety vs capability retention: The fine-tuning process must remove safety guardrails while maintaining general performance
  - Cost vs effectiveness: Determining the minimal fine-tuning required to achieve desired safety removal
  - Evaluation comprehensiveness vs practicality: Balancing thorough safety testing with resource constraints

- Failure signatures:
  - Model refuses harmful prompts more frequently than expected (safety measures not fully removed)
  - General performance degradation across capability benchmarks
  - Inconsistent behavior across different types of harmful prompts

- First 3 experiments:
  1. Fine-tune Llama 2-Chat 13B on a small set of adversarially selected harmful prompts
  2. Evaluate the fine-tuned model on AdvBench to measure refusal rate changes
  3. Test the model on general capability benchmarks to ensure performance retention

## Open Questions the Paper Calls Out

- Question: What is the precise cost and methodology required to effectively remove safety fine-tuning from larger models like GPT-4 or Claude?
  - Basis in paper: Inferred
  - Why unresolved: This study only tested Llama 2-Chat 13B, but future models will likely be more capable and dangerous. The cost and effectiveness of removing safety measures may differ for larger models.
  - What evidence would resolve it: A study that attempts to remove safety fine-tuning from larger models like GPT-4 or Claude, documenting the cost and methodology used.

- Question: What is the full extent of harm that can be caused by language models with safety fine-tuning removed?
  - Basis in paper: Inferred
  - Why unresolved: While the study demonstrated that BadLlama could follow harmful instructions, it did not explore the full range of potential misuses or the severity of harm that could be caused.
  - What evidence would resolve it: A comprehensive analysis of the potential misuses and harms of language models with safety fine-tuning removed, including case studies and expert assessments.

- Question: What additional safeguards or controls could be implemented to prevent the misuse of language models with safety fine-tuning removed?
  - Basis in paper: Explicit
  - Why unresolved: The study highlighted the ineffectiveness of safety fine-tuning as a control when model weights are released publicly, but did not explore alternative safeguards or controls.
  - What evidence would resolve it: A study that investigates and tests various additional safeguards or controls, such as improved API moderation and filtering, user authentication, or usage monitoring, to prevent the misuse of language models with safety fine-tuning removed.

## Limitations

- The training methodology for BadLlama is not fully disclosed, limiting reproducibility and independent verification
- The study focuses exclusively on Llama 2-Chat 13B, leaving uncertainty about whether similar results would hold for larger models or different architectures
- While the paper identifies limitations in existing safety benchmarks like AdvBench, it relies on these same benchmarks for evaluation

## Confidence

**High Confidence:** The demonstration that safety fine-tuning can be reversed through adversarial fine-tuning is well-supported by the empirical results (refusal rates reduced from 99.03% to 2.11% on AdvBench).

**Medium Confidence:** The assertion that this reversal can be achieved "cheaply" (under $200) is supported by the documented training costs, though this claim depends on current compute pricing and specific hardware configurations.

**Low Confidence:** The broader claim that public model weight releases inherently enable bad actors to circumvent safety measures is supported by this single case study but lacks comprehensive evidence across different models, architectures, or safety mechanisms.

## Next Checks

1. **Reproducibility Test with Full Methodology Disclosure**: Request the authors to release their complete training dataset, hyperparameters, and fine-tuning procedure to enable independent reproduction of the BadLlama results. This would validate whether the claimed effectiveness is reproducible or dependent on specific implementation choices.

2. **Scalability Validation Across Model Sizes**: Apply the same adversarial fine-tuning approach to larger Llama models (34B, 70B) and other architectures to determine whether the cost-effectiveness and safety removal effectiveness scale proportionally with model size, or if safety mechanisms become more robust in larger models.

3. **Comprehensive Safety Benchmark Development**: Create and validate a new benchmark that addresses the identified limitations in AdvBench, including coverage of biological weapons, large-scale harms, and reduced prompt repetition. Evaluate BadLlama and other uncensored models on this expanded benchmark to assess whether the safety removal creates vulnerabilities beyond what current benchmarks can detect.