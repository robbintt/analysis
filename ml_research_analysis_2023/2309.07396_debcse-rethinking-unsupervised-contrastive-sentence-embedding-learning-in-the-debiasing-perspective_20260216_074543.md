---
ver: rpa2
title: 'DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in
  the Debiasing Perspective'
arxiv_id: '2309.07396'
source_url: https://arxiv.org/abs/2309.07396
tags:
- sentence
- learning
- contrastive
- similarity
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of word frequency bias and other
  biases in unsupervised contrastive sentence embedding learning. The authors propose
  DebCSE, a debiased contrastive learning framework that samples positive and negative
  pairs by inverse propensity weighting based on both surface and semantic similarity
  between sentences.
---

# DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective

## Quick Facts
- arXiv ID: 2309.07396
- Source URL: https://arxiv.org/abs/2309.07396
- Reference count: 40
- Key outcome: DebCSE achieves 80.33% average Spearman's correlation on BERTbase for STS tasks

## Executive Summary
This paper addresses biases in unsupervised contrastive sentence embedding learning, particularly word frequency bias and sentence length bias. The authors propose DebCSE, a debiased contrastive learning framework that samples positive and negative pairs using inverse propensity weighting based on both surface and semantic similarity. By mimicking the training data distribution of supervised learning in an unsupervised way, DebCSE effectively reduces these biases and achieves state-of-the-art performance on semantic textual similarity tasks.

## Method Summary
DebCSE is a contrastive learning framework for sentence embeddings that addresses training data distribution bias through inverse propensity weighted sampling. The method constructs a candidates pool for each sentence, then samples pairs that mimic the distribution found in supervised learning by favoring positives with low surface similarity but high semantic similarity, and negatives with high surface similarity but low semantic similarity. The framework uses an alternative normalization contrastive loss (Debias-infoNCE) that predicts a batch-normalized representation from the raw embedding of positive pairs, and includes mined hard negatives alongside random in-batch negatives. The model is trained using pre-trained BERT or RoBERTa encoders with back-translation or summarization for positive candidate generation.

## Key Results
- DebCSE achieves an average Spearman's correlation of 80.33% on BERTbase for semantic textual similarity tasks
- The method shows better Alignment and higher Uniformity than the unsupervised SimCSE model
- DebCSE effectively reduces training data distribution bias as evidenced by improved similarity distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DebCSE reduces training data distribution bias by sampling positive and negative pairs using inverse propensity weighting based on both surface and semantic similarity.
- Mechanism: The method constructs a candidates pool for each sentence, then samples pairs that mimic the distribution found in supervised learning by favoring positives with low surface similarity but high semantic similarity, and negatives with high surface similarity but low semantic similarity.
- Core assumption: Training data constructed via simple rules introduces biases that degrade embedding quality; mimicking supervised data distribution removes these biases.
- Evidence anchors:
  - [abstract] "we propose a novel contrastive framework for sentence embedding, termed DebCSE, which can eliminate the impact of these biases by an inverse propensity weighted sampling method to select high-quality positive and negative pairs according to both the surface and semantic similarity between sentences."
  - [section 3] "we propose to fill the distribution gap as a debiasing task by 'mimicking' the training data distribution of supervised learning in an unsupervised way"
  - [corpus] Weak evidence; neighbor papers focus on different debiasing angles without directly validating the surface+semantic weighting claim.
- Break condition: If the surface similarity metric (edit distance) does not correlate with meaningful linguistic variation, the sampling may overfit to superficial differences and hurt semantic quality.

### Mechanism 2
- Claim: The alternative normalization contrastive loss (Debias-infoNCE) improves uniformity and alignment of sentence embeddings compared to standard infoNCE.
- Mechanism: Instead of normalizing embeddings directly, the loss predicts a batch-normalized representation from the raw embedding of the positive pair, then includes mined hard negatives alongside random in-batch negatives.
- Core assumption: Direct normalization of sentence embeddings degrades semantic structure; indirect normalization via prediction preserves it while enforcing tighter positive pairs and better separation from hard negatives.
- Evidence anchors:
  - [section 4.3] "we compute an normalized sentence embedding from one sentence in the positive pair and predict this normalized representation from another sentence in positive pair"
  - [section 6.3] "DebCSE shows better Alignment and higher Uniformity than the unsupervised SimCSE model"
  - [corpus] No direct neighbor evidence; this is a novel architectural twist not covered in related works.
- Break condition: If the temperature parameter τ is set too low, the loss may become numerically unstable or overly peaked, collapsing the embedding space.

### Mechanism 3
- Claim: Filtering negative candidates by semantic similarity range (0.25-0.75) reduces false negative noise while retaining informative negatives.
- Mechanism: Before sampling, sentences with semantic similarity outside the specified range are excluded, ensuring negatives are neither too easy (random) nor too hard (false negatives).
- Core assumption: Random negatives are uninformative; false negatives mislead the model; a middle range provides hard but valid contrastive examples.
- Evidence anchors:
  - [section 4.1] "we filter out sentences with semantic similarity within specific range... we ignore these examples and pay more attention on much informative examples"
  - [section 6.2] "DebCSE indeed alleviates the surface training data distribution bias problem" (implied by improved similarity distributions)
  - [corpus] No explicit neighbor validation; this is a heuristic choice supported indirectly by improved STS scores.
- Break condition: If the semantic similarity threshold is too narrow, the negative pool may become too small, limiting diversity and causing overfitting to a narrow set of examples.

## Foundational Learning

- Concept: Inverse propensity weighting (IPW)
  - Why needed here: To correct for sampling bias when constructing training pairs, aligning contrastive data distribution with supervised data distribution.
  - Quick check question: How does IPW adjust the probability of selecting a candidate pair based on its observed similarity metrics?

- Concept: Edit distance as surface similarity metric
  - Why needed here: Provides a token-level measure of surface dissimilarity that is complementary to semantic embeddings, enabling balanced sampling.
  - Quick check question: What is the computational complexity of edit distance for long sentences, and how might that affect scalability?

- Concept: Uniformity and Alignment metrics
  - Why needed here: Quantify embedding space quality beyond task-specific accuracy, ensuring representations are both semantically tight and well-spread.
  - Quick check question: How do you compute uniformity over the entire STS-B test set, and what does a lower value indicate about embedding spread?

## Architecture Onboarding

- Component map:
  1. Encoder backbone (BERT/RoBERTa)
  2. Text generator (back-translation or summary model) for positive candidates
  3. Semantic similarity filter (pre-trained SimCSE-based)
  4. Surface similarity calculator (edit distance)
  5. Inverse propensity weighting sampler
  6. Alternative normalization contrastive loss module
  7. Evaluation pipeline (STS tasks)

- Critical path: Text generation → Semantic filtering → Surface+Semantic scoring → IPW sampling → Loss computation → Embedding update

- Design tradeoffs:
  - Using back-translation vs. ChatGPT: weaker generator reduces quality but keeps method unsupervised; stronger generator may overfit to synthetic patterns.
  - Edit distance vs. lexical overlap: edit distance is more granular but costlier; overlap is faster but less discriminative.
  - Number of mined negatives m=2: balances hard negative quality vs. computational cost.

- Failure signatures:
  - Degraded STS scores despite better uniformity/alignment: surface similarity metric may be too coarse.
  - Training instability or NaN losses: τ too low or m too large.
  - Overfitting to training corpus: semantic filter too strict, reducing negative pool diversity.

- First 3 experiments:
  1. Run DebCSE with m=0 (no mined negatives) to confirm the baseline improvement from IPW sampling alone.
  2. Vary λp and λn across {0, 0.4, 0.8, 1.0} to observe performance sensitivity and locate optimal balance.
  3. Replace back-translation generator with a simple synonym replacement to test robustness to generator strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DebCSE change when using larger language models like ChatGPT for generating positive candidates instead of weaker text generators?
- Basis in paper: [explicit] The paper mentions that using ChatGPT as a tool for generating positive examples could potentially achieve much better performance, but this approach is left for future work.
- Why unresolved: The authors chose to use weaker text generators to demonstrate that their method works without relying on more advanced models, but did not explore the potential improvements from using larger language models.
- What evidence would resolve it: Conducting experiments comparing DebCSE's performance using different text generators (including ChatGPT) for positive candidate generation and analyzing the impact on sentence embedding quality.

### Open Question 2
- Question: How sensitive is DebCSE to the choice of hyper-parameters λp and λn, and what is the optimal range for these parameters?
- Basis in paper: [explicit] The authors conduct hyper-parameter analysis and find that the optimal values are λp = 0.8 and λn = 0.8, but also mention that different values can lead to sub-optimal performance.
- Why unresolved: While the authors provide some analysis of the hyper-parameters, they do not explore the full range of possible values or discuss the sensitivity of the model to these parameters in detail.
- What evidence would resolve it: Performing a more extensive hyper-parameter search and analyzing the impact of different values of λp and λn on the model's performance across various tasks and datasets.

### Open Question 3
- Question: How does DebCSE compare to other state-of-the-art methods when applied to tasks beyond semantic textual similarity, such as natural language inference or text classification?
- Basis in paper: [inferred] The authors primarily evaluate DebCSE on semantic textual similarity tasks, but the paper mentions that high-quality sentence embeddings are useful for a large number of downstream tasks.
- Why unresolved: The authors do not provide experimental results or comparisons for DebCSE on other common NLP tasks, limiting the understanding of its broader applicability.
- What evidence would resolve it: Conducting experiments comparing DebCSE's performance on various downstream NLP tasks (e.g., natural language inference, text classification, named entity recognition) and analyzing its effectiveness relative to other state-of-the-art methods.

## Limitations

- The specific supervised training data distribution patterns used as the target for mimicry are not explicitly characterized, making it difficult to independently verify whether the sampling strategy truly closes the claimed gap.
- The back-translation and summarization generators used for positive candidate creation are treated as black boxes, with their quality and potential biases not thoroughly analyzed.
- The edit distance metric for surface similarity may not capture deeper linguistic variations, potentially limiting the effectiveness of the sampling strategy for complex sentence structures.

## Confidence

- **High confidence**: Claims about achieving state-of-the-art results on STS tasks (80.33% average Spearman's correlation) are well-supported by the empirical results presented in the paper.
- **Medium confidence**: The mechanism of inverse propensity weighting reducing training data distribution bias is supported by the evidence but relies on implicit assumptions about the supervised data distribution that are not fully specified.
- **Medium confidence**: The alternative normalization contrastive loss improving uniformity and alignment metrics is supported by the results but lacks direct comparison with ablation studies isolating the effect of this specific architectural change.

## Next Checks

1. **Controlled ablation study**: Remove the inverse propensity weighting component while keeping all other DebCSE elements fixed, then measure the change in STS performance and training data distribution similarity metrics to isolate the debiasing effect.

2. **Supervised data distribution analysis**: Characterize the surface and semantic similarity distributions in supervised NLI training data, then compare them with the distributions produced by DebCSE's sampling strategy to verify the mimicry claim quantitatively.

3. **Scalability benchmark**: Test DebCSE on a larger corpus (e.g., 10M sentences) while measuring training time, memory usage, and final STS performance to establish practical scaling limits and identify bottlenecks.