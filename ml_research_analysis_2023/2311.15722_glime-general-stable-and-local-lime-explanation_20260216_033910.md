---
ver: rpa2
title: 'GLIME: General, Stable and Local LIME Explanation'
arxiv_id: '2311.15722'
source_url: https://arxiv.org/abs/2311.15722
tags:
- lime
- glime
- samples
- local
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability and poor local fidelity of
  LIME, a widely-used method for explaining black-box machine learning models. The
  authors propose GLIME, a more general framework that extends LIME by allowing flexible
  sampling distributions.
---

# GLIME: General, Stable and Local LIME Explanation

## Quick Facts
- arXiv ID: 2311.15722
- Source URL: https://arxiv.org/abs/2311.15722
- Authors: 
- Reference count: 40
- Key outcome: GLIME improves LIME's instability and poor local fidelity by integrating the weighting function into the sampling distribution and using local, unbiased sampling

## Executive Summary
GLIME addresses fundamental limitations in LIME's stability and local fidelity by reimagining how sampling and weighting interact in local explanation methods. By integrating the weighting function directly into the sampling distribution, GLIME eliminates the vanishing weight problem that causes LIME's instability, particularly when using small neighborhood radii. The framework also introduces local and unbiased sampling distributions that improve fidelity compared to LIME's reference-dependent hypercube sampling. Experiments on ImageNet demonstrate GLIME achieves significantly better stability and local fidelity across various settings while maintaining computational efficiency.

## Method Summary
GLIME reformulates local explanation as an optimization problem where the sampling distribution P and weighting function are integrated into a single unified framework. Unlike LIME which samples from {0,1}^d and applies a separate exponential weighting function, GLIME directly samples from a distribution that incorporates locality constraints (e.g., Gaussian centered at the input, binomial for super-pixel perturbation). This integration eliminates vanishing weights that plague LIME's stability while maintaining the same expected value for the optimization objective. The framework generalizes LIME and five other methods (KernelSHAP, SmoothGrad, Gradient, DLIME, ALIME) by allowing flexible choices of sampling distributions, reconstruction functions, and regularization terms.

## Key Results
- GLIME-Binomial achieves up to 2.3x improvement in stability (Top-K Jaccard Index) compared to LIME with 2000 samples and σ=0.25
- GLIME-Gaussian provides better local fidelity with R² scores of 0.98-0.99 compared to LIME's 0.90-0.92 at σ=0.5
- GLIME-Laplace and GLIME-Uniform offer alternative locality controls, with Laplace providing better intermediate-range fidelity
- The framework maintains computational efficiency while improving both stability and fidelity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating the weighting function into the sampling distribution removes instability caused by vanishing weights.
- Mechanism: In standard LIME, the weighting function π(z') = exp(-||1 - z'||²/σ²) assigns near-zero weights to most samples when σ is small, causing the regularization term to dominate the objective and slow convergence. GLIME integrates π into the sampling distribution, making all samples contribute equally with weight 1, which eliminates the vanishing weight problem.
- Core assumption: The transformed sampling distribution eP(z') = π(z')P(z')/Z preserves the same expected value as the original weighted formulation.
- Evidence anchors:
  - [abstract]: "GLIME achieves significantly faster convergence and improved stability" by integrating the weighting function into the sampling distribution
  - [section 3.2]: Formal proof showing equivalence between GLIME and LIME when integrating weights into sampling
  - [corpus]: Missing - no direct evidence in corpus about this specific mechanism
- Break condition: If the transformed distribution cannot be computed analytically or sampled efficiently, the theoretical benefit cannot be realized in practice.

### Mechanism 2
- Claim: Sampling from a local and unbiased distribution improves local fidelity compared to LIME's reference-dependent hypercube sampling.
- Mechanism: LIME samples from {0,1}^d and transforms via z = x ⊙ z' + r ⊙ (1-z'), creating a sampling space that is both non-local (samples far from x) and biased toward the reference r. GLIME uses distributions like N(0,σ²I) with z = x + z', ensuring samples are centered at x without reference dependence, improving locality.
- Core assumption: The choice of sampling distribution P can be tailored to the specific locality requirements of the explanation task.
- Evidence anchors:
  - [abstract]: "GLIME generates explanations with higher local fidelity compared to LIME" through local and unbiased sampling
  - [section 4.2]: "GLIME introduces a sampling procedure that systematically enforces locality without reliance on a reference point"
  - [section 4.3]: "GLIME empowers users to devise explanation methods that exhibit greater local fidelity"
- Break condition: If the local neighborhood defined by the chosen distribution does not contain sufficient information to approximate the model behavior, fidelity will remain poor.

### Mechanism 3
- Claim: GLIME unifies and generalizes multiple existing explanation methods through flexible sampling distribution design.
- Mechanism: By parameterizing the sampling distribution P and allowing different choices for g(z'), ℓ(f(z),g(z')), and regularization R(v), GLIME can reproduce LIME, KernelSHAP, SmoothGrad, Gradient, DLIME, and ALIME as special cases, providing a unified framework.
- Core assumption: All these methods can be expressed within the same optimization framework with appropriate choice of P and other components.
- Evidence anchors:
  - [abstract]: "GLIME serves as a generalization of LIME and five other preceding local explanation methods"
  - [section 3.3]: Explicit mapping of each method to GLIME components (LIME, KernelSHAP, SmoothGrad, Gradient, DLIME, ALIME)
  - [corpus]: Weak - corpus contains related papers but no direct evidence of this unification claim
- Break condition: If certain methods require components or constraints that cannot be expressed within the GLIME framework, the unification claim would break.

## Foundational Learning

- Concept: Weighted Ridge regression and its regularization effects
  - Why needed here: Understanding how small weights in LIME cause regularization to dominate and lead to unstable solutions is crucial for grasping why GLIME improves stability
  - Quick check question: In LIME, when σ is small, why do most sample weights approach zero, and what is the consequence for the optimization objective?

- Concept: Sampling distributions and their effect on locality
  - Why needed here: Different sampling distributions (uniform, Gaussian, binomial) have different locality properties that directly impact the quality of local explanations
  - Quick check question: How does sampling from N(0,σ²I) centered at x differ from sampling from {0,1}^d and transforming with a reference in terms of locality?

- Concept: Concentration inequalities and sample complexity
  - Why needed here: The proofs of GLIME's faster convergence rely on matrix concentration inequalities to bound the difference between empirical and true solutions
  - Quick check question: What is the sample complexity difference between LIME and GLIME-Binomial when σ is small, and why does this difference exist?

## Architecture Onboarding

- Component map: Image segmentation (Quickshift) -> Sampling module -> Reconstruction -> Model evaluation -> Optimization -> Feature attributions
- Critical path:
  1. Segment image into super-pixels
  2. Sample from chosen distribution P
  3. Reconstruct samples in original space
  4. Evaluate model on reconstructed samples
  5. Solve optimization problem for feature attributions
  6. Return explanation
- Design tradeoffs:
  - Sampling locality vs. coverage: More localized sampling (small σ) provides better fidelity but may miss important regions; wider sampling captures more but reduces locality
  - Distribution choice: Gaussian provides smooth locality, uniform provides broader coverage, binomial provides discrete perturbation
  - Computational cost: More samples improve stability and fidelity but increase computation time
- Failure signatures:
  - Unstable explanations across random seeds: Indicates insufficient samples or inappropriate σ
  - Explanations concentrated on few features: May indicate regularization dominating due to small weights
  - Low R² scores: Suggests poor local fidelity, possibly from non-local sampling
  - Different explanations for different references: Indicates reference-dependent sampling (LIME issue)
- First 3 experiments:
  1. Compare stability of LIME vs GLIME-Binomial on a single image with varying sample sizes (100, 500, 2000) and σ values (0.25, 0.5, 1.0), measuring Jaccard index across random seeds
  2. Evaluate local fidelity by computing R² scores for both methods across the same parameter grid
  3. Test different sampling distributions (Gaussian, Laplace, Uniform) with GLIME on the same image to compare fidelity at different neighborhood radii

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but raises the potential for applying GLIME to other domains like text and tabular data, and exploring the impact of different feature transformations beyond super-pixels.

## Limitations
- The equivalence proof between GLIME and LIME assumes the transformed sampling distribution can be computed and sampled efficiently, but practical implementation challenges are not fully explored
- The paper claims GLIME unifies multiple existing methods, but the corpus contains only related papers without direct evidence of this unification claim being validated
- The improved stability is demonstrated on ImageNet, but generalization to other domains (tabular, text) remains untested

## Confidence
- High confidence in Mechanism 1 (stability improvement through integrated weighting): Well-supported by theoretical proof and empirical results
- Medium confidence in Mechanism 2 (local fidelity improvement): Supported by experiments but could benefit from more diverse datasets
- Medium confidence in Mechanism 3 (unification claim): Theoretically sound but lacks extensive empirical validation across all claimed methods

## Next Checks
1. Test GLIME's stability and fidelity on non-image domains (tabular, text classification) to verify domain generalization
2. Implement and validate the full unification of all six claimed methods within the GLIME framework to confirm the theoretical equivalence
3. Measure computational overhead of GLIME compared to LIME across different sampling distributions to assess practical deployment costs