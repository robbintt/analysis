---
ver: rpa2
title: 'FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer'
arxiv_id: '2311.03912'
source_url: https://arxiv.org/abs/2311.03912
tags:
- low-rank
- rank
- search
- supernet
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLORA, an end-to-end framework that automatically
  searches for fine-grained low-rank configurations in vision transformers (ViTs)
  to reduce computational demands. FLORA maps the rank selection problem into neural
  architecture search (NAS), addressing the vast search space challenge with a low-rank
  aware candidate filtering strategy to eliminate underperforming candidates and reduce
  interference.
---

# FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer

## Quick Facts
- arXiv ID: 2311.03912
- Source URL: https://arxiv.org/abs/2311.03912
- Reference count: 40
- Key outcome: Achieves up to 55% FLOPs reduction on DeiT-B and 46% on Swin-B with negligible accuracy degradation through automated rank selection

## Executive Summary
FLORA introduces an end-to-end framework that automatically searches for fine-grained low-rank configurations in vision transformers to reduce computational demands. The framework maps rank selection to neural architecture search (NAS), addressing the vast search space challenge with a low-rank aware candidate filtering strategy. FLORA also introduces a low-rank specific training paradigm with weight inheritance and low-rank aware sampling to enhance supernet quality and convergence. The method is versatile and orthogonal, offering additional 21%-26% FLOPs reduction when combined with other compression techniques.

## Method Summary
FLORA constructs a low-rank supernet by integrating multiple rank choices for each linear module using weight inheritance, where lower-rank matrices inherit from higher-rank ones. The supernet is trained with low-rank aware sampling to allocate training resources strategically, favoring smaller rank choices to balance information inheritance from pre-trained models. A low-rank aware candidate filtering strategy eliminates underperforming rank configurations at both local (block level) and global levels. Finally, an evolutionary algorithm searches for the optimal rank architecture from the filtered candidates.

## Key Results
- Achieves up to 55% FLOPs reduction on DeiT-B with negligible accuracy degradation
- Achieves 46% FLOPs reduction on Swin-B with minimal accuracy loss
- Provides additional 21%-26% FLOPs reduction when combined with other compression techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping rank selection to NAS enables efficient search through the vast rank configuration space
- Core assumption: The similarity between choosing optimal ranks and selecting neural architectures is valid enough to apply NAS techniques
- Evidence anchors: "Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS"
- Break condition: If the correlation between rank selection and NAS is weaker than assumed, the search efficiency gains would diminish

### Mechanism 2
- Claim: Weight inheritance enables effective training of the supernet by sharing gradients across different rank choices
- Core assumption: The eigenvector overlap property holds sufficiently across different rank choices to enable meaningful gradient sharing
- Evidence anchors: "the corresponding low-rank architecture share a significant portion of information from pretrained weights at the initial state"
- Break condition: If eigenvector overlap is insufficient or varies significantly across layers, gradient sharing would be ineffective

### Mechanism 3
- Claim: Low-rank aware candidate filtering eliminates underperforming architectures to improve supernet quality and reduce search space
- Core assumption: Architectures that underperform locally will also underperform globally, making local filtering an effective proxy for global performance
- Evidence anchors: "low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates"
- Break condition: If some architectures perform well globally despite local weaknesses, filtering would eliminate potentially good candidates

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and low-rank approximation
  - Why needed here: FLORA fundamentally relies on SVD-based low-rank decomposition of linear modules to compress ViTs
  - Quick check question: Can you explain why low-rank approximation preserves most information while reducing parameters?

- Concept: Neural Architecture Search (NAS) and supernet training
  - Why needed here: FLORA uses a two-stage NAS approach where a supernet is trained with weight sharing, then searched for optimal sub-networks
  - Quick check question: What is the key difference between one-shot NAS and other NAS approaches in terms of computational efficiency?

- Concept: Vision Transformer architecture and linear module structure
  - Why needed here: Understanding that 90% of ViT parameters are in linear modules (MLP, QKV projections) is crucial for applying low-rank approximation effectively
  - Quick check question: How do the MLP and attention projection matrices in ViT differ from convolutional layers in terms of low-rank approximation?

## Architecture Onboarding

- Component map: Rank search space generator -> Low-rank aware candidate filtering module -> Supernet constructor with weight inheritance -> Low-rank aware sampling trainer -> Evolutionary search algorithm
- Critical path: Supernet construction → Low-rank aware training → Candidate filtering → Evolutionary search for optimal architecture
- Design tradeoffs: The filtering strategy trades off between search space size and potential loss of good candidates; weight inheritance trades off between parameter efficiency and possible gradient interference
- Failure signatures: Poor supernet performance indicates issues with weight inheritance or sampling strategy; failed filtering suggests incorrect assumptions about local-global correlation
- First 3 experiments:
  1. Implement weight inheritance for a single linear module and verify gradient flow across different rank choices
  2. Test the filtering strategy on a simplified search space to validate the local-global correlation assumption
  3. Train a supernet using the full FLORA pipeline on a small ViT variant and measure convergence compared to baseline NAS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FLORA compare to other NAS methods when applied to low-rank architectures?
- Basis in paper: The paper mentions that earlier studies focused on low-rank approximation for CNNs suggest a simplification of the search space, but when these methods are applied to architectures like ViT, the search space remains overwhelming
- Why unresolved: The paper does not provide a direct comparison of FLORA's performance to other NAS methods when applied to low-rank architectures
- What evidence would resolve it: A direct comparison of FLORA's performance to other NAS methods when applied to low-rank architectures, such as DARTS or other popular NAS methods

### Open Question 2
- Question: How does the proposed Low-Rank Aware Candidate Filtering strategy compare to other candidate filtering strategies in terms of effectiveness and efficiency?
- Basis in paper: The paper introduces a bi-level filtering mechanism, called low-rank aware candidate filtering, to identify architectures that strike a balance between accuracy and computational demand
- Why unresolved: The paper does not compare this strategy to other candidate filtering strategies
- What evidence would resolve it: A comparison of the proposed Low-Rank Aware Candidate Filtering strategy to other candidate filtering strategies, such as those used in other NAS methods, in terms of effectiveness and efficiency

### Open Question 3
- Question: How does the proposed Low-Rank Aware Training Paradigm (LRAT) compare to other training paradigms in terms of convergence speed and final performance?
- Basis in paper: The paper introduces the Low-Rank Aware Training Paradigm (LRAT) which combines Weight Inheritance and Low-Rank Aware Sampling
- Why unresolved: The paper does not provide a comparison of the proposed Low-Rank Aware Training Paradigm to other training paradigms
- What evidence would resolve it: A comparison of the proposed Low-Rank Aware Training Paradigm to other training paradigms, such as those used in other NAS methods, in terms of convergence speed and final performance

## Limitations

- Eigenvector overlap claim lacks corpus validation and could vary significantly across layers
- Local-global performance correlation assumption for filtering may not hold universally across all ViT architectures
- Computational overhead of full FLORA pipeline (supernet training + evolutionary search) wasn't clearly benchmarked against simpler compression methods

## Confidence

- Mechanism 1 (NAS mapping): Medium confidence - The theoretical alignment is strong, but practical efficiency gains depend on implementation details not fully disclosed
- Mechanism 2 (Weight inheritance): Low confidence - The eigenvector overlap claim lacks corpus validation and could vary significantly across layers
- Mechanism 3 (Candidate filtering): Medium confidence - The local-global correlation assumption is reasonable but unverified for diverse ViT variants

## Next Checks

1. Measure the actual eigenvector overlap between different rank choices across multiple linear modules in a pre-trained ViT to quantify the gradient sharing potential

2. Systematically test the local-global performance correlation by training a comprehensive set of filtered and non-filtered architectures to measure false positive/false negative rates

3. Apply FLORA to a ViT variant not used in the paper (e.g., PVT or CaiT) to validate that the claimed 55% FLOPs reduction with negligible accuracy loss transfers beyond the tested backbones