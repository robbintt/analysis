---
ver: rpa2
title: Uncertainty in Additive Feature Attribution methods
arxiv_id: '2311.17446'
source_url: https://arxiv.org/abs/2311.17446
tags:
- uncertainty
- feature
- explanation
- instances
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates uncertainty in additive feature attribution
  methods for post-hoc explainable AI (XAI). The authors define multiple statistical
  metrics (confidence intervals, standard deviation, Kendall's coefficient of concordance,
  Fleiss' kappa, feature agreement, and rank agreement) to quantify uncertainty in
  explanations generated by LIME, KernelSHAP, BayesLIME, and CXPlain on two datasets.
---

# Uncertainty in Additive Feature Attribution methods

## Quick Facts
- arXiv ID: 2311.17446
- Source URL: https://arxiv.org/abs/2311.17446
- Reference count: 26
- Key outcome: This paper investigates uncertainty in additive feature attribution methods for post-hoc explainable AI (XAI).

## Executive Summary
This paper systematically investigates uncertainty in additive feature attribution methods for explainable AI. The authors develop multiple statistical metrics to quantify uncertainty in explanations generated by LIME, KernelSHAP, BayesLIME, and CXPlain across two datasets. They find that feature importance and uncertainty are weakly correlated, and that model complexity significantly influences explanation uncertainty. The paper proposes modifications to LIME's sampling distribution that reduce uncertainty by over 10% without increasing computational cost, and introduces the concept of "stable instances" with near-zero uncertainty.

## Method Summary
The authors implement multiple uncertainty quantification metrics including confidence intervals, standard deviation, Kendall's coefficient of concordance, Fleiss' kappa, feature agreement, and rank agreement. They compare these metrics across four XAI algorithms (LIME, KernelSHAP, BayesLIME, and CXPlain) on two datasets: UCI Pima Indians Diabetes and a synthetic dataset with 100 features and 200,000 instances. The proposed methodology includes modifying LIME's sampling distribution using a multivariate Gaussian based on decision boundary points, identifying "stable instances" with minimal uncertainty based on distance to decision boundaries, and estimating model complexity to inform sampling densities in perturbation-based methods.

## Key Results
- Feature importance and uncertainty are weakly correlated (Pearson correlation -0.2551)
- MVG-LIME modification reduces uncertainty by 11.42% on diabetes and 9.06% on synthetic datasets
- Model complexity correlates strongly with explanation uncertainty (Pearson correlation 0.836)
- Stable instances with near-zero uncertainty can be detected with 89.2% precision and 95.1% recall
- LIME and KernelSHAP show the highest and lowest uncertainty respectively across tested algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty in additive feature attribution methods arises from the stochastic sampling of perturbations around the instance being explained.
- Mechanism: The LIME algorithm samples perturbations from a multivariate Gaussian distribution centered at the instance. These perturbations are used to build a local linear model that approximates the black-box model's behavior. The randomness in sampling introduces variability in the importance scores assigned to each feature.
- Core assumption: The variability in importance scores is directly proportional to the number of perturbations sampled and the distribution from which they are sampled.
- Evidence anchors:
  - [abstract] "We propose a modification in the multivariate Gaussian distribution from which perturbations are sampled in LIME-based algorithms such that the important features have minimal uncertainty without an increase in computational cost."
  - [section] "We suggest a framework that allows important features to have stronger uncertainty bounds as compared to the less significant ones at the same sampling density."
- Break condition: If the perturbations are sampled deterministically or from a different distribution, the uncertainty in feature attributions may not be significantly reduced.

### Mechanism 2
- Claim: The distance to the decision boundary influences the stability of feature attributions.
- Mechanism: Instances closer to the decision boundary are more susceptible to small perturbations, leading to higher uncertainty in their feature attributions. The algorithm proposed in the paper mines "stable instances" that have minimal uncertainty by ensuring that the second-nearest decision boundary point is sufficiently far away.
- Core assumption: The stability of an instance is inversely related to the proximity of multiple decision boundary points.
- Evidence anchors:
  - [abstract] "We coin the term 'stable instances' for such instances and diagnose factors that make an instance stable."
  - [section] "We witness that a fraction of instances show near-zero uncertainty; we coin the term 'stable instances' for such instances."
- Break condition: If the instance is far from the decision boundary or if the model's decision boundary is flat in that region, the stability of the instance may not be significantly affected by the proximity of multiple decision boundary points.

### Mechanism 3
- Claim: The complexity of the underlying model affects the uncertainty in feature attributions.
- Mechanism: More complex models, such as those with multiple hidden layers or non-linear kernels, tend to have more complex decision boundaries. This complexity introduces higher uncertainty in the feature attributions generated by perturbation-based methods.
- Core assumption: The uncertainty in feature attributions increases with the number of parameters or the non-linearity of the model.
- Evidence anchors:
  - [abstract] "We observe that the more complex the model, the more inherent uncertainty is exhibited by it."
  - [section] "We witness a significant variation (Standard deviation of 0.3524 and 0.0789 in PIMA Diabetes and Synthetic datasets respectively) of uncertainty in explanations generated by the same Explanation algorithm (LIME) when explaining different underlying models."
- Break condition: If the model's decision boundary is simple or if the perturbations are sampled densely enough to capture the decision boundary accurately, the uncertainty may not increase significantly with model complexity.

## Foundational Learning

- Concept: Statistical measures of uncertainty (confidence intervals, standard deviation, Kendall's coefficient of concordance, Fleiss' kappa, feature agreement, and rank agreement)
  - Why needed here: These measures are used to quantify the uncertainty in feature attributions generated by additive feature attribution methods.
  - Quick check question: Can you explain the difference between Kendall's coefficient of concordance and Fleiss' kappa in the context of measuring uncertainty in feature attributions?

- Concept: Perturbation-based explanation methods (LIME, KernelSHAP, BayesLIME, and CXPlain)
  - Why needed here: These methods generate local explanations for black-box models by sampling perturbations around the instance being explained and fitting a local linear model.
  - Quick check question: How does the sampling distribution in LIME affect the uncertainty in feature attributions?

- Concept: Decision boundary and its influence on feature attributions
  - Why needed here: The proximity of an instance to the decision boundary affects the stability of its feature attributions. The algorithm proposed in the paper mines "stable instances" based on the distance to the second-nearest decision boundary point.
  - Quick check question: Why is it important to consider the distance to the second-nearest decision boundary point when diagnosing the stability of an instance?

## Architecture Onboarding

- Component map:
  - Data -> Models -> XAI Methods -> Uncertainty Measures -> Algorithms
  - UCI Pima Indians Diabetes dataset and synthetic dataset (100 features, 200,000 instances) -> Binary classifiers (logistic regression, SVM with linear and RBF kernels, MLP with different hidden layers) -> LIME, KernelSHAP, BayesLIME, and CXPlain -> Confidence intervals, standard deviation, Kendall's coefficient of concordance, Fleiss' kappa, feature agreement, and rank agreement -> Growing spheres algorithm for computing nearest decision boundary point, ModelComplexityFinder for estimating model complexity

- Critical path:
  1. Generate feature attributions for instances using XAI methods
  2. Compute uncertainty measures for the generated attributions
  3. Analyze the relationship between feature importance and uncertainty
  4. Propose modifications to reduce uncertainty in important features
  5. Identify stable instances based on their proximity to decision boundary points
  6. Estimate the complexity of the underlying model and its impact on uncertainty

- Design tradeoffs:
  - Sampling density vs. computational cost: Increasing the number of perturbations sampled can reduce uncertainty but also increases computational cost.
  - Stability vs. representativeness: Stable instances may not be representative of the entire feature space, leading to biased explanations.

- Failure signatures:
  - High uncertainty in feature attributions despite dense sampling
  - Inability to identify stable instances due to complex decision boundaries
  - Poor correlation between model complexity and uncertainty in feature attributions

- First 3 experiments:
  1. Generate feature attributions for a small set of instances using LIME and compute the uncertainty using Kendall's coefficient of concordance.
  2. Implement the proposed modification to LIME's sampling distribution using multivariate Gaussian based on decision boundary points. For this, implement the growing spheres algorithm to find the nearest decision boundary point (DBP) for each instance. Compare uncertainty reduction between original LIME and the modified version (MVG-LIME) on both datasets.
  3. Identify stable instances in the dataset and analyze their proximity to decision boundary points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can uncertainty in non-perturbation-based explanation algorithms be reduced?
- Basis in paper: [inferred] The paper focuses on perturbation-based methods and suggests future work on reducing uncertainty in non-perturbation methods.
- Why unresolved: The paper does not explore methods for non-perturbation-based XAI algorithms, leaving this as an open research direction.
- What evidence would resolve it: Comparative studies of uncertainty reduction techniques across perturbation and non-perturbation XAI methods.

### Open Question 2
- Question: What is the relationship between feature uncertainty and explanation fragility in XAI?
- Basis in paper: [inferred] The paper mentions that some studies focus on explanation fragility but do not directly link it to uncertainty.
- Why unresolved: The paper does not investigate the correlation between uncertainty and fragility metrics, leaving this relationship unexplored.
- What evidence would resolve it: Empirical analysis of feature uncertainty and fragility across multiple XAI methods and datasets.

### Open Question 3
- Question: How does model complexity affect the stability of explanations across different types of classifiers?
- Basis in paper: [explicit] The paper observes that more complex models exhibit higher uncertainty but does not explore stability across different classifier types.
- Why unresolved: The paper focuses on a limited set of classifiers and does not generalize findings to other model architectures.
- What evidence would resolve it: Systematic experiments comparing explanation stability across diverse classifier types with varying complexities.

## Limitations
- The study focuses on binary classification tasks with limited dataset diversity (diabetes and synthetic data), which may limit generalizability to multi-class problems or real-world datasets with different characteristics
- The proposed MVG-LIME modification requires computing nearest decision boundary points using the growing spheres algorithm, which adds computational overhead that may scale poorly with high-dimensional data
- The correlation between model complexity and uncertainty (0.836 Pearson) is observed but not causally established - the relationship could be influenced by other factors not controlled in the experiments

## Confidence

**High confidence:** In the statistical methodology for uncertainty quantification and the observation that feature importance and uncertainty are weakly correlated

**Medium confidence:** In the effectiveness of MVG-LIME modification due to limited testing on only two datasets and one model architecture; In the "stable instances" detection algorithm due to precision (89.2%) and recall (95.1%) being evaluated on the same datasets used for development

## Next Checks
1. Test MVG-LIME on diverse real-world datasets (e.g., image classification, text classification) and multi-class problems to verify generalizability
2. Evaluate the computational complexity of the growing spheres algorithm on high-dimensional datasets (>1000 features) to assess scalability
3. Conduct ablation studies removing different components of the MVG-LIME modification to isolate which aspects contribute most to uncertainty reduction