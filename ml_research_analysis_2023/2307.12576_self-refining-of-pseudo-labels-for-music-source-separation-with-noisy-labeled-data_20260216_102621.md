---
ver: rpa2
title: Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled
  Data
arxiv_id: '2307.12576'
source_url: https://arxiv.org/abs/2307.12576
tags:
- dataset
- instrument
- music
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training music source separation
  (MSS) models when faced with partially mislabeled instrument tracks in large datasets.
  The authors propose a self-refining technique that leverages noisy-labeled data
  to improve label quality.
---

# Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data

## Quick Facts
- arXiv ID: 2307.12576
- Source URL: https://arxiv.org/abs/2307.12576
- Reference count: 0
- One-line primary result: Self-refining technique achieves only 1% accuracy degradation in multi-label instrument recognition and improves MSS performance by 2.45 dB SDR for Demucs and 2.31 dB for X-UMX models.

## Executive Summary
This paper addresses the challenge of training music source separation (MSS) models when faced with partially mislabeled instrument tracks in large datasets. The authors propose a self-refining technique that leverages noisy-labeled data to improve label quality. Their approach involves training a multi-label instrument classifier directly on the noisy dataset using a random mixing technique to synthesize multi-labeled mixtures. The classifier is then used to refine the original noisy dataset. The refined dataset is subsequently used to train MSS models. Key results include: the self-refined classifier achieves only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on clean-labeled data; MSS models trained on the self-refined dataset show comparable or superior performance to those trained on clean-labeled data, with an average SDR improvement of 2.45 dB for Demucs and 2.31 dB for X-UMX models; the self-refining technique is effective with only two iterations, and further refinement does not significantly improve performance.

## Method Summary
The authors propose a self-refining technique for music source separation with noisy labeled data. The method involves: (1) training a ConvNeXt-based multi-label instrument classifier on the noisy dataset using random mixing of stems to synthesize multi-labeled mixtures, (2) using the trained classifier to refine the noisy dataset, and (3) training MSS models (Demucs v3 and X-UMX) on the refined dataset. The random mixing technique exploits the acoustic music domain property that combining different instrumental tracks still produces natural-sounding music, allowing the creation of correctly labeled pseudo-data from mislabeled stems.

## Key Results
- The self-refined classifier achieves only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on clean-labeled data.
- MSS models trained on the self-refined dataset show comparable or superior performance to those trained on clean-labeled data, with an average SDR improvement of 2.45 dB for Demucs and 2.31 dB for X-UMX models.
- The self-refining technique is effective with only two iterations, and further refinement does not significantly improve performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-refining leverages random mixing to synthesize correct pseudo labels from mislabeled stems.
- Mechanism: When random mixing combines a correctly labeled drum track with a track mislabeled as vocals but actually containing both drums and vocals, the mixture becomes correctly labeled (drums + vocals). This allows the classifier to learn from corrupted data.
- Core assumption: The label noise in the dataset is not uniformly random but contains structural correlations that can be exploited through combination.
- Evidence anchors:
  - [section]: "For instance, if we randomly select one correctly labeled drum track and a track that contains both sources of drums and vocals but is mislabeled as vocals, the mixing process synthesizes a correctly labeled mixture."
  - [abstract]: "Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset."
- Break condition: If the label noise is completely random with no correlation between actual content and assigned labels, random mixing cannot create correctly labeled pseudo data.

### Mechanism 2
- Claim: The random mixing technique exploits domain-specific properties of acoustic music signals.
- Mechanism: Unlike image data, combining different instrumental tracks still produces natural-sounding music, making random mixing a valid augmentation strategy for multi-label training.
- Core assumption: Music mixtures created by summing stems maintain perceptual validity even when stems are selected randomly.
- Evidence anchors:
  - [section]: "The random mixing technique takes advantage of the acoustic music domain in that mixing sources of different instrument tracks still leads to natural output mixture, whereas naively combining different images in the image domain is likely to produce unrealistic results."
  - [section]: "If there exist two different instruments in one audio signal, that can be classified into two instruments."
- Break condition: If the dataset contains tracks that are heavily processed or contain non-instrument sounds, random mixing may produce unnatural results that don't reflect real music mixtures.

### Mechanism 3
- Claim: False-negative samples have a more significant impact on MSS performance than false-positive samples.
- Mechanism: When a sample is misclassified as a non-vocal stem (e.g., drums + bass) despite containing vocals, the MSS model must allocate vocals to the wrong stems, causing confusion and degraded performance. Conversely, false-positive samples simply require the model to predict silence.
- Core assumption: The MSS model architecture and training framework are sensitive to the distribution of errors in the input labels.
- Evidence anchors:
  - [section]: "An additional factor to consider is the distinctive nature of the MSS model training framework in our approach. MSS models utilize the output of the classifier as input. The performance of the MSS model can be affected differently depending on the type of error in the classifier's output."
  - [section]: "This not only negatively affects the performance of the mislabeled stems but also the vocal stem itself."
- Break condition: If the MSS model architecture is robust to label noise or if the training framework explicitly handles label uncertainty, the asymmetry between false-positive and false-negative errors may not significantly impact performance.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: The problem requires identifying multiple instruments present in each track simultaneously, unlike traditional single-label classification.
  - Quick check question: How does the model output differ between multi-label and single-label classification?

- Concept: Signal-to-Distortion Ratio (SDR) metric
  - Why needed here: SDR is the primary evaluation metric for assessing source separation quality, measuring how well the separated sources match the ground truth.
  - Quick check question: What does an SDR improvement of 2.45 dB represent in terms of perceived audio quality?

- Concept: Random mixing augmentation
  - Why needed here: Random mixing is the key technique that enables learning from noisy-labeled data by creating correctly labeled pseudo mixtures.
  - Quick check question: Why can't this technique be directly applied to image datasets?

## Architecture Onboarding

- Component map:
  - Instrument Classifier (Î¨): ConvNeXt-based multi-label classifier that refines noisy labels
  - MSS Models: Demucs and X-UMX for actual source separation
  - Data Processing Pipeline: Random mixing, audio effects application, and label refinement

- Critical path:
  1. Train instrument classifier on noisy dataset using random mixing
  2. Refine noisy dataset using trained classifier
  3. Train MSS models on refined dataset
  4. Evaluate MSS performance on clean test set

- Design tradeoffs:
  - Using only noisy data vs. requiring clean labels for training
  - Single-iteration vs. multi-iteration self-refining
  - Fixed threshold vs. adaptive thresholds for classification

- Failure signatures:
  - Performance plateaus after self-refining iterations indicate diminishing returns
  - Degradation in MSS performance when using adaptive thresholds suggests F1 maximization hurts recall
  - Low recall values in instrument recognition correlate with poor MSS performance

- First 3 experiments:
  1. Train instrument classifier on noisy data and evaluate on clean test set to establish baseline
  2. Refine noisy dataset with trained classifier and retrain classifier on refined data
  3. Train MSS models on original noisy data vs. refined data to measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the percentage of noisy labels in a dataset and the effectiveness of the self-refining technique?
- Basis in paper: [inferred] The paper mentions that further analysis on the correlation between the percentage of noisy labels and the effectiveness of self-refining remains challenging due to the unknown percentage of corruption in the MDX 2023 dataset.
- Why unresolved: The study did not investigate how different levels of label noise impact the performance of the self-refining technique.
- What evidence would resolve it: Experiments with datasets containing varying levels of label noise to determine the effectiveness of the self-refining technique at different noise levels.

### Open Question 2
- Question: How does the choice of threshold for the classifier during training of MSS models impact the overall performance?
- Basis in paper: [explicit] The paper discusses experiments with different thresholds (0.9 and 0.5) and adaptive thresholds for the classifier during MSS model training.
- Why unresolved: While the paper shows that reducing the threshold to 0.5 only exhibits a small degradation in performance, it does not explore the impact of other threshold values or the optimal threshold for different datasets or models.
- What evidence would resolve it: Systematic experiments with various threshold values to determine the optimal threshold for different datasets and models.

### Open Question 3
- Question: What is the impact of iterative self-refining on the performance of MSS models, and how many iterations are optimal?
- Basis in paper: [explicit] The paper discusses the impact of iterative self-refining on both multi-instrument recognition and MSS performance, showing that self-refining the dataset twice is sufficient for optimal performance with the MDX2023 dataset.
- Why unresolved: The study does not explore the impact of iterative self-refining on other datasets or models, and the optimal number of iterations may vary depending on the dataset and model.
- What evidence would resolve it: Experiments with different datasets and models to determine the optimal number of iterations for self-refining.

## Limitations
- The approach relies on the assumption that random mixing can create correctly labeled pseudo-data from mislabeled stems, which may not hold for datasets with completely random label noise.
- The effectiveness of the method depends on the structural correlation between actual content and assigned labels, which may not generalize to all datasets.
- The claim about false-negative samples having more significant impact than false-positive samples is based on the specific MSS model architecture used and may not apply universally across different separation frameworks.

## Confidence
- **High**: The core self-refining mechanism and its effectiveness in improving MSS performance when using the MDX2023 dataset with the specified models (Demucs and X-UMX)
- **Medium**: The generalizability of the random mixing technique to other music source separation datasets and models
- **Low**: The universal applicability of the asymmetric impact of false-positive versus false-negative label errors across different MSS architectures

## Next Checks
1. Test the random mixing approach on a dataset with completely random label noise to verify if the structural correlation assumption holds
2. Evaluate the impact of false-positive versus false-negative errors on different MSS architectures to confirm the asymmetric effect claim
3. Apply the self-refining technique to a different music source separation dataset (e.g., MUSDB18-HQ) with different instrumentation to assess generalizability