---
ver: rpa2
title: 'The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language
  Evaluation'
arxiv_id: '2311.10057'
source_url: https://arxiv.org/abs/2311.10057
tags:
- dataset
- music
- data
- please
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Song Describer Dataset (SDD), a new crowdsourced
  corpus of 1.1k high-quality audio-caption pairs for evaluating music-and-language
  models. The dataset consists of human-written natural language descriptions of 706
  music recordings, all publicly accessible under Creative Common licenses.
---

# The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation

## Quick Facts
- arXiv ID: 2311.10057
- Source URL: https://arxiv.org/abs/2311.10057
- Reference count: 40
- Key outcome: Introduces SDD, a crowdsourced corpus of 1.1k high-quality audio-caption pairs for evaluating music-and-language models

## Executive Summary
This paper introduces the Song Describer Dataset (SDD), a new crowdsourced corpus of 1.1k high-quality audio-caption pairs for evaluating music-and-language models. The dataset consists of human-written natural language descriptions of 706 music recordings, all publicly accessible under Creative Common licenses. To demonstrate the use of SDD, the authors benchmark popular models on three key music-and-language tasks: music captioning, text-to-music generation, and music-language retrieval. The experiments highlight the importance of cross-dataset evaluation and provide insights into how researchers can use SDD to gain a broader understanding of model performance.

## Method Summary
The Song Describer Dataset was collected through crowdsourcing on Amazon Mechanical Turk, with 1.1k high-quality audio-caption pairs generated by non-expert annotators. The dataset includes 706 music recordings from the MTG-Jamendo dataset, with each recording described by multiple annotators. The authors evaluate three music-and-language tasks: music captioning (using CNN/LSTM and CNN/transformer models), text-to-music generation (using Riffusion and AudioLDM), and music-language retrieval (using TTMR and CLAP). Performance is measured using BLEU, METEOR, ROUGE, and BERT-score for captioning; FAD, IS, KLD, and CCC for text-to-music; and R@k, MedR for retrieval.

## Key Results
- SDD contains 1.1k high-quality audio-caption pairs from 706 music recordings
- Cross-dataset evaluation reveals significant performance gaps between MusicCaps and SDD
- The dataset is available under CC BY-SA 4.0 license with audio files under respective CC licenses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's use of diverse, non-expert annotators improves generalizability of music-language models.
- Mechanism: By sourcing annotations from a broad pool of non-experts with varied backgrounds, the dataset captures a wider range of descriptive language and perspectives on music, making it more representative of real-world user queries.
- Core assumption: Non-expert annotators provide diverse and representative descriptions of music.
- Evidence anchors:
  - [abstract] "a larger pool of annotators, composed of non-experts with diverse backgrounds and more representative of the general public"
  - [section] "The analysis of this data together with the code to reproduce the figures is available online"
  - [corpus] Weak evidence: corpus analysis focuses on related models, not annotator diversity.
- Break condition: If annotator diversity leads to inconsistent or low-quality annotations that don't capture essential musical features.

### Mechanism 2
- Claim: The use of openly licensed, persistent audio data enables reproducible and ethical research.
- Mechanism: By distributing audio files with clear Creative Commons licenses and ensuring data persistence, the dataset allows for transparent and ethical research practices, avoiding issues of data leakage or copyright infringement.
- Core assumption: Open licenses and data persistence are crucial for ethical and reproducible research.
- Evidence anchors:
  - [abstract] "all publicly accessible and released under Creative Common licenses"
  - [section] "SDD is characterised by (1) annotations of longer track segments (95% of the tracks are 2 minutes long); (2) openly accessible audio, persistently stored, and with explicit licenses"
  - [corpus] Weak evidence: corpus focuses on model performance, not data licensing or persistence.
- Break condition: If open licenses or data persistence are not strictly enforced, leading to legal issues or data unavailability.

### Mechanism 3
- Claim: Cross-dataset evaluation reveals model performance gaps and biases.
- Mechanism: By benchmarking models on both the proposed dataset and existing ones like MusicCaps, researchers can identify performance differences that highlight model limitations and biases towards specific data distributions.
- Core assumption: Cross-dataset evaluation provides insights into model generalizability and biases.
- Evidence anchors:
  - [abstract] "our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use SDD to gain a broader understanding of model performance"
  - [section] "For music captioning, in Table 3 we observe that, beyond inter-model differences ascribable to the different model architectures and pre-training, both systems display a sizeable gap between in-domain (MC) and out-of-domain (SDD) performance"
  - [corpus] Weak evidence: corpus focuses on model performance, not cross-dataset evaluation insights.
- Break condition: If cross-dataset evaluation does not reveal meaningful performance differences or biases.

## Foundational Learning

- Concept: Music information retrieval (MIR)
  - Why needed here: Understanding MIR is crucial for grasping the challenges and tasks involved in music-language models, such as music captioning and text-to-music generation.
  - Quick check question: What are the main challenges in music information retrieval, and how do they relate to the tasks evaluated in this paper?

- Concept: Multimodal learning
  - Why needed here: The paper focuses on models that process both audio and text, requiring knowledge of how to effectively combine and learn from multiple modalities.
  - Quick check question: How do multimodal models handle the alignment and fusion of audio and text data, and what are the key challenges in this process?

- Concept: Automatic evaluation metrics for natural language generation
  - Why needed here: The paper uses metrics like BLEU, METEOR, and BERT-score to evaluate music captioning models, requiring understanding of how these metrics assess the quality and relevance of generated text.
  - Quick check question: What are the strengths and limitations of BLEU, METEOR, and BERT-score in evaluating music captions, and how do they differ from human evaluation?

## Architecture Onboarding

- Component map: Data collection platform -> Annotation guidelines -> Manual validation -> Evaluation metrics -> Benchmark models
- Critical path: 1. Annotate music tracks with descriptive captions 2. Validate and curate high-quality annotations 3. Distribute dataset with open licenses 4. Benchmark models on music-language tasks 5. Analyze performance gaps and biases
- Design tradeoffs:
  - Dataset size vs. annotation quality: Larger datasets may be noisier, while smaller ones may lack diversity
  - Expert vs. non-expert annotators: Experts may provide more consistent annotations, but non-experts capture real-world language diversity
  - Open vs. proprietary licenses: Open licenses enable wider research use but may limit commercial applications
- Failure signatures:
  - Low inter-annotator agreement: Indicates inconsistent or subjective annotation guidelines
  - Performance gaps between datasets: Suggests model overfitting or bias towards specific data distributions
  - High variability in evaluation metrics: Implies unreliable or unstable model performance
- First 3 experiments:
  1. Compare music captioning performance on SDD vs. MusicCaps to assess model generalizability
  2. Evaluate text-to-music generation quality and semantic relevance on SDD and MusicCaps
  3. Benchmark music-text retrieval models on SDD to identify performance differences and biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the dataset perform if annotations were collected from a more diverse global population, particularly from non-English speaking countries and different cultural backgrounds?
- Basis in paper: [explicit] The authors note that "audio and language queries" are "becoming increasingly important" and mention that "the pool of annotators" remains "limited in the current version," specifically noting participants were "mostly from English-speaking countries (US, GB, IN) and Western Europe (ES, FR, DE, IT)."
- Why unresolved: The dataset was collected from a relatively homogeneous group of annotators primarily from Western countries, which may limit its generalizability to different musical cultures and languages.
- What evidence would resolve it: Collecting a parallel dataset with annotators from diverse geographic regions, cultures, and language backgrounds, then comparing cross-dataset evaluation results and assessing differences in annotation styles, vocabulary used, and music aspects emphasized.

### Open Question 2
- Question: Would increasing the number of captions per track beyond the current maximum of 5 improve the reliability of evaluation metrics, particularly for tasks like music captioning and retrieval?
- Basis in paper: [explicit] The authors state that "recordings in SDD can also be connected to additional metadata" and note that "a portion of recordings (25%) with more than one caption produced by different annotators, making it particularly suitable for evaluation via automatic metrics for captioning."
- Why unresolved: The current dataset has variable numbers of captions per track (up to 5), and it's unclear whether this provides sufficient inter-annotator agreement for robust evaluation or if more captions would yield more reliable metrics.
- What evidence would resolve it: Conducting experiments comparing evaluation metric stability and variance when using different numbers of captions per track (e.g., 3, 5, 10, 20), measuring inter-annotator agreement and metric consistency across these conditions.

### Open Question 3
- Question: How would the performance of music-language models change if evaluated on SDD versus MusicCaps when the models are trained on different pre-training datasets, particularly regarding domain adaptation and out-of-distribution generalization?
- Basis in paper: [explicit] The authors demonstrate "sizeable gap between in-domain (MC) and out-of-domain (SDD) performance" and note that "evaluating on in-domain distributions may lead to inflated results," specifically mentioning that "audio in MusicCaps is derived from AudioSet, which is often used for pre-training large audio models, thus posing a risk of data leakage."
- Why unresolved: The paper shows performance differences between datasets but doesn't systematically explore how these differences vary across different pre-training regimes and model architectures.
- What evidence would resolve it: Conducting comprehensive cross-dataset evaluations across multiple pre-training datasets (AudioSet, Musdb, FMA, etc.) and model architectures, measuring performance degradation/gain patterns to understand the relationship between pre-training data distribution and out-of-domain evaluation performance.

## Limitations
- Relatively small dataset size (1.1k audio-caption pairs) compared to established benchmarks like MusicCaps (5.5k pairs)
- Evaluation focuses on a narrow set of tasks (captioning, text-to-music, and retrieval) without exploring other applications
- Reliance on non-expert annotators introduces potential inconsistencies in annotation quality and style

## Confidence
- **Dataset Quality and Representativeness (Medium Confidence)**: The claims about dataset quality are supported by manual validation and analysis, but the reliance on non-expert annotators introduces uncertainty about annotation consistency and musical expertise.
- **Cross-Dataset Evaluation Insights (High Confidence)**: The performance gaps observed between MusicCaps and Song Describer are clearly demonstrated and well-documented, providing strong evidence for the importance of cross-dataset evaluation.
- **Model Performance Comparisons (Medium Confidence)**: While the benchmarking methodology is sound, the limited dataset size may affect the statistical significance of performance differences between models.

## Next Checks
1. **Annotator Agreement Analysis**: Conduct a systematic analysis of inter-annotator agreement rates and identify patterns in annotation disagreements to quantify the impact of using non-expert annotators.
2. **Extended Evaluation Protocol**: Test models on additional music-language tasks beyond the three evaluated (e.g., music recommendation, emotion classification) to assess the dataset's broader applicability.
3. **Dataset Scaling Study**: Investigate the relationship between dataset size and model performance by training models on progressively larger subsets of Song Describer and MusicCaps to determine optimal dataset sizes for different tasks.