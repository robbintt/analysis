---
ver: rpa2
title: Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate
  Descent
arxiv_id: '2301.12184'
source_url: https://arxiv.org/abs/2301.12184
tags:
- descent
- coordinate
- learning
- methods
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies semi-supervised learning on multilayer hypergraphs
  by extending graph-based p-Laplacian regularization to higher-order interactions.
  The authors formulate the problem as minimizing an objective combining data fidelity
  with a multilayer hypergraph p-Laplacian regularizer, which can be optimized independently
  across classes.
---

# Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate Descent

## Quick Facts
- arXiv ID: 2301.12184
- Source URL: https://arxiv.org/abs/2301.12184
- Reference count: 40
- Primary result: Greedy coordinate descent significantly outperforms gradient descent for SSL on multilayer hypergraphs

## Executive Summary
This paper extends graph-based p-Laplacian regularization to multilayer hypergraphs for semi-supervised learning. The authors formulate an optimization problem combining data fidelity with a multilayer hypergraph p-Laplacian regularizer that can be optimized independently across classes. They propose block coordinate descent methods with cyclic, random, and greedy variable selection rules, and demonstrate through extensive experiments that greedy coordinate descent achieves superior performance in terms of both objective value and classification accuracy per unit of computational work compared to gradient descent methods.

## Method Summary
The paper addresses semi-supervised learning on multilayer hypergraphs by minimizing an objective that combines data fidelity with a multilayer hypergraph p-Laplacian regularizer. The optimization problem is solved using block coordinate descent methods with three selection rules: cyclic, random, and greedy. The greedy method selects variables that maximally reduce the objective using first-order gradient information, exploiting problem sparsity. Experiments compare these methods against gradient descent on synthetic stochastic block model graphs and seven real-world datasets, measuring performance in terms of objective function value and classification accuracy evaluated on unlabeled nodes.

## Key Results
- Greedy coordinate descent significantly outperforms gradient descent in both objective value and accuracy per computational work
- p-Laplacian regularization (p ≠ 2) can improve classification accuracy compared to standard quadratic regularization
- Block coordinate descent methods scale better than gradient descent for large multilayer hypergraphs due to reduced per-iteration computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GCD outperforms GD in objective value and accuracy per unit computational work
- Mechanism: GCD selects variables that maximally reduce the objective using first-order gradient information, exploiting sparsity by focusing computational effort on most impactful updates
- Core assumption: Computing gradient and identifying maximum gradient component is cheaper per iteration than computing full gradient
- Evidence anchors: Experimental results show GCD "always reaches a good solution... much faster than other methods"

### Mechanism 2
- Claim: p-Laplacian regularization (p ≠ 2) improves classification accuracy
- Mechanism: p-Laplacian penalizes large gradients more heavily when p > 2, discouraging sharp spikes near labeled data and encouraging smoother solutions
- Core assumption: Underlying data distribution benefits from higher-order smoothness
- Evidence anchors: Experiments with p ≠ 2 show "better performances" compared to p = 2

### Mechanism 3
- Claim: Block coordinate descent scales better than gradient descent for large hypergraphs
- Mechanism: Each coordinate update requires only one gradient component and single-variable update, reducing memory and computational overhead
- Core assumption: Graph/hypergraph incidence matrices are sparse, making single gradient component computation cheap
- Evidence anchors: Per-iteration complexity analysis shows GCD uses one flop per iteration vs N flops for GD

## Foundational Learning

- Concept: Graph and hypergraph Laplacian matrices
  - Why needed here: Essential for deriving gradients and implementing coordinate updates efficiently in the optimization problem
  - Quick check question: How does the normalized Laplacian differ from the unnormalized Laplacian, and why is normalization used in this SSL formulation?

- Concept: Block coordinate descent algorithms
  - Why needed here: Core optimization method adapted to solve the SSL problem; understanding selection rules and convergence is critical
  - Quick check question: What is the difference between Gauss-Seidel (cyclic) and Gauss-Southwell (greedy) selection rules in coordinate descent?

- Concept: p-Laplacian regularization and its properties
  - Why needed here: Regularization term generalizes standard quadratic regularization; understanding p's effect on smoothness is key to interpreting results
  - Quick check question: What happens to the p-Laplacian regularizer as p approaches 1 versus as p approaches infinity?

## Architecture Onboarding

- Component map: Multilayer hypergraph (incidence matrices Kℓ, edge weights wℓ(e)) -> Block coordinate descent solver (CCD, RCD, GCD) -> Label scores Z* -> Predicted class for each node

- Critical path:
  1. Parse hypergraph structure and initialize Z⁰
  2. For each iteration: Select working set Wk according to chosen rule, compute gradient ∇ϑ(Zk) incrementally, update Zk+1 via single-variable move with Lipschitz-based stepsize
  3. Repeat until convergence or max iterations

- Design tradeoffs:
  - GCD vs CCD/RCD: GCD faster convergence per iteration but higher per-iteration cost; CCD/RCD cheaper per iteration but slower convergence
  - p = 2 vs p ≠ 2: Quadratic simpler and faster; general p may yield better accuracy but requires more complex gradients
  - Dense vs sparse hypergraphs: Sparse hypergraphs favor coordinate methods; dense graphs may favor full gradient methods

- Failure signatures:
  - Slow convergence: Poor stepsize, unsuitable selection rule, or ill-conditioned problem
  - Numerical instability: Large p or ill-scaled edge weights causing overflow
  - Degenerate solutions: Too few labeled nodes or inappropriate p leading to overly smooth solutions

- First 3 experiments:
  1. Run GCD on small synthetic SBM graph (4 communities, 125 nodes each, 3% labels) with p = 2; verify objective decreases and accuracy improves
  2. Compare CCD, RCD, and GCD on same graph; record objective value and accuracy per flop to confirm GCD superiority
  3. Vary p ∈ {1.8, 1.9, 2, 2.25, 2.5} on cora dataset; measure final accuracy and runtime to identify optimal p

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GCD performance scale with hypergraph size and order compared to graph-based methods?
- Basis in paper: Experiments use fixed-size datasets without systematic scaling analysis
- Why unresolved: Limited range of dataset sizes tested
- What evidence would resolve it: Controlled experiments varying node count, hyperedge size, and layers while measuring convergence rates

### Open Question 2
- Question: What is the optimal choice of p in the p-Laplacian regularizer across different data characteristics?
- Basis in paper: Experiments show "behavior of the methods do not change much by varying p" but limited p range tested
- Why unresolved: Only p ∈ {1.8, 1.9, 2.25, 2.5} tested without systematic analysis
- What evidence would resolve it: Comprehensive experiments with wider p range and data-dependent analysis

### Open Question 3
- Question: How does choice of regularization parameters λℓ affect solution quality and convergence speed?
- Basis in paper: Only λℓ = 1 for all ℓ tested without sensitivity analysis
- Why unresolved: No exploration of λ patterns or adaptive strategies
- What evidence would resolve it: Experiments with varying λ patterns and cross-validation approaches

## Limitations

- Implementation details for efficient GCD rule (max-heap structure, caching strategies) are not explicitly specified
- Limited theoretical analysis of convergence rates for coordinate descent methods on p-Laplacian SSL problem
- No sensitivity analysis of regularization parameters or comparison with alternative SSL methods

## Confidence

- **High**: GCD outperforms GD in objective value and accuracy per computational work for multilayer hypergraph SSL
- **Medium**: p-Laplacian regularization (p ≠ 2) improves classification accuracy compared to quadratic regularization
- **Medium**: Block coordinate descent scales better than gradient descent for large sparse hypergraphs

## Next Checks

1. Implement the efficient GCD rule using max-heap structure and caching strategies to verify reported speedup over GD
2. Conduct ablation studies on synthetic SBM datasets to measure impact of varying p values (1.5, 2, 2.5) and λℓ on accuracy and convergence
3. Test coordinate descent methods on additional real-world datasets (e.g., Pubmed, Citeseer) not mentioned in the paper to evaluate generalizability