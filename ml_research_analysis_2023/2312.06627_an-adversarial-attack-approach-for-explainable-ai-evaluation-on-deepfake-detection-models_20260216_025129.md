---
ver: rpa2
title: An adversarial attack approach for eXplainable AI evaluation on deepfake detection
  models
arxiv_id: '2312.06627'
source_url: https://arxiv.org/abs/2312.06627
tags:
- image
- adversarial
- tools
- images
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating eXplainable AI (XAI)
  tools for deepfake detection models. The authors demonstrate that generic XAI evaluation
  methods, such as pixel or segment removal/insertion, are not suitable for deepfake
  detection due to the models' reliance on face artifacts.
---

# An adversarial attack approach for eXplainable AI evaluation on deepfake detection models

## Quick Facts
- arXiv ID: 2312.06627
- Source URL: https://arxiv.org/abs/2312.06627
- Authors: 
- Reference count: 40
- Primary result: Proposes adversarial attack approach to evaluate XAI tools for deepfake detection models by correlating salient regions between real and fake images

## Executive Summary
This paper addresses the challenge of evaluating eXplainable AI (XAI) tools for deepfake detection models. Traditional XAI evaluation methods like pixel or segment removal/insertion fail for deepfake detection because these models rely on specific face artifacts rather than global features. The authors propose an innovative adversarial attack approach that identifies visual concepts highlighted by XAI tools on real images and perturbs the same concepts in corresponding fake images. Experiments on MesoNet and XceptionNet models using FaceForensics++ and Celeb-DF datasets demonstrate that this approach effectively evaluates the faithfulness of XAI tools in deepfake detection contexts.

## Method Summary
The method involves computing explanations for real images using various XAI tools (SOBOL, XRAI, RISE, LIME, Grad-CAM), ranking image segments based on their importance, and then using Natural Evolutionary Strategies (NES) to generate adversarial fake images by perturbing the highest-ranked segments. The approach leverages the correlation between salient regions in real and fake image pairs, assuming similar face orientations allow pixel indices from real image explanations to be applied to fake images. The effectiveness of each XAI tool is evaluated by measuring the reduction in model accuracy on these adversarial images compared to the original fake images.

## Key Results
- Different XAI tools exhibit varying levels of faithfulness in deepfake detection contexts
- SOBOL XAI tool consistently shows higher performance in generating adversarial images that reduce model accuracy
- Evaluation results vary between image resolutions (256x256 vs 299x299), with lower resolution images enabling more effective adversarial generation
- The proposed approach successfully identifies faithful XAI tools while traditional removal/insertion methods fail due to model reliance on face artifacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generic XAI evaluation methods (pixel/segment removal/insertion) fail for deepfake detection because they distort the face artifacts that the model relies on for detection.
- Mechanism: When salient pixels or segments are removed or inserted in deepfake images, the visual concepts (face artifacts like eyes, nose, mouth) are distorted, leading to unexpected model behavior that cannot be reliably attributed to the removal of important features.
- Core assumption: Deepfake detection models learn to identify manipulated regions of faces, so any alteration to those regions disrupts the model's ability to make accurate predictions.
- Evidence anchors:
  - [abstract] "Generic XAI evaluation methods like insertion or removal of salient pixels/segments are applicable for general image classification tasks but may produce less meaningful results when applied on deepfake detection models due to their functionality."
  - [section] "In the context of deepfake images, a fake image indicates an image of a person wherein a portion of or the entire face has been tampered. The tampering may be done manually using popular photoshop applications or artificially generated using AI models."
  - [corpus] Weak evidence - the corpus focuses on adversarial attacks but doesn't directly address XAI evaluation limitations in deepfake detection.
- Break condition: If deepfake detection models shift to learning global features rather than face-specific artifacts, or if XAI tools can highlight non-face regions that are more robust to manipulation.

### Mechanism 2
- Claim: Correlating salient regions between real and fake images enables targeted adversarial attacks that reduce model accuracy.
- Mechanism: By identifying visual concepts in real images (which the model uses to classify as real) and perturbing the same concepts in corresponding fake images, adversarial fake images can be generated that flip the model's prediction while maintaining visual similarity.
- Core assumption: Real and fake images of the same person have similar face orientations, allowing pixel indices from real image explanations to be applied to fake images.
- Evidence anchors:
  - [abstract] "The core idea is to identify the visual concepts highlighted by XAI tools on real images and perturb the same concepts in corresponding fake images to generate adversarial fake images."
  - [section] "Using this simple intuition, we propose an adversarial attack evaluation approach based on visual concepts. For a given real-fake image pair, we first identify the visual concept highlighted by a tool on a real image and perturb the same concept in the corresponding fake image to generate an adversarial fake image."
  - [corpus] Weak evidence - the corpus discusses adversarial attacks but doesn't specifically address the correlation between real and fake image explanations.
- Break condition: If real and fake images have significantly different face orientations, or if the XAI tools highlight different visual concepts for the same face region across real and fake images.

### Mechanism 3
- Claim: NES-based black-box adversarial attack can generate imperceptible perturbations that reduce model accuracy when constrained to salient regions.
- Mechanism: Natural Evolutionary Strategies estimate gradients in the direction that increases the probability of the real class for fake images, while the perturbations are constrained to the top-ranked segments identified by XAI tools and limited by L∞ norm to maintain visual similarity.
- Core assumption: The NES gradient estimation with antithetic sampling can effectively find adversarial directions even when perturbations are constrained to specific image regions.
- Evidence anchors:
  - [abstract] "We propose and implement an XAI evaluation approach specifically suited for deepfake detection models."
  - [section] "NES is a popular black-box technique used to estimate gradients. The main idea behind NES is to maximise the probability F(x)y for a given target class y."
  - [corpus] Weak evidence - the corpus discusses adversarial attacks but doesn't specifically address NES or region-constrained perturbations.
- Break condition: If the model becomes robust to adversarial examples, or if the NES estimation fails to converge within the iteration limit.

## Foundational Learning

- Concept: Visual concepts in face images
  - Why needed here: Understanding how deepfake detection models rely on specific face regions (eyes, nose, mouth) rather than global features is crucial for developing effective evaluation methods.
  - Quick check question: Why can't we simply remove salient pixels from deepfake images to evaluate XAI tools, as we do in object detection tasks?

- Concept: Black-box adversarial attacks
  - Why needed here: The evaluation approach requires generating adversarial examples without access to model internals, making black-box methods like NES essential.
  - Quick check question: What's the key difference between white-box and black-box adversarial attacks, and why is the latter more practical for XAI evaluation?

- Concept: Image segmentation and feature correlation
  - Why needed here: Segmenting images and correlating features between real and fake pairs enables targeted perturbations that exploit the model's reliance on specific visual concepts.
  - Quick check question: How does segmenting images into 100 parts help ensure that perturbations affect similar features across real and fake image pairs?

## Architecture Onboarding

- Component map: XAI tool → Real image explanation → Segment ranking → Adversarial image generation (NES) → Model accuracy measurement
- Critical path: Real-fake image pair → XAI explanation → Segment selection → Adversarial generation → Accuracy evaluation
- Design tradeoffs: Computation time vs. evaluation accuracy (more segments = better evaluation but slower), perturbation magnitude vs. visual similarity (larger perturbations = better attacks but less imperceptible)
- Failure signatures: Model accuracy doesn't decrease after adversarial generation (tools not faithful), or adversarial images are visually obvious (perturbation too large)
- First 3 experiments:
  1. Run the proposed approach on MesoNet with FaceForensics++ DF dataset using SOBOL XAI tool
  2. Compare evaluation results between image sizes 256x256 and 299x299 on XceptionNet
  3. Measure computation time for explanation and adversarial generation across different XAI tools

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can XAI evaluation methods specific to deepfake detection be developed that are both computationally efficient and effective in assessing tool faithfulness?
- Basis in paper: [explicit] The paper demonstrates that generic removal/insertion XAI evaluation methods are not suitable for deepfake detection models due to their reliance on face artifacts.
- Why unresolved: While the paper proposes an adversarial attack approach tailored for deepfake detection, it acknowledges the need for further research and development of evaluation methods specific to this task.
- What evidence would resolve it: Development and experimental validation of new XAI evaluation methods specifically designed for deepfake detection models, demonstrating their effectiveness and efficiency compared to existing methods.

### Open Question 2
- Question: How can XAI evaluation methods be adapted to work effectively with adversarially robust deepfake detection models?
- Basis in paper: [inferred] The paper mentions that its approach may not work on deepfake detectors that are robust to adversarial attacks, highlighting the need for evaluation methods that can handle such robustness.
- Why unresolved: The paper does not provide a solution for evaluating XAI tools on adversarially robust deepfake detection models, leaving this as an open challenge.
- What evidence would resolve it: Development and experimental validation of XAI evaluation methods that can effectively assess tool faithfulness even when applied to adversarially robust deepfake detection models.

### Open Question 3
- Question: How does the choice of image resolution affect the performance of XAI evaluation methods in deepfake detection?
- Basis in paper: [explicit] The paper includes a comparison of evaluation results on XceptionNet between images of size 256 x 256 x 3 and 299 x 299 x 3, showing that the ability to generate adversarial images reduces when the image resolution is increased.
- Why unresolved: While the paper provides some insights into the impact of image resolution on evaluation performance, further research is needed to fully understand the relationship between resolution and evaluation effectiveness.
- What evidence would resolve it: Systematic experiments investigating the impact of different image resolutions on the performance of various XAI evaluation methods in deepfake detection, providing a comprehensive understanding of the resolution-evaluation relationship.

## Limitations

- The approach assumes strong similarity between real and fake face orientations, which may not hold across all deepfake generation methods
- The NES-based adversarial generation is computationally intensive and may not scale well for large-scale evaluation
- The method may not work on deepfake detectors that are robust to adversarial attacks

## Confidence

- Mechanism 1 (XAI method limitations): Medium - well-reasoned but lacks direct empirical comparison
- Mechanism 2 (Real-fake correlation): Medium - logical but assumes consistent face alignment
- Mechanism 3 (NES perturbations): High - standard technique with clear implementation

## Next Checks

1. Compare evaluation results when using segments ranked by different XAI tools to verify the correlation assumption between real and fake explanations
2. Test the approach on deepfake images with significant face orientation differences to assess robustness to alignment variations
3. Measure and report computational overhead of the full evaluation pipeline across all XAI tools and datasets