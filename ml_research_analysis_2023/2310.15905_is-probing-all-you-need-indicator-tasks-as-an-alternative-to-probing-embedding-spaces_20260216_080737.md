---
ver: rpa2
title: Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding
  Spaces
arxiv_id: '2310.15905'
source_url: https://arxiv.org/abs/2310.15905
tags:
- probes
- tasks
- representations
- indicator
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Probing tasks are commonly used to evaluate linguistic information
  in word representations, but their reliance on trainable classifiers introduces
  uncertainty about what is actually being measured. This work introduces "indicator
  tasks" as non-trainable alternatives that query representations directly without
  auxiliary models.
---

# Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces

## Quick Facts
- arXiv ID: 2310.15905
- Source URL: https://arxiv.org/abs/2310.15905
- Reference count: 9
- Key outcome: Indicator tasks can detect linguistic information in word representations that linear probes miss, particularly for non-linearly separable properties

## Executive Summary
This paper challenges the dominance of trainable probe classifiers in evaluating linguistic information in word representations by introducing "indicator tasks" - non-trainable methods that directly query embeddings without auxiliary models. Through two test cases (gender debiasing and morphological feature removal), the authors demonstrate that while linear probes often suggest successful information removal, indicator tasks frequently reveal residual information that probes miss. The work argues that probes' reliance on linear separability limits their effectiveness, and that indicator tasks should be used alongside probes to provide a more complete picture of what information exists in embedding spaces.

## Method Summary
The authors compare linear probe classifiers with indicator tasks for evaluating information in word representations. For gender debiasing, they use RLACE for bias removal and evaluate with linear probes, WEAT (Word Embedding Association Test), and KNN-bias correlation. For morphological property removal, they employ Iterative Nullspace Projection (I2NLP) in both regressive and non-regressive variants, using linear probes and a custom DEOD (Detecting Outliers Of Dimensionality) indicator task. The DEOD task identifies outliers among sets of morphologically related words, testing whether semantic or morphological similarity is prioritized in the representations.

## Key Results
- Linear probes showed complete gender bias removal with RLACE, while WEAT and KNN-bias correlation indicators still detected residual bias
- For morphological property removal, probes suggested the non-regressive I2NLP variant was superior, while DEOD indicators showed the opposite result
- Indicator tasks demonstrated sensitivity to non-linearly expressed properties that linear probes failed to detect
- The study concludes that using both probes and indicators provides a more comprehensive evaluation than either method alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probing tasks with trainable classifiers can miss information that exists in embedding spaces but is not linearly separable
- Mechanism: Linear probes can only detect information that is linearly separable in the embedding space, while indicator tasks can detect both linear and non-linear patterns
- Core assumption: The embedding space contains both linearly and non-linearly separable information
- Evidence anchors:
  - [abstract]: "probes may miss non-linearly expressed properties"
  - [section 5.3]: "While the linear probe indicates the successful removal of the bias, it is only able to tell whether there is a linear separation between the potentially biased words"
  - [corpus]: Weak - no direct evidence found about linear vs non-linear separation in corpus

### Mechanism 2
- Claim: Indicator tasks provide a more accurate picture of information erasure than probes
- Mechanism: Indicator tasks evaluate information directly from representations without an auxiliary model, avoiding entanglement between probe results and classifier nature
- Core assumption: Direct evaluation of representations is more accurate than evaluation through a trainable model
- Evidence anchors:
  - [abstract]: "probes may miss non-linearly expressed properties"
  - [section 3]: "indicators can be perceived as a resurgence of intrinsic evaluation methods"
  - [section 5.3]: "While probes point to no bias at all, both indicators point only to a small drop"

### Mechanism 3
- Claim: The specific design of indicator tasks can evaluate multiple dimensions of information simultaneously
- Mechanism: Indicator tasks can be designed to evaluate multiple types of information (e.g., morphological and semantic) in a single evaluation, which is difficult with probes
- Core assumption: Complex linguistic information can be evaluated through carefully designed similarity-based tasks
- Evidence anchors:
  - [section 6.2]: "By predicting a single outlier from each set, models take part in a zero sum game, and we get to evaluate which of the two dimensions of meaning is prioritized in the representations"
  - [section 6.4]: "The indicator's double purpose allows evaluation of more aspects of the space"

## Foundational Learning

- Concept: Linear vs non-linear separability in high-dimensional spaces
  - Why needed here: Understanding why probes (linear classifiers) might miss certain types of information in embeddings
  - Quick check question: Can you give an example of information that would be linearly separable vs non-linearly separable in a 2D space?

- Concept: Adversarial concept erasure methods
  - Why needed here: Understanding the methods used to remove gender bias and morphological information
  - Quick check question: What is the key difference between RLACE and INLP in terms of their approach to concept erasure?

- Concept: Intrinsic vs extrinsic evaluation of word representations
  - Why needed here: Understanding the historical context and motivation for indicator tasks
  - Quick check question: What was the main criticism of intrinsic evaluation methods that led to the rise of probing tasks?

## Architecture Onboarding

- Component map: Data preprocessing -> Representation extraction (BERT embeddings) -> Probe classifiers -> Indicator tasks (WEAT, KNN-bias correlation, DEOD) -> Evaluation and comparison -> Visualization

- Critical path: Load and preprocess data -> Extract contextualized representations -> Apply concept erasure method -> Run probe evaluations -> Run indicator task evaluations -> Compare and visualize results

- Design tradeoffs:
  - Simplicity vs comprehensiveness of probes
  - Task-specific vs general-purpose indicator design
  - Computational cost of multiple evaluation methods
  - Interpretability of results from different methods

- Failure signatures:
  - Probes show perfect erasure but indicators still detect information
  - Contradictory results between different indicator tasks
  - High variance in indicator task results across different runs
  - Probes fail to converge or overfit on training data

- First 3 experiments:
  1. Reproduce the gender debiasing experiment with RLACE to verify that probes show complete erasure while WEAT and KNN still detect bias
  2. Implement the DEOD indicator task for morphological properties and test it on a small set of words to verify it can detect both morphological and semantic outliers
  3. Compare the regressive and non-regressive variants of I2NLP using both probes and the DEOD indicator to verify the contradictory results claimed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically develop indicator tasks for properties where simple vector similarity-based approaches are not immediately applicable?
- Basis in paper: [explicit] The authors acknowledge that designing indicator tasks requires "more creativity" and "tailor-made decisions" compared to standard probes
- Why unresolved: The paper provides the general principle of using vector similarities but doesn't offer a concrete methodology for developing indicators for arbitrary properties beyond the specific examples given
- What evidence would resolve it: A systematic framework or set of principles for designing indicator tasks across different linguistic properties, along with empirical validation showing these indicators work consistently across multiple domains

### Open Question 2
- Question: To what extent do indicator tasks capture the same information as probes when properties are expressed in linear versus non-linear ways in embedding spaces?
- Basis in paper: [explicit] The authors demonstrate that probes detect linear gender bias while indicators detect non-linear bias, but don't systematically characterize when each approach succeeds or fails
- Why unresolved: The paper shows the methods can give contradictory results but doesn't provide a general theory of when linear probes versus non-linear indicators will be more informative
- What evidence would resolve it: Empirical studies across multiple properties showing the correlation between probe performance and indicator performance as a function of the linearity of the property's expression in the embedding space

### Open Question 3
- Question: How can we interpret the absolute performance scores from indicator tasks to determine whether a property is meaningfully present in representations?
- Basis in paper: [explicit] The authors note that "the interpretability of the absolute results is still unclear with indicators as it is with probes" and ask whether 87% performance indicates a property exists
- Why unresolved: The paper identifies this as a limitation but doesn't propose solutions for determining what constitutes meaningful detection versus chance performance
- What evidence would resolve it: Development of statistical significance tests or benchmarks for indicator tasks, or methods to establish baselines for comparison across different properties and datasets

## Limitations

- The paper lacks empirical validation for claims about linear vs non-linear separability in embedding spaces
- DEOD indicator task design relies on unspecified semantic similarity thresholds affecting reproducibility
- The claim that indicator tasks provide "more accurate" evaluation is not conclusively demonstrated

## Confidence

- **High confidence**: The observation that probes and indicators can produce contradictory results in concept erasure evaluation
- **Medium confidence**: The claim that probes may miss non-linearly expressed properties (theoretically sound but lacks direct empirical validation)
- **Low confidence**: The assertion that indicator tasks provide a "more accurate" picture of information erasure than probes

## Next Checks

1. **Direct separability analysis**: Analyze the BERT embedding space to determine what proportion of linguistic information is linearly versus non-linearly separable, providing empirical support for the claim about probe limitations.

2. **Indicator task sensitivity analysis**: Systematically vary the semantic similarity thresholds and other hyperparameters in the DEOD indicator task to understand how these choices affect the evaluation results and their comparability with probe outcomes.

3. **Cross-method correlation study**: Conduct a comprehensive study correlating probe results with multiple indicator task results across different types of linguistic information to establish when and why the methods agree or disagree.