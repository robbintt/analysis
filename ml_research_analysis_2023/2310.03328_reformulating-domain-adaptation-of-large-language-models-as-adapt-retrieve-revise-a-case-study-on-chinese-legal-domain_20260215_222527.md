---
ver: rpa2
title: 'Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise:
  A Case Study on Chinese Legal Domain'
arxiv_id: '2310.03328'
source_url: https://arxiv.org/abs/2310.03328
tags:
- legal
- answer
- gpt-4
- retrieval
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of domain adaptation for large
  language models in specialized domains, specifically focusing on the Chinese legal
  domain. The authors propose a novel framework called "Adapt-Retrieve-Revise" that
  reformulates the generation process to overcome the limitations of direct application
  of large language models like GPT-4 in specific domains.
---

# Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain

## Quick Facts
- arXiv ID: 2310.03328
- Source URL: https://arxiv.org/abs/2310.03328
- Reference count: 18
- Key outcome: Improves GPT-4 accuracy on Chinese legal tasks by 33.3% vs direct generation

## Executive Summary
This paper addresses the challenge of applying large language models to specialized domains by proposing a novel "Adapt-Retrieve-Revise" framework. The authors demonstrate that instead of directly applying powerful models like GPT-4 to domain-specific tasks, it's more effective to first adapt a smaller 7B model to the domain, use it to generate draft answers, retrieve supporting evidence, and then have GPT-4 revise the draft based on this evidence. The method significantly outperforms both direct GPT-4 generation and two retrieval-based baselines on Chinese legal tasks, achieving 33.3% accuracy improvement over GPT-4's direct generation.

## Method Summary
The method consists of three key steps: First, a 7B LLM is adapted to the target Chinese legal domain through continued learning on in-domain data (50B tokens of legal corpora plus 70K legal instructions). Second, this domain-adapted model generates draft answers to legal questions. Third, the draft answers are used to retrieve supporting evidence from an external knowledge base using sentence embeddings (E5 model), and GPT-4 revises the draft answer based on this retrieved evidence. The framework is evaluated on four Chinese legal tasks including Legal Case Retrieval, Chinese Contract Problem, LegalQA, and JEC-QA.

## Key Results
- 33.3% improvement in accuracy on Chinese legal tasks compared to direct GPT-4 generation
- Outperforms two stronger retrieval-based baselines by 15.4% and 23.9%
- Demonstrates effectiveness of combining efficient domain adaptation with powerful revision capabilities
- Shows answer-based retrieval outperforms query-based retrieval in this domain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Domain-adapted 7B LLM generates more accurate drafts than direct GPT-4 generation in specialized domains.
- **Mechanism**: Continuing learning on in-domain data enables the 7B model to acquire domain-specific knowledge, reducing hallucinations.
- **Core assumption**: A 7B model trained on sufficient in-domain data can learn task-relevant patterns without catastrophic forgetting.
- **Evidence anchors**:
  - [abstract]: "adapt an affordable 7B LLM to the target domain by continuing learning on in-domain data"
  - [section 2.1]: "continually train a Chinese pre-trained LLM on the Chinese legal domain corpora to derive a domain-adapted legal LLM"
  - [corpus]: No direct corpus evidence for catastrophic forgetting prevention; this is an implicit assumption.
- **Break condition**: If in-domain data is insufficient or too noisy, the 7B model may fail to acquire useful patterns and hallucinate similarly to GPT-4.

### Mechanism 2
- **Claim**: Answer-based retrieval improves evidence quality over query-based retrieval.
- **Mechanism**: The draft answer contains more semantic information than the query, leading to better similarity matching with evidence.
- **Core assumption**: Semantic similarity between draft answers and evidence is higher than between queries and evidence.
- **Evidence anchors**:
  - [abstract]: "the draft answer will be used to retrieve supporting evidence candidates from an external in-domain knowledge base"
  - [section 4.1.1]: "the top-1 retrieved law clause based on the answer competes with the top-5 law clauses based on the query"
  - [corpus]: No corpus evidence explicitly comparing embedding quality; this is based on internal experimental results.
- **Break condition**: If the draft answer is too noisy or hallucinatory, the retrieval may return irrelevant evidence, harming performance.

### Mechanism 3
- **Claim**: GPT-4 can effectively revise draft answers using retrieved evidence to produce final accurate responses.
- **Mechanism**: GPT-4's evidence-assessing capability allows it to validate and correct the draft answer based on retrieved evidence.
- **Core assumption**: GPT-4 can accurately assess the relevance and correctness of retrieved evidence.
- **Evidence anchors**:
  - [abstract]: "let GPT-4 assess the evidence and revise the draft answer to generate the final answer"
  - [section 3.3]: "using the draft answers generated by the 7B legal model for retrieval and revision, the performance significantly exceeded two query-based retrieval baselines"
  - [corpus]: No corpus evidence for GPT-4's evidence-assessing capability; this is an empirical finding from the paper.
- **Break condition**: If retrieved evidence is irrelevant or insufficient, GPT-4 may fail to produce accurate revisions.

## Foundational Learning

- **Concept**: Continual learning on domain-specific data
  - Why needed here: To enable smaller models to acquire domain knowledge without requiring full-scale training of large LLMs
  - Quick check question: What is the minimum amount of in-domain data needed to achieve meaningful domain adaptation?

- **Concept**: Sentence embedding for retrieval
  - Why needed here: To compute semantic similarity between draft answers and evidence paragraphs
  - Quick check question: How does the choice of sentence embedding model affect retrieval recall?

- **Concept**: Prompt engineering for model revision
  - Why needed here: To guide GPT-4 in effectively revising draft answers using retrieved evidence
  - Quick check question: What prompt format yields the best revision performance for domain-specific tasks?

## Architecture Onboarding

- **Component map**: Query → Domain-adapted 7B LLM (draft generator) → Answer embedding (E5) → Evidence retrieval (kNN search) → GPT-4 (revise module) → Final answer

- **Critical path**: Query → Draft generation → Answer embedding → Evidence retrieval → GPT-4 revision → Final answer

- **Design tradeoffs**:
  - Model size vs. domain knowledge acquisition
  - Retrieval recall vs. computational cost
  - GPT-4 API cost vs. performance improvement

- **Failure signatures**:
  - Low retrieval recall: Draft answers are too generic or hallucinatory
  - GPT-4 revision fails: Retrieved evidence is irrelevant or insufficient
  - Overall performance drop: Domain adaptation is ineffective or evidence base is poor quality

- **First 3 experiments**:
  1. Compare retrieval recall between query-based and answer-based approaches on a small sample
  2. Measure draft answer quality with and without domain adaptation
  3. Test GPT-4 revision performance with synthetic evidence pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Adapt-Retrieve-Revise method vary across different legal domains or jurisdictions beyond Chinese law?
- Basis in paper: [explicit] The paper focuses on Chinese legal tasks but mentions that the framework is flexible and could be adapted to other domains.
- Why unresolved: The study only evaluates the method on Chinese legal tasks, leaving open the question of its generalizability to other legal systems or domains.
- What evidence would resolve it: Testing the Adapt-Retrieve-Revise method on legal tasks from different countries or non-legal domains would provide insights into its broader applicability.

### Open Question 2
- Question: What is the impact of varying the size of the domain-adapted LLM on the performance of the Adapt-Retrieve-Revise method?
- Basis in paper: [explicit] The paper uses a 7B LLM for domain adaptation, but mentions that larger models might not be feasible for continual learning.
- Why unresolved: The paper does not explore how different model sizes affect the performance, which could provide insights into the optimal balance between model size and adaptation efficiency.
- What evidence would resolve it: Conducting experiments with domain-adapted LLMs of different sizes (e.g., 1B, 3B, 13B) would help determine the impact of model size on performance.

### Open Question 3
- Question: How does the quality of the draft answer generated by the domain-adapted LLM influence the effectiveness of the evidence retrieval and subsequent revision by GPT-4?
- Basis in paper: [inferred] The paper discusses the importance of the draft answer in the retrieval process but does not explicitly analyze how its quality affects the overall performance.
- Why unresolved: While the paper shows that the draft answer is crucial for retrieval, it does not delve into how variations in answer quality impact the final output after revision.
- What evidence would resolve it: Analyzing the relationship between draft answer quality and the accuracy of retrieved evidence, as well as the effectiveness of GPT-4's revision, would clarify this aspect.

### Open Question 4
- Question: What are the computational costs and efficiency trade-offs of using the Adapt-Retrieve-Revise method compared to direct generation by large LLMs like GPT-4?
- Basis in paper: [explicit] The paper mentions the efficiency of adapting a smaller 7B model and the evidence-assessing capability of GPT-4 but does not provide a detailed cost analysis.
- Why unresolved: The paper highlights the benefits of the proposed method but does not quantify the computational costs or compare them to the costs of using large LLMs directly.
- What evidence would resolve it: A detailed analysis of the computational resources required for each step of the Adapt-Retrieve-Revise method, compared to the costs of direct generation, would provide a clearer picture of its efficiency.

### Open Question 5
- Question: How does the choice of sentence embedding model (e.g., E5) affect the quality of evidence retrieval in the Adapt-Retrieve-Revise method?
- Basis in paper: [explicit] The paper uses Multilingual-E5-large for sentence embeddings but does not explore alternative models or their impact on retrieval quality.
- Why unresolved: While the paper demonstrates the effectiveness of the chosen embedding model, it does not investigate how different models might influence the retrieval process.
- What evidence would resolve it: Comparing the performance of the Adapt-Retrieve-Revise method using different sentence embedding models would reveal the impact of this choice on evidence retrieval quality.

## Limitations
- Limited evaluation scope to Chinese legal domain only
- No detailed analysis of retrieval quality or failure modes
- Hyperparameter details for domain adaptation not fully specified

## Confidence
**High Confidence**: The proposed Adapt-Retrieve-Revise framework is a novel and effective approach for domain adaptation of large language models. The experimental results demonstrate significant improvements over baseline methods.

**Medium Confidence**: The effectiveness of domain adaptation for the 7B model and the quality of retrieved evidence are crucial for the method's success. However, the paper lacks detailed analysis of these components.

**Low Confidence**: The generalizability of the results to other domains and languages is uncertain due to the limited evaluation scope and lack of cross-domain validation.

## Next Checks
1. Cross-domain evaluation: Test the Adapt-Retrieve-Revise framework on other specialized domains (e.g., medical, financial) to assess its generalizability and robustness.

2. Hyperparameter sensitivity analysis: Conduct experiments to determine the impact of different hyperparameters (e.g., learning rate, batch size) on the performance of the domain-adapted 7B model.

3. Retrieval quality analysis: Perform a detailed analysis of the retrieval module, including recall, precision, and the impact of retrieved evidence quality on the final performance.