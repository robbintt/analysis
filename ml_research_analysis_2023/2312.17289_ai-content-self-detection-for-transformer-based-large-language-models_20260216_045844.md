---
ver: rpa2
title: AI Content Self-Detection for Transformer-based Large Language Models
arxiv_id: '2312.17289'
source_url: https://arxiv.org/abs/2312.17289
tags:
- text
- chatgpt
- detection
- claude
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for detecting AI-generated text
  by asking the generative model to self-identify its own output. Experiments with
  three leading models (ChatGPT, Bard, Claude) show varying abilities to distinguish
  their own generated text from human-written text, with Bard performing best at 94%
  accuracy.
---

# AI Content Self-Detection for Transformer-based Large Language Models

## Quick Facts
- arXiv ID: 2312.17289
- Source URL: https://arxiv.org/abs/2312.17289
- Reference count: 32
- Primary result: Proposed self-detection method achieves 94% accuracy with Bard, compared to 56% with conventional ZeroGPT detector

## Executive Summary
This paper introduces the novel concept of direct origin detection, proposing that transformer-based generative AI models can self-identify their own generated text through zero-shot prompting. The study evaluates three leading models (ChatGPT, Bard, Claude) across 50 topics, testing their ability to distinguish their own generated text from human-written text and paraphrased versions. Results show significant variation in self-detection capabilities, with Bard achieving 94% accuracy while ChatGPT and Claude performed notably worse. The approach offers a promising alternative to conventional AI content detectors but faces practical limitations when the original generation prompt is unavailable.

## Method Summary
The study generates essays on 50 topics using three AI models (ChatGPT, Bard, Claude) and collects 50 human-written essays from news sources. Each model then attempts to self-detect its own generated text, paraphrased versions, and distinguish these from human text using zero-shot prompting with the query "If the following text matches its writing pattern and choice of words." Detection accuracy, detection rate, and precision are calculated and compared against a ZeroGPT baseline detector. The methodology tests three hypotheses: H1 (models can self-detect their own text), H2 (models can detect paraphrased versions of their own text), and H3 (models cannot detect other models' generated text).

## Key Results
- Bard achieved 94% accuracy in self-detecting its own generated text
- ChatGPT showed 74% accuracy in self-detection but only 58% accuracy for paraphrased text
- Claude performed poorly at 64% accuracy for original text but achieved 92% accuracy for paraphrased text
- All models performed near-chance level (50-55%) when detecting other models' generated text

## Why This Works (Mechanism)

### Mechanism 1
Transformer-based models can self-detect their own generated text because they have learned the exact probability distribution used during generation. During generation, the model computes P(ui+1|ui-k, ..., ui) using attention mechanisms that capture token relationships. When presented with complete text (prompt + generated text), the model can check if the sequence is consistent with its learned function by examining attention patterns and probability distributions. Core assumption: The model retains knowledge of its generation parameters and can access this during inference for detection purposes.

### Mechanism 2
Models can detect paraphrased versions of their own text because the artifacts created during paraphrasing still contain detectable patterns from the original generation process. When a model paraphrases its own generated text, it applies transformations while maintaining some of the original generation artifacts. The model can compare the paraphrased text against its knowledge of how it typically generates text, looking for consistent patterns in word choice, structure, and attention weights. Core assumption: Paraphrasing preserves enough of the original generation artifacts for detection, even though the prompt context is missing.

### Mechanism 3
Different models cannot detect each other's generated text because they learn different probability distributions and artifacts based on their training data and architecture. Each model learns a unique probability distribution based on its training corpus, fine-tuning process, and architecture. When asked to detect text from another model, it lacks the specific knowledge of that model's generation patterns and artifacts. Core assumption: Generation artifacts are specific enough to each model that cross-detection is difficult without access to the other model's parameters.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention works is crucial for grasping why models might detect their own text based on learned token relationships
  - Quick check question: How does the attention mechanism allow a transformer to modify tokens based on previously seen tokens in the sequence?

- Concept: Probability distributions in language modeling
  - Why needed here: The core mechanism relies on the model having learned a specific probability distribution for next-token prediction that it can later check for consistency
  - Quick check question: What is the mathematical form of the probability distribution that transformer-based models learn during pre-training?

- Concept: Zero-shot learning
  - Why needed here: The paper uses zero-shot prompting to ask models to self-detect without additional fine-tuning, which is a key methodological choice
  - Quick check question: What distinguishes zero-shot learning from few-shot or fine-tuning approaches in the context of asking models to perform new tasks?

## Architecture Onboarding

- Component map: Input processing (tokenization and embedding) -> Core processing (transformer blocks with self-attention) -> Generation (autoregressive next-token prediction) -> Detection (pattern matching against learned generation artifacts)
- Critical path: Token input → embedding → attention computation → probability distribution → token prediction → (for detection) pattern comparison
- Design tradeoffs: Accuracy vs. speed (more thorough detection requires more computation), Generalization vs. specificity (detecting own text may be easier than detecting other models' text), Prompt dependency (detection may rely on having access to generation context)
- Failure signatures: False positives (classifying human text as AI-generated), False negatives (failing to identify own generated text), Inconsistent results (performance varying significantly across different text types)
- First 3 experiments: 1) Test self-detection on simple, controlled text samples with known generation parameters, 2) Test cross-model detection to verify H3 (different models cannot detect each other's text), 3) Test detection performance on paraphrased text to understand the impact of context removal

## Open Questions the Paper Calls Out

### Open Question 1
Can transformer-based models reliably self-detect paraphrased versions of their own generated text when the original prompt is not available? The paper shows varying results across models with ChatGPT performing poorly (58% accuracy) while Bard performs better (92% accuracy), but only tested paraphrasing in one direction and found inconsistent results.

### Open Question 2
What specific features or artifacts in generated text enable successful self-detection by transformer-based models? While the paper demonstrates that self-detection is possible for some models, it doesn't analyze which linguistic, stylistic, or structural features of the text enable this detection.

### Open Question 3
How does the availability of the original prompt affect the ability of transformer-based models to self-detect their generated text? The paper explicitly states that "the prompt text is available during text generation and is included in the attention calculation. The used prompt is typically not available during self-detection."

### Open Question 4
Why does Claude show opposite performance patterns in self-detection between original and paraphrased text compared to other models? The paper notes that "Claude has difficulties in self-detecting its originally generated content but can detect content that it has paraphrased with a high degree of accuracy" but doesn't provide an explanation.

### Open Question 5
Can self-detection methods be effectively scaled to detect text from new or evolving AI models without requiring model-specific training? The study only tested three existing models and didn't explore how self-detection would perform with new models or models that have been significantly updated or fine-tuned.

## Limitations

- The artificial nature of the detection task where prompts are available during testing doesn't reflect real-world detection scenarios
- The study relies on untested assumptions about models' access to generation parameters during inference
- Limited evidence base for the hypothesis that models cannot detect each other's generated text

## Confidence

- High confidence: Bard's 94% self-detection accuracy is well-supported by experimental results
- Medium confidence: The claim that transformer models can self-detect based on learned probability distributions is plausible but relies on untested assumptions
- Low confidence: The hypothesis that models cannot detect each other's generated text requires further testing with different model architectures

## Next Checks

1. Test self-detection performance using only the generated text without the original prompt to better simulate real-world conditions
2. Evaluate cross-detection capabilities between different versions of the same model (e.g., GPT-3.5 vs GPT-4) to understand model evolution impacts
3. Assess detection accuracy across multiple domains (technical, creative, academic) to establish robustness boundaries