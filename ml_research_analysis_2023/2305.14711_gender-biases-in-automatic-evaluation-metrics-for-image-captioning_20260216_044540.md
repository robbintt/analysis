---
ver: rpa2
title: Gender Biases in Automatic Evaluation Metrics for Image Captioning
arxiv_id: '2305.14711'
source_url: https://arxiv.org/abs/2305.14711
tags:
- evaluation
- gender
- metrics
- biases
- clipscore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates gender biases in automatic evaluation metrics
  for image captioning tasks. It first curates a large dataset, PAO-EVALBIAS, comprising
  images of people with 90 professions, 52 activities, and 39 objects, each associated
  with stereotypical gender associations.
---

# Gender Biases in Automatic Evaluation Metrics for Image Captioning

## Quick Facts
- **arXiv ID**: 2305.14711
- **Source URL**: https://arxiv.org/abs/2305.14711
- **Reference count**: 11
- **Primary result**: Pretrained model-based metrics like CLIPScore encode gender biases that can propagate to image captioning models through reinforcement learning, but a hybrid metric combining CLIPScore and CIDEr significantly reduces these biases.

## Executive Summary
This paper investigates gender biases in automatic evaluation metrics for image captioning tasks. The authors curate a large dataset, PAO-EVALBIAS, comprising images of people with 90 professions, 52 activities, and 39 objects, each associated with stereotypical gender associations. Through systematic analysis, they demonstrate that pretrained model-based evaluation metrics, particularly CLIPScore, cannot distinguish between biased and unbiased outputs and often underperform statistical metrics in this regard. Furthermore, the study shows that biases encoded in these metrics can propagate to image captioning models through reinforcement learning. To address this issue, the authors propose a simple and effective hybrid metric by linearly combining n-gram matching-based and pretrained model-based metrics, which significantly reduces gender biases while maintaining strong correlations with human judgments.

## Method Summary
The study creates the PAO-EVALBIAS dataset containing 98,213 images with controlled gender variations across professions, activities, and objects. The authors evaluate multiple metrics (BLEU-4, METEOR, ROUGE, CIDEr, SPICE, CLIPScore) on this dataset to identify gender biases by measuring accuracy discrepancies between male and female associations. They then demonstrate bias propagation through reinforcement learning experiments using the FIBER model with different reward metrics. Finally, they propose and validate a hybrid metric combining CLIPScore and CIDEr to reduce gender biases while maintaining correlation with human judgments.

## Key Results
- CLIPScore exhibits significant gender bias, with 65.88%-69.23% of words showing significant bias under CLIPScore evaluation
- Biases in evaluation metrics propagate to image captioning models through reinforcement learning, amplifying stereotypical associations
- The hybrid CLIPScore+CIDEr metric eliminates gender biases on PAO-EVALBIAS while achieving improved correlation with human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIPScore cannot distinguish between biased and unbiased outputs, underperforming n-gram matching metrics in this regard.
- Mechanism: CLIPScore relies on cross-modal semantic similarity learned from large-scale image-text pairs, which may encode societal biases present in the training data. When evaluating captions with swapped gender terms, CLIPScore fails to differentiate between correct and incorrect gender associations because its learned representations reflect and amplify existing stereotypes.
- Core assumption: CLIPScore's cross-modal representations encode gender stereotypes from training data, leading to systematic preference for stereotypical gender assignments in captions.
- Evidence anchors:
  - [abstract]: "pretrained model-based evaluation metrics...cannot distinguish between biased and unbiased outputs, often underperforming statistical metrics in this regard"
  - [section 2.2]: "there are 65.88%, 69.23%, and 66.67% of the words that are significantly biased...under CLIPScore evaluation"
  - [corpus]: Weak evidence - corpus contains related metrics work but lacks direct CLIPScore bias studies

### Mechanism 2
- Claim: Biases encoded in model-based metrics can propagate to image captioning models through reinforcement learning.
- Mechanism: When reinforcement learning uses a biased evaluation metric as reward, the optimization process reinforces the metric's biases in the generation model. The model learns to generate captions that maximize the biased score, amplifying stereotypical associations rather than correcting them.
- Core assumption: RL optimization will follow the gradient of the biased evaluation metric, embedding its preferences into the generation model.
- Evidence anchors:
  - [abstract]: "the biases encoded in these metrics can be propagated to image captioning models through reinforcement learning"
  - [section 3.2]: "the use of CLIPScore as the reward can lead to gender prediction errors, which increases bias in the generated output"
  - [corpus]: Weak evidence - corpus shows related RL evaluation work but not specific bias propagation studies

### Mechanism 3
- Claim: A hybrid metric combining CLIPScore and CIDEr can reduce gender biases while maintaining strong correlation with human judgments.
- Mechanism: N-gram matching metrics like CIDEr inherently favor lexical overlap with reference captions containing correct gender terms, while CLIPScore captures semantic similarity. Combining them balances stereotype reduction with semantic evaluation quality.
- Core assumption: The n-gram component's lexical preference for correct gender terms counteracts the model-based component's stereotype amplification.
- Evidence anchors:
  - [abstract]: "a simple and effective hybrid metric by linearly combining n-gram matching-based and pretrained model-based metrics, which significantly reduces gender biases while maintaining strong correlations with human judgments"
  - [section 4.1]: "CLIPScore+CIDEr does not encode gender biases on the PAO-EVALBIAS dataset" and "CLIPScore+CIDEr achieves an improved correlation with human judgments"
  - [corpus]: Weak evidence - corpus contains related hybrid metric work but not specific bias mitigation studies

## Foundational Learning

- **Cross-modal representation learning**: Understanding how CLIPScore works and why it might encode biases requires knowledge of how models learn joint image-text representations from large-scale data
  - Why needed here: To understand why CLIPScore's learned representations reflect and amplify gender stereotypes
  - Quick check question: What is the primary difference between how CLIPScore and CIDEr evaluate image captions?

- **Reinforcement learning with non-differentiable rewards**: The paper shows how biased evaluation metrics affect generation models through RL, requiring understanding of how score-based rewards influence model training
  - Why needed here: To understand how using a biased metric as reward affects the optimization trajectory in RL
  - Quick check question: How does using a biased evaluation metric as reward affect the optimization trajectory in RL?

- **Bias measurement in NLP**: The paper's methodology for quantifying gender bias in evaluation metrics requires understanding of how to measure performance discrepancies across protected groups
  - Why needed here: To understand how the study measures accuracy differences between gender groups
  - Quick check question: What statistical test is used to determine if accuracy differences between gender groups are significant?

## Architecture Onboarding

- **Component map**: PAO-EVALBIAS dataset creation → bias evaluation framework → generation model training → RL fine-tuning with different reward metrics → hybrid metric development
- **Critical path**: 
  1. Create biased dataset with controlled gender variations
  2. Evaluate existing metrics for gender bias using accuracy disparity
  3. Demonstrate bias propagation through RL experiments
  4. Develop and validate hybrid metric solution
- **Design tradeoffs**:
  - Reference-free vs reference-based evaluation: CLIPScore offers reference-free evaluation but may encode biases; n-gram metrics need references but are less biased
  - Semantic vs lexical matching: CLIPScore captures semantic similarity but may amplify stereotypes; n-gram metrics ensure lexical correctness but miss semantic nuances
  - Computational efficiency vs bias detection: Model-based metrics are computationally expensive but may miss lexical gender errors that n-gram metrics catch
- **Failure signatures**:
  - If hybrid metric shows increased bias compared to individual components
  - If RL experiments show no difference between biased and debiased models
  - If human correlation drops significantly when combining metrics
- **First 3 experiments**:
  1. Reproduce CLIPScore bias analysis on PAO-EVALBIAS to verify percentage of biased words
  2. Test hybrid metric (CLIPScore + CIDEr) on the same dataset to confirm bias reduction
  3. Run RL experiment comparing CLIPScore, CIDEr, and hybrid rewards on FIBER model to observe bias propagation differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do biases in CLIPScore propagate through reinforcement learning when using other pretrained model-based evaluation metrics like UniEval or GPTScore as rewards?
- Basis in paper: [inferred] The paper demonstrates that using CLIPScore as a reward in reinforcement learning amplifies gender biases. It also mentions that other pretrained model-based metrics like UniEval and GPTScore exist.
- Why unresolved: The study focuses specifically on CLIPScore and does not explore the effects of other model-based metrics in reinforcement learning scenarios.
- What evidence would resolve it: Experiments comparing the propagation of biases through RL when using different model-based evaluation metrics (e.g., CLIPScore, UniEval, GPTScore) as rewards.

### Open Question 2
- Question: What is the impact of combining n-gram matching-based metrics with different pretrained model-based metrics on bias reduction and correlation with human judgments?
- Basis in paper: [explicit] The paper proposes combining CLIPScore with CIDEr to reduce gender biases while maintaining human judgment correlation. It mentions that the method is compatible with other statistical metrics.
- Why unresolved: The study only tests CLIPScore+CIDEr and CLIPScore+BLEU4 combinations, leaving the potential of other metric combinations unexplored.
- What evidence would resolve it: Systematic evaluation of hybrid metrics combining various n-gram matching-based metrics (e.g., BLEU, METEOR, ROUGE, SPICE) with different pretrained model-based metrics.

### Open Question 3
- Question: How do evaluation metric biases affect generation models across different multimodal tasks beyond image captioning?
- Basis in paper: [inferred] The paper discusses biases in evaluation metrics and their impact on image captioning models, but acknowledges that other multimodal generation tasks are worth investigating.
- Why unresolved: The study is limited to image captioning tasks and does not explore the effects of biased evaluation metrics on other multimodal generation tasks.
- What evidence would resolve it: Analysis of evaluation metric biases and their impact on generation models in various multimodal tasks such as visual question answering, image generation from text, and video captioning.

## Limitations

- The study's conclusions about CLIPScore's inability to distinguish biased from unbiased outputs rely heavily on a controlled synthetic dataset (PAO-EVALBIAS) with stereotypical gender associations
- The findings about RL bias propagation, though theoretically sound, were demonstrated using a single generation model (FIBER) and may not generalize across different architectures or training regimes
- The hybrid metric solution, while showing promise, requires empirical validation on diverse, real-world datasets to confirm its effectiveness beyond the synthetic test environment

## Confidence

- **High Confidence**: The observation that n-gram matching metrics exhibit lower gender bias compared to model-based metrics is well-supported by systematic analysis across 65.88%-69.23% of biased words in CLIPScore evaluations
- **Medium Confidence**: The claim about bias propagation through reinforcement learning is theoretically plausible and supported by experimental evidence, but the demonstration using a single generation model limits generalizability
- **Low Confidence**: The assertion that the CLIPScore+CIDEr hybrid metric significantly reduces gender biases while maintaining human correlation requires broader validation on diverse, unconstrained datasets

## Next Checks

1. **Real-world validation**: Test the hybrid CLIPScore+CIDEr metric on established image captioning datasets (COCO, Flickr30k) with human-annotated captions to verify that bias reduction generalizes beyond the synthetic PAO-EVALBIAS environment.

2. **Architecture generalization**: Repeat the RL bias propagation experiments using different generation architectures (e.g., BLIP, SimVLM) to determine whether the observed bias amplification is architecture-dependent or a more general phenomenon.

3. **Temporal stability analysis**: Evaluate whether the gender biases in CLIPScore and other model-based metrics change over time as the underlying vision-language models receive updates, and whether the hybrid metric's bias reduction properties remain stable across model versions.