---
ver: rpa2
title: Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse
  Relation Recognition
arxiv_id: '2309.07561'
source_url: https://arxiv.org/abs/2309.07561
tags:
- knowledge
- answer
- prompt
- space
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a continuous prompt learning method with connective
  knowledge distillation for implicit discourse relation recognition (IDRR). The method
  uses learnable vectors to form continuous prompt templates and automatically selects
  optimal templates through gradient search in embedding space.
---

# Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse Relation Recognition

## Quick Facts
- arXiv ID: 2309.07561
- Source URL: https://arxiv.org/abs/2309.07561
- Reference count: 40
- Primary result: Achieves F1 scores of 66.16%, 70.67%, and 71.79% with BERT, DeBERTa, and RoBERTa respectively on PDTB Corpus V3.0

## Executive Summary
This paper introduces AdaptPrompt, a continuous prompt learning method for implicit discourse relation recognition that addresses the challenge of manual prompt template design. The method uses learnable virtual tokens to form continuous prompt templates that are automatically optimized through gradient search in embedding space. Additionally, it employs a teacher-student architecture to transfer implicit connective knowledge through both response-based (soft labels) and feature-based (hidden embeddings) knowledge distillation. Experiments on PDTB Corpus V3.0 demonstrate state-of-the-art performance across multiple PLM backbones.

## Method Summary
AdaptPrompt proposes a continuous prompt learning approach with connective knowledge distillation for IDRR. The method uses learnable virtual tokens ([V] vectors) to form continuous prompt templates that are optimized through gradient search in embedding space. Virtual answers are generated through hierarchical mapping from third-level to top-level relations, initialized using average embeddings of connectives. A teacher model with integrated connectives transfers knowledge to student models via both response-based (soft labels) and feature-based (hidden embeddings) distillation, with final predictions obtained through ensemble averaging.

## Key Results
- AdaptPrompt with BERT achieves 66.16% F1 score on PDTB 3.0
- AdaptPrompt with DeBERTa achieves 70.67% F1 score on PDTB 3.0  
- AdaptPrompt with RoBERTa achieves 71.79% F1 score on PDTB 3.0
- Both response-based and feature-based knowledge distillation improve performance over baselines
- Ensemble of both distillation approaches yields best overall results

## Why This Works (Mechanism)

### Mechanism 1: Continuous Template Optimization via Gradient Search
- Claim: Learnable virtual tokens form continuous prompt templates optimized in embedding space
- Mechanism: Continuous template T(Arg1; Arg2) = [CLS] + Arg1 + [V]1, ..., [V]m + [MASK] + [V]m+1, ..., [V]m + Arg2 + [SEP] is fine-tuned through backpropagation
- Core assumption: Embedding space contains viable template configurations better than manual discrete templates
- Evidence anchors: Abstract mentions gradient search in embedding space; Section III.B describes fine-tuning through back-propagation
- Break condition: If embedding space lacks viable configurations, optimization converges to suboptimal performance

### Mechanism 2: Virtual Answer Space with Hierarchical Mapping
- Claim: Virtual answers from averaged connective embeddings better represent answer space than substantive answers
- Mechanism: Answer-relation mapping maps to third-level relations then top-level relations, with virtual answers initialized as average embeddings of connectives
- Core assumption: Virtual answers initialized from connective embeddings capture semantic relationships better than manually selected words
- Evidence anchors: Abstract describes answer-relation mapping rule; Section III.C explains initialization using average of word embeddings
- Break condition: If connective distributions are too sparse or overlapping, initialization fails to capture discriminative features

### Mechanism 3: Two-Stage Knowledge Distillation with Response and Feature Knowledge
- Claim: Combining response-based and feature-based knowledge distillation improves performance over single-type distillation
- Mechanism: Teacher model generates soft labels via temperature-scaled softmax and feature knowledge via last hidden states, learned by student models using KL divergence and MSE loss
- Core assumption: Both soft label distributions and hidden feature representations contain complementary knowledge
- Evidence anchors: Abstract mentions response-based (soft labels) and feature-based (hidden embeddings) distillation; Section III.D describes both approaches
- Break condition: If one distillation type dominates or contains noise, ensemble performance may degrade

## Foundational Learning
The method builds upon continuous prompt learning and knowledge distillation frameworks. It leverages the observation that manual prompt templates are suboptimal and that connective knowledge can be transferred through soft labels and hidden features. The hierarchical mapping approach suggests that implicit relations can be effectively represented through averaged connective embeddings.

## Architecture Onboarding
AdaptPrompt integrates with PLM backbones (BERT, DeBERTa, RoBERTa) by modifying their input templates with learnable virtual tokens. The teacher-student architecture requires two model instances: one with connective integration for knowledge generation and one or more students for knowledge absorption. The ensemble mechanism averages predictions from both distillation approaches.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of continuous prompt learning to larger template spaces and the effectiveness of knowledge distillation when teacher and student models have different architectures. It also questions whether the virtual answer initialization approach generalizes beyond PDTB Corpus.

## Limitations
The approach requires additional computational overhead for knowledge distillation training and may face challenges when scaling to languages with fewer annotated implicit discourse relations. The effectiveness depends on the quality of connective annotations and may not transfer well to domains with different connective distributions.

## Confidence
High confidence in the reported results based on state-of-the-art performance across multiple PLM backbones. The method's effectiveness is supported by both quantitative results and qualitative analysis of virtual token optimization.

## Next Checks
- Verify implementation details of hierarchical mapping from third-level to top-level relations
- Examine convergence behavior of virtual token optimization during training
- Analyze the impact of temperature scaling in response-based knowledge distillation
- Investigate the sensitivity of ensemble weights on final performance
- Validate the generalizability of virtual answer initialization across different discourse relation datasets