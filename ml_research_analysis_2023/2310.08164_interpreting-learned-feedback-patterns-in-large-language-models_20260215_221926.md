---
ver: rpa2
title: Interpreting Learned Feedback Patterns in Large Language Models
arxiv_id: '2310.08164'
source_url: https://arxiv.org/abs/2310.08164
tags:
- reward
- features
- feature
- learned
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel technique to interpret learned reward
  models in language models fine-tuned via reinforcement learning from human feedback
  (RLHF). The method trains sparse autoencoders on model activations to extract interpretable
  features, then uses GPT-4 to explain these features.
---

# Interpreting Learned Feedback Patterns in Large Language Models

## Quick Facts
- arXiv ID: 2310.08164
- Source URL: https://arxiv.org/abs/2310.08164
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: Proposes sparse autoencoder-based technique to interpret reward models in RLHF-tuned LLMs

## Executive Summary
This work introduces a novel approach to interpret learned reward models in language models fine-tuned via reinforcement learning from human feedback (RLHF). The method employs sparse autoencoders trained on model activations to extract interpretable features, which are then explained using GPT-4. By comparing features between base and RLHF-tuned models, the technique identifies divergences linked to reward modeling efficacy. Experiments on sentiment classification and utility table tasks demonstrate the approach's potential to quantify reward modeling accuracy and provide insights into learned behaviors.

## Method Summary
The method involves computing parameter divergence between base and RLHF-tuned models to identify high-divergence layers, then training sparse autoencoders on activations from these layers to extract interpretable features. GPT-4 is used to interpret these features by predicting descriptions from discretized activation patterns. The approach is validated by comparing feature descriptions between base and RLHF models, and by quantifying reward modeling efficacy through utility tables. The technique aims to provide insights into how LLMs internalize reward functions during RLHF training.

## Key Results
- Sparse autoencoders can extract interpretable features from LLM activations that correlate with reward modeling efficacy
- GPT-4 can automate interpretation of autoencoder features by predicting descriptions from discretized activation patterns
- Comparing feature explanations between base and RLHF-tuned models reveals divergences linked to reward modeling efficacy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders with sparsity constraints can extract interpretable features from LLM activations that correlate with reward modeling efficacy
- Mechanism: Autoencoders trained on high-divergence layers between base and RLHF-tuned models reconstruct activations using fewer active neurons, forcing the model to learn compressed representations of key activation patterns
- Core assumption: Features with high similarity between differently sized autoencoders represent ground truth features in the model
- Evidence anchors: [abstract], [section] on sparse autoencoders, [corpus] weak citations
- Break condition: If sparsity constraint is too weak or too strong, the autoencoder may fail to compress activations sufficiently or lose too much information

### Mechanism 2
- Claim: GPT-4 can automate interpretation of autoencoder features by predicting descriptions from discretized activation patterns
- Mechanism: Given normalized and discretized activations for a feature, GPT-4 predicts a human-readable explanation of what the neuron represents
- Core assumption: GPT-4's predictions, when validated against actual weights, provide accurate interpretations of the underlying features
- Evidence anchors: [section] on GPT-4 feature interpretation, [corpus] weak citations
- Break condition: If GPT-4's predictions are systematically inaccurate or validation metric is too permissive

### Mechanism 3
- Claim: Comparing feature explanations between base and RLHF-tuned models reveals divergences linked to reward modeling efficacy
- Mechanism: By analyzing which features emerge or change after RLHF training, we can infer how the model internalizes the reward function
- Core assumption: Differences in feature descriptions between models directly reflect differences in how well each model has learned the training objective
- Evidence anchors: [abstract], [section] on case studies, [corpus] weak citations
- Break condition: If feature differences are due to other factors rather than reward learning, interpretations may be misleading

## Foundational Learning

- Concept: Sparse coding and dictionary learning
  - Why needed here: The method relies on autoencoders with sparsity constraints to extract meaningful features from high-dimensional activations
  - Quick check question: What happens to the reconstruction error if the sparsity constraint is relaxed too much?

- Concept: Mechanistic interpretability frameworks
  - Why needed here: The work builds on prior methods for interpreting neural networks (e.g., polytope lens, neuron prediction)
  - Quick check question: How does the polytope lens approach differ from sparse autoencoders in interpreting neural activations?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: The method is designed specifically to interpret reward models learned through RLHF
  - Quick check question: In RLHF, what role does the reward model play in fine-tuning the base LLM?

## Architecture Onboarding

- Component map: Base model (Mbase) -> RLHF-tuned model (MRLHF) -> Layer divergence computation -> Sparse autoencoders -> Feature extraction via MMCS -> GPT-4 interpretation -> Utility table validation

- Critical path:
  1. Compute parameter divergence between models
  2. Train sparse autoencoders on high-divergence layers
  3. Extract high-similarity features via MMCS
  4. Use GPT-4 to interpret features
  5. Compare feature descriptions between models
  6. Quantify reward modeling efficacy via utility table

- Design tradeoffs:
  - Autoencoder size: Larger dictionaries capture more features but may include noise; smaller dictionaries are cleaner but risk missing important features
  - Sparsity constraint: Too weak → loss of interpretability; too strong → loss of information
  - GPT-4 interpretation: Powerful but computationally expensive; may introduce bias if validation is weak

- Failure signatures:
  - High reconstruction error → autoencoder not capturing key patterns
  - Low MMCS similarity → features not repeatable across runs
  - Poor GPT-4 validation scores → interpretations not aligned with actual weights
  - No meaningful differences between base and RLHF models → method not detecting reward-related changes

- First 3 experiments:
  1. Train autoencoders on Pythia-70m base vs RLHF-tuned (sentiment classifier reward) and compare top-k features
  2. Repeat with utility table reward function and validate feature utility scores against the lexicon
  3. Scale up to Pythia-410m and assess computational feasibility of feature interpretation

## Open Questions the Paper Calls Out

- Question: How can the interpretability technique be validated to ensure it provides faithful, complete, and minimal explanations of learned reward models?
  - Basis in paper: [explicit] The paper acknowledges the need for rigorous validation to ensure the technique provides accurate interpretations
  - Why unresolved: The current method relies on GPT-4's explanations, which may not always be accurate or complete
  - What evidence would resolve it: Developing quantitative metrics to evaluate faithfulness, completeness, and minimality of explanations

- Question: How can the sparse autoencoder-based technique be scaled up to interpret reward models in larger language models with hundreds or thousands of features?
  - Basis in paper: [inferred] The paper mentions that in larger models, interpreting many hundreds or thousands of features becomes computationally intensive
  - Why unresolved: The current technique may become impractical for very large models with many features
  - What evidence would resolve it: Developing more efficient autoencoder architectures or alternative feature extraction methods

- Question: How can the technique be extended to provide a more formal and complete interpretation of the internal structure of learned reward models?
  - Basis in paper: [explicit] The paper suggests that future work could attempt to completely map the internal structure of a learned reward model
  - Why unresolved: The current technique focuses on individual features but does not provide a comprehensive understanding of how these features interact
  - What evidence would resolve it: Developing methods to analyze relationships and interactions between identified features

## Limitations
- Method relies heavily on GPT-4's interpretation accuracy, which may introduce systematic biases
- Sparse autoencoder hyperparameters are not fully specified, making reproducibility challenging
- Approach is computationally expensive for larger models, limiting practical scalability

## Confidence
- High confidence in the core mechanism of using sparse autoencoders for feature extraction
- Medium confidence in GPT-4's ability to accurately interpret complex neural features
- Medium confidence in the method's ability to detect reward modeling efficacy through feature comparison
- Low confidence in scalability claims due to limited experimental scope

## Next Checks
1. Test the method on a larger model (e.g., Pythia-1.4B) to assess computational feasibility and interpretation quality
2. Conduct ablation studies on autoencoder hyperparameters to determine optimal sparsity constraints and dictionary sizes
3. Validate GPT-4 interpretations using human annotators on a subset of features to measure interpretation accuracy and consistency