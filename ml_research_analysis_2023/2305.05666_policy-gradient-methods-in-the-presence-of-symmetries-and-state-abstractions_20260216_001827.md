---
ver: rpa2
title: Policy Gradient Methods in the Presence of Symmetries and State Abstractions
arxiv_id: '2305.05666'
source_url: https://arxiv.org/abs/2305.05666
tags:
- dhpg
- policy
- udcurlymod
- divides
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends Markov Decision Process (MDP) homomorphisms
  to continuous state and action spaces, enabling state-action abstraction in continuous
  control settings. The authors derive a policy gradient theorem for both stochastic
  and deterministic policies that leverages MDP homomorphisms for improved sample
  efficiency and generalization.
---

# Policy Gradient Methods in the Presence of Symmetries and State Abstractions

## Quick Facts
- **arXiv ID**: 2305.05666
- **Source URL**: https://arxiv.org/abs/2305.05666
- **Reference count**: 40
- **Primary result**: Extends MDP homomorphisms to continuous spaces and derives policy gradient theorems for stochastic and deterministic policies, showing improved sample efficiency and generalization on continuous control tasks.

## Executive Summary
This paper extends the theory of MDP homomorphisms to continuous state and action spaces, enabling state-action abstraction in continuous control settings. The authors derive policy gradient theorems (HPG) for both stochastic and deterministic policies that leverage MDP homomorphisms for improved sample efficiency and generalization. They propose a deep actor-critic algorithm called DHPG that simultaneously learns the policy and MDP homomorphism map using the lax bisimulation metric. Experiments on DeepMind Control Suite and novel environments with continuous symmetries demonstrate DHPG's effectiveness in representation learning and policy optimization, outperforming strong baselines.

## Method Summary
DHPG is a deep actor-critic algorithm that learns both a policy and an MDP homomorphism mapping (f, gs) that reduces the complexity of the original MDP. The MDP homomorphism is learned using the lax bisimulation metric, which measures the distance between abstract states based on their future reward distributions and transition dynamics. The algorithm uses n-step TD error to train both the actual and abstract critics, and the policy gradient is computed using either the standard policy gradient theorem (stochastic) or deterministic policy gradient (deterministic). For stochastic policies, a policy lifting procedure couples the actual and abstract policies to ensure they remain aligned during training.

## Key Results
- DHPG outperforms strong baselines (DBC, DeepMDP, SAC-AE, DrQ-v2) on DeepMind Control Suite tasks with pixel observations.
- Stochastic DHPG is superior to deterministic DHPG in environments with continuous symmetries, achieving higher levels of action abstraction.
- Learned abstractions lead to improved sample efficiency, with performance profiles showing DHPG solving more tasks with fewer samples.
- Visualizations of the learned MDP homomorphism map reveal the structure of the state-action abstractions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous MDP homomorphisms preserve value and optimal value equivalence in continuous state and action spaces.
- Mechanism: The MDP homomorphism mapping (f, gs) maintains reward invariance and equivariant transitions, allowing the policy gradient to be derived on the abstract MDP while preserving the performance measure of the original MDP.
- Core assumption: The state and action mappings f and gs are measurable and surjective, and the MDPs are continuous with respect to the given assumptions.
- Evidence anchors:
  - [abstract] "We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies."
  - [section 4] "In this section, we will formalize MDP homomorphisms for general continuous domains...We will see the reason we require this condition."
- Break condition: If the MDP homomorphism map fails to preserve reward invariance or equivariant transitions, the value equivalence breaks.

### Mechanism 2
- Claim: The homomorphic policy gradient (HPG) theorem allows for leveraging approximate symmetries of the environment for improved sample efficiency and generalization.
- Mechanism: By training the abstract critic on a simplified problem represented by the MDP homomorphic image, each abstract state-action pair represents all equivalent pairs under the MDP homomorphism relation, leading to improved sample efficiency.
- Core assumption: The MDP homomorphism mapping effectively reduces the complexity of the problem by exploiting environmental symmetries.
- Evidence anchors:
  - [abstract] "Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization."
  - [section 5] "The abstract critic Qπθ is trained on a simplified problem...each abstract state-action pair (s, a) used to train Qπθ represents all (s, a) pairs that are equivalent under the MDP homomorphism relation."
- Break condition: If the MDP homomorphism map fails to effectively reduce the complexity of the problem, the sample efficiency gains are lost.

### Mechanism 3
- Claim: Stochastic DHPG outperforms deterministic DHPG in environments with continuous symmetries due to its ability to achieve higher levels of action abstraction.
- Mechanism: Stochastic DHPG does not impose any structure on the action encoder gs, allowing for strict collapses in the action space and effective dimensionality reduction, while deterministic DHPG requires gs to be a local diffeomorphism.
- Core assumption: The environment exhibits continuous symmetries that can be exploited for action abstraction.
- Evidence anchors:
  - [abstract] "Stochastic DHPG is superior to deterministic DHPG in environments with continuous symmetries as it is capable of a more powerful action abstraction."
  - [section 5] "A key aspect of MDP homomorphisms is the notion of 'collapse'...However, such action reductions do not meet the conditions of the deterministic HPG theorem."
- Break condition: If the environment lacks continuous symmetries, the advantage of stochastic DHPG over deterministic DHPG diminishes.

## Foundational Learning

- Concept: Measure Theory
  - Why needed here: Continuous MDP homomorphisms require measure-theoretic definitions to handle general state and action spaces.
  - Quick check question: What is the difference between a probability measure and a pushforward measure?

- Concept: Differential Geometry
  - Why needed here: Local diffeomorphism is a key requirement for the deterministic HPG theorem, and it is defined in the context of differential manifolds.
  - Quick check question: What is the difference between a diffeomorphism and a local diffeomorphism?

- Concept: Functional Analysis
  - Why needed here: The existence of a lifted policy for continuous MDP homomorphisms is proven using results from functional analysis, specifically the Hahn-Banach and Riesz Representation theorems.
  - Quick check question: What is the Riesz Representation theorem, and how is it used to prove the existence of a lifted policy?

## Architecture Onboarding

- Component map:
  - MDP homomorphism map (hφ,η): learns the state and action mappings (f, gs)
  - Pixel encoder (Eµ): encodes pixel observations into states
  - Actual critic (Qψ) and policy (π↑θ): operates on the original MDP
  - Abstract critic (Qψ) and policy (πθ): operates on the abstract MDP
  - Reward predictor (Rρ) and transition dynamics (τν): learn the MDP homomorphism

- Critical path:
  1. Encode pixel observations into states using the pixel encoder.
  2. Learn the MDP homomorphism map using the lax bisimulation metric.
  3. Train the actual and abstract critics using n-step TD error.
  4. Update the actual policy using the standard policy gradient theorem (stochastic) or deterministic policy gradient (deterministic).
  5. Update the abstract policy using the homomorphic policy gradient theorem.
  6. Apply the policy lifting procedure to couple the actual and abstract policies.

- Design tradeoffs:
  - Stochastic vs. deterministic policies: Stochastic policies allow for higher levels of action abstraction but may require more samples, while deterministic policies are more sample-efficient but limited in their ability to exploit continuous symmetries.
  - Number of steps for n-step TD error: Higher values improve sample efficiency but may introduce bias.

- Failure signatures:
  - Poor performance: The MDP homomorphism map may not be effectively reducing the complexity of the problem.
  - High variance in policy gradient estimates: The homomorphic policy gradient may not be providing a stable estimate of the gradient.
  - Unstable training: The policy lifting procedure may not be effectively coupling the actual and abstract policies.

- First 3 experiments:
  1. Train DHPG on a simple continuous control task (e.g., pendulum) and compare the performance with a baseline method.
  2. Visualize the learned MDP homomorphism map and the latent states to verify that the abstract MDP is effectively reducing the complexity of the problem.
  3. Evaluate the sample efficiency of stochastic vs. deterministic DHPG on a task with continuous symmetries (e.g., cylinder rotation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of MDP homomorphism map structure (e.g., convolutional vs MLP) affect the learned state-action abstractions and policy performance?
- Basis in paper: [inferred] The paper mentions using MLPs for the MDP homomorphism map but does not explore alternative architectures or their impact on performance.
- Why unresolved: The paper does not conduct experiments comparing different architectures for the MDP homomorphism map or analyze how architectural choices affect the quality of learned abstractions.
- What evidence would resolve it: Experiments comparing different architectures (e.g., convolutional vs MLP) for the MDP homomorphism map on the same tasks, along with analysis of the resulting state-action abstractions and policy performance.

### Open Question 2
- Question: Can DHPG effectively leverage symmetries in environments with multiple, interacting symmetries (e.g., a system with both rotational and translational symmetries)?
- Basis in paper: [explicit] The paper introduces environments with continuous symmetries but only explores single-symmetry cases (e.g., rotational or translational).
- Why unresolved: The paper does not investigate how DHPG handles environments with multiple, interacting symmetries, which are common in real-world robotics applications.
- What evidence would resolve it: Experiments on environments with multiple, interacting symmetries, comparing DHPG's performance to baselines and analyzing the learned MDP homomorphism map's ability to capture these symmetries.

### Open Question 3
- Question: How does the choice of n-step return (n) impact the sample efficiency and final performance of DHPG, particularly in environments with long time horizons or delayed rewards?
- Basis in paper: [explicit] The paper mentions using n-step returns and presents an ablation study comparing 1-step and 3-step returns, but does not explore a wider range of n values or analyze the impact on environments with long time horizons.
- Why unresolved: The paper does not investigate the optimal choice of n for different environments or provide insights into how n affects the trade-off between bias and variance in the n-step return estimator.
- What evidence would resolve it: Experiments on environments with varying time horizons and reward structures, comparing DHPG's performance with different n values and analyzing the bias-variance trade-off in the n-step return estimator.

## Limitations
- The theoretical foundations rely heavily on measure-theoretic and differential geometric assumptions that may be difficult to verify in practice.
- The policy lifting procedure for stochastic policies is not fully specified, creating potential reproducibility challenges.
- Benefits are primarily demonstrated on tasks with clear geometric symmetries; effectiveness in environments without exploitable symmetries remains unclear.

## Confidence
**High Confidence**: The theoretical framework for continuous MDP homomorphisms is mathematically rigorous, with clear proofs for the deterministic HPG theorem. The experimental results showing performance improvements on DeepMind Control Suite tasks are compelling and well-analyzed.

**Medium Confidence**: The stochastic HPG theorem and its implementation details (particularly the policy lifting procedure) are less thoroughly explained. While the experimental results show superior performance in symmetry-rich environments, the gap between stochastic and deterministic DHPG is not fully characterized across different task types.

**Low Confidence**: The practical implications of the measure-theoretic and differential geometric assumptions in real-world applications. The scalability of the approach to more complex, high-dimensional environments with less obvious symmetries.

## Next Checks
1. **Verify Policy Lifting Implementation**: Implement and test the stochastic policy lifting procedure on a simple continuous control task (e.g., pendulum) to ensure the abstract and actual policies remain coupled during training.

2. **Evaluate on Non-Symmetric Environments**: Test DHPG on control tasks without clear geometric symmetries (e.g., robotic manipulation with irregular objects) to determine if the approach provides benefits beyond symmetry exploitation.

3. **Ablation Study on MDP Homomorphism Components**: Perform controlled experiments varying the strength of the MDP homomorphism constraints (e.g., relaxing the local diffeomorphism requirement) to understand which theoretical assumptions are practically essential for performance gains.