---
ver: rpa2
title: Graph schemas as abstractions for transfer learning, inference, and planning
arxiv_id: '2302.07350'
source_url: https://arxiv.org/abs/2302.07350
tags:
- schemas
- learning
- schema
- graph
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Graph schemas are proposed as a model of abstraction for transfer
  learning in navigation tasks. The approach leverages learned latent graph structures
  as schemas, which can be reused in novel environments with rapid binding of new
  observations.
---

# Graph schemas as abstractions for transfer learning, inference, and planning

## Quick Facts
- **arXiv ID**: 2302.07350
- **Source URL**: https://arxiv.org/abs/2302.07350
- **Reference count**: 40
- **Primary result**: Graph schemas learn navigation environments in far fewer episodes than previous methods and enable optimal planning through rapid transfer and compositionality

## Executive Summary
Graph schemas are proposed as a model of abstraction for transfer learning in navigation tasks. The approach leverages learned latent graph structures as schemas, which can be reused in novel environments with rapid binding of new observations. Experiments on benchmarks like Memory & Planning Game and One-Shot StreetLearn show that graph schemas learn environments in far fewer episodes than previous methods and enable optimal planning. In highly aliased 2D and 3D environments, schemas can be matched and reused across size and observation variations, with recognition typically achieved within 100 steps. The method also demonstrates rapid learning of composed environments by reusing known schemas, outperforming full learning from scratch.

## Method Summary
The method learns latent graph structures using Clone-structured Cognitive Graphs (CSCG), which model the environment as a hidden Markov model with cloned states. During training, the model learns transition tensors and emission matrices from observation-action sequences. Schemas are extracted from learned models and reused by learning new emission matrices for novel observations in test environments. The approach enables rapid transfer by treating learned latent graphs as flexible templates with slots that bind new observations to existing topology, preserving the transition structure while adapting to new perceptual inputs.

## Key Results
- Graph schemas learn navigation environments in far fewer episodes than previous methods
- Schemas enable optimal planning in novel environments through rapid matching and binding
- In highly aliased 2D and 3D environments, schemas can be matched and reused across size and observation variations within 100 steps
- Compositionality allows rapid learning of composed environments by reusing known schemas

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph schemas enable rapid transfer learning by treating learned latent graphs as reusable templates with slot structures that bind new observations to existing topology.
- **Mechanism:** The model first learns a latent graph structure (topology + clone structure) in training environments. When faced with a new environment, it performs fast binding of new observations to the existing clone structure while keeping the transition tensor fixed, enabling quick adaptation without full relearning.
- **Core assumption:** The underlying graph topology (connectivity and transition probabilities) is preserved across environments, and only the observations (emissions) change.
- **Evidence anchors:**
  - [abstract]: "Graph schemas start with latent graph learning where perceptually aliased observations are disambiguated in the latent space using contextual information... Our insight is that a latent graph can be treated as a flexible template -- a schema -- that models concepts and behaviors, with slots that bind groups of latent nodes to the specific observations or groundings."
  - [section 3.2]: "The portion of a CSCG that gets transferred across environments is called a schema, and it consists of a transition tensor together with its unbound emission structure... if we know that a new environment preserves the emission structure... we can further restrict the observation model to respect this constraint during learning"
  - [corpus]: Weak - most neighbor papers focus on discrete abstractions or planning but don't address the specific mechanism of slot-based binding in latent graphs with perceptual aliasing.

### Mechanism 2
- **Claim:** Perceptual aliasing is resolved through higher-order latent graph structures that disambiguate observations using temporal context.
- **Mechanism:** Multiple hidden states share the same emission (clones), but the latent state can only be disambiguated through temporal context. The model learns to track which clone is active based on the sequence of actions and observations, effectively building a cognitive map in the latent space.
- **Core assumption:** The true underlying graph topology is consistent and can be recovered from observation sequences despite identical observations at different nodes.
- **Evidence anchors:**
  - [abstract]: "Graph schemas start with latent graph learning where perceptually aliased observations are disambiguated in the latent space using contextual information."
  - [section 3.1]: "With this, it is possible to formulate a graphical model for the sequence of observations given the actions... Multiple hidden states are forced to share the same emission: if states i and j are clones of the same emission, then p(yn|zn=i) = p(yn|zn=j)."
  - [section 3.1]: "Different latent states can produce similar visible observations, and the latent state can only be disambiguated through temporal context."

### Mechanism 3
- **Claim:** Compositionality of schemas enables rapid learning of novel environments by reusing known graph structures as building blocks.
- **Mechanism:** When learning a new environment that is composed of known schema components, the model initializes a transition tensor with known schemas as block-diagonal components and learns only the connections between these blocks, drastically reducing the learning space.
- **Core assumption:** Novel environments can be decomposed into combinations of previously learned schemas with new inter-schema connections.
- **Evidence anchors:**
  - [abstract]: "We also demonstrate learning, matching, and reusing graph schemas in more challenging 2D and 3D environments with extensive perceptual aliasing and size variations, and show how different schemas can be composed to model larger 2D and 3D environments."
  - [section 3.2]: "Schemas can also be used as building blocks to rapidly learn novel environments that are composed of matching topologies. This comprises learning transitions and emissions but reusing known schemas where they fit."
  - [section 4.4]: "We generated novel environments by composing rooms with known schemas... We constructed exact schemas for each of the six smaller rooms, and used these to construct the initial joint transition tensor with six blocks... connecting schema exit states to entrance states of every other schema."

## Foundational Learning

- **Concept:** Expectation-Maximization (EM) algorithm for learning CSCG parameters
  - Why needed here: The model needs to learn both the transition tensor and emission matrix from observation-action sequences, and EM provides a principled way to handle the latent variables in the cloned HMM structure.
  - Quick check question: How does the EM algorithm exploit the sparsity pattern in the emission matrix to achieve computational savings compared to standard HMM learning?

- **Concept:** Clone structure in latent graphs
  - Why needed here: The clone structure is what enables perceptual aliasing resolution and allows for rapid binding of new observations to existing schemas by preserving the emission structure across environments.
  - Quick check question: Why does forcing multiple hidden states to share the same emission (clones) help with modeling aliased observations?

- **Concept:** Action-conditional transition probabilities
  - Why needed here: The model needs to learn how actions affect state transitions in the latent graph, which is essential for planning and navigation tasks.
  - Quick check question: How does the action-conditional transition tensor T differ from standard HMM transition matrices, and why is this important for navigation tasks?

## Architecture Onboarding

- **Component map:** CSCG model (T, E, Î¼, clone structure) -> Schema learning (EM algorithm) -> Schema matching (NLL computation) -> Planning (Viterbi decoding) -> Continuous observation handling (K-means clustering)

- **Critical path:** Learn T in training environment -> Match schema in test environment -> Bind new observations (learn E) -> Plan using matched schema -> Execute and re-plan as needed

- **Design tradeoffs:**
  - Clone structure vs. full expressiveness: Clones enable aliasing resolution but limit model flexibility
  - Hard vs. soft evidence in EM: Hard evidence is computationally efficient but may miss subtle patterns
  - Pseudocount and smoothing parameters: Need careful tuning for stable learning and generalization
  - Schema composition vs. full learning: Compositionality speeds up learning but requires decomposable environments

- **Failure signatures:**
  - Poor schema matching: High NLL across all schemas, inability to disambiguate test environments
  - Planning failures: Incorrect path predictions, excessive re-planning, failure to reach goals
  - Learning instability: High variance in learned parameters, sensitivity to initialization
  - Generalization issues: Poor performance on size variations or compositionality tasks

- **First 3 experiments:**
  1. **Memory & Planning Game baseline:** Implement CSCG schema learning on the 4x4 grid, measure episodes to learn perfect schema vs. EPN baseline
  2. **Schema matching in rooms:** Learn schemas on medium-sized rooms, test matching on size variations with NLL comparison
  3. **One-shot StreetLearn planning:** Implement explore-exploit strategy, measure steps to goal vs. optimal planning baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can schemas be actively learned from a continuous stream of experiences rather than being predefined?
- Basis in paper: [explicit] The paper mentions that "Learning reusable schemas from a continuous stream of experiences could be an interesting future work" and discusses potential methods like graph clustering.
- Why unresolved: Current methods require predefined schemas or learning them independently, but real-world environments don't provide differentiated experiences belonging to distinct schemas.
- What evidence would resolve it: Demonstration of an algorithm that can discover and maintain schemas from continuous experience streams, showing improved transfer learning compared to predefined schemas.

### Open Question 2
- Question: What is the optimal exploration strategy for schema discovery and binding in novel environments?
- Basis in paper: [explicit] The paper notes that "active exploration to resolve or learn new structures" is a limitation and suggests using schema-based action-conditioned beliefs for optimal disambiguation.
- Why unresolved: Current experiments use random or hardcoded exploration policies, which may be suboptimal for discovering schemas efficiently.
- What evidence would resolve it: Comparison of schema learning efficiency and planning performance between random exploration, optimal exploration, and schema-guided exploration in various environments.

### Open Question 3
- Question: How does schema maintenance and updating affect long-term transfer learning performance?
- Basis in paper: [explicit] The paper discusses that "it is possible to update the schemas with new experiences" and mentions children's schema development as inspiration.
- Why unresolved: Current experiments use fixed schemas, but real-world learning likely involves schema adaptation and flexibility.
- What evidence would resolve it: Longitudinal study showing how schema updating mechanisms improve transfer learning over time compared to fixed schemas, with metrics on adaptation speed and planning accuracy.

## Limitations
- Empirical validation relies heavily on synthetic environments with controlled perceptual aliasing
- Generalization to more complex, real-world environments remains to be fully demonstrated
- Compositionality mechanism validated only on environments composed of discrete rooms rather than truly complex structures

## Confidence

- **High Confidence**: The CSCG learning algorithm and its ability to resolve perceptual aliasing through temporal context
- **Medium Confidence**: Transfer learning claims based on schema matching and binding, particularly in 3D StreetLearn environments
- **Medium Confidence**: Compositionality claims for building larger environments from known schemas

## Next Checks

1. **Real-world generalization test**: Evaluate schema transfer on more complex, real-world navigation tasks beyond the synthetic MPG and curated StreetLearn environments, particularly focusing on environments with dynamic changes or varying graph topologies.

2. **Ablation on clone structure**: Systematically test the necessity and sufficiency of the clone structure by comparing performance with alternative aliasing resolution methods (e.g., attention-based disambiguation) across varying levels of perceptual aliasing.

3. **Compositionality stress test**: Create environments with nested and overlapping schema structures, including cases where schemas share substructures, to test the limits of the compositionality mechanism and identify failure modes.