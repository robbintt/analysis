---
ver: rpa2
title: Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms
arxiv_id: '2308.10542'
source_url: https://arxiv.org/abs/2308.10542
tags:
- convex
- regularizer
- weakly
- denoising
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for learning weakly convex regularizers
  for image reconstruction algorithms with provable convergence guarantees. The authors
  design a regularizer that mimics sparsity-promoting functions while maintaining
  an upper bound on its weak-convexity modulus.
---

# Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms

## Quick Facts
- arXiv ID: 2308.10542
- Source URL: https://arxiv.org/abs/2308.10542
- Authors: 
- Reference count: 40
- Key outcome: This paper proposes a method for learning weakly convex regularizers for image reconstruction algorithms with provable convergence guarantees.

## Executive Summary
This paper introduces a novel approach for learning non-convex regularizers with controlled weak-convexity for image reconstruction tasks. The authors develop a parameterization that ensures the regularizer remains 1-weakly convex while being expressive enough to capture sparsity-promoting behavior. This enables the use of convergent algorithms for inverse problems while maintaining the benefits of non-convex regularization. The method is demonstrated to outperform convex methods and BM3D on natural image denoising, and achieves state-of-the-art results on MRI and CT reconstruction tasks.

## Method Summary
The method involves learning a regularizer R(x) = Σᵢ ψᵢ(hᵢ * x) where ψ are learnable activation functions and h are convolutional filters. The activation functions are parameterized as monotonic splines with bounded second derivatives to ensure weak-convexity. Training is performed on a multi-noise-level denoising task using stochastic optimization. The learned regularizer is then deployed for inverse problems using safeguarded accelerated gradient descent, with convergence guarantees provided by the weak-convexity property.

## Key Results
- Outperforms convex methods and BM3D denoising on BSD68 test set
- Achieves state-of-the-art results on MRI reconstruction compared to other energy-based approaches
- Achieves state-of-the-art results on low-dose CT reconstruction with convergence guarantees
- Learns regularizer profiles that closely resemble the minimax concave penalty function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak convexity ensures convexity of the overall energy while allowing non-convex sparsity-promoting behavior.
- Mechanism: The regularizer R is designed to be 1-weakly convex, meaning R + ½‖·‖² is convex. This guarantees that the denoising problem min ½‖x - y‖² + R(x) is convex and thus globally solvable, while still permitting non-convex sparsity patterns through the weak-convexity modulus.
- Core assumption: The learned regularizer's Hessian eigenvalues lie in [-1, +∞), which is enforced via the parameterization with controlled weak-convexity modulus.
- Evidence anchors:
  - [abstract]: "We propose to learn non-convex regularizers with a prescribed upper bound on their weak-convexity modulus."
  - [section 2]: "To obtain a CNC model in (4), R needs to be 1-weakly convex so that the overall objective remains convex."
  - [corpus]: Weak evidence; related papers discuss proximal operators and non-convex regularizers but don't directly confirm the specific weak-convexity mechanism.
- Break condition: If the weak-convexity modulus exceeds 1, the denoising problem becomes non-convex and global optimality is no longer guaranteed.

### Mechanism 2
- Claim: Parameterizing the regularizer as a sum of convolutional ridges with bounded second derivatives ensures weak convexity.
- Mechanism: The regularizer R(x) = Σᵢ ψᵢ(hᵢ * x) is shown to be ρ-weakly convex when the second derivatives of ψ are bounded below by -ρ. The parameterization allows explicit control of this bound through the activation function design.
- Core assumption: The potential functions ψ can be decomposed into monotonic splines that preserve the weak-convexity bound while allowing expressive non-convex behavior.
- Evidence anchors:
  - [section 3.1]: Proposition 3.1 provides the formal proof that the ridge-based regularizer is ρ-weakly convex when the second derivatives are bounded.
  - [section 3.5]: The learned profiles closely resemble the minimax concave penalty function, which is known to promote sparsity while maintaining weak-convexity.
  - [corpus]: Weak evidence; related works mention ridge-based regularizers but don't provide the specific weak-convexity proof.
- Break condition: If the spline parameterization fails to maintain the required second derivative bounds, the weak-convexity guarantee is lost.

### Mechanism 3
- Claim: The denoising problem's convexity enables efficient global optimization via accelerated gradient descent.
- Mechanism: Since the denoising objective is convex (due to weak-convexity of R), AGD can be applied with guaranteed convergence to the global minimum, unlike typical non-convex optimization problems.
- Core assumption: The proximal operator of the regularizer is well-defined and can be computed efficiently using gradient-based methods on the convex denoising problem.
- Evidence anchors:
  - [section 2]: "The proximal operator is well-defined for any ρ-weakly convex R with ρ < 1" and provides the Lipschitz bound for the denoiser.
  - [section 3.2]: The denoising is performed using AGD with a tolerance of 10⁻⁴ for the relative change between iterates.
  - [corpus]: Moderate evidence; related papers discuss proximal operators and AGD for non-convex problems but don't confirm the specific convergence guarantees.
- Break condition: If the weak-convexity modulus is misestimated or the Lipschitz constant is too large, AGD may not converge or may converge slowly.

## Foundational Learning

- Concept: Weak convexity and its relationship to convexity
  - Why needed here: Understanding weak convexity is crucial because it's the key theoretical property that enables both the convexity of the denoising problem and the expressiveness of the regularizer.
  - Quick check question: What is the difference between ρ-strongly convex and ρ-weakly convex functions, and why is weak convexity sufficient for our application?

- Concept: Proximal operators and their properties
  - Why needed here: The proximal operator is the fundamental building block that connects the regularizer to the denoising operation, and its properties (like Lipschitz continuity) are essential for the overall algorithm design.
  - Quick check question: Why is the proximal operator well-defined for weakly convex regularizers with modulus ρ < 1, and what Lipschitz constant does it have?

- Concept: Accelerated gradient descent for non-convex problems
  - Why needed here: While the denoising problem is convex, the inverse problem (2) is potentially non-convex, requiring specialized optimization techniques that can handle this case.
  - Quick check question: How does AGD adapt to weakly convex objectives, and what are the convergence guarantees in this setting?

## Architecture Onboarding

- Component map: WCRR-NN architecture -> Multi-noise-level training -> Inverse problem deployment
- Critical path: Design → Train → Deploy → Validate
  - Design: Choose parameterization ensuring weak-convexity
  - Train: Optimize on denoising task with multi-noise-level data
  - Deploy: Use AGD with safeguards for inverse problems
  - Validate: Test on BSD68 and MRI/CT reconstruction tasks
- Design tradeoffs:
  - Expressiveness vs. interpretability: Shallow architecture vs. deep CNNs
  - Parameter count: ~15K vs. millions in deep networks
  - Training complexity: Convex denoising vs. non-convex optimization
- Failure signatures:
  - Training divergence: Check weak-convexity modulus and Lipschitz bounds
  - Poor denoising performance: Verify multi-noise-level training and spline parameterization
  - Inverse problem convergence issues: Check initialization and regularization parameter tuning
- First 3 experiments:
  1. Implement and verify the weak-convexity bound for a simple 2-filter ridge regularizer
  2. Train a WCRR-NN on a subset of BSD68 with fixed profiles and learnable filters
  3. Deploy the trained regularizer on a simple CT reconstruction problem and compare with TV regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed weakly convex regularizer be extended to color images without significant performance degradation?
- Basis in paper: [explicit] The paper focuses on grayscale images and mentions that color image denoising would require additional considerations.
- Why unresolved: The paper does not provide any experiments or analysis for color image denoising using the proposed regularizer.
- What evidence would resolve it: Experiments comparing the performance of the regularizer on color images against state-of-the-art color image denoising methods.

### Open Question 2
- Question: How does the choice of the linear spline parameterization (number of knots, knot spacing) affect the performance of the regularizer?
- Basis in paper: [explicit] The paper mentions that the linear splines are parameterized with M+1 equally distant knots and a spacing of ∆, but does not provide a detailed analysis of the impact of these choices.
- Why unresolved: The paper does not explore the sensitivity of the regularizer to the linear spline parameterization.
- What evidence would resolve it: Experiments varying the number of knots and knot spacing to assess their impact on denoising performance and convergence speed.

### Open Question 3
- Question: Can the proposed regularizer be adapted for other types of noise models beyond Gaussian noise?
- Basis in paper: [explicit] The paper focuses on Gaussian denoising and does not discuss extensions to other noise models.
- Why unresolved: The paper does not provide any analysis or experiments for denoising under non-Gaussian noise conditions.
- What evidence would resolve it: Experiments evaluating the performance of the regularizer on images corrupted with Poisson, salt-and-pepper, or multiplicative noise, and comparisons with specialized denoising methods for these noise types.

## Limitations
- The spline parameterization and spectral normalization techniques are described conceptually but lack implementation specifics
- The multi-noise-level training approach may be sensitive to hyperparameter choices not fully explored
- The paper focuses on grayscale images, with color image extensions left as future work

## Confidence
- **High Confidence:** The weak-convexity mechanism and its relationship to the denoising problem's convexity is well-established theoretically. The convergence guarantees for Algorithm 1 are mathematically rigorous.
- **Medium Confidence:** The practical effectiveness of the learned regularizers on real image datasets, while demonstrated, depends on implementation details that could affect reproducibility. The comparison with state-of-the-art methods appears comprehensive but may be sensitive to hyperparameter tuning.
- **Low Confidence:** The exact implementation of the spline toolbox and spectral normalization, which are critical for maintaining weak-convexity during training, are not fully specified.

## Next Checks
1. **Theoretical Verification:** Implement a simplified 2-filter ridge regularizer and verify the weak-convexity bound through numerical computation of the Hessian eigenvalues across the input space.
2. **Implementation Validation:** Reproduce the denoising results on a small subset of BSD68 using a fixed WCRR-NN architecture with predetermined profiles, focusing on verifying the training pipeline and convergence behavior.
3. **Application Testing:** Deploy the trained regularizer on a simple 2D tomographic reconstruction problem with known ground truth, comparing convergence speed and reconstruction quality against TV regularization under identical optimization settings.