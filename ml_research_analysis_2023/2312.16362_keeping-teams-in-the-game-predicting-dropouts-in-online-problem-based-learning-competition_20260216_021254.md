---
ver: rpa2
title: 'Keeping Teams in the Game: Predicting Dropouts in Online Problem-Based Learning
  Competition'
arxiv_id: '2312.16362'
source_url: https://arxiv.org/abs/2312.16362
tags:
- online
- learning
- data
- dropout
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting student dropout
  in online problem-based learning (PBL) environments. It proposes a methodological
  triangulation approach combining Discourse forum activity data, the Online Strategies
  for Learning Questionnaire (OSLQ) for self-regulated learning, and qualitative interviews.
---

# Keeping Teams in the Game: Predicting Dropouts in Online Problem-Based Learning Competition

## Quick Facts
- arXiv ID: 2312.16362
- Source URL: https://arxiv.org/abs/2312.16362
- Reference count: 1
- Key outcome: Random Forest achieved AUC-ROC scores up to 0.90 for predicting task completion and stage advancement in an online robotics competition

## Executive Summary
This study addresses the challenge of predicting student dropout in online problem-based learning environments by combining Discourse forum activity data, the Online Learning Strategies Questionnaire (OSLQ), and qualitative interviews. The researchers developed machine learning models using behavioral indicators from forum interactions and psychological measures of self-regulated learning to predict whether teams would complete tasks or advance stages in an online robotics competition. The approach achieved strong predictive performance (AUC-ROC up to 0.90, F1-scores around 0.93) while validating the OSLQ instrument in a large, team-based online setting. The methodological triangulation approach provides both statistical prediction capabilities and contextual understanding of dropout behavior.

## Method Summary
The study employed methodological triangulation combining Discourse forum activity features (topics entered, posts read, likes given/received), OSLQ self-regulated learning questionnaire responses, and qualitative interview insights. Machine learning models including Logistic Regression, Decision Trees, and Random Forest were trained to predict task completion and stage advancement in the e-Yantra Robotics Competition. The dataset was balanced using SMOTE to address class imbalance, and OSLQ subscales were validated using Confirmatory Factor Analysis and Cronbach's alpha for internal consistency. The models were evaluated using standard metrics including accuracy, precision, recall, F1-score, and AUC-ROC.

## Key Results
- Random Forest achieved AUC-ROC scores up to 0.90 for predicting task completion and stage advancement
- OSLQ subscales demonstrated good internal consistency with Cronbach's alpha ranging from 0.729 to 0.864
- Time Management subscale showed the highest factor loading (0.877) in CFA validation
- The combined approach of forum activity and SRL questionnaire data outperformed models using either data source alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of Discourse forum activity data and OSLQ SRL questionnaire improves dropout prediction accuracy by capturing both behavioral and psychological indicators of student engagement.
- Mechanism: Discourse activity features serve as behavioral proxies for engagement while OSLQ subscales measure self-regulated learning capacity, providing complementary signals that machine learning models can use to distinguish between likely dropouts and persisters.
- Core assumption: Students who exhibit both high SRL scores and active forum participation are less likely to drop out, and these two data streams capture independent but complementary aspects of student behavior.
- Evidence anchors: [abstract] "proposes a methodological triangulation approach combining Discourse forum activity data, the Online Strategies for Learning Questionnaire (OSLQ) for self-regulated learning, and qualitative interviews"; [section] "Through methodological triangulation, the study aims to predict dropout behavior via the contributions of Discourse discussion forum 'activities' of participating teams, along with a self-reported Online Learning Strategies Questionnaire (OSLQ)"

### Mechanism 2
- Claim: Random Forest outperforms simpler models because it captures non-linear relationships between SRL questionnaire scores, forum activity, and dropout outcomes.
- Mechanism: Random Forest ensembles multiple decision trees trained on different subsets of data, allowing it to model complex interactions between features that linear models cannot capture.
- Core assumption: The relationships between SRL factors, forum behavior, and dropout risk are non-linear and involve interactions between variables rather than simple additive effects.
- Evidence anchors: [abstract] "Machine learning models (Logistic Regression, Decision Trees, Random Forest) were trained to predict task completion and stage advancement in an online robotics competition, achieving AUC-ROC scores up to 0.90 (Random Forest)"; [section] "Random Forest and Decision Trees performed better than the Logistic Regression models. Decision Trees and RandomForest seemingly worked well as they incorporated non-linear relationships between the variables"

### Mechanism 3
- Claim: Methodological triangulation enhances prediction reliability by validating model assumptions and uncovering context-specific dropout factors.
- Mechanism: Quantitative models identify statistical patterns in large datasets while qualitative interviews provide contextual understanding of why those patterns exist, allowing for model refinement and interpretation that pure ML approaches miss.
- Core assumption: Dropout behavior has both measurable statistical patterns and underlying contextual factors that require human interpretation to fully understand and predict.
- Evidence anchors: [abstract] "The study also uses Qualitative interviews to enhance the ground truth and results"; [section] "Through methodological triangulation, the study aims to predict dropout behavior via the contributions of Discourse discussion forum 'activities' of participating teams, along with a self-reported Online Learning Strategies Questionnaire (OSLQ). The study also uses Qualitative interviews to enhance the ground truth and results"

## Foundational Learning

- Concept: Confirmatory Factor Analysis (CFA)
  - Why needed here: CFA was used to validate the OSLQ questionnaire structure and ensure the six subscales are measuring distinct but related constructs of self-regulated learning.
  - Quick check question: What is the difference between CFA and Exploratory Factor Analysis (EFA), and why was CFA chosen for this study instead of EFA?

- Concept: SMOTE (Synthetic Minority Over-sampling Technique)
  - Why needed here: SMOTE was applied to balance the imbalanced dataset where most teams completed tasks while fewer dropped out, which is crucial for training effective classification models.
  - Quick check question: How does SMOTE work to balance class distributions, and what are the potential risks of using it in dropout prediction?

- Concept: AUC-ROC (Area Under the Receiver Operating Characteristic Curve)
  - Why needed here: AUC-ROC was used as a performance metric for the classification models, providing a threshold-independent measure of model discrimination ability between dropout and completion classes.
  - Quick check question: Why is AUC-ROC preferred over simple accuracy for imbalanced datasets in dropout prediction, and what does an AUC-ROC of 0.90 indicate about model performance?

## Architecture Onboarding

- Component map: e-Yantra Robotics Competition platform -> Discourse forum -> OSLQ questionnaire -> Feature engineering -> Machine learning models (Random Forest, Decision Tree, Logistic Regression) -> Validation metrics (AUC-ROC, F1-score) -> Qualitative interviews
- Critical path: Data collection → Feature engineering → Model training with SMOTE balancing → Performance evaluation → Qualitative validation
- Design tradeoffs: 
  - Simplicity vs. performance: Simpler models (Logistic Regression) are more interpretable but less accurate
  - Data richness vs. practicality: More features could improve prediction but increase complexity and data collection burden
  - Early prediction vs. accuracy: Using only pre-deadline data limits predictive power but enables timely interventions
- Failure signatures:
  - Low AUC-ROC scores (<0.7) indicating poor discrimination between dropouts and completers
  - High variance in model performance across different tasks suggesting instability
  - Qualitative interviews revealing dropout factors not captured in quantitative features
  - SMOTE introducing artifacts that degrade model generalization
- First 3 experiments:
  1. Train baseline Logistic Regression model with only OSLQ features to establish minimum performance threshold
  2. Train Random Forest with both OSLQ and Discourse features to evaluate combined feature set performance
  3. Perform ablation study removing each feature type to identify most predictive features

## Open Questions the Paper Calls Out
- What are the specific thresholds or "early indicators" within forum activity metrics that reliably signal impending dropout?
- How do individual participant demographics (Year of Study, Engineering Branch, Gender) interact with team-level dropout prediction models?
- What specific interventions based on OSLQ subscale scores would most effectively reduce dropout in online PBL environments?

## Limitations
- The study relies on self-reported OSLQ data subject to response bias and social desirability effects
- Findings are limited to team-based online PBL competitions similar to eYRC with specific task structures
- Methodological triangulation is theoretically sound but has limited empirical validation within this study
- Performance metrics are based on relatively small sample sizes per task (9-20 teams), raising concerns about model stability

## Confidence
- **High Confidence**: The basic methodology of combining Discourse activity features with OSLQ questionnaire data for dropout prediction is well-established in educational data mining literature
- **Medium Confidence**: The specific claim that Random Forest outperforms simpler models due to capturing non-linear relationships is supported by reported performance metrics but lacks direct comparative analysis
- **Medium Confidence**: The assertion that methodological triangulation enhances prediction reliability is theoretically justified but the paper provides limited evidence of how qualitative insights specifically improved or validated the quantitative models

## Next Checks
1. Conduct cross-validation across multiple eYRC seasons or similar competitions to assess model stability and generalizability
2. Perform ablation studies systematically removing each feature type (forum activity vs. OSLQ subscales) to quantify their independent and combined predictive contributions
3. Implement qualitative validation by coding interview transcripts for themes that either support or contradict the quantitative model predictions, then assess alignment rates between qualitative insights and model outputs