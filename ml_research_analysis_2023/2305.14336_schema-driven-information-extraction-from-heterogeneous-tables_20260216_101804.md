---
ver: rpa2
title: Schema-Driven Information Extraction from Heterogeneous Tables
arxiv_id: '2305.14336'
source_url: https://arxiv.org/abs/2305.14336
tags:
- table
- data
- extraction
- tables
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting structured data
  from complex tables using large language models. It introduces schema-driven information
  extraction, a task that transforms tabular data into structured records following
  a human-authored schema.
---

# Schema-Driven Information Extraction from Heterogeneous Tables

## Quick Facts
- arXiv ID: 2305.14336
- Source URL: https://arxiv.org/abs/2305.14336
- Reference count: 26
- Key outcome: Schema-driven extraction achieves F1 scores of 74.2-96.1 without task-specific labels

## Executive Summary
This paper introduces schema-driven information extraction, a method for transforming tabular data into structured records using only a human-authored schema. The authors propose INSTRUC TE, an approach that uses instruction-tuned language models with an error-recovery strategy to handle traversal order issues. They evaluate their method on a new benchmark spanning machine learning papers, chemistry literature, and webpages, demonstrating that compact models can be effectively distilled from larger models while maintaining performance.

## Method Summary
The method transforms tabular data into structured JSON records using instruction-tuned LLMs with a schema-driven approach. The process involves providing the LLM with input tables, extraction schemas, task-specific instructions, and initial record templates. An error-recovery strategy monitors the traversal order and corrects deviations from the instructed sequence. The approach also includes knowledge distillation, where larger models generate synthetic data that smaller models are fine-tuned on to reduce API reliance while maintaining performance.

## Key Results
- INSTRUC TE achieves F1 scores ranging from 74.2 to 96.1 across three domains without task-specific training data
- Error-recovery strategy effectively handles LLM traversal disorder, improving extraction accuracy
- Distilled models maintain comparable performance to larger models while significantly reducing computational requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The error-recovery strategy addresses LLM traversal disorder by detecting and correcting deviations from the instructed cell order.
- Mechanism: INSTRUC TE implements a canonical order (e.g., left to right, top to bottom) and monitors the predicted sequence of JSON objects. When deviation is detected, it truncates the output and re-prompts with the truncated sequence plus the next cell in the canonical order.
- Core assumption: LLMs tend to traverse table cells in a disorganized manner, leading to incomplete extraction and difficulties in associating generated records with specific cells.

### Mechanism 2
- Claim: Schema-driven formulation enables zero-shot table extraction by providing structured extraction schemas instead of task-specific labels.
- Mechanism: The extraction schema outlines target attributes and their data types for different types of table cells. The LLM is prompted to fill in JSON templates based on this schema, extracting relevant information from the table.
- Core assumption: Providing a human-constructed extraction schema is sufficient for the LLM to understand what information to extract and how to structure it, without requiring task-specific training data.

### Mechanism 3
- Claim: Knowledge distillation from larger models to smaller ones maintains performance while reducing API reliance and costs.
- Mechanism: Synthetic data is generated by performing zero-shot inference on unlabeled tables using a larger model (e.g., code-davinci-002). A smaller model (e.g., T5-11B) is then fine-tuned on this synthetic data to create a compact table extraction model.
- Core assumption: The predictions from the larger model can serve as high-quality training data for the smaller model, preserving the extraction capabilities while significantly reducing model size and computational requirements.

## Foundational Learning

- Concept: JSON template-filling for structured data extraction
  - Why needed here: The SCHEMA-TO-JSON task requires transforming unstructured table data into structured JSON records following a human-authored schema.
  - Quick check question: How does the JSON template-filling approach differ from traditional information extraction methods?

- Concept: Error recovery and iterative prompting
  - Why needed here: LLMs tend to traverse table cells in a disorganized manner, leading to incomplete extraction and difficulties in associating generated records with specific cells.
  - Quick check question: What are the key components of the error-recovery strategy implemented in INSTRUC TE?

- Concept: Knowledge distillation for model compression
  - Why needed here: To reduce API reliance and costs while maintaining performance, compact models are distilled from larger models using synthetic data.
  - Quick check question: How does the knowledge distillation process work in the context of table extraction?

## Architecture Onboarding

- Component map: Input table -> Extraction schema -> Task-specific instruction -> LLM prompt -> JSON record generation -> Error recovery monitoring -> Iterative correction

- Critical path:
  1. Input table and extraction schema are provided
  2. LLM generates initial record based on the prompt
  3. Error-recovery strategy monitors and corrects the traversal order
  4. Process continues until all cells are covered
  5. For knowledge distillation, synthetic data is generated and used to fine-tune a smaller model

- Design tradeoffs:
  - Using larger context lengths (8K) vs. smaller ones (2048) affects input encoding and error-recovery strategies
  - Zero-shot vs. fine-tuned approaches balance performance and data requirements
  - API-based models (e.g., code-davinci-002) vs. open-source models (e.g., StarCoder) affect cost and accessibility

- Failure signatures:
  - Incomplete extraction due to disorganized cell traversal
  - Incorrect attribute values or types in the generated JSON records
  - Error-recovery process becoming computationally expensive
  - Distilled models failing to maintain performance compared to the larger model

- First 3 experiments:
  1. Test INSTRUC TE with a simple table and a basic extraction schema to verify the core functionality
  2. Evaluate the error-recovery strategy by providing a table with cells in a non-canonical order
  3. Compare the performance of the distilled model with the larger model on a subset of the benchmark data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of schema-driven extraction vary across different table formats (e.g., LaTeX, XML, HTML) when using instruction-tuned language models?
- Basis in paper: [explicit] The paper mentions that each domain adheres to a unique textual format, namely, LaTeX, XML, and HTML, thereby challenging each model's ability to generalize across domains and adapt to different formats.
- Why unresolved: While the paper shows that code-davinci-002 achieves strong performance across these formats, it doesn't provide a detailed comparison of effectiveness across each format.
- What evidence would resolve it: A comprehensive analysis comparing the F1 scores of schema-driven extraction across different table formats, potentially highlighting which formats pose more challenges and why.

### Open Question 2
- Question: Can the error-recovery strategy used in INSTRUC TE be generalized to other structured data input scenarios beyond table extraction?
- Basis in paper: [explicit] The paper states that the error-recovery strategy might have broader applications in other structured data input scenarios, such as semantic parsing.
- Why unresolved: The paper introduces the error-recovery strategy for table extraction but does not explore its applicability to other domains.
- What evidence would resolve it: Experiments applying the error-recovery strategy to other structured data tasks, such as semantic parsing or data transformation, with a comparison of performance improvements.

### Open Question 3
- Question: How does the inclusion of retrieved paragraphs impact the performance of schema-driven extraction, and what is the optimal amount of supplementary text?
- Basis in paper: [explicit] The ablation study shows that decreasing the number of retrieved paragraphs negatively affects performance, highlighting the importance of additional textual information.
- Why unresolved: The paper demonstrates the importance of retrieved paragraphs but does not specify the optimal amount or explore the diminishing returns of excessive text.
- What evidence would resolve it: An analysis determining the optimal number of retrieved paragraphs for maximizing performance without overwhelming the model, potentially using a curve of performance vs. text volume.

## Limitations
- Prompt Engineering Dependence: The method's success heavily relies on the quality of the schema and prompt formulation, which are not fully specified in the paper.
- Error Recovery Effectiveness: The error recovery strategy's performance is demonstrated but lacks detailed analysis of its failure cases and frequency of occurrence.
- Distillation Quality Assurance: The knowledge distillation process shows promising results, but the evaluation doesn't deeply examine potential degradation in handling edge cases or complex table structures.

## Confidence
- High Confidence: The core premise that schema-driven extraction can work without task-specific training data is well-supported by the results across multiple domains.
- Medium Confidence: The error recovery mechanism's effectiveness is demonstrated but not exhaustively analyzed.
- Medium Confidence: The distillation results show comparable performance between teacher and student models, but the evaluation is limited to a subset of models and datasets.

## Next Checks
1. **Prompt Generalization Test**: Apply the same prompt template to a new table domain (e.g., financial reports) to assess how well the schema-driven approach generalizes without prompt engineering for the specific domain.

2. **Error Recovery Stress Test**: Systematically evaluate the error recovery mechanism by creating test cases with varying levels of traversal disorder to quantify its effectiveness and computational overhead.

3. **Distillation Robustness Analysis**: Test the distilled models on adversarial examples and edge cases to identify potential performance gaps compared to the larger models that might not be apparent in standard evaluation metrics.