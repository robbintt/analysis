---
ver: rpa2
title: 'Safurai 001: New Qualitative Approach for Code LLM Evaluation'
arxiv_id: '2309.11385'
source_url: https://arxiv.org/abs/2309.11385
tags:
- answer
- code
- list
- numbers
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Safurai-001, a large language model for code
  assistance, and a new evaluation framework using GPT-4 for qualitative assessment.
  Safurai-001 is trained on a curated dataset enhanced with advanced data transformation
  techniques such as Chain of Thoughts, Tree of Thoughts, and instructional teaching
  methods.
---

# Safurai 001: New Qualitative Approach for Code LLM Evaluation

## Quick Facts
- **arXiv ID**: 2309.11385
- **Source URL**: https://arxiv.org/abs/2309.11385
- **Reference count**: 2
- **Primary result**: Safurai-001 outperforms GPT-3.5 by 1.58% and WizardCoder by 18.78% in code readability using a novel GPT-4-based multi-parameter evaluation framework

## Executive Summary
This paper introduces Safurai-001, a large language model for code assistance, and a new evaluation framework using GPT-4 for qualitative assessment. Safurai-001 is trained on a curated dataset enhanced with advanced data transformation techniques such as Chain of Thoughts, Tree of Thoughts, and instructional teaching methods. The model is evaluated using both traditional quantitative benchmarks like HumanEval and a new qualitative GPT-4-based multi-parameter system assessing code correctness, efficiency, readability, and relevance. Results show Safurai-001 outperforming GPT-3.5 by 1.58% and WizardCoder by 18.78% in code readability, with strong overall performance in multi-turn conversational coding tasks. The study highlights the importance of high-quality datasets and innovative evaluation methods for advancing coding LLM capabilities.

## Method Summary
Safurai-001 is developed by fine-tuning StarCoder 15B on a proprietary dataset of 200,000 Q&A examples, which includes coding, logic, and math problems. The training data is enhanced using data transformation techniques such as Chain of Thoughts, Tree of Thoughts, and instructional teaching methods. The model is fine-tuned for 10 hours using DeepSpeed ZeRO-3 with a batch size of 512, learning rate of 2e-5, and three epochs. Evaluation is performed using both traditional benchmarks like HumanEval and a new GPT-4-based multi-parameter framework that assesses code correctness, efficiency, readability, and relevance.

## Key Results
- Safurai-001 outperforms GPT-3.5 by 1.58% and WizardCoder by 18.78% in code readability using the GPT-4-based multi-parameter evaluation
- The model achieves strong performance in multi-turn conversational coding tasks
- Traditional benchmarks like HumanEval are complemented by the new qualitative evaluation framework to provide a more comprehensive assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safurai-001 achieves improved code readability by using advanced data transformation techniques (Chain of Thoughts, Tree of Thoughts, Teacher CoT) to create more educational and structured training data.
- Mechanism: The model is fine-tuned on a dataset that includes step-by-step reasoning explanations, edge case handling, and teaching-oriented responses, which enhances its ability to generate readable and well-explained code.
- Core assumption: Including explicit reasoning and educational elements in the training data improves the model's ability to generate code that is both correct and easy to understand.
- Evidence anchors:
  - [abstract]: "Safurai-001 is trained on a curated dataset enhanced with advanced data transformation techniques such as Chain of Thoughts, Tree of Thoughts, and instructional teaching methods."
  - [section 3.3]: Describes data transformation techniques like "Chain of thoughts reasoning", "Tree of thoughts reasoning", "Teaching the response", etc.
- Break condition: If the model's code readability gains are not sustained when evaluated on new, unseen problems, or if the explanations generated are not pedagogically effective.

### Mechanism 2
- Claim: The GPT-4 based MultiParameters Evaluation method provides a more nuanced assessment of coding LLM performance than traditional benchmarks like HumanEval.
- Mechanism: By evaluating code on multiple dimensions (correctness, efficiency, readability, relevance), this method uncovers strengths and weaknesses not captured by binary pass/fail metrics.
- Core assumption: Multi-dimensional evaluation better reflects real-world utility and identifies actionable areas for improvement.
- Evidence anchors:
  - [abstract]: "Results show Safurai-001 outperforming GPT-3.5 by 1.58% and WizardCoder by 18.78% in code readability, with strong overall performance in multi-turn conversational coding tasks."
  - [section 3.7.2]: Details the four-parameter rating system: Code Correctness and Completeness, Efficiency, Readability and Best Practices, Relevance to Problem.
- Break condition: If the qualitative ratings are inconsistent or do not correlate with practical coding performance.

### Mechanism 3
- Claim: Fine-tuning on a combination of coding and logic/math datasets improves the model's general reasoning and problem-solving capabilities.
- Mechanism: Incorporating datasets like "Logic Q&A Dataset" and "Math Q&A Dataset" alongside code data enhances the model's ability to reason through complex problems and handle edge cases.
- Core assumption: Cross-domain training (code + logic/math) broadens the model's reasoning abilities beyond pure code generation.
- Evidence anchors:
  - [section 3.1]: Mentions the use of "Logic Q&A Dataset (22k)" and "Math Q&A Dataset (15k)" in addition to code datasets.
  - [section 3.3]: Describes data transformation experiments aimed at teaching reasoning and handling edge cases.
- Break condition: If the model does not show improved reasoning or edge case handling compared to models trained only on code data.

## Foundational Learning

- Concept: Data transformation and augmentation techniques (Chain of Thoughts, Tree of Thoughts, Teacher CoT)
  - Why needed here: These techniques create richer, more educational training examples that improve the model's ability to explain and generate high-quality code.
  - Quick check question: What is the difference between Chain of Thoughts and Tree of Thoughts in prompt engineering?

- Concept: Multi-parameter evaluation vs. traditional benchmarks
  - Why needed here: Multi-parameter evaluation provides deeper insights into model performance, identifying specific strengths and weaknesses that single metrics like pass@1 miss.
  - Quick check question: Why might a model score high on HumanEval but still be hard to use in practice?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: Fine-tuning leverages a strong foundation model (StarCoder 15B) and adapts it for coding tasks, which is more efficient and effective than training from scratch.
  - Quick check question: What are the advantages of using a pre-trained model like StarCoder as a base for Safurai-001?

## Architecture Onboarding

- Component map:
  - Base model: StarCoder 15B
  - Fine-tuning datasets: Safurai Code Instructor (16k), Logic Q&A Dataset (22k), Teacher Code Instructor (70k), Math Q&A Dataset (15k), Teacher Code Instructor with Potential Errors (21k), ToT Code Instructor (30k), CoT Code Instructor (26k)
  - Training framework: DeepSpeed ZeRO-3
  - Evaluation method: GPT-4 based MultiParameters Evaluation

- Critical path:
  1. Collect and curate diverse datasets (code, logic, math)
  2. Apply data transformation techniques to enhance educational value
  3. Fine-tune StarCoder 15B using DeepSpeed ZeRO-3
  4. Evaluate using GPT-4 based MultiParameters Evaluation
  5. Iterate based on evaluation feedback

- Design tradeoffs:
  - Using a large base model (StarCoder 15B) provides strong initial capabilities but increases computational cost
  - Multi-parameter evaluation is more informative but also more time-consuming than traditional benchmarks
  - Combining code, logic, and math datasets broadens capabilities but may dilute focus on pure code generation

- Failure signatures:
  - If code readability improvements are not reflected in user studies or practical coding tasks
  - If the model's conversational abilities do not match its evaluation scores
  - If fine-tuning does not improve performance on new, unseen problems

- First 3 experiments:
  1. Compare Safurai-001's performance on HumanEval before and after fine-tuning
  2. Evaluate the impact of different data transformation techniques on code readability
  3. Test the model's ability to handle edge cases and explain its reasoning on logic/math problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Safurai-001's conversational ability compare to other coding LLMs in real-world developer scenarios beyond benchmark evaluations?
- Basis in paper: [inferred] The paper mentions Safurai-001 aims for a more conversational approach and outperforms WizardCoder in code readability, but doesn't provide real-world usage comparisons.
- Why unresolved: The paper focuses on benchmark evaluations and doesn't report on actual developer experience or usage studies in real-world coding scenarios.
- What evidence would resolve it: User studies comparing developer productivity and satisfaction when using Safurai-001 versus other coding LLMs in real coding tasks, including metrics like task completion time, code quality, and developer preference ratings.

### Open Question 2
- Question: What is the performance ceiling for GPT-4 based MultiParameters Evaluation Benchmark when evaluating models that surpass GPT-4's capabilities?
- Basis in paper: [explicit] The paper explicitly states "The GPT4-based MultiParameters Evaluation method can only evaluate up to a certain limit: It is benchmarked at the GPT4 level of performance."
- Why unresolved: No current model surpasses GPT-4, but future models may, creating uncertainty about the benchmark's applicability.
- What evidence would resolve it: Testing the benchmark with hypothetical or simulated superior models, or developing a new evaluation framework that can assess models beyond GPT-4's capabilities.

### Open Question 3
- Question: How consistent are the qualitative ratings provided by GPT-4 across different evaluations of the same code, and how does this variance affect the reliability of the GPT4-based MultiParameters Evaluation?
- Basis in paper: [explicit] The paper notes "Another constraint associated with the GPT4-based MultiParameters Evaluation method is the variation in GPT4's response. It does not always provide consistent responses."
- Why unresolved: The paper acknowledges variance but doesn't quantify it or provide strategies to mitigate its impact on evaluation reliability.
- What evidence would resolve it: Statistical analysis of rating consistency across multiple evaluations of the same code samples, and development of methods to reduce variance (e.g., majority voting, confidence thresholds).

## Limitations

- The evaluation methodology relies heavily on GPT-4-based assessments, which may introduce subjective bias and lack reproducibility
- The claimed improvements in code readability are based on qualitative ratings rather than objective, reproducible metrics
- The study lacks transparency in crucial areas such as the specific prompts used for data transformation and the exact composition of proprietary datasets

## Confidence

- **High Confidence**: The technical approach of fine-tuning StarCoder 15B and using DeepSpeed ZeRO-3 for training is well-established and documented. The general methodology of data transformation using Chain of Thoughts and Tree of Thoughts is supported by existing literature.
- **Medium Confidence**: The reported improvements in code readability and overall performance are based on the new evaluation framework, which, while systematic, introduces potential biases through its reliance on GPT-4 ratings. The comparison with other models is limited to a small set of benchmarks.
- **Low Confidence**: The practical significance of the improvements in real-world coding scenarios is not fully established. The study lacks user studies or external validation of the model's conversational coding abilities beyond the evaluation framework.

## Next Checks

1. Conduct blind evaluations of Safurai-001's code by independent developers to verify the model's readability and correctness claims beyond the GPT-4-based assessment
2. Test the model's performance on a diverse set of coding problems over multiple sessions to ensure that the improvements in conversational coding are consistent and not limited to specific problem types
3. Evaluate Safurai-001 against more recent, larger models (e.g., GPT-4, Claude 3) on both HumanEval and the proposed multi-parameter framework to contextualize its performance gains