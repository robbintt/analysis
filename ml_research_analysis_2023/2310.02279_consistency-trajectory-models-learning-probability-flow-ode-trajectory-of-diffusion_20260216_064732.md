---
ver: rpa2
title: 'Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of
  Diffusion'
arxiv_id: '2310.02279'
source_url: https://arxiv.org/abs/2310.02279
tags:
- sampling
- training
- diffusion
- time
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Consistency Trajectory Models (CTMs) address the trade-off between
  speed and sample quality in diffusion model sampling. CTM unifies score-based and
  distillation models by learning both the probability flow ODE trajectory and its
  score function, enabling flexible traversal between any initial and final time in
  a single forward pass.
---

# Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion

## Quick Facts
- arXiv ID: 2310.02279
- Source URL: https://arxiv.org/abs/2310.02279
- Reference count: 40
- Key outcome: Achieves state-of-the-art FID scores of 1.73 on CIFAR-10 and 1.92 on ImageNet 64×64 for single-step diffusion model sampling

## Executive Summary
Consistency Trajectory Models (CTMs) address the speed-quality trade-off in diffusion model sampling by learning both the probability flow ODE trajectory and its score function in a single neural network. This unification enables flexible traversal between any initial and final time with a single forward pass, achieving state-of-the-art single-step sampling performance. The model introduces γ-sampling, a new family of sampling schemes that enables unrestricted traversal along ODE solution trajectories without quality degradation, and enables exact likelihood computation unlike previous distillation approaches.

## Method Summary
CTM uses a neural network gθ that parametrizes both the integral (via Gθ) and the integrand (via gθ) of the probability flow ODE, allowing it to serve as both a score estimator and a trajectory predictor. The model combines reconstruction loss, denoising score matching loss, and adversarial loss for training. For sampling, CTM employs γ-sampling with γ=0 for deterministic results, using N=18 timesteps for CIFAR-10 and N=40 for ImageNet. The architecture builds on EDM-style skip connections and output scaling to ensure stable training.

## Key Results
- Achieves FID scores of 1.73 on CIFAR-10 and 1.92 on ImageNet 64×64 for single-step sampling
- Enables exact likelihood computation, unlike previous distillation models
- Introduces γ-sampling that allows unrestricted traversal along ODE solution trajectories without quality degradation

## Why This Works (Mechanism)

### Mechanism 1
CTM unifies score-based and distillation models by learning both the probability flow ODE trajectory and its score function, enabling flexible traversal between any initial and final time in a single forward pass. The model uses a dual-output neural network that parametrizes both the integral (via Gθ) and the integrand (via gθ) of the probability flow ODE, allowing it to serve as both a score estimator and a trajectory predictor.

### Mechanism 2
CTM's access to the score function enables exact likelihood computation, unlike previous distillation models. By learning gθ(xt, t, t) which converges to E[x|xt] as s approaches t, CTM can approximate the score function directly, which is necessary for exact likelihood computation.

### Mechanism 3
CTM introduces γ-sampling, a new family of sampling schemes that enables unrestricted traversal along ODE solution trajectories, avoiding quality degradation seen in previous distillation models. γ-sampling alternates between denoising and noising steps along the solution trajectory, with γ controlling the level of stochasticity.

## Foundational Learning

- **Probability Flow ODE**: The deterministic reverse process of diffusion described by an ordinary differential equation. Why needed: CTM is built on this framework to model the trajectory of diffusion processes.
- **Denoising Score Matching**: A method to learn the score function by minimizing the difference between estimated and true scores of noisy data. Why needed: CTM uses this to learn the score function component of its dual-output network.
- **Adversarial Training**: Training with a discriminator to improve sample quality by matching the data distribution. Why needed: CTM combines this with denoising score matching to enhance trajectory estimation and beat the teacher model.

## Architecture Onboarding

- **Component map**: Input x0 -> Noise addition -> gθ(xt, t, s) -> Dual outputs (Gθ, gθ) -> ODE solver -> Output x0
- **Critical path**: 1) Sample x0 from data distribution 2) Add noise to get xt 3) Sample s ∈ [0, t] and u ∈ [s, t) 4) Calculate Gθ(xt, t, s) and Gsg(θ)(Solver(xt, t, u; ϕ), u, s) 5) Calculate loss and update θ and η
- **Design tradeoffs**: Using self-ODE solver vs pre-trained teacher model for target calculation; choice of γ value in γ-sampling (deterministic vs stochastic); balance between reconstruction loss, denoising score matching loss, and adversarial loss
- **Failure signatures**: Poor sample quality with small NFE suggests issues with score estimation; degrading sample quality with increasing NFE suggests error accumulation in γ-sampling; instability during training suggests issues with loss balance or network capacity
- **First 3 experiments**: 1) Train CTM on CIFAR-10 with NFE=1 and compare FID to EDM and CM 2) Evaluate CTM's likelihood computation on CIFAR-10 3) Test γ-sampling with different γ values on CIFAR-10 and observe quality vs NFE tradeoff

## Open Questions the Paper Calls Out

1. **Generalizability to other modalities**: Can CTM be effectively extended to other data modalities beyond images, such as text, audio, or graphs, and what architectural modifications would be necessary to handle the unique characteristics of these domains?

2. **Optimal loss balancing**: How does the incorporation of adversarial training in CTM impact the stability and convergence of the model, and what are the optimal strategies for balancing the reconstruction, denoising, and adversarial losses?

3. **Theoretical understanding of γ-sampling**: What is the precise mathematical relationship between γ-sampling and other diffusion-based sampling methods, and how does the choice of γ affect the trade-off between sampling speed and quality in high-dimensional spaces?

## Limitations

- The paper does not provide rigorous mathematical proofs of convergence or error bounds for the dual-output approximation
- γ-sampling effectiveness depends critically on the smoothness of the ODE solution trajectory, which may not hold for all datasets
- Evaluation of likelihood computation was only performed on CIFAR-10, with no comparison to other likelihood estimation methods for diffusion models

## Confidence

- **High confidence** in empirical results showing state-of-the-art FID scores for single-step sampling on CIFAR-10 and ImageNet 64×64
- **Medium confidence** in theoretical claims about unifying score-based and distillation models
- **Low confidence** in the generality of γ-sampling across all diffusion model applications

## Next Checks

1. **Validate likelihood computation accuracy**: Implement exact likelihood computation using CTM on CIFAR-10 and compare with established diffusion model likelihood estimation methods to verify the claimed advantage.

2. **Test γ-sampling robustness**: Evaluate γ-sampling on datasets with more complex structures (e.g., LSUN bedroom or FFHQ) and systematically vary γ values to identify breaking points where quality degrades significantly.

3. **Analyze dual-output network capacity**: Conduct ablation studies varying the network depth and width to determine the minimum capacity required for the dual-output approximation to work effectively, and test whether the unification claim holds across different model scales.