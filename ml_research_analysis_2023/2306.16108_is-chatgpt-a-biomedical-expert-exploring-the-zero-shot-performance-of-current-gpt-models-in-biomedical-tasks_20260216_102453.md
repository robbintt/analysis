---
ver: rpa2
title: Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current
  GPT Models in Biomedical Tasks
arxiv_id: '2306.16108'
source_url: https://arxiv.org/abs/2306.16108
tags:
- query
- question
- systems
- these
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-3.5-Turbo and GPT-4 on BioASQ biomedical
  tasks, finding strong performance in Phase B question answering using zero-shot
  learning. GPT-4 and GPT-3.5-Turbo achieved competitive results with leading systems
  in factoid and list answer formats, even outperforming them in some cases.
---

# Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks

## Quick Facts
- arXiv ID: 2306.16108
- Source URL: https://arxiv.org/abs/2306.16108
- Reference count: 20
- GPT-3.5-Turbo and GPT-4 achieved competitive results in biomedical question answering using zero-shot learning

## Executive Summary
This paper evaluates GPT-3.5-Turbo and GPT-4 on BioASQ biomedical tasks, finding strong performance in Phase B question answering using zero-shot learning. GPT-4 and GPT-3.5-Turbo achieved competitive results with leading systems in factoid and list answer formats, even outperforming them in some cases. For retrieval (Phase A), query expansion improved results, but models lagged behind specialized systems. GPT-3.5-Turbo often matched GPT-4 in grounded Q&A, despite being smaller and cheaper. Results suggest large language models can rival fine-tuned systems in biomedical domains with minimal task-specific training.

## Method Summary
The study used zero-shot learning with GPT-3.5-Turbo and GPT-4 on BioASQ 2023 tasks without fine-tuning. For Task 11b Phase A (retrieval), they implemented query expansion, reformulation, and reranking using the models. For Phase B (question answering), they generated ideal answers, yes/no responses, factoid answers, and list answers using relevant biomedical snippets as context. The study also evaluated on the MedProcNER task in Spanish. All experiments used temperature=0 for reproducibility with retry loops for API failures.

## Key Results
- GPT-4 and GPT-3.5-Turbo achieved MRR scores of 0.5789 for factoid questions, taking first and second place
- GPT-3.5-Turbo matched GPT-4 performance in grounded Q&A for factoid and list answers despite being smaller and cheaper
- Query expansion through zero-shot learning improved retrieval performance, though specialized systems still outperformed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting with relevant biomedical snippets enables GPT models to achieve competitive performance in biomedical question answering without domain-specific fine-tuning.
- Mechanism: By providing context-relevant snippets alongside questions, the models can ground their responses in domain-specific knowledge, compensating for the lack of specialized pre-training.
- Core assumption: The snippets contain sufficient information for the model to generate accurate answers, and the model's general language understanding is adequate for biomedical tasks.
- Evidence anchors:
  - [abstract] "Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets."
  - [section] "In Phase B, the participating systems receive the same questions as in Phase A, along with a set of gold (correct) articles and snippets selected by biomedical experts. They should then generate an ideal paragraph sized (at most 200 words) answer based on these snippets."
- Break condition: If the snippets are incomplete or misleading, or if the model lacks sufficient biomedical knowledge to interpret the snippets, performance will degrade significantly.

### Mechanism 2
- Claim: Query expansion through zero-shot learning improves biomedical document retrieval performance.
- Mechanism: The models generate expanded queries by incorporating synonyms and related terms, increasing recall while maintaining precision.
- Core assumption: The models have sufficient knowledge of biomedical terminology and PubMed query syntax to generate effective expanded queries.
- Evidence anchors:
  - [abstract] "In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance..."
  - [section] "We used zero-shot learning for query expansion, query reformulation and reranking directly with the models."
- Break condition: If the expanded queries become too broad or include irrelevant terms, precision will suffer. If the models don't understand PubMed's query syntax, the expansion will be ineffective.

### Mechanism 3
- Claim: GPT-3.5-Turbo can compete with GPT-4 in grounded biomedical question answering tasks despite being smaller and cheaper.
- Mechanism: The grounding information (snippets) provides sufficient context that the additional capabilities of GPT-4 become less critical for extractive tasks like factoid and list answers.
- Core assumption: The difference in capabilities between GPT-3.5-Turbo and GPT-4 is less pronounced when working with provided context rather than relying on internal knowledge.
- Evidence anchors:
  - [abstract] "Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&A setting on factoid and list answers."
  - [section] "In the Factoid question format, both grounded GPT-4 and grounded GPT-3.5-turbo achieved an MRR score of 0.5789 taking first and second place..."
- Break condition: If the task requires more complex reasoning or generation beyond what can be derived from the snippets, GPT-4's additional capabilities may become more valuable.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study relies on models performing tasks without task-specific training, using only general language understanding and provided context.
  - Quick check question: What distinguishes zero-shot learning from few-shot learning in the context of this study?

- Concept: Prompt engineering
  - Why needed here: The study uses carefully crafted prompts to guide model behavior, including query expansion and answer generation.
  - Quick check question: How does the prompt for query expansion differ from the prompt for ideal answer generation?

- Concept: Biomedical domain knowledge
  - Why needed here: Understanding why biomedical tasks are challenging for general models and how context provision helps.
  - Quick check question: Why might biomedical question answering be particularly challenging for general-purpose language models?

## Architecture Onboarding

- Component map: Query processing pipeline -> PubMed search -> Reranking -> Answer generation (with snippets) -> Evaluation
- Critical path: Question → Query expansion → PubMed search → Reranking → Answer generation (with snippets) → Evaluation
- Design tradeoffs: Using zero-shot learning trades off potential performance gains from fine-tuning for reduced development time and data requirements. Using GPT-3.5-Turbo instead of GPT-4 trades off some performance for cost and speed.
- Failure signatures: Poor retrieval performance indicates ineffective query expansion or reranking. Low answer quality indicates insufficient context in snippets or model limitations in biomedical understanding.
- First 3 experiments:
  1. Compare retrieval performance with and without query expansion on a small test set.
  2. Test answer quality with and without snippet grounding for different question types.
  3. Compare GPT-3.5-Turbo vs GPT-4 performance on a subset of factoid questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance of GPT-4 and GPT-3.5-Turbo compare across different biomedical task formats, particularly in factoid and list question answering?
- Basis in paper: [explicit] The paper states that GPT-3.5-Turbo often matched GPT-4 in grounded Q&A for factoid and list answers, despite being smaller and cheaper.
- Why unresolved: While the paper presents results, it does not provide a detailed analysis of why GPT-3.5-Turbo can compete with GPT-4 in these specific formats.
- What evidence would resolve it: A detailed analysis of the performance metrics of GPT-4 and GPT-3.5-Turbo across different biomedical task formats, with a focus on factoid and list question answering.

### Open Question 2
- Question: How does the performance of GPT models compare to fine-tuned systems in biomedical tasks, and what are the implications for future research and development?
- Basis in paper: [explicit] The paper concludes that GPT models can rival fine-tuned systems in biomedical domains with minimal task-specific training.
- Why unresolved: The paper does not explore the broader implications of this finding for future research and development in the field.
- What evidence would resolve it: A comprehensive analysis of the potential impact of GPT models on the development of future biomedical systems and research methodologies.

### Open Question 3
- Question: What are the ethical implications of using large language models like GPT-4 and GPT-3.5-Turbo in biomedical tasks, and how can these be addressed?
- Basis in paper: [explicit] The paper discusses data privacy, factual accuracy, economic barriers, and transparency as key ethical considerations.
- Why unresolved: The paper does not provide a detailed framework for addressing these ethical implications.
- What evidence would resolve it: A comprehensive framework for addressing the ethical implications of using large language models in biomedical tasks, including guidelines for data privacy, factual accuracy, economic accessibility, and transparency.

## Limitations
- Evaluation focused on a single biomedical task benchmark (BioASQ) with specific GPT models
- Models rely on provided snippets rather than demonstrating internalized biomedical knowledge
- Cost implications of using these models at scale in production biomedical systems were not addressed

## Confidence
- High Confidence: Claims about GPT-3.5-Turbo achieving competitive performance with GPT-4 in grounded question answering are well-supported by the MRR scores and ranking results presented.
- Medium Confidence: The assertion that query expansion improves retrieval performance is supported, but the magnitude of improvement and comparison to specialized systems could vary with different datasets or query types.
- Low Confidence: Broader claims about LLMs rivaling fine-tuned systems in biomedical domains should be interpreted cautiously, as the study doesn't test a comprehensive range of biomedical tasks or compare against all existing systems in the field.

## Next Checks
1. Test the zero-shot approach on additional biomedical datasets beyond BioASQ to assess generalizability across different task types and domains.
2. Implement a cost analysis comparing API usage costs for GPT-3.5-Turbo versus GPT-4 across typical query volumes in a biomedical setting.
3. Conduct ablation studies removing snippet grounding to determine the exact contribution of context versus model knowledge to performance.