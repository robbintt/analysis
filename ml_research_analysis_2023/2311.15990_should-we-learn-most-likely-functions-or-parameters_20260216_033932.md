---
ver: rpa2
title: Should We Learn Most Likely Functions or Parameters?
arxiv_id: '2311.15990'
source_url: https://arxiv.org/abs/2311.15990
tags:
- function
- fs-map
- ps-map
- where
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates function-space maximum a posteriori (MAP)
  estimation as an alternative to standard parameter-space MAP. The authors argue
  that parameters are only meaningful in the context of the function they parameterize,
  and the most likely parameters under the parameter posterior do not generally correspond
  to the most likely function.
---

# Should We Learn Most Likely Functions or Parameters?

## Quick Facts
- arXiv ID: 2311.15990
- Source URL: https://arxiv.org/abs/2311.15990
- Reference count: 40
- Primary result: Function-space MAP estimation provides flatter minima and better generalization than parameter-space MAP, with a scalable approximation (L-MAP) achieving comparable performance on image classification

## Executive Summary
This paper investigates function-space maximum a posteriori (MAP) estimation as an alternative to standard parameter-space MAP. The authors argue that parameters are only meaningful in the context of the function they parameterize, and the most likely parameters under the parameter posterior do not generally correspond to the most likely function. They derive a function-space MAP objective through a generalization of the change of variables formula, showing that it is invariant to model parameterization and leads to flatter minima, better generalization, and improved robustness to overfitting. However, they also identify limitations, including pathological solutions and scalability challenges for large neural networks. To address the latter, they propose a scalable approximation using Laplacian regularization (L-MAP), which achieves comparable or slightly better accuracy than standard MAP on image classification tasks, while often providing better calibration.

## Method Summary
The paper proposes function-space MAP estimation, which directly estimates the most likely function implied by the model and data, rather than the most likely parameters. The function-space MAP objective is derived by generalizing the change of variables formula to account for function-space geometry, resulting in an additional Jacobian determinant term that penalizes parameters whose small changes cause large changes in the function output. To address the computational limitations of the full function-space MAP objective, the authors propose a scalable approximation using Laplacian regularization (L-MAP), which adds a term to the standard MAP objective that approximates the log determinant of the Jacobian using a small perturbation of parameters.

## Key Results
- Function-space MAP estimation leads to flatter minima and better generalization than parameter-space MAP on synthetic and real-world datasets
- L-MAP, the scalable approximation of function-space MAP, achieves comparable or slightly better accuracy than standard MAP on image classification tasks
- L-MAP often provides better calibration than standard MAP, as measured by expected calibration error (ECE)
- The benefits of function-space MAP are greatest when the prior is well-motivated, which is often not the case in real-world scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The function-space MAP objective finds flatter minima that generalize better than parameter-space MAP
- Mechanism: The Jacobian determinant term in the function-space MAP objective penalizes parameters whose small changes cause large changes in the function output, encouraging solutions where the function is less sensitive to parameter perturbations
- Core assumption: The training data distribution matches the evaluation distribution pX used in the Jacobian calculation
- Evidence anchors:
  - [abstract]: "function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting"
  - [section 3.2]: "we show that optimization of the function-space MAP objective indeed results in more probable functions than standard parameter-space MAP, and these functions often correspond to flat minima"
  - [corpus]: Weak - corpus doesn't directly address flatness properties
- Break condition: If the evaluation distribution pX doesn't match the true data distribution, the Jacobian regularization may encourage functions that generalize poorly

### Mechanism 2
- Claim: Parameter-space MAP is not invariant to model reparameterization while function-space MAP is
- Mechanism: When changing parameterization θ → θ', the parameter-space MAP objective gains a Jacobian determinant term that changes the optimization landscape, while function-space MAP depends only on the function itself which is invariant to parameterization
- Core assumption: The function-space MAP objective correctly captures the posterior over functions without singularities
- Evidence anchors:
  - [abstract]: "we can reparametrize a model such that any setting of parameters can maximize the parameter posterior"
  - [section A]: "Parameter-space MAP estimation has conceptual and theoretical shortcomings stemming from the lack of reparameterization-invariance of MAP estimation"
  - [corpus]: Weak - corpus doesn't directly address reparameterization invariance
- Break condition: If the function-space MAP objective has pathological singularities (infinite prior density), reparameterization invariance breaks down

### Mechanism 3
- Claim: L-MAP provides a scalable approximation of function-space MAP for large neural networks
- Mechanism: The Laplacian regularization term approximates the log determinant of the Jacobian using a small perturbation of parameters, reducing computational complexity from O(SKP²) to O(SP)
- Core assumption: The perturbation variance β² is small enough for the first-order approximation to be accurate
- Evidence anchors:
  - [abstract]: "To help address the computational limitations, we provide in Section 4 a scalable approximation to the function-space MAP objective applicable to large neural networks using Laplacian regularization, which we refer to as L-MAP"
  - [section 4.4]: Detailed derivation showing how the Laplacian approximation works
  - [corpus]: Weak - corpus doesn't provide empirical validation of the approximation accuracy
- Break condition: If β² is too large, the first-order approximation breaks down and the regularization becomes ineffective

## Foundational Learning

- Concept: Change of variables formula for probability densities
  - Why needed here: The core innovation relies on generalizing this formula to account for function-space geometry
  - Quick check question: What happens to a probability density when we transform variables from θ to fθ(x)?

- Concept: Jacobian determinant and its role in volume transformations
  - Why needed here: The Jacobian determinant in the function-space MAP objective measures how parameter changes affect function outputs
  - Quick check question: How does the determinant of Jθ(x)⊤Jθ(x) relate to the volume of function space changes?

- Concept: Laplace approximation and Hessian-based curvature analysis
  - Why needed here: Understanding why flat minima generalize better requires knowledge of how Hessian curvature relates to generalization
  - Quick check question: Why do flatter minima in the loss landscape typically lead to better generalization?

## Architecture Onboarding

- Component map:
  - Standard MAP objective: -log p(D|θ) - log p(θ)
  - Function-space MAP objective: -log p(D|θ) - log p(θ) - ½log det J(θ;pX)
  - L-MAP approximation: -log p(D|θ) - log p(θ) - ½ϵβ²Eψ[d(θ,θ+ψ)]
  - Evaluation distribution pX: Specifies which inputs the Jacobian regularization cares about

- Critical path:
  1. Choose evaluation distribution pX
  2. Compute Jacobian Jθ(x) for each input
  3. Estimate J(θ;pX) = EpX[Jθ(X)⊤Jθ(X)]
  4. Compute log det J(θ;pX) or its Laplacian approximation
  5. Add to standard MAP objective

- Design tradeoffs:
  - More evaluation points → better approximation but higher computation
  - Larger jitter ϵ → removes singularities but weakens regularization
  - Smaller perturbation β² → better approximation but potential numerical instability

- Failure signatures:
  - Singular Jacobian → infinite prior density, pathological solutions
  - Mismatched pX → regularization encourages wrong function properties
  - Too small ϵ → numerical instability in log determinant computation

- First 3 experiments:
  1. Simple regression with known ground truth to verify FS-MAP finds more probable functions
  2. Compare PS-MAP vs L-MAP on UCI datasets to measure generalization gap
  3. Test sensitivity to evaluation distribution pX by using different distributions on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does function-space MAP (FS-MAP) estimation perform on real-world datasets with complex priors and models compared to parameter-space MAP (PS-MAP)?
- Basis in paper: [inferred] The paper shows that FS-MAP can outperform PS-MAP in synthetic regression tasks and on image classification datasets using ResNet-18. However, it also mentions that the benefits of FS-MAP are greatest when the prior is well-motivated, which is often not the case in real-world scenarios.
- Why unresolved: The paper does not provide extensive empirical evidence on how FS-MAP performs on real-world datasets with complex priors and models. The experiments are limited to synthetic regression tasks and image classification using ResNet-18.
- What evidence would resolve it: Experiments comparing FS-MAP and PS-MAP on a wide range of real-world datasets with complex priors and models, including datasets with high-dimensional inputs and outputs, non-linear relationships, and non-Gaussian noise.

### Open Question 2
- Question: Can the scalability issues of FS-MAP estimation be addressed for large neural networks without compromising its benefits?
- Basis in paper: [explicit] The paper acknowledges that the Jacobian term in the FS-MAP objective can require orders of magnitude more computation and memory than PS-MAP, making it difficult to scale to modern neural networks. The proposed Laplacian regularization (L-MAP) is a scalable approximation but may not fully capture the benefits of FS-MAP.
- Why unresolved: The paper proposes a scalable approximation (L-MAP) but does not provide extensive empirical evidence on its effectiveness compared to FS-MAP on large neural networks. It is unclear whether L-MAP can fully capture the benefits of FS-MAP without the scalability issues.
- What evidence would resolve it: Experiments comparing the performance of FS-MAP and L-MAP on large neural networks, such as ResNet-50 or larger, on various tasks and datasets. The experiments should assess the trade-off between scalability and the benefits of FS-MAP.

### Open Question 3
- Question: How does the choice of evaluation distribution pX in FS-MAP estimation affect the performance and generalization of the learned functions?
- Basis in paper: [explicit] The paper discusses the importance of the choice of evaluation distribution pX in FS-MAP estimation. It shows that specifying pX to correctly reflect the distribution of inputs at test time is crucial for optimal performance. The paper also mentions that the evaluation distribution corresponds to a choice of the metric tensor in function space, which affects the geometry of the function space.
- Why unresolved: The paper provides some insights into the effect of the evaluation distribution pX on the performance of FS-MAP. However, it does not provide a comprehensive analysis of how different choices of pX affect the generalization and robustness of the learned functions. It is unclear how to choose an appropriate pX for different tasks and datasets.
- What evidence would resolve it: A systematic study of the effect of different evaluation distributions pX on the performance and generalization of FS-MAP on various tasks and datasets. The study should investigate the trade-offs between different choices of pX and provide guidelines for choosing an appropriate pX for different scenarios.

## Limitations
- The full function-space MAP objective requires computing log determinants of large Jacobian matrices, making it intractable for standard neural network architectures
- The proposed L-MAP approximation relies on a first-order Taylor expansion that may break down for larger perturbations
- The function-space MAP formulation can produce pathological solutions with infinite posterior density when the Jacobian is singular, requiring ad-hoc fixes like adding jitter

## Confidence
- **High confidence**: The reparameterization invariance property of function-space MAP (Mechanism 2) - this follows directly from the mathematical formulation and is well-established in differential geometry
- **Medium confidence**: The generalization benefits of FS-MAP and L-MAP (Mechanism 1) - supported by experimental results but limited by the scalability approximation
- **Low confidence**: The practical scalability of L-MAP for very large networks - while the approximation is theoretically sound, its effectiveness for deep networks with millions of parameters remains untested

## Next Checks
1. **Singularity detection**: Systematically test when and how often the Jacobian becomes singular during training across different architectures and datasets, and evaluate whether the jitter fix introduces bias.

2. **Approximation accuracy**: Compare the Laplacian regularization approximation against exact FS-MAP computation on small networks to quantify the approximation error and its impact on function selection.

3. **Distribution shift sensitivity**: Evaluate how sensitive the FS-MAP/L-MAP benefits are to the choice of evaluation distribution pX by testing with multiple different distributions and measuring the degradation in performance.