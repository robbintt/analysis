---
ver: rpa2
title: Discrete Denoising Diffusion Approach to Integer Factorization
arxiv_id: '2309.05295'
source_url: https://arxiv.org/abs/2309.05295
tags:
- neural
- diffusion
- steps
- training
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to integer factorization
  using deep neural networks and discrete denoising diffusion. The method employs
  a seq2seq neural network architecture to iteratively correct errors in partially-correct
  solutions, making the task more tractable.
---

# Discrete Denoising Diffusion Approach to Integer Factorization

## Quick Facts
- arXiv ID: 2309.05295
- Source URL: https://arxiv.org/abs/2309.05295
- Reference count: 39
- Primary result: Neural network learns to correct errors in partially-correct factorization solutions rather than predicting full factors from scratch

## Executive Summary
This paper introduces a novel approach to integer factorization using deep neural networks and discrete denoising diffusion. The method employs a seq2seq neural network architecture to iteratively correct errors in partially-correct solutions, making the task more tractable. The authors develop a new neural architecture, adapt the reverse diffusion process, and employ relaxed categorical distributions to improve factorization performance. The approach can factorize integers up to 56 bits long, with success rates depending on the number of sampling steps. Analysis reveals that longer training leads to exponential decreases in required sampling steps, potentially counteracting the exponential increase in run-time for longer numbers. However, the method's efficiency is limited by the computational resources used, and further research with more extensive computing resources is needed to fully explore its potential.

## Method Summary
The method uses a seq2seq neural network architecture with Convolutional Shuffle Units to iteratively denoise partially-correct factorization solutions. Training data consists of odd random numbers up to 56 bits, with their products encoded as binary one-hot vectors. The model employs a relaxed categorical distribution (Gumbel-Softmax) instead of discrete bit flips to provide smoother gradients during training. A modified reverse diffusion process with additive updates (γ=0.9) helps the model recover from confident mistakes more easily than standard multiplicative updates. The network is trained on 10 million examples for 1 million steps using KL divergence loss and AdaBelief optimizer, then evaluated on 1,000 composite test numbers using an iterative sampling loop that continues until a correct factorization is found or a maximum number of steps is reached.

## Key Results
- Success rates of 60-80% on 56-bit numbers with sufficient sampling steps
- Exponential decrease in required sampling steps with longer training
- Method outperforms standard diffusion sampling on 32-bit numbers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network learns to correct errors in partially-correct solutions rather than predicting full factors from scratch.
- Mechanism: By training on noisy but partially-correct factor pairs, the model learns a denoising function that can iteratively refine guesses.
- Core assumption: Factorization is too hard to learn directly, but error correction is tractable.
- Evidence anchors:
  - [abstract]: "We present an approach to factorization utilizing deep neural networks and discrete denoising diffusion that works by iteratively correcting errors in a partially-correct solution."
  - [section]: "Therefore we took the denoising approach where the neural network is asked to correct errors in a partially correct solution instead of outputting a fully correct solution from scratch."
- Break condition: If the initial noisy input is too far from any valid factorization, the model cannot recover.

### Mechanism 2
- Claim: Modified diffusion sampling retains full probability distributions across steps, allowing additive updates instead of multiplicative ones.
- Mechanism: Standard diffusion multiplies probabilities, making recovery from confident mistakes hard. The additive update with γ=0.9 lets the model correct errors more easily.
- Core assumption: The neural network's approximations are noisy enough that multiplicative updates hinder convergence.
- Evidence anchors:
  - [abstract]: "We adapt the reverse diffusion process to cope better with inaccuracies in the denoising step."
  - [section]: "The proposed modifications remedy these pitfalls. Use of the additive update allows easy recovery from confident mistakes..."
- Break condition: If γ is too low, updates become too noisy; if too high, convergence slows.

### Mechanism 3
- Claim: Relaxed categorical distributions (Gumbel-Softmax) provide finer-grained noise than discrete bit flips, improving training and inference.
- Mechanism: The relaxed distribution allows smoother gradients during training, making the denoising task easier to learn.
- Core assumption: Categorical noise with hard bit flips is too harsh for the model to learn effective corrections.
- Evidence anchors:
  - [abstract]: "we employ relaxed categorical distribution and adapt the reverse diffusion process..."
  - [section]: "Another modification is that we use relaxed categorical distribution... Their difference is how samples are produced... Relaxed distribution has more fine-grained noise which facilitates training..."
- Break condition: If temperature is too high, the distribution becomes too smooth to represent discrete choices.

## Foundational Learning

- Concept: Denoising diffusion models
  - Why needed here: They provide a principled way to iteratively refine noisy factor guesses toward correct solutions.
  - Quick check question: What is the key difference between forward and reverse diffusion processes?

- Concept: Error correction learning
  - Why needed here: Direct factorization prediction is intractable, but correcting partial solutions is learnable.
  - Quick check question: Why might training on noisy-but-partially-correct examples be easier than training on fully correct ones?

- Concept: Relaxed categorical distributions (Gumbel-Softmax)
  - Why needed here: They provide smoother gradients than hard categorical sampling, making training more stable.
  - Quick check question: How does the temperature parameter affect the behavior of Gumbel-Softmax samples?

## Architecture Onboarding

- Component map: Input encoder -> Denoising network (CSU) -> Sampling loop -> Output decoder
- Critical path: Input → Denoising Network → Sampled guess → Check if correct → Repeat until solution found
- Design tradeoffs:
  - Recurrent depth (max(n/2, 4⌈log₂(n)⌉) CSUs) balances expressiveness vs. training speed
  - Feature map size (m=384) chosen for performance within 2-week training budget
  - Dropout rate (0.1) prevents overfitting on 10M examples
- Failure signatures:
  - Convergence to composite factors instead of primes (see Appendix B)
  - Success rate plateaus despite more training steps (indicates learning limit)
  - Very high sampling steps needed (suggests model isn't learning effective corrections)
- First 3 experiments:
  1. Train on 16-bit numbers, test with 1024 sampling steps, verify success rate >90%
  2. Compare relaxed vs discrete distributions on same setup, measure success rate difference
  3. Test modified sampling vs standard sampling on 32-bit numbers, count steps to 25% success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed neural network architecture be validated as the best choice for discrete denoising diffusion tasks?
- Basis in paper: [explicit] The paper introduces a new seq2seq neural network architecture that outperforms existing ones like Transformer, Neural GPU, and Residual Shuffle-Exchange Networks, but acknowledges that the comparison was limited without extensive hyperparameter tuning.
- Why unresolved: The paper does not provide a comprehensive comparison with other architectures or extensive hyperparameter tuning to conclusively prove the superiority of the proposed architecture.
- What evidence would resolve it: Extensive experiments comparing the proposed architecture with a wide range of other architectures, including various hyperparameter settings, on different discrete denoising diffusion tasks would provide strong evidence for its effectiveness.

### Open Question 2
- Question: How does the proposed method scale with increasing bit-length of integers to be factorized?
- Basis in paper: [explicit] The paper states that the success rate decreases exponentially with sequence length, but the current research is limited to numbers up to 56 bits.
- Why unresolved: The paper does not explore the performance of the method on integers with bit-lengths greater than 56, which would provide insights into its scalability.
- What evidence would resolve it: Experiments demonstrating the performance of the method on integers with bit-lengths significantly larger than 56 would help understand its scalability and potential limitations.

### Open Question 3
- Question: Can the proposed method be improved by incorporating the properties of primality in the algorithm?
- Basis in paper: [explicit] The paper mentions that virtually all classical factorization algorithms exploit the additional information that the factors themselves are primes, and suggests investigating how to incorporate the properties of primality in the algorithm.
- Why unresolved: The paper does not explore any modifications to the algorithm that explicitly consider the primality of the factors, leaving room for potential improvements.
- What evidence would resolve it: Experiments comparing the performance of the proposed method with and without modifications that consider the primality of the factors would provide insights into the potential benefits of incorporating primality properties.

## Limitations

- Computational resource constraints prevented testing on numbers beyond 56 bits
- Success rates still require hundreds of sampling steps for optimal performance
- The theoretical advantage over classical algorithms remains unproven for larger numbers

## Confidence

- **High**: The core mechanism of using denoising diffusion for error correction in factorization is sound and empirically validated on tested bit lengths
- **Medium**: The claim that longer training leads to exponential decreases in required sampling steps, potentially counteracting exponential runtime increases for longer numbers
- **Low**: The method's ultimate scalability and efficiency compared to classical algorithms without access to substantially more computing resources

## Next Checks

1. **Scaling Validation**: Test the model on 64-80 bit numbers using distributed training across multiple GPUs to verify if the exponential decrease in sampling steps continues as predicted
2. **Efficiency Benchmark**: Compare the complete factorization pipeline (training + sampling) runtime against state-of-the-art classical algorithms on numbers of equivalent bit lengths
3. **Generalization Test**: Evaluate the model's performance on cryptographic-strength semiprimes with very close prime factors to test robustness against worst-case scenarios