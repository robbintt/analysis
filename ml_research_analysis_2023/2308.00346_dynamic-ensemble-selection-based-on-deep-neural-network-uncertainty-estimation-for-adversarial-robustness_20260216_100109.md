---
ver: rpa2
title: Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation
  for Adversarial Robustness
arxiv_id: '2308.00346'
source_url: https://arxiv.org/abs/2308.00346
tags:
- robustness
- ensemble
- dynamic
- adversarial
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep neural networks
  to adversarial attacks by proposing a dynamic ensemble selection method based on
  uncertainty estimation. The key idea is to train an ensemble of lightweight models
  using Dirichlet prior over the predictive distribution and diversity constraints
  in the parameter space.
---

# Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness

## Quick Facts
- arXiv ID: 2308.00346
- Source URL: https://arxiv.org/abs/2308.00346
- Reference count: 40
- Key outcome: Dynamic ensemble selection using Dirichlet uncertainty estimation achieves superior transfer-based black-box attack robustness on CIFAR-10 while maintaining accuracy

## Executive Summary
This paper proposes a dynamic ensemble selection method that improves adversarial robustness by selecting sub-models based on their uncertainty estimates. The method constructs lightweight ensembles through low-rank projections of a pre-trained model and trains them with Dirichlet prior over predictive distributions and diversity constraints. During inference, sub-models with the lowest uncertainty estimates are dynamically selected, ensuring robust predictions without sacrificing accuracy. Experiments demonstrate superior transfer-based black-box attack robustness compared to traditional static and dynamic defense methods.

## Method Summary
The method constructs an ensemble of lightweight sub-models using low-rank vectorization of a pre-trained base model. Each sub-model is trained with a Dirichlet prior over the predictive distribution and diversity constraints in parameter space. During testing, uncertainty estimates (computed via differential entropy of the Dirichlet distribution) are used to select the most confident sub-model for the final prediction. The training process includes adversarial sample generation for uncertainty correction, forcing sub-models to have higher uncertainty on adversarial examples.

## Key Results
- Superior transfer-based black-box attack robustness compared to static ensemble averaging and decision-level fusion methods
- Maintains competitive white-box attack robustness when using adversarial training models as pre-trained base
- Achieves robustness improvements without compromising accuracy on benign samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic ensemble selection based on uncertainty estimation improves adversarial robustness by choosing the most confident sub-model at inference time.
- Mechanism: Each sub-model estimates prediction uncertainty using Dirichlet distribution over predictive output. During inference, the sub-model with lowest uncertainty is selected for final prediction.
- Core assumption: Uncertainty estimates from Dirichlet distribution accurately reflect model confidence and correlate with robustness to adversarial examples.
- Evidence anchors:
  - [abstract]: "In test phase, the certain sub-models are dynamically selected based on their rank of uncertainty value for the final prediction to ensure the majority accurate principle in ensemble robustness and accuracy."
  - [section]: "In the test phase, when each sub-model obtains the uncertainty estimation of the same image through 8, the output with the minimum uncertainty estimation will be selected as the ensemble model output for the final classification."
  - [corpus]: Weak evidence; no direct citation of Dirichlet-based uncertainty estimation in the corpus.
- Break condition: If uncertainty estimation fails to distinguish between adversarial and benign samples, or if all sub-models have similarly high uncertainty.

### Mechanism 2
- Claim: Lightweight ensemble construction via low-rank vectorization preserves diversity and computational efficiency.
- Mechanism: Each sub-model's parameters are projected from a shared base model using low-rank Hadamard products, enabling multiple diverse sub-models with minimal additional parameters.
- Core assumption: Low-rank projections can maintain sufficient diversity between sub-models to improve ensemble robustness, and repulsive terms effectively prevent parameter collapse.
- Evidence anchors:
  - [section]: "In order to simplify the complicated diversity constraint in lightweight conditions, this work transforms the more rigorous diversity constraint based on model parameter repulsive into the repulsive force between low-rank projection matrices."
  - [section]: "The constraint of ensemble diversity is an essential research direction of robustness. D'Angelo et al. [45] first regarded the diversity constraint as a repulsive term during the training phase and derived the relation between diversity and Bayesian property under the parameter space."
  - [corpus]: Weak evidence; no direct citation of low-rank projection in the corpus.
- Break condition: If low-rank projections do not generate sufficiently diverse sub-models, or if repulsive terms are too weak or too strong.

### Mechanism 3
- Claim: Uncertainty correction using adversarial samples improves robustness estimation.
- Mechanism: During training, adversarial examples are generated and used to compute uncertainty estimates for each sub-model. The difference in uncertainty between benign and adversarial samples is constrained.
- Core assumption: Adversarial examples have distinct uncertainty profiles from benign samples, and constraining these differences will improve robustness estimation.
- Evidence anchors:
  - [section]: "Since adversarial samples with stronger concealment significantly differ from noise or corruption samples in their distribution, they construct a particular class of data uncertainty in which its estimation is unreliable."
  - [section]: "The loss includes two main parts. The first part is similar to the margin loss defined by the uncertain correction... The uncertainty estimation between benign and adversarial samples under the same sub-model is constrained to force their difference variance."
  - [corpus]: Weak evidence; no direct citation of uncertainty correction with adversarial samples in the corpus.
- Break condition: If adversarial examples do not have reliably distinguishable uncertainty profiles, or if constraint is too weak to influence training.

## Foundational Learning

- Concept: Dirichlet distribution and its conjugate prior properties with multinomial distribution
  - Why needed here: To parameterize uncertainty over predictive output of each sub-model, enabling reliable uncertainty estimation
  - Quick check question: How does Dirichlet distribution's conjugacy with multinomial distribution allow for tractable uncertainty estimation in classification tasks?

- Concept: Low-rank matrix factorization and Hadamard products
  - Why needed here: To construct lightweight ensemble models by projecting shared parameters into diverse sub-models with minimal additional parameters
  - Quick check question: How does Hadamard product of low-rank vectors achieve parameter sharing while enabling model diversity?

- Concept: Evidential theory and differential entropy as uncertainty measures
  - Why needed here: To quantify uncertainty from Dirichlet distribution parameters and use it as selection criterion
  - Quick check question: Why is differential entropy a suitable measure of uncertainty for Dirichlet-distributed predictions?

## Architecture Onboarding

- Component map: Pre-trained WRN-28-10 -> Low-rank projection matrices (ri, si) -> Sub-models -> Dirichlet prior layer -> Diversity constraint module -> Uncertainty correction module -> Dynamic selection policy

- Critical path:
  1. Initialize ensemble from pre-trained model via low-rank projections
  2. Train with variational Dirichlet loss and diversity constraints
  3. Generate adversarial samples and apply uncertainty correction loss
  4. At inference, compute uncertainty for each sub-model
  5. Select sub-model with minimum uncertainty for final prediction

- Design tradeoffs:
  - More sub-models increase diversity but also computational cost
  - Stronger diversity constraints may reduce individual sub-model accuracy
  - Higher perturbation budgets may improve uncertainty estimation but require more training time

- Failure signatures:
  - All sub-models consistently select the same model (lack of diversity)
  - Uncertainty estimates are uniformly high or low across sub-models
  - Accuracy on benign samples drops significantly with increasing sub-models

- First 3 experiments:
  1. Train ensemble with 2 sub-models and evaluate uncertainty estimation on CIFAR-10
  2. Compare dynamic selection accuracy vs. average ensemble under transfer attacks
  3. Vary perturbation budget in adversarial training and measure impact on robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pre-trained model (e.g., adversarial training vs. baseline) affect the long-term robustness of the dynamic ensemble selection method?
- Basis in paper: [explicit] The paper mentions that using an adversarial training model as the pre-trained model provides competitive robustness under white-box attacks and optimal transfer-based black-box robustness.
- Why unresolved: The paper only briefly discusses the impact of using different pre-trained models but does not provide a detailed analysis of how this choice affects long-term robustness or performance on larger datasets.
- What evidence would resolve it: Experiments comparing the long-term robustness of the method using various pre-trained models on larger datasets and over extended training periods.

### Open Question 2
- Question: Can the uncertainty estimation-based dynamic selection policy be effectively extended to other types of attacks, such as physical-world adversarial attacks or attacks targeting specific features?
- Basis in paper: [inferred] The paper focuses on defending against adversarial attacks on image recognition tasks but does not explore the effectiveness of the method against other types of attacks.
- Why unresolved: The paper's experiments are limited to specific attack methods and datasets, leaving open the question of how well the method generalizes to other attack scenarios.
- What evidence would resolve it: Experiments testing the method's performance against a wider range of attack types, including physical-world attacks and feature-specific attacks, on various datasets.

### Open Question 3
- Question: How does the dynamic ensemble selection method compare to other ensemble methods, such as Bayesian model averaging or boosting, in terms of robustness and computational efficiency?
- Basis in paper: [explicit] The paper compares the proposed method to other ensemble methods, such as static ensemble averaging and decision-level fusion, but does not directly compare it to Bayesian model averaging or boosting.
- Why unresolved: The paper's focus is on demonstrating the advantages of the dynamic ensemble selection method over static and decision-level fusion methods, but it does not provide a comprehensive comparison with other ensemble approaches.
- What evidence would resolve it: Experiments comparing the proposed method to Bayesian model averaging and boosting in terms of robustness and computational efficiency on various datasets and attack scenarios.

## Limitations
- Implementation details of low-rank projection mechanism and diversity constraints are not fully specified, making exact reproduction challenging
- The perturbation budget for adversarial training and its relationship to uncertainty estimation accuracy is not empirically validated
- Comparison against recent ensemble methods like [17] (Ensemble Adversarial Defense) is missing, limiting claimed superiority assessment

## Confidence
- High confidence: The theoretical framework connecting Dirichlet uncertainty estimation to adversarial robustness
- Medium confidence: The effectiveness of low-rank projections for maintaining diversity in lightweight ensembles
- Low confidence: The specific implementation of the dynamic selection policy and its hyperparameter sensitivity

## Next Checks
1. Implement ablation studies varying the number of sub-models (2, 5, 10) to quantify the trade-off between diversity and computational efficiency
2. Test the method against adaptive white-box attacks where the attacker knows the selection mechanism
3. Validate whether the uncertainty estimates from Dirichlet distribution correlate with actual prediction errors on adversarial examples