---
ver: rpa2
title: Domain-Agnostic Neural Architecture for Class Incremental Continual Learning
  in Document Processing Platform
arxiv_id: '2307.05399'
source_url: https://arxiv.org/abs/2307.05399
tags:
- learning
- data
- methods
- neural
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DE&E, a domain-agnostic neural architecture
  for class incremental continual learning in document processing. The key idea is
  to use a Mixture of Experts model with a differentiable soft KNN layer to select
  relevant classifiers and weight their predictions based on similarity to input embeddings.
---

# Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform

## Quick Facts
- arXiv ID: 2307.05399
- Source URL: https://arxiv.org/abs/2307.05399
- Reference count: 25
- Key outcome: DE&E achieves up to 3% higher accuracy than E&E and over 59% higher than memory-based methods in class incremental continual learning

## Executive Summary
This paper introduces DE&E, a domain-agnostic neural architecture for class incremental continual learning in document processing platforms. The key innovation is a Mixture of Experts model with a differentiable soft KNN layer that selects relevant classifiers and weights their predictions based on similarity to input embeddings. Experiments across text, audio, and image datasets demonstrate that DE&E outperforms state-of-the-art methods without requiring a memory buffer, achieving up to 3% higher accuracy than the reference E&E method and over 59% higher than memory-based approaches.

## Method Summary
DE&E uses a domain-agnostic neural architecture combining a frozen pre-trained feature extractor with an ensemble of single-layer classifiers. The key innovation is a differentiable soft KNN layer that approximates nearest neighbor selection through optimal transport, enabling end-to-end training without a memory buffer. The voting layer weights predictions by both cosine similarity and soft KNN scores, while the entire architecture is trained incrementally on streaming data with sign-based gradient updates.

## Key Results
- Achieves up to 3% higher accuracy than the reference E&E method
- Outperforms memory-based approaches by over 59% in accuracy
- Demonstrates domain-agnostic performance across text, audio, and image datasets
- Eliminates need for memory buffer while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The differentiable soft KNN layer enables end-to-end training without requiring a memory buffer.
- Mechanism: Bregman iterative projections approximate nearest neighbor selection with a differentiable function, allowing gradients to flow through expert selection during training.
- Core assumption: Bregman method converges to stable approximation that can be differentiated.
- Evidence anchors: Abstract mentions differentiable KNN layer; section describes Bregman projections; no corpus evidence found.
- Break condition: If Bregman iterations fail to converge or approximation becomes too noisy, gradient flow will be unreliable.

### Mechanism 2
- Claim: Voting layer weighting by cosine similarity and soft KNN scores improves ensemble accuracy.
- Mechanism: Final prediction is weighted sum where each expert's output is multiplied by both its cosine similarity to input and soft KNN score.
- Core assumption: Combining similarity-based and relevance-based weighting captures complementary aspects of classifier quality.
- Evidence anchors: Abstract mentions novel aggregation approach; section describes using both cn and γ for weighting; no corpus evidence found.
- Break condition: If similarity or KNN scores become uncorrelated with classifier quality, weighting will amplify poor predictions.

### Mechanism 3
- Claim: Frozen feature extractor and incremental expert specialization prevent catastrophic forgetting.
- Mechanism: Freezing feature extractor prevents embedding space drift while expert ensemble learns to specialize incrementally without overwriting previous knowledge.
- Core assumption: Feature space stability is more critical than fine-tuning embeddings for incremental learning.
- Evidence anchors: Abstract mentions use of pre-trained model with frozen parameters; section describes preventing forgetting by isolating feature space learning; no corpus evidence found.
- Break condition: If frozen feature extractor produces suboptimal embeddings for new classes, ensemble cannot adapt effectively.

## Foundational Learning

- Concept: Optimal Transport and Bregman Projections
  - Why needed here: Enable differentiable approximation of non-differentiable KNN algorithm
  - Quick check question: Can you explain how Bregman projection method transforms discrete KNN selection into continuous, differentiable operation?

- Concept: Mixture of Experts and Gating Mechanisms
  - Why needed here: Ensemble of specialized classifiers selected by gating mechanism allows nuanced adaptation to incremental class arrivals
  - Quick check question: How does soft KNN gating mechanism differ from traditional softmax gating in Mixture of Experts models?

- Concept: Catastrophic Forgetting and its Mitigation Strategies
  - Why needed here: Addresses fundamental challenge of training neural networks on streaming data without losing performance on previously learned classes
  - Quick check question: Why might freezing feature extractor be more effective than regularization-based approaches for preventing catastrophic forgetting?

## Architecture Onboarding

- Component map: Input → Feature Extractor → Cosine Distance to Keys → Soft KNN Approximation → Voting Layer → Output Prediction
- Critical path: Data flows through frozen feature extractor, cosine distance computation, soft KNN selection, and weighted voting to produce final prediction
- Design tradeoffs:
  - Frozen feature extractor ensures stability but limits adaptability to new data distributions
  - Larger ensembles improve accuracy but increase computational cost and training time
  - Soft KNN adds differentiability but requires careful tuning of hyperparameters
- Failure signatures:
  - Training instability or divergence: likely issues with soft KNN convergence or learning rate
  - Poor accuracy on new classes: frozen feature extractor may not capture relevant features
  - Excessive training time: large ensemble size or inefficient soft KNN implementation
- First 3 experiments:
  1. Verify soft KNN layer produces reasonable nearest neighbor approximations by comparing to standard KNN on small dataset
  2. Test voting layer weighting strategy by comparing ensemble performance with and without similarity-based weighting
  3. Evaluate impact of ensemble size on accuracy using simple text classification dataset with 5-10 classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does proposed architecture scale to larger datasets with hundreds or thousands of classes, given quadratic memory complexity of soft KNN layer?
- Basis in paper: Paper mentions O(n) complexity but doesn't discuss scalability to large numbers of classes
- Why unresolved: Only evaluates on datasets with 5-10 classes, doesn't address performance with significantly more classes
- What evidence would resolve it: Experiments showing accuracy and resource usage as class count increases from 10 to 1000+, analysis of memory/computation scaling

### Open Question 2
- Question: What is impact of different feature extractor choices on final classification accuracy, and optimal strategy for selecting/fine-tuning extractors for specific domains?
- Basis in paper: Paper states accuracy is proportional to feature quality but doesn't systematically evaluate impact of different extractors
- Why unresolved: Uses different pre-trained extractors for different domains without comparing alternatives
- What evidence would resolve it: Systematic experiments varying feature extractor while keeping DE&E constant, quantifying relationship between extractor quality and final accuracy

### Open Question 3
- Question: How robust is method to noisy or corrupted input data, and what are failure modes when input quality degrades?
- Basis in paper: Paper mentions increasing experts is ineffective with low-quality features but doesn't explore robustness to input noise
- Why unresolved: Focuses on clean datasets, doesn't evaluate performance with noise, corruption, or adversarial examples
- What evidence would resolve it: Experiments introducing various types and levels of input noise/corruption, analysis of accuracy degradation

### Open Question 4
- Question: How does method perform in online/streaming scenarios where data arrives continuously rather than in discrete batches?
- Basis in paper: Paper states method enables training when examples presented separately but doesn't evaluate true online/streaming performance
- Why unresolved: Evaluates batch-wise incremental learning but doesn't address practical continuous streaming scenario
- What evidence would resolve it: Experiments measuring performance as data streams in real-time, analysis of computational/memory requirements for continuous updates

## Limitations
- Soft KNN layer implementation details are sparse, making exact reproduction challenging
- Voting layer with thresholding may implicitly function as selective memory not fully characterized
- Comparison against baselines depends on specific hyperparameter settings without sensitivity analysis

## Confidence
- High confidence in core architectural design and theoretical foundation in Mixture of Experts and optimal transport theory
- Medium confidence in empirical performance claims, as results are compelling but depend on implementation details not fully specified
- Low confidence in claim of being truly "domain-agnostic" without testing on broader range of domain types

## Next Checks
1. Replicate soft KNN layer convergence behavior on simple 2D dataset to verify Bregman projection implementation
2. Conduct ablation studies comparing DE&E performance with and without cosine similarity weighting in voting layer
3. Test architecture's performance degradation when frozen feature extractor is replaced with randomly initialized weights