---
ver: rpa2
title: Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates
arxiv_id: '2310.04352'
source_url: https://arxiv.org/abs/2310.04352
tags:
- feature
- fairfis
- fairness
- decision
- trees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new feature importance score, FairFIS, to
  interpret how each feature contributes to fairness or bias in trees, tree-based
  ensembles, or tree-based surrogates of any complex ML system. Inspired by the popular
  mean decrease in impurity (MDI) for trees, FairFIS is defined based on the mean
  decrease (or increase) in group bias.
---

# Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates

## Quick Facts
- arXiv ID: 2310.04352
- Source URL: https://arxiv.org/abs/2310.04352
- Reference count: 40
- Primary result: FairFIS is a new feature importance score that measures how features contribute to fairness or bias in trees, tree-based ensembles, and tree-based surrogates of complex ML systems.

## Executive Summary
This paper introduces FairFIS, a feature importance score designed to interpret how each feature contributes to fairness or bias in tree-based models and their surrogates. Inspired by the popular mean decrease in impurity (MDI) method, FairFIS measures the mean decrease (or increase) in group bias across tree splits. The method is demonstrated through extensive simulations and real-world examples on benchmark fairness datasets, showing its effectiveness in providing valid interpretations for both tree-based ensembles and tree-based surrogates of other ML systems.

## Method Summary
FairFIS is a feature importance score that extends the concept of mean decrease in impurity (MDI) to measure fairness contributions in decision trees. The method calculates the difference in group bias (using metrics like Demographic Parity and Equality of Opportunity) between parent and child nodes at each split, aggregating these differences across all splits for each feature. For tree-based ensembles, FairFIS averages the scores across all trees. The approach is also extended to tree-based surrogates of complex ML systems by fitting a fully grown decision tree to the original model's predictions and applying FairFIS to this surrogate tree.

## Key Results
- FairFIS successfully interprets feature contributions to fairness in tree-based ensembles and surrogates
- The method demonstrates positive FairFIS scores for features that improve fairness and negative scores for those that worsen fairness
- Extensive validation on benchmark fairness datasets confirms FairFIS provides valid and interpretable fairness insights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairFIS captures feature bias by measuring the change in group bias between parent and child nodes during tree splits.
- Mechanism: For each split in the tree, FairFIS calculates the difference in bias between the parent level (containing both child nodes) and the child level (individual child nodes), aggregating this difference across all splits using a given feature.
- Core assumption: Group bias metrics like DP and EQOP can be meaningfully defined at the node level and their differences reflect the contribution of a feature to fairness.
- Evidence anchors:
  - [abstract] "Like the popular mean decrease in impurity for trees, our Fair Feature Importance Score is defined based on the mean decrease (or increase) in group bias."
  - [section 2.2] "Thus, for a given nodet, there are never any differences between the predictions based on protected group status... we propose to consider the difference in bias between the split that produced node t and the split at nodet that produces nodet's children."
- Break condition: If the bias metric is zero at all nodes due to constant predictions, the method fails to distinguish fairness contributions.

### Mechanism 2
- Claim: FairFIS extends to tree-based ensembles and tree-based surrogates by averaging the score across all trees in the ensemble.
- Mechanism: For tree-based ensembles, FairFIS is computed for each individual tree and then averaged (or weighted averaged) across the ensemble. For surrogates, a fully grown decision tree is fit to the complex model's predictions, and FairFIS is computed on this surrogate tree.
- Core assumption: The surrogate tree accurately captures the bias patterns of the original complex model.
- Evidence anchors:
  - [abstract] "we outline how to use FairFIS to interpret tree-based ensembles and tree-based global surrogates of complex ML systems."
  - [section 2.4] "For these tree-based ensembles, FIS is averaged (or averaged with weights) over all the trees in the ensemble... We propose to extend FairFIS in the exact same manner."
- Break condition: If the surrogate tree does not accurately reproduce the predictions of the original model, FairFIS will not reflect the true fairness of the complex system.

### Mechanism 3
- Claim: FairFIS can distinguish between features that improve fairness (positive score) and those that worsen fairness (negative score).
- Mechanism: By calculating the change in bias between parent and child levels, FairFIS assigns a positive score when the split reduces bias and a negative score when the split increases bias.
- Core assumption: The direction of bias change (improvement or worsening) is meaningful for interpreting feature contributions.
- Evidence anchors:
  - [abstract] "Positive FairFIS indicates the split improves fairness, while negative values indicate the split worsens fairness."
  - [section 2.2] "Thus, FairFIS will be positive when the split at nodet improved the bias and negative when the split at nodet made the bias worse."
- Break condition: If the bias metric does not consistently measure fairness (e.g., if it is not symmetric or bounded), the sign interpretation may be misleading.

## Foundational Learning

- Concept: Mean Decrease in Impurity (MDI) for feature importance in decision trees.
  - Why needed here: FairFIS is directly inspired by MDI, replacing the impurity/loss function with a group bias metric.
  - Quick check question: What is the key difference between FairFIS and traditional MDI?

- Concept: Group fairness metrics like Demographic Parity (DP) and Equality of Opportunity (EQOP).
  - Why needed here: FairFIS uses these metrics to measure bias at each node in the tree.
  - Quick check question: How does DP differ from EQOP in measuring group bias?

- Concept: Decision tree structure and terminology (nodes, levels, children, splits).
  - Why needed here: FairFIS is defined in terms of the tree's structure, requiring understanding of parent-child relationships and levels.
  - Quick check question: What is the difference between a node's level and its child level in a decision tree?

## Architecture Onboarding

- Component map: Decision tree/surrogate -> Node bias calculation -> Split bias difference -> Feature aggregation -> FairFIS scores
- Critical path: 1. Build or load decision tree 2. For each node, identify the feature used for the split 3. Compute bias at the parent level (both child nodes) 4. Compute bias at the child level (individual child nodes) 5. Calculate the difference in bias for the split 6. Aggregate differences for each feature across all splits 7. Normalize scores to sum to one (optional)
- Design tradeoffs:
  - Using soft predictions vs. hard labels in classification (chosen: probabilistic trees for interpretable bias)
  - Normalizing FairFIS scores vs. keeping raw values (chosen: normalization for comparability with FIS)
  - Extending to other bias metrics beyond DP and EQOP (possible but not implemented)
- Failure signatures:
  - All FairFIS scores are zero: likely due to constant predictions at nodes, causing zero bias
  - FairFIS scores do not align with expected bias patterns: check bias metric implementation and tree structure
  - FairFIS scores are all positive or all negative: check if bias metric is consistently increasing or decreasing
- First 3 experiments:
  1. Implement FairFIS on a simple decision tree with known bias patterns (e.g., features correlated with protected attribute) and verify sign and magnitude of scores.
  2. Compare FairFIS scores on a tree-based ensemble to those on individual trees to verify averaging behavior.
  3. Apply FairFIS to a tree-based surrogate of a simple black-box model and compare to the original model's fairness to verify surrogate accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is FairFIS to noisy or corrupted features in real-world datasets?
- Basis in paper: [inferred] The paper validates FairFIS on benchmark datasets but does not explicitly test robustness to feature noise or corruption.
- Why unresolved: The simulations and case studies focus on clean data, leaving the behavior under feature corruption unclear.
- What evidence would resolve it: Experiments adding varying levels of noise or missingness to features and measuring FairFIS stability would clarify robustness.

### Open Question 2
- Question: Can FairFIS be effectively extended to fairness metrics beyond DP and EQOP, such as individual fairness or causal fairness?
- Basis in paper: [explicit] The paper notes that the framework is "conducive to other group metrics as well" but only implements DP and EQOP.
- Why unresolved: Only two fairness metrics are empirically validated, limiting generalizability to other fairness notions.
- What evidence would resolve it: Applying FairFIS to other fairness definitions and comparing results would test extensibility.

### Open Question 3
- Question: What is the computational complexity of FairFIS for very large trees or ensembles, and how does it scale with dataset size?
- Basis in paper: [inferred] The paper claims FairFIS is "fast" and "computationally efficient," but no complexity analysis or runtime benchmarks are provided.
- Why unresolved: Without explicit runtime analysis, scalability claims remain unverified.
- What evidence would resolve it: Complexity analysis and runtime experiments on datasets of increasing size would clarify scalability.

## Limitations

- The generalizability of FairFIS to fairness metrics beyond DP and EQOP remains unproven, as the paper only validates with these two metrics despite claiming broader applicability.
- The computational complexity and scalability of FairFIS for very large trees or ensembles is not analyzed, leaving practical limitations unclear.
- The validity of FairFIS interpretations through tree-based surrogates assumes perfect surrogate accuracy, which is rarely achieved in practice and not quantified in the paper.

## Confidence

- **High Confidence**: The core mechanism of FairFIS (measuring bias change between parent and child nodes) is well-defined and theoretically sound based on the MDI analogy. The extension to ensembles through averaging is straightforward and expected to work correctly.
- **Medium Confidence**: The interpretation of positive/negative FairFIS scores as improving/worsening fairness depends on the properties of the bias metric used. While the paper uses well-established metrics (DP and EQOP), the sign interpretation could be problematic for metrics that are not bounded or symmetric.
- **Low Confidence**: The claim that FairFIS provides valid interpretations for any complex ML system through tree-based surrogates assumes perfect surrogate accuracy, which is rarely achieved in practice. The paper doesn't quantify surrogate error or its impact on FairFIS validity.

## Next Checks

1. Test FairFIS with alternative bias metrics beyond DP and EQOP (e.g., disparate mistreatment, conditional demographic parity) to verify the claim of metric-agnostic validity and ensure sign interpretations remain meaningful.

2. Measure computational time complexity of FairFIS on increasingly large datasets and deeper trees to identify practical scalability limits and compare against traditional FIS computation times.

3. Evaluate FairFIS sensitivity to surrogate tree accuracy by systematically degrading surrogate quality and measuring the resulting error in FairFIS scores compared to ground truth bias patterns in the original model.