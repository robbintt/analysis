---
ver: rpa2
title: 'MISGENDERED: Limits of Large Language Models in Understanding Pronouns'
arxiv_id: '2306.03950'
source_url: https://arxiv.org/abs/2306.03950
tags:
- pronouns
- pronoun
- name
- language
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MISGENDERED, a framework for evaluating large
  language models' ability to correctly use gender-neutral and neo-pronouns. The authors
  create a dataset of 3.8 million instances where models must predict a missing pronoun
  given an individual's declared pronoun set.
---

# MISGENDERED: Limits of Large Language Models in Understanding Pronouns

## Quick Facts
- arXiv ID: 2306.03950
- Source URL: https://arxiv.org/abs/2306.03950
- Reference count: 11
- Key outcome: Large language models perform significantly worse on non-binary pronouns than binary pronouns, with few-shot learning plateauing at 45.4% accuracy for neo-pronouns.

## Executive Summary
This paper introduces MISGENDERED, a framework for evaluating large language models' ability to correctly use gender-neutral and neo-pronouns. The authors create a dataset of 3.8 million instances where models must predict a missing pronoun given an individual's declared pronoun set. When prompted out-of-the-box, models perform poorly on non-binary pronouns (7.6% accuracy for neo-pronouns, 31% for gender-neutral) compared to binary pronouns (75.3%). This is attributed to the scarcity of non-binary pronouns in training data and name-pronoun associations. Few-shot learning improves neo-pronoun performance to 45.4% with 6 examples, but does not scale further. The authors release their dataset, code, and demo to enable further research on pronoun understanding in language models.

## Method Summary
The authors create a dataset of 3.8 million evaluation instances using templates that require predicting a missing pronoun given an individual's declared pronoun set. They evaluate multiple model families (GPT-2, GPT-J, OPT, BART, T5) at different scales using both zero-shot and few-shot in-context learning. Pronoun prediction is performed using constrained decoding, and accuracy is measured across binary, gender-neutral, and neo-pronoun categories. Corpus analysis is conducted on C4, Pile, and OpenWebText to examine pronoun representation in pre-training data.

## Key Results
- Zero-shot accuracy: 75.3% for binary pronouns, 31% for gender-neutral, 7.6% for neo-pronouns
- Few-shot plateau: Accuracy improves to 45.4% for neo-pronouns with 6 examples but does not scale further
- Explicit pronoun declarations slightly improve neo-pronoun accuracy (from 6% to 9%)
- Corpus analysis shows neo-pronouns are substantially rarer than binary pronouns in pre-training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot pronoun prediction fails due to low corpus representation and name-pronoun memorization.
- **Mechanism:** Models trained on imbalanced corpora learn to associate pronouns with names based on frequency, not gender identity. Neo-pronouns are so rare that the model never learns the correct mapping.
- **Core assumption:** Pre-training corpora contain vastly more binary pronouns and gendered names than non-binary ones, and the model memorizes these associations.
- **Evidence anchors:**
  - [abstract] "This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations."
  - [section 4.2] "Using the Elastic Search indices of C4 (Raffel et al., 2020), and Pile (pre-training corpus for GPT-J) (Gao et al., 2020), we count the number of documents in each corpus that contain tokens for each pronoun in Table 1."
  - [corpus] "In both cases, neo-pronouns are substantially rarer than binary pronouns (Table 11). Further, even the documents that contain non-binary pronoun tokens often do not use them semantically as pronouns."

### Mechanism 2
- **Claim:** Few-shot learning plateaus because models lack true generalization ability and rely on pattern matching.
- **Mechanism:** With few examples, models can memorize correct pronoun mappings for specific instances but cannot extrapolate to new names or contexts. The plateau suggests a hard limit on few-shot adaptation.
- **Core assumption:** The model's ability to learn from examples is constrained by the shallow pattern matching capacity of the transformer architecture in few-shot regimes.
- **Evidence anchors:**
  - [abstract] "Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns."
  - [section 4.3] "Both GPT-J-6B and OPT-6.7b perform better for non-binary pronouns as more examples are provided (up to 6, Table 13). However, this performance does not keep improving, and we see lower performance for 20 shots."

### Mechanism 3
- **Claim:** Explicit pronoun declaration improves accuracy by providing clear signal to the model.
- **Mechanism:** When pronouns are declared explicitly (e.g., "Aamari's pronouns are xe/xem..."), the model receives an unambiguous cue about the correct pronoun set, reducing ambiguity and reliance on name associations.
- **Core assumption:** The model can parse and use explicit pronoun declarations as a strong signal for subsequent pronoun prediction.
- **Evidence anchors:**
  - [abstract] "When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging 34.2% accuracy)."
  - [section 4.1] "Declaring pronouns explicitly is slightly better for correctly predicting neo-pronouns (from 6% accuracy to 9%)."

## Foundational Learning

- **Concept:** Pronoun forms (nominative, accusative, possessive-dependent, possessive-independent, reflexive)
  - Why needed here: The evaluation framework requires mapping each pronoun to its correct grammatical form, and models must learn these mappings.
  - Quick check question: Can you list all five pronoun forms and give an example for each?

- **Concept:** Zero-shot vs few-shot learning
  - Why needed here: The paper contrasts model performance with no examples (zero-shot) versus with examples in the prompt (few-shot), showing the limits of adaptation.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of this paper?

- **Concept:** Masked language models vs auto-regressive models
  - Why needed here: The evaluation uses both types, requiring different input/output formats and decoding strategies.
  - Quick check question: How do masked and auto-regressive models differ in their prediction approach for this task?

## Architecture Onboarding

- **Component map:** Dataset generation (templates + name/pronoun population) -> Evaluation framework (constrained decoding for both LM types) -> Model zoo (multiple model families at different scales) -> Analysis pipeline (accuracy by pronoun type, form, name gender, shot count)

- **Critical path:**
  1. Generate evaluation instances (templates + names + pronouns)
  2. Format inputs for each model type
  3. Run constrained decoding to predict pronouns
  4. Aggregate results by pronoun type, form, name gender
  5. Analyze effects of explicit vs parenthetical declaration and shot count

- **Design tradeoffs:**
  - Using templates ensures controlled evaluation but may not reflect natural usage.
  - Constrained decoding simplifies comparison but may not capture model uncertainty.
  - Evaluating multiple model families and sizes gives breadth but increases compute cost.

- **Failure signatures:**
  - Low accuracy across all pronouns: likely due to dataset or decoding issue
  - High accuracy for binary, low for non-binary: expected, confirms bias
  - Inconsistent scaling with model size: may indicate architecture-specific effects
  - Plateau in few-shot: suggests hard limit on adaptation

- **First 3 experiments:**
  1. Run zero-shot evaluation on all models with explicit pronoun declarations
  2. Compare accuracy by pronoun type and form to confirm expected biases
  3. Run 2- and 4-shot experiments on GPT-J and OPT to observe adaptation curve

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the precise mechanisms by which name-pronoun associations are learned in language models during pre-training?
- **Basis in paper:** [explicit] The paper shows evidence of memorized associations between pronouns and names, but does not explain the learning mechanisms.
- **Why unresolved:** Understanding the learning process would require detailed analysis of training data and model internals that goes beyond the scope of this paper.
- **What evidence would resolve it:** Analysis of attention patterns, gradient flows, or probing tasks that reveal how and when name-pronoun associations are formed during training.

### Open Question 2
- **Question:** Can we develop effective few-shot prompting strategies that consistently improve neo-pronoun prediction accuracy across different model families?
- **Basis in paper:** [explicit] The paper shows that few-shot learning plateaus at 45.4% accuracy for neo-pronouns, but does not explore alternative prompting strategies.
- **Why unresolved:** The paper only tests a limited set of example configurations and does not investigate the space of possible prompting strategies.
- **What evidence would resolve it:** Systematic comparison of different few-shot approaches (e.g., chain-of-thought, demonstrations vs. explanations) across multiple model families.

### Open Question 3
- **Question:** How do language models perform on non-binary pronouns in languages other than English?
- **Basis in paper:** [explicit] The paper explicitly states it is limited to English and does not evaluate cross-linguistic performance.
- **Why unresolved:** The paper's framework is English-specific and does not address how models handle non-binary pronouns in other languages.
- **What evidence would resolve it:** Extending the MISGENDERED framework to other languages and evaluating model performance on non-binary pronouns in those languages.

## Limitations
- Corpus representativeness: Analysis relies on counts from C4, Pile, and OpenWebText, which may not fully capture all instances of non-binary pronouns or reflect natural usage.
- Template-based evaluation: Fixed templates with specific pronoun declaration styles may not reflect diverse real-world contexts.
- Few-shot plateau interpretation: The cause of the plateau at 6 shots is not definitively established; alternative explanations are not ruled out.

## Confidence
- **High:** The core finding that large language models perform significantly worse on non-binary pronouns than binary pronouns in zero-shot settings, and that this is linked to corpus imbalance, is well-supported by the data and corpus analysis.
- **Medium:** The interpretation that few-shot learning plateaus at 6 shots due to model architecture limits is plausible but not conclusively proven; alternative explanations (e.g., example diversity, context richness) are not ruled out.
- **Medium:** The improvement from explicit pronoun declarations is demonstrated, but the magnitude and generalizability of this effect across different contexts or prompt styles is less certain.

## Next Checks
1. Conduct a detailed qualitative review of documents containing non-binary pronouns to understand how and why these tokens are used, and whether the corpus analysis captures true pronoun usage.
2. Expand the evaluation to include a wider variety of pronoun declaration styles and sentence contexts, to test whether the current templates and formats are limiting the models' performance.
3. Systematically vary the diversity and semantic richness of few-shot examples to determine whether the plateau at 6 shots is due to the number of examples or their quality/diversity.