---
ver: rpa2
title: Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data
arxiv_id: '2310.02541'
source_url: https://arxiv.org/abs/2310.02541
tags:
- have
- inequality
- lemma
- uses
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first theoretical analysis of benign overfitting
  and grokking in two-layer ReLU networks trained on XOR cluster data with label noise.
  The authors show that after one gradient descent step, the network perfectly fits
  the noisy training data but achieves near-random test accuracy.
---

# Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data

## Quick Facts
- arXiv ID: 2310.02541
- Source URL: https://arxiv.org/abs/2310.02541
- Reference count: 40
- Key outcome: First theoretical analysis showing ReLU networks exhibit benign overfitting and grokking on XOR cluster data, with catastrophic overfitting after one step followed by a transition to generalization

## Executive Summary
This paper provides the first theoretical analysis of benign overfitting and grokking in two-layer ReLU networks trained on XOR cluster data with label noise. The authors demonstrate that after one gradient descent step, the network achieves perfect training accuracy but near-random test accuracy. With continued training, the network undergoes a grokking transition where it maintains perfect training accuracy while achieving near-optimal test accuracy. The key insight is that early training behaves like a linear classifier that can overfit but not generalize for the XOR distribution, while later training learns generalizable features corresponding to the cluster means.

## Method Summary
The authors analyze two-layer ReLU networks trained on XOR cluster data using gradient descent with logistic loss. They study the dynamics after one step and for subsequent steps up to √n, showing that neurons gradually align with cluster means through the training process. The analysis relies on high-dimensional data geometry where training samples become near-orthogonal, enabling perfect fitting in early steps. The method tracks neuron weights and their alignment with cluster means to explain the transition from overfitting to generalization.

## Key Results
- After one gradient descent step, networks achieve 100% training accuracy but near-random test accuracy
- With continued training, networks undergo grokking transition maintaining perfect training accuracy while reaching near 100% test accuracy
- Positive neurons align with ±µ1 while negative neurons align with ±µ2, enabling generalization
- Results hold for high-dimensional data distributions that are not linearly separable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early in training, the network behaves like a linear classifier, enabling perfect training accuracy but near-random test accuracy
- Mechanism: After one gradient descent step, neurons approximately implement a linear decision boundary that can fit the high-dimensional training data but fails to capture the XOR structure
- Core assumption: Training data are near-orthogonal due to high dimensionality, allowing linear classifiers to achieve zero training error
- Evidence anchors:
  - [abstract] "after the first step of GD, the network achieves 100% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy"
  - [section] "the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps"
  - [corpus] "Weak evidence - neighboring papers focus on benign overfitting but don't specifically discuss the one-step catastrophic overfitting mechanism"

### Mechanism 2
- Claim: Gradual alignment of positive and negative neurons with cluster means enables generalization
- Mechanism: Positive neurons align with ±µ1 while negative neurons align with ±µ2, creating separable feature representations for the XOR structure
- Core assumption: Gradient descent dynamics cause neurons to align with the cluster means through the training process
- Evidence anchors:
  - [abstract] "the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps"
  - [section] "positive neurons' weights wj to align with ±µ1, and negative neurons' weights to align with ±µ2; we prove that this is satisfied for t ∈ [Cn0.01, √n]"
  - [corpus] "Strong evidence - multiple neighboring papers discuss directional convergence and alignment of neurons with cluster means"

### Mechanism 3
- Claim: The grokking transition occurs when neurons shift from fitting individual samples to learning cluster-based features
- Mechanism: Initial fitting focuses on individual sample weights, while later training emphasizes cluster mean alignment through accumulated gradient updates
- Core assumption: The optimization landscape naturally transitions from sample-level fitting to feature-level learning
- Evidence anchors:
  - [abstract] "after training for more steps, the network undergoes a 'grokking' period from catastrophic to benign overfitting—it eventually reaches near 100% test accuracy"
  - [section] "neurons gradually align with the core features ±µ1 and ±µ2, which is sufficient for generalization"
  - [corpus] "Moderate evidence - neighboring papers discuss grokking but don't specifically address the transition from sample to feature learning"

## Foundational Learning

- Concept: High-dimensional data geometry and near-orthogonality
  - Why needed here: Enables linear classifiers to achieve zero training error despite label noise
  - Quick check question: If p=100 and n=50, would you expect the training data to be near-orthogonal? Why or why not?

- Concept: Feature learning dynamics in neural networks
  - Why needed here: Explains how neurons gradually align with cluster means during training
  - Quick check question: What happens to neuron alignment if you increase the step size significantly?

- Concept: XOR distribution and its challenges for linear classifiers
  - Why needed here: The XOR structure creates two clusters per class, making linear separation impossible
  - Quick check question: Why can't a linear classifier achieve better than 50% accuracy on XOR data with two clusters per class?

## Architecture Onboarding

- Component map:
  - Input layer: High-dimensional data from XOR clusters
  - Hidden layer: ReLU neurons with learned weights
  - Output layer: Linear combination of neuron activations
  - Training: Gradient descent on logistic loss

- Critical path:
  1. Random initialization of weights
  2. First gradient step → catastrophic overfitting
  3. Continued training → neuron alignment with cluster means
  4. Feature learning → grokking transition
  5. Final state → benign overfitting

- Design tradeoffs:
  - High dimensionality enables near-orthogonality but increases computational cost
  - Two-layer architecture balances expressiveness and analytical tractability
  - ReLU activation enables sparse feature learning but complicates analysis

- Failure signatures:
  - No catastrophic overfitting: Training accuracy doesn't reach 100% in first step
  - No grokking: Test accuracy plateaus at random level
  - Premature convergence: Neurons align too quickly, missing the grokking transition

- First 3 experiments:
  1. Vary dimensionality p while keeping n fixed to test near-orthogonality threshold
  2. Adjust step size α to control timing of catastrophic overfitting and grokking
  3. Modify cluster separation to test robustness of feature learning dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the grokking phenomenon change when training continues beyond √n steps?
- Basis in paper: [explicit] The paper states their analysis requires an upper bound on iterations due to technical constraints, and leaves generalization for longer training times as future work.
- Why unresolved: The mathematical analysis relies on maintaining certain concentration properties that break down for larger t.
- What evidence would resolve it: Experimental results showing training accuracy, test accuracy, and feature alignment at t > √n steps.

### Open Question 2
- Question: How does the benign overfitting behavior change when the data dimension p is small relative to n?
- Basis in paper: [inferred] The paper mentions that prior experiments showed overfitting is less benign when dimension is small relative to samples, and asks for precise characterization of high-dimensional data effects.
- Why unresolved: The theoretical analysis relies on near-orthogonality of training data which requires p >> n.
- What evidence would resolve it: Comparative experiments with varying p/n ratios showing the transition point where benign overfitting breaks down.

### Open Question 3
- Question: What is the precise relationship between the signal-to-noise ratio (SNR) and the ability to achieve benign overfitting?
- Basis in paper: [explicit] Assumption (A1) requires ∥µ∥2 ≥ Cn0.51√p and mentions the order 0.51 can be extended to any constant strictly larger than 1/2.
- Why unresolved: The paper only establishes results for sufficiently high SNR but doesn't characterize the exact threshold or behavior near the threshold.
- What evidence would resolve it: Systematic experiments varying ∥µ∥ while keeping other parameters fixed to identify the minimum SNR for benign overfitting.

## Limitations
- Analysis is limited to specific time window [Cn^0.01, √n], behavior outside this range is less characterized
- Results depend on high-dimensional data assumptions that may not generalize to lower dimensions
- XOR cluster setting represents highly structured data that may not capture real-world complexity

## Confidence
- Neuron alignment with cluster means: High
- Catastrophic overfitting after one step: Medium
- Grokking transition mechanism: Medium

## Next Checks
1. Empirical validation of the near-orthogonality threshold by systematically varying p/n ratios and measuring training accuracy after one gradient step
2. Ablation study on step size α to determine its effect on the timing and completeness of the grokking transition
3. Extension of theoretical analysis beyond the time window [Cn^0.01, √n] to characterize early and late-time behavior