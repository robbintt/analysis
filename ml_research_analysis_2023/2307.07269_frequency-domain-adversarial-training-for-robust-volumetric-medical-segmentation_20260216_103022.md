---
ver: rpa2
title: Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation
arxiv_id: '2307.07269'
source_url: https://arxiv.org/abs/2307.07269
tags:
- adversarial
- attacks
- medical
- frequency
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a frequency-domain adversarial attack and training
  method for volumetric medical image segmentation. The key idea is to perturb the
  3D-DCT coefficients of the input data to launch attacks, which is shown to be more
  effective than conventional voxel-domain attacks.
---

# Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation

## Quick Facts
- arXiv ID: 2307.07269
- Source URL: https://arxiv.org/abs/2307.07269
- Authors: 
- Reference count: 34
- Key outcome: Proposes VAFA attack and VAFT training for volumetric medical segmentation, showing improved robustness against both voxel and frequency-domain attacks while maintaining clean sample performance

## Executive Summary
This paper introduces a novel frequency-domain adversarial attack and training framework for 3D medical image segmentation. The approach leverages 3D Discrete Cosine Transform (DCT) to perturb frequency coefficients directly, creating more effective adversarial examples than traditional voxel-domain attacks. The method is evaluated on two datasets (Synapse and ACDC) using UNETR and UNETR++ models, demonstrating superior fooling rates and improved robustness through frequency consistency regularization.

## Method Summary
The method consists of two components: VAFA (Volumetric Adversarial Frequency Attack) and VAFT (Volumetric Adversarial Frequency Training). VAFA generates adversarial examples by applying 3D DCT to input patches, quantizing DCT coefficients using a learnable quantization table, and reconstructing the adversarial sample via inverse 3D DCT. VAFT trains segmentation models using both clean and adversarial samples with a frequency consistency loss that encourages similar frequency-domain outputs between clean and adversarial inputs.

## Key Results
- VAFA attack achieves higher fooling rates compared to voxel-domain attacks (PGD/FGSM) while maintaining comparable perceptual similarity
- VAFT training improves robustness against both voxel-domain and frequency-domain attacks
- Frequency consistency loss improves performance on clean samples while maintaining adversarial robustness
- Model retains or improves performance on clean samples after VAFT training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-domain perturbations are more effective than voxel-domain perturbations for volumetric medical image segmentation.
- Mechanism: The attack perturbs 3D DCT coefficients directly, disrupting frequency components that the model relies on for segmentation, bypassing defenses optimized for voxel-space noise.
- Core assumption: The segmentation model's decision-making process depends significantly on frequency-domain features of the volumetric data.
- Evidence anchors:
  - [abstract] "demonstrate its advantages over conventional input or voxel domain attacks"
  - [section 2.1] "our attack perturbs the 3D-DCT coefficient to launch a frequency-domain attack for 3D medical image segmentation"
  - [corpus] Weak - corpus papers focus on 2D frequency attacks, not 3D volumetric cases
- Break condition: If the model's feature extraction does not rely heavily on frequency information, or if frequency consistency loss effectively regularizes against frequency perturbations.

### Mechanism 2
- Claim: Frequency consistency loss improves robustness on clean samples while defending against adversarial attacks.
- Mechanism: The loss encourages the model to produce similar frequency-domain outputs for clean and adversarial inputs, stabilizing predictions across both domains.
- Core assumption: Maintaining frequency consistency between clean and adversarial outputs preserves useful signal while reducing attack effectiveness.
- Evidence anchors:
  - [abstract] "introduce a novel frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples"
  - [section 2.2] "we develop a novel frequency consistency loss... to encourage frequency domain representation of the model's output... close to the adversarial sample"
  - [corpus] Weak - no direct corpus evidence of frequency consistency loss in medical segmentation
- Break condition: If frequency consistency prevents the model from learning useful frequency-domain features necessary for segmentation.

### Mechanism 3
- Claim: 3D adversarial training in frequency domain creates a model robust to both voxel and frequency attacks.
- Mechanism: The adversarial training process explicitly optimizes against frequency-domain perturbations, creating invariance to attacks in both domains.
- Core assumption: Training on frequency-domain adversaries generalizes to robustness against voxel-domain attacks.
- Evidence anchors:
  - [abstract] "demonstrate improved robustness against both voxel and frequency-domain attacks while retaining or improving performance on clean samples"
  - [section 2.2] "we propose updating the model parameters by simultaneously minimizing the segmentation loss on both clean and adversarial samples"
  - [corpus] Moderate - some corpus papers on adversarial robustness mention domain transfer, but not specifically for 3D medical segmentation
- Break condition: If voxel-domain attacks exploit features that frequency-domain training does not cover.

## Foundational Learning

- Concept: 3D Discrete Cosine Transform (DCT) and its inverse
  - Why needed here: The attack and training rely on transforming volumetric data between voxel and frequency domains using 3D DCT
  - Quick check question: What is the mathematical relationship between spatial voxel values and their frequency-domain DCT coefficients?

- Concept: Adversarial training framework (min-max optimization)
  - Why needed here: The approach uses adversarial training to optimize the model against frequency-domain perturbations
  - Quick check question: How does the min-max optimization framework balance between attack effectiveness and model robustness?

- Concept: Structural Similarity Index (SSIM) for perceptual quality
  - Why needed here: SSIM loss is used to maintain perceptual similarity between clean and adversarial samples
  - Quick check question: How does SSIM differ from simple pixel-wise L2 distance in measuring perceptual similarity?

## Architecture Onboarding

- Component map: Input (3D volume) -> 3D DCT -> Quantization -> IDCT -> Adversarial Sample -> Segmentation Model -> Output (segmentation mask)

- Critical path:
  1. Apply 3D DCT to input patches
  2. Quantize DCT coefficients using learnable quantization table
  3. Apply inverse 3D DCT to obtain adversarial sample
  4. Compute losses (Dice, SSIM, frequency consistency)
  5. Update quantization table and model parameters

- Design tradeoffs:
  - Higher quantization threshold (qmax) increases attack strength but reduces perceptual quality
  - Larger patch size reduces computational cost but may miss fine-grained frequency features
  - Frequency consistency loss improves clean sample performance but may reduce attack strength

- Failure signatures:
  - Model performance degrades significantly on clean samples (over-regularization)
  - Adversarial samples become visibly distorted (excessive quantization)
  - Training becomes unstable due to conflicting loss objectives

- First 3 experiments:
  1. Implement VAFA attack with different qmax values on UNETR model and measure DSC/HD95/LPIPS
  2. Train VAFT model with and without frequency consistency loss and compare performance on clean vs. adversarial samples
  3. Test robustness of VAFT-trained model against both voxel-domain (PGD/FGSM) and frequency-domain (VAFA) attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VAFA compare to other frequency-domain attacks on volumetric medical data?
- Basis in paper: [explicit] The paper states that VAFA achieves a higher fooling rate compared to other voxel-domain attacks while maintaining comparable perceptual similarity. However, it does not provide a direct comparison with other frequency-domain attacks specifically designed for volumetric medical data.
- Why unresolved: The paper focuses on comparing VAFA with voxel-domain attacks and does not explore its performance against other frequency-domain attacks specifically designed for volumetric medical data.
- What evidence would resolve it: Conducting experiments comparing VAFA with other frequency-domain attacks on volumetric medical data and reporting their performance metrics (e.g., fooling rate, perceptual similarity) would provide insights into VAFA's relative effectiveness.

### Open Question 2
- Question: How does the choice of quantization threshold (qmax) affect the trade-off between fooling rate and perceptual similarity in VAFA?
- Basis in paper: [explicit] The paper mentions that higher values of qmax increase the perceptual quality of adversarial samples but also lead to a drop in the model's accuracy. However, it does not provide a detailed analysis of the trade-off between fooling rate and perceptual similarity for different values of qmax.
- Why unresolved: The paper does not explore the relationship between qmax and the balance between fooling rate and perceptual similarity in depth.
- What evidence would resolve it: Conducting experiments with different values of qmax and analyzing the corresponding fooling rates and perceptual similarity scores would help understand the trade-off and determine an optimal value for qmax.

### Open Question 3
- Question: How does the patch size affect the performance of VAFA on volumetric medical data?
- Basis in paper: [explicit] The paper mentions that the patch size affects the performance of VAFA, but it does not provide a detailed analysis of the impact of different patch sizes on the fooling rate and perceptual similarity.
- Why unresolved: The paper does not explore the relationship between patch size and the effectiveness of VAFA in depth.
- What evidence would resolve it: Conducting experiments with different patch sizes and analyzing the corresponding fooling rates and perceptual similarity scores would help understand the impact of patch size on VAFA's performance.

## Limitations
- The method requires 3D-DCT computation which is computationally expensive compared to voxel-domain attacks
- The frequency consistency loss, while improving clean sample performance, may limit the attack strength
- The evaluation focuses on two specific datasets and models, leaving uncertainty about performance across different anatomical structures and segmentation architectures

## Confidence
- **High confidence**: The VAFA attack generates more effective adversarial examples than voxel-domain attacks (supported by quantitative DSC/HD95/LPIPS metrics)
- **Medium confidence**: Frequency consistency loss improves the robustness-accuracy tradeoff (some evidence from results but limited ablation studies)
- **Low confidence**: The approach generalizes to all types of volumetric medical segmentation tasks (limited dataset and model diversity in evaluation)

## Next Checks
1. Test VAFT robustness against transfer attacks from different model architectures not seen during training
2. Evaluate computational overhead of 3D-DCT operations in real-time clinical deployment scenarios
3. Assess whether frequency-domain training improves robustness to common corruptions (noise, blur, compression) beyond adversarial attacks