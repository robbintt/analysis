---
ver: rpa2
title: Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate
  Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens
arxiv_id: '2307.06970'
source_url: https://arxiv.org/abs/2307.06970
tags:
- classification
- algorithms
- machine
- decision
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study applied supervised machine learning classification
  algorithms to estimate the Ultimate Tensile Strength (UTS) of FDM-fabricated PLA
  specimens using Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature
  as inputs. Four algorithms were compared: Logistic Classification, Gradient Boosting
  Classification, Decision Tree, and K-Nearest Neighbor.'
---

# Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens

## Quick Facts
- arXiv ID: 2307.06970
- Source URL: https://arxiv.org/abs/2307.06970
- Reference count: 24
- Primary result: KNN algorithm achieved F1 score of 0.71 and AUC of 0.79 for classifying UTS strength classes

## Executive Summary
This study applied supervised machine learning classification algorithms to estimate Ultimate Tensile Strength (UTS) of FDM-fabricated PLA specimens using four process parameters as inputs. The research compared four algorithms - Logistic Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest Neighbor - on a dataset of 31 specimens. The K-Nearest Neighbor algorithm demonstrated superior performance with an F1 score of 0.71 and AUC score of 0.79, making it the most effective algorithm for classifying UTS strength levels based on the given process parameters.

## Method Summary
The study tested four supervised machine learning classification algorithms to predict UTS classes (above/below 80% of base material) using four input parameters: Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature. The dataset consisted of 31 PLA specimens. The algorithms tested included Logistic Classification, Gradient Boosting Classification, Decision Tree (with depth fixed at 3), and K-Nearest Neighbor. Performance was evaluated using F1 score and AUC-ROC metrics. The Decision Tree algorithm was implemented with a fixed depth of 3 without systematic hyperparameter optimization.

## Key Results
- K-Nearest Neighbor algorithm achieved the best performance with F1 score of 0.71 and AUC score of 0.79
- Decision Tree Classification algorithm showed the lowest performance with F1 score of 0.43
- Logistic Classification and K-Nearest Neighbors both achieved F1 scores of 0.71

## Why This Works (Mechanism)

### Mechanism 1
- Claim: K-Nearest Neighbors (KNN) outperforms other classifiers because it captures local similarity patterns in the parameter space that are predictive of UTS classes.
- Mechanism: KNN assigns a class based on the majority vote of the K closest training samples in feature space, making it sensitive to local clusters of similar infill percentage, layer height, print speed, and extrusion temperature combinations.
- Core assumption: The UTS classes are not linearly separable but exhibit local clustering that KNN can detect.
- Evidence anchors:
  - [abstract] "The K-Nearest Neighbor algorithm achieved the best performance with an F1 score of 0.71 and AUC score of 0.79"
  - [section] "KNN algorithm possesses a superior AUC score, it can be deduced that KNN is the more optimal algorithm for classification"
- Break condition: If the data distribution becomes highly noisy or the optimal K value is very large relative to dataset size, KNN performance degrades.

### Mechanism 2
- Claim: Decision Tree's lower performance stems from overfitting on the small dataset (n=31) and inability to capture non-axis-aligned decision boundaries.
- Mechanism: Decision trees create splits based on single-feature thresholds, which can create complex boundaries that fit noise rather than underlying patterns in small datasets.
- Core assumption: The true decision boundary between UTS classes requires combinations of parameters that are not aligned with individual feature thresholds.
- Evidence anchors:
  - [section] "Decision Tree Classification algorithm demonstrated the lowest performance among the tested algorithms, with an F1 score of 0.4286"
  - [section] "This result indicates that the Decision Tree classifier's ability to accurately classify the material properties based on the given dataset was comparatively limited"
- Break condition: When dataset size increases significantly, Decision Tree may improve if appropriate pruning is applied.

### Mechanism 3
- Claim: Logistic Regression performs moderately because it assumes linear separability between classes, which partially holds but not completely for this UTS prediction problem.
- Mechanism: Logistic Regression models the log-odds of class membership as a linear combination of input features, providing reasonable performance when classes show some linear trends.
- Core assumption: The relationship between print parameters and UTS classes has some linear components but also non-linear aspects that Logistic Regression cannot capture.
- Evidence anchors:
  - [section] "Logistic Classification and K-Nearest Neighbors (KNN) Classification performed similarly, both achieving F1 scores of 0.7143"
  - [section] "Logistic Regression models the log-odds of class membership as a linear combination of input features"
- Break condition: If the true decision boundary is highly non-linear, Logistic Regression's performance will plateau regardless of regularization.

## Foundational Learning

- Concept: Supervised classification algorithms
  - Why needed here: The study aims to predict discrete UTS classes (above/below 80% of base material) from continuous print parameters
  - Quick check question: What is the difference between classification and regression in machine learning?

- Concept: Evaluation metrics (F1 score, AUC-ROC)
  - Why needed here: These metrics provide balanced assessment of classifier performance, especially important for potentially imbalanced classes
  - Quick check question: Why is F1 score preferred over accuracy when dealing with imbalanced datasets?

- Concept: Feature scaling and distance metrics
  - Why needed here: KNN relies on distance calculations between samples, making feature scaling critical for fair comparison
  - Quick check question: How does the choice of distance metric (Euclidean vs Manhattan) affect KNN performance?

## Architecture Onboarding

- Component map: CSV loading -> feature extraction (infill %, layer height, print speed, extrusion temp) -> label encoding (UTS class) -> four parallel classification algorithms -> metric calculation (F1, AUC-ROC) -> result comparison
- Critical path: Data preprocessing -> model training -> metric calculation -> result interpretation
- Design tradeoffs:
  - Small dataset (n=31) favors simpler models over complex ones
  - Binary classification simplifies the problem but may lose granularity
  - KNN provides good performance but is computationally expensive at prediction time
- Failure signatures:
  - Low F1 scores across all models indicate fundamental data issues
  - Large variance in cross-validation scores suggests overfitting
  - Perfect classification (F1=1.0) may indicate data leakage or insufficient variability
- First 3 experiments:
  1. Test KNN with different K values (1, 3, 5, 7) to find optimal neighbor count
  2. Implement cross-validation to assess model stability on small dataset
  3. Compare performance when using all four features vs. individual feature subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the classification performance change with a larger and more diverse dataset of FDM-fabricated PLA specimens?
- Basis in paper: [explicit] The study used 31 specimens and acknowledged this as a limitation in generalizability
- Why unresolved: Limited sample size may not capture the full variability in FDM parameters and resulting UTS values
- What evidence would resolve it: Testing the same algorithms on a dataset with hundreds of specimens varying across broader ranges of infill percentage, layer height, print speed, and extrusion temperature

### Open Question 2
- Question: Would incorporating additional material properties (e.g., layer adhesion strength, crystallinity, or thermal history) improve the predictive accuracy of the machine learning models?
- Basis in paper: [inferred] The current model uses only four process parameters, suggesting potential for additional predictive features
- Why unresolved: The study focused on a minimal set of parameters without exploring other potentially influential material characteristics
- What evidence would resolve it: Experiments comparing model performance with and without additional material property features, using the same classification algorithms

### Open Question 3
- Question: How would the algorithms perform when predicting continuous UTS values rather than binary classification of strength classes?
- Basis in paper: [explicit] The study used binary classification (above/below 80% of base material UTS) rather than regression
- Why unresolved: Classification may lose information about the magnitude of strength differences between specimens
- What evidence would resolve it: Implementation and comparison of regression algorithms (e.g., Random Forest Regression, Support Vector Regression) on the same dataset

## Limitations

- Small sample size (n=31) limits statistical power and generalizability
- Binary classification approach may oversimplify continuous nature of tensile strength variations
- Limited hyperparameter tuning and no cross-validation raises concerns about overfitting

## Confidence

- Confidence: Medium
- Basis: Small dataset with limited sample size and lack of comprehensive validation procedures

## Next Checks

1. Perform 5-fold cross-validation to assess model stability and prevent overfitting on the small dataset
2. Test additional algorithms including Random Forest and Support Vector Machines to verify KNN's superiority
3. Conduct sensitivity analysis on the 80% threshold to determine its impact on classification performance