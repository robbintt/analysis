---
ver: rpa2
title: Pay Attention to What You Need
arxiv_id: '2307.13365'
source_url: https://arxiv.org/abs/2307.13365
tags:
- attention
- weights
- context
- transition
- comprehension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Scaled ReAttention (SRA) to
  enhance large language models' (LLMs) long-context comprehension without additional
  resources or training. The method strategically manipulates attention scores during
  inference to strengthen the models' ability to interpret and retrieve information.
---

# Pay Attention to What You Need

## Quick Facts
- arXiv ID: 2307.13365
- Source URL: https://arxiv.org/abs/2307.13365
- Reference count: 22
- Enhances LLMs' long-context comprehension through inference-time attention manipulation without training

## Executive Summary
This paper proposes Scaled ReAttention (SRA), a method to enhance large language models' long-context comprehension without additional resources or training. The approach strategically manipulates attention scores during inference by eliminating less important weights and redistributing them to strengthen connections between distant tokens. Experiments on XSum demonstrate significant performance improvements across various downstream tasks, showcasing the practical potential for enhancing language understanding without the overhead of traditional training approaches.

## Method Summary
The Scaled ReAttention (SRA) method manipulates attention scores during inference to enhance long-context comprehension. It identifies and eliminates small attention weights (below threshold like 0.1-0.15) and redistributes them to more important connections, particularly between distant tokens. This compensates for the "Long-Term Decay" property of rotary embeddings. The method uses a Decision Maker to determine which intervals and layers to apply attention transition, and a Dispenser to execute the elimination and redistribution of attention weights.

## Key Results
- Significant performance boost on XSum summarization across multiple token lengths (800-1900 tokens)
- Demonstrates effectiveness without requiring additional training or model resources
- Shows that strategic attention manipulation during inference can enhance comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SRA enhances long-context comprehension by strategically manipulating attention scores during inference to strengthen information retrieval.
- Mechanism: Identifies and eliminates less important attention weights (below threshold like 0.1 or 0.15) and redistributes them to more important connections, particularly those linking distant tokens. This compensates for the "Long-Term Decay" property of rotary embeddings.
- Core assumption: Small attention weights contribute minimally to information flow, and removing them while redistributing the mass does not degrade model performance.
- Evidence anchors: [abstract]: "strategically manipulating their attention scores during inference"; [section]: "it's reasonable to eliminate smaller attention weights and rescale them into 1"
- Break condition: If the threshold for elimination is too aggressive, or if critical information happens to have low weights due to token length, performance will degrade.

### Mechanism 2
- Claim: Attention Transition improves context retrieval by augmenting the connection strength between distant tokens and recent tokens.
- Mechanism: The Dispenser component selectively increases attention weights between intervals of tokens (e.g., from a later interval to an earlier one), bypassing the gradual decay enforced by rotary embeddings. This is done in a staged manner across layers to avoid excessive disturbance.
- Core assumption: The model's robustness allows it to tolerate some disturbance in attention distributions without catastrophic performance loss.
- Evidence anchors: [section]: "To facilitate the transfer of important information to further distance... our objective is to surpass the limitations imposed by rotary embedding"; [section]: "it's unnecessary to accomplish the augmentation in one fell swoop"
- Break condition: Over-augmentation in too many intervals or layers leads to repeating, unreasoning, or chaotic outputs.

### Mechanism 3
- Claim: SRA is effective because LLMs are trained to be robust to small perturbations in attention distributions.
- Mechanism: The model can absorb changes in attention weight distributions without significant changes in output logits or perplexity, allowing manual intervention without retraining.
- Core assumption: The learned representations are sufficiently stable that targeted changes in attention do not propagate into major output shifts.
- Evidence anchors: [section]: "LLM itself is robust enough to persist the information with disturbance"; [section]: "we can eliminate most of the attention weights and the outputs will not change at all"
- Break condition: If perturbation magnitude exceeds a certain threshold (e.g., eliminating >0.2 of weights), the square difference in logits increases exponentially, causing output degradation.

## Foundational Learning

- Concept: Rotary Embeddings and their "Long-Term Decay" property
  - Why needed here: Understanding why distant tokens naturally receive less attention is crucial to designing SRA's redistribution strategy.
  - Quick check question: In rotary embeddings, how does the upper bound of attention weights change as relative distance increases?

- Concept: Self-Attention Mechanism and Multi-Head Attention
  - Why needed here: SRA operates on attention scores; understanding how Q, K, V matrices and softmax weighting work is essential.
  - Quick check question: In the attention formula, what role does the scaling factor √dk play?

- Concept: Inference-time vs. Training-time Model Modification
  - Why needed here: SRA is applied during inference without retraining, which is a key design constraint and benefit.
  - Quick check question: What is the primary advantage of modifying attention during inference rather than retraining the model?

## Architecture Onboarding

- Component map: Decision Maker -> Dispenser -> Attention Weights Tensor (multi-head)
- Critical path:
  1. Input sequence processed through LLaMA layers.
  2. At selected layers/intervals, attention weights are modified by Dispenser.
  3. Modified attention weights are used in subsequent computations.
  4. Final logits passed through LM-Head for output.
- Design tradeoffs:
  - Aggressive elimination improves redistribution but risks information loss.
  - Applying to more layers increases effect but risks instability.
  - Larger redistribution multipliers (β) strengthen effect but increase perplexity.
- Failure signatures:
  - Repeating or circular text generation.
  - Unreasoning or hallucinated content.
  - Completely chaotic or off-topic outputs.
- First 3 experiments:
  1. Apply SRA with α=0.5, β=0.1, lay=2 on 800-token sequence; measure output quality vs. baseline.
  2. Vary β from 0.1 to 0.5 while holding other parameters constant; observe stability.
  3. Apply SRA to only the first interval in each layer; compare with full-interval application.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Attention Transition scale with different model sizes beyond LLaMA-7b, and are there diminishing returns for larger models?
- Basis in paper: [explicit] The paper states that "For LLMs with larger sizes, our technique is also useful, in that no matter the model size, long context comprehension is defined relatively, for example, 10k context with 30 billion parameters."
- Why unresolved: The paper only briefly mentions larger models without providing detailed experimental results or analysis of performance scaling.
- What evidence would resolve it: Comprehensive experiments testing Attention Transition across a range of model sizes (e.g., LLaMA-13B, LLaMA-30B, GPT-3 variants) with varying context lengths, accompanied by quantitative performance comparisons.

### Open Question 2
- Question: What is the theoretical limit of context length improvement achievable through Attention Transition, and how does this limit vary across different attention mechanisms and model architectures?
- Basis in paper: [inferred] The paper mentions limitations with very long contexts (e.g., 2400 tokens) and states that the technique cannot "break through the inherent limitations of the model itself," but doesn't provide a detailed analysis of these boundaries.
- Why unresolved: The paper identifies practical limits but doesn't explore the theoretical underpinnings of these constraints or compare them across different architectural choices.
- What evidence would resolve it: Mathematical analysis of attention mechanism capacity, ablation studies with different attention variants (sparse attention, linear attention), and systematic testing of context length limits across multiple architectures.

### Open Question 3
- Question: How does Attention Transition affect the model's ability to maintain coherence and factual consistency in multi-turn conversations or document-level understanding tasks?
- Basis in paper: [inferred] While the paper demonstrates improvements in summarization tasks, it doesn't investigate the technique's impact on maintaining long-term coherence or factual consistency across extended interactions.
- Why unresolved: The experimental setup focuses on single-document summarization without examining the technique's effects on dialogue systems or document-level reasoning tasks.
- What evidence would resolve it: Evaluation on dialogue datasets, document-level QA tasks, and coherence metrics across multiple turns, comparing models with and without Attention Transition.

## Limitations
- Limited testing across different model sizes beyond LLaMA-7b
- Unclear parameter selection process for different token lengths
- No evaluation on tasks beyond summarization

## Confidence

**High Confidence**: The core observation that rotary embeddings cause long-term decay in attention weights is well-established in the literature, and the mathematical formulation of attention score manipulation is sound.

**Medium Confidence**: The effectiveness of the staged approach is supported by experimental results, but the exact sensitivity to parameter choices could benefit from more extensive ablation studies.

**Low Confidence**: The specific parameter selection process for different token lengths is not fully explained, making it difficult to assess whether the reported improvements are optimal.

## Next Checks

1. **Robustness Boundary Analysis**: Systematically vary the attention weight elimination threshold from 0.05 to 0.3 in increments of 0.05 and measure the point at which output quality (measured by GPT-4 scoring) begins to degrade significantly.

2. **Cross-Domain Generalization Test**: Apply SRA to at least two additional tasks beyond summarization, such as question answering and long-form generation, using different datasets. Compare performance gains across tasks to assess whether the method generalizes beyond summarization.

3. **Parameter Sensitivity Sweep**: Conduct a comprehensive grid search over the three key parameters (α ∈ [0.3, 0.5, 0.7], β ∈ [0.05, 0.1, 0.2, 0.3], lay ∈ [1, 2, 3]) for each token length to identify whether the reported parameter settings represent global optima or local maxima.