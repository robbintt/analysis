---
ver: rpa2
title: 'Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning
  of Code Data'
arxiv_id: '2312.02418'
source_url: https://arxiv.org/abs/2312.02418
tags:
- data
- code
- pruning
- cluster
- low-quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SCIP, a method for improving code generation
  model performance by pruning low-quality data from large code datasets. SCIP uses
  synthetic corruption to understand how low-quality code behaves in embedding space,
  then prunes data that exhibits similar characteristics.
---

# Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data

## Quick Facts
- **arXiv ID**: 2312.02418
- **Source URL**: https://arxiv.org/abs/2312.02418
- **Reference count**: 40
- **Primary result**: 3% improvement on HumanEval using synthetic corruption-based pruning

## Executive Summary
This paper introduces SCIP (Synthetic Corruption for Improved Pruning), a method for improving code generation model performance by pruning low-quality data from large code datasets. SCIP uses synthetic corruption to understand how low-quality code behaves in embedding space, then prunes data that exhibits similar characteristics. By clustering code embeddings and removing data in small clusters or far from centroids, SCIP improves HumanEval performance by up to 3% compared to no pruning, outperforming prior embedding-based pruning methods. This approach is more accessible and open-source than methods relying on proprietary models for quality assessment.

## Method Summary
SCIP improves code generation model performance by pruning low-quality data using synthetic corruption analysis in embedding space. The method first clusters code embeddings using k-means (k=100) with cosine similarity. It then generates synthetic corruptions (syntax and content errors) to understand how low-quality code behaves in embedding space. Based on this analysis, SCIP prunes data from small clusters and data far from cluster centroids, using a weighted combination of these two metrics. The pruned dataset is then used to fine-tune a 1.5B LLaMA model, achieving up to 3% improvement on HumanEval compared to no pruning.

## Key Results
- 3% improvement on HumanEval benchmark using SCIP pruning
- Outperforms prior embedding-based pruning methods like SSL-prototypes, SemDeDup, and D4
- Demonstrates effectiveness on both HumanEval and MBPP benchmarks
- Uses open-source StarEncoder embeddings instead of proprietary models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Low-quality code data moves to smaller clusters and farther from cluster centroids in embedding space after synthetic corruption.
- **Core assumption**: Synthetically corrupted code embeddings reliably mimic the spatial properties of naturally occurring low-quality code.
- **Evidence**: Corruption moves points to smaller clusters or further out from centroids.
- **Break condition**: If synthetic corruptions do not accurately model real low-quality code patterns, pruning would remove useful data and degrade performance.

### Mechanism 2
- **Claim**: Pruning data from small clusters and far from centroids improves HumanEval performance by up to 3%.
- **Core assumption**: The removed data points are predominantly low-quality and their removal does not eliminate important diverse examples.
- **Evidence**: Up to 3% performance improvement over no pruning.
- **Break condition**: If pruned data contains valuable diverse examples, removing them could harm model generalization.

### Mechanism 3
- **Claim**: SCIP is more accessible and open-source than methods relying on proprietary models for quality assessment.
- **Core assumption**: Synthetic corruption and open-source embeddings provide sufficient signal for quality assessment without proprietary models.
- **Evidence**: Uses StarEncoder embeddings instead of GPT-3.5/GPT-4.
- **Break condition**: If open-source embeddings and synthetic corruption prove insufficient for accurate quality assessment, the accessibility advantage becomes irrelevant.

## Foundational Learning

- **Concept**: Understanding of code embedding spaces and their properties
  - **Why needed**: Method relies on clustering code embeddings and using spatial properties (cluster size, distance to centroid) to identify low-quality data
  - **Quick check**: How does StarEncoder represent code snippets in embedding space, and what properties are used for clustering?

- **Concept**: K-means clustering algorithm and its application to high-dimensional data
  - **Why needed**: Method uses k-means clustering with cosine similarity to group code embeddings into clusters
  - **Quick check**: What parameters (e.g., number of clusters K) are critical for clustering step, and how do they affect pruning outcome?

- **Concept**: Synthetic data generation and its role in understanding data quality
  - **Why needed**: Method creates synthetic corruptions to understand how low-quality code behaves in embedding space
  - **Quick check**: What types of synthetic corruptions are applied, and how do they simulate different forms of low-quality code?

## Architecture Onboarding

- **Component map**: StarEncoder (embedding generation) → K-means clustering → Synthetic corruption analysis → Pruning metric calculation → Dataset pruning → Model training
- **Critical path**: Embedding generation → Clustering → Synthetic corruption analysis → Pruning metric calculation → Dataset pruning → Model training
- **Design tradeoffs**:
  - Number of clusters (K=100) vs. computational cost and granularity of pruning
  - Synthetic corruption types vs. representativeness of low-quality code patterns
  - α parameter (interpolation between cluster size and distance pruning) vs. benchmark performance
- **Failure signatures**:
  - Performance degradation indicates over-aggressive pruning or unrepresentative synthetic corruptions
  - No improvement suggests pruning metric is not capturing meaningful quality differences
  - High variance indicates instability in clustering or pruning process
- **First 3 experiments**:
  1. Verify synthetic corruption moves embeddings to smaller clusters/farther from centroids
  2. Test different values of α to find optimal interpolation between cluster size and distance pruning
  3. Compare SCIP against baseline pruning methods on HumanEval and MBPP

## Open Questions the Paper Calls Out

- **Open Question 1**: How would SCIP perform on code datasets in languages other than Python?
  - **Basis**: Paper only evaluates SCIP on Python code, but mentions Stack dataset includes 358 different programming languages
  - **What evidence would resolve**: Running SCIP on code datasets in other popular languages like JavaScript, Java, or C++ and comparing performance to Python

- **Open Question 2**: What is the optimal percentage of data to prune using SCIP for maximum performance improvement?
  - **Basis**: Paper uses fixed 20% pruning rate, but mentions experimenting with different values of α
  - **What evidence would resolve**: Systematically varying pruning rate and α values, then measuring performance on benchmarks

- **Open Question 3**: How does SCIP compare to supervised pruning methods that use model predictions (e.g., GPT-4) for data quality assessment?
  - **Basis**: Authors mention prior study using GPT-3.5 and GPT-4 predictions for pruning, noting SCIP is more accessible
  - **What evidence would resolve**: Implementing supervised pruning method on same dataset and comparing performance to SCIP

## Limitations

- Method's effectiveness depends heavily on assumption that synthetic corruption patterns accurately represent real low-quality code distributions
- Clustering approach with k=100 is relatively coarse and may miss nuanced quality distinctions
- Synthetic corruption types limited to syntax and content errors, potentially missing other important forms of code quality issues

## Confidence

- **Mechanism 1 (Synthetic corruption behavior)**: Medium confidence
- **Mechanism 2 (3% performance improvement)**: Medium confidence
- **Mechanism 3 (Accessibility advantage)**: High confidence

## Next Checks

1. Apply SCIP to a different code dataset (e.g., CodeSearchNet or CodeContests) and evaluate whether similar performance improvements are observed across different model sizes and architectures.

2. Systematically vary the α parameter (currently 0.8/0.2) and the number of clusters (currently k=100) to determine their impact on pruning effectiveness and model performance.

3. Manually examine code samples removed by SCIP to verify they represent low-quality code, and analyze samples that were incorrectly pruned to identify failure modes of the method.