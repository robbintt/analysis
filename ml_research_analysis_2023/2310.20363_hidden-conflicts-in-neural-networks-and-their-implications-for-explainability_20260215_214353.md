---
ver: rpa2
title: Hidden Conflicts in Neural Networks and Their Implications for Explainability
arxiv_id: '2310.20363'
source_url: https://arxiv.org/abs/2310.20363
tags:
- attribution
- cafe
- scores
- input
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CAFE (Conflict-Aware Feature-wise Explanations),
  a novel feature attribution method for neural networks that addresses three key
  limitations of existing approaches: disregard for conflicting features, lack of
  consideration for bias terms, and excessive sensitivity to activation function variations.
  CAFE provides safeguards against overestimating feature effects and separately traces
  positive and negative influences of inputs and biases, enabling more faithful explanations.'
---

# Hidden Conflicts in Neural Networks and Their Implications for Explainability

## Quick Facts
- arXiv ID: 2310.20363
- Source URL: https://arxiv.org/abs/2310.20363
- Reference count: 40
- Introduces CAFE, a novel feature attribution method that addresses conflicting features, bias terms, and activation function sensitivity

## Executive Summary
This paper introduces CAFE (Conflict-Aware Feature-wise Explanations), a novel feature attribution method for neural networks that addresses three key limitations of existing approaches. CAFE separately traces positive and negative influences of inputs and biases, provides safeguards against overestimating feature effects, and maintains computational efficiency comparable to gradient-based methods. Theoretical analysis proves that CAFE satisfies desirable properties including missingness, linearity, and completeness.

## Method Summary
CAFE uses a single forward pass through the network to compute separate positive and negative attribution scores for each feature and bias term. The method implements three core rules: an input layer rule that computes initial differences from a reference input, a linear layer rule that propagates scores using weighted sums while separating positive and negative contributions, and an activation rule that computes input effects, rectified activation deltas, peak flows, and clipped linear flows to prevent attribution score explosion.

## Key Results
- CAFE with higher conflict sensitivity constants consistently outperforms baselines in identifying correct attribution scores on synthetic data with conflicting features
- Variants of CAFE achieve the best overall fidelity compared to gradient-based methods, sampling-based approaches, and other baselines on real-world datasets
- CAFE maintains computational efficiency comparable to the fastest gradient-based methods while providing more reliable and insightful explanations

## Why This Works (Mechanism)

### Mechanism 1
- CAFE surfaces conflicting features by tracking both positive and negative input effects and propagating them separately through activation layers
- Computes positive input effect (sum of positive scores) and negative input effect (sum of negative scores), using these to derive rectified activation deltas
- Assumes activation function behavior over combined input intervals can be approximated by rectified deltas from positive/negative effects separately

### Mechanism 2
- CAFE prevents attribution score explosion by clipping linear approximations of activation functions to peak attribution flows
- Computes linear attribution flows as slope between reference and current activation, comparing to peak flows and taking minimum
- Assumes linear approximation slope provides reasonable estimate unless it grossly overestimates, in which case peak flows bound it

### Mechanism 3
- CAFE accounts for bias terms by computing separate attribution scores for input features and biases
- In linear layer rule, bias scores are propagated separately using max(σ(bias), 0) for positive and max(σ(-bias), 0) for negative
- Assumes bias effects should be quantified independently from input features to avoid masking or distorting feature attributions

## Foundational Learning

- **Activation function behavior over intervals**: Understanding how activation functions change over intervals defined by reference and current pre-activations is crucial for CAFE's rectified activation deltas
  - Quick check: Given ReLU activation and pre-activation values of -1, 0, and 1, what are the rectified deltas for positive and negative slopes between these points?

- **Attribution score propagation through layers**: CAFE computes scores layer-by-layer using modified forward passes that track positive/negative contributions separately
  - Quick check: In a linear layer with weights [2, -1] and input scores [1, -1], what are the output scores if we only track positive contributions?

- **Reference input selection and its impact**: CAFE computes scores relative to a reference input, affecting what changes are attributed to features
  - Quick check: If reference input is zero and actual input is [1, 2], what are the initial attribution scores for each feature?

## Architecture Onboarding

- **Component map**: Input layer rule -> Linear layer rule (forward pass with modified weights) -> Activation rule (complex rule application) -> Final aggregation
- **Critical path**: Input layer → Linear layers → Activation layers → Output layer
- **Design tradeoffs**: Separate positive/negative scores increase complexity but enable conflict detection; clipping to peak flows prevents explosion but may underestimate some effects; separate bias scoring increases clarity but requires additional computation
- **Failure signatures**: Scores not summing to output delta indicates completeness violation; extremely large or small scores suggest clipping thresholds too loose/tight; similar scores for conflicting features suggest sensitivity constant too low
- **First 3 experiments**:
  1. Apply CAFE to XNOR network example to verify conflict detection (should show +1/-1 for both features)
  2. Apply CAFE to GELU network example to verify clipping prevents score explosion
  3. Apply CAFE to a simple regression network with known feature importance to verify correctness against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- How can CAFE be effectively extended to handle convolutional neural networks and transformer architectures?
- The paper suggests this is a promising direction for future work but doesn't provide concrete methods or empirical validation
- Successful implementation and experimental validation on CNNs and transformers would resolve this question

### Open Question 2
- What are the most effective user study methodologies to evaluate whether CAFE's surfacing of feature conflicts, separation of positive and negative attributions, and consideration of biases enhances user understanding of model behavior?
- The paper suggests conducting user studies but doesn't provide specific designs or metrics
- Well-designed user studies with quantitative and qualitative measures would demonstrate improved model understanding

### Open Question 3
- How can CAFE be integrated into deeper, mechanistic explanations of machine learning models to provide more comprehensive insights?
- The paper suggests using CAFE as a component for constructing deeper explanations but doesn't provide concrete approaches
- Demonstrations of CAFE being used in conjunction with other explanation techniques would provide multi-layered, mechanistic insights

## Limitations
- Practical implementation details for rectified activation deltas and peak attribution flows are not fully specified
- Theoretical guarantees may face challenges with complex activation functions or non-tabular data
- Exact parameters for synthetic dataset generation and network training are unspecified

## Confidence
- **High confidence**: CAFE's core mechanism of separating positive/negative contributions and preventing score explosion through clipping
- **Medium confidence**: Theoretical properties hold under specified conditions and baseline comparisons show consistent improvements
- **Medium confidence**: Computational efficiency claims relative to sampling-based methods, though exact runtime comparisons depend on implementation specifics

## Next Checks
1. Implement and test CAFE on synthetic XOR-like datasets with known feature conflicts to verify conflict detection sensitivity across different activation functions
2. Conduct ablation studies varying the conflict sensitivity constant to determine optimal values for different network architectures and datasets
3. Compare CAFE's attribution stability across multiple reference points to assess robustness to reference input selection