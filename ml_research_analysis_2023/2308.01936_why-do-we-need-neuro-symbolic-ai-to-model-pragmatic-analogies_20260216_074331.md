---
ver: rpa2
title: Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?
arxiv_id: '2308.01936'
source_url: https://arxiv.org/abs/2308.01936
tags:
- analogies
- mapping
- knowledge
- llms
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines Large Language Models'' (LLMs) ability to
  model analogies of increasing complexity. The authors propose a taxonomy distinguishing
  four levels: lexical (proportional), syntactic, semantic (metaphor), and pragmatic
  (rich, multi-sentence) analogies.'
---

# Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?

## Quick Facts
- arXiv ID: 2308.01936
- Source URL: https://arxiv.org/abs/2308.01936
- Authors: 
- Reference count: 12
- Primary result: LLMs struggle with complex pragmatic analogies requiring external knowledge, necessitating neuro-symbolic approaches

## Executive Summary
This paper argues that Large Language Models (LLMs) face significant limitations in modeling complex pragmatic analogies that require diverse external knowledge beyond what can be captured through text statistics alone. The authors propose a taxonomy of analogies spanning four levels of complexity - lexical, syntactic, semantic, and pragmatic - demonstrating how LLM performance degrades as analogies become more knowledge-intensive. To address this limitation, they advocate for a neuro-symbolic approach that combines deep learning with knowledge graphs to enrich unstructured text representation and enable more sophisticated analogical mapping. This hybrid method aims to maintain LLM efficiency while adding the capability to generate explanations for pedagogical applications.

## Method Summary
The method employs a neuro-symbolic framework combining deep learning with symbolic AI through knowledge graphs. Unstructured analogy text undergoes Analogy Concept Extraction (ACE) to identify relevant concepts, which are then enriched using domain-specific Knowledge Graphs (KGs) like Cybersecurity KG or Greek Mythology KG, as well as general KGs like WordNet and ConceptNet. This creates Analogy Concept Graphs (ACGs) that capture context and abstraction. The mapping process is decomposed into three levels: Entity-Level Mapping (ELM), Relational-Level Mapping (RLM), and Subgraph-Level Mapping (SLM), with subgraph mapping using graph neural networks for approximate isomorphism calculations. The system preserves LLM efficiency while enabling explanation generation through interpretable ACG representations.

## Key Results
- LLMs handle simple lexical and syntactic analogies effectively but struggle with complex semantic and pragmatic analogies
- Pragmatic analogies require external knowledge beyond text statistics, limiting purely data-driven approaches
- Neuro-symbolic AI combining LLMs with knowledge graphs enables richer analogy representation and explanation generation
- The multi-level mapping approach (entity, relational, subgraph) addresses different abstraction requirements in analogical reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuro-symbolic AI improves pragmatic analogy modeling by enriching unstructured text with external knowledge graphs.
- Mechanism: The system uses knowledge graphs to extract, score, and enrich relevant concepts from analogy text, creating Analogy Concept Graphs (ACGs) that preserve context and abstraction.
- Core assumption: Knowledge graphs contain domain-relevant information that complements LLM limitations in capturing rare and abstract relationships.
- Evidence anchors:
  - [abstract]: "We discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content"
  - [section]: "To address this challenge, computation must focus on identifying triples that capture knowledge relevant to Cybersecurity, such as <harmless-looking attachment, contain, malware>"
  - [corpus]: Weak - no direct corpus evidence supporting knowledge graph enrichment for analogies
- Break condition: Knowledge graphs lack coverage of the domain or contain insufficient abstraction for the analogy mapping.

### Mechanism 2
- Claim: Neuro-symbolic AI enables multi-level mapping (entity, relational, subgraph) that LLMs cannot achieve alone.
- Mechanism: The system partitions analogy mapping into three distinct computational problems, each solved with specialized methods including graph neural networks for subgraph mapping.
- Core assumption: Different mapping levels require different computational approaches, and knowledge graphs provide necessary semantic context for relational and subgraph mapping.
- Evidence anchors:
  - [abstract]: "Our knowledge-informed approach maintains the efficiency of LLMs while preserving the ability to explain analogies"
  - [section]: "SLM is the highest level of abstraction in analogical mapping... Subgraphs include a subset of entities and relationships and represent several abstract concepts"
  - [corpus]: Weak - corpus lacks specific evidence about multi-level mapping performance
- Break condition: Graph neural network similarity calculations fail to capture semantic equivalence between subgraphs.

### Mechanism 3
- Claim: Neuro-symbolic AI preserves explanation capability for pedagogical applications while maintaining LLM efficiency.
- Mechanism: By creating interpretable ACGs as intermediate representations, the system enables explanation generation while still leveraging LLM strengths in processing large unstructured text.
- Core assumption: Pedagogical applications require explainable reasoning, which requires interpretable intermediate representations beyond pure neural embeddings.
- Evidence anchors:
  - [abstract]: "Our knowledge-informed approach maintains the efficiency of LLMs while preserving the ability to explain analogies for pedagogical applications"
  - [section]: "ACG provides two critical functions. First, it captures the purpose of analysis, thereby directing subsequent processing. Second, it supports the recovery of an explanation for pedagogical applications"
  - [corpus]: Weak - corpus lacks evidence about pedagogical explanation quality
- Break condition: ACG construction becomes too complex or loses critical information, making explanations unreliable or incomplete.

## Foundational Learning

- Concept: Analogical reasoning taxonomy (lexical, syntactic, semantic, pragmatic)
  - Why needed here: The paper builds its argument on demonstrating LLM limitations across increasing analogy complexity levels
  - Quick check question: What distinguishes pragmatic analogies from semantic analogies in terms of knowledge requirements?

- Concept: Knowledge graphs and their role in AI
  - Why needed here: Knowledge graphs are the primary symbolic component used to enrich text representations
  - Quick check question: How do knowledge graphs help overcome LLM limitations in capturing abstract mappings?

- Concept: Graph isomorphism and approximate matching
  - Why needed here: Subgraph-level mapping requires approximate graph isomorphism calculations
  - Quick check question: Why is subgraph-level mapping considered NP-Complete and what approximation methods are used?

## Architecture Onboarding

- Component map: Unstructured text → ACE (Analogy Concept Extraction) → ACG Enrichment → ELM/RLM/SLM (Entity/Relational/Subgraph Mapping) → Output explanation
- Critical path: Text parsing → Knowledge graph lookup → ACG construction → Mapping computation → Explanation generation
- Design tradeoffs: Rich knowledge integration vs. computational efficiency; interpretability vs. model complexity
- Failure signatures: Poor analogy quality when knowledge graphs lack coverage; slow processing with large knowledge graphs; incorrect mappings from imperfect graph isomorphism
- First 3 experiments:
  1. Test ACE on simple lexical analogies with and without knowledge graph enrichment
  2. Compare RLM performance with and without relational context from knowledge graphs
  3. Evaluate SLM accuracy using different graph neural network architectures for subgraph similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific knowledge graph structures or representations would be most effective for supporting pragmatic analogy mapping across different domains?
- Basis in paper: [explicit] The paper discusses using knowledge graphs for analogy representation and mapping, but doesn't specify optimal KG structures
- Why unresolved: The paper acknowledges the need for knowledge graphs but doesn't explore which specific KG architectures or representations work best for different types of analogies
- What evidence would resolve it: Empirical comparison of different KG structures (e.g., hierarchical vs. flat, domain-specific vs. general) for analogy mapping performance across various domains

### Open Question 2
- Question: How can we quantitatively evaluate the quality of pragmatic analogies generated by neuro-symbolic systems compared to human-generated analogies?
- Basis in paper: [inferred] The paper mentions explanation generation for pedagogical applications but doesn't address evaluation metrics for analogy quality
- Why unresolved: While the paper discusses generating explanations, it doesn't provide methods for measuring the effectiveness or quality of these analogies compared to human standards
- What evidence would resolve it: Development and validation of metrics that can assess both the accuracy and pedagogical value of machine-generated analogies against human-created ones

### Open Question 3
- Question: What is the computational complexity trade-off between using large language models alone versus neuro-symbolic approaches for different analogy types?
- Basis in paper: [explicit] The paper discusses maintaining LLM efficiency while adding knowledge graph support, but doesn't provide detailed complexity analysis
- Why unresolved: The paper advocates for a neuro-symbolic approach but doesn't quantify the computational costs versus benefits compared to pure LLM approaches
- What evidence would resolve it: Empirical studies measuring processing time, memory usage, and accuracy for both pure LLM and neuro-symbolic approaches across different analogy complexities

## Limitations

- The paper lacks empirical validation and performance benchmarks against LLM-only baselines for analogy modeling
- No evidence is provided for the pedagogical quality of explanations generated by the neuro-symbolic system
- The computational complexity and efficiency trade-offs of the proposed approach versus pure LLM methods are not quantified

## Confidence

- Mechanism 1 (Knowledge Graph Enrichment): Medium confidence - The theoretical framework is sound, but lacks empirical validation against LLM baselines
- Mechanism 2 (Multi-level Mapping): Medium confidence - The decomposition approach is logical, but effectiveness depends on specific implementation details not provided
- Mechanism 3 (Pedagogical Explanations): Low confidence - No evidence presented for explanation quality or pedagogical utility

## Next Checks

1. **Benchmark Comparison**: Implement a direct comparison between the neuro-symbolic approach and a state-of-the-art LLM (e.g., GPT-4 or Claude) fine-tuned on cybersecurity analogies to measure actual performance gains from knowledge graph integration.

2. **Explanation Quality Assessment**: Design a human evaluation study where domain experts rate the pedagogical quality and correctness of explanations generated by the neuro-symbolic system versus pure LLM outputs for the same analogies.

3. **Knowledge Graph Coverage Analysis**: Systematically evaluate how ACG extraction quality degrades as the knowledge graph coverage decreases (using subsets of the graph), quantifying the minimum knowledge graph requirements for effective analogy modeling.