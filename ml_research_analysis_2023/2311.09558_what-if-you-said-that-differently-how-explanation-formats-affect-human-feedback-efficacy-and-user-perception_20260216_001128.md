---
ver: rpa2
title: 'What if you said that differently?: How Explanation Formats Affect Human Feedback
  Efficacy and User Perception'
arxiv_id: '2311.09558'
source_url: https://arxiv.org/abs/2311.09558
tags:
- rationale
- question
- feedback
- answer
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to format intermediate rationales
  in decomposed QA systems to make them easier for users to understand and provide
  feedback on. The authors compare five rationale formats: markup-and-mask, annotated
  report, procedural, subquestions, and decision tree.'
---

# What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception

## Quick Facts
- **arXiv ID**: 2311.09558
- **Source URL**: https://arxiv.org/abs/2311.09558
- **Reference count**: 16
- **Key outcome**: Rationale formats significantly affect both the ease with which users can provide feedback and the model's ability to incorporate that feedback, with attributed formats and depth of reasoning enhancing user-reported understanding and trust.

## Executive Summary
This paper investigates how different formats for presenting intermediate rationales in decomposed question answering systems impact user feedback efficacy and perception. The authors compare five rationale formats: markup-and-mask, annotated report, procedural, subquestions, and decision tree. Through human evaluation on two datasets (Quoref and PubMedQA), they find that formats with attribution and depth of reasoning significantly enhance user understanding and trust. However, the study reveals an important disconnect: formats that are easiest for users to provide feedback on (like markup-and-mask) are not necessarily the most effective at improving model performance.

## Method Summary
The study employs a decomposed QA pipeline where an X2R model generates intermediate rationales from passages and questions, followed by an R2Y model that generates answers from these rationales. Five different rationale formats are compared: markup_and_mask (highlighting context), annotated_report (attributed reasoning steps), procedural (primitive operations), subquestions (breaking down the main question), and decision_tree (hierarchical reasoning). Human annotators provide feedback on incorrect answers, rating rationale sufficiency and faithfulness, then offering natural language suggestions for revision. The feedback is incorporated using GPT-3.5-turbo to revise rationales and regenerate answers, with evaluation based on edit accuracy and final answer accuracy.

## Key Results
- Annotated reports with attributions and depth of reasoning are rated as most understandable and trustworthy by users
- Markup-and-mask format is easiest for users to provide feedback on, but this feedback is less effective at improving model performance
- Rationale formats that expose sequential reasoning (procedural and subquestions) are easier to repair through feedback
- Certain formats like annotated_report show higher final answer accuracy after feedback incorporation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Intermediate rationales with attribution and depth of reasoning are more effective for user feedback and trust.
- **Mechanism**: Attribution provides grounding to the context by including quoted phrases, while depth of reasoning exposes the model's step-by-step thinking process. This combination helps users understand and verify the model's reasoning, making it easier to provide targeted feedback and increasing trust in the outputs.
- **Core assumption**: Users can effectively use quoted phrases and detailed reasoning steps to understand and critique the model's logic.
- **Evidence anchors**:
  - [abstract] "certain formats, like annotated reports with attributions to the context and depth of reasoning, significantly enhance user-reported understanding and trust of model outputs."
  - [section 6.3] "rationales with attributions and a sufficient amount of depth (annotated_report and procedural) are most easy to understand and trust for Quoref."
- **Break condition**: If users find the depth of reasoning overwhelming or the attributions insufficient to ground the reasoning, the mechanism may fail.

### Mechanism 2
- **Claim**: The format of rationales affects both the ease of providing feedback and the model's ability to incorporate that feedback.
- **Mechanism**: Different rationale formats expose different levels of the model's reasoning process. Formats that clearly expose the reasoning steps (like procedural and subquestions) are easier for users to critique and for models to incorporate feedback. Formats that lack this clarity (like markup_and_mask) may be easier to provide feedback for but less effective in actually improving the model.
- **Core assumption**: Users can identify errors in the reasoning process when it is clearly exposed, and models can effectively use this feedback to revise their rationales.
- **Evidence anchors**:
  - [abstract] "rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback."
  - [section 5.5] "rationale formats that expose the reasoning of the model are easier to repair through feedback."
- **Break condition**: If the model's reasoning process is too complex to be clearly exposed in any format, or if users struggle to identify errors even when the reasoning is exposed, the mechanism may fail.

### Mechanism 3
- **Claim**: Annotator ease of providing feedback does not necessarily correlate with the actual effectiveness of that feedback in repairing rationales.
- **Mechanism**: Annotators may find it easier to provide feedback for simpler formats like markup_and_mask, but this feedback may not be as effective in actually improving the model's performance. More complex formats like annotated_report may be harder to provide feedback for but result in more effective revisions.
- **Core assumption**: The complexity of the rationale format does not directly translate to the quality of the feedback provided.
- **Evidence anchors**:
  - [section 5.5] "markup_mask is easiest to provide feedback for, because it may be easy to verbalize when information is missing from the rationale. However, these judgements do not correlate with actual effectiveness of the feedback for rationale repair."
- **Break condition**: If annotators become more proficient at providing feedback for complex formats, or if the model becomes better at incorporating feedback from simpler formats, the mechanism may fail.

## Foundational Learning

- **Concept**: Decomposed QA pipeline
  - **Why needed here**: The paper investigates how to format intermediate rationales in a decomposed QA system, where the model first generates a rationale and then uses that rationale to generate an answer. Understanding this pipeline is crucial for grasping the paper's contributions.
  - **Quick check question**: What are the two main stages of a decomposed QA pipeline, and how do they differ from a standard end-to-end QA model?

- **Concept**: Rationale formats and their properties
  - **Why needed here**: The paper compares five different rationale formats (markup_and_mask, annotated_report, procedural, subquestions, decision_tree) and analyzes how their properties (like attribution, depth of reasoning, sequential reasoning) affect user feedback and trust. Understanding these formats and properties is essential for interpreting the results.
  - **Quick check question**: What are the key differences between the five rationale formats considered in the paper, and how do their properties vary?

- **Concept**: Human feedback in NLP
  - **Why needed here**: The paper explores how to elicit and incorporate human feedback to improve the model's rationales and answers. Understanding the role of human feedback in NLP is important for contextualizing the paper's contributions.
  - **Quick check question**: How does the paper collect and use human feedback to improve the model's rationales and answers, and what are the potential benefits and challenges of this approach?

## Architecture Onboarding

- **Component map**: Passage/Question -> X2R model -> Rationale -> R2Y model -> Answer -> Human Feedback -> Revised Rationale -> Revised Answer
- **Critical path**: The critical path is the flow from the passage and question through the X2R model to generate a rationale, then through the R2Y model to generate an answer, and finally through the human feedback loop to revise the rationale and regenerate the answer.
- **Design tradeoffs**:
  - Simplicity vs. effectiveness of rationale formats: Simpler formats like markup_and_mask may be easier to implement but less effective for user feedback and trust.
  - User effort vs. model improvement: More complex formats like annotated_report may require more user effort but result in more effective model improvements.
- **Failure signatures**:
  - If the X2R model generates poor rationales, the entire pipeline will suffer.
  - If the R2Y model cannot effectively use the rationales to generate answers, the benefits of the decomposed approach will be lost.
  - If the human feedback is not incorporated effectively, the model will not improve.
- **First 3 experiments**:
  1. Compare the performance of different rationale formats on a standard QA task to establish a baseline.
  2. Collect human feedback on the rationales and measure its effectiveness in repairing the model's outputs.
  3. Analyze user perceptions of the different rationale formats in terms of understandability and trust.

## Open Questions the Paper Calls Out
- **Open Question 1**: What are the precise mechanisms by which attribution and depth of reasoning enhance user understanding and trust in model outputs?
- **Open Question 2**: How do different rationale formats impact the efficiency and effectiveness of human feedback in real-world applications?
- **Open Question 3**: What are the potential trade-offs between rationale faithfulness and critiquability, and how can they be balanced?

## Limitations
- Small sample size (50 examples per dataset) may not capture full variability in human feedback patterns
- Use of GPT-3.5-turbo for both rationale generation and feedback incorporation introduces potential confounding factors
- Does not explore how feedback quality varies across different user expertise levels

## Confidence
- **High Confidence**: User perception findings showing that attributed formats with depth of reasoning (annotated_report and procedural) are rated as most understandable and trustworthy.
- **Medium Confidence**: Claims about which formats are easiest to provide feedback for.
- **Low Confidence**: Claims about the actual effectiveness of feedback in improving model performance.

## Next Checks
1. Scale the evaluation to a larger sample size (e.g., 500+ examples) across multiple datasets to assess generalizability and increase statistical power.
2. Test with human models by having human annotators provide feedback and revise rationales for a subset of examples, then compare the quality of human-revised rationales to those generated by the model.
3. Conduct a longitudinal analysis to track how models improve over multiple feedback iterations within the same session, and whether initial rationale format affects this learning trajectory.