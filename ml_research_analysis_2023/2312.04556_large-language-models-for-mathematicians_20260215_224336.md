---
ver: rpa2
title: Large Language Models for Mathematicians
arxiv_id: '2312.04556'
source_url: https://arxiv.org/abs/2312.04556
tags:
- language
- arxiv
- llms
- mathematical
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) such as ChatGPT have received immense
  interest for their general-purpose language understanding and, in particular, their
  ability to generate high-quality text or computer code. For many professions, LLMs
  represent an invaluable tool that can speed up and improve the quality of work.
---

# Large Language Models for Mathematicians

## Quick Facts
- arXiv ID: 2312.04556
- Source URL: https://arxiv.org/abs/2312.04556
- Reference count: 40
- Key outcome: Large language models can assist professional mathematicians with proof generation, literature search, and collaborative writing, but struggle with precise arithmetic and may produce mathematically incorrect outputs requiring human verification.

## Executive Summary
This paper provides a comprehensive analysis of how large language models (LLMs) like ChatGPT can aid professional mathematicians. The authors describe the transformer architecture underlying these models, present evidence of their mathematical capabilities through specific examples, and outline both best practices and potential pitfalls. While LLMs show promise for tasks like proof generation, error detection, and collaborative writing, they face significant limitations in precise arithmetic and may produce mathematically incorrect results. The paper emphasizes that successful integration of LLMs into mathematical workflows requires careful prompt engineering and human oversight.

## Method Summary
The paper evaluates LLM performance through case studies using the GHOSTS dataset containing 709 prompts across various mathematical domains. Three LLM versions (ChatGPT 9-Jan-2023, ChatGPT 30-Jan-2023, and GPT-4) are compared on the same prompts. Professional mathematicians rate the responses on a 1-5 scale. The evaluation focuses on proofs, definitions, computations, and literature search tasks. However, the exact prompts and detailed rating criteria are not publicly available, limiting reproducibility.

## Key Results
- LLMs can generate syntactically valid mathematical proofs when prompts are well-structured, but may produce plausible yet incorrect results
- Interactive feedback from users significantly improves LLM-generated mathematical content by guiding reasoning paths
- LLMs struggle with precise arithmetic and symbolic manipulation, relying on token-level predictions that poorly represent exact computation
- Professional mathematicians can effectively use LLMs for literature search, brainstorming, proof-checking, and collaborative writing with appropriate oversight

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can parse and produce mathematically correct proofs when prompts are well-structured and context is provided.
- Mechanism: Autoregressive transformer architecture maps embeddings of tokens through self-attention layers, capturing contextual dependencies that allow construction of syntactically valid proofs.
- Core assumption: The model has seen sufficient similar proof structures during pretraining so that its probability estimates for next tokens align with valid logical steps.
- Evidence anchors: ChatGPT generated a valid proof for the inequality ∫|fg|dµ≤∥g∥1∥f∥∞ using measure theory notation; the transformer is defined by composition of L blocks with self-attention maps Aℓ, entrywise applied normalizing layers, and feed-forward perceptrons.

### Mechanism 2
- Claim: LLMs struggle with precise arithmetic and symbolic manipulation, leading to errors in proofs requiring computation.
- Mechanism: Without built-in numerical solver, the model relies solely on learned token distributions which poorly represent exact arithmetic operations for large or complex numbers.
- Core assumption: Token-level predictions for numbers and operations cannot capture the deterministic nature of arithmetic.
- Evidence anchors: ChatGPT provided an incorrect example of a function continuous at precisely one point, giving f(x) = {x² if x≠0, 1 if x=0} which is actually discontinuous at x=0.

### Mechanism 3
- Claim: Interactive feedback from users improves LLM proof generation by guiding the model toward correct reasoning paths.
- Mechanism: User-provided corrections adjust the context embedding, influencing self-attention weights to favor valid logical continuations over hallucinated ones.
- Core assumption: Each user interaction updates the prompt context sufficiently to shift the probability distribution of next tokens toward correct steps.
- Evidence anchors: LLMs can find mistakes in given proofs that can often be confirmed as actual mistakes by mathematicians; collaborative writing allows LLMs to improve parts, repair errors, and add details after user feedback.

## Foundational Learning

- Concept: Subword tokenization and embedding mapping
  - Why needed here: Understanding how text is converted to token indices and embeddings is essential to grasp how transformers process mathematical notation.
  - Quick check question: If the word "continuity" is split into "contin" and "uity", what is the dimensionality of each token's embedding vector?

- Concept: Self-attention and autoregressive decoding
  - Why needed here: The ability to attend to previous tokens and predict the next one is the core of proof generation; errors here propagate through the proof.
  - Quick check question: In a decoder-only transformer, can the model attend to future tokens when predicting the next token?

- Concept: Cross-entropy loss and gradient descent in training
  - Why needed here: Knowing how the model is trained to minimize prediction error helps explain its strengths and weaknesses in mathematical reasoning.
  - Quick check question: If a model predicts token A with probability 0.9 but the ground truth is token B, what is the cross-entropy loss contribution for this prediction?

## Architecture Onboarding

- Component map: Tokenizer → Embedding layer → Positional encoding → Transformer blocks (Self-attention + MLP + Norm) → Prediction head → Sampling
- Critical path: Prompt → Tokenization → Embedding lookup → Positional encoding → Self-attention computation → Residual connections → MLP → Prediction head → Argmax sampling → Next token
- Design tradeoffs: Larger embedding dimension improves expressiveness but increases computation and memory; more transformer layers increase context capture but risk overfitting and slow inference; sampling temperature trades diversity for correctness.
- Failure signatures: Inconsistent logical steps likely indicate self-attention misalignment or training data bias; arithmetic errors indicate token-based prediction fails on exact computation; hallucinations indicate prediction head or sampling strategy not constrained to valid mathematical syntax.
- First 3 experiments: 1) Feed a simple proof step and inspect next predicted token distribution; 2) Prompt model to fill a deliberate gap in a known proof and evaluate validity; 3) Provide a proof with intentional error and ask model to find it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs reliably generate complete and mathematically rigorous proofs for complex theorems?
- Basis in paper: The paper discusses limitations of LLMs in generating proofs, noting that a single wrong statement can invalidate an entire proof and that LLMs do not typically revisit or revise their arguments.
- Why unresolved: While LLMs can assist in certain aspects of mathematical reasoning, they struggle with generating complete, rigorous proofs for complex theorems due to their autoregressive nature and susceptibility to errors.
- What evidence would resolve it: Developing and testing specialized LLM architectures designed specifically for theorem proving, potentially incorporating interactive theorem provers, could provide evidence of whether LLMs can reliably generate complete proofs for complex theorems.

### Open Question 2
- Question: What is the optimal way to combine human expertise with LLM capabilities for mathematical research and education?
- Basis in paper: The paper suggests that a collaborative approach incorporating human expertise is advisable when using LLMs for mathematical tasks, outlining strategies including literature search, brainstorming, proof-checking, and collaborative writing.
- Why unresolved: While the paper identifies potential strategies for combining human and LLM capabilities, it does not provide a definitive answer on the optimal approach as effectiveness may vary depending on the specific task and human expertise.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different collaborative approaches between mathematicians and LLMs on various mathematical tasks could provide insights into the optimal way to combine human expertise with LLM capabilities.

### Open Question 3
- Question: How can the energy consumption and environmental impact of training large language models be reduced?
- Basis in paper: The paper mentions high energy consumption and CO2 emissions associated with training LLMs, citing examples of models that consume hundreds of megawatt-hours and emit tens of tons of CO2.
- Why unresolved: While the paper acknowledges the environmental impact of LLM training, it does not propose specific solutions for reducing energy consumption and emissions, which is important as the size and number of LLMs continue to grow.
- What evidence would resolve it: Research into more energy-efficient training algorithms, hardware optimizations, and alternative energy sources for data centers could provide evidence of how to reduce the environmental impact of training LLMs.

## Limitations

- The evidence presented is primarily anecdotal and based on a small number of case studies rather than systematic evaluation or statistical analysis.
- The findings may not generalize to more advanced or specialized areas of mathematics where proofs require deeper conceptual understanding or novel constructions.
- The paper lacks quantification of the time and expertise required for mathematicians to verify and correct LLM-generated mathematics, which is crucial for practical adoption.

## Confidence

**High Confidence:** LLMs can generate syntactically valid mathematical LaTeX code and follow basic proof structures; LLMs struggle with precise arithmetic and symbolic manipulation tasks; interactive feedback from users can improve LLM-generated mathematical content.

**Medium Confidence:** LLMs can parse and produce mathematically correct proofs when prompts are well-structured; LLMs may construct valid proofs for questions different from those posed due to prompt ambiguity; LLMs can assist in finding mistakes in proofs through collaborative writing.

**Low Confidence:** LLMs will fundamentally change how mathematicians work in the near future; the specific improvements observed between ChatGPT versions are directly attributable to architectural changes rather than fine-tuning; the transformer mechanism described is directly responsible for observed mathematical capabilities without considering pretraining data composition.

## Next Checks

1. **Systematic Evaluation Protocol**: Design and execute a controlled experiment using a standardized mathematical benchmark to measure LLM performance across multiple mathematical domains with statistical significance, including inter-rater reliability for human evaluations.

2. **Error Analysis Framework**: Develop a taxonomy of mathematical errors produced by LLMs (arithmetic errors, logical gaps, hallucination of theorems, etc.) and measure their frequency and severity across different types of mathematical problems, then correlate these error types with specific architectural features of the transformer model.

3. **Human-AI Collaboration Study**: Conduct a time-motion study measuring the actual time saved or lost when mathematicians work with LLMs versus working independently, accounting for the time required to verify LLM outputs and the learning curve for effective prompt engineering.