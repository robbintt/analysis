---
ver: rpa2
title: 'What Happens During Finetuning of Vision Transformers: An Invariance Based
  Investigation'
arxiv_id: '2307.06006'
source_url: https://arxiv.org/abs/2307.06006
tags:
- learning
- forgetting
- layers
- pretrained
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how pretrained vision transformers (ViTs)
  change during finetuning on various downstream tasks. To characterize these changes,
  the authors propose metrics based on shared invariances between pretrained and finetuned
  models, leveraging the STIR approach.
---

# What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation

## Quick Facts
- arXiv ID: 2307.06006
- Source URL: https://arxiv.org/abs/2307.06006
- Reference count: 40
- This work investigates how pretrained vision transformers (ViTs) change during finetuning on various downstream tasks using invariance-based metrics.

## Executive Summary
This paper investigates the dynamics of vision transformers during finetuning by measuring shared invariances between pretrained and finetuned models using the STIR (Similarity Through Inverted Representations) approach. The authors find that pretraining instills reusable invariances in early layers that persist through finetuning, while invariances from deeper pretrained layers compress toward shallower layers during adaptation. They also discover a strong correlation between the variability of forgetting across layers and model robustness to data corruption, suggesting this could be used as an early stopping indicator for robust model training.

## Method Summary
The study uses pretrained ViT models finetuned on various downstream tasks (classification and reconstruction) with SGD optimizer (learning rate 0.001, momentum 0.9, weight decay 0.0001, cosine scheduler, 100 epochs). The key innovation is using STIR-based metrics to measure shared invariances between pretrained and finetuned models across layers, tracking learning and forgetting of invariances. The Invariance Flow Matrix captures how invariances migrate between layers during finetuning, and correlation analysis examines the relationship between forgetting dynamics and robustness to data corruption.

## Key Results
- Pretraining instills reusable invariances in early layers that persist with substantially lower learning and forgetting during finetuning
- Finetuning compresses invariances from deeper pretrained layers toward shallower layers
- Higher variability in forgetting across layers correlates strongly (0.78) with improved robustness to data corruption

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining instills reusable invariances in early layers that persist through finetuning
- Mechanism: Early layers capture low-level visual features (edges, textures) that are task-agnostic. When finetuning, these layers change less because their learned invariances remain useful across tasks
- Core assumption: Visual invariances learned on ImageNet are broadly transferable to other vision tasks
- Evidence anchors:
  - [abstract]: "pretraining induces transferable invariances in shallow layers"
  - [section]: "We observe that the finetuned models that start from pretrained models exhibit a substantially lower learning and forgetting in the early layers (1-6)"
  - [corpus]: Weak correlation; related work discusses invariance but not specifically early-layer persistence through finetuning
- Break condition: If the pretraining and finetuning domains are too dissimilar (e.g., natural images vs. medical imaging), early-layer invariances may not transfer

### Mechanism 2
- Claim: Finetuning compresses invariances from deeper pretrained layers toward shallower layers
- Mechanism: As finetuning adapts the model to a specific task, invariances from deeper layers migrate to earlier layers to preserve capacity in later layers for task-specific learning
- Core assumption: The Invariance Flow metric accurately captures migration of invariances between layers
- Evidence anchors:
  - [abstract]: "invariances from deeper pretrained layers are compressed towards shallower layers during finetuning"
  - [section]: "we observe a compression of the pretrained invariances, visible in the lower left part of the matrix"
  - [corpus]: No direct evidence; related work focuses on representation similarity rather than invariance flow
- Break condition: If the finetuning task requires similar invariances at all depths, compression may not occur

### Mechanism 3
- Claim: Higher variability in forgetting across layers correlates with improved robustness to data corruption
- Mechanism: When different layers forget different amounts, the model may develop more diverse representations that generalize better to corrupted inputs
- Core assumption: Standard deviation of forgetting across layers is a valid proxy for representation diversity
- Evidence anchors:
  - [section]: "we observe a strong correlation (0.78 as correlation value) between the standard deviation of forgetting across layers and robustness accuracy"
  - [section]: "this correlation becomes even stronger when fewer epochs are considered"
  - [corpus]: Weak evidence; related work doesn't connect forgetting dynamics to robustness
- Break condition: If the correlation is driven by later training epochs rather than early dynamics, it may not be useful for early stopping

## Foundational Learning

- Concept: Invariance in neural networks
  - Why needed here: The entire paper measures how much models share invariances between layers and between pretrained/finetuned states
  - Quick check question: What transformations of input would not change the output of a layer that has learned translation invariance?

- Concept: Representation similarity metrics (CKA vs STIR)
  - Why needed here: The paper contrasts CKA (which measures representation similarity) with STIR (which measures shared invariances), showing they capture different phenomena
  - Quick check question: If two models have identical representations for all inputs but different robustness to perturbations, would CKA or STIR show they share invariances?

- Concept: Catastrophic forgetting and the stability-plasticity dilemma
  - Why needed here: The paper frames learning and forgetting of invariances as a balance between retaining old knowledge and acquiring new knowledge
  - Quick check question: In continual learning, what problem arises if a model is too plastic? Too stable?

## Architecture Onboarding

- Component map: ViT with 12 layers, patch size 32x32, image resolution 224x224; pretrained on ImageNet or CIFAR; finetuned on classification or reconstruction tasks
- Critical path: Pretrain → Compute STIR between pretrained layers → Finetune → Compute STIR between pretrained and finetuned layers → Calculate learning/forgetting metrics
- Design tradeoffs: STIR is directional and computationally expensive (requires optimization to find invariant inputs); CKA is symmetric and faster but doesn't capture invariance
- Failure signatures: High learning/forgetting values throughout all layers suggests pretraining provided little benefit; compression effects only in reconstruction task suggests task dependency
- First 3 experiments:
  1. Replicate the ImageNet→CIFAR10 classification experiment to verify lower learning/forgetting in early layers
  2. Test ImageNet→CIFAR10 reconstruction to observe compression effects in the Invariance Flow matrix
  3. Compare STIR vs CKA trends across layers to confirm they capture different phenomena

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different transformer architectures (e.g., BERT, RoBERTa) compare in terms of forgetting and learning of invariances during finetuning?
- Basis in paper: [inferred] The paper focuses on Vision Transformers (ViT) but does not explore other transformer architectures or their behavior during finetuning.
- Why unresolved: The study is limited to ViT models, leaving open the question of whether other transformer architectures exhibit similar patterns of forgetting and learning invariances.
- What evidence would resolve it: Conducting similar experiments with other transformer architectures like BERT or RoBERTa and comparing the forgetting and learning metrics across layers would provide insights into whether these patterns are architecture-specific or general across transformers.

### Open Question 2
- Question: What is the impact of dataset size on the forgetting and learning of invariances during finetuning?
- Basis in paper: [inferred] The paper uses specific datasets (CIFAR10, CIFAR100, Oxford-IIT Pet, EuroSAT) but does not explore the effect of dataset size on the forgetting and learning metrics.
- Why unresolved: The study does not vary the dataset size, leaving open the question of how the amount of finetuning data influences the retention and acquisition of invariances.
- What evidence would resolve it: Experimenting with datasets of varying sizes while keeping other factors constant would help determine if larger datasets lead to less forgetting and more learning of invariances.

### Open Question 3
- Question: How do different learning rate schedules affect the dynamics of forgetting and learning during finetuning?
- Basis in paper: [explicit] The paper uses a cosine scheduler for the learning rate but does not explore the effects of other learning rate schedules on forgetting and learning.
- Why unresolved: The study uses a single learning rate schedule, leaving open the question of how other schedules (e.g., step decay, exponential decay) might influence the forgetting and learning of invariances.
- What evidence would resolve it: Comparing the forgetting and learning metrics across different learning rate schedules during finetuning would provide insights into how the choice of schedule affects the retention and acquisition of invariances.

### Open Question 4
- Question: What is the relationship between the robustness of a model to adversarial attacks and the forgetting and learning of invariances during finetuning?
- Basis in paper: [explicit] The paper explores the correlation between the variability of forgetting across layers and model robustness to data corruption, but does not investigate adversarial robustness.
- Why unresolved: The study focuses on data corruption robustness but does not explore the relationship between adversarial robustness and the forgetting and learning of invariances.
- What evidence would resolve it: Conducting adversarial robustness tests alongside the forgetting and learning metrics during finetuning would help determine if models that retain more invariances are also more robust to adversarial attacks.

## Limitations
- The directional nature of STIR introduces potential bias in measuring invariances
- The compression phenomenon is only demonstrated for reconstruction tasks
- The correlation between forgetting variability and robustness may be task-dependent

## Confidence
- Mechanism 1 (early-layer invariances): Medium - supported by layer-wise learning/forgetting patterns but correlation is moderate
- Mechanism 2 (invariance compression): Low-Medium - only demonstrated in reconstruction task, no theoretical justification provided
- Mechanism 3 (forgetting-robustness correlation): Medium-High - strong statistical correlation but mechanism unclear

## Next Checks
1. Replicate the invariance compression effect on classification tasks by comparing Invariance Flow matrices across different downstream tasks
2. Test whether the forgetting-robustness correlation holds for non-natural image datasets (e.g., medical imaging) with different corruption types
3. Verify STIR directional bias by computing invariance flow in both forward and backward directions and comparing results