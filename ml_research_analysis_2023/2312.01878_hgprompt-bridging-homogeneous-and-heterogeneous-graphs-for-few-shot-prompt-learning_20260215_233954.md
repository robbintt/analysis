---
ver: rpa2
title: 'HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt
  Learning'
arxiv_id: '2312.01878'
source_url: https://arxiv.org/abs/2312.01878
tags:
- graph
- graphs
- tasks
- pre-training
- heterogeneous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of bridging the gap between homogeneous
  and heterogeneous graph neural networks, particularly in few-shot learning scenarios.
  The proposed HGPROMPT framework unifies pre-training and downstream tasks on both
  graph types through a dual-template design: a graph template converts heterogeneous
  graphs into multiple homogeneous subgraphs, while a task template standardizes various
  tasks into a common subgraph similarity prediction format.'
---

# HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning

## Quick Facts
- arXiv ID: 2312.01878
- Source URL: https://arxiv.org/abs/2312.01878
- Reference count: 13
- Key outcome: Novel pre-training and prompting framework that bridges homogeneous and heterogeneous graphs through dual-template design and dual-prompt mechanism, significantly outperforming state-of-the-art baselines in few-shot node and graph classification tasks.

## Executive Summary
HGPROMPT addresses the challenge of applying graph neural networks to heterogeneous graphs in few-shot learning scenarios. The framework introduces a dual-template design that converts heterogeneous graphs into multiple homogeneous subgraphs and unifies various downstream tasks as subgraph similarity prediction. A dual-prompt mechanism handles feature variations and heterogeneity differences across tasks through feature prompts and heterogeneity prompts. Experiments on ACM, DBLP, and Freebase datasets demonstrate significant performance improvements over state-of-the-art methods in few-shot node and graph classification.

## Method Summary
HGPROMPT unifies pre-training and downstream tasks on both homogeneous and heterogeneous graphs through a dual-template design: a graph template converts heterogeneous graphs into multiple homogeneous subgraphs, while a task template standardizes various tasks into subgraph similarity prediction. The framework employs a dual-prompt mechanism consisting of feature prompts to handle input feature variations and heterogeneity prompts to adjust the importance of different node/edge types across tasks. During few-shot learning, the pre-trained model weights remain frozen while only lightweight prompt vectors are tuned for specific downstream tasks.

## Key Results
- HGPROMPT significantly outperforms state-of-the-art baselines in few-shot node classification and graph classification tasks
- The enhanced version HGPROMPT+ shows particular effectiveness when leveraging heterogeneous information during pre-training
- Framework demonstrates flexibility across different GNN backbones with consistent performance improvements
- Ablation studies confirm the effectiveness of both graph template and dual-prompt components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HGPROMPT bridges pre-training and downstream tasks on heterogeneous graphs through dual-template unification
- Mechanism: The framework converts heterogeneous graphs into multiple homogeneous subgraphs using a graph template, then applies a task template to unify various downstream tasks into subgraph similarity prediction
- Core assumption: The semantic relationships captured by subgraph similarity prediction are preserved across the conversion from heterogeneous to homogeneous representations
- Evidence anchors:
  - [abstract] "HGPROMPT, a novel pre-training and prompting framework to unify not only pre-training and downstream tasks but also homogeneous and heterogeneous graphs via a dual-template design."
  - [section] "In HGPROMPT we propose a unified graph template to convert a heterogeneous graph into multiple homogeneous subgraphs... On the other hand, in HGPROMPT we propose a unified graph template to convert a heterogeneous graph into multiple homogeneous subgraphs, as illustrated in Fig. 1(b)."

### Mechanism 2
- Claim: The dual-prompt design addresses feature variations and heterogeneity differences across tasks
- Mechanism: Feature prompts modify input features before subgraph readout to help tasks focus on relevant features, while heterogeneity prompts adjust aggregation weights across different homogeneous subgraphs to emphasize task-relevant facets of heterogeneity
- Core assumption: Different downstream tasks require different feature subsets and different importance weighting of heterogeneous node/edge types
- Evidence anchors:
  - [abstract] "we propose dual-prompt in HGPROMPT to assist a downstream task in locating the most relevant prior to bridge the gaps caused by not only feature variations but also heterogeneity differences across tasks."
  - [section] "Under the prompt, the readout layer for some subgraph S becomes READ OUT({pfeat ⊙ hv | v ∈ V (S)})... which gives the embedding vector s for the subgraph S."

### Mechanism 3
- Claim: Prompt tuning with few learnable parameters enables effective few-shot learning
- Mechanism: Instead of fine-tuning the entire pre-trained model, HGPROMPT only tunes lightweight prompt vectors while keeping the pre-trained weights frozen, making it parameter-efficient for few-shot scenarios
- Core assumption: The pre-trained model has captured sufficient general knowledge that can be adapted to specific tasks through minimal prompt adjustments
- Evidence anchors:
  - [abstract] "the parameters of the pre-trained model are frozen, and only a light-weight, task-specific prompt vector is tuned for a downstream task."
  - [section] "During prompt tuning, only the light-weight prompt vectors are tuned, while the pre-trained weights Θ0 are frozen without any fine-tuning."

## Foundational Learning

- Concept: Heterogeneous graph representation learning
  - Why needed here: The paper addresses the challenge of applying graph neural networks to heterogeneous graphs with multiple node and edge types
  - Quick check question: What are the key differences between homogeneous and heterogeneous graph neural networks in handling node and edge types?

- Concept: Graph pre-training and fine-tuning paradigm
  - Why needed here: HGPROMPT builds on the "pre-train, prompt, predict" approach, requiring understanding of how pre-training on self-supervised tasks can be leveraged for downstream applications
  - Quick check question: How does the gap between pre-training and fine-tuning objectives affect downstream task performance?

- Concept: Prompt learning in graph neural networks
  - Why needed here: The paper extends prompt learning from natural language processing to graphs, requiring understanding of how prompts can bridge task gaps
  - Quick check question: How do feature prompts and heterogeneity prompts differ in their approach to modifying graph neural network inputs?

## Architecture Onboarding

- Component map: Pre-training stage (link prediction) -> Graph template (heterogeneous to homogeneous subgraphs) -> Task template (subgraph similarity prediction) -> Dual-prompt tuning (feature and heterogeneity prompts)
- Critical path: Pre-training → Graph template application → Task template application → Dual-prompt tuning for downstream tasks
- Design tradeoffs: The dual-template design adds complexity but enables cross-domain application; prompt tuning reduces parameter count but may limit adaptation capability
- Failure signatures: Poor performance when heterogeneity information is critical and lost during conversion; failure when prompt tuning cannot adequately bridge the task gap
- First 3 experiments:
  1. Test graph template conversion by comparing subgraph embeddings from original heterogeneous graph vs. converted homogeneous subgraphs
  2. Validate dual-prompt effectiveness by comparing performance with and without each prompt component
  3. Benchmark few-shot performance across different numbers of shots to establish the framework's effectiveness in label-scarce settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the dual-template design be extended to handle dynamic heterogeneous graphs where node and edge types evolve over time?
- Basis in paper: [inferred] The paper focuses on static heterogeneous graphs but doesn't address temporal evolution of graph structure and heterogeneity
- Why unresolved: The current framework assumes fixed node/edge types and doesn't account for temporal dynamics in heterogeneous graphs
- What evidence would resolve it: Empirical results showing the framework's performance on dynamic heterogeneous graphs with evolving types, or theoretical analysis of how the dual-template would need to adapt to temporal changes

### Open Question 2
- Question: What is the theoretical relationship between the effectiveness of the heterogeneity prompt and the structural properties of the underlying heterogeneous graph (e.g., degree distribution, clustering coefficient)?
- Basis in paper: [inferred] While the paper demonstrates empirical effectiveness of the heterogeneity prompt, it doesn't provide theoretical analysis of when and why it works best
- Why unresolved: The paper shows practical benefits but lacks formal analysis connecting prompt effectiveness to graph properties
- What evidence would resolve it: Mathematical proofs or extensive experiments correlating heterogeneity prompt performance with specific structural graph metrics across diverse datasets

### Open Question 3
- Question: How does the performance of HGPROMPT scale with graph size and complexity, particularly for very large-scale heterogeneous graphs with thousands of node/edge types?
- Basis in paper: [inferred] The paper evaluates on three datasets but doesn't systematically study scaling behavior or performance on graphs with high heterogeneity
- Why unresolved: Current experiments focus on moderate-sized graphs with limited node/edge types, leaving questions about scalability unanswered
- What evidence would resolve it: Controlled experiments varying graph size and number of types, with performance metrics showing how each component of the framework scales

## Limitations
- Limited implementation details for critical components like subgraph sampling strategies and prompt vector dimensions
- No standard deviation reporting across runs or extensive hyperparameter sensitivity analysis
- Untested assumption that subgraph similarity prediction can effectively bridge heterogeneous and homogeneous graph representations
- Potential scalability issues when applying the graph template to very large heterogeneous graphs with many node/edge types

## Confidence
- Claim: Dual-template design effectively bridges homogeneous and heterogeneous graph pre-training and downstream tasks
- Confidence: Medium - well-motivated theoretically but lacks detailed implementation specifications and extensive ablation studies
- Claim: Dual-prompt mechanism significantly improves few-shot learning performance
- Confidence: Medium - empirical results show strong performance but limited hyperparameter analysis and no theoretical analysis of prompt effectiveness
- Claim: Framework generalizes across different GNN backbones and benchmark datasets
- Confidence: Medium - demonstrates effectiveness on three datasets but doesn't test on diverse heterogeneous graph types or very large-scale graphs

## Next Checks
1. Implement ablation studies comparing HGPROMPT performance with and without the graph template conversion to quantify information loss during heterogeneous-to-homogeneous transformation
2. Conduct extensive hyperparameter sensitivity analysis for prompt dimensions, subgraph sampling parameters, and learning rates to establish robustness
3. Test HGPROMPT on additional heterogeneous graph datasets with different characteristics (e.g., bipartite graphs, knowledge graphs) to evaluate generalizability beyond the three benchmark datasets used