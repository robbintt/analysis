---
ver: rpa2
title: Forward-Backward Reasoning in Large Language Models for Mathematical Verification
arxiv_id: '2308.07758'
source_url: https://arxiv.org/abs/2308.07758
tags:
- reasoning
- answer
- backward
- question
- hours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose FOBAR, a method that combines forward and backward reasoning
  for verification in large language models. FOBAR first samples multiple reasoning
  chains with candidate answers (forward reasoning) and then masks a token in the
  question, asking the LLM to predict the masked token when given a candidate answer
  (backward reasoning).
---

# Forward-Backward Reasoning in Large Language Models for Mathematical Verification

## Quick Facts
- arXiv ID: 2308.07758
- Source URL: https://arxiv.org/abs/2308.07758
- Reference count: 10
- One-line primary result: FOBAR achieves state-of-the-art performance on six mathematical datasets by combining forward and backward reasoning for verification

## Executive Summary
This paper proposes FOBAR, a method that combines forward and backward reasoning to improve mathematical verification in large language models. FOBAR first generates multiple reasoning chains with candidate answers using forward reasoning, then verifies these candidates by asking the LLM to predict masked tokens in the original question given each candidate answer. The final answer is selected based on a probability estimated by combining forward and backward reasoning. Extensive experiments on six mathematical datasets demonstrate that FOBAR outperforms existing verification methods.

## Method Summary
FOBAR combines forward and backward reasoning for mathematical verification in LLMs. The forward reasoning generates multiple candidate answers through sampling, while backward reasoning creates question-answer pairs by masking a token in the original question and asking the LLM to predict it given a candidate answer. The method combines the probabilities from both directions using a weighted product with a smoothing factor α, selecting the answer with the highest combined probability. This approach avoids complex question rewriting by using a simple template-based backward question creation.

## Key Results
- FOBAR achieves state-of-the-art performance across six mathematical datasets (AddSub, MultiArith, SingleEQ, SVAMP, GSM8K, AQuA)
- The method outperforms existing verification methods including Self-Consistency and Self-Verification
- FOBAR demonstrates the effectiveness of combining forward and backward reasoning for improving mathematical verification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backward reasoning can detect and filter out incorrect candidate answers from forward reasoning chains.
- Mechanism: For each candidate answer, mask a number in the original question and ask the LLM to solve for the masked number using the candidate answer as a given. If the LLM successfully predicts the masked number, it strongly suggests the candidate answer is correct.
- Core assumption: LLMs can accurately perform the backward reasoning task when provided with the correct answer context, and this prediction accuracy correlates with the correctness of the original answer.
- Evidence anchors:
  - [abstract] "Instead of using forward or backward reasoning alone, we propose FOBAR to combine FOrward and BAckward Reasoning for verification."
  - [section] "In backward reasoning (Figure 2), we create question-answer pairs by asking LLM to predict a masked token in the question when providing a candidate answer."
  - [corpus] Weak correlation evidence; no direct backward reasoning performance data in corpus.
- Break condition: The backward reasoning task is too complex or ambiguous for the LLM, leading to low accuracy even for correct candidate answers, breaking the correlation assumption.

### Mechanism 2
- Claim: Combining forward and backward reasoning probabilities using a weighted product (with smoothing factor α) improves answer selection over using either alone.
- Mechanism: Compute P(A;forward) as the vote proportion from forward chains, P(A;backward) as the correctness rate in backward chains, then combine as P(A) ∝ P(A;forward)^α * P(A;backward)^(1-α). Select the answer with highest combined probability.
- Core assumption: Forward and backward reasoning errors are at least partially independent, so their combination reduces overall error rate.
- Evidence anchors:
  - [abstract] "By combining backward and forward reasoning, we estimate the probability as P( ˆA) ∝ P( ˆA; forward)^α * P( ˆA; backward)^(1-α), where α ∈ [0, 1] is a smooth factor."
  - [section] "We estimate the probability of a candidate answer ˆAi ∈ A being correct as the proportion of votes it receives, i.e., P( ˆAi; forward) = PMf t=1 I(At = ˆAi)/Mf"
  - [corpus] No direct corpus evidence for the α-weighted combination; assumed from methodology.
- Break condition: Strong correlation between forward and backward reasoning errors (e.g., both fail on the same answer types) would negate the benefit of combination.

### Mechanism 3
- Claim: Using a simple template-based backward question creation is more effective and robust than complex rewriting approaches.
- Mechanism: Mask a number in the question and append "If we know the answer of the above question is {candidate answer}, what is the value of unknown variable x?" rather than rephrasing the entire question into a declarative statement.
- Core assumption: The template approach preserves the original question's structure and meaning while being simpler to apply consistently, leading to more reliable backward reasoning.
- Evidence anchors:
  - [abstract] "Different from Self-Verification (Weng et al., 2022), which needs to rewrite the question into a declarative statement, e.g., 'How many books would John read in 6 weeks?' (with a candidate answer 168) is rewritten into 'John would read 168 books in 6 weeks', we append a simple template to the question without rewriting."
  - [section] "To use backward reasoning in verifying answers, we mask a token in the question and ask the LLM to predict the masked token when a candidate answer is provided."
  - [corpus] No direct comparison evidence in corpus; relies on methodology description.
- Break condition: The template approach fails to create valid backward questions for certain problem types (e.g., non-numerical or complex dependencies), making rewriting necessary.

## Foundational Learning

- Concept: Self-Consistency sampling with temperature to generate diverse reasoning chains
  - Why needed here: FOBAR builds on Self-Consistency by adding backward verification; understanding how diverse chains are generated is crucial for implementing the forward reasoning component.
  - Quick check question: What temperature value was used for sampling in the experiments, and how does temperature affect the diversity of generated chains?

- Concept: Chain-of-Thought (CoT) prompting and its variants (standard CoT vs Complex CoT)
  - Why needed here: FOBAR uses CoT or Complex CoT as the base prompt for forward reasoning; knowing the differences helps in choosing the right base prompt.
  - Quick check question: How does Complex CoT differ from standard CoT in terms of the examples selected for prompting?

- Concept: Template-based prompt engineering for specific task types (e.g., arithmetic reasoning)
  - Why needed here: Backward reasoning relies on a specific template to create questions; understanding template design is key to extending FOBAR to new domains.
  - Quick check question: What is the exact template used for creating backward questions in arithmetic reasoning tasks?

## Architecture Onboarding

- Component map: Forward Reasoning Module -> Candidate Answer Deduplication -> Backward Reasoning for each candidate -> Probability Calculation -> Answer Selection
- Critical path: Forward Reasoning → Candidate Answer Deduplication → Backward Reasoning for each candidate → Probability Calculation → Answer Selection
- Design tradeoffs: Higher Mf/Mb increases verification accuracy but also computation cost and API token usage; α=1 recovers Self-Consistency (no backward), α=0 uses only backward (no forward diversity)
- Failure signatures: Low P(A;backward) for all candidates indicates backward reasoning is failing (wrong template, too hard); high correlation between P(A;forward) and P(A;backward) suggests errors are not independent
- First 3 experiments:
  1. Implement FOBAR with α=1 (should match Self-Consistency performance as baseline)
  2. Test backward reasoning alone (α=0) on a simple dataset to verify the template works
  3. Sweep α from 0 to 1 on a validation set to find optimal value for your LLM/model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FOBAR perform on non-mathematical reasoning tasks compared to mathematical tasks?
- Basis in paper: [explicit] The paper mentions "Extensions to non-mathematical problems are also discussed and validated empirically."
- Why unresolved: The paper focuses on mathematical datasets and does not provide detailed results for non-mathematical reasoning tasks.
- What evidence would resolve it: Conducting experiments on a diverse set of non-mathematical reasoning datasets and comparing FOBAR's performance to other methods.

### Open Question 2
- Question: What is the impact of the smooth factor α on FOBAR's performance across different types of reasoning tasks?
- Basis in paper: [explicit] The paper mentions "α is set to 0.9 for all data sets and LLMs" and studies its effects in Section 4.3.
- Why unresolved: The paper does not explore the optimal α value for different types of reasoning tasks or provide a detailed analysis of its impact.
- What evidence would resolve it: Conducting experiments with varying α values for different reasoning tasks and analyzing the performance trends.

### Open Question 3
- Question: How does FOBAR's performance scale with the size of the language model?
- Basis in paper: [inferred] The paper experiments with three different LLMs (GPT-3.5, GPT-3.5-turbo, and GPT-4) but does not provide a comprehensive analysis of scaling behavior.
- Why unresolved: The paper does not provide a detailed analysis of how FOBAR's performance changes as the size of the language model increases.
- What evidence would resolve it: Conducting experiments with larger language models and analyzing the scaling behavior of FOBAR's performance.

## Limitations

- The paper lacks direct comparison with all relevant baselines, such as ReClor or Direct Optimize.
- No evidence is provided showing that backward reasoning accuracy correlates with final answer correctness.
- The assumption that forward and backward errors are independent is not explicitly analyzed or validated.

## Confidence

- Forward-Backward Combination Effectiveness: Medium
- Backward Reasoning as Filter: Medium
- Template-Based Backward Questions: Medium
- Independence of Forward/Backward Errors: Low

## Next Checks

1. **Correlation Analysis**: Analyze the correlation between backward reasoning accuracy and final answer correctness across different answer types to validate the filtering mechanism assumption.

2. **Error Independence Test**: Compute and visualize the correlation matrix between forward and backward reasoning errors for each candidate answer to quantify the independence assumption underlying the α-weighted combination.

3. **Generalization Test**: Apply FOBAR to a non-arithmetic reasoning dataset (e.g., logical inference or commonsense reasoning) to evaluate whether the template-based backward question approach generalizes beyond mathematical problems.