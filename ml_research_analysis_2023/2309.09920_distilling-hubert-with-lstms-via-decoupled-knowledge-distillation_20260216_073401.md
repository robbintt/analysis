---
ver: rpa2
title: Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation
arxiv_id: '2309.09920'
source_url: https://arxiv.org/abs/2309.09920
tags:
- hubert
- knowledge
- speech
- distillation
- distilhubert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using knowledge distillation (KD) and decoupled
  KD (DKD) to compress HuBERT, a large self-supervised speech model. Instead of distilling
  internal features like previous methods, the authors distill HuBERT's pre-training
  outputs, allowing more flexibility in the student model architecture.
---

# Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation

## Quick Facts
- **arXiv ID:** 2309.09920
- **Source URL:** https://arxiv.org/abs/2309.09920
- **Reference count:** 0
- **Key outcome:** Proposes LSTM-based student model distilled from HuBERT using KD/DKD, reducing parameters below DistilHuBERT while improving PR/ASR performance

## Executive Summary
This paper presents a knowledge distillation approach for compressing HuBERT, a large self-supervised speech model. Rather than distilling internal features, the authors propose distilling HuBERT's pre-training outputs (cluster predictions) to allow more flexible student architectures. They introduce an LSTM-based student model with 4 bidirectional LSTM layers of size 384 that reduces parameters below DistilHuBERT while achieving improved performance on phoneme recognition and automatic speech recognition tasks. The Decoupled Knowledge Distillation (DKD) method, with hyperparameter β=4, further enhances performance on phoneme recognition, ASV, and query-by-example tasks.

## Method Summary
The authors propose distilling HuBERT's cluster prediction capability by training a student model to match the teacher's logits (pre-training outputs) rather than internal features. They use knowledge distillation (KD) and decoupled KD (DKD) loss functions to train an LSTM-based student architecture with 4 bidirectional LSTM layers of size 384. The distillation process uses HuBERT's cluster assignments as pseudo-labels to extract logits from the teacher model, which are then used to train the student via cross-entropy loss. The student model is evaluated on the SUPERB benchmark across various downstream speech tasks including phoneme recognition, automatic speech recognition, keyword spotting, query-by-example, speaker identification, and others.

## Key Results
- LSTM-based student model reduces parameters below DistilHuBERT while improving performance on PR and ASR tasks
- DKD method with β=4 improves scores on PR, ASV, and QbE tasks compared to standard KD
- Proposed models maintain similar memory footprint and execution time to DistilHuBERT
- Performance across tasks is generally stable with KD/DKD, except for significant downgrade in speaker identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation (KD) transfers HuBERT's cluster prediction capability by training the student to match teacher logits rather than internal features.
- Mechanism: The student model learns to predict the same cluster assignments as HuBERT by minimizing the KL divergence between softmax probability distributions of logits, effectively compressing the teacher's pre-training output.
- Core assumption: The teacher's pre-training objective (cluster prediction) can be framed as classification, making logits suitable targets for KD.
- Evidence anchors:
  - [abstract] "we argue that distillation methods based on the model's scores for each class (logits) could be used to compress it."
  - [section] "Given an input speech utterance X encoded into T frames, the pre-training loss is the cross-entropy computed over masked timesteps... This classification loss over softmaxed logits makes it suitable for distillation via KD and DKD."
  - [corpus] Weak - related papers focus on feature-based distillation rather than logit-based methods, suggesting this approach is less explored.
- Break condition: If HuBERT's cluster assignments are not reliable pseudo-labels, the logits will not contain meaningful information for the student to learn.

### Mechanism 2
- Claim: Decoupled Knowledge Distillation (DKD) improves performance by separately weighting target class and non-target class knowledge transfer.
- Mechanism: DKD splits the KL divergence loss into Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD), with hyperparameters α and β controlling their relative importance.
- Core assumption: The NCKD term contains valuable information about non-target classes that is suppressed when the teacher has high confidence in the target class.
- Evidence anchors:
  - [section] "The authors therefore propose to decouple the two terms, weighting them with hyper-parameters α and β instead, resulting in the DKD loss"
  - [section] "The authors show that DKD improves on the performance of KD in computer vision tasks without any additional trainable parameters"
  - [corpus] Weak - corpus papers focus on different distillation techniques, providing limited direct evidence for DKD's effectiveness on HuBERT.
- Break condition: If β is set too high, the student may overfit to non-target class information and lose focus on the primary task.

### Mechanism 3
- Claim: Using an LSTM-based student architecture provides better ASR and phoneme recognition performance compared to Transformer-based distillations.
- Mechanism: LSTMs with 4 bidirectional layers and hidden size 384 capture sequential dependencies in speech data more effectively than shallow Transformer distillations, while using fewer parameters.
- Core assumption: The sequential processing of LSTMs is advantageous for speech tasks that rely heavily on temporal patterns, despite Transformers' superior parallelization.
- Evidence anchors:
  - [abstract] "we propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition."
  - [section] "LSTMs have been extensively used in audio applications and alongside temporal convolutional networks (TCNs) were the standard neural-network method for sequence modeling for years before the introduction of Transformers"
  - [corpus] Weak - corpus papers focus on different architectures and do not provide direct evidence for LSTM superiority in this context.
- Break condition: If the speech task requires global context rather than local sequential patterns, LSTMs may underperform compared to Transformers.

## Foundational Learning

- Concept: Self-supervised learning (SSL) and masked prediction
  - Why needed here: HuBERT is pre-trained using SSL with a masked prediction objective over cluster assignments, which the distillation method must preserve.
  - Quick check question: How does HuBERT generate pseudo-labels for its pre-training objective, and why is this relevant to knowledge distillation?

- Concept: Knowledge distillation and KL divergence
  - Why needed here: The student model is trained using KD to match the teacher's probability distribution over cluster assignments via KL divergence.
  - Quick check question: What is the difference between training with hard labels versus soft targets in the context of knowledge distillation?

- Concept: Decoupled Knowledge Distillation (DKD)
  - Why needed here: DKD separates target class and non-target class knowledge transfer, allowing more effective distillation when the teacher has high confidence.
  - Quick check question: Why might the non-target class knowledge (NCKD) be valuable even when the teacher is confident about the target class?

## Architecture Onboarding

- Component map:
  - Teacher: HuBERT LARGE (24 Transformer layers, 1024 feature size)
  - Student: LSTM-based model (4 bidirectional LSTM layers, 384 hidden size)
  - Convolutional feature encoder (shared architecture with teacher)
  - Linear projection layer (for classification during training)
  - Downstream task adapters (for fine-tuning on SUPERB benchmark)

- Critical path:
  1. Extract logits from HuBERT LARGE using cluster assignments as pseudo-labels
  2. Train LSTM student to match teacher logits via KD or DKD loss
  3. Fine-tune student on downstream tasks using SUPERB methodology
  4. Evaluate performance across content, speaker, semantics, and paralinguistics tasks

- Design tradeoffs:
  - LSTM vs Transformer: LSTMs have linear complexity with sequence length and better streaming capability but worse parallelization during training
  - Model depth vs width: 4 LSTM layers with 384 hidden size balances parameter count with representational power
  - KD vs DKD: DKD allows separate weighting of target and non-target class knowledge but introduces additional hyperparameters

- Failure signatures:
  - Performance degradation on speaker identification tasks (observed in results)
  - Memory allocation scaling quadratically with sequence length (not observed in results)
  - Training instability when β is set too high in DKD experiments

- First 3 experiments:
  1. Train KD LSTM HuBERT with default hyperparameters and evaluate on SUPERB benchmark
  2. Train DKD LSTM HuBERT with β=4 and compare to KD version on PR, ASV, and QbE tasks
  3. Profile memory and execution time for increasing utterance lengths to validate resource efficiency claims

## Open Questions the Paper Calls Out
- How does the hyperparameter α in decoupled knowledge distillation (DKD) affect the performance of the distilled LSTM model?
- Why does the LSTM-based distilled model perform significantly worse on speaker identification (SID) compared to other tasks, despite improvements in phoneme recognition and ASR?
- Can the knowledge distillation method be applied to other self-supervised learning models beyond HuBERT, and what are the potential challenges?

## Limitations
- Architecture specificity limits generalizability of results to other model scales or tasks
- Hyperparameter sensitivity analysis is incomplete, particularly for α and temperature τ
- Evaluation scope is limited to SUPERB benchmark without testing cross-domain generalization

## Confidence

**High Confidence:**
- The proposed LSTM-based student architecture reduces parameters below DistilHuBERT while maintaining similar memory footprint and execution time
- Knowledge distillation via matching HuBERT's cluster prediction outputs (logits) is a valid compression approach
- The DKD method with β=4 shows improvement on PR, ASV, and QbE tasks compared to standard KD

**Medium Confidence:**
- LSTMs provide better ASR and phoneme recognition performance compared to Transformer-based distillations for speech tasks
- The proposed model achieves state-of-the-art performance among HuBERT distillations on the SUPERB benchmark
- The approach generalizes well across multiple speech tasks (content, speaker, semantics, paralinguistics)

**Low Confidence:**
- The specific architectural choices (4 layers, 384 hidden size) are optimal for the compression task

## Next Checks
- Test the proposed KD and DKD methods with different LSTM architectures (varying number of layers from 2-6 and hidden sizes from 256-512)
- Systematically vary β in DKD from 0.5 to 8 in increments of 0.5 and test different temperature values τ in the range [0.5, 2.0]
- Apply the proposed distillation method to a different speech dataset and test performance on additional speech tasks not included in SUPERB