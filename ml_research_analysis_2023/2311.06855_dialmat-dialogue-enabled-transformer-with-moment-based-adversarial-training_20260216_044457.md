---
ver: rpa2
title: 'DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training'
arxiv_id: '2311.06855'
source_url: https://arxiv.org/abs/2311.06855
tags:
- task
- dialfred
- adversarial
- which
- dialmat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DialMAT, a dialogue-enabled embodied instruction-following
  agent that achieves state-of-the-art performance on the DialFRED benchmark. The
  key innovation is Moment-based Adversarial Training (MAT), which applies adversarial
  perturbations to the latent representations of language, image, and action.
---

# DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training

## Quick Facts
- **arXiv ID:** 2311.06855
- **Source URL:** https://arxiv.org/abs/2311.06855
- **Reference count:** 7
- **Primary result:** 39% success rate (SR) and 23% path weighted success rate (PWSR) on DialFRED pseudo test set, winning the DialFRED Challenge at CVPR 2023

## Executive Summary
DialMAT introduces a dialogue-enabled embodied instruction-following agent that achieves state-of-the-art performance on the DialFRED benchmark. The key innovation is Moment-based Adversarial Training (MAT), which applies adversarial perturbations to latent representations of language, image, and action modalities. The model also employs crossmodal parallel feature extraction using foundation models (CLIP and DeBERTa v3) for both language and image inputs. This combination improves success rate by 8 percentage points over the baseline, achieving 39% SR and 23% PWSR on the DialFRED pseudo test set.

## Method Summary
DialMAT uses a two-module architecture consisting of a Questioner module (LSTM with attention for question selection) and a MAPer module (main processing pipeline). The MAPer extracts crossmodal features using foundation models CLIP and DeBERTa v3 for text, and CLIP and ResNet for images, then applies MAT perturbations to these representations before processing through an N-layer transformer to predict actions. The model was trained on the DialFRED dataset with reinforcement learning for question selection and cross-entropy loss for action prediction.

## Key Results
- Achieved 39% success rate (SR) and 23% path weighted success rate (PWSR) on DialFRED pseudo test set
- Improved over baseline by 8 percentage points in success rate
- Won first place in the DialFRED Challenge at CVPR 2023
- Demonstrates effective combination of adversarial training and foundation model features

## Why This Works (Mechanism)

### Mechanism 1
Moment-based Adversarial Training improves model robustness by introducing controlled perturbations into latent representations. MAT computes gradients of loss with respect to perturbations, applies moving averages (mt and vt) to smooth updates, and projects perturbations onto an epsilon sphere. This creates more robust feature representations that generalize better to unseen scenarios.

### Mechanism 2
Crossmodal parallel feature extraction using foundation models enhances representation quality. The model extracts separate embeddings for language (hctxt from CLIP, hdeb from DeBERTa v3) and image (hcimg from CLIP, hres from ResNet), then combines them before MAT application. Foundation models pre-trained on large-scale multimodal data provide richer, more generalizable features than task-specific encoders.

### Mechanism 3
The combination of MAT and crossmodal parallel feature extraction creates synergistic effects. MAT perturbations applied to concatenated foundation model embeddings force the model to learn invariant features across modalities while maintaining high-level semantic information. This interaction produces representations that are both robust and semantically rich.

## Foundational Learning

- **Concept:** Adversarial training and its variants
  - Why needed: Understanding how adversarial perturbations improve model robustness and generalization is crucial for grasping MAT's role in DialMAT
  - Quick check: How does Moment-based Adversarial Training differ from standard adversarial training approaches like FGSM or PGD?

- **Concept:** Vision-language foundation models (CLIP, DeBERTa v3)
  - Why needed: These models provide the initial feature representations that MAT operates on, so understanding their architecture and training objectives is essential
  - Quick check: What are the key architectural differences between CLIP and DeBERTa v3 that make them suitable for crossmodal feature extraction?

- **Concept:** Embodied instruction following tasks
  - Why needed: DialFRED is a specific embodied task requiring navigation, object manipulation, and dialogue understanding, which informs the design choices in DialMAT
  - Quick check: What are the main challenges in embodied instruction following that differentiate it from standard vision-language navigation tasks?

## Architecture Onboarding

- **Component map:** Input → Questioner (question selection) → MAPer (feature extraction + MAT) → Transformer → Action prediction

- **Critical path:** The main processing flow goes through the Questioner module for selecting appropriate questions, then through the MAPer module for feature extraction and MAT application, followed by transformer layers, and finally action prediction

- **Design tradeoffs:** Using multiple foundation models increases representation quality but adds computational overhead; MAT perturbations improve robustness but may hurt clean data performance; crossmodal parallel extraction provides better features but increases memory usage

- **Failure signatures:** High validation loss with low training loss indicates overfitting to adversarial examples; sudden drops in clean data performance suggest MAT perturbations are too aggressive; poor question selection indicates LSTM attention mechanism not learning effectively

- **First 3 experiments:** 1) Ablation study removing MAT perturbations to quantify contribution; 2) Crossmodal extraction evaluation comparing CLIP alone vs. CLIP + DeBERTa v3 + ResNet; 3) Perturbation magnitude sweep varying epsilon values to find optimal strength

## Open Questions the Paper Calls Out

### Open Question 1
How does performance change with different adversarial perturbation magnitudes (epsilon values) in MAT? The paper uses a fixed epsilon but doesn't explore sensitivity or provide ablation studies. Experiments varying epsilon values would reveal optimal ranges and demonstrate robustness to perturbation magnitude.

### Open Question 2
How does crossmodal parallel feature extraction compare to sequential extraction in computational efficiency and performance? The paper introduces parallel extraction but doesn't compare it to sequential methods, leaving uncertainty about benefits and trade-offs. Comparing computational time and performance metrics would demonstrate advantages or disadvantages.

### Open Question 3
How does performance change when using different foundation models for feature extraction? The paper uses CLIP and DeBERTa v3 but doesn't explore performance impact of different combinations or individual models. Experiments with various model combinations would reveal each model's contribution and identify most effective combinations.

## Limitations

- Critical hyperparameters for MAT (epsilon bounds, learning rate η, smoothing coefficients) are unspecified, making exact replication difficult
- Architecture details of N-layer Transformer are missing, particularly number of layers, attention heads, and hidden dimensions
- Improvement over baseline not extensively validated through ablation studies showing individual contributions of MAT and crossmodal feature extraction

## Confidence

- **High confidence:** General architecture design (two-module system with Questioner and MAPer) and overall approach of combining foundation models with adversarial training
- **Medium confidence:** Specific implementation of Moment-based Adversarial Training and its claimed benefits for robustness
- **Low confidence:** Synergistic effects between MAT and crossmodal parallel feature extraction without quantitative ablation evidence

## Next Checks

1. Implement controlled ablation study comparing DialMAT with variants: (a) without MAT perturbations, (b) without crossmodal parallel feature extraction, and (c) without both, measuring individual and combined contributions to success rate improvements

2. Conduct perturbation sensitivity analysis by systematically varying epsilon constraint in MAT and measuring trade-off between robustness and clean data performance across multiple runs

3. Verify alignment quality of foundation model representations by testing performance when replacing CLIP and DeBERTa v3 with alternative vision-language models (e.g., BLIP, Florence) while keeping MAT unchanged