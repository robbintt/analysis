---
ver: rpa2
title: Diversity from Human Feedback
arxiv_id: '2310.06648'
source_url: https://arxiv.org/abs/2310.06648
tags:
- behavior
- human
- divhf
- diversity
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of learning a behavior space
  for diversity optimization from human feedback, addressing the challenge that experts
  often struggle to define appropriate behavioral metrics. The proposed Diversity
  from Human Feedback (DivHF) method queries human preferences to train a behavior
  descriptor model that captures the essence of the desired diversity space.
---

# Diversity from Human Feedback

## Quick Facts
- arXiv ID: 2310.06648
- Source URL: https://arxiv.org/abs/2310.06648
- Authors: 
- Reference count: 12
- Key outcome: This paper introduces the problem of learning a behavior space for diversity optimization from human feedback, addressing the challenge that experts often struggle to define appropriate behavioral metrics. The proposed Diversity from Human Feedback (DivHF) method queries human preferences to train a behavior descriptor model that captures the essence of the desired diversity space. DivHF integrates with quality-diversity optimization algorithms like MAP-Elites to generate diverse solutions. Experiments on the QDax suite demonstrate that DivHF learns behavior descriptors more consistent with human preferences than data-driven approaches, achieving superior performance in terms of accuracy metrics (e.g., preference accuracy of 0.563 vs. 0.171 for Auto-encoder on HalfCheetah). The learned behavior space effectively guides the optimization algorithm to produce solutions that are more diverse under human preference, with higher QD-Scores and Coverage.

## Executive Summary
This paper introduces Diversity from Human Feedback (DivHF), a method for learning behavior spaces that align with human preferences for diversity optimization. Traditional approaches often require experts to define behavior metrics, which can be challenging and may not capture the intended diversity aspects. DivHF addresses this by querying human preferences on triples of solutions and training a behavior descriptor model using cross-entropy loss. The learned behavior space can be integrated with quality-diversity optimization algorithms to generate diverse solutions that better reflect human notions of diversity. Experiments on the QDax suite demonstrate that DivHF outperforms data-driven approaches in terms of preference accuracy and QD metrics.

## Method Summary
DivHF queries human preference on triples of solutions, asking which pair is more similar or diverse. The behavior descriptor model, consisting of a bidirectional LSTM followed by an MLP, is trained using cross-entropy loss to predict these human preferences. The learned behavior space is then integrated with MAP-Elites, a quality-diversity optimization algorithm, to generate diverse solutions. DivHF is evaluated on the QDax suite, which includes HalfCheetah, Walker2D, Ant, and Humanoid tasks. Performance is measured using preference accuracy metrics and QD metrics such as QD-Score, Coverage, and Max Fitness.

## Key Results
- DivHF achieves higher preference accuracy (0.563) compared to data-driven approaches like Auto-encoder (0.171) on HalfCheetah.
- DivHF leads to higher QD-Scores and Coverage when integrated with MAP-Elites compared to data-driven approaches.
- The learned behavior space from DivHF is more consistent with human preferences than behavior spaces learned directly from data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DivHF learns a behavior space that is more aligned with human preferences than data-driven approaches.
- Mechanism: DivHF queries human preference for triples of solutions, training a behavior descriptor model using cross-entropy loss on predicted similarities. This ensures the learned behavior space reflects how humans perceive similarity and diversity.
- Core assumption: Humans can reliably distinguish which pairs of solutions are more similar or diverse, even if they cannot define an explicit behavior space.
- Evidence anchors:
  - [abstract] "DivHF learns a behavior descriptor consistent with human preference by querying human feedback."
  - [section] "With the human preference data, we want to train the model that can give behaviors that are consistent with human preference by a loss function and an optimizer."
  - [corpus] Weak - corpus does not contain direct evidence for this mechanism.
- Break condition: If humans provide inconsistent or noisy preferences, the learned behavior space may not accurately reflect true human preferences.

### Mechanism 2
- Claim: DivHF can be integrated with any diversity optimization algorithm.
- Mechanism: The learned behavior descriptor can be combined with any distance measure to define a diversity measure, which can then be used by arbitrary diversity optimization algorithms.
- Core assumption: The behavior descriptor learned by DivHF is compatible with existing diversity optimization frameworks.
- Evidence anchors:
  - [abstract] "The learned behavior descriptor can be combined with any distance measure to define a diversity measure."
  - [section] "Our proposed method DivHF is general, which can cooperate with arbitrary diversity optimization algorithms."
  - [corpus] Weak - corpus does not contain direct evidence for this mechanism.
- Break condition: If the learned behavior space is not compatible with the chosen diversity optimization algorithm, integration may fail.

### Mechanism 3
- Claim: DivHF outperforms data-driven approaches in generating diverse solutions under human preference.
- Mechanism: By incorporating human feedback, DivHF learns a behavior space that better captures the aspects of diversity that humans care about, leading to solutions that are more diverse under human preference when used with quality-diversity optimization algorithms.
- Core assumption: The diversity aspects captured by DivHF are more relevant to humans than those captured by data-driven approaches.
- Evidence anchors:
  - [abstract] "The results show that DivHF learns a behavior space that aligns better with human requirements compared to direct data-driven approaches and leads to more diverse solutions under human preference."
  - [section] "The results demonstrate that the behavior space learned by DivHF is much more consistent with human requirements than the one learned directly from the data themselves without human feedback."
  - [corpus] Weak - corpus does not contain direct evidence for this mechanism.
- Break condition: If human preferences do not align with the actual diversity needed for the task, DivHF may not outperform data-driven approaches.

## Foundational Learning

- Concept: Human-in-the-loop preference learning
  - Why needed here: DivHF relies on human feedback to learn a behavior space that aligns with human preferences.
  - Quick check question: What type of human feedback does DivHF use to learn the behavior descriptor?

- Concept: Quality-Diversity (QD) optimization
  - Why needed here: DivHF is integrated with QD algorithms like MAP-Elites to generate diverse solutions.
  - Quick check question: What is the goal of QD optimization algorithms?

- Concept: Behavior descriptor learning
  - Why needed here: DivHF learns a behavior descriptor model that maps solutions to a behavior space consistent with human preference.
  - Quick check question: What type of model is used in DivHF to extract features from trajectory data?

## Architecture Onboarding

- Component map: Trajectory data -> Behavior descriptor model (LSTM -> MLP) -> Human preference query -> Cross-entropy loss training -> Learned behavior space -> Integration with MAP-Elites -> Diverse solutions
- Critical path: Human preference queries → Behavior descriptor model training → Integration with QD algorithm → Generation of diverse solutions
- Design tradeoffs: Using human feedback increases alignment with human preferences but requires more effort and may introduce noise. Using data-driven approaches is easier but may not capture the desired diversity aspects.
- Failure signatures: Poor accuracy in predicting human preferences, low QD-Score or Coverage when integrated with QD algorithms, or behavior spaces that do not align with human expectations.
- First 3 experiments:
  1. Evaluate the accuracy of the learned behavior descriptor in predicting human preferences on a held-out test set.
  2. Integrate the learned behavior descriptor with MAP-Elites and compare the QD-Score and Coverage to those obtained using data-driven approaches.
  3. Visualize the solutions in the learned behavior space and compare them to those in the oracle behavior space to assess alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of learned behavior descriptors from human feedback be further improved?
- Basis in paper: [explicit] The authors acknowledge that "compared with the oracle behavior descriptor, the QD-Score and Coverage of DivHF still have a large gap, implying that further improvement is expected in the future."
- Why unresolved: The paper identifies the need for improvement but does not provide specific methods or directions for achieving higher accuracy in learned behavior descriptors.
- What evidence would resolve it: Experimental results demonstrating improved accuracy of behavior descriptors using proposed methods or techniques.

### Open Question 2
- Question: How can the number of human queries required for training behavior descriptors be reduced?
- Basis in paper: [explicit] The authors mention "reducing the number of human queries" as an interesting future work direction.
- Why unresolved: The paper does not explore or propose specific strategies for minimizing the number of queries needed while maintaining or improving the quality of learned behavior descriptors.
- What evidence would resolve it: Studies comparing the performance of behavior descriptors trained with varying numbers of human queries, demonstrating minimal query requirements for effective learning.

### Open Question 3
- Question: How can DivHF effectively handle diverse underlying preferences of different humans?
- Basis in paper: [explicit] The authors note "effectively handling the diverse underlying preferences of different humans" as a future work consideration.
- Why unresolved: The paper does not address how to reconcile or aggregate potentially conflicting preferences from multiple human sources.
- What evidence would resolve it: Experiments showing successful application of DivHF across multiple human evaluators with varying preferences, and analysis of how well the learned behavior descriptors generalize across these differences.

## Limitations
- DivHF relies heavily on the quality and consistency of human preference judgments, which may introduce noise or bias into the learned behavior space.
- The paper does not thoroughly investigate how the number of human preference queries impacts the quality of the learned behavior descriptor, leaving questions about the scalability of the approach.
- Experiments are limited to the QDax suite, and it is unclear how well DivHF would generalize to other domains or types of diversity optimization tasks.

## Confidence
- Claim: DivHF outperforms data-driven approaches in terms of preference accuracy and QD metrics. Label: Medium
- Claim: DivHF can be integrated with any diversity optimization algorithm. Label: Medium
- Claim: DivHF learns a behavior space more aligned with human preferences than data-driven approaches. Label: Medium

## Next Checks
1. Conduct a sensitivity analysis to determine how the number of human preference queries impacts the quality of the learned behavior descriptor.
2. Investigate the robustness of DivHF to noisy or inconsistent human preferences by introducing controlled noise into the preference data and measuring the impact on performance.
3. Evaluate DivHF on a broader range of diversity optimization tasks and domains to assess its generalizability and scalability.