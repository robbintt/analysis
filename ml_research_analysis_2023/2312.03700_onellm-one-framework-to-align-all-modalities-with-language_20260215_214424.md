---
ver: rpa2
title: 'OneLLM: One Framework to Align All Modalities with Language'
arxiv_id: '2312.03700'
source_url: https://arxiv.org/abs/2312.03700
tags:
- arxiv
- video
- onellm
- modalities
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OneLLM introduces a unified multimodal encoder and progressive
  alignment pipeline to support eight data modalities in a single large language model
  framework. Unlike prior MLLMs that use modality-specific encoders, OneLLM employs
  a shared frozen vision-language encoder and a universal projection module composed
  of mixed projection experts and dynamic routing.
---

# OneLLM: One Framework to Align All Modalities with Language

## Quick Facts
- arXiv ID: 2312.03700
- Source URL: https://arxiv.org/abs/2312.03700
- Authors: 
- Reference count: 40
- Primary result: Unified multimodal encoder achieves state-of-the-art or competitive performance across 25 benchmarks with eight data modalities using a frozen vision-language encoder and progressive alignment pipeline

## Executive Summary
OneLLM introduces a unified multimodal encoder framework that supports eight diverse data modalities (image, audio, video, point cloud, depth/normal map, IMU, fMRI) using a single frozen CLIP-ViT encoder rather than modality-specific encoders. The model employs a universal projection module with mixed projection experts and dynamic routing to align all modalities to a language model. Through progressive multimodal alignment starting from image-text pretraining and extending to additional modalities, OneLLM achieves state-of-the-art or competitive performance across 25 diverse benchmarks, demonstrating that joint multimodal training benefits data-scarce modalities and that a unified encoder framework is scalable and effective.

## Method Summary
OneLLM uses a frozen CLIP-ViT as a universal cross-modal encoder, paired with a universal projection module (UPM) containing multiple projection experts and a dynamic routing mechanism. The model progressively aligns modalities by starting with image-text pretraining and sequentially adding audio, video, point clouds, depth/normal maps, IMU, and fMRI data. At each stage, balanced sampling from previously trained and current modalities prevents catastrophic forgetting. A lightweight modality tokenizer converts each modality to fixed-length tokens, and modality tokens help the model switch between different input types. The entire framework is finetuned on a curated 2M-item multimodal instruction dataset.

## Key Results
- Achieves state-of-the-art or competitive performance across 25 diverse benchmarks including VQA, captioning, and classification
- Outperforms specialist models and larger MLLMs despite using a unified framework
- Joint multimodal training substantially benefits data-scarce modalities through cross-modal transfer learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Frozen pretrained transformers can serve as universal cross-modal encoders, eliminating the need for modality-specific encoders.
- **Mechanism**: The paper leverages the transfer learning capability of frozen CLIP-ViT, which was pretrained on extensive image-text data, to act as a universal encoder for all modalities. This is possible because vision-language models learn robust alignment between vision and language, making them adaptable to other modalities through shared representations.
- **Core assumption**: Pretrained vision-language models capture generalizable cross-modal representations that can be transferred to other modalities without further training of the encoder.
- **Evidence anchors**:
  - [abstract]: "We get inspiration from recent works on transferring pretrained transformers to downstream modalities [51, 57, 88, 103]."
  - [section]: "Following previous works [51, 103], we keep the parameters of CLIP-ViT frozen during training."
  - [corpus]: Weak; only general mention of "Multimodal Neurons in Pretrained Text-Only Transformers" without direct evidence for cross-modal transfer.
- **Break condition**: If the frozen encoder fails to capture modality-specific nuances or if the initial pretraining lacks sufficient diversity to generalize across modalities.

### Mechanism 2
- **Claim**: A universal projection module with dynamic routing can effectively align diverse modalities to the language model.
- **Mechanism**: The UPM consists of multiple projection experts (each a stack of transformer layers) and a dynamic modality router. The router calculates routing weights for each expert based on the input, creating a soft mixture-of-experts approach that adapts to different modalities.
- **Core assumption**: Different modalities benefit from specialized projection pathways, and dynamic routing can adaptively select the most appropriate experts for each input.
- **Evidence anchors**:
  - [abstract]: "To increase the model capability, we also design a dynamic router to control the weight of each expert for the given inputs, which turns UPM into soft mixtures-of-experts [66]."
  - [section]: "We propose a dynamic modality router R to control each expert's contribution and increase the model capacity."
  - [corpus]: Missing; no direct corpus evidence supporting the effectiveness of dynamic routing for multimodal alignment.
- **Break condition**: If the router fails to learn meaningful modality distinctions or if the mixture of experts becomes too complex to train effectively.

### Mechanism 3
- **Claim**: Progressive multimodal alignment prevents catastrophic forgetting and handles data imbalance across modalities.
- **Mechanism**: The model starts with image-text alignment, then progressively adds more modalities in stages based on data magnitude. At each stage, data is sampled evenly from both previously trained modalities and the current modality to maintain balance.
- **Core assumption**: Gradual addition of modalities with balanced sampling preserves previously learned representations while allowing the model to adapt to new modalities.
- **Evidence anchors**:
  - [abstract]: "We formulate multimodal-text alignment as a continual learning process [80]. At timestamp t, we have trained the model on a set of modalities M1 ∪ M2 ··· Mt-1, and the current training data is from Mt."
  - [section]: "To prevent catastrophic forgetting, we will sample evenly from both previous trained data and current data."
  - [corpus]: Weak; only general mention of "Three scenarios for continual learning" without specific evidence for this progressive alignment approach.
- **Break condition**: If the balanced sampling fails to prevent mode collapse or if the progressive stages introduce significant performance degradation for earlier modalities.

## Foundational Learning

- **Concept**: Cross-modal representation learning
  - **Why needed here**: Understanding how different modalities can share a common representation space is crucial for building a unified encoder.
  - **Quick check question**: How does CLIP learn to align images and text in a shared embedding space, and why might this be generalizable to other modalities?

- **Concept**: Mixture-of-experts architecture
  - **Why needed here**: The UPM uses multiple projection experts with dynamic routing, requiring understanding of how MoE systems distribute computation and learn specialized pathways.
  - **Quick check question**: What are the advantages and challenges of using soft routing versus sparse routing in mixture-of-experts models?

- **Concept**: Continual learning and catastrophic forgetting
  - **Why needed here**: The progressive alignment approach relies on preventing the model from forgetting previously learned modalities while adding new ones.
  - **Quick check question**: What techniques are commonly used in continual learning to prevent catastrophic forgetting, and how does balanced sampling help?

## Architecture Onboarding

- **Component map**: Input signal → Modality tokenizer → Universal encoder → UPM (experts + router) → Modality tokens → LLM
- **Critical path**: Input signal → Modality tokenizer → Universal encoder → UPM (experts + router) → Modality tokens → LLM
- **Design tradeoffs**:
  - Frozen vs. trainable encoder: Frozen saves memory but may limit adaptation; trainable allows more flexibility but requires more resources
  - Number of projection experts: More experts increase capacity but also complexity and parameter count
  - Router type: Soft routing provides smooth adaptation but may be less interpretable than sparse routing
- **Failure signatures**:
  - Poor performance on data-scarce modalities suggests insufficient transfer from the universal encoder
  - Degraded performance on earlier modalities indicates catastrophic forgetting
  - High memory usage or slow training may suggest the frozen encoder or multiple experts are too large
- **First 3 experiments**:
  1. Ablation study on number of projection experts (1, 3, 5) to find optimal capacity
  2. Comparison of frozen vs. trainable CLIP-ViT to assess transfer learning effectiveness
  3. Router type comparison (constant, sparse, soft) to determine best dynamic routing approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How scalable is OneLLM's unified framework to additional modalities beyond the eight currently supported, and what are the theoretical limits to this scalability?
- **Basis in paper**: [explicit] The authors state "If we want to support new modalities, we can repeat the training episode" and note their framework "can be easily extended to incorporate more data modalities," but do not empirically test beyond eight modalities.
- **Why unresolved**: The paper only demonstrates eight modalities, leaving uncertainty about how well the universal encoder and projection module would generalize to more diverse or higher-dimensional modalities like 3D point clouds or medical imaging.
- **What evidence would resolve it**: Systematic experiments testing OneLLM with additional modalities (e.g., LiDAR, SAR, hyperspectral images) and measuring performance degradation or architectural bottlenecks.

### Open Question 2
- **Question**: Does the frozen CLIP-ViT encoder limit OneLLM's ability to capture modality-specific features compared to training modality-specific encoders from scratch?
- **Basis in paper**: [explicit] The authors acknowledge frozen CLIP-ViT may "break the pretrained vision-language representations" but choose it for memory efficiency, noting this trade-off without directly comparing to trained encoders.
- **Why unresolved**: While the paper shows competitive results, it doesn't establish whether the frozen encoder is truly optimal or merely sufficient, leaving open whether better performance could be achieved with trained encoders.
- **What evidence would resolve it**: Head-to-head comparison of OneLLM using frozen CLIP-ViT versus trained modality-specific encoders across all eight modalities, measuring both performance and computational trade-offs.

### Open Question 3
- **Question**: How does OneLLM's joint training approach affect cross-modal knowledge transfer compared to separate training of modality-specific models?
- **Basis in paper**: [explicit] The authors show joint training "substantially benefits data-scarce modalities" through cross-modal transfer, but don't quantify what specific knowledge transfers occur or whether this creates any negative interference.
- **Why unresolved**: The paper demonstrates improved performance but doesn't analyze which modalities benefit most, whether certain modality combinations help or hinder each other, or if transfer learning effects are consistent across tasks.
- **What evidence would resolve it**: Detailed ablation studies measuring knowledge transfer between specific modality pairs, analysis of attention patterns during cross-modal inference, and systematic evaluation of whether certain modality combinations create interference.

## Limitations

- Reliance on frozen CLIP-ViT encoder may not capture modality-specific nuances as effectively as modality-specific encoders
- Progressive alignment requires careful hyperparameter tuning to prevent catastrophic forgetting
- Lack of direct corpus evidence for key claims about dynamic routing and transfer learning effectiveness

## Confidence

**High confidence**: The experimental results showing state-of-the-art or competitive performance across 25 diverse benchmarks are well-supported by the provided data and demonstrate the effectiveness of the unified framework.

**Medium confidence**: The mechanism of using frozen CLIP-ViT as a universal encoder is theoretically sound and supported by prior work on transfer learning, but lacks direct evidence for its effectiveness across all eight modalities, particularly for data-scarce modalities like IMU and fMRI.

**Low confidence**: The claims about the dynamic routing mechanism's ability to adaptively align different modalities are not well-supported by corpus evidence, and the paper does not provide sufficient ablation studies to demonstrate the necessity and effectiveness of this component.

## Next Checks

1. **Ablation study on encoder freezing**: Compare performance of OneLLM using frozen CLIP-ViT versus a trainable modality-specific encoder for each of the eight modalities to quantify the impact of the universal encoder approach on modality-specific performance.

2. **Router effectiveness analysis**: Conduct ablation studies comparing different router types (constant, sparse, soft) and evaluate their impact on both overall performance and computational efficiency to validate the claimed benefits of dynamic routing.

3. **Cross-modal transfer evaluation**: Design experiments to test the model's ability to transfer knowledge from data-rich modalities (like images) to data-scarce modalities (like IMU and fMRI), measuring performance improvements when training on multiple modalities versus single modalities.