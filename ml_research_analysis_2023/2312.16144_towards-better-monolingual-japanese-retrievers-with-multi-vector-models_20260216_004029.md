---
ver: rpa2
title: Towards Better Monolingual Japanese Retrievers with Multi-Vector Models
arxiv_id: '2312.16144'
source_url: https://arxiv.org/abs/2312.16144
tags:
- retrieval
- training
- japanese
- jacolbert
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents JaColBERT, a Japanese-specific multi-vector
  retriever that outperforms all existing monolingual Japanese retrieval models while
  competing with multilingual models despite being trained on two magnitudes less
  data. The core idea is adapting the ColBERT architecture to Japanese, using contextualized
  late interaction and training on a dataset of 10 million triplets augmented with
  hard negatives.
---

# Towards Better Monolingual Japanese Retrievers with Multi-Vector Models

## Quick Facts
- arXiv ID: 2312.16144
- Source URL: https://arxiv.org/abs/2312.16144
- Authors: 
- Reference count: 21
- This paper presents JaColBERT, a Japanese-specific multi-vector retriever that outperforms all existing monolingual Japanese retrieval models while competing with multilingual models despite being trained on two magnitudes less data.

## Executive Summary
This paper introduces JaColBERT, a Japanese-specific adaptation of the ColBERT architecture for document retrieval. The model leverages contextualized late interaction, representing documents as multiple contextualized vectors rather than single dense representations. Trained on a Japanese subset of the MMARCO dataset with hard negatives, JaColBERT achieves state-of-the-art performance on three Japanese retrieval benchmarks while using significantly less training data than multilingual alternatives.

## Method Summary
JaColBERT adapts the ColBERT architecture to Japanese by initializing from bert-base-japanese-v3 and training on 10 million triplets from the MMARCO dataset augmented with hard negatives. The model encodes documents as bags-of-centroids using multiple contextualized vectors, then computes similarity through late interaction scoring between query and document tokens. Training was performed on 8 NVidia L4 GPUs for 10 hours with specific hyperparameters including batch size 128, learning rate 5e-6, and 2-bit compression.

## Key Results
- JaColBERT achieves an average Recall@10 of 0.813 across JSQuAD, MIRACL, and Mr.TyDi benchmarks
- Outperforms all existing monolingual Japanese retrievers on all datasets
- Competes with strongest multilingual models on out-of-domain tasks despite being trained on two magnitudes less data
- Uses only 110 million parameters while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JaColBERT achieves competitive performance with far less training data by using contextualized late interaction.
- Mechanism: Instead of representing documents as single dense vectors, JaColBERT encodes each document into multiple contextualized vectors (tokens). During retrieval, it performs late interaction between query and document vectors, computing similarity for each query token against all document tokens, then aggregating. This allows fine-grained matching without requiring billion-scale training data.
- Core assumption: Multiple contextualized vectors can capture semantic nuances better than a single dense vector, especially for languages with rich morphology or script differences like Japanese.
- Evidence anchors:
  - [abstract]: "Our strongest model largely outperform all existing monolingual Japanese retrievers on all dataset, as well as the strongest existing multilingual models on all out-of-domain tasks, highlighting the need for specialised models able to handle linguistic specificities."
  - [section]: "ColBERT, and its second version [12], leverage multiple tricks to build upon the strong representation power of transformer models such as BERT [3] to represent documents as bags-of-centroids by representing documents as being composed of many smaller contextualised vectors, rather than a single, large dense representation."
  - [corpus]: Weak evidence - the corpus neighbors mention ColBERT-XM and other multi-vector approaches but do not directly support the specific claim about JaColBERT's mechanism.
- Break condition: If the late interaction computation becomes too slow at scale, or if the multiple vectors fail to capture semantic similarity better than dense embeddings in Japanese.

### Mechanism 2
- Claim: Hard negatives significantly improve the discriminative ability of the retrieval model.
- Mechanism: Hard negatives are generated by retrieving documents similar to the query but known to be irrelevant. These force the model to learn subtle differences between relevant and irrelevant passages, improving precision.
- Core assumption: Including examples that "look relevant" but are not actually relevant teaches the model better discrimination.
- Evidence anchors:
  - [abstract]: Not directly mentioned in abstract.
  - [section]: "In both the existing literature [18] and informal discussions, the importance of hard negatives in training retrieval models is highlighted as particularly important. A hard negative is a negative example that looks very similar to a positive example, and serves to improve a model's ability to discriminate between relevant and 'relevant-looking' irrelevant passages."
  - [corpus]: Weak evidence - corpus does not provide supporting details on hard negative generation.
- Break condition: If hard negatives are incorrectly labeled (false negatives), the model may learn to avoid relevant documents.

### Mechanism 3
- Claim: Using a Japanese-specific base model (bert-base-japanese-v3) is crucial for capturing linguistic nuances.
- Mechanism: Initializing from a Japanese BERT variant rather than multilingual or English BERT allows the model to start with representations already tuned to Japanese tokenization, scripts, and grammar.
- Core assumption: Japanese linguistic features (kanji, hiragana, katakana, lack of spaces) require specialized tokenization and embeddings that general multilingual models may not capture well.
- Evidence anchors:
  - [abstract]: "We believe our results show great promise to support Japanese retrieval-enhanced application pipelines in a wide variety of domains."
  - [section]: "We initialise JaColBERT from Tohoku University's bert-base-japanese-v3. BERT is generally considered as the strongest base model for ColBERT models, as RoBERTa has been anecdotally noted to struggle to learn the kind of representation needed for this approach to work."
  - [corpus]: Weak evidence - corpus does not mention Japanese-specific base models.
- Break condition: If the Japanese base model does not provide meaningful advantages over multilingual embeddings in downstream retrieval tasks.

## Foundational Learning

- Concept: Contextualized embeddings and late interaction
  - Why needed here: JaColBERT's core innovation relies on representing documents as multiple contextualized vectors and computing similarity via late interaction, which is different from standard dense retrieval.
  - Quick check question: What is the difference between early interaction (dot product of dense vectors) and late interaction (per-token similarity aggregation)?

- Concept: Hard negative mining
  - Why needed here: The paper explicitly uses hard negatives to train JaColBERT, which is critical for its strong performance.
  - Quick check question: How do hard negatives differ from random negatives in training retrieval models?

- Concept: Japanese language processing specifics
  - Why needed here: The model is specifically designed for Japanese, so understanding tokenization, script usage, and morphological differences is important.
  - Quick check question: What are the main challenges in processing Japanese text compared to English (e.g., no spaces, multiple scripts)?

## Architecture Onboarding

- Component map: Input -> bert-base-japanese-v3 Encoder -> Token Vectors -> Late Interaction Scoring -> Ranked Documents
- Critical path:
  1. Encode query into token vectors
  2. Encode document into token vectors
  3. Compute similarity scores between each query token and all document tokens
  4. Aggregate similarities (e.g., max or sum)
  5. Rank documents by aggregated score
- Design tradeoffs:
  - Multi-vector vs. single dense vector: Multi-vector allows finer-grained matching but increases computation.
  - Hard negatives vs. random negatives: Hard negatives improve discrimination but require additional computation to generate.
  - Japanese-specific base vs. multilingual base: Japanese base may capture nuances better but lacks multilingual generalization.
- Failure signatures:
  - Low recall: Model may not be capturing semantic similarity well; check if hard negatives are too hard or base model is inadequate.
  - Slow inference: Multi-vector late interaction is more expensive; consider dimensionality reduction or approximation methods.
  - Poor out-of-domain performance: Model may be overfit to MMARCO; need more diverse training data.
- First 3 experiments:
  1. Train JaColBERT without hard negatives to measure their impact on performance.
  2. Replace bert-base-japanese-v3 with a multilingual base model to assess language-specific benefits.
  3. Compare performance with different aggregation methods in late interaction (max vs. sum vs. average).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would JaColBERT perform if trained on the full MMARCO dataset rather than the 10 million triplet subset used in this study?
- Basis in paper: [explicit] The authors explicitly state they only used 10 million triplets from MMARCO and note this was constrained by limited resources, suggesting full-scale training could yield better results.
- Why unresolved: The paper was limited by compute resources and only trained on a subset of available data.
- What evidence would resolve it: Training JaColBERT on the complete MMARCO dataset and evaluating its performance on the three benchmarks would show whether scaling up training data improves results.

### Open Question 2
- Question: Would incorporating the scoring from already strong models as teachers improve JaColBERT's retrieval performance?
- Basis in paper: [explicit] The authors mention that using teacher model scores has been shown to improve retrieval performance in ColBERTv2 but was not implemented in this work due to resource constraints.
- Why unresolved: This teacher-student approach was not explored in the current study.
- What evidence would resolve it: Implementing knowledge distillation from strong multilingual models and evaluating the resulting model on the benchmarks would show if this approach provides gains.

### Open Question 3
- Question: How would JaColBERT perform if trained on the Japanese-specific datasets (MIRACL and Mr.TyDi) rather than just MMARCO?
- Basis in paper: [explicit] The authors note they deliberately kept MIRACL and Mr.TyDi unseen for out-of-domain evaluation but suggest these datasets could be valuable for training.
- Why unresolved: The study deliberately avoided using these datasets during training to test out-of-domain performance.
- What evidence would resolve it: Training JaColBERT on MIRACL and Mr.TyDi data and comparing in-domain vs out-of-domain performance would show the impact of dataset specificity on retrieval quality.

## Limitations

- Data Scope and Generalization: The training data consists of only 10 million triplets from MMARCO, raising questions about generalization to specialized domains or long-tail queries.
- Computational Trade-offs: The multi-vector approach introduces significant computational overhead compared to single-vector dense retrieval models, with no comprehensive latency or throughput measurements provided.
- Reproducibility Constraints: Implementation details for the ColBERT architecture and hard negative generation process are not fully specified, making faithful reproduction challenging.

## Confidence

- High Confidence: The claim that JaColBERT outperforms existing monolingual Japanese retrievers is well-supported by experimental results across three benchmark datasets with statistically significant improvements in Recall@10.
- Medium Confidence: The assertion that JaColBERT achieves "competitive" performance with multilingual models is supported by out-of-domain evaluations, but comparison is somewhat limited as it doesn't include all major multilingual baselines.
- Low Confidence: The claim about JaColBERT's effectiveness with "two magnitudes less data" compared to multilingual models is based on a comparison with unspecified models and lacks detailed analysis of how training data size specifically impacts performance across different model architectures.

## Next Checks

1. **Ablation Study on Hard Negatives**: Train a baseline JaColBERT model without hard negative mining and compare performance degradation across all three benchmarks to quantify the exact contribution of this technique to the reported improvements.

2. **Cross-Lingual Evaluation**: Test JaColBERT's zero-shot cross-lingual retrieval capabilities by evaluating it on Japanese queries against English document collections (and vice versa) to assess whether the model's strong Japanese performance translates to multilingual scenarios.

3. **Scalability Analysis**: Measure inference latency and memory usage for JaColBERT at different document collection sizes (10K, 100K, 1M documents) and compare with single-vector dense retrievers to determine the practical limits of the multi-vector approach for production deployments.