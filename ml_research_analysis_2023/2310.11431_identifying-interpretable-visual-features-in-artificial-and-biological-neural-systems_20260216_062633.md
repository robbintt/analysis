---
ver: rpa2
title: Identifying Interpretable Visual Features in Artificial and Biological Neural
  Systems
arxiv_id: '2310.11431'
source_url: https://arxiv.org/abs/2310.11431
tags:
- neurons
- interpretability
- interpretable
- neural
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of measuring interpretability
  of individual neurons and directions in activation space of neural networks, both
  artificial and biological. The core method is to use an automated metric of interpretability
  based on similarity of maximally exciting images (MEIs) for a neuron or direction,
  validated against human psychophysics data.
---

# Identifying Interpretable Visual Features in Artificial and Biological Neural Systems

## Quick Facts
- **arXiv ID:** 2310.11431
- **Source URL:** https://arxiv.org/abs/2310.11431
- **Reference count:** 24
- **Primary result:** Non-axis aligned directions found via K-means clustering are more interpretable than individual neurons in both artificial and biological neural systems

## Executive Summary
This paper addresses the fundamental challenge of measuring interpretability in neural networks by proposing an automated metric based on similarity of maximally exciting images (MEIs). The key finding is that non-axis aligned directions in activation space—found using methods like K-means clustering—are more interpretable than individual neurons themselves. This supports the "superposition" hypothesis that features are represented across multiple neurons rather than in single, dedicated neurons. The results are validated against human psychophysics data and show that this pattern holds across both artificial neural networks (CNNs) and biological neural recordings from macaque visual cortex.

## Method Summary
The study uses a ResNet50 pre-trained on CIFAR-10 to analyze activation spaces, extracting maximally exciting images for each neuron and direction. An interpretability index (II) is computed as the average pairwise similarity of the top 5 MEIs using LPIPS as the similarity metric. K-means clustering is applied to find non-axis aligned directions in the activation space, which are then compared against individual neurons. The same pipeline is applied to three neuroscience datasets (Vinken et al. 2023; Higgins et al. 2021; Franke et al.). The II scores are validated against human psychophysics data through in-silico experiments measuring predictability and synergy between neuron/direction representations.

## Key Results
- K-means directions show significantly higher interpretability than individual neurons (p < 0.05 across multiple similarity metrics and layers)
- The interpretability gap persists across different layers of the network and is validated against human perceptual judgments
- More interpretable directions demonstrate greater robustness to input perturbations
- The superposition hypothesis is supported by experiments on both artificial CNNs and biological neural data from macaque visual cortex

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directions in activation space that are not aligned with individual neurons can be more interpretable than neurons themselves.
- Mechanism: The model uses K-means clustering to find non-axis aligned directions in the activation space of neural networks. These directions correspond to groups of images that activate similar patterns across multiple neurons, revealing more coherent visual features than single neurons.
- Core assumption: Features in deep networks are represented in superposition, meaning multiple features can be encoded along the same axis by different neurons.
- Evidence anchors:
  - [abstract] "features in deep networks may be represented in superposition, i.e., on non-orthogonal axes by multiple neurons"
  - [section] "we find non-axis aligned directions in the neural state space that are more interpretable than individual neurons"
  - [corpus] Weak. No direct mention of superposition in the corpus papers.
- Break condition: If features are not actually represented in superposition, or if K-means clustering fails to identify meaningful directions, this mechanism would break.

### Mechanism 2
- Claim: An automated interpretability index (II) based on similarity of maximally exciting images (MEIs) correlates with human perceptual judgments of interpretability.
- Mechanism: The II is calculated as the average pairwise similarity of the top 5 MEIs for a given neuron or direction. This metric is validated against human psychophysics data, showing that directions with high II scores are indeed more interpretable to humans.
- Core assumption: The similarity of MEIs for a neuron or direction is a good proxy for human perception of interpretability.
- Evidence anchors:
  - [abstract] "an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability"
  - [section] "we find that the K-means approach detects directions within subsets of uninterpretable neurons as well as within interpretable neurons"
  - [corpus] Weak. The corpus papers don't discuss automated interpretability metrics validated against human judgments.
- Break condition: If the similarity of MEIs does not correlate with human perception, or if the validation against human data is flawed, this mechanism would break.

### Mechanism 3
- Claim: More interpretable neurons are more robust to input perturbations.
- Mechanism: The study investigates the sensitivity of interpretable directions by perturbing each direction and quantifying whether the perturbed direction is still interpretable. It finds that more interpretable directions are less sensitive to input perturbations.
- Core assumption: Interpretability and robustness to input perturbations are related.
- Evidence anchors:
  - [section] "we observe thatµa and µb, corresponding to interpolation factors α “ 0 and α “ 1 have higher II and that the II strongly drops for interpolating directionsvpαq"
  - [section] "we find a weak but significant negative correlation (Spearmanρ “ ´0.18 p ă 3.110´5 between the interpretability and the minimal gradient norm"
  - [corpus] Weak. The corpus papers don't discuss the relationship between interpretability and robustness to input perturbations.
- Break condition: If interpretability and robustness to input perturbations are not related, or if the analysis of sensitivity is flawed, this mechanism would break.

## Foundational Learning

- **Concept:** Neural networks and their activation spaces
  - Why needed here: Understanding how neural networks process information and represent features is crucial for interpreting the results of this study.
  - Quick check question: What is the difference between a neuron and a direction in activation space?

- **Concept:** Interpretability in machine learning
  - Why needed here: The study aims to quantify and improve the interpretability of neural network representations, which is a key challenge in machine learning.
  - Quick check question: Why is interpretability important in machine learning, especially for neural networks?

- **Concept:** Psychophysics and human perception
  - Why needed here: The study validates its automated interpretability metric against human perceptual judgments, requiring an understanding of psychophysics and how humans perceive visual features.
  - Quick check question: How do psychophysics experiments help in understanding human perception of visual features?

## Architecture Onboarding

- **Component map:** CIFAR-10 images -> ResNet50 -> Activation space -> K-means clustering -> Interpretability Index (II) -> Human psychophysics validation
- **Critical path:** 1) Pass images through the neural network to obtain activations 2) Apply K-means clustering to find interpretable directions in activation space 3) Compute the Interpretability Index (II) for each direction 4) Validate the II against human psychophysics data
- **Design tradeoffs:** Using K-means clustering vs. other methods (PCA, ICA, NMF) for finding interpretable directions; choosing the number of clusters (K) in K-means; selecting the similarity metric (LPIPS, color, label) for computing the II
- **Failure signatures:** K-means clustering fails to find meaningful directions; the II does not correlate with human perceptual judgments; the interpretability gap between neurons and directions is not observed
- **First 3 experiments:** 1) Apply K-means clustering to the activation space of a pre-trained neural network and visualize the resulting directions 2) Compute the Interpretability Index (II) for both neurons and K-means directions, and compare their distributions 3) Validate the II against human psychophysics data for a subset of neurons and directions

## Open Questions the Paper Calls Out
The paper itself does not explicitly call out open questions. However, based on the limitations and future directions implied by the work, several important open questions emerge regarding the universality of superposition across architectures, the relationship between interpretability and mathematical concepts like disentanglement, and how interpretability varies across different data domains.

## Limitations
- The analysis of biological neural data relies on three neuroscience datasets without full methodological details on preprocessing and alignment procedures
- Validation against human psychophysics data uses an in-silico approach rather than direct human experiments
- The study focuses primarily on CNNs and natural images, limiting generalizability to other architectures and data domains

## Confidence
- **High confidence**: The finding that K-means directions outperform individual neurons in interpretability (p < 0.05 across multiple similarity metrics and layers)
- **Medium confidence**: The superposition hypothesis explanation, as alternative explanations for the neuron-direction gap cannot be fully ruled out
- **Medium confidence**: The biological neural data results, limited by dataset accessibility and preprocessing details

## Next Checks
1. Replicate the neuron-direction interpretability gap using alternative clustering methods (e.g., Gaussian Mixture Models, hierarchical clustering) to test the robustness of K-means findings
2. Conduct direct human psychophysics experiments comparing neuron vs. direction interpretability rather than relying solely on in-silico validation
3. Test whether the observed effects persist across different network architectures (e.g., Vision Transformers, recurrent networks) and datasets beyond CIFAR-10