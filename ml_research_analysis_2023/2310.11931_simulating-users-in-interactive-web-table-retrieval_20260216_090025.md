---
ver: rpa2
title: Simulating Users in Interactive Web Table Retrieval
arxiv_id: '2310.11931'
source_url: https://arxiv.org/abs/2310.11931
tags:
- retrieval
- user
- query
- information
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of simulated interactive
  web table retrieval sessions by modeling different user types and query strategies.
  It introduces a novel query generation method based on Doc2Query that dynamically
  incorporates growing user knowledge through relevance feedback.
---

# Simulating Users in Interactive Web Table Retrieval

## Quick Facts
- arXiv ID: 2310.11931
- Source URL: https://arxiv.org/abs/2310.11931
- Reference count: 40
- Primary result: Dynamic query reformulation with Doc2Query and relevance feedback outperforms static methods in interactive web table retrieval.

## Executive Summary
This paper investigates simulating interactive web table retrieval sessions by modeling different user types and query strategies. It introduces a novel query generation method based on Doc2Query that dynamically incorporates growing user knowledge through relevance feedback. The study evaluates both query-wise (sDCG) and time-wise cost paradigms to capture realistic search effort. Results show that Doc2Query with relevance feedback outperforms static query generation and that no single table modality suffices for relevance prediction. Hybrid browsing strategies balancing exploitation and exploration yield the best performance.

## Method Summary
The study simulates interactive web table retrieval sessions using the WTR test collection with 60 topics and 1.6M tables. It employs BM25 retrieval across multimodal contexts (page title, text before/after, entity) and implements Doc2Query-based query generation with and without relevance feedback. The SIMIIR 2.0 framework models user behavior through simulated agents that make click decisions and reformulate queries based on observed relevance. Two cost paradigms are evaluated: query-wise measures (sDCG) and time-wise measures (information gain vs. effort in seconds).

## Key Results
- Doc2Query with relevance feedback outperforms static query generation and GPT-3.5 baselines
- No single table modality suffices for relevance prediction; hybrid strategies perform best
- Dual evaluation reveals limitations of query-wise measures and highlights importance of time-based effort assessment
- Simulated users benefit from random exploration due to serendipity effects in long-term search sessions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic query reformulation incorporating relevance feedback outperforms static generation.
- Mechanism: The system uses Doc2Query to generate query terms from tables the simulated user has seen, and refines these terms by filtering for those from relevant tables. This produces queries that reflect the user's growing knowledge and preference.
- Core assumption: Relevance judgments from simulated users are reliable enough to guide query reformulation.
- Evidence anchors:
  - [abstract] "Doc2Query with relevance feedback outperforms static query generation"
  - [section] "This approach is particularly suitable for user simulations in interactive information retrieval since each search iteration integrates the user's newly acquired knowledge"
- Break condition: If relevance feedback is noisy or the Doc2Query model produces irrelevant terms, the reformulation could degrade performance.

### Mechanism 2
- Claim: No single table modality suffices for relevance prediction.
- Mechanism: The simulation tests page titles, text before/after, and entity modalities separately and together. It finds that each modality alone has limitations, but hybrid strategies balancing exploitation and exploration yield best performance.
- Core assumption: Relevance is a multi-dimensional concept that requires context from multiple modalities.
- Evidence anchors:
  - [abstract] "no single table modality suffices for relevance prediction"
  - [section] "Using the relevance scores of all modalities, we simulate users who only decide to click on a snippet based on its relevance and thus examine the complete table"
- Break condition: If the dataset has strong correlations between a single modality and relevance, the benefit of hybrid strategies may diminish.

### Mechanism 3
- Claim: Comprehensive cost evaluation reveals limitations of query-wise measures.
- Mechanism: In addition to Session-based DCG (sDCG), the system measures time-based effort and information gain. This dual evaluation shows that query-wise costs can be too simplistic and hide inefficiencies in browsing strategies.
- Core assumption: Real user effort includes actions beyond formulating queries, such as scanning snippets and making relevance judgments.
- Evidence anchors:
  - [abstract] "Our evaluations include two perspectives on user effectiveness by considering different cost paradigms, namely query-wise and time-oriented measures of effort"
  - [section] "We stress that users' search behavior typically covers more interactions that could result in costs"
- Break condition: If all costs are proportional to query count, the additional evaluation may not change conclusions.

## Foundational Learning

- Concept: Doc2Query query expansion
  - Why needed here: It generates context-aware queries based on document content, enabling dynamic reformulation in interactive sessions.
  - Quick check question: How does Doc2Query differ from traditional query expansion methods like Rocchio feedback?

- Concept: User simulation in IR
  - Why needed here: Simulates realistic search sessions to evaluate retrieval systems without expensive user studies.
  - Quick check question: What are the core interaction steps modeled in user simulation frameworks?

- Concept: Multimodal retrieval
  - Why needed here: Web tables have multiple contextual signals (title, text around, entities) that influence relevance.
  - Quick check question: Why might a single modality fail to capture table relevance?

## Architecture Onboarding

- Component map: BM25 retriever -> simulated user agent -> Doc2Query query generator -> relevance feedback filter -> evaluation metrics (sDCG + time-based gain)
- Critical path: Query generation -> retrieval -> click decision -> relevance judgment -> next query reformulation
- Design tradeoffs: Static query generation is simpler but less adaptive; Doc2Query with feedback is more effective but requires reliable relevance signals.
- Failure signatures: Stagnating sDCG curves, mismatched modality effectiveness, over-exploration leading to high costs.
- First 3 experiments:
  1. Compare static Doc2Query vs. feedback-based Doc2Query on a small topic subset.
  2. Test single modality click decisions vs. hybrid strategies.
  3. Vary browsing depth and measure both sDCG and time-based effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of table modalities (page title, text before/after, entity) perform as relevance proxies when used in hybrid configurations for interactive web table retrieval?
- Basis in paper: [explicit] The paper states that "a single modality cannot comprise the entire notion of relevance" and suggests exploring "different combinations of modalities" as future work, noting that relevance is "multi-dimensional."
- Why unresolved: The paper only evaluates single modalities individually and does not test hybrid configurations, leaving the optimal combination of modalities unexplored.
- What evidence would resolve it: Experiments comparing various weighted combinations of modalities in click decisions and their impact on sDCG and time-wise information gain metrics.

### Open Question 2
- Question: What is the optimal balance between exploitation and exploration strategies in simulated interactive web table retrieval sessions?
- Basis in paper: [explicit] The paper identifies that "random behavior is more effective, especially in the long run" due to serendipity effects, and concludes that "it is more effective to apply a hybrid browsing strategy that emphasizes exploitation but which is also exploratory to some extent."
- Why unresolved: The paper tests different click probability thresholds but does not determine the optimal parameter settings or the precise exploitation/exploration tradeoff that maximizes effectiveness.
- What evidence would resolve it: Systematic parameter tuning experiments across different exploitation-exploration weightings, measuring both short-term and long-term effectiveness.

### Open Question 3
- Question: How does the feedback-based Doc2Query query generation method compare to other query generation approaches in real user studies, beyond simulation?
- Basis in paper: [inferred] The paper demonstrates that feedback-based Doc2Query outperforms GPT-3.5 and static Doc2Query in simulations, but acknowledges this is only validated through simulation rather than real users.
- Why unresolved: The study relies entirely on simulated users, and there is no validation of whether these simulated advantages translate to real-world user behavior and effectiveness.
- What evidence would resolve it: Comparative user studies measuring real user performance with different query generation strategies in interactive table retrieval tasks.

## Limitations

- Simulation framework relies on algorithmically generated relevance judgments rather than real user studies
- Doc2Query effectiveness depends on reliability of WTR collection relevance assessments
- Multimodal context features may not capture all real-world signals users consider when evaluating table relevance

## Confidence

- **High confidence**: The core finding that Doc2Query with relevance feedback outperforms static generation is well-supported by evaluation metrics and theoretical framework
- **Medium confidence**: The claim that no single table modality suffices for relevance prediction is supported by hybrid strategy results, though specific modality contributions need further study
- **Medium confidence**: Dual cost paradigm evaluation reveals meaningful limitations of traditional measures, but time-based metrics rely on assumptions about user behavior

## Next Checks

1. Conduct an ablation study to isolate the contribution of each table modality to relevance prediction accuracy
2. Test the Doc2Query feedback mechanism with varying levels of noise in relevance judgments to assess robustness
3. Compare simulation results with a small-scale user study to validate the behavioral assumptions in the SIMIIR 2.0 framework