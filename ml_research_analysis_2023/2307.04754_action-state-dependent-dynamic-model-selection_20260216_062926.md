---
ver: rpa2
title: Action-State Dependent Dynamic Model Selection
arxiv_id: '2307.04754'
source_url: https://arxiv.org/abs/2307.04754
tags:
- where
- lemma
- page
- e-05
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a reinforcement learning algorithm for dynamic
  model selection under state-dependent rewards and switching costs. The method uses
  a Bellman equation framework with a state-action value function approximated via
  iterative regression on augmented data.
---

# Action-State Dependent Dynamic Model Selection

## Quick Facts
- **arXiv ID**: 2307.04754
- **Source URL**: https://arxiv.org/abs/2307.04754
- **Reference count**: 40
- **Primary result**: RL-based policy for dynamic model selection outperforms fixed model selection and greedy switching strategies in portfolio applications.

## Executive Summary
This paper develops a reinforcement learning algorithm for dynamic model selection when performance depends on both state variables and switching costs. The method uses a Bellman equation framework with a state-action value function approximated via iterative regression on augmented data. Theoretical analysis establishes consistency of the estimated policy, with finite-sample performance validated through simulations. In an empirical portfolio application using S&P500 stocks and macroeconomic covariates, the RL-based policy outperforms both fixed model selection and greedy switching strategies, demonstrating superior cumulative rewards, particularly under transaction costs. The approach handles complex state spaces and avoids computationally intensive dynamic programming.

## Method Summary
The paper proposes an iterative reinforcement learning algorithm (Algorithm 1) that estimates the optimal policy for dynamic model selection by approximating the Q-function through regression on augmented data. The algorithm generates action-state pairs by randomly exploring the state space, computes rewards for each possible action, and updates the Q-function estimate via regression on basis functions. The policy is updated to choose the action with the highest estimated Q-value. Theoretical consistency is established under assumptions about the state process (beta mixing) and reward function (finite moments), while empirical validation uses S&P500 stock data and macroeconomic covariates to demonstrate superior performance over static strategies.

## Key Results
- The RL-based policy consistently estimates the optimal model selection strategy across different market conditions
- Outperforms fixed model selection and greedy switching strategies in terms of cumulative rewards, especially under transaction costs
- Demonstrates robustness to both bull and bear market periods in the 2000-2010 evaluation window
- Handles high-dimensional state spaces with complex interactions between multiple macroeconomic covariates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The algorithm consistently estimates the optimal policy by iteratively approximating the state-action value function using regression.
- **Mechanism**: At each iteration, the algorithm computes rewards for each possible action given the current state, then regresses the sum of the reward and the estimated value of the next state onto a set of basis functions. The regression coefficients define the current estimate of the Q-function. The policy is updated to choose the action with the highest estimated Q-value.
- **Core assumption**: The state-action space is visited sufficiently often by the initial random policy to ensure consistent estimation. The approximation error of the Q-function using the chosen basis functions is bounded.
- **Evidence anchors**:
  - [abstract]: "A Reinforcement learning algorithm is used to approximate and estimate from the data the optimal solution to this dynamic programming problem. The algorithm is shown to consistently estimate the optimal policy..."
  - [section]: "Algorithm 1 summarizes the procedure. For the sake of clarity, in Algorithm 1, Γa n ˆV (j−1) (St) = R (Sa t+1; θa (St)) + γ ˆQ(j−1) (Sa t+1, πj−1 (Sa t+1)); note that ˆQ(j−1) (Sa t+1, πj−1 (Sa t+1)) = maxa′∈A ˆQ(j−1) (Sa t+1, a′), which is ˆV (j−1) (St)."
- **Break condition**: If the initial random policy fails to explore the state-action space adequately (i.e., some actions are never chosen in some states), the algorithm will not converge to the optimal policy.

### Mechanism 2
- **Claim**: The algorithm handles state-dependent rewards and switching costs by incorporating them into the reward function.
- **Mechanism**: The reward function is defined as a deterministic function of the next state and the model's prediction for that state. Switching costs are incorporated into the reward function as a penalty proportional to the turnover of the portfolio weights when switching models. This ensures that the algorithm chooses the model that maximizes the expected total discounted reward, accounting for both the immediate reward and the future costs of switching.
- **Core assumption**: The reward function is additive in time and can be written as R (s; θπ (r)) where s is the next state and θπ (r) is the model's prediction for the next period given the current state r.
- **Evidence anchors**:
  - [abstract]: "Performance is measured in terms of either a negative loss or a utility function, conditional on the state variables."
  - [section]: "In order to describe how rewards can be represented, we need further notation. For each a ∈ A, let θa (Sπ t) be a function of Sπ t for model a taking values in some measurable set. This function summarizes the output of model a for the next period prediction, conditioning on the fact that we are on state Sπ t."
- **Break condition**: If the reward function is not additive in time or cannot be written as a function of the next state and the model's prediction, the algorithm will not work correctly.

### Mechanism 3
- **Claim**: The algorithm's consistency is established under weak assumptions about the state process and the reward function.
- **Mechanism**: The consistency of the estimated policy is proven using techniques from empirical process theory and dynamic programming. The key assumptions are that the state process is a strictly stationary Markov process with exponentially decaying beta mixing coefficients, the reward function has finite moments, and the approximation error of the Q-function using the chosen basis functions is bounded.
- **Core assumption**: The state process satisfies the beta mixing condition and the reward function has finite moments.
- **Evidence anchors**:
  - [section]: "Assumption 2. The beta mixing coefficient can be defined as βq := 1/2 supX Ai∈C,Bj ∈D |Pr (Ai ∩ Bj) − Pr (Ai) Pr (Bj)|, q ≥ 1 where the supremum is over all partitions of the sample space, and C is the sigma algebra generated by (Ss)s≤t and D is the sigma algebra generated by (Ss)s≥t+q."
  - [section]: "Assumption 3. We have that maxa∈A |R (T a·; θa (·))|v,P < ∞ for v ≥ 2."
- **Break condition**: If the state process is not a strictly stationary Markov process with exponentially decaying beta mixing coefficients, or the reward function does not have finite moments, the consistency of the algorithm cannot be established.

## Foundational Learning

- **Concept**: Reinforcement Learning (RL)
  - Why needed here: RL provides a framework for learning optimal policies in sequential decision-making problems. The algorithm uses RL techniques to approximate the optimal policy for model selection.
  - Quick check question: What is the Bellman equation and how is it used in RL to find optimal policies?

- **Concept**: Dynamic Programming
  - Why needed here: The problem of dynamic model selection can be formulated as a stochastic dynamic programming problem. The algorithm uses dynamic programming techniques to solve this problem.
  - Quick check question: What is the difference between value iteration and policy iteration in dynamic programming?

- **Concept**: Empirical Process Theory
  - Why needed here: The consistency of the estimated policy is proven using techniques from empirical process theory. These techniques are used to bound the estimation error of the Q-function.
  - Quick check question: What is the Glivenko-Cantelli theorem and how is it used in empirical process theory?

## Architecture Onboarding

- **Component map**: Raw states -> Augmented data (states + random actions) -> Iterative regression -> Q-function estimates -> Optimal policy
- **Critical path**:
  1. Generate initial random policy and state-action pairs
  2. For each iteration:
     a. Compute rewards for each action
     b. Regress rewards + estimated value of next state onto basis functions
     c. Update policy to choose action with highest estimated Q-value
  3. Output final estimated policy
- **Design tradeoffs**:
  - Complexity of basis functions vs. approximation error
  - Exploration vs. exploitation in initial random policy
  - Computational cost vs. accuracy of estimated policy
- **Failure signatures**:
  - Algorithm fails to converge to optimal policy
  - Estimated policy performs poorly out-of-sample
  - Computational cost is too high
- **First 3 experiments**:
  1. Simulate data from a simple model selection problem and compare the performance of the RL algorithm to a greedy policy.
  2. Vary the complexity of the basis functions and assess the impact on the approximation error and computational cost.
  3. Test the algorithm on a real-world portfolio selection problem and compare its performance to a fixed model selection strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the algorithm's performance scale with increasing model complexity (number of models |A|) in high-dimensional state spaces?
- Basis in paper: [inferred] The paper mentions that the bound does not account for |A| and states "This is equivalent to |A| being relatively small, which is the case for the applications that we have in mind." However, it does not provide explicit analysis for large |A|.
- Why unresolved: The theoretical analysis assumes |A| is bounded and relatively small. The computational complexity and estimation accuracy for large |A| in high-dimensional state spaces remains unexplored.
- What evidence would resolve it: Empirical studies varying |A| and state dimensionality, computational complexity analysis showing scaling behavior, and theoretical bounds incorporating |A| explicitly.

### Open Question 2
- Question: How robust is the algorithm to misspecification of the reward function or state variables?
- Basis in paper: [explicit] The paper states "The novelty is that the procedure is agnostic about the procedure used to construct the models" but does not explicitly analyze robustness to reward function or state variable misspecification.
- Why unresolved: While the paper shows consistency under certain assumptions, it does not explore what happens when these assumptions are violated, particularly regarding reward function or state variable misspecification.
- What evidence would resolve it: Simulation studies with intentionally misspecified reward functions or state variables, analysis of sensitivity to different types of misspecification, and development of diagnostic tools for detecting misspecification.

### Open Question 3
- Question: What are the practical limitations of the algorithm in terms of sample size requirements for consistent estimation?
- Basis in paper: [explicit] The paper provides theoretical convergence rates but does not give concrete guidance on minimum sample sizes needed for practical applications.
- Why unresolved: While the paper establishes consistency under certain conditions, it does not translate these theoretical results into practical sample size requirements for real-world applications.
- What evidence would resolve it: Extensive simulation studies varying sample sizes, empirical applications with different data frequencies, and development of sample size rules of thumb based on problem characteristics.

## Limitations

- The consistency results rely on strong assumptions about the state process (beta mixing) that are difficult to verify empirically
- The algorithm's performance depends heavily on the initial random policy adequately exploring the state-action space
- Theoretical bounds do not account for the number of models |A|, limiting applicability to problems with many candidate models

## Confidence

- **High Confidence**: The RL algorithm's ability to handle state-dependent rewards and switching costs through the reward function formulation
- **Medium Confidence**: The consistency results, given that they rely on standard assumptions in the RL literature that are difficult to verify empirically
- **Medium Confidence**: The empirical performance claims, as the out-of-sample evaluation period (2000-2010) includes both bull and bear markets which could affect generalizability

## Next Checks

1. Implement diagnostic tests for beta mixing coefficients on the actual state process to verify the key assumption for consistency
2. Conduct sensitivity analysis varying the exploration rate in the initial random policy to ensure adequate state-action space coverage
3. Test the algorithm's performance across multiple sub-periods within the evaluation window to assess robustness to different market conditions