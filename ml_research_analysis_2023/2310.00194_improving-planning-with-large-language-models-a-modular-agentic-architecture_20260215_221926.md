---
ver: rpa2
title: 'Improving Planning with Large Language Models: A Modular Agentic Architecture'
arxiv_id: '2310.00194'
source_url: https://arxiv.org/abs/2310.00194
tags:
- list
- goal
- move
- configuration
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular architecture for improving planning
  in large language models (LLMs), inspired by the human prefrontal cortex (PFC).
  The architecture, called Modular Agentic Planner (MAP), consists of specialized
  LLM modules that perform functions like conflict monitoring, state prediction, state
  evaluation, task decomposition, and orchestration.
---

# Improving Planning with Large Language Models: A Modular Agentic Architecture

## Quick Facts
- arXiv ID: 2310.00194
- Source URL: https://arxiv.org/abs/2310.00194
- Reference count: 31
- Key outcome: Modular agentic architecture inspired by prefrontal cortex improves LLM planning performance across multiple benchmarks

## Executive Summary
This paper introduces the Modular Agentic Planner (MAP), a novel architecture for improving planning in large language models by decomposing planning into specialized modules inspired by human prefrontal cortex functions. The architecture consists of six specialized LLM modules - TaskDecomposer, Actor, Monitor, Predictor, Evaluator, and TaskCoordinator - that work together to perform tree search through state space. When evaluated on graph traversal, Tower of Hanoi, PlanBench, and multi-step reasoning tasks, MAP significantly outperforms standard LLM methods and competitive baselines, demonstrating both superior performance and transfer capabilities.

## Method Summary
The LLM-PFC architecture implements planning through a modular decomposition inspired by human prefrontal cortex functions. It uses six specialized modules (TaskDecomposer, Actor, Monitor, Predictor, Evaluator, TaskCoordinator) that interact through well-defined interfaces. Each module is implemented as a separate GPT-4 instance with specific prompting and few-shot in-context learning. The system performs tree search with fixed branching factor (B=2) and depth (L=2), where the Actor proposes actions, Predictor estimates next states, and Evaluator assigns values to terminal states. The architecture is evaluated on three planning tasks (graph traversal, Tower of Hanoi, PlanBench) and an NLP task requiring multi-step reasoning, compared against baselines including zero-shot prompting, in-context learning, chain-of-thought, multi-agent debate, and tree-of-thought.

## Key Results
- MAP significantly outperforms standard LLM methods (zero-shot prompting, in-context learning) across all tested planning tasks
- MAP achieves superior transfer performance compared to baseline methods when evaluated on out-of-distribution problems
- The modular architecture can be effectively combined with smaller, more cost-efficient LLMs like Llama3-70B while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition enables parallelization of distinct cognitive functions that LLMs can perform in isolation but fail to coordinate autonomously.
- Mechanism: The architecture assigns each PFC-inspired function (conflict monitoring, state prediction, state evaluation, task decomposition, orchestration) to a separate LLM module, allowing them to specialize and interact through well-defined interfaces.
- Core assumption: LLMs can reliably perform each individual cognitive function when isolated but struggle with coordinating these functions toward a goal.
- Evidence anchors:
  - [abstract] "We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal."
  - [section] "In the present work, we take a step toward improving planning in LLMs, by taking inspiration from the planning mechanisms employed by the human brain. Planning is generally thought to depend on the prefrontal cortex (PFC) (Owen, 1997; Russin et al., 2020; Brunec & Momennejad, 2022; Momennejad et al., 2018; Momennejad, 2020; Mattar & Lengyel, 2022), a region in the frontal lobe that is broadly involved in executive function, decision-making, and reasoning (Miller & Cohen, 2001)."
- Break condition: If LLM modules cannot maintain consistency when exchanging information across modules, the coordinated behavior breaks down.

### Mechanism 2
- Claim: Tree search through state space using predicted outcomes enables systematic exploration of action sequences.
- Mechanism: The architecture implements recursive search where the Predictor estimates next states and the Evaluator assigns values to terminal states, allowing the system to select actions leading to most valuable predicted outcomes.
- Core assumption: LLMs can generate reasonable state predictions and value estimates when prompted appropriately.
- Evidence anchors:
  - [abstract] "This approach combines action proposal, state prediction, and state evaluation to perform tree search."
  - [section] "It is suggested that the coordinated activity of multiple PFC subregions performs tree search during planning (Owen, 1997; Daw et al., 2005; Wunderlich et al., 2012; Doll et al., 2015). Thus, our approach combines action proposal, state prediction, and state evaluation to perform tree search."
- Break condition: If state predictions become unreliable or diverge significantly from actual outcomes, the search becomes ineffective.

### Mechanism 3
- Claim: Hierarchical task decomposition reduces complex goals into manageable subgoals that can be solved sequentially.
- Mechanism: The TaskDecomposer module breaks down high-level goals into intermediate subgoals, which are then pursued one at a time with the TaskCoordinator verifying completion before proceeding.
- Core assumption: LLMs can generate valid intermediate subgoals that progressively move toward the final goal when properly prompted.
- Evidence anchors:
  - [abstract] "The architecture improves planning through the interaction of specialized modules that break down a larger problem into multiple brief automated calls to the LLM."
  - [section] "This approach is inspired by the role that aPFC plays in task decomposition. This involves the decomposition of tasks into smaller, more manageable tasks, and the coordinated sequential execution of these component tasks (Ramnani & Owen, 2004)."
- Break condition: If subgoals are not properly aligned with the final goal or if the TaskCoordinator cannot accurately assess subgoal completion, the planning process fails.

## Foundational Learning

- Concept: State space representation and transition modeling
  - Why needed here: The architecture relies on accurate state prediction to explore possible future states during planning.
  - Quick check question: Can you explain how the Predictor module estimates the next state given current state and proposed action?

- Concept: Tree search algorithms and heuristic evaluation
  - Why needed here: The system performs limited-depth tree search to evaluate action sequences based on predicted state values.
  - Quick check question: How does the Evaluator estimate the value of a predicted state in relation to the goal?

- Concept: Task decomposition and hierarchical planning
  - Why needed here: Complex planning tasks are broken down into smaller subgoals that can be solved sequentially.
  - Quick check question: What strategy does the TaskDecomposer use to generate intermediate subgoals from a final goal?

## Architecture Onboarding

- Component map:
  - State → TaskDecomposer → (Search loop with Actor/Monitor/Predictor/Evaluator) → TaskCoordinator → Plan
- Critical path: State → TaskDecomposer → (Search loop with Actor/Monitor/Predictor/Evaluator) → TaskCoordinator → Plan
- Design tradeoffs:
  - Fixed branching factor (B=2) and search depth (L=2) balance computational cost with planning quality
  - Separate LLM instances for each module enable specialization but increase resource usage
  - Prompt-based specialization avoids fine-tuning but may be less reliable than trained modules
- Failure signatures:
  - Invalid actions from Actor despite Monitor: Monitor logic or constraints need adjustment
  - Poor state predictions from Predictor: Prompt or context examples need refinement
  - Subgoals that don't lead toward final goal: TaskDecomposer strategy needs revision
  - Excessive search iterations: Search depth or branching factor may need adjustment
- First 3 experiments:
  1. Test each module in isolation with representative inputs to verify basic functionality
  2. Run end-to-end planning on simple Tower of Hanoi problems with 2 disks
  3. Evaluate graph traversal on small graphs with known optimal paths to verify search quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LLM-PFC architecture be effectively combined with smaller, more cost-efficient LLMs to achieve similar performance?
- Basis in paper: [explicit] The paper mentions that LLM-PFC can be effectively combined with smaller and more cost-efficient LLMs like Llama3-70B.
- Why unresolved: The paper only briefly mentions this potential combination without providing detailed results or analysis of its effectiveness.
- What evidence would resolve it: Detailed experiments comparing the performance of LLM-PFC when combined with different sizes of LLMs, along with cost and efficiency analysis.

### Open Question 2
- Question: How does the performance of LLM-PFC scale with the complexity of the planning tasks?
- Basis in paper: [inferred] The paper tests LLM-PFC on specific planning tasks but does not explore how it performs on increasingly complex tasks.
- Why unresolved: The experiments conducted focus on a limited set of tasks, and there is no exploration of how the architecture handles tasks of varying complexity.
- What evidence would resolve it: Experiments testing LLM-PFC on a wider range of planning tasks with varying levels of complexity, and analysis of how performance changes with task difficulty.

### Open Question 3
- Question: What are the limitations of using prompting and in-context learning for specializing LLM-PFC's modules?
- Basis in paper: [explicit] The paper mentions that the inherent limitations of prompting and in-context learning may contribute to less than optimal performance on Tower of Hanoi.
- Why unresolved: The paper does not provide a detailed analysis of the specific limitations of these methods for module specialization.
- What evidence would resolve it: A comparative study of LLM-PFC's performance when using different methods for module specialization, such as fine-tuning, and analysis of the trade-offs between these methods.

## Limitations
- Performance gains may not generalize to more complex or open-ended planning scenarios beyond the tested benchmarks
- Reliance on prompt engineering and few-shot examples may limit scalability to domains requiring nuanced reasoning or domain-specific knowledge
- The architecture requires multiple separate LLM instances, increasing computational resource requirements

## Confidence

- High confidence: The modular architecture improves planning performance compared to standard LLM methods (zero-shot prompting, in-context learning) on the tested tasks
- Medium confidence: The architecture demonstrates superior transfer across tasks compared to baseline methods
- Medium confidence: The modular approach can be effectively combined with smaller, more cost-efficient LLMs

## Next Checks

1. **Cross-task generalization test**: Evaluate MAP on a novel planning task (e.g., Blocks World or a new graph problem) that shares structural similarities with training tasks but requires different domain knowledge to assess true transfer capabilities.

2. **Module failure robustness**: Systematically disable individual modules (e.g., Predictor or Evaluator) and measure performance degradation to identify which components are most critical for planning success and where redundancy might be needed.

3. **Scaling efficiency validation**: Test MAP with smaller LLM variants (e.g., GPT-3.5 or specialized smaller models) to empirically verify the claim that the architecture can maintain performance with more cost-efficient models, measuring both accuracy and computational costs.