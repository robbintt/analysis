---
ver: rpa2
title: Adaptive Sampling for Deep Learning via Efficient Nonparametric Proxies
arxiv_id: '2311.13583'
source_url: https://arxiv.org/abs/2311.13583
tags:
- loss
- data
- sampling
- sketch
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of improving neural network training
  speed through data sampling, while maintaining or improving accuracy. The key insight
  is that a static sampling approach can be made nearly as effective as dynamic methods
  by using a novel sketch-based approximation of the Nadaraya-Watson estimator to
  predict the importance of each data point.
---

# Adaptive Sampling for Deep Learning via Efficient Nonparametric Proxies

## Quick Facts
- **arXiv ID**: 2311.13583
- **Source URL**: https://arxiv.org/abs/2311.13583
- **Reference count**: 11
- **Key outcome**: Nadaraya-Watson sketch (NWS) enables static sampling to match dynamic methods' performance, achieving up to 1.9x speedup while improving final accuracy on four datasets.

## Executive Summary
This paper introduces the Nadaraya-Watson sketch (NWS), a novel approach for adaptive data sampling in deep learning that bridges the gap between static and dynamic sampling methods. The core innovation is a locality-sensitive hashing-based approximation of the Nadaraya-Watson estimator that learns to predict the importance of each data point during training. By maintaining O(Nd) training complexity and O(1) inference complexity, the NWS enables efficient importance sampling that adapts to changing network parameters throughout training. Experiments demonstrate that this approach outperforms baseline sampling strategies in both wall-clock time and final accuracy across four different datasets.

## Method Summary
The method centers on the Nadaraya-Watson sketch (NWS), which approximates kernel regression using locality-sensitive hashing to enable efficient loss estimation for importance sampling. During a warm-up phase, the NWS is populated with true loss values from forward passes. In subsequent iterations, the sketch is queried to estimate losses without additional forward passes, enabling importance sampling based on these estimates. The approach uses sign random projection LSH kernels with median-of-means estimation to provide exponential convergence guarantees while maintaining low computational overhead. This allows the sampling distribution to adapt to changing network parameters throughout training without the cost of full dynamic methods.

## Key Results
- Achieves up to 1.9x speedup compared to baseline sampling while improving final accuracy
- Demonstrates exponential convergence guarantees for the Nadaraya-Watson sketch approximation
- Outperforms static sampling methods across four different datasets (MRPC, Financial-phrasebank, Twitter-financial-news, Sentiment140)
- Maintains O(Nd) training complexity and O(1) inference complexity for scalable implementation

## Why This Works (Mechanism)

### Mechanism 1
The NWS approximates the Nadaraya-Watson kernel regression estimator with exponential convergence guarantees using locality-sensitive hashing. LSH maps data points into buckets where weighted and unweighted sketches accumulate kernel-weighted sums. The ratio of median-of-means estimates over these sketches approximates the kernel regression ratio with bounded error, provided the LSH collision probability forms a valid kernel and sufficient collisions occur in each bucket.

### Mechanism 2
The NWS estimates data point losses without explicit forward passes by storing kernel-weighted averages during warm-up and querying them in subsequent iterations. This provides an efficient proxy for the loss that adapts to changing network parameters, assuming the loss landscape is sufficiently smooth and predictable for nonparametric kernel regression approximation.

### Mechanism 3
Dynamic importance sampling based on NWS estimates focuses training on high-loss data points, accelerating convergence. Points with higher estimated losses are sampled more frequently, ensuring the model focuses on "hard" examples that contribute most to the loss. This strategy is effective when the loss-loss relationship is strong and the NWS provides reliable estimates.

## Foundational Learning

- **Nonparametric kernel regression (Nadaraya-Watson estimator)**: Provides a flexible, model-free way to estimate conditional expectations. *Quick check: Can you derive the Nadaraya-Watson estimator from the definition of conditional expectation?*

- **Locality-sensitive hashing (LSH) and LSH kernels**: Efficiently approximates kernel sums using hash-based bucketing. *Quick check: What are the key properties of an LSH family, and how do they relate to the kernel function used in the NWS?*

- **Importance sampling in stochastic optimization**: Accelerates optimization by focusing on informative examples. *Quick check: How does importance sampling reduce the variance of stochastic gradient estimates, and what is the optimal importance sampling distribution for SGD?*

## Architecture Onboarding

- **Component map**: Data points -> LSH hash functions -> Sketch buckets (weighted + unweighted) -> Loss estimates -> Importance sampling -> Model training -> NWS update

- **Critical path**: Warm-up iterations: forward pass → compute losses → update NWS; Subsequent iterations: query NWS → sample data → train model → update NWS

- **Design tradeoffs**: Sketch size (R, W) vs memory/computation; LSH family choice affects kernel quality; Update frequency vs adaptation quality

- **Failure signatures**: Poor accuracy/slow convergence (inaccurate loss estimates); High memory usage (oversized sketch); Slow training (expensive LSH computation)

- **First 3 experiments**: 1) Validate NWS approximation on small dataset with known ground truth; 2) Compare different LSH families on benchmark dataset; 3) Benchmark end-to-end training speed/accuracy on MRPC with different sketch sizes

## Open Questions the Paper Calls Out

- **Scalability with dimensionality and dataset size**: How does NWS performance scale with high-dimensional data and large-scale datasets beyond the medium-sized experiments tested?

- **Extension to non-LSH kernels**: Can the NWS framework be effectively applied to other kernel types like Gaussian or polynomial kernels beyond the LSH kernels explored?

- **Performance in online learning**: How does NWS perform in streaming data scenarios with concept drift compared to traditional online learning algorithms?

- **Impact of warm-up phase length**: What is the optimal warm-up phase length across different datasets and model architectures, and how does it affect final performance?

## Limitations

- Theoretical convergence guarantees depend on specific data distribution assumptions that may not hold in practice for neural network training
- Warm-up phase introduces initial overhead that could slow training on very small datasets before benefits materialize
- Empirical validation is limited to relatively small-scale experiments, leaving scalability questions unanswered

## Confidence

- **High confidence**: The core NWS technical contribution and O(Nd) complexity claims are well-founded and clearly demonstrated
- **Medium confidence**: Empirical speedup and accuracy improvements are convincing but based on small-scale experiments with limited generalization
- **Low confidence**: Theoretical exponential convergence claims may not fully translate to practice given complex, non-stationary loss landscapes in neural network training

## Next Checks

1. Rigorously test Theorem 2.1's assumptions on real neural network training data to verify LSH kernel properties and data distribution conditions

2. Conduct scalability experiments on ImageNet-scale datasets and deeper models (ResNet, ViT) to verify speedup benefits scale with problem size

3. Perform systematic ablation studies varying NWS parameters (R, W, warm-up length) across different datasets to identify robust default settings and understand sensitivity