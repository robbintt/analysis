---
ver: rpa2
title: Multiobjective Evolutionary Pruning of Deep Neural Networks with Transfer Learning
  for improving their Performance and Robustness
arxiv_id: '2302.10253'
source_url: https://arxiv.org/abs/2302.10253
tags:
- network
- which
- dataset
- mo-evoprunedeeptl
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MO-EvoPruneDeepTL is a multi-objective evolutionary pruning algorithm
  for deep neural networks that integrates transfer learning and considers performance,
  complexity, and robustness. It evolves pruning patterns in the last layers of pre-trained
  networks using a genetic algorithm guided by three objectives: classification accuracy,
  number of active neurons, and OoD detection capability.'
---

# Multiobjective Evolutionary Pruning of Deep Neural Networks with Transfer Learning for improving their Performance and Robustness

## Quick Facts
- arXiv ID: 2302.10253
- Source URL: https://arxiv.org/abs/2302.10253
- Reference count: 16
- Multi-objective evolutionary pruning of deep neural networks with transfer learning improves performance, complexity, and robustness.

## Executive Summary
MO-EvoPruneDeepTL is a multi-objective evolutionary pruning algorithm for deep neural networks that integrates transfer learning and considers performance, complexity, and robustness. It evolves pruning patterns in the last layers of pre-trained networks using a genetic algorithm guided by three objectives: classification accuracy, number of active neurons, and out-of-distribution (OoD) detection capability. Experiments on six datasets show promising results in all objectives with a trade-off between accuracy, robustness, and network complexity. The algorithm identifies key neurons influencing predictions and enables improved performance through ensemble modeling of diverse pruned architectures.

## Method Summary
The approach uses NSGA-II to evolve binary pruning patterns for the fully-connected layers of a pre-trained ResNet-50 feature extractor. The chromosome encodes which connections in the dense layer are active (1) or pruned (0). Each candidate solution is decoded into a sparse network, trained on the in-distribution (InD) dataset using SGD, and evaluated on classification accuracy, memory usage (percentage of active neurons), and OoD detection capability (AUROC using the ODIN method). The evolutionary process iteratively selects, crosses over, and mutates individuals to converge to a Pareto front of optimal pruning patterns balancing the three objectives.

## Key Results
- Evolutionary pruning successfully balances classification accuracy, model complexity, and OoD detection capability.
- Ensemble modeling of diverse pruned architectures from the Pareto front improves both accuracy and robustness.
- Identified pruning patterns preserve key input regions influencing predictions, as confirmed by Grad-CAM visualizations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evolutionary search efficiently prunes the last dense layers while preserving performance and robustness.
- Mechanism: MO-EvoPruneDeepTL replaces the dense layers of a pre-trained ResNet-50 with a sparse layer whose connections are encoded as a binary chromosome. NSGA-II evolves populations of such chromosomes, evaluating each pruned network on accuracy (ACCval), memory usage (MEM), and OoD detection capability (AUROC). The multi-objective optimization balances reduced complexity against maintained generalization and robustness.
- Core assumption: Pruning can be localized to the dense layers without significantly harming feature extraction from convolutional layers.
- Evidence anchors:
  - [abstract] "evolves pruning patterns in the last layers of pre-trained networks using a genetic algorithm guided by three objectives: classification accuracy, number of active neurons, and OoD detection capability."
  - [section] "pruning can be defined as a binary vector p = {pj}P j=1, where P denotes the length of the feature vectors extracted by Fφ(x)... pj = 0 indicates that neural links that connect the j-th component of the feature vector to the rest of neurons in the dense part Gθ(·) of the network are disconnected."
  - [corpus] Weak: no corpus neighbor directly discusses evolutionary pruning of dense layers in transfer learning.
- Break condition: If pruning patterns harm feature extraction or lead to overfitting on small datasets, accuracy and AUROC will drop despite complexity gains.

### Mechanism 2
- Claim: Out-of-Distribution detection improves robustness by identifying inputs from unseen distributions.
- Mechanism: The ODIN method scores each input by its maximum softmax probability after temperature scaling (T=1000). AUROC is computed over InD test data and OoD samples from other datasets. Lower scores for OoD inputs indicate better robustness.
- Core assumption: OoD samples tend to have lower maximum softmax probabilities than InD samples, and this difference can be amplified via temperature scaling.
- Evidence anchors:
  - [abstract] "the robustness measured by the capability of a OoD detection technique to detect InD and OoD samples."
  - [section] "ODIN... relies on the simple observation that ID instances tend to have greater Maximum Softmax Probability... By simply applying a threshold to this score, they achieved acceptable performance..."
  - [corpus] Weak: corpus does not contain neighbors focusing on OoD detection in the context of evolutionary pruning.
- Break condition: If OoD and InD distributions overlap significantly, AUROC will be low, indicating that robustness gains are minimal.

### Mechanism 3
- Claim: Ensemble modeling of diverse pruned networks improves both accuracy and robustness.
- Mechanism: Models from different regions of the Pareto front have complementary pruning patterns. Ensembling them averages predictions, reducing variance and improving classification of both InD and OoD samples.
- Core assumption: Diversity in pruning patterns leads to diverse decision boundaries, which ensemble averaging can exploit.
- Evidence anchors:
  - [abstract] "by virtue of the diversity within the Pareto front of pruning patterns produced by the proposal, it is shown that an ensemble of differently pruned models improves the overall performance and robustness of the trained networks."
  - [section] "The overall performance in the three cases is positive since the diversity of the models allows us to find new models that improve the accuracy for each quantile interval..."
  - [corpus] Weak: no corpus neighbor directly discusses ensembles of pruned networks for robustness.
- Break condition: If pruned models are too similar (low diversity), ensemble benefits will be negligible.

## Foundational Learning

- Concept: Multi-Objective Evolutionary Algorithms (MOEAs)
  - Why needed here: MO-EvoPruneDeepTL must optimize accuracy, complexity, and robustness simultaneously; MOEAs are designed for such trade-offs.
  - Quick check question: What is the main advantage of using a Pareto front instead of optimizing a weighted sum of objectives?
- Concept: Transfer Learning
  - Why needed here: Pre-trained ResNet-50 provides rich feature extraction, allowing pruning to focus on task-specific dense layers without retraining the whole network.
  - Quick check question: Why is freezing the convolutional layers important in this approach?
- Concept: Out-of-Distribution (OoD) Detection
  - Why needed here: Robustness is measured by how well the pruned network identifies samples from unseen distributions; ODIN is used for this purpose.
  - Quick check question: What role does temperature scaling play in the ODIN method?

## Architecture Onboarding

- Component map: Pre-trained ResNet-50 → Sparse dense layer (evolved by NSGA-II) → Output layer. ODIN detector evaluates AUROC on OoD datasets.
- Critical path: Initialize population → Decode chromosomes to sparse networks → Train on InD → Evaluate ACCval, MEM, AUROC → Select, crossover, mutate → Repeat until convergence.
- Design tradeoffs: Pruning reduces complexity but risks accuracy; temperature scaling improves OoD detection but adds computation; ensemble improves robustness but increases inference time.
- Failure signatures: Accuracy drops >10% after pruning; AUROC <0.8 on OoD; ensemble fails to improve over best individual.
- First 3 experiments:
  1. Run MO-EvoPruneDeepTL on CATARACT dataset, compare ACCval/MEM/AUROC of best Pareto solutions to non-pruned baseline.
  2. Visualize Grad-CAM heatmaps for top 10 neurons in Pareto front to confirm key input regions are preserved.
  3. Build ensembles from models in different accuracy/AUROC quantiles and measure ensemble ACCval and AUROC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MO-EvoPruneDeepTL's robustness objective (OOD detection) impact the generalization performance on unseen datasets compared to traditional pruning methods?
- Basis in paper: [explicit] The paper mentions that robustness is rarely considered an objective in NAS and introduces an OoD detection technique as an additional objective in MO-EvoPruneDeepTL.
- Why unresolved: The paper demonstrates improved performance on specific OoD datasets but doesn't thoroughly compare generalization across truly unseen distributions or different OoD detection methods.
- What evidence would resolve it: Comparative experiments on multiple unseen datasets using different OoD detection techniques and analysis of how robustness correlates with true generalization.

### Open Question 2
- Question: What is the optimal balance between the number of active neurons and model robustness for different types of datasets and tasks?
- Basis in paper: [explicit] The paper observes a trade-off between accuracy, robustness, and network complexity, noting that a minimum number of active neurons is needed for balanced models.
- Why unresolved: The paper shows this trade-off exists but doesn't provide systematic guidelines for determining optimal complexity-robustness trade-offs across different problem domains.
- What evidence would resolve it: Empirical studies across diverse dataset types showing how optimal neuron counts vary with task complexity, dataset size, and domain characteristics.

### Open Question 3
- Question: Can the pruning patterns discovered by MO-EvoPruneDeepTL be transferred or adapted to different but related tasks through transfer learning?
- Basis in paper: [inferred] The paper uses transfer learning in its approach but doesn't explore whether the evolved pruning patterns themselves can be transferred between tasks.
- Why unresolved: The study focuses on evolving pruning patterns for specific tasks without investigating the transferability of these patterns themselves across related problems.
- What evidence would resolve it: Experiments showing performance when using pruning patterns evolved for one task as starting points for related tasks, measuring adaptation efficiency and performance retention.

## Limitations
- Pruning is limited to dense layers; effects on convolutional feature extraction are not explored.
- OoD detection relies on ODIN's temperature scaling, which may be sensitive to hyperparameter choice.
- Ensemble improvements are asserted but not quantified against single best models.

## Confidence
- **High**: Evolutionary pruning framework design, dataset usage, and multi-objective evaluation setup are well-specified.
- **Medium**: Claims about robustness and ensemble improvements need more empirical validation.
- **Low**: Long-term generalization of pruned models on unseen domains is not evaluated.

## Next Checks
1. **Ablation on Pruning Scope**: Compare performance and robustness when pruning only dense layers vs. also pruning early convolutional blocks.
2. **Temperature Sensitivity**: Measure AUROC across a range of temperature values in ODIN to find optimal settings per dataset.
3. **Ensemble vs. Single Model**: Quantify accuracy and AUROC improvements from ensembles relative to the single best model in each Pareto front.