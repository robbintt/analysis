---
ver: rpa2
title: Towards human-like spoken dialogue generation between AI agents from written
  dialogue
arxiv_id: '2310.01088'
source_url: https://arxiv.org/abs/2310.01088
tags:
- spoken
- dialogue
- dialogues
- units
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHATS (CHatty Agents Text-to-Speech), a discrete
  token-based system for generating spoken dialogues from written dialogues. CHATS
  addresses the challenge of transforming written dialogues, which lack elements like
  backchannels, laughter, and smooth turn-taking, into human-like spoken dialogues.
---

# Towards human-like spoken dialogue generation between AI agents from written dialogue

## Quick Facts
- arXiv ID: 2310.01088
- Source URL: https://arxiv.org/abs/2310.01088
- Reference count: 40
- Primary result: CHATS generates more natural spoken dialogues with backchannels and turn-taking from written dialogues than text-to-speech baselines

## Executive Summary
This paper introduces CHATS (CHatty Agents Text-to-Speech), a discrete token-based system for generating spoken dialogues from written dialogues. CHATS addresses the challenge of transforming written dialogues, which lack elements like backchannels, laughter, and smooth turn-taking, into human-like spoken dialogues. The system employs a dual-tower Transformer architecture to generate discrete acoustic tokens, conditioning on phonemes for speaker content and generating listener responses like backchannels and laughter unconditionally. A novel turn-taking mechanism predicts appropriate silence durations and overlapping speech to mimic natural human conversation. Evaluations show that CHATS outperforms text-to-speech baselines, producing more interactive and fluid spoken dialogues while retaining clarity and intelligibility. Human evaluations further demonstrate that CHATS generates significantly more natural dialogues compared to baselines, though there is still room for improvement to match human-level performance.

## Method Summary
CHATS transforms written dialogues into human-like spoken dialogues through a dual-tower Transformer architecture that generates discrete acoustic tokens. The system conditions on phoneme sequences for the speaker side while unconditionally generating listener responses like backchannels and laughter. A turn-taking mechanism predicts when to insert silence or allow overlapping speech based on phoneme sequences. The approach uses pre-trained discrete HuBERT representations, a multi-speaker DLM for two-channel generation, and a speaker embedding in the vocoder to maintain speaker characteristics. The system was trained on an internal Japanese spoken dialogue corpus (74h) with pre-training on the Corpus of Spontaneous Japanese (CSJ).

## Key Results
- CHATS achieved lower phoneme error rates (13.03) compared to baselines without pre-training (15.32)
- Human evaluations showed CHATS produced more natural dialogues (MOS 4.09) than baselines, though still below human performance (4.41)
- The system successfully generated listener-side backchannels and laughter without requiring transcriptions of these elements
- Turn-taking mechanism effectively predicted natural overlap durations and silence periods, with Pearson correlations of 0.60-0.65 with ground truth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CHATS can generate listener-side utterances like backchannels and laughter without needing transcriptions for them.
- Mechanism: The dual-tower Transformer architecture generates speech for both speaker and listener channels simultaneously, conditioning the speaker side on phonemes while generating the listener side unconditionally.
- Core assumption: Listener responses can be predicted based on the speaker's input content without explicit transcription.
- Evidence anchors:
  - [abstract]: "Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter."
  - [section 3.1.2]: "By conditioning dGSLM with the phonetic transcription of speaker's utterance, our system can generate meaningful and contextually proper utterances on the speaker side. Simultaneously, it generates various backchannels and laughter without transcription on the listener side."
  - [corpus]: Weak - related papers focus on textless dialogue but don't address this exact conditioning mechanism.
- Break condition: If the model cannot learn to generate appropriate listener responses from speaker context alone, or if listener responses require more specific contextual information not available from the speaker's phonemes.

### Mechanism 2
- Claim: CHATS can handle overlapping speech naturally through turn-taking mechanism.
- Mechanism: The turn-taking mechanism predicts when to insert silence or allow overlapping speech based on phoneme sequences of current and next utterances.
- Core assumption: Overlapping speech timing can be predicted from phoneme information of consecutive utterances.
- Evidence anchors:
  - [abstract]: "Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap."
  - [section 3.2]: "The uLM is trained to predict the duration of trailing silence in the no overlap scenario, and pinpoint the onset of overlap in the overlap scenario."
  - [corpus]: Weak - related papers mention turn-taking but don't detail this specific phoneme-based overlap prediction approach.
- Break condition: If phoneme sequences don't contain sufficient information to predict natural overlap timing, or if the model fails to generate coherent overlapping speech segments.

### Mechanism 3
- Claim: Pre-training on CSJ dataset improves textual fidelity in TTS setting.
- Mechanism: Pre-training the single-channel uLM on a large corpus of spontaneous Japanese speech provides better phoneme-to-unit mappings.
- Core assumption: Learning general speech patterns from spontaneous speech improves the model's ability to map text to speech accurately.
- Evidence anchors:
  - [section 4.2]: "Pre-training and use of the context units were effective" and "Proposed w/o pre-training 15.32" vs "Proposed 13.03" in PER results.
  - [section 4.1]: "A single-channel variant of our uLM was pre-trained on the CSJ dataset."
  - [corpus]: Weak - no related papers directly address pre-training benefits for this specific architecture.
- Break condition: If the pre-training dataset is too different from the target dialogue data, or if the model overfits to the pre-training domain.

## Foundational Learning

- Concept: Discrete token-based speech representation (HuBERT + k-means clustering)
  - Why needed here: Enables efficient modeling of speech as sequences of discrete units rather than continuous waveforms, making it compatible with Transformer architectures.
  - Quick check question: Why use discrete tokens instead of raw waveforms for speech generation in this system?

- Concept: Transformer language models for sequence generation
  - Why needed here: The dual-tower Transformer architecture can simultaneously generate speech for two speakers while handling complex dependencies between utterances.
  - Quick check question: How does the dual-tower architecture differ from standard single-channel speech generation models?

- Concept: Turn-taking patterns in human conversation
  - Why needed here: Understanding how humans naturally overlap speech and use backchannels is crucial for generating realistic spoken dialogues.
  - Quick check question: What are the two main scenarios of turn-taking that CHATS handles, and how does it distinguish between them?

## Architecture Onboarding

- Component map: Written dialogue → phoneme conversion → uLM generation → u2s synthesis → spoken output
- Critical path: Written dialogue → phoneme conversion → uLM generation → u2s synthesis → spoken output
- Design tradeoffs:
  - Using discrete tokens trades some audio quality for computational efficiency and sequence modeling capability
  - Conditioning only on speaker phonemes (not full context) limits context length but enables continuous generation
  - Dual-channel generation increases complexity but enables realistic turn-taking
- Failure signatures:
  - Low phoneme error rate but unnatural prosody indicates issues with pitch representation
  - Excessive backchannels suggests turn-taking mechanism is generating overlaps incorrectly
  - Mismatched speaker characteristics indicates conditioning tokens aren't being utilized properly
- First 3 experiments:
  1. Generate spoken dialogue from simple written dialogue with clear turn-taking and measure PER
  2. Test listener response generation by providing dialogues with and without backchannels in input
  3. Evaluate turn-taking by creating dialogues with known overlap patterns and checking if model reproduces them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed system handle situations where the written dialogue contains non-verbal elements like laughter or interjections, which are typically not present in standard written dialogues?
- Basis in paper: [explicit] The paper discusses the conversion of spoken dialogue transcriptions into written dialogues, which includes elements like backchannels and laughter. However, it does not explicitly mention how the system handles these elements when they are already present in the written dialogue.
- Why unresolved: The paper does not provide information on how the system deals with non-verbal elements in the written dialogue, which could potentially affect the quality of the generated spoken dialogue.
- What evidence would resolve it: An explanation or experiment demonstrating how the system processes and incorporates non-verbal elements from the written dialogue into the generated spoken dialogue would resolve this question.

### Open Question 2
- Question: What is the impact of the size of the dataset on the performance of the proposed system?
- Basis in paper: [inferred] The paper mentions that the performance of dGSLM declined when the dataset was reduced, suggesting that the size of the dataset might influence the performance of similar systems.
- Why unresolved: The paper does not provide a detailed analysis of the impact of dataset size on the proposed system's performance.
- What evidence would resolve it: An experiment or study comparing the performance of the proposed system using datasets of varying sizes would provide insights into the impact of dataset size on the system's performance.

### Open Question 3
- Question: How does the proposed system ensure the consistency of speaking style across the generated spoken dialogue?
- Basis in paper: [explicit] The paper discusses the use of a speaker embedding in the u2s module to capture the speaker's unique characteristics. However, it does not explicitly mention how the system ensures the consistency of speaking style throughout the dialogue.
- Why unresolved: The paper does not provide information on how the system maintains the consistency of speaking style across the generated spoken dialogue.
- What evidence would resolve it: An analysis or experiment demonstrating how the system maintains the consistency of speaking style throughout the generated spoken dialogue would resolve this question.

## Limitations
- Evaluation relies on internal datasets without public availability, making independent validation difficult
- System assumes speaker transcripts are available, limiting applicability to scenarios where only audio exists
- Complex dual-tower architecture introduces multiple potential failure points, particularly in the turn-taking mechanism

## Confidence
- **High confidence**: The discrete token-based approach for speech generation is well-established (HuBERT + k-means has been validated in prior work). The dual-tower architecture for simultaneous speaker/listener generation follows logically from existing multi-speaker TTS frameworks. The improvement from pre-training on CSJ is clearly demonstrated through objective metrics.
- **Medium confidence**: The turn-taking mechanism's effectiveness is supported by quantitative metrics (Pearson correlations, duration distributions) but the subjective naturalness improvements (MOS scores) show CHATS still lags behind human performance significantly (4.09 vs 4.41). The claim that conditioning only on phonemes is sufficient for generating appropriate listener responses is plausible but not thoroughly validated across diverse dialogue contexts.
- **Low confidence**: The claim that CHATS "eliminates the need for transcriptions of backchannels or laughter" assumes these can be generated appropriately without explicit context, which may not hold for complex conversational dynamics. The paper doesn't adequately address how the system handles speaker identity changes or maintains consistent speaker characteristics across long dialogues.

## Next Checks
1. **Cross-corpus validation**: Test CHATS on a publicly available spontaneous dialogue dataset (like Switchboard or ICSI) to verify that performance gains generalize beyond the internal Japanese corpus. This would validate whether the pre-training benefits and architecture design are robust across languages and speaking styles.

2. **Listener response quality analysis**: Conduct detailed analysis of generated backchannels and laughter by measuring their semantic appropriateness and timing accuracy against ground truth listener responses in the test set. This would validate whether unconditional generation on the listener side produces contextually appropriate responses or just statistically common patterns.

3. **Ablation study on turn-taking mechanism**: Systematically disable the turn-taking mechanism components (overlap detection, silence prediction) to quantify their individual contributions to naturalness. This would clarify whether the complex overlap prediction is necessary or if simpler approaches (fixed silence durations) could achieve similar results.