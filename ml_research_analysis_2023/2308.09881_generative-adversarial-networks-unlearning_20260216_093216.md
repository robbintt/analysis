---
ver: rpa2
title: Generative Adversarial Networks Unlearning
arxiv_id: '2308.09881'
source_url: https://arxiv.org/abs/2308.09881
tags:
- unlearning
- images
- cascaded
- class
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first systematic investigation of GAN
  unlearning, addressing challenges in maintaining latent space continuity and defining
  discriminator criteria. The authors propose a cascaded unlearning approach combining
  a substitute mechanism and fake label to enable both item and class unlearning in
  GAN models.
---

# Generative Adversarial Networks Unlearning

## Quick Facts
- arXiv ID: 2308.09881
- Source URL: https://arxiv.org/abs/2308.09881
- Reference count: 40
- Introduces first systematic investigation of GAN unlearning with cascaded approach

## Executive Summary
This paper addresses the challenging problem of unlearning specific data from Generative Adversarial Networks (GANs). The authors identify two main challenges: maintaining latent space continuity during generator unlearning and defining appropriate discriminator criteria for unlearning images. They propose a cascaded unlearning approach that combines a substitute mechanism with a fake label strategy to enable both item and class unlearning in GAN models. The method demonstrates significant efficiency improvements over retraining, achieving up to 284× time reduction while maintaining model performance and downstream task accuracy.

## Method Summary
The proposed cascaded unlearning approach operates in two phases: an unlearning phase that removes specific data using a substitute mechanism (average, truncation, or projection) and a fake label criterion for the discriminator, followed by a learning phase that uses remaining data (few-shot) or the raw model (zero-shot) to prevent over-unlearning. The substitute mechanism maintains latent space continuity by providing alternative mappings for unlearned images, while the fake label (set to -1, 0.1, or 0.5) serves as a consistent baseline criterion for discriminator unlearning. The method is evaluated on MNIST, CIFAR-10, and FFHQ datasets using StyleGAN2 models, measuring unlearning effectiveness (AUCl,u), model fidelity (FIDl), downstream task performance (ACC), and efficiency (T).

## Key Results
- Achieves 185× and 284× time reductions compared to retraining for item and class unlearning respectively
- Maintains model performance with minimal degradation in downstream task accuracy
- Successfully enables both item unlearning (removing specific images) and class unlearning (removing entire classes)
- Demonstrates effectiveness across three datasets (MNIST, CIFAR-10, FFHQ) with varying complexity

## Why This Works (Mechanism)

### Mechanism 1
The substitute mechanism maintains latent space continuity during GAN unlearning by providing alternative mappings for unlearned images. When an image x0 is unlearned, the substitute mechanism S(x0) provides an alternative image that maintains continuity in the latent space by ensuring the corresponding latent codes remain close. The generator maps z0 → S(x0) instead of z0 → x0. This works because the latent space has inherent continuity where close points should not yield entirely disparate images, and GAN inversion can accurately find ground-truth latent codes.

### Mechanism 2
The fake label mechanism enables effective discriminator unlearning by providing a consistent baseline criterion for unlearning images. A fixed fake label Flabel (such as -1, 0.1, or 0.5) is defined as the target output for discriminator when processing unlearning images. This replaces unreliable test image outputs as the unlearning criterion. The approach works because discriminator outputs for training and test images show less pronounced differences than classifier outputs, making test images unsuitable as unlearning baselines.

### Mechanism 3
Cascaded unlearning with unlearning and learning phases prevents over-unlearning while achieving effective data removal. The algorithm operates in cascaded manner where unlearning phase removes specific data using substitute mechanism and fake label, followed by learning phase that uses remaining data (few-shot) or raw model (zero-shot) to prevent over-unlearning. This works because without balancing unlearning with learning, the model would lose capability on remaining data, making unlearning ineffective.

## Foundational Learning

- Concept: GAN architecture (generator and discriminator components)
  - Why needed here: Understanding how GANs work is essential to grasp why unlearning is challenging - both generator and discriminator need separate unlearning approaches
  - Quick check question: What are the two main components of a GAN and what roles do they play in the learning process?

- Concept: Latent space properties (continuity and completeness)
  - Why needed here: The paper's main challenge revolves around maintaining latent space continuity during unlearning, which is fundamental to GAN functionality
  - Quick check question: What does it mean for a latent space to be continuous, and why is this property important for GAN image generation?

- Concept: Membership inference attacks
  - Why needed here: The paper uses membership inference as an evaluation tool to verify successful unlearning by checking if unlearning images can no longer be detected as training data
  - Quick check question: How does a membership inference attack work, and why is it relevant for evaluating machine unlearning?

## Architecture Onboarding

- Component map: Pre-trained GAN model (G0, D0) -> Substitute mechanism S(·) -> Fake label Flabel -> Cascaded unlearning algorithm (unlearning phase -> learning phase) -> Evaluation metrics (AUCl,u, FIDu, FIDl, ACC, T)
- Critical path: For each unlearning image: 1) Find ground-truth latent code via GAN inversion, 2) Apply substitute mechanism, 3) Update generator and discriminator using unlearning losses, 4) Apply learning phase to prevent over-unlearning
- Design tradeoffs: Few-shot vs zero-shot approaches trade privacy (no learning images needed) against efficiency and performance stability. Different substitute mechanisms (average, truncation, projection) balance simplicity against effectiveness.
- Failure signatures: If AUCl,u doesn't increase significantly, unlearning is ineffective. If FIDl increases dramatically, model performance degrades. If ACC drops substantially, downstream tasks are affected.
- First 3 experiments:
  1. Implement substitute mechanism with average approach on MNIST for item unlearning with 64 images
  2. Test fake label parameter sensitivity (-1, 0.1, 0.5) on CIFAR-10
  3. Compare few-shot vs zero-shot cascaded unlearning on CIFAR-10 with 64 images using average substitute mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How can GAN unlearning methods be scaled to handle high-quality images (e.g., 1024 × 1024 × 3) without significantly degrading model fidelity? The paper mentions that unlearning minor images can diminish model fidelity for high-quality datasets like FFHQ (256 × 256 × 3), indicating challenges in scaling to even higher quality images. Current methods struggle with maintaining model fidelity when unlearning even small numbers of images from high-quality datasets, suggesting scalability limitations.

### Open Question 2
Can unlearning be performed without requiring all unlearning images, particularly in class unlearning scenarios? The paper notes that while cascaded unlearning reduces learning image requirements, it still requires all unlearning images, suggesting privacy concerns. Current approaches need full access to unlearning data, which contradicts privacy-preserving goals in some scenarios.

### Open Question 3
What is the optimal balance between perceptual loss involvement (λ2) and unlearning effectiveness across different dataset complexities? The paper's parameter evaluation shows conflicting results - perceptual loss improves efficiency in class unlearning but degrades performance in item unlearning. The optimal λ2 value appears to depend on task type and dataset complexity, with no clear universal solution provided.

## Limitations
- Scalability challenges with high-quality images (1024 × 1024 × 3) where unlearning degrades model fidelity
- Requirement for all unlearning images in current approach, raising privacy concerns
- Conflicting results for perceptual loss parameter (λ2) across different tasks and dataset complexities

## Confidence

High confidence in the core architectural claims about substitute mechanisms and fake label approaches, as experimental results show consistent effectiveness across datasets. Medium confidence in the cascaded unlearning framework's effectiveness, as significant efficiency gains are demonstrated but with some performance degradation in complex datasets. Low confidence in the scalability claims across diverse GAN architectures beyond StyleGAN2, as the paper focuses primarily on one model type.

## Next Checks

1. Test the cascaded unlearning approach on alternative GAN architectures (DCGAN, BigGAN) to verify architectural independence
2. Conduct extended runtime evaluations (beyond 100 epochs) to assess stability and performance drift
3. Perform ablation studies varying substitute mechanism types and fake label values systematically across all three datasets