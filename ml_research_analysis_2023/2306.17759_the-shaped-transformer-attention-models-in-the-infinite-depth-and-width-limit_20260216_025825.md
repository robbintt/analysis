---
ver: rpa2
title: 'The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit'
arxiv_id: '2306.17759'
source_url: https://arxiv.org/abs/2306.17759
tags:
- attention
- covariance
- shaped
- where
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the stability of Transformers in the proportional
  infinite-depth-and-width limit. The authors show that without architectural modifications,
  the attention mechanism leads to a degenerate covariance matrix, causing numerical
  instabilities.
---

# The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit

## Quick Facts
- arXiv ID: 2306.17759
- Source URL: https://arxiv.org/abs/2306.17759
- Reference count: 40
- Key outcome: This work studies the stability of Transformers in the proportional infinite-depth-and-width limit. The authors show that without architectural modifications, the attention mechanism leads to a degenerate covariance matrix, causing numerical instabilities. To address this, they propose a "shaped attention" mechanism that modifies the softmax by centering it at the identity and scaling the logits with a width-dependent temperature parameter. In the infinite limit, the limiting distribution is characterized by a stochastic differential equation, whose stability can be controlled with residual connections. Simulations demonstrate that the SDE accurately approximates finite-size networks and that shaped attention successfully prevents rank collapse compared to standard Transformers. Preliminary training experiments show that shaped attention allows for stable training with larger learning rates.

## Executive Summary
This paper addresses the fundamental problem of rank collapse in Transformers by analyzing their behavior in the proportional infinite-depth-and-width limit. The authors demonstrate that standard attention mechanisms lead to degenerate covariance matrices, causing numerical instabilities during training. They propose a "shaped attention" mechanism that modifies the softmax function to maintain stability in this limit. The theoretical framework characterizes the limiting behavior using stochastic differential equations, providing insights into how architectural choices affect stability. Preliminary experiments show that shaped attention prevents rank collapse and enables stable training with larger learning rates.

## Method Summary
The authors propose shaped attention by modifying the standard softmax function to center its output at the identity matrix and scale logits with a width-dependent temperature parameter. This modification ensures that the attention matrix remains a small perturbation of identity in the infinite limit, preventing rank collapse. The framework extends to shaped ReLU activations and analyzes stability through stochastic differential equations that characterize the evolution of neural covariance. Residual connections with appropriately scaled parameters are shown to control the stability of the limiting distribution by damping drift and diffusion terms in the SDE.

## Key Results
- Standard Transformers suffer from rank collapse in the proportional infinite-depth-and-width limit due to the softmax attention mechanism
- Shaped attention prevents rank collapse by ensuring the attention matrix remains close to identity
- The limiting distribution in the proportional limit can be characterized by a stochastic differential equation
- Shaped attention enables stable training with larger learning rates compared to standard Transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shaped attention mechanism prevents rank collapse by ensuring the attention matrix remains a small perturbation of the identity, stabilizing the neural covariance structure.
- Mechanism: The shaped attention modifies the standard Softmax-based attention by (1) centering the Softmax output at identity, (2) scaling logits with a width-dependent temperature parameter, and (3) removing the zero-order term that causes large drifts. This keeps the attention matrix close to identity and ensures updates to the covariance matrix remain infinitesimal.
- Core assumption: The Taylor expansion of the Softmax function converges and the modifications result in a matrix close enough to identity to prevent rank collapse.
- Evidence anchors:
  - [abstract] "To achieve a well-defined stochastic limit, the Transformer's attention mechanism is modified by centering the Softmax output at identity, and scaling the Softmax logits by a width-dependent temperature parameter."
  - [section] "The shaped attention presents three modifications to the Softmax attention... we actually require Aℓ to be approximately the identity matrix."
  - [corpus] Weak evidence; corpus neighbors discuss differential equation limits and scaling limits but do not directly address rank collapse prevention.
- Break condition: If the temperature scaling is incorrect (too small or too large), the approximation breaks down and the attention matrix deviates significantly from identity, reintroducing rank collapse.

### Mechanism 2
- Claim: The proportional infinite-depth-and-width limit provides a tractable yet faithful description of finite-size Transformers, enabling analysis of stability through stochastic differential equations.
- Mechanism: As depth d and width n both go to infinity with fixed ratio d/n, the neural covariance matrix converges to a solution of a stochastic differential equation (SDE). The SDE captures the evolution of covariance structure over depth, allowing analysis of stability via drift and diffusion terms.
- Core assumption: The neural covariance in the proportional limit can be approximated by a Markov chain that converges to an SDE.
- Evidence anchors:
  - [abstract] "We show that at initialization the limiting distribution can be described by a stochastic differential equation (SDE) indexed by the depth-to-width ratio."
  - [section] "The effect of the non-linearity accumulates over the d layers, and the covariance matrix Vℓ converges weakly to the solution of the SDE."
  - [corpus] Weak evidence; corpus neighbors discuss differential equation limits and scaling limits but do not directly address the specific SDE convergence in the proportional limit.
- Break condition: If the depth-to-width ratio varies or if the network architecture deviates significantly from the studied model (e.g., includes normalization layers), the SDE approximation may fail.

### Mechanism 3
- Claim: Skip connections with appropriately scaled residual branch parameters prevent covariance degeneracy by controlling the scale of drift and diffusion in the SDE.
- Mechanism: Residual connections modify the SDE coefficients, with the residual parameter γ damping both drift and diffusion terms. This effectively reduces the "effective depth" and stabilizes the covariance structure.
- Core assumption: The residual branch parameter γ can be tuned to control the stability of the SDE solution.
- Evidence anchors:
  - [section] "The residual branch parameter γ < 1 dampens both the drift and the variance of the Brownian motion by γ², thus it can be interpreted as a time change."
  - [section] "The dependence on the depth-to-width ratio implies the existence of a stable non-commutative limit for residual networks."
  - [corpus] Weak evidence; corpus neighbors discuss residual networks and scaling limits but do not directly address the specific role of γ in controlling SDE stability.
- Break condition: If γ is set too high, random fluctuations dominate and the SDE may become unstable; if too low, the network may not learn effectively.

## Foundational Learning

- Concept: Stochastic differential equations (SDEs) and their convergence properties
  - Why needed here: The paper characterizes the neural covariance evolution in the infinite-depth-and-width limit as an SDE, requiring understanding of SDE convergence and stability.
  - Quick check question: What is the difference between strong and weak convergence of SDEs, and which type is used in this paper?
- Concept: Taylor series expansion and approximation of functions
  - Why needed here: The shaped attention mechanism relies on Taylor expanding the Softmax function to ensure the attention matrix is a perturbation of identity.
  - Quick check question: What are the first three terms of the Taylor expansion of the Softmax function around τ⁻¹ = 0?
- Concept: Covariance matrix and rank collapse in neural networks
  - Why needed here: The paper addresses the problem of rank collapse in Transformers, where token representations become perfectly aligned, causing numerical instability.
  - Quick check question: What is rank collapse, and how does it manifest in the neural covariance matrix of Transformers?

## Architecture Onboarding

- Component map:
  - Input tokens (X₀) -> Shaped attention mechanism (Aℓ) -> Residual connections (λXℓ + γAℓXℓ) -> Shaped ReLU activation (σs) -> Feedforward layer (1/√n Wpre ℓ, Wpost ℓ) -> Output tokens (Xℓ+1)
- Critical path:
  1. Initialize input tokens X₀
  2. Apply shaped attention: Aℓ = I + Softmax(τ⁻¹Yℓ) - (1/m)11⊤
  3. Apply residual connection: Zℓ = λXℓ + γAℓXℓ
  4. Apply shaped ReLU activation: σs(Zℓ)
  5. Apply feedforward layer: Xℓ+1 = λZℓ + γσs(Zℓ)
  6. Repeat steps 2-5 for each layer
- Design tradeoffs:
  - Temperature scaling (τ): Larger τ makes the attention matrix closer to identity but may reduce expressivity
  - Residual strength (γ): Larger γ increases the effect of residual connections but may destabilize the SDE
  - Shaped ReLU slopes (s±): Control the non-linearity of the activation function
- Failure signatures:
  - Rank collapse: Correlation between token representations approaches 1
  - Numerical instability: Exploding or vanishing gradients during training
  - Poor trainability: Slow convergence or failure to learn meaningful representations
- First 3 experiments:
  1. Compare the neural covariance evolution of a shaped Transformer vs. a standard Transformer in the proportional limit
  2. Vary the temperature scaling parameter τ and observe its effect on rank collapse and training stability
  3. Tune the residual strength parameter γ and analyze its impact on the SDE stability and finite-size network behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do shaped attention and shaped activation functions interact during training to influence the final learned representations and generalization performance?
- Basis in paper: [inferred] The paper discusses shaped attention and shaped ReLU activations separately, but does not analyze their combined effect during training beyond initialization.
- Why unresolved: The theoretical analysis focuses on the initialization regime, and preliminary experiments only briefly test feasibility without comprehensive training analysis.
- What evidence would resolve it: Extensive training experiments comparing shaped attention with shaped and unshaped activations, measuring final performance, convergence speed, and analyzing learned representations.

### Open Question 2
- Question: Can the proportional infinite-depth-and-width limit framework be extended to characterize the training dynamics and generalization of shaped Transformers, similar to how NTK theory characterizes training for infinite-width networks?
- Basis in paper: [explicit] The paper mentions that the proportional limit holds potential for a theory of training and generalization, but does not provide such a theory.
- Why unresolved: The current analysis is limited to the initialization regime, and extending it to training would require significant theoretical development.
- What evidence would resolve it: A theoretical framework characterizing the training dynamics and generalization of shaped Transformers in the proportional limit, along with empirical validation on real datasets.

### Open Question 3
- Question: What is the optimal way to schedule the shaped attention parameters (γ1, γ2, and the shaped ReLU slopes) during training to maximize performance and stability?
- Basis in paper: [explicit] The paper proposes two ways to schedule these parameters (Recover and Learn) but does not provide a definitive answer on the optimal scheduling strategy.
- Why unresolved: The paper only briefly tests these scheduling strategies in preliminary experiments without a comprehensive comparison.
- What evidence would resolve it: Extensive experiments comparing different scheduling strategies for shaped attention parameters, measuring final performance, convergence speed, and stability across various tasks and datasets.

## Limitations

- The theoretical analysis assumes proportional scaling between depth and width, which may not hold for practical architectures
- The shaped attention mechanism requires careful tuning of temperature parameters, but practical guidelines are not fully developed
- Empirical validation is limited to preliminary experiments, with no comprehensive comparison to standard Transformers on real tasks

## Confidence

**High Confidence:** The characterization of rank collapse in standard Transformers and the mathematical framework for shaped attention are well-established through rigorous proofs and simulations.

**Medium Confidence:** The claim that shaped attention successfully prevents rank collapse is supported by simulations but lacks extensive empirical validation on diverse tasks and architectures.

**Low Confidence:** The practical impact of shaped attention on training stability and performance in real-world scenarios remains largely untested beyond preliminary experiments.

## Next Checks

1. Conduct comprehensive ablation studies varying the temperature parameter τ across multiple orders of magnitude to identify optimal ranges and failure modes
2. Compare shaped Transformers against standard Transformers on diverse language modeling tasks, measuring both training stability and final performance metrics
3. Test shaped attention in architectures with normalization layers (LayerNorm) to validate the theoretical framework extends beyond the studied setup