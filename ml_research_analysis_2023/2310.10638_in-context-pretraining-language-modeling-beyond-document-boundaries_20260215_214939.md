---
ver: rpa2
title: 'In-context Pretraining: Language Modeling Beyond Document Boundaries'
arxiv_id: '2310.10638'
source_url: https://arxiv.org/abs/2310.10638
tags:
- documents
- pretraining
- language
- document
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces In-Context Pretraining, a new method for
  training language models by exposing them to sequences of related documents instead
  of randomly concatenated ones. The key idea is to reorder the pretraining data so
  that each input context contains semantically related documents, thereby encouraging
  the model to learn to reason across document boundaries.
---

# In-context Pretraining: Language Modeling Beyond Document Boundaries

## Quick Facts
- arXiv ID: 2310.10638
- Source URL: https://arxiv.org/abs/2310.10638
- Reference count: 21
- Key outcome: +8% in in-context learning, +15% in reading comprehension, +16% in faithfulness to previous contexts, +5% in long-context reasoning, and +9% in retrieval augmentation

## Executive Summary
This paper introduces In-Context Pretraining, a novel approach to language model pretraining that sequences related documents rather than randomly concatenating them. The method leverages nearest neighbor search to find semantically similar documents and constructs coherent input contexts via a graph traversal algorithm. By pretraining models on these related document sequences, the approach explicitly encourages cross-document reasoning and improves performance on tasks requiring contextual understanding. The authors demonstrate significant improvements across multiple downstream tasks, including in-context learning, reading comprehension, and long-context reasoning.

## Method Summary
The approach involves three main steps: first, documents are embedded using a contriever model and FAISS is used to build a nearest neighbor index for efficient similarity search; second, a graph traversal algorithm constructs document sequences by treating the problem as a maximum traveling salesman problem, ensuring each document appears only once while maximizing contextual similarity; third, language models ranging from 0.3B to 7B parameters are pretrained on these constructed contexts using standard language modeling objectives. The method was evaluated on English CommonCrawl dataset with 235 million documents and 306 billion tokens.

## Key Results
- +8% improvement in in-context learning accuracy
- +15% improvement in reading comprehension performance
- +16% improvement in faithfulness to previous contexts

## Why This Works (Mechanism)

### Mechanism 1
Training on semantically related document sequences provides stronger learning signals that connect concepts across documents, encouraging richer representations that span multiple texts. The core assumption is that semantic similarity between consecutive documents provides meaningful learning signals for improving contextual reasoning abilities.

### Mechanism 2
Efficient nearest neighbor search using FAISS enables construction of coherent input contexts at scale without data repetition. The retrieval model maps documents to embeddings, then uses approximate nearest neighbor search to find related documents for each input document, allowing building document graphs that maximize contextual similarity while ensuring each document appears only once.

### Mechanism 3
Treating document sorting as a maximum traveling salesman problem ensures optimal document ordering for contextual reasoning. The greedy algorithm starts with minimum degree documents and progressively extends paths by visiting unvisited neighboring documents with highest similarity weights, creating a single path that visits each document once.

## Foundational Learning

- Document embedding and similarity search: Why needed - The entire approach relies on mapping documents to vector representations and finding semantically similar documents efficiently. Quick check - Can you explain how product quantization in FAISS enables scalable similarity search across billions of documents?

- Graph traversal algorithms: Why needed - The document sorting problem is formulated as finding paths through a document similarity graph. Quick check - What are the key differences between this graph traversal approach and standard BFS/DFS, and why is the minimum degree heuristic important?

- Traveling salesman problem approximations: Why needed - The document ordering is cast as a maximum weight path problem, requiring understanding of TSP solution approaches. Quick check - Why might a greedy algorithm work well for this specific TSP variant despite being suboptimal in general cases?

## Architecture Onboarding

- Component map: Retrieval model (contriever) → FAISS index for similarity search → Document graph construction → Graph traversal algorithm → Context building → Language model pretraining pipeline
- Critical path: Document embedding → Similarity search → Graph construction → Context ordering → Model training
- Design tradeoffs: Higher similarity thresholds reduce noise but may create disconnected graph components; lower thresholds increase connectivity but add irrelevant documents
- Failure signatures: Poor downstream task performance despite good perplexity scores; training instability when duplicate documents appear in contexts
- First 3 experiments:
  1. Compare model performance with different k values in nearest neighbor search to find optimal trade-off between context coherence and coverage
  2. Test different graph traversal strategies (pure greedy vs. minimum degree heuristic) to validate the importance of the starting node selection
  3. Evaluate the impact of semantic deduplication thresholds on both training stability and downstream performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between semantic relevance and diversity in the document sorting process for In-Context Pretraining? The paper discusses the challenge of maximizing document similarity while avoiding repetition, but doesn't explore the tradeoff between these two objectives.

### Open Question 2
How does the performance of In-Context Pretrained models scale with increasing context window size beyond 8192 tokens? The paper evaluates models with an 8192-length context window, but doesn't explore how performance changes with larger context windows.

### Open Question 3
How does the performance of In-Context Pretrained models compare to models trained with explicit cross-document attention mechanisms? The paper introduces a method for encouraging cross-document reasoning, but doesn't compare its effectiveness to models with explicit cross-document attention mechanisms.

## Limitations
- The evaluation focuses on a relatively narrow set of tasks, limiting generalizability to other domains or languages
- The computational overhead of nearest neighbor search and graph construction at scale is not thoroughly analyzed
- The paper doesn't provide detailed ablations showing how much each component contributes to final performance

## Confidence
- High Confidence: The core methodology of using related documents for pretraining is well-grounded in the literature and experimental results show consistent improvements
- Medium Confidence: The document graph traversal algorithm and its effectiveness in creating optimal contexts is reasonably supported but would benefit from more extensive ablation studies
- Low Confidence: The scalability claims regarding FAISS index construction and nearest neighbor search at billion-document scale are asserted but not independently verified

## Next Checks
1. Conduct ablation studies removing or modifying each key component to quantify their individual contributions to performance improvements
2. Measure computational overhead of nearest neighbor search and graph construction as dataset size scales from millions to billions of documents
3. Evaluate pretrained models on a broader range of tasks including multilingual datasets, code generation, and mathematical reasoning to assess versatility beyond the current task set