---
ver: rpa2
title: 'ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction'
arxiv_id: '2310.07253'
source_url: https://arxiv.org/abs/2310.07253
tags:
- data
- drug
- noise
- property
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces ADMEOOD, a systematic benchmark for out-of-distribution
  (OOD) drug property prediction, addressing challenges posed by noise and inconsistency
  in long-accumulated chemical data. The benchmark incorporates 27 ADME drug properties
  from ChEMBL and introduces two OOD data shifts: Noise Shift (categorizing data by
  confidence levels) and Concept Conflict Drift (CCD, identifying inconsistent labels
  across experiments).'
---

# ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction

## Quick Facts
- arXiv ID: 2310.07253
- Source URL: https://arxiv.org/abs/2310.07253
- Reference count: 19
- This study introduces ADMEOOD, a systematic benchmark for out-of-distribution (OOD) drug property prediction, addressing challenges posed by noise and inconsistency in long-accumulated chemical data.

## Executive Summary
This paper introduces ADMEOOD, a novel benchmark for out-of-distribution drug property prediction that addresses the challenges of noise and inconsistency in long-accumulated chemical data. The benchmark incorporates 27 ADME drug properties from ChEMBL and introduces two types of OOD data shifts: Noise Shift (categorizing data by confidence levels) and Concept Conflict Drift (CCD, identifying inconsistent labels across experiments). The benchmark evaluates four OOD algorithms (ERM, IRM, DeepCORAL, Mixup) and demonstrates significant performance gaps between in-distribution and OOD data, while highlighting the need for better OOD algorithms tailored to drug property prediction.

## Method Summary
ADMEOOD leverages 27 ADME drug properties from ChEMBL database, incorporating molecular weight, logP, pKa, pchembl_value, and SMILES sequences. The benchmark introduces two novel OOD shifts: Noise Shift, which categorizes experimental records into different confidence levels based on noise measurements, and Concept Conflict Drift (CCD), which identifies data with inconsistent labels across experiments. The benchmark evaluates four algorithms (ERM, IRM, DeepCORAL, Mixup) using graph neural networks (GCN, GIN, GAT, MGCN) on both SMILES and molecular graph representations, measuring performance through AUC scores across different domains.

## Key Results
- Significant performance gaps between IID and OOD data: AUC drops of 8.38%-33.71%
- ERM consistently performs better than other domain generalization methods for Noise Shift
- Domain generalization methods show improved generalization but ERM remains competitive
- Performance degradation observed in scaffold domain under Noise Shift and assay domain under CCD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark improves model robustness by explicitly partitioning data into noise-level domains.
- Mechanism: By categorizing experimental records into different confidence environments, the model is trained and evaluated on data with varying noise levels, simulating real-world conditions where measurement accuracy varies.
- Core assumption: Noise levels in chemical data are sufficiently distinct and separable to form meaningful distribution shifts.
- Evidence anchors:
  - [abstract] "Noise Shift responds to the noise level by categorizing the environment into different confidence levels."
  - [section] "Bioactivity data exhibits a multitude of diverse noise sources... ADMEOOD combined disordered data with two different levels of noise using various filter configurations."
  - [corpus] No direct evidence of noise-level separation in related work, suggesting this is a novel contribution.
- Break condition: If noise levels are not consistently measurable or if the partitioning method introduces artificial artifacts rather than reflecting true data quality differences.

### Mechanism 2
- Claim: The benchmark captures concept drift by identifying inconsistent labels across experiments.
- Mechanism: Concept Conflict Drift (CCD) identifies data where the same molecule receives conflicting labels in different experiments, creating a one-to-many mapping that violates standard machine learning assumptions.
- Core assumption: Inconsistent labeling across experiments is a meaningful and detectable phenomenon in chemical property data.
- Evidence anchors:
  - [abstract] "CCD describes the data which has inconsistent label among the original data."
  - [section] "CCD refers to the situation where the labels of the data appear to conflict with each other due to different sources of data and experimental scenarios."
  - [corpus] No explicit mention of CCD in related work, suggesting this is an original methodological contribution.
- Break condition: If inconsistent labeling is due to measurement error rather than genuine concept drift, or if the conflict detection method is too sensitive to minor variations.

### Mechanism 3
- Claim: The benchmark improves evaluation rigor by using multiple domain partitions and measurement types.
- Mechanism: By testing models across assay domains (different experimental protocols) and scaffold domains (different molecular structures), the benchmark reveals how well models generalize across different sources of distribution shift.
- Core assumption: Different assay protocols and molecular scaffolds create meaningful distribution shifts that affect model performance.
- Evidence anchors:
  - [section] "The assay domain cause significant performance degradation because molecules from different backbones often have different properties and noise may mask or interfere with features."
  - [section] "When it comes to CCD, the assay domain can lead to a drop in performance, as various labs carrying out the same experiment under different circumstances may produce varying outcomes."
  - [corpus] Related work mentions domain shifts but doesn't specifically address assay and scaffold domains in drug property prediction.
- Break condition: If the domain partitions are too coarse or if the measurement types are not sufficiently distinct to create meaningful distribution shifts.

## Foundational Learning

- Concept: Out-of-distribution generalization
  - Why needed here: The benchmark specifically addresses the challenge of models performing well on data that differs from their training distribution, which is critical for drug property prediction where real-world data is noisy and inconsistent.
  - Quick check question: Can you explain the difference between in-distribution and out-of-distribution data in the context of drug property prediction?

- Concept: Noise modeling and confidence levels
  - Why needed here: The benchmark introduces a novel way to model noise by categorizing data into different confidence environments, which is essential for understanding how noise affects model performance.
  - Quick check question: How does the Noise Shift method in ADMEOOD differ from traditional approaches to handling noisy data in machine learning?

- Concept: Concept drift detection
  - Why needed here: The CCD method identifies when the same molecule receives conflicting labels across experiments, which is a form of concept drift that traditional benchmarks don't capture.
  - Quick check question: What is the mathematical relationship between the one-to-many mapping described in the CCD section and standard supervised learning assumptions?

## Architecture Onboarding

- Component map: Data Curator -> Domain Splitter -> Benchmark Runner -> Evaluation Module
- Critical path: Data Curator → Domain Splitter → Benchmark Runner → Evaluation Module
- Design tradeoffs:
  - Using SMILES sequences vs molecular graphs: Sequences are simpler but may lose structural information; graphs are more informative but computationally expensive
  - Number of confidence levels: More levels provide finer-grained evaluation but increase complexity
  - Choice of algorithms: ERM serves as baseline but may not capture domain-specific patterns
- Failure signatures:
  - Poor performance on OOD data but good performance on IID data suggests the model overfits to training distribution
  - Inconsistent results across different random seeds indicates instability in the training process
  - Large performance gaps between Noise Shift and CCD suggest different algorithms handle different types of distribution shifts differently
- First 3 experiments:
  1. Run ERM on Noise Shift dataset with assay domain to establish baseline performance
  2. Run IRM on CCD dataset with scaffold domain to test invariance-based methods
  3. Compare Mixup and DeepCORAL on the same dataset to evaluate data augmentation vs. domain alignment approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ERM compare to domain generalization methods specifically for Noise Shift versus CCD scenarios in drug property prediction?
- Basis in paper: [explicit] The paper states "ERM consistently performs better than others" for Noise Shift and "ERM remains competitive" indicating ERM performs comparably or better than other domain generalization methods in some cases.
- Why unresolved: While the paper provides comparative results, it does not definitively establish whether ERM's superior performance is consistent across all noise types or specific to certain scenarios. The nuanced differences between Noise Shift and CCD are not fully explored.
- What evidence would resolve it: Detailed ablation studies comparing ERM performance specifically across Noise Shift and CCD scenarios with statistical significance testing would clarify when ERM outperforms or underperforms other methods.

### Open Question 2
- Question: What are the underlying factors that contribute to the degradation of model performance in the scaffold domain under Noise Shift and the assay domain under CCD?
- Basis in paper: [explicit] The paper mentions that "in Noise Shift, the scaffold domain cause significant performance degradation" and "when it comes to CCD, the assay domain can lead to a drop in performance" but does not specify the exact reasons for these degradations.
- Why unresolved: The paper identifies the domains where performance drops but lacks an in-depth analysis of the specific characteristics of the data or noise types that lead to these degradations.
- What evidence would resolve it: A detailed analysis correlating specific types of noise, measurement errors, or data inconsistencies with performance degradation in each domain would help identify the root causes of the observed performance drops.

### Open Question 3
- Question: How effective are current denoising techniques in improving the quality of the data for drug property prediction, and what specific techniques show the most promise?
- Basis in paper: [explicit] The paper suggests that "By further denoising techniques, it is possible to improve the quality of the data, thereby enabling the training of more precise drug property prediction models" but does not evaluate or recommend specific denoising methods.
- Why unresolved: While the paper acknowledges the potential of denoising techniques, it does not provide empirical evidence or comparisons of different denoising methods to determine their effectiveness.
- What evidence would resolve it: Comparative studies evaluating various denoising techniques on the ADMEOOD benchmark, including quantitative improvements in model performance metrics, would identify the most effective approaches.

## Limitations
- The benchmark focuses on ADME properties from ChEMBL which may not represent the full chemical space
- CCD detection may be sensitive to minor label variations that don't represent true concept drift
- The assumption that noise levels can be meaningfully partitioned into discrete confidence categories

## Confidence
- High confidence in novel contributions (CCD methodology, Noise Shift partitioning)
- Medium confidence in generalizability across all drug properties
- Medium confidence in effectiveness of current OOD algorithms for drug property prediction

## Next Checks
1. Test the benchmark's sensitivity to different noise threshold values in the Noise Shift partitioning
2. Validate CCD detection by manually inspecting conflicting labels across multiple experimental records
3. Compare performance when using molecular graphs instead of SMILES sequences as input representations