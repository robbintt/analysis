---
ver: rpa2
title: An Examination of the Robustness of Reference-Free Image Captioning Evaluation
  Metrics
arxiv_id: '2305.14998'
source_url: https://arxiv.org/abs/2305.14998
tags:
- captions
- clipscore
- umic
- image
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of recently proposed reference-free
  image captioning evaluation metrics, CLIPScore and UMIC. The study aims to evaluate
  these metrics' ability to distinguish between captions with high lexical overlap
  but different meanings.
---

# An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics

## Quick Facts
- arXiv ID: 2305.14998
- Source URL: https://arxiv.org/abs/2305.14998
- Reference count: 15
- Primary result: CLIPScore and UMIC fail to reliably distinguish captions with high lexical overlap but different meanings

## Executive Summary
This paper investigates the robustness of recently proposed reference-free image captioning evaluation metrics, CLIPScore and UMIC. The study reveals that despite their high correlation with human judgments, these metrics struggle to identify fine-grained errors, comprehend negation, and consistently distinguish between correct and incorrect captions with high lexical overlap. The authors conduct controlled experiments to assess the metrics' sensitivity to various factors including plausibility, visual grounding, negation, caption length, object mentions, object size, and sentence structure.

## Method Summary
The study evaluates CLIPScore and UMIC using controlled experiments on the VQAv2 and TDIUC datasets. The authors generate controlled captions by converting QA pairs to captions using GPT-J and creating templates with object mentions. They conduct systematic experiments to probe the metrics' sensitivity to various factors including fine-grained errors, plausibility, visual grounding, negation, caption length, object mentions, object size, and sentence structure.

## Key Results
- Both metrics fail to identify fine-grained errors in 45-46% of cases
- CLIPScore and UMIC show limited sensitivity to caption implausibility errors but are more responsive to visual grounding errors
- Both metrics fail to comprehend negation, with CLIPScore failing in 41.36% and UMIC in 44.24% of cases
- CLIPScore is sensitive to object mentions and size, while UMIC is more sensitive to sentence structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIPScore and UMIC fail to distinguish captions with high lexical overlap but different meanings because they rely primarily on cross-modal similarity between the image and text, not fine-grained semantic alignment.
- Mechanism: Both metrics use pretrained vision-language models (CLIP for CLIPScore, UNITER for UMIC) that compute similarity scores based on global embedding alignment. When captions share most words, the embeddings remain close, masking subtle semantic differences such as negation, implausibility, or fine-grained factual errors.
- Core assumption: The vision-language model embeddings capture sufficient semantic nuance to differentiate captions with high lexical overlap.
- Evidence anchors:
  - [abstract] "Despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors."
  - [section] "Both metrics frequently fail to distinguish between ground truth captions and their negated counterparts."
  - [corpus] Weak. No direct corpus support for this specific claim about semantic alignment.
- Break condition: If the model embeddings were fine-tuned to detect fine-grained semantic differences, the metrics would better distinguish captions with high lexical overlap.

### Mechanism 2
- Claim: CLIPScore is more sensitive to the number and size of objects mentioned in the caption because it uses CLIP embeddings that encode object-level visual features more explicitly.
- Mechanism: CLIPScore's reliance on CLIP embeddings, which are trained on large-scale image-text pairs, makes it responsive to the presence and prominence of objects in the image. More objects or larger objects in the caption correlate with higher visual grounding, thus increasing the CLIPScore.
- Core assumption: CLIP embeddings encode object-level visual information in a way that correlates with object mention frequency and size in captions.
- Evidence anchors:
  - [section] "CLIPScore shows high sensitivity to the number of image-relevant objects mentioned in the caption while UMIC is not notably sensitive to it."
  - [section] "Both CLIPScore and UMIC are sensitive to the size of image-relevant objects mentioned in the caption; however, to our surprise CLIPScore increases with size while UMIC score decreases."
  - [corpus] Weak. No direct corpus support for this specific claim about object sensitivity.
- Break condition: If CLIP embeddings were not sensitive to object frequency or size, CLIPScore would not show this behavior.

### Mechanism 3
- Claim: UMIC is more sensitive to sentence structure than CLIPScore because it is fine-tuned via contrastive learning on hard negative samples that include structural variations.
- Mechanism: UMIC's training process involves distinguishing reference captions from hard negatives created by altering word order and other structural aspects. This fine-tuning makes UMIC more responsive to sentence structure, while CLIPScore, which uses raw CLIP embeddings, is less sensitive to structural changes.
- Core assumption: UMIC's contrastive learning process effectively captures structural nuances in captions.
- Evidence anchors:
  - [section] "UMIC is sensitive to sentence structure, whereas CLIPScore is not."
  - [section] "UMIC consistently assigns lower scores to shuffled captions compared to captions with one, two, or three object mentions and correct sentence structure."
  - [corpus] Weak. No direct corpus support for this specific claim about structural sensitivity.
- Break condition: If UMIC's contrastive learning did not focus on structural variations, it would not show this behavior.

## Foundational Learning

- Concept: Fine-grained error detection in image captioning
  - Why needed here: The paper focuses on evaluating metrics' ability to detect subtle errors in captions, which is crucial for understanding their limitations.
  - Quick check question: What is an example of a fine-grained error in image captioning?
- Concept: Visual grounding in image captioning
  - Why needed here: The paper examines how metrics respond to visual grounding errors, which is a key aspect of caption quality.
  - Quick check question: How does visual grounding affect the quality of an image caption?
- Concept: Negation comprehension in NLP
  - Why needed here: The paper investigates metrics' ability to understand negation, which is a fundamental aspect of language understanding.
  - Quick check question: Why is negation comprehension important in image captioning evaluation?

## Architecture Onboarding

- Component map: Image and Caption -> CLIPScore/UMIC -> Similarity Score
- Critical path: Input: Image and candidate caption -> CLIPScore: Compute CLIP image and text embeddings, calculate scaled cosine similarity -> UMIC: Compute UNITER image and text embeddings, apply sigmoid function to output score
- Design tradeoffs:
  - CLIPScore: Simpler, relies on pretrained CLIP embeddings, less sensitive to sentence structure
  - UMIC: More complex, fine-tuned on hard negatives, more sensitive to sentence structure but less to object mentions
- Failure signatures:
  - CLIPScore: Fails to detect fine-grained errors, insensitive to negation, sensitive to object mentions and size
  - UMIC: Fails to detect fine-grained errors, insensitive to negation, sensitive to caption length and sentence structure
- First 3 experiments:
  1. Test metrics on captions with high lexical overlap but different meanings (e.g., negation, implausibility)
  2. Evaluate metrics' sensitivity to object mentions and size in captions
  3. Assess metrics' response to sentence structure variations (e.g., shuffling words)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reference-free metrics like CLIPScore and UMIC handle negation in image captions?
- Basis in paper: [explicit] The paper states that both metrics fail to comprehend negation, with CLIPScore failing in 41.36% of cases and UMIC in 44.24% of cases.
- Why unresolved: The paper does not provide a detailed analysis of why these metrics struggle with negation or suggest potential solutions.
- What evidence would resolve it: Further experiments that analyze the specific aspects of negation that confuse the metrics, and the development of new techniques to improve their understanding of negation.

### Open Question 2
- Question: How sensitive are reference-free metrics to the number and size of objects mentioned in image captions?
- Basis in paper: [explicit] The paper shows that CLIPScore is sensitive to the number and size of image-relevant objects mentioned in the caption, while UMIC is only sensitive to the size of objects.
- Why unresolved: The paper does not explore the reasons behind this difference in sensitivity or investigate how these metrics could be improved to better handle varying numbers and sizes of objects.
- What evidence would resolve it: Additional experiments that analyze the impact of object number and size on metric performance, and the development of new techniques to improve their sensitivity to these factors.

### Open Question 3
- Question: How do reference-free metrics handle hallucinated objects in image captions?
- Basis in paper: [explicit] The paper demonstrates that both CLIPScore and UMIC can distinguish between real and hallucinated objects in image captions.
- Why unresolved: The paper does not explore the limits of this capability or investigate how these metrics might be fooled by more sophisticated hallucinations.
- What evidence would resolve it: Further experiments that test the metrics' ability to detect more complex hallucinations, and the development of new techniques to improve their robustness against such attacks.

## Limitations
- The controlled experiments may not fully capture the complexity of real-world image captioning scenarios
- The use of templates and generated captions may not reflect the diversity and nuance of naturally occurring captions
- The study focuses on only two specific metrics, which may not be representative of the entire landscape of reference-free evaluation metrics

## Confidence
- High Confidence: The finding that both CLIPScore and UMIC struggle to identify fine-grained errors is well-supported by the experimental results
- Medium Confidence: The observation that CLIPScore is more sensitive to object mentions and size, while UMIC is more sensitive to sentence structure, is based on the experiments conducted
- Low Confidence: The study's conclusions about the overall reliability of reference-free metrics are based on a limited set of experiments

## Next Checks
1. Apply the metrics to a diverse set of real-world image captions to assess their performance in more natural settings
2. Evaluate the performance of other reference-free metrics (e.g., PAC-S) on the same controlled experiments
3. Investigate whether fine-tuning the underlying vision-language models on datasets with fine-grained errors can improve the metrics' ability to detect such errors