---
ver: rpa2
title: How Powerful are Decoder-Only Transformer Neural Models?
arxiv_id: '2305.17026'
source_url: https://arxiv.org/abs/2305.17026
tags:
- decoder-only
- transformer
- input
- turing
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes the Turing completeness of decoder-only\
  \ transformer models, a theoretical foundation previously lacking despite their\
  \ widespread use in modern large language models. The authors show that even single-layer,\
  \ single-head decoder-only transformers can simulate arbitrary recurrent neural\
  \ networks (RNNs) by encoding both the current input and hidden state into the model\u2019\
  s output vector."
---

# How Powerful are Decoder-Only Transformer Neural Models?

## Quick Facts
- arXiv ID: 2305.17026
- Source URL: https://arxiv.org/abs/2305.17026
- Reference count: 8
- Primary result: Decoder-only transformers are Turing complete

## Executive Summary
This paper establishes the Turing completeness of decoder-only transformer models, a theoretical foundation previously lacking despite their widespread use in modern large language models. The authors show that even single-layer, single-head decoder-only transformers can simulate arbitrary recurrent neural networks (RNNs) by encoding both the current input and hidden state into the model's output vector. A key finding is that the model's dimensionality must exceed the token embedding size to ensure sufficient space for state representation, highlighting the importance of embedding sparsity. The work classifies these models as "B machines," a variant of Turing machines without erasure, and identifies open questions about computational limits under output constraints.

## Method Summary
The authors prove Turing completeness by constructing a simulation where decoder-only transformers encode both the hidden state and input from the previous time step into their output vector at each step. This requires the model dimension to be larger than the token embedding dimension (d = 2 · dembed + 3) to provide sufficient space for state representation. The attention mechanism selects the correct previous time step's vector based on position encoding, and the feedforward network implements arbitrary RNN weight matrices and state transitions. The construction uses hardmax attention and shows that decoder-only transformers can simulate any RNN, which are known to be Turing complete.

## Key Results
- Single-layer, single-head decoder-only transformers can simulate arbitrary RNNs
- Model dimension must exceed token embedding dimension (d > dembed) for Turing completeness
- Decoder-only transformers are classified as "B machines" (non-erasing Turing machines)
- Sparsity/compressibility of word embeddings is necessary for Turing completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoder-only transformers simulate RNNs by encoding both the current input and hidden state into the model's output vector at each time step.
- Mechanism: At each step t, the model's attention mechanism attends to the vector containing the hidden state h_t and input x_t from the previous step. The feedforward network then computes h_{t+1} based on these values, effectively simulating one step of an RNN.
- Core assumption: The model can maintain sufficient information in its vector representation to encode both the hidden state and input at each time step.
- Evidence anchors:
  - [abstract] "The authors show that even single-layer, single-head decoder-only transformers can simulate arbitrary recurrent neural networks (RNNs) by encoding both the current input and hidden state into the model's output vector."
  - [section 5.1] "Recall the base embedding has a dimension of dembed. Therefore, the input dimension of the FFN must be 2 · dembed + 3. From the requirements inherited from transformer conventions, the model dimension must be equivalent to the input dimension of the FFN. So, we choose d = 2 · dembed + 3. Each y ∈ Y is composed as y = [fb(σk), fb(σlatent), i = pos(k), t, stop]."

### Mechanism 2
- Claim: The attention mechanism can select the correct previous time step's vector to attend to based on position information.
- Mechanism: The attention query q_t is constructed such that its dot product with key vectors k_i will be highest when i = t, allowing the model to attend to the vector from the correct previous time step.
- Core assumption: The attention mechanism can reliably select the correct previous time step based on position encoding.
- Evidence anchors:
  - [section 5.2.1] "Lemma 5.4 (Attending to xt) Selection of tth element is readily doable based on the method used in (Bhattamishra et al., 2020) which is to use the position value minus the current position value as the score function."
  - [section 5.2.1] "Being precise, qt = [σt, 0dembed, i, t, stop] and k = [0 dembed, 0dembed, 1, -1, 0]. Therefore, ⟨qt, k⟩ = i - t. By application of a nonlinear function, the attention on each v ∈ V is αk = -|i - t|. Therefore, hardmax(V) = 1 when k = t and 0 ∀k ≠ t. Therefore, for all t ≤ k, Attn(qt, K, V) = xt."

### Mechanism 3
- Claim: The feedforward network can implement arbitrary RNN weight matrices and state transitions.
- Mechanism: The FFN weights are set to implement the specific RNN being simulated, computing the next hidden state h_{t+1} = f(h_t, x_t) where f is the RNN's transition function.
- Core assumption: Feedforward networks are universal function approximators capable of implementing arbitrary functions given sufficient capacity.
- Evidence anchors:
  - [section 5.2.2] "We define FFNb such that the output of the network is zL_k = [ht+1, 0db, k + 1, t + 1, stop]. The first dembed elements of the vector requires the FFN to implement the weights of the RNN."
  - [section 5.2.2] "The ability of an FFN to implement the weights of a neural network is a tautology. An always one and always zero output from an FFN can be implemented via extraordinarily large and extraordinarily small bias, respectively."

## Foundational Learning

- Concept: Turing completeness
  - Why needed here: The paper's central claim is that decoder-only transformers are Turing complete, which is a fundamental theoretical result about their computational power.
  - Quick check question: What does it mean for a computational model to be Turing complete, and why is this property significant for language models?

- Concept: Recurrent neural networks (RNNs)
  - Why needed here: The proof of Turing completeness relies on showing that decoder-only transformers can simulate arbitrary RNNs, which are known to be Turing complete.
  - Quick check question: How do RNNs maintain state across time steps, and what is the significance of this property for sequential computation?

- Concept: Attention mechanisms
  - Why needed here: The attention mechanism is crucial for the transformer's ability to select and process information from previous time steps, which is essential for the simulation of RNNs.
  - Quick check question: How does the attention mechanism in transformers differ from the recurrent connections in RNNs, and what advantages does it offer for long-range dependencies?

## Architecture Onboarding

- Component map: Input tokens with positional encoding -> Multi-head self-attention -> Feedforward network -> Output vector containing state and input

- Critical path:
  1. Token embedding and positional encoding
  2. Self-attention to select previous time step's vector
  3. Feedforward network to compute next state
  4. Append output to input sequence
  5. Repeat until termination condition met

- Design tradeoffs:
  - Model dimension vs. token embedding size: Must have d > dembed to encode both state and input
  - Attention head count: Single head sufficient for proof, but multiple heads may improve practical performance
  - Activation function: Hardmax used for theoretical proof, softmax more common in practice

- Failure signatures:
  - Insufficient model dimension leading to information loss
  - Attention mechanism failing to select correct time step
  - FFN unable to implement target RNN's weight matrix
  - Position encoding corruption causing misalignment

- First 3 experiments:
  1. Implement a simple counter RNN (incrementing a counter at each step) and verify the transformer can simulate it correctly
  2. Test the transformer's ability to simulate a finite state machine with a small number of states
  3. Verify that the transformer can simulate a parity checking RNN (outputting whether the number of 1s seen so far is even or odd)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are decoder-only transformers Turing complete without the sparsity/compressibility of word embeddings?
- Basis in paper: [explicit] The authors state "the sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold" and show that model dimensionality must exceed token embedding size for Turing completeness.
- Why unresolved: The paper only establishes that sparsity/compressibility is necessary, not whether it's sufficient or what happens in the absence of this property.
- What evidence would resolve it: A formal proof showing that decoder-only transformers can or cannot achieve Turing completeness with non-sparse, non-compressible embeddings, or empirical evidence from models using dense embeddings.

### Open Question 2
- Question: What is the computational expressiveness of decoder-only transformers when output constraints limit access to auxiliary computational space?
- Basis in paper: [explicit] The authors discuss how decoder-only models are more like "B machines" (non-erasing Turing machines) and note that "It remains an open question whether we can dispense with auxiliary squares and still be able to compute all recursive functions."
- Why unresolved: The paper identifies this as an "apparently unconsidered point" and suggests it's an important direction for future research, but doesn't provide a formal evaluation.
- What evidence would resolve it: A formal proof establishing the relationship between output constraints and computational expressiveness, or empirical studies comparing models with different output constraints.

### Open Question 3
- Question: Does the relationship between model size and effectiveness stem from application-induced limitations on computational expressivity?
- Basis in paper: [inferred] The authors suggest "it is our opinion, that the strong link between model size and model effectiveness is linked to application induced limitations on the computational expressivity."
- Why unresolved: This is presented as an opinion based on the authors' theoretical analysis, not as a proven fact, and would require empirical validation.
- What evidence would resolve it: Empirical studies comparing model performance across different applications with varying computational expressivity requirements, or theoretical work establishing a formal link between expressivity limitations and model effectiveness.

## Limitations
- Requires model dimension to exceed token embedding dimension, creating practical limitations
- Proof uses hardmax attention while practical implementations use softmax
- Theoretical results may not fully translate to practical performance gains
- Single-layer, single-head constraint may not capture full computational power of practical architectures

## Confidence

**High Confidence**:
- Decoder-only transformers with d > dembed can simulate arbitrary RNNs
- The construction provides a valid Turing-completeness proof under specified conditions
- Attention mechanism can reliably select previous time steps using position encoding

**Medium Confidence**:
- The dimensionality requirement is necessary (could be specific to the proof construction)
- Single-layer, single-head architecture captures the full computational power of decoder-only transformers
- Hardmax attention is sufficient for Turing completeness (softmax implications unclear)

**Low Confidence**:
- Practical implications of the theoretical results for real LLM performance
- Whether the "B machine" classification has meaningful practical consequences
- How computational constraints in practice affect the theoretical guarantees

## Next Checks
1. **Architectural Variation Experiment**: Test whether multi-layer, multi-head decoder-only transformers can simulate more complex computational models (e.g., pushdown automata or linear-bounded automata) that RNNs cannot handle, potentially exceeding Turing completeness in specific regimes.

2. **Dimensionality Reduction Study**: Systematically reduce the model dimension relative to embedding size and measure at what point the transformer loses the ability to simulate specific RNNs, establishing practical bounds on the theoretical requirement.

3. **Softmax Behavior Analysis**: Replace hardmax with softmax in the theoretical construction and analyze how the probabilistic attention affects the simulation fidelity, particularly for long sequences where softmax variance might accumulate.