---
ver: rpa2
title: 'Structure Learning with Continuous Optimization: A Sober Look and Beyond'
arxiv_id: '2304.02146'
source_url: https://arxiv.org/abs/2304.02146
tags:
- noise
- learning
- structure
- golem-nv
- variances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critically examines continuous optimization for directed
  acyclic graph (DAG) structure learning, challenging claims about the role of varsortability
  and data standardization. Through theoretical analysis and empirical studies, the
  authors show that: (1) high varsortability does not guarantee good performance,
  even with equal noise variances; (2) nonconvexity is a major concern, especially
  for non-equal noise variances, causing poor performance regardless of initialization;
  (3) thresholding and sparsity penalty choices significantly impact results, with
  adaptive methods potentially improving reliability.'
---

# Structure Learning with Continuous Optimization: A Sober Look and Beyond

## Quick Facts
- arXiv ID: 2304.02146
- Source URL: https://arxiv.org/abs/2304.02146
- Authors: 
- Reference count: 40
- Key outcome: Continuous optimization for DAG structure learning faces significant challenges with nonconvexity and thresholding, particularly in non-equal noise variance settings, requiring improved methods for reliable structure recovery.

## Executive Summary
This paper critically examines continuous optimization approaches for directed acyclic graph (DAG) structure learning, challenging the prevailing understanding of why these methods work. The authors demonstrate that varsortability—the agreement between marginal variance order and topological order—is not the primary driver of good performance, especially when noise variances are non-equal. Through extensive empirical studies, they show that nonconvexity in the optimization landscape is a major concern that causes poor performance regardless of initialization strategy. The paper also highlights the critical role of thresholding and sparsity penalty choices in the final structure recovery, suggesting that current approaches may be overly optimistic about their performance.

## Method Summary
The paper investigates continuous optimization for DAG structure learning through a comprehensive empirical study. It compares equal noise variance (EV) formulations like NOTEARS-EV and GOLEM-EV against non-equal noise variance (NV) formulations like GOLEM-NV across various synthetic data settings. The methods use least squares scores and constrained optimization to learn DAG structures from linear Gaussian SEMs with different noise ratios, variable counts, and graph densities. Evaluation metrics include Structural Hamming Distance (SHD), F1 scores, and recall, comparing continuous methods against discrete approaches like PC and FGES. The study systematically examines initialization strategies, DAG constraints, thresholding approaches, and sparsity penalties to understand their impact on performance.

## Key Results
- High varsortability does not guarantee good performance, even with equal noise variances, as nonconvexity can still cause poor optimization results
- Nonconvexity is a major concern for non-equal noise variances, causing optimization to get trapped in suboptimal local minima regardless of initialization strategy
- Thresholding and sparsity penalty choices significantly impact final structure quality, with fixed thresholds often being suboptimal and adaptive methods potentially improving reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous optimization for DAG structure learning relies on the agreement between marginal variance order and topological order (varsortability) to guide optimization.
- **Mechanism**: The least squares score used in NOTEARS-EV and GOLEM-EV implicitly encodes varsortability when noise variances are equal. If a variable has lower marginal variance than its descendants, the least squares score will favor the correct edge direction.
- **Core assumption**: Varsortability is high in the data generating process and the noise variances are equal across variables.
- **Evidence anchors**:
  - [abstract] "Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order"
  - [section] "Consider the least squares score used by Zheng et al. (2018); Loh and Bühlmann (2014). For bivariate case, say X = (X1,X 2) with ground truth X1→X2, Reisach et al. (2021) showed that inferences based on varsortability and least squares are consistent in the sense that v = 1, i.e., Var(X1)< Var(X2) if and only if the least squares score computed with X1→X2 is smaller than that with X2→X1"
  - [corpus] Found related work on DAG estimation and varsortability, but no direct evidence about this specific mechanism
- **Break condition**: When noise variances are non-equal, the least squares score no longer encodes varsortability, and the optimization can find structures that minimize the score but are not the true DAG.

### Mechanism 2
- **Claim**: The nonconvexity of the optimization landscape in the non-equal noise variances case leads to poor performance regardless of initialization.
- **Mechanism**: The likelihood function for non-equal noise variances creates a highly nonconvex optimization landscape with many local minima. Gradient-based optimization methods get trapped in these suboptimal solutions.
- **Core assumption**: The optimization landscape is indeed highly nonconvex and contains many suboptimal local minima.
- **Evidence anchors**:
  - [abstract] "We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve improvement in this case"
  - [section] "Empirical studies. We consider 50-variable linear Gaussian model with ER1 graphs and noise ratios r∈{ 1, 2, 4, 8, 16, 32, 64}. We use a large sample size 106 to reduce the errors caused by finite samples and to focus on the aspect of nonconvex optimization. We experiment with different initialization schemes, including GOLEM-NV initialized by zero matrix, by solution of GOLEM-EV, and by solution of FGES"
  - [corpus] Found related work on nonconvex optimization in DAG learning, but no direct evidence about this specific mechanism
- **Break condition**: When noise variances are equal, the optimization landscape is less nonconvex and gradient-based methods can find better solutions.

### Mechanism 3
- **Claim**: Thresholding and sparsity penalty choices significantly impact the final structure learned by continuous optimization methods.
- **Mechanism**: The continuous optimization methods output real-valued edge weights, which must be thresholded to obtain a binary adjacency matrix. The choice of threshold and sparsity penalty determines which edges are included or excluded.
- **Core assumption**: The continuous optimization methods output edge weights that can be meaningfully thresholded to obtain the true structure.
- **Evidence anchors**:
  - [abstract] "Lastly, we provide insights into other aspects of the search procedure, including thresholding and sparsity, and show that they play an important role in the final solutions"
  - [section] "We investigate the impact of threshold specification on continuous structure learning approaches, and show that it plays an important role for the final solutions. Existing continuous approaches (Zheng et al., 2018; Ng et al., 2020; Yu et al., 2021; Bello et al., 2022) typically use edge weights sampled uniformly from [−2,−0.5]∪ [0.5, 2] for the true weighted adjacency matrix and apply a threshold of 0.3 to identify the final structure"
  - [corpus] Found related work on thresholding in DAG learning, but no direct evidence about this specific mechanism
- **Break condition**: When the true edge weights are small relative to the threshold, many true edges will be missed.

## Foundational Learning

- **Concept**: Directed Acyclic Graphs (DAGs) and their properties
  - Why needed here: The entire paper is about learning DAG structures from data, so understanding what DAGs are and their properties is fundamental
  - Quick check question: What is a DAG and what property makes it different from other graph types?

- **Concept**: Linear Structural Equation Models (SEMs)
  - Why needed here: The paper focuses on linear SEMs as the data generating process, so understanding how they work is crucial
  - Quick check question: How does a linear SEM relate to a DAG?

- **Concept**: Continuous optimization and gradient-based methods
  - Why needed here: The paper proposes using continuous optimization to learn DAG structures, so understanding how these methods work is essential
  - Quick check question: What is the difference between constrained and unconstrained optimization?

## Architecture Onboarding

- **Component map**: Data generation -> Continuous optimization (NOTEARS-EV/GOLEM-EV/GOLEM-NV) -> Thresholding and sparsity penalty application -> Structure evaluation
- **Critical path**: Data generation → Continuous optimization → Thresholding and sparsity penalty application → Structure evaluation
- **Design tradeoffs**: Using continuous optimization allows leveraging gradient-based methods but introduces nonconvexity issues. Handling non-equal noise variances makes the approach more general but significantly harder to optimize. Different search strategies and DAG constraints offer various tradeoffs between computational efficiency and solution quality.
- **Failure signatures**: Poor performance on data with non-equal noise variances, sensitivity to initialization, and suboptimal thresholding leading to many false discoveries or missed edges.
- **First 3 experiments**:
  1. Verify the nonconvexity issue by comparing optimization results with different initializations on data with non-equal noise variances
  2. Test the impact of different thresholding strategies on final structure quality
  3. Compare the performance of different sparsity penalties (L1, SCAD, MCP) on structure recovery

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific algorithmic modifications to continuous optimization approaches would effectively mitigate the nonconvexity issues identified in the non-equal noise variances formulation?
  - Basis in paper: [explicit] The paper identifies nonconvexity as a major concern, especially for non-equal noise variances formulation, causing poor performance regardless of initialization.
  - Why unresolved: The paper demonstrates the problem but does not provide concrete algorithmic solutions to address it.
  - What evidence would resolve it: A new continuous optimization algorithm that achieves comparable performance to discrete methods like PC and FGES on non-equal noise variances cases, with rigorous theoretical guarantees of convergence to global optima.

- **Open Question 2**: How can adaptive thresholding be designed to reliably identify edges from continuous structure learning solutions across different settings while reducing false discoveries from both finite-sample errors and nonconvexity?
  - Basis in paper: [explicit] The paper shows that fixed thresholding (e.g., 0.3) is often suboptimal and that thresholding can help reduce false discoveries from both finite-sample errors and nonconvexity.
  - Why unresolved: While the paper identifies the importance of thresholding, it does not propose a specific adaptive thresholding methodology.
  - What evidence would resolve it: An adaptive thresholding algorithm that consistently outperforms fixed thresholding across various simulation settings and provides statistical guarantees on edge identification accuracy.

- **Open Question 3**: What are the fundamental theoretical limits on the performance of continuous optimization approaches for DAG structure learning in high-dimensional settings with non-equal noise variances?
  - Basis in paper: [inferred] The paper demonstrates that continuous approaches struggle with non-equal noise variances and nonconvexity, but does not establish theoretical performance bounds.
  - Why unresolved: The paper focuses on empirical analysis and practical observations rather than theoretical characterization of fundamental limits.
  - What evidence would resolve it: Information-theoretic lower bounds on sample complexity and error rates for continuous optimization approaches in high-dimensional DAG structure learning, along with matching upper bounds for specific algorithms.

## Limitations

- The paper's findings about nonconvexity and thresholding sensitivity are based primarily on synthetic experiments, leaving real-world applicability uncertain
- The proposed adaptive sparsity penalties lack extensive validation across diverse data distributions and graph structures
- The paper does not provide concrete algorithmic solutions to address the identified nonconvexity issues in non-equal noise variance settings

## Confidence

- High confidence: Claims about equal noise variance performance being well-understood and reliable
- Medium confidence: Claims about nonconvexity causing optimization failures in non-equal noise cases
- Low confidence: Claims about specific adaptive thresholding and sparsity penalty improvements without broader empirical validation

## Next Checks

1. Test the nonconvexity hypothesis by comparing optimization trajectories from multiple random initializations on non-equal noise data, specifically examining whether different initializations converge to similar objective values or get trapped in distinct local minima
2. Validate the thresholding sensitivity findings on real-world datasets (e.g., gene expression, economic indicators) where noise variance heterogeneity is expected, comparing performance across different thresholding strategies
3. Implement and test the proposed adaptive sparsity penalties (SCAD, MCP) on benchmark causal discovery datasets to assess whether they provide consistent improvements over standard L1 penalties across varying graph densities and noise structures