---
ver: rpa2
title: 'Learning Capacity: A Measure of the Effective Dimensionality of a Model'
arxiv_id: '2305.17332'
source_url: https://arxiv.org/abs/2305.17332
tags:
- learning
- capacity
- number
- test
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a quantity called the "learning capacity" to
  measure the effective dimensionality of a model. It exploits a formal correspondence
  between statistical inference and thermodynamics, where the number of samples can
  be thought of as the inverse temperature.
---

# Learning Capacity: A Measure of the Effective Dimensionality of a Model

## Quick Facts
- arXiv ID: 2305.17332
- Source URL: https://arxiv.org/abs/2305.17332
- Reference count: 40
- The learning capacity measures effective dimensionality by capturing the drop in test loss when adding one more training sample.

## Executive Summary
This paper introduces "learning capacity" as a measure of a model's effective dimensionality, analogous to heat capacity in thermodynamics. The quantity captures how much a model's test loss decreases when one additional training sample is added. Unlike raw parameter counts, learning capacity reflects the number of degrees of freedom actually constrained by the data. The authors demonstrate that this measure is typically much smaller than the parameter count for deep networks and provides practical guidance for when to collect more data versus when to seek better architectures.

## Method Summary
The authors estimate learning capacity by training models on varying numbers of samples and measuring the change in test loss. For each sample size N, they perform k-fold cross-validation to estimate average energy (test loss), then fit a polynomial or sigmoid curve to these values. The learning capacity is calculated as the second derivative of the log-partition function with respect to N. They validate their approach using both direct cross-validation and MCMC-based estimation methods. Models are trained using SGD with Nesterov momentum and cosine learning rate schedules, with architectures ranging from simple MLPs to LeNet, ALLCNN, and Wide-ResNet for images, plus random forests and k-NN for tabular data.

## Key Results
- Learning capacity is typically a tiny fraction of the number of parameters for deep networks trained on typical datasets
- The test loss as a function of learning capacity does not exhibit double descent
- Learning capacity saturates at very small and very large sample sizes, providing guidelines for data collection vs. architecture search
- The measure works for both parametric models (neural networks) and non-parametric models (random forests, k-NN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning capacity measures the effective dimensionality of a model by capturing the drop in test negative log-likelihood when adding one more datum.
- Mechanism: Uses the formal correspondence between thermodynamics and inference, where sample size N acts as inverse temperature. The learning capacity is derived as the second derivative of the log-partition function with respect to N, analogous to heat capacity in physics.
- Core assumption: The model's generalization behavior can be captured by thermodynamic analogues when viewed through the lens of statistical inference.
- Evidence anchors:
  - [abstract] "The learning capacity is proportional to the drop in the test negative log-likelihood if one additional datum is added to the training dataset."
  - [section] "The learning capacity of a model is proportional to the drop in the test negative log-likelihood if one additional datum is added to the training dataset."
  - [corpus] "Average neighbor FMR=0.383, average citations=0.0" (weak corpus support)
- Break condition: If the model's loss landscape doesn't follow thermodynamic principles or if the correspondence between samples and inverse temperature breaks down.

### Mechanism 2
- Claim: Learning capacity correlates well with test loss and is much smaller than the number of parameters for deep networks.
- Mechanism: The learning capacity captures the number of constrained degrees of freedom in the trained model, which is a small fraction of total parameters due to redundancies and symmetries in the weight space.
- Core assumption: The number of effectively constrained degrees of freedom is what determines generalization performance, not the raw parameter count.
- Evidence anchors:
  - [abstract] "the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets"
  - [section] "the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets"
  - [corpus] "Complexity Matters: Effective Dimensionality as a Measure for Adversarial Robustness" (relevant but weak citation)
- Break condition: If the model's parameter space lacks the symmetries or redundancies that make most parameters unconstrained.

### Mechanism 3
- Claim: Learning capacity provides guidelines for when to procure more data vs. when to search for new architectures.
- Mechanism: Learning capacity saturates at very small and very large sample sizes, with a transition threshold indicating diminishing returns from adding more data.
- Core assumption: The saturation behavior of learning capacity reflects fundamental limits of the model's representational capacity given the data.
- Evidence anchors:
  - [abstract] "The learning capacity saturates at very small and very large sample sizes; the threshold that characterizes the transition between these two regimes provides guidelines as to when one should procure more data and when one should search for a different architecture to improve performance."
  - [section] "The learning capacity of a model saturates at very small and very large sample sizes; the threshold that characterizes the transition between these two regimes provides guidelines as to when one should procure more data or whether one should search for new architectures, to improve performance."
  - [corpus] No strong corpus support found
- Break condition: If the model architecture can adapt its effective dimensionality beyond what learning capacity measures, or if the sample size regime doesn't follow the expected saturation pattern.

## Foundational Learning

- Concept: Thermodynamic-Inference Correspondence
  - Why needed here: Provides the theoretical foundation for defining learning capacity as a thermodynamic quantity (heat capacity analogue)
  - Quick check question: How does treating sample size N as inverse temperature β enable the use of thermodynamic concepts in machine learning?

- Concept: PAC-Bayes Generalization Bounds
  - Why needed here: Used to validate learning capacity estimates by comparing with established effective dimensionality measures
  - Quick check question: What role does the Hessian of the loss function play in the PAC-Bayes calculation of effective dimensionality?

- Concept: Singular Learning Theory
  - Why needed here: Provides the theoretical framework for understanding why learning capacity is much smaller than parameter count
  - Quick check question: How does the log-canonical threshold in singular learning theory relate to the effective dimensionality captured by learning capacity?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Cross-validation loss calculation -> Thermodynamic quantity estimation -> Saturation analysis and interpretation
- Critical path: Sample selection → Model training → Cross-validation loss calculation → Thermodynamic quantity estimation → Saturation analysis and interpretation
- Design tradeoffs: MCMC-based estimation is more computationally efficient but noisier than direct training; polynomial fitting vs. sigmoid fitting for capacity estimation; choice of sample sizes for estimation accuracy
- Failure signatures: Noisy or inconsistent capacity estimates, failure to observe saturation at expected sample sizes, mismatch between capacity and test loss trends
- First 3 experiments:
  1. Estimate learning capacity for a simple fully-connected network on MNIST with varying hidden layer sizes
  2. Compare learning capacity estimates using MCMC vs. cross-validation approaches on the same dataset
  3. Test the data saturation hypothesis by training on subsets of increasing size and observing capacity behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the learning capacity and the double descent phenomenon in deep learning models?
- Basis in paper: [explicit] The paper shows that the test loss as a function of learning capacity does not exhibit double descent, contrasting with the typical double descent curve when plotting test loss against the number of parameters.
- Why unresolved: The paper provides empirical evidence that learning capacity does not show double descent, but a theoretical explanation for why this relationship holds is not provided.
- What evidence would resolve it: A theoretical framework explaining why learning capacity, as a measure of effective dimensionality, prevents double descent, and experimental validation on a wider variety of architectures and datasets.

### Open Question 2
- Question: How does the learning capacity vary with different training algorithms, and what does this imply about the nature of solutions found?
- Basis in paper: [inferred] The paper mentions that the MCMC-based learning capacity estimates are higher than those using SGD, suggesting differences in the solutions found by different algorithms.
- Why unresolved: The paper does not explore in depth how different training algorithms affect the learning capacity and what this implies about the nature of the solutions found.
- What evidence would resolve it: Systematic experiments comparing the learning capacity across various training algorithms (e.g., SGD, Adam, RMSprop) and analysis of the solutions' properties (e.g., flatness, sharpness) to understand the implications.

### Open Question 3
- Question: What is the theoretical justification for the freezing phenomenon observed in the learning capacity at both small and large sample sizes?
- Basis in paper: [explicit] The paper observes that the learning capacity saturates at very small and very large sample sizes, with different architectures having the same capacity at small sample sizes.
- Why unresolved: The paper notes the freezing phenomenon but does not provide a theoretical explanation for why this occurs.
- What evidence would resolve it: A theoretical model explaining the freezing behavior of learning capacity, possibly drawing from statistical physics or singular learning theory, and empirical validation on a broad range of models and datasets.

### Open Question 4
- Question: How does the learning capacity scale with the complexity of the dataset, and what are the implications for model selection in practice?
- Basis in paper: [explicit] The paper shows that datasets with higher complexity (e.g., CIFAR-10 vs. MNIST) have higher learning capacities.
- Why unresolved: While the paper demonstrates a correlation between dataset complexity and learning capacity, it does not provide guidelines on how to use this information for model selection in practice.
- What evidence would resolve it: Experimental studies correlating learning capacity with various dataset complexity measures (e.g., effective dimensionality, intrinsic dimensionality) and practical guidelines for using learning capacity to select models for different types of datasets.

## Limitations
- The thermodynamic correspondence between samples and inverse temperature relies on specific assumptions about the loss landscape that may not hold for all architectures
- The MCMC-based estimation method, while computationally efficient, introduces additional noise that could affect the precision of capacity estimates
- The saturation behavior at extreme sample sizes needs more rigorous validation across diverse model families

## Confidence
- High confidence in the core thermodynamic framework and the mathematical derivation of learning capacity as second derivative of log-partition function
- Medium confidence in the empirical validation showing learning capacity is much smaller than parameter count for deep networks
- Medium confidence in the practical utility of capacity estimates for guiding data collection vs. architecture search decisions, though this requires more extensive real-world testing

## Next Checks
1. Validate the thermodynamic correspondence empirically by testing if capacity estimates remain consistent when varying the temperature-like parameter (sample size) across multiple orders of magnitude.

2. Compare MCMC-based capacity estimation with the computationally intensive cross-validation approach on identical model-dataset pairs to quantify the noise introduced by the MCMC approximation.

3. Test the architecture selection guidance by training models with identical capacities but different architectures on the same dataset to verify if performance differences correlate with capacity-derived recommendations.