---
ver: rpa2
title: Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large
  Language Models
arxiv_id: '2305.15074'
source_url: https://arxiv.org/abs/2305.15074
tags:
- gpt-4
- problems
- problem
- reasoning
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present JEEBench, a challenging problem-solving benchmark
  dataset comprising 450 pre-engineering level mathematics, physics, and chemistry
  problems from the IIT JEE-Advanced exam. They evaluate various GPT models on this
  benchmark using techniques like Chain-of-Thought prompting and Self-Consistency.
---

# Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models

## Quick Facts
- arXiv ID: 2305.15074
- Source URL: https://arxiv.org/abs/2305.15074
- Reference count: 32
- Primary result: GPT-4 achieves less than 40% accuracy on JEEBench, a challenging pre-engineering problem-solving benchmark

## Executive Summary
This paper introduces JEEBench, a challenging problem-solving benchmark dataset comprising 450 pre-engineering level mathematics, physics, and chemistry problems from the IIT JEE-Advanced exam. The authors evaluate various GPT models on this benchmark using techniques like Chain-of-Thought prompting and Self-Consistency. Despite these advanced techniques, the highest performance remains below 40%. The benchmark aims to guide future research in problem-solving using large language models by identifying key challenges in concept retrieval, mathematical grounding, and algebraic manipulation.

## Method Summary
The authors construct JEEBench by curating 450 problems from past 7 years of JEE-Advanced exam, categorized by subject and question type. They evaluate GPT-3, GPT-3.5, and GPT-4 models using zero-shot Chain-of-Thought prompting and Self-Consistency techniques. The evaluation pipeline involves generating multiple responses per question and selecting the most consistent answer. Detailed error analysis is performed on GPT-4 responses to identify conceptual, grounding, and computation errors.

## Key Results
- GPT-4 achieves the highest performance among tested models, but remains below 40% accuracy
- Performance improves with newer models (GPT-4 > GPT-3.5 > GPT-3)
- Chain-of-Thought prompting and Self-Consistency techniques provide marginal improvements
- Primary error sources include difficulties in retrieving domain-specific concepts, grounding them into mathematical equations, and performing algebraic manipulations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in the JEEBench benchmark.
- **Mechanism:** Problems require retrieval of domain-specific concepts, grounding them into mathematical equations, and performing algebraic manipulations.
- **Core assumption:** LLMs need to understand and apply complex scientific concepts before they can solve the problems.
- **Evidence anchors:**
  - [abstract] "Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark."
  - [section] "Solution to problems in the dataset require the correct application of high-level domain specific concepts, grounding them into mathematical equations or constraints, followed by algebraic manipulation and arithmetic operations."
- **Break condition:** If the LLM fails to retrieve relevant concepts or perform algebraic manipulations correctly, it will not be able to solve the problems.

### Mechanism 2
- **Claim:** Chain-of-Thought prompting and Self-Consistency techniques can improve reasoning capabilities of LLMs on complex problems.
- **Mechanism:** These techniques help LLMs break down complex problems into smaller steps and aggregate multiple responses to arrive at the correct answer.
- **Core assumption:** LLMs can benefit from explicit step-by-step reasoning and aggregation of multiple attempts.
- **Evidence anchors:**
  - [abstract] "Our evaluation on the GPT series of LLMs reveals that although performance improves with newer models, the best being GPT-4, the highest performance, even after using techniques like Self-Consistency and Chain-of-Thought prompting, is less than 40%."
  - [section] "We conduct a qualitative and quantitative study of the GPT series of LLMs on these problems. Additionally, we assess the efficacy of methods such as Chain-of-Thought prompting (Kojima et al., 2023) and Self-Consistency (Wang et al., 2023) which have been proposed to improve the reasoning abilities of LLMs on our benchmark."
- **Break condition:** If the LLM still struggles with fundamental concepts or arithmetic, these techniques may not be sufficient to improve performance.

### Mechanism 3
- **Claim:** Post-hoc confidence-thresholding method over self-consistency can enable effective response selection in the presence of negative marking.
- **Mechanism:** This method helps LLMs decide whether to attempt a question or not, based on their confidence in the answer.
- **Core assumption:** LLMs can assess their own confidence in answers and use that to guide response selection.
- **Evidence anchors:**
  - [abstract] "Given the challenging nature of the benchmark, we hope that it can guide future research in problem-solving using LLMs."
  - [section] "Another aspect of evaluating LLMs on exam-based benchmarks is comparison with human performance. With negative marking and time budget in play, the objective of maximizing test score becomes a planning problem. GPT-4 has a disadvantage in this scenario, partly due to its sub-par planning abilities and the deterioration in output calibration after RLHF, due to which it is not able to decide whether to attempt the question or not."
- **Break condition:** If the LLM cannot accurately assess its own confidence or if the confidence thresholds are not properly calibrated, this method may not be effective.

## Foundational Learning

- **Concept:** Deep understanding of pre-engineering level Physics, Chemistry, and Mathematics concepts
  - **Why needed here:** The benchmark requires application of complex scientific concepts and mathematical reasoning.
  - **Quick check question:** Can you explain the concept of static equilibrium and how it applies to a physics problem?

- **Concept:** Algebraic manipulation and arithmetic operations
  - **Why needed here:** Solving the problems requires accurate algebraic manipulation and arithmetic calculations.
  - **Quick check question:** Can you solve a quadratic equation and explain each step of the process?

- **Concept:** Long-horizon reasoning and problem decomposition
  - **Why needed here:** Complex problems need to be broken down into smaller, manageable steps.
  - **Quick check question:** Can you break down a multi-step problem into individual steps and explain the reasoning behind each step?

## Architecture Onboarding

- **Component map:** Data collection → Problem categorization → Model evaluation → Error analysis → Insights
- **Critical path:** Data collection and preprocessing → Model evaluation → Error analysis → Insights and future research directions
- **Design tradeoffs:** Balancing the difficulty of the benchmark with the current capabilities of LLMs, ensuring a diverse range of problem types and topics
- **Failure signatures:** Inability to retrieve relevant concepts, errors in algebraic manipulation, and failure to ground concepts into mathematical equations
- **First 3 experiments:**
  1. Evaluate a simple baseline model on a subset of the benchmark to establish a performance baseline
  2. Implement Chain-of-Thought prompting on the baseline model and measure the improvement in performance
  3. Analyze the errors made by the model in the previous experiments to identify areas for improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can large language models be augmented with scientific calculators or other tools to improve their performance on complex mathematical and scientific problem-solving tasks?
- **Basis in paper:** [explicit] The paper mentions that algebraic manipulation and calculation are still challenging for GPT models and suggests leveraging a blackbox scientific calculator as an API, as done by Toolformer.
- **Why unresolved:** While the idea of using tools like scientific calculators is proposed, the paper does not explore or demonstrate the effectiveness of such an approach in detail.
- **What evidence would resolve it:** Experiments comparing the performance of tool-augmented GPT models with standard GPT models on JEEBench or similar benchmarks, showing a significant improvement in accuracy and reliability.

### Open Question 2
- **Question:** Can a self-refinement mechanism be developed to verify and improve the mathematical reasoning of large language models in natural language?
- **Basis in paper:** [explicit] The paper suggests that a verifier capable of checking the correctness of mathematical reasoning and providing feedback on errors could greatly improve the reasoning capabilities of LLMs.
- **Why unresolved:** The paper does not propose a specific method or architecture for implementing such a verifier, nor does it provide experimental results to support its potential effectiveness.
- **What evidence would resolve it:** Development and evaluation of a self-refinement system that can accurately identify and correct errors in mathematical reasoning generated by LLMs, leading to improved performance on problem-solving benchmarks.

### Open Question 3
- **Question:** How can the planning and decision-making abilities of large language models be enhanced to better handle exam-like scenarios with negative marking and time constraints?
- **Basis in paper:** [explicit] The paper discusses that GPT-4 struggles with the planning aspect of maximizing test scores in the presence of negative marking and time budget, partly due to sub-par planning abilities and output calibration issues.
- **Why unresolved:** The paper does not provide a detailed analysis of the specific challenges in planning and decision-making for LLMs in exam settings, nor does it propose solutions to address these challenges.
- **What evidence would resolve it:** Development and evaluation of planning and decision-making frameworks that enable LLMs to effectively navigate exam-like scenarios, considering factors such as negative marking, time constraints, and question difficulty, resulting in improved performance and score optimization.

## Limitations

- The evaluation relies heavily on GPT models from a single provider, which may not generalize to other LLM architectures
- The error analysis is based on manual inspection of responses and could be subject to interpretation bias
- The benchmark's difficulty may be partially attributed to the specific format of JEE-Advanced questions rather than general problem-solving complexity

## Confidence

- **High confidence**: The benchmark construction methodology and problem categorization are clearly described and reproducible. The finding that GPT-4 achieves less than 40% accuracy, even with advanced prompting techniques, is well-supported by the presented data.
- **Medium confidence**: The error analysis framework for categorizing GPT-4 mistakes provides useful insights but relies on subjective judgment in some cases. The comparison with human performance lacks detailed methodology.
- **Low confidence**: Claims about specific mechanisms of failure (e.g., "inability to ground concepts into mathematical equations") are inferred from error patterns rather than directly tested.

## Next Checks

1. **Cross-model validation**: Test the benchmark with open-source LLMs (e.g., LLaMA, Mistral) using identical evaluation protocols to determine if the performance ceiling is specific to GPT models or representative of LLMs generally.

2. **Error analysis replication**: Have multiple independent annotators categorize a subset of GPT-4 errors to establish inter-rater reliability and identify potential biases in the current error classification scheme.

3. **Targeted intervention study**: Design and test specific prompting strategies aimed at each major error category (conceptual, grounding, computation) to determine whether improvements in one area generalize to overall performance gains.