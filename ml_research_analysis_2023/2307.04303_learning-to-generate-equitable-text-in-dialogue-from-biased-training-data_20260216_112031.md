---
ver: rpa2
title: Learning to Generate Equitable Text in Dialogue from Biased Training Data
arxiv_id: '2307.04303'
source_url: https://arxiv.org/abs/2307.04303
tags:
- dialogue
- learning
- data
- equity
- protected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a learning-theoretic framework for studying
  equitable text generation in dialogue. Formal definitions of equity are provided
  based on score parity, and it is shown that algorithms for improving equity reduce
  to algorithms for improving human-likeness on augmented data.
---

# Learning to Generate Equitable Text in Dialogue from Biased Training Data

## Quick Facts
- arXiv ID: 2307.04303
- Source URL: https://arxiv.org/abs/2307.04303
- Authors: 
- Reference count: 39
- One-line primary result: Learning theory can improve equity in dialogue generation by minimizing test divergence on augmented data

## Executive Summary
This paper introduces a learning-theoretic framework for studying equitable text generation in dialogue systems. The authors formalize equity as score parity and demonstrate that algorithms for improving equity reduce to algorithms for improving human-likeness on augmented data. They show that under reasonable assumptions about context-awareness and context-preservation in human data, dialogue systems can learn to generate equitable text even from biased training data. The framework is validated on the GuessWhat?! visual dialogue game, where it accurately predicts relative performance of multiple algorithms in generating equitable text.

## Method Summary
The approach involves pre-training models on human-human dialogue data, followed by a cooperative learning phase using machine-machine dialogue with or without data augmentation strategies. Three algorithms are implemented: Cooperative Learning (CL), LEATHER (an extension of CL incorporating human data), and Downsampled (DS) which balances the dataset by downsampling the majority gender. The methods are evaluated on human-likeness metrics (accuracy, lexical diversity, question diversity) and equity metrics (parity gaps, test divergence) using the GuessWhat?! dataset with gender as the protected attribute.

## Key Results
- Algorithms for improving equity reduce to algorithms for improving human-likeness on augmented data
- LEATHER framework can learn equitable text when context-awareness and context-preservation assumptions are met
- Data augmentation strategies can reduce data efficiency and model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithms for improving equity reduce to algorithms for improving human-likeness on augmented data.
- Mechanism: By augmenting training data to create an equitable goal distribution, minimizing test divergence (TD) indirectly optimizes parity gaps, leading to equitable text generation.
- Core assumption: The goal distribution G is equitable (balanced and satisfies score parity), and the test function h is chosen as the scoring function s from the parity definition.
- Evidence anchors:
  - [abstract] "algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data)"
  - [section 3.2] "Small Test Divergence Implies Equity" and Theorem 3.1
  - [corpus] Weak evidence; corpus neighbors are not directly relevant to this mechanism.
- Break condition: If the augmented data does not create an equitable goal distribution, or if the assumptions of Theorem 3.1 are violated.

### Mechanism 2
- Claim: Learning to be equitable and human-like can occur simultaneously from biased training data under certain context assumptions.
- Mechanism: If the human data is context-aware and context-preserving, minimizing TD on this data can lead to both human-like and equitable dialogue.
- Core assumption: The human data satisfies context-awareness (humans use context correctly) and context-preservation (presentation of context for attributes is invariant).
- Evidence anchors:
  - [abstract] "algorithms based on learning theory can even learn to generate equitable text from some types of biased training data"
  - [section 3.3] "Learning to be Equitable and Human-like" and Theorem 3.2
  - [corpus] Weak evidence; corpus neighbors are not directly relevant to this mechanism.
- Break condition: If the human data does not satisfy context-awareness or context-preservation, or if the assumptions of Theorem 3.2 are violated.

### Mechanism 3
- Claim: Data augmentation strategies can reduce data efficiency and model performance when used to improve equity.
- Mechanism: Augmenting training data to achieve equitable constraints introduces dependencies, reducing the number of i.i.d. data points and potentially harming data efficiency.
- Core assumption: The augmentation strategy reduces the effective dataset size and introduces dependencies.
- Evidence anchors:
  - [abstract] "potential for data augmentation strategies to reduce data efficiency and model performance"
  - [section 3.3] "Data Efficiency" discussion
  - [corpus] Weak evidence; corpus neighbors are not directly relevant to this mechanism.
- Break condition: If the augmentation strategy does not reduce data efficiency or if the impact is negligible.

## Foundational Learning

- Concept: Test Divergence
  - Why needed here: Test divergence (TD) is the key metric for evaluating both human-likeness and equity in dialogue systems.
  - Quick check question: How does test divergence relate to the parity gap in the context of equitable dialogue generation?

- Concept: Score Parity
  - Why needed here: Score parity is the formal definition of equity used in this paper, ensuring that the system uses language in the same way regardless of the protected attribute.
  - Quick check question: How does the choice of scoring function s determine the "way" in which language should be invariant to the protected attribute?

- Concept: Context-Awareness and Context-Preservation
  - Why needed here: These assumptions are crucial for the theory that learning to be equitable and human-like can occur simultaneously from biased training data.
  - Quick check question: How do context-awareness and context-preservation assumptions affect the validity of the theoretical results?

## Architecture Onboarding

- Component map: QGenθ -> Encβ -> Guesα (for question-player), Encβ -> Guesα (for answer-player)

- Critical path:
  1. Pre-train models on human-human dialogue.
  2. Conduct cooperative learning phase, with or without data augmentation.
  3. Evaluate human-likeness and equity using metrics like accuracy, lexical diversity, and parity gaps.

- Design tradeoffs:
  - Data augmentation can improve equity but may reduce data efficiency and model performance.
  - Balancing the trade-off between equity and human-likeness is crucial for optimal results.

- Failure signatures:
  - High test divergence indicates poor human-likeness or equity.
  - Large parity gaps suggest inequitable text generation.
  - Low data efficiency may result from aggressive data augmentation strategies.

- First 3 experiments:
  1. Implement the CL algorithm and evaluate its performance on human-likeness and equity metrics.
  2. Modify the CL algorithm to incorporate LEATHER framework and assess improvements in both human-likeness and equity.
  3. Apply data augmentation strategies (e.g., downsampling) to the LEATHER-based algorithm and compare its performance with the original LEATHER algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the DS algorithm compare to LEATHER in terms of equity metrics when the dataset contains multiple protected attributes (e.g., race, religion) instead of just gender?
- Basis in paper: Inferred from the paper's discussion on the DS algorithm's performance in the experiments with gender as the protected attribute.
- Why unresolved: The paper focuses on gender as the protected attribute and does not explore the impact of multiple protected attributes on the DS algorithm's performance.
- What evidence would resolve it: Empirical results comparing the performance of the DS algorithm with multiple protected attributes to the LEATHER algorithm in terms of equity metrics.

### Open Question 2
- Question: How does the data efficiency term (b) in the upper bound on test divergence (Eq. (11)) impact the overall learning performance when the dataset size is small, and how can this be mitigated?
- Basis in paper: Inferred from the paper's discussion on the data efficiency term (b) and its impact on learning performance.
- Why unresolved: The paper does not provide a detailed analysis of the data efficiency term's impact on learning performance in small datasets or propose mitigation strategies.
- What evidence would resolve it: Empirical results showing the impact of the data efficiency term on learning performance in small datasets and proposed mitigation strategies.

### Open Question 3
- Question: How does the choice of the scoring function s in the score parity definition affect the overall equity of the generated text, and what are the trade-offs between different scoring functions?
- Basis in paper: Explicit from the paper's discussion on the choice of scoring function s in the score parity definition.
- Why unresolved: The paper does not explore the impact of different scoring functions on the overall equity of the generated text or discuss the trade-offs between them.
- What evidence would resolve it: Empirical results comparing the equity of generated text using different scoring functions and a discussion of the trade-offs between them.

## Limitations

- Core mechanisms rely heavily on idealized assumptions about context-awareness and context-preservation in human data
- Theory's applicability is currently demonstrated on a single domain (GuessWhat?!), limiting generalizability
- Data efficiency analysis is primarily theoretical without extensive empirical validation

## Confidence

- High: The core theoretical results linking test divergence minimization to equity improvement (Theorem 3.1 and 3.2)
- Medium: The empirical validation on GuessWhat?!, given the controlled experimental setup but limited scope
- Low: The data efficiency analysis, which is primarily theoretical without extensive empirical validation

## Next Checks

1. Test the LEATHER framework on additional dialogue datasets (e.g., MultiWOZ, PersonaChat) to evaluate generalizability across domains
2. Conduct ablation studies on the context-awareness and context-preservation assumptions to quantify their impact on equity outcomes
3. Implement and evaluate alternative data augmentation strategies (e.g., counterfactual data augmentation) to compare their effectiveness and data efficiency tradeoffs