---
ver: rpa2
title: How much informative is your XAI? A decision-making assessment task to objectively
  measure the goodness of explanations
arxiv_id: '2312.04379'
source_url: https://arxiv.org/abs/2312.04379
tags:
- users
- task
- robot
- system
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an assessment task to objectively and quantitatively
  measure the goodness of XAI systems by their information power, defined as the amount
  of information provided to users during interaction. The proposed task involves
  a decision-making scenario with a simulated nuclear power plant, where users interact
  with an AI robot to learn about the task and underlying rules.
---

# How much informative is your XAI? A decision-making assessment task to objectively measure the goodness of explanations

## Quick Facts
- arXiv ID: 2312.04379
- Source URL: https://arxiv.org/abs/2312.04379
- Reference count: 8
- Key outcome: Introduces an assessment task to measure XAI goodness by quantifying how much new knowledge users acquire from interacting with the system

## Executive Summary
This paper addresses the critical need for objective and quantitative measures to evaluate the goodness of Explainable AI (XAI) systems. The authors propose an assessment task that measures XAI informativeness through "information power," defined as the amount of knowledge users gain during interaction with an AI system. The task involves a simulated nuclear power plant management scenario where users interact with a decision tree AI through what/why questions. The framework aims to objectively compare classical and user-aware XAI approaches by measuring rule understanding, generalization, and user satisfaction, moving beyond indirect performance metrics to directly assess how informative the explanations are.

## Method Summary
The proposed method involves a decision-making task with a simulated nuclear power plant where users interact with an AI robot to learn about the task and underlying rules. Users start without knowledge about the environment and can ask what and why questions to understand the robot's suggestions. The core metric, "information power," is calculated based on the number of rules learned by users relative to each feature, weighted by their informative weight. The framework includes a decision tree AI trained with Conservative Q-Improvement, and users interact through an interface allowing them to query the system. Secondary measures include performance, rule understanding, generalization, satisfaction, and robot perception. The authors plan to use this task to compare classical feature-based explanations with user-aware approaches that provide counterfactual explanations based on predicted user behavior.

## Key Results
- Introduces information power as a novel metric for objectively measuring XAI goodness
- Proposes a decision-making assessment task with a simulated nuclear power plant scenario
- Plans to compare classical and user-aware XAI approaches using the framework
- Includes secondary measures: performance, rule understanding, generalization, satisfaction, and robot perception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task measures XAI goodness by quantifying how much new knowledge users acquire from interacting with the system.
- Mechanism: Users start without knowledge about the task or environment, interact with the AI/robot through what/why questions, and their acquired rules are measured as "information power."
- Core assumption: Knowledge gained by users during interaction equals information power provided by the XAI system.
- Evidence anchors:
  - [abstract] "the goodness of XAI systems in terms of their information power, which we intended as the amount of information the system provides to the users during the interaction"
  - [section] "we need to collect measures for each rule and combine them to obtain the model's information power"
- Break condition: If users have prior knowledge about nuclear power plants or the task domain, the measurement conflates pre-existing knowledge with new information gained from the system.

### Mechanism 2
- Claim: User-centered XAI approaches are more informative because they maintain user models and provide counterfactual explanations.
- Mechanism: User-aware XAI tracks user actions and predicts future behavior, then uses these predictions to create counterfactual explanations that bridge the gap between what the robot suggests and what the user might do.
- Core assumption: Counterfactual explanations based on predicted user behavior are more informative than classical feature-based explanations.
- Evidence anchors:
  - [section] "The user-aware XAI approach, through monitoring and scaffolding, maintains a model of the users to take track of their previous actions and predict their future behaviour"
  - [section] "the predicted users' actions can be used to produce counterfactual explanations: the fact (the robot's suggestion) and the foil (the predicted users' action)"
- Break condition: If the user model is inaccurate or if users don't act predictably, counterfactual explanations may be misleading rather than informative.

### Mechanism 3
- Claim: The assessment task generalizes to other XAI systems by requiring decision-making tasks with expert AI and non-expert users.
- Mechanism: The task framework can be applied to any domain with decision-making, expert AI, and non-expert users by measuring rule understanding and generalization.
- Core assumption: Decision-making tasks with expert AI and non-expert users are sufficiently similar across domains for this assessment framework to apply.
- Evidence anchors:
  - [section] "we can easily generalise it. To generalise our task, we need the following: A decision-making task with characteristics similar to the ones listed in Section"
  - [section] "An assessment task with those characteristics is flexible enough to test different AI models and XAI techniques as long as they allow interaction between the user and the system"
- Break condition: If the decision-making task requires domain expertise that cannot be abstracted away, the framework may not apply.

## Foundational Learning

- Concept: Information power as a metric for XAI goodness
  - Why needed here: The paper's core contribution is defining information power as an objective measure of how informative an XAI system is
  - Quick check question: How is information power calculated according to the paper?

- Concept: Counterfactual explanations
  - Why needed here: The user-aware XAI approach uses counterfactual explanations based on predicted user behavior
  - Quick check question: What distinguishes counterfactual explanations from classical feature-based explanations in this context?

- Concept: Decision tree interpretation
  - Why needed here: The robot uses a decision tree AI that can be directly queried for both what and why questions
  - Quick check question: How does the decision tree structure enable both what and why question answering?

## Architecture Onboarding

- Component map:
  - Simulated nuclear power plant environment (8 features: 4 continuous, 4 discrete)
  - Decision tree AI (trained with Conservative Q-Improvement)
  - Classical XAI strategy (feature relevance-based explanations)
  - User-aware XAI strategy (user modeling and counterfactual explanations)
  - Assessment framework (measures rule understanding, generalization, satisfaction, robot perception)

- Critical path: User interaction → AI suggestion → User query → XAI explanation → User action → Measure rule acquisition → Compute information power

- Design tradeoffs:
  - Using a nuclear power plant simulation limits domain applicability but ensures no user prior knowledge
  - Decision tree AI provides transparency but may lack the performance of more complex models
  - User modeling adds complexity but enables personalized counterfactual explanations

- Failure signatures:
  - Low information power scores despite sophisticated XAI techniques may indicate the assessment framework doesn't capture relevant aspects of XAI goodness
  - High satisfaction but low information power suggests explanations may be engaging but not informative
  - Poor generalization scores despite high rule understanding may indicate overfitting to the specific task

- First 3 experiments:
  1. Run the assessment with the classical XAI approach only to establish baseline information power
  2. Run with user-aware XAI approach to measure improvement in information power
  3. Run with a non-social robot interface vs. social robot interface to test HRI effects on information acquisition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the information power of a user-aware XAI system compare to a classical XAI system in terms of the number and understanding of rules learned by users?
- Basis in paper: [explicit] The paper plans to use the proposed assessment task to compare classical and user-aware XAI approaches in terms of their effectiveness in informing users.
- Why unresolved: The paper introduces the assessment task but does not provide results comparing the two approaches.
- What evidence would resolve it: Experimental results from the proposed assessment task showing the information power, rule understanding, and generalization capabilities of both classical and user-aware XAI systems.

### Open Question 2
- Question: What is the optimal way to assign informative weights to features in the information power calculation, and how do different weighting schemes affect the assessment results?
- Basis in paper: [explicit] The paper suggests at least two ways to set the features' information weights: making them all equal or defining the weights using experimental data.
- Why unresolved: The paper does not provide a definitive answer on the best approach for assigning informative weights.
- What evidence would resolve it: Experimental results comparing the assessment task's outcomes using different informative weight assignment methods.

### Open Question 3
- Question: How does user interaction with a social robot differ from interaction with a non-social robot in terms of users' receptiveness to learning and understanding the robot's suggestions and explanations?
- Basis in paper: [explicit] The paper mentions plans to use the proposed task to test whether interacting with a social robot makes users more receptive to learning than interacting with a non-social one.
- Why unresolved: This is a planned future study and has not been conducted yet.
- What evidence would resolve it: Results from the proposed future study comparing user interaction and learning outcomes with social and non-social robots.

## Limitations
- The framework assumes users have no prior knowledge about the task domain, limiting applicability to real-world scenarios where some domain expertise exists
- The assessment is built around decision tree AI models, which may not generalize well to other AI architectures
- The nuclear power plant simulation scenario may not translate to all decision-making domains, potentially limiting the framework's generalizability

## Confidence
- **High confidence**: The assessment framework's structure and metrics are well-defined, with clear mathematical formulations for information power and secondary measures.
- **Medium confidence**: The claim that user-aware XAI approaches are more informative due to counterfactual explanations, as this relies on the accuracy of user modeling and the effectiveness of counterfactual explanations in practice.
- **Low confidence**: The generalizability of the assessment task to domains beyond nuclear power plant management, given the specific constraints and assumptions built into the current implementation.

## Next Checks
1. Test the assessment framework with a non-nuclear domain (e.g., medical diagnosis or financial decision-making) to validate generalizability.
2. Compare information power scores with traditional performance metrics to determine if the new metric captures different aspects of XAI effectiveness.
3. Conduct user studies with varying levels of domain expertise to assess whether the framework's assumption of no prior knowledge holds in practice.