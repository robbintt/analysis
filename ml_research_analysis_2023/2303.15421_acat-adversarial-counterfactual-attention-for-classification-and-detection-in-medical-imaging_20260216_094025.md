---
ver: rpa2
title: 'ACAT: Adversarial Counterfactual Attention for Classification and Detection
  in Medical Imaging'
arxiv_id: '2303.15421'
source_url: https://arxiv.org/abs/2303.15421
tags:
- saliency
- attention
- classi
- image
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ACAT, a framework that improves classification
  accuracy in medical imaging by generating soft spatial attention masks from saliency
  maps at different scales of the network. The saliency maps are obtained from adversarially
  generated counterfactual images, without requiring manual annotations.
---

# ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging

## Quick Facts
- arXiv ID: 2303.15421
- Source URL: https://arxiv.org/abs/2303.15421
- Reference count: 23
- Primary result: ACAT increases brain CT classification accuracy from 71.39% to 72.55% and lung CT accuracy from 67.71% to 70.84% without manual annotations

## Executive Summary
This paper introduces ACAT, a framework that improves medical image classification by generating soft spatial attention masks from saliency maps derived from adversarially generated counterfactual images. The method operates at multiple scales of the network without requiring manual annotations. ACAT demonstrates improved classification accuracy on both brain CT scans for lesion detection and lung CT scans for COVID-19 detection, while also enhancing lesion localization capabilities. The approach leverages adversarial counterfactual generation to create high-quality saliency maps that guide attention mechanisms, ultimately reducing pre-activation variance and improving robustness to input perturbations.

## Method Summary
ACAT generates soft spatial attention masks by processing saliency maps obtained from adversarially generated counterfactual images. The framework employs an autoencoder and classifier trained jointly, where small latent space perturbations shift the classifier's output toward the opposite class, creating counterfactual images. The absolute difference between original and counterfactual images yields saliency maps. These maps are processed into attention masks at early, middle, and late stages of the network, which are then combined through an attention fusion layer to modulate image features. This multi-scale attention approach improves classification accuracy and localization without manual annotations.

## Key Results
- Brain CT classification accuracy improved from 71.39% to 72.55%
- Lung CT classification accuracy improved from 67.71% to 70.84%
- Brain CT lesion localization score of 65.05% versus 61.29% for best competing method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulating image features with soft spatial attention masks learned from saliency maps improves classification accuracy in low signal-to-noise medical imaging tasks.
- Mechanism: Saliency maps derived from adversarially generated counterfactual images are processed into spatial attention masks at multiple network scales. These masks modulate the corresponding image features via element-wise multiplication, suppressing irrelevant regions and amplifying informative ones.
- Core assumption: The saliency maps accurately highlight the most informative regions for classification, and the attention mechanism can effectively filter noisy background signals without losing critical local features.
- Evidence anchors: [abstract] "ACAT increases the baseline classification accuracy of lesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related findings in lung CT scans from 67.71% to 70.84%"

### Mechanism 2
- Claim: Adversarial counterfactual image generation via latent space optimization improves the quality of saliency maps for medical image attention.
- Mechanism: An autoencoder and classifier are trained jointly. For each input, small latent space perturbations are applied to shift the classifier's output toward the opposite class, producing counterfactual images. The absolute difference between the original and counterfactual images yields high-quality saliency maps.
- Core assumption: The latent space is semantically meaningful and small perturbations correspond to clinically relevant changes; the autoencoder reconstruction preserves diagnostic features.
- Evidence anchors: [abstract] "we investigate how different methods to generate saliency maps... propose a way to obtain them from adversarially generated counterfactual images"

### Mechanism 3
- Claim: Reducing pre-activation variance through attention masks increases robustness to input perturbations and improves generalization.
- Mechanism: The attention masks act as learned feature selectors that reduce the variance of pre-activations across network layers, thereby dampening the impact of random noise or adversarial perturbations on the final classification output.
- Core assumption: Lower pre-activation variance correlates with increased robustness; the attention masks selectively suppress high-variance, low-signal regions.
- Evidence anchors: [abstract] "makes the model more robust to perturbations of the inputs by reducing the variance of the pre-activations of the network"

## Foundational Learning

- Concept: Saliency maps and their role in interpretability
  - Why needed here: The method relies on saliency maps to identify informative regions for attention-based feature modulation; understanding how saliency is generated and evaluated is crucial for debugging and improving the approach.
  - Quick check question: What is the difference between gradient-based saliency methods (e.g., Grad-CAM) and counterfactual-based methods used in ACAT?

- Concept: Counterfactual explanations in machine learning
  - Why needed here: ACAT generates counterfactual images to create saliency maps; understanding counterfactual generation and its relationship to adversarial attacks is essential for grasping the novelty and limitations of the approach.
  - Quick check question: How does the latent space optimization in ACAT differ from traditional adversarial attack methods?

- Concept: Attention mechanisms in deep learning
  - Why needed here: The core innovation involves applying attention at multiple scales and fusing them; understanding how spatial attention modulates features is key to implementing and extending the architecture.
  - Quick check question: Why does applying attention at multiple network scales (early, middle, late) help in low signal-to-noise tasks compared to single-scale attention?

## Architecture Onboarding

- Component map: Original image + saliency map → attention masks (early, middle, late) → attention fusion layer → fused mask → modulated image features → classification
- Critical path: Saliency map → attention mask generation → multi-scale modulation → attention fusion → classification
- Design tradeoffs: Multiple attention layers increase parameter count and training complexity but capture features at different scales; using pre-computed saliency maps decouples saliency generation from classification training but requires a reliable baseline model for counterfactual generation; soft attention allows gradient flow but may be less precise than hard attention
- Failure signatures: Accuracy drops if saliency maps are noisy or misaligned with true lesion regions; vanishing gradients if attention masks are too sparse or fusion weights collapse; overfitting if the attention mechanism memorizes training-specific noise patterns
- First 3 experiments: 1) Replace ACAT's adversarial saliency generation with a simple gradient-based method (e.g., Grad-CAM) and compare classification accuracy; 2) Remove the attention fusion layer and evaluate whether early/middle/late attention masks alone suffice; 3) Add random Gaussian noise to inputs and measure changes in pre-activation variance and classification robustness with and without ACAT

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the presented content and methodology.

## Limitations
- Limited comparison to state-of-the-art transformer-based models and attention methods
- Relatively small absolute improvements in classification accuracy (1.16% for brain CT, 3.13% for lung CT)
- Unclear optimal hyperparameters for adversarial counterfactual generation and attention fusion

## Confidence
- Classification accuracy improvements: Medium
- Localization performance: Medium
- Variance reduction and robustness claims: Low

## Next Checks
1. Implement Grad-CAM or other gradient-based saliency methods as an alternative to the adversarial counterfactual approach, and compare classification accuracy to isolate the impact of saliency generation method.
2. Conduct ablation studies removing the attention fusion layer to quantify the contribution of multi-scale attention versus single-scale attention.
3. Test model robustness by systematically adding Gaussian noise at different levels and measuring changes in classification accuracy and pre-activation variance compared to baseline models.