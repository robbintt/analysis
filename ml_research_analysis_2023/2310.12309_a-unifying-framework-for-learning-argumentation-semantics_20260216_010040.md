---
ver: rpa2
title: A Unifying Framework for Learning Argumentation Semantics
arxiv_id: '2310.12309'
source_url: https://arxiv.org/abs/2310.12309
tags:
- semantics
- arguments
- argumentation
- learning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework for learning acceptability
  semantics of argumentation frameworks using Inductive Logic Programming (ILP). The
  proposed LASarg framework learns answer set programs that capture the semantics
  of several argumentation frameworks, including Abstract Argumentation Frameworks
  (AAF), Bipolar Argumentation Frameworks (BAF), Value-based Argumentation Frameworks
  (VAF), and Assumption-based Argumentation Frameworks (ABA).
---

# A Unifying Framework for Learning Argumentation Semantics

## Quick Facts
- arXiv ID: 2310.12309
- Source URL: https://arxiv.org/abs/2310.12309
- Reference count: 5
- Key outcome: LASarg learns answer set programs equivalent to ASPARTIX encodings, outperforming existing solvers in computation time and accuracy

## Executive Summary
This paper introduces LASarg, a novel framework for learning acceptability semantics of argumentation frameworks using Inductive Logic Programming (ILP). The framework learns answer set programs that capture the semantics of Abstract Argumentation Frameworks (AAF), Bipolar Argumentation Frameworks (BAF), Value-based Argumentation Frameworks (VAF), and Assumption-based Argumentation Frameworks (ABA). The learned programs are shown to be equivalent to manually engineered ASP encodings used in ASPARTIX while demonstrating superior computational performance and requiring significantly fewer examples for learning compared to Deep Learning approaches.

## Method Summary
LASarg employs a Learning from Answer Sets (LAS) approach using ILASP to learn optimal answer set programs for computing argumentation framework extensions. The method involves generating positive and negative examples of arguments in/out of extensions, providing background knowledge with predicates like `arg`, `att`, `support`, and `val`, and defining mode declarations to specify the hypothesis space. The learned programs are evaluated against ASPARTIX encodings for equivalence and compared with µ-toksia and Deep Learning approaches for computational performance and accuracy.

## Key Results
- Learned answer set programs are equivalent to ASPARTIX encodings for stable, complete, and admissible semantics
- LASarg achieves 100% accuracy with only 7-27 examples, compared to Deep Learning's requirement for significantly more examples
- The framework demonstrates superior computational performance for larger argumentation frameworks (500-700 arguments) compared to both ASPARTIX and µ-toksia

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learned answer set programs are equivalent to manually engineered ASP encodings used in ASPARTIX for admissible, complete, and stable semantics.
- Mechanism: The ILASP system learns from examples and background knowledge to generate answer set programs that satisfy the same logical constraints as ASPARTIX's hand-crafted encodings.
- Core assumption: The examples provided to the learning system are representative of the full space of argumentation frameworks and their semantics.
- Evidence anchors:
  - [abstract] states that learned programs are "equivalent to manually engineered ASP encodings used in ASPARTIX"
  - [section] proves this equivalence in Theorem 1 for stable, complete, and admissible semantics
- Break condition: If the example set does not cover edge cases or complex interactions between arguments, the learned programs may fail to capture all constraints.

### Mechanism 2
- Claim: Learned encodings exhibit better computational performance than both ASPARTIX and µ-toksia for larger argumentation frameworks.
- Mechanism: The learned programs are more compact and efficient, avoiding the complexity and auxiliary predicates used in ASPARTIX encodings for preferred and grounded semantics.
- Core assumption: Simpler programs with fewer rules and predicates will execute faster in ASP solvers.
- Evidence anchors:
  - [section] shows that learned encodings are "much simpler and more compact" and demonstrates better time performance for larger frameworks (500-700 arguments)
  - [section] notes that ASPARTIX encodings for preferred and grounded semantics "are complex and involve many auxiliary predicates"
- Break condition: If the ASP solver's optimization strategies change, or if the complexity of argumentation frameworks increases significantly, the performance advantage may diminish.

### Mechanism 3
- Claim: LASarg framework requires significantly fewer examples to achieve perfect accuracy compared to Deep Learning approaches.
- Mechanism: ILP-based learning from answer sets can generalize from a small set of examples by capturing the underlying logical structure, while Deep Learning requires large datasets to approximate the same logic.
- Core assumption: The logical structure of argumentation semantics can be captured with a small, well-chosen set of examples.
- Evidence anchors:
  - [section] reports that LASarg achieves "100% accuracy and an MCC metric value of 1" with only 7-27 examples, depending on the semantics
  - [section] contrasts this with Deep Learning, which "requires significantly fewer examples" and only achieves MCC=1 for grounded semantics even with 30 examples
- Break condition: If the space of argumentation frameworks becomes too diverse or if the semantics become too complex, the small example set may no longer be sufficient for perfect generalization.

## Foundational Learning

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP is the formalism used to represent and reason about argumentation semantics, both in manually engineered encodings (ASPARTIX) and learned programs.
  - Quick check question: What are the three main types of rules in ASP, and how do they differ in their function?
- Concept: Inductive Logic Programming (ILP)
  - Why needed here: ILP is the machine learning paradigm used by LASarg to learn answer set programs from examples, extending it with the ability to learn from answer sets specifically.
  - Quick check question: How does ILP differ from other machine learning approaches in terms of the type of knowledge it learns and the interpretability of that knowledge?
- Concept: Argumentation Frameworks and Semantics
  - Why needed here: Understanding the different types of argumentation frameworks (AAF, BAF, VAF, ABA) and their semantics (stable, complete, admissible, grounded, preferred) is crucial for constructing the right examples and background knowledge for learning.
  - Quick check question: What is the key difference between abstract argumentation frameworks (AAF) and structured argumentation frameworks (ABA) in terms of how arguments and attacks are represented?

## Architecture Onboarding

- Component map:
  - LASarg framework: Main system for learning argumentation semantics
    - ILASP learner: Core ILP component that learns answer set programs
    - Example generator: Creates positive and negative examples of argumentation frameworks
    - Background knowledge module: Provides definitions for special constructs like defeated, not defended, and supported arguments
    - Mode declarations: Define the hypothesis space for the learner
  - Solvers for comparison: ASPARTIX, µ-toksia, and Deep Learning approaches
- Critical path: Generate examples → Provide background knowledge and mode declarations → Run ILASP learner → Obtain learned answer set program → Use learned program to compute extensions
- Design tradeoffs: The use of ILP allows for interpretable, compact programs but may require careful construction of examples and background knowledge. Deep Learning approaches may require more data but can potentially capture more complex patterns.
- Failure signatures: Poor performance or incorrect results may indicate issues with the example set, background knowledge, or mode declarations. Comparing the learned program's output with known correct encodings (like ASPARTIX) can help identify specific failure modes.
- First 3 experiments:
  1. Generate a small set of examples (5-10 arguments) for a specific semantics (e.g., stable) and run the ILASP learner to see if it produces a program equivalent to the ASPARTIX encoding.
  2. Test the learned program on a larger, randomly generated argumentation framework (50-100 arguments) to compare computational performance with ASPARTIX.
  3. Gradually increase the complexity of the examples (e.g., adding support relations for BAFs) and observe how the learner adapts the learned program.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the LASarg framework be extended to learn domain-specific custom acceptability semantics beyond the five standard semantics (stable, complete, admissible, grounded, and preferred)?
- Basis in paper: [explicit] The paper mentions that future work will explore learning domain-specific custom semantics, recognizing that individual reasoning about the acceptability of arguments may vary widely in real-world contexts.
- Why unresolved: The current framework is designed to learn the standard semantics of argumentation frameworks. Extending it to learn custom semantics would require adapting the learning process to accommodate different criteria for argument acceptance, which is not yet addressed.
- What evidence would resolve it: Experimental results showing successful learning of custom acceptability semantics for various real-world domains, demonstrating the framework's adaptability and effectiveness in handling diverse argumentation scenarios.

### Open Question 2
- Question: How can the LASarg framework be integrated with other Machine Learning or Natural Language Processing tools to extract arguments from real-world dialogues and determine accepted arguments?
- Basis in paper: [explicit] The paper suggests integrating the learned encodings with other Machine Learning or NLP tools dedicated to argument extraction from dialogues for practical applications.
- Why unresolved: While the paper proposes the integration of LASarg with other tools, it does not provide details on how this integration would be implemented or evaluated in practice.
- What evidence would resolve it: A case study or experimental results demonstrating the successful integration of LASarg with argument extraction tools, showing improved accuracy and efficiency in identifying accepted arguments from real-world dialogues.

### Open Question 3
- Question: Can the LASarg framework be adapted to handle argumentation frameworks with more complex structures, such as those involving cyclic dependencies or nested arguments?
- Basis in paper: [inferred] The paper focuses on learning acceptability semantics for several argumentation frameworks, but does not explicitly address more complex structures like cyclic dependencies or nested arguments.
- Why unresolved: The current framework may not be equipped to handle the intricacies of more complex argumentation structures, which could limit its applicability in certain domains.
- What evidence would resolve it: Experimental results showing the framework's ability to learn and reason about acceptability semantics in argumentation frameworks with complex structures, such as those involving cycles or nested arguments, would demonstrate its robustness and versatility.

## Limitations

- Example representativeness: The framework's success depends heavily on the quality and coverage of the example set. Edge cases or rare argumentation scenarios may not be adequately captured with the small example sizes used (7-27 examples).
- Background knowledge dependency: The learning process relies on pre-defined background knowledge, which may not cover all possible argumentation scenarios or may introduce biases that affect the learned programs.
- Scalability concerns: While the framework shows better performance for larger argumentation frameworks (500-700 arguments), its scalability to extremely large or complex frameworks remains untested.

## Confidence

- **High confidence**: The equivalence between learned programs and ASPARTIX encodings for stable, complete, and admissible semantics (Theorem 1).
- **Medium confidence**: The computational performance advantage of learned encodings over ASPARTIX and µ-toksia for larger argumentation frameworks.
- **Medium confidence**: The claim that LASarg requires significantly fewer examples than Deep Learning approaches to achieve perfect accuracy.

## Next Checks

1. **Edge case analysis**: Systematically test the learned programs on edge cases and rare argumentation scenarios not covered by the original example set to assess generalization capabilities.
2. **Background knowledge ablation study**: Remove or modify parts of the background knowledge and observe how it affects the learned programs' correctness and performance.
3. **Extreme scale testing**: Evaluate the framework's performance and correctness on argumentation frameworks with thousands of arguments to validate scalability claims.