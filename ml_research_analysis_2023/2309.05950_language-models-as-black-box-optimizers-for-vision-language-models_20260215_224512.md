---
ver: rpa2
title: Language Models as Black-Box Optimizers for Vision-Language Models
arxiv_id: '2309.05950'
source_url: https://arxiv.org/abs/2309.05950
tags:
- prompts
- arxiv
- prompt
- language
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a black-box optimization framework that uses
  conversational LLMs to automatically engineer text prompts for vision-language models
  (VLMs). It formulates prompt optimization as a hill-climbing search process, where
  ChatGPT is provided with both high-performing and low-performing prompt examples
  as feedback, enabling the LLM to implicitly learn a "gradient" direction for more
  efficient search.
---

# Language Models as Black-Box Optimizers for Vision-Language Models

## Quick Facts
- arXiv ID: 2309.05950
- Source URL: https://arxiv.org/abs/2309.05950
- Reference count: 40
- 61.1% average accuracy on 1-shot image classification with CLIP across 11 datasets

## Executive Summary
This work presents a novel black-box optimization framework that uses conversational LLMs to automatically engineer text prompts for vision-language models (VLMs). The approach formulates prompt optimization as a hill-climbing search process, where ChatGPT is provided with both high-performing and low-performing prompt examples as feedback, enabling the LLM to implicitly learn a "gradient" direction for more efficient search. The method achieves state-of-the-art performance on few-shot image classification while discovering interpretable natural language prompts that transfer better across different VLM architectures compared to continuous prompts.

## Method Summary
The method uses ChatGPT to optimize text prompts for vision-language models through a conversational hill-climbing framework. It starts with an initial pool of 2M templates sampled from LAION-COCO captions, then iteratively evaluates prompts using the black-box VLM scoring function, sorts them by performance, and provides both top and bottom performing examples as feedback to ChatGPT. The LLM generates new candidate prompts based on this feedback, with the process repeating for multiple iterations and restarts. The approach is evaluated on 1-shot image classification across 11 datasets using CLIP models.

## Key Results
- Achieves 61.1% average accuracy on 1-shot image classification across 11 datasets, surpassing white-box CoOp by 1.5% and OpenAI's human-engineered prompts
- Discovered natural language prompts are more interpretable than continuous prompts and transfer better across different CLIP architectures
- The method is more efficient than black-box alternatives like APE, requiring fewer API calls to reach optimal performance

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT learns implicit gradient direction from textual feedback combining positive and negative prompts. When provided with both high-performing and low-performing prompt examples, ChatGPT implicitly learns the "gradient" direction that improves prompt quality by understanding patterns that distinguish good from bad prompts. This meta-optimization through in-context learning enables efficient search without explicit numerical gradients.

### Mechanism 2
Natural language prompts discovered through the method generalize better across different CLIP architectures than continuous prompts. Natural language prompts capture domain-specific and stylistic attributes that transfer across architectures, while continuous prompts overfit to specific model parameters and embeddings. The search space of natural language provides better regularization than the continuous text-embedding space.

### Mechanism 3
Conversational feedback with iterative prompt updating leads to more efficient optimization than single-direction prompting. By iteratively replacing positive and negative prompts with the most recent candidates generated by ChatGPT, the feedback loop becomes more informative and focused, leading to faster convergence. The iterative refinement process mimics human communication patterns for improved learning.

## Foundational Learning

- Concept: In-context learning and meta-optimization
  - Why needed here: The method relies on ChatGPT's ability to learn optimization patterns from examples without gradient-based training
  - Quick check question: How does ChatGPT perform optimization without access to model parameters or gradients?

- Concept: Hill-climbing optimization with random restarts
  - Why needed here: The method uses a gradient-free optimization framework to search for optimal prompts
  - Quick check question: What is the trade-off between exploration (random restarts) and exploitation (iterative refinement) in this context?

- Concept: Prompt engineering and template-based text generation
  - Why needed here: The method searches for effective class-agnostic templates for vision-language models
  - Quick check question: How do natural language templates differ from continuous prompts in terms of search space and interpretability?

## Architecture Onboarding

- Component map: LAION-COCO caption pool -> Initial prompt sampling -> VLM scoring function -> Performance sorting -> ChatGPT feedback -> New prompt generation -> Iterative optimization

- Critical path: 1) Sample initial prompts from text corpus 2) Evaluate prompts using black-box scoring function 3) Sort prompts by performance and select top/bottom k 4) Provide feedback to ChatGPT with both positive and negative examples 5) Generate new prompts based on feedback 6) Iterate until convergence or budget exhaustion

- Design tradeoffs:
  - Exploration vs. exploitation: More resets encourage exploration but fewer iterations per reset reduce exploitation
  - Prompt diversity vs. specificity: Broader initial prompts may discover more generalizable solutions but risk missing optimal local solutions
  - Cost vs. performance: More API calls improve accuracy but increase computational expense

- Failure signatures:
  - Plateaued accuracy across iterations indicates local optima or ineffective feedback
  - High variance in prompt performance suggests poor pattern learning by ChatGPT
  - Rapid degradation in transfer performance indicates overfitting to specific architecture

- First 3 experiments:
  1. Baseline comparison: Run the method with only positive prompts (APE-style) to verify the importance of negative feedback
  2. Transfer test: Evaluate discovered prompts on different CLIP architectures to verify generalization claims
  3. Shot scaling: Test the method on 4-shot and 16-shot scenarios to understand performance scaling with more data

## Open Questions the Paper Calls Out

### Open Question 1
How does the efficiency of conversational prompting compare to other black-box optimization methods when scaling to larger and more complex vision-language models? The paper mentions that their approach is more efficient than APE and other black-box methods, but does not provide a comprehensive comparison across different model scales.

### Open Question 2
What is the impact of the choice of initial prompts on the final performance of the conversational prompting method? The paper mentions sampling initial prompts from a text corpus but does not deeply analyze how different initial prompts affect the final outcomes.

### Open Question 3
How do the discovered prompts perform on tasks beyond image classification, such as text-to-image generation or image retrieval? The paper briefly mentions applying the framework to optimize DALL-E 3 for text-to-image generation but does not provide detailed results.

## Limitations
- Performance claims rely heavily on specific implementation details of ChatGPT interaction and quality of initial prompt corpus
- Evaluation focuses on 1-shot learning across 11 datasets, with untested effectiveness at higher shot counts
- Method's effectiveness on different vision-language model architectures beyond CLIP variants remains untested

## Confidence
**High Confidence**: The core claim that conversational feedback with both positive and negative examples improves prompt optimization efficiency is well-supported by experimental results (61.1% accuracy across 11 datasets).

**Medium Confidence**: The claim about natural language prompts transferring better across CLIP architectures than continuous prompts is supported but limited to a few architectural variants.

**Low Confidence**: The specific mechanism by which ChatGPT learns "implicit gradient direction" from textual feedback remains somewhat speculative.

## Next Checks
1. Evaluate discovered prompts on a broader range of vision-language models beyond CLIP variants, including non-OpenAI architectures like BLIP or Flamingo
2. Systematically remove each component of the method (negative feedback, iterative updates, template format) to quantify individual contributions
3. Test the method at 4-shot, 16-shot, and 64-shot learning scenarios to understand performance scaling with more training data