---
ver: rpa2
title: An Unified Search and Recommendation Foundation Model for Cold-Start Scenario
arxiv_id: '2309.08939'
source_url: https://arxiv.org/abs/2309.08939
tags:
- search
- recommendation
- query
- foundation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of jointly modeling search and
  recommendation tasks across multiple domains, which is challenging due to data imbalance,
  heterogeneous item sets, and negative transfer issues. The authors propose the S&R
  Multi-Domain Foundation model, which uses large language models (LLMs) to extract
  domain-invariant text features and an Aspect Gating Fusion module to merge ID, text,
  and sparse features.
---

# An Unified Search and Recommendation Foundation Model for Cold-Start Scenario

## Quick Facts
- arXiv ID: 2309.08939
- Source URL: https://arxiv.org/abs/2309.08939
- Authors: 
- Reference count: 29
- Key outcome: Proposed S&R Multi-Domain Foundation model achieves up to 0.0459 absolute gain in AUC and 17.54% relative improvement in PVCTR for cold-start recommendation, successfully deployed in Alipay's mobile application.

## Executive Summary
This paper addresses the challenge of jointly modeling search and recommendation tasks across multiple domains with data imbalance, heterogeneous items, and negative transfer issues. The authors propose an S&R Multi-Domain Foundation model that leverages large language models to extract domain-invariant text features, uses an Aspect Gating Fusion module to merge ID, text, and sparse features, and employs Domain Adaptive Multi-Task learning to handle domain shift problems. The model is pretrained on multiple search and recommendation datasets and fine-tuned for cold-start scenarios, demonstrating superior performance compared to state-of-the-art multi-domain models.

## Method Summary
The S&R Multi-Domain Foundation model uses LLMs to extract domain-invariant text features from user queries and items, then combines these with ID and sparse features through an Aspect Gating Fusion module that dynamically weights different feature aspects based on domain context. A Domain Adaptive Multi-Task module applies domain alignment using Jensen-Shannon Divergence to reduce distribution divergence across domains, implemented through a Domain Adaptive Layer that maps features to a common vector space. The model is pretrained on seven Alipay search and recommendation datasets, then fine-tuned for cold-start scenarios by freezing certain layers and training on limited data for new items.

## Key Results
- Achieves up to 0.0459 absolute gain in AUC over state-of-the-art multi-domain models
- Demonstrates 17.54% relative improvement in PVCTR for cold-start recommendation scenarios
- Successfully deployed in Alipay's mobile application with proven online performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-invariant text features from LLMs help mitigate negative transfer and item heterogeneity in multi-domain settings.
- Mechanism: Large Language Models extract low-level text representations that are shared across domains, providing consistent semantic understanding of queries and items regardless of domain differences.
- Core assumption: Text-based features contain sufficient semantic information to bridge domain gaps and reduce the need for domain-specific feature engineering.
- Evidence anchors:
  - [abstract]: "With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks."
  - [section]: "We apply LLMs in S&R Multi-Domain Foundation model, and extract domain invariant text features to help mitigate the negative transferring and item heterogeneous issues in the multi-domain settings."
  - [corpus]: Weak evidence - no directly comparable papers in the corpus, but related concepts exist in RecGPT and One Model for All.

### Mechanism 2
- Claim: Domain Adaptive Multi-Task Learning with domain alignment reduces marginal and conditional distribution divergence across domains.
- Mechanism: A Domain Adaptive Layer maps input features from multiple domains into a common vector space using domain embeddings, and applies Jensen-Shannon Divergence to constrain the distributions.
- Core assumption: Minimizing the divergence between domain distributions leads to better shared representation learning and reduces negative transfer.
- Evidence anchors:
  - [section]: "We propose to add a Domain Adaptive Layer to the input features xáµ¢, which maps the inputs from multiple domains to a common vector space... Láµ£â‚‘g = Ãáµ¢,â±¼ âˆˆ {1,2,...,K} d(p(xÌ‚áµ¢)||p(xÌ‚â±¼))."
  - [section]: "And we find the Jensen-Shannon Divergence achieves the best performance as Láµ£â‚‘g = âˆ‘áµ¢,â±¼ âˆˆ {1,2,...,K} JS(p(xÌ‚áµ¢)||p(xÌ‚â±¼))."
  - [corpus]: Weak evidence - no direct counterpart in corpus, but related concepts in Curriculum-scheduled Knowledge Distillation and Gaussian Mixture Flow Matching.

### Mechanism 3
- Claim: Aspect Gating Fusion dynamically balances the importance of ID, text, and sparse features based on domain context.
- Mechanism: A gating network computes weights for different feature aspects (ID, text, sparse) using domain embeddings, allowing the model to emphasize text features in cold-start scenarios where ID features are sparse.
- Core assumption: Different feature types have varying importance depending on the domain and availability of training data, and this importance can be learned dynamically.
- Evidence anchors:
  - [section]: "The Domain-Gating Strategy... To model the differences across domains, we randomly initialize the domain embedding ð¸ð· = [ð¸ð·â‚, ð¸ð·â‚‚, ..., ð¸ð·â‚–] âˆˆ Rá´· Ã—á´´ as the representations of different domains. And the domain-specific gating is calculated as wáµƒ = eá´±á´°áµá´±áµƒ / âˆ‘â‚âˆˆ|A|eá´±á´°áµá´±áµƒ."
  - [section]: "Very few training samples contain ID features of cold start items and can't represent them well, and generic text features play more important role."
  - [corpus]: Weak evidence - no directly comparable papers, but related concepts in Multi-Aspect Dense Retrieval.

## Foundational Learning

- Concept: Multi-task learning with domain adaptation
  - Why needed here: The model needs to jointly learn search and recommendation tasks across multiple domains while handling data imbalance and distribution shift.
  - Quick check question: What is the difference between multi-task learning and multi-domain learning, and how does domain adaptation help in this context?

- Concept: Large Language Model feature extraction
  - Why needed here: LLMs provide a way to extract semantic representations of text that are consistent across domains, reducing the need for domain-specific feature engineering.
  - Quick check question: How do LLMs extract domain-invariant features, and why are these features useful for cold-start scenarios?

- Concept: Gating mechanisms in neural networks
  - Why needed here: Gating allows the model to dynamically adjust the importance of different feature types based on domain context, which is crucial for handling heterogeneous data.
  - Quick check question: What is the purpose of the gating mechanism in Aspect Gating Fusion, and how does it differ from simple feature concatenation?

## Architecture Onboarding

- Component map: User-Query-Item Encoding -> Aspect Gating Fusion -> Domain Adaptive Multi-Task -> Loss Calculation -> Parameter Updates
- Critical path: User-Query-Item Encoding â†’ Aspect Gating Fusion â†’ Domain Adaptive Multi-Task â†’ Loss Calculation â†’ Parameter Updates
- Design tradeoffs:
  - Using LLMs for feature extraction adds computational overhead but provides domain-invariant representations
  - Domain adaptive regularization helps reduce negative transfer but may introduce additional complexity
  - Freezing LLM parameters during fine-tuning can improve stability but may limit adaptation to specific domains
- Failure signatures:
  - Poor performance on cold-start scenarios: May indicate insufficient domain-invariant text features or ineffective fine-tuning
  - High variance across domains: May indicate negative transfer or ineffective domain adaptation
  - Overfitting on specific domains: May indicate insufficient regularization or imbalanced training data
- First 3 experiments:
  1. Evaluate the impact of domain-invariant text features by comparing performance with and without LLM feature extraction
  2. Test different domain adaptation techniques (e.g., MMD vs. JS-Divergence) to determine the most effective approach
  3. Assess the effectiveness of Aspect Gating Fusion by comparing performance with different gating strategies (e.g., Mean-Pooling vs. Domain-Gating)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM (e.g., BERT vs. GPT vs. ChatGLM) affect the quality of domain-invariant text features and downstream performance?
- Basis in paper: [explicit] The authors compare different LLMs (BERT, ChatGLM-6B, ChatGLM2-6B) for encoding text features in experiments, noting performance differences.
- Why unresolved: The paper provides comparative results but does not deeply analyze why certain LLMs perform better or under what conditions specific models excel.
- What evidence would resolve it: Systematic ablation studies varying LLM architectures and training objectives, with detailed analysis of feature quality metrics and task-specific performance across diverse domains.

### Open Question 2
- Question: What is the optimal strategy for freeze-finetune split in the pretrain-finetune framework for cold-start scenarios?
- Basis in paper: [explicit] The authors test different freeze-finetune splits (L0-L1 vs L1-L2) and report performance differences, but do not provide a definitive recommendation.
- Why unresolved: The paper shows that freezing levels L0 and L1 performs best in their experiments, but this may not generalize across all cold-start scenarios or domain combinations.
- What evidence would resolve it: Comprehensive experiments across diverse cold-start scenarios varying in data availability, domain similarity, and task types to identify robust freeze-finetune strategies.

### Open Question 3
- Question: How do different domain divergence metrics (e.g., Jensen-Shannon vs. Maximum Mean Discrepancy) impact the effectiveness of the Domain Adaptive Multi-Task module?
- Basis in paper: [explicit] The authors compare JS-Divergence and MMD for domain adaptation and report better performance with JS-Divergence, but do not analyze why.
- Why unresolved: The paper demonstrates empirical differences but lacks theoretical analysis of when and why certain divergence metrics are more effective for specific types of domain shifts.
- What evidence would resolve it: Theoretical analysis connecting divergence metric properties to domain shift characteristics, combined with empirical validation across varied domain distributions.

### Open Question 4
- Question: How does the Aspect Gating Fusion strategy (Mean-Pooling vs. [CLS]-Gating vs. Domain-Gating) affect model performance across different feature distributions?
- Basis in paper: [explicit] The authors compare three gating strategies and find Domain-Gating performs best, but do not explore why this occurs or under what conditions other strategies might be preferable.
- Why unresolved: The paper provides empirical comparison without investigating the underlying mechanisms that make certain gating strategies more effective for specific feature characteristics.
- What evidence would resolve it: Detailed analysis of feature distribution properties (e.g., variance, correlation) and their relationship to gating strategy effectiveness, potentially leading to adaptive gating selection.

## Limitations
- Domain-invariant feature quality: Reliance on generic text features assumes sufficient semantic overlap across domains, which may not hold for highly specialized domains
- Generalization across domains: Performance is evaluated in a specific context (Alipay's mobile application) with unclear generalizability to other multi-domain settings
- Computational overhead: Use of LLMs for feature extraction adds computational overhead without detailed analysis of trade-offs

## Confidence

- High confidence: The overall approach of using domain-invariant text features and domain adaptive multi-task learning is well-grounded in existing literature
- Medium confidence: The specific implementation details of the Domain Adaptive Multi-Task module and the effectiveness of the Aspect Gating Fusion strategy are not fully explored
- Low confidence: The paper's claims about the superiority of the Jensen-Shannon Divergence for domain alignment are not thoroughly validated against other divergence metrics

## Next Checks

1. Conduct an ablation study to quantify the contribution of LLM-extracted text features versus domain-specific features in mitigating negative transfer and improving cold-start performance

2. Evaluate the model's performance on a different multi-domain search and recommendation dataset to assess its generalization capabilities and robustness to varying data distributions

3. Analyze the computational overhead introduced by LLM feature extraction and domain adaptive regularization, comparing the trade-off between performance gains and computational costs with alternative approaches