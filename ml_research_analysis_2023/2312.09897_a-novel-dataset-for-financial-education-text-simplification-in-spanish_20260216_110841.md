---
ver: rpa2
title: A Novel Dataset for Financial Education Text Simplification in Spanish
arxiv_id: '2312.09897'
source_url: https://arxiv.org/abs/2312.09897
tags:
- text
- simplification
- dataset
- gpt-3
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents FEINA, a manually curated Spanish financial
  text simplification dataset consisting of 5,314 complex-simple sentence pairs. The
  dataset was created by expert philology students following explicit simplification
  rules designed to enhance accessibility for visually impaired readers using screen
  readers.
---

# A Novel Dataset for Financial Education Text Simplification in Spanish

## Quick Facts
- arXiv ID: 2312.09897
- Source URL: https://arxiv.org/abs/2312.09897
- Reference count: 38
- Key outcome: FEINA dataset with 5,314 manual complex-simple sentence pairs outperforms automatic simplifications on lexical simplicity and semantic preservation metrics

## Executive Summary
This paper introduces FEINA, a manually curated Spanish financial text simplification dataset designed to enhance accessibility for visually impaired readers using screen readers. The dataset was created by expert philology students following explicit simplification rules and consists of 5,314 complex-simple sentence pairs extracted from Spanish financial education books. The authors benchmark FEINA against automatic simplifications generated by GPT-3, mT5, and Tuner, demonstrating that manual simplifications achieve superior results in lexical simplicity, semantic preservation, and human evaluation, while GPT-3 provides comparable quality with significantly reduced effort.

## Method Summary
The FEINA dataset was created through a multi-step process involving rule definition, text extraction from four Spanish financial education books, and manual simplification by six advanced philology students following 21 explicit guidelines. Automatic simplifications were generated using GPT-3, mT5, and Tuner models. The evaluation employed multiple metrics including reference-less indices (Fernandez-Huerta, Szigriszt-Pazos, Gutierrez-Polini), semantic similarity using BERT embeddings, reference-based metrics (SARI, BLEU), and human evaluation by two visually impaired experts using Likert scales. The dataset's ability to discriminate between complex and simple text segments was also assessed through classifier performance.

## Key Results
- Manual simplifications achieved higher lexical simplicity scores than all automatic methods across multiple established indices
- GPT-3-generated simplifications matched manual quality in semantic preservation and human evaluation while requiring significantly less effort
- The FEINA dataset demonstrated superior discrimination ability between complex and simple text segments compared to automatically generated datasets
- Human evaluation confirmed that manual and GPT-3 simplifications were more suitable for visually impaired users than mT5 and Tuner outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual simplification by philology students produces higher lexical simplicity scores than automatic methods.
- Mechanism: Students apply explicit simplification rules targeting low-frequency words, nominalizations, and complex structures, reducing syllable counts and word lengths.
- Core assumption: Students consistently follow the 21 simplification guidelines and identify all target complexity attributes.
- Evidence anchors:
  - [abstract] states manual simplifications outperform GPT-3, mT5, and Tuner across multiple metrics.
  - [section 2.1] details the creation of simplification rules and manual simplification by advanced philology students.
- Break condition: If students deviate from guidelines or misidentify complexity attributes, lexical simplicity gains may be inconsistent.

### Mechanism 2
- Claim: GPT-3 generates simplifications comparable to manual ones in terms of semantic preservation and human evaluation.
- Mechanism: GPT-3 leverages its large language model training to produce fluent simplifications that maintain meaning while reducing complexity.
- Core assumption: GPT-3's training data and architecture enable it to understand and simplify Spanish financial texts without extensive domain adaptation.
- Evidence anchors:
  - [abstract] indicates GPT-3 achieves comparable quality to manual simplifications with significantly less effort.
  - [section 2.2] describes GPT-3's performance in generating simplifications that preserve meaning.
- Break condition: If GPT-3's training data lacks sufficient Spanish financial domain examples, simplifications may lose domain-specific accuracy.

### Mechanism 3
- Claim: The dataset enables discrimination between complex and simple text segments better than automatically generated datasets.
- Mechanism: Manual simplifications apply consistent transformations, creating clear differences from source texts that classifiers can detect.
- Core assumption: The simplification rules create measurable changes in lexical and structural features that remain consistent across the dataset.
- Evidence anchors:
  - [section 3.4] shows manual dataset achieves higher accuracy and F1 scores in text segment classification than other datasets.
  - [section 2.3] explains the evaluation metrics used to assess dataset quality, including discrimination measures.
- Break condition: If simplification rules are inconsistently applied, classifier performance may degrade.

## Foundational Learning

- Concept: Text simplification metrics (Fernandez-Huerta, Szigriszt-Pazos, Gutierrez-Polini indices)
  - Why needed here: These metrics quantify lexical simplicity improvements, allowing comparison of manual vs. automatic simplifications.
  - Quick check question: How do syllable count and word length reductions affect the Fernandez-Huerta index value?

- Concept: Semantic preservation evaluation using sentence embeddings
  - Why needed here: Ensures simplifications maintain original meaning while reducing complexity, critical for accessibility.
  - Quick check question: What cosine distance threshold between BERT embeddings indicates acceptable semantic preservation?

- Concept: Human evaluation protocols for accessibility assessment
  - Why needed here: Visually impaired users are the target audience, requiring subjective evaluation of simplification effectiveness.
  - Quick check question: How does the Likert scale response distribution indicate whether simplifications meet accessibility needs?

## Architecture Onboarding

- Component map: Dataset creation (rule definition → text extraction → manual simplification) → Automatic simplification (GPT-3, mT5, Tuner) → Evaluation (lexical metrics, semantic preservation, human judgment)
- Critical path: Rule creation → Manual simplification → Dataset validation → Automatic model testing → Evaluation and comparison
- Design tradeoffs: Manual vs. automatic simplification speed vs. quality, domain specificity vs. generalization, accessibility metrics vs. traditional NLP metrics
- Failure signatures: Inconsistent rule application, semantic drift in automatic simplifications, classifier inability to distinguish complex/simple pairs
- First 3 experiments:
  1. Validate simplification rules by having multiple students simplify the same text segments and measure inter-rater agreement
  2. Compare GPT-3 simplifications against manual ones using lexical simplicity and semantic preservation metrics
  3. Train and evaluate text segment classifiers on each dataset to measure discrimination ability between complex and simple versions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of GPT-3's performance make it comparable to manual simplifications for visually impaired users?
- Basis in paper: [explicit] The authors note that GPT-3's results are close in quality to manual simplifications based on human evaluation metrics, particularly in understandability and suitability for visually impaired users.
- Why unresolved: The paper provides general comparisons but doesn't delve into specific linguistic features or mechanisms that make GPT-3 effective.
- What evidence would resolve it: A detailed linguistic analysis of GPT-3's simplifications versus manual ones, focusing on specific attributes like sentence structure, word choice, and readability metrics tailored to visually impaired users.

### Open Question 2
- Question: How can text simplification evaluation metrics be improved to better assess effectiveness for specific user groups like visually impaired individuals?
- Basis in paper: [explicit] The authors question the suitability of current metrics, particularly regarding meaning preservation and lexical simplification for visually impaired users.
- Why unresolved: Existing metrics may not capture the unique needs of visually impaired users who rely on screen readers.
- What evidence would resolve it: Development and validation of new evaluation metrics specifically designed for visually impaired users, incorporating feedback from this population on what aspects of text simplification are most beneficial.

### Open Question 3
- Question: What is the potential for using large language models like GPT-3 to augment text simplification datasets in languages other than Spanish?
- Basis in paper: [inferred] The paper discusses the scarcity of high-quality simplification datasets in Spanish and suggests GPT-3's potential for data augmentation, implying this could be relevant for other languages.
- Why unresolved: The study focuses on Spanish, so the generalizability to other languages is not explored.
- What evidence would resolve it: Comparative studies applying similar methodologies to other languages with limited simplification datasets, evaluating the effectiveness of large language models in augmenting these datasets.

## Limitations
- The manual simplification process involved only 6 philology students, which may limit diversity and introduce systematic biases
- Human evaluation was conducted by only two visually impaired experts, raising questions about robustness across the target user population
- The comparison between manual and automatic simplifications may be affected by different evaluation contexts and lack of domain-specific fine-tuning for automatic methods

## Confidence

**High Confidence**: The claim that manual simplifications outperform automatic methods on lexical simplicity metrics is well-supported by the quantitative results and established evaluation metrics.

**Medium Confidence**: The assertion that GPT-3 achieves comparable quality to manual simplifications with significantly less effort is supported by the results but requires careful interpretation given the different evaluation contexts.

**Low Confidence**: The claim about the dataset's ability to discriminate between complex and simple text segments is based on classifier performance, but the paper does not provide sufficient detail about classifier architecture and training procedures.

## Next Checks

1. **Inter-rater reliability assessment**: Replicate the manual simplification process with a larger group of philology students (at least 10) simplifying the same subset of 100 sentence pairs to measure inter-rater agreement using Cohen's kappa or similar statistics.

2. **Expanded human evaluation**: Conduct human evaluation with a diverse group of 10-15 visually impaired users across different age ranges and levels of visual impairment to validate the accessibility claims and assess whether the two-expert evaluation adequately represents the target user population.

3. **Domain-specific fine-tuning**: Fine-tune mT5 and Tuner on the FEINA dataset itself, then compare their performance against GPT-3 and manual simplifications using the same evaluation metrics to determine whether the performance gap is due to model architecture limitations or insufficient domain adaptation.