---
ver: rpa2
title: Multi-task convolutional neural network for image aesthetic assessment
arxiv_id: '2305.09373'
source_url: https://arxiv.org/abs/2305.09373
tags:
- aesthetic
- multi-task
- scores
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-task convolutional neural network for
  image aesthetic assessment, aiming to predict overall aesthetic scores and attribute
  scores simultaneously. The proposed approach leverages a VGG16-based architecture
  to learn shared representations across multiple aesthetic attributes.
---

# Multi-task convolutional neural network for image aesthetic assessment

## Quick Facts
- arXiv ID: 2305.09373
- Source URL: https://arxiv.org/abs/2305.09373
- Reference count: 40
- Primary result: VGG16-based multi-task CNN achieves state-of-the-art performance on AADB dataset with Spearman's rank correlation of 0.7067 for overall aesthetic score prediction.

## Executive Summary
This study presents a multi-task convolutional neural network for image aesthetic assessment that predicts overall aesthetic scores and attribute scores simultaneously. The approach leverages a VGG16-based architecture to learn shared representations across multiple aesthetic attributes, achieving state-of-the-art performance on the AADB dataset while requiring fewer parameters than previous methods. The model also pioneers multi-task learning on the EVA dataset, achieving a Spearman's rank correlation of 0.695. Results demonstrate that the multi-task setting consistently outperforms single-task approaches, with the model showing near-human performance on overall aesthetic score prediction.

## Method Summary
The proposed method uses a VGG16-based architecture with global average pooling, two fully-connected layers (128 and 64 units with ReLU), dropout (0.35), and sigmoid output layer. The model is trained in two stages: first freeze all VGG16 layers for 5 epochs with Adam optimizer (lr=0.001), then fine-tune the last two convolutional layers in block 4 for 3 epochs with Adam (lr=0.0001, decay=0.50 every 125 steps). The approach predicts both overall aesthetic scores and attribute scores simultaneously using mean squared error loss, with outputs constrained to [0,1] via sigmoid activation.

## Key Results
- Achieves state-of-the-art Spearman's rank correlation of 0.7067 on AADB dataset for overall aesthetic score prediction
- Demonstrates multi-task learning consistently outperforms single-task approaches across both AADB and EVA datasets
- Achieves Spearman's rank correlation of 0.695 on EVA dataset, pioneering multi-task learning on this benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves overall aesthetic score prediction through shared feature representations.
- Mechanism: By jointly training on overall aesthetic scores and attribute scores, the model learns a shared feature representation that captures both global and local aesthetic patterns, leading to better generalization than single-task models.
- Core assumption: Aesthetic attributes are correlated with overall aesthetic quality and can provide useful inductive bias for the overall score prediction task.
- Evidence anchors:
  - [abstract] "This multi-task learning framework allows for effective generalization through the utilization of shared representations."
  - [section] "Our evaluation shows that the multi-task setting consistently outperforms the single-task setting for the same neural network architecture across both datasets."
  - [corpus] Weak evidence - the corpus contains related papers but none directly confirm this mechanism experimentally.
- Break condition: If attribute scores are uncorrelated with overall aesthetic quality or contain significant noise, the shared representation may harm performance rather than help it.

### Mechanism 2
- Claim: Fine-tuning only the last two convolutional layers of VGG16 balances learning capacity and overfitting prevention.
- Mechanism: Freezing the initial layers preserves general visual features learned from ImageNet, while fine-tuning the last two layers adapts them to the specific aesthetic domain without overfitting to the small datasets.
- Core assumption: Early convolutional layers capture generic visual features that transfer well to aesthetic assessment, while later layers need task-specific adaptation.
- Evidence anchors:
  - [section] "In the first stage, we apply the Adam algorithm with an initial learning rate of 0.001... we freeze the weights for all five blocks... In the second stage, we fine-tune the multi-task CNN by unfreezing the last two convolutional layer in the fourth block of VGG16."
  - [section] "After fine-tuning our model, we observe an increase in the correlations for all attributes except symmetry."
  - [corpus] Weak evidence - the corpus contains related papers but none directly confirm this specific fine-tuning strategy experimentally.
- Break condition: If the aesthetic assessment task requires very different visual features from ImageNet classification, freezing early layers may limit performance.

### Mechanism 3
- Claim: Using sigmoid activation with mean squared error loss effectively handles the normalized aesthetic score regression task.
- Mechanism: The sigmoid activation constrains outputs to [0,1], matching the normalized ground truth scores, while MSE provides smooth gradients for optimization.
- Core assumption: Aesthetic scores follow a distribution that can be well-approximated by the sigmoid function's output range.
- Evidence anchors:
  - [section] "The output layer applies sigmoid activation function, and all the output units share the same hidden representation."
  - [section] "We use mean squared error as the loss function on the training set X to minimize the error between the predictions and the ground-truth values."
  - [corpus] Weak evidence - the corpus contains related papers but none directly confirm this specific activation and loss combination experimentally.
- Break condition: If the actual score distribution has heavy tails or multimodality outside [0,1], sigmoid may introduce systematic bias.

## Foundational Learning

- Concept: Transfer learning with pretrained CNN backbones
  - Why needed here: Both AADB and EVA datasets are relatively small (10,000 and 4,070 images respectively), making it difficult to train deep networks from scratch without overfitting
  - Quick check question: Why is VGG16 chosen over training a custom architecture from scratch for this task?

- Concept: Multi-task learning framework
  - Why needed here: The aesthetic assessment problem naturally decomposes into predicting overall quality and specific attributes, and learning these jointly can improve generalization
  - Quick check question: What would happen to the overall score prediction performance if we removed the attribute prediction branches?

- Concept: Spearman's rank correlation as evaluation metric
  - Why needed here: Aesthetic assessment is inherently ordinal - we care about relative ranking of images rather than absolute score accuracy
  - Quick check question: Why might Spearman's correlation be more appropriate than Pearson's correlation for evaluating aesthetic assessment models?

## Architecture Onboarding

- Component map: Input image -> VGG16 feature extraction (frozen first 3 blocks, fine-tuned last 2) -> Global Average Pooling -> 128-unit FC (ReLU) -> 64-unit FC with 0.35 dropout -> Output layer with sigmoid activation (12 units for AADB, 5 units for EVA)
- Critical path: Input image -> VGG16 feature extraction -> GAP -> FC layers -> Output predictions
- Design tradeoffs: VGG16 provides strong feature extraction but adds significant parameters; multi-task output layer increases efficiency but may create interference between tasks
- Failure signatures: Overfitting (validation loss increases while training loss decreases), poor attribute predictions indicating representation issues, low Spearman correlation despite low MSE
- First 3 experiments:
  1. Train single-task baseline (only overall score prediction) with frozen VGG16 to establish performance floor
  2. Train multi-task model with frozen VGG16 to verify multi-task benefits
  3. Fine-tune last two VGG16 blocks and compare to frozen baseline to measure transfer learning impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-task learning approach compare to single-task approaches when using other backbone architectures beyond VGG16?
- Basis in paper: [explicit] The authors compare their multi-task CNN to single-task approaches and find multi-task learning improves performance. They also mention they experimented with several candidate pre-trained CNNs to determine the optimal architecture.
- Why unresolved: The paper only reports results using VGG16 as the backbone architecture, leaving open whether these findings generalize to other architectures like ResNet or EfficientNet.
- What evidence would resolve it: Systematic experiments comparing multi-task vs single-task performance across different backbone architectures (VGG16, ResNet, EfficientNet) on both AADB and EVA datasets.

### Open Question 2
- Question: What is the impact of including motion blur, repetition, and symmetry attributes on model performance, given these attributes are predominantly rated as neutral in the AADB dataset?
- Basis in paper: [explicit] The authors include all 11 attributes from AADB in their model despite noting that motion blur, repetition, and symmetry are mostly rated neutral (Fig. 3), which contradicts previous studies that excluded these attributes.
- Why unresolved: The paper does not provide an ablation study removing these three attributes to quantify their contribution to overall performance.
- What evidence would resolve it: Performance comparison (Spearman's correlation) between models trained with all 11 attributes versus models trained with only the 8 non-neutral attributes on the AADB dataset.

### Open Question 3
- Question: How does the proposed multi-task CNN generalize to other aesthetic datasets beyond AADB and EVA, particularly those with different attribute structures or rating scales?
- Basis in paper: [explicit] The authors conduct cross-dataset evaluation between AADB and EVA but acknowledge limitations in generalization due to subjective nature of aesthetic preferences.
- Why unresolved: The cross-dataset evaluation is limited to only two datasets, and the authors note that generalization may be limited by differences in rating scales and attribute definitions.
- What evidence would resolve it: Cross-dataset performance evaluation on additional aesthetic datasets (e.g., AVA, Photo.net) with varying attribute structures and rating scales, along with analysis of domain adaptation techniques.

## Limitations
- Small dataset sizes (10,000 images for AADB, 4,070 for EVA) constrain model's ability to learn generalizable aesthetic representations
- Limited evaluation metrics (only Spearman's correlation) without complementary metrics for comprehensive performance assessment
- Multi-task framework assumes attribute scores are informative for overall score prediction without ablation studies quantifying individual attribute contributions

## Confidence
- High confidence in overall methodology and experimental setup (clearly described training procedures and evaluation metrics)
- Medium confidence in superiority of multi-task learning (supported by empirical results but lacking theoretical justification or extensive ablation studies)
- Medium confidence in VGG16 fine-tuning strategy (demonstrated improvement but not compared against other architectures or fine-tuning approaches)
- Low confidence in generalization to other datasets (only tested on two specific aesthetic datasets without cross-dataset validation)

## Next Checks
1. Perform ablation studies removing individual aesthetic attribute prediction branches to quantify their specific contribution to overall score prediction performance
2. Test the model on a third, independent aesthetic dataset not used in training to evaluate generalization capability
3. Compare the multi-task approach against ensemble methods where separate models predict overall and attribute scores independently, to isolate the benefit of shared representations versus model capacity