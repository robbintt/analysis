---
ver: rpa2
title: 'Adapting Machine Learning Diagnostic Models to New Populations Using a Small
  Amount of Data: Results from Clinical Neuroscience'
arxiv_id: '2308.03175'
source_url: https://arxiv.org/abs/2308.03175
tags:
- data
- target
- group
- groups
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of building robust machine learning
  models for clinical neuroimaging applications across heterogeneous patient populations
  and acquisition settings. The authors propose a weighted empirical risk minimization
  (ERM) approach that combines large source datasets with a small amount of target
  data (as little as 10%) to adapt models to new groups.
---

# Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience

## Quick Facts
- arXiv ID: 2308.03175
- Source URL: https://arxiv.org/abs/2308.03175
- Reference count: 0
- Primary result: Weighted empirical risk minimization with 10% target data achieves AUC > 0.95 for AD, AUC > 0.7 for SZ, MAE < 5 years for brain age prediction

## Executive Summary
This paper addresses the challenge of adapting machine learning diagnostic models to new patient populations using limited target data in clinical neuroimaging. The authors propose a weighted empirical risk minimization approach that combines large source datasets with as little as 10% target data to improve model performance on under-represented groups. Applied to Alzheimer's disease, schizophrenia diagnosis, and brain age prediction across 15,363 individuals from 20 studies, the method consistently outperforms source-only training and often beats models trained on target data alone. The approach also enhances prognostic tasks and provides better clinical insights through improved brain age residual analysis.

## Method Summary
The method uses weighted empirical risk minimization (ERM) to combine large source datasets with a small fraction of labeled target data (approximately 10%) for domain adaptation. The approach employs ensemble learning with bagging, boosting, and stacking of diverse models including neural networks, random forests, and gradient boosting machines. Extensive preprocessing handles multi-modal data from MRI, demographics, clinical variables, genetic factors, and cognitive scores. Hyperparameter α in the weighted ERM objective balances contributions from source and target distributions. The framework is validated through nested cross-validation and MMD-based analysis of distributional differences between groups.

## Key Results
- Weighted ERM with 10% target data achieves strong performance: AUC > 0.95 for Alzheimer's disease classification
- Ensemble methods consistently outperform single models across all target groups and tasks
- Models adapted using source + 10% target data often outperform those trained on target data alone
- MMD statistic effectively quantifies domain shift between groups, guiding adaptation strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small fraction of labeled target data (≈10%) combined with large source data via weighted ERM significantly improves model performance on target groups compared to source-only training.
- Mechanism: Weighted ERM with hyperparameter α balances contributions from source and target distributions, allowing adaptation without overfitting to scarce target samples.
- Core assumption: Target data distribution differs from source but is related enough that joint training improves generalization.
- Evidence anchors:
  - [abstract] "we develop a theoretical argument for a weighted empirical risk minimization (ERM) objective that optimally combines all data from the source group and a small fraction of the data from the target group (in practice, as small as 10%)"
  - [section] "We have developed a mathematical argument... that elucidates inevitable trade-offs in the predictive ability when invariant representations are learned using heterogeneous data... Based on this theory, we have developed a method that uses a weighted-ERM objective to adapt to new groups."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.315, average citations=0.0. (Weak corpus evidence for this specific mechanism.)
- Break condition: If target and source distributions are too dissimilar, α-weighted ERM may not help and could degrade performance.

### Mechanism 2
- Claim: Ensembles of diverse models (neural nets, random forests, boosting, stacking) outperform single models on heterogeneous neuroimaging data.
- Mechanism: Combining multiple models reduces variance and captures different aspects of data heterogeneity, improving robustness across groups.
- Core assumption: Different models learn complementary features from heterogeneous data, and ensembling mitigates individual model biases.
- Evidence anchors:
  - [abstract] "we first show that ensembles built using bagging, boosting and stacking... can predict accurately on target groups, which is significantly better than deep neural networkstrainedonthesamedata"
  - [section] "For all three problems, for all groups, our ensembles (bold markers in Figs. 1c and S.1c) predict more accurately than the corresponding neural networks (translucent markers) (p < 10−3)."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.315, average citations=0.0. (Weak corpus evidence for this specific mechanism.)
- Break condition: If models are too similar or data is homogeneous, ensembling provides minimal benefit.

### Mechanism 3
- Claim: The maximum mean discrepancy (MMD) statistic quantifies distributional differences between groups, guiding adaptation strategy.
- Mechanism: MMD measures distance between learned feature distributions of different groups; larger MMD indicates greater domain shift requiring more target data or stronger adaptation.
- Core assumption: MMD is a reliable proxy for domain shift magnitude and can guide how much target data to use.
- Evidence anchors:
  - [section] "This involves building a multi-layer perceptron (MLP)-based model that classifies subjects as belonging to different groups... The empirical two-sample test statistic on these learned features can be used to quantify the differences in the data distribution across groups; we use the maximum-mean discrepancy (MMD)"
  - [section] "For all attributes, the hypothesis that different groups have the same distribution does not hold (p < 10−4). In our visualization, the angular distance between groups is proportional to the MMD statistic"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.315, average citations=0.0. (Weak corpus evidence for this specific mechanism.)
- Break condition: If MMD calculation is unstable or features don't capture group differences, this guidance becomes unreliable.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD) as a distribution distance metric
  - Why needed here: Quantifies how different source and target group distributions are, informing adaptation strategy
  - Quick check question: What does a high MMD statistic between two groups indicate about their data distributions?

- Concept: Weighted Empirical Risk Minimization (ERM)
  - Why needed here: Allows optimal combination of source and limited target data for adaptation
  - Quick check question: How does the α hyperparameter in weighted ERM control the trade-off between source and target data?

- Concept: Ensemble learning (bagging, boosting, stacking)
  - Why needed here: Reduces variance and captures diverse patterns in heterogeneous neuroimaging data
  - Quick check question: Why might an ensemble of diverse models outperform a single model on heterogeneous data?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction -> Ensemble training -> Weighted ERM adaptation -> Evaluation
- Critical path: 1. Preprocess source and target data 2. Train base models on source data 3. Apply weighted ERM using source data + 10% target data 4. Evaluate on held-out target test set
- Design tradeoffs:
  - More target data improves adaptation but may be costly
  - Complex ensembles reduce variance but increase computation
  - MMD-based guidance requires stable feature learning
- Failure signatures:
  - Poor performance when source-target MMD is very high
  - Overfitting when α is too high with limited target data
  - Underfitting when α is too low regardless of target data
- First 3 experiments:
  1. Train neural network on source data only, evaluate on target
  2. Apply weighted ERM with varying α values (0.1, 0.5, 0.9)
  3. Compare ensemble vs single model performance with 10% target data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed domain adaptation techniques perform when applied to imaging modalities beyond MRI, such as PET or EEG?
- Basis in paper: [inferred] The paper focuses on MRI data and does not explore the generalizability of the weighted-ERM approach to other imaging modalities.
- Why unresolved: The study is limited to MRI data, and the effectiveness of the approach for other imaging modalities is unknown.
- What evidence would resolve it: Experimental results demonstrating the performance of the weighted-ERM approach on PET or EEG data for similar diagnostic tasks.

### Open Question 2
- Question: What is the impact of incorporating additional clinical variables or genetic factors on the performance of the domain adaptation models?
- Basis in paper: [explicit] The paper mentions using demographic, clinical variables, genetic factors, and cognitive scores as features, but does not explore the impact of adding more variables.
- Why unresolved: The study uses a fixed set of features and does not investigate the potential benefits of incorporating additional variables.
- What evidence would resolve it: Experimental results comparing the performance of models with and without additional clinical or genetic variables.

### Open Question 3
- Question: How does the performance of the weighted-ERM approach compare to other state-of-the-art domain adaptation techniques, such as adversarial training or meta-learning?
- Basis in paper: [inferred] The paper mentions existing domain adaptation techniques but does not provide a direct comparison with the proposed weighted-ERM approach.
- Why unresolved: The study focuses on the weighted-ERM approach and does not benchmark it against other methods.
- What evidence would resolve it: Experimental results comparing the performance of the weighted-ERM approach with other domain adaptation techniques on the same datasets and tasks.

## Limitations

- The theoretical foundation for weighted ERM relies on assumptions about distributional similarity that are not fully validated across diverse clinical scenarios
- The claim that source-only trained models often outperform target-only models may be dataset-specific rather than a general principle
- The MMD statistic, while theoretically sound, may not capture all relevant aspects of domain shift in complex clinical neuroimaging data

## Confidence

- **High confidence**: Ensemble methods outperform single models on heterogeneous data (supported by extensive statistical testing)
- **Medium confidence**: Weighted ERM with 10% target data achieves strong adaptation (limited to specific tasks/datasets)
- **Low confidence**: Theoretical guarantees about optimal α values generalize across different clinical applications

## Next Checks

1. Test the weighted ERM approach on completely new clinical datasets not used in the original study to assess generalizability
2. Conduct ablation studies varying the proportion of target data (5%, 15%, 20%) to determine if 10% is truly optimal
3. Evaluate model performance when source and target groups have minimal overlap in demographic or clinical characteristics