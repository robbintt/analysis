---
ver: rpa2
title: Mixing-Denoising Generalizable Occupancy Networks
arxiv_id: '2311.12125'
source_url: https://arxiv.org/abs/2311.12125
tags:
- point
- cloud
- reconstruction
- implicit
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first only-MLP feed-forward generalizable
  occupancy network from point clouds, achieving state-of-the-art results while using
  half the parameters of the previous best convolutional model. The core method combines
  denoising of input point clouds with a novel decoder that uses both local and global
  feature pooling guided by denoised relative positional encoding.
---

# Mixing-Denoising Generalizable Occupancy Networks

## Quick Facts
- arXiv ID: 2311.12125
- Source URL: https://arxiv.org/abs/2311.12125
- Reference count: 40
- Only-MLP feed-forward generalizable occupancy network from point clouds that achieves state-of-the-art results while using half the parameters of the previous best convolutional model

## Executive Summary
This paper introduces the first only-MLP feed-forward generalizable occupancy network from point clouds, achieving state-of-the-art results while using half the parameters of the previous best convolutional model. The core method combines denoising of input point clouds with a novel decoder that uses both local and global feature pooling guided by denoised relative positional encoding. The model outperforms existing methods on ShapeNet, FAUST, and ScanNet benchmarks, especially under high noise and sparsity. The approach shows that MLP-only architectures can match or surpass convolutional ones in this task, challenging the necessity of convolution-based inductive biases for 3D reconstruction from point clouds.

## Method Summary
The model uses an MLP-only architecture with a U-Net PointMixer encoder that processes noisy point clouds. A denoising head predicts displacement vectors to clean the point cloud, which is then used for relative positional encoding in the decoder. The decoder employs "Extra-set mixing" with both local (K nearest) and global (Nd downsampled) feature pooling to predict occupancy probabilities for query points. The method is trained with combined reconstruction loss and denoising loss, achieving state-of-the-art performance on multiple benchmarks while using significantly fewer parameters than convolutional baselines.

## Key Results
- Outperforms state-of-the-art convolutional method [7] while using half the number of model parameters
- Achieves superior performance on ShapeNet, FAUST, and ScanNet benchmarks, especially under high noise and sparsity
- Demonstrates that MLP-only architectures can match or surpass convolutional ones for 3D reconstruction from point clouds

## Why This Works (Mechanism)

### Mechanism 1
The "Extra-set mixing" decoder achieves better local feature aggregation than convolutional methods by using denoised relative positional encoding. The decoder pools features from the K nearest denoised points using softmax-based order-invariant pooling. By using denoised point coordinates instead of noisy ones for positional encoding, the aggregation becomes more accurate and less affected by noise.

### Mechanism 2
Combining local and global feature pooling makes the model more robust to outliers and local noise. The local decoder uses K nearest denoised points for fine-grained feature aggregation, while the global decoder uses all Nd downsampled denoised points for coarse context. This two-level approach prevents over-reliance on potentially noisy local information.

### Mechanism 3
The MLP-only architecture can match or surpass convolutional methods while using half the parameters. The PointMixer architecture applies MLPs to both the feature dimension and spatial dimension separately, replacing convolutions. The order-invariant softmax pooling handles the irregularity of point clouds.

## Foundational Learning

- **Point cloud processing fundamentals**: Understanding how to extract features from unordered point sets is crucial for implementing the encoder
  - Why needed here: The model processes point clouds as input, requiring understanding of point cloud representations and feature extraction
  - Quick check question: What is the difference between PointNet and PointNet++ in terms of local feature aggregation?

- **Implicit function representation**: The model represents shapes as occupancy functions, so understanding level sets and signed distance functions is essential
  - Why needed here: The model predicts occupancy probabilities for query points, requiring understanding of implicit shape representations
  - Quick check question: How does an occupancy network differ from a signed distance function network?

- **Denoising techniques for point clouds**: The model includes a denoising component that improves reconstruction quality
  - Why needed here: The denoising head is a critical component that enables better feature aggregation through cleaner positional encoding
  - Quick check question: What are the trade-offs between supervised and unsupervised point cloud denoising?

## Architecture Onboarding

- **Component map**: Noisy point cloud → U-Net PointMixer encoder → Denoising head → Local+Global decoder → Occupancy probabilities
- **Critical path**: Point cloud → Encoder → Denoising → Local+Global Decoder → Occupancy
- **Design tradeoffs**: Using denoised coordinates vs. original coordinates for positional encoding; local vs. global feature aggregation balance; MLP-only vs. convolutional architecture choice; number of nearest neighbors (K) for local pooling
- **Failure signatures**: Poor performance on noisy inputs suggests denoising is insufficient; over-smoothing suggests too much global context; failure to capture fine details suggests inadequate local feature aggregation
- **First 3 experiments**:
  1. Compare reconstruction with and without denoising to isolate its impact
  2. Test different values of K for local pooling to find optimal neighborhood size
  3. Evaluate local-only vs. local+global decoding to verify the global component's contribution

## Open Questions the Paper Calls Out

### Open Question 1
How does the MLP-only architecture's performance scale with different point cloud sizes beyond 3000 points? The paper mentions using 3000 points for ShapeNet and 10k for Synthetic Rooms but does not explore larger point clouds. Experiments showing performance comparisons with point clouds of 50k, 100k, or 1M points, including memory usage and inference time comparisons, would resolve this.

### Open Question 2
What is the exact contribution of denoising to generalization performance across different domain shifts? The paper shows denoising helps but quantifies the gap as "fourfold" only for ShapeNet to ScanNet. Controlled experiments testing denoising with different types of domain shifts (style transfer, different sensors, synthetic-to-real) and noise distributions (Gaussian, Poisson, sensor-specific) would resolve this.

### Open Question 3
How does the global decoder's performance compare to alternative global feature aggregation methods? The paper introduces a global decoder but only compares against its own ablated version without denoising. Head-to-head comparisons with transformer-based global feature aggregation and other hierarchical pooling methods on the same benchmarks would resolve this.

## Limitations
- The paper lacks direct empirical validation of key mechanisms - no ablation studies isolate the denoising impact on positional encoding
- Architectural details of the PointMixer components are underspecified, making faithful reproduction challenging
- The synthetic benchmarks may not adequately represent real-world noise distributions and scanning artifacts

## Confidence

- **High confidence**: The core claim that MLP-only architectures can match convolutional methods is supported by quantitative results showing superior performance on multiple benchmarks
- **Medium confidence**: The denoising mechanism's contribution is supported by results but lacks ablation studies; the effectiveness of denoised positional encoding is inferred rather than directly validated
- **Low confidence**: The architectural innovations are difficult to verify without implementation details

## Next Checks

1. Implement an ablation study comparing reconstruction quality with original vs. denoised positional encoding to directly measure denoising's contribution to feature aggregation accuracy
2. Run controlled experiments varying K (nearest neighbors for local pooling) and Nd (downsampled points for global pooling) to determine optimal configuration and validate the local+global pooling design choice
3. Re-implement the model using only the architectural specifications provided, documenting any ambiguities encountered and how they were resolved to assess reproducibility