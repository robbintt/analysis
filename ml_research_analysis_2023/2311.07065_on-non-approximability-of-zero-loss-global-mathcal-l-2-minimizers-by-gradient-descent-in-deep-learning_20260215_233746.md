---
ver: rpa2
title: On non-approximability of zero loss global ${\mathcal L}^2$ minimizers by gradient
  descent in Deep Learning
arxiv_id: '2311.07065'
source_url: https://arxiv.org/abs/2311.07065
tags:
- gradient
- descent
- cost
- global
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the geometric properties of gradient descent
  in deep learning networks, focusing on whether global L2 minimizers can be achieved
  through this method. It compares gradient descent with a constructive approach for
  finding global minimizers in underparameterized ReLU networks.
---

# On non-approximability of zero loss global ${\mathcal L}^2$ minimizers by gradient descent in Deep Learning

## Quick Facts
- arXiv ID: 2311.07065
- Source URL: https://arxiv.org/abs/2311.07065
- Authors: 
- Reference count: 10
- The paper proves that gradient descent cannot achieve zero-loss global minimizers in underparameterized ReLU networks due to rank deficiency constraints in the Jacobian matrix.

## Executive Summary
This paper analyzes the geometric properties of gradient descent in deep learning networks, specifically examining whether global L2 minimizers can be achieved through this method. The authors compare gradient descent with a constructive approach for finding global minimizers in underparameterized ReLU networks. They establish that in underparameterized networks (where the number of parameters K is less than the total output dimension QN), gradient descent is fundamentally constrained by the rank deficiency of the Jacobian matrix, preventing it from reaching the zero-loss global minimizers that can be constructed explicitly.

## Method Summary
The paper analyzes gradient descent dynamics ∂sZ(s) = -∇ZC[x[Z(s)]] for weights/biases Z in underparameterized ReLU networks. The key insight is that the Jacobian matrix D[Z(s)] has rank at most K (number of parameters), which is less than QN (output dimension) in underparameterized networks. This creates a constrained dynamical system where P⊥[Z(s)]∂sx(s) = 0, preventing the gradient from pointing in directions that would achieve zero loss. The authors prove Theorem 1.5 establishing that stationary points of gradient descent have positive cost values given by C[x[Z*]] = N/2 ||P⊥[Z*]∇xC[x[Z*]]||², where rank(P⊥[Z*]) ≥ QN - K.

## Key Results
- Gradient descent cannot reach zero-loss global minimizers in underparameterized networks due to rank deficiency constraints
- Stationary points of gradient descent in underparameterized networks have positive cost values
- The constructive global minimizers from [4] and gradient descent are disjoint methods, producing entirely different solution sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent cannot reach zero-loss global minimizers in underparameterized networks due to rank deficiency constraints
- Mechanism: The Jacobian matrix D[Z(s)] has rank at most K (number of parameters), which is less than QN (output dimension) in underparameterized networks. This creates a constrained dynamical system where P⊥[Z(s)]∂sx(s) = 0, preventing the gradient from pointing in directions that would achieve zero loss
- Core assumption: The training data is generic (not clustered) and the network uses ReLU activation
- Evidence anchors:
  - [abstract]: "zero loss minimization can generically not be attained" in underparametrized networks
  - [section]: Theorem 1.5 establishes that rank(P⊥[Z*]) ≥ QN - K, and Corollary 1.6 concludes that global minimizers "can generically not be approximated by gradient descent"
  - [corpus]: Weak evidence - neighbor papers discuss similar geometric constraints but don't directly confirm this specific mechanism
- Break condition: If the network becomes overparameterized (K ≥ QN) or if training data is non-generic (clustered as assumed in [4]), the constraint can be overcome

### Mechanism 2
- Claim: Stationary points of gradient descent in underparameterized networks have positive cost values
- Mechanism: At any stationary point Z*, the gradient of the cost function must lie in the range of D[Z*], which is constrained by the rank deficiency. This forces P⊥[Z*]∇xC[x[Z*]] ≠ 0, resulting in positive cost C[x[Z*]] = N/2||P⊥[Z*]∇xC[x[Z*]]||²
- Core assumption: The network is underparameterized (K < QN) and the stationary point satisfies the rank constraint
- Evidence anchors:
  - [section]: Theorem 1.5 proves that at stationary points, 0 = P[Z*]∇xC[x[Z*]] and C[x[Z*]] = N/2||P⊥[Z*]∇xC[x[Z*]]||² where rank(P⊥[Z*]) ≥ QN - K
  - [abstract]: States that "stationary points of gradient descent in underparameterized networks have positive cost values"
  - [corpus]: No direct evidence found in neighbors; this appears to be the novel contribution
- Break condition: If the rank constraint is violated (K ≥ QN) or if the stationary point doesn't satisfy the projector condition

### Mechanism 3
- Claim: The constructive global minimizers from [4] and gradient descent are disjoint methods
- Mechanism: The constructive method in [4] creates zero-loss minimizers by design through explicit construction, while gradient descent is fundamentally constrained by the rank deficiency. Since gradient descent cannot reach zero loss in underparameterized networks, these two approaches produce entirely different solution sets
- Core assumption: The network is underparameterized and uses ReLU activation as in [4]
- Evidence anchors:
  - [abstract]: "we conclude that the method introduced in [4] is disjoint from the gradient descent method"
  - [section]: Corollary 1.6 explicitly states that "global minimizers constructed in [4] can generically not be approximated by gradient descent"
  - [corpus]: Neighbor paper 235832 discusses geometric structure of shallow networks but doesn't address this disjointness directly
- Break condition: If the network becomes overparameterized or if non-ReLU activations are used, the methods may intersect

## Foundational Learning

- Concept: Rank deficiency and its implications for constrained optimization
  - Why needed here: The core argument relies on understanding why D[Z]DT[Z] being rank-deficient (K < QN) creates fundamental limitations on gradient descent
  - Quick check question: In an underparameterized network with K parameters and QN outputs, what is the maximum possible rank of D[Z]DT[Z]?

- Concept: Projector operators and constrained dynamical systems
  - Why needed here: The paper uses P[Z] and P⊥[Z] projectors to decompose the gradient flow into constrained and free components
  - Quick check question: If P[Z] projects onto the range of D[Z]DT[Z], what space does P⊥[Z] project onto?

- Concept: Geometric interpretation of loss landscapes
  - Why needed here: Understanding how the topology of the loss landscape differs between overparameterized and underparameterized regimes is crucial for grasping why zero-loss minimizers are unreachable
  - Quick check question: How does the geometry of the loss landscape change when transitioning from K < QN to K ≥ QN?

## Architecture Onboarding

- Component map:
  - Input layer (RM) → Hidden layers (RMℓ) with ReLU activation → Output layer (RQ)
  - Parameter vector Z ∈ RK containing all weights and biases
  - Cost function C[x[Z]] = 1/(2N)∑||xj[Z] - yω(j)||²
  - Jacobian matrix D[Z] ∈ RQN×K mapping parameter changes to output changes
  - Projectors P[Z] and P⊥[Z] for constrained dynamics analysis

- Critical path:
  1. Initialize Z(0) randomly
  2. Compute gradient ∇ZC[x[Z(s)]] at current Z(s)
  3. Update Z(s+1) = Z(s) - η∇ZC[x[Z(s)]] for small step size η
  4. Monitor convergence and check if stationary point satisfies zero-loss condition
  5. If K < QN, expect positive cost at stationary points due to rank constraints

- Design tradeoffs:
  - Overparameterization (K ≥ QN): Can achieve zero loss but may overfit
  - Underparameterization (K < QN): Cannot achieve zero loss but may generalize better
  - Activation function choice: ReLU creates the rank constraints discussed, other activations may behave differently
  - Data distribution: Generic data creates the geometric constraints, clustered data may allow zero loss even in underparameterized networks

- Failure signatures:
  - Gradient norm approaching zero but cost remaining positive (indicating rank-constrained stationary point)
  - Training loss plateauing well above zero in underparameterized networks
  - Large discrepancy between training and validation loss suggesting the network cannot fit the training data

- First 3 experiments:
  1. Train an underparameterized ReLU network on generic data and verify that the final cost remains positive even after gradient descent convergence
  2. Compare training dynamics between overparameterized (K > QN) and underparameterized (K < QN) networks on the same dataset
  3. Test whether clustered training data (as assumed in [4]) allows gradient descent to achieve zero loss in underparameterized networks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can gradient descent find any zero-loss global minimizers in underparameterized ReLU networks?
- Basis in paper: [explicit] Theorem 1.5 and Corollary 1.6 explicitly state that gradient descent cannot reach the zero-loss global minimizers constructed in [4] for underparameterized networks
- Why unresolved: The paper proves this impossibility for the specific construction in [4], but doesn't establish whether gradient descent can find ANY zero-loss minimizers in underparameterized networks
- What evidence would resolve it: Finding even a single example where gradient descent does reach a zero-loss global minimizer in an underparameterized network, or proving that no such minimizers exist in the underparameterized regime

### Open Question 2
- Question: What structural properties of the Jacobian matrix D[Z(s)] determine whether gradient descent can achieve zero loss?
- Basis in paper: [explicit] Theorem 1.5 discusses the rank deficiency of the Jacobian matrix D[Z(s)] and its role in constraining the dynamics
- Why unresolved: The paper identifies rank deficiency as a key issue but doesn't fully characterize what specific structural properties of the Jacobian are necessary and sufficient for achieving zero loss
- What evidence would resolve it: A complete characterization of the conditions on D[Z(s)] that enable or prevent zero-loss attainment, possibly through geometric or algebraic analysis

### Open Question 3
- Question: How does the initialization of weights and biases affect the ability of gradient descent to find global minimizers?
- Basis in paper: [inferred] The paper mentions that initial data Z0 is often chosen randomly in computational applications, but doesn't analyze how different initializations affect convergence to global minimizers
- Why unresolved: While the paper proves gradient descent cannot reach certain global minimizers, it doesn't explore whether different initializations could enable reaching different global minimizers
- What evidence would resolve it: Empirical or theoretical analysis showing how different initialization strategies affect the basins of attraction for global minimizers, or proving that initialization cannot overcome the fundamental geometric constraints identified in the paper

## Limitations
- The analysis is limited to ReLU networks with generic training data and assumes specific architectural constraints (M = Mℓ = Q = L)
- The results rely heavily on the geometric properties established in reference [4], which is not fully detailed in this paper
- The claims about disjointness between gradient descent and constructive methods are conditional on the underparameterized regime and may not generalize to other activation functions or data distributions

## Confidence
- High confidence in Theorem 1.5 and Corollary 1.6 for the stated conditions
- Medium confidence in the broader implications about optimization landscape geometry
- Low confidence in the disjointness claim without access to the full details of [4]

## Next Checks
1. Verify the rank constraint behavior experimentally by training networks with varying K/QN ratios and measuring the achievable loss
2. Test whether the geometric constraints persist with non-ReLU activation functions
3. Examine the effect of clustered vs. generic training data on the ability of gradient descent to achieve zero loss in underparameterized networks