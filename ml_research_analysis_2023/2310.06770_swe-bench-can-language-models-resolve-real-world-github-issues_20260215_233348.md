---
ver: rpa2
title: 'SWE-bench: Can Language Models Resolve Real-World GitHub Issues?'
arxiv_id: '2310.06770'
source_url: https://arxiv.org/abs/2310.06770
tags:
- test
- task
- tests
- issue
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWE-bench is a challenging benchmark for evaluating language models
  on real-world software engineering tasks. It consists of 2,294 GitHub issues and
  corresponding pull requests from 12 popular Python repositories.
---

# SWE-bench: Can Language Models Resolve Real-World GitHub Issues?

## Quick Facts
- arXiv ID: 2310.06770
- Source URL: https://arxiv.org/abs/2310.06770
- Reference count: 40
- State-of-the-art models solve only 1.96% (Claude 2) and 1.7% (GPT-4) of real-world GitHub issues

## Executive Summary
SWE-bench is a challenging benchmark for evaluating language models on real-world software engineering tasks. It consists of 2,294 GitHub issues and corresponding pull requests from 12 popular Python repositories. Models are tasked with generating patches to resolve the issues, which often require understanding and coordinating changes across multiple files. Evaluation shows that even state-of-the-art models like Claude 2 and GPT-4 struggle significantly, solving only 1.96% and 1.7% of tasks respectively, even with oracle file retrieval.

## Method Summary
SWE-bench evaluates language models by tasking them with generating patches to resolve real GitHub issues from popular Python repositories. Models receive issue descriptions and codebase context (either via oracle or BM25 retrieval), generate patch files, and are evaluated based on whether patches apply successfully and all related tests pass. The benchmark includes a train-test split with disjoint repositories to minimize data contamination, and features a diverse set of tasks ranging from bug fixes to feature implementations.

## Key Results
- Claude 2 solves only 1.96% of tasks with BM25 retrieval, 4.81% with oracle retrieval
- GPT-4 solves only 1.7% of tasks with oracle retrieval
- SWE-Llama, a fine-tuned CodeLlama model, performs slightly better but still fails to resolve most issues
- Model-generated patches average 30.1 lines compared to 74.5 lines for human-written solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SWE-bench tasks require understanding and coordinating changes across multiple functions, classes, and files simultaneously
- Mechanism: Models must navigate large codebases with thousands of files and identify the relatively small number of lines needing modification among extensive context
- Core assumption: The complexity of real-world software engineering tasks cannot be reduced to self-contained problems that can be solved in a few lines of code
- Evidence anchors:
  - [abstract] "Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously"
  - [section] "SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed)"
  - [corpus] SWE-bench instances average 3,010 files and 438K lines of code per codebase
- Break condition: If tasks could be decomposed into independent subproblems or if models could reliably identify relevant context without extensive cross-file reasoning

### Mechanism 2
- Claim: Performance drops significantly when models are provided with large amounts of context, even when BM25 retrieval achieves reasonable recall
- Mechanism: Models struggle with localizing problematic code needing to be updated when presented with a sea of tokens, becoming distracted by irrelevant context
- Core assumption: Models are pre-trained on long sequences of code but are typically asked to generate single functions at a time with limited context
- Evidence anchors:
  - [abstract] "performance drops further to 1.96% for Claude 2" when using BM25 retrieval instead of oracle retrieval
  - [section] "as total context length increases, Claude 2's performance drops considerably"
  - [corpus] BM25 retrieval only retrieves a superset of oracle files in about 40% of instances with the 27,000 token context limit
- Break condition: If models could develop effective attention mechanisms to focus on relevant context or if retrieval systems could perfectly identify all necessary context

### Mechanism 3
- Claim: Models tend to generate shorter, simpler edits compared to human-written solutions, often failing to address the full scope of issues
- Mechanism: Model-generated patch files average 30.1 lines compared to 74.5 lines for gold patches, rarely editing more than a single file
- Core assumption: Models are trained on standard code files and rarely see patch files, making structured output generation challenging
- Evidence anchors:
  - [abstract] "Model generated patch files that apply correctly are less than half the total length (74.5 versus 30.1 lines) of gold edit patch files"
  - [section] "Models' generations also reflect a 'greedy' approach of solving the problem exactly, with little regard for code style or logical constraints"
  - [corpus] Average gold patches edit 1.7 files while model patches rarely edit more than one file
- Break condition: If models could be trained specifically on patch generation tasks or if they could better understand the broader context of code changes

## Foundational Learning

- Concept: Long context processing and attention mechanisms
  - Why needed here: SWE-bench codebases regularly contain thousands of files and hundreds of thousands of lines, requiring models to process extremely long contexts
  - Quick check question: Can the model identify relevant code sections within a 100,000+ token context without being overwhelmed by irrelevant information?

- Concept: Structured output generation
  - Why needed here: Models must generate patch files with specific formatting (diff format) that specify file paths, line numbers, and added/removed lines
  - Quick check question: Can the model generate syntactically correct patch files that apply cleanly to the codebase using unix patch?

- Concept: Cross-file reasoning and dependency understanding
  - Why needed here: Issues often require changes that span multiple files and understanding how different parts of the codebase interact
  - Quick check question: Can the model identify and modify interdependent functions across different files to resolve an issue?

## Architecture Onboarding

- Component map: Issue parser → Codebase retriever → Context selector → Model generator → Patch applicator → Test runner → Results aggregator
- Critical path: Issue text → Context selection → Model generation → Patch application → Test execution → Results evaluation
- Design tradeoffs:
  - Context window size vs. computational cost: Larger contexts improve recall but increase inference costs
  - Retrieval method choice: Oracle retrieval is unrealistic but provides upper bounds; BM25 is more realistic but less effective
  - Patch generation format: Generating patches is more efficient than entire files but requires structured output capability
- Failure signatures:
  - Low patch application rate: Indicates issues with patch formatting or context relevance
  - High pass-to-pass test failures: Suggests models are breaking existing functionality
  - Poor correlation between retrieval recall and performance: Indicates models struggle with context localization
- First 3 experiments:
  1. Compare model performance with oracle vs. BM25 retrieval on a small subset to establish baseline effectiveness of context selection
  2. Test different context window sizes with fixed retrieval method to identify optimal context length for each model
  3. Evaluate model performance on tasks partitioned by input length to understand how context size affects different difficulty levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models perform on SWE-bench tasks that require understanding and processing images embedded in issue descriptions?
- Basis in paper: [explicit] The paper mentions that some issues contain images (e.g., 32% of matplotlib and 10% of seaborn instances), and that solving these instances may require multi-modal LMs or external tool use to process images.
- Why unresolved: The paper does not evaluate models on image-containing tasks, and the baseline models used (Claude 2, GPT-4, SWE-Llama) are not multi-modal.
- What evidence would resolve it: Evaluation of multi-modal language models or models augmented with image processing capabilities on the subset of SWE-bench tasks that contain images.

### Open Question 2
- Question: How does the performance of language models on SWE-bench tasks vary based on the programming language of the codebase?
- Basis in paper: [inferred] The paper notes that all current SWE-bench tasks are in Python, and mentions the potential to extend the benchmark to other languages.
- Why unresolved: The paper does not provide any data or analysis on how models would perform on codebases in languages other than Python.
- What evidence would resolve it: Creation and evaluation of SWE-bench tasks in multiple programming languages, comparing model performance across languages.

### Open Question 3
- Question: What is the impact of model training data on performance, particularly regarding potential data contamination from repositories included in SWE-bench?
- Basis in paper: [explicit] The paper states that SWE-bench-train uses disjoint repositories from the evaluation set to minimize contamination risk, and notes that model performance is similar for issues created before and after 2023, suggesting limited "cheating."
- Why unresolved: The analysis of temporal performance is limited, and there's no direct measurement of contamination effects.
- What evidence would resolve it: Detailed analysis of model performance on tasks from repositories with varying degrees of overlap with training data, or controlled experiments with known contamination levels.

### Open Question 4
- Question: How do language models perform on SWE-bench tasks that require leveraging existing third-party libraries or codebase functionality, rather than writing primitive code?
- Basis in paper: [explicit] The qualitative analysis notes that models tend to write primitive Python code and do not leverage existing third-party libraries or the rest of the codebase for their solutions.
- Why unresolved: The paper does not provide quantitative analysis of this behavior or evaluate models on their ability to use existing libraries effectively.
- What evidence would resolve it: Quantitative analysis of model-generated solutions, measuring their use of existing libraries vs. primitive code, and evaluation of models on tasks specifically designed to test library usage skills.

### Open Question 5
- Question: How does the length and complexity of issue descriptions affect model performance on SWE-bench tasks?
- Basis in paper: [explicit] The paper mentions that issue descriptions are typically long (195 words on average) and notes that performance drops as context length increases.
- Why unresolved: The paper does not provide a detailed analysis of how issue description length or complexity specifically impacts performance.
- What evidence would resolve it: Analysis of model performance on tasks with varying issue description lengths and complexities, controlling for other factors.

## Limitations

- The benchmark is relatively small (2,294 tasks) despite being constructed from large, complex codebases
- Evaluation assumes oracle retrieval is achievable in practice, which may not reflect real-world deployment scenarios
- Results may not generalize across different programming languages and repository types

## Confidence

- Current models "barely work" on SWE-bench tasks: Low confidence
- Models struggle with long context: Medium confidence
- Model-generated patches are significantly shorter than gold patches: High confidence

## Next Checks

1. Cross-language generalization: Evaluate model performance on SWE-bench tasks translated to or replicated in other programming languages to assess whether results are specific to Python or generalize across languages

2. Fine-tuning impact: Train models specifically on patch generation tasks using synthetic data and evaluate whether this improves performance on SWE-bench, isolating whether current limitations stem from architecture or training

3. Human performance baseline: Have experienced developers attempt to resolve a subset of SWE-bench issues without access to the original solutions to establish human performance benchmarks and better understand task difficulty