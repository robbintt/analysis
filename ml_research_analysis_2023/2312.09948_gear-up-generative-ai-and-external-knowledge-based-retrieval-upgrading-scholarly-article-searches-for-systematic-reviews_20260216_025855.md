---
ver: rpa2
title: 'GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading Scholarly
  Article Searches for Systematic Reviews'
arxiv_id: '2312.09948'
source_url: https://arxiv.org/abs/2312.09948
tags:
- query
- review
- articles
- librarian
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEAR-Up, a system designed to assist librarians
  in conducting systematic reviews by automating query expansion and article retrieval.
  The approach leverages Generative AI (e.g., ChatGPT) and external knowledge sources
  (knowledge graphs, PubMed) to enrich user queries with additional context and generate
  related search terms.
---

# GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading Scholarly Article Searches for Systematic Reviews

## Quick Facts
- arXiv ID: 2312.09948
- Source URL: https://arxiv.org/abs/2312.09948
- Reference count: 4
- Primary result: GEAR-Up automates systematic review literature searches using generative AI and external knowledge sources to reduce librarian workload while maintaining search quality

## Executive Summary
GEAR-Up is a modular system that assists librarians in conducting systematic reviews by automating query expansion and article retrieval. The system leverages Generative AI (ChatGPT) and external knowledge sources (knowledge graphs, PubMed) to enrich user queries with additional context and generate related search terms. These expanded queries are then used to retrieve relevant scholarly articles via PubMed searches and FAISS-based filtering. The system was evaluated qualitatively by an in-house librarian comparing retrieved articles against ground-truth sentinel articles, showing effectiveness in reducing workload while providing high-quality, relevant articles comparable to manual searches.

## Method Summary
The system implements a three-module pipeline: (1) Query Expansion Module extracts seed concepts from user queries and enriches them using knowledge graphs and masked language models to obtain additional context terms and relationships; (2) Additional Related Query Generation Module prompts ChatGPT with the expanded query context to generate related search queries; (3) Article Search and Retrieval Module uses the generated queries to search PubMed, then applies FAISS-based retrieval to narrow down to most relevant articles based on titles, abstracts, and passages. The modular design allows for future extensions including incorporating unstructured and semi-structured knowledge sources and improving traceability of system outputs to their knowledge origins.

## Key Results
- GEAR-Up effectively reduces librarian workload by automating query expansion and retrieval processes
- Retrieved articles are of high quality and comparable to those obtained through manual systematic review searches
- The system's modular architecture enables future extensions and improvements in traceability and knowledge source integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GEAR-Up reduces librarian workload by automating query expansion and retrieval, producing results comparable to manual searches.
- Mechanism: The system takes user queries, expands them using external knowledge graphs and language models, generates related queries via ChatGPT, and retrieves articles through PubMed and FAISS-based filtering.
- Core assumption: Automated query expansion with external knowledge sources improves the relevance of retrieved articles compared to simple keyword searches.
- Evidence anchors:
  - [abstract] "The results indicate that GEAR-Up effectively reduces the librarian's workload by providing high-quality, relevant articles comparable to those obtained manually."
  - [section] "We demonstrate a pipeline to assist librarians with structured, systematic review search processes."
- Break condition: If the external knowledge sources provide irrelevant or misleading information, the expanded queries may retrieve off-topic articles, negating the benefits of automation.

### Mechanism 2
- Claim: The modular design allows for future extensions, including incorporating unstructured and semi-structured knowledge sources and improving traceability of system outputs to their knowledge origins.
- Mechanism: The system is structured into three modules (Query Expansion, Related Query Generation, Article Search and Retrieval) that can be independently modified or extended.
- Core assumption: A modular architecture enables easier integration of new knowledge sources and improvements without disrupting the entire system.
- Evidence anchors:
  - [abstract] "The modular design allows for future extensions, including incorporating unstructured and semi-structured knowledge sources and improving traceability of system outputs to their knowledge origins."
  - [section] "Our method is modular and can be described as follows: (1) Query Expansion Module... (2) Additional Related Query Generation Module... (3) Article Search and Retrieval Module..."
- Break condition: If the modules become too tightly coupled or if changes in one module have unforeseen effects on others, the modular design's benefits may be compromised.

### Mechanism 3
- Claim: The system's use of Generative AI (ChatGPT) and external knowledge sources improves the quality of generated queries compared to simple query reformulation techniques.
- Mechanism: ChatGPT is prompted with expanded queries containing additional context from knowledge graphs and language models to generate related search queries.
- Core assumption: Generative AI models can leverage the additional context provided by external knowledge sources to generate more relevant and diverse search queries.
- Evidence anchors:
  - [abstract] "Due to the revolutionary advances in (1) Generative AI such as ChatGPT, and (2) External knowledge-augmented information extraction efforts such as Retrieval-Augmented Generation, In this work, we explore the use of techniques from (1) and (2) for SR."
  - [section] "The concepts and relationships from the previous steps are fed into ChatGPT with an appropriate prompt to obtain a set of reformulated queries."
- Break condition: If ChatGPT generates irrelevant or incoherent queries, or if the additional context from external knowledge sources is not effectively utilized, the quality of generated queries may not improve.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: KGs provide structured information about relationships between concepts, which is used to expand user queries with additional context.
  - Quick check question: What is the primary purpose of using knowledge graphs in the Query Expansion Module of GEAR-Up?
- Concept: Natural Language Processing (NLP) and Information Retrieval (IR)
  - Why needed here: NLP techniques are used to extract seed concepts from user queries, while IR techniques are used to search for relevant articles in scholarly databases.
  - Quick check question: How do NLP and IR techniques contribute to the overall functionality of GEAR-Up?
- Concept: Generative AI (ChatGPT) and Prompt Engineering
  - Why needed here: ChatGPT is used to generate related search queries based on the expanded queries, and prompt engineering is crucial for eliciting relevant responses from the model.
  - Quick check question: What role does prompt engineering play in the Additional Related Query Generation Module of GEAR-Up?

## Architecture Onboarding

- Component map: User query -> Query Expansion Module (NLP tools, knowledge graphs, masked language models) -> Additional Related Query Generation Module (ChatGPT) -> Article Search and Retrieval Module (PubMed searches, FAISS-powered retriever)
- Critical path: User query → Query Expansion Module → Additional Related Query Generation Module → Article Search and Retrieval Module → Librarian review
- Design tradeoffs:
  - Balancing the trade-off between query expansion and specificity: expanding queries too much may lead to less relevant results, while expanding too little may miss important related concepts.
  - Choosing between using more external knowledge sources (which may improve query expansion) and keeping the system computationally efficient.
- Failure signatures:
  - If the system consistently retrieves irrelevant articles, it may indicate issues with the Query Expansion Module or the quality of external knowledge sources.
  - If the system fails to retrieve relevant articles, it may indicate problems with the Additional Related Query Generation Module or the Article Search and Retrieval Module.
- First 3 experiments:
  1. Evaluate the system's performance with different query expansion strategies (e.g., using different knowledge graphs or language models) to determine the optimal approach.
  2. Test the system's ability to handle queries from different domains (e.g., medical, social sciences) to assess its generalizability.
  3. Measure the system's performance in terms of precision and recall compared to manual searches conducted by librarians to quantify the improvement in efficiency and effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GEAR-Up compare quantitatively to traditional manual systematic review methods in terms of precision, recall, and time saved?
- Basis in paper: [inferred] The paper mentions qualitative evaluation by an in-house librarian but does not provide quantitative metrics comparing GEAR-Up to manual methods.
- Why unresolved: The paper only presents qualitative evaluations and does not include specific performance metrics or statistical analyses comparing the system's outputs to manual review processes.
- What evidence would resolve it: A detailed quantitative comparison study measuring precision, recall, and time efficiency of GEAR-Up against traditional manual systematic review methods.

### Open Question 2
- Question: How does the system handle queries that involve ambiguous or context-dependent terminology, and what are the limitations of this approach?
- Basis in paper: [explicit] The paper mentions that safety is context-sensitive and that controls can be placed to avoid sensitive content, but does not elaborate on how the system handles ambiguous terminology.
- Why unresolved: The paper does not provide specific examples or mechanisms for dealing with ambiguous or context-dependent terms, nor does it discuss potential limitations in handling such cases.
- What evidence would resolve it: Case studies or examples demonstrating how the system processes ambiguous queries and the outcomes of such processing, along with any identified limitations.

### Open Question 3
- Question: What are the scalability limitations of GEAR-Up when applied to large-scale systematic reviews involving thousands of articles?
- Basis in paper: [inferred] The paper does not discuss the system's performance or limitations when scaling up to handle large volumes of data.
- Why unresolved: There is no mention of how the system performs with large datasets or any identified bottlenecks or scalability issues.
- What evidence would resolve it: Performance benchmarks and scalability tests showing how GEAR-Up handles large-scale systematic reviews, including any identified limitations or necessary optimizations.

## Limitations
- Evaluation methodology relies entirely on qualitative assessment by a single in-house librarian, limiting generalizability and introducing potential bias
- Key implementation details remain unspecified, including exact FAISS configuration parameters, ChatGPT prompt templates, and specific knowledge graph sources beyond PubMed
- System performance across different domains and query types has not been demonstrated

## Confidence
- High confidence: The modular system architecture and its three main components (Query Expansion, Related Query Generation, Article Search and Retrieval) are clearly described and technically feasible
- Medium confidence: The qualitative improvement in librarian workflow efficiency is supported by in-house evaluation, but lacks independent verification or quantitative validation
- Low confidence: Claims about the system's generalizability across different systematic review domains and its ability to handle complex, multi-disciplinary queries without extensive customization

## Next Checks
1. Conduct blind evaluation with multiple independent librarians across different systematic review domains, measuring both qualitative satisfaction and quantitative metrics (precision, recall, F1 scores) compared to manual searches
2. Perform ablation studies to quantify the individual contributions of each component (knowledge graph expansion, ChatGPT query generation, FAISS filtering) to overall system performance
3. Test system robustness by evaluating performance on queries with varying complexity levels, including multi-disciplinary topics and queries requiring nuanced understanding of specialized terminology