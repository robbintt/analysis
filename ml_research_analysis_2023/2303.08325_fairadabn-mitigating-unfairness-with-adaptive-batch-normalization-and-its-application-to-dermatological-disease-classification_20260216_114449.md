---
ver: rpa2
title: 'FairAdaBN: Mitigating unfairness with adaptive batch normalization and its
  application to dermatological disease classification'
arxiv_id: '2303.08325'
source_url: https://arxiv.org/abs/2303.08325
tags:
- uni00000013
- fairness
- fairadabn
- uni00000011
- unfairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FairAdaBN, a framework that improves fairness
  in medical image classification by replacing batch normalization layers with adaptive
  versions that learn subgroup-specific parameters. The method addresses performance
  disparities across demographic subgroups by sharing most network parameters while
  allowing subgroup-specific normalization.
---

# FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification

## Quick Facts
- arXiv ID: 2303.08325
- Source URL: https://arxiv.org/abs/2303.08325
- Reference count: 32
- Key outcome: FairAdaBN achieves the highest FATEEOpp0 score of 48.79×10−2, approximately double that of the next best method, by combining adaptive batch normalization with subgroup-specific parameters and statistical disparity loss

## Executive Summary
FairAdaBN is a novel framework designed to address fairness issues in medical image classification by modifying batch normalization layers to learn subgroup-specific parameters while sharing most network parameters. The method combines adaptive batch normalization with a statistical disparity loss to minimize prediction differences between demographic subgroups. When applied to dermatological disease classification on Fitzpatrick-17k and ISIC 2019 datasets, FairAdaBN demonstrates substantial improvements in fairness metrics while maintaining high accuracy, achieving a FATE score approximately twice that of competing methods.

## Method Summary
FairAdaBN addresses fairness in medical image classification by replacing standard batch normalization layers with adaptive versions that learn subgroup-specific parameters. The framework shares convolutional parameters across all subgroups while allowing different normalization parameters for privileged and unprivileged groups. Additionally, it introduces a statistical disparity loss that minimizes prediction probability differences between subgroups during training. The method is evaluated on two dermatological datasets using a ResNet-152 backbone, with training conducted for 600 epochs using AdamW optimizer and a combined loss function of cross-entropy plus statistical disparity loss.

## Key Results
- FairAdaBN achieves the highest FATEEOpp0 score of 48.79×10−2, approximately double that of the next best method
- The framework improves fairness metrics (EOpp0, EOpp1, EOdd) while maintaining high classification accuracy
- FairAdaBN outperforms existing fairness mitigation methods including Independent models, CFair, ReWeight, and EnD on both Fitzpatrick-17k and ISIC 2019 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairAdaBN mitigates subgroup performance disparity by allowing subgroup-specific normalization parameters while sharing convolution parameters.
- Mechanism: The framework replaces standard batch normalization layers with FairAdaBN, which learns separate affine parameters (gamma and beta) for each subgroup. This allows the network to adapt feature normalization differently for privileged and unprivileged groups while maintaining shared convolutional feature extraction.
- Core assumption: The subgroup performance disparity stems primarily from differences in feature normalization statistics rather than fundamental feature representation differences.
- Evidence anchors:
  - [abstract] "FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups"
  - [section] "FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups"
- Break condition: If the performance disparity is caused by fundamental feature representation differences rather than normalization statistics, this mechanism would be insufficient.

### Mechanism 2
- Claim: The Statistical Disparity Loss (LSD) explicitly minimizes prediction probability differences between subgroups.
- Mechanism: LSD adds a regularization term to the loss function that penalizes differences in prediction probabilities between subgroups for each class, encouraging the model to converge with more equitable predictions across demographic groups.
- Core assumption: The primary source of unfairness is the model's tendency to make different predictions for the same input features depending on subgroup membership.
- Evidence anchors:
  - [abstract] "we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness"
  - [section] "The loss function consists of two parts: (i) the cross-entropy loss, LCE, constraining the prediction precision, and (2) the statistical disparity loss LSD as in Eq. 4, aiming to minimize the difference of prediction probability between subgroups and give extra limits on fairness"
- Break condition: If the prediction disparities are a symptom of deeper feature representation issues rather than direct prediction bias, LSD alone may not resolve the fundamental unfairness.

### Mechanism 3
- Claim: FATE metric provides a balanced evaluation framework that prevents unfair optimization toward either accuracy or fairness alone.
- Mechanism: FATE normalizes fairness improvement relative to accuracy drop, creating a single metric that captures the trade-off between model performance and fairness. This encourages development of methods that improve both simultaneously rather than sacrificing one for the other.
- Core assumption: Traditional fairness metrics that only measure fairness improvement can incentivize methods that dramatically reduce accuracy to achieve fairness gains.
- Evidence anchors:
  - [abstract] "we propose a new metric, named Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness improvement over accuracy drop"
  - [section] "we propose a novel metric for evaluating the Fairness-Accuracy Trade-off Efficiency (FATE), urging researchers to pay attention to the performance and fairness simultaneously when building prediction models"
- Break condition: If stakeholders prioritize different aspects of the trade-off depending on application context, a single normalized metric may not capture the full picture.

## Foundational Learning

- Concept: Batch Normalization mechanics and parameter sharing
  - Why needed here: FairAdaBN modifies batch normalization by introducing subgroup-specific parameters while maintaining shared convolution layers. Understanding BN mechanics is essential to grasp how FairAdaBN works.
  - Quick check question: How does batch normalization typically normalize feature maps, and what parameters does it learn?

- Concept: Fairness metrics (equal opportunity, equalized odds)
  - Why needed here: The paper evaluates fairness using specific metrics like EOpp0, EOpp1, and EOdd. Understanding these metrics is crucial for interpreting results and comparing methods.
  - Quick check question: What's the difference between equal opportunity and equalized odds fairness criteria?

- Concept: Loss function composition and hyperparameter tuning
  - Why needed here: FairAdaBN combines cross-entropy loss with LSD using a hyperparameter α. Understanding how to balance these terms is critical for implementation.
  - Quick check question: How does changing the weight α on the fairness loss affect the balance between accuracy and fairness?

## Architecture Onboarding

- Component map: Input → ResNet-152 backbone → FairAdaBN layers (subgroup-specific parameters) → Output. Loss = Cross-entropy + α × LSD
- Critical path: Input → Convolution layers (shared) → FairAdaBN (subgroup-specific) → ReLU → Output
- Design tradeoffs: FairAdaBN increases parameter count slightly (by subgroup count × BN parameters) but maintains most parameters shared. Requires sensitive attribute information at both training and inference time.
- Failure signatures: If FATE score is negative, the method is reducing accuracy more than improving fairness. If fairness metrics don't improve despite training, the LSD weight may be too low or the subgroup data distribution may be too imbalanced.
- First 3 experiments:
  1. Replace all BN layers in a simple CNN with FairAdaBN and train on balanced data to verify subgroup-specific parameter learning
  2. Train with varying α values to find the optimal balance point on a validation set
  3. Compare FairAdaBN with independent models (Ind) approach on a small dataset to verify parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FairAdaBN's performance generalize to other medical imaging modalities beyond dermatological datasets?
- Basis in paper: [explicit] The authors note that current study only evaluates FairAdaBN on dermatology datasets and its generalization ability on other datasets (chest X-Ray, brain MRI) needs evaluation in future work.
- Why unresolved: The method has only been validated on two dermatological datasets, limiting understanding of its broader applicability.
- What evidence would resolve it: Experimental results demonstrating FairAdaBN's effectiveness on diverse medical imaging tasks and modalities, particularly those mentioned (chest X-ray, brain MRI).

### Open Question 2
- Question: Can FairAdaBN be effectively applied to tasks beyond classification, such as segmentation and detection?
- Basis in paper: [explicit] The authors mention planning to explore unfairness mitigation effectiveness for other universal models and tasks including segmentation and detection.
- Why unresolved: The paper focuses exclusively on classification tasks, leaving uncertainty about performance on other computer vision tasks.
- What evidence would resolve it: Demonstrated successful application of FairAdaBN to segmentation and detection tasks with documented fairness improvements.

### Open Question 3
- Question: What is the optimal method for determining sensitive attributes when they are not readily available in test data?
- Basis in paper: [explicit] The authors identify a limitation that FairAdaBN needs sensitive attributes in test stage, which is unnecessary for other methods like EnD and CFair.
- Why unresolved: The paper acknowledges this as a limitation but doesn't propose solutions for scenarios where sensitive attributes are unavailable during inference.
- What evidence would resolve it: Development and validation of methods to infer or estimate sensitive attributes from input data, or alternative approaches that don't require explicit sensitive attribute knowledge at test time.

## Limitations
- The method requires sensitive attribute information at both training and inference time, which may not be available in real-world deployments
- Current validation is limited to dermatological datasets, with generalization to other medical imaging modalities remaining untested
- The FATE metric, while innovative, may not fully capture the severity of fairness violations in medical contexts where false negatives in certain subgroups could have severe consequences

## Confidence
- **Medium**: The core mechanism of FairAdaBN (subgroup-specific normalization parameters) is well-founded in batch normalization theory, but the claim that this alone substantially addresses fairness disparities may be overstated. The improvement could partially result from the combined effect of FairAdaBN and LSD rather than FairAdaBN alone.
- **Medium**: The statistical disparity loss (LSD) effectively encourages fairness during training, but its generalization to unseen subgroups or when sensitive attribute information is noisy remains unclear. The paper assumes perfect access to sensitive attributes at inference time, which may not hold in real-world deployments.
- **Low**: The FATE metric, while innovative, may not fully capture the complexity of fairness-accuracy trade-offs in medical contexts where false negatives in certain subgroups could have severe consequences. The metric normalizes improvements but doesn't account for the severity of fairness violations.

## Next Checks
1. **Ablation study validation**: Implement FairAdaBN without LSD to isolate the contribution of subgroup-specific normalization parameters versus the statistical disparity loss component.

2. **Robustness testing**: Evaluate FairAdaBN performance when sensitive attribute labels are partially corrupted or when applied to subgroups not seen during training to assess generalization.

3. **Medical consequence analysis**: Beyond FATE scores, analyze the actual clinical impact of fairness improvements by examining false negative rates in underrepresented subgroups and their potential medical consequences.