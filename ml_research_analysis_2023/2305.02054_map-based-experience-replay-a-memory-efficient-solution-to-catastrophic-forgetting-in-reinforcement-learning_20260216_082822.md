---
ver: rpa2
title: 'Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting
  in Reinforcement Learning'
arxiv_id: '2305.02054'
source_url: https://arxiv.org/abs/2305.02054
tags:
- memory
- learning
- replay
- size
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in reinforcement learning,
  where neural networks lose previously learned skills when adapting to new tasks.
  The proposed method, GWR-R, uses a growing self-organizing network (based on the
  Grow-When-Required algorithm) to compress state transitions into a concise graph
  structure, merging similar states into nodes and storing actions/rewards along directed
  edges.
---

# Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.02054
- Source URL: https://arxiv.org/abs/2305.02054
- Reference count: 6
- Primary result: 40-80% memory reduction with minor performance drops across four MuJoCo environments

## Executive Summary
This paper addresses catastrophic forgetting in reinforcement learning by introducing GWR-R, a memory-efficient experience replay method that uses a growing self-organizing network to compress state transitions. The approach merges similar states into nodes and stores actions/rewards along directed edges, reducing memory usage by 40-80% while maintaining comparable performance across four MuJoCo environments. However, this comes at the cost of increased training time (2-22x) due to the computational overhead of maintaining the growing network structure.

## Method Summary
The method uses a Growing-When-Required (GWR) network to create a compressed graph representation of state transitions. Instead of storing full (state, action, reward, next_state) tuples, similar states are merged into nodes, with temporal edges storing averaged action-reward pairs between sequentially activated nodes. The system starts with two random nodes and grows by adding nodes when new states fall outside an activation threshold. During training, random nodes are sampled and transitions are simulated by following temporal edges, with action-reward values averaged using a moving average that weights newer samples more heavily.

## Key Results
- Memory usage reduced by 40-80% compared to standard replay buffers
- Performance maintained within minor degradation across all four MuJoCo environments
- Best results achieved with tuned activation threshold parameter
- Training time increased by 2-22x due to computational overhead

## Why This Works (Mechanism)

### Mechanism 1
Merging similar states into nodes increases pairwise distance among samples, improving training diversity. When similar states are merged into single nodes, larger spatial gaps form between represented states in the compressed graph, increasing minimum distances between stored samples and leading to more diverse mini-batches during training. This assumes the self-organizing network's topology preserves meaningful distance relationships that correlate with state dissimilarity.

### Mechanism 2
Temporal edges store action-reward pairs that can simulate transitions without storing full state transitions. The GWR-R maintains temporal edge matrices (T_A, T_R) that store average actions and rewards between sequentially activated BMUs. When sampling, the system retrieves a random node as the current state and follows a temporal edge to get the next state and associated action-reward pair, simulating the transition without storing the actual next state. This assumes averaged action-reward values along edges adequately represent transition dynamics between merged state nodes.

### Mechanism 3
The growing nature of the network adapts to input space complexity, allocating nodes where needed. The GWR-R starts with two random nodes and grows by adding nodes only when input states fall outside the activation threshold of existing nodes. This creates a compact representation that allocates resources to regions of the state space that are more complex or frequently visited. The activation threshold must appropriately balance compression against representational fidelity.

## Foundational Learning

- Self-organizing maps: Understanding competitive learning and how SOMs map high-dimensional state spaces to lower-dimensional graph structures where similar states cluster together. Quick check: How does BMU selection work in a self-organizing map, and what determines which nodes become neighbors?

- Reinforcement learning fundamentals: Understanding how experience replay works in RL, particularly the role of storing (state, action, reward, next_state) tuples for stable training. Quick check: What problem does experience replay solve in deep RL, and how does uniform sampling from a replay buffer help?

- Catastrophic forgetting: Understanding why neural networks lose previously learned skills when adapting to new tasks, and how correlation in training data exacerbates this problem. Quick check: What causes catastrophic forgetting in neural networks?

## Architecture Onboarding

- Component map: State arrives → Find BMU → Check thresholds → Update existing node or create new one → Process temporal edge → Store (count, action, reward) → Sampling: pick random node → Follow temporal edge → Retrieve full tuple → RL agent trains on batch

- Critical path: State arrives → find BMU → check thresholds → update existing node or create new one → process temporal edge: increment count, update action/reward averages → sampling: pick random node → follow temporal edge probabilistically → retrieve full tuple → RL agent trains on batch → continues interaction

- Design tradeoffs: Memory vs. performance (lower thresholds save memory but reduce performance), Compression vs. fidelity (more merging creates larger state abstractions but loses detail), Computation vs. storage (GWR-R requires more computation but less memory than standard buffers)

- Failure signatures: Training instability if temporal edges contain averaged values that poorly represent actual transitions, Memory inefficiency if thresholds are poorly tuned causing unnecessary network growth, Slow learning if node distribution doesn't match state visitation patterns

- First 3 experiments: 1) Run baseline DDPG with standard replay buffer on InvertedPendulum to establish performance baseline, 2) Run GWR-R with high activation threshold (a_T=0.98) to verify minimal memory savings with baseline performance, 3) Run GWR-R with moderate activation threshold (a_T=0.92) to measure tradeoff between memory reduction and performance impact

## Open Questions the Paper Calls Out
- How does performance degradation scale with memory reduction across different RL algorithms beyond DDPG?
- What is the theoretical upper bound on memory reduction before performance becomes unacceptably poor?
- How does GWR-R's memory efficiency compare to other selective storage methods when implemented with the same RL algorithm?
- Does the increased training time from GWR-R negate its memory efficiency benefits in resource-constrained scenarios?
- How does the error propagation mechanism in GWR-R edges affect long-term policy learning stability?

## Limitations
- Significant computational overhead (2-22x training time increase) may limit practical deployment
- Performance heavily depends on hyperparameter tuning, particularly the activation threshold
- Lacks ablation studies to isolate contributions of state abstraction versus temporal edge modeling

## Confidence
- High confidence in memory reduction claims (40-80%) based on empirical results across four environments
- Medium confidence in performance preservation claims due to minor performance drops requiring careful threshold tuning
- Low confidence in claimed benefits of increased pairwise distance among samples, as theoretical mechanism is not directly validated

## Next Checks
1. Conduct ablation study testing GWR-R variants with temporal edges disabled to isolate impact of state abstraction versus temporal modeling
2. Perform systematic threshold sensitivity analysis across wider range to map full performance-memory tradeoff landscape
3. Test whether compressed graph representations enable better knowledge transfer between related tasks compared to standard replay buffers