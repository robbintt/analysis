---
ver: rpa2
title: Linear Representations of Sentiment in Large Language Models
arxiv_id: '2310.15154'
source_url: https://arxiv.org/abs/2310.15154
tags:
- sentiment
- direction
- logit
- patching
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) represent
  sentiment, a key latent variable in text generation. The authors find that sentiment
  is represented linearly across various models: a single direction in activation
  space captures the feature across tasks, with one extreme for positive and the other
  for negative.'
---

# Linear Representations of Sentiment in Large Language Models

## Quick Facts
- arXiv ID: 2310.15154
- Source URL: https://arxiv.org/abs/2310.15154
- Authors: 40
- Reference count: 40
- Primary result: Sentiment is represented as a single linear direction in activation space across multiple models, with causal relevance demonstrated through ablation experiments

## Executive Summary
This paper investigates how large language models represent sentiment through a combination of experimental methods and circuit analysis. The authors find that sentiment is represented linearly across various models - a single direction in activation space captures the feature across tasks, with one extreme for positive and the other for negative. Through causal interventions like activation patching, they demonstrate this direction is causally relevant for sentiment tasks. The work reveals a "summarization motif" where sentiment is summarized at intermediate positions without inherent sentiment (like punctuation and names), and this summarized sentiment is causally important for final predictions. These findings provide insights into how LLMs create internal representations of sentiment and suggest future directions for understanding model behavior.

## Method Summary
The authors employ a multi-method approach to investigate sentiment representations in LLMs. They create toy datasets (ToyMovieReview and ToyMoodStory) with controlled sentiment expressions and use real-world datasets like Stanford Sentiment Treebank. The sentiment direction is found using K-means clustering, linear regression, DAS (Distributed Alignment Search), and PCA on residual stream activations. Directions are evaluated through visualization, correlation with lexical sentiment, and negation handling. Causal relevance is tested using activation patching and ablation experiments. The analysis focuses primarily on GPT-2 and Pythia models ranging from 85M to 2.8B parameters.

## Key Results
- Sentiment is represented as a single linear direction in activation space across multiple models
- The sentiment direction is causally relevant for sentiment tasks, demonstrated through activation patching
- A "summarization motif" is discovered where sentiment is summarized at intermediate tokens (punctuation, names) rather than solely on emotionally charged words
- Ablating the sentiment direction at comma positions caused a 36% drop in zero-shot classification accuracy on SST, nearly half of the total 76% drop from full ablation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentiment is represented as a single linear direction in activation space.
- Mechanism: A single direction captures the feature across a range of tasks with one extreme for positive and the other for negative.
- Core assumption: The model's residual stream space contains sparse, interpretable linear features.
- Evidence anchors:
  - [abstract]: "we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative."
  - [section 3.2]: "We binned the sentiment activations of OpenWebText tokens from the first residual stream layer of GPT2-small into 20 equal-width buckets and sampled 20 tokens from each. Then we asked GPT-4 to classify into Positive/Neutral/Negative."
  - [corpus]: Weak - only 1 paper directly addressing linear sentiment representations in LLMs.
- Break condition: If sentiment representations are shown to be non-linear or multi-dimensional in similar models.

### Mechanism 2
- Claim: Sentiment summarization occurs at intermediate tokens without inherent sentiment.
- Mechanism: Sentiment is summarized at intermediate positions (e.g., punctuation and names) rather than directly from valenced words to the final output.
- Core assumption: The model uses intermediate tokens as information bottlenecks for sentiment processing.
- Evidence anchors:
  - [abstract]: "we discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarized at intermediate positions without inherent sentiment, such as punctuation and names."
  - [section 4.1]: "we present circuit analyses that give qualitative hints of the summarization motif, and restrict quantitative analysis of the summarization motif to 4.2."
  - [corpus]: Weak - limited literature on summarization motifs in sentiment processing.
- Break condition: If ablation experiments show that intermediate tokens do not significantly affect sentiment predictions.

### Mechanism 3
- Claim: The sentiment direction is causally relevant for sentiment tasks.
- Mechanism: Through causal interventions like activation patching, the sentiment direction is shown to be causally significant in both toy tasks and real-world datasets.
- Core assumption: Linear directions in activation space can be manipulated to causally influence model outputs.
- Evidence anchors:
  - [abstract]: "Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank."
  - [section 3.3]: "We evaluate the sentiment direction using directional patching... We report the best result found across layers."
  - [corpus]: Weak - few studies directly examining causal relevance of linear directions in LLMs.
- Break condition: If causal interventions fail to demonstrate significant effects on sentiment predictions.

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: Understanding how sentiment is represented as a linear direction in activation space.
  - Quick check question: What does it mean for a feature to be represented linearly in a model's activation space?

- Concept: Activation patching
  - Why needed here: Used to determine the causal relevance of the sentiment direction.
  - Quick check question: How does activation patching help in identifying causally relevant model components?

- Concept: Information bottleneck
  - Why needed here: Explains how sentiment is summarized at intermediate tokens.
  - Quick check question: What is an information bottleneck and how does it relate to the summarization motif?

## Architecture Onboarding

- Component map: Token embeddings → residual stream → attention heads → sentiment summarization → final prediction
- Critical path: Token embeddings → residual stream → attention heads → sentiment summarization → final prediction
- Design tradeoffs: Using linear representations for sentiment simplifies interpretation but may miss nuanced sentiment features. Summarization at intermediate tokens reduces direct dependency on valenced words but may lose context.
- Failure signatures: If sentiment direction is not linear, if ablation of intermediate tokens does not affect predictions, or if causal interventions do not show significance.
- First 3 experiments:
  1. Use K-means clustering to find sentiment direction in GPT-2's residual stream and visualize the sentiment activations.
  2. Perform activation patching on the sentiment direction to test its causal relevance in a toy dataset.
  3. Conduct ablation experiments on intermediate tokens to quantify the importance of the summarization motif.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there additional distinct sentiment directions beyond the single bipolar axis found in this study, or can all sentiment representations be explained by this one dimension?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, noting "Did we find a truly universal sentiment direction, or merely the first principal component of directions used across different sentiment tasks?"
- Why unresolved: The study only investigated a single direction and didn't systematically explore the possibility of multiple distinct sentiment features. The heavy-tailed distribution of neuron-out directions with sentiment suggests there might be additional components.
- What evidence would resolve it: Comprehensive experiments using distributed alignment search with multiple dimensions, combined with ablation studies to determine if multiple directions are independently causal for sentiment tasks.

### Open Question 2
- Question: How does the summarization motif change as models scale in context length and capability?
- Basis in paper: [inferred] The authors speculate "our findings suggest that this motif may only increase in relative importance as models grow in context length" but don't test this directly.
- Why unresolved: The study focused on relatively small models (up to 2.8B parameters) and didn't systematically vary context length to test the relationship between summarization importance and model scale.
- What evidence would resolve it: Comparative analysis across a wide range of model sizes and context lengths, measuring the relative importance of summary positions versus original sentiment-bearing phrases.

### Open Question 3
- Question: What is the precise role of MLPs versus attention heads in implementing the sentiment circuit, particularly for handling negations?
- Basis in paper: [explicit] The authors note "we focused almost exclusively on attention heads rather than MLPs" and observed "early results suggest that further investigation of the role of MLPs and individual neurons is likely to yield interesting results."
- Why unresolved: The circuit analysis primarily focused on attention heads, leaving the contribution of MLPs largely unexplored despite evidence that neurons show interpretable relationships with sentiment.
- What evidence would resolve it: Detailed path patching and neuron-level analysis comparing the importance of MLP layers versus attention heads, particularly for negation handling where neurons like L3N1605 appear to play key roles.

## Limitations
- The analysis focuses primarily on GPT-2 and Pythia models without examining other architectures
- The summarization motif evidence is primarily quantitative without detailed mechanistic explanation of how the summarization occurs
- The toy datasets, while controlled, may not capture all real-world sentiment complexities

## Confidence
- **High** confidence: Sentiment is represented as a linear direction in activation space across multiple models, with the direction being causally relevant for sentiment tasks
- **Medium** confidence: The summarization motif - while evidence shows intermediate tokens can influence sentiment predictions, the mechanistic explanation for why this specific pattern emerges remains less certain

## Next Checks
1. Test whether the summarization motif persists when using sentiment-specific templates that avoid punctuation-dependent patterns (e.g., using sentiment adjectives between content words rather than at punctuation boundaries)

2. Perform fine-grained ablation experiments to determine which intermediate tokens (punctuation, names, function words) contribute most to the summarization effect, and whether this varies by model size

3. Investigate whether the sentiment direction found through linear methods captures all sentiment-relevant information by comparing against non-linear probing techniques on the same datasets