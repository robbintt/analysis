---
ver: rpa2
title: Differentially Private Domain Adaptation with Theoretical Guarantees
arxiv_id: '2306.08838'
source_url: https://arxiv.org/abs/2306.08838
tags:
- private
- adaptation
- algorithm
- upriv
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two (\u03B5,\u03B4)-differentially private\
  \ algorithms for supervised domain adaptation, addressing the challenge of leveraging\
  \ publicly available source domain data to improve predictions on a private target\
  \ domain while preserving privacy. The first algorithm is designed for regression\
  \ with linear predictors and solves a joint convex optimization problem over predictor\
  \ parameters and sample weights."
---

# Differentially Private Domain Adaptation with Theoretical Guarantees

## Quick Facts
- arXiv ID: 2306.08838
- Source URL: https://arxiv.org/abs/2306.08838
- Reference count: 40
- Primary result: Introduces two (ε,δ)-differentially private algorithms for supervised domain adaptation that achieve performance close to non-private counterparts for larger target sample sizes or higher privacy budgets.

## Executive Summary
This paper addresses the challenge of leveraging publicly available source domain data to improve predictions on a private target domain while preserving privacy. The authors introduce two algorithms for supervised domain adaptation under (ε,δ)-differential privacy: one for regression with linear predictors and another for more general non-convex settings. Both algorithms solve joint optimization problems over predictor parameters and sample weights, with formal privacy and convergence guarantees. Empirically, the private algorithms achieve performance close to their non-private counterparts, demonstrating the practical viability of private domain adaptation.

## Method Summary
The paper presents two algorithms for differentially private domain adaptation. The first algorithm, PCnvxAdap, is designed for regression with linear predictors and solves a joint convex optimization problem over predictor parameters and sample weights. The second algorithm, PNCnvxAdap, handles more general non-convex settings with Lipschitz and smooth loss functions using a reparameterization technique combined with softmax approximation to enable private optimization. Both algorithms use a reweighting approach that leverages the labeled discrepancy between source and target distributions to find an optimal predictor.

## Key Results
- The convex algorithm achieves error bounds scaling as Õ(d/T + √d/(εT) + d/(nε²)) where d is input dimension, T is iterations, and n is target sample size
- The non-convex algorithm achieves convergence to stationary points with error bounds scaling as Õ(√(d/T) + √d/(εT) + d/(nε²))
- Private algorithms perform close to non-private counterparts when target sample size is large or privacy budget ε is high
- Both algorithms outperform adaptation baselines like discrepancy minimization and kernel mean matching on multiple regression and classification datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reparameterizing the weight vector q as u = 1/q transforms the non-convex adaptation problem into a jointly convex optimization problem in (w, u).
- Mechanism: By introducing u_i = 1/q_i for all sample weights, the quadratic-over-linear structure of the loss term ℓ(w, x, y)²/u_i becomes jointly convex in (w, u). The constraints on u (u_i ≥ m/α for public samples, u_i ≥ n/(1-α) for private samples) are affine, preserving convexity of the feasible set.
- Core assumption: The loss function must be convex in w (e.g., squared loss for regression) for this reparameterization to work.
- Evidence anchors:
  - [abstract]: "after a suitable reparameterization of the weights assigned to the sample losses, the optimization problem for adaptation can be formulated as a joint convex optimization problem over the choice of the predictor and that of the reparameterized weights"
  - [section 4]: "It turns out, however, that we can further enhance the sensitivity if we resort to the reparameterization technique described in Section 4. As we show in the sequel, by applying the transformation of variables ui = 1/qi, i∈[m+n], we are able to reduce the sensitivity of the gradient components, and hence achieve better convergence guarantees."
  - [corpus]: No direct corpus evidence found. The corpus focuses on different privacy mechanisms and does not discuss this specific reparameterization technique.
- Break condition: If the loss function is non-convex in w (e.g., neural network with cross-entropy loss), this mechanism fails and a different approach is needed.

### Mechanism 2
- Claim: Using the softmax approximation for the infinity norm term in the objective function enables gradient-based optimization with theoretical convergence guarantees in the non-convex setting.
- Mechanism: The infinity norm term ||q||_∞ is replaced with its µ-softmax approximation 1/µ log(∑ e^(µ/ui)). This smooth approximation allows the use of gradient-based methods while maintaining convergence to stationary points. The approximation error is bounded by O(log(m+n)/µ), which can be made small by choosing µ appropriately.
- Core assumption: The µ-softmax approximation must be sufficiently smooth (controlled by µ) to enable gradient-based optimization while maintaining accuracy.
- Evidence anchors:
  - [section 5]: "To address this issue, we replace that term with its µ-softmax approximation, namely, 1/µ log(∑m+n_i=1 e^(µ/ui)), where µ > 0 is the softmax approximation parameter."
  - [section 5]: "Thus, our goal is to privately find a stationary point of the following optimization problem: min_{w,u} m+n ∑_i=1 ℓ(w, xi, yi) + dDP 1i≤m/ui + λ1[1−∑m+n_i=1 1/ui] + λ2[∑m+n_i=1 1/u^2_i]^(1/2) + λ∞/µ log(∑m+n_i=1 e^(µ/ui))"
  - [corpus]: No direct corpus evidence found. The corpus neighbors do not discuss softmax approximations in the context of differentially private optimization.
- Break condition: If µ is chosen too small, the approximation error becomes large and degrades performance. If µ is too large, the approximation becomes too sharp and the smoothness benefits are lost.

### Mechanism 3
- Claim: The sensitivity of the gradient with respect to private data is reduced by reparameterizing q as u = 1/q, enabling more efficient privacy-preserving optimization.
- Mechanism: The original weight vector q has gradient sensitivity Ω(1) with respect to private data. By reparameterizing as u = 1/q, the gradient sensitivity of the transformed problem becomes O(1/n) for the public weights and O(1/n²) for the private weights. This reduction in sensitivity allows adding less noise for the same privacy guarantee, improving convergence.
- Core assumption: The transformation u = 1/q must preserve the sensitivity structure in a way that benefits the privacy analysis.
- Evidence anchors:
  - [section 5]: "One issue with the objective when expressed as a function of q = (qPub, qPriv) is that its gradient with respect to qPriv admits an Ω(1) sensitivity. That can be improved by introducing new variables ˜qPub = α/m qPub and ˜qPriv = (1-α)/n qPriv, thereby reducing the sensitivity of the gradient with respect to ˜qPriv to O(1/n)."
  - [section 5]: "It turns out, however, that we can further enhance the sensitivity if we resort to the reparameterization technique described in Section 4. As we show in the sequel, by applying the transformation of variables ui = 1/qi, i∈[m+n], we are able to reduce the sensitivity of the gradient components, and hence achieve better convergence guarantees."
  - [corpus]: No direct corpus evidence found. The corpus neighbors do not discuss gradient sensitivity in the context of reparameterization for differentially private optimization.
- Break condition: If the sample sizes m and n are too small, the reduced sensitivity may not provide meaningful benefits over simpler approaches.

## Foundational Learning

- Concept: Differential Privacy (ε,δ)
  - Why needed here: The entire framework relies on ensuring that the adapted predictor preserves privacy of the target domain data while leveraging public source data. Understanding the formal definition and guarantees of (ε,δ)-differential privacy is essential for analyzing the algorithms.
  - Quick check question: What is the difference between pure ε-differential privacy and (ε,δ)-differential privacy, and when is each appropriate?

- Concept: Domain Adaptation and Discrepancy
  - Why needed here: The paper builds on theoretical results about domain adaptation using discrepancy as a divergence measure. Understanding how discrepancy bounds the difference between source and target distributions is crucial for grasping the optimization problem formulation.
  - Quick check question: How does labeled discrepancy differ from the L1 distance between distributions, and why is it more suitable for adaptation problems?

- Concept: Convex vs Non-convex Optimization
  - Why needed here: The paper presents both a convex algorithm for linear regression and a non-convex algorithm for general settings. Understanding the differences in convergence guarantees and algorithmic approaches for these two cases is essential for implementing the methods correctly.
  - Quick check question: What is the difference between finding a global minimum and finding a stationary point, and why does this distinction matter for the non-convex algorithm?

## Architecture Onboarding

- Component map: Data preprocessing -> Discrepancy estimation -> Parameter reparameterization -> Optimization loop -> Output predictor
- Critical path: Data preprocessing → Discrepancy estimation → Parameter reparameterization → Optimization loop → Output predictor
- Design tradeoffs:
  - Privacy vs accuracy: Higher ε allows less noise but weaker privacy
  - Sample size vs performance: Larger target sample size improves convergence
  - Smoothness vs accuracy: Larger µ in softmax approximation improves smoothness but reduces accuracy
- Failure signatures:
  - Poor performance: Check if target sample size is too small or ε is too restrictive
  - Convergence issues: Verify step sizes are appropriate for the smoothness of the objective
  - Privacy violations: Ensure sensitivity calculations are correct for the reparameterized problem
- First 3 experiments:
  1. Run the convex algorithm on a simple regression dataset (e.g., Wind) with varying ε values to observe the privacy-accuracy tradeoff
  2. Compare the convex algorithm with the non-convex algorithm on a classification dataset (e.g., Adult) to understand the limitations of the convex approach
  3. Test the impact of the softmax approximation parameter µ on the non-convex algorithm's performance by sweeping across different values

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The convex algorithm is restricted to linear predictors and squared loss
- The non-convex algorithm requires Lipschitz and smooth loss functions, limiting applicability to more complex models
- The sensitivity reduction mechanism relies on specific problem structure that may not generalize to all adaptation scenarios

## Confidence

- High confidence in privacy guarantees and theoretical analysis
- Medium confidence in practical performance due to limited empirical evaluation
- Medium confidence in mechanism validity based on mathematical proofs
- Low confidence in scalability to high-dimensional or complex model settings

## Next Checks

1. **Sensitivity Verification**: Implement and verify the gradient sensitivity calculations for both the original and reparameterized problems, ensuring the O(1/n) and O(1/n²) improvements are correctly realized.

2. **Softmax Approximation Impact**: Systematically evaluate how the µ parameter in the softmax approximation affects both privacy budget usage and final accuracy, particularly for datasets with varying sample sizes.

3. **Domain Gap Scenarios**: Test algorithm performance on datasets with varying degrees of domain discrepancy to understand when private adaptation provides meaningful benefits over non-adaptive approaches.