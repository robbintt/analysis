---
ver: rpa2
title: 'NEWTON: Are Large Language Models Capable of Physical Reasoning?'
arxiv_id: '2310.07018'
source_url: https://arxiv.org/abs/2310.07018
tags:
- object
- newton
- language
- reasoning
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NEWTON, a repository and benchmark for evaluating
  physical reasoning abilities of large language models (LLMs). NEWTON includes a
  collection of 2800 object-attribute pairs across 700+ objects and 8 physical attributes,
  along with a pipeline for generating customized evaluation questions.
---

# NEWTON: Are Large Language Models Capable of Physical Reasoning?

## Quick Facts
- arXiv ID: 2310.07018
- Source URL: https://arxiv.org/abs/2310.07018
- Reference count: 14
- Primary result: GPT-4 achieves 50% agreement on NEWTON benchmark vs. 84% for humans

## Executive Summary
NEWTON introduces a comprehensive benchmark for evaluating large language models' physical reasoning capabilities through 160K questions spanning 700+ objects and 8 physical attributes. The benchmark features three progressively challenging tracks: foundational attribute comprehension, explicit application, and implicit scenario-based analysis. While GPT-4 demonstrates strong performance on complex reasoning tasks, it exhibits less consistency than humans in object-attribute reasoning, achieving only 50% agreement compared to human performance of 84%. The platform enables both evaluation and improvement of models through fine-tuning capabilities, with demonstrated effectiveness in enhancing BERT's reasoning performance.

## Method Summary
NEWTON employs a three-track evaluation framework using a repository of 2800 object-attribute pairs with 8 physical attributes. The benchmark generates questions through template-based systems populated with high-confidence annotations filtered using inter-annotator agreement thresholds. Models are evaluated across foundational comprehension tasks, explicit multiple-choice applications, and implicit scenario-based reasoning. The system supports fine-tuning capabilities, demonstrated by improved BERT performance on implicit reasoning tasks after training on explicit attribute questions.

## Key Results
- GPT-4 achieves 50% agreement on NEWTON benchmark vs. 84% human performance
- NEWTON demonstrates potential for both evaluation and enhancement of physical reasoning in LLMs
- Fine-tuning BERT on NEWTON Track 2 questions improves performance on Track 3 implicit reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NEWTON works because it systematically bridges the gap between implicit human knowledge of object attributes and explicit language model understanding through structured crowd-sourced annotations.
- Mechanism: The repository uses a 3-point Likert scale annotation framework to capture extreme opinions on object-attribute pairs, which are then filtered using inter-annotator agreement thresholds (0.75) to ensure high-confidence labels. These annotations populate pre-defined question templates that are automatically filled using condition statements, enabling generation of diverse evaluation prompts.
- Core assumption: Human annotators can reliably agree on object-attribute relationships when provided with clear visual and textual prompts, and language models can benefit from structured, consistent training data.
- Evidence anchors:
  - [section] "Each object-attribute sample has a minimum of four overlapping annotations, the agreement between which is used to filter the annotations and form the NEWTON repository of object-attribute pairs."
  - [section] "A stringent filtering threshold of 0.75 is applied, necessitating at least three out of the four annotations to exhibit agreement."
- Break condition: If inter-annotator agreement falls below the threshold consistently, the repository's quality degrades and the generated questions lose reliability.

### Mechanism 2
- Claim: NEWTON's multi-track benchmark design progressively evaluates language models from basic comprehension to complex scenario-based reasoning.
- Mechanism: The three tracks (Foundational Attribute Comprehension, Explicit Application, Implicit Scenario-Based Analysis) are designed to assess increasing cognitive complexity. Foundational track uses direct attribute-object questions, explicit application tests application of knowledge in Boolean/multiple-choice formats, and implicit analysis requires reasoning without explicit attribute mentions.
- Core assumption: Language models' reasoning abilities can be reliably assessed through structured progression from simple to complex tasks, and the tracks effectively capture different aspects of physical reasoning.
- Evidence anchors:
  - [abstract] "The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks."
  - [section] "These tracks are selected to align with facets within Bloom's cognitive taxonomy, including comprehension, application, and analysis."
- Break condition: If models perform inconsistently across tracks without clear patterns, the progression may not accurately reflect reasoning capabilities.

### Mechanism 3
- Claim: NEWTON enables both evaluation and improvement of language models through fine-tuning capabilities.
- Mechanism: The repository and pipeline allow researchers to use the annotated object-attribute pairs and generated questions not just for testing, but also for fine-tuning pre-trained models. The paper demonstrates that fine-tuning BERT on NEWTON Track 2 questions improves performance on Track 3 implicit reasoning tasks.
- Core assumption: Models can transfer knowledge from explicit attribute reasoning to implicit scenario-based reasoning, and the scale and diversity of NEWTON data is sufficient for effective fine-tuning.
- Evidence anchors:
  - [section] "Figure 3A reveals significant enhancement in language models (e.g., BERT) when NEWTON is part of pre-training, with increasing performance as fine-tuning samples rise."
  - [section] "Fine-tuning focuses on multiple-choice tasks using the base BERT model, initially trained on SWAG."
- Break condition: If fine-tuning on NEWTON data does not generalize to other physical reasoning tasks or if performance plateaus despite increased data, the approach may be limited to the specific dataset characteristics.

## Foundational Learning

- Concept: Object-attribute relationship modeling
  - Why needed here: Understanding how everyday objects possess physical attributes (malleability, elasticity, etc.) is fundamental to evaluating physical reasoning capabilities
  - Quick check question: Can you explain why a rubber band is elastic but a glass bottle is brittle?

- Concept: Inter-annotator agreement and filtering
  - Why needed here: The quality of the NEWTON repository depends on filtering annotations to retain only high-confidence object-attribute pairs
  - Quick check question: If 4 annotators give scores of 1, 1, 3, and 3 for an object-attribute pair, would this pass the 0.75 agreement threshold?

- Concept: Template-based question generation
  - Why needed here: The pipeline's ability to generate diverse questions relies on understanding how condition statements and object slots work together
  - Quick check question: How would you design a template to compare the malleability of two objects using the NEWTON pipeline?

## Architecture Onboarding

- Component map: Repository (crowdsourced annotations) -> Pipeline (filtering and template generation) -> Benchmark (160K questions across 3 tracks)
- Critical path: The most critical path is the annotation-to-filtering-to-template generation flow. Any failure in obtaining high-quality annotations or filtering them correctly will cascade through to the benchmark quality.
- Design tradeoffs: The system trades off between comprehensiveness (8 attributes, 700+ objects) and manageability (3-point scale, minimum 4 annotators per pair). The filtering threshold (0.75) balances quality against data quantity.
- Failure signatures: Low inter-annotator agreement indicates ambiguous object-attribute relationships or unclear prompts. Model performance plateaus across all tracks suggest the benchmark may not be challenging enough or the dataset lacks diversity.
- First 3 experiments:
  1. Run the pipeline on a small subset (10 objects, 2 attributes) to verify the template generation and filtering works correctly
  2. Test model performance on Track 1 questions to establish baseline agreement percentages
  3. Fine-tune a small model on NEWTON Track 2 data and evaluate on Track 3 to verify transfer learning capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific prompting strategies that would yield the highest performance for different language models on the NEWTON benchmark?
- Basis in paper: [explicit] The paper discusses various prompting strategies for different models, including V1, V2, V3, Ensemble, and Instruction methods, and their impact on performance.
- Why unresolved: The paper provides initial findings but does not explore a comprehensive range of prompting strategies or their effectiveness across all models and tasks.
- What evidence would resolve it: Systematic experimentation with a wider variety of prompting strategies, tailored to each model's strengths, and their impact on performance across all tracks of the NEWTON benchmark.

### Open Question 2
- Question: How can the NEWTON benchmark be extended to include more complex physical reasoning tasks, such as those involving dynamic interactions or multi-object scenarios?
- Basis in paper: [explicit] The paper mentions the potential for extending the dataset to a vision-language setting and the need for evaluation schemes that consider specific deployment situations.
- Why unresolved: The current benchmark focuses on static object-attribute pairs and does not address dynamic or multi-object reasoning tasks, which are crucial for real-world applications.
- What evidence would resolve it: Development and integration of new question templates and scenarios that involve dynamic interactions, multi-object reasoning, and real-world complexities into the NEWTON benchmark.

### Open Question 3
- Question: What are the underlying reasons for the inconsistency in language models' performance across different physical attributes, and how can these be addressed?
- Basis in paper: [explicit] The paper highlights inconsistent performance across attributes and identifies common error modes such as hallucination, conservatism, and misunderstanding.
- Why unresolved: The paper does not delve into the root causes of these inconsistencies or propose methods to mitigate them.
- What evidence would resolve it: Detailed analysis of model architectures, training data, and fine-tuning techniques to understand and address the specific challenges in reasoning about different physical attributes.

## Limitations
- The 0.75 inter-annotator agreement threshold may exclude potentially valuable object-attribute pairs where human consensus is naturally lower
- Human performance baseline represents only a subset of questions, limiting generalizability
- Focus on 8 specific physical attributes may not capture the full spectrum of physical reasoning capabilities

## Confidence
- High confidence: The repository creation methodology and basic pipeline functionality
- Medium confidence: The three-track benchmark design effectively captures progressive reasoning complexity
- Medium confidence: Fine-tuning on NEWTON data transfers to improved implicit reasoning performance

## Next Checks
1. **Cross-dataset generalization test**: Evaluate whether models trained on NEWTON perform better on external physical reasoning datasets like PIQA or PROST to verify transfer learning effectiveness beyond the benchmark itself.

2. **Annotation consistency audit**: Re-annotate a random sample of 100 object-attribute pairs with fresh annotators to verify the original 0.75 agreement threshold maintains consistent quality standards and doesn't exclude borderline but valid pairs.

3. **Attribute coverage expansion**: Test model performance on a broader set of physical attributes (e.g., conductivity, magnetism) to determine if the current 8-attribute focus captures sufficient physical reasoning diversity or represents an arbitrary constraint.