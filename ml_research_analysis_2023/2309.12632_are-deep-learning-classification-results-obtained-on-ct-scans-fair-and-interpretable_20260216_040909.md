---
ver: rpa2
title: Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?
arxiv_id: '2309.12632'
source_url: https://arxiv.org/abs/2309.12632
tags:
- deep
- images
- nodule
- training
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability and interpretability of
  deep learning models for lung nodule classification in CT scans, highlighting the
  importance of patient-wise data separation. It compares unfair data splitting (random
  image shuffling) with fair splitting (strict patient isolation), revealing that
  unfair methods lead to overfitting and misleading high accuracy rates.
---

# Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?

## Quick Facts
- arXiv ID: 2309.12632
- Source URL: https://arxiv.org/abs/2309.12632
- Reference count: 40
- One-line primary result: Patient-wise data splitting significantly improves both performance and interpretability of deep learning models for lung nodule classification in CT scans.

## Executive Summary
This study investigates the impact of data splitting strategies on deep learning models for lung nodule classification in CT scans. It demonstrates that traditional random image shuffling leads to overfitting and misleading high accuracy rates, while patient-wise data splitting ensures both better performance on unseen data and improved interpretability. The research uses three DNN architectures (MobileNetV2, EfficientNet, and VGG16) and introduces interpretability metrics based on attention heat maps and correlation analysis.

## Method Summary
The study employs three pre-trained DNN architectures (MobileNetV2, EfficientNetB0, and VGG16) to classify lung nodules as benign or malignant. Data augmentation techniques are applied to balance the dataset, and both unfair (random image shuffling) and fair (patient-wise data splitting) training methods are compared. Monte Carlo Cross-Validation is used to evaluate model performance and interpretability across multiple train/validation/test partitions.

## Key Results
- Unfair data splitting leads to overfitting and inflated accuracy rates that don't generalize to new patient data
- Fair patient-wise splitting maintains consistent accuracy rates across test and challenge datasets
- Heat-map visualizations show fair models focus better on actual nodule regions compared to unfair models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair patient-wise data splitting prevents information leakage between training and test sets, leading to more generalizable models.
- Mechanism: When images from the same patient appear in both training and test sets (unfair splitting), the model learns patient-specific features rather than generalizable nodule characteristics. Fair splitting ensures each patient's data appears in only one set, forcing the model to learn features that generalize across patients.
- Core assumption: Patient-specific imaging characteristics (scanner settings, positioning, etc.) can be inadvertently learned as predictive features when data is unfairly split.
- Evidence anchors:
  - [abstract]: "most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets."
  - [section]: "All of these manuscripts report that the whole image lot was shuffled completely, and train-test separation was done randomly. In such a case, it is perfectly possible that some images from the same patient scan may go to the training set, while the rest may go to the test set, making an unfair splitting that is prone to overfitting with too high accuracy results."

### Mechanism 2
- Claim: Fair splitting improves model interpretability by ensuring attention focuses on nodule regions rather than patient-specific artifacts.
- Mechanism: Unfairly trained models learn to focus on irrelevant features (patient-specific artifacts) because these features correlate with the target in the training data. Fair splitting prevents this, forcing the model to learn and attend to actual nodule characteristics.
- Core assumption: Interpretability (as measured by heat maps) reflects genuine focus on diagnostic features rather than artifacts.
- Evidence anchors:
  - [abstract]: "Heat-map visualizations of the activations of the deep neural networks trained with strict patient-level separation indicate a higher degree of focus on the relevant nodules."
  - [section]: "The heat map in Figure 5-b shows that the regional activation (hence visual attention) of the fair model is high at and around the actual tumor region. On the other hand, Figure 5-c, which shows the heat map from the unfair model, indicates no visual focus on the locations around the tumor region."

### Mechanism 3
- Claim: Fair splitting prevents misleading accuracy reports by ensuring test performance reflects true generalization ability.
- Mechanism: Unfair splitting leads to inflated accuracy scores because the model has already seen similar images during training. Fair splitting provides a more realistic assessment of performance on truly unseen data.
- Core assumption: Test accuracy on fair splits is a more reliable indicator of real-world performance than test accuracy on unfair splits.
- Evidence anchors:
  - [abstract]: "Whenthedeepneuralnetworkstrainedonthetraditional,unfairdatashufflingmethodarechallenged with new patient images, it is observed that the trained models perform poorly. In contrast, deep neural networks trained with strict patient-level separation maintain their accuracy rates even when new patient images are tested."
  - [section]: "The obvious observation is that the reported test accuracies (left-side column) of the unfairly trained network are far from being valid for the challenge set (right-side column), whereas the performance of the fair-trained network is totally consistent with the reported test accuracies."

## Foundational Learning

- Concept: Patient-wise data splitting
  - Why needed here: To prevent information leakage between training and test sets in medical imaging datasets where multiple images come from the same patient.
  - Quick check question: If a patient has 100 CT images, how many of these should appear in the training set if we're doing strict patient-wise splitting?

- Concept: Heat map interpretability
  - Why needed here: To visualize where the model is focusing its attention when making predictions, allowing assessment of whether it's looking at relevant features.
  - Quick check question: What does a heat map show when overlaid on a medical image?

- Concept: Monte Carlo Cross-Validation
  - Why needed here: To perform patient-wise splitting while still using multiple train/validation/test partitions for robust evaluation.
  - Quick check question: How does Monte Carlo Cross-Validation differ from standard k-fold cross-validation in the context of patient-wise splitting?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture (MobileNetV2/EfficientNet/VGG16) -> Training pipeline -> Evaluation -> Interpretability analysis
- Critical path: Data splitting → Model training → Heat map generation → Interpretability analysis → Performance evaluation
- Design tradeoffs:
  - Model complexity vs interpretability: Simpler models may be more interpretable but less accurate
  - Data augmentation vs overfitting: More augmentation helps with small datasets but may introduce artifacts
  - Monte Carlo iterations vs computation time: More iterations provide better estimates but increase training time
- Failure signatures:
  - High training accuracy but low test/challenge accuracy: Indicates overfitting, likely due to unfair splitting
  - Heat maps focusing on non-nodule regions: Indicates the model is learning irrelevant features
  - Large discrepancy between test and challenge accuracy: Suggests the model isn't generalizing well
- First 3 experiments:
  1. Implement unfair splitting baseline: Train each architecture with random image shuffling and evaluate on test and challenge sets
  2. Implement fair splitting: Train each architecture with patient-wise splitting using Monte Carlo Cross-Validation and evaluate
  3. Interpretability analysis: Generate heat maps for both fair and unfair models on a subset of test images and calculate correlation metrics between heat maps and ground truth nodule regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does patient-wise data splitting affect the generalization ability of deep learning models for lung nodule classification in real-world clinical settings?
- Basis in paper: [explicit] The paper highlights the importance of patient-wise data splitting and its impact on the reliability and interpretability of deep learning models.
- Why unresolved: While the paper demonstrates improved performance with patient-wise splitting in controlled experiments, it does not provide evidence of how these models perform in real-world clinical settings with diverse patient populations.
- What evidence would resolve it: Clinical trials or studies evaluating the performance of models trained with patient-wise splitting in real-world clinical settings.

### Open Question 2
- Question: What are the specific mechanisms by which patient-wise data splitting improves the interpretability of deep learning models for lung nodule classification?
- Basis in paper: [explicit] The paper suggests that patient-wise data splitting enhances interpretability by improving the model's focus on nodule regions, as evidenced by heat-map visualizations.
- Why unresolved: The paper does not delve into the underlying mechanisms that explain why patient-wise splitting leads to better interpretability.
- What evidence would resolve it: Further research investigating the internal workings of models trained with patient-wise splitting to identify the specific mechanisms that improve interpretability.

### Open Question 3
- Question: How do different deep learning architectures (e.g., MobileNetV2, EfficientNet, VGG16) compare in terms of their ability to generalize and maintain interpretability when trained with patient-wise data splitting?
- Basis in paper: [explicit] The paper compares three deep learning architectures and their performance with patient-wise data splitting.
- Why unresolved: While the paper provides a comparison, it does not explore the reasons behind the differences in performance and interpretability among the architectures.
- What evidence would resolve it: Comparative studies analyzing the internal structures and learning processes of different architectures when trained with patient-wise splitting.

## Limitations

- The study focuses specifically on lung nodule classification in CT scans, and findings may not generalize to other medical imaging tasks or modalities
- The interpretability analysis relies on heat maps, which may not fully capture the model's decision-making process
- The study does not explore the computational overhead of patient-wise splitting compared to traditional methods

## Confidence

- High confidence in the mechanism of unfair splitting causing overfitting due to patient-specific information leakage
- Medium confidence in the generalizability of fair splitting benefits across different medical imaging tasks
- Medium confidence in the interpretability metrics based on heat maps, as the correlation between visual attention and actual decision-making process is not fully validated

## Next Checks

1. Test the fair splitting approach on a different medical imaging dataset (e.g., brain tumor classification) to assess generalizability
2. Implement additional interpretability metrics beyond heat maps (e.g., feature importance scores, ablation studies) to validate the findings
3. Conduct ablation studies to isolate the impact of patient-specific imaging characteristics from other potential sources of variance in the dataset