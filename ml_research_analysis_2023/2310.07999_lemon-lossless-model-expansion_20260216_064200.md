---
ver: rpa2
title: 'LEMON: Lossless model expansion'
arxiv_id: '2310.07999'
source_url: https://arxiv.org/abs/2310.07999
tags:
- expansion
- lossless
- training
- learning
- expanded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEMON introduces a method to initialize and train scaled models
  using pre-trained smaller models, reducing training time compared to random initialization.
  It employs lossless model expansion compatible with various network structures,
  including Vision Transformers and BERT, and an optimized learning rate scheduler.
---

# LEMON: Lossless model expansion

## Quick Facts
- **arXiv ID**: 2310.07999
- **Source URL**: https://arxiv.org/abs/2310.07999
- **Reference count**: 40
- **Primary result**: LEMON enables lossless expansion of pre-trained models with up to 56.7% computational cost savings compared to training from scratch.

## Executive Summary
LEMON introduces a method to initialize and train scaled models using pre-trained smaller models, reducing training time compared to random initialization. The approach employs lossless model expansion compatible with various network structures, including Vision Transformers and BERT, and uses an optimized learning rate scheduler. By preserving the exact functional mapping of smaller models while expanding to larger architectures, LEMON achieves significant computational cost savings while maintaining model performance.

## Method Summary
LEMON uses lossless model expansion through weight replication with symmetry breaking, where duplicated neurons receive asymmetric fan-out weights to ensure unique contributions. The method supports both divisible and indivisible width expansions, using circular replication for the former and average expansion for the latter. An optimized learning rate scheduler with faster decay is applied, taking advantage of the smaller model's knowledge to accelerate convergence. The approach is compatible with various network structures including Vision Transformers and BERT, and demonstrates computational cost savings of up to 56.7% for Vision Transformers and 33.2% for BERT compared to training from scratch.

## Key Results
- Achieves computational cost savings of up to 56.7% for Vision Transformers and 33.2% for BERT compared to training from scratch
- Maintains lossless expansion preserving exact functional mapping of smaller models in larger architectures
- Compatible with indivisible width and depth increments, supporting various network structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lossless expansion preserves the exact functional mapping of the smaller model in the larger model, enabling faster convergence during training.
- Mechanism: The method uses circular replication and asymmetric fan-out weight initialization to break symmetry in duplicated neurons, ensuring each neuron contributes uniquely to the model's learning.
- Core assumption: The choice of asymmetric weights (α ≠ β) for replicated neurons is sufficient to break symmetry in nonlinear activation functions.
- Evidence anchors:
  - [abstract] LEMON introduces a method to initialize and train scaled models using pre-trained smaller models, reducing training time compared to random initialization.
  - [section] Out of simplicity, we use a single-hidden-layer MLP for illustration... we can set the expanded fan-out weights to be αv1,1 and βv1,1 where α + β = 1 to ensure lossless expansion.
- Break condition: If the nonlinear activation function is linear or if the asymmetric weights do not sufficiently differentiate neuron outputs, symmetry breaking may fail.

### Mechanism 2
- Claim: Optimized learning rate scheduling accelerates the training of expanded models by decaying the learning rate faster than training from scratch.
- Mechanism: Expanded models start with a smaller training loss due to inherited knowledge, so a faster decaying learning rate scheduler is used to maintain optimization progress without overshooting the local minimum.
- Core assumption: The expanded model's initial loss proximity to the local minimum justifies a faster learning rate decay to prevent loss of generalization.
- Evidence anchors:
  - [abstract] This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch.
  - [section] We observe that maintaining the default maximum learning rate is pivotal to recovering the performance of the large model.
- Break condition: If the initial loss of the expanded model is not significantly lower than training from scratch, a faster decay might hinder convergence.

### Mechanism 3
- Claim: Average expansion for LayerNorm layers enables lossless expansion even when the width increment is not divisible.
- Mechanism: By expanding the input to LayerNorm with its average and adjusting the scaling factor, the output can be zero-padded without loss of information.
- Core assumption: The LayerNorm layer can handle the expanded input without introducing errors, provided the scaling factor is correctly adjusted.
- Evidence anchors:
  - [section] We now show that if the input of LayerNorm is average expanded, lossless width expansion is possible.
  - [section] Define average expanded of x ∈ RDS to be x∗ ∈ RDT. It can be shown that LN(x∗; µ∗, b∗) = Concat [LN(x; µ, b), 0] if µ∗ = Concat [ηµ, ζ] and b∗ = Concat [b, 0].
- Break condition: If the LayerNorm implementation deviates from the standard definition or if the scaling factor is incorrectly computed, lossless expansion may not hold.

## Foundational Learning

- Concept: Linear Algebra (Matrix Operations, Vector Spaces)
  - Why needed here: Understanding how to manipulate and expand weight matrices and vectors is crucial for implementing lossless expansion.
  - Quick check question: Can you explain how circular replication of a vector works and why it's used in this context?

- Concept: Neural Network Architecture (Transformers, LayerNorm, Multi-Head Attention)
  - Why needed here: Familiarity with the components of Transformer models is necessary to apply the expansion methods correctly.
  - Quick check question: What is the role of LayerNorm in a Transformer block, and how does its position affect the model's behavior?

- Concept: Optimization and Learning Rate Scheduling
  - Why needed here: Knowledge of how learning rate affects model convergence and generalization is important for applying the optimized scheduler.
  - Quick check question: How does a learning rate that decays too quickly or too slowly impact the training of a neural network?

## Architecture Onboarding

- Component map: Embedding Layer → LayerNorm → MHA → MLP → Decoder Layer
- Critical path: Embedding Layer → LayerNorm → MHA → MLP → Decoder Layer
- Design tradeoffs:
  - Symmetry Breaking: Using asymmetric weights vs. introducing noise
  - Learning Rate Scheduler: Faster decay vs. standard decay
  - Width Increment: Divisible vs. indivisible increments require different expansion methods
- Failure signatures:
  - Model performance does not improve or degrades compared to training from scratch
  - Training loss does not decrease as expected
  - The expanded model does not converge within the expected number of epochs
- First 3 experiments:
  1. Expand a small Vision Transformer to a larger one and compare training time and accuracy with training from scratch
  2. Test the effect of different learning rate schedules on the expanded model's performance
  3. Verify lossless expansion by checking if the expanded model maintains the same output as the smaller model on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LEMON be extended to larger models beyond the scale tested in this paper?
- Basis in paper: [explicit] The paper mentions "Looking ahead, we are working on extending the application of LEMON to larger models"
- Why unresolved: The current paper only evaluates LEMON on relatively small models like ViT(6, 512) to ViT(12, 768) and BERT(6, 384) to BERT(12, 768). The effectiveness and efficiency of LEMON on truly large-scale models like GPT-3 or GPT-4 remain untested.
- What evidence would resolve it: Empirical results demonstrating LEMON's performance on significantly larger models, including computational cost savings and preservation of model quality, would provide the necessary evidence.

### Open Question 2
- Question: What is the optimal method for selecting the free parameters when initializing LEMON?
- Basis in paper: [explicit] The paper states "we are working on developing methodologies for selecting optimal free parameters when initializing LEMON"
- Why unresolved: While the paper provides some guidance on parameter choices for specific experiments, it acknowledges that a more systematic approach to selecting these parameters is needed, especially for different model architectures and scales.
- What evidence would resolve it: A comprehensive study or theoretical framework that establishes guidelines for choosing optimal free parameters based on model characteristics and training objectives would address this question.

### Open Question 3
- Question: How does the performance of LEMON compare to LiGO when both methods are applied to the same model architectures?
- Basis in paper: [explicit] The paper mentions "LiGO (Wang et al., 2023a) is unavailable for direct comparison due to the absence of open-source code" and provides a comparison with reported values
- Why unresolved: The paper only provides a limited comparison with LiGO based on reported values, and the comparison is not on identical model architectures (LEMON is tested on Pre-LN Transformers while LiGO results are mainly for Post-LN BERT and RoBERTa). A direct, head-to-head comparison using the same model architectures and datasets would provide a more accurate assessment of their relative performance.
- What evidence would resolve it: Implementing both LEMON and LiGO on the same set of model architectures and datasets, followed by a comprehensive evaluation of their performance and computational efficiency, would provide the necessary evidence.

## Limitations

- The paper lacks precise parameter guidelines for asymmetric weight initialization, potentially leading to inconsistent results across different model architectures
- Claims of "lossless" expansion require careful validation as small implementation variations could introduce subtle errors
- The generalization claims across different architectures and tasks require further validation, particularly for models with complex layer interactions

## Confidence

- **High Confidence**: The core mathematical framework for lossless expansion using circular replication and average expansion is well-established and reproducible
- **Medium Confidence**: The computational cost savings claims are supported by experiments but depend heavily on specific hardware configurations and implementation details
- **Low Confidence**: The generalization claims across different architectures and tasks require further validation, particularly for models with complex layer interactions

## Next Checks

1. Implement multiple runs with different random seeds for the asymmetric weight initialization and measure the variance in final model performance to quantify sensitivity to initialization choices

2. Create a comprehensive test suite comparing outputs of expanded models against their smaller counterparts across various input distributions, including edge cases and adversarial examples, to verify the lossless property holds under all conditions

3. Apply LEMON to non-Transformer architectures (e.g., convolutional networks, RNNs) with varying depth and width ratios to evaluate the method's generalizability beyond the demonstrated use cases