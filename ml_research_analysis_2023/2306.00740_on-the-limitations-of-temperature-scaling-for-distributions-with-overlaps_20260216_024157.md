---
ver: rpa2
title: On the Limitations of Temperature Scaling for Distributions with Overlaps
arxiv_id: '2306.00740'
source_url: https://arxiv.org/abs/2306.00740
tags:
- class
- data
- training
- confidence
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a significant hurdle to model calibration
  in deep learning: trained neural networks exhibit large regions of near-certain
  confidence around their training data points. This phenomenon persists across diverse
  architectures and datasets, even when models are trained to zero training error.'
---

# On the Limitations of Temperature Scaling for Distributions with Overlaps
## Quick Facts
- arXiv ID: 2306.00740
- Source URL: https://arxiv.org/abs/2306.00740
- Reference count: 36
- The paper identifies that trained neural networks exhibit large regions of near-certain confidence around their training data points, making standard calibration techniques like temperature scaling ineffective for distributions with overlapping class supports.

## Executive Summary
This paper investigates a fundamental limitation in model calibration: deep neural networks trained to zero training error develop large regions of near-certain confidence around training data points. This phenomenon persists across diverse architectures and datasets, and the authors prove that for data distributions with overlapping class supports, this behavior makes it impossible to achieve good calibration using standard post-training techniques like temperature scaling. The authors show that Mixup data augmentation can mitigate this issue by constraining model behavior away from training points, leading to significantly better calibration.

## Method Summary
The authors downsample CIFAR-10, CIFAR-100, and SVHN datasets to 5000 training points and train models using both ERM and Mixup until achieving zero training error. They compute OCNN (Other-Class Nearest Neighbor) distances for each training point, then sample points at various distances from training points to measure mean confidence. Temperature scaling is applied to evaluate calibration performance, and the results are compared between ERM and Mixup training methods.

## Key Results
- Trained neural networks exhibit large regions of near-certain confidence around their training data points, even when trained to zero training error
- For data distributions with overlapping class supports, temperature scaling cannot achieve good calibration regardless of access to the true distribution
- Mixup data augmentation significantly improves calibration by constraining model behavior away from training points

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Deep neural networks trained to zero training error develop large regions of near-certain confidence around each training point
- Mechanism: When minimizing cross-entropy loss, the model drives softmax outputs to 1 for correct classes at training points, creating a plateau of high confidence in the neighborhood
- Core assumption: The model has sufficient capacity to interpolate the training data perfectly (zero training error)
- Evidence anchors:
  - [abstract] "trained neural networks exhibit large regions of near-certain confidence around their training data points"
  - [section] "deep neural networks have large neighborhoods of almost certain confidence around their training data points"
  - [corpus] Weak evidence: No direct citations found in corpus papers about this specific phenomenon
- Break condition: If the model cannot achieve zero training error (e.g., insufficient capacity, strong regularization), the phenomenon disappears

### Mechanism 2
- Claim: Temperature scaling cannot fix calibration when classes have overlapping supports because the model is uniformly confident in overlapping regions
- Mechanism: In overlapping regions, the true posterior assigns probability to multiple classes, but the model assigns near-1 probability to a single class everywhere in the neighborhood. Temperature scaling can only flatten the softmax uniformly
- Core assumption: The data distribution has class supports with significant overlap
- Evidence anchors:
  - [abstract] "for data distributions with overlapping class supports, this behavior makes it impossible to achieve good calibration using standard post-training techniques like temperature scaling"
  - [section] "we prove that for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random"
  - [corpus] No direct citations found in corpus papers about temperature scaling limitations with overlapping classes
- Break condition: If class supports are disjoint or have negligible overlap, temperature scaling may still work

### Mechanism 3
- Claim: Mixup training constrains model behavior away from training points, preventing uniform confidence and enabling better calibration
- Mechanism: Mixup trains the model on convex combinations of data points, forcing it to interpolate between predictions. This creates a "buffer zone" around training points where confidence is lower and more appropriate for overlapping regions
- Core assumption: The mixing distribution D_λ is continuous and covers a reasonable range of interpolation weights
- Evidence anchors:
  - [abstract] "training with Mixup data augmentation can mitigate this issue by constraining model behavior away from training points, leading to significantly better calibration"
  - [section] "it is possible to circumvent this defect by changing the training process to use a modified loss based on the Mixup data augmentation technique"
  - [corpus] Weak evidence: One related paper mentions "Adaptive Set-Mass Calibration with Conformal Prediction" but doesn't discuss Mixup specifically
- Break condition: If the mixing distribution is degenerate (e.g., always λ=1) or if the model capacity is too low to handle mixed inputs

## Foundational Learning
- Concept: KL divergence and its role in calibration
  - Why needed here: The paper uses expected KL divergence as the miscalibration metric, which measures how far the model's predictive distribution is from the true conditional distribution
  - Quick check question: If a model assigns probability 0.9 to the correct class when it should be 0.5, what is the KL divergence contribution from that sample?

- Concept: Temperature scaling as a post-hoc calibration method
  - Why needed here: The paper shows temperature scaling fails in certain cases, so understanding how it works is crucial for appreciating the limitation
  - Quick check question: How does temperature scaling modify the softmax output, and what parameter does it introduce?

- Concept: Empirical Risk Minimization (ERM) vs modified training objectives
  - Why needed here: The paper contrasts ERM (which leads to uniform confidence) with Mixup-based training (which avoids it)
  - Quick check question: What is the key difference between the ERM objective and the Mixup objective in terms of what the model is trained to predict?

## Architecture Onboarding
- Component map: Data pipeline -> Model zoo -> Training loop -> Evaluation -> Calibration analysis
- Critical path:
  1. Train models using ERM and Mixup on subsampled datasets
  2. Compute OCNN distances for each training point
  3. Sample points at various distances from training points
  4. Measure mean confidence in neighborhoods
  5. Apply temperature scaling and evaluate calibration
  6. Compare ERM vs Mixup performance

- Design tradeoffs:
  - Subsampling vs full dataset: Subsampling reduces computational cost but may miss rare phenomena
  - Fixed radius vs OCNN-proportional radius: OCNN-proportional gives more meaningful comparisons across points
  - Number of samples per neighborhood: More samples give better estimates but increase computation

- Failure signatures:
  - Uniform confidence: Mean confidence stays near 1 across a wide range of neighborhood sizes
  - Temperature scaling failure: Calibration metrics don't improve after temperature scaling
  - Mixup effectiveness: Confidence decreases in neighborhoods and calibration improves

- First 3 experiments:
  1. Train ResNet-18 on CIFAR-10 using ERM, compute OCNN distances, measure confidence at 0.1, 0.2, 0.3 OCNN distances
  2. Repeat experiment 1 with Mixup training (α=1.0), compare confidence profiles
  3. Apply temperature scaling to ERM model, measure change in calibration metrics (ECE, KL divergence)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the theoretical properties of the OCNN (Other-Class Nearest Neighbor) distance and how does it relate to model calibration?
- Basis in paper: [explicit] The authors introduce OCNN distance as a metric to measure the distance from a data point to the nearest point of a different class, and use it to analyze model confidence behavior around training points
- Why unresolved: While the paper uses OCNN distance empirically to demonstrate uniform confidence behavior, it doesn't provide theoretical analysis of this metric or establish formal connections to calibration performance
- What evidence would resolve it: A theoretical framework that relates OCNN distance distribution to calibration metrics, or empirical studies showing correlation between OCNN distance properties and calibration performance across different datasets and architectures

### Open Question 2
- Question: Can regularization techniques beyond Mixup effectively mitigate the uniform confidence phenomenon?
- Basis in paper: [inferred] The paper demonstrates that Mixup training can prevent uniform confidence behavior, but doesn't systematically explore other regularization methods
- Why unresolved: The paper focuses specifically on Mixup as a solution, but there may be other training modifications or architectural choices that could similarly constrain model behavior away from training points
- What evidence would resolve it: Comparative studies of various regularization techniques (data augmentation, architectural constraints, loss modifications) showing their effectiveness in preventing uniform confidence while maintaining accuracy

### Open Question 3
- Question: How does the uniform confidence phenomenon scale with dataset size and input dimensionality?
- Basis in paper: [explicit] The authors acknowledge computational constraints limited their experiments to downsampled datasets and discuss this as a limitation
- Why unresolved: While the authors show uniform confidence persists at full scale for CIFAR datasets, they don't explore how this phenomenon behaves on larger datasets or in higher-dimensional spaces
- What evidence would resolve it: Systematic experiments on larger-scale datasets (ImageNet, etc.) and in higher dimensions (text, audio) showing whether uniform confidence scales similarly or exhibits different characteristics

## Limitations
- The theoretical analysis assumes perfect zero-training-error models, which may not hold in practical scenarios with noisy labels or regularization
- Empirical evaluation focuses primarily on image classification benchmarks with artificially introduced label noise, limiting generalizability to other domains
- The paper doesn't extensively explore the computational overhead of Mixup training or investigate optimal mixing coefficients for different dataset characteristics

## Confidence
- High confidence: The existence of uniform confidence regions around training points (supported by extensive experiments)
- Medium confidence: Temperature scaling's fundamental limitations with overlapping classes (strong theoretical proof but limited empirical breadth)
- Medium confidence: Mixup's effectiveness in mitigating calibration issues (demonstrated empirically but with unexplored hyperparameters)

## Next Checks
1. Test Mixup's effectiveness on non-image domains (text, tabular data) with naturally overlapping class distributions to assess generalizability
2. Investigate the relationship between dataset size, label noise levels, and Mixup's calibration benefits through systematic ablation studies
3. Compare Mixup against alternative training strategies like label smoothing or confidence penalties for calibration improvement