---
ver: rpa2
title: Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear
  Subspaces
arxiv_id: '2303.00783'
source_url: https://arxiv.org/abs/2303.00783
tags:
- data
- adversarial
- training
- network
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the vulnerability of neural networks to adversarial
  examples in directions orthogonal to a low-dimensional data manifold. Focusing on
  two-layer ReLU networks, it proves that training on data lying on a low-dimensional
  linear subspace results in large gradients in the orthogonal directions, making
  the network susceptible to small adversarial perturbations.
---

# Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces

## Quick Facts
- **arXiv ID:** 2303.00783
- **Source URL:** https://arxiv.org/abs/2303.00783
- **Reference count:** 40
- **Key outcome:** Training two-layer ReLU networks on data confined to low-dimensional linear subspaces creates large gradients orthogonal to the data manifold, enabling small adversarial perturbations of size O(√d/ℓ).

## Executive Summary
This paper reveals a fundamental vulnerability in neural networks: when trained on data lying on a low-dimensional linear subspace, two-layer ReLU networks develop large gradients in directions orthogonal to that subspace. This makes the networks susceptible to small adversarial perturbations in these orthogonal directions. The gradient magnitude scales as √(kℓ/m) where k is the fraction of active neurons, ℓ is the subspace dimension, and m is the total number of neurons. The paper proves that adversarial perturbations of size O(√d/ℓ) exist in these orthogonal directions. Crucially, the authors show that reducing initialization scale or adding L2 regularization can mitigate this vulnerability by shrinking the weight projections onto the orthogonal subspace.

## Method Summary
The authors analyze two-layer ReLU networks trained on synthetic datasets where data points lie exactly on low-dimensional linear subspaces embedded in higher-dimensional space. They use gradient-based training with logistic loss to find decision boundaries, then mathematically characterize the gradient in directions orthogonal to the data manifold. The analysis considers Kaiming initialization for the first layer and fixed ±1/√m initialization for the second layer. They prove both upper and lower bounds on the gradient magnitude and demonstrate the existence of universal adversarial perturbations that work for any input from the subspace. The theoretical analysis is validated through experiments on synthetic datasets with varying subspace dimensions and network widths.

## Key Results
- The gradient norm in directions orthogonal to the data manifold scales as √(kℓ/m) where ℓ is the subspace dimension and m is the network width
- Adversarial perturbations of size O(√d/ℓ) exist in these orthogonal directions, with direction independent of the specific input
- Decreasing initialization scale (β = 1/d) or adding L2 regularization reduces the orthogonal gradient magnitude and improves robustness
- The vulnerability is most pronounced when the subspace dimension ℓ is small relative to the ambient dimension d

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large gradient in directions orthogonal to the data manifold enables small adversarial perturbations.
- Mechanism: Training a two-layer ReLU network on data confined to a low-dimensional linear subspace results in large gradients in the orthogonal subspace. The gradient norm scales as √(kℓ/m) where k is the fraction of active neurons, ℓ is the subspace dimension, and m is the total neurons.
- Core assumption: Data lies exactly on a linear subspace P of dimension d-ℓ.
- Evidence anchors:
  - [abstract] "training on data lying on a low-dimensional linear subspace results in large gradients in the orthogonal directions"
  - [section 4] "The gradient's norm depends on the term ℓ/d"
  - [corpus] No direct matches, but similar concepts in related works about gradient-based vulnerability
- Break condition: If data is not confined to a linear subspace, or if regularization is applied to suppress orthogonal gradients

### Mechanism 2
- Claim: Decreasing initialization scale or adding L2 regularization reduces adversarial vulnerability.
- Mechanism: Smaller initialization variance (β = 1/d) or L2 regularization causes weight vectors' projections onto the orthogonal subspace to shrink, thereby reducing the orthogonal gradient magnitude. This makes the network more robust to perturbations orthogonal to the data manifold.
- Core assumption: Weight vectors' projections onto the orthogonal subspace remain fixed during training without regularization.
- Evidence anchors:
  - [abstract] "decreasing the initialization scale of the training algorithm, or adding L2 regularization, can make the trained network more robust"
  - [section 6.1] "For β = 1/√d, this upper bound matches the lower bound from Theorem 4.1"
  - [corpus] No direct matches, but related to robustness-improving methods in other works
- Break condition: If regularization strength is insufficient, or if other layers' weights dominate the behavior

### Mechanism 3
- Claim: Existence of universal adversarial perturbations orthogonal to the data manifold.
- Mechanism: For data on a low-dimensional subspace, there exists a single perturbation direction (orthogonal to the data) that can flip the output for any input from the subspace. The perturbation size depends on the network output and subspace dimension.
- Core assumption: The perturbation direction does not depend on the specific input, only its size does.
- Evidence anchors:
  - [abstract] "adversarial perturbations of size O(√d/ℓ) exist in these directions"
  - [section 5] "The direction of the perturbation z does not depend on the input data x0, only its size depends on x0"
  - [corpus] No direct matches, but relates to universal adversarial perturbations in other contexts
- Break condition: If the network output is too small, or if ℓ is not sufficiently large relative to m

## Foundational Learning

- Concept: ReLU activation function and its properties
  - Why needed here: The analysis relies on ReLU's piecewise linearity and the fact that inactive neurons don't contribute to gradients
  - Quick check question: What happens to the gradient contribution from a neuron when its pre-activation is negative?

- Concept: Low-dimensional data manifolds and linear subspaces
  - Why needed here: The vulnerability arises specifically when data is confined to a low-dimensional subspace embedded in high-dimensional space
  - Quick check question: If data lies on a d-ℓ dimensional subspace in Rd, what is the dimension of the orthogonal subspace?

- Concept: Gradient flow and implicit bias in neural network training
  - Why needed here: The paper analyzes how gradient-based training leads to specific weight configurations that create vulnerability
  - Quick check question: Why do weight vectors' projections onto the orthogonal subspace remain fixed during training without regularization?

## Architecture Onboarding

- Component map: Input layer (d-dim) -> Hidden layer (m ReLU neurons) -> Output layer (single linear neuron) -> Logistic loss training
- Critical path:
  1. Initialize network weights using Kaiming initialization
  2. Train on data confined to low-dimensional subspace until margin ≥ 0.3
  3. Analyze gradient in orthogonal directions
  4. Verify existence of adversarial perturbations
  5. Test robustness improvements via initialization scale or regularization
- Design tradeoffs:
  - Larger m increases capacity but also increases orthogonal gradient magnitude
  - Smaller initialization scale improves robustness but may slow convergence
  - L2 regularization improves robustness but requires tuning λ
  - Deeper networks may exhibit similar behavior but require more complex analysis
- Failure signatures:
  - If data is not confined to a linear subspace, the theoretical guarantees don't apply
  - If ℓ is too small relative to m, the adversarial perturbation may not exist
  - If initialization scale is too small, the network may not learn effectively
  - If regularization is too strong, the network may underfit
- First 3 experiments:
  1. Train a two-layer network on synthetic data confined to a line in 2D, measure orthogonal gradient magnitude
  2. Repeat with smaller initialization scale, compare gradient magnitude and adversarial robustness
  3. Add L2 regularization, verify that orthogonal gradients decrease and robustness improves

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the dimensionality of the data manifold affect the robustness of neural networks beyond the two-layer ReLU case studied here?
  - Basis in paper: The paper focuses on two-layer ReLU networks trained on data lying on a low-dimensional linear subspace, but notes that extending these results to deeper networks and other architectures is an open direction.
  - Why unresolved: The paper provides theoretical analysis for two-layer networks, but does not extend the analysis to deeper networks or other architectures like convolutions.
  - What evidence would resolve it: Empirical studies on deeper networks and theoretical analysis extending the results to other architectures.

- **Open Question 2**: How do different norms (e.g., L1, L∞) affect the robustness of neural networks to adversarial examples orthogonal to the data manifold?
  - Basis in paper: The paper analyzes robustness using the L2 norm and suggests that analyzing robustness with respect to other norms like L1 or L∞ is an interesting future direction.
  - Why unresolved: The analysis in the paper is specific to the L2 norm, and the effects of other norms on robustness are not explored.
  - What evidence would resolve it: Theoretical and empirical studies analyzing the impact of different norms on robustness to adversarial examples.

- **Open Question 3**: What are the conditions under which adversarial perturbations orthogonal to the data manifold exist for general manifolds beyond linear subspaces?
  - Basis in paper: The paper conjectures conditions for the existence of adversarial perturbations orthogonal to the tangent space of a general manifold, but this is left as an open problem.
  - Why unresolved: The analysis in the paper is limited to linear subspaces, and the conditions for general manifolds are not fully explored.
  - What evidence would resolve it: Theoretical analysis establishing conditions for the existence of adversarial perturbations for general manifolds.

## Limitations

- The analysis assumes exact linear subspace data structure, while real-world data often lies on nonlinear manifolds
- The theoretical bounds rely on asymptotic behavior that may not hold for finite-width networks with practical parameters
- Extension to deeper networks remains theoretical without empirical validation

## Confidence

- **High confidence**: The gradient scaling relationship √(kℓ/m) and its dependence on subspace dimension ℓ is mathematically rigorous
- **Medium confidence**: The robustness improvements from smaller initialization or L2 regularization are theoretically supported but require more extensive empirical validation
- **Low confidence**: The direct applicability to deep networks and practical adversarial defense strategies needs further investigation

## Next Checks

1. Test the gradient scaling law empirically across varying subspace dimensions ℓ and network widths m to verify theoretical predictions
2. Implement and benchmark initialization scale and L2 regularization techniques on real-world datasets (MNIST, CIFAR-10) to assess practical robustness gains
3. Extend the analysis to three-layer networks and verify whether similar vulnerability patterns emerge in deeper architectures