---
ver: rpa2
title: 'Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning'
arxiv_id: '2312.14878'
source_url: https://arxiv.org/abs/2312.14878
tags:
- agent
- functions
- agents
- tasks
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pangu-Agent, a general framework for integrating
  structured reasoning into AI agent policies. The framework uses intrinsic functions
  to transform an agent's memory and extrinsic functions to interact with the environment.
---

# Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning

## Quick Facts
- arXiv ID: 2312.14878
- Source URL: https://arxiv.org/abs/2312.14878
- Reference count: 40
- Primary result: Framework improves decision-making task performance through structured reasoning and fine-tuning, increasing success rates from 27% to 82% in ALFWorld

## Executive Summary
Pangu-Agent is a general framework for integrating structured reasoning into AI agent policies through intrinsic and extrinsic functions. The framework transforms an agent's memory using intrinsic functions before action selection, enabling modular reasoning structures that improve learning efficiency. It supports fine-tuning via supervised learning and reinforcement learning, allowing agents to adapt to new tasks beyond pre-trained capabilities. Experiments demonstrate significant performance improvements across multiple decision-making tasks.

## Method Summary
Pangu-Agent introduces intrinsic functions into the RL policy formulation to enable modular reasoning structures that improve learning efficiency. The framework uses intrinsic functions to transform an agent's memory and extrinsic functions to interact with the environment. It supports fine-tuning via supervised learning (SFT) and reinforcement learning fine-tuning (RLFT), enabling agents to adapt to new tasks. The method successfully implements a rejection-sampling-based SFT pipeline and demonstrates improved performance in ALFWorld domain with success rates increasing from 27% to 82%.

## Key Results
- Success rates in ALFWorld increased from 27% to 82% after fine-tuning
- Composite reasoning methods outperform first-order nesting methods in achieved returns
- Framework supports multi-agent collaboration and can be extended with external tools for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing intrinsic functions into the RL policy formulation enables modular reasoning structures that improve learning efficiency.
- Mechanism: By nesting intrinsic functions (µ) that transform the agent's memory before action selection, the policy can perform structured reasoning steps without requiring environmental interaction, thereby enriching the state representation beyond raw observations.
- Core assumption: The intrinsic functions can be parameterized and optimized alongside the policy to improve returns, and that these functions provide meaningful transformations of memory state.
- Evidence anchors: [abstract] mentions modularity found in human brain and use of intrinsic/extrinsic functions; [section 2] notes absence of nested intrinsic functions in standard RL formulations.

### Mechanism 2
- Claim: Fine-tuning LLMs via supervised learning (SFT) and reinforcement learning (RLFT) enables adaptation to new tasks beyond pre-trained capabilities.
- Mechanism: Successful trajectories collected using structured reasoning prompting methods are used to perform SFT on the LLM parameters, followed by RLFT using PPO to further improve task-specific performance through environmental interaction.
- Core assumption: The LLM's pre-trained weights provide a strong prior that can be efficiently adapted through fine-tuning, and that trajectory quality is sufficient for effective learning.
- Evidence anchors: [abstract] reports success rate improvements from 27% to 82% in ALFWorld after fine-tuning; [section 4.2.1] describes successful SFT pipeline implementation.

### Mechanism 3
- Claim: Structured reasoning methods (e.g., Chain-of-Thought, React, Self-Consistency) improve LLM decision-making performance compared to direct prompting.
- Mechanism: By explicitly prompting the LLM to generate intermediate reasoning steps before actions, the agent can leverage the LLM's reasoning capabilities more effectively, leading to better decision-making in complex tasks.
- Core assumption: The LLM has sufficient reasoning capability to benefit from structured prompting, and the intermediate thoughts are useful for improving action selection.
- Evidence anchors: [abstract] indicates AI agents perform better when organized reasoning is embedded; [section 4.1] shows composite methods outperform first-order nesting methods.

## Foundational Learning

- Concept: Reinforcement Learning (RL) fundamentals (policies, value functions, reward maximization)
  - Why needed here: Pangu-Agent is fundamentally an RL framework that optimizes policies to maximize returns, with intrinsic functions integrated into the RL objective.
  - Quick check question: Can you explain the difference between model-free and model-based RL, and how value functions are used in policy optimization?

- Concept: Large Language Model (LLM) prompting techniques (few-shot, chain-of-thought, in-context learning)
  - Why needed here: Pangu-Agent relies heavily on LLMs for decision-making, and the effectiveness of different prompting strategies is a key component of the framework's performance.
  - Quick check question: What is the difference between few-shot prompting and chain-of-thought prompting, and when might each be more effective?

- Concept: Supervised Fine-Tuning (SFT) and Reinforcement Learning Fine-Tuning (RLFT) of LLMs
  - Why needed here: Pangu-Agent supports both SFT and RLFT to adapt LLMs to new tasks, and understanding these techniques is crucial for effectively using the framework.
  - Quick check question: What are the key differences between SFT and RLFT, and what are the trade-offs between these two fine-tuning approaches?

## Architecture Onboarding

- Component map: Observation → Intrinsic functions → Memory update → Prompt generation → LLM response → Action execution → Reward collection → Fine-tuning
- Critical path: Observation → Intrinsic functions → Memory update → Prompt generation → LLM response → Action execution → Reward collection → Fine-tuning (if applicable)
- Design tradeoffs: Modularity vs. end-to-end learning (prioritizing modularity provides interpretability but may limit optimization), Prompt engineering vs. fine-tuning (simplicity vs. task-specific performance), Memory management (balancing information storage vs. context length constraints)
- Failure signatures: Poor performance (inadequate memory transformations or prompting), Slow inference (LLM generation speed bottleneck), Context overflow (memory state exceeding LLM context limits)
- First 3 experiments: 1) Implement and evaluate simple intrinsic function (think step-by-step) in GSM8K comparing against direct prompting, 2) Test prompting strategies (few-shot, chain-of-thought, React) on ALFWorld multi-step task, 3) Implement and evaluate SFT pipeline on BabyAI-GoToObj-v0 using successful trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Pangu-Agent compare to other LLM-based agent frameworks when fine-tuned on the same tasks and data?
- Basis in paper: [inferred] The paper claims Pangu-Agent improves performance but does not directly compare its performance to other frameworks.
- Why unresolved: The paper does not provide a direct comparison of Pangu-Agent's performance to other frameworks on the same tasks and data.
- What evidence would resolve it: A controlled experiment comparing Pangu-Agent's performance to other frameworks on the same tasks and data after fine-tuning.

### Open Question 2
- Question: How does the choice of intrinsic functions impact the performance of Pangu-Agent in different domains and tasks?
- Basis in paper: [explicit] The paper discusses the importance of intrinsic functions and evaluates different first-order and composite methods, but does not systematically study their impact on performance across domains.
- Why unresolved: The paper provides a general evaluation of intrinsic functions but does not conduct a systematic study of their impact on performance across different domains and tasks.
- What evidence would resolve it: A comprehensive study evaluating the performance of Pangu-Agent with different combinations of intrinsic functions across a diverse set of domains and tasks.

### Open Question 3
- Question: How does the fine-tuning process in Pangu-Agent affect the interpretability and explainability of the agent's decisions?
- Basis in paper: [inferred] The paper discusses fine-tuning Pangu-Agent but does not address the interpretability and explainability of the agent's decisions after fine-tuning.
- Why unresolved: The paper focuses on the performance improvements from fine-tuning but does not explore how the fine-tuning process affects the interpretability and explainability of the agent's decisions.
- What evidence would resolve it: An analysis of the interpretability and explainability of Pangu-Agent's decisions before and after fine-tuning, potentially using techniques like attention visualization or saliency maps.

## Limitations

- Limited empirical validation scope, with most experiments focusing on single-agent decision-making tasks rather than complex multi-agent scenarios
- Uncertainty about generalization boundaries across diverse domains, as current results show success in specific domains but may not generalize to all potential applications
- Lack of detailed computational overhead analysis, particularly regarding the impact of nested intrinsic functions on inference speed and memory usage during fine-tuning

## Confidence

The confidence level for the core claims is **Medium**. While the framework presents a theoretically sound approach to integrating structured reasoning into RL policies, several limitations affect confidence:

1. **Empirical validation scope**: Most experiments focus on single-agent decision-making tasks. The framework's effectiveness in complex multi-agent scenarios or real-world applications remains to be demonstrated. Claims about multi-agent collaboration capabilities are currently theoretical extensions rather than empirically validated results.

2. **Generalization boundaries**: The framework assumes that LLMs can effectively adapt to new tasks through fine-tuning, but the extent of this adaptability across diverse domains is uncertain. The current results show success in specific domains (ALFWorld, GSM8K, etc.) but may not generalize to all potential applications.

3. **Computational overhead**: The paper does not provide detailed analysis of the computational cost of the framework, particularly regarding the impact of nested intrinsic functions on inference speed and memory usage during fine-tuning.

## Next Checks

1. **Scale validation**: Test the framework on a significantly larger and more diverse set of tasks, including real-world applications, to evaluate generalization capabilities and identify potential domain-specific limitations.

2. **Performance bounds**: Conduct systematic ablation studies to determine the contribution of individual components (intrinsic functions, fine-tuning methods, prompting strategies) to overall performance, establishing clearer performance boundaries.

3. **Efficiency analysis**: Measure and analyze the computational overhead introduced by the framework, particularly the impact of nested intrinsic functions on inference speed and memory usage, to ensure practical applicability.