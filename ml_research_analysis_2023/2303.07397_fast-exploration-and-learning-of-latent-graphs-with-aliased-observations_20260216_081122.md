---
ver: rpa2
title: Fast exploration and learning of latent graphs with aliased observations
arxiv_id: '2303.07397'
source_url: https://arxiv.org/abs/2303.07397
tags:
- graph
- steps
- efex
- exploration
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces eFeX, an algorithm for fast exploration and
  recovery of aliased latent graphs from observation-action sequences. The core idea
  is to use an information-theoretic policy based on clone-structured causal graphs
  (CSCGs) that computes a closed-form utility function for selecting actions that
  maximize information gain.
---

# Fast exploration and learning of latent graphs with aliased observations

## Quick Facts
- arXiv ID: 2303.07397
- Source URL: https://arxiv.org/abs/2303.07397
- Reference count: 40
- Primary result: eFeX achieves exponentially faster exploration than random policy in aliased graph recovery, with expected log-likelihood and weighted coverage outperforming baselines.

## Executive Summary
This paper introduces eFeX, an algorithm for efficient exploration and recovery of aliased latent graphs from observation-action sequences. The key innovation is an information-theoretic policy based on clone-structured causal graphs (CSCGs) that computes a closed-form utility function for selecting actions that maximize information gain. This enables efficient exploration even when multiple nodes emit the same observation, a challenging aliasing scenario. Experiments demonstrate that eFeX is exponentially faster than random exploration for difficult aliased graph topologies while remaining competitive with state-of-the-art baselines in the unaliased case.

## Method Summary
eFeX operates on the premise that multiple latent nodes can emit the same observation (aliasing), making exact localization impossible. The algorithm uses CSCGs with overparameterization - allocating multiple clones per observation type to create an overcomplete emission matrix. It alternates between two phases: first, estimating the transition tensor via EM algorithm with Viterbi decoding to recover the most likely hidden state sequence; second, computing a closed-form information-theoretic utility based on Jensen-Shannon divergence between current and expected transition beliefs under Dirichlet posteriors. This utility is propagated via value iteration to obtain globally optimal exploration policies that maximize expected cumulative information gain.

## Key Results
- eFeX achieves exponentially faster exploration than random policy in aliased graph recovery tasks
- Expected log-likelihood values are consistently higher for eFeX compared to random exploration across various graph topologies
- Weighted coverage metrics show eFeX discovers more graph structure with fewer steps in aliased environments
- Performance remains competitive with state-of-the-art baselines (like MAX) in unaliased cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information-theoretic utility computation enables closed-form, exact selection of actions that maximize information gain in latent graph recovery.
- Mechanism: The algorithm computes Jensen-Shannon divergence between current belief distribution over transition probabilities and expected transition distribution under Dirichlet posterior, yielding a closed-form expression that avoids sampling approximations.
- Core assumption: Transition tensor follows Dirichlet distribution with conjugate prior, enabling exact posterior computation after observing transitions.
- Evidence anchors: [abstract] "closed-form utility function", [section] "Dirichlet prior with parameter ùõº", [corpus] Weak - no direct mention in related work
- Break condition: If transition distributions deviate significantly from Dirichlet assumptions or posterior updates become intractable.

### Mechanism 2
- Claim: The algorithm efficiently handles aliasing by using clone-structured causal graphs with overparameterization and Viterbi decoding.
- Mechanism: Multiple clones per observation type create overcomplete emission matrix. After EM estimates transition probabilities, Viterbi decoding recovers most likely hidden state sequence, effectively de-aliasing observations.
- Core assumption: Overparameterization with more clones than ground truth nodes enables EM to find good optima for latent state sequence.
- Evidence anchors: [abstract] "aliased latent graphs", [section] "Viterbi withùëá and ùê∏ onDùëõ", [corpus] Weak - related work focuses on community detection not specifically aliased graph recovery
- Break condition: If clone allocation insufficient or EM gets stuck in poor local optima, leading to incorrect state decoding.

### Mechanism 3
- Claim: Value iteration with utility as reward provides globally optimal action sequences despite utility changing during exploration.
- Mechanism: Local information-theoretic utility serves as reward in value iteration, computing optimal policies that maximize expected cumulative utility. Approximation assumes utility evolves slowly enough for policy validity.
- Core assumption: Utility function changes slowly relative to discount factor, making value iteration results approximately valid for multiple steps.
- Evidence anchors: [abstract] "information-theoretic policy", [section] "utility as a local reward", [corpus] Weak - related work mentions RL but not specifically value iteration with information-theoretic rewards
- Break condition: If utility changes rapidly between policy updates, leading to suboptimal exploration paths.

## Foundational Learning

- Concept: Hidden Markov Models and their extensions
  - Why needed here: Problem setup is essentially an HMM where observations are aliased, requiring extensions like CSCGs to handle multiple nodes emitting same observation
  - Quick check question: How does the observation model differ between a standard HMM and the aliased case considered here?

- Concept: Dirichlet distributions and conjugate priors
  - Why needed here: Transition probabilities modeled with Dirichlet priors, enabling exact posterior updates and closed-form utility computation
  - Quick check question: What is the posterior distribution after observing transitions when using a Dirichlet prior?

- Concept: Value iteration and reinforcement learning
  - Why needed here: Local information-theoretic utility is propagated globally using value iteration to obtain optimal exploration policies
  - Quick check question: How does the discount factor ùõæ affect the balance between immediate and future information gain?

## Architecture Onboarding

- Component map: Observations ‚Üí State decoding (EM + Viterbi) ‚Üí Utility computation (Dirichlet + JS divergence) ‚Üí Policy optimization (value iteration) ‚Üí Actions ‚Üí New observations

- Critical path: The EM-Viterbi loop that recovers hidden states from aliased observations must run efficiently at each exploration step to enable timely policy updates.

- Design tradeoffs: Algorithm trades computational complexity (value iteration and EM) for exploration efficiency. Using more clones improves de-aliasing but increases EM computation time. Closed-form utility avoids sampling but requires Dirichlet conjugacy.

- Failure signatures: Slow exploration indicates poor state decoding (Viterbi failing to disambiguate aliased states) or utility computation errors. Poor graph recovery suggests insufficient clone allocation or EM converging to suboptimal solutions.

- First 3 experiments:
  1. Run on simple unaliased chain graph to verify basic functionality and compare against random exploration
  2. Test with controlled aliasing (multiple nodes per observation type) to verify de-aliasing capability
  3. Scale to larger maze-like topologies to evaluate exploration efficiency and computational scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does eFeX's performance degrade as the number of clones per observation increases beyond twice the ground truth number?
- Basis in paper: [explicit] Mentions allocating "on average twice as many" clones as ground truth, references overparameterization benefits
- Why unresolved: Only tests twice-as-many parameter setting, doesn't explore how performance changes with different clone allocations
- What evidence would resolve it: Empirical results showing expected log-likelihood, weighted coverage, and precision across multiple clone allocation ratios (e.g., 1x, 2x, 4x ground truth clones)

### Open Question 2
- Question: Can eFeX's utility computation (Equation 2) be extended to handle stochastic emissions (non-deterministic observation probabilities)?
- Basis in paper: [inferred] Current utility derivation assumes deterministic emissions, but mentions handling stochastic emissions would require "modification in which emission matrix would no longer be fixed a priori, but would have to be learned instead"
- Why unresolved: No derivation or experiments for stochastic emission case provided
- What evidence would resolve it: Complete derivation of utility function for stochastic emissions and experimental validation on environments with probabilistic observations

### Open Question 3
- Question: How does eFeX's exploration efficiency compare to model-based active exploration (MAX) in aliased environments where both methods could theoretically apply?
- Basis in paper: [explicit] States that "MAX and deep Q-learning are not applicable here due to aliasing" but doesn't explore scenarios where aliasing might be reduced or absent
- Why unresolved: Only compares eFeX to random policies in aliased settings and to MAX in unaliased settings, never directly comparing them in same environment
- What evidence would resolve it: Head-to-head experiments on unaliased versions of test topologies measuring expected log-likelihood and steps to full coverage for both algorithms

## Limitations
- Algorithm performance critically depends on number of clones allocated per observation type, but paper lacks clear guidance on optimal hyperparameter selection
- Computational complexity of value iteration and EM updates scales poorly with graph size, creating practical limitations for large-scale problems
- Assumption of Dirichlet conjugate priors for transition probabilities may not hold for all graph topologies, potentially limiting applicability

## Confidence
- **High confidence**: The core information-theoretic utility computation mechanism and its closed-form derivation
- **Medium confidence**: The clone-based de-aliasing approach with Viterbi decoding - works well in practice but theoretical guarantees are limited
- **Medium confidence**: The value iteration policy approximation - empirically effective but relies on slow utility evolution assumption

## Next Checks
1. **Robustness to clone allocation**: Systematically vary number of clones per observation from 1 to 5 and measure performance degradation curves to establish sensitivity and identify optimal settings for different graph topologies

2. **Distribution assumption validation**: Test algorithm on graphs with non-Markovian transitions (e.g., where transition probabilities depend on history beyond current state) to verify Dirichlet assumption breakdown and measure performance degradation

3. **Scalability analysis**: Implement algorithm on progressively larger graph sizes (10x, 100x, 1000x original scale) and measure wall-clock time for each component (EM, Viterbi, value iteration) to identify computational bottlenecks and scaling laws