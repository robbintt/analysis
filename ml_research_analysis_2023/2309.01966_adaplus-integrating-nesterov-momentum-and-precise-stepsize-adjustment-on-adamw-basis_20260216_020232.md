---
ver: rpa2
title: 'AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on
  AdamW Basis'
arxiv_id: '2309.01966'
source_url: https://arxiv.org/abs/2309.01966
tags:
- adaplus
- adamw
- adabelief
- adam
- nadam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaPlus is a novel optimizer that integrates Nesterov momentum
  and precise stepsize adjustment into AdamW, combining advantages of AdamW, Nadam,
  and AdaBelief without introducing extra hyperparameters. It treats momentum as a
  prediction of the next gradient and adapts stepsize based on the belief in the current
  gradient direction.
---

# AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis

## Quick Facts
- arXiv ID: 2309.01966
- Source URL: https://arxiv.org/abs/2309.01966
- Reference count: 0
- Primary result: AdaPlus achieves 1.97%-2.36% accuracy improvement over AdamW, Nadam, and AdaBelief on CIFAR-10 image classification

## Executive Summary
AdaPlus is a novel optimizer that integrates Nesterov momentum and precise stepsize adjustment into AdamW, combining advantages of AdamW, Nadam, and AdaBelief without introducing extra hyperparameters. It treats momentum as a prediction of the next gradient and adapts stepsize based on the belief in the current gradient direction. Experiments on image classification (CIFAR-10), language modeling (Penn TreeBank), and GAN training show AdaPlus outperforms six state-of-the-art optimizers. On CIFAR-10, it achieves 1.97%-2.36% accuracy improvement over AdamW, Nadam, and AdaBelief. For language modeling, it attains the lowest perplexity among all methods. On GAN training, AdaPlus consistently achieves the lowest FID scores, demonstrating superior stability.

## Method Summary
AdaPlus is an adaptive optimizer that builds on AdamW by incorporating Nesterov momentum and precise stepsize adjustment. The optimizer calculates momentum as an exponential moving average of gradients, then treats this momentum as a prediction of the next gradient. Stepsize is adjusted based on how closely the current gradient matches this prediction - increasing when predictions are accurate and decreasing when they deviate. Weight decay is decoupled from gradient updates as in AdamW. The method uses standard hyperparameters (β1=0.9, β2=0.999, ε=1e-8) without introducing additional ones, aiming to combine the fast convergence of AdamW with the anticipatory properties of Nesterov momentum and adaptive stepsize control.

## Key Results
- Achieves 1.97%-2.36% accuracy improvement over AdamW, Nadam, and AdaBelief on CIFAR-10 image classification
- Attains the lowest perplexity among all tested optimizers for language modeling on Penn TreeBank
- Consistently achieves the lowest FID scores in GAN training, demonstrating superior stability across WGAN and WGAN-GP architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdaPlus achieves improved generalization on image classification by combining decoupled weight decay with Nesterov momentum.
- Mechanism: Decoupled weight decay prevents overfitting by regularizing weights independently of gradient updates, while Nesterov momentum provides a predictive update direction that anticipates future gradients, leading to smoother convergence.
- Core assumption: The combination of decoupled weight decay and Nesterov momentum maintains the regularization benefits of AdamW while improving the momentum's anticipatory behavior.
- Evidence anchors:
  - [abstract] "combines the advantages of AdamW, Nadam, and AdaBelief" and "performs most comparable with (even slightly better than) SGD with momentum on image classification tasks"
  - [section] "incorporates Nesterov momentum as in Nadam and precise stepsize adjustment as in AdaBelief" and Table 1 showing accuracy improvements over AdamW, Nadam, and AdaBelief
  - [corpus] Weak evidence - no direct corpus support for this specific combination mechanism

### Mechanism 2
- Claim: AdaPlus improves training stability on GANs by integrating precise stepsize adjustment based on gradient prediction accuracy.
- Mechanism: The optimizer adapts the stepsize based on how closely the current gradient matches the predicted gradient (mt), increasing stepsize when predictions are accurate and decreasing when they deviate, which stabilizes adversarial training dynamics.
- Core assumption: GAN training benefits from dynamic stepsize adjustment that responds to the consistency between predicted and actual gradients.
- Evidence anchors:
  - [abstract] "outperforms other state-of-the-art optimizers on language modeling tasks and illustrates pretty high stability when training GANs" and "consistently achieves the lowest FID scores"
  - [section] "AdaPlus regards mt as the forecast for gt, escalates the stepsize when gt approaches mt and decreases the stepsize when gt deviates from the prediction mt" and Table 3 showing lowest FID scores for both WGAN and WGAN-GP
  - [corpus] Weak evidence - no direct corpus support for this specific GAN stability mechanism

### Mechanism 3
- Claim: AdaPlus achieves faster convergence on language modeling by combining multiple adaptive strategies without introducing extra hyperparameters.
- Mechanism: By building on AdamW and incorporating Nesterov momentum and precise stepsize adjustment, AdaPlus leverages the fast convergence of AdamW while benefiting from the anticipatory properties of Nesterov momentum and the adaptive stepsize control, all without requiring additional hyperparameter tuning.
- Core assumption: The synergistic combination of these three components provides benefits that exceed the sum of their individual contributions.
- Evidence anchors:
  - [abstract] "performs most comparable with (even slightly better than) SGD with momentum on image classification tasks" and "outperforms other state-of-the-art optimizers on language modeling tasks"
  - [section] "AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief" and Figure 3 showing lowest perplexity for all LSTM configurations
  - [corpus] Weak evidence - no direct corpus support for this specific convergence mechanism

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: AdaPlus uses EMA to maintain momentum estimates (mt) and variance estimates (vt, st) that smooth gradient information over time
  - Quick check question: How does the EMA formula β * previous_value + (1-β) * current_value affect the responsiveness of momentum estimates?

- Concept: Nesterov Momentum
  - Why needed here: AdaPlus incorporates Nesterov momentum to provide a predictive update direction that looks ahead to where the parameters will be after the momentum step
  - Quick check question: What is the key difference between classical momentum and Nesterov momentum in terms of when the gradient is evaluated?

- Concept: Weight Decay vs. L2 Regularization
  - Why needed here: AdaPlus uses decoupled weight decay (AdamW style) which applies regularization separately from gradient updates, unlike traditional L2 regularization
  - Quick check question: Why does decoupled weight decay often generalize better than L2 regularization in adaptive optimizers?

## Architecture Onboarding

- Component map: Gradient computation → Momentum update (β1 * mt-1 + (1-β1) * gt) → Stepsize adjustment (based on gt vs mt prediction) → Parameter update → Weight decay application

- Critical path: Gradient computation → Momentum update → Stepsize adjustment → Parameter update → Weight decay application

- Design tradeoffs: No extra hyperparameters vs. potentially suboptimal default values for specific tasks; complexity of combining multiple strategies vs. performance benefits

- Failure signatures: Oscillating validation loss (stepsize adjustment too aggressive), plateaued training loss (momentum too conservative), overfitting despite weight decay (decay rate too low)

- First 3 experiments:
  1. Compare training curves (loss vs. epochs) of AdaPlus vs AdamW on CIFAR-10 with VGG-11 to verify the claimed accuracy improvements
  2. Test stepsize sensitivity by running with β1 values of 0.8, 0.9, and 0.95 to find optimal momentum
  3. Evaluate GAN stability by training WGAN-GP with AdaPlus and measuring FID score progression across training epochs

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the experiments, several important questions remain unanswered regarding AdaPlus's performance on modern architectures, hyperparameter sensitivity, scalability to larger datasets, and comparison with newer optimization methods.

## Limitations

- No detailed implementation specifications for model architectures and data preprocessing, critical for exact reproduction
- No ablation studies provided to isolate the contribution of each component to overall performance
- Computational overhead of the stepsize adjustment mechanism is not quantified or discussed

## Confidence

- **High confidence**: The mathematical formulation of AdaPlus is clearly presented and builds logically on existing optimizers.
- **Medium confidence**: The experimental results showing performance improvements over baseline optimizers, though exact reproduction would require additional implementation details.
- **Low confidence**: The claimed stability improvements for GAN training, as the paper provides limited analysis of training dynamics and no comparison of training stability metrics beyond FID scores.

## Next Checks

1. Implement AdaPlus and verify the gradient prediction and stepsize adjustment mechanism matches the mathematical formulation.
2. Conduct ablation studies to measure the individual contribution of Nesterov momentum and stepsize adjustment to overall performance.
3. Compare training stability metrics (gradient norms, loss variance) between AdaPlus and baseline optimizers during GAN training.