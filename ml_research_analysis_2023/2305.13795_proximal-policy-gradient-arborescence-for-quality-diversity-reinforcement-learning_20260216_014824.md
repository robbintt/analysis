---
ver: rpa2
title: Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning
arxiv_id: '2305.13795'
source_url: https://arxiv.org/abs/2305.13795
tags:
- policy
- gradient
- search
- objective
- ppga
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Proximal Policy Gradient Arborescence (PPGA),
  a novel Quality Diversity Reinforcement Learning (QD-RL) algorithm that combines
  the differentiable quality diversity (DQD) framework with on-policy Proximal Policy
  Optimization (PPO). Unlike prior QD-RL methods that rely on sample-efficient off-policy
  algorithms like TD3, PPGA leverages massive parallelism to accurately estimate gradients
  and efficiently explore high-dimensional behavioral spaces.
---

# Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.13795
- Source URL: https://arxiv.org/abs/2305.13795
- Reference count: 40
- Primary result: Introduces PPGA, achieving 4x improvement in best reward on humanoid locomotion compared to baselines

## Executive Summary
This paper introduces Proximal Policy Gradient Arborescence (PPGA), a novel Quality Diversity Reinforcement Learning (QD-RL) algorithm that combines differentiable quality diversity (DQD) with massively parallel Proximal Policy Optimization (PPO). Unlike prior QD-RL methods that rely on sample-efficient off-policy algorithms, PPGA leverages massive parallelism to accurately estimate gradients and efficiently explore high-dimensional behavioral spaces. The method maintains a single search policy that branches into many nearby policies for parallel evaluation, forming a gradient arborescence. Experiments on four Brax locomotion tasks demonstrate state-of-the-art performance, with a 4x improvement in best reward on the challenging humanoid domain compared to baselines.

## Method Summary
PPGA combines differentiable quality diversity (DQD) with massively parallel PPO to solve the quality diversity reinforcement learning problem. The algorithm maintains a single search policy that branches into multiple nearby policies for parallel evaluation, forming a gradient arborescence. PPO is used to estimate objective and measure gradients from these branched policies, while xNES (natural evolution strategy) optimizes the gradient coefficients. The method introduces Markovian Measure Proxies (MMPs) to enable gradient-based optimization of non-Markovian measure functions by converting them into state-dependent, Markovian equivalents. The algorithm trades sample efficiency for wall-clock time by leveraging massive parallelism across multiple GPUs.

## Key Results
- Achieves 4x improvement in best reward on humanoid locomotion task compared to baselines
- Demonstrates superior performance across four Brax environments (Ant, Walker2d, Half-Cheetah, Humanoid)
- Maintains good coverage of behavioral space while finding high-performing policies
- Requires substantial computational resources but delivers state-of-the-art QD-RL performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Massively parallel PPO with DQD enables high-quality gradient estimates for efficient exploration
- **Mechanism:** Parallel PPO rollouts compute accurate on-policy gradient estimates for both objective and measure functions. These gradients guide the search policy through the behavioral embedding space, forming an efficient gradient arborescence.
- **Core assumption:** Behavioral embedding space is continuous and differentiable enough for accurate local gradient estimates
- **Evidence anchors:** Abstract mentions leveraging parallelism for accurate gradient estimates; section discusses parallel PPO for gradient estimation

### Mechanism 2
- **Claim:** xNES provides more stable step-size adaptation than CMA-ES in noisy QD-RL domains
- **Mechanism:** CMA-ES's cumulative step-size adaptation is unstable in noisy, non-stationary RL domains, leading to exploding updates. xNES uses natural gradient step-size adaptation (∇σ = tr(∇Σ)/d) which is more robust to noise and non-stationarity.
- **Core assumption:** Non-stationarity and noise in QD-RL significantly impact CMA-ES stability
- **Evidence anchors:** Section explains CMA-ES instability and xNES's natural gradient approach

### Mechanism 3
- **Claim:** Markovian Measure Proxies enable RL to estimate gradients for non-Markovian measure functions
- **Mechanism:** Standard non-Markovian measures (like proportional foot contact time) depend on entire trajectories. MMPs convert these into state-dependent functions that obey Markov property, allowing RL algorithms to estimate their gradients.
- **Core assumption:** State-dependent surrogate functions can be constructed that are positively correlated with original measures
- **Evidence anchors:** Section introduces MMP concept and explains construction for locomotion tasks

## Foundational Learning

- **Concept: Quality Diversity Optimization**
  - Why needed: Foundational framework combining exploitation and exploration to produce diverse, high-performing solutions
  - Quick check: What is QD optimization's main objective and how does it differ from standard optimization?

- **Concept: Differentiable Quality Diversity (DQD)**
  - Why needed: Enables gradient-based optimization of QD problems by maintaining single search point
  - Quick check: How does DQD differ from traditional QD and why is this important for on-policy RL?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed: On-policy RL algorithm used for gradient estimation and policy walking
  - Quick check: What is PPO's key innovation for massively parallel regime and how does it differ from TRPO?

## Architecture Onboarding

- **Component map:** Search policy → Branched policies (VPPO) → Archive → xNES optimizer → Updated search policy

- **Critical path:**
  1. Initialize search policy and xNES parameters
  2. VPPO estimates objective and measure gradients from current search policy
  3. Sample gradient coefficients from xNES distribution and branch new policies
  4. Evaluate branched policies and update MAP-Elites archive
  5. Adapt xNES parameters based on archive improvement
  6. VPPO walks search policy toward greatest archive improvement
  7. Repeat until convergence

- **Design tradeoffs:**
  - Sample efficiency vs. wall-clock time: Sacrifices sample efficiency for faster wall-clock time through massive parallelism
  - Computational resources: Substantial GPU requirements due to parallel environments and multiple PPO iterations
  - Stability vs. exploration: Adaptive vs. fixed standard deviation in PPO affects exploration-exploitation balance

- **Failure signatures:**
  - Exploding step-size updates in xNES indicating optimization instability
  - Poor behavioral space coverage suggesting ineffective gradient guidance
  - Low best reward compared to baselines indicating failure to find high-performing policies

- **First 3 experiments:**
  1. Run PPGA on simple Ant task with small archive, observe convergence and coverage
  2. Compare PPGA with different N1/N2 values (gradient estimation vs walking steps)
  3. Test PPGA with/without adaptive standard deviation in PPO to evaluate exploration effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does PPGA performance scale with increasing parallelization and GPU resources?
- **Basis:** Paper mentions benefits from massive parallelism but doesn't explore scaling limits
- **Why unresolved:** No experiments varying parallelization levels or GPU resources
- **Evidence needed:** Experiments varying parallel environments and GPU resources while measuring performance metrics

### Open Question 2
- **Question:** What is optimal trade-off between N1 (gradient estimation) and N2 (policy walking) steps?
- **Basis:** Ablation study shows asymmetric choices affect performance but lacks systematic analysis
- **Why unresolved:** Only tests few specific N1/N2 combinations without exploring full parameter space
- **Evidence needed:** Comprehensive parameter sweep across N1/N2 values with learning curve analysis

### Open Question 3
- **Question:** How does PPGA perform on real robotic tasks vs simulation and what challenges arise?
- **Basis:** Paper demonstrates effectiveness in simulation but doesn't test on physical robots
- **Why unresolved:** Doesn't address sim-to-real transfer problem crucial for practical applications
- **Evidence needed:** Experiments on real robotic platforms comparing to simulated results

## Limitations

- Computational requirements are substantial, requiring significant GPU resources for parallel execution
- Scaling behavior with problem dimensionality remains untested beyond 3-dimensional measure spaces
- Claims about xNES superiority over CMA-ES lack direct empirical validation in QD-RL context

## Confidence

- **High confidence:** Core mechanism of parallel PPO for gradient estimation and general PPGA framework are well-validated
- **Medium confidence:** xNES replacement for CMA-ES is theoretically justified but lacks direct empirical comparison
- **Medium confidence:** MMP effectiveness for non-Markovian measures is demonstrated but not thoroughly validated across diverse measure types

## Next Checks

1. **Benchmark xNES vs CMA-ES:** Implement CMA-ES in PPGA and compare performance/stability across four Brax tasks, measuring step-size stability and convergence speed.

2. **Stress test high-dimensionality:** Evaluate PPGA on tasks with 4+ dimensional behavioral spaces to assess performance and coverage maintenance as dimensionality increases.

3. **Ablation study on parallelism:** Compare PPGA performance with varying degrees of parallelism (different numbers of branched policies) to determine minimum computational requirements for effective performance.