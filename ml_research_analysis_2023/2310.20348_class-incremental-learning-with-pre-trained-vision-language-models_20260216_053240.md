---
ver: rpa2
title: Class Incremental Learning with Pre-trained Vision-Language Models
arxiv_id: '2310.20348'
source_url: https://arxiv.org/abs/2310.20348
tags:
- learning
- tasks
- adapter
- clip
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of pre-trained vision-language models
  (specifically CLIP) for class incremental learning (CIL). Instead of freezing the
  pre-trained model as in zero-shot approaches, the authors augment CLIP with adapter
  layers (linear, self-attention, or prompt tuning) to allow incremental adaptation
  to new tasks.
---

# Class Incremental Learning with Pre-trained Vision-Language Models

## Quick Facts
- arXiv ID: 2310.20348
- Source URL: https://arxiv.org/abs/2310.20348
- Reference count: 10
- Key outcome: Simple linear adapter with parameter retention achieves state-of-the-art performance on CIFAR-100, ImageNet-100, and ImageNet-R for class incremental learning using CLIP

## Executive Summary
This work addresses class incremental learning (CIL) by leveraging pre-trained vision-language models, specifically CLIP. Rather than freezing CLIP as in zero-shot approaches, the authors augment it with adapter layers (linear, self-attention, or prompt tuning) to enable incremental adaptation to new tasks. A parameter retention strategy is introduced to mitigate catastrophic forgetting by preserving important adapter weights from previous tasks based on their drift. Experiments demonstrate that this simple approach outperforms conventional CIL methods and other adapter-based approaches.

## Method Summary
The method augments a frozen CLIP model with adapter layers after the image encoder, allowing incremental adaptation while preserving the pre-trained vision-language knowledge. Three adapter types are explored: linear, self-attention, and prompt tuning. A parameter retention strategy based on parameter drift is proposed to mitigate forgetting by preserving a subset of adapter weights from previous tasks. The approach is evaluated on CIFAR-100, ImageNet-100, ImageNet-R, and ImageNet-Full datasets using average incremental accuracy and accuracy at the last task as metrics.

## Key Results
- Linear adapter with parameter retention achieves state-of-the-art performance on CIL benchmarks
- The approach outperforms conventional CIL methods and other adapter-based approaches
- Simple adapter augmentation provides effective adaptation without full fine-tuning of CLIP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a small number of adapter parameters after the CLIP image encoder enables incremental adaptation while freezing the large pre-trained model to avoid catastrophic forgetting.
- Mechanism: The adapter layer transforms the CLIP image embeddings into a task-specific space. Since only the adapter parameters are updated, the pre-trained CLIP weights remain stable, preserving general vision-language knowledge.
- Core assumption: The CLIP image encoder produces useful embeddings that can be linearly transformed to fit new tasks without fine-tuning the entire model.
- Evidence anchors:
  - [abstract] "We augment a pre-trained CLIP model with additional layers after the Image Encoder... only component that needs to be updated, as we freeze all other parameters in CLIP."
  - [section] "Assume we are at task t... we input the images into the image encoder to extract the latent embedding... pass it through a Linear Adapter layer... to compute an adapted image feature with more capacity."
  - [corpus] Weak - no direct evidence in cited neighbors.

### Mechanism 2
- Claim: Parameter retention based on distance between old and new weights preserves important knowledge from previous tasks and mitigates forgetting.
- Mechanism: After each task, a subset of adapter parameters with the smallest change (low drift) are retained from the previous task, while the rest are updated. This balances stability (retaining stable parameters) and plasticity (updating others).
- Core assumption: Parameters with low drift between tasks are more important for preserving previous task knowledge.
- Evidence anchors:
  - [abstract] "We propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning."
  - [section] "Measuring the drift of parameter i in W with ∆W i = |W i 1 − W i 2|... we select a proportion γ ∈ [0, 1.0] of parameters from W1, while the rest are replaced with the new parameters from W2."
  - [corpus] Weak - no direct evidence in cited neighbors.

### Mechanism 3
- Claim: The pre-trained CLIP model's zero-shot capability provides a strong initialization that adapts better to incremental tasks than training from scratch.
- Mechanism: The CLIP model already encodes rich vision-language relationships from large-scale pre-training. By only adding small adapters, the model can leverage this knowledge while adapting to new classes without catastrophic forgetting.
- Core assumption: The CLIP pre-trained representations are sufficiently general and transferable to new incremental tasks.
- Evidence anchors:
  - [abstract] "Continual-CLIP provides a very strong and simple baseline that exploits the zero-shot learning capabilities of CLIP, although the pre-trained model is frozen and is thus unable to further improve with new data."
  - [section] "Starting from the zero-shot evaluation protocol proposed in Continual-CLIP... we aim to augment the original architecture with new modules that enable better adaptation to more downstream tasks."
  - [corpus] Weak - no direct evidence in cited neighbors.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why the pre-trained CLIP model must be frozen and why parameter retention is necessary
  - Quick check question: What happens to neural network weights when training on new tasks without any regularization?

- Concept: Adapter modules in pre-trained models
  - Why needed here: Understanding how adding small trainable layers enables task adaptation without full fine-tuning
  - Quick check question: How does an adapter layer modify the output of a frozen pre-trained layer?

- Concept: Cross-modal alignment in vision-language models
  - Why needed here: Understanding why CLIP's image-text matching is useful for classification tasks
  - Quick check question: How does CLIP use image and text embeddings to classify images?

## Architecture Onboarding

- Component map: CLIP Image Encoder (frozen) -> Linear Adapter (trainable) -> CLIP Text Encoder (frozen) -> Cross-entropy loss
- Critical path: Image → CLIP Image Encoder → Linear Adapter → Text Encoder → Classification
- Design tradeoffs:
  - Adapter size vs. adaptation capacity
  - Parameter retention rate vs. forgetting vs. underfitting
  - Exemplar storage vs. performance
- Failure signatures:
  - Underfitting: Low accuracy even on current task
  - Overfitting: High current task accuracy but rapid forgetting
  - Divergence: Parameter retention fails to preserve useful weights
- First 3 experiments:
  1. Test zero-shot CLIP performance on incremental tasks (baseline)
  2. Add linear adapter without parameter retention (test adaptation)
  3. Add linear adapter with parameter retention at different rates (optimize γ)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed parameter retention strategy perform when applied to other continual learning scenarios beyond class incremental learning, such as task incremental learning or domain incremental learning?
- Basis in paper: [inferred] The paper focuses on class incremental learning but mentions the parameter retention strategy could be applied to other scenarios.
- Why unresolved: The paper only evaluates the parameter retention strategy within the context of class incremental learning, leaving its effectiveness in other continual learning scenarios unexplored.
- What evidence would resolve it: Experiments applying the parameter retention strategy to task incremental learning and domain incremental learning benchmarks, comparing performance against existing methods in those settings.

### Open Question 2
- Question: What is the impact of different CLIP model architectures (e.g., different vision encoders or text encoders) on the effectiveness of the proposed approach for class incremental learning?
- Basis in paper: [inferred] The paper uses a specific CLIP model architecture but doesn't investigate the impact of using different architectures.
- Why unresolved: The paper's experiments are limited to a single CLIP model architecture, so the generalizability of the findings to other architectures is unknown.
- What evidence would resolve it: Experiments using different CLIP model architectures (e.g., CLIP models with different vision encoders like ConvNeXt or text encoders) and evaluating their performance in class incremental learning.

### Open Question 3
- Question: How does the proposed approach scale to larger datasets and more classes, and what are the limitations in terms of memory and computational resources?
- Basis in paper: [inferred] The paper experiments with datasets of varying sizes but doesn't explicitly discuss scalability limitations or resource requirements.
- Why unresolved: The paper doesn't provide a detailed analysis of the approach's scalability or resource consumption when applied to larger datasets or more classes.
- What evidence would resolve it: Experiments with significantly larger datasets (e.g., ImageNet-21K) and more classes, measuring memory usage, computational time, and performance degradation as the scale increases.

### Open Question 4
- Question: How does the proposed approach handle imbalanced class distributions within tasks, and what are the potential biases introduced by the parameter retention strategy?
- Basis in paper: [inferred] The paper mentions the class imbalance problem in continual learning but doesn't explicitly address how the parameter retention strategy affects learning for imbalanced classes.
- Why unresolved: The paper doesn't investigate the potential biases introduced by the parameter retention strategy when dealing with imbalanced class distributions within tasks.
- What evidence would resolve it: Experiments with datasets containing imbalanced class distributions, analyzing the performance on minority classes and investigating potential biases introduced by the parameter retention strategy.

## Limitations
- Limited evaluation of parameter retention strategy effectiveness across different continual learning scenarios
- No detailed analysis of computational overhead and scalability to larger datasets
- Potential biases in parameter retention strategy when dealing with imbalanced class distributions

## Confidence
- **High confidence**: CLIP's zero-shot capabilities provide useful initialization for CIL (supported by abstract and method description)
- **Medium confidence**: Parameter retention based on drift effectively mitigates forgetting (mechanism described but not experimentally validated in the provided text)
- **Medium confidence**: Linear adapter with parameter retention achieves SotA performance (stated in abstract but specific results not shown in provided text)

## Next Checks
1. **Experiment**: Compare parameter retention with random retention to validate whether drift-based selection improves performance over chance
2. **Experiment**: Test adapter performance on out-of-distribution incremental tasks to verify the transferability assumption of CLIP embeddings
3. **Experiment**: Analyze the computational overhead of different adapter types (linear vs. self-attention) relative to performance gains