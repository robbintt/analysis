---
ver: rpa2
title: Feature Space Renormalization for Semi-supervised Learning
arxiv_id: '2311.04055'
source_url: https://arxiv.org/abs/2311.04055
tags:
- feature
- data
- frematch
- learning
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a feature space renormalization (FSR) mechanism
  for semi-supervised learning (SSL), aiming to improve discriminative feature learning.
  Unlike existing methods that focus on consistency regularization in the label space,
  FSR normalizes feature representations using an empirical model to ensure homeomorphic
  feature spaces.
---

# Feature Space Renormalization for Semi-supervised Learning

## Quick Facts
- arXiv ID: 2311.04055
- Source URL: https://arxiv.org/abs/2311.04055
- Reference count: 40
- One-line primary result: Feature Space Renormalization (FSR) combined with pseudo-labelling achieves state-of-the-art semi-supervised learning performance on CIFAR-10, SVHN, and STL-10.

## Executive Summary
This paper introduces Feature Space Renormalization (FSR), a novel mechanism for semi-supervised learning that focuses on improving discriminative feature learning by aligning feature spaces of weakly and strongly augmented data. Unlike existing methods that rely on consistency regularization in label space, FSR normalizes feature representations using an orthogonal transformation to ensure homeomorphic feature spaces. The authors propose FreMatch, which combines FSR with pseudo-labelling, achieving superior performance on standard SSL benchmark datasets, particularly when moderate numbers of labeled samples are available.

## Method Summary
FreMatch employs two parallel models - a basic model and an empirical model (updated with momentum). The method consists of two key steps: first, FSR applies an orthogonal transformation to align the covariance matrices of feature representations from weakly and strongly augmented data, ensuring isomorphic feature spaces. Second, pseudo-labelling generates high-confidence artificial labels for unlabelled data based on the renormalized features. The training uses SGD with momentum 0.9, cosine learning rate decay, and combines cross-entropy loss on labeled data with unsupervised loss (FSR + pseudo-labelling).

## Key Results
- On CIFAR-10 with 4000 labels, FreMatch achieves 3.47% error rate, outperforming FixMatch (4.26%) and FlexMatch (4.19%)
- Consistently matches or exceeds state-of-the-art SSL methods across CIFAR-10, SVHN, and STL-10 benchmarks
- FSR mechanism generalizes well when integrated with other SSL approaches, enhancing their performance
- Particularly effective when moderate numbers of labeled samples are available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FSR improves discriminative feature learning by enforcing homeomorphism between feature spaces of basic and empirical models.
- **Mechanism:** FSR applies orthogonal transformation ùìí to align covariance matrices of feature representations from weakly and strongly augmented data.
- **Core assumption:** Covariance matrix groups of homeomorphic feature spaces are isomorphic, and aligning them improves downstream classification.
- **Evidence anchors:** Abstract states FSR substitutes consistency regularization to learn better discriminative features; paper describes feature space renormalization on unlabelled data.
- **Break condition:** If covariance matrix groups are not isomorphic or orthogonal transformation ùìí fails to align feature spaces adequately.

### Mechanism 2
- **Claim:** Combining FSR with pseudo-labelling yields superior SSL performance compared to using either alone.
- **Mechanism:** FSR first improves feature representation quality by aligning feature spaces, then pseudo-labelling uses these refined features to generate high-confidence artificial labels.
- **Core assumption:** Sequential application of FSR followed by pseudo-labelling results in better feature representations and classification than parallel application or using only one method.
- **Evidence anchors:** Abstract mentions combining FSR with pseudo-labelling to obtain FreMatch; paper describes learning process divided into two successive steps.
- **Break condition:** If pseudo-label quality is poor or FSR transformation introduces noise, combined approach may degrade performance.

### Mechanism 3
- **Claim:** Using an empirical model with momentum-based weight updates provides more stable feature representation than using a single model.
- **Mechanism:** Empirical model aggregates weights from basic model using momentum coefficient, creating smoothed feature representation that reduces impact of noisy pseudo-labels.
- **Core assumption:** Momentum-based aggregation of model weights results in more stable and generalizable feature representations than using basic model alone.
- **Evidence anchors:** Abstract mentions momentum-based updating approach for empirical model; paper states empirical model has better intermediate representation for unlabelled data.
- **Break condition:** If momentum coefficient is not properly tuned, empirical model may become too slow to adapt or too unstable, degrading performance.

## Foundational Learning

- **Concept:** Covariance matrix and its role in feature space characterization
  - Why needed here: FSR relies on aligning covariance matrices of feature representations to ensure homeomorphic feature spaces.
  - Quick check question: What does the covariance matrix of a set of feature vectors represent, and why is it useful for characterizing the distribution of features in a space?

- **Concept:** Group representation theory and isomorphism
  - Why needed here: FSR uses concept of isomorphic covariance matrix groups to define homeomorphic feature spaces.
  - Quick check question: What does it mean for two groups to be isomorphic, and how does this relate to the concept of homeomorphic spaces?

- **Concept:** Orthogonal transformations and their properties
  - Why needed here: FSR uses orthogonal transformation ùìí to align covariance matrices, ensuring transformation preserves distances and angles.
  - Quick check question: What are the key properties of orthogonal matrices, and why are they useful for feature space alignment?

## Architecture Onboarding

- **Component map:** Input -> Weak and strong augmentation -> Basic and empirical models (feature extractor + classifier) -> FSR (orthogonal transformation ùìí) -> Pseudo-labelling -> Loss calculation -> Model update

- **Critical path:**
  1. Input: Unlabelled data
  2. Weak and strong augmentation
  3. Feature extraction by basic and empirical models
  4. FSR: Align feature spaces using ùìí
  5. Pseudo-labelling: Generate artificial labels
  6. Loss calculation and model update

- **Design tradeoffs:**
  - Using empirical model vs. single model: Empirical model provides more stable feature representations but increases computational cost
  - Orthogonal transformation vs. other alignment methods: Orthogonal transformations preserve distances and angles but may be less flexible than other methods
  - Sequential FSR and pseudo-labelling vs. parallel application: Sequential application allows for better feature refinement but may introduce additional complexity

- **Failure signatures:**
  - Poor classification performance: May indicate issues with FSR alignment, pseudo-label quality, or model architecture
  - Unstable training: May indicate issues with momentum coefficient, learning rate, or data augmentation
  - High variance in results: May indicate sensitivity to hyperparameters or data noise

- **First 3 experiments:**
  1. Test FSR alone: Apply FSR to feature representations and evaluate classification performance without pseudo-labelling
  2. Test pseudo-labelling alone: Apply pseudo-labelling to feature representations without FSR and evaluate classification performance
  3. Test different momentum coefficients: Vary momentum coefficient for updating empirical model and evaluate its impact on classification performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the FSR mechanism generalize to semi-supervised learning scenarios where the feature space is not well-behaved or exhibits complex non-linear structures?
- **Basis in paper:** Explicit - paper discusses theoretical foundation based on group representation theory and covariance matrix groups, but doesn't address limitations with highly non-linear or irregular feature spaces
- **Why unresolved:** Theoretical framework assumes well-defined covariance matrix group, which may not hold for all data distributions or feature representations; paper lacks experimental evidence or theoretical analysis for such cases
- **What evidence would resolve it:** Experiments comparing FSR's performance on datasets with known non-linear or irregular feature structures, or theoretical proofs extending group representation framework to handle such cases

### Open Question 2
- **Question:** What is the optimal strategy for selecting the threshold Œ∑ in the pseudo-labelling component of FreMatch to balance between high-confidence predictions and utilization of unlabelled data?
- **Basis in paper:** Explicit - paper mentions higher threshold (e.g., 0.95) yields better performance but doesn't provide systematic method for determining optimal threshold value
- **Why unresolved:** Threshold value likely depends on specific dataset characteristics and model's confidence in predictions; one-size-fits-all approach may not be optimal
- **What evidence would resolve it:** Detailed analysis of how threshold value affects FreMatch's performance on various datasets, or method for automatically determining optimal threshold based on data and model

### Open Question 3
- **Question:** How does the FSR mechanism affect the interpretability and explainability of the learned features in FreMatch?
- **Basis in paper:** Inferred - paper focuses on technical aspects of FSR and its impact on performance but doesn't discuss interpretability of learned features or how they relate to underlying data structure
- **Why unresolved:** Understanding interpretability of learned features is crucial for gaining insights into model's decision-making process and identifying potential biases or limitations
- **What evidence would resolve it:** Techniques for visualizing and analyzing learned features, or studies comparing interpretability of features learned by FreMatch with other SSL methods

## Limitations

- The theoretical justification for why specific FSR alignment mechanism outperforms consistency regularization remains limited
- Claim that FSR generalizes well when integrated with other SSL methods is demonstrated only through limited ablation studies rather than systematic integration experiments
- Core assumption about covariance matrix alignment relies on well-behaved feature spaces that may not hold for all data distributions

## Confidence

- High confidence in empirical results and performance claims on benchmark datasets
- Medium confidence in theoretical justification for FSR mechanism
- Medium confidence in claim that combining FSR with pseudo-labelling is superior to either alone
- Medium confidence in claim that momentum-based empirical model updates provide stability

## Next Checks

1. Test FSR alone without pseudo-labelling to isolate its contribution to performance improvement
2. Evaluate sensitivity of results to different momentum coefficients in the empirical model update
3. Conduct systematic ablation studies testing FSR integration with other SSL frameworks beyond FreMatch