---
ver: rpa2
title: Unsupervised Feature Learning with Emergent Data-Driven Prototypicality
arxiv_id: '2307.01421'
source_url: https://arxiv.org/abs/2307.01421
tags:
- images
- hyperbolic
- space
- learning
- prototypicality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes HACK, an unsupervised feature learning method\
  \ that maps images to hyperbolic space such that proximity indicates visual similarity\
  \ while radial distance encodes prototypicality. The method first generates uniformly\
  \ packed particles in the Poincar\xE9 ball via a repulsion loss, then assigns each\
  \ image to a particle using bipartite matching to minimize total hyperbolic distance."
---

# Unsupervised Feature Learning with Emergent Data-Driven Prototypicality

## Quick Facts
- arXiv ID: 2307.01421
- Source URL: https://arxiv.org/abs/2307.01421
- Reference count: 40
- Primary result: Maps images to hyperbolic space where proximity indicates visual similarity and radial distance encodes prototypicality, enabling unsupervised instance selection that reduces sample complexity by 16.54% and improves adversarial robustness by up to 8.7%

## Executive Summary
This paper introduces HACK, an unsupervised feature learning method that embeds images in hyperbolic space to capture both visual similarity and prototypicality. The method generates uniformly packed particles in the Poincaré ball and assigns each image to a particle using bipartite matching to minimize hyperbolic distance. During training, prototypical images naturally emerge near the origin due to hyperbolic geometry, while visual similarity is encoded by proximity. Experiments demonstrate that the method successfully discovers typical/atypical examples and organizes images by semantic similarity on MNIST and CIFAR10 datasets, enabling improved sample complexity and adversarial robustness.

## Method Summary
HACK generates uniformly packed particles in the Poincaré ball through a repulsion loss, then assigns each image to a particle using the Hungarian algorithm to minimize total hyperbolic distance. The image encoder (LeNet for MNIST, ResNet20 for CIFAR10) is trained to spread instances in hyperbolic space while the assignment is optimized every other epoch. The method is applied separately to each class, with prototypical images naturally emerging near the origin due to the geometry of hyperbolic space. The learned prototypicality structure enables unsupervised instance selection that reduces sample complexity and improves adversarial robustness.

## Key Results
- Congealed images (known to be more typical) are correctly placed at the center of the Poincaré ball
- 16.54% higher accuracy achieved using 10% most atypical images for training
- Up to 8.7% improvement in adversarial robustness through prototypical instance selection
- Successful discovery of typical/atypical examples and semantic organization in both MNIST and CIFAR10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proximity in hyperbolic space indicates visual similarity while radial distance encodes prototypicality
- Mechanism: Hyperbolic space has exponential distance growth toward the boundary, so similar images naturally cluster together and prototypical images (similar to many others) get pulled toward the origin by the assignment loss
- Core assumption: Images can be meaningfully embedded in hyperbolic space such that the geometry reflects both similarity and prototypicality
- Evidence anchors:
  - [abstract]: "proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is"
  - [section 4.4]: "the root of the tree can be embedded in the center of the Poincaré ball and the leaves are embedded close to the boundary"
  - [corpus]: Weak - neighboring papers focus on hyperbolic space for graph and hierarchical data, not prototypicality

### Mechanism 2
- Claim: Uniform particle packing in hyperbolic space enables instance discrimination without pushing all points to the boundary
- Mechanism: Predefined uniform particle distribution provides anchors that the assignment loss pulls images toward, preventing the collapse to boundary that happens with pure contrastive objectives
- Core assumption: Uniform particle distribution in hyperbolic space can serve as effective anchors for instance discrimination
- Evidence anchors:
  - [abstract]: "generates uniformly packed particles in the Poincaré ball via a repulsion loss, then assigns each image to a particle using bipartite matching"
  - [section 4.2]: "We derive a simple repulsion loss function to encourage the particles to be equally distant from each other"
  - [corpus]: Weak - neighboring papers use hyperbolic space for graph embeddings, not particle packing for instance discrimination

### Mechanism 3
- Claim: Bipartite matching between images and particles discovers prototypical structure without supervision
- Mechanism: Hungarian algorithm finds minimum cost assignment where prototypical images (close to many others) naturally get assigned to central particles
- Core assumption: Bipartite matching can discover meaningful structure without labels when combined with hyperbolic geometry
- Evidence anchors:
  - [abstract]: "assigns each image to a particle using bipartite matching to minimize total hyperbolic distance"
  - [section 4.3]: "The assignment should be one-to-one, i.e., each image should be assigned to one particle and each particle is allowed to be associated with one image"
  - [corpus]: Missing - no neighboring papers discuss unsupervised bipartite matching for prototypicality discovery

## Foundational Learning

- Concept: Hyperbolic geometry and Poincaré ball model
  - Why needed here: The exponential distance growth toward boundary is what enables prototypicality encoding
  - Quick check question: What happens to hyperbolic distance as points approach the boundary of the Poincaré ball?

- Concept: Bipartite matching and Hungarian algorithm
  - Why needed here: Global optimization of image-to-particle assignments enables discovery of prototypical structure
  - Quick check question: What is the time complexity of the Hungarian algorithm and why is it acceptable here?

- Concept: Sphere packing in hyperbolic space
  - Why needed here: Uniform particle distribution provides anchors for instance discrimination without boundary collapse
  - Quick check question: How does the repulsion loss ensure particles remain uniformly distributed?

## Architecture Onboarding

- Component map:
  Particle generator (repulsion + boundary loss) -> Image encoder (LeNet/ResNet → 2D Euclidean → hyperbolic) -> Bipartite matcher (Hungarian algorithm) -> Training loop (alternating assignment and feature learning)

- Critical path:
  1. Generate uniform particles in hyperbolic space
  2. Initialize random image-particle assignments
  3. For each batch: compute features, run Hungarian matching, update encoder
  4. Repeat until prototypical structure emerges

- Design tradeoffs:
  - Particle packing vs. computational cost (more particles = better discrimination but slower Hungarian)
  - Hyperbolic dimension (2D chosen for interpretability vs. higher dimensions for more expressive power)
  - Assignment frequency (every other epoch balances stability vs. responsiveness)

- Failure signatures:
  - Particles collapse to boundary or cluster together
  - Hungarian matching becomes unstable or random
  - Features don't show prototypical structure (no radial gradient)
  - Training loss plateaus early

- First 3 experiments:
  1. Train on synthetic data with known prototypical examples (congealed images) and verify they appear at origin
  2. Visualize feature norms vs. K-NN density to confirm prototypicality encoding
  3. Test instance selection on MNIST/CIFAR10 and measure sample complexity reduction

## Open Questions the Paper Calls Out

- **Open Question 1**: How does HACK perform when applied to entire datasets without class-specific separation, and what challenges arise in differentiating between classes while preserving prototypicality structure?
  - Basis in paper: [inferred] The paper states "Currently, HACK is applied to each class separately. Thus, it would be interesting to apply HACK to all the classes at once without supervision."
  - Why unresolved: The authors acknowledge this as a limitation and future direction, noting it would be "much more challenging since we need to differentiate between examples from different classes as well as the prototypical and semantic structure."
  - What evidence would resolve it: Experiments applying HACK to multi-class datasets without per-class separation, showing how well it maintains class boundaries while organizing instances by prototypicality within each class.

- **Open Question 2**: Can HACK be extended to discover and organize data according to geometric structures beyond uniform sphere packing in hyperbolic space?
  - Basis in paper: [explicit] "We consider uniform packing in hyperbolic space to organize the images. It is also possible to extend HACK by specifying other geometrical structures to encourage the corresponding organization to emerge from the dataset."
  - Why unresolved: The authors explicitly identify this as an open direction, suggesting other geometric patterns could be specified to encourage different types of data organization.
  - What evidence would resolve it: Demonstrations of HACK using different geometric arrangements (e.g., spiral packing, hierarchical clustering patterns) and showing how these lead to different emergent data organizations.

- **Open Question 3**: How does the performance of HACK scale with dataset size and dimensionality, particularly regarding the computational complexity of the Hungarian algorithm used for bipartite matching?
  - Basis in paper: [inferred] The paper mentions the Hungarian algorithm has "a complexity of O(b³), where b is the batch size" but doesn't extensively explore scaling behavior.
  - Why unresolved: While the paper notes batch-limited assignment keeps time/memory complexity "tolerable," it doesn't provide systematic analysis of how performance degrades with larger datasets or higher-dimensional features.
  - What evidence would resolve it: Empirical studies varying dataset size and dimensionality, showing how assignment quality and computational requirements change, potentially leading to optimized approximations for large-scale applications.

## Limitations

- The method is currently applied to each class separately, making it challenging to differentiate between classes while preserving prototypicality structure
- Relies on the assumption that image manifolds have hierarchical structure suitable for hyperbolic embedding, which may not generalize to all datasets
- Computational complexity of Hungarian algorithm may limit scalability to very large datasets

## Confidence

- **High Confidence**: The hyperbolic geometry provides exponential distance growth enabling prototypicality encoding; uniform particle packing prevents boundary collapse; bipartite matching finds minimum cost assignments
- **Medium Confidence**: The method successfully discovers prototypical structure in real datasets; prototypical images naturally emerge at the origin; learned features improve sample complexity and robustness
- **Low Confidence**: The method generalizes to arbitrary datasets beyond MNIST/CIFAR10; the hyperbolic embedding captures all relevant aspects of prototypicality; the specific choice of 2D hyperbolic space is optimal

## Next Checks

1. Test on datasets with known hierarchical structure (like ImageNet) to verify prototypicality encoding scales beyond simple datasets
2. Compare performance with different hyperbolic dimensions (2D vs higher) to validate the choice of 2D space
3. Evaluate the method's sensitivity to particle packing parameters by systematically varying k and r values