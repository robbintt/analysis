---
ver: rpa2
title: 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models'
arxiv_id: '2310.06117'
source_url: https://arxiv.org/abs/2310.06117
tags:
- step
- question
- reasoning
- prompting
- back
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STEP -BACK PROMPTING improves reasoning in large language models
  by first prompting them to derive high-level concepts and principles from a specific
  question, then using these abstractions to guide the reasoning process. Experiments
  with PaLM-2L, GPT-4, and Llama2-70B on tasks including STEM, Knowledge QA, and Multi-Hop
  Reasoning show substantial performance gains, such as 7-11% on MMLU Physics/Chemistry,
  27% on TimeQA, and 7% on MuSiQue.
---

# Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models

## Quick Facts
- arXiv ID: 2310.06117
- Source URL: https://arxiv.org/abs/2310.06117
- Reference count: 40
- Key outcome: Step-Back Prompting improves reasoning in LLMs by first deriving high-level concepts from specific questions, then using these abstractions to guide reasoning, achieving 7-11% gains on MMLU Physics/Chemistry, 27% on TimeQA, and 7% on MuSiQue.

## Executive Summary
Step-Back Prompting is a novel prompting strategy that significantly enhances reasoning capabilities in large language models by introducing an abstraction step before reasoning. The method prompts models to first derive high-level concepts and principles from specific questions, then uses these abstractions to guide the subsequent reasoning process. Experiments across multiple models (PaLM-2L, GPT-4, Llama2-70B) and diverse tasks (STEM, Knowledge QA, Multi-Hop Reasoning) demonstrate substantial performance improvements, with error analysis revealing that most failures occur in the reasoning step rather than the abstraction step, indicating the method's effectiveness in reducing reasoning errors.

## Method Summary
Step-Back Prompting employs a two-step approach: first, the model is prompted to derive high-level concepts and principles from a specific question (abstraction), then these abstractions are used to guide the reasoning process toward the solution. The method is sample-efficient, requiring only a few demonstrations to teach abstraction skills to LLMs. When combined with retrieval-augmented generation (RAG), the model can retrieve relevant facts about the high-level concepts to further support reasoning. The approach contrasts with traditional chain-of-thought prompting by focusing on making questions more abstract rather than decomposing them into simpler sub-questions.

## Key Results
- Achieved 7-11% improvement on MMLU Physics/Chemistry tasks
- Demonstrated 27% improvement on TimeQA knowledge QA benchmark
- Showed 7% improvement on MuSiQue multi-hop reasoning tasks
- Error analysis revealed most failures occur in reasoning step rather than abstraction
- Sample efficiency demonstrated with performance robust to varying numbers of few-shot exemplars

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-Back Prompting improves reasoning by abstracting questions to higher-level concepts, reducing the cognitive load of intermediate reasoning steps.
- Mechanism: By prompting the LLM to first derive high-level concepts or principles from a specific question, the model can retrieve relevant facts or principles that are easier to access than low-level details. These abstractions then guide the reasoning process, reducing errors in intermediate steps.
- Core assumption: LLMs have sufficient intrinsic reasoning ability once grounded on correct abstractions, and the abstraction step is easier for the model to learn than multi-step reasoning.
- Evidence anchors: Abstract states "Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution." Section 2 describes leveraging reasoning ability to ground solutions on high-level concepts.

### Mechanism 2
- Claim: Step-Back Prompting is sample-efficient, requiring few-shot examples to teach abstraction skills to LLMs.
- Mechanism: The model can learn the skill of abstraction from a small number of demonstrations, as shown by the ablation study where performance was robust to varying numbers of few-shot exemplars.
- Core assumption: Abstraction is a simpler skill for LLMs to acquire compared to complex reasoning, and a single demonstration is often sufficient.
- Evidence anchors: Section 4.3 shows "Adding more demonstration examples beyond a single example is not helpful any more. This indicates that the task of retrieving the relevant principles and concepts is relatively easy to learn and a single demonstration suffices." Section 5.3 notes performance robustness against the number of exemplars.

### Mechanism 3
- Claim: Step-Back Prompting reduces reasoning errors by grounding the solution on high-level concepts, making the reasoning process more reliable.
- Mechanism: By retrieving facts about high-level concepts or principles, the model can avoid errors in intermediate reasoning steps that often occur when reasoning directly on low-level details.
- Core assumption: Facts about high-level concepts are more accessible and less prone to hallucination than low-level details, making the reasoning process more reliable.
- Evidence anchors: Abstract reveals "Error analysis reveals that most failures occur in the reasoning step rather than abstraction, indicating the method's effectiveness in reducing reasoning errors." Section 4.3 shows "Principle Error in fact comprises only a small fraction of the errors the model makes: more than 90% of the errors happen at the Reasoning step."

## Foundational Learning

- Concept: Abstraction in human cognition
  - Why needed here: Understanding how humans use abstraction to solve complex tasks provides the motivation for Step-Back Prompting and helps in designing effective abstractions for LLMs.
  - Quick check question: Can you provide an example of how humans use abstraction to simplify a complex problem in your field of expertise?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is used in combination with Step-Back Prompting to retrieve relevant facts about high-level concepts, which are then used to guide the reasoning process.
  - Quick check question: How does RAG work, and what are its advantages and limitations in the context of Step-Back Prompting?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT is a baseline method for improving reasoning in LLMs, and understanding its limitations helps in appreciating the advantages of Step-Back Prompting.
  - Quick check question: What are the key differences between CoT and Step-Back Prompting, and in what scenarios might each method be more effective?

## Architecture Onboarding

- Component map: Question -> Abstraction (Step-Back Prompting) -> Reasoning (CoT-style) -> Answer
- Critical path: 1. Receive a question 2. Generate a step-back question (abstraction) 3. Retrieve relevant facts about the high-level concept (RAG, optional) 4. Use the retrieved facts to guide reasoning and derive the final answer
- Design tradeoffs: Abstraction vs. Decomposition: Step-Back Prompting focuses on making questions more abstract, while decomposition breaks down questions into simpler sub-questions; Sample efficiency: Step-Back Prompting requires few-shot examples, while other methods may need more demonstrations; RAG integration: Optional use of RAG depending on the nature of the task
- Failure signatures: Abstraction step fails to retrieve correct high-level concept or principle; RAG fails to retrieve relevant information despite correct abstraction; Reasoning step fails despite correct abstraction and retrieved facts
- First 3 experiments: 1. Implement Step-Back Prompting without RAG on a simple QA task and compare performance with CoT and baseline; 2. Add RAG to Step-Back Prompting and evaluate performance on a knowledge-intensive QA task; 3. Conduct an ablation study to determine the optimal number of few-shot examples for teaching abstraction skills

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of task complexity where STEP-BACK PROMPTING remains effective, and how does this limit scale with model size?
- Basis in paper: [inferred] The paper shows effectiveness on various reasoning tasks but doesn't establish boundaries of task complexity where the method fails.
- Why unresolved: The experiments cover a range of reasoning tasks but don't systematically explore increasingly complex tasks to find the breaking point.
- What evidence would resolve it: A systematic study varying task complexity parameters (number of reasoning steps, abstract concept depth, etc.) across different model sizes to identify where performance degrades.

### Open Question 2
- Question: How does STEP-BACK PROMPTING performance compare to fine-tuning approaches for the same tasks?
- Basis in paper: [inferred] The paper only compares to prompting baselines, not parameter-updating methods.
- Why unresolved: Without comparing to fine-tuned models, we can't determine if the abstraction approach is more efficient than updating model parameters.
- What evidence would resolve it: Head-to-head comparison of STEP-BACK PROMPTING against models fine-tuned specifically on the target tasks, measuring both performance and computational costs.

### Open Question 3
- Question: Can STEP-BACK PROMPTING be extended to generate useful abstractions automatically rather than relying on handcrafted step-back questions?
- Basis in paper: [explicit] The paper states "The step-back question is extracted from the model output using the prompt" suggesting manual construction of examples.
- Why unresolved: The current method requires manual creation of step-back question demonstrations, limiting scalability to new domains.
- What evidence would resolve it: A method that can generate appropriate step-back questions from scratch given only the original question, validated across diverse domains.

## Limitations

- The exact prompt templates and few-shot examples are not fully specified, making precise replication challenging
- The evaluation methodology using PaLM-2L to score equivalence between target answers and model predictions lacks detailed description
- The paper doesn't thoroughly investigate whether improvements stem from better reasoning versus improved retrieval of factual knowledge

## Confidence

**High Confidence**: The core mechanism of using abstraction to guide reasoning is well-supported by the empirical results showing consistent improvements across multiple tasks and model sizes.

**Medium Confidence**: The claim of sample efficiency appears well-supported by the ablation studies, but the generality of this finding across different types of abstractions and reasoning tasks remains uncertain.

**Low Confidence**: The interaction between Step-Back Prompting and RAG is not thoroughly explored, and the conditions under which RAG adds value versus potentially introducing noise are unclear.

## Next Checks

1. **Replication with Controlled Prompts**: Implement the Step-Back Prompting method using multiple independent prompt templates while keeping the core abstraction mechanism constant. Compare performance across different prompt variations to establish the robustness of the approach beyond the specific prompts used in the original paper.

2. **Ablation of RAG Component**: Conduct experiments that systematically vary the presence and quality of retrieved facts during the reasoning step. This will help determine whether performance gains are primarily due to improved reasoning versus enhanced knowledge retrieval, and identify scenarios where RAG integration is most beneficial.

3. **Transfer Learning Assessment**: Test whether models trained with Step-Back Prompting on one domain (e.g., physics) can effectively apply abstraction skills to novel domains (e.g., biology or social reasoning) without additional fine-tuning. This would validate the generality of the abstraction learning mechanism claimed by the authors.