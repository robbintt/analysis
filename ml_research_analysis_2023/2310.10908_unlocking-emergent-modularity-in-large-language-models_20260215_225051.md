---
ver: rpa2
title: Unlocking Emergent Modularity in Large Language Models
arxiv_id: '2310.10908'
source_url: https://arxiv.org/abs/2310.10908
tags:
- emoe
- experts
- dense
- gmoe
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that fine-tuning pre-trained transformers
  as Mixture-of-Experts (MoEs) without introducing extra parameters can improve downstream
  in-domain and out-of-domain generalization compared to standard fine-tuning. The
  authors propose a method called EMoE that leverages emergent modularity in pre-trained
  transformers by clustering neurons in the Feed-Forward Networks (FFNs) and using
  an average-key gating mechanism to route inputs to the resulting experts.
---

# Unlocking Emergent Modularity in Large Language Models

## Quick Facts
- arXiv ID: 2310.10908
- Source URL: https://arxiv.org/abs/2310.10908
- Authors: 
- Reference count: 40
- Primary result: Fine-tuning pre-trained transformers as Mixture-of-Experts without extra parameters improves downstream generalization

## Executive Summary
This paper demonstrates that pre-trained transformers develop emergent modular structures that can be leveraged for more effective fine-tuning. The authors propose EMoE (Emergent Mixture-of-Experts), which clusters neurons in Feed-Forward Networks and uses average-key gating to route inputs to experts. This approach improves both in-domain and out-of-domain generalization without introducing additional parameters, outperforming standard fine-tuning and achieving results comparable to parameter-intensive GMoE.

## Method Summary
EMoE fine-tunes pre-trained transformers by first clustering neurons in FFNs to create expert groups based on functional similarity. It then applies an average-key gating mechanism to route inputs through a Top-k selection of these experts. The method preserves the pre-trained model's parameters while only updating gating weights during fine-tuning, enabling efficient adaptation to downstream tasks without the computational overhead of traditional MoE architectures.

## Key Results
- EMoE consistently outperforms vanilla fine-tuning across vision and language tasks
- Achieves comparable performance to GMoE while using no additional parameters
- Demonstrates improved data efficiency, requiring less than 20% of data for superior results
- Shows robustness across different expert configurations and scales to large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMoE improves fine-tuning performance by deactivating neurons with negative transfer effects from pre-trained dense models.
- Mechanism: During fine-tuning, EMoE's Top-k expert selection blocks activation of neurons that would otherwise contribute negatively to downstream task performance, while preserving positive contributions.
- Core assumption: Some neurons in pre-trained FFNs are specialized for tasks or features that harm performance on new downstream tasks.
- Evidence anchors:
  - "EMoE masks neurons with negative transfer impacts. The only difference between EMoE and vanilla LoRA tuning is EMoE blocks some activated neurons during training by Top-k expert selection."
  - "LoRA tuning results with 'Bottom-k' and 'Not-top-k' expert selections are worse than vanilla LoRA tuning. This further corroborates that masked neurons have negative transfer effects."
- Break condition: If downstream tasks benefit from all pre-trained knowledge equally, or if gating cannot distinguish helpful from harmful neurons.

### Mechanism 2
- Claim: EMoE leverages emergent modular structures in pre-trained transformers to create heterogeneous expert groups.
- Mechanism: Clustering similar key vectors in FFNs creates experts with distinct functional specializations that match the pre-trained model's modular structure, providing better routing for downstream tasks.
- Core assumption: Pre-trained transformers develop implicit modular structures during training, with neurons naturally grouping by function.
- Evidence anchors:
  - "They indicate that such modular structures spontaneously exhibit during the early pre-training phase."
  - "Figure 4 demonstrates that clustering can decompose modular components within the dense model."
  - "It's noteworthy that while cluster top-k exhibits a significant improvement over dense, random top-k is conversely worse than dense baseline."
- Break condition: If clustering fails to capture meaningful functional groupings, or if pre-trained models lack modular structure.

### Mechanism 3
- Claim: EMoE achieves improved data efficiency through modular routing compared to dense fine-tuning.
- Mechanism: By activating only relevant expert groups for each input, EMoE reduces interference between unrelated feature representations, requiring less data to learn task-specific patterns.
- Core assumption: Sparse expert activation reduces crosstalk between incompatible learned representations.
- Evidence anchors:
  - "EMoE consistently outperforms the dense across different data factions. EMoE achieves superior results even when using less than 20% of the data."
  - "Previous research has indicated that modular architectures offer improved data efficiency."
- Break condition: If downstream tasks require integration of all pre-trained knowledge simultaneously.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: EMoE builds on MoE principles to create modular routing without extra parameters.
  - Quick check question: What distinguishes a MoE layer from a standard dense layer in terms of computation and routing?

- Concept: Clustering and vector similarity
  - Why needed here: Clustering key vectors in FFNs creates expert groups based on functional similarity.
  - Quick check question: Why does grouping similar key vectors help create functionally specialized experts?

- Concept: Transfer learning and negative transfer
  - Why needed here: Understanding when pre-trained knowledge helps or harms new task learning is central to EMoE's mechanism.
  - Quick check question: How can we empirically identify whether a neuron or feature is causing negative transfer?

## Architecture Onboarding

- Component map:
  Input → Clustering-based expert decomposition → Average-key gating → Top-k routing → Expert computation → Weighted sum
  Dense FFNs replaced with clustered experts and gating mechanism
  No new parameters added, only reorganization of existing weights

- Critical path:
  1. Clustering keys in pre-trained FFNs to form expert groups
  2. Computing average keys per expert for gating
  3. Routing input through Top-k selected experts
  4. Fine-tuning with frozen expert structure but updated gating weights

- Design tradeoffs:
  - Fixed expert structure vs. learnable gating weights
  - Number of experts vs. computational efficiency
  - Top-k selection ratio vs. expressiveness vs. negative transfer risk

- Failure signatures:
  - Performance worse than dense baseline indicates poor clustering or harmful expert selection
  - Unstable training suggests gating conflicts or expert load imbalance
  - No improvement on small datasets may indicate insufficient modular structure

- First 3 experiments:
  1. Apply EMoE to a small pre-trained model on a single GLUE task, compare with dense baseline
  2. Test different numbers of experts (16, 32, 64) on the same task to find optimal configuration
  3. Compare Top-k selection with random expert selection to validate clustering benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the emergent modularity in pre-trained transformers evolve during the pre-training process, and at what point does it become stable?
- Basis in paper: The paper mentions that modularity structures emerge and remain stable after approximately one-fourth of the pre-training phase, but does not provide detailed analysis of this evolution.
- Why unresolved: The paper only briefly mentions this observation without providing a detailed study of how modularity evolves during pre-training.
- What evidence would resolve it: A detailed longitudinal study tracking the emergence and stability of modularity throughout the entire pre-training process would provide insights into when and how emergent modularity develops.

### Open Question 2
- Question: Can the benefits of emergent modularity be further enhanced by combining EMoE with other parameter-efficient fine-tuning methods or architectural modifications?
- Basis in paper: The paper demonstrates the effectiveness of EMoE in improving downstream performance but does not explore combinations with other techniques.
- Why unresolved: The study focuses on EMoE as a standalone method and does not investigate potential synergies with other approaches.
- What evidence would resolve it: Experiments combining EMoE with various parameter-efficient fine-tuning methods or architectural modifications, and comparing their performance to EMoE alone, would reveal potential enhancements.

### Open Question 3
- Question: How does the performance of EMoE scale with model size, particularly for very large language models like LLaMA?
- Basis in paper: The paper mentions that the findings have not been validated on very large models like LLaMA due to computational constraints.
- Why unresolved: The experiments were conducted on models up to 1.5B parameters, leaving the performance of EMoE on much larger models unexplored.
- What evidence would resolve it: Scaling experiments applying EMoE to increasingly larger models, up to and including very large language models, would demonstrate its effectiveness across a broader range of model sizes.

## Limitations
- Empirical evidence for negative transfer is indirect, relying on ablation studies rather than direct measurement
- Does not fully address how clustering handles neurons with mixed or multi-functional roles
- Limited analysis of computational overhead and scalability to very large models

## Confidence
- Mechanism 1 (Negative transfer mitigation): Medium - supported by ablations but indirect evidence
- Mechanism 2 (Emergent modularity): Medium-High - clustering results align with claims but functional validation is limited
- Mechanism 3 (Data efficiency): Low-Medium - empirical results show improvement but mechanism is under-explained

## Next Checks
1. **Transfer Effect Isolation**: Design an experiment that directly measures the contribution of individual neurons to downstream performance, rather than inferring negative transfer from ablation studies alone.

2. **Cluster Stability Analysis**: Evaluate the consistency of neuron clusters across multiple random initializations and different pre-training checkpoints to assess the reliability of the emergent modular structure.

3. **Comparative Efficiency Benchmark**: Compare EMoE's data efficiency against other parameter-efficient fine-tuning methods (like LoRA, prefix tuning) under identical conditions to determine if the benefit is specific to the MoE architecture or a general property of sparse fine-tuning approaches.