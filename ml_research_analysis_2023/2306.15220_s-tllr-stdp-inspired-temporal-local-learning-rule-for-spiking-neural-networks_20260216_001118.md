---
ver: rpa2
title: 'S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks'
arxiv_id: '2306.15220'
source_url: https://arxiv.org/abs/2306.15220
tags:
- learning
- bptt
- s-tllr
- spiking
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel three-factor temporal local learning
  rule for spiking neural networks (SNNs), called S-TLLR, inspired by the STDP mechanism.
  S-TLLR addresses the challenges of training SNNs by leveraging both causal and non-causal
  relationships between pre- and post-synaptic activities, while maintaining low memory
  and time complexity.
---

# S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks

## Quick Facts
- **arXiv ID**: 2306.15220
- **Source URL**: https://arxiv.org/abs/2306.15220
- **Reference count**: 40
- **Primary result**: S-TLLR achieves BPTT-level accuracy on event-based tasks with 5-50x memory reduction and 1.3-6.6x fewer multiply-accumulate operations.

## Executive Summary
This paper introduces S-TLLR, a three-factor temporal local learning rule for spiking neural networks inspired by STDP. The method addresses the challenge of training SNNs by computing an instantaneous eligibility trace that captures both causal and non-causal spike-time relationships. Unlike backpropagation through time, S-TLLR maintains temporal locality while achieving comparable accuracy to BPTT. Extensive experiments on DVS Gesture, SHD, DVS CIFAR-10, and MVSEC datasets demonstrate that S-TLLR reduces memory complexity from O(n²) to O(n) and significantly decreases computational requirements while maintaining competitive performance.

## Method Summary
S-TLLR is a temporal local learning rule that computes an instantaneous eligibility trace based on pre- and post-synaptic activities, modulated by a global learning signal. The eligibility trace captures both causal (pre-then-post) and non-causal (post-then-pre) spike-time relationships through a generalized STDP formulation. Weight updates are applied only during the last Tl timesteps of a sequence, making the method computationally efficient. The learning signal can be generated via backpropagation through layers or direct feedback alignment. This approach achieves temporal locality by avoiding storage of past spike states, reducing memory complexity from O(n²) to O(n) while maintaining performance comparable to BPTT.

## Key Results
- S-TLLR achieves VGG-9 accuracy of 96.2% on DVS Gesture (BPTT: 96.5%) with 5.2x reduction in multiply-accumulate operations
- On SHD RSNN, S-TLLR reaches 91.1% accuracy (BPTT: 91.3%) with 3.4x fewer MACs
- For MVSEC optical flow, S-TLLR achieves AEE of 0.372 (BPTT: 0.364) with 6.6x reduction in computations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S-TLLR achieves temporal locality by using an instantaneous eligibility trace that does not require storing past spike states.
- Mechanism: The eligibility trace e_ij[t] is computed forward in time from pre- and post-synaptic activities using a generalized STDP formulation. This avoids the O(n²) memory cost of recurrent eligibility traces (8) and reduces it to O(n).
- Core assumption: Temporal information necessary for plasticity can be captured in a single-time-step computation using low-pass filtered spike traces.
- Evidence anchors:
  - [abstract] "S-TLLR has low memory and time complexity, which is independent of the number of time steps"
  - [section] "Since all the variables involved in the computation of e_ij can be computed forward in time, our method is temporally local"
- Break condition: If the instantaneous trace fails to encode sufficient temporal credit assignment, accuracy will degrade compared to BPTT.

### Mechanism 2
- Claim: Including non-causal spike-time relationships improves generalization in tasks with strong temporal structure.
- Mechanism: The generalized STDP equation in (10) adds a term that correlates current pre-synaptic activity with past post-synaptic activity (non-causal). This reinforces synchrony patterns, beneficial in recurrent networks and audio tasks.
- Core assumption: Non-causal spike correlations carry task-relevant information that causal-only rules miss.
- Evidence anchors:
  - [abstract] "S-TLLR considers both causal and non-causal relationships between pre and post-synaptic activities"
  - [section] "using α_post = 1 improves the average performance over using only casual terms, as shown in Fig. 5a"
- Break condition: If the non-causal term introduces noise or destabilizes learning in spatially dominated tasks, accuracy will drop.

### Mechanism 3
- Claim: Weight updates are sparse in time, reducing compute by 1.1-10x while maintaining accuracy.
- Mechanism: The learning signal δ_i[t] is applied only for a subset of time steps (Tl), typically the last few. This skips gradient computation in earlier timesteps, drastically cutting multiply-accumulate operations.
- Core assumption: Most temporal credit assignment can be captured in the final timesteps of a sequence.
- Evidence anchors:
  - [abstract] "S-TLLR achieved high accuracy with a reduction in the number of computations between 1.1 − 10×"
  - [section] "Tl = 5 for DVS Gesture, Tl = 90 for SHD, and Tl = 1 for optical flow"
- Break condition: If early-time errors are critical, performance will suffer when Tl is too small.

## Foundational Learning

- Concept: Leaky Integrate-and-Fire (LIF) dynamics
  - Why needed here: S-TLLR updates weights based on membrane potential u_i[t] and spike output y_i[t], so understanding LIF is essential to follow the eligibility trace computation.
  - Quick check question: What happens to u_i[t] when y_i[t] = 1 in hard reset mode?

- Concept: STDP temporal windows
  - Why needed here: S-TLLR’s eligibility trace (10) generalizes STDP’s causal/non-causal terms; knowing how λ_pre and λ_post decay over time is critical to tune performance.
  - Quick check question: How does increasing λ_pre affect the weight of past pre-synaptic activity in the eligibility trace?

- Concept: Three-factor learning rules
  - Why needed here: S-TLLR modulates an eligibility trace with a global learning signal; understanding the interplay between these factors is key to debugging training.
  - Quick check question: If δ_i[t] is zero for all t, what happens to weight updates?

## Architecture Onboarding

- Component map: LIF neurons -> Eligibility trace module -> Learning signal generator -> Weight update block
- Critical path:
  1. Forward pass: u(t) -> y(t) -> e(t)
  2. Learning signal computation (last Tl timesteps)
  3. Weight update (only during Tl)
- Design tradeoffs:
  - Memory vs accuracy: O(n) memory eligibility trace trades off some temporal precision vs O(n²) recurrent trace
  - Compute vs performance: Reducing Tl saves compute but may hurt accuracy if early errors matter
- Failure signatures:
  - Weights stop changing -> check if δ_i[t] is non-zero and eligibility trace is computed correctly
  - Degraded accuracy vs BPTT -> verify α_post sign and magnitude, ensure Tl is not too small
  - Unstable training -> check λ_pre/λ_post values and ensure membrane potential updates are stable
- First 3 experiments:
  1. Train VGG-9 on DVS Gesture with S-TLLR, compare accuracy and MAC count to BPTT baseline
  2. Vary α_post ∈ {-1, 0, 1} on SHD RSNN, observe impact on convergence and accuracy
  3. Set Tl = 1 on MVSEC optical flow, verify 10x reduction in MACs while maintaining AEE within 0.1 of BPTT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between causal and non-causal terms in S-TLLR for different types of spatio-temporal tasks?
- Basis in paper: [explicit] The paper shows experimental results demonstrating improved performance when including non-causal terms for certain tasks (DVS Gesture, SHD), but not for others (DVS CIFAR10 with DFA). It also explores the effects of varying the STDP parameters λpre, αpost, and αpre.
- Why unresolved: The paper only explores a limited range of parameter values and does not provide a systematic analysis of the optimal trade-off for different task types. The choice of parameters seems to depend on the specific task characteristics (spatial vs. temporal information predominance).
- What evidence would resolve it: A comprehensive experimental study varying the STDP parameters across a wide range of spatio-temporal tasks, analyzing the performance as a function of the causal/non-causal balance. This could involve meta-learning or automatic hyperparameter optimization techniques.

### Open Question 2
- Question: How does S-TLLR's performance scale with network depth and width compared to BPTT and other temporal local learning rules?
- Basis in paper: [inferred] The paper mentions that S-TLLR has linear memory complexity with the number of neurons, unlike other methods with quadratic complexity. It demonstrates competitive performance on tasks with VGG-9, U-Net-like, and recurrent architectures. However, it does not provide a systematic analysis of scaling behavior with network size.
- Why unresolved: The paper focuses on specific network architectures and does not explore how S-TLLR's performance and computational efficiency scale with increasing network depth and width. This is crucial for understanding its practical applicability to large-scale deep learning tasks.
- What evidence would resolve it: Experiments systematically varying the number of layers and neurons in the network, comparing S-TLLR's performance and computational requirements (memory, time) to BPTT and other methods across different tasks. This could involve scaling studies and theoretical analysis of complexity.

### Open Question 3
- Question: Can S-TLLR be extended to handle more complex learning scenarios, such as continual learning, few-shot learning, or reinforcement learning?
- Basis in paper: [inferred] The paper focuses on supervised learning tasks (classification, optical flow estimation) using event-based datasets. While it mentions the potential for online learning, it does not explore more complex learning scenarios. The use of a global learning signal suggests potential for incorporating feedback from different sources.
- Why unresolved: The paper demonstrates S-TLLR's effectiveness for standard supervised learning tasks but does not investigate its applicability to more challenging learning scenarios. This is an important area for future research, as these scenarios are common in real-world applications.
- What evidence would resolve it: Experiments applying S-TLLR to continual learning benchmarks, few-shot learning tasks, or reinforcement learning environments. This would involve adapting the learning rule to handle new classes or tasks without forgetting previous knowledge, learning from limited data, or optimizing for long-term rewards. Theoretical analysis of S-TLLR's properties in these contexts could also provide insights.

## Limitations
- Several implementation details remain unspecified, including the exact form of the secondary activation function Ψ and the specific surrogate gradient function used during learning signal generation
- The paper does not provide a systematic analysis of how S-TLLR's performance scales with network depth and width compared to BPTT
- Limited exploration of S-TLLR's applicability to more complex learning scenarios beyond standard supervised learning tasks

## Confidence

- **High confidence**: Memory complexity claims (O(n) vs O(n²)) and the general mechanism of temporal locality through instantaneous eligibility traces
- **Medium confidence**: The performance benefits of including non-causal terms (α_post = 1), as this is supported by experimental results but the underlying mechanism is not fully explained
- **Medium confidence**: The effectiveness of sparse weight updates (reducing to last Tl timesteps), as this represents a significant architectural choice with limited ablation study coverage

## Next Checks
1. Implement and test the exact form of the secondary activation function Ψ to verify its impact on eligibility trace computation and learning stability
2. Systematically vary α_post values ({-1, 0, 1}) on multiple datasets to quantify the trade-off between causal and non-causal spike-time relationships
3. Conduct an ablation study on Tl (time window for weight updates) across different task types to identify the minimum viable value that maintains performance within 5% of BPTT