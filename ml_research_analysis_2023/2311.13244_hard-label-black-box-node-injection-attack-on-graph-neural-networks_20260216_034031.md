---
ver: rpa2
title: Hard Label Black Box Node Injection Attack on Graph Neural Networks
arxiv_id: '2311.13244'
source_url: https://arxiv.org/abs/2311.13244
tags:
- graph
- node
- attack
- nodes
- injected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first hard-label black-box node injection\
  \ attack on graph neural networks for graph classification tasks. The method builds\
  \ on edge perturbation attacks by restricting optimization to node injection, allowing\
  \ no access to model architecture, gradients, or logits\u2014only final labels."
---

# Hard Label Black Box Node Injection Attack on Graph Neural Networks

## Quick Facts
- arXiv ID: 2311.13244
- Source URL: https://arxiv.org/abs/2311.13244
- Reference count: 0
- First hard-label black-box node injection attack on graph neural networks for graph classification tasks

## Executive Summary
This paper introduces the first hard-label black-box node injection attack on graph neural networks for graph classification tasks. The method builds on edge perturbation attacks by restricting optimization to node injection, allowing no access to model architecture, gradients, or logits—only final labels. It evaluates on COIL-DEL (100-class object classification), IMDB-BINARY (binary social network classification), and NCI1 (binary chemical property classification). The attack injects nodes with features initialized based on dataset-specific strategies (random, mean, or perturbed from existing node features) and connects them via random or mode (highest-degree node) strategies. Results show success rates ranging from ~48% to ~72% depending on dataset and injection budget, with mode connection initialization generally outperforming random. The attack achieves non-negligible success while keeping perturbations subtle, though effectiveness varies with dataset class complexity.

## Method Summary
The attack injects k new nodes with features initialized using dataset-specific strategies (random, mean, or perturbed from existing nodes). Connections are established via random or mode (highest-degree node) strategies. The optimization process iteratively flips edges incident to injected nodes using a gradient approximation method under budget constraints to maximize label change. The attack assumes no access to model architecture, gradients, or logits—only final labels are available. Feature initialization varies by dataset: COIL-DEL uses Gaussian sampling from node coordinates, IMDB-BINARY uses all-ones vectors, and NCI1 uses perturbed one-hot atom-type encodings. The perturbation matrix Θ controls which edges are flipped to shift the classification decision.

## Key Results
- Success rates range from ~48% to ~72% depending on dataset and injection budget
- Mode connection initialization generally outperforms random initialization
- IMDB-BINARY requires the most edge perturbations, suggesting decision boundaries are farther from original graph embeddings
- Attack effectiveness correlates with dataset class complexity, with binary classification tasks requiring more perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack succeeds by injecting new nodes whose features are initialized to blend with the existing graph structure while their connections are perturbed to influence the graph classification boundary.
- Mechanism: The injected nodes are initialized with features derived from dataset statistics (random, mean, or perturbed from existing nodes). They are then connected to the original graph via either random or mode (highest-degree node) strategies. The perturbation matrix Θ controls which edges are flipped to shift the classification decision.
- Core assumption: The graph neural network aggregates neighborhood information such that nodes connected to influential (high-degree) pivot nodes can disproportionately affect the final graph-level prediction.
- Evidence anchors:
  - [abstract] "restricting optimization to node injection, allowing no access to model architecture, gradients, or logits—only final labels."
  - [section] "Our attack is based on an existing edge perturbation attack, from which we restrict the optimization process to formulate a node injection attack."
- Break condition: If the dataset has many classes or the decision boundary is far from the original graph embedding, the perturbation required to flip the label becomes too large, making the attack less effective.

### Mechanism 2
- Claim: The success rate is higher when injected nodes are connected to the node with the highest degree (mode initialization) compared to random connection.
- Mechanism: High-degree nodes act as hubs in graph neural networks, meaning they aggregate more neighborhood information and thus have greater influence on the final graph embedding. Connecting adversarial nodes to these hubs amplifies their impact on the classification outcome.
- Core assumption: Graph neural networks use message passing where central nodes accumulate more influence over time through repeated aggregation steps.
- Evidence anchors:
  - [section] "the node with highest rank in the graph should have the greatest influence on the final prediction of the graph, and we can such node as pivot node."
  - [section] "the experiments demonstrated that just initializing the connection is sufficient for keeping the new nodes connected to the graph through out the attack."
- Break condition: In datasets with highly uniform degree distributions, the advantage of mode initialization over random initialization diminishes.

### Mechanism 3
- Claim: Feature manipulation of injected nodes (using slightly perturbed features from existing nodes) improves attack success rate and reduces the number of edge perturbations needed.
- Mechanism: By selecting features that are close to existing nodes but distinct enough to avoid detection, the injected nodes can blend in while still providing adversarial influence. Connecting all injected nodes to a pivot node ensures maximum impact during message passing.
- Core assumption: The GNN model is sensitive to feature similarity and uses features as part of the node embedding process that influences graph classification.
- Evidence anchors:
  - [section] "Since it is hard to conduct optimization process in a hard-label black box attack setting, we only focus on manipulating the injected features at the initialization phase."
  - [section] "we choose features from the original graph and slightly perturb the feature to make sure it is not same to any node in the graph or to any other injected nodes."
- Break condition: If the dataset has highly discriminative features, even small perturbations may make the injected nodes stand out, reducing the attack's stealthiness.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: The attack relies on understanding how GNNs aggregate information from neighbors to influence graph-level predictions.
  - Quick check question: In a GNN with k layers, how many hops of neighbors does each node incorporate into its embedding?

- Concept: Adversarial machine learning in discrete domains
  - Why needed here: The attack must work in a discrete graph structure where gradients are not directly available, requiring optimization techniques like binary search and iterative perturbation.
  - Quick check question: Why is it harder to perform adversarial attacks on graphs compared to continuous image data?

- Concept: Hard-label black-box attack methodology
  - Why needed here: The attack only has access to final labels, not logits or gradients, requiring techniques like zeroth-order optimization or random search to find effective perturbations.
  - Quick check question: What is the main challenge of hard-label attacks compared to soft-label attacks?

## Architecture Onboarding

- Component map:
  Input -> Node injection module -> Connection initialization -> Perturbation optimization -> Output

- Critical path:
  1. Initialize injected node features based on dataset statistics
  2. Connect injected nodes using mode or random strategy
  3. Optimize perturbation matrix Θ to flip the classification label
  4. Ensure perturbation budget is not exceeded
  5. Validate success by checking label change

- Design tradeoffs:
  - Feature initialization: Random features are simpler but less stealthy; dataset-specific features are more complex but blend better.
  - Connection strategy: Mode initialization is more effective but more noticeable; random is stealthier but less potent.
  - Budget allocation: More nodes increase success rate but reduce stealthiness.

- Failure signatures:
  - Low success rate across all datasets indicates the perturbation budget is insufficient or the initialization strategy is ineffective.
  - High number of edge perturbations suggests the decision boundary is far from the original graph embedding.
  - No change in prediction after initialization indicates the injected nodes are not well-positioned to influence the graph representation.

- First 3 experiments:
  1. Run the attack on COIL-DEL with 10% node injection budget using mode connection initialization; expect ~50% success rate.
  2. Compare mode vs random connection initialization on IMDB-BINARY with 15% budget; expect mode to perform better.
  3. Test feature manipulation on COIL-DEL with 20% budget; expect improved success rate and fewer edge perturbations compared to random feature initialization.

## Open Questions the Paper Calls Out

- Question: Does the attack success rate correlate with the number of classes in the dataset (100-class vs binary classification)?
  - Basis in paper: [explicit] Authors observed significantly higher perturbation edge counts for IMDB-BINARY and NCI1 (binary classification) compared to COIL-DEL (100-class), and attributed this to decision boundaries being "very far away" in binary classification tasks.
  - Why unresolved: The paper provides correlation but not causation analysis. They don't test intermediate-class datasets or systematically vary class numbers to establish the relationship.
  - What evidence would resolve it: Experiments testing the attack across datasets with varying class counts (e.g., 2, 5, 10, 50, 100 classes) to measure how success rate and perturbation requirements change.

- Question: What is the optimal strategy for feature initialization of injected nodes that balances attack effectiveness with stealthiness?
  - Basis in paper: [explicit] Authors note that current injected features are "all fixed after initialization" and suggest "how to develop a concrete method to update and find the optimal injected node feature is a very interesting topic" as future work.
  - Why unresolved: The paper only tests static initialization methods (random, mean, perturbed features) and doesn't explore dynamic optimization or learnable feature strategies.
  - What evidence would resolve it: Comparative experiments testing dynamic feature optimization methods against static initialization across multiple datasets, measuring both attack success and detection likelihood.

- Question: How does the attack perform against different GNN architectures (beyond GIN) under the same hard-label black-box setting?
  - Basis in paper: [explicit] The experiments only evaluate against GIN models, with the authors noting they "assume no prior knowledge about the model architecture" but only test one architecture.
  - Why unresolved: The paper doesn't test transferability or performance consistency across different GNN architectures like GCN, GraphSAGE, or GAT models.
  - What evidence would resolve it: Experiments running the same attack methodology against multiple GNN architectures on identical datasets to measure success rate variance and transferability properties.

## Limitations
- Limited dataset diversity with only three graph classification tasks, making generalization claims uncertain
- Unknown how attack performance scales with larger graphs or more complex class boundaries
- Lacks detailed analysis of how detectable the injected nodes and edge perturbations are to defenders

## Confidence
- High confidence in the attack methodology and optimization framework, as it builds on established edge perturbation techniques with clear implementation details
- Medium confidence in effectiveness claims, as success rates vary significantly across datasets and budgets, with COIL-DEL showing particularly volatile results
- Low confidence in stealth claims, as the paper lacks detailed analysis of how detectable the injected nodes and edge perturbations are to defenders

## Next Checks
1. Test attack transferability across different GNN architectures (GraphSAGE, GAT, GCN) to validate robustness beyond GIN
2. Measure detection rate by defenders using anomaly detection on node degree distributions and feature similarity scores
3. Evaluate attack performance on datasets with more classes (e.g., 20-50 classes) to assess scalability limits