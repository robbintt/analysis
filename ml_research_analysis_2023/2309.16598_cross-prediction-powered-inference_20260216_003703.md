---
ver: rpa2
title: Cross-Prediction-Powered Inference
arxiv_id: '2309.16598'
source_url: https://arxiv.org/abs/2309.16598
tags:
- inference
- cross-prediction
- data
- classical
- prediction-powered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-prediction is a method for semi-supervised inference that
  uses machine learning to impute missing labels and applies debiasing to ensure valid
  statistical inference. Given a small labeled dataset and a large unlabeled dataset,
  it partitions the labeled data into folds, trains models on all but one fold, and
  uses the trained models to impute predictions on the unlabeled data while debiasing
  using the labeled data.
---

# Cross-Prediction-Powered Inference

## Quick Facts
- arXiv ID: 2309.16598
- Source URL: https://arxiv.org/abs/2309.16598
- Reference count: 40
- Cross-prediction imputes missing labels via ML and applies debiasing to achieve valid inference with improved power.

## Executive Summary
Cross-prediction is a semi-supervised inference method that combines machine learning predictions with classical statistical techniques to perform valid inference when labeled data is scarce but unlabeled data is abundant. The method partitions labeled data into folds, trains models on subsets, and uses these models to impute predictions on unlabeled data while applying a debiasing correction. This approach achieves the desired error probability while being more powerful than classical inference that only uses labeled data.

## Method Summary
Cross-prediction partitions labeled data into K folds, trains a machine learning model on all but one fold, and uses these models to predict labels for unlabeled data. The method then applies a debiasing correction using the labeled data to ensure valid statistical inference. The estimator solves a modified M-estimation problem where the loss function is debiased by a term that estimates the prediction bias. Asymptotic normality of the estimator allows construction of confidence intervals with the desired coverage.

## Key Results
- Cross-prediction achieves valid inference with target coverage while leveraging unlabeled data
- The method provides tighter confidence intervals than classical inference using labeled data only
- Cross-prediction yields more stable inferences with lower variability than prediction-powered inference and classical approaches
- In deforestation analysis, cross-prediction produced tighter confidence intervals than gold-standard measurements alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-prediction achieves valid inference by debiasing predicted labels using labeled data.
- **Mechanism**: The estimator adds a correction term that estimates the bias between predicted and true labels on the labeled data.
- **Core assumption**: The labeled data provides an accurate estimate of the prediction bias.
- **Evidence anchors**: Abstract states debiasing remedies prediction inaccuracies; section explains the correction term estimates prediction bias.
- **Break condition**: If labeled dataset is too small or unrepresentative, debiasing term becomes inaccurate.

### Mechanism 2
- **Claim**: Cross-prediction improves statistical power by leveraging unlabeled data through machine learning predictions.
- **Mechanism**: Using predictions on large unlabeled dataset decreases estimator variance by averaging over many predictions.
- **Core assumption**: ML models trained on labeled data generalize well to unlabeled data.
- **Evidence anchors**: Abstract mentions more powerful inferences; section discusses variance reduction when models are accurate.
- **Break condition**: Poor model training or feature distribution differences prevent variance reduction.

### Mechanism 3
- **Claim**: Cross-prediction provides more stable inference by averaging multiple model fits.
- **Mechanism**: Training K different models and averaging their predictions reduces variability from model selection or training randomness.
- **Core assumption**: Variability between different model fits is smaller than variability of a single model fit.
- **Evidence anchors**: Abstract notes lower variability in confidence intervals; section mentions efficient data leverage.
- **Break condition**: Too few folds or highly unstable models prevent variability reduction.

## Foundational Learning

- **Concept**: Cross-validation and cross-fitting
  - **Why needed here**: Cross-prediction relies on partitioning labeled data into folds for unbiased debiasing.
  - **Quick check question**: Why do we need to leave out each fold when training the model f(j)?

- **Concept**: M-estimation and loss minimization
  - **Why needed here**: The estimand is defined as minimizer of expected loss, foundation for cross-prediction estimator.
  - **Quick check question**: How does cross-prediction estimator ensure unbiasedness of the loss function?

- **Concept**: Central limit theorem and asymptotic normality
  - **Why needed here**: Inference relies on asymptotic normality of cross-prediction estimator for confidence intervals.
  - **Quick check question**: What condition ensures predictions are "stable" enough for CLT to hold?

## Architecture Onboarding

- **Component map**: Labeled data -> K-fold model training -> Predictions on unlabeled data -> Debiasing correction -> Estimation -> Inference
- **Critical path**: Labeled data → K-fold model training → Predictions on unlabeled data → Debiasing correction → Estimation → Inference
- **Design tradeoffs**: Number of folds K (more folds reduce bias but increase computation); Model complexity (complex models may overfit); Labeled data size n (small n leads to high variance in debiasing term)
- **Failure signatures**: Poor coverage (debiasing term inaccurate or stability assumption fails); High variance (models poorly trained or unlabeled data different); Unstable intervals (too few folds or unstable models)
- **First 3 experiments**: 1) Synthetic data with known θ∗: Test coverage and interval width as n and N vary; 2) Mean estimation with R²=0 vs R²=1: Compare power of cross-prediction vs classical; 3) Galaxy image classification: Validate stability and coverage on high-dimensional features

## Open Questions the Paper Calls Out
The paper identifies several open questions including: how stability assumption holds for specific models like deep neural networks or random forests; how cross-prediction compares to other semi-supervised inference methods; how choice of number of folds K affects bias-variance tradeoff; and whether cross-prediction can be extended to handle complex semi-supervised scenarios with non-i.i.d. unlabeled data or multiple data sources.

## Limitations
- Debiasing mechanism relies heavily on labeled data being representative of unlabeled data distribution
- Paper assumes asymptotic normality without providing finite-sample guarantees
- Performance degrades significantly when labeled and unlabeled data distributions differ

## Confidence
- **High confidence**: Variance reduction through leveraging large unlabeled datasets is well-established theoretically
- **Medium confidence**: Debiasing approach works as claimed but depends on quality and representativeness of labeled data
- **Medium confidence**: Stability improvements through cross-fitting are theoretically sound but vary across ML models and datasets

## Next Checks
1. **Coverage validation**: Test confidence interval coverage across different ratios of N/n (e.g., N/n = 10, 100, 1000) to verify asymptotic normality holds in finite samples
2. **Distribution shift sensitivity**: Systematically evaluate how performance degrades when labeled and unlabeled data come from slightly different distributions
3. **Fold number optimization**: Experiment with different numbers of folds K to determine optimal tradeoff between bias reduction and computational cost for different problem scales