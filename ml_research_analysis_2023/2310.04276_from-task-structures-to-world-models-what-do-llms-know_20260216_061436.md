---
ver: rpa2
title: 'From task structures to world models: What do LLMs know?'
arxiv_id: '2310.04276'
source_url: https://arxiv.org/abs/2310.04276
tags:
- knowledge
- page
- arxiv
- instrumental
- worldlyknowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the nature of knowledge in large language models
  (LLMs) like GPT-4, proposing that these models possess "instrumental knowledge"
  - defined by their ability to perform a wide range of tasks. The authors examine
  how this instrumental knowledge relates to ordinary, "worldly" knowledge exhibited
  by humans, focusing on the extent to which instrumental knowledge incorporates structured
  world models from cognitive science.
---

# From task structures to world models: What do LLMs know?

## Quick Facts
- arXiv ID: 2310.04276
- Source URL: https://arxiv.org/abs/2310.04276
- Authors: 
- Reference count: 0
- Key outcome: This paper explores how LLMs possess "instrumental knowledge" defined by task performance, examining its relationship to worldly knowledge and the extent to which it incorporates structured world models from cognitive science.

## Executive Summary
This paper investigates the nature of knowledge in large language models (LLMs) by proposing that they possess "instrumental knowledge" - defined by their ability to perform a wide range of tasks. The authors examine how this instrumental knowledge relates to ordinary, "worldly" knowledge exhibited by humans, focusing on the extent to which instrumental knowledge incorporates structured world models from cognitive science. They suggest that LLMs could recover varying degrees of worldly knowledge through compression, but this recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands. The paper discusses ways LLMs might incorporate world models, drawing on recent studies that have shown LLMs can recover aspects of domain-specific world models, such as those underlying the game of Othello and color perception.

## Method Summary
The paper employs theoretical analysis and literature review to explore how LLMs acquire instrumental knowledge through next-token prediction on internet-scale natural language data. The authors examine the mechanism by which compression during training might recover underlying world models, and discuss how resource-rational tradeoffs between world models and task demands could govern this recovery. They reference existing studies on LLMs' ability to recover domain-specific world models in settings like the game of Othello and color perception tasks.

## Key Results
- LLMs acquire instrumental knowledge by inferring task structure from natural language context during next-token prediction
- Compression during pre-training can potentially recover underlying world models from text data
- The recovery of worldly knowledge in LLMs is governed by an implicit, resource-rational tradeoff between world models and task demands

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs acquire instrumental knowledge by inferring task structure from natural language context during next-token prediction.
- Mechanism: During pre-training, LLMs compress vast amounts of text data, learning to identify task-related patterns (e.g., summarization cues like "TL;DR" or translation signals across languages). This compression enables the model to infer task structure and condition predictions accordingly.
- Core assumption: Natural language text contains sufficient task-related patterns that can be inferred without explicit labeling.
- Evidence anchors:
  - [abstract] "Internet-scalenatural languagedatacanbeseenasalargedataset of amultitudeof tasks, posedinvaryingwaysandforms"
  - [section] "For instance, theabbreviation“TL;DR” or aparagraphthat startswiththephrase“Insummary, …” might signal asummarization-liketask"
  - [corpus] Weak evidence: corpus titles focus on LLM knowledge boundaries but don't directly support task structure inference mechanism
- Break condition: If natural language lacks consistent task signals, or if compression fails to preserve task structure, the mechanism breaks.

### Mechanism 2
- Claim: Compression during next-token prediction can recover underlying world models from text data.
- Mechanism: The compression objective forces the model to identify low-dimensional state spaces that factorize relevant dimensions of variation and their dependencies. These compressed representations can correspond to causal abstractions of real-world processes (world models).
- Core assumption: The text data contains traces of underlying world models that can be recovered through compression.
- Evidence anchors:
  - [abstract] "next wordpredictioninLLMsreflectscompressionof thevastamountsof text datacrawledontheInternet"
  - [section] "Alower-dimensional state-spacefactorizingtherelevant dimensionsof variationof agivendomainandthedependenceof thesedimensionsoneachother cansimultaneouslyenablecompressionandprediction"
  - [corpus] No direct corpus evidence for world model recovery through compression
- Break condition: If the compression objective prioritizes linguistic patterns over world structure, or if world models are too complex to compress effectively.

### Mechanism 3
- Claim: Instrumental knowledge in LLMs is governed by an implicit resource-rational tradeoff between world models and task demands.
- Mechanism: LLMs implicitly allocate computational resources between maintaining detailed world models and performing task-specific predictions. For tasks requiring only coarse-grained understanding, detailed world models are unnecessary and costly.
- Core assumption: LLMs operate under computational constraints that necessitate tradeoffs between world model fidelity and task performance.
- Evidence anchors:
  - [abstract] "suchrecoverywill begovernedbyanimplicit, resource-rational tradeoff betweenworldmodelsandtaskdemands"
  - [section] "Thus, wesuggest thedegreetowhichworldlyknowledgeispart of instrumental knowledgeinLLMsmaybedeterminedinaresource-rational way"
  - [corpus] No corpus evidence specifically addressing resource-rational tradeoffs in LLMs
- Break condition: If LLMs have sufficient computational resources to maintain detailed world models without performance degradation, or if task demands consistently require detailed world models.

## Foundational Learning

- Concept: World models as causal abstractions
  - Why needed here: The paper's core argument depends on understanding how LLMs might recover structured representations of real-world entities and processes
  - Quick check question: Can you explain how a world model of object motion differs from a statistical pattern of object mentions in text?

- Concept: Compression and prediction duality
  - Why needed here: The mechanism for world model recovery depends on understanding how compression enables prediction
  - Quick check question: Why does finding a lower-dimensional representation that enables both compression and prediction suggest it might capture real-world structure?

- Concept: Resource-rational analysis
  - Why needed here: The paper's explanation for why LLMs might not fully recover world models depends on understanding tradeoffs between computational costs and task demands
  - Quick check question: How does the idea that "good enough" solutions can be optimal under resource constraints apply to LLM knowledge acquisition?

## Architecture Onboarding

- Component map: Pre-training module -> Compression module -> Task inference module -> World model recovery module -> Resource management module

- Critical path: Pre-training → Task structure inference → Compression → Potential world model recovery → Instrumental knowledge

- Design tradeoffs:
  - Model size vs. world model detail: Larger models may recover more detailed world models but at higher computational cost
  - Training data diversity vs. task specificity: More diverse data may provide better world model traces but dilute task-specific signals
  - Compression level vs. world model fidelity: Higher compression may lose world model details but improve generalization

- Failure signatures:
  - If world models are not recovered: Models may perform well on linguistic tasks but fail on physical reasoning
  - If resource-rational tradeoffs are not effective: Models may be either too computationally expensive or too simplistic for task requirements
  - If task structure inference fails: Models may treat all inputs as generic text rather than recognizing task boundaries

- First 3 experiments:
  1. Train a small transformer on synthetic data with clear world model traces (like Othello board states) and test for world model recovery through probing
  2. Vary model size and training data diversity to measure the impact on world model recovery fidelity
  3. Design tasks with varying granularity requirements to test the resource-rational tradeoff hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can the instrumental knowledge of LLMs be formalized using task-conditioned world models, and how does this formalization relate to existing theories of amortized inference and resource-rational solutions in Bayesian inference?
- Basis in paper: [explicit] The paper discusses the need for future work to construct goal-conditioned world models and explore the relationship between instrumental knowledge and amortized inference or data-driven proposals in Bayesian inference.
- Why unresolved: The paper suggests this as a direction for future research but does not provide a concrete formalization or exploration of the relationship.
- What evidence would resolve it: Development of a formal framework for task-conditioned world models in LLMs and empirical studies demonstrating the relationship between instrumental knowledge and amortized inference.

### Open Question 2
- Question: What is the impact of fine-tuning on an LLM's instrumental knowledge and recovery of worldly knowledge, and how does this impact vary across different fine-tuning techniques?
- Basis in paper: [explicit] The paper mentions that some fine-tuned LLMs, including GPT-4, can generate compelling answers to prompts requiring worldly knowledge, but explaining this performance solely as a consequence of fine-tuning is difficult.
- Why unresolved: The paper acknowledges the impact of fine-tuning but does not provide a detailed analysis of how different fine-tuning techniques affect the recovery of worldly knowledge.
- What evidence would resolve it: Comparative studies of instrumental knowledge and worldly knowledge recovery in LLMs with different fine-tuning techniques and datasets.

### Open Question 3
- Question: How can structured world models be incorporated into AI systems like LLMs for safer and better-aligned deployment, and what are the potential risks and benefits of such incorporation?
- Basis in paper: [explicit] The paper discusses the potential of world models to enable safer, truthful, and better-aligned AI systems by providing interpretable mid- or high-level interfaces for control and intervention.
- Why unresolved: While the paper suggests the potential benefits, it does not provide a concrete methodology for incorporating world models into AI systems or discuss the potential risks and benefits in detail.
- What evidence would resolve it: Development and testing of AI systems that incorporate structured world models, along with risk-benefit analyses and empirical studies on the safety and alignment of these systems.

## Limitations

- The central claim about resource-rational tradeoffs between world models and task demands lacks empirical validation
- The mechanism by which compression during next-token prediction recovers world models remains speculative with no concrete demonstrations
- The paper lacks precise definition of what constitutes a recovered world model versus learned statistical patterns

## Confidence

**High confidence**: The claim that LLMs acquire instrumental knowledge through task structure inference from natural language context is well-supported by existing literature and aligns with established understanding of how LLMs process text.

**Medium confidence**: The assertion that compression can recover underlying world models from text data has theoretical plausibility but lacks direct empirical evidence in the paper. The connection between compression and world model recovery remains a hypothesis rather than a demonstrated finding.

**Low confidence**: The resource-rational tradeoff hypothesis is the most speculative claim, with no experimental validation provided. While the concept is theoretically sound, its application to LLMs requires empirical demonstration.

## Next Checks

1. **World model recovery experiment**: Design a controlled experiment using synthetic data with known world model structure (e.g., simple physics simulations or game states) to test whether LLMs can recover these structures through compression, using interpretability techniques to probe for recovered representations.

2. **Resource-rational tradeoff measurement**: Create a series of tasks with varying requirements for world knowledge versus pure pattern matching, then measure how LLMs allocate computational resources across these tasks to test whether they exhibit resource-rational behavior.

3. **World model vs. pattern distinction test**: Develop benchmark tasks that can distinguish between genuine world model understanding and sophisticated pattern matching, such as counterfactual reasoning tasks where surface patterns remain constant but underlying world states change.