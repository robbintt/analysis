---
ver: rpa2
title: JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using
  Instruction-tuning
arxiv_id: '2310.10083'
source_url: https://arxiv.org/abs/2310.10083
tags:
- medical
- llms
- japanese
- domain
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the effectiveness of LoRA-based instruction-tuning
  for adapting large language models (LLMs) to the medical domain, specifically for
  Japanese medical question-answering tasks. The research evaluates multiple models,
  including Japanese-centric OpenCALM-7B and MedCALM, and English-centric Llama2-70B-chat-hf,
  using three metrics: accuracy, exact match, and Gestalt score.'
---

# JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning

## Quick Facts
- arXiv ID: 2310.10083
- Source URL: https://arxiv.org/abs/2310.10083
- Authors: 
- Reference count: 20
- Japanese medical domain adaptation using LoRA-based instruction-tuning on LLMs

## Executive Summary
This study investigates the effectiveness of LoRA-based instruction-tuning for adapting large language models to the Japanese medical domain. The research evaluates multiple models including OpenCALM-7B, MedCALM, and Llama2-70B-chat-hf using three metrics: accuracy, exact match, and Gestalt score. Results indicate that LoRA-based instruction-tuning can partially incorporate domain-specific knowledge, with larger models showing more pronounced effects. The study finds that a single epoch of instruction-tuning is more effective than multiple epochs, while additional pretraining does not significantly improve performance.

## Method Summary
The study employs LoRA for OpenCALM-7B and QLoRA for Llama2-70B-chat-hf, using instruction-tuning on a dataset of 77,422 examples generated by ChatGPT. Models are evaluated on Japanese medical question-answering tasks using the IgakuQA and JJSIMQA datasets. Three metrics are used: accuracy, exact match, and Gestalt score. The research compares performance across different fine-tuning epochs (1, 3, and 10) and examines both zero-shot and one-shot capabilities before and after instruction-tuning.

## Key Results
- LoRA-based instruction-tuning partially incorporates domain-specific knowledge into LLMs
- Single epoch of instruction-tuning (1k steps) improves performance more than multiple epochs
- Larger English-centric models like Llama2-70B demonstrate superior performance compared to smaller Japanese-centric models
- Additional pretraining on OpenCALM-7B does not contribute to performance improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA-based instruction-tuning partially incorporates domain-specific knowledge into LLMs
- Mechanism: Low-Rank Adaptation modifies weight matrices with small trainable matrices while freezing original weights, enabling efficient fine-tuning on domain-specific data without full model retraining. The instruction-tuning format provides structured prompts that guide the model to apply medical knowledge to question-answering tasks.
- Core assumption: Medical knowledge can be partially acquired through fine-tuning on structured instruction data without requiring full pretraining on medical corpora
- Evidence anchors:
  - [abstract]: "Our findings suggest that LoRA-based instruction-tuning can partially incorporate domain-specific knowledge into LLMs"
  - [section 4.1]: "We have observed notable score improvements with LoRA after an appropriate number of steps"
  - [corpus]: Weak evidence - corpus contains studies on LoRA but none specifically confirm partial knowledge incorporation mechanism
- Break condition: If instruction data quality degrades or LoRA rank is set too low to capture domain patterns

### Mechanism 2
- Claim: Larger English-centric models outperform smaller Japanese-centric models in domain adaptation
- Mechanism: Scale provides better generalization and richer pre-existing knowledge representations. English-centric models benefit from massive pretraining corpora that capture more diverse linguistic patterns, enabling better transfer to specialized domains when fine-tuned.
- Core assumption: Model capacity and pretraining scale are more important than language alignment for domain adaptation tasks
- Evidence anchors:
  - [abstract]: "with larger models demonstrating more pronounced effects" and "our results underscore the potential of adapting English-centric models for Japanese applications"
  - [section 4.1]: "particularly with Llama2-70B showing the most significant enhancement"
  - [corpus]: Moderate evidence - several papers compare model scales but limited Japanese-specific validation
- Break condition: If task requires deep Japanese cultural/linguistic nuances that English-centric models cannot capture

### Mechanism 3
- Claim: Single epoch of instruction-tuning is more effective than multiple epochs
- Mechanism: Overfitting occurs when fine-tuning for too many epochs on limited domain data, causing the model to memorize training patterns rather than generalize. Single epoch provides optimal balance between learning domain patterns and maintaining generalization ability.
- Core assumption: Limited domain instruction data is insufficient for multiple epochs without causing overfitting
- Evidence anchors:
  - [section 4.1]: "our results show that a single epoch (1k steps) of instruction-tuning improves the performance but increasing the number of epochs exacerbates the model"
  - [abstract]: "a single epoch of instruction-tuning was found to be more effective than multiple epochs"
  - [corpus]: Weak evidence - corpus lacks specific studies on epoch optimization for instruction-tuning
- Break condition: If training dataset size increases substantially or if catastrophic forgetting becomes problematic

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) mechanics
  - Why needed here: Understanding how LoRA modifies weight matrices is crucial for debugging training failures and optimizing rank parameters
  - Quick check question: What is the mathematical relationship between original weight matrix W and LoRA adaptation matrices A and B?

- Concept: Instruction-tuning format and prompt engineering
  - Why needed here: The instruction format directly impacts model's ability to learn domain-specific reasoning patterns
  - Quick check question: How does the instruction format differ from standard fine-tuning, and why is this important for domain adaptation?

- Concept: Evaluation metrics for domain-specific LLMs
  - Why needed here: Understanding the limitations of accuracy, exact match, and Gestalt score is essential for proper model assessment
  - Quick check question: Why might exact match be an insufficient metric for medical question-answering tasks?

## Architecture Onboarding

- Component map: Base model → LoRA adapter → Instruction dataset → Evaluation pipeline → Metrics calculation
- Critical path: Dataset preparation → Model loading → LoRA configuration → Training loop → Evaluation → Analysis
- Design tradeoffs: LoRA rank vs. parameter efficiency, epoch count vs. overfitting risk, dataset size vs. instruction quality
- Failure signatures: Performance degradation after multiple epochs, loss of 1-shot capabilities, metric inconsistencies across evaluation methods
- First 3 experiments:
  1. Single epoch LoRA training on base OpenCALM-7B with default parameters
  2. QLoRA training on Llama2-70B with varying rank values (8, 64, 128)
  3. Comparison of 0-shot vs. 1-shot performance before and after instruction-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Japanese medical LLMs compare to their English counterparts when fine-tuned with similar amounts of domain-specific data?
- Basis in paper: [inferred]
- Why unresolved: The paper suggests that larger English-centric models like Llama2-70B perform better than smaller Japanese-centric models like OpenCALM-7B, but it does not provide a direct comparison with similar amounts of domain-specific data.
- What evidence would resolve it: Conducting experiments with equivalent amounts of domain-specific data for both Japanese and English models would provide a clearer comparison.

### Open Question 2
- Question: What is the optimal number of fine-tuning steps for LoRA-based instruction-tuning in medical domain adaptation, and how does it vary with model size and dataset size?
- Basis in paper: [explicit]
- Why unresolved: The paper finds that a single epoch (1k steps) of instruction-tuning is effective, but it does not explore the optimal number of steps in detail or how it varies with different factors.
- What evidence would resolve it: Conducting a comprehensive study varying the number of fine-tuning steps, model sizes, and dataset sizes would help determine the optimal configuration.

### Open Question 3
- Question: How does additional pretraining with a broader range of medical domain documents affect the performance of LLMs in domain-specific tasks?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that additional pretraining did not contribute to performance improvement with scarce training data, but it does not explore the impact of using a broader range of medical documents.
- What evidence would resolve it: Experimenting with additional pretraining using diverse medical documents and evaluating the performance in downstream tasks would provide insights into its effectiveness.

## Limitations

- Limited evaluation dataset size (50 examples for IgakuQA and JJSIMQA) raises concerns about statistical significance
- Potential cultural and linguistic blind spots due to limited Japanese medical domain expertise in evaluation
- Comparison between Japanese-centric and English-centric models may be affected by inherent differences in pretraining corpora size and quality
- Focus on LoRA/QLoRA approaches means other efficient fine-tuning methods remain unexplored

## Confidence

**High Confidence:**
- LoRA-based instruction-tuning can partially incorporate domain-specific knowledge into LLMs
- Single epoch of instruction-tuning outperforms multiple epochs for this task
- Llama2-70B demonstrates superior performance compared to smaller Japanese models

**Medium Confidence:**
- English-centric models can be effectively adapted for Japanese medical applications
- Additional pretraining on OpenCALM-7B does not improve performance

**Low Confidence:**
- The 1-shot performance deterioration in OpenCALM-based models is solely attributable to instruction-tuning
- Gestalt score provides meaningful additional insight beyond accuracy and exact match metrics

## Next Checks

1. **Dataset Quality and Size Validation**: Replicate the study using a larger, independently curated Japanese medical instruction-tuning dataset (minimum 200k examples) and evaluate on expanded test sets (minimum 200 examples) to verify if the single-epoch advantage and performance patterns hold with more robust data.

2. **Cross-Validation of English-Centric Adaptation**: Test the adaptation approach on additional English-centric models (e.g., GPT-4, Claude) using the same Japanese medical instruction-tuning methodology to determine if Llama2-70B's performance is representative or model-specific.

3. **1-Shot Performance Investigation**: Conduct ablation studies comparing pre-instruction-tuning and post-instruction-tuning 1-shot capabilities across different LoRA rank values and fine-tuning durations to isolate the mechanisms causing performance degradation and identify potential mitigation strategies.