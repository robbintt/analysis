---
ver: rpa2
title: 'Expository Text Generation: Imitate, Retrieve, Paraphrase'
arxiv_id: '2305.03276'
source_url: https://arxiv.org/abs/2305.03276
tags:
- expository
- text
- university
- factual
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces expository text generation, a new task for
  automatically creating informative, structured documents from a topic and factual
  corpus. The authors propose IRP, an iterative framework that explicitly handles
  content planning, fact retrieval, and faithful rephrasing by using an Imitator,
  Retriever, and Paraphraser.
---

# Expository Text Generation: Imitate, Retrieve, Paraphrase

## Quick Facts
- arXiv ID: 2305.03276
- Source URL: https://arxiv.org/abs/2305.03276
- Reference count: 40
- Key outcome: Introduces IRP, an iterative framework that outperforms state-of-the-art models in expository text generation through explicit content planning, fact retrieval, and faithful rephrasing.

## Executive Summary
This paper introduces expository text generation, a new task for automatically creating informative, structured documents from a topic and factual corpus. The authors propose IRP, an iterative framework that explicitly handles content planning, fact retrieval, and faithful rephrasing by using an Imitator, Retriever, and Paraphraser. Experiments on three diverse datasets show IRP outperforms state-of-the-art models in both traditional and factuality metrics, producing more accurate and well-structured outputs. Human evaluation confirms IRP's strong balance of factual accuracy and style adherence. Ablation studies highlight the importance of fine-grained queries and iterative generation.

## Method Summary
The IRP framework addresses expository text generation through three specialized modules: an Imitator (GPT-2) generates stylistic content plans, a Retriever (DistilBERT) finds relevant facts from the corpus using these plans, and a Paraphraser (BART) rewrites the facts in the document's style. The system operates iteratively, generating one sentence at a time while maintaining factual consistency. The framework was trained and evaluated on three newly-collected datasets: university descriptions, medical drug information, and computer science history sections.

## Key Results
- IRP achieves state-of-the-art performance across three diverse datasets, outperforming models like LLaMa, LED, and RAG
- The framework shows superior balance of factual accuracy and style adherence in human evaluation
- Ablation studies demonstrate that fine-grained queries and iterative generation are essential components

## Why This Works (Mechanism)

### Mechanism 1
Separating content planning, fact retrieval, and rephrasing into distinct modules reduces hallucination compared to end-to-end generation. IRP's modular design allows each component to specialize—Imitator plans content without retrieval pressure, Retriever focuses solely on finding relevant facts, and Paraphraser only needs to rephrase retrieved information. Core assumption: Each module can be trained independently without degradation in overall quality. Evidence: Analysis of factual errors produced by IRP (§5.4) - weak evidence; needs deeper error analysis.

### Mechanism 2
Using fine-grained queries (stylistic content plans) instead of document titles improves fact retrieval accuracy. Stylistic content plans provide more specific search terms than generic document titles, allowing the Retriever to locate relevant facts more precisely within the corpus. Core assumption: The Retriever can effectively embed and match content plans to factual sentences even when plans contain hallucinated entities. Evidence: Weak; need ablation showing Topic Query performs worse than Stylistic Content Plans.

### Mechanism 3
Iterative sentence-by-sentence generation preserves factual consistency better than generating entire documents at once. By focusing on one sentence at a time, IRP can maintain tighter control over factual accuracy for each output segment. Core assumption: Shorter generation contexts reduce the likelihood of introducing inconsistencies or hallucinations. Evidence: Ablation study (Table 3) shows Gen All produces more hallucinations than IRP Full.

## Foundational Learning

- **Concept**: Causal language modeling for content planning
  - **Why needed here**: The Imitator needs to generate plausible next sentences in the expository style without access to ground truth
  - **Quick check question**: How does GPT-2's causal mask prevent the Imitator from "cheating" by looking at future tokens?

- **Concept**: Maximum Inner-Product Search for efficient retrieval
  - **Why needed here**: The Retriever must quickly find the top-k most relevant facts from potentially thousands of sentences
  - **Quick check question**: Why is cosine similarity between DistilBERT embeddings a good proxy for semantic relevance?

- **Concept**: Syntactic exemplar-based paraphrasing
  - **Why needed here**: The Paraphraser needs to maintain the expository style while accurately representing retrieved facts
  - **Quick check question**: How does BART's seq2seq architecture enable it to combine style from content plans with facts from multiple sentences?

## Architecture Onboarding

- **Component map**: Imitator (GPT-2) → Retriever (DistilBERT) → Paraphraser (BART) → Output, with iterative loop
- **Critical path**: Content plan generation → Fact retrieval → Fact paraphrasing → Append to output → Repeat
- **Design tradeoffs**: Modular design vs. end-to-end training efficiency; Fine-grained queries vs. retrieval speed; Sentence-level generation vs. document coherence
- **Failure signatures**: Repetitive sentences → Retriever embedding threshold too low; Factual errors → Paraphraser not properly combining facts; Style drift → Imitator generating off-topic content plans
- **First 3 experiments**:
  1. Verify Imitator generates coherent content plans matching training data style
  2. Test Retriever's ability to find relevant facts using content plans vs. document titles
  3. Validate Paraphraser maintains factual accuracy while adopting content plan style

## Open Questions the Paper Calls Out

### Open Question 1
How can we leverage fact verification to accurately select information when multiple options exist? The paper's investigation of factual errors produced by IRP found that the majority of errors were due to inconsistencies in the retrieved facts rather than weaknesses of the Retriever, Paraphraser, or data collection. This suggests a need for methods to handle inconsistencies in retrieved facts and select the most accurate information.

### Open Question 2
How can we develop models that can retrieve information from the web in real time during inference? The paper notes a large performance gap between the "with doc" and "without doc" datasets, suggesting that future models should prioritize the knowledge acquisition step. The authors propose studying models that search the web during inference as a promising next step.

### Open Question 3
How can we develop safeguards to detect and combat potential risks of seq2seq language models being used to produce misinformation or deceptive claims? The paper's discussion of ethical considerations mentions the possibility of someone creating their own Paraphraser aligned with the goal of producing misinformation or deceptive claims from true information and plugging this malicious component into IRP.

## Limitations

- The evaluation framework relies heavily on automated factuality metrics which may not fully capture nuanced factual errors or stylistic fidelity
- The human evaluation is limited to only 50 samples per condition, which may not be statistically robust
- The modular design introduces potential error propagation between components that isn't fully characterized

## Confidence

**High confidence**: The modular architecture design and its separation of content planning, retrieval, and paraphrasing tasks is well-supported by experimental results.

**Medium confidence**: The claim that fine-grained stylistic content plans improve retrieval accuracy over document titles is supported by results but lacks direct ablation evidence.

**Low confidence**: The assertion that iterative sentence-by-sentence generation is essential for preserving factual consistency, while supported by Table 3, needs more detailed error analysis.

## Next Checks

1. Conduct a controlled ablation study directly comparing Stylistic Content Plans against Topic Queries as retrieval inputs, measuring both retrieval accuracy and downstream generation quality.

2. Perform detailed error analysis on the specific types of factual errors produced by IRP, categorizing them by module (Imitator hallucination, Retriever mismatch, Paraphraser error) to understand error propagation patterns.

3. Scale up human evaluation to 200+ samples per condition with expert annotators trained in expository writing to better validate automated factuality metrics and capture stylistic fidelity dimensions.