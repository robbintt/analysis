---
ver: rpa2
title: Weight Averaging Improves Knowledge Distillation under Domain Shift
arxiv_id: '2309.11446'
source_url: https://arxiv.org/abs/2309.11446
tags:
- domain
- distillation
- training
- knowledge
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation under domain shift,
  where a small student model learns from a large teacher model trained on source
  domains but is evaluated on unseen target domains. The authors apply domain generalization
  techniques based on weight averaging, such as SWAD and SMA, to improve the performance
  of knowledge distillation under distribution shift.
---

# Weight Averaging Improves Knowledge Distillation under Domain Shift
## Quick Facts
- arXiv ID: 2309.11446
- Source URL: https://arxiv.org/abs/2309.11446
- Authors: 
- Reference count: 40
- Primary result: Weight averaging techniques improve knowledge distillation performance under domain shift by up to 1.6 percentage points

## Executive Summary
This paper investigates knowledge distillation under domain shift, where a small student model learns from a large teacher model trained on source domains but is evaluated on unseen target domains. The authors apply domain generalization techniques based on weight averaging, such as SWAD and SMA, to improve the performance of knowledge distillation under distribution shift. They also propose a simple weight averaging strategy that averages all model weights from a fixed iteration until the end of training, without requiring validation evaluation during training. Experiments on PACS and OfficeHome datasets with ResNet and DeiT architectures show that weight averaging improves target domain accuracy by up to 1.6 percentage points compared to standard knowledge distillation, with the proposed approach performing on par with SWAD and SMA.

## Method Summary
The paper applies domain generalization techniques (SWAD, SMA, and a proposed simple weight averaging strategy) to knowledge distillation under domain shift. The simple strategy averages all model weights from a fixed iteration (10% of training) until the end without requiring validation evaluation. Experiments use ResNet-50/18 and DeiT-Small/Tiny architectures on PACS and OfficeHome datasets, with 5,000 iterations for teacher training and 50,000 iterations for distillation using KL-divergence objective with temperature τ=5.

## Key Results
- Weight averaging improves target domain accuracy by up to 1.6 percentage points compared to standard knowledge distillation
- The proposed simple weight averaging strategy performs on par with more complex methods (SWAD and SMA)
- Weight averaging consistently improves performance across different architecture pairs and datasets
- There remains a significant performance gap between teacher and student on target domains

## Why This Works (Mechanism)
### Mechanism 1
Weight averaging improves knowledge distillation under domain shift by encouraging convergence to flatter minima, which correlates with better out-of-distribution generalization. By averaging weights from multiple checkpoints along the training trajectory, the resulting model represents an ensemble of models that have navigated different parts of the loss landscape, effectively smoothing the optimization surface and reducing sensitivity to domain shift.

### Mechanism 2
The proposed simple weight averaging strategy (averaging all weights from a fixed iteration to the end of training) performs on par with more complex methods like SWAD and SMA while being computationally cheaper. This strategy captures the benefits of weight averaging without requiring validation-based model selection, reducing computational overhead while maintaining performance gains.

### Mechanism 3
Knowledge distillation under domain shift transfers the teacher's ability to generalize to unseen domains to the student model. The student learns not just to match the teacher's predictions on source domains but also implicitly learns the teacher's learned invariances and feature representations that enable domain generalization.

## Foundational Learning
- **Domain Generalization (DG)**: Understanding DG is crucial because the paper's setting involves training on source domains and evaluating on unseen target domains, which is the core challenge of DG. *Quick check*: What is the main difference between domain generalization and domain adaptation?
- **Knowledge Distillation (KD)**: KD is the primary technique being studied, where a student model learns from a teacher model, and understanding its mechanisms is essential for grasping how weight averaging improves its performance under domain shift. *Quick check*: How does the temperature parameter τ in KD affect the student's learning process?
- **Weight Averaging Techniques**: The paper applies weight averaging techniques like SWAD and SMA to KD, so understanding how these techniques work and their benefits is fundamental to the research. *Quick check*: What is the key difference between SWAD and the proposed simple weight averaging strategy?

## Architecture Onboarding
- **Component map**: Teacher model (ResNet-50 or DeiT-Small) → Distillation process → Student model (ResNet-18 or DeiT-Tiny) → Weight averaging → Evaluation on target domain
- **Critical path**: Teacher training (using SWAD) → KD training (50,000 iterations with temperature τ=5) → Weight averaging (either SWAD, SMA, or proposed simple strategy) → Target domain accuracy evaluation
- **Design tradeoffs**: Using weight averaging improves target domain accuracy but increases computational cost during training; the proposed simple strategy reduces this cost but may miss optimal averaging segments
- **Failure signatures**: No improvement in target domain accuracy with weight averaging, significant performance gap between teacher and student on target domains, instability in weight averaging convergence
- **First 3 experiments**:
  1. Implement standard KD (ERM baseline) with ResNet-50 teacher and ResNet-18 student on PACS dataset, measuring target domain accuracy
  2. Apply SWAD to the KD process and compare target domain accuracy with the baseline
  3. Implement the proposed simple weight averaging strategy (averaging from 10% to 100% of training) and compare its performance with SWAD and SMA

## Open Questions the Paper Calls Out
### Open Question 1
How does the performance gap between teachers and students under domain shift vary with different teacher-student architecture pairs beyond ResNet and DeiT? The authors note that "in all cases there is still a significant gap between the target domain performance of the teacher and the student" and suggest room for improvement, but only test ResNet-50 to ResNet-18 and DeiT-Small to DeiT-Tiny pairs.

### Open Question 2
What is the theoretical relationship between flat minima and improved domain generalization in knowledge distillation? The authors mention that "better understanding the reasons behind the said improvements and their connection to flat minima" is a future research direction, and reference SWAD's argument that "finding flat minima leads to better performance under domain shift."

### Open Question 3
How do different knowledge distillation objectives compare in terms of domain generalization performance when combined with weight averaging techniques? The authors use only the standard KL-divergence objective and note that "exploring other distillation objectives" is a future direction.

## Limitations
- Weight averaging improvements are modest (maximum 1.6 percentage points), suggesting limited practical impact in many scenarios
- The paper lacks ablation studies examining different starting points for the simple averaging strategy or varying the length of averaging segments
- The mechanism by which weight averaging improves domain generalization is not fully elucidated, particularly regarding how teacher model generalization transfers to students

## Confidence
- **High confidence**: Weight averaging techniques (SWAD and SMA) improve knowledge distillation under domain shift, as demonstrated across multiple datasets and architectures
- **Medium confidence**: The proposed simple weight averaging strategy performs comparably to more complex methods while reducing computational cost, though specific hyperparameter choices remain underexplored
- **Low confidence**: The mechanism by which weight averaging improves domain generalization is not fully elucidated, particularly regarding how teacher model generalization transfers to students

## Next Checks
1. Conduct an ablation study varying the starting iteration for the simple averaging strategy (currently fixed at 10% of training) to determine optimal performance boundaries
2. Test the weight averaging approach on additional domain shift scenarios, such as cross-dataset adaptation between non-overlapping datasets, to assess generalizability beyond PACS and OfficeHome
3. Compare weight averaging with alternative domain generalization techniques for KD, such as meta-learning or adversarial feature alignment, to establish relative effectiveness