---
ver: rpa2
title: 'Human Preference Score: Better Aligning Text-to-Image Models with Human Preference'
arxiv_id: '2303.14420'
source_url: https://arxiv.org/abs/2303.14420
tags:
- images
- human
- dataset
- generated
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aligning text-to-image generation
  models with human aesthetic preferences. The authors collect a large-scale dataset
  of human choices on images generated by Stable Diffusion from the Stable Foundation
  Discord channel.
---

# Human Preference Score: Better Aligning Text-to-Image Models with Human Preference

## Quick Facts
- arXiv ID: 2303.14420
- Source URL: https://arxiv.org/abs/2303.14420
- Reference count: 40
- Key outcome: This paper addresses the problem of aligning text-to-image generation models with human aesthetic preferences.

## Executive Summary
This paper introduces a Human Preference Score (HPS) to better align text-to-image generation models with human aesthetic preferences. The authors collect a large-scale dataset of human choices on images generated by Stable Diffusion from the Stable Foundation Discord channel and find that existing evaluation metrics like Inception Score, FID, and CLIP score do not correlate well with human preferences. They train a human preference classifier by fine-tuning CLIP on their dataset and derive HPS. Using HPS, they adapt Stable Diffusion to generate images more aligned with human preferences, significantly outperforming the original model in human evaluations.

## Method Summary
The method involves collecting a dataset of human choices between generated images from Stable Diffusion, fine-tuning CLIP on this data to create a human preference classifier, and deriving the Human Preference Score (HPS) from the classifier. The HPS is then used to adapt Stable Diffusion via LoRA, where a special prefix is associated with non-preferred images and used as a negative prompt during inference. The adapted model is evaluated through user studies comparing it with the original model.

## Key Results
- HPS better aligns with human preferences than existing metrics like IS, FID, and CLIP score
- Adapted Stable Diffusion model significantly outperforms original in human evaluations
- HPS shows good generalization capability to other models like DALL-E
- Images generated by adapted model have fewer artifacts and better capture user intentions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Human Preference Score (HPS) better aligns with human aesthetic preferences than existing metrics.
- **Mechanism:** HPS is derived by fine-tuning CLIP on a dataset of human choices between generated images. This fine-tuning adapts the model to capture subtle aspects of human preference beyond exact prompt-image alignment.
- **Core assumption:** Human choices on generated images reflect aesthetic preferences that are not fully captured by existing metrics like IS, FID, or CLIP score.
- **Evidence anchors:**
  - [abstract] "Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices."
  - [section 2] "By evaluating on this dataset, we find that the Inception Score (IS), the FrÃ©chet Inception distance (FID) and the CLIP score do not fully match the human choice."
- **Break condition:** If human choices are inconsistent or do not reflect stable aesthetic preferences, HPS would fail to generalize.

### Mechanism 2
- **Claim:** Adapting Stable Diffusion with HPS-labeled data improves image quality according to human preferences.
- **Mechanism:** The model is trained to associate a special prefix with non-preferred images. During inference, this prefix is used as a negative prompt in classifier-free guidance to avoid generating undesirable artifacts.
- **Core assumption:** The concept of "non-preferred" can be learned by the model and generalized to new prompts during inference.
- **Evidence anchors:**
  - [section 6] "By adapting Stable Diffusion on this dataset via LoRA, we enhance the model's ability to learn the concept of non-preferred images, which can subsequently be avoided during inference."
  - [section 7.2] "The adapted model can better capture the user intention from the prompt, and generate more preferable images with less artifacts."
- **Break condition:** If the model overfits to specific artifacts in the training data, it may not generalize to new types of aesthetic issues.

### Mechanism 3
- **Claim:** HPS has good generalization capability to other generative models.
- **Mechanism:** The human preference classifier is trained on choices between Stable Diffusion images but can predict preferences for images from other models like DALL-E.
- **Core assumption:** Human aesthetic preferences are consistent across different generative models and can be captured by a single classifier.
- **Evidence anchors:**
  - [section 7.1] "We evaluate HPS' generalization capability towards other generative models by user studies... HPS is better aligned with human preference compared to CLIP score, and its agreement with human approaches the agreement between humans."
- **Break condition:** If different models have fundamentally different failure modes that humans judge differently, HPS would not generalize well.

## Foundational Learning

- **Concept:** Diffusion models and reverse diffusion process
  - **Why needed here:** The paper adapts Stable Diffusion, which is based on diffusion models. Understanding how diffusion models work is crucial for understanding the adaptation method.
  - **Quick check question:** How does the reverse diffusion process in Stable Diffusion generate images from noise?

- **Concept:** CLIP model and its use in text-to-image tasks
  - **Why needed here:** HPS is derived from fine-tuning CLIP. Understanding CLIP's architecture and how it's used for text-image alignment is essential.
  - **Quick check question:** How does CLIP compute the similarity between text prompts and images?

- **Concept:** Classifier-free guidance in diffusion models
  - **Why needed here:** The adaptation method uses classifier-free guidance with a negative prompt prefix. Understanding how this works is crucial for the implementation.
  - **Quick check question:** How does classifier-free guidance work in diffusion models, and how can it be used with negative prompts?

## Architecture Onboarding

- **Component map:** Human Choice Data -> CLIP Fine-tuning -> Human Preference Classifier -> HPS Derivation -> LoRA Adaptation -> Adapted Stable Diffusion
- **Critical path:**
  1. Collect human choice data from Discord
  2. Train human preference classifier on this data
  3. Derive HPS from the classifier
  4. Create training data with HPS labels for Stable Diffusion
  5. Adapt Stable Diffusion using LoRA with negative prompts
  6. Evaluate the adapted model through user studies

- **Design tradeoffs:**
  - Using CLIP as base vs. training from scratch: CLIP provides strong visual and text representations but may have biases from its original training
  - Fine-tuning last layers only vs. full model: Faster training but may miss important features
  - LoRA vs. full fine-tuning: More efficient but may limit adaptation capacity

- **Failure signatures:**
  - If HPS doesn't correlate with human choices in validation: The classifier may not have learned the right features
  - If adapted model produces worse images: The negative prompt approach may be too aggressive or not specific enough
  - If user studies show no improvement: The adaptation may not have addressed the right aspects of image quality

- **First 3 experiments:**
  1. Evaluate HPS correlation with human choices on a held-out validation set
  2. Test the adapted model on a small set of prompts and compare with original
  3. Conduct a user study with a small number of participants to validate improvements

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions.

## Limitations

- Dataset collection from Discord may introduce sampling bias toward certain user preferences
- Human preferences may not be stable across different cultural contexts or time periods
- LoRA adaptation may have limited capacity compared to full fine-tuning

## Confidence

- **High confidence:** The correlation analysis showing existing metrics poorly align with human choices
- **Medium confidence:** The effectiveness of HPS in capturing human preferences
- **Medium confidence:** The generalization capability of HPS to DALL-E

## Next Checks

1. Test HPS on a diverse set of prompts spanning different aesthetic styles and content domains to verify consistent alignment with human preferences across contexts
2. Conduct a larger-scale user study with demographic diversity to validate whether the adaptation improvements generalize beyond the initial test group
3. Evaluate the adapted model's performance on prompts with known failure modes (e.g., text rendering, anatomical correctness) to assess specific improvement areas