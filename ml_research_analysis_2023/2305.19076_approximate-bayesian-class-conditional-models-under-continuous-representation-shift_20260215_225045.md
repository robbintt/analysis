---
ver: rpa2
title: Approximate Bayesian Class-Conditional Models under Continuous Representation
  Shift
arxiv_id: '2305.19076'
source_url: https://arxiv.org/abs/2305.19076
tags:
- learning
- deepccg
- representation
- methods
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of representation shift in online
  continual learning, where updating the representation encoder can cause previously
  learned classifiers to become misaligned, leading to catastrophic forgetting. The
  proposed DeepCCG method uses a Bayesian class-conditional Gaussian classifier that
  can instantly adapt to representation shifts by recomputing its posterior in one
  step.
---

# Approximate Bayesian Class-Conditional Models under Continuous Representation Shift

## Quick Facts
- **arXiv ID**: 2305.19076
- **Source URL**: https://arxiv.org/abs/2305.19076
- **Reference count**: 40
- **Primary result**: DeepCCG achieves average accuracy improvements of 2.145% in task-incremental and 7.72% in class-incremental settings over state-of-the-art methods

## Executive Summary
This paper addresses representation shift in online continual learning, where updating the representation encoder causes previously learned classifiers to become misaligned, leading to catastrophic forgetting. The proposed DeepCCG method uses a Bayesian class-conditional Gaussian classifier that can instantly adapt to representation shifts by recomputing its posterior in one step. This is combined with a novel log conditional marginal likelihood loss to update the embedding function, and a new memory sample selection method based on minimizing KL divergence to maintain representative examples. Experiments on CIFAR-10, CIFAR-100, and MiniImageNet in both task-incremental and class-incremental settings show DeepCCG consistently outperforms state-of-the-art methods.

## Method Summary
DeepCCG is a continual learning method that uses a Bayesian class-conditional Gaussian classifier with a conjugate prior to maintain tractable posterior updates. The method consists of three key components: (1) an embedding function trained with a log conditional marginal likelihood loss that conditions on stored memory examples, (2) a posterior update mechanism that adapts the classifier in one step when representations shift, and (3) a memory sample selection method that minimizes KL divergence between true and induced posteriors to maintain representative examples. The approach is evaluated on CIFAR-10, CIFAR-100, and MiniImageNet with both task-incremental and class-incremental settings, using a ResNet18 backbone with reduced filters and Instance Normalization.

## Key Results
- DeepCCG achieves average accuracy improvements of 2.145% in task-incremental and 7.72% in class-incremental settings over state-of-the-art methods
- The method shows consistent improvement across all datasets (CIFAR-10, CIFAR-100, MiniImageNet) in both learning paradigms
- Ablation study confirms that both the conditional likelihood loss and memory selection mechanisms are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DeepCCG's class-conditional Gaussian classifier can instantly adapt to representation shift by recomputing its posterior in one step.
- **Mechanism**: When the embedding function updates and shifts representations, the posterior of the class-conditional Gaussian classifier is recalculated using stored memory examples. This recalculation uses the shifted representations to update the posterior over class means, allowing the classifier to realign with the new representation space without requiring iterative updates.
- **Core assumption**: The posterior distribution of class means remains tractable and can be updated analytically when representations shift.
- **Evidence anchors**:
  - [abstract]: "DeepCCG works by updating the posterior of a class conditional Gaussian classifier such that the classifier adapts in one step to representation shift."
  - [section 4]: "The posterior is tractable and easy to compute in one step... as we use a class-conditional Gaussian model and a conjugate prior for the per-class means."
- **Break condition**: If the representation shift becomes too large or non-i.i.d., the memory buffer may no longer contain representative examples, making the posterior approximation inaccurate.

### Mechanism 2
- **Claim**: DeepCCG uses a log conditional marginal likelihood loss to update the embedding function, which reduces unnecessary representation shift.
- **Mechanism**: The embedding function is trained using a loss that conditions the likelihood on stored memory examples. This stabilizes training by providing a better signal about where embedded data should be positioned, pulling examples toward their class means and away from others.
- **Core assumption**: Conditioning the likelihood on memory examples provides a more informative posterior than using the prior alone.
- **Evidence anchors**:
  - [abstract]: "The use of a class conditional Gaussian classifier also enables DeepCCG to use a log conditional marginal likelihood loss to update the representation."
  - [section 4]: "This can be seen as a new type of replay; replay methods have been shown to reduce representation shift, increasing performance."
- **Break condition**: If the memory buffer becomes unrepresentative of the true data distribution, the conditional likelihood may provide misleading gradients.

### Mechanism 3
- **Claim**: DeepCCG's sample selection mechanism maintains representative examples by minimizing KL divergence between true and induced posteriors.
- **Mechanism**: When selecting which examples to store in the fixed-size memory buffer, DeepCCG chooses samples that minimize the KL divergence between the posterior computed from all available data and the posterior computed from the memory subset. This ensures the memory buffer best represents the current data distribution.
- **Core assumption**: The KL divergence between posteriors can be approximated by the squared Euclidean distance between class mean embeddings.
- **Evidence anchors**:
  - [section 4]: "To perform sample selection we minimise the KL divergence between two posterior distributions: the posterior over parameters induced by the new memory being optimized, and the posterior induced by the current batch and the old memory."
  - [section 4]: "This sample selection mechanism is robust to certain kinds of representation shift."
- **Break condition**: If the representation shift is non-i.i.d. or too large, the sample selection mechanism may fail to maintain representative examples.

## Foundational Learning

- **Concept**: Bayesian inference with conjugate priors
  - Why needed here: DeepCCG relies on conjugate priors (Gaussian prior for Gaussian likelihood) to maintain tractable posterior updates when representations shift.
  - Quick check question: What property of conjugate priors makes them suitable for DeepCCG's online posterior updates?

- **Concept**: Class-conditional Gaussian models
  - Why needed here: The paper uses a class-conditional Gaussian classifier where each class has its own Gaussian distribution in representation space, enabling tractable posterior computation.
  - Quick check question: How does the class-conditional structure simplify the posterior computation compared to a general Gaussian mixture model?

- **Concept**: KL divergence and its relationship to Euclidean distance
  - Why needed here: The sample selection mechanism minimizes KL divergence between posteriors, which under certain assumptions reduces to minimizing squared Euclidean distance between class means.
  - Quick check question: Under what conditions does minimizing KL divergence between Gaussian posteriors reduce to minimizing Euclidean distance between means?

## Architecture Onboarding

- **Component map**: Data → Embedding → Classification → Memory Update → Sample Selection → Embedding Update
- **Critical path**: Data → Embedding → Classification → Memory Update → Sample Selection → Embedding Update
- **Design tradeoffs**:
  - Fixed memory buffer size vs. representation quality
  - Number of classes vs. memory requirements per class
  - Computational cost of posterior recomputation vs. accuracy
- **Failure signatures**:
  - Catastrophic forgetting when memory buffer becomes unrepresentative
  - Poor adaptation to representation shift when KL divergence approximation fails
  - Degraded performance when classes have highly overlapping representations
- **First 3 experiments**:
  1. Test posterior recomputation with synthetic representation shifts to verify one-step adaptation
  2. Compare memory selection methods (KL divergence vs. random sampling) on a small dataset
  3. Validate conditional likelihood loss by training on a simple dataset with known class structure

## Open Questions the Paper Calls Out
1. How does DeepCCG perform in class-incremental learning compared to task-incremental learning, and what is the potential impact of the lack of interaction between class mean parameters on performance?
2. How does DeepCCG's sample selection mechanism perform when dealing with imbalanced and noisy data?
3. How does DeepCCG's performance compare to other continual learning methods when the memory size is increased or decreased?

## Limitations
- The method assumes i.i.d. representation shift, which may not hold in real-world scenarios with non-i.i.d. or large shifts
- Critical implementation details for memory sample selection (number of inner iterations B, exact loss formulation) are underspecified
- Performance depends on fixed memory buffer sizes and specific hyperparameters that may limit generalizability

## Confidence
- **High confidence**: The core mechanism of using Bayesian class-conditional Gaussian classifiers for one-step posterior updates is well-grounded and theoretically sound
- **Medium confidence**: The empirical results showing consistent improvement over baselines are compelling, but the fixed memory buffer sizes and specific hyperparameters may limit generalizability
- **Low confidence**: The assumption that KL divergence minimization ensures representative memory samples under all types of representation shift requires further validation

## Next Checks
1. Test DeepCCG on datasets with non-i.i.d. representation shifts (e.g., domain shifts with different data distributions) to evaluate robustness beyond the assumed i.i.d. setting
2. Conduct ablation studies varying memory buffer sizes to determine the minimum buffer size needed for effective performance
3. Implement and test the exact memory sample selection algorithm with the specified number of inner iterations and loss formulation to verify faithful reproduction of the claimed performance