---
ver: rpa2
title: Dealing with negative samples with multi-task learning on span-based joint
  entity-relation extraction
arxiv_id: '2309.09713'
source_url: https://arxiv.org/abs/2309.09713
tags:
- entity
- extraction
- span
- relation
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the data imbalance problem in span-based joint
  entity-relation extraction, where negative samples (non-entity spans or irrelevant
  span pairs) significantly outnumber positive samples. The authors propose SpERT.MT,
  a multi-task learning approach that separates entity recognition and relation extraction
  into identification and classification subtasks.
---

# Dealing with negative samples with multi-task learning on span-based joint entity-relation extraction

## Quick Facts
- arXiv ID: 2309.09713
- Source URL: https://arxiv.org/abs/2309.09713
- Reference count: 25
- F1 scores: 73.61% (CoNLL04), 53.72% (SciERC), 83.72% (ADE)

## Executive Summary
This paper addresses the data imbalance problem in span-based joint entity-relation extraction, where negative samples (non-entity spans or irrelevant span pairs) significantly outnumber positive samples. The authors propose SpERT.MT, a multi-task learning approach that separates entity recognition and relation extraction into identification and classification subtasks. Key innovations include using IoU-based dynamic scaling for entity recognition to handle hard negative samples, and incorporating entity logits into span pair embeddings for relation extraction. Experiments on three datasets (CoNLL04, SciERC, ADE) show F1 scores of 73.61%, 53.72%, and 83.72% respectively, outperforming the baseline SpERT model and achieving state-of-the-art results on SciERC.

## Method Summary
The proposed SpERT.MT model employs a multi-task learning framework that decomposes both entity recognition and relation extraction into identification and classification subtasks. For entity recognition, the identification subtask determines whether a span is an entity using binary cross-entropy loss with IoU-based dynamic scaling, while the classification subtask assigns entity types using pairwise ranking loss. For relation extraction, the identification subtask determines whether a span pair has a relation, and the classification subtask assigns relation types, with entity logits from the entity recognition stage concatenated to the span pair embeddings. The model is trained with 120 negative samples per batch and evaluated on CoNLL04, SciERC, and ADE datasets using BERT, BioBERT, and SciBERT encoders respectively.

## Key Results
- Achieved F1 scores of 73.61% on CoNLL04, 53.72% on SciERC, and 83.72% on ADE datasets
- Outperformed baseline SpERT model on all three datasets
- Achieved state-of-the-art results on SciERC dataset
- Particularly effective in handling scenarios with excessive negative samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-task framework mitigates the impact of negative samples on both entity recognition and relation extraction.
- Mechanism: By decomposing each task into identification (binary classification of whether a span/pair is relevant) and classification (categorizing the span/pair into a specific type), the model reduces the noise introduced by excessive negative samples. The identification subtask allows the model to filter out irrelevant spans early, while the classification subtask focuses on categorizing only the relevant ones.
- Core assumption: Separating identification from classification prevents the classification model from being overwhelmed by the vast number of negative samples.
- Evidence anchors:
  - [abstract] "This approach employs the multitask learning to alleviate the impact of negative samples on entity and relation classifiers."
  - [section 3.3] "For the identification task, it is employed to determine whether the current entity or entity pair is indeed an entity or whether a relationship exists. This is a standard binary classification task, thus we opt to use binary cross-entropy as the loss function..."
- Break condition: If the identification subtask becomes too lenient or too strict, it could either let too many negative samples through or discard too many positive samples, negating the benefit.

### Mechanism 2
- Claim: IoU-based dynamic scaling improves entity boundary detection for hard negative samples.
- Mechanism: By calculating the Intersection over Union (IoU) between a candidate span and all gold entity spans, the model can quantify how much a span overlaps with an actual entity. This IoU score is used as a scaling factor in the loss function, increasing the penalty for hard negative samples (those that overlap significantly with real entities but are not labeled as such).
- Core assumption: Hard negative samples that overlap with true entities are more difficult to distinguish and thus require a higher loss penalty during training.
- Evidence anchors:
  - [section 3.4] "In the identification task of the entity recognition multi-task framework, we introduce the positional information between spans using the Intersection over Union (IoU) metric... we employ the Intersection over Union (IoU) within the multi-task framework to analyze hard negative samples quantitatively."
- Break condition: If the IoU calculation is too coarse (e.g., not accounting for entity boundaries precisely), it may misclassify some negative samples, reducing the effectiveness of the scaling.

### Mechanism 3
- Claim: Incorporating entity logits into span pair embeddings enriches relation extraction input.
- Mechanism: The logits (raw scores) from the entity recognition subtask contain information about how confident the model is in identifying a span as a particular entity type. By concatenating these logits with the span pair embeddings, the relation extraction subtask receives additional semantic context that helps distinguish between relevant and irrelevant entity pairs.
- Core assumption: Entity logits carry useful semantic information that can improve relation classification beyond what the span representations alone provide.
- Evidence anchors:
  - [section 3.5] "We observe that the Logits from the entity recognition stage may contain useful semantic information. Therefore, we further embed the Logits from the entity recognition stage to enrich the input information."
- Break condition: If the entity logits are not well-calibrated or are too noisy, they could introduce confusion rather than clarity into the relation extraction task.

## Foundational Learning

- Concept: Intersection over Union (IoU)
  - Why needed here: IoU is used to quantify the overlap between candidate spans and gold entity spans, enabling the model to identify hard negative samples that are difficult to distinguish from true entities.
  - Quick check question: How is IoU calculated between two spans, and why is it useful for detecting hard negative samples?

- Concept: Multi-task learning with shared representations
  - Why needed here: The model leverages shared span representations for both entity recognition and relation extraction, allowing the two tasks to benefit from each other's learned features.
  - Quick check question: What are the advantages of using a shared representation in multi-task learning for entity and relation extraction?

- Concept: Pairwise ranking loss
  - Why needed here: The pairwise ranking loss is used in the classification subtasks to optimize the model's ability to rank correct classes higher than incorrect ones, which is particularly useful when dealing with imbalanced data.
  - Quick check question: How does pairwise ranking loss differ from standard cross-entropy loss, and why is it beneficial for imbalanced datasets?

## Architecture Onboarding

- Component map:
  BERT embeddings -> Span representations -> Entity Recognition (Identification + Classification) -> Entity logits -> Relation Extraction (Identification + Classification)

- Critical path:
  1. Tokenize input sentence and generate BERT embeddings.
  2. Enumerate all possible spans and generate span representations.
  3. Entity Recognition: Identify relevant spans and classify them into entity types.
  4. Relation Extraction: Identify relevant span pairs and classify their relations, using entity logits for enrichment.

- Design tradeoffs:
  - Using max-pooling for span representation simplifies computation but may lose fine-grained positional information.
  - The multi-task framework adds complexity but improves handling of negative samples.
  - Incorporating entity logits enriches relation extraction but increases input dimensionality.

- Failure signatures:
  - Poor entity recognition recall: Likely due to insufficient IoU scaling or overly strict identification threshold.
  - Low relation extraction precision: Could be caused by noisy entity logits or insufficient negative sampling.
  - Degraded performance with fewer negative samples: Indicates the model relies too heavily on negative samples for training.

- First 3 experiments:
  1. Vary the IoU scaling factor (ENIoU) to observe its impact on entity recognition performance, especially for hard negative samples.
  2. Test the model with different numbers of negative samples to find the optimal balance for training.
  3. Remove entity logits from the relation extraction input to quantify their contribution to relation classification performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance scale with increasingly imbalanced datasets beyond the tested range?
- Basis in paper: [explicit] The paper mentions that performance stabilizes and then declines as negative samples increase beyond 100-120, suggesting a limit to the model's ability to handle extreme imbalance.
- Why unresolved: The experiments only tested up to 200 negative samples per sentence. The paper doesn't explore what happens with even more extreme imbalance ratios.
- What evidence would resolve it: Systematic experiments testing the model with progressively higher negative-to-positive sample ratios (e.g., 1000:1, 10000:1) to identify the breaking point and characterize the performance degradation curve.

### Open Question 2
- Question: Would incorporating entity type information into the relation extraction input representation further improve performance?
- Basis in paper: [inferred] The paper mentions that entity Logits contain semantic information and are used in relation extraction, but doesn't explicitly test incorporating entity type embeddings or other entity-specific features.
- Why unresolved: The experiments only tested using Logits from the entity recognition stage. The paper doesn't explore whether additional entity-level information could further enhance relation extraction.
- What evidence would resolve it: Controlled experiments comparing the current approach to variants that include explicit entity type embeddings or other entity-specific features in the relation extraction input representation.

### Open Question 3
- Question: How does the proposed multi-task framework compare to alternative approaches for handling negative samples in span-based joint extraction?
- Basis in paper: [explicit] The paper claims the multi-task approach effectively handles excessive negative samples, but doesn't compare against other methods like focal loss, class-balanced sampling, or adversarial training.
- Why unresolved: The ablation study only removes components of the proposed method, but doesn't compare against alternative negative sampling strategies or loss functions.
- What evidence would resolve it: Comparative experiments between the proposed multi-task framework and alternative approaches (focal loss, class-balanced sampling, adversarial training) on the same datasets with identical hyperparameter tuning.

## Limitations
- The model's effectiveness depends on careful calibration of the IoU scaling factor, which may not generalize across datasets
- The specific negative sampling ratio (120 per batch) may be suboptimal for different domains or datasets
- The approach may be sensitive to the quality of entity logits used in relation extraction

## Confidence
- **High Confidence:** The multi-task framework with separate identification and classification subtasks improves entity recognition and relation extraction performance by reducing the impact of negative samples; the approach achieves state-of-the-art results on SciERC and competitive performance on CoNLL04 and ADE datasets.
- **Medium Confidence:** IoU-based dynamic scaling specifically improves handling of hard negative samples in entity recognition; incorporating entity logits into span pair embeddings for relation extraction provides meaningful semantic enrichment.
- **Low Confidence:** The model's robustness to variations in negative sampling ratios and IoU scaling parameters; the generalizability of the approach to domains or datasets with different entity/relation distributions.

## Next Checks
- **Validation Check 1:** Systematically vary the IoU scaling factor (Î´) across a wide range (e.g., 0.1 to 10.0) and measure its impact on entity recognition performance, particularly focusing on recall for hard negative samples.
- **Validation Check 2:** Test the model with different negative sampling ratios (e.g., 30, 60, 120, 240 negatives per batch) to determine the optimal balance and assess whether the current choice of 120 negatives is optimal or could be improved.
- **Validation Check 3:** Train and evaluate the model with and without entity logits in the relation extraction input, while keeping all other components constant, to quantify the actual contribution of entity logits to relation extraction performance.