---
ver: rpa2
title: Expectation-Complete Graph Representations with Homomorphisms
arxiv_id: '2306.05838'
source_url: https://arxiv.org/abs/2306.05838
tags:
- graph
- graphs
- counts
- embedding
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of designing graph embeddings that
  are both efficient to compute and complete in expectation, meaning they can distinguish
  non-isomorphic graphs with high probability. Previous methods either sacrifice completeness
  for efficiency or cannot be computed efficiently for all graphs.
---

# Expectation-Complete Graph Representations with Homomorphisms

## Quick Facts
- arXiv ID: 2306.05838
- Source URL: https://arxiv.org/abs/2306.05838
- Reference count: 16
- Key outcome: Expectation-complete embeddings distinguish all non-isomorphic graphs with high probability through sampling homomorphism counts

## Executive Summary
This paper introduces a novel approach to graph representation learning that combines computational efficiency with theoretical completeness guarantees. The method leverages graph homomorphism counts sampled from carefully designed distributions to create embeddings that can distinguish all non-isomorphic graphs in expectation. By sampling patterns with controlled treewidth and using Hoeffding bounds to ensure approximation quality, the approach achieves polynomial-time computation while maintaining strong expressiveness guarantees. The authors demonstrate that combining these expectation-complete embeddings with graph neural networks consistently improves performance on multiple benchmark tasks.

## Method Summary
The method involves sampling graph homomorphism counts from a distribution with full support on all graphs up to maximum size n, then using these counts as features for downstream learning tasks. The key insight is that repeated sampling from a full-support distribution eventually allows distinguishing all non-isomorphic graphs with high probability. Pattern graphs are sampled with decreasing probability for higher treewidth to ensure polynomial-time computation. The homomorphism counts can be combined with GNN representations to create universal graph embeddings that are both computationally efficient and theoretically complete.

## Key Results
- Expectation-complete embeddings distinguish all non-isomorphic graphs with high probability through repeated sampling
- The method achieves polynomial-time computation on average by controlling pattern treewidth
- Combining with GNNs consistently improves performance on real-world datasets
- Outperforms baseline approaches on synthetic expressiveness datasets (CSL, PAULUS25)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Expectation-complete embeddings eventually distinguish all non-isomorphic graphs with high probability.
- **Mechanism:** By sampling pattern graphs from a full-support distribution, each non-isomorphic graph pair will be separated by at least one sampled pattern with nonzero probability. Repeated sampling makes the joint embedding complete in expectation.
- **Core assumption:** The distribution over patterns has full support on the set of all graphs up to the maximum size in the dataset.
- **Evidence anchors:**
  - [abstract] "expectation-complete embeddings that are efficiently computable in polynomial time on average and show that repeated sampling eventually allows distinguishing all pairs of non-isomorphic graphs."
  - [section 3] Lemma 3 and Theorem 4 prove that expectation-completeness leads to eventual universality.
- **Break condition:** If the sampling distribution lacks support for certain patterns (e.g., zero probability for patterns with specific treewidth), some non-isomorphic graphs may never be separated.

### Mechanism 2
- **Claim:** Sampling homomorphism counts approximates the full Lovász vector, enabling efficient computation.
- **Mechanism:** Instead of computing the infinite-dimensional Lovász vector exactly, the method samples a finite number of patterns and uses homomorphism counts as features. Hoeffding bounds guarantee convergence to the full vector with high probability.
- **Core assumption:** The sampling distribution over patterns is known and allows computing homomorphism counts in polynomial time in expectation.
- **Evidence anchors:**
  - [section 3.1] Theorem 8 provides explicit Hoeffding-based bounds on the number of samples needed for ε-approximation.
  - [section 4] Theorem 14 shows polynomial-time computation by sampling patterns with controlled treewidth.
- **Break condition:** If pattern treewidth is too large, homomorphism counting becomes computationally expensive, breaking the polynomial-time guarantee.

### Mechanism 3
- **Claim:** Combining expectation-complete embeddings with GNN representations creates a universal graph representation.
- **Mechanism:** The direct sum of the sampled homomorphism vector and the GNN's learned representation is expectation-complete, preserving permutation invariance and expressiveness.
- **Core assumption:** The GNN component alone is not complete, but adding the homomorphism vector restores completeness in expectation.
- **Evidence anchors:**
  - [section 5] "Combining any GNN graph level representation with our embedding for a fixed set of sampled patterns F as shown in Figure 2 is straightforward and allows to make any GNN architecture more expressive."
  - [section 7] Empirical results show GNN+hom consistently outperforms baseline GNNs across multiple datasets.
- **Break condition:** If the GNN's learned representation is already complete (which is unlikely for standard MPNNs), adding homomorphism counts provides diminishing returns.

## Foundational Learning

- **Concept: Graph homomorphism and treewidth**
  - Why needed here: Homomorphism counts are the core feature used for embedding, and treewidth bounds computational complexity.
  - Quick check question: Why does restricting pattern treewidth to k ensure polynomial-time computation for all graphs up to size n?

- **Concept: Weisfeiler-Lehman (WL) test and expressiveness**
  - Why needed here: The method's expressiveness is tied to the WL hierarchy, and understanding this connection is crucial for interpreting results.
  - Quick check question: How does the k-WL test relate to homomorphism counts of patterns with treewidth up to k?

- **Concept: Random Fourier features and kernel approximation**
  - Why needed here: The sampling-based approximation of the Lovász kernel follows similar principles to random Fourier features for kernel approximation.
  - Quick check question: What is the role of Hoeffding bounds in guaranteeing the quality of the sampled kernel approximation?

## Architecture Onboarding

- **Component map:** Pattern sampler -> Homomorphism counter -> Embedding constructor -> Classifier/learner
- **Critical path:**
  1. Sample ℓ patterns from the distribution D.
  2. For each graph in the dataset, compute homomorphism counts to all ℓ patterns.
  3. Construct the embedding vector (optionally combine with GNN features).
  4. Train the downstream model using the embedding as input.

- **Design tradeoffs:**
  - Pattern sampling vs. computational cost: More patterns increase expressiveness but also runtime.
  - Treewidth distribution: Lower maximum treewidth reduces computation but may limit expressiveness.
  - Fixed vs. adaptive sampling: Fixed sampling is simpler but adaptive sampling could target informative patterns.

- **Failure signatures:**
  - Low variance in homomorphism counts across patterns → poor discrimination.
  - Runtime grows faster than expected → incorrect treewidth distribution or large patterns.
  - No improvement over baseline GNN → sampled patterns not sufficiently expressive for the task.

- **First 3 experiments:**
  1. Implement homomorphism counting for small patterns (size ≤ 4) and verify counts match brute-force enumeration.
  2. Sample 10 patterns from a simple distribution (e.g., uniform over all graphs up to size 5) and compute embeddings for a small dataset.
  3. Train a simple classifier (e.g., logistic regression) on the embeddings and compare to a baseline using only graph size as a feature.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does approximate homomorphism counting affect the expressiveness of expectation-complete graph embeddings?
  - Basis in paper: [explicit] The authors mention that approximate counts would lose permutation-invariance and reference Abboud et al. (2021) who retained permutation-invariance "in expectation" with random node initialization.
  - Why unresolved: The paper acknowledges this as future work but doesn't analyze the trade-offs between computational efficiency and theoretical guarantees when using approximate counts.
  - What evidence would resolve it: Empirical studies comparing exact vs approximate homomorphism counting on benchmark datasets, showing the impact on both runtime and graph classification accuracy.

- **Open Question 2:** What is the optimal sampling distribution for patterns that balances expressiveness and computational efficiency?
  - Basis in paper: [inferred] The authors propose sampling from distributions with full support and discuss using treewidth-based distributions, but acknowledge that the optimal distribution depends on the specific dataset and learning task.
  - Why unresolved: The paper presents theoretical bounds but doesn't provide concrete guidance on how to select the best distribution for practical applications, nor does it explore adaptive sampling strategies.
  - What evidence would resolve it: Systematic experiments comparing different sampling distributions across multiple datasets, potentially including learned or data-dependent distributions that adapt to the specific graph characteristics.

- **Open Question 3:** Can expectation-complete graph embeddings be efficiently computed for graphs with unbounded size?
  - Basis in paper: [explicit] The authors prove that no distribution with full support on G∞ can achieve polynomial runtime in v(G) for all graphs, and propose restricting to bounded-size graphs (Gn) or using a different embedding approach for G∞.
  - Why unresolved: The paper provides theoretical limitations but doesn't offer a practical solution for computing expectation-complete embeddings on graphs with unbounded size, which is a significant limitation for real-world applications.
  - What evidence would resolve it: Development of a practical algorithm that can efficiently compute expectation-complete embeddings for graphs of arbitrary size, potentially through a combination of sampling strategies and approximation techniques.

## Limitations
- The polynomial-time guarantee depends on careful control of pattern treewidth, but the paper doesn't fully specify how to construct such distributions in practice.
- While the theoretical expressiveness is strong, empirical results show modest improvements on real-world datasets, suggesting practical limitations in learning useful features from homomorphism counts alone.
- The expectation-completeness guarantee relies heavily on the sampling distribution having full support over all patterns up to maximum graph size.

## Confidence
- **High confidence**: The theoretical framework connecting homomorphism sampling to expectation-completeness is well-established and rigorously proven. The polynomial-time computation bounds under treewidth constraints are sound.
- **Medium confidence**: The empirical results showing consistent but modest improvements across datasets. While statistically significant, the magnitude of improvement suggests practical constraints not fully addressed in the theory.
- **Medium confidence**: The claim that combining with GNNs creates universal representations. While theoretically sound, the practical benefit depends on the specific GNN architecture and dataset characteristics.

## Next Checks
1. **Distribution validation**: Implement and test multiple pattern sampling strategies (e.g., uniform vs. treewidth-weighted) to verify the full support assumption holds and affects distinguishability of non-isomorphic graphs.
2. **Treewidth impact study**: Systematically vary maximum treewidth in sampled patterns and measure both computational cost and expressiveness on synthetic datasets to identify optimal tradeoffs.
3. **Embedding sensitivity analysis**: Compare performance when using homomorphism counts as standalone features vs. concatenated with GNN features across different GNN architectures to isolate the contribution of each component.