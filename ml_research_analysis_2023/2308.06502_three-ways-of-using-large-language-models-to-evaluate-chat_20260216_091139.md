---
ver: rpa2
title: Three Ways of Using Large Language Models to Evaluate Chat
arxiv_id: '2308.06502'
source_url: https://arxiv.org/abs/2308.06502
tags:
- examples
- metrics
- used
- chatgpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We present three methods to evaluate chatbot responses using large
  language models (LLMs): (1) manual prompting with fixed few-shot examples, (2) training
  a regressor on top of LLM embeddings, and (3) using dynamically retrieved few-shot
  examples from a vector store. The dynamic few-shot prompting with ChatGPT achieved
  the best results, placing second in the DSTC 11 Track 4 ChatEval competition.'
---

# Three Ways of Using Large Language Models to Evaluate Chat

## Quick Facts
- arXiv ID: 2308.06502
- Source URL: https://arxiv.org/abs/2308.06502
- Reference count: 9
- Key outcome: Dynamic few-shot prompting with ChatGPT achieved best results in DSTC 11 Track 4 ChatEval competition, placing second with 0.61 Spearman correlation for appropriateness.

## Executive Summary
This paper presents three methods for evaluating chatbot responses using large language models: manual prompting with fixed few-shot examples, training a regressor on LLM embeddings, and using dynamically retrieved few-shot examples from a vector store. The dynamic few-shot prompting approach with ChatGPT showed the most promise, placing second in the competition. An ablation study revealed that Llama 2 models are closing the performance gap with ChatGPT while benefiting less from few-shot examples. The work demonstrates the potential of LLMs for automated dialogue evaluation while highlighting challenges in data representation and prompt engineering.

## Method Summary
The paper explores three approaches for LLM-based dialogue evaluation. First, simple prompting uses fixed few-shot examples with different LLM models (OPT-30B, TK-Instruct-11B, GPT-NeoX-20B). Second, a regression approach trains a feed-forward network on LLM embeddings to predict metric scores. Third, dynamic few-shot prompting retrieves contextually similar examples from a vector store of embedded dialogue turns using MPNet and FAISS, then prompts ChatGPT with these examples. The system evaluates turn-level dialogue metrics including appropriateness, content richness, grammatical correctness, and relevance using Spearman correlation between predicted and human-annotated scores.

## Key Results
- Dynamic few-shot prompting with ChatGPT achieved second place in DSTC 11 Track 4 ChatEval competition
- Llama 2 models showed significant improvement over previous open-source models, reaching 0.61 Spearman correlation for appropriateness
- Fixed few-shot examples improved Llama 2 performance but less dramatically than ChatGPT
- Regression approach successfully handled malformed LLM outputs but showed limited competitive performance

## Why This Works (Mechanism)

### Mechanism 1
Dynamic few-shot examples from a vector store improve LLM chat evaluation performance by providing contextually relevant demonstrations. At inference time, the input dialogue is embedded and used to retrieve most similar examples from a pre-built vector store of annotated dialogues. These retrieved examples are included in the prompt to ChatGPT, providing relevant few-shot demonstrations that improve the model's understanding of the evaluation task. The vector store must contain sufficiently diverse and representative examples that capture the range of dialogue contexts and evaluation criteria needed for accurate scoring.

### Mechanism 2
Open-source LLMs like Llama 2 can approach ChatGPT performance in chat evaluation when given carefully designed prompts, though they benefit less from few-shot examples. Llama 2 models achieve high Spearman correlation scores through improved base capabilities and prompt engineering, closing the performance gap with ChatGPT without requiring extensive few-shot demonstrations. The Llama 2 models have sufficient instruction-following capability built into their pretraining to perform well on evaluation tasks with minimal prompting adjustments.

### Mechanism 3
Training a regression model on LLM embeddings can bypass the fragility of direct prompting while leveraging the semantic understanding captured in embeddings. LLM responses to evaluation prompts are processed through a simple feed-forward network trained to predict metric scores from the embedding representations, avoiding issues with malformed direct outputs. The LLM's contextual embeddings contain sufficient information about dialogue quality even when the decoder produces imperfect outputs.

## Foundational Learning

- **Few-shot learning with LLMs**: The evaluation task requires the model to understand nuanced dialogue quality criteria without extensive task-specific training, making few-shot demonstrations essential for guiding the LLM's reasoning. *Quick check: What distinguishes effective few-shot examples from ineffective ones in this context?*
- **Vector similarity search and embeddings**: Dynamic retrieval of relevant examples requires converting dialogue contexts into meaningful vector representations that capture semantic similarity for effective retrieval. *Quick check: How does the choice of embedding model affect the quality of retrieved examples?*
- **Spearman correlation for ranking evaluation**: The task evaluates turn-level quality through ranking rather than absolute score prediction, requiring correlation metrics that measure rank agreement rather than value agreement. *Quick check: Why might Spearman correlation be preferred over Pearson correlation for this evaluation task?*

## Architecture Onboarding

- **Component map**: Vector store (FAISS-based similarity search index) -> Embedding model (MPNet-based sentence representation) -> LLM interface (ChatGPT API or open-source LLM inference) -> Prompt template (standardized format with example placeholders) -> Regression model (simple feed-forward network)
- **Critical path**: Preprocess development data into vector store with embedded examples -> At inference, embed input dialogue and retrieve similar examples -> Construct prompt with retrieved examples and evaluation instructions -> Submit to LLM and parse the output score -> For regression approach, extract embeddings and pass through FNN
- **Design tradeoffs**: Vector store size vs. retrieval speed (larger stores provide more diverse examples but increase latency), fixed vs. dynamic examples (fixed examples are simpler but less contextually relevant), single-metric vs. multi-metric prompts (single-metric prompts are more reliable but require multiple LLM calls)
- **Failure signatures**: Poor retrieval quality (retrieved examples are irrelevant or too similar to each other), prompt misinterpretation (LLM responds with dialogue continuation instead of scores), score distribution mismatch (predicted scores don't match human annotation distributions)
- **First 3 experiments**: Test retrieval quality (embed a known dialogue and verify retrieved examples are semantically similar), validate prompt format (run simple prompt with hardcoded examples and verify correct output format), compare fixed vs. dynamic examples (evaluate same inputs with both approaches to measure performance difference)

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different instruction-tuned LLM checkpoints compare in terms of reliability and format adherence when evaluating chatbot responses? The paper states that instruction-tuned LLM checkpoints produce results with intended formats more reliably compared to non-instruction-tuned models, but doesn't provide detailed comparisons or quantitative data.

- **Open Question 2**: What is the impact of data distribution mismatch between rehearsal and test sets on the performance of chatbot evaluation systems? The paper observes that the rehearsal set is not representative of the test set, which likely resulted in model selection and hyperparameter search being detrimental to the final performance, but doesn't provide a detailed analysis.

- **Open Question 3**: How do dynamic few-shot examples from a vector store compare to static few-shot examples in terms of improving the performance of LLM-based chatbot evaluation systems? The paper mentions that using examples dynamically obtained from the vector store instead of hand-picked fixed examples did not bring any additional improvements in their ablation study, but doesn't explore potential reasons for this outcome.

## Limitations

- The heuristic mapping between development set metrics and target metrics is described but not explicitly defined, making exact reproduction difficult
- Prompt templates for simple prompting are stated to be "slightly adapted" for different models but the specific adaptations are not provided
- While the vector store approach shows improvement, the analysis of retrieval quality and example diversity is minimal

## Confidence

- **Dynamic few-shot prompting with ChatGPT**: Medium confidence - Strong competition results but lacks detailed prompt templates and full evaluation details
- **Llama 2 performance claims**: Medium confidence - Promising ablation study results but conducted after competition deadline with incomplete experimental setup details
- **Regression model approach**: Low confidence - Described but results not clearly presented in main evaluation, minimal regression architecture details

## Next Checks

1. **Reproduce the vector store retrieval quality**: Implement the MPNet embedding and FAISS retrieval system, then evaluate whether retrieved examples are semantically similar to query dialogues using cosine similarity and qualitative inspection of example pairs.

2. **Test prompt template variations**: Create and test multiple prompt formats with fixed few-shot examples to identify which template variations produce the most consistent and correctly formatted outputs across different LLMs.

3. **Compare score distribution alignment**: Analyze the distribution of predicted scores from each approach against the human-annotated rehearsal set to verify that models are calibrated to the same score ranges as human annotators.