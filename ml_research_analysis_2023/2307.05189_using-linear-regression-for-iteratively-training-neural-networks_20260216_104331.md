---
ver: rpa2
title: Using Linear Regression for Iteratively Training Neural Networks
arxiv_id: '2307.05189'
source_url: https://arxiv.org/abs/2307.05189
tags:
- ills
- adam
- training
- neural
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ILLS (Iterative Linear Least Squares), a novel
  approach to training neural networks that leverages the inherent linear relationship
  between consecutive layers. Unlike backpropagation, which computes gradients of
  the loss with respect to parameters, ILLS alternates between solving for optimal
  parameters and hidden activations in a linear fashion.
---

# Using Linear Regression for Iteratively Training Neural Networks

## Quick Facts
- arXiv ID: 2307.05189
- Source URL: https://arxiv.org/abs/2307.05189
- Authors: 
- Reference count: 6
- One-line primary result: ILLS achieves lower training error, faster convergence, and higher stability than Adam for small feedforward networks with invertible activation functions.

## Executive Summary
This paper introduces ILLS (Iterative Linear Least Squares), a novel approach to training neural networks that leverages the inherent linear relationship between consecutive layers. Unlike backpropagation, which computes gradients of the loss with respect to parameters, ILLS alternates between solving for optimal parameters and hidden activations in a linear fashion. The method is demonstrated on simple feedforward networks with invertible activation functions (tanh), showing significant advantages over standard backpropagation with Adam optimizer.

## Method Summary
ILLS alternates between solving for optimal parameters and hidden activations using linear least squares. It exploits the linear relationship between consecutive layers to iteratively solve for ideal input values and update parameters. The approach requires invertible activation functions to compute ideal input values from outputs. ILLS directly updates parameters without learning rates but uses small linearized steps for hidden activations. The method is tested on synthetic and real-world datasets, showing lower training error, faster convergence, and higher stability compared to Adam, especially with higher learning rates.

## Key Results
- ILLS achieves lower training error than Adam on synthetic and real-world datasets
- ILLS converges faster and is more stable, especially with higher learning rates
- ILLS can achieve good approximations of true parameters, as evidenced by lower L2 norm error compared to Adam

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ILLS achieves lower training error than Adam by solving linear equations for optimal parameters and hidden activations instead of computing gradients.
- Mechanism: The approach exploits the linear relationship between consecutive layers to iteratively solve for ideal input values and update parameters using linear least squares. This avoids gradient descent's sensitivity to learning rates and allows larger updates.
- Core assumption: The activation function is invertible, allowing computation of ideal input values from outputs.
- Evidence anchors:
  - [abstract] "Unlike backpropagation, which computes gradients of the loss with respect to parameters, ILLS alternates between solving for optimal parameters and hidden activations in a linear fashion."
  - [section] "Since f is assumed to be invertible, we can directly compute the vector H = f^−1(y) for the data set, which is the total input required for the output neuron."
  - [corpus] No direct corpus evidence for this specific linear relationship exploitation mechanism.
- Break condition: If activation functions are not invertible or the linear approximation breaks down in deeper networks.

### Mechanism 2
- Claim: ILLS is more stable than Adam, especially with higher learning rates, due to direct parameter updates without gradient chains.
- Mechanism: By solving linear least squares problems for parameters and using small linearized steps for hidden activations, ILLS avoids the instability that comes from chained non-linearities in backpropagation.
- Core assumption: The linear approximation of the activation function around current parameter values is valid.
- Evidence anchors:
  - [abstract] "Experiments on synthetic and real-world datasets reveal that ILLS achieves lower training error, converges faster, and is more stable, especially with higher learning rates."
  - [section] "Since this is a linear approximation, it is only valid in a small region around ˆh21, ˆh22. Therefore we define a learning rate ρ for updating the values."
  - [corpus] No direct corpus evidence for this specific stability advantage mechanism.
- Break condition: If the linear approximation becomes invalid (e.g., large parameter updates or highly non-linear activation functions).

### Mechanism 3
- Claim: ILLS can achieve good approximations of true parameters, as evidenced by lower L2 norm error compared to Adam.
- Mechanism: The iterative linear least squares approach directly optimizes for parameter values that minimize output error, leading to better parameter estimates than gradient-based methods.
- Core assumption: The underdetermined system of equations (more unknowns than equations) can be solved effectively using linear least squares.
- Evidence anchors:
  - [abstract] "Experiments on synthetic and real-world datasets reveal that ILLS achieves lower training error, converges faster, and more stable, especially with higher learning rates."
  - [section] "Instead of solving (6) directly to get ˆh21, ˆh22, we solve for the deviation in the previous layer parameters, which lets us (approximately) satisfy the input-output relationship of the neuron."
  - [corpus] No direct corpus evidence for this specific parameter approximation mechanism.
- Break condition: If the linear least squares solution becomes ill-conditioned or the underdetermined system cannot be solved effectively.

## Foundational Learning

- Concept: Linear Least Squares
  - Why needed here: ILLS relies on solving linear least squares problems to update parameters and hidden activations iteratively.
  - Quick check question: How does the normal equation (A^T A)x = A^T b relate to solving linear least squares problems?

- Concept: Invertibility of Activation Functions
  - Why needed here: ILLS requires invertible activation functions to compute ideal input values from outputs.
  - Quick check question: Why is it important for the activation function to be invertible in ILLS, and how does this relate to computing f^−1(y)?

- Concept: Underdetermined Systems
  - Why needed here: ILLS deals with underdetermined systems of equations (more unknowns than equations) and uses linear least squares to find solutions.
  - Quick check question: What is an underdetermined system, and how can linear least squares be used to find solutions in such cases?

## Architecture Onboarding

- Component map: Input -> Hidden Layer(s) with tanh activation -> Output Layer
- Critical path: Initialize parameters → Forward pass → Solve for optimal parameters using linear least squares → Update hidden activations using linearized steps → Backward pass → Repeat until convergence
- Design tradeoffs:
  - ILLS vs. Backpropagation: ILLS avoids gradient computation and chain of non-linearities but requires invertible activation functions
  - Stability vs. Convergence speed: ILLS is more stable with higher learning rates but may converge slower than optimized backpropagation for large networks
  - Parameter updates vs. Hidden activation updates: ILLS directly updates parameters without learning rates but uses small linearized steps for hidden activations
- Failure signatures:
  - Divergence: ILLS may diverge if the linear approximation becomes invalid or the underdetermined system cannot be solved effectively
  - Slow convergence: ILLS may converge slower than optimized backpropagation for large networks or non-invertible activation functions
  - Ill-conditioned solutions: ILLS may produce ill-conditioned solutions if the linear least squares problem is poorly conditioned
- First 3 experiments:
  1. Implement ILLS on a simple 2-layer network with tanh activation and compare training error with Adam optimizer
  2. Test ILLS with different learning rates and initializations to assess stability and convergence speed
  3. Extend ILLS to a 1-layer network with 3 inputs and evaluate its performance on a synthetic dataset with known ground truth parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How will ILLS perform on larger networks with many hidden layers compared to backpropagation?
- Basis in paper: [inferred] from "Computationally, ILLS does not appear to have any higher overheads than standard gradient based backpropagation. However, the numerical stability of the procedure is untested at this time."
- Why unresolved: The paper only tested ILLS on small networks with 1-2 hidden layers. Scaling to deeper networks may introduce numerical instability or computational challenges not observed in the experiments.
- What evidence would resolve it: Testing ILLS on deep networks with 10+ layers on various datasets and comparing performance metrics like convergence speed, final accuracy, and computational resources required compared to backpropagation.

### Open Question 2
- Question: Can ILLS be extended to work with non-invertible activation functions like ReLU?
- Basis in paper: [explicit] from "How will ILLS extend to non-invertible activations? In this case, we note that while there are common activation functions such as ReLU with a many-to-one relationship from input to output, there are almost none with a many-to-many relationship."
- Why unresolved: The paper only demonstrated ILLS with invertible activation functions like tanh. Extending to non-invertible functions would require modifications to handle the one-to-many mapping problem.
- What evidence would resolve it: Implementing ILLS with ReLU activation and comparing its performance to backpropagation on standard datasets. Investigating modifications like solving for subsets of active neurons or using approximate inverses.

### Open Question 3
- Question: How does ILLS compare to backpropagation on more complex loss functions like cross-entropy?
- Basis in paper: [explicit] from "How will ILLS extend to loss functions such as cross-entropy? In this case, the ‘ideal’ outputs y are one-hot vectors, which are not useful for ILLS because the ideal input values correspond to ±∞."
- Why unresolved: The paper only demonstrated ILLS on mean squared error loss. Cross-entropy loss with one-hot targets poses challenges for the ILLS approach which relies on computing ideal input values.
- What evidence would resolve it: Implementing ILLS with cross-entropy loss and comparing its performance to backpropagation on classification tasks. Testing modifications like using softened targets or output scaling to handle the one-hot target issue.

## Limitations

- The experimental validation is limited to simple feedforward networks with invertible activation functions (tanh).
- The claims of superior performance, stability, and convergence speed are based on synthetic datasets and a single real-world time series dataset.
- The paper does not provide a clear characterization of the conditions under which ILLS is expected to outperform backpropagation or other methods.

## Confidence

- High confidence: The core ILLS algorithm and its implementation details are clearly specified, and the basic mechanism of alternating between solving for optimal parameters and hidden activations using linear least squares is well-explained.
- Medium confidence: The claims of lower training error, faster convergence, and higher stability with ILLS compared to Adam are supported by the presented experiments, but the evidence is limited to simple networks and specific datasets.
- Low confidence: The generalizability of ILLS to deeper networks, non-invertible activation functions, and diverse real-world datasets is not thoroughly explored or discussed in the paper.

## Next Checks

1. Extend the ILLS implementation to deeper networks (e.g., 3-4 hidden layers) and evaluate its performance on standard benchmark datasets (e.g., MNIST, CIFAR-10) compared to state-of-the-art optimizers.
2. Investigate the behavior of ILLS with non-invertible activation functions (e.g., ReLU, sigmoid) and assess the impact on convergence, stability, and final training error.
3. Analyze the computational complexity and memory requirements of ILLS compared to backpropagation, especially for large-scale networks, and explore potential optimizations or approximations to improve efficiency.