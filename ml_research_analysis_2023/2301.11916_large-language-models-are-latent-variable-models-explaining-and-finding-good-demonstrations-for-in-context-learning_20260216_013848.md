---
ver: rpa2
title: 'Large Language Models Are Latent Variable Models: Explaining and Finding Good
  Demonstrations for In-Context Learning'
arxiv_id: '2301.11916'
source_url: https://arxiv.org/abs/2301.11916
tags:
- learning
- demonstrations
- concept
- language
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the phenomenon of in-context learning in
  large language models (LLMs) by proposing a Bayesian explanation. The authors hypothesize
  that LLMs implicitly infer a latent concept variable from demonstration examples
  during in-context learning.
---

# Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning

## Quick Facts
- arXiv ID: 2301.11916
- Source URL: https://arxiv.org/abs/2301.11916
- Authors: 
- Reference count: 33
- Key outcome: The proposed demonstration selection method improves in-context learning performance by 12.5% on average across eight GPT2 and GPT3 models on eight text classification datasets.

## Executive Summary
This paper investigates in-context learning in large language models by proposing a Bayesian interpretation where LLMs implicitly infer latent concept variables from demonstration examples. The authors develop an algorithm that selects optimal demonstrations by fine-tuning concept token embeddings on a small model (GPT2-large) and then transferring these demonstrations to larger models. Their experiments show significant performance improvements over random selection baselines, supporting their hypothesis that effective demonstrations help LLMs infer task-related latent concepts. The method works across different model sizes trained on the same pre-training objective.

## Method Summary
The method consists of two phases: first, fine-tuning a small LLM (GPT2-large) to learn concept tokens for each dataset by adding new tokens to the vocabulary and optimizing their embeddings on diverse tasks; second, using these learned concept tokens to compute demonstration scores and select top-k examples from a candidate set based on their probability of inducing the concept tokens. The same demonstrations are then applied across different model sizes without retraining. The approach treats LLMs as performing implicit Bayesian inference over latent concepts, similar to topic models.

## Key Results
- Average 12.5% improvement in in-context learning performance over random selection baseline
- Significant improvements across eight GPT2 and GPT3 models (small to large)
- Consistent performance gains across eight real-world text classification datasets
- Successful transfer of demonstrations across different model sizes trained on same pre-training objective

## Why This Works (Mechanism)

### Mechanism 1
LLMs implicitly infer a latent concept variable from demonstrations during in-context learning, similar to how topic models infer topics from documents. The LLM's internal representations approximate a Bayesian posterior over latent concepts conditioned on demonstration tokens, enabling it to generalize from examples. This assumes the LLM's pre-training data distribution approximates the task data distribution, and demonstrations that maximize the posterior probability of the task's concept variable improve performance.

### Mechanism 2
Fine-tuning embeddings of newly added concept tokens allows the LLM to learn task-specific representations that approximate the true latent concept variable. By minimizing the loss between the LLM's output given concept tokens and the true labels, the concept tokens' embeddings converge to represent the task's underlying concept. This relies on the concept tokens' embeddings being optimizable to approximate the true latent concept variable, and the LLM's architecture allowing effective learning of these representations.

### Mechanism 3
Selecting demonstrations that maximize the probability of the task's concept tokens improves in-context learning performance by providing better evidence for the latent concept. Demonstrations are chosen based on their ability to induce the LLM to predict the task's concept tokens, effectively selecting examples that best represent the underlying concept. This assumes the demonstrations that maximize concept token probability are also the most informative for the task, and the LLM's posterior over concepts given demonstrations correlates with task performance.

## Foundational Learning

- Concept: Bayesian inference and latent variable models
  - Why needed here: The paper's explanation of in-context learning relies on viewing LLMs as implicitly performing Bayesian inference over latent concept variables, similar to topic models.
  - Quick check question: Can you explain how a topic model like LDA uses Bayesian inference to infer latent topics from documents?

- Concept: Causal reasoning and structural equations
  - Why needed here: The paper assumes a causal relationship between inputs, outputs, and latent concepts, which is crucial for understanding how demonstrations influence the LLM's inference of the task concept.
  - Quick check question: Can you describe the difference between the X→Y←θ and Y→X←θ causal directions, and why the paper focuses on the former?

- Concept: Fine-tuning and embedding optimization
  - Why needed here: The paper's method for learning concept tokens involves fine-tuning the embeddings of newly added tokens, which requires understanding how fine-tuning works in the context of LLMs.
  - Quick check question: Can you explain how fine-tuning only the embeddings of concept tokens, while keeping other parameters frozen, allows the LLM to learn task-specific representations?

## Architecture Onboarding

- Component map: GPT2-large (concept token learning) -> Concept token embeddings -> Demonstration selection algorithm -> GPT2/GPT3 models (in-context learning)

- Critical path:
  1. Fine-tune concept token embeddings on a diverse set of tasks
  2. Compute the probability of each demonstration example inducing the concept tokens
  3. Select top-k demonstrations based on this probability
  4. Use the selected demonstrations for in-context learning on new tasks

- Design tradeoffs:
  - Using a small number of concept tokens (c=10) vs. a larger number: more tokens may better represent the concept but increase computational cost
  - Fine-tuning concept token embeddings vs. full model fine-tuning: less computationally expensive but may limit the model's ability to learn complex concepts
  - Selecting demonstrations based on concept token probability vs. other metrics (e.g., similarity to test input): concept token probability may better capture the underlying task concept

- Failure signatures:
  - Poor in-context learning performance despite high concept token probability of selected demonstrations
  - Concept token embeddings failing to converge or representing irrelevant concepts
  - Demonstrations that maximize concept token probability not correlating with task performance

- First 3 experiments:
  1. Fine-tune concept token embeddings on a diverse set of tasks and visualize the learned embeddings to check if they capture task-specific information
  2. Select demonstrations based on concept token probability and compare in-context learning performance to random selection on a simple task (e.g., sentiment analysis)
  3. Test the transferability of selected demonstrations across different LLM sizes and pre-training objectives (e.g., GPT2-small, GPT2-medium, GPT3-ada)

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed demonstration selection method compare when using a more diverse set of tasks S for latent concept learning? The authors note that the selection of the diverse tasks S is currently left to the discretion of the user and suggest that a more principled approach to constructing such a task set is needed. Experiments comparing the performance of the proposed method using different sets of tasks S, with varying levels of diversity and size, would provide insight into the importance of this choice and help determine an optimal approach.

### Open Question 2
How does the proposed method perform on tasks that are not text classification, such as machine translation or question answering? The authors mention that the method has been tested on multiple-choice problems like math questions and found to be ineffective, but they do not provide results for other types of tasks. Experiments applying the proposed method to a variety of NLP tasks, such as machine translation, question answering, or summarization, would help determine its generalizability and identify areas for improvement.

### Open Question 3
How does the proposed method compare to other in-context learning approaches, such as chain-of-thought prompting or meta-learning? The authors briefly mention chain-of-thought prompting as a related work but do not provide a direct comparison between their method and other in-context learning approaches. Experiments comparing the proposed method to other in-context learning approaches, such as chain-of-thought prompting, meta-learning, or other demonstration selection methods, would provide insight into its effectiveness and potential advantages or disadvantages.

## Limitations

- The paper assumes LLMs' pre-training distribution approximates task distributions without rigorous validation of this relationship
- The interpretation of concept tokens as representing true latent concepts remains somewhat speculative
- The method's effectiveness across diverse task types beyond the tested text classification datasets is unproven

## Confidence

- **High confidence**: The demonstration selection algorithm improves in-context learning performance over random baselines
- **Medium confidence**: The Bayesian interpretation provides a plausible explanation for observed phenomena
- **Low confidence**: The concept token fine-tuning method reliably learns task-specific latent concepts

## Next Checks

1. Test the method on a broader range of task types (e.g., numerical reasoning, code generation) to assess generalizability beyond text classification
2. Conduct ablation studies comparing concept token-based selection against alternative demonstration selection methods (e.g., similarity-based, diversity-based)
3. Perform controlled experiments varying pre-training data distribution to test the sensitivity of concept token learning to distribution shifts