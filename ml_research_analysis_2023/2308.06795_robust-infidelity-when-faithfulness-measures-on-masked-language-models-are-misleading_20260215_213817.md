---
ver: rpa2
title: 'Robust Infidelity: When Faithfulness Measures on Masked Language Models Are
  Misleading'
arxiv_id: '2308.06795'
source_url: https://arxiv.org/abs/2308.06795
tags:
- adversarial
- masking
- tokens
- samples
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that faithfulness metrics based on iterative
  masking are not suitable for comparing the interpretability of neural text classifiers
  due to model-specific behaviors and out-of-distribution inputs. The authors show
  that iterative masking produces large variation in faithfulness scores between otherwise
  comparable Transformer encoder text classifiers.
---

# Robust Infidelity: When Faithfulness Measures on Masked Language Models Are Misleading

## Quick Facts
- arXiv ID: 2308.06795
- Source URL: https://arxiv.org/abs/2308.06795
- Reference count: 7
- Primary result: Iterative masking produces embeddings outside the training distribution, making faithfulness scores unreliable for cross-model interpretability comparisons

## Executive Summary
This paper demonstrates that faithfulness metrics based on iterative masking are not suitable for comparing the interpretability of neural text classifiers due to model-specific behaviors and out-of-distribution inputs. The authors show that iterative masking produces large variation in faithfulness scores between otherwise comparable Transformer encoder text classifiers. They demonstrate that iteratively masked samples produce embeddings outside the distribution seen during training, resulting in unpredictable behavior. The paper further explores task-specific considerations that undermine principled comparison of interpretability using iterative masking, such as an underlying similarity to salience-based adversarial attacks.

## Method Summary
The study evaluates BERT and RoBERTa models on four task datasets using integrated gradients for feature attributions. Fidelity scores are calculated by iteratively masking tokens in descending order of feature importance until the model's prediction changes. The research examines both clean and adversarially attacked samples, with and without adversarial training, to analyze the impact on faithfulness scores across different models and datasets.

## Key Results
- Iterative masking causes embeddings to move outside the training distribution, leading to unpredictable model behavior
- Faithfulness scores vary significantly between comparable models due to dataset-specific properties
- Adversarial training does not consistently improve faithfulness scores across different datasets and attack types
- The frequency of samples where predicted class doesn't change during masking is high across all datasets and models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative masking produces embeddings outside the training distribution
- Mechanism: Masking tokens changes the input representation so much that the resulting embeddings fall outside the manifold of the training data, leading to unpredictable model behavior
- Core assumption: Models behave predictably only on in-distribution data
- Evidence anchors: Large variation in faithfulness scores between comparable models; embedding analysis showing increased cosine distance between original dataset and masked samples

### Mechanism 2
- Claim: Faithfulness scores are not comparable across models due to dataset-specific properties
- Mechanism: Whether masking changes the true class depends on the dataset's class distribution and the semantic relationship between tokens
- Core assumption: The relationship between token importance and class prediction is consistent across datasets and models
- Evidence anchors: Wikipedia Toxic Comments dataset shows that removing tokens from inoffensive comments rarely creates toxic samples; fundamental assumption that removing salient tokens should change model output is not intuitive for all datasets

### Mechanism 3
- Claim: Adversarial training does not consistently improve faithfulness scores
- Mechanism: Adversarial training affects model robustness differently depending on attack type and dataset characteristics
- Core assumption: Adversarial training has uniform effect on model behavior under iterative masking
- Evidence anchors: Fidelity scores generally higher on successful adversarial samples but vary significantly across datasets; adversarial training shows no consistent impact on fidelity scores across different datasets

## Foundational Learning

- Concept: Manifold learning and out-of-distribution behavior
  - Why needed here: The paper shows that iterative masking pushes samples outside the training manifold, causing unpredictable behavior
  - Quick check question: What happens when a model encounters an input representation that was never seen during training?

- Concept: Adversarial attacks and robustness
  - Why needed here: The paper explores how iterative masking resembles an adversarial attack and how adversarial training affects faithfulness scores
  - Quick check question: How does an adversarial attack differ from a standard input perturbation in terms of the attacker's goal?

- Concept: Feature attribution methods (specifically Integrated Gradients)
  - Why needed here: The paper uses Integrated Gradients to generate feature attributions for calculating faithfulness scores
  - Quick check question: What is the difference between a gradient-based feature attribution method and a perturbation-based method?

## Architecture Onboarding

- Component map: Input text → Feature attribution (Integrated Gradients) → Iterative masking based on attribution scores → Model prediction on masked input → Comparison of original vs masked predictions → Faithfulness score calculation
- Critical path: The sequence of operations from input text through masking to faithfulness score calculation
- Design tradeoffs: Using more sophisticated feature attribution methods vs. simpler ones, the choice of masking strategy (percentage vs. class change), the number of steps in Integrated Gradients calculation
- Failure signatures: Large variation in faithfulness scores between comparable models, high frequency of samples where predicted class doesn't change during masking, embeddings moving far from the original dataset distribution
- First 3 experiments:
  1. Calculate fidelity scores for multiple models on the same dataset to observe variation
  2. Analyze embedding distributions of original vs. iteratively masked samples using UMAP
  3. Test adversarial training impact on faithfulness scores using different attack types

## Open Questions the Paper Calls Out
- Under what specific conditions, if any, can iterative masking be reliably used to compare the interpretability of neural text classifiers across different models?
- How do the distributional shifts caused by iterative masking specifically affect the internal representations of different neural architectures?
- What alternative faithfulness measures could provide more principled cross-model comparisons of interpretability without relying on out-of-distribution samples?

## Limitations
- Focuses primarily on Transformer-based encoders without examining whether findings generalize to other architectures
- Does not explore whether alternative masking strategies might avoid the out-of-distribution problem
- Does not investigate whether other robustness-enhancing techniques might address the core issue

## Confidence

High confidence: The empirical demonstration that iterative masking pushes samples outside the training distribution, causing unpredictable model behavior.

Medium confidence: The claim that iterative masking resembles adversarial attacks and that this similarity undermines faithfulness as an interpretability measure.

Medium confidence: The conclusion that iterative masking is generally inappropriate for cross-model interpretability comparison.

## Next Checks

1. Test whether the out-of-distribution behavior persists when using alternative feature attribution methods (e.g., SHAP, LIME) instead of Integrated Gradients.

2. Evaluate whether applying domain adaptation or fine-tuning techniques on iteratively masked samples can bring them back into-distribution.

3. Conduct a systematic analysis of class distribution effects by creating synthetic datasets with controlled imbalance ratios.