---
ver: rpa2
title: 'A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated
  Learning and Domain Incremental Learning for Multicenter Collaboration'
arxiv_id: '2309.17192'
source_url: https://arxiv.org/abs/2309.17192
tags:
- learning
- data
- center
- training
- centers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of different continual
  learning regularization methods for multicenter collaboration in medical applications.
  The authors adapt a conventional domain/task incremental learning framework for
  incremental transfer learning, where models are trained sequentially across multiple
  centers while preserving performance on previous centers.
---

# A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration

## Quick Facts
- **arXiv ID**: 2309.17192
- **Source URL**: https://arxiv.org/abs/2309.17192
- **Reference count**: 40
- **Primary result**: LWF, EBLL, IMM-mean, and IMM-mode regularization methods effectively reduce performance oscillations and overall performance drops when training on non-IID data in multicenter collaboration settings

## Executive Summary
This paper proposes an incremental transfer learning (ITL) framework that combines peer-to-peer federated learning with domain incremental learning for multicenter collaboration, particularly in medical applications. The framework addresses data privacy constraints by training models sequentially across multiple centers while preserving performance on previous centers through continual learning regularization. The study comprehensively evaluates various regularization methods on heterogeneous datasets including Tiny ImageNet, retinal fundus images for diabetic retinopathy, and glioma segmentation from T1 contrast-enhanced MRI images.

## Method Summary
The ITL framework uses a single-head classifier setting where a shared feature extractor is jointly optimized with center-specific classifiers. Models are trained sequentially across centers using cyclic weight transfer (CWT) where centers can be revisited for additional training. Various continual learning regularization methods are applied including LWF (Learning without Forgetting), EWC (Elastic Weight Consolidation), SI (Synaptic Intelligence), MAS (Memory Aware Synapses), EBLL (Encoder-based Lifelong Learning), and IMM (Implicit Model Maintenance with mean and mode variants). The framework includes adaptive optimizers (Adam), early stopping based on validation loss, and random center order initialization to ensure fair evaluation.

## Key Results
- LWF, EBLL, IMM-mean, and IMM-mode regularization methods effectively reduce performance oscillations and overall performance drops when training on non-IID data
- The order of training centers matters, with heterogeneous data centers preferred at the end of the training sequence
- CWT achieves superior overall accuracy compared to SWT, though at the cost of more frequent weight transfers
- Single-head setting preserves feature extractor-classifier coherence and reduces catastrophic forgetting compared to multi-head approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continual learning regularization reduces catastrophic forgetting when models are sequentially trained across centers with non-IID data.
- **Mechanism**: The regularization methods penalize changes to important parameters learned in previous centers, preserving feature extractor-classifier compatibility.
- **Core assumption**: Data heterogeneity exists across centers and the single-head setting maintains feature extractor-classifier coherence.
- **Evidence anchors**:
  - [abstract]: "LWF, EBLL, IMM-mean, and IMM-mode regularization methods effectively reduce performance oscillations and overall performance drops when training on non-IID data"
  - [section]: "The catastrophic forgetting occurs. To avoid such mismatch, the single-head setting is used in our proposed ITL framework so that the feature extractor and the classifier are always optimized jointly."
  - [corpus]: Weak - corpus papers focus on federated learning generally, not specific regularization methods for domain incremental learning.
- **Break condition**: If data from different centers is IID, regularization methods show no significant benefit over naive fine-tuning.

### Mechanism 2
- **Claim**: The order of training centers matters when data is heterogeneous, with heterogeneous centers best placed at the end of the training sequence.
- **Mechanism**: Early training on heterogeneous data creates suboptimal initial models that negatively influence subsequent training, while late training allows models to first learn common features before adapting to domain-specific variations.
- **Core assumption**: Heterogeneous data centers create different feature distributions that can confuse early training.
- **Evidence anchors**:
  - [abstract]: "the order of centers matters, with heterogeneous data centers preferred at the end of the training sequence"
  - [section]: "The fundamental reason is similar to the reliance on initialization: if the center with heterogeneous data is located in the beginning or middle of the training sequence, a suboptimal/inferior model for all the test data is obtained at that center"
  - [corpus]: Missing - corpus does not discuss center ordering strategies.
- **Break condition**: If all centers have identical data distributions, center ordering has no impact on performance.

### Mechanism 3
- **Claim**: Cyclic weight transfer (CWT) outperforms single weight transfer (SWT) for multicenter collaboration by allowing models to revisit centers and refine learning.
- **Mechanism**: CWT enables the model to iteratively improve across all centers multiple times, reducing overfitting to local data and improving generalization.
- **Core assumption**: Centers can be revisited for additional training without violating privacy constraints.
- **Evidence anchors**:
  - [abstract]: "CWT is shown to be superior to SWT in overall accuracy, albeit with more frequent weight transfers"
  - [section]: "For ITL, datasets from different centers can be re-accessed via CWT and CWT has been reported to achieve better performance than SWT [13]–[15]"
  - [corpus]: Weak - corpus papers discuss federated learning but not specific weight transfer strategies.
- **Break condition**: If communication costs are prohibitive or centers cannot be revisited, SWT may be necessary despite lower accuracy.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why models lose performance on previous tasks when learning new ones is fundamental to the problem being solved
  - Quick check question: What happens to a neural network's performance on task A when it's trained on task B without any regularization?

- **Concept**: Domain adaptation vs domain incremental learning
  - Why needed here: The paper distinguishes between improving performance on new domains (adaptation) versus preserving performance across all domains (incremental learning)
  - Quick check question: In domain adaptation, is the primary goal to improve performance on source domain, target domain, or both?

- **Concept**: Multi-head vs single-head classifier settings
  - Why needed here: The choice of classifier architecture affects catastrophic forgetting and is a key design decision in the proposed framework
  - Quick check question: Why does using separate classifiers for each center lead to catastrophic forgetting in this context?

## Architecture Onboarding

- **Component map**: Data centers → Weight transfer mechanism (SWT or CWT) → Regularization methods (LWF, EBLL, IMM, EWC, SI, MAS) → Sequential training across centers → Evaluation across all centers

- **Critical path**: Data → Center 1 training → Weight transfer → Center 2 training (with regularization) → ... → Center N training → Evaluation across all centers

- **Design tradeoffs**: Single-head setting preserves feature extractor-classifier coherence but may limit center-specific optimization; multi-head setting allows optimal center-specific performance but causes catastrophic forgetting.

- **Failure signatures**: Large accuracy drops after training on certain centers indicate data heterogeneity issues; oscillations in performance curves suggest insufficient regularization or poor weight transfer frequency.

- **First 3 experiments**:
  1. Implement naive fine-tuning (FT) with single-head setting on synthetic heterogeneous data to establish baseline performance and identify forgetting patterns
  2. Add LWF regularization to the baseline and compare accuracy stability across centers
  3. Implement CWT with LWF and compare against SWT+LWF to quantify the benefit of cyclic training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do importance-based continual learning methods (EWC, SI, MAS) perform on multicenter collaboration tasks compared to other regularization methods when different hyperparameter values are used?
- **Basis in paper**: [explicit] The paper states that EWC, SI, and MAS did not outperform fine-tuning (FT) in the experiments, even when importance weights were modified to allow more freedom for important parameters.
- **Why unresolved**: The experiments tested only a limited range of λ values and a modified objective function. It's unclear if other hyperparameter configurations or modifications could improve performance.
- **What evidence would resolve it**: Comprehensive experiments testing a wider range of λ values and alternative modifications to the importance-based methods, demonstrating improved performance over FT.

### Open Question 2
- **Question**: How does the order of centers with heterogeneous data affect the performance of continual learning regularization methods in multicenter collaboration?
- **Basis in paper**: [explicit] The paper found that the order of centers matters, with centers containing heterogeneous data preferred at the end of the training sequence. However, the experiments only tested a limited number of center orderings.
- **Why unresolved**: The experiments did not exhaustively test all possible center orderings or explore the impact of different degrees of data heterogeneity on the optimal center order.
- **What evidence would resolve it**: Systematic experiments testing various center orderings with different levels of data heterogeneity, identifying the optimal center order for each continual learning regularization method.

### Open Question 3
- **Question**: How do continual learning regularization methods perform on multicenter collaboration tasks when the model is initialized with a high-accuracy model trained on a subset of the data?
- **Basis in paper**: [explicit] The paper found that using a high-accuracy initial model improved the performance of certain continual learning regularization methods (LWF, EBLL, IMM-mean, IMM-mode) compared to using a random initial model.
- **Why unresolved**: The experiments only tested the impact of a high-accuracy initial model on a limited number of continual learning regularization methods and did not explore the effect of different initial model accuracies or training strategies.
- **What evidence would resolve it**: Experiments comparing the performance of various continual learning regularization methods when initialized with high-accuracy models trained using different strategies (e.g., different data subsets, training durations), demonstrating the impact of initial model quality on subsequent performance.

## Limitations

- Experimental validation is constrained by limited dataset diversity, focusing primarily on medical imaging applications
- Computational overhead of cyclic weight transfer compared to single weight transfer is not quantified
- Center ordering strategy assumes static data distributions, which may not hold in dynamic real-world scenarios

## Confidence

- **High**: The effectiveness of LWF, EBLL, IMM-mean, and IMM-mode regularization methods in reducing performance oscillations and overall performance drops on non-IID data
- **Medium**: The superiority of CWT over SWT in overall accuracy, as this depends on specific implementation details and computational resources
- **Low**: The generalizability of center ordering strategies to dynamic data distributions and non-medical applications

## Next Checks

1. **Generalizability Test**: Validate the proposed ITL framework on non-medical datasets (e.g., CIFAR-100 or ImageNet subsets) to assess performance across different domains and data distributions.

2. **Computational Overhead Analysis**: Quantify the computational and communication costs of CWT versus SWT to determine practical feasibility in resource-constrained environments.

3. **Dynamic Data Distribution Study**: Evaluate the impact of changing data distributions across centers over time to assess the robustness of center ordering strategies in dynamic scenarios.