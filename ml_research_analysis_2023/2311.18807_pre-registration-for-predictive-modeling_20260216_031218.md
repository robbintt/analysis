---
ver: rpa2
title: Pre-registration for Predictive Modeling
arxiv_id: '2311.18807'
source_url: https://arxiv.org/abs/2311.18807
tags:
- data
- they
- pre-registration
- test
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores pre-registration as a way to address reproducibility
  issues in predictive modeling, similar to its use in explanatory modeling. A lightweight
  two-phase pre-registration template is introduced and tested with six ML researchers.
---

# Pre-registration for Predictive Modeling

## Quick Facts
- arXiv ID: 2311.18807
- Source URL: https://arxiv.org/abs/2311.18807
- Reference count: 40
- One-line primary result: Pre-registration protocol tested with six ML researchers reduces data-dependent decisions and improves research intentionality in predictive modeling

## Executive Summary
This paper introduces a lightweight two-phase pre-registration protocol for predictive modeling, adapted from practices in explanatory modeling. The protocol requires researchers to specify research questions, variables, data construction, transformations, evaluation metrics, and baselines before training (Phase A), and prediction methods, model details, test data access, and secondary analyses before testing (Phase B). A pilot study with six ML researchers found that pre-registration helped reduce data-dependent decisions and improve intentionality, though some participants struggled with committing to choices early. The protocol shows promise for preventing overfitting and test set leakage if properly adhered to.

## Method Summary
The method involves a two-phase pre-registration protocol for predictive modeling research. Phase A requires completing questions about the research question, dependent/independent variables, training/test data construction, data transformations, evaluation metrics, and baselines before any model training begins. Phase B requires declaring the prediction method, model training details (including random seeds and hyperparameters), test data access, and secondary analyses before testing begins. The protocol was tested with six ML researchers using a dataset of 12th graders' survey responses about depression-related behaviors, with 12 potential outcome variables and 24 potential predictor variables.

## Key Results
- Participants found pre-registration valuable for reducing data-dependent decisions and improving intentionality in research design
- The two-phase protocol was generally easy to use, though some struggled with committing to choices before seeing model results
- Pre-registration could prevent overfitting and test set leakage if researchers adhere to their pre-registered plans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-registration reduces data-dependent decisions that can lead to inflated performance estimates.
- Mechanism: By forcing researchers to specify the research question, variables, data transformations, and evaluation metrics before seeing modeling results, pre-registration decouples the operationalization of the problem from exploratory modeling. This reduces the risk of cherry-picking favorable outcomes after the fact.
- Core assumption: Researchers are motivated to report accurate performance rather than maximize perceived success.
- Evidence anchors:
  - [abstract]: "Participants found pre-registration valuable for reducing data-dependent decisions and improving intentionality"
  - [section 5.2.2]: Participants needed reminders to commit to choices before proceeding to training, suggesting they would otherwise have explored multiple options.
  - [corpus]: No direct evidence, but the general theme of pre-registration in ML literature supports this mechanism.
- Break condition: If researchers ignore or circumvent the pre-registration after it is completed, or if the problem specification is too vague to be actionable.

### Mechanism 2
- Claim: Pre-registration helps prevent test set leakage and multiple accesses to the test set.
- Mechanism: Phase B of the protocol requires declaring that the test data has not been accessed and specifying model details before testing. This creates a clear boundary between training/validation and testing phases, reducing the temptation to re-optimize based on test performance.
- Core assumption: Researchers are able to resist the urge to re-access test data even if initial performance is lower than expected.
- Evidence anchors:
  - [abstract]: "The protocol could prevent overfitting and test set leakage if adhered to"
  - [section 5.2.6]: Some participants described strategies that would re-access test data if performance was low, indicating this is a real risk.
  - [corpus]: The paper cites prior work on "over-hyping" and test set leakage as common problems in ML.
- Break condition: If researchers have prior knowledge of the test set or are allowed to update the pre-registration after seeing test results.

### Mechanism 3
- Claim: Pre-registration encourages more thoughtful and transparent research design.
- Mechanism: The act of explicitly documenting the research plan forces researchers to think through the implications of their choices and makes it easier for others to evaluate the validity of the study. This transparency can lead to better-designed studies overall.
- Core assumption: The process of documentation itself leads to improved reflection and design.
- Evidence anchors:
  - [abstract]: "participants found pre-registration valuable... improving intentionality"
  - [section 5.2.6]: Participants noted benefits such as more structured approaches and greater transparency.
  - [corpus]: No direct evidence, but aligns with broader literature on pre-registration in other fields.
- Break condition: If the documentation process is seen as a checkbox exercise without genuine reflection, or if the pre-registration is not made publicly available for scrutiny.

## Foundational Learning

- Concept: The distinction between exploratory and confirmatory research
  - Why needed here: Pre-registration is primarily aimed at confirmatory research, where the goal is to test a pre-specified hypothesis. Understanding this distinction helps researchers know when pre-registration is most appropriate.
  - Quick check question: If you are trying to discover new patterns in data without a specific hypothesis, is pre-registration the right tool?

- Concept: The train/validate/test paradigm and its purpose
  - Why needed here: Pre-registration is designed to work within the standard ML workflow of splitting data into training, validation, and test sets. Understanding why this paradigm exists (to prevent overfitting) is key to understanding the value of pre-registration.
  - Quick check question: What is the primary purpose of having a held-out test set in machine learning?

- Concept: Degrees of freedom in model development
  - Why needed here: Pre-registration aims to reduce the impact of researcher degrees of freedom, such as choices about feature engineering, model selection, and evaluation metrics. Recognizing these choices helps researchers identify what should be pre-specified.
  - Quick check question: Can you list three decisions in a typical ML pipeline that could introduce bias if made after seeing the data?

## Architecture Onboarding

- Component map: Phase A (problem definition) -> Model training/validation -> Phase B (model details) -> Test model -> Report results
- Critical path: Complete Phase A (problem definition) → Train and validate model → Complete Phase B (model details) → Test model → Report results including any deviations from pre-registration
- Design tradeoffs: The protocol allows for some flexibility (reporting deviations in Phase B) to avoid overly constraining researchers, but this also means it relies on researcher honesty. The two-phase structure separates problem definition from model details, but may be confusing if not clearly explained.
- Failure signatures: Researchers make choices that contradict their pre-registration without reporting them, or the pre-registration is so vague that it doesn't actually constrain the analysis. Participants struggle to complete Phase A because they lack sufficient domain knowledge upfront.
- First 3 experiments:
  1. Apply the protocol to a simple binary classification problem with a small, well-known dataset (e.g., Iris) to test ease of use and clarity of questions.
  2. Have two researchers independently pre-register and then execute the same problem to see if they produce comparable results and if the protocol constrains variability.
  3. Conduct a user study where researchers attempt to retrospectively create a pre-registration for a completed project to identify which aspects are most challenging to specify in advance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific challenges and benefits of pre-registering prediction methods (B.1) in Phase A versus Phase B, and how does this impact the validity of out-of-sample performance estimates?
- Basis in paper: [inferred] The paper discusses the debate around whether to pre-register the prediction method in Phase A (before training) or Phase B (after validation). While the authors lean towards Phase B, they acknowledge that some participants suggested moving it to Phase A.
- Why unresolved: The paper does not provide a definitive answer on the optimal timing for pre-registering prediction methods, and the impact of this decision on the validity of out-of-sample performance estimates is not explicitly explored.
- What evidence would resolve it: A controlled experiment comparing the validity of out-of-sample performance estimates when prediction methods are pre-registered in Phase A versus Phase B, or a comprehensive analysis of the potential biases introduced by pre-registering prediction methods at different stages.

### Open Question 2
- Question: How does pre-registration affect the generalizability of predictive models, especially when dealing with distribution shifts in covariates, targets, or the relationship between them?
- Basis in paper: [explicit] The paper acknowledges that pre-registration does not directly address issues of generalizability, particularly in cases of distribution shift. It mentions that the training and test data may be drawn from different distributions, making reliable statements about test set performance challenging.
- Why unresolved: The paper does not provide a clear framework for how pre-registration can be adapted to address distribution shifts or improve generalizability in predictive modeling.
- What evidence would resolve it: A study demonstrating how pre-registration can be modified to account for distribution shifts, or empirical evidence showing the impact of pre-registration on the generalizability of predictive models in the presence of distribution shifts.

### Open Question 3
- Question: What are the optimal reporting guidelines and consensus-based checklists for predictive modeling research, and how can they be integrated with pre-registration to ensure reproducibility and transparency?
- Basis in paper: [explicit] The paper mentions the existence of reporting guidelines and checklists for ML research, such as Datasheets for datasets and Model Cards. However, it does not provide a comprehensive framework for integrating these with pre-registration.
- Why unresolved: The paper does not explore the specific reporting guidelines and checklists that are most effective for predictive modeling research, nor does it provide a clear strategy for integrating them with pre-registration.
- What evidence would resolve it: A systematic review and evaluation of existing reporting guidelines and checklists for predictive modeling research, along with a proposed framework for integrating them with pre-registration.

## Limitations
- Small sample size: Only six participants in the pilot study limits generalizability of findings
- Domain specificity: Study focused on teen depression prediction from survey data, which may not represent full diversity of ML applications
- Adherence uncertainty: Protocol effectiveness depends on researchers following pre-registered plans, but study did not systematically track adherence

## Confidence
- High confidence: The observation that pre-registration helps reduce data-dependent decisions and improves research intentionality is well-supported by participant feedback in the study.
- Medium confidence: The claim that the two-phase protocol is "easy to use" is based on limited participant feedback and may not generalize to researchers with different backgrounds.
- Low confidence: The specific effectiveness of the proposed protocol in preventing overfitting and test set leakage cannot be fully evaluated without actual implementation and adherence monitoring across multiple studies.

## Next Checks
1. Replication study with larger sample: Conduct a user study with 30+ ML researchers across different domains to validate the protocol's usability and effectiveness, tracking both stated intentions and actual adherence to pre-registrations.
2. A/B testing comparison: Compare model development outcomes between researchers using the pre-registration protocol versus those using standard practices, measuring differences in performance estimation accuracy and researcher degrees of freedom.
3. Long-term adherence monitoring: Track a cohort of researchers using the protocol over 6-12 months to assess whether pre-registrations are actually followed and whether this impacts reproducibility and transparency in published results.