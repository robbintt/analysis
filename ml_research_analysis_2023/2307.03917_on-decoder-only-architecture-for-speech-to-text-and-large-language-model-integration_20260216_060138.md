---
ver: rpa2
title: On decoder-only architecture for speech-to-text and large language model integration
arxiv_id: '2307.03917'
source_url: https://arxiv.org/abs/2307.03917
tags:
- speech
- arxiv
- language
- audio
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of integrating speech signals
  into large language models (LLMs) for improved human-computer interaction. The authors
  propose a novel approach called Speech-LLaMA that effectively incorporates acoustic
  information into text-based LLMs using a decoder-only architecture.
---

# On decoder-only architecture for speech-to-text and large language model integration

## Quick Facts
- arXiv ID: 2307.03917
- Source URL: https://arxiv.org/abs/2307.03917
- Authors: 
- Reference count: 0
- Speech-LLaMA achieves BLEU score gains up to 5.6 points over strong baselines

## Executive Summary
This paper introduces Speech-LLaMA, a novel approach for integrating speech signals into large language models (LLMs) using a decoder-only architecture. The method leverages Connectionist Temporal Classification (CTC) for sequence length reduction and a simple audio encoder to map compressed acoustic features to the continuous semantic space of the LLM. Experimental results on multilingual speech-to-text translation tasks demonstrate significant improvements over strong baselines, with a smaller decoder-only model trained from scratch achieving comparable performance to a larger encoder-decoder baseline.

## Method Summary
The proposed method involves a pre-trained text neural LM (LLaMA-7B), an audio encoder, and a CTC compressor. The audio encoder maps the compressed speech signal to the continuous semantic space of the LLM, while the CTC compressor reduces the sequence length of the acoustic features. The model is trained using cross-entropy loss between the LM output and the reference text. LoRA fine-tuning is applied to adapt the LLM for the new task. The architecture uses 80-dim log mel-filterbank features extracted with 25ms window and 10ms hop size as input.

## Key Results
- BLEU score gains up to 5.6 points over strong baselines on multilingual speech-to-text translation
- Smaller decoder-only model trained from scratch achieves comparable performance to larger encoder-decoder baseline with 40% fewer parameters
- Frame-averaging compression method shows 1.5 better average BLEU score compared to blank-removing method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CTC compressor effectively aligns speech duration with text by compressing redundant frames, enabling decoder-only LLM to consume speech signals at compatible length
- Mechanism: The CTC compressor reduces sequence length via blank removal or frame averaging while preserving semantic content. It maps acoustic frames to discrete labels and removes consecutive frames belonging to same label (blank-removing) or averages them (frame-averaging), achieving 4x compression while maintaining semantic alignment with text tokens
- Core assumption: The compressed acoustic representation maintains sufficient information for accurate speech-to-text conversion when integrated into LLM semantic space
- Evidence anchors:
  - [abstract]: "Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM"
  - [section]: "The CTC compressor reduces the sequence length of the input speech filter-bank to match the length of the text"
  - [corpus]: Weak evidence - no direct citations to CTC compressor performance in this context
- Break condition: If compression rate exceeds semantic preservation threshold, resulting in information loss that degrades transcription quality

### Mechanism 2
- Claim: Direct mapping of compressed acoustic features to LLM semantic space enables deeper integration than discrete token approaches
- Mechanism: Audio encoder transforms compressed speech representations into continuous vectors in the same semantic space as LLM text embeddings, allowing joint conditioning on both modalities during autoregressive generation
- Core assumption: Continuous semantic space alignment between speech and text modalities enables effective cross-modal learning without requiring intermediate discretization steps
- Evidence anchors:
  - [abstract]: "our model directly maps the continuous representation of speech into the semantic space defined by the LM"
  - [section]: "the proposed audio encoder is directly optimized to map the compressed acoustic signal to the continuous semantic space of LM, allowing a deep integration between the audio encoder and the language model"
  - [corpus]: No direct corpus evidence for continuous vs discrete integration benefits
- Break condition: If semantic alignment fails, the LLM cannot effectively condition on acoustic information, resulting in degraded performance

### Mechanism 3
- Claim: LoRA fine-tuning with minimal additional parameters enables effective adaptation of frozen LLM to speech-to-text tasks
- Mechanism: Low-rank adaptation introduces small adapter matrices to attention layers while keeping original LLM parameters frozen, allowing efficient task-specific fine-tuning without full parameter updates
- Core assumption: Low-rank decomposition can capture task-specific adaptations needed for speech-to-text while maintaining general language capabilities
- Evidence anchors:
  - [section]: "we apply the LoRA to four attention matrices in each layer of the LLaMA Transformer... The entire system is trained with cross entropy loss between the LM output and the reference text"
  - [section]: "we use rank 2 for LoRA fine-tuning, i.e., 8 rank 2 matrices in shape of 2 × 4096 are introduced to each LLaMA Transformer layer as adaptor, which results in 2.1M more parameters"
  - [corpus]: Weak evidence - no corpus citations for LoRA effectiveness in speech-to-text specifically
- Break condition: If rank is too low, adaptation capacity is insufficient; if too high, efficiency gains diminish

## Foundational Learning

- CTC (Connectionist Temporal Classification):
  - Why needed here: CTC provides the mechanism for duration compression and frame alignment between speech and text sequences, addressing the fundamental length mismatch problem
  - Quick check question: How does CTC handle variable-length speech sequences when aligning with fixed-length text tokens?

- Transformer attention mechanisms:
  - Why needed here: Understanding self-attention and cross-attention is crucial for grasping how the audio encoder representations are integrated with LLM processing through attention masks and weighted combinations
  - Quick check question: What is the difference between causal and non-causal attention masks in the context of speech-to-text processing?

- Low-rank matrix decomposition:
  - Why needed here: LoRA relies on low-rank approximation to efficiently adapt large models, requiring understanding of how matrix factorization enables parameter-efficient fine-tuning
  - Quick check question: Why does LoRA use rank-2 matrices instead of full-rank adaptation matrices?

## Architecture Onboarding

- Component map:
  Speech features (80-dim log mel-filterbank) -> CTC Compressor (2 Conv2D + 4 Transformer layers) -> Compressed representations -> Audio Encoder (4 Transformer layers) -> Semantic space vectors -> LLM (LLaMA-7B) + LoRA adapters -> Text output

- Critical path:
  1. Speech features → CTC compressor → compressed representations
  2. Compressed representations → audio encoder → semantic space vectors
  3. Text prompt embeddings + audio encoder outputs → LLM
  4. LLM autoregressive decoding → target text

- Design tradeoffs:
  - Frozen vs fine-tuned components: Freezing LLaMA preserves general language capabilities but may limit speech-specific adaptation
  - Compression method choice: Blank-removing preserves more frames but may retain redundancy; frame-averaging reduces information but improves efficiency
  - Attention mask strategy: Causal masks maintain autoregressive property but may limit acoustic context utilization

- Failure signatures:
  - Poor BLEU scores with specific languages suggest acoustic feature misalignment or insufficient compression quality
  - Training instability indicates LoRA rank selection issues or learning rate problems
  - Length mismatch errors point to CTC compressor configuration problems

- First 3 experiments:
  1. Validate CTC compressor: Test compression quality by comparing original vs compressed sequence lengths and conducting ablation with/without compression
  2. Audio encoder initialization: Train audio encoder with frozen LLaMA and CTC compressor, measuring semantic space alignment through similarity metrics
  3. LoRA fine-tuning stability: Test different LoRA ranks (1, 2, 4) while monitoring training loss and validation performance to find optimal balance between adaptation capacity and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed decoder-only architecture compare to the encoder-decoder architecture when both are trained from scratch on speech-to-text tasks?
- Basis in paper: [explicit] The paper mentions that the decoder-only model trained from scratch achieves comparable performance to the seq2seq baseline with around 40% less parameters.
- Why unresolved: The paper does not provide a detailed comparison of the two architectures in terms of training efficiency, parameter efficiency, and overall performance across various speech-to-text tasks.
- What evidence would resolve it: A comprehensive study comparing the decoder-only and encoder-decoder architectures on multiple speech-to-text tasks, including training time, parameter efficiency, and performance metrics such as BLEU scores and word error rates.

### Open Question 2
- Question: How does the choice of audio length compression method (blank-removing vs. frame-averaging) affect the performance of the Speech-LLaMA model?
- Basis in paper: [explicit] The paper mentions that the frame-averaging method shows 1.5 better average BLEU score over the blank-removing method, but does not provide a detailed analysis of the reasons behind this difference.
- Why unresolved: The paper does not explore the impact of different audio length compression methods on the model's performance in depth, leaving the underlying reasons for the observed differences unclear.
- What evidence would resolve it: A detailed analysis of the impact of different audio length compression methods on the model's performance, including a study of the information loss and robustness to compression errors for each method.

### Open Question 3
- Question: How does the use of source language transcriptions during the training of the CTC compressor affect the performance of the Speech-LLaMA model?
- Basis in paper: [inferred] The paper mentions that the CTC compressor leverages the transcription of each source language during pre-training, but does not explore the impact of using source transcriptions during the training of the entire model.
- Why unresolved: The paper does not investigate the potential benefits of incorporating source language transcriptions during the training of the Speech-LLaMA model, leaving the question of whether this could lead to improved performance unanswered.
- What evidence would resolve it: An experimental study comparing the performance of the Speech-LLaMA model with and without the use of source language transcriptions during training, including an analysis of the impact on BLEU scores and other relevant metrics.

## Limitations
- Evaluation limited to multilingual speech-to-text translation without testing on monolingual ASR tasks or other speech-related applications
- Architecture specifications contain gaps that prevent direct reproduction (exact LoRA implementation details, attention mechanism configurations)
- Claims about efficiency advantages lack computational complexity analysis and training time comparisons

## Confidence
- High Confidence: The fundamental approach of using CTC compression to align speech and text sequence lengths is well-established and experimental results show consistent improvements
- Medium Confidence: Specific architectural choices (audio encoder dimensions, LoRA rank selection) are described but lack comparative analysis or sensitivity studies
- Low Confidence: Claims about efficiency advantages versus encoder-decoder architectures are not supported by computational complexity analysis or training time comparisons

## Next Checks
1. **Ablation study on compression methods**: Compare blank-removing versus frame-averaging compression approaches with varying compression ratios (2x, 4x, 8x) to determine optimal balance between sequence length reduction and semantic preservation. Measure both BLEU scores and semantic similarity metrics.

2. **Cross-task generalization validation**: Test the trained models on non-translation speech tasks including monolingual ASR, speech summarization, and speech-based question answering to evaluate the claimed flexibility of the decoder-only architecture beyond the translation domain.

3. **Architecture specification verification**: Implement and compare multiple LoRA configurations (ranks 1, 2, 4, 8) while measuring both performance and parameter efficiency. Include computational complexity analysis comparing training time and inference latency against encoder-decoder baselines.