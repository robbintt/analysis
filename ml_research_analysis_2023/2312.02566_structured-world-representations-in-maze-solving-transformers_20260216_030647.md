---
ver: rpa2
title: Structured World Representations in Maze-Solving Transformers
arxiv_id: '2312.02566'
source_url: https://arxiv.org/abs/2312.02566
tags:
- path
- layer
- mazes
- token
- maze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal representations of small transformer
  models trained to solve mazes. The authors use a variety of interpretability techniques
  to study these models, finding evidence for the emergence of structured internal
  representations of maze topology and valid paths.
---

# Structured World Representations in Maze-Solving Transformers

## Quick Facts
- **arXiv ID:** 2312.02566
- **Source URL:** https://arxiv.org/abs/2312.02566
- **Reference count:** 40
- **One-line primary result:** Small transformers trained on maze-solving tasks develop structured internal representations that can be linearly decoded to reconstruct maze topology

## Executive Summary
This paper investigates how small transformer models learn to solve mazes by developing structured internal representations. Using interpretability techniques including linear probing, attention head analysis, and Tuned Lens, the authors demonstrate that residual streams capture maze topology, embeddings develop spatial structure, and specific attention heads facilitate path-following behavior. The work provides insights into how transformers represent and reason about structured environments.

## Method Summary
The authors train small decoder-only transformer models (hallway: 1.2M params, jirpy: 9.6M params) on autoregressive next-token prediction for maze-solving tasks. Mazes are generated using randomized depth-first search (RDFS), forkless, and percolation-based algorithms via the maze-dataset library. The models are analyzed using linear probes on residual streams to reconstruct maze connectivity, direct logit attribution to identify attention heads, and Tuned Lens to analyze neighbor information encoding.

## Key Results
- Residual stream of a single token can be linearly decoded to faithfully reconstruct the entire maze
- Learned embeddings of coordinate tokens develop spatial structure correlating with maze geometry
- Specific attention heads (adjacency heads) consistently attend to tokens at path length 1 from current position
- Probe accuracy correlates with model performance and grokking-like transitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual stream of a single token can linearly decode entire maze structure
- Mechanism: Early layers form structured internal representations where maze connectivity is linearly decodable via probes
- Core assumption: Maze topology is captured in linear combinations of residual stream activations
- Evidence anchors:
  - [abstract] "We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze."
  - [section 3.4] "For a maze with m × m positions, we train nlayers × m × m × 4 probes p on residual stream activations Rl(t) collected across many rollouts"
  - [corpus] Weak evidence - only general maze-solving transformer studies found
- Break condition: If probe accuracy drops below ~90% or representations shift to nonlinear

### Mechanism 2
- Claim: Adjacency heads learn to respect maze topology by attending to neighboring coordinates
- Mechanism: Specific attention heads attend to tokens at path length 1 from current position, enabling valid path-following
- Core assumption: Attention patterns can be isolated and interpreted as maze-navigation circuitry
- Evidence anchors:
  - [abstract] "Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed adjacency heads)"
  - [section 3.3] "we find that Layer 5, Head 0 simply places attention on the recent occurrences of the current coordinate token" and "Layer 5, Head 3, which we term an Adjacency Head, consistently attends to tokens of path length 1"
  - [corpus] Weak evidence - adjacency head concept specific to this work
- Break condition: If attention patterns shift to non-topology-respecting behavior

### Mechanism 3
- Claim: Embedding space develops spatial structure correlating with maze geometry
- Mechanism: Learned embeddings of coordinate tokens develop spatial relationships mirroring actual maze coordinates
- Core assumption: Training can induce geometric structure in otherwise orthogonal embeddings
- Evidence anchors:
  - [abstract] "We also find that the learned embeddings of individual tokens have spatial structure"
  - [section 3.2] "we note that a correlation between the coordinate distance and distance between embedding vectors emerges for short distances"
  - [corpus] Weak evidence - similar findings in Othello world models but maze-specific details absent
- Break condition: If embedding correlations disappear or become random

## Foundational Learning

- Concept: Linear probing for interpretability
  - Why needed here: Enables quantification of what information is encoded in model representations
  - Quick check question: If a probe achieves 95% accuracy on wall detection, what does this tell us about the model's internal representation?

- Concept: Attention head analysis
  - Why needed here: Identifies specific components responsible for path-following behavior
  - Quick check question: What distinguishes an adjacency head from other attention heads in this model?

- Concept: Tuned Lens methodology
  - Why needed here: Provides interpretable view of intermediate layer representations
  - Quick check question: How does applying a learned translator to a layer's output help us understand what that layer computes?

## Architecture Onboarding

- Component map: Token → Embedding → Layer 1-2 (maze representation) → Adjacency heads (Layer 5) → Path following → Output logits
- Critical path: Token → Embedding → Layer 1-2 (maze representation) → Adjacency heads (Layer 5) → Path following → Output logits
- Design tradeoffs: Smaller models (hallway) offer interpretability but limited maze complexity; larger models (jirpy) handle complex mazes but harder to analyze
- Failure signatures: Invalid paths (wall jumps), target not reached despite reaching goal, poor performance on first_path_choice task
- First 3 experiments:
  1. Run linear probes on <PATH_START> token across all layers and measure wall detection accuracy
  2. Visualize attention patterns of Layer 5 heads on path-following tasks
  3. Apply Tuned Lens to compare neighbor probability distributions across layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do adjacency heads (like Layer 5, Head 3) causally contribute to maze-solving performance?
- Basis in paper: [inferred] The paper identifies adjacency heads through DLA and hypothesizes they respect maze topology, but does not perform ablation studies to confirm causality.
- Why unresolved: The analysis only correlates head behavior with performance, not proving direct causation.
- What evidence would resolve it: Ablation experiments showing degraded performance when adjacency heads are removed or modified.

### Open Question 2
- Question: What is the precise relationship between linear probe accuracy and model generalization?
- Basis in paper: [explicit] The paper observes that improved probe accuracy correlates with grokking-like transitions, but this is presented as suggestive rather than causal.
- Why unresolved: The analysis is correlational; the paper does not manipulate representations to test if improved decoding causes better generalization.
- What evidence would resolve it: Experiments showing that improving linear probe accuracy (e.g., through training or intervention) leads to improved maze-solving generalization.

### Open Question 3
- Question: Are the emergent spatial structures in embedding space universal across different maze representations?
- Basis in paper: [inferred] The paper finds spatial structure in embeddings but only for one tokenization scheme and maze generation method.
- Why unresolved: The analysis is limited to specific maze datasets and tokenization; generalization to other representations is untested.
- What evidence would resolve it: Replicating the embedding analysis on mazes represented with different tokenizations or input formats (e.g., images or graphs).

## Limitations
- Findings may not generalize to more complex or stochastic environments with less predictable structure
- Linear probe methodology assumes useful information can be linearly extracted from representations
- Study doesn't address potential distribution shifts that could affect models' performance in unseen maze types

## Confidence
- **High Confidence Claims:** Residual stream contains decodable maze topology information; Adjacency heads exist and show path-following behavior; Embedding space develops spatial correlations
- **Medium Confidence Claims:** Specific layers consistently capture maze structure; Adjacency heads are primary mechanism for path-following; Learned embeddings reflect maze geometry
- **Low Confidence Claims:** These mechanisms generalize to larger, more complex environments; Specific attention patterns are optimal for maze-solving; Findings apply to other structured world tasks beyond mazes

## Next Checks
1. Test linear decodability and attention patterns on mazes with increased complexity (larger sizes, more branching, loops) to establish whether observed structures scale with problem difficulty

2. Apply same interpretability methodology to transformer models trained on other structured domains (e.g., Sokoban, Othello, or graph navigation) to determine if similar representation patterns emerge across different types of structured worlds

3. Perform targeted ablation of identified adjacency heads and measure impact on path-following performance to establish whether these heads are truly necessary for observed behavior rather than merely correlated with it