---
ver: rpa2
title: 2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level
  Supervision
arxiv_id: '2310.12817'
source_url: https://arxiv.org/abs/2310.12817
tags:
- point
- segmentation
- cloud
- features
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multimodal Interlaced Transformer (MIT)
  for weakly supervised point cloud segmentation, using only scene-level class tags.
  Unlike previous methods requiring additional annotations (e.g., sparse labels, image-level
  tags), MIT integrates 2D and 3D information through a transformer with two encoders
  and a decoder.
---

# 2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision

## Quick Facts
- arXiv ID: 2310.12817
- Source URL: https://arxiv.org/abs/2310.12817
- Reference count: 40
- Primary result: MIT achieves 31.7% mIoU on ScanNet and 27.7% mIoU on S3DIS using only scene-level class tags

## Executive Summary
This paper introduces the Multimodal Interlaced Transformer (MIT) for weakly supervised point cloud segmentation, achieving strong performance using only scene-level class tags without requiring additional annotations like sparse labels or image-level tags. MIT integrates 2D and 3D information through a transformer architecture with two encoders and a decoder that alternates between 3D and 2D cross-attention layers. The method implicitly fuses features without requiring camera poses or depth maps, instead relying on a contrastive loss to align class tokens across modalities. MIT outperforms existing weakly supervised methods on both ScanNet and S3DIS benchmarks while demonstrating robustness across different backbone networks.

## Method Summary
MIT uses a transformer architecture with separate 2D and 3D encoders that process multi-view images and point clouds respectively, each with prepended class tokens for scene-level supervision. The decoder implements interlaced cross-attention where 3D voxel tokens attend to 2D view tokens in odd layers, and vice versa in even layers, creating a bidirectional feature fusion loop. Each 2D view is aggregated into a single token via global average pooling to maintain computational efficiency. A contrastive loss aligns class tokens across modalities, while multi-label classification losses train both encoders. The method achieves segmentation through class-aware layers that map supervoxel features to class scores, all without requiring camera poses, depth maps, or dense annotations.

## Key Results
- Achieves 31.7% mIoU on ScanNet and 27.7% mIoU on S3DIS using only scene-level class tags
- Outperforms existing weakly supervised methods that require additional annotations like sparse labels
- Demonstrates generalization to other weak supervision settings and robustness across different backbone networks (PointNet++, MinkowskiNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The interlaced cross-attention between 3D and 2D modalities iteratively enriches both feature representations without requiring explicit camera poses or depth maps.
- Mechanism: In odd layers, 3D voxel tokens serve as queries and attend to 2D view tokens (acting as key-value pairs), enriching 3D geometry with semantic texture information. In even layers, the roles switch: 2D view tokens attend to 3D voxel tokens, augmenting 2D features with geometric context. This alternating pattern creates a bidirectional feature fusion loop.
- Core assumption: Implicit 2D-3D correspondences can be learned through attention mechanisms alone, without explicit geometric alignment.
- Evidence anchors:
  - [abstract]: "The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion."
  - [section 3.3]: "In odd layers of the decoder, 3D voxels are enriched by 2D image features, while in even layers, 2D views are augmented by 3D geometric features."
  - [corpus]: Weak - no direct citation, but the approach is novel in this space.
- Break condition: If the attention patterns converge to degenerate solutions where one modality dominates the other, or if the semantic similarity between 3D and 2D tokens becomes too weak to establish meaningful cross-attention.

### Mechanism 2
- Claim: Multi-class tokens prepended to both 3D and 2D encoders enable scene-level supervision to guide per-point segmentation through contrastive alignment.
- Mechanism: Class tokens are prepended to the token sequence in both encoders, and their attended features are used for multi-label classification with scene-level tags. A contrastive loss aligns these class tokens across modalities, ensuring semantic consistency between 3D geometry and 2D texture features.
- Core assumption: Scene-level tags can effectively supervise per-point predictions when class tokens are semantically aligned across modalities.
- Evidence anchors:
  - [abstract]: "The two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively... multi-class tokens [59] are included to match the class-level annotations."
  - [section 3.2]: "We prepend C learnable class tokens [59] with S supervoxel tokens... the dependencies of the class and supervoxel tokens are captured, producing the self-attended 3D features F3D."
  - [corpus]: Weak - the concept of multi-class tokens is referenced but not deeply explored in the corpus.
- Break condition: If the contrastive loss fails to properly align class tokens, or if the class tokens become overly generic and lose discriminative power for segmentation.

### Mechanism 3
- Claim: The global average pooling of 2D features into single tokens per view enables scalable cross-attention without exploding computational cost.
- Mechanism: Instead of treating each 2D pixel as a token, each view image is pooled into a single token, making N2D = C + T (class tokens plus T views) rather than the full spatial resolution. This keeps the cross-attention complexity linear in the number of 2D tokens.
- Core assumption: Aggregated view-level features retain sufficient discriminative information for effective 2D-3D fusion.
- Evidence anchors:
  - [section 3.2]: "We apply global average pooling to image features s2D along the spatial dimensions... producing T view tokens."
  - [section 4.3.2]: "Since we cast each 2D view into a token via global average pooling, N2D = C + T, where C is the number of classes and T is the number of 2D views."
  - [corpus]: Weak - this specific design choice isn't directly supported in the corpus.
- Break condition: If global pooling loses too much spatial information critical for distinguishing objects, or if the number of views T is insufficient to capture scene diversity.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture with self-attention and cross-attention
  - Why needed here: The paper relies on transformer encoders to extract modality-specific features and a decoder with interlaced cross-attention to fuse 2D and 3D features. Understanding how attention mechanisms work is fundamental to grasping the method.
  - Quick check question: What is the difference between self-attention in the encoder and cross-attention in the decoder?

- Concept: Weakly supervised learning with scene-level labels
  - Why needed here: The method only uses scene-level class tags (no per-point or per-pixel annotations), which is the core challenge being addressed. Understanding how scene-level supervision can guide segmentation is essential.
  - Quick check question: How does the model learn per-point segmentation from only scene-level class tags?

- Concept: Multi-view geometry and feature aggregation
  - Why needed here: The method uses multi-view 2D images to enhance 3D point cloud segmentation. Understanding how 2D and 3D features can be complementary is key to the paper's contribution.
  - Quick check question: Why might 2D texture features be useful for 3D point cloud segmentation?

## Architecture Onboarding

- Component map: 2D Backbone (ResNet-50) -> 2D Encoder -> Interlaced Decoder <- 3D Encoder <- 3D Backbone (MinkowskiNet/PointNet++)
- Critical path: 2D Backbone → 2D Encoder → Interlaced Decoder ← 3D Encoder ← 3D Backbone
- Design tradeoffs:
  - Global average pooling vs. patch tokens: Global pooling reduces computation but may lose spatial detail
  - Number of views T: More views improve coverage but increase computation
  - Number of interlaced blocks R: More blocks allow deeper fusion but may overfit
- Failure signatures:
  - Poor segmentation: Cross-attention failed to establish meaningful correspondences
  - Degraded 2D classification: 3D features overwhelmed 2D semantic information
  - Slow convergence: Contrastive loss too strong or class tokens not properly initialized
- First 3 experiments:
  1. Run with 3D-only (no 2D views) to establish baseline performance
  2. Add 2D views with global average pooling and observe segmentation improvement
  3. Increase number of views from 4 to 32 and measure mIoU changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MIT scale with the number of views in terms of both accuracy and computational cost?
- Basis in paper: [explicit] The paper states that "we can achieve good results by giving T = 16 views" and provides an ablation study showing mIoU performance with different numbers of views (4, 16, 32, 64).
- Why unresolved: The paper only tests up to 64 views, and doesn't explore the trade-off between accuracy gains and computational cost beyond this point.
- What evidence would resolve it: Experiments testing performance and computational cost with view numbers ranging from 16 to several hundred would clarify the scaling relationship and identify an optimal number of views.

### Open Question 2
- Question: How does MIT perform on real-world datasets with occlusions and varying lighting conditions compared to synthetic datasets?
- Basis in paper: [inferred] The paper mentions that multi-view aggregation can handle occlusion and view quality differences, but doesn't explicitly test on datasets with significant occlusion or lighting variation.
- Why unresolved: The paper focuses on ScanNet and S3DIS datasets which may not fully represent the challenges of real-world deployment with occlusions and varying lighting.
- What evidence would resolve it: Testing MIT on datasets like SemanticKITTI or nuScenes, which contain challenging real-world conditions, would demonstrate its robustness to occlusion and lighting variations.

### Open Question 3
- Question: Can MIT be extended to handle dynamic scenes where objects move between different views?
- Basis in paper: [inferred] The paper doesn't address temporal consistency or object tracking across views, focusing only on static scene segmentation.
- Why unresolved: The current architecture assumes static scenes and doesn't incorporate any temporal information or tracking mechanisms.
- What evidence would resolve it: Implementing temporal consistency constraints or integrating object tracking capabilities into MIT and evaluating performance on dynamic scene datasets would demonstrate whether the method can be extended to handle moving objects.

## Limitations
- Performance relies heavily on implicit 2D-3D correspondence learning without explicit geometric alignment, which may fail in challenging scenarios
- Limited ablation studies on the impact of using camera poses or depth maps compared to the implicit alignment approach
- Claims about generalization to other weak supervision settings are based on minimal experimentation beyond scene-level tags

## Confidence
- **High Confidence**: The architectural design of the interlaced decoder and the use of multi-class tokens for scene-level supervision are clearly specified and reproducible. The reported performance gains over existing weakly supervised methods are supported by quantitative results.
- **Medium Confidence**: The claim that the method generalizes well to other weak supervision settings (e.g., sparse labels) is based on limited experimentation. The robustness across different backbone networks is demonstrated but with minimal hyperparameter tuning.
- **Low Confidence**: The assertion that implicit 2D-3D correspondences can be learned through attention alone, without geometric priors, is the most speculative claim. The paper lacks comparative analysis against methods that use camera poses or depth maps.

## Next Checks
1. **Ablation Study on Geometric Alignment**: Compare MIT's performance when using camera poses and depth maps versus the implicit alignment approach. Measure the degradation in mIoU to quantify the cost of avoiding explicit geometric alignment.

2. **Cross-Dataset Generalization**: Evaluate MIT on a dataset with different characteristics (e.g., outdoor scenes or synthetic data) to test the robustness of the 2D-3D feature fusion mechanism beyond indoor ScanNet and S3DIS environments.

3. **Attention Pattern Analysis**: Visualize the cross-attention weights in the interlaced decoder to verify that meaningful semantic correspondences are being established between 2D and 3D features. Check for degenerate patterns where attention collapses to trivial solutions.