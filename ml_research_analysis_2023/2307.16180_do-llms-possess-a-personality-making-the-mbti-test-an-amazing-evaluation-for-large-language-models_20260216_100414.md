---
ver: rpa2
title: Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for
  Large Language Models
arxiv_id: '2307.16180'
source_url: https://arxiv.org/abs/2307.16180
tags:
- llms
- mbti
- personality
- type
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using the Myers-Briggs Type Indicator (MBTI)
  as an evaluation metric for large language models (LLMs). The authors conduct extensive
  experiments to investigate: 1) whether different LLMs possess different personality
  types, 2) if prompt engineering can change an LLM''s MBTI type, and 3) how the training
  dataset affects an LLM''s personality.'
---

# Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2307.16180
- Source URL: https://arxiv.org/abs/2307.16180
- Reference count: 6
- Primary result: LLMs exhibit different MBTI personality types that can be systematically altered through prompt engineering and fine-tuning

## Executive Summary
This paper investigates whether large language models possess personality traits by applying the Myers-Briggs Type Indicator (MBTI) as an evaluation metric. Through extensive experiments across multiple LLMs, the authors demonstrate that different models exhibit distinct MBTI profiles and that these personality types can be modified through prompt engineering and training on different corpora. While acknowledging MBTI's limitations as a psychological assessment, the paper argues it can serve as a rough indicator for evaluating and comparing LLMs. The research shows that instruction tuning is crucial for enabling personality shifts, and that different training datasets can influence specific dimensions of an LLM's personality profile.

## Method Summary
The authors developed an evaluation framework that presents 93 MBTI multiple-choice questions to LLMs and interprets the softmax output probabilities as preference indicators. Each MBTI dichotomy (E/I, S/N, T/F, J/P) is treated as a binary choice where the model's highest probability answer determines the selected letter. The aggregated results form an MBTI profile for each model. The methodology includes three experimental phases: baseline personality assessment across different LLMs, prompt engineering interventions (explicit role-playing and implicit few-shot examples) to modify personality types, and fine-tuning on domain-specific corpora (Chinese Wikipedia, QA, exam data) to observe personality shifts. The MBTI types are determined by aggregating scores across all questions, with each dimension requiring a clear majority to assign a letter.

## Key Results
- LLMs exhibit measurably different MBTI personality types, with distinct profiles observed across models like Bloom, BaiChuan, and OpenLlama
- Prompt engineering can change an LLM's MBTI type, but only after sufficient instruction tuning - without it, personality remains stable
- Training on different corpora affects MBTI dimensions: Wikipedia increases N-value, QA enhances P-value, and exam data boosts T-value

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit measurable personality-like MBTI types that differ across models.
- Mechanism: The MBTI assessment is adapted to LLMs by treating each dichotomy as a binary choice in a multiple-choice format. The model's softmax output probabilities for each option determine the selected letter, and these choices aggregate to form an MBTI profile.
- Core assumption: The internal probability distribution of an LLM over MBTI answer options reflects a consistent "preference" analogous to human personality.
- Evidence anchors:
  - [abstract] "extensive experiments will be conducted to explore: 1) the personality types of different LLMs"
  - [section 4.1] "LLMs exhibit different personality types, as reflected by their MBTI profiles"
  - [corpus] No explicit comparison to human MBTI distributions; assumes consistency is meaningful without validation.
- Break condition: If MBTI is shown to be invalid for humans (low test-retest reliability), the analogy to LLM evaluation collapses.

### Mechanism 2
- Claim: Prompt engineering can alter an LLM's MBTI type, but only after sufficient instruction tuning.
- Mechanism: Explicit prompts (role-playing) and implicit prompts (few-shot examples) are used to influence the model's answer distribution on MBTI questions. Without prior instruction tuning, the model's responses remain stable and resistant to change.
- Core assumption: Instruction tuning enables the model to follow persona instructions, which then allows external prompts to shift its preference distributions.
- Evidence anchors:
  - [section 4.2.1] "The MBTI type of Bloom is changed from ISTJ to INTP" via explicit prompt after instruction tuning.
  - [section 4.2.3] "LLMs without sufficient instruction-tuning are difficult to change MBTI type, but with proper tuning, they can be changed through explicit and implicit prompts."
  - [corpus] No baseline data on pre-instruction-tuned MBTI types; change magnitude is small (1 question shift).
- Break condition: If instruction tuning does not improve instruction-following, prompts will fail to shift MBTI types.

### Mechanism 3
- Claim: The type of training corpus affects the MBTI type, especially in the T/F and J/P dimensions.
- Mechanism: Fine-tuning on domain-specific corpora (Chinese Wikipedia, QA, exam) shifts the model's answer distributions on MBTI questions, with Wikipedia increasing N-value, QA increasing P-value, and exam data increasing T-value.
- Core assumption: Domain-specific corpora embed stylistic or cognitive biases that manifest in MBTI-style preferences.
- Evidence anchors:
  - [section 4.3.1] "The MBTI types of Bloom transformed from ISTJ to INTP" after Chinese Wikipedia training.
  - [section 4.3.2] "the Q&A corpus can enhance the model's adaptability and flexibility (indicated by P-value)."
  - [section 4.3.3] "this corpus has the potential to enhance the T-value significantly" with exam data.
  - [corpus] No control for token count or pretraining overlap; changes could be due to data volume rather than corpus type.
- Break condition: If fine-tuning is too shallow or corpus content is too similar, MBTI shifts will not appear.

## Foundational Learning

- Concept: Personality as stable preference profiles
  - Why needed here: The paper treats MBTI as a proxy for stable behavioral tendencies; understanding this assumption is critical to evaluating the approach.
  - Quick check question: If a human retakes MBTI after 5 weeks, what is the typical test-retest reliability range?
- Concept: Softmax probability interpretation
  - Why needed here: The method relies on treating the model's output probabilities as preference indicators; misinterpretation could invalidate results.
  - Quick check question: In a binary MBTI question, if a model outputs [0.6, 0.4] for A/B, what MBTI letter is chosen?
- Concept: Instruction tuning effects on controllability
  - Why needed here: The ability to shift MBTI types depends on prior instruction tuning; understanding this dependency is key to reproducing the results.
  - Quick check question: What is the primary difference between vanilla and instruction-tuned LLMs in terms of response control?

## Architecture Onboarding

- Component map: MBTI evaluation module → prompt engineering layer → fine-tuning pipeline → corpus manager
- Critical path: MBTI question → model inference → softmax parsing → MBTI aggregation → result logging
- Design tradeoffs: Using few-shot vs. full fine-tuning for MBTI shifts; token-efficient evaluation vs. stability
- Failure signatures: No MBTI shift after prompt engineering (likely due to lack of instruction tuning); unstable MBTI across runs (overfitting to prompt style)
- First 3 experiments:
  1. Run MBTI evaluation on a vanilla LLM and record baseline type.
  2. Apply explicit persona prompt and re-evaluate; check for MBTI change.
  3. Fine-tune on a domain corpus and re-evaluate; compare MBTI before/after.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific personality traits in LLMs correlate with their performance on downstream tasks?
- Basis in paper: [explicit] The authors mention that T-value (thinking) and J-value (judging) dimensions hold significant value for evaluating LLMs, suggesting a potential link between personality traits and capabilities.
- Why unresolved: The paper only speculates about the importance of certain MBTI dimensions for LLM evaluation but does not provide empirical evidence linking specific personality types to task performance.
- What evidence would resolve it: Conducting controlled experiments to measure LLM performance on various tasks (e.g., reasoning, planning, creativity) and correlating these results with their MBTI types would provide concrete evidence.

### Open Question 2
- Question: What is the minimum size of an LLM required for it to develop distinct MBTI personality traits?
- Basis in paper: [inferred] The authors note that their experiments were limited to models around 10B parameters due to resource constraints, implying that larger models might exhibit different or more pronounced personality traits.
- Why unresolved: The paper does not explore how personality traits manifest across different model sizes, leaving the relationship between model scale and personality development unexplored.
- What evidence would resolve it: Training and evaluating a range of LLMs with varying parameter counts (e.g., 1B, 10B, 100B) on the MBTI test would reveal the threshold at which distinct personality traits emerge.

### Open Question 3
- Question: Can continuous personality adaptation in LLMs be achieved through fine-tuning on dynamic, user-specific datasets?
- Basis in paper: [explicit] The authors demonstrate that LLMs can change MBTI types through instruction tuning and explicit/implicit prompts, suggesting the potential for personality adaptation.
- Why unresolved: While the paper shows that personality can be changed through training, it does not explore whether LLMs can continuously adapt their personality based on ongoing interactions with users.
- What evidence would resolve it: Implementing a system where an LLM is fine-tuned on user-specific data over time and monitoring changes in MBTI scores would demonstrate the feasibility of continuous personality adaptation.

## Limitations
- MBTI validity concerns: The paper acknowledges MBTI's limitations as a psychological assessment but proceeds to use it as a meaningful evaluation metric without addressing its low test-retest reliability
- Change significance unclear: Observed MBTI shifts of 1-2 questions may be within the margin of error for the evaluation method itself
- Confounding factors: The study does not control for token count in different corpora or potential overlap with pretraining data

## Confidence

- **High Confidence**: The technical implementation of MBTI evaluation using softmax probabilities and the experimental methodology for prompt engineering and fine-tuning are sound and reproducible.
- **Medium Confidence**: The observation that LLMs exhibit different MBTI profiles and that these profiles can be systematically altered through intervention is supported by the data, though the interpretation of these changes as "personality" remains debatable.
- **Low Confidence**: The claim that MBTI serves as a meaningful evaluation metric for LLMs is not well-supported, given the lack of validation against human MBTI distributions and the questionable validity of MBTI itself as a personality measure.

## Next Checks

1. Conduct a human study comparing LLM MBTI distributions against human MBTI distributions to establish baseline validity and identify any systematic differences.
2. Implement test-retest reliability assessment by evaluating the same LLM multiple times with different random seeds and measuring consistency in MBTI outcomes.
3. Design control experiments using random or semantically unrelated prompts to determine if observed MBTI shifts exceed what would be expected from random variation or superficial response patterns.