---
ver: rpa2
title: How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language
  Models?
arxiv_id: '2306.06048'
source_url: https://arxiv.org/abs/2306.06048
tags:
- detection
- performance
- fine-tuning
- fpr95
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies how parameter-efficient fine-tuning
  affects out-of-distribution (OOD) detection in large vision-language models like
  CLIP. It evaluates various fine-tuning methods (prompt learning, adaptors) and OOD
  scoring functions on few-shot downstream tasks.
---

# How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?

## Quick Facts
- arXiv ID: 2306.06048
- Source URL: https://arxiv.org/abs/2306.06048
- Reference count: 13
- Key outcome: Parameter-efficient fine-tuning, especially prompt learning with MCM score, improves both in-distribution accuracy and OOD detection performance over zero-shot baseline.

## Executive Summary
This paper systematically investigates how parameter-efficient fine-tuning methods affect out-of-distribution detection in large vision-language models like CLIP. Through extensive experiments on few-shot downstream tasks, the authors demonstrate that prompt learning consistently outperforms other fine-tuning approaches for OOD detection when paired with appropriate scoring functions. The maximum concept matching (MCM) score emerges as particularly effective, providing significant improvements in FPR95 and AUROC metrics across multiple datasets and model architectures.

## Method Summary
The paper evaluates four adaptation methods (ZOCLIP, TipAdaptor, TipAdaptorF, CoOp, CoCoOp) with three OOD scoring functions (SMS, SMCM, MSP) on few-shot ID datasets (Caltech-101, Stanford-Cars, Food-101, Oxford-Pets, ImageNet-1k) using 16 shots per class. The evaluation framework compares OOD detection performance while maintaining ID classification accuracy, using pre-trained CLIP models as the baseline. Standard training protocols are followed for each method, with proper temperature scaling applied to scoring functions.

## Key Results
- Parameter-efficient fine-tuning improves both ID accuracy and OOD detection when using appropriate scoring functions
- MCM score consistently outperforms SMS and MSP for fine-tuned models, achieving 10-20% improvement in FPR95
- Prompt learning methods (CoOp, CoCoOp) show superior OOD detection performance compared to adaptor-based methods
- Fine-tuning does not deteriorate pre-trained features but rather enhances ID-OOD separability in feature space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient fine-tuning methods, especially prompt learning, improve OOD detection performance by enhancing the alignment between visual and textual feature representations in the CLIP model.
- Mechanism: Prompt learning introduces learnable context vectors that optimize the textual prompts, leading to better representation of the ID classes. This results in more discriminative features for both ID and OOD samples, as evidenced by the improved ID-OOD separability in the feature space.
- Core assumption: The textual prompts effectively capture the semantic concepts of the ID classes, and the fine-tuned features maintain a meaningful structure in the multi-modal feature space.
- Evidence anchors:
  - [abstract]: "Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consistently."
  - [section]: "We show that parameter-efficient fine-tuning do not deteriorate pre-trained features. Instead, they can improve both ID and OOD performance with a proper OOD scoring function, especially the MCM score."
- Break condition: If the textual prompts fail to capture the semantic concepts of the ID classes or if the fine-tuned features become too distorted, the OOD detection performance may degrade.

### Mechanism 2
- Claim: The maximum concept matching (MCM) score is particularly effective for OOD detection in fine-tuned CLIP models because it leverages the aligned visual and textual feature representations.
- Mechanism: MCM score computes the maximum softmax similarity between the visual features of an input and the textual prototypes of the ID classes. By using the aligned feature space, MCM can effectively measure the semantic distance between the input and the ID classes, leading to improved OOD detection.
- Core assumption: The visual and textual features are well-aligned in the CLIP model, and the softmax scaling with a proper temperature is crucial for optimal OOD detection performance.
- Evidence anchors:
  - [abstract]: "In particular, the maximum concept matching (MCM) score provides a promising solution consistently."
  - [section]: "Ming et al. (2022) suggest that softmax scaling with a proper temperature τ provably leads to state-of-the-art performance under the zero-shot (training-free) setting."
- Break condition: If the visual and textual features are not well-aligned or if the temperature scaling is not properly tuned, the MCM score may not perform optimally.

### Mechanism 3
- Claim: Prompt learning methods improve OOD detection by increasing the angular distance between OOD samples and the nearest ID prototypes while decreasing the angular distance for ID samples.
- Mechanism: By optimizing the context vectors in the prompts, prompt learning perturbs the pre-trained feature space in a way that enhances the separability between ID and OOD samples. This leads to better ID-OOD discrimination and improved OOD detection performance.
- Core assumption: The perturbation introduced by prompt learning is modest and does not significantly distort the overall structure of the feature space.
- Evidence anchors:
  - [abstract]: "We show that prompt learning consistently demonstrates the state-of-the-art OOD detection performance over the zero-shot counterpart."
  - [section]: "Compared to zero-shot CLIP, CoOp and CoCoOp decrease the angular distance for ID inputs to the nearest concept prototype while simultaneously increasing the angular distance for OOD inputs."
- Break condition: If the perturbation introduced by prompt learning is too large or if it significantly distorts the feature space, the OOD detection performance may degrade.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) like CLIP
  - Why needed here: Understanding the architecture and pre-training objectives of VLMs is crucial for comprehending how fine-tuning impacts their OOD detection capabilities.
  - Quick check question: What is the main objective of pre-training CLIP, and how does it achieve alignment between visual and textual features?

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: OOD detection is the primary task being investigated in this paper, and understanding its definition and evaluation metrics is essential for interpreting the results.
  - Quick check question: How is OOD detection formulated in the context of fine-tuned VLMs, and what are the common evaluation metrics used?

- Concept: Parameter-Efficient Fine-Tuning Methods
  - Why needed here: The paper focuses on the impact of parameter-efficient fine-tuning methods, such as prompt learning and adaptors, on OOD detection. Understanding these methods is crucial for interpreting the experimental results.
  - Quick check question: What are the main differences between prompt learning and adaptor-based methods, and how do they optimize the CLIP model during fine-tuning?

## Architecture Onboarding

- Component map:
  - Image → Vision encoder → Visual features → OOD scoring function → OOD uncertainty score
  - Textual description → Text encoder → Textual features → Prompt learning → Optimized prompts
  - Visual features + Textual features → Aligned feature space → ID classification

- Critical path:
  - Input image → Vision encoder → Visual features → MCM scoring → OOD detection
  - Input textual description → Text encoder → Textual features → Prompt learning → Optimized prompts
  - ID classification → Few-shot training → Fine-tuned model → Improved OOD detection

- Design tradeoffs:
  - Fine-tuning vs. zero-shot: Fine-tuning improves ID accuracy but may impact OOD detection if not done properly
  - Prompt learning vs. adaptors: Prompt learning optimizes textual prompts, while adaptors directly optimize feature representations
  - OOD scoring functions: Different scoring functions may be more suitable for different fine-tuning methods and ID datasets

- Failure signatures:
  - Poor ID-OOD separability in the feature space
  - Significant overlap between ID and OOD score distributions
  - Degradation in ID classification accuracy after fine-tuning

- First 3 experiments:
  1. Compare the OOD detection performance of zero-shot CLIP with fine-tuned CLIP using prompt learning and adaptors on a few-shot ID dataset
  2. Evaluate the impact of different OOD scoring functions (e.g., MCM, MS, MSP) on the OOD detection performance of fine-tuned CLIP models
  3. Analyze the feature space geometry before and after fine-tuning to understand how the perturbations introduced by prompt learning affect ID-OOD separability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MCM score's performance scale with increasingly large and diverse OOD datasets, and what are its limitations compared to other OOD detection methods?
- Basis in paper: [explicit] The paper highlights that MCM consistently outperforms other OOD scores, especially for fine-tuning methods like prompt learning. It mentions ImageNet-1k as a challenging benchmark and suggests that MCM's effectiveness extends to larger models like CLIP-L/14.
- Why unresolved: The paper provides limited analysis on how MCM performs with extremely large or diverse OOD datasets beyond ImageNet-1k. It doesn't explore the limitations of MCM compared to other methods in such scenarios.
- What evidence would resolve it: Extensive experiments on a variety of large and diverse OOD datasets, comparing MCM with other state-of-the-art OOD detection methods, would provide insights into its scalability and limitations.

### Open Question 2
- Question: What are the theoretical foundations and mathematical justifications for why softmax scaling with temperature tuning is particularly effective for OOD detection in fine-tuned vision-language models?
- Basis in paper: [explicit] The paper mentions that Ming et al. (2022) suggest that softmax scaling with a proper temperature τ provably leads to state-of-the-art performance under the zero-shot setting. It also highlights the importance of softmax and temperature scaling for OOD detection with fine-tuning.
- Why unresolved: The paper doesn't provide a detailed theoretical explanation for why softmax scaling is effective, especially in the context of fine-tuning. It only mentions the empirical success of this approach.
- What evidence would resolve it: A rigorous mathematical analysis of the properties of softmax scaling and its impact on OOD detection, particularly when combined with fine-tuning, would provide theoretical foundations for its effectiveness.

### Open Question 3
- Question: How does the choice of prompt learning method (e.g., CoOp vs. CoCoOp) affect the geometry of the feature space and the resulting OOD detection performance, and are there scenarios where one method is clearly superior?
- Basis in paper: [explicit] The paper compares CoOp and CoCoOp, showing that CoCoOp generally achieves better OOD detection performance. It also analyzes how prompt learning impacts the feature space, noting that it decreases the angular distance for ID inputs while increasing it for OOD inputs.
- Why unresolved: While the paper provides some insights into the effects of different prompt learning methods, it doesn't offer a comprehensive analysis of how the choice of method impacts the feature space geometry and OOD detection performance across various scenarios.
- What evidence would resolve it: A detailed study comparing the feature space geometry and OOD detection performance of different prompt learning methods across a wide range of datasets and model architectures would clarify their relative strengths and weaknesses.

## Limitations
- Evaluation is primarily focused on CLIP-based architectures, limiting generalizability to other VLM frameworks
- Study uses relatively small ID datasets (few-shot settings) which may not represent real-world deployment scenarios
- Standard OOD benchmarks may not fully capture the complexity of real-world OOD data distributions

## Confidence
- High confidence: The empirical results showing MCM score superiority for fine-tuned models (FPR95 and AUROC improvements are consistently observed across multiple datasets)
- Medium confidence: The mechanistic explanations for why prompt learning improves OOD detection (while supported by evidence, the exact feature space dynamics require further theoretical analysis)
- Medium confidence: The generalizability of findings across different VLM architectures (results are robust within CLIP variants but need validation on other architectures)

## Next Checks
1. Test the MCM score and prompt learning benefits on non-CLIP VLMs (e.g., BLIP, Florence) to assess architecture-agnostic performance
2. Evaluate the impact of fine-tuning on OOD detection when ID and OOD distributions have higher semantic overlap to stress-test the method
3. Conduct ablation studies varying temperature τ values systematically across different model scales to better understand the scaling behavior observed