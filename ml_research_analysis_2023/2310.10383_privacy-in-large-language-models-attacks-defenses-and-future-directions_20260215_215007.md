---
ver: rpa2
title: 'Privacy in Large Language Models: Attacks, Defenses and Future Directions'
arxiv_id: '2310.10383'
source_url: https://arxiv.org/abs/2310.10383
tags:
- privacy
- attacks
- llms
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines privacy attacks and defenses
  in large language models (LLMs). It categorizes privacy attacks based on adversary
  capabilities, including backdoor attacks (data poisoning, poisoned pre-trained models,
  fine-tuned models), prompt injection attacks, training data extraction attacks,
  membership inference attacks, and attacks leveraging extra information like embeddings
  and gradients.
---

# Privacy in Large Language Models: Attacks, Defenses and Future Directions

## Quick Facts
- arXiv ID: 2310.10383
- Source URL: https://arxiv.org/abs/2310.10383
- Reference count: 40
- This survey comprehensively examines privacy attacks and defenses in large language models (LLMs)

## Executive Summary
This survey provides a systematic analysis of privacy threats and protective mechanisms for large language models. It categorizes attacks based on adversary capabilities, including backdoor attacks, prompt injection, training data extraction, membership inference, and gradient-based attacks. The paper also surveys defense strategies such as differential privacy, secure multi-party computation, federated learning, and specific countermeasures against various attack types.

## Method Summary
The paper employs a comprehensive survey methodology, systematically categorizing privacy attacks and defenses in LLMs based on adversary capabilities and attack vectors. Rather than presenting new empirical results, it reviews and synthesizes existing research across multiple categories including backdoor attacks (data poisoning, poisoned pre-trained models), prompt injection attacks, training data extraction attacks, membership inference attacks, and attacks leveraging extra information like embeddings and gradients. The survey methodology involves identifying and organizing the primary literature cited for each attack and defense category, providing a structured framework for understanding the current state of LLM privacy research.

## Key Results
- Comprehensive taxonomy of privacy attacks on LLMs based on adversary capabilities
- Survey of defense mechanisms including differential privacy, SMPC, and federated learning approaches
- Identification of key limitations in existing approaches, particularly utility degradation from privacy mechanisms
- Outline of future research directions including privacy alignment to human perception and contextualized privacy judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential privacy (DP) provides worst-case privacy guarantees by adding calibrated noise to gradients during training.
- Mechanism: DP bounds the influence any single training example can have on the model output by ensuring that removing or adding one data point changes the output distribution by at most a factor of e^ε.
- Core assumption: The adversary has full knowledge of the training data distribution and can measure output changes precisely.
- Evidence anchors:
  - [abstract] "DP offers a theoretical worst-case privacy guarantee for the protected data"
  - [section] "Differential privacy incorporates random noise into aggregated data to allow data mining without exposing participants’ private information"
  - [corpus] Weak evidence - related papers discuss DP but don't provide concrete attack/defence case studies
- Break condition: If the privacy budget ε is too large or noise is improperly calibrated, privacy guarantees break down and training data can be extracted.

### Mechanism 2
- Claim: Secure Multi-Party Computation (SMPC) enables privacy-preserving LLM inference without revealing model parameters or input data.
- Mechanism: SMPC protocols split computations across multiple parties so that no single party sees both the model and the input, using techniques like secret sharing or homomorphic encryption.
- Core assumption: All parties follow the protocol honestly and communication channels are secure.
- Evidence anchors:
  - [abstract] "SMPC enables a group of mutually untrusted data owners to collaboratively compute a function while safeguarding the privacy of their data"
  - [section] "SMPC is mainly used in the inference phase of LLMs to protect both the model parameters and inference data"
  - [corpus] Moderate evidence - related work mentions SMPC but specific LLM implementations are limited
- Break condition: If any party deviates from the protocol or if non-linear operations cannot be efficiently computed, privacy and performance break down.

### Mechanism 3
- Claim: Federated learning with differential privacy can train LLMs on distributed data while preventing individual client data leakage.
- Mechanism: Clients train locally on private data and share only noisy gradients with a central server, which aggregates updates to update the global model.
- Core assumption: Clients' local datasets are representative and gradient updates cannot be inverted to recover private data.
- Evidence anchors:
  - [section] "Federated learning (FL) is a privacy-preserving distributed learning paradigm enabling multiple parties to train or fine-tune their LLMs collaboratively without sharing private data"
  - [section] "While FL can potentially protect data privacy by preventing adversaries from directly accessing private data, a variety of research works have demonstrated that FL algorithms without adopting any privacy protection have the risk of leaking data privacy"
  - [corpus] Moderate evidence - FL is mentioned in related work but specific LLM applications are not well-documented
- Break condition: If gradient leakage attacks succeed or if the aggregation mechanism is compromised, individual client privacy is violated.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: DP provides the mathematical foundation for quantifying and bounding privacy loss in machine learning systems.
  - Quick check question: If ε=0.1 and δ=10^-5, is this considered strong privacy protection? (Answer: Yes, smaller ε means stronger privacy)

- Concept: Secure Multi-Party Computation
  - Why needed here: SMPC allows collaborative computation without data sharing, crucial for privacy-preserving LLM deployment.
  - Quick check question: Can SMPC handle non-linear operations like softmax efficiently? (Answer: With significant computational overhead and approximation)

- Concept: Federated Learning
  - Why needed here: FL enables distributed training while keeping data local, but requires additional privacy mechanisms to be secure.
  - Quick check question: What's the main privacy risk in vanilla FL? (Answer: Gradient leakage attacks that can reconstruct training data)

## Architecture Onboarding

- Component map: Data → Privacy protection (DP/SMPC/FL) → Model training/inference → Privacy evaluation → Feedback loop
- Critical path: Data → Privacy protection (DP/SMPC/FL) → Model training/inference → Privacy evaluation → Feedback loop
- Design tradeoffs:
  - Privacy vs utility: Stronger privacy (smaller ε) degrades model performance
  - Efficiency vs security: SMPC adds computational overhead
  - Centralization vs decentralization: FL trades off control for privacy
- Failure signatures:
  - Unexpected accuracy drops after DP application
  - Slow inference times indicating SMPC overhead
  - Privacy budget exhaustion in federated settings
  - False positives in prompt sanitization
- First 3 experiments:
  1. Implement DP-SGD on a small BERT model and measure privacy-utility tradeoff
  2. Set up SMPC-based inference for a pre-trained GPT-2 using secret sharing
  3. Deploy federated learning with DP on a text classification task and test for gradient leakage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of backdoor attacks on LLMs when integrated into real-world applications like autonomous driving or smart assistants?
- Basis in paper: [explicit] The paper discusses backdoor attacks on LLMs and their potential severe privacy and security repercussions if deployed in real-life systems.
- Why unresolved: The paper provides a comprehensive analysis of existing backdoor attacks but does not delve into the long-term impacts of such attacks in practical applications.
- What evidence would resolve it: Empirical studies tracking the performance and security of LLMs over time in real-world applications, especially those susceptible to backdoor attacks.

### Open Question 2
- Question: How can privacy-preserving mechanisms be developed that balance both utility and privacy without significant degradation in LLM performance?
- Basis in paper: [explicit] The paper identifies the trade-off between privacy and utility as a key challenge, especially in the context of differential privacy, which often results in degraded utility.
- Why unresolved: While the paper reviews various defense strategies, it highlights the limitations of current approaches, particularly the utility degradation caused by differential privacy.
- What evidence would resolve it: Development and empirical validation of privacy-preserving mechanisms that achieve comparable performance to non-private models on complex downstream tasks.

### Open Question 3
- Question: What are the most effective methods for evaluating the privacy of LLMs beyond worst-case theoretical bounds provided by differential privacy?
- Basis in paper: [explicit] The paper suggests that empirical privacy evaluation is needed, as differential privacy provides worst-case bounds that may not reflect practical privacy risks.
- Why unresolved: The paper calls for better privacy evaluation metrics but does not propose specific methods or frameworks for empirical evaluation.
- What evidence would resolve it: Creation and validation of new privacy evaluation metrics that accurately reflect the practical privacy risks of LLMs in real-world scenarios.

## Limitations
- The rapidly evolving nature of LLM privacy research makes comprehensive coverage challenging
- Many proposed defenses were developed for general ML rather than specifically for LLMs, with effectiveness at trillion-parameter scale largely theoretical
- Lack of empirical validation and quantitative assessments of attack/defence effectiveness across the survey

## Confidence
**High Confidence Claims:**
- The taxonomy of privacy attacks (backdoor attacks, prompt injection, training data extraction, membership inference) is well-established and widely accepted in the research community.
- Differential privacy and federated learning are legitimate privacy-preserving techniques with mathematical foundations.
- Current defenses face significant trade-offs between privacy guarantees and model utility.

**Medium Confidence Claims:**
- SMPC-based approaches can be adapted for LLM inference, though practical implementations at scale remain unproven.
- The effectiveness of existing defenses against emerging attacks like prompt injection is not fully characterized.
- Future directions identified (privacy alignment to human perception, contextualized privacy judgment) represent reasonable research trajectories.

**Low Confidence Claims:**
- Specific quantitative bounds on privacy leakage for LLM-scale models using current techniques.
- Practical feasibility of implementing certain defenses (like full SMPC for trillion-parameter models) without significant performance degradation.
- The relative importance and prevalence of different attack vectors in real-world deployments.

## Next Checks
1. Implement DP-SGD on a small BERT model and measure privacy-utility tradeoff
2. Set up SMPC-based inference for a pre-trained GPT-2 using secret sharing
3. Deploy federated learning with DP on a text classification task and test for gradient leakage