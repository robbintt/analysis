---
ver: rpa2
title: A preferential interpretation of MultiLayer Perceptrons in a conditional logic
  with typicality
arxiv_id: '2305.00304'
source_url: https://arxiv.org/abs/2305.00304
tags:
- fuzzy
- interpretation
- network
- knowledge
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops a fuzzy Description Logic semantics for weighted\
  \ knowledge bases with typicality, enabling a logical interpretation of multilayer\
  \ neural networks (MLPs). The core method introduces ALCFT, a fuzzy extension of\
  \ ALC with typicality, and defines three closure constructions\u2014faithful, coherent,\
  \ and \u03D5-coherent semantics\u2014for weighted KBs."
---

# A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality

## Quick Facts
- arXiv ID: 2305.00304
- Source URL: https://arxiv.org/abs/2305.00304
- Authors: Andrea Formisano, Marco Manna, Francesco Ricca, Eugenia Ternovska
- Reference count: 40
- One-line primary result: Fuzzy Description Logic semantics with typicality enables logical interpretation of multilayer neural networks, supporting verification of properties like "typical happiness implies certain facial features".

## Executive Summary
This paper introduces a novel logical framework for interpreting multilayer perceptrons (MLPs) using fuzzy Description Logic with typicality. The approach maps neural network activations to fuzzy membership degrees and constructs a weighted knowledge base from synaptic weights. Three closure constructions—faithful, coherent, and ϕ-coherent semantics—are defined to interpret these KBs, inducing multi-preferential models where preferences are concept-wise and based on fuzzy membership degrees. The framework allows MLPs to be verified for properties expressed as fuzzy conditional statements, providing a bridge between neural networks and symbolic reasoning.

## Method Summary
The method involves training a feedforward neural network on labeled data, encoding it as a weighted conditional knowledge base where synaptic weights become typicality inclusion weights, and interpreting this KB under a fuzzy Description Logic with typicality. The interpretation maps neural activations to fuzzy concept membership degrees and induces preference relations for each concept. Properties of the network are then verified using model checking or entailment under the chosen semantics. A finitely-valued approximation of the continuous semantics enables scalable verification for larger networks.

## Key Results
- ALCFT with Gödel combination functions satisfies all KLM postulates of a preferential consequence relation (except Rational Monotonicity).
- The ϕ-coherent semantics approximates the fuzzy case in the finitely-valued setting, providing computational tractability.
- Experiments on emotion recognition networks show that properties like "typical happiness implies certain facial features" can be verified, with model checking being computationally cheaper than entailment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fuzzy Description Logic semantics for weighted knowledge bases can capture multilayer perceptron (MLP) behavior via concept-wise preferences and typicality.
- Mechanism: The activation levels of MLP units for input stimuli are mapped to membership degrees in fuzzy concepts, inducing a preference relation for each concept. Typicality concepts then identify most representative inputs for each category, allowing verification of fuzzy conditional properties like "typical happiness implies certain facial features".
- Core assumption: The neural network reaches a stationary state for each input, so unit activations correspond to stable membership degrees in the associated concepts.
- Evidence anchors:
  - [abstract] The semantics is used to provide a preferential interpretation of MLPs based on activity levels of units for input stimuli.
  - [section 6.2] Definition 10 explicitly constructs the fuzzy multi-preferential interpretation by setting concept degrees equal to neuron outputs for each input.
  - [corpus] Weak. Neighbors do not directly discuss the semantic interpretation of MLPs.
- Break condition: If the network contains cycles and does not reach a stationary state, the interpretation may not capture the intended semantics.

### Mechanism 2
- Claim: A trained MLP can be regarded as a weighted conditional knowledge base, where synaptic weights become typicality inclusion weights.
- Mechanism: For each unit with incoming edges, a set of weighted typicality inclusions is created with weights equal to the synaptic weights. This KB can be interpreted under faithful, coherent, or ϕ-coherent semantics, each imposing different conditions on how concept membership degrees relate to preference weights.
- Core assumption: The interpretation of typicality concepts and concept membership degrees in the KB aligns with the MLP's activation dynamics.
- Evidence anchors:
  - [section 6.3] Proposition 6 proves the fuzzy interpretation built from the MLP is a ϕ-coherent model of the associated KB under the stated conditions.
  - [section 6.3] The KB construction from synaptic weights is described and the logical correspondence is established.
  - [corpus] Weak. Neighbors focus on complexity of reasoning, not on the MLP-KB correspondence.
- Break condition: If activation functions are not monotonic (for faithful/coherent models), the KB may not faithfully represent the network.

### Mechanism 3
- Claim: Finitely-valued approximations of the ϕ-coherent semantics (ϕn-coherent) provide a computationally tractable way to reason about MLP properties.
- Mechanism: A continuous activation function ϕ is approximated by a stepwise function ϕn over a finite truth space. The sequence of ϕn-coherent models converges to the full fuzzy semantics, allowing model checking and entailment verification in the finitely-valued case.
- Core assumption: The activation function is continuous and the chosen t-norm, s-norm, and negation functions are also continuous.
- Evidence anchors:
  - [section 7] Proposition 8 proves that the approximation preserves coherence conditions in the limit.
  - [section 7] Lemma 1 shows uniform convergence of ϕn to ϕ under the stated conditions.
  - [section 8] Experiments use the finitely-valued case (C5) for scalable verification.
- Break condition: If the activation function is discontinuous or the truth space is too coarse, the approximation may not capture important network behaviors.

## Foundational Learning

- Concept: Fuzzy Description Logic (Fuzzy DL)
  - Why needed here: Provides the semantic framework to represent vagueness and graded membership in MLP concepts.
  - Quick check question: In fuzzy DL, what does it mean for an element to have membership degree 0.7 in a concept?

- Concept: Multi-preferential semantics with typicality
  - Why needed here: Allows defining preferences for each concept independently, enabling representation of prototypical instances and non-monotonic reasoning.
  - Quick check question: How does the preference relation <C for concept C differ from a global preference relation?

- Concept: Closure constructions (faithful, coherent, ϕ-coherent)
  - Why needed here: Strengthen the base semantics to better capture the network's weighted knowledge and activation dynamics.
  - Quick check question: What additional condition does the coherent semantics impose compared to the faithful semantics?

## Architecture Onboarding

- Component map:
  - MLP network (units, weights, activation functions)
  - Domain of input stimuli (finite set of vectors)
  - Fuzzy interpretation mapping (unit activations → concept membership degrees)
  - Preference relations (induced by concept degrees)
  - Weighted KB (synaptic weights → typicality inclusions)
  - Reasoning engine (model checking, entailment under chosen semantics)
  - Approximation layer (continuous → finitely-valued semantics)

- Critical path:
  1. Train MLP on data.
  2. Select input stimuli domain.
  3. Build fuzzy interpretation from unit activations.
  4. Construct weighted KB from synaptic weights.
  5. Choose semantics (faithful/coherent/ϕ-coherent).
  6. Verify properties via model checking or entailment.

- Design tradeoffs:
  - Faithful vs coherent semantics: Faithful is weaker, allowing more models but potentially less precise verification.
  - Continuous vs finitely-valued semantics: Continuous is more expressive but computationally intractable; finitely-valued is scalable but approximate.
  - Full vs partial interpretation: Including all units gives richer semantics but increases complexity; focusing on relevant units simplifies verification.

- Failure signatures:
  - Property verification fails despite network performing well: Possible mismatch between input domain and real usage.
  - Model checking too slow: Domain too large or truth space too fine-grained.
  - Entailment engine times out: KB too large or search space too big.

- First 3 experiments:
  1. Verify simple typicality property (e.g., T(happiness) ⊑ au12 ≥ 0.6) on small MLP with 2-3 hidden layers.
  2. Compare faithful vs coherent semantics on the same property to see impact of stricter condition.
  3. Test finitely-valued approximation (n=3) vs continuous semantics on a small domain to evaluate accuracy loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-preferential semantics be extended to handle nested typicality operators in ALCFT?
- Basis in paper: [explicit] The paper restricts typicality to non-nested occurrences and does not address nested cases.
- Why unresolved: Allowing nested typicality could enable richer representation of hierarchical preferences but may break the semantic closure constructions.
- What evidence would resolve it: A formal extension of ALCFT with nested typicality, proving whether KLM postulates and closure properties still hold.

### Open Question 2
- Question: How does the choice of fuzzy combination functions (e.g., product logic) affect the KLM properties in ALCFT?
- Basis in paper: [explicit] The paper proves KLM properties hold for Gödel logic but explicitly notes failure for product and Łukasiewicz logics.
- Why unresolved: Different combination functions may offer trade-offs between expressiveness and computational properties; a general characterization is missing.
- What evidence would resolve it: A systematic study classifying which combination functions preserve KLM properties and under what conditions.

### Open Question 3
- Question: Is there a complete finite-valued approximation of the ϕ-coherent semantics for ALCFT with roles?
- Basis in paper: [explicit] The paper proves completeness for LCFT (no roles) but leaves the role-inclusive case open.
- Why unresolved: The presence of roles and role restrictions introduces additional complexity that may prevent finite approximation.
- What evidence would resolve it: A proof or counterexample showing whether ϕn-coherence approximates ϕ-coherence in ALCFT with full role constructors.

### Open Question 4
- Question: Can the model checking approach be extended to counterfactual reasoning about neural network properties?
- Basis in paper: [inferred] The paper notes counterfactual reasoning is unexplored and the model checking approach is model-agnostic.
- Why unresolved: Counterfactuals require reasoning about hypothetical changes, which is not directly supported by the current preferential interpretation.
- What evidence would resolve it: An extension of the multi-preferential model to support counterfactual interventions and a corresponding model checking algorithm.

## Limitations

- The assumption that MLPs reach a stationary state for each input may not hold for networks with recurrent connections or complex activation dynamics.
- The experiments are conducted on small networks (3 layers, ~10 hidden units) and a specific domain (emotion recognition from facial expressions). The approach's effectiveness on larger, more diverse networks is unknown.
- The computational complexity of entailment in the fuzzy case is high (coNP^NP), limiting its use for large knowledge bases or complex properties.

## Confidence

- High confidence in the mathematical formulation of ALCFT and its KLM postulates satisfaction.
- Medium confidence in the practical applicability to real-world MLPs, as the experiments are limited to small networks and specific domains.
- Low confidence in the scalability of the approach to large, deep networks with continuous inputs.

## Next Checks

1. Test the interpretation on a larger MLP (e.g., 5+ layers, 100+ hidden units) trained on a more diverse dataset to assess scalability and robustness.
2. Experiment with different activation functions (e.g., sigmoid, tanh) and t-norms to evaluate the approach's flexibility and generalization.
3. Conduct a user study to evaluate the interpretability and usefulness of the verified properties for explaining MLP decisions in real-world applications.