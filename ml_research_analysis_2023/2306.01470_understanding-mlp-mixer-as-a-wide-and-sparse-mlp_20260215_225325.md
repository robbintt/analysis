---
ver: rpa2
title: Understanding MLP-Mixer as a Wide and Sparse MLP
arxiv_id: '2306.01470'
source_url: https://arxiv.org/abs/2306.01470
tags:
- matrix
- mlp-mixer
- mixers
- width
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the MLP-Mixer through the lens of wide and
  sparse MLPs. It shows that mixing layers in the Mixer can be expressed as extremely
  wide MLPs with sparse weights, using Kronecker products and permutation matrices.
---

# Understanding MLP-Mixer as a Wide and Sparse MLP

## Quick Facts
- **arXiv ID**: 2306.01470
- **Source URL**: https://arxiv.org/abs/2306.01470
- **Reference count**: 40
- **Key outcome**: MLP-Mixer's mixing layers can be expressed as extremely wide MLPs with sparse weights using Kronecker products and permutation matrices, and increasing width while fixing parameter count improves performance.

## Executive Summary
This paper reveals that MLP-Mixer architectures can be understood as wide and sparse MLPs through Kronecker product formulations. The authors show that mixing layers in the Mixer can be expressed as extremely wide MLPs with sparse weights, using Kronecker products and permutation matrices. This reveals a "Permuted-Kronecker (PK)" family of architectures that includes both standard and randomized mixing layers. Based on the hypothesis that increasing width while fixing parameter count improves performance, the authors determine optimal mixing layer dimensions by maximizing effective width. Experiments on CIFAR and ImageNet show that wider, sparser mixing layers improve accuracy, and that even random mixing layers perform nearly as well as structured ones.

## Method Summary
The paper analyzes MLP-Mixer through the lens of wide and sparse MLPs by expressing mixing layers using Kronecker products and permutation matrices. The authors implement three variants: normal MLP-Mixer with identity or commutation permutations, RP-Mixer with random permutations, and SW-MLP with static sparse weights. They train these models on CIFAR-10, CIFAR-100, STL-10, and ImageNet-1k datasets with standard preprocessing and optimization (SGD/Nesterov or AdamW with cosine annealing). The key experimental approach involves varying mixing layer dimensions while fixing parameter count to test the width-sparsity performance hypothesis.

## Key Results
- Wider, sparser mixing layers improve accuracy on CIFAR and ImageNet when parameter count is fixed
- Random permutation mixing layers (RP-Mixer) achieve nearly comparable performance to structured permutations
- MLP-Mixer connects mathematically to Monarch matrices, revealing shared structural properties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MLP-Mixer's mixing layers can be interpreted as a wide and sparse MLP using Kronecker products and permutation matrices.
- **Mechanism**: The mixing layers vectorize matrix operations into Kronecker-product-based weight matrices, yielding an effective MLP with width equal to the product of spatial and channel dimensions.
- **Core assumption**: The relationship between matrix operations and Kronecker products (vec(WXV) = (V^T ⊗ W)vec(X)) holds for all mixing layers.
- **Evidence anchors**:
  - [abstract] "We show that mixing layers in the Mixer can be expressed as extremely wide MLPs with sparse weights, using Kronecker products and permutation matrices."
  - [section] "vec(H) = ϕ((V^T ⊗ IS)ϕ((IC ⊗ W)x))" and "vec(H) = ϕ(Jc^T(IS ⊗ V^T)ϕ(Jc(IC ⊗ W)x))"
  - [corpus] No direct mention of Kronecker product or sparse parameterization in neighbor papers, but this mechanism is fundamental to the paper's contribution.
- **Break condition**: If the Kronecker product relationship fails for certain layer configurations, the effective width calculation would be invalid.

### Mechanism 2
- **Claim**: Increasing effective width while keeping parameter count fixed improves prediction accuracy.
- **Mechanism**: By fixing the number of non-zero entries (Ω) and maximizing SC (the product of spatial and channel dimensions), the model achieves higher sparsity and better generalization.
- **Core assumption**: Golubeva et al.'s hypothesis that width increase with fixed parameters improves performance applies to PK family architectures.
- **Evidence anchors**:
  - [abstract] "Following a guiding principle proposed by Golubeva, Neyshabur and Gur-Ari (2021), which fixes the number of connections and increases the width and sparsity, the Mixers can demonstrate improved performance."
  - [section] "By maximizing the width of the effective MLP expression of the PK family, we find the appropriate sizes of channel mixing and token mixing layers of the Mixers."
  - [corpus] Neighbor papers focus on different architectures without explicitly addressing this width-sparsity relationship.
- **Break condition**: If the width increase leads to insufficient gradient flow or optimization difficulties, the performance gain may plateau or reverse.

### Mechanism 3
- **Claim**: Random permutation matrices in mixing layers can achieve performance comparable to structured permutations.
- **Mechanism**: RP-Mixers replace structured permutations (identity or commutation) with random permutations, creating unstructured sparse weights while maintaining similar effective width and performance characteristics.
- **Core assumption**: The inductive bias of RP-Mixers is sufficiently similar to normal Mixers despite random permutations.
- **Evidence anchors**:
  - [abstract] "Experiments on CIFAR and ImageNet show that wider, sparser mixing layers improve accuracy, and that even random mixing layers perform nearly as well as structured ones."
  - [section] "RP-Mixers can achieve (a bit worse but) almost comparable accuracy in experiments."
  - [corpus] No direct evidence in neighbor papers about random permutations in mixing layers.
- **Break condition**: If random permutations significantly disrupt local spatial correlations in early layers, performance may degrade more substantially than observed.

## Foundational Learning

- **Concept**: Kronecker product and vectorization relationship (vec(WXV) = (V^T ⊗ W)vec(X))
  - **Why needed here**: Essential for understanding how mixing layers translate to wide sparse MLPs
  - **Quick check question**: What is the effective width of a mixing layer with S=64 and C=128?

- **Concept**: Sparse weight matrices and their effect on generalization
  - **Why needed here**: Core to understanding why wider, sparser architectures perform better
  - **Quick check question**: How does fixing parameter count while increasing width affect sparsity?

- **Concept**: Permutation matrices and their properties
  - **Why needed here**: Critical for understanding both structured and random mixing layer implementations
  - **Quick check question**: What property of permutation matrices makes them useful for rearranging matrix elements?

## Architecture Onboarding

- **Component map**: Input image → Per-patch FC layer → Token-mixing block → Channel-mixing block → Skip connection → Output
- **Critical path**: Per-patch FC → Token-mixing → Channel-mixing → Skip connection
- **Design tradeoffs**:
  - Width vs. sparsity: Higher width means better performance but potentially more difficult optimization
  - Structured vs. random permutations: Structured is slightly better but random is much simpler to implement
  - Expansion factor γ: Higher γ increases capacity but also computational cost
- **Failure signatures**:
  - Training accuracy much higher than test accuracy (overfitting)
  - Gradient norms exploding or vanishing in early layers
  - Performance plateaus despite increasing width
- **First 3 experiments**:
  1. Train S-Mixer with (S,C) = (64,64) and compare to (128,32) while fixing Ω
  2. Replace one mixing block with random permutation and measure performance impact
  3. Sweep expansion factor γ from 1 to 8 while monitoring training stability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical limit of the effective width in MLP-Mixers before trainability breaks down?
- **Basis in paper**: [explicit] The paper discusses trainability issues arising from extreme sparsity in SW-MLPs, while Mixers maintain trainability up to their effective width limit (Ω/γ)²/³. The authors hypothesize this is due to bounded singular values.
- **Why unresolved**: The paper only provides empirical observations and theoretical singular value bounds. It doesn't rigorously establish a trainability threshold for Mixers specifically.
- **What evidence would resolve it**: Systematic experiments mapping performance vs. width in Mixers beyond their theoretical maximum, combined with analysis of gradient norms and loss landscape curvature at different widths.

### Open Question 2
- **Question**: How does the Monarch matrix connection affect the inductive bias of Mixers compared to general sparse MLPs?
- **Basis in paper**: [explicit] The paper shows Mixers approximate Monarch matrices under linear activations, suggesting shared structure. It also notes RP-Mixers with unstructured sparse weights perform nearly as well as normal Mixers.
- **Why unresolved**: The paper establishes the mathematical connection but doesn't experimentally compare Mixer performance against MLPs using actual Monarch matrices or other structured sparsities.
- **What evidence would resolve it**: Direct performance comparison between Mixers, MLPs with Monarch matrices, and MLPs with other structured sparsities (e.g., block-diagonal, Toeplitz) on equivalent architectures.

### Open Question 3
- **Question**: What is the optimal depth-width relationship for RP-Mixers to match normal Mixer performance?
- **Basis in paper**: [explicit] Figure 5 shows RP-Mixers can surpass normal Mixers when depth increases sufficiently. Figure 6 suggests upstream layers are functionally commutative with RP blocks.
- **Why unresolved**: The paper provides empirical observations of depth effects but doesn't characterize the depth-width relationship mathematically or explain why increased depth compensates for RP's structural randomness.
- **What evidence would resolve it**: Theoretical analysis of information flow through RP blocks showing depth compensates for randomness, plus experimental mapping of optimal depth as a function of width and expansion factor.

## Limitations

- The analysis focuses primarily on vision tasks (CIFAR, ImageNet) and may not generalize to other domains like NLP or audio
- The connection to Monarch matrices is theoretical and requires further empirical validation with direct performance comparisons
- The study doesn't rigorously establish trainability limits for extreme width configurations

## Confidence

- **Core width-sparsity hypothesis**: Medium - supported by CIFAR/ImageNet results but limited to vision tasks
- **Random permutation performance**: Medium - experimental results show comparable performance but may not generalize to all tasks
- **Mathematical framework**: High - Kronecker product formulations appear sound and well-established

## Next Checks

1. Test RP-Mixer variants on non-vision tasks (text, audio) to verify generalization of random permutation benefits
2. Implement ablation studies removing skip connections to isolate their contribution to width advantages
3. Compare performance when varying the expansion factor γ systematically across different width configurations