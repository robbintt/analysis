---
ver: rpa2
title: 'CHIP: Contrastive Hierarchical Image Pretraining'
arxiv_id: '2310.08304'
source_url: https://arxiv.org/abs/2310.08304
tags:
- level
- classes
- image
- phase
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of one-shot/few-shot classification
  for objects from unseen classes by proposing a hierarchical contrastive learning
  approach. The method uses a three-level hierarchical contrastive loss with a ResNet152
  classifier, trained on animal classes from ImageNet and evaluated on 20 unseen animal
  classes.
---

# CHIP: Contrastive Hierarchical Image Pretraining

## Quick Facts
- arXiv ID: 2310.08304
- Source URL: https://arxiv.org/abs/2310.08304
- Reference count: 0
- One-line primary result: Achieves 93-95% accuracy on unseen animal classes using hierarchical contrastive learning with ResNet152

## Executive Summary
This paper proposes a hierarchical contrastive learning approach for one-shot/few-shot classification of objects from unseen classes. The method uses a three-level hierarchical structure built from ImageNet animal classes, with separate ResNet-152 models trained at each level to map images to hierarchical embeddings. The approach demonstrates strong performance on both seen classes (94% contrastive accuracy) and unseen classes (93-95% accuracy), showing that hierarchical clustering can enable generalization to novel categories through shared parent/super-parent embeddings.

## Method Summary
The method consists of three phases: Phase 1 creates hierarchical embeddings by clustering ImageNet animal classes into three levels using K-Means; Phase 2 fine-tunes three separate ResNet-152 models (one per level) using contrastive loss against the hierarchical embeddings; Phase 3 applies the trained models to unseen classes by computing cosine similarity to cluster targets and optionally fine-tuning on the unseen data. The entire pipeline relies on the assumption that semantically similar classes can be grouped hierarchically and that unseen classes will share enough semantic similarity with seen clusters to enable accurate classification.

## Key Results
- 94% contrastive accuracy for seen classes across three hierarchical levels
- Cosine similarity >0.85 for Level 0 and Level 1, >0.74 for Level 2
- 93.8%, 93.9%, and 95% accuracy on unseen classes at Levels 0, 1, and 2 respectively
- Cosine similarities for unseen classes: 81.5%, 82%, and 74.5% at Levels 0, 1, and 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive loss enables the model to learn hierarchical embeddings by pulling semantically similar classes together and pushing dissimilar ones apart.
- Mechanism: In Phase 2, embeddings generated for each image are compared to target embeddings (class means, cluster centroids) using cosine similarity and contrastive loss, allowing the model to refine feature representations at each hierarchical level.
- Core assumption: Hierarchical grouping in Phase 1 captures semantically meaningful clusters that can be used as targets for contrastive learning.
- Evidence anchors:
  - [abstract] "Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier..."
  - [section] "For each image, an embedding is generated and Contrastive loss is calculated between the generated image embedding and the corresponding target embedding retrieved from the mapping dataframe generated in Phase 1."
  - [corpus] Weak. Corpus neighbors discuss contrastive learning and few-shot learning but do not directly support the specific hierarchical contrastive mechanism.
- Break condition: If the clustering in Phase 1 fails to produce semantically coherent groups, contrastive learning will reinforce incorrect groupings, leading to poor performance on unseen classes.

### Mechanism 2
- Claim: Hierarchical clustering of ImageNet classes creates a semantic tree that generalizes to unseen classes through shared parent/super-parent embeddings.
- Mechanism: Phase 1 clusters animal classes into Level 0 (individual classes), Level 1 (parents), and Level 2 (super-parents). Unseen classes are mapped to these clusters by cosine similarity, allowing them to inherit the learned embeddings of their nearest parent/super-parent group.
- Core assumption: Unseen animal classes will be semantically similar to at least one seen class within the same cluster hierarchy.
- Evidence anchors:
  - [abstract] "Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding..."
  - [section] "Using these mean embeddings from1, these 366 animal classes are then grouped using K-Means clustering..."
  - [corpus] Weak. Corpus neighbors discuss few-shot learning and hierarchical classification but do not directly validate the clustering-based generalization mechanism.
- Break condition: If unseen classes are semantically distant from any seen cluster, cosine similarity mapping will assign them to incorrect parent/super-parent groups, degrading classification accuracy.

### Mechanism 3
- Claim: Fine-tuning separate ResNet-152 models for each hierarchical level allows the network to specialize in different levels of semantic abstraction.
- Mechanism: Phase 2 trains three distinct ResNet-152 models—one for Level 0 (leaf classes), one for Level 1 (parents), and one for Level 2 (super-parents)—each optimized to map images to their respective target embeddings.
- Core assumption: Different levels of abstraction require different feature representations, which can be better learned by dedicated models rather than a single multi-task model.
- Evidence anchors:
  - [abstract] "Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier..."
  - [section] "In this phase, we have three separate models for three levels. We use ResNet-152 encoder for Level 0 Ground Truth..."
  - [corpus] Weak. Corpus neighbors discuss multi-task learning and hierarchical models but do not directly support the multi-model hierarchical specialization approach.
- Break condition: If the feature space is too constrained or the number of classes per level is too small, separate models may overfit and fail to generalize.

## Foundational Learning

- Concept: Contrastive learning and metric learning
  - Why needed here: The entire hierarchical classification relies on learning embeddings where semantically similar classes are close in embedding space, measured by cosine similarity and optimized via contrastive loss.
  - Quick check question: How does contrastive loss differ from standard cross-entropy loss in terms of what it optimizes for?
- Concept: Hierarchical clustering and semantic similarity
  - Why needed here: The model depends on grouping classes into meaningful parent/super-parent clusters to generalize to unseen classes.
  - Quick check question: What clustering metric would you use if your goal is to maximize intra-cluster similarity and inter-cluster dissimilarity?
- Concept: Transfer learning with pre-trained models
  - Why needed here: The ResNet-152 backbone is pre-trained on ImageNet, providing strong feature extractors that are then fine-tuned on the hierarchical task.
  - Quick check question: Why might freezing earlier layers of a pre-trained model be beneficial when adapting to a new task with limited data?

## Architecture Onboarding

- Component map:
  - Phase 1: ResNet-152 → Mean embeddings → K-Means clustering → Hierarchical cluster mapping
  - Phase 2: Three ResNet-152 models (one per level) → Contrastive loss → Fine-tuning on seen classes
  - Phase 3: Unseen class embeddings → Cosine similarity to cluster targets → Fine-tuning on unseen classes
- Critical path: Phase 1 (clustering) → Phase 2 (model training) → Phase 3 (unseen class adaptation)
- Design tradeoffs:
  - Separate models per level vs. single multi-task model: Separate models allow specialization but increase parameter count and training complexity.
  - K-Means clustering vs. other clustering methods: K-Means is simple and fast but assumes spherical clusters; density-based methods might capture more complex hierarchies.
  - One-shot vs. few-shot/fine-tuning on full dataset: One-shot forces the model to generalize from minimal examples, which is more challenging but better tests the hierarchical abstraction.
- Failure signatures:
  - Low cosine similarity between embeddings and targets suggests poor feature learning or misaligned clustering.
  - High accuracy on seen classes but poor accuracy on unseen classes indicates overfitting to seen class features without learning generalizable hierarchical patterns.
  - Silhouette scores dropping in later clustering levels suggest clusters are not semantically coherent.
- First 3 experiments:
  1. Run Phase 1 clustering and inspect silhouette scores and cluster visualizations to ensure semantically meaningful groupings.
  2. Train Phase 2 models on a small subset of classes and evaluate contrastive accuracy to verify the training pipeline works.
  3. Test Phase 3 with a small set of unseen classes and manually inspect the cosine similarity mappings to ensure correct parent/super-parent assignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of clusters for each hierarchical level that maximizes classification accuracy while maintaining meaningful semantic groupings?
- Basis in paper: [explicit] The paper mentions silhouette analysis was performed to determine optimal K values, finding 88 clusters for Level 1 and 8 clusters for Level 2, but doesn't explore whether these are truly optimal
- Why unresolved: The paper only presents results for specific K values (88 and 8) without exploring the full range of possibilities or comparing against alternative clustering methods
- What evidence would resolve it: Systematic evaluation of different K values across a wider range, including comparison with DBSCAN, HDBSCAN, or other clustering approaches, measuring both accuracy and semantic coherence

### Open Question 2
- Question: How does the hierarchical contrastive learning approach perform on datasets beyond ImageNet animal classes, particularly for datasets with non-hierarchical or multi-modal relationships?
- Basis in paper: [inferred] The evaluation is limited to animal classes from ImageNet and a custom dataset of 20 unseen animal classes, with no testing on diverse datasets
- Why unresolved: The model's generalization capabilities beyond animal taxonomy remain untested
- What evidence would resolve it: Testing on diverse datasets including non-biological categories, cross-domain datasets, and datasets with complex hierarchical relationships

### Open Question 3
- Question: What is the impact of different feature extraction architectures beyond ResNet-152 on the hierarchical contrastive learning performance?
- Basis in paper: [explicit] The paper uses pre-trained ResNet-152 throughout all phases without exploring alternative architectures
- Why unresolved: The paper doesn't compare performance with other backbone networks like ViT, EfficientNet, or newer architectures
- What evidence would resolve it: Comparative evaluation using different feature extraction models while keeping the contrastive learning framework constant

### Open Question 4
- Question: How does the model's performance degrade as the semantic distance between seen and unseen classes increases?
- Basis in paper: [inferred] The unseen classes are not explicitly evaluated for their semantic similarity to seen classes, and no analysis of performance degradation is provided
- Why unresolved: The paper reports overall accuracy but doesn't analyze how semantic distance affects classification accuracy
- What evidence would resolve it: Systematic evaluation measuring performance based on taxonomic distance between seen and unseen classes, including analysis of confusion matrices to identify specific misclassification patterns

## Limitations

- The method's performance depends heavily on the quality of hierarchical clustering in Phase 1, with no exploration of alternative clustering methods or validation of semantic coherence
- The claim of "one-shot" learning is ambiguous and may not reflect true one-shot learning if multiple examples are used during Phase 2 training
- The approach is only evaluated on animal classes from ImageNet, limiting confidence in its generalizability to other domains or non-hierarchical relationships

## Confidence

**Confidence: Medium** on the hierarchical contrastive mechanism effectiveness. The reported high accuracy on unseen classes is promising, but the methodology relies heavily on Phase 1 clustering quality without sufficient validation.

**Confidence: Low** on the true "one-shot" learning claim due to ambiguity about how many examples are used per class during training.

**Confidence: Medium** on generalizability, as the approach is only tested on animal classes without validation on diverse datasets.

## Next Checks

1. **Cluster quality validation**: Run Phase 1 clustering on the ImageNet animal subset and compute silhouette scores, visualize t-SNE embeddings of the clusters, and manually inspect whether the hierarchical groupings make semantic sense.

2. **One-shot vs few-shot verification**: Examine the training procedure to determine exactly how many examples per class are used during Phase 2 fine-tuning and whether this truly qualifies as one-shot learning.

3. **Unseen class similarity analysis**: For the 20 unseen classes tested in Phase 3, compute the distribution of cosine similarities between each unseen class and its assigned parent/super-parent cluster to identify potential failure cases.