---
ver: rpa2
title: 'Navigating Perplexity: A linear relationship with the data set size in t-SNE
  embeddings'
arxiv_id: '2308.15513'
source_url: https://arxiv.org/abs/2308.15513
tags:
- data
- embedding
- perplexity
- perp
- t-sne
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-level sampling-based approach for computing
  t-SNE embeddings of large datasets. The key insight is that perplexity should scale
  linearly with the number of data points being embedded.
---

# Navigating Perplexity: A linear relationship with the data set size in t-SNE embeddings

## Quick Facts
- arXiv ID: 2308.15513
- Source URL: https://arxiv.org/abs/2308.15513
- Reference count: 26
- Primary result: A two-level sampling-based approach for computing t-SNE embeddings that scales perplexity linearly with dataset size, achieving faster computation without sacrificing embedding quality

## Executive Summary
This paper presents a novel approach for computing t-SNE embeddings of large datasets by leveraging a two-level sampling strategy. The key insight is that perplexity should scale linearly with the number of data points being embedded. By embedding a small random sample with proportionally reduced perplexity and then prolonging the result to the full dataset, the authors achieve significant computational speedups while maintaining or improving embedding quality. Experiments on the MNIST dataset demonstrate that this method produces embeddings with better neighborhood preservation compared to standard t-SNE, particularly for large datasets where high perplexity values would otherwise be computationally infeasible.

## Method Summary
The method employs a two-level approach to t-SNE embedding. First, a uniform random sample of the data is taken at rate ρ, and t-SNE is computed on this sample using perplexity ρPerp. The resulting embedding is then prolonged to the full dataset by placing each unsampled point at the location of its nearest neighbor in the sample. Finally, a few iterations of low-perplexity gradient descent are applied to smooth local inconsistencies. This approach allows the use of larger effective perplexity values on the full dataset while maintaining computational efficiency through the initial coarse embedding.

## Key Results
- Demonstrated that perplexity should scale linearly with dataset size for optimal t-SNE embeddings
- Achieved faster computation times compared to standard t-SNE on large datasets
- Showed improved neighborhood preservation in precision/recall metrics compared to baseline t-SNE
- Validated approach on MNIST dataset with 70,000 images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling rate ρ linearly scales the effective perplexity from coarse to fine embedding
- Mechanism: By uniformly sampling ρn points and using perplexity Perp′ on the coarse level, the geometric neighborhood radius around each point is preserved, effectively simulating the fine-level neighborhood size of ρPerp
- Core assumption: The distribution of points in the high-dimensional space is preserved under uniform random sampling
- Evidence anchors:
  - When restricting to a uniformly chosen subset S of D, picking each point with probability ρ ∈ (0, 1], the expected number of points to lie within the d(xi, x′k)-ball around xi is exactly ρk
  - In the continuous case, a perplexity Perp is given and for each point xi, a bandwidth value σi is chosen according to [equation] such that Perp = Perp(pi) = 2−P j pj|i log(pj|i)
  - No direct corpus evidence for this specific linear scaling claim; authors acknowledge this is derived theoretically but needs Monte Carlo validation
- Break condition: If the data distribution is highly skewed or clustered such that uniform sampling misses critical regions

### Mechanism 2
- Claim: Prolongation followed by low-perplexity smoothing preserves global structure while reducing computational cost
- Mechanism: After embedding a small sample with adjusted perplexity, each fine-level point is placed at its nearest neighbor's location in the coarse embedding, then a few iterations of low-perplexity gradient descent smooth local inconsistencies
- Core assumption: The coarse embedding captures the global data structure sufficiently well that local refinement can fix placement errors
- Evidence anchors:
  - For each point xi /∈ S, find the nearest neighbor to xi in S... Then, we place each such point xi at the embedding location of s
  - Our experiments show that the general global structure of the embedding is already fixed after prolongation... it satisfies to run these steps with a rather low perplexity, e.g., 30
  - No corpus evidence for this specific two-level prolongation approach
- Break condition: If the coarse embedding poorly represents global structure due to insufficient sampling or inappropriate perplexity scaling

### Mechanism 3
- Claim: Two-level approach improves nearest-neighbor preservation compared to baseline t-SNE
- Mechanism: By computing the embedding on a smaller sample with appropriately scaled perplexity, the algorithm can use larger effective perplexity values that would be computationally prohibitive on the full dataset, leading to better neighborhood preservation
- Core assumption: Larger perplexity values on the full dataset would improve neighborhood preservation if computationally feasible
- Evidence anchors:
  - The approach outlined above has the potential to achieve just this. Instead of embedding the full data set, we only embed a small sample set, which reduces computational costs
  - In order to quantify the local quality of our embeddings, we turn to the precision/recall metric... our sampling-based approach achieves better neighborhood preservation than base-line t-SNE
  - No corpus evidence for this specific precision/recall improvement claim
- Break condition: If the sampling introduces significant noise that outweighs the benefits of larger effective perplexity

## Foundational Learning

- Concept: t-SNE perplexity and its relationship to neighborhood size
  - Why needed here: The entire method hinges on understanding how perplexity controls the effective neighborhood size in t-SNE embeddings
  - Quick check question: If you have a dataset of 10,000 points and want to use perplexity 500 on the full dataset, what perplexity should you use on a 1,000-point sample to achieve equivalent results?

- Concept: Uniform random sampling and ε-nets
  - Why needed here: The method relies on uniform random sampling preserving the geometric structure of the data
  - Quick check question: What property of uniform random sampling ensures that every point in the original dataset has a nearby point in the sample with high probability?

- Concept: Multigrid methods and prolongation operators
  - Why needed here: The approach borrows concepts from multigrid solvers, particularly the idea of solving on a coarse grid and prolonging to a fine grid
  - Quick check question: In the context of this paper, what is the purpose of the prolongation step after computing the coarse embedding?

## Architecture Onboarding

- Component map: Input data -> Sampling module -> Coarse embedding -> Prolongation -> Smoothing -> Final embedding
- Critical path: Sampling → Coarse embedding → Prolongation → Smoothing
- Design tradeoffs:
  - Sampling rate ρ: Higher ρ gives better embeddings but increases computation time
  - Perplexity scaling: Linear scaling is simple but may not be optimal for all distributions
  - Number of smoothing iterations: More iterations improve local structure but increase computation time
- Failure signatures:
  - Poor neighborhood preservation in precision/recall metrics
  - Visible artifacts or discontinuities in the final embedding
  - Clusters that are split or merged incorrectly compared to ground truth
- First 3 experiments:
  1. Run baseline t-SNE on MNIST with perplexity 30 and compare to sampling-based approach with ρ=0.1, Perp=300
  2. Vary sampling rate ρ from 0.05 to 0.5 while keeping target perplexity constant, measure precision/recall and computation time
  3. Test on a dataset with known hierarchical structure (e.g., Swiss roll) to verify that global structure is preserved through the sampling process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed sampling-based t-SNE approach perform on data sets significantly larger than MNIST, such as single-cell RNA-seq data with millions of cells?
- Basis in paper: The paper mentions that the approach is particularly useful for large datasets like those from single-cell RNA-seq, but does not provide extensive experiments on such datasets.
- Why unresolved: The paper primarily uses the MNIST dataset for experiments, which is relatively small compared to real-world single-cell data.
- What evidence would resolve it: Experiments on large-scale single-cell RNA-seq datasets demonstrating runtime improvements and embedding quality preservation compared to standard t-SNE.

### Open Question 2
- Question: What is the optimal strategy for choosing the sampling rate ρ for different types of data distributions?
- Basis in paper: The paper suggests that ρ should be chosen small enough for quick coarse embedding computation, but does not provide a systematic method for determining the optimal sampling rate.
- Why unresolved: The paper mentions the need to choose ρ carefully but does not explore how different data characteristics affect the optimal sampling rate.
- What evidence would resolve it: A comprehensive study analyzing the relationship between data distribution properties and optimal sampling rates, potentially through empirical experiments or theoretical analysis.

### Open Question 3
- Question: Can the linear relationship between perplexity and dataset size be extended to other dimensionality reduction techniques beyond t-SNE?
- Basis in paper: The paper focuses exclusively on t-SNE and mentions that UMAP uses stochastic gradient descent, making it difficult to establish a similar relationship.
- Why unresolved: The paper does not explore whether the linear scaling principle could apply to other non-linear dimensionality reduction methods.
- What evidence would resolve it: Experiments applying similar sampling-based approaches to other dimensionality reduction techniques (e.g., LargeVis, TriMap) to determine if a linear relationship between perplexity and dataset size exists.

## Limitations
- Theoretical foundation relies on strong assumptions about uniform data distribution that may not hold in practice
- Linear scaling relationship between perplexity and dataset size lacks experimental validation through Monte Carlo experiments
- Limited experimental validation beyond the MNIST dataset

## Confidence

- **Low confidence**: The claim that perplexity should scale linearly with dataset size. While theoretically derived, this lacks experimental validation and may not hold for non-uniform data distributions.
- **Medium confidence**: The two-level prolongation approach improves computational efficiency. This is supported by the experimental results on MNIST, though the sample size is limited.
- **Medium confidence**: The precision/recall metric shows improved neighborhood preservation. The experimental results are promising, but the comparison methodology could be more rigorous.

## Next Checks

1. **Distribution Sensitivity Analysis**: Test the method on datasets with varying density distributions (e.g., mixture models, datasets with outliers) to evaluate how well the linear scaling assumption holds.

2. **Perplexity Scaling Validation**: Conduct Monte Carlo experiments to empirically verify the relationship between sampling rate and effective perplexity across different data distributions.

3. **Generalization to Other Datasets**: Apply the method to datasets beyond MNIST (e.g., CIFAR-10, gene expression data) with known ground truth structures to validate the neighborhood preservation claims.