---
ver: rpa2
title: Counterfactual Explanation Policies in RL
arxiv_id: '2307.13192'
source_url: https://arxiv.org/abs/2307.13192
tags:
- policy
- counterfactual
- policies
- learning
- rtarget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COUNTERPOL, a framework for generating counterfactual
  explanations of RL policies by identifying minimal changes to the policy that achieve
  a desired target return. The authors formalize the problem and propose an optimization
  objective combining return penalty and KL-divergence regularization.
---

# Counterfactual Explanation Policies in RL

## Quick Facts
- arXiv ID: 2307.13192
- Source URL: https://arxiv.org/abs/2307.13192
- Authors: 
- Reference count: 13
- Key outcome: Introduces COUNTERPOL framework for generating counterfactual explanations of RL policies by identifying minimal changes to achieve target returns while maintaining similarity to original policy

## Executive Summary
This paper introduces COUNTERPOL, a framework for generating counterfactual explanations of reinforcement learning policies by identifying minimal changes that achieve desired target returns. The authors formalize the problem as optimizing a weighted sum of return penalty and KL-divergence regularization, and establish a theoretical connection to trust region-based policy optimization methods. Experiments on five OpenAI Gym environments demonstrate the framework's ability to generate counterfactual policies for both learning and unlearning skills while maintaining proximity to original policies.

## Method Summary
The paper proposes optimizing counterfactual policies using on-policy Monte Carlo policy gradients with iterative KL-pivoting. The objective combines return penalty (to achieve target performance) with KL-divergence regularization (to maintain similarity to the original policy). The optimization alternates between gradient updates and KL-divergence checks, adjusting the policy parameters to minimize the objective while keeping changes bounded. The method is evaluated on five OpenAI Gym environments using trained A2C and PPO policies as starting points.

## Key Results
- COUNTERPOL successfully generates counterfactual policies achieving target returns across five OpenAI Gym environments
- The framework enables both learning (increasing return) and unlearning (decreasing return) of skills
- Generated counterfactual policies remain close to original policies while achieving desired performance targets
- Theoretical connection established between COUNTERPOL and trust region-based policy optimization methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual policy optimization uses weighted sum of return penalty and KL-divergence
- Mechanism: Return penalty ensures target performance while KL-divergence maintains policy similarity
- Core assumption: Weighted sum provides well-behaved optimization landscape
- Evidence anchors: Objective formulation and optimization details in section 4

### Mechanism 2
- Claim: On-policy Monte Carlo policy gradients effectively optimize counterfactual objective
- Mechanism: Gradients estimated from sampled trajectories guide policy updates
- Core assumption: Unbiased gradient estimates enable effective optimization
- Evidence anchors: Policy gradient theorem application in section 4.1

### Mechanism 3
- Claim: Equivalence to trust region methods under specific conditions
- Mechanism: ℓ1-norm return penalty with Rtarget = Rmax yields trust region formulation
- Core assumption: Equivalence holds when maximum return target is used
- Evidence anchors: Theorem 4.1 proof in section 4.2

## Foundational Learning

- Concept: Counterfactual explanations in supervised learning
  - Why needed here: Provides foundation for extending explanations to RL policies
  - Quick check question: What's the main difference between supervised and RL counterfactual explanations?

- Concept: Trust region-based policy optimization methods
  - Why needed here: Theoretical connection explains optimization properties
  - Quick check question: How do trust region methods ensure monotonic improvement?

- Concept: Policy gradient theorem
  - Why needed here: Enables gradient computation for counterfactual optimization
  - Quick check question: What's the main idea behind policy gradient theorem?

## Architecture Onboarding

- Component map: Original policy π0 -> Counterfactual policy πcf -> Objective function (return penalty + KL-divergence) -> Optimization algorithm (on-policy Monte Carlo policy gradients) -> Environments (OpenAI Gym)

- Critical path: 1) Initialize original policy and target return, 2) Compute objective using sampled trajectories, 3) Estimate gradients using policy gradient theorem, 4) Update policy parameters, 5) Iterate until target achieved

- Design tradeoffs:
  - KL regularization weight: Higher ensures similarity but may hinder target achievement
  - Number of KL-pivoting iterations: More iterations allow finer adjustments but increase cost
  - Return penalty norm: ℓ1 provides linear penalty while ℓ2 penalizes larger deviations more heavily

- Failure signatures:
  - Target return consistently missed: KL weight too high or optimization not converging
  - Large deviations from original policy: KL weight too low or optimization landscape issues
  - Unstable optimization: High gradient variance from insufficient sampling

- First 3 experiments:
  1. Test on simple control task with known optimal policy to verify target achievement
  2. Test robustness to different KL weights and iteration counts across environments
  3. Compare with alternative explanation techniques like saliency maps

## Open Questions the Paper Calls Out

- Question: How to adapt COUNTERPOL to hierarchical RL settings with multiple policy levels?
  - Basis: Paper mentions exploring unlearning options in hierarchical RL would be interesting
  - Why unresolved: Current work focuses on standard RL environments
  - Evidence needed: Empirical results in hierarchical RL settings

- Question: Can COUNTERPOL handle non-stochastic policies and deterministic action selection?
  - Basis: Framework assumes stochastic policies throughout
  - Why unresolved: Practical RL often uses deterministic policies
  - Evidence needed: Modified framework for deterministic policies

- Question: What are optimal strategies for selecting regularization hyperparameter k?
  - Basis: k treated as manually tuned hyperparameter
  - Why unresolved: No systematic strategy provided
  - Evidence needed: Principled hyperparameter selection methods

## Limitations

- Theoretical equivalence relies on specific conditions (ℓ1-norm, Rtarget = Rmax) that may not hold in practice
- Empirical evaluation lacks comprehensive ablation studies on component importance
- Scalability to high-dimensional continuous control tasks remains unexplored

## Confidence

- High Confidence: Basic formulation and Monte Carlo optimization is theoretically sound
- Medium Confidence: Trust region equivalence proven but practical implications need investigation
- Medium Confidence: Empirical results show method works but analysis could be more comprehensive

## Next Checks

1. Conduct ablation studies to evaluate impact of KL weight, KL-pivoting iterations, and return penalty norm
2. Extend evaluation to challenging continuous control tasks with higher-dimensional spaces
3. Compare COUNTERPOL with alternative explanation techniques on common tasks