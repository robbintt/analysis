---
ver: rpa2
title: 'GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction'
arxiv_id: '2311.14786'
source_url: https://arxiv.org/abs/2311.14786
tags:
- pedestrian
- crossing
- behavior
- gpt-4v
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study is the first to comprehensively evaluate GPT-4V(ision)
  for pedestrian behavior prediction in autonomous driving using publicly available
  datasets (JAAD and PIE). The model achieved a 57% accuracy in predicting pedestrian
  crossing actions, which is lower than state-of-the-art domain-specific models (70%).
---

# GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction

## Quick Facts
- **arXiv ID**: 2311.14786
- **Source URL**: https://arxiv.org/abs/2311.14786
- **Reference count**: 40
- **Primary result**: GPT-4V achieves 57% accuracy in pedestrian crossing prediction, lower than state-of-the-art domain-specific models (70%)

## Executive Summary
This study is the first comprehensive evaluation of GPT-4V(ision) for pedestrian behavior prediction in autonomous driving using publicly available datasets (JAAD and PIE). The model demonstrates promise in interpreting complex traffic scenarios and understanding pedestrian behaviors through zero-shot learning, achieving 57% accuracy in predicting pedestrian crossing actions. However, it struggles with smaller pedestrians and assessing relative motion between pedestrians and vehicles. The findings highlight both the potential and limitations of VLMs in autonomous driving applications, suggesting the need for further research and development in this area.

## Method Summary
The study employs a zero-shot evaluation approach, using GPT-4V(ision) to predict pedestrian behaviors without domain-specific fine-tuning. Researchers use predefined prompts and structured JSON output format to analyze JAAD and PIE datasets containing pedestrian behavior annotations, ego-vehicle dynamics, and contextual information. The evaluation includes batch analysis on JAAD with varying input parameters and qualitative analysis through interactive dialogues with ChatGPT using GPT-4V on all three datasets. Performance is measured using standard classification metrics including accuracy, AUC, F1 score, precision, and recall.

## Key Results
- GPT-4V achieves 57% accuracy in predicting pedestrian crossing actions on JAAD dataset
- Model shows promise in interpreting complex traffic scenarios and understanding group behaviors
- Performance significantly degrades for smaller pedestrians (<1% of image area) and when relative motion between pedestrians and vehicles is ambiguous

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4V can interpret complex traffic scenarios and understand pedestrian behaviors in zero-shot manner
- **Mechanism**: The model leverages its multimodal training to extract contextual cues from images, apply reasoning about pedestrian-vehicle interactions, and output structured predictions in JSON format
- **Core assumption**: The visual-linguistic pretraining of GPT-4V provides sufficient common sense and scene understanding to generalize to pedestrian behavior prediction without domain-specific fine-tuning
- **Evidence anchors**: 
  - [abstract] "GPT-4V shows promise in interpreting complex traffic scenarios and understanding pedestrian behaviors"
  - [section III-A] Quantitative results showing 57% accuracy on JAAD dataset in zero-shot manner
  - [corpus] Weak evidence - related works focus on trajectory prediction, not VLM-based approaches
- **Break condition**: Performance degrades significantly when pedestrians are small (<1% of image area) or when relative motion between pedestrian and vehicle is ambiguous

### Mechanism 2
- **Claim**: GPT-4V can detect and analyze groups of pedestrians, understanding collective behaviors
- **Mechanism**: The model uses spatial relationships and proximity cues to identify groups, then applies behavioral reasoning to individual members and the group as a whole
- **Core assumption**: The model's training data included sufficient examples of social group interactions to enable this capability
- **Evidence anchors**:
  - [section III-B.3] Qualitative results showing GPT-4V detecting groups and providing insights about group formation and individual actions
  - [section III-A] Quantitative results showing group detection capabilities in WiDEVIEW dataset
  - [corpus] Weak evidence - related works focus on individual pedestrian prediction, not group analysis with VLMs
- **Break condition**: Model struggles with occluded individuals within groups or when group boundaries are ambiguous

### Mechanism 3
- **Claim**: GPT-4V's performance depends on pedestrian size in the image, with larger pedestrians yielding more accurate predictions
- **Mechanism**: Larger pedestrians provide more visual detail and context, allowing the model to better discern subtle behavioral cues and motion
- **Core assumption**: The model's visual processing capabilities scale with the amount of visual information available per pedestrian
- **Evidence anchors**:
  - [section III-A.4] Figure 2 showing accuracy increases with pedestrian size for crossing and action behaviors
  - [section III-A.5] Figure 4 showing prediction accuracy correlates with pedestrian size in future frame analysis
  - [corpus] No direct evidence - this appears to be a novel finding specific to this work
- **Break condition**: Performance plateaus or degrades for extremely large pedestrians due to perspective distortion or occlusion of contextual information

## Foundational Learning

- **Concept**: Multimodal pretraining and zero-shot learning
  - **Why needed here**: GPT-4V is being evaluated without domain-specific fine-tuning, relying on its general visual-linguistic understanding
  - **Quick check question**: What enables a model to perform tasks it wasn't explicitly trained for on specific datasets?

- **Concept**: Pedestrian behavior prediction metrics (accuracy, precision, recall, F1, ROC AUC)
  - **Why needed here**: The paper evaluates model performance using these standard classification metrics
  - **Quick check question**: How do precision and recall differ in evaluating pedestrian crossing prediction?

- **Concept**: Relative motion analysis in autonomous driving
  - **Why needed here**: The paper highlights GPT-4V's difficulty in accounting for relative motion between pedestrians and vehicles
  - **Quick check question**: Why is relative motion between pedestrian and vehicle important for predicting crossing behavior?

## Architecture Onboarding

- **Component map**: OpenAI API client → GPT-4V(ision) model → Image preprocessing → Prompt engineering → JSON response parsing → Evaluation metrics calculation
- **Critical path**: Image input → Prompt construction → Model inference → Response validation → Metric computation
- **Design tradeoffs**: Zero-shot approach vs. fine-tuning (speed and generalization vs. accuracy), single prompt vs. multi-step reasoning (simplicity vs. complexity handling)
- **Failure signatures**: Low accuracy on small pedestrians, inconsistent outputs across runs, confusion between parallel walking and crossing, failure to account for relative motion
- **First 3 experiments**:
  1. Replicate quantitative evaluation on JAAD dataset with different prompt formulations to test sensitivity
  2. Test model performance on PIE dataset to evaluate generalization to different data distributions
  3. Implement a simple relative motion compensation in the prompt to see if explicit instructions improve crossing detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does GPT-4V's performance in pedestrian behavior prediction compare to future versions of VLMs specifically trained for autonomous driving tasks?
- **Basis in paper**: [explicit] The paper states that GPT-4V achieves 57% accuracy, which is lower than state-of-the-art domain-specific models (70%). It suggests that further research and development is needed.
- **Why unresolved**: This study only evaluates GPT-4V. Future VLMs specifically designed and trained for autonomous driving may show improved performance.
- **What evidence would resolve it**: Comparative studies evaluating newer VLMs trained specifically for autonomous driving tasks against both GPT-4V and domain-specific models.

### Open Question 2
- **Question**: What is the minimum pedestrian size threshold that GPT-4V can reliably detect and analyze for behavior prediction?
- **Basis in paper**: [explicit] The paper notes that GPT-4V struggles with smaller pedestrians and that accuracy increases with pedestrian size, with better performance observed when the area ratio exceeds 1.0%.
- **Why unresolved**: The paper provides general trends but does not specify an exact size threshold for reliable detection and analysis.
- **What evidence would resolve it**: Systematic experiments testing GPT-4V's performance across a range of pedestrian sizes to determine the minimum reliable detection threshold.

### Open Question 3
- **Question**: How can GPT-4V be adapted or fine-tuned to better handle the relative motion between pedestrians and vehicles?
- **Basis in paper**: [explicit] The paper identifies challenges in assessing relative motion between pedestrians and vehicles, leading to incorrect behavior predictions.
- **Why unresolved**: The paper identifies the problem but does not propose specific solutions or adaptations to address this limitation.
- **What evidence would resolve it**: Experiments testing various fine-tuning approaches or architectural modifications to improve GPT-4V's handling of relative motion in dynamic scenarios.

## Limitations

- GPT-4V's 57% accuracy is significantly lower than state-of-the-art domain-specific models (70%), raising questions about real-world deployment viability
- The model struggles with smaller pedestrians (<1% of image area) and assessing relative motion between pedestrians and vehicles, which are common scenarios in autonomous driving
- Zero-shot approach may not leverage the full potential of VLMs compared to fine-tuned alternatives, and entropy measurements indicate output variability concerns

## Confidence

**High confidence**: Findings related to GPT-4V's general scene understanding capabilities and group detection abilities, supported by consistent qualitative and quantitative evidence across multiple datasets

**Medium confidence**: Relative performance comparison between GPT-4V and state-of-the-art models, as the study only evaluates a single VLM approach and baseline comparisons may not capture the full landscape of pedestrian prediction methods

**Low confidence**: Absolute accuracy numbers for real-world deployment, given the significant performance gap with specialized models and entropy measurements indicating output variability

## Next Checks

1. **Cross-dataset generalization test**: Evaluate GPT-4V performance on additional pedestrian datasets (e.g., nuScenes, Cityscapes) to assess whether the observed size-dependent performance patterns hold across different environmental conditions and camera perspectives

2. **Prompt engineering optimization**: Systematically test variations in prompt structure, including explicit instructions for relative motion compensation and multi-step reasoning approaches, to determine if performance can be improved without fine-tuning

3. **Safety-critical scenario analysis**: Conduct targeted evaluation on edge cases including occluded pedestrians, ambiguous group boundaries, and challenging lighting conditions to quantify failure modes in scenarios most relevant to autonomous vehicle safety requirements