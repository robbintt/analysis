---
ver: rpa2
title: 'VerityMath: Advancing Mathematical Reasoning by Self-Verification Through
  Unit Consistency'
arxiv_id: '2311.07172'
source_url: https://arxiv.org/abs/2311.07172
tags:
- unit
- units
- consistency
- each
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic approach for enhancing the reasoning
  skills of large language models (LLMs) in mathematical word problems. The proposed
  method, called VerityMath, focuses on unit consistency by defining units for each
  quantity and ensuring their consistency during mathematical operations.
---

# VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency

## Quick Facts
- arXiv ID: 2311.07172
- Source URL: https://arxiv.org/abs/2311.07172
- Reference count: 8
- Key outcome: VerityMath slightly underperforms compared to approach without unit consistency

## Executive Summary
This paper presents a systematic approach for enhancing the reasoning skills of large language models (LLMs) in mathematical word problems. The proposed method, called VerityMath, focuses on unit consistency by defining units for each quantity and ensuring their consistency during mathematical operations. The authors developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. They fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce their VerityMath variants. The results indicate that VerityMath slightly underperforms compared to an approach that does not incorporate unit consistency. However, the authors suggest future improvements to address the limitations of UCPs and further enhance the reasoning capabilities of LLMs.

## Method Summary
VerityMath uses a unit consistency approach to enhance LLM mathematical reasoning by defining units for each quantity and ensuring their consistency during operations. The method employs Unit Consistency Programs (UCPs) - an annotated dataset of math word problems paired with programs containing unit specifications and verification routines using Python Counter objects and assert statements. The authors fine-tuned Code Llama 7B, Llama 2 7B, and Mistral 7B models with UCPs using QLoRA technique (learning rate 2e-4, batch size 32, max context length 1024, 20 epochs) to generate unit-consistent programs that can verify their own calculations during execution.

## Key Results
- VerityMath achieves 43.17% accuracy on GSM8K test set, slightly underperforming the PAL baseline at 53.62%
- 40% of GSM8K problems involve multiple units, highlighting the complexity of unit-consistent reasoning
- Fine-tuned models generate programs with Counter objects and assert statements for unit verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Counter class and assert statements provide a systematic way to catch unit mismatches during intermediate computation steps, preventing error propagation.
- Mechanism: The Counter objects track dimensional units (e.g., "hours": 1, "days": -1) for each variable. Assert statements after each operation verify that the resulting unit matches the expected outcome. For example, in a multiplication like `pizzas_total = people_total * pizza_slices_per_person`, the assert ensures that the resulting unit is "slices" by checking `Counter({"slices": 1}) == Counter({"people": 1}) + Counter({"slices": 1, "people": -1})`.
- Core assumption: Unit mismatches are a significant source of errors in multi-step math problems, and catching them early prevents cascading mistakes.
- Evidence anchors:
  - [abstract] "We propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations."
  - [section] "These assert statements verify the consistency of units within the equation and can trigger an assert error when inconsistent units are detected."
  - [corpus] "Found 25 related papers... Top related titles: UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"

### Mechanism 2
- Claim: The fine-tuning approach with GPT-4 generated Unit Consistency Programs (UCPs) teaches the model to incorporate unit tracking into its reasoning pipeline.
- Mechanism: GPT-4 is used with few-shot prompting to generate UCPs for the first 1k training examples. These UCPs include Counter objects and assert statements, which are then used to fine-tune smaller models like Code Llama 7B. This trains the model to generate programs with unit consistency checks embedded.
- Core assumption: Exposing the model to UCPs during fine-tuning will improve its ability to generate unit-consistent programs independently.
- Evidence anchors:
  - [section] "We conducted our experiments mainly on GSM8K... We perform few-shot prompting with GPT-4... for the first 1k examples in the training set using 6 manually crafted UCPs samples."
  - [section] "Using our annotated dataset Duc, we adopt standard causal language modeling to fine-tune smaller models."
  - [corpus] Weak evidence - no direct citations about GPT-4 fine-tuning for unit consistency in the neighbor papers.

### Mechanism 3
- Claim: The distinction between single-unit and multi-unit problems reveals where unit consistency checking is most beneficial.
- Mechanism: By categorizing problems into those with a single unit (e.g., counting marbles) and those with multiple units (e.g., hours, days, weeks), the approach can focus on the more error-prone multi-unit problems where unit mismatches are likely.
- Core assumption: Problems involving multiple units are inherently more challenging and prone to errors due to the need for unit conversions and consistency checks.
- Evidence anchors:
  - [section] "We observe that LLMs struggle with math word problems that consist of quantities spanning multiple types or units... we used ChatGPT-3.5-turbo to categorize both the train and test datasets into two distinct categories."
  - [section] "We see that around 40% of the problems in the train and test set involve multiple units."
  - [corpus] No direct evidence in neighbor papers about categorizing problems by unit complexity.

## Foundational Learning

- Concept: Unit tracking in mathematical operations
  - Why needed here: To ensure that mathematical operations between quantities with different units are valid and to prevent errors like adding "hours" to "days" without conversion.
  - Quick check question: What is the expected unit of the result when multiplying a quantity in "meters" by a quantity in "seconds"?
    Answer: The result should have units of "meter-seconds", which is a compound unit indicating a different physical quantity (e.g., momentum if mass is involved).

- Concept: Python Counter objects for unit representation
  - Why needed here: The Counter class is used to represent units in a way that supports addition and subtraction, which correspond to multiplication and division of units, respectively.
  - Quick check question: How would you represent "miles per hour" using a Counter?
    Answer: Counter({"miles": 1, "hours": -1})

- Concept: Assert statements for runtime verification
  - Why needed here: Assert statements are used to enforce unit consistency at runtime, immediately flagging any mismatches that occur during the computation.
  - Quick check question: What happens when an assert statement fails during program execution?
    Answer: The program raises an AssertionError and stops execution, indicating a unit inconsistency.

## Architecture Onboarding

- Component map: GPT-4 -> UCP dataset -> Fine-tuning (Code Llama 7B) -> VerityMath model -> Unit-consistent program generation
- Critical path: 1) GPT-4 generates UCPs for training examples 2) UCPs are filtered to include only correct solutions 3) Fine-tuning on the UCP dataset 4) Model generates programs for new problems 5) Programs execute with unit verification
- Design tradeoffs:
  - Accuracy vs. complexity: UCPs add complexity (Counter objects, assert statements) which may increase the chance of errors in program generation
  - Dataset size vs. quality: Using only the first 1k examples for GPT-4 annotation limits the size of the fine-tuning dataset but ensures higher quality annotations
  - Unit granularity: Fine-grained unit tracking (e.g., distinguishing "dollars per book" from "dollars") vs. coarser tracking for simplicity
- Failure signatures:
  - Programs that fail to execute due to syntax errors
  - Programs that pass syntax but fail unit consistency checks (assert errors)
  - Incorrect final answers despite passing unit checks
  - Inability to generate UCPs for new problems
- First 3 experiments:
  1. Manually create UCPs for a small set of problems and verify that the Counter objects and assert statements correctly catch unit mismatches.
  2. Fine-tune a small model on a hand-annotated UCP dataset and evaluate its ability to generate unit-consistent programs on a held-out test set.
  3. Compare the performance of the fine-tuned model on single-unit vs. multi-unit problems to validate the focus on the more challenging category.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would increasing the size of the annotated dataset significantly improve the performance of VerityMath on multi-unit problems?
- Basis in paper: [inferred] The authors note that UCPs add extra complexity and length to programs, and suggest that the current dataset size (9.6% of GSM8K) may be insufficient for reliable Counter instance generation.
- Why unresolved: The paper only used the first 1k examples due to budget constraints, limiting the ability to test the impact of dataset size on performance.
- What evidence would resolve it: Fine-tuning VerityMath with a larger annotated dataset (e.g., 50% or more of GSM8K) and measuring the accuracy improvement on multi-unit problems compared to the current 37.5% overall accuracy.

### Open Question 2
- Question: How does the performance of VerityMath compare to traditional program-aided approaches when evaluated on problems specifically designed to test unit consistency?
- Basis in paper: [explicit] The authors observe that VerityMath underperforms compared to PAL-based approaches, particularly on multi-unit problems, but suggest unit consistency could still be valuable for error detection.
- Why unresolved: The current evaluation uses GSM8K, which wasn't designed to test unit consistency, making it difficult to isolate the benefits of