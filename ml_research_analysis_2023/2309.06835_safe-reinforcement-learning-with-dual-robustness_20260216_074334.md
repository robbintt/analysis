---
ver: rpa2
title: Safe Reinforcement Learning with Dual Robustness
arxiv_id: '2309.06835'
source_url: https://arxiv.org/abs/2309.06835
tags:
- safety
- policy
- robust
- which
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for safe and robust reinforcement
  learning that simultaneously handles safety constraints and performance robustness
  against disturbances. The authors formulate a constrained zero-sum Markov game where
  the objective is twofold: maximize rewards inside the robust invariant set while
  maintaining safety, and minimize constraint violation outside this set.'
---

# Safe Reinforcement Learning with Dual Robustness

## Quick Facts
- arXiv ID: 2309.06835
- Source URL: https://arxiv.org/abs/2309.06835
- Authors: 
- Reference count: 38
- Primary result: DRAC achieves higher rewards while maintaining safety under adversarial disturbances compared to SAC, RSAC, SAC-Lag, and RAC baselines

## Executive Summary
This paper introduces a framework for safe and robust reinforcement learning that simultaneously handles safety constraints and performance robustness against disturbances. The authors formulate a constrained zero-sum Markov game where the objective is twofold: maximize rewards inside the robust invariant set while maintaining safety, and minimize constraint violation outside this set. A dual policy iteration scheme is proposed, jointly optimizing a task policy and a safety policy, with convergence guarantees proved through monotone contraction properties and Bellman operator analysis. The method is implemented as DRAC, a deep RL algorithm that uses adversarial networks and Lagrange multipliers for practical learning. Experimental results on CartPole, RacingCar, and Walker2D show that DRAC maintains persistent safety under both safety and performance attacks while achieving higher rewards than baseline methods including SAC, RSAC, SAC-Lag, and RAC.

## Method Summary
The paper proposes Dually Robust Actor-Critic (DRAC), a deep RL algorithm that addresses safe and robust reinforcement learning through dual policy iteration. The method formulates the problem as a constrained zero-sum Markov game with two types of adversaries: performance adversaries (µ) that attack rewards and safety adversaries (µh) that attack safety constraints. DRAC maintains two separate policies - a task policy (π) for maximizing rewards and a safety policy (πh) for maintaining safety - optimized through a four-step iteration process: safety policy evaluation, safety policy improvement, task policy evaluation, and task policy improvement. The algorithm uses Lagrange multiplier networks to handle infinite safety constraints in continuous state and action spaces, and adversarial training to build robustness against worst-case disturbances. The method is implemented using actor-critic architectures with separate value networks for performance (Q1, Q2) and safety (Qh).

## Key Results
- DRAC achieves higher cumulative rewards than SAC, RSAC, SAC-Lag, and RAC across all tested environments
- Maintains safety constraint satisfaction under both safety and performance adversarial attacks
- Demonstrates persistent safety in worst-case scenarios where baselines frequently violate constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual policy iteration scheme converges to optimal policies because safety policy optimization contracts the safety value function and task policy optimization contracts the value function on the robust invariant set.
- Mechanism: The safety policy evaluation step uses a monotone contraction operator (Th) on the safety value function, which guarantees convergence to a unique fixed point. The task policy improvement step transforms the safety constraints into state-dependent action spaces, enabling standard policy improvement on the induced zero-sum Markov game.
- Core assumption: The discount factor γh is sufficiently close to 1 for the safety operator to approximate the original safety values, and the robust invariant set Sπh_r is expanding during safety policy iteration.
- Evidence anchors:
  - [abstract] "The convergence of safety policy is established by exploiting the monotone contraction property of safety self-consistency operators"
  - [section] "The convergence of this iteration scheme is proved by separately discussing different behavioral properties of safety policy and task policy"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.526, average citations=0.0. Top related titles: Robust Safe Reinforcement Learning under Adversarial Disturbances, Action Robust Reinforcement Learning via Optimal Adversary Aware Policy Optimization, Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving.
- Break condition: If the discount factor γh is too small, the safety operator no longer approximates the original safety values, breaking the convergence guarantee.

### Mechanism 2
- Claim: The twofold objective formulation allows the algorithm to handle feasibility and optimality separately in worst-case scenarios.
- Mechanism: For states inside the robust invariant set S*_r, the algorithm maximizes total rewards while satisfying safety constraints. For states outside S*_r, the algorithm optimizes safety values to drive the system back to S*_r, since pursuing rewards in this region is meaningless under worst-case attacks.
- Core assumption: The constraint function h(x) = min{h1(x), h2(x), ...} correctly captures all safety requirements, and the system dynamics are deterministic.
- Evidence anchors:
  - [abstract] "For states inside the maximal robust invariant set, the goal is to pursue rewards under the condition of guaranteed safety; for states outside the maximal robust invariant set, the goal is to reduce the extent of constraint violation"
  - [section] "The crux is to propose an iteration scheme for this constrained optimization problem, prove its convergence, and design a deep RL algorithm for practical implementation"
  - [corpus] Weak evidence - no direct citations found for this specific twofold objective formulation.
- Break condition: If the safety constraints are not properly specified or the system dynamics are stochastic, the twofold objective may not correctly separate feasibility and optimality.

### Mechanism 3
- Claim: The Lagrange multiplier network enables handling infinite safety constraints in continuous state and action spaces.
- Mechanism: Instead of explicitly enforcing infinite constraints, the Lagrange multiplier network λ(x; ζ) learns to weight the safety constraint violation in the Lagrangian, allowing the algorithm to focus on critical safety violations while optimizing performance.
- Core assumption: The Lagrange multiplier network can effectively learn the appropriate weights for different states and actions.
- Evidence anchors:
  - [section] "Since there are infinite constraints on the task policy, we also introduce a Lagrange multiplier network to facilitate the constrained optimization process"
  - [section] "The loss function of the Lagrange multiplier network is composed of two parts, i.e., Lλ(ζ) = L_A^λ(ζ) + L_B^λ(ζ)"
  - [corpus] Weak evidence - no direct citations found for this specific Lagrange multiplier network approach in RL.
- Break condition: If the Lagrange multiplier network fails to learn appropriate weights, the algorithm may either over-constrain or under-constrain the safety requirements.

## Foundational Learning

- Concept: Constrained optimization with Lagrange multipliers
  - Why needed here: To handle the infinite safety constraints in continuous state and action spaces
  - Quick check question: What is the role of the Lagrange multiplier in constrained optimization?

- Concept: Zero-sum Markov games
  - Why needed here: To model the interaction between the protagonist (control inputs) and the adversary (external disturbances)
  - Quick check question: How does a zero-sum Markov game differ from a standard Markov Decision Process?

- Concept: Hamilton-Jacobi reachability analysis
  - Why needed here: To compute the robust invariant set and safety value functions
  - Quick check question: What is the relationship between safety value functions and invariant sets?

## Architecture Onboarding

- Component map:
  - Task policy network (π)
  - Safety policy network (πh)
  - Performance adversary network (µ)
  - Safety adversary network (µh)
  - Performance value networks (Q1, Q2)
  - Safety value network (Qh)
  - Lagrange multiplier network (λ)

- Critical path: Safety policy evaluation → Safety policy improvement → Task policy evaluation → Task policy improvement

- Design tradeoffs:
  - Using two adversary networks increases computational complexity but enables handling both performance and safety attacks
  - The Lagrange multiplier network approximates infinite constraints but may introduce approximation errors
  - The dual policy iteration scheme ensures convergence but requires multiple iterations per update

- Failure signatures:
  - Safety constraints are frequently violated during evaluation
  - Performance degrades significantly under adversarial attacks
  - The algorithm fails to converge or converges slowly

- First 3 experiments:
  1. Test the algorithm on a simple CartPole environment with no adversaries to verify basic functionality
  2. Introduce safety adversaries only to evaluate safety-preserving capability
  3. Introduce performance adversaries only to evaluate reward-seeking robustness
  4. Test the full algorithm with both types of adversaries on the RacingCar environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the algorithm perform in environments with continuous state and action spaces compared to discrete spaces?
- Basis in paper: [explicit] The paper mentions that DRAC is designed for continuous state and action spaces, but does not provide extensive comparisons between discrete and continuous environments.
- Why unresolved: The experimental results focus on specific continuous environments (CartPole, RacingCar, Walker2D) without comparing to discrete space benchmarks.
- What evidence would resolve it: Comparative experiments showing performance across both discrete and continuous state/action space environments.

### Open Question 2
- Question: What is the impact of different hyperparameters (e.g., learning rates, discount factors) on the convergence and performance of DRAC?
- Basis in paper: [inferred] The paper mentions hyperparameters like γh and learning rates but does not provide a detailed sensitivity analysis.
- Why unresolved: The experimental section does not include ablation studies or hyperparameter tuning results.
- What evidence would resolve it: Systematic hyperparameter sensitivity analysis showing performance across different settings.

### Open Question 3
- Question: How does DRAC scale to high-dimensional state spaces with complex dynamics?
- Basis in paper: [inferred] While DRAC is tested on moderate-dimensional environments, the paper does not address scalability to very high-dimensional systems.
- Why unresolved: The experimental environments (CartPole, RacingCar, Walker2D) have relatively low-dimensional state spaces compared to real-world applications.
- What evidence would resolve it: Experiments on high-dimensional environments demonstrating scalability and performance retention.

## Limitations

- The convergence guarantees rely heavily on deterministic system dynamics and specific properties of discount factors, which may not hold in all practical scenarios
- Computational complexity of maintaining dual policies and adversarial networks could limit scalability to high-dimensional problems
- Performance improvements, while statistically significant, show relatively modest gains in some environments (e.g., 200-300 reward difference in Walker2D)

## Confidence

- High confidence: The dual policy iteration convergence proof and the general framework architecture
- Medium confidence: The practical implementation details and hyperparameter choices, as these are not fully specified in the paper
- Medium confidence: The experimental results, particularly the relative performance differences between baselines

## Next Checks

1. Test DRAC on additional continuous control benchmarks (e.g., Humanoid, Ant) to assess scalability and generalization across different task complexities
2. Conduct ablation studies removing either the safety policy optimization or the adversarial training to quantify their individual contributions to overall performance
3. Evaluate the algorithm's sensitivity to hyperparameter choices (discount factors, learning rates, network architectures) to establish robustness to implementation details