---
ver: rpa2
title: 'Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize
  and Function Splitting in ERM'
arxiv_id: '2307.14528'
source_url: https://arxiv.org/abs/2307.14528
tags:
- have
- lemma
- then
- proof
- fuval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops variants of SGD with adaptive step sizes that
  use sampled loss values, specifically for solving empirical risk minimization problems.
  It introduces the SPS+ method, a variant of the stochastic Polyak stepsize that
  enforces positivity of the step size, and proves it achieves the best known convergence
  rates for SGD in the Lipschitz non-smooth setting.
---

# Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM

## Quick Facts
- arXiv ID: 2307.14528
- Source URL: https://arxiv.org/abs/2307.14528
- Reference count: 40
- Key outcome: Introduces SPS+ (adaptive SGD with Polyak stepsize) and FUVAL (function value learning method), but FUVAL shows no theoretical or practical advantages over standard SGD.

## Executive Summary
This paper develops variants of SGD with adaptive step sizes for empirical risk minimization. The SPS+ method enforces positivity in the Polyak stepsize to achieve optimal convergence rates in the Lipschitz non-smooth setting. The FUVAL method learns loss values at optimality gradually instead of requiring them upfront. While FUVAL is analyzed from three viewpoints (projection, prox-linear, online SGD), the analysis shows no advantage over standard SGD, with only minor full batch advantages in step size sensitivity. The authors conjecture that large mini-batches are needed for FUVAL to be competitive.

## Method Summary
The paper introduces two main methods. SPS+ is a variant of the stochastic Polyak stepsize that enforces positivity of step sizes by taking the positive part of the numerator, achieving optimal convergence rates under star-convexity assumptions. FUVAL is a method where loss values at optimality are learned gradually through slack variables. It can be interpreted as a projection method (projecting onto linearized constraints), a prox-linear method (using a more accurate surrogate model), or an online SGD method (applied to time-varying objectives). The method updates both model parameters and slack variables at each iteration.

## Key Results
- SPS+ achieves best known convergence rates for SGD in Lipschitz non-smooth setting by enforcing positive step sizes
- FUVAL's convergence analysis shows no theoretical advantage over standard SGD
- Only full batch FUVAL shows minor advantages in step size sensitivity
- The paper conjectures that large mini-batches are required to make FUVAL competitive

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SPS+ achieves the best known rates for SGD in the Lipschitz non-smooth setting by enforcing positive step sizes.
- **Mechanism:** Uses Polyak stepsize formula with positive part of numerator to ensure non-negative step sizes, maintaining Fejér monotonicity and convergence without interpolation.
- **Core assumption:** Loss functions are star-convex around optimal solution.
- **Evidence anchors:** Abstract states SPS+ achieves best known rates; section 2.1 derives optimal one-step progress with positive part; related work on SPS methods supports similar guarantees.
- **Break condition:** Star-convexity failure breaks monotonicity and convergence rates.

### Mechanism 2
- **Claim:** FUVAL is an online SGD method on time-varying objective with surrogate functions.
- **Mechanism:** Reformulates problem with slack variables creating surrogate objective depending on gradient norm; online SGD updates implicitly learn optimal loss values.
- **Core assumption:** Original objective is convex and smooth enough for valid surrogate modeling.
- **Evidence anchors:** Section 3.3.1 describes online SGD viewpoint; Theorem 3.9 shows O(1/T) convergence under smoothness and convexity; weak corpus evidence linking surrogate view to known methods.
- **Break condition:** Non-smooth or non-convex functions break surrogate-based convergence theory.

### Mechanism 3
- **Claim:** Projection-based interpretation provides geometric intuition via incremental projections.
- **Mechanism:** Each iteration projects onto sampled linearized constraint hyperplane, driving solution toward feasibility.
- **Core assumption:** Constraint functions are convex for valid linear outer approximations.
- **Evidence anchors:** Section 2.2 derives SPS+ as projection method; section 3.1 gives closed-form projection solution; related ADMM and projection methods in literature.
- **Break condition:** Non-convex constraints break projection interpretation and convergence guarantees.

## Foundational Learning

- **Concept:** Star-convexity
  - **Why needed here:** Convergence analysis of SPS+ relies on star-convexity of individual loss functions around optimal point.
  - **Quick check question:** Given a convex function f and point w*, is f always star-convex around w*? (Answer: No, only if f(w*) is minimum value.)

- **Concept:** Interpolation condition
  - **Why needed here:** Under interpolation, noise radius in convergence bound becomes zero, leading to faster rates.
  - **Quick check question:** What does interpolation condition fi(w*) = inf fi imply about loss landscape? (Answer: Model perfectly fits data at w*.)

- **Concept:** Prox-linear methods
  - **Why needed here:** FUVAL's prox-linear viewpoint uses more accurate model than standard SGD by linearizing inside positive part.
  - **Quick check question:** How does prox-linear model differ from standard linearization in SGD? (Answer: Approximates objective more accurately by keeping positive part structure.)

## Architecture Onboarding

- **Component map:** Sample index jt -> Compute step size τt -> Update wt+1 and st+1
- **Critical path:** 1) Sample data point/index, 2) Evaluate loss and gradient, 3) Compute adaptive step size using current slack values, 4) Update model parameters and slack variables, 5) Check convergence criteria
- **Design tradeoffs:** Slack variables add memory overhead but enable function splitting; adaptive step size removes manual tuning but requires tracking slack values; full batch vs stochastic affects convergence speed and noise
- **Failure signatures:** Diverging iterates (incorrect step size scaling or assumption violation), slack variables not converging (poor initialization or step size issues), no improvement over standard SGD (insufficient batch size or interpolation failure)
- **First 3 experiments:** 1) Implement full batch FUVAL on small convex problem, compare step size sensitivity vs GD, 2) Test stochastic FUVAL with varying batch sizes on interpolation problem to observe noise radius effects, 3) Compare convergence under interpolation vs non-interpolation settings to verify theoretical noise radius claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific value of penalty parameter c ≥ 1 in prox-linear and online SGD viewpoints yields optimal convergence rates?
- Basis in paper: Paper states solving ℓ1-penalty reformulation with c ≥ 1 is equivalent to original problem but provides no guidance on optimal c value.
- Why unresolved: Paper only requires c ≥ 1 for equivalence, doesn't specify optimal choice.
- What evidence would resolve it: Numerical experiments varying c and analyzing convergence rates would provide evidence for optimal choice.

### Open Question 2
- Question: How does relaxation parameter γ ∈ ]0, 1] in online SGD viewpoint impact convergence rates and hyperparameter sensitivity?
- Basis in paper: Introduces relaxation step with parameter γ but only analyzes case γ = 1.
- Why unresolved: Paper analyzes γ = 1 but doesn't explore impact of different γ values.
- What evidence would resolve it: Numerical experiments varying γ and analyzing convergence rates and sensitivity would provide evidence for optimal choice.

### Open Question 3
- Question: What is impact of unit choices for tunable parameters λ and δ on convergence and stability?
- Basis in paper: Discusses importance of matching units for scale invariance but doesn't explore impact of different unit choices.
- Why unresolved: Paper provides framework for unit choice but doesn't explore convergence/stability impact.
- What evidence would resolve it: Numerical experiments varying units of λ and δ and analyzing convergence rates and stability would provide evidence for optimal unit choices.

## Limitations

- FUVAL shows no theoretical convergence advantage over standard SGD
- Only minor advantages observed in full batch settings, less competitive in stochastic settings
- Large mini-batches conjectured to be required for FUVAL competitiveness, but unproven
- No clear practical advantages demonstrated despite multiple theoretical viewpoints

## Confidence

- SPS+ convergence guarantees: High confidence (well-supported theoretical proofs and established techniques)
- Projection and prox-linear interpretations: Medium confidence (reasonably well-founded under stated assumptions)
- Practical advantages of FUVAL: Low confidence (no theoretical advantage shown, only minor empirical benefits in full batch settings)

## Next Checks

1. Test FUVAL on non-interpolation problems with varying batch sizes to empirically verify mini-batch requirement conjecture
2. Implement and compare all three viewpoints (projection, prox-linear, online SGD) to verify they produce identical iterates
3. Benchmark FUVAL against modern adaptive methods like Adam on problems where interpolation does not hold