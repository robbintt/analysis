---
ver: rpa2
title: Enhanced Knowledge Injection for Radiology Report Generation
arxiv_id: '2311.00399'
source_url: https://arxiv.org/abs/2311.00399
tags:
- knowledge
- report
- generation
- reports
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating accurate radiology
  reports by proposing an enhanced knowledge injection framework that incorporates
  two types of domain-specific knowledge: weighted clinical concepts and multimodal
  retrieval knowledge. The framework uses a Transformer-based encoder-decoder architecture
  with a Mixture of Knowledge (MoK) module to fuse the current image features with
  knowledge from the Weighted Concept Knowledge (WCK) and Multimodal Retrieval Knowledge
  (MRK) branches.'
---

# Enhanced Knowledge Injection for Radiology Report Generation

## Quick Facts
- arXiv ID: 2311.00399
- Source URL: https://arxiv.org/abs/2311.00399
- Authors: 
- Reference count: 26
- Key outcome: Proposed method achieves state-of-the-art BLEU-4 scores of 0.207 on IU-Xray and 0.119 on MIMIC-CXR datasets

## Executive Summary
This paper addresses the challenge of generating accurate radiology reports by proposing an enhanced knowledge injection framework that incorporates two types of domain-specific knowledge: weighted clinical concepts and multimodal retrieval knowledge. The framework uses a Transformer-based encoder-decoder architecture with a Mixture of Knowledge (MoK) module to fuse current image features with knowledge from the Weighted Concept Knowledge (WCK) and Multimodal Retrieval Knowledge (MRK) branches. Extensive experiments on IU-Xray and MIMIC-CXR datasets demonstrate that the proposed method achieves state-of-the-art performance, with BLEU-4 scores of 0.207 and 0.119, respectively.

## Method Summary
The proposed framework consists of a Transformer-based encoder-decoder architecture with two knowledge injection branches. The Weighted Concept Knowledge (WCK) branch uses TF-IDF weighting on ClinicalBERT embeddings of 76 clinical concepts to address class imbalance. The Multimodal Retrieval Knowledge (MRK) branch retrieves similar reports using MGCA and extracts structured triplets in {entity, position, exist} format. A Mixture of Knowledge (MoK) module fuses these knowledge sources with image features through cross-attention before feeding into the decoder.

## Key Results
- State-of-the-art BLEU-4 scores of 0.207 on IU-Xray and 0.119 on MIMIC-CXR datasets
- The WCK branch effectively mitigates class imbalance by emphasizing rare abnormal concepts through TF-IDF weighting
- The MRK branch introduces structured triplet knowledge extraction, providing precise clinical information for report generation

## Why This Works (Mechanism)

### Mechanism 1
TF-IDF scoring reweights clinical concepts based on their frequency in the current report relative to their frequency across the corpus. High TF-IDF scores are assigned to concepts that are frequent in the current report but rare overall, which typically correspond to abnormalities. This addresses the class imbalance problem where normal findings dominate training data.

### Mechanism 2
The MRK branch extracts triplets from similar reports in {entity, position, exist} format, transforming raw report text into structured knowledge. This provides precise and non-redundant information for report generation while avoiding linguistic noise from raw text.

### Mechanism 3
The MoK module uses cross-attention to fuse image features with both WCK and MRK knowledge sources. The image feature acts as query while knowledge features act as keys and values, allowing selective attention to relevant knowledge based on current image context.

## Foundational Learning

- Concept: TF-IDF weighting
  - Why needed here: To address class imbalance in radiology datasets where normal findings dominate and abnormalities are underrepresented
  - Quick check question: How does TF-IDF score change for a term that appears frequently in a single report but rarely across the entire corpus?

- Concept: Multimodal retrieval and embedding
  - Why needed here: To find semantically similar images and extract clinically relevant knowledge from their reports
  - Quick check question: What is the role of cosine similarity in the retrieval process, and why is it used here?

- Concept: Cross-attention in multimodal fusion
  - Why needed here: To integrate knowledge from different modalities by allowing the model to attend to the most relevant parts of each source
  - Quick check question: How does cross-attention differ from standard self-attention in Transformers?

## Architecture Onboarding

- Component map: Image → Visual Extractor → MoK (with WCK & MRK) → Transformer → Report
- Critical path: Visual features flow through MoK module where they are enriched with weighted clinical concepts and retrieved triplet knowledge before reaching the Transformer decoder
- Design tradeoffs: TF-IDF weighting depends on corpus statistics; triplet format simplifies knowledge but may lose nuance; retrieval quality affects knowledge relevance
- Failure signatures: Misaligned WCK/MRK features introduce noise; over-reliance on retrieval ignores image-specific details; triplet extraction misses key entities
- First 3 experiments: 1) Train baseline Transformer encoder-decoder without knowledge injection, 2) Add WCK branch only to assess weighted concept impact, 3) Add MRK branch only to evaluate retrieval triplet contribution

## Open Questions the Paper Calls Out

### Open Question 1
How does the model perform when incorporating knowledge from different domains (e.g., natural images) compared to domain-specific knowledge? The paper does not explicitly compare the performance of incorporating knowledge from different domains versus domain-specific knowledge.

### Open Question 2
How does the proposed method handle the generation of radiology reports for rare or uncommon abnormalities? While the paper addresses class imbalance through TF-IDF weighting, it does not specifically discuss handling rare abnormalities.

### Open Question 3
How does the model's performance change when using different pre-trained vision-language models for retrieving similar images? The paper uses MGCA but does not explore the impact of using different pre-trained models on performance.

## Limitations

- Performance gains demonstrated on only two public datasets with limited ablation studies
- BLEU-4 scores remain relatively low (0.207 and 0.119) despite being state-of-the-art
- Effectiveness of TF-IDF weighting for class imbalance is assumed but not directly validated through controlled experiments

## Confidence

**High Confidence**
- Overall architecture combining WCK, MRK, and MoK modules is technically sound
- Cross-attention for multimodal fusion is a valid approach supported by prior work
- Datasets used (IU-Xray and MIMIC-CXR) are appropriate benchmarks

**Medium Confidence**
- TF-IDF weighting strategy effectively addresses class imbalance
- Triplet extraction captures clinically relevant information without significant loss
- Retrieval-based knowledge injection improves report accuracy

**Low Confidence**
- Specific choice of 76 clinical concepts is optimal
- MGCA retrieval model is the best choice for this application
- Performance improvements are solely due to knowledge injection mechanisms

## Next Checks

1. Perform controlled ablation experiments removing either WCK or MRK individually to quantify their specific contributions to performance gains

2. Implement metrics to measure the relevance and accuracy of retrieved triplets and the effectiveness of TF-IDF weighting independently of final report quality

3. Evaluate the model on a held-out subset where concept distributions differ significantly from training data to test TF-IDF weighting robustness and knowledge injection mechanisms