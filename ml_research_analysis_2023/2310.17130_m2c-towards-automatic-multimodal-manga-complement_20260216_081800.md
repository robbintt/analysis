---
ver: rpa2
title: 'M2C: Towards Automatic Multimodal Manga Complement'
arxiv_id: '2310.17130'
source_url: https://arxiv.org/abs/2310.17130
tags:
- manga
- visual
- text
- language
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multimodal Manga Complement (M2C) task
  to address missing or damaged text in comics by leveraging both visual and language
  information. The authors propose a fine-grained visual prompt generation strategy
  (FVP-M2) that aggregates global and local visual features to improve text generation,
  along with a three-hop manga Chain-of-Thought (MCoT) method to exploit timeline
  and logical knowledge in comics.
---

# M2C: Towards Automatic Multimodal Manga Complement

## Quick Facts
- arXiv ID: 2310.17130
- Source URL: https://arxiv.org/abs/2310.17130
- Reference count: 17
- Key outcome: FVP-M2 significantly outperforms text-only baselines and achieves the best BLEU scores on M2C benchmark dataset

## Executive Summary
This paper introduces the Multimodal Manga Complement (M2C) task to address missing or damaged text in comics by leveraging both visual and language information. The authors propose a fine-grained visual prompt generation strategy (FVP-M2) that aggregates global and local visual features to improve text generation, along with a three-hop manga Chain-of-Thought (MCoT) method to exploit timeline and logical knowledge in comics. Experiments on a new M2C benchmark dataset covering English and French show that FVP-M2 significantly outperforms text-only baselines and achieves the best BLEU scores, demonstrating the effectiveness of combining vision and language modalities for manga complement.

## Method Summary
The M2C task treats missing comic text as a multimodal generation problem where visual inputs provide contextual cues that supplement incomplete textual information. The proposed FVP-M2 method uses a mapping network to encode global visual features as a bias term, which is combined with locally aggregated visual features through self-attention to generate fine-grained visual prompts. These prompts are fed into a co-Transformer to guide language token generation. The three-hop Manga Chain-of-Thought (MCoT) method uses a three-step reasoning process (inferring theme, opinion, and future) to extract implicit knowledge from comic text and guide the generation of complementary content.

## Key Results
- FVP-M2 achieves significant improvements over text-only baselines on M2C benchmark dataset
- Fine-grained visual prompts improve dialogue complement by capturing both global and local visual information
- Three-hop Manga Chain-of-Thought method exploits timeline and logical knowledge in comics to improve generation stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained visual prompts improve dialogue complement by capturing both global and local visual information
- Mechanism: The FVP-M2 method uses a mapping network to encode global visual features as a bias term, which is then combined with locally aggregated visual features through self-attention to generate fine-grained visual prompts. These prompts are fed into a co-Transformer to guide language token generation
- Core assumption: Visual information at both global (scene context) and local (specific objects/regions) levels is necessary for accurate manga text completion
- Evidence anchors:
  - [abstract]: "we adopt a new way to aggregate global and local visual information"
  - [section 4.2.2]: "the local visual features are aggregated in a self-attention way. while a mapping network...is leveraged to generate the global visual feature, which will serve as a bias term added into the calculation of local visual features"
  - [corpus]: Weak - no direct citations about visual prompt effectiveness in manga context, though related multimodal works exist
- Break condition: If either global or local visual information becomes irrelevant to the missing text, the fine-grained visual prompts would lose effectiveness

### Mechanism 2
- Claim: Three-hop Manga Chain-of-Thought (MCoT) exploits timeline and logical knowledge in comics to improve generation stability
- Mechanism: MCoT uses a three-step reasoning process (inferring theme, opinion, and future) to extract implicit knowledge from comic text. This structured reasoning guides the generation of complementary content by providing contextual understanding of the narrative flow
- Core assumption: Comic text and images contain temporal and logical relationships that can be systematically extracted to improve generation quality
- Evidence anchors:
  - [abstract]: "design a manga Chain-of-Thought method to exploit the logical information and the event knowledge implied by the comics"
  - [section 3]: "we design a three-hop manga Chain-of-Thought to facilitate step-by-step reasoning (inferring theme, opinion and future)"
  - [corpus]: Moderate - several related works on multimodal reasoning and comic understanding exist, but no direct citations about MCoT effectiveness
- Break condition: If the comic lacks clear temporal or logical structure, the three-hop reasoning process may introduce noise rather than helpful context

### Mechanism 3
- Claim: Combining visual and language modalities reduces data sparsity and ambiguity in manga complement
- Mechanism: The M2C task framework treats missing comic text as a multimodal generation problem, where visual inputs provide contextual cues that supplement incomplete textual information. This dual-modality approach addresses the limitations of text-only generation
- Core assumption: Visual and textual information in comics are complementary, with images providing context that disambiguates incomplete text
- Evidence anchors:
  - [abstract]: "aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding"
  - [introduction]: "This task extends conventional text-based content complement by incorporating corresponding images as additional inputs to mitigate data sparsity and ambiguity"
  - [section 5.3]: "visual modality is complementary rather than redundant if the text is insufficient"
- Break condition: If visual information becomes redundant or contradictory to textual context, the multimodal approach may degrade performance

## Foundational Learning

- Concept: Transformer architecture for multimodal tasks
  - Why needed here: The paper uses Transformer-based models for both feature encoding and generation, requiring understanding of self-attention mechanisms and multimodal fusion strategies
  - Quick check question: How does co-attention differ from standard self-attention in multimodal Transformers?

- Concept: Vision-language pretraining and CLIP model
  - Why needed here: The implementation uses the vision branch of CLIP for visual feature extraction, requiring knowledge of vision-language model architectures and their pretraining objectives
  - Quick check question: What are the key differences between CLIP's image-text contrastive learning and traditional supervised vision tasks?

- Concept: Chain-of-Thought prompting and reasoning
  - Why needed here: The MCoT method extends CoT prompting to manga understanding, requiring understanding of how structured reasoning can improve generation in complex domains
  - Quick check question: How does the three-hop reasoning structure (theme→opinion→future) help extract implicit knowledge from comic text?

## Architecture Onboarding

- Component map: Vision encoder (CLIP ViT-L/14) → Visual feature extraction → Fine-grained visual prompt generation module → Visual context aggregation → Co-Transformer → Vision-guided source token fusion → Transformer decoder → Target sequence generation → MCoT module → Reasoning augmentation

- Critical path: Vision encoder → FVPG → Co-Transformer → Decoder
  The most performance-critical components are the visual feature extraction and the co-attention mechanism that fuses vision and language

- Design tradeoffs:
  - Global vs. local visual features: The mapping network adds computational overhead but captures scene-level context
  - Three-hop reasoning: Provides better context but increases complexity and potential for error propagation
  - Fine-grained vs. coarse visual prompts: More detailed visual information improves quality but increases model complexity

- Failure signatures:
  - Low BLEU scores with MCoT enabled but high scores without it → MCoT may be introducing incorrect reasoning
  - Performance drops when using global visual features → The mapping network or global context may be noisy
  - High variance in generation quality → The co-attention mechanism may not be properly aligning vision and language

- First 3 experiments:
  1. Ablation study: Test FVP-M2 without FVPG to measure impact of fine-grained visual prompts
  2. Ablation study: Test FVP-M2 without MCoT to measure impact of three-hop reasoning
  3. Vision encoder comparison: Replace CLIP with alternative vision backbones to assess feature quality impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FVP-M2 method perform on multilingual datasets beyond English and French?
- Basis in paper: [explicit] The paper states that the M2C benchmark dataset covers two languages, English and French, and the experimental results show the effectiveness of the FVP-M2 method for these two languages. However, the paper does not explore the performance of the method on other languages.
- Why unresolved: The paper does not provide any evidence or discussion on the performance of the FVP-M2 method on other languages, which leaves the question of its generalizability to other languages unanswered.
- What evidence would resolve it: Conducting experiments on datasets with more languages and comparing the performance of the FVP-M2 method with other methods on these datasets would provide evidence to resolve this question.

### Open Question 2
- Question: What is the impact of using different vision backbones on the performance of the FVP-M2 method?
- Basis in paper: [explicit] The paper mentions that the vision branch of CLIP based on the ViT-L/14 model is used as the vision backbone in the FVP-M2 method. However, the paper does not explore the impact of using different vision backbones on the performance of the method.
- Why unresolved: The paper does not provide any evidence or discussion on the impact of using different vision backbones on the performance of the FVP-M2 method, which leaves the question of the optimal vision backbone for the method unanswered.
- What evidence would resolve it: Conducting experiments with different vision backbones and comparing the performance of the FVP-M2 method with these backbones would provide evidence to resolve this question.

### Open Question 3
- Question: How does the FVP-M2 method handle cases where the visual information is ambiguous or misleading?
- Basis in paper: [inferred] The paper proposes the FVP-M2 method to handle missing or damaged text in comics by leveraging both visual and language information. However, the paper does not discuss how the method handles cases where the visual information is ambiguous or misleading.
- Why unresolved: The paper does not provide any evidence or discussion on how the FVP-M2 method handles ambiguous or misleading visual information, which leaves the question of the method's robustness to such cases unanswered.
- What evidence would resolve it: Conducting experiments on datasets with ambiguous or misleading visual information and analyzing the performance of the FVP-M2 method in these cases would provide evidence to resolve this question.

## Limitations

- Limited baseline comparisons: The paper only compares against text-only models without including other multimodal approaches that could serve as stronger baselines
- Narrow evaluation metrics: Relying solely on BLEU scores provides an incomplete picture of generation quality
- Dataset generalization: The M2C benchmark covers only English and French manga, raising questions about the approach's effectiveness across different manga styles, languages, and cultural contexts

## Confidence

- **High Confidence**: The basic premise that visual information can help complete missing manga text is well-supported by the multimodal learning literature and the experimental results show consistent improvements over text-only baselines
- **Medium Confidence**: The effectiveness of fine-grained visual prompts (FVP-M2) is moderately supported, though the ablation studies are incomplete and don't fully isolate the contribution of visual features versus other architectural choices
- **Low Confidence**: The three-hop Manga Chain-of-Thought (MCoT) method shows mixed results in the provided experiments, with some cases showing improvement and others showing degradation, suggesting the method may be sensitive to input characteristics

## Next Checks

1. **Ablation Study with Complete Isolation**: Conduct a controlled ablation study that separately evaluates the impact of global visual features, local visual features, and MCoT reasoning to determine which components contribute most to performance improvements

2. **Human Evaluation Study**: Implement human evaluation metrics to assess whether the generated text maintains manga-appropriate style, tone, and contextual coherence beyond what BLEU scores can capture

3. **Cross-Domain Generalization Test**: Evaluate the model on manga from different genres, publication periods, and artistic styles to assess whether the approach generalizes beyond the training distribution or overfits to specific manga characteristics