---
ver: rpa2
title: 'Code Llama: Open Foundation Models for Code'
arxiv_id: '2308.12950'
source_url: https://arxiv.org/abs/2308.12950
tags:
- code
- llama
- python
- pass
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Code Llama is a family of large language models for code generation,
  derived from Llama 2 and released under a permissive license. It includes variants
  specialized for Python and instruction following, with sizes ranging from 7B to
  70B parameters.
---

# Code Llama: Open Foundation Models for Code

## Quick Facts
- arXiv ID: 2308.12950
- Source URL: https://arxiv.org/abs/2308.12950
- Authors: Meta
- Reference count: 40
- Primary result: Code Llama demonstrates state-of-the-art performance among open models on code benchmarks (67% HumanEval, 65% MBPP)

## Executive Summary
Code Llama is a family of large language models for code generation, derived from Llama 2 and released under a permissive license. It includes variants specialized for Python and instruction following, with sizes ranging from 7B to 70B parameters. All models are trained on sequences of 16k tokens and support long input contexts up to 100k tokens. Code Llama demonstrates state-of-the-art performance among open models on code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on these benchmarks, and all models outperform other publicly available models on MultiPL-E.

## Method Summary
Code Llama models are initialized from Llama 2 model weights and trained on 500B tokens from a code-heavy dataset. Python-specific models are further trained on 100B tokens using a Python-heavy dataset. All models are fine-tuned to handle long contexts up to 100k tokens. Instruction-following models are further fine-tuned on a mix of proprietary instruction data and a machine-generated self-instruct dataset. The models demonstrate improvements on inputs with up to 100k tokens.

## Key Results
- Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP benchmarks
- All Code Llama models outperform other publicly available models on MultiPL-E
- Models support long input contexts up to 100k tokens with stable behavior
- Infilling capabilities come at low cost in code generation performance

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning from Llama 2 models yields better code performance than training from scratch on code data. Llama 2 models provide strong general language understanding and reasoning capabilities that transfer to code tasks when fine-tuned on code data. Core assumption: General language understanding skills are beneficial for code understanding and generation.

### Mechanism 2
Infilling training enables code completion capabilities at low cost to code generation performance. Infilling training teaches the model to predict missing parts of code given surrounding context, enabling applications like real-time completion in IDEs. Core assumption: The ability to predict missing code given context is a useful skill for code models and can be learned without significant degradation in standard code generation performance.

### Mechanism 3
Long context fine-tuning enables effective processing of long code sequences up to 100k tokens. Fine-tuning on long sequences modifies the RoPE positional embeddings to allow the model to leverage information from distant parts of the input. Core assumption: The ability to process long sequences is beneficial for code tasks that involve reasoning about large codebases.

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: Code Llama is an autoregressive model that predicts the next token given previous tokens
  - Quick check question: How does an autoregressive model differ from a masked language model like BERT?

- Concept: Fine-tuning
  - Why needed here: Code Llama is fine-tuned from Llama 2 models on code data to specialize it for code tasks
  - Quick check question: What is the difference between fine-tuning and training from scratch?

- Concept: Positional embeddings
  - Why needed here: Code Llama uses rotary positional embeddings (RoPE) to encode the position of tokens in the input sequence
  - Quick check question: Why are positional embeddings necessary in transformer models?

## Architecture Onboarding

- Component map: Input tokens → Embedding layer → Transformer layers → Softmax output layer → Predicted tokens
- Critical path: Tokenize input prompt → Pass through model to get output logits → Apply sampling (e.g. top-k, temperature) to logits → Decode sampled tokens to final output
- Design tradeoffs: Code Llama uses large model sizes (up to 70B parameters) for strong performance, which increases computational requirements. It supports long input contexts (up to 100k tokens), useful for some code tasks but increases memory usage.
- Failure signatures: Common failure modes include generating incorrect or nonsensical code, failing to follow instructions, or being unable to handle long input contexts. These can often be diagnosed by examining model output and comparing to expected output.
- First 3 experiments:
  1. Evaluate the model on a held-out test set of code generation tasks to measure performance
  2. Test the model's ability to handle long input contexts by prompting with increasingly long code sequences
  3. Evaluate the model's infilling capabilities by prompting with code that has missing parts and checking if it can fill in the gaps correctly

## Open Questions the Paper Calls Out

Open Question 1
- Question: What is the impact of training Code Llama on additional synthetic exercises, similar to those used in HumanEval and MBPP benchmarks?
- Basis in paper: The paper mentions that they did not train their foundation models on additional synthetic exercises, as they did not want to risk reducing the scope of their models to simple coding exercises similar to those contained in HumanEval and MBPP.
- Why unresolved: It is unclear how training on synthetic exercises would impact the model's performance on real-world coding tasks and its ability to generalize to new problems.
- What evidence would resolve it: Comparing the performance of Code Llama models trained with and without synthetic exercises on a variety of coding benchmarks and real-world coding tasks.

Open Question 2
- Question: How does the performance of Code Llama models scale with increasing model size beyond 34B parameters?
- Basis in paper: The paper observes that scaling the number of parameters matters for models specialized for coding and that larger models outperform their smaller counterparts on almost every metric from HumanEval, MBPP and APPS.
- Why unresolved: The paper only evaluates models up to 34B parameters and it is unclear how much further performance gains can be achieved with even larger models.
- What evidence would resolve it: Training and evaluating Code Llama models with sizes larger than 34B parameters on the same benchmarks and comparing their performance to the existing models.

Open Question 3
- Question: How do Code Llama models perform on code generation tasks in programming languages not included in the MultiPL-E benchmark?
- Basis in paper: The paper evaluates Code Llama models on Python, C++, Java, PHP, TypeScript, C#, and Bash using the MultiPL-E benchmark, but there are many other programming languages that were not included in the evaluation.
- Why unresolved: It is unclear how well Code Llama models generalize to other programming languages and whether their performance is consistent across different language families and paradigms.
- What evidence would resolve it: Evaluating Code Llama models on code generation tasks in a diverse set of programming languages not included in MultiPL-E and comparing their performance to models specifically trained on those languages.

## Limitations
- Dataset transparency issues prevent full assessment of potential biases or benchmark contamination
- Significant computational requirements limit accessibility for independent verification of largest model's performance
- Limited detail on safety evaluation methodology and mitigation strategies for potentially harmful code generation

## Confidence
High Confidence: The architectural framework (fine-tuning Llama 2 on code data, supporting long contexts) is well-established and technically sound based on the literature.
Medium Confidence: The performance improvements over baseline models are likely real but may be somewhat inflated due to potential benchmark contamination from training data overlap.
Low Confidence: Claims about safety and responsible use lack sufficient empirical backing or detailed evaluation methodology.

## Next Checks
1. Independent benchmark evaluation: Test Code Llama models on a held-out set of programming tasks not used in training or standard benchmarks to verify generalization claims, particularly for the long-context capabilities.
2. Dataset analysis: Conduct a thorough analysis of the overlap between training data and benchmark datasets to quantify potential contamination effects on reported performance metrics.
3. Safety stress testing: Systematically evaluate the models' propensity to generate vulnerable, biased, or otherwise problematic code using established security and ethics benchmarks for code generation systems.