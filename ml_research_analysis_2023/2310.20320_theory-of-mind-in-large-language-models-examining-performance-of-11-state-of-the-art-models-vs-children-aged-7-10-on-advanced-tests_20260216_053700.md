---
ver: rpa2
title: 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art
  models vs. Children Aged 7-10 on Advanced Tests'
arxiv_id: '2310.20320'
source_url: https://arxiv.org/abs/2310.20320
tags:
- llms
- language
- performance
- test
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 11 state-of-the-art language models against
  children aged 7-10 on Theory of Mind (ToM) tasks, including false-belief tests,
  non-literal language interpretation, and recursive intentionality. By using varied
  test scenarios, open questions, and deviations from standard test formats, the research
  examines model robustness and generalization.
---

# Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests

## Quick Facts
- arXiv ID: 2310.20320
- Source URL: https://arxiv.org/abs/2310.20320
- Reference count: 24
- Primary result: Instruction-tuned LLMs outperform base-LLMs and often surpass children on advanced ToM tasks

## Executive Summary
This study benchmarks 11 state-of-the-art language models against children aged 7-10 on Theory of Mind (ToM) tasks including false-belief tests, non-literal language interpretation, and recursive intentionality. The research uses varied test scenarios, open questions, and deviations from standard formats to examine model robustness and generalization. Results show that instruction-tuned models, particularly from the GPT family, outperform base-LLMs and often surpass children, especially in complex scenarios. Base-LLMs generally perform below child level even with specialized prompts.

## Method Summary
The study tested 11 LLMs (4 base, 7 instruction-tuned) on three ToM tasks: Sally-Anne false-belief scenarios, Strange Stories for non-literal language, and Imposing Memory for recursive intentionality. Models were prompted with original and two rewritten deviations of each test scenario. Children's responses (n=73, ages 7-10) were collected via self-paced digital format. Performance was measured using accuracy scores (0-1) for each item and motivation ratings (0-2) for open answers, with evaluations conducted by expert consensus.

## Key Results
- Instruction-tuned models consistently outperformed base-LLMs on all ToM tasks
- GPT family models surpassed children's performance on complex scenarios
- Base-LLMs struggled with ToM tasks even with specialized prompting
- Performance declined with increased deviation from original test scenarios
- Open-ended responses showed larger performance gaps than closed questions

## Why This Works (Mechanism)

### Mechanism 1
Instruction-tuning enhances ToM-like capabilities by rewarding cooperative communication that takes into account interlocutor and context. The instruction-tuning process uses human feedback to align models with desired interaction formats, teaching them to apply knowledge appropriately in social contexts. This mirrors how human communication relies on basic abilities and willingness to engage in mental coordination.

### Mechanism 2
Base-LLMs struggle with ToM tasks due to limitations in prompt format and length, while instruction-tuned models are better at following instructions. Base-LLMs are trained for text completion rather than instruction following, making them less effective at answering open questions or providing motivations. Instruction-tuned models are specifically trained to understand and follow instructions, giving them an advantage in ToM tasks.

### Mechanism 3
LLMs have limited access to behaviorally-situated reasoning during training, making tasks like Sally-Anne harder compared to Strange Stories which rely on language understanding. LLMs are trained on text data which may not capture the behavioral and contextual aspects of social interactions as effectively as language understanding. Tasks requiring understanding of false beliefs in specific scenarios are harder than those requiring interpretation of non-literal language.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: Understanding ToM is crucial as it is the core cognitive capacity being evaluated in the study
  - Quick check question: Can you explain what Theory of Mind is and why it is important in human social interactions?

- Concept: Large Language Models (LLMs)
  - Why needed here: The study compares the performance of different LLMs on ToM tasks, so understanding how LLMs work is essential
  - Quick check question: What are the main differences between base-LLMs and instruction-tuned LLMs, and how are they trained?

- Concept: False-belief tasks
  - Why needed here: False-belief tasks are a common method for assessing ToM, and the study uses variations of these tasks
  - Quick check question: Can you describe a false-belief task and explain what it measures?

## Architecture Onboarding

- Component map: Models (Base-LLMs -> Instruction-tuned LLMs) -> ToM Tasks (Sally-Anne -> Strange Stories -> Imposing Memory) -> Evaluation (Accuracy/Motivation Scores) -> Comparison (vs Children)
- Critical path: Prompt LLMs with test scenarios → Generate responses → Score responses (accuracy/motivation) → Compare to children's performance
- Design tradeoffs: Model size vs. instruction-tuning benefits; task complexity vs. performance consistency
- Failure signatures: Base-LLMs generating off-topic completions; performance decline with test deviations; models performing below child level
- First 3 experiments:
  1. Test a base-LLM and an instruction-tuned LLM of similar size on a simple ToM task to compare their performance
  2. Vary the prompt format for a base-LLM to see if it can be improved to perform better on ToM tasks
  3. Test a model on a novel ToM task that it has not been specifically trained on to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does instruction-tuning enhance Theory of Mind (ToM) capabilities in large language models (LLMs), and can this be quantified in terms of specific mechanisms or improvements in performance?
- Basis in paper: [explicit] The paper suggests that instruction-tuning rewards cooperative communication, which enhances ToM-like capabilities in models
- Why unresolved: The paper does not provide a detailed mechanism or quantifiable metrics for how instruction-tuning specifically enhances ToM capabilities
- What evidence would resolve it: Experimental studies comparing base-LLMs and instruction-tuned LLMs on a variety of ToM tasks with detailed performance metrics and analysis of the differences in reasoning processes

### Open Question 2
What are the limitations of current LLM architectures in handling recursive intentionality, and how can these be addressed to improve performance on complex Theory of Mind tasks?
- Basis in paper: [inferred] The paper indicates that LLMs struggle with recursive intentionality, especially in the Imposing Memory test
- Why unresolved: The paper does not explore specific architectural changes or enhancements that could address these limitations
- What evidence would resolve it: Research into new LLM architectures or modifications to existing ones that specifically target recursive intentionality, along with empirical testing to demonstrate improvements

### Open Question 3
How do variations in test scenarios and deviations from standardized formats impact the generalizability of LLM performance on Theory of Mind tasks, and what does this imply about the robustness of their ToM-like capabilities?
- Basis in paper: [explicit] The paper uses deviations from original test formats to gauge LLM robustness and generalizability
- Why unresolved: The paper does not fully explore the implications of these findings or propose methods to enhance robustness across varied contexts
- What evidence would resolve it: Comparative studies using a broader range of test scenarios and deviations, coupled with analysis of model adaptations and their impact on performance consistency

## Limitations

- Model specification gaps: Incomplete specification of exact model versions, parameter counts, and training details for several models
- Prompt engineering variability: Limited exploration of alternative prompting strategies for base-LLMs
- Children's performance distribution: Lack of detailed statistical analysis of children's performance variability across age groups

## Confidence

- High Confidence: Instruction-tuned models outperform base-LLMs on ToM tasks
- Medium Confidence: GPT family models surpass children on complex ToM scenarios
- Medium Confidence: Instruction-tuning enhances ToM by rewarding cooperative communication

## Next Checks

1. Prompt Engineering Ablation: Systematically test multiple prompt formats for base-LLMs on the same ToM tasks to determine whether performance gaps are primarily due to model capability or prompt compatibility issues

2. Cross-Age Group Analysis: Conduct a more granular analysis of children's performance by age and task type to identify specific scenarios where LLMs show the largest advantages or disadvantages relative to different age groups

3. Behavioral Grounding Test: Design ToM tasks that specifically require understanding of physical actions and consequences to test the hypothesis that LLMs struggle with behaviorally-situated reasoning compared to language-based inference tasks