---
ver: rpa2
title: 'FunQA: Towards Surprising Video Comprehension'
arxiv_id: '2306.14899'
source_url: https://arxiv.org/abs/2306.14899
tags:
- video
- videos
- understanding
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FunQA is a video question-answering dataset focused on surprising
  videos, including humor, creativity, and magic. It contains 312K QA pairs derived
  from 4.3K video clips totaling 24 hours.
---

# FunQA: Towards Surprising Video Comprehension

## Quick Facts
- **arXiv ID:** 2306.14899
- **Source URL:** https://arxiv.org/abs/2306.14899
- **Reference count:** 40
- **Primary result:** FunQA is a video question-answering dataset focused on surprising videos, including humor, creativity, and magic. It contains 312K QA pairs derived from 4.3K video clips totaling 24 hours. The dataset includes tasks for timestamp localization, detailed video description, and reasoning around counter-intuitiveness.

## Executive Summary
FunQA is a novel video question-answering dataset designed to evaluate models' ability to understand and reason about surprising content in videos. The dataset contains 312K QA pairs derived from 4.3K video clips totaling 24 hours, focusing on three types of surprising videos: humor, creativity, and magic. Each subset includes tasks for timestamp localization, detailed video description, and reasoning about counter-intuitiveness, along with higher-level tasks like assigning titles and scoring creativity.

## Method Summary
FunQA is constructed from videos with high surprise scores, filtered for video quality and diversity. The dataset includes 4.3K video clips (24 hours total) across three subsets: HumorQA (1,769 clips, avg. 7s), CreativeQA (927 clips, avg. 48s), and MagicQA (1,672 clips, avg. 10s). Each subset contains 312K free-text QA pairs with tasks for timestamp localization, detailed description, and reasoning. Evaluation uses multiple metrics including IOU for localization and BLEU-4/ROUGE-L/CIDEr/BLEURT/GPT-4 for free-text tasks.

## Key Results
- Models show significant performance gaps on FunQA tasks, highlighting limitations in video reasoning capabilities
- Traditional metrics yield near-zero scores on free-text questions, necessitating GPT-4-based evaluation
- Performance varies significantly across video types, with MagicQA showing highest IOU and BLEU scores while HumorQA shows lowest
- Longer videos (CreativeQA) present particular challenges for current models due to temporal reasoning requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FunQA's tasks demand understanding of counter-intuitive events that require combining visual information with commonsense reasoning beyond what standard VideoQA datasets require
- Mechanism: The dataset is structured around three types of surprising videos (HumorQA, CreativeQA, MagicQA), each requiring models to identify not just what happened, but why it violates expectations
- Core assumption: Visual commonsense violations are fundamentally harder to detect and explain than routine events
- Evidence anchors:
  - [abstract]: "Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos"
  - [section 1]: "Psychological research has demonstrated that humor arises from the incongruity [41, 42] between reality and expectations"

### Mechanism 2
- Claim: The free-text answer format in FunQA forces models to generate longer, more detailed responses that better test true understanding versus pattern matching
- Mechanism: By requiring detailed descriptions and reasoning explanations (average 34.24 words per answer), FunQA prevents models from simply selecting from predefined options
- Core assumption: Free-text generation requires deeper understanding than multiple choice selection
- Evidence anchors:
  - [section 3.3]: "For the description and reasoning tasks, the average length of the words in their free-text answers reached 34.24, which is much longer than existing VideoQA datasets"

### Mechanism 3
- Claim: The three-task structure (localization, description, reasoning) progressively builds model capability assessment, with each task requiring successful completion of the previous ones
- Mechanism: Timestamp localization requires basic video understanding, description requires information extraction, and reasoning requires integrating both with commonsense knowledge
- Core assumption: Video understanding capabilities are hierarchical, with each level building on the previous
- Evidence anchors:
  - [section 3.2]: "Counter-intuitive Timestamp Localization Task... This task serves as the basis for the subsequent two main tasks in the three subsets"

## Foundational Learning

- **Concept: Visual commonsense reasoning**
  - Why needed here: Understanding why videos are surprising requires knowing what constitutes normal behavior and how violations of these norms create surprise
  - Quick check question: What makes a video of someone slipping on a banana peel funny versus just unfortunate?

- **Concept: Temporal and spatial reasoning in video**
  - Why needed here: Identifying the specific moments when counter-intuitive events occur requires tracking objects and actions across time and space
  - Quick check question: How would you determine the exact moment when ketchup splatters on someone's face in a video?

- **Concept: Multimodal integration**
  - Why needed here: Combining visual information with language understanding to generate coherent explanations of surprising events
  - Quick check question: How do you integrate what you see in a video with what you know about the world to explain why something is surprising?

## Architecture Onboarding

- **Component map:** Video clips (4.3K total, 24 hours) → Annotation (localization, description, reasoning) → Translation/expansion → Model evaluation using multiple metrics
- **Critical path:** Video preprocessing → Annotation (localization, description, reasoning) → Translation/expansion → Model evaluation using multiple metrics
- **Design tradeoffs:** Free-text answers provide richer evaluation but are harder to score automatically; multiple choice is easier to score but may not test true understanding
- **Failure signatures:** Near-zero scores on traditional metrics for free-text tasks, inability to identify specific timestamp locations, generation of irrelevant or incorrect explanations
- **First 3 experiments:**
  1. Test model on timestamp localization task only - measure intersection-over-union performance
  2. Test model on description task only - compare BLEU/ROUGE/CIDEr scores against ground truth
  3. Test model on reasoning task only - use GPT-4 scoring to evaluate quality of explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance on FunQA change if the dataset were expanded to include more diverse types of surprising videos beyond humor, creativity, and magic?
- Basis in paper: [inferred] The paper suggests that FunQA's three subsets represent distinct types of surprising videos requiring different reasoning capabilities
- Why unresolved: The paper only explores these three types of surprising videos and doesn't test whether other categories of counter-intuitive content would present additional challenges
- What evidence would resolve it: Experiments comparing model performance on FunQA versus an expanded dataset including additional surprising video categories

### Open Question 2
- Question: What specific aspects of GPT-4's evaluation methodology make it more effective at assessing free-text similarity compared to traditional metrics like BLEU and ROUGE?
- Basis in paper: [explicit] The paper states that "intuitively, GPT-4 is found to show preliminary capabilities in assessing free-text in deep understanding"
- Why unresolved: The paper mentions GPT-4's superior performance but doesn't provide detailed analysis of what specific features enable this improvement
- What evidence would resolve it: Comparative analysis of GPT-4's evaluation process versus traditional metrics with specific examples

### Open Question 3
- Question: How would model performance on FunQA change if video samples were extended beyond the current 128-frame limitation to include longer temporal sequences?
- Basis in paper: [explicit] The paper notes that "Otter can only train and test on 128 frames from a video" and suggests this "is insufficient for comprehensive reasoning"
- Why unresolved: The paper identifies this limitation but doesn't test whether extending the temporal window would improve performance on tasks requiring spatial-temporal reasoning
- What evidence would resolve it: Experiments comparing model performance on FunQA using different temporal window sizes

## Limitations
- Dataset relies on humor, creativity, and magic as domains for surprising content, potentially limiting generalizability
- Translation and annotation process from English to Chinese and back may introduce semantic drift
- GPT-4-based scoring depends on specific prompting strategy which is not fully detailed

## Confidence
- **High Confidence:** The dataset's construction methodology, video statistics, and task definitions are clearly specified and reproducible
- **Medium Confidence:** The evaluation metrics and baseline results are well-documented, though the exact GPT-4 scoring prompts remain unspecified
- **Medium Confidence:** The claim that surprising videos require deeper reasoning is supported by existing psychological research

## Next Checks
1. **Cross-linguistic validation:** Test FunQA tasks with models trained only on English data to assess whether translation artifacts affect model performance
2. **Domain generalization test:** Evaluate FunQA-trained models on non-surprising video QA datasets to measure transfer of counter-intuitive reasoning capabilities
3. **Human benchmark replication:** Have human annotators complete the three core tasks on a subset of videos to establish upper performance bounds and validate the difficulty claims