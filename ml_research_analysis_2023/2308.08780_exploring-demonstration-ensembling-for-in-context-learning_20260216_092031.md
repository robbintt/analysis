---
ver: rpa2
title: Exploring Demonstration Ensembling for In-context Learning
arxiv_id: '2308.08780'
source_url: https://arxiv.org/abs/2308.08780
tags:
- ensembling
- bucket
- demonstrations
- language
- buckets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Demonstration Ensembling (DENSE), an alternative
  to standard in-context learning (ICL) that concatenates demonstrations. The key
  idea is to partition demonstrations into non-empty subsets (buckets) and then combine
  the output probabilities resulting from each subset to produce the final prediction.
---

# Exploring Demonstration Ensembling for In-context Learning

## Quick Facts
- arXiv ID: 2308.08780
- Source URL: https://arxiv.org/abs/2308.08780
- Reference count: 16
- Primary result: Weighted max ensembling improves ICL performance by up to 2.4 points over vanilla concatenation

## Executive Summary
This paper introduces Demonstration Ensembling (DENSE), an approach that partitions demonstrations into non-empty subsets (buckets) and combines their output probabilities to produce final predictions. The method addresses limitations of standard in-context learning where concatenating all demonstrations can lead to information dilution and reduced relevance for specific test inputs. The authors explore three ensembling methods—product-of-experts (PoE), mixture-of-experts (MoE), and max ensembling—along with two bucket allocation strategies: arbitrary assignment and clustering-based approaches. Experiments on 12 language tasks using GPT-j (6B) demonstrate that weighted max ensembling consistently outperforms vanilla concatenation by as large as 2.4 average points.

## Method Summary
DENSE partitions n demonstrations into b non-empty buckets, generates predictions for each bucket using the language model, and combines these predictions using one of three ensembling methods. The method can optionally weight buckets based on their similarity to the test input using cosine similarity of embeddings. Three ensembling strategies are explored: product-of-experts (multiplying probabilities), mixture-of-experts (weighted sum of probabilities), and max ensembling (taking the maximum probability). Bucket allocation can be arbitrary (sequential assignment) or based on clustering (similar-together or diverse allocation). The approach is evaluated on 12 language tasks using GPT-j (6B) from HuggingFace, comparing against standard concatenation baselines.

## Key Results
- Weighted max ensembling outperforms vanilla concatenation by 2.4 points in the 6-shot setting
- Performance improves with increasing bucket count up to a certain point, particularly in the 6-shot case
- Similarity-based bucket weighting consistently boosts ensembling performance across all methods
- The improvements are consistent across multiple task types including sentiment analysis, NLI, fact verification, and question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demonstration ensembling improves ICL performance by allowing the model to leverage multiple partial views of the task space rather than a single concatenated view.
- Mechanism: Partitioning demonstrations into buckets and combining outputs from each bucket creates an ensemble that can capture different aspects of the task. The weighted combination based on similarity ensures more relevant demonstrations have greater influence.
- Core assumption: Different demonstrations contain complementary information about the task, and combining predictions from multiple subsets provides more robust generalization than any single subset.
- Evidence anchors: [abstract] "DENSE predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction"; [section 4.2] "weighted max ensembling outperforms concat and concat-sort by 2.4 and 1.2, respectively" in the 6-shot case
- Break condition: If all demonstrations are highly redundant or contain similar information, the benefits of ensembling would diminish since buckets would provide little complementary value.

### Mechanism 2
- Claim: Weighted bucket contributions based on input-similarity improve performance by prioritizing demonstrations that are more relevant to the test example.
- Mechanism: Each bucket's weight is determined by the average cosine similarity between bucket examples and the test input, giving more influence to buckets containing demonstrations similar to the input.
- Core assumption: Language models benefit more from demonstrations that are semantically similar to the input example, as they provide more relevant context for the task.
- Evidence anchors: [section 3.3] "weighing the buckets based on the similarity with inputs boosts the ensembling performance in all cases"; [section 4.2] "weighted max ensembling outperforms concat and concat-sort by 2.4 and 1.2, respectively" in the 6-shot case
- Break condition: If similarity metrics fail to capture true relevance (e.g., when surface similarity doesn't indicate task relevance), weighting could prioritize less useful demonstrations.

### Mechanism 3
- Claim: Increasing the number of buckets improves performance up to a point by creating more granular ensembles that can capture finer distinctions in demonstration relevance.
- Mechanism: As bucket count increases, each bucket contains fewer demonstrations, allowing for more specific similarity-based weighting and reducing the dilution effect of mixing highly relevant and irrelevant demonstrations in the same bucket.
- Core assumption: Smaller buckets can achieve better specialization, with each bucket representing a more coherent subset of demonstrations that share similar characteristics.
- Evidence anchors: [section 4.3] "Using a small b seems to perform worse across the board. Interestingly, the performance improves as b increases for all ensembling methods in the 6-shot setting"; [section 3.1] "DENSE allocates the n demos in D to b non-empty buckets"
- Break condition: If bucket count becomes too large relative to the number of demonstrations, some buckets may become too small to provide meaningful predictions, degrading performance.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The paper builds on ICL as the baseline approach, comparing ensembling against standard concatenation-based ICL
  - Quick check question: What is the standard way to provide demonstrations to language models for few-shot learning?

- Concept: Ensemble methods in machine learning
  - Why needed here: DENSE applies ensemble principles to ICL by combining predictions from multiple demonstration subsets
  - Quick check question: How do product-of-experts and mixture-of-experts differ in combining ensemble member predictions?

- Concept: Cosine similarity and embedding representations
  - Why needed here: Bucket weighting relies on cosine similarity between demonstration embeddings and input embeddings to determine contribution weights
  - Quick check question: Why might cosine similarity be preferred over other distance metrics for comparing text embeddings in this context?

## Architecture Onboarding

- Component map: Demonstration partitioning module -> Bucket prediction engine -> Weight calculation module -> Ensemble combination layer -> Final prediction output

- Critical path:
  1. Partition demonstrations into buckets
  2. Compute embeddings and similarity scores
  3. Calculate bucket weights
  4. Generate predictions for each bucket
  5. Combine predictions using selected ensembling method
  6. Output final prediction

- Design tradeoffs:
  - Bucket size vs. number of buckets: Smaller buckets allow finer-grained specialization but may lack sufficient examples for robust predictions
  - Weighting strategy: Similarity-based weighting improves performance but adds computational overhead
  - Clustering approach: Similar-together clustering may create more coherent buckets but could reduce diversity

- Failure signatures:
  - Degraded performance with high bucket counts relative to demonstration count
  - Poor results when similarity metrics don't align with task relevance
  - Suboptimal performance with arbitrary bucket allocation on tasks requiring specific demonstration ordering

- First 3 experiments:
  1. Implement and compare PoE, MoE, and max ensembling with arbitrary bucket allocation on a simple text classification task
  2. Add similarity-based weighting to the best-performing ensembling method from experiment 1
  3. Compare clustering-based bucket allocation (similar-together vs. diverse) with the best method from experiment 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different bucket allocation strategies affect the performance of ensembling for various types of tasks (e.g., classification, generation, retrieval)?
- Basis in paper: [inferred] The paper briefly mentions that diverse bucket allocation might be beneficial for tasks with more demonstrations, but does not provide a comprehensive analysis of different allocation strategies across task types.
- Why unresolved: The paper focuses on comparing different ensembling methods (PoE, MoE, max) rather than extensively exploring the impact of bucket allocation strategies. The effect of clustering vs. arbitrary allocation is only briefly discussed.
- What evidence would resolve it: A systematic study comparing various bucket allocation strategies (e.g., similarity-based, diversity-based, random) across a wider range of task types, including both classification and generation tasks, would provide insights into the optimal allocation strategy for each task type.

### Open Question 2
- Question: How does the choice of ensembling method (PoE, MoE, max) interact with the number of demonstrations and bucket count to influence performance?
- Basis in paper: [explicit] The paper observes that max ensembling performs best overall, but does not provide a detailed analysis of how the optimal ensembling method varies with the number of demonstrations and bucket count.
- Why unresolved: The paper presents average performance across different numbers of demonstrations and bucket counts, but does not explore the nuanced interactions between these factors and the choice of ensembling method.
- What evidence would resolve it: A comprehensive analysis examining the performance of each ensembling method across different combinations of demonstration counts and bucket counts would reveal the optimal method for each configuration.

### Open Question 3
- Question: How does the length and complexity of the input affect the performance of ensembling compared to concatenation?
- Basis in paper: [inferred] The paper mentions that concatenation can be problematic for lengthy inputs due to context window limitations, but does not directly compare the performance of ensembling and concatenation on long or complex inputs.
- Why unresolved: The paper focuses on the general comparison of ensembling and concatenation, but does not investigate the specific challenges posed by lengthy or complex inputs.
- What evidence would resolve it: Experiments comparing the performance of ensembling and concatenation on tasks with varying input lengths and complexities would reveal the strengths and limitations of each approach in handling different input characteristics.

## Limitations

- Generalizability concerns: Experiments focus exclusively on GPT-j (6B) and 12 specific language tasks, leaving uncertainty about whether the ensembling benefits transfer to other model architectures, sizes, or domains.
- Bucket allocation effectiveness: The improvements might partly reflect gains from basic similarity weighting rather than the ensembling mechanism itself, as the arbitrary bucket allocation baseline is relatively weak.
- Computational overhead: The paper mentions that DENSE is more computationally expensive than concatenation but doesn't provide quantitative comparisons of inference time or memory usage.

## Confidence

**High Confidence**: The core technical contribution (demonstration ensembling via bucket partitioning and probability combination) is clearly specified with explicit equations and reproducible methodology.

**Medium Confidence**: The claim that ensembling "consistently outperforms" concatenation is supported by the experiments but is based on a limited set of tasks and model variants.

**Low Confidence**: The paper's assertion that ensembling works by creating "multiple partial views of the task space" is presented as a mechanism without strong empirical validation.

## Next Checks

1. **Ablation Study on Weighting**: Run experiments comparing DENSE with and without similarity-based weighting across all ensembling methods to isolate the contribution of weighting from the ensembling mechanism itself.

2. **Cross-Model Generalization**: Test DENSE on a different model architecture (e.g., LLaMA or OPT) to verify whether the performance gains are model-specific or represent a general principle for ICL improvement.

3. **Computation Overhead Analysis**: Measure and report the actual increase in inference time and memory usage when using DENSE versus standard concatenation, particularly as the number of buckets increases.