---
ver: rpa2
title: Bounded P-values in Parametric Programming-based Selective Inference
arxiv_id: '2307.11351'
source_url: https://arxiv.org/abs/2307.11351
tags:
- proposed
- selective
- precision
- decision
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to reduce the computational cost of
  parametric programming-based selective inference (PP-based SI) while guaranteeing
  desired precision. The key idea is to compute lower and upper bounds of p-values
  during the data space exploration, allowing early termination when sufficient precision
  is achieved or hypothesis testing decisions can be made.
---

# Bounded P-values in Parametric Programming-based Selective Inference

## Quick Facts
- arXiv ID: 2307.11351
- Source URL: https://arxiv.org/abs/2307.11351
- Reference count: 17
- Key outcome: Reduces computational cost of parametric programming-based selective inference while guaranteeing desired precision

## Executive Summary
This paper addresses the computational challenge of parametric programming-based selective inference (PP-based SI), which can be slow due to exhaustive data space exploration. The authors propose a method that computes lower and upper bounds on p-values during the search process, allowing early termination when sufficient precision is achieved or decisions can be made. Three search strategies are introduced to efficiently improve these bounds, demonstrating that the method achieves the same statistical power as exhaustive PP-based SI while significantly reducing computation time.

## Method Summary
The method reduces computational cost in parametric programming-based selective inference by computing bounds on p-values rather than exhaustive search. During data space exploration, the algorithm computes unions of intervals where the selection event holds, using these to establish upper and lower bounds on selective p-values. Three search strategies guide the exploration: π1 focuses on points near the observed statistic, π2 targets high-density regions, and π3 examines endpoints of the largest interval containing the observed statistic. The search can terminate early when bounds are sufficiently tight or when decisions can be made based on the significance level.

## Key Results
- The proposed method achieves same statistical power as exhaustive PP-based SI for feature selection and DNN attention region identification
- Significant computational time reduction compared to exhaustive search methods
- Type I error rates are well-controlled at specified significance levels (0.05 or 0.01)

## Why This Works (Mechanism)

### Mechanism 1
Early termination of data space exploration is possible by computing bounds on p-values instead of exhaustive search. During each iteration, the algorithm computes intervals where the selection event holds, using these to establish upper and lower bounds on the selective p-value. If bounds are tight enough or sufficient to make a decision, the search can terminate early without exploring the entire data space. This works because the conditional distribution under the null is known (normal for z-test, chi-squared for chi-test), and the computed bounds are guaranteed to contain the true p-value.

### Mechanism 2
The three search strategies reduce the number of iterations needed to achieve desired precision by guiding the selection of the next point in the data space. They maximize the probability mass covered by intervals added in each iteration, either by focusing on points close to the observed statistic (π1), points with high density (π2), or endpoints of the largest interval containing the observed statistic (π3). This works because the unconditional density of the test statistic is available and can be used to guide the search toward regions more likely to improve the bounds.

### Mechanism 3
The bounds on p-values converge to the true p-value as more of the data space is explored. The lower and upper bounds are defined as the minimum and maximum of truncated p-values over explored and unexplored intervals. As more intervals are explored, the bounds become tighter and eventually converge to the true p-value. This works because the sequence of searched intervals is monotonically increasing and covers the entire real line as the number of iterations goes to infinity.

## Foundational Learning

- Concept: Parametric programming for selective inference
  - Why needed here: The method relies on parametric programming to characterize the selection event as linear/quadratic inequalities, which can then be used to compute bounds on p-values
  - Quick check question: What is the main advantage of using parametric programming over other methods for selective inference?

- Concept: Truncated distributions
  - Why needed here: The selective p-value is computed by conditioning on the selection event, resulting in a truncated distribution of the test statistic
  - Quick check question: How does the truncated distribution differ from the unconditional distribution in the context of selective inference?

- Concept: Over-conditioning in selective inference
  - Why needed here: The method addresses over-conditioning, which occurs when additional conditions are introduced for computational tractability, leading to loss of power
  - Quick check question: What is the trade-off between computational tractability and statistical power in selective inference?

## Architecture Onboarding

- Component map: Parametric programming engine -> Interval generation -> Bound computation -> Search strategy selector -> Termination criterion checker -> Decision making
- Critical path: Parametric programming → Interval generation → Bound computation → Termination check → Decision making
- Design tradeoffs:
  - Exhaustive vs. bounded search: Exhaustive search guarantees exact p-value but is computationally expensive, while bounded search trades some precision for speed
  - Precision vs. decision: The method can be used to achieve desired precision or make decisions at given significance level, but not both simultaneously
  - Search strategy choice: Different strategies may be more effective depending on the problem and shape of selection event
- Failure signatures:
  - Bounds do not tighten: Indicates search strategies are not effective or selection event is too complex
  - Termination criterion not met: Suggests desired precision/significance level is too stringent or computational budget insufficient
  - Incorrect decision: May indicate bug in bound computation or issue with parametric programming engine
- First 3 experiments:
  1. Implement parametric programming engine for simple selection event (feature selection in linear model with small number of features)
  2. Implement bound computation module and verify it produces correct bounds for known selection event
  3. Compare performance of three search strategies on synthetic dataset with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's computational efficiency scale with increasing dimensionality of the feature space in linear models? The paper mentions computational cost reduction but doesn't provide scaling analysis for high-dimensional settings. Experiments only test up to p=10 features, which is relatively low-dimensional. Systematic experiments varying p from low to very high dimensions while measuring both accuracy and computational time would resolve this.

### Open Question 2
Can the three proposed search strategies be optimally combined or dynamically selected based on problem characteristics? The paper proposes three search strategies but notes difficulty in evaluating their relative advantages for decision-making. The paper treats search strategies as fixed choices rather than learning which to use when. Development and testing of adaptive strategy selection methods that choose or combine strategies based on intermediate results would resolve this.

### Open Question 3
How would the method perform on neural network architectures beyond those with piecewise-linear activation functions? The method assumes piecewise-linear activation functions and suggests approximation for non-linear functions. The paper only demonstrates on networks with ReLU and similar activations. Systematic evaluation on networks with sigmoid, tanh, or other non-piecewise-linear activations showing accuracy and efficiency trade-offs would resolve this.

## Limitations

- The parametric programming framework may not be applicable to selection procedures involving complex non-linear operations or non-convex optimization
- The effectiveness of search strategies depends on the assumption that unconditional density is a good guide, which may not hold for selection events with complex geometry
- While type I error control is guaranteed, the upper bound may not be tight, potentially leading to conservative conclusions

## Confidence

- **High Confidence**: Theoretical foundation of parametric programming-based selective inference and proof of type I error control
- **Medium Confidence**: Effectiveness of search strategies in reducing computational cost, demonstrated empirically but with limited theoretical guarantees
- **Low Confidence**: Generalizability to complex selection procedures beyond feature selection and attention region identification

## Next Checks

1. **Convergence Analysis**: Empirically evaluate convergence rate of p-value bounds for different types of selection events and data distributions. Compare number of iterations required to achieve given precision level with random search and three proposed strategies.

2. **Generalization to Complex Selection Procedures**: Apply the method to more complex selection procedures such as multi-step feature selection or deep learning model selection, and assess its effectiveness in these settings.

3. **Robustness to Misspecification**: Investigate robustness to misspecification of null distribution or selection event. Evaluate impact of such misspecifications on type I error rate and tightness of p-value bounds.