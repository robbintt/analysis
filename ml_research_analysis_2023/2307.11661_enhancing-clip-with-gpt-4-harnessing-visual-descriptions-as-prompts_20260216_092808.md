---
ver: rpa2
title: 'Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts'
arxiv_id: '2307.11661'
source_url: https://arxiv.org/abs/2307.11661
tags:
- prompt
- clip
- information
- visual
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve CLIP's 0-shot and few-shot
  transfer performance by using GPT-4 to generate visually descriptive text (VDT)
  sentences for each class. The VDT sentences are appended to the prompt template
  to form a prompt ensemble that is used for classification.
---

# Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts

## Quick Facts
- arXiv ID: 2307.11661
- Source URL: https://arxiv.org/abs/2307.11661
- Reference count: 40
- Primary result: GPT-A improves CLIP's 0-shot accuracy on fine-grained datasets by up to 7% using GPT-4 generated visual descriptions

## Executive Summary
This paper introduces GPT-A, a method that improves CLIP's zero-shot and few-shot transfer performance by using GPT-4 to generate visually descriptive text (VDT) sentences for each class. These sentences are appended to prompt templates to form prompt ensembles that provide richer class representations than default prompts. The method also includes a simple few-shot adapter that learns to select the best subset of VDT sentences for each dataset. GPT-A significantly outperforms CLIP's default prompt on specialized fine-grained datasets and surpasses recent few-shot methods like CoCoOP.

## Method Summary
The method generates visually descriptive text (VDT) sentences for each class using GPT-4, then constructs prompt ensembles by appending these sentences to the default CLIP prompt. For zero-shot classification, the VDT prompts are averaged to form class representations. For few-shot learning, a self-attention adapter or MLP adapter is trained to select the most discriminative VDT sentences. The method is evaluated on 12 benchmark datasets, showing substantial improvements in both zero-shot and few-shot transfer performance.

## Key Results
- 0-shot accuracy improvements of ~7% on EuroSAT and DTD, ~4.6% on SUN397, and ~3.3% on CUB compared to CLIP's default prompt
- Few-shot adapter outperforms CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets
- GPT-A-self adapter (self-attention) and GPT-A-mlp adapter (MLP) both effective for few-shot learning

## Why This Works (Mechanism)

### Mechanism 1
GPT-4 generated VDT sentences improve CLIP's zero-shot transfer by providing semantically richer class representations than default prompts. The default prompt ("a photo of {class name}") only encodes the class name, which may not capture distinguishing visual features for fine-grained datasets. GPT-4 generates sentences describing visual attributes (color, shape, composition) for each class. These sentences are ensembled to form richer class representations that better align with visual features in the joint embedding space.

### Mechanism 2
The self-attention adapter learns to select the most discriminative subset of VDT sentences for each class, improving few-shot generalization. Given M GPT-generated sentences per class, the self-attention adapter applies attention over sentence embeddings to learn which sentences are most relevant for distinguishing classes. This creates adapted classifier weights that focus on discriminative visual information rather than spurious features.

### Mechanism 3
Prompt ensembling with VDT reduces CLIP's performance sensitivity to small prompt variations. Instead of relying on a single prompt template, the method constructs multiple prompts by appending different VDT sentences to the template. Averaging these prompts in the embedding space creates a more stable classifier that's less sensitive to prompt engineering variations.

## Foundational Learning

- **Concept**: Contrastive learning and joint embedding spaces
  - Why needed here: CLIP's performance depends on how well image and text embeddings align in a shared space. Understanding this is crucial for grasping why VDT sentences can improve classification.
  - Quick check question: How does CLIP's contrastive loss ensure that images and their corresponding text descriptions have similar embeddings in the joint space?

- **Concept**: Prompt engineering for vision-language models
  - Why needed here: The method relies on constructing effective prompts that guide the text encoder to produce useful class representations. Understanding prompt engineering principles is essential.
  - Quick check question: Why might a simple prompt like "a photo of {class name}" be insufficient for fine-grained classification tasks?

- **Concept**: Self-attention mechanisms
  - Why needed here: The few-shot adapter uses self-attention to select relevant VDT sentences. Understanding how self-attention works is crucial for grasping this component.
  - Quick check question: How does self-attention allow a model to focus on the most relevant parts of its input when processing sequences?

## Architecture Onboarding

- **Component map**: GPT-4 → VDT sentences → CLIP text encoder → Prompt ensemble → Classification
  - For few-shot: Add self-attention/MMP adapter → Adapted classifier weights

- **Critical path**: GPT-4 generates VDT sentences → CLIP text encoder encodes them → Prompt ensemble averages embeddings → Classification layer computes similarity with image embeddings

- **Design tradeoffs**:
  - GPT-4 generation vs. domain expert annotations: GPT-4 is scalable but may generate noisy or inaccurate descriptions
  - Self-attention vs. MLP adapter: Self-attention can select relevant sentences but adds complexity; MLP is simpler but treats all sentences equally
  - Prompt ensembling vs. single prompt: Ensembling is more robust but computationally more expensive

- **Failure signatures**:
  - Poor 0-shot performance: GPT-4 generating irrelevant or inaccurate VDT sentences
  - Overfitting in few-shot: Adapter learning spurious correlations from limited data
  - Inconsistent results across datasets: VDT sentences not generalizable across different domains

- **First 3 experiments**:
  1. **Baseline comparison**: Run CLIP with default prompts vs. VDT prompt ensemble on a single dataset (e.g., CUB) to verify the core mechanism
  2. **GPT-4 quality check**: Manually inspect GPT-4 generated VDT sentences for accuracy and relevance on a small dataset
  3. **Adapter ablation**: Compare GPT-A-self vs. GPT-A-mlp vs. no adapter on a few-shot task to understand the contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of GPT-generated visual descriptions compare to human-annotated visual descriptions in terms of improving CLIP's zero-shot performance across different domains? The paper only provides a direct comparison for the CUB dataset, and the quality of GPT-generated descriptions may vary across different domains and types of objects.

### Open Question 2
What is the impact of the number and diversity of visual descriptive sentences on CLIP's zero-shot and few-shot performance? The paper uses a fixed number of visual descriptive sentences generated by GPT-4 for each class but does not explore the impact of varying the number or diversity of sentences.

### Open Question 3
How does the performance of GPT-generated visual descriptions compare to other methods of incorporating semantic information, such as using large language models to generate class embeddings or leveraging knowledge graphs? The paper mentions related works that use LLMs to generate semantic information for zero-shot classification but does not compare the performance of GPT-generated visual descriptions to these methods.

## Limitations
- Relies heavily on GPT-4's ability to generate accurate and relevant visual descriptions, with limited discussion of cases where GPT-4 might generate misleading or inaccurate descriptions
- Method's scalability to larger, more diverse datasets remains untested
- Limited ablation studies on prompt sensitivity and the impact of different numbers of VDT sentences

## Confidence
- **High confidence**: The core mechanism of using GPT-4 generated VDT sentences to improve CLIP's 0-shot accuracy on fine-grained datasets is well-supported by the reported results
- **Medium confidence**: The few-shot adapter's ability to select discriminative VDT sentences is demonstrated, but results depend on quality of few-shot training data
- **Low confidence**: The claim about prompt ensembling reducing CLIP's sensitivity to prompt variations lacks ablation studies showing sensitivity to different prompt variations

## Next Checks
1. **Cross-dataset generalization test**: Apply GPT-A to a new fine-grained dataset (e.g., Oxford Flowers) not used in the original study to verify if improvements generalize beyond tested datasets

2. **GPT-4 description quality audit**: Manually evaluate a random sample of GPT-4 generated VDT sentences for accuracy and relevance, comparing them against ground truth class descriptions to quantify hallucination rates

3. **Prompt sensitivity analysis**: Conduct controlled experiments varying the number and content of VDT sentences in the prompt ensemble to measure the actual impact on classification stability and identify optimal ensembling strategies