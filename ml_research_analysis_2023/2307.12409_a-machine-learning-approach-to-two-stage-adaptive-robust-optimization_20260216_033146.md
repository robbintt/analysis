---
ver: rpa2
title: A Machine Learning Approach to Two-Stage Adaptive Robust Optimization
arxiv_id: '2307.12409'
source_url: https://arxiv.org/abs/2307.12409
tags:
- algorithm
- problem
- decisions
- strategies
- here-and-now
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a machine learning approach to solve two-stage
  linear adaptive robust optimization (ARO) problems with binary here-and-now variables
  and polyhedral uncertainty sets. The approach encodes the optimal here-and-now decisions,
  the worst-case scenarios associated with the optimal here-and-now decisions, and
  the optimal wait-and-see decisions into a strategy.
---

# A Machine Learning Approach to Two-Stage Adaptive Robust Optimization

## Quick Facts
- arXiv ID: 2307.12409
- Source URL: https://arxiv.org/abs/2307.12409
- Reference count: 5
- One-line primary result: ML approach solves ARO problems drastically faster than state-of-the-art algorithms with high accuracy.

## Executive Summary
This paper proposes a machine learning approach to solve two-stage linear adaptive robust optimization (ARO) problems with binary here-and-now variables and polyhedral uncertainty sets. The approach encodes the optimal here-and-now decisions, worst-case scenarios, and wait-and-see decisions into a strategy representation. By pre-solving multiple similar ARO instances using the column-and-constraint generation algorithm and training ML models on the extracted strategies, the method predicts high-quality solutions for new instances orders of magnitude faster than traditional methods.

## Method Summary
The approach generates a training set by solving multiple ARO instances using the column-and-constraint generation (CCG) algorithm, extracting optimal strategies (here-and-now decisions, worst-case scenarios, and wait-and-see decisions). Three ML models are trained using XGBOOST or Optimal Policy Trees to predict these strategies from input parameters. The method includes a partitioning algorithm to reduce the number of distinct target classes for predicting wait-and-see decisions. For new instances, the approach predicts strategies and solves a deterministic subproblem for the wait-and-see decisions.

## Key Results
- Solves ARO problems drastically faster than state-of-the-art CCG algorithms while maintaining high accuracy
- ML predictions achieve feasibility rates above 95% with suboptimality below 0.01% on test sets
- Partitioning algorithm reduces the number of distinct target classes by 60-80% without sacrificing solution quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ML model reduces solve time by replacing expensive iterative optimization with direct strategy prediction
- Mechanism: CCG algorithm iteratively solves restricted master problems to near-optimality. ML learns to directly predict decisions, bypassing iterative process
- Core assumption: Training and test instances follow similar distributions
- Evidence anchors: [abstract] approach solves ARO problems drastically faster than state-of-the-art algorithms with high accuracy
- Break condition: If test instances fall outside training distribution or ML predictions become highly inaccurate

### Mechanism 2
- Claim: Encoding worst-case scenarios and wait-and-see decisions into strategy representation allows end-to-end prediction
- Mechanism: Predicts tuple (here-and-now decision, worst-case scenario, tight constraints) rather than only here-and-now decisions
- Core assumption: Worst-case scenarios and tight constraints are stable enough to predict with reasonable accuracy
- Evidence anchors: [section 4.1] defines optimal strategy for worst-case scenarios as sd(θ) = (x*, d*)
- Break condition: If predicted worst-case scenario is not true worst-case, solution quality degrades

### Mechanism 3
- Claim: Partitioning tight constraints reduces prediction complexity without sacrificing solution quality
- Mechanism: Groups overlapping tight constraints into partitions and predicts their union
- Core assumption: Tight constraints from different instances overlap substantially
- Evidence anchors: [section 6.4] tight constraints of different instances mostly overlap
- Break condition: If union constraints become too large or too loose, downstream optimization may become infeasible

## Foundational Learning

- Concept: Column-and-constraint generation (CCG) algorithm
  - Why needed here: State-of-the-art iterative method for solving ARO problems to near-optimality, replaced by ML
  - Quick check question: What is the role of the restricted master problem in CCG?

- Concept: Polyhedral uncertainty sets and extreme points
  - Why needed here: Worst-case scenarios are extreme points of uncertainty set; understanding this is key to predicting them
  - Quick check question: How does size of uncertainty set affect number of possible worst-case scenarios?

- Concept: Tight constraints in deterministic optimization
  - Why needed here: Wait-and-see decisions depend on which constraints are tight; ML predicts these to ensure feasibility
  - Quick check question: How are tight constraints identified in a linear program?

## Architecture Onboarding

- Component map: Data generation (Sample θ → CCG → extract strategies) → Model training (train classifiers for sx, sd, sy) → Prediction (θ̂, d̂ → ŝx, ŝd, ŝy → ŷ)
- Critical path: θ → CCG → (sx, sd, sy) → ML training → θ̂, d̂ → (ŝx, ŝd, ŝy) → ŷ
- Design tradeoffs: (a) Using exact vs. suboptimal strategies in training; (b) Number of predictions k in Phase 3; (c) Partitioning vs. no partitioning for tight constraints
- Failure signatures: (a) Infeasible ŷ due to wrong tight constraints; (b) Large suboptimality if predicted worst-case is not true worst-case; (c) Degraded accuracy if training and test distributions differ
- First 3 experiments:
  1. Small synthetic facility location: Verify feasibility and speed-up on n=3, m=3
  2. Inventory control: Test partitioning algorithm effectiveness on large-scale instances
  3. Unit commitment: Compare solve time and suboptimality under relaxed CCG tolerances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ML approach vary with different uncertainty set shapes (ellipsoidal, discrete) beyond polyhedral sets?
- Basis in paper: [inferred] Focuses on polyhedral uncertainty sets; no exploration of other uncertainty set shapes
- Why unresolved: Paper does not test or analyze impact of using non-polyhedral uncertainty sets
- What evidence would resolve it: Numerical experiments comparing approach's performance across various uncertainty set shapes

### Open Question 2
- Question: What is the impact of ML model's interpretability on decision-making in real-world applications of ARO?
- Basis in paper: [inferred] Mentions OPT provides interpretability but does not explore practical impact
- Why unresolved: Paper does not provide evidence or analysis on how interpretability affects real-world decision-making
- What evidence would resolve it: Case studies or user feedback from applying approach in real-world scenarios

### Open Question 3
- Question: How does proposed approach scale with increasing problem dimensions in terms of computational efficiency and solution quality?
- Basis in paper: [explicit] Mentions approach solves problems drastically faster but does not provide detailed scalability analysis
- Why unresolved: Paper does not provide comprehensive analysis of how approach's performance scales with problem dimensions
- What evidence would resolve it: Detailed numerical experiments testing approach on problems with varying dimensions

## Limitations

- Assumes sufficient similarity between training and test instances, which may not hold in practice
- Relies on predicting worst-case scenarios and tight constraints that could be unstable if uncertainty set or problem structure changes
- CCG algorithm's computational cost for generating training data could be prohibitive for very large-scale problems

## Confidence

- High confidence: Replacing iterative CCG with direct ML predictions for here-and-now decisions is sound and well-supported
- Medium confidence: Strategy encoding approach (predicting worst-case scenarios and tight constraints) is theoretically valid but may suffer from instability in practice
- Medium confidence: Partitioning algorithm's effectiveness depends on empirical overlap of constraints, which is problem-dependent

## Next Checks

1. **Distribution shift test**: Systematically vary parameter distributions between training and test sets to quantify accuracy degradation
2. **Worst-case stability test**: Measure how often predicted worst-case scenarios match true worst-case scenarios across different uncertainty set sizes
3. **Partitioning sensitivity test**: Evaluate partitioning algorithm's performance across diverse problem instances with varying constraint overlap