---
ver: rpa2
title: 'NCART: Neural Classification and Regression Tree for Tabular Data'
arxiv_id: '2307.12198'
source_url: https://arxiv.org/abs/2307.12198
tags:
- learning
- data
- deep
- decision
- ncart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NCART, a novel interpretable neural network
  for tabular data analysis. NCART combines the interpretability of decision trees
  with the end-to-end capabilities of deep learning by replacing fully-connected layers
  in a ResNet architecture with multiple differentiable oblivious decision trees.
---

# NCART: Neural Classification and Regression Tree for Tabular Data

## Quick Facts
- **arXiv ID**: 2307.12198
- **Source URL**: https://arxiv.org/abs/2307.12198
- **Reference count**: 40
- **Primary result**: NCART achieves competitive performance on tabular data while maintaining interpretability and faster computation than deep learning models

## Executive Summary
NCART is a novel neural network architecture for tabular data that combines the interpretability of decision trees with the end-to-end learning capabilities of deep learning. The model replaces fully-connected layers in ResNet with multiple differentiable oblivious decision trees, allowing for both accurate predictions and feature importance extraction. Extensive experiments on 20 datasets demonstrate that NCART achieves competitive performance compared to state-of-the-art deep learning models and tree-based methods, particularly excelling on small-scale datasets.

## Method Summary
NCART is a modified ResNet architecture that replaces fully-connected layers with multiple differentiable oblivious decision trees (ODTs). The model incorporates feature selection layers using sparse projections to identify influential features, and ensembles multiple ODTs within each block for robust predictions. The architecture supports both classification and regression tasks, with the output layer adapting based on the specific problem type. Training is performed end-to-end using standard backpropagation.

## Key Results
- NCART achieves competitive performance across 20 datasets compared to state-of-the-art deep learning models and tree-based methods
- Superior performance on small-scale datasets with lower computational costs than deep learning alternatives
- Provides valuable insights into feature importance through its tree-based structure, similar to traditional decision tree models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NCART replaces fully-connected layers with differentiable oblivious decision trees to maintain interpretability while enabling end-to-end training
- Mechanism: The paper replaces fully-connected layers in ResNet with multiple differentiable ODTs, where each ODT performs soft, differentiable splits on the same feature at each level
- Core assumption: The approximation of standard decision trees by summing multiple ODTs is accurate enough for practical performance
- Evidence anchors:
  - [abstract] "NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees."
  - [section 3.1] "Instead of using the traditional decision tree, we opt for the use of the oblivious decision tree [39]. This allows for the incorporation of a 'soften' decision function within the internal tree nodes, ensuring interpretability while also enabling the differentiable aspect of the model."
- Break condition: If the number of ODTs required to approximate a standard decision tree becomes prohibitively large, or if the soft splits cannot sufficiently approximate hard thresholds for certain datasets

### Mechanism 2
- Claim: Feature selection layers improve model performance by reducing dimensionality and focusing on influential features
- Mechanism: NCART optionally includes feature selection blocks that learn sparse projection matrices to identify and use only the most relevant features for each differentiable tree
- Core assumption: Sparse projections learned via Sparsemax or entmax functions can effectively identify relevant features without significant information loss
- Evidence anchors:
  - [section 3.2] "The feature selection is adopted to discover the most influential features and improve the model performance... A learnable matrix A, h is a sparse function similar to the softmax, but able to output sparse probabilities..."
  - [abstract] "NCART exhibits interpretability, generating feature importance similar to those produced by traditional decision tree models."
- Break condition: If the feature selection process consistently selects irrelevant features or if the sparse projection loses critical information needed for accurate predictions

### Mechanism 3
- Claim: The ensemble of multiple differentiable trees in each NCART block provides robust and generalizable predictions
- Mechanism: Each NCART block contains multiple differentiable ODTs, and their outputs are combined via weighted voting to produce the final prediction
- Core assumption: The ensemble approach effectively captures complex decision boundaries while maintaining interpretability
- Evidence anchors:
  - [section 3.2] "All the outputs are together sent to the last layer to take a weighted vote and generate the final output..."
  - [abstract] "Extensive numerical experiments reveal that NCART achieves competitive performance across datasets of different sizes..."
- Break condition: If the ensemble approach leads to overfitting on small datasets or fails to capture important interactions between features

## Foundational Learning

- Concept: Decision Trees and Gradient Boosting
  - Why needed here: Understanding how decision trees split data and how gradient boosting combines multiple trees is essential for grasping NCART's architecture
  - Quick check question: What is the difference between a standard decision tree and an oblivious decision tree?

- Concept: Differentiable Programming and Softmax/Sparsemax Functions
  - Why needed here: NCART uses differentiable functions to replace hard decision boundaries in trees, requiring knowledge of continuous relaxations of discrete operations
  - Quick check question: How does the sigmoid function enable differentiability in decision tree splits?

- Concept: Residual Networks and Skip Connections
  - Why needed here: NCART is based on ResNet architecture, so understanding residual learning and skip connections is crucial for implementing the model
  - Quick check question: What problem does the residual connection solve in deep neural networks?

## Architecture Onboarding

- Component map: Input → Batch Normalization → Feature Selection (optional) → Multiple NCART Blocks → Feature Selection (optional) → Output
- Critical path: Data preprocessing → Feature selection (if enabled) → Ensemble of differentiable trees → Residual connection to next block
- Design tradeoffs:
  - Number of differentiable trees per block vs. computational cost
  - Feature selection enabled vs. using all features
  - Depth of the network vs. risk of overfitting on small datasets
- Failure signatures:
  - Poor performance on regression tasks (consistent with paper findings)
  - High variance in results across different datasets
  - GPU memory overflow with high-dimensional categorical features
- First 3 experiments:
  1. Implement a single NCART block with 2 differentiable trees on a small binary classification dataset (e.g., diabetes) and compare with a standard ResNet
  2. Add feature selection to the NCART block and measure impact on both performance and interpretability
  3. Increase the number of trees per block and measure the trade-off between performance and computation time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NCART's performance scale with increasing dataset size compared to tree-based methods and other deep learning approaches?
- Basis in paper: [inferred] The paper mentions that NCART is well-suited for datasets of varying sizes and has lower computational costs than state-of-the-art deep learning models, but doesn't provide detailed scaling analysis
- Why unresolved: The paper provides performance comparisons on 20 datasets of varying sizes, but doesn't systematically analyze how NCART's performance and computational requirements change as dataset size increases
- What evidence would resolve it: A comprehensive study comparing NCART's accuracy, training time, and memory usage across datasets spanning several orders of magnitude in size, alongside equivalent benchmarks for tree-based methods and other deep learning approaches

### Open Question 2
- Question: What is the optimal number of differentiable oblivious decision trees (ODTs) per NCART block for different types of tabular datasets?
- Basis in paper: [explicit] The paper mentions that NCART combines multiple ODTs to approximate the representation ability of a standard decision tree, but doesn't explore the impact of varying this number
- Why unresolved: The paper uses a fixed number of ODTs per block but doesn't investigate how this hyperparameter affects performance across different datasets or problem types
- What evidence would resolve it: A systematic ablation study varying the number of ODTs per block and measuring the resulting impact on accuracy, training stability, and computational efficiency across diverse tabular datasets

### Open Question 3
- Question: How does NCART's feature importance compare to that of traditional decision trees when used for model interpretability and feature selection in real-world applications?
- Basis in paper: [explicit] The paper discusses NCART's interpretability and compares its feature importance calculation to traditional decision trees, but doesn't validate this against practical feature selection scenarios
- Why unresolved: While the paper demonstrates that NCART can generate feature importance scores similar to decision trees, it doesn't validate whether these scores are practically useful for feature selection or if they align with domain knowledge in real applications
- What evidence would resolve it: A user study or case study applying NCART's feature importance to real-world feature selection tasks, comparing the results to domain expert knowledge and measuring the downstream impact on model performance and interpretability

## Limitations
- The paper does not provide detailed implementation specifics for the differentiable oblivious decision trees
- Performance on regression tasks appears significantly weaker than classification
- Computational efficiency gains over deep learning models need further validation

## Confidence
- **High confidence**: The core architectural concept of replacing fully-connected layers with differentiable trees is well-defined and technically sound
- **Medium confidence**: Performance claims on classification tasks are supported by extensive experiments across 20 datasets, though regression results are less convincing
- **Low confidence**: The interpretability claims require further validation, as the paper does not provide detailed case studies demonstrating how NCART's feature importance compares to traditional tree models

## Next Checks
1. Implement a minimal NCART prototype and verify that the differentiable tree splits can be learned through backpropagation on a simple synthetic dataset
2. Compare NCART's runtime performance against standard ResNet and XGBoost on a medium-sized dataset to validate computational efficiency claims
3. Conduct ablation studies removing feature selection and varying the number of trees per block to understand their impact on both performance and interpretability