---
ver: rpa2
title: 'PrOnto: Language Model Evaluations for 859 Languages'
arxiv_id: '2305.12612'
source_url: https://arxiv.org/abs/2305.12612
tags:
- languages
- language
- which
- tasks
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of evaluation datasets for low-resource
  languages by creating a new dataset called PrOnto. The dataset is constructed by
  aligning verses in the New Testament with those in the English OntoNotes corpus
  and projecting annotations from English to the target language.
---

# PrOnto: Language Model Evaluations for 859 Languages

## Quick Facts
- arXiv ID: 2305.12612
- Source URL: https://arxiv.org/abs/2305.12612
- Reference count: 10
- Key outcome: New dataset PrOnto created for evaluating pretrained language models across 859 languages using annotation projection from English OntoNotes

## Executive Summary
PrOnto addresses the lack of evaluation datasets for low-resource languages by creating a new dataset through annotation projection. The method aligns verses in New Testament translations with those in English OntoNotes and projects annotations without requiring manual annotation. The authors apply this method to 1051 New Testament translations across 859 languages and conduct experiments demonstrating the dataset's utility for assessing pretrained language model quality.

## Method Summary
The PrOnto method involves aligning verses between target language Bible translations and the OntoNotes English corpus, then projecting sentence-level annotations from English to the target language without manual annotation. The dataset covers 859 languages and includes five task types: Non-pronominal Mention Counting, Proper Noun in Subject, Sentence Mood, Same Sense, and Same Argument Count. Models are trained using HuggingFace AutoModelForSequenceClassification with default hyperparameters.

## Key Results
- PrOnto dataset successfully created for 859 languages using annotation projection from English OntoNotes
- Experiments show monolingual models generally outperform multilingual ones across tasks
- Performance degrades for low-resource languages as expected, validating the dataset's discriminative power

## Why This Works (Mechanism)

### Mechanism 1
Projection of sentence-level annotations from English OntoNotes to other languages is feasible because New Testament verses are highly consistent across translations. Aligning verses across translations provides sentence-like alignments, enabling annotation projection without token-level alignment noise. This relies on verse boundaries being consistent enough across translations to serve as reliable alignment anchors.

### Mechanism 2
Sentence-level properties like sentence mood are preserved across translations, making them suitable for projection. Semantic-pragmatic properties of sentences are more likely to be preserved than formal properties during translation. This assumes properties such as sentence mood are more semantic-pragmatic than form-based, so they remain stable across translations.

### Mechanism 3
Pretrained models can be evaluated on projected tasks to assess their quality across languages. Models perform differently on tasks with varying difficulty and linguistic properties, revealing quality differences. This assumes different models will show differential performance on projected tasks based on their quality.

## Foundational Learning

- Concept: Cross-lingual annotation projection
  - Why needed here: The core method relies on projecting annotations from English to target languages without manual annotation.
  - Quick check question: What are the key assumptions and limitations of annotation projection methods?

- Concept: Bible verse alignment across translations
  - Why needed here: The method depends on consistent verse boundaries across different language translations.
  - Quick check question: How consistent are verse boundaries across different Bible translations?

- Concept: Pretrained language model evaluation
  - Why needed here: The dataset is designed to evaluate the quality of pretrained language models across many languages.
  - Quick check question: What makes a good evaluation task for assessing pretrained language model quality?

## Architecture Onboarding

- Component map: Parse OntoNotes -> Align verses -> Project annotations -> Generate tasks -> Evaluate models
- Critical path: Parse OntoNotes → Align verses → Project annotations → Generate tasks → Evaluate models
- Design tradeoffs: Using sentence-level projection avoids token alignment complexity but may miss some annotation types; focusing on Bible text limits generalizability to other domains.
- Failure signatures: Poor alignment quality leads to noisy tasks; inconsistent property preservation across languages reduces task validity; low-resource models may perform at chance levels.
- First 3 experiments:
  1. Evaluate a monolingual BERT model on ERV (English) tasks to establish baseline performance.
  2. Evaluate the same model on a non-English language (e.g., French) to test cross-lingual task validity.
  3. Evaluate a low-resource language model (e.g., Wolof) to assess task difficulty and discriminative power.

## Open Questions the Paper Calls Out

### Open Question 1
How do the projected annotations perform on languages that are extremely typologically distant from English, such as those with non-configurational word order? The paper mentions conducting experiments on a wide range of languages with respect to typological variables, but doesn't provide detailed results for extremely distant languages.

### Open Question 2
What is the minimum amount of training data required for the projected annotations to be useful for model evaluation? The paper mentions that some low-resource languages with very small amounts of data (around 500K tokens) still showed some model differentiation, but doesn't establish a clear threshold.

### Open Question 3
How sensitive are the projected annotations to differences in Bible translation style or register? The paper compares two English translations (ERV and WBT) and notes performance differences, but doesn't explore other translation styles or registers.

## Limitations

- Domain specificity: The method relies entirely on Bible text, which may not generalize to other domains.
- Verse alignment quality: The paper acknowledges that verse boundaries don't always align perfectly with sentence boundaries, requiring discarding of combined verses.
- Low-resource performance: Several tasks show near-chance performance for RoBERTa models, suggesting the projected tasks may be too difficult.

## Confidence

- High confidence: The technical feasibility of the projection method and dataset creation pipeline is well-demonstrated.
- Medium confidence: The claim that the dataset enables meaningful cross-lingual model comparison is partially supported by experiments.
- Low confidence: The assertion that projected sentence-level properties (particularly sentence mood) are preserved across translations lacks direct empirical validation.

## Next Checks

1. Measure the proportion of verses successfully aligned between OntoNotes ERV and target translations, and quantify how many are discarded due to combined verses. Compare alignment consistency across different language families.

2. Select 100 sentences from 5 diverse target languages and have bilingual annotators verify projected annotations for sentence mood and proper noun in subject tasks. Calculate inter-annotator agreement and compare with projected labels.

3. Evaluate the same pretrained models on PrOnto tasks and on human-annotated evaluation datasets from different domains (news, social media, literature) for the same languages. Measure correlation between performance across domains to assess generalizability.