---
ver: rpa2
title: 'Modelling Sentiment Analysis: LLMs and data augmentation techniques'
arxiv_id: '2311.04139'
source_url: https://arxiv.org/abs/2311.04139
tags:
- data
- techniques
- augmentation
- sentiment
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates binary sentiment classification on a small
  dataset (2,000 training examples) using large language models (LLMs) and data augmentation
  techniques. The study employs RoBERTa and XLNet models, exploring fine-tuning strategies
  including soft prompting and two data augmentation methods: synonym replacement
  (nlpaug) and synthetic data generation with GPT-2.'
---

# Modelling Sentiment Analysis: LLMs and data augmentation techniques

## Quick Facts
- arXiv ID: 2311.04139
- Source URL: https://arxiv.org/abs/2311.04139
- Reference count: 2
- Primary result: Data augmentation and soft prompting significantly improve binary sentiment classification accuracy on small datasets, with optimal results at 2-4× augmentation and single soft prompt.

## Executive Summary
This paper investigates binary sentiment classification on a small dataset (2,000 training examples) using large language models and data augmentation techniques. The study employs RoBERTa and XLNet models, exploring fine-tuning strategies including soft prompting and two data augmentation methods: synonym replacement (nlpaug) and synthetic data generation with GPT-2. Results show that augmenting the dataset 2-4 times with nlpaug improves accuracy to 0.77-0.78, while combining GPT-2 augmentation with soft prompting yields the best performance (0.78). Using more than one soft prompt or excessive augmentation (nlpaug > 4, GPT-2 > 4) degrades performance. XLNet underperforms compared to RoBERTa across all configurations. The findings demonstrate that carefully controlled data augmentation and soft prompting significantly enhance sentiment classification accuracy on small datasets, with optimal results achieved through moderate augmentation levels and single-prompt fine-tuning.

## Method Summary
The study fine-tunes RoBERTa and XLNet models on a binary sentiment classification task using a small dataset of 2,000 examples. Three main strategies are tested: (1) data augmentation using nlpaug synonym replacement at 2-4× the original dataset size, (2) synthetic data generation using GPT-2, and (3) soft prompting by concatenating task-specific prompts at the beginning of each sentence. Models are evaluated on an 11,000-sentence validation set, with accuracy scores obtained from Kaggle. The experiments systematically vary augmentation levels and prompt quantities to identify optimal configurations.

## Key Results
- RoBERTa outperforms XLNet across all configurations in binary sentiment classification
- Data augmentation with nlpaug at 2-4× improves accuracy to 0.77-0.78
- Combining GPT-2 augmentation with single soft prompting yields the best performance (0.78)
- Using multiple soft prompts or excessive augmentation (>4×) degrades model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled data augmentation improves model performance on small datasets.
- Mechanism: Generating synthetic examples via synonym replacement and GPT-2 preserves semantic content while expanding the training distribution, reducing overfitting on limited data.
- Core assumption: The augmented data remains representative of the true data distribution and does not introduce label noise.
- Evidence anchors:
  - [abstract] "augmenting the dataset 2-4 times with nlpaug improves accuracy to 0.77-0.78"
  - [section] "After trying GPT for data augmentation without Soft Prompting, the results were not better than nlpaug with 2 augmentations, although after mixing GPT augmentations and soft prompting, the result was the best one obtained."
  - [corpus] Weak; no direct citations found in the 25 nearest neighbors for this specific augmentation approach.
- Break condition: Augmentation beyond 4× or with excessive synonym diversity causes performance degradation due to semantic drift.

### Mechanism 2
- Claim: Soft prompting guides the PLM toward the desired sentiment classification task.
- Mechanism: Concatenating task-specific prompts at the beginning of each sentence provides contextual priming, effectively conditioning the model without full fine-tuning.
- Core assumption: The prompt embeddings are task-relevant and do not interfere with the semantic meaning of the input.
- Evidence anchors:
  - [abstract] "combining GPT-2 augmentation with soft prompting yields the best performance (0.78)"
  - [section] "the performance of the models increased with one single soft prompt, but increasing the number of prompts did not increase the accuracy of the model."
  - [corpus] Weak; no direct evidence in neighbor papers for soft prompting specifically.
- Break condition: Using multiple soft prompts or overly long prompts degrades accuracy by confusing the model.

### Mechanism 3
- Claim: RoBERTa outperforms XLNet on this binary sentiment classification task.
- Mechanism: RoBERTa's dynamic masking and larger batch sizes during pre-training yield better generalization for sentiment classification on small datasets.
- Core assumption: Pre-training objectives and architecture differences directly translate to downstream task performance.
- Evidence anchors:
  - [abstract] "XLNet underperforms compared to RoBERTa across all configurations."
  - [section] "XLNet provided state-of-the-art results in other studies but did not perform as well as BERT related models in this one."
  - [corpus] Weak; no direct comparison in the nearest neighbors, though one neighbor mentions "Fine-tuning BERT with Bidirectional LSTM for Fine-grained Movie Reviews Sentiment Analysis."
- Break condition: If dataset size increases significantly, XLNet's autoregressive objective might recover relative performance.

## Foundational Learning

- Concept: Transformers and attention mechanisms
  - Why needed here: LLMs like RoBERTa and XLNet are transformer-based; understanding attention is critical for interpreting how these models process text.
  - Quick check question: How does self-attention differ from standard recurrent architectures in handling long-range dependencies?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: The study compares full fine-tuning with soft prompting; knowing when each is appropriate prevents resource waste.
  - Quick check question: What are the computational trade-offs between fine-tuning all layers versus only updating prompt embeddings?

- Concept: Data augmentation strategies
  - Why needed here: The paper relies on synonym replacement and synthetic data generation; understanding augmentation limits prevents semantic drift.
  - Quick check question: How can you measure whether augmented data introduces label noise or semantic drift?

## Architecture Onboarding

- Component map: Data preprocessing → Augmentation (nlpaug, GPT-2) → Model (RoBERTa/XLNet) → Soft prompting → Fine-tuning → Prediction
- Critical path: Preprocessing → Augmentation → Model fine-tuning → Evaluation
- Design tradeoffs:
  - More augmentation → Risk of semantic drift; less augmentation → Insufficient data coverage.
  - Single prompt → Better focus; multiple prompts → Potential confusion.
- Failure signatures:
  - Accuracy drops sharply when augmentation > 4×.
  - Performance degrades when using >1 soft prompt.
  - XLNet consistently underperforms RoBERTa in this setup.
- First 3 experiments:
  1. Baseline: Train RoBERTa with no augmentation, no soft prompting.
  2. nlpaug augmentation (×2) with single soft prompt.
  3. GPT-2 augmentation (×2) with single soft prompt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of soft prompts for sentiment classification with RoBERTa and XLNet models?
- Basis in paper: [explicit] The paper states "the performance of the models increased with one single soft prompt, but increasing the number of prompts did not increase the accuracy of the model."
- Why unresolved: The study only tested one and seven soft prompts, leaving uncertainty about whether other quantities might yield better results.
- What evidence would resolve it: Systematic testing of soft prompt quantities ranging from 2 to 10 to identify the optimal number for sentiment classification accuracy.

### Open Question 2
- Question: How does the quality of synthetic data generated by GPT-2 affect sentiment classification performance?
- Basis in paper: [inferred] The paper mentions using GPT-2 for synthetic data generation but does not evaluate the quality or diversity of the generated data.
- Why unresolved: The study focuses on quantity of augmentation rather than quality assessment of synthetic examples.
- What evidence would resolve it: Human evaluation or automated metrics (e.g., perplexity, semantic similarity) to assess the quality and relevance of GPT-2 generated synthetic data for sentiment classification.

### Open Question 3
- Question: What is the impact of different preprocessing techniques on sentiment classification with small datasets?
- Basis in paper: [explicit] The paper mentions basic preprocessing but does not explore different preprocessing strategies or their effects on model performance.
- Why unresolved: Only minimal preprocessing was applied, and the study does not compare alternative preprocessing methods.
- What evidence would resolve it: Comparative analysis of various preprocessing techniques (e.g., different tokenization, stopword removal strategies, stemming) and their impact on sentiment classification accuracy.

### Open Question 4
- Question: How does the length and complexity of soft prompts affect sentiment classification performance?
- Basis in paper: [inferred] The paper uses soft prompts but does not investigate how prompt length or complexity influences model performance.
- Why unresolved: The study does not explore variations in soft prompt design beyond quantity.
- What evidence would resolve it: Systematic testing of soft prompts with varying lengths and complexities to determine their effect on sentiment classification accuracy.

## Limitations
- Data distribution unknown: The original dataset's class balance, domain specificity, and inherent noise levels are not described, making it difficult to assess whether the observed improvements generalize beyond this specific small dataset.
- Augmentation quality not measured: While the paper reports accuracy gains, it does not evaluate whether augmented data introduces semantic drift or label noise—critical factors that could explain why performance degrades beyond 4× augmentation.
- Soft prompt design unspecified: The exact prompt templates used are not provided, limiting reproducibility and making it unclear whether the prompts are truly task-relevant or merely acting as regularization.

## Confidence

- **High confidence**: RoBERTa outperforms XLNet in this binary sentiment task, given consistent experimental results across all configurations tested.
- **Medium confidence**: Data augmentation improves accuracy on small datasets, but the optimal augmentation level (2-4×) may be dataset-dependent and not universally applicable.
- **Low confidence**: Soft prompting is the primary driver of performance gains, as the paper does not isolate its effect from data augmentation or provide ablation studies.

## Next Checks
1. **Ablation study**: Run experiments with (a) no augmentation, no soft prompt; (b) augmentation only; (c) soft prompt only; (d) both. This isolates the contribution of each technique.
2. **Semantic drift analysis**: Measure cosine similarity between original and augmented sentences to quantify semantic drift. If drift exceeds a threshold (e.g., 0.8), consider reducing augmentation diversity.
3. **Generalization test**: Repeat the experiment on a larger dataset (e.g., 10,000+ examples) to determine whether the 2-4× augmentation sweet spot holds or if the benefit diminishes with more data.