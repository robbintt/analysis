---
ver: rpa2
title: Efficient and Joint Hyperparameter and Architecture Search for Collaborative
  Filtering
arxiv_id: '2307.11004'
source_url: https://arxiv.org/abs/2307.11004
tags:
- search
- space
- hyperparameters
- architecture
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an automated joint hyperparameter and architecture
  search method for collaborative filtering (CF). The method reduces the large search
  space by screening hyperparameters based on their performance ranking across datasets
  and employs a two-stage search algorithm.
---

# Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering

## Quick Facts
- **arXiv ID**: 2307.11004
- **Source URL**: https://arxiv.org/abs/2307.11004
- **Reference count**: 40
- **Primary result**: Proposes a two-stage search method that outperforms hand-designed and previous searched CF models, achieving up to 13.02% improvement in Recall@20 on MovieLens-100K

## Executive Summary
This paper introduces an automated joint hyperparameter and architecture search method for collaborative filtering that addresses the computational challenges of exploring large search spaces. The method employs a two-stage search algorithm where the first stage uses subsampled datasets to efficiently search for candidate architectures and hyperparameters, and the second stage transfers this knowledge to fine-tune on the full dataset. By screening hyperparameters based on performance rankings across datasets and leveraging a surrogate model (Random Forest + BORE), the approach achieves superior performance while reducing computational costs. Experiments on four real-world datasets demonstrate significant improvements over both hand-designed and previous AutoML approaches for CF.

## Method Summary
The proposed method consists of a two-stage search framework that first uses subsampled datasets to explore the search space efficiently, then transfers knowledge to the full dataset for fine-tuning. Hyperparameter screening reduces the search space by evaluating individual hyperparameters across datasets and selecting only top-performing values. The architecture search space includes input features, embedding operations (NN-based and graph-based), interaction functions, and prediction functions. The surrogate model (Random Forest + BORE) guides the search in the first stage using 20% subsampled data, then top candidates are fine-tuned on the full dataset. The method is evaluated on MovieLens-100K, MovieLens-1M, Yelp, and Amazon-Book datasets using Recall@20 and NDCG@20 metrics.

## Key Results
- Achieved up to 13.02% improvement in Recall@20 on MovieLens-100K compared to hand-designed models
- Outperformed both hand-designed and previous searched CF models across all four datasets
- Ablation studies validated the effectiveness of hyperparameter screening, sampling ratio selection, and two-stage search approach
- Demonstrated that joint optimization of hyperparameters and architectures is more effective than sequential optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Screening hyperparameter choices based on performance ranking improves search efficiency and effectiveness.
- Mechanism: The method evaluates each hyperparameter individually while fixing others, then shrinks the search space by selecting only the top-performing values for each hyperparameter. This reduces the number of configurations to evaluate without sacrificing performance.
- Core assumption: The relative ranking of hyperparameter values is consistent across different architectures and datasets.
- Evidence anchors: [abstract] "we reduce the space by screening out usefulness hyperparameter choices through a comprehensive understanding of individual hyperparameters"

### Mechanism 2
- Claim: The two-stage search algorithm with subsampling enables efficient knowledge transfer from small to large datasets.
- Mechanism: First stage uses subsampled datasets to explore the search space and train a surrogate model. Second stage uses the trained surrogate model to guide search on the full dataset, leveraging knowledge learned from the smaller samples.
- Core assumption: The relative performance ranking of architectures is preserved between subsampled and full datasets.
- Evidence anchors: [abstract] "In the first stage, we leverage knowledge from subsampled datasets to reduce evaluation costs; in the second stage, we efficiently fine-tune top candidate models on the whole dataset"

### Mechanism 3
- Claim: Joint optimization of hyperparameters and architectures is more effective than sequential optimization.
- Mechanism: By considering hyperparameters and architectures together in the same search space, the method can find configurations where the optimal hyperparameters are specific to each architecture, rather than using generic hyperparameters.
- Core assumption: The optimal hyperparameters for a CF architecture depend on the specific architecture design.
- Evidence anchors: [abstract] "However, existing works either search architectures or hyperparameters while ignoring the fact they are intrinsically related and should be considered together"

## Foundational Learning

- **Concept**: Collaborative Filtering (CF)
  - Why needed here: The entire paper is about optimizing CF models, so understanding the CF problem formulation and evaluation metrics is essential.
  - Quick check question: What is the difference between user-based and item-based CF approaches?

- **Concept**: Automated Machine Learning (AutoML)
  - Why needed here: The paper uses AutoML techniques (NAS and HPO) to automate the design of CF models.
  - Quick check question: How does Bayesian Optimization differ from Random Search in hyperparameter optimization?

- **Concept**: Graph Neural Networks (GNNs)
  - Why needed here: The paper includes GNN-based architectures in the search space for CF models.
  - Quick check question: What is the key difference between GraphSAGE and GCN in how they aggregate neighbor information?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Search space definition -> Screening -> Subsampling -> Stage 1 search -> Surrogate model transfer -> Stage 2 search -> Evaluation

- **Critical path**: Data preprocessing → Search space definition → Screening → Subsampling → Stage 1 search → Surrogate model transfer → Stage 2 search → Evaluation

- **Design tradeoffs**:
  - Sampling ratio vs. evaluation time vs. search quality
  - Search space size vs. computational cost
  - Surrogate model accuracy vs. training time
  - Joint vs. sequential hyperparameter and architecture search

- **Failure signatures**:
  - Poor performance on full dataset despite good performance on subsampled data
  - Surrogate model predictions diverge from actual evaluations
  - Search gets stuck in local optima
  - Hyperparameter screening removes actually good values

- **First 3 experiments**:
  1. Run the full pipeline on MovieLens-100K with default settings to verify basic functionality
  2. Test the subsampling consistency by comparing architecture rankings between sampled and full datasets
  3. Validate the hyperparameter screening by comparing search results with and without screening

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed two-stage search algorithm scale to extremely large-scale recommendation datasets (e.g., with billions of interactions)?
- Basis in paper: [inferred] The paper discusses the effectiveness of the two-stage approach on datasets up to Amazon-Book (3M records), but does not evaluate scalability to much larger datasets or discuss potential bottlenecks in memory or computation time.
- Why unresolved: The paper's experimental setup and ablation studies focus on moderate-sized datasets. Scaling to industrial-scale datasets would require addressing issues like distributed training, more efficient sampling strategies, or hierarchical search approaches that are not covered in the current work.
- What evidence would resolve it: Experiments demonstrating the algorithm's performance and efficiency on datasets with billions of interactions, along with analysis of memory usage, training time, and any modifications needed for large-scale deployment.

### Open Question 2
- Question: What is the impact of the proposed hyperparameter screening method on search performance when applied to other recommendation tasks beyond collaborative filtering, such as sequential recommendation or multi-behavior recommendation?
- Basis in paper: [explicit] The paper mentions that the hyperparameter screening is based on understanding individual hyperparameters through ranking across datasets, and demonstrates effectiveness specifically for CF tasks. However, it does not test this methodology on other recommendation paradigms.
- Why unresolved: The screening methodology relies on the assumption that certain hyperparameters have consistent ranking distributions across different datasets. This assumption may not hold for recommendation tasks with different characteristics (e.g., sequential patterns, multiple interaction types).
- What evidence would resolve it: Applying the screening method to other recommendation tasks and comparing the search efficiency and final model performance against baseline methods that do not use screening.

### Open Question 3
- Question: How sensitive is the two-stage search framework to the choice of sampling ratio (γ) and what is the theoretical basis for selecting the optimal ratio for different types of datasets?
- Basis in paper: [explicit] The paper conducts experiments with different sampling ratios (5%, 10%, 20%, 50%) and finds that 20% works well, but does not provide theoretical justification for this choice or explore the relationship between sampling ratio and dataset characteristics like sparsity, user/item distribution, or interaction density.
- Why unresolved: The selection of sampling ratio appears to be empirical without a clear theoretical framework explaining why certain ratios work better for certain dataset properties. The paper shows that consistency (SRCC) is higher at certain ratios but doesn't establish a predictive model for ratio selection.
- What evidence would resolve it: A theoretical analysis connecting sampling ratio to dataset properties (e.g., sparsity, power-law distribution of user/item frequencies) and empirical validation showing that the proposed ratio selection method generalizes across diverse dataset types.

## Limitations
- Effectiveness of hyperparameter screening relies on the assumption that ranking distributions remain consistent across different architectures and datasets
- Subsampling approach's success depends on maintaining relative performance rankings between sampled and full datasets, with limited empirical validation
- Computational overhead of the two-stage search, particularly surrogate model training, is not thoroughly analyzed
- Generalization to non-interaction-based CF datasets or cold-start scenarios remains unexplored

## Confidence

- **High Confidence**: The overall experimental methodology and evaluation framework are sound, with clear results showing improvements over baseline methods.
- **Medium Confidence**: The two-stage search mechanism is well-motivated, but its effectiveness depends on untested assumptions about knowledge transfer between subsampled and full datasets.
- **Medium Confidence**: The hyperparameter screening approach is theoretically justified but lacks comprehensive ablation studies across different CF architectures.

## Next Checks

1. **Transfer Consistency Validation**: Systematically test the preservation of architecture rankings between subsampled and full datasets across all four datasets, quantifying the correlation of performance rankings.

2. **Screening Robustness Analysis**: Conduct controlled experiments varying the screening threshold and evaluating how different levels of hyperparameter space reduction affect final search performance.

3. **Computational Overhead Profiling**: Measure and compare the total computation time of the two-stage search versus single-stage approaches, including surrogate model training costs, to assess practical efficiency gains.