---
ver: rpa2
title: Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex
  Questions
arxiv_id: '2311.13982'
source_url: https://arxiv.org/abs/2311.13982
tags:
- film
- director
- answer
- when
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Probabilistic Tree-of-thought Reasoning (ProbTree) addresses knowledge-intensive
  complex question answering by decomposing questions into query trees and performing
  probabilistic reasoning over them. The method combines closed-book QA (parametric
  knowledge), open-book QA (retrieved external knowledge), and child-aggregating QA
  (reasoning with sub-question answers) to answer questions from leaf to root nodes,
  selecting the most confident answer based on explanation logits.
---

# Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions

## Quick Facts
- arXiv ID: 2311.13982
- Source URL: https://arxiv.org/abs/2311.13982
- Reference count: 19
- Primary result: Achieves 64.1%, 41.5%, and 71.8% F1 scores on HotpotQA, MuSiQue, and 2WikiMultiHopQA datasets, outperforming state-of-the-art methods by 3.9-8.0% F1

## Executive Summary
Probabilistic Tree-of-thought Reasoning (ProbTree) addresses knowledge-intensive complex question answering by decomposing questions into query trees and performing probabilistic reasoning over them. The method combines closed-book QA (parametric knowledge), open-book QA (retrieved external knowledge), and child-aggregating QA (reasoning with sub-question answers) to answer questions from leaf to root nodes, selecting the most confident answer based on explanation logits. On three Complex QA datasets, ProbTree achieves significant improvements over state-of-the-art methods by leveraging tree structure for error recovery and probabilistic reasoning for confidence calibration.

## Method Summary
ProbTree consists of two phases: query tree generation and probabilistic reasoning. In the understanding phase, complex questions are decomposed into query trees using few-shot prompting with demonstrations from IRCoT. The reasoning phase employs three QA modules: child-aggregating QA that reasons with child question-answer pairs, open-book QA that reasons with retrieved external knowledge using BM25 retrieval, and closed-book QA that employs parametric knowledge encoded in the LLM. Answers are selected based on confidence scores calculated from explanation logits, enabling recovery from local errors through hierarchical reasoning.

## Key Results
- Outperforms state-of-the-art methods by 3.9-8.0% F1 on three Complex QA datasets
- Achieves 64.1% F1 on HotpotQA, 41.5% F1 on MuSiQue, and 71.8% F1 on 2WikiMultiHopQA
- Demonstrates tree structure's advantage in error recovery compared to chain-based approaches
- Shows effective integration of parametric and external knowledge through probabilistic reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Hierarchical tree structure provides broader sight and error recovery compared to chain-based reasoning
- LLMs can look backward and forward through child nodes, allowing recovery from local errors that would propagate in chains
- Core assumption: LLMs can effectively meta-reason with information from child nodes to recover from wrong decompositions or incorrect answers
- Evidence anchors: [abstract] hierarchical structure enables global reasoning with child node information; [section 3.2] LLMs meta-reason with child node information for comprehensive answers
- Break condition: If LLMs cannot effectively integrate information from child nodes, or if error propagation becomes too severe before recovery

### Mechanism 2
- Probabilistic reasoning with explanation logits quantifies answer confidence and integrates parametric and external knowledge
- Likelihood of explanation indicates LLM confidence, allowing selection of most confident answer from three QA modules
- Core assumption: LLMs are well-calibrated and low probability indicates lack of knowledge
- Evidence anchors: [abstract] LLMs choose confident answers from closed-book and open-book QA; [section 3.2] confidence calculated as average log-likelihood of explanation
- Break condition: If explanation logits do not correlate with actual answer correctness, or if LLM calibration is poor

### Mechanism 3
- Simultaneous Closed-book and Open-book QA eliminates negative retrieval problem
- Evaluates both parametric knowledge (Closed-book) and external knowledge (Open-book) simultaneously, choosing more confident answer
- Core assumption: Parametric knowledge provides reliable alternative when external knowledge is misleading
- Evidence anchors: [abstract] simultaneous evaluation eliminates negative retrieval problem; [section 3.2] three QA modules enable confident answer selection
- Break condition: If parametric knowledge is insufficient or if LLMs cannot effectively access it through Closed-book QA

## Foundational Learning

- Concept: Tree data structures and traversal algorithms
  - Why needed here: The query tree is the fundamental data structure that organizes complex questions and enables hierarchical reasoning
  - Quick check question: Can you explain the difference between BFS and DFS traversal and when each would be appropriate for tree processing?

- Concept: Probabilistic reasoning and confidence calibration
  - Why needed here: The method relies on quantifying answer confidence using explanation logits and selecting the most confident answer from multiple QA modules
  - Quick check question: How would you implement a confidence scoring system for LLM outputs based on explanation likelihood?

- Concept: Retrieval-augmented generation and knowledge integration
  - Why needed here: The method integrates both parametric knowledge (through Closed-book QA) and external knowledge (through Open-book QA) to answer complex questions
  - Quick check question: What are the key challenges in combining retrieved external knowledge with LLM-generated reasoning?

## Architecture Onboarding

- Component map: Query tree generation → Probabilistic reasoning with three QA modules → Answer selection based on confidence scores
- Critical path: Complex question → Query tree decomposition → Leaf node answering (Closed/Open-book QA) → Non-leaf node answering (Child-aggregating QA) → Root node answer
- Design tradeoffs: Tree structure provides error recovery but adds computational overhead; probabilistic reasoning adds complexity but improves accuracy; simultaneous QA modules increase API calls but reduce negative retrieval
- Failure signatures: Poor tree decomposition leading to incorrect answers; low confidence scores across all QA modules; error propagation despite tree structure; retrieval failures affecting Open-book QA
- First 3 experiments:
  1. Implement query tree generation with few-shot prompting and verify JSON output structure
  2. Implement single node answering with all three QA modules and confidence scoring
  3. Chain together tree traversal and answer selection to produce complete system output

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided. However, based on the limitations section and the discussion of the method, several implicit questions emerge regarding error recovery mechanisms, computational efficiency, and backend model dependencies that warrant further investigation.

## Limitations

- Limited empirical evidence showing how often error recovery actually occurs versus when errors propagate
- Assumption of LLM well-calibration not validated with calibration curves or reliability diagrams
- No ablation studies on retrieval failures to measure negative retrieval elimination effectiveness

## Confidence

- Tree structure error recovery mechanism: Medium - supported by theoretical arguments but limited empirical validation
- Probabilistic confidence scoring: Medium - intuitive but unverified calibration properties
- Negative retrieval elimination: Medium - plausible mechanism but no ablation studies on retrieval failures

## Next Checks

1. Conduct ablation studies removing the tree structure to quantify actual error recovery rates versus chain-based approaches
2. Generate calibration curves for explanation logits to verify they correlate with true answer correctness probabilities
3. Test the system on adversarial examples with deliberately misleading retrieved knowledge to measure negative retrieval elimination effectiveness