---
ver: rpa2
title: 'EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise
  Optimization'
arxiv_id: '2309.11531'
source_url: https://arxiv.org/abs/2309.11531
tags:
- quantization
- hessian
- eptq
- loss
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EPTQ is a post-training quantization method that optimizes quantization
  parameters across the entire network simultaneously, guided by a novel Label-Free
  Hessian approximation. This approximation enables the method to focus optimization
  on layers more sensitive to quantization errors without requiring labeled data.
---

# EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization

## Quick Facts
- arXiv ID: 2309.11531
- Source URL: https://arxiv.org/abs/2309.11531
- Reference count: 40
- EPTQ achieves state-of-the-art accuracy across diverse tasks including ImageNet classification, COCO object detection, and Pascal VOC semantic segmentation

## Executive Summary
EPTQ introduces a novel post-training quantization method that optimizes quantization parameters across entire neural networks simultaneously using a Label-Free Hessian approximation. This approximation enables the method to identify and prioritize quantization-sensitive layers without requiring labeled data. By combining adaptive knowledge distillation with Hessian-aware weighting and mixed precision search, EPTQ achieves superior accuracy compared to existing PTQ methods across diverse architectures including CNNs, transformers, hybrids, and MLP-only models.

## Method Summary
EPTQ is a post-training quantization method that optimizes quantization parameters network-wide using Label-Free Hessian approximation to identify sensitive layers. The method uses knowledge distillation with adaptive layer weighting based on Hessian trace estimates, enabling effective optimization with small unlabeled representative datasets. It incorporates mixed precision search to improve accuracy under memory constraints and requires no manual layer selection during optimization.

## Key Results
- Achieves state-of-the-art accuracy across diverse tasks including ImageNet classification, COCO object detection, and Pascal VOC semantic segmentation
- Demonstrates compatibility with various architectures including CNNs, transformers, hybrids, and MLP-only models
- Requires no manual layer selection during optimization and works with small unlabeled datasets

## Why This Works (Mechanism)

### Mechanism 1
The Label-Free Hessian approximation allows EPTQ to identify and prioritize quantization-sensitive layers without requiring labeled data. The method approximates the Hessian trace of the task loss using only unlabeled data by leveraging the property that many common loss functions have Hessians independent of labels. This trace indicates which layers are most sensitive to quantization errors. The core assumption is that Ey,x[∇rLtask(y,r)|r=f(x)ui,j(x)] ≈ 0 holds for converged models, allowing the label-free approximation to be valid.

### Mechanism 2
Adaptive knowledge distillation with Hessian-aware weighting improves quantization accuracy compared to uniform weighting. During optimization, layers with higher Hessian trace values receive greater attention in the knowledge distillation loss, focusing optimization resources where they matter most for maintaining task accuracy. The core assumption is that the relative ordering of Hessian traces across layers remains stable during the short optimization process.

### Mechanism 3
The Hutchinson algorithm modification enables efficient computation of the Hessian trace approximation without computing full Jacobians. Instead of computing the full Jacobian matrix Jz(x), the method uses random vectors to estimate Tr(JzT Jz) through multiple forward passes, making the computation tractable even for large models. The core assumption is that the trace approximation converges with a reasonable number of random vectors (M ~ 50).

## Foundational Learning

- **Concept: Hessian matrix and its trace**
  - Why needed here: The Hessian trace serves as the sensitivity metric for identifying quantization-critical layers
  - Quick check question: What does a high Hessian trace value indicate about a layer's sensitivity to quantization?

- **Concept: Knowledge distillation in neural networks**
  - Why needed here: EPTQ uses knowledge distillation to transfer information from the floating-point model to the quantized model
  - Quick check question: How does the adaptive weighting modify the standard knowledge distillation loss?

- **Concept: Quantization step size initialization via MSE minimization**
  - Why needed here: Proper initialization of quantization parameters is crucial before the adaptive optimization begins
  - Quick check question: Why is MSE minimization used for initial quantization step size estimation?

## Architecture Onboarding

- **Component map:** Pre-trained floating-point model → BatchNorm folding → MSE-based initialization → Bit-width search → LFH computation → Adaptive optimization → Mixed precision refinement → Quantized model
- **Critical path:** BatchNorm folding → MSE-based initialization → Bit-width search → LFH computation → Adaptive optimization → Mixed precision refinement
- **Design tradeoffs:**
  - Memory vs accuracy: Mixed precision allows trading memory usage for accuracy by assigning different bit-widths to different layers
  - Computation time vs optimization quality: More optimization iterations improve accuracy but increase runtime (80K vs 20K iterations)
  - Dataset size vs performance: Surprisingly, even small datasets (64 samples) work well, but larger datasets may provide marginal improvements
- **Failure signatures:**
  - Accuracy drop during optimization: Could indicate poor Hessian approximation or insufficient learning rate
  - No improvement with more iterations: May suggest convergence issues or incorrect initialization
  - Memory constraints violated: Bit-width search didn't find feasible solution within constraints
- **First 3 experiments:**
  1. Quantize ResNet18 with 4-bit weights and 8-bit activations using EPTQ, compare to baseline AdaRound
  2. Test LFH approximation quality by comparing against true Hessian trace on a small network
  3. Validate mixed precision search by constraining model size and measuring accuracy degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the Label-Free Hessian (LFH) approximation perform for loss functions that do not satisfy Equation 2, such as Sharpness-Aware Minimization (SAM)?
- **Open Question 2:** What is the optimal number of optimization iterations for EPTQ to achieve the best trade-off between accuracy and computational efficiency across different model architectures?
- **Open Question 3:** How sensitive is EPTQ's performance to the choice of representative dataset size and composition?

## Limitations

- The theoretical justification relies on assumptions about model convergence that may not hold in practice
- The sensitivity of accuracy gains to optimization hyperparameters remains unclear
- The ILP formulation and solver details for mixed precision search require clarification for exact reproduction

## Confidence

- Label-Free Hessian approximation: Medium confidence
- Adaptive weighting mechanism: Medium confidence  
- Mixed precision search: Medium confidence

## Next Checks

1. **Stability analysis:** Run EPTQ optimization on a fixed model and dataset with multiple random seeds to measure variance in final accuracy
2. **Dataset sensitivity:** Compare EPTQ performance using 64, 256, 1024, and 4096 representative samples to quantify dataset size impact
3. **Optimization trajectory:** Monitor Hessian trace estimates and layer sensitivities throughout optimization to verify assumption stability