---
ver: rpa2
title: Multi-label and Multi-target Sampling of Machine Annotation for Computational
  Stance Detection
arxiv_id: '2311.04495'
source_url: https://arxiv.org/abs/2311.04495
tags:
- stance
- detection
- annotation
- label
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of large language models (LLMs)
  for automated stance detection annotation, identifying sensitivity to task instructions
  and inherent biases as key challenges. To address these issues, the authors propose
  a multi-label and multi-target sampling strategy that enhances annotation quality
  by incorporating explicit/implicit target referral and adversarial multi-target
  sampling.
---

# Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection

## Quick Facts
- arXiv ID: 2311.04495
- Source URL: https://arxiv.org/abs/2311.04495
- Reference count: 13
- The paper proposes multi-label and multi-target sampling strategies to improve machine annotation quality for stance detection tasks.

## Executive Summary
This paper addresses the challenge of using large language models (LLMs) for automated stance detection annotation, identifying sensitivity to task instructions and inherent biases as key obstacles. The authors propose a multi-label and multi-target sampling strategy that enhances annotation quality by incorporating explicit/implicit target referral and adversarial multi-target sampling. Experiments on benchmark datasets show significant improvements in both in-domain and cross-domain performance, with relative gains up to 61.2% in out-of-domain settings. The approach demonstrates that machine-annotated data, when properly optimized, can effectively replace or complement human annotation for stance detection tasks.

## Method Summary
The paper employs zero-shot inference with LLMs (Alpaca-13B, Vicuna-13B, and GPT-3.5-turbo) to generate stance detection labels for training data. The proposed methods include an enhanced two-hop instruction process that first identifies whether stance targets are explicitly or implicitly mentioned, followed by stance prediction. Additionally, an adversarial multi-target sampling strategy extracts noun phrases from text to serve as additional stance targets, creating a more diverse training dataset. A RoBERTa-base model is then trained on the machine-annotated data and evaluated on benchmark stance detection corpora.

## Key Results
- Multi-label instruction improves in-domain performance on stance detection tasks
- Multi-target sampling enhances out-of-domain and cross-target performance
- Machine-annotated data achieves competitive results compared to human-annotated data
- Vicuna-13B with enhanced two-hop instruction becomes comparable to GPT-3.5-turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-label instruction improves stance detection performance by explicitly modeling target referral information
- Mechanism: The two-hop instruction process first identifies whether the stance target is explicitly or implicitly mentioned in the text, then predicts the stance label based on this context-target relationship
- Core assumption: Stance detection models benefit from explicitly modeling whether the stance target is directly mentioned in the text
- Evidence anchors:
  - [abstract] "we add one step to describe the context-object relation (i.e., implicit or explicit mention)"
  - [section 4] "we add one step to describe the context-object relation (i.e., implicit or explicit mention). With the enhanced two-hop instruction, Vicuna-13B becomes comparable to the GPT-3.5-turbo"
  - [corpus] Weak - the paper does not provide corpus-level evidence for this mechanism
- Break condition: If the target referral information does not correlate with stance prediction accuracy, this mechanism would fail

### Mechanism 2
- Claim: Multi-target sampling reduces model bias by training on diverse stance targets
- Mechanism: The adversarial multi-target sampling extracts noun phrases from text and uses them as additional stance targets, creating a more diverse training dataset
- Core assumption: Training on multiple stance targets reduces overfitting to specific target patterns
- Evidence anchors:
  - [abstract] "we apply an adversarial multi-target sampling for machine annotation... This adversarial multi-target sampling helps models to condition their prediction more on the given target"
  - [section 4] "To reduce superficial target-related patterns and biases from single target labeling, following previous work (Liu et al., 2023), we apply an adversarial multi-target sampling for machine annotation"
  - [corpus] Weak - the paper mentions extracting noun phrases but does not provide corpus-level evidence for this mechanism
- Break condition: If the extracted noun phrases do not represent valid stance targets, this mechanism would fail

### Mechanism 3
- Claim: Machine-annotated data can effectively replace human-annotated data for stance detection
- Mechanism: The proposed multi-label and multi-target sampling strategy optimizes machine annotation quality to match human annotation performance
- Core assumption: Optimized machine annotation can achieve comparable performance to human annotation
- Evidence anchors:
  - [abstract] "Experimental results on the benchmark stance detection corpora show that our method can significantly improve performance and learning efficacy"
  - [section 5] "when applying our proposed methods, we observe that: (1) The multi-label instruction can improve in-domain performance... (2) The multi-target sampling can improve out-of-domain and cross-target performance"
  - [corpus] Weak - the paper does not provide corpus-level evidence for this mechanism
- Break condition: If the machine-annotated data contains too much noise or bias, it cannot effectively replace human annotation

## Foundational Learning

- Concept: Stance detection as target-based classification
  - Why needed here: Understanding that stance detection requires both text and specified target as input is crucial for designing the annotation framework
  - Quick check question: What are the two essential components of stance detection input?

- Concept: Large language models for zero-shot inference
  - Why needed here: The paper leverages LLMs for automated stance labeling without requiring model training
  - Quick check question: How do LLMs perform stance detection without being fine-tuned on the task?

- Concept: Adversarial training and sampling
  - Why needed here: The multi-target sampling strategy uses adversarial approaches to create diverse training data
  - Quick check question: What is the purpose of using adversarial sampling in machine annotation?

## Architecture Onboarding

- Component map: Text samples and stance targets → Multi-label instruction module → Multi-target sampling module → Machine annotation module → Stance labels
- Critical path: Text → Multi-label instruction → Multi-target sampling → Machine annotation → Output labels
- Design tradeoffs:
  - Quality vs. quantity: More diverse targets may introduce noise
  - Complexity vs. performance: Two-hop instruction adds complexity but improves results
  - Open vs. closed models: Using open-source models vs. GPT-3.5-turbo
- Failure signatures:
  - Inconsistent performance across different prompts
  - High variance in annotation quality
  - Poor generalization to unseen targets
- First 3 experiments:
  1. Compare vanilla vs. enhanced two-hop instruction performance on a single dataset
  2. Evaluate multi-target sampling impact on out-of-domain performance
  3. Test different LLM models (Alpaca-13B, Vicuna-13B, GPT-3.5-turbo) for annotation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-label and multi-target sampling machine annotation compare to human annotation on stance detection tasks?
- Basis in paper: [explicit] The paper discusses the potential of machine annotation as an alternative to human annotation and presents methods to improve its quality.
- Why unresolved: The paper focuses on improving machine annotation quality but does not provide a direct comparison with human annotation performance.
- What evidence would resolve it: Conducting a study where human annotators and the proposed machine annotation methods label the same dataset, followed by a performance comparison.

### Open Question 2
- Question: What are the specific biases in stance detection tasks that machine annotation methods might introduce, and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions that models exhibit biases on target description and label arrangement, and discusses the challenges of capturing subtle semantic features.
- Why unresolved: While the paper introduces methods to improve machine annotation, it does not deeply explore the specific biases that might be introduced or provide detailed mitigation strategies.
- What evidence would resolve it: Analyzing the machine-annotated data for biases, developing bias detection and mitigation techniques, and evaluating their effectiveness.

### Open Question 3
- Question: How does the proposed multi-label and multi-target sampling strategy perform in other natural language processing tasks beyond stance detection?
- Basis in paper: [inferred] The paper focuses on stance detection but the methods introduced could potentially be applied to other NLP tasks.
- Why unresolved: The paper does not explore the application of the proposed methods to other tasks.
- What evidence would resolve it: Applying the multi-label and multi-target sampling strategy to other NLP tasks and evaluating its performance compared to existing methods.

### Open Question 4
- Question: How does the quality of machine-annotated data affect the performance of downstream tasks that use the data for training?
- Basis in paper: [explicit] The paper discusses the use of machine-annotated data for training a supervised stance detection model and evaluates its performance.
- Why unresolved: The paper does not explore the long-term impact of using machine-annotated data on the performance of downstream tasks.
- What evidence would resolve it: Conducting a longitudinal study where models are trained and evaluated on machine-annotated data over time, and comparing their performance with models trained on human-annotated data.

## Limitations
- Heavy reliance on LLM outputs without sufficient validation of annotation quality
- Limited discussion of noise and bias inherent in machine-annotated data
- Evaluation focuses on downstream task performance rather than direct measurement of annotation accuracy
- Missing detailed implementation specifics for critical components

## Confidence

- Multi-label instruction effectiveness: Medium confidence
- Multi-target sampling benefits: Medium confidence
- Machine annotation as human replacement: Low confidence

## Next Checks

1. Conduct a human evaluation study comparing machine-annotated labels against human annotations on a held-out sample to directly measure annotation quality and identify systematic biases in the proposed approach.

2. Implement ablation studies to isolate the individual contributions of the multi-label instruction and multi-target sampling components, providing clearer understanding of which mechanisms drive performance improvements.

3. Test the proposed approach on additional stance detection datasets and domains not included in the original evaluation to assess generalizability and identify potential failure modes in different contexts.