---
ver: rpa2
title: Evaluation of GPT and BERT-based models on identifying protein-protein interactions
  in biomedical text
arxiv_id: '2303.17728'
source_url: https://arxiv.org/abs/2303.17728
tags:
- protein
- language
- performance
- gpt-3
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated GPT and BERT-based models for protein-protein
  interaction (PPI) identification in biomedical literature. The authors compared
  three GPT models (GPT-3, GPT-3.5, GPT-4) with seven BERT-based models using a manually
  curated LLL corpus containing 164 PPIs in 77 sentences.
---

# Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text

## Quick Facts
- arXiv ID: 2303.17728
- Source URL: https://arxiv.org/abs/2303.17728
- Reference count: 40
- Key outcome: BERT-based models outperformed GPT models on PPI identification, with PubMedBERT achieving highest precision (85.25%) and F1-score (86.47%), while GPT-4 showed comparable performance to top BERT models when provided with protein names

## Executive Summary
This study evaluates GPT and BERT-based models for identifying protein-protein interactions (PPIs) in biomedical literature. The authors compare three GPT models (GPT-3, GPT-3.5, GPT-4) with seven BERT-based models using a manually curated LLL corpus. BERT-based models generally outperformed GPT models, with PubMedBERT achieving the highest precision and F1-score. Notably, GPT-4 demonstrated comparable performance to top BERT models when provided with protein names, achieving precision of 83.34%, recall of 76.57%, and F1-score of 79.18%. These results suggest that GPT models can effectively detect PPIs from biomedical text and show promise for literature mining applications, though BERT models still maintain an edge in overall performance.

## Method Summary
The study uses the LLL corpus containing 77 sentences with 164 manually annotated PPIs. GPT models were evaluated using zero-shot learning with three different prompt formulations, including base prompts, prompts with protein names, and prompts with normalized protein names. BERT-based models were fine-tuned using 10-fold cross-validation with document-level partitioning to prevent data leakage. The authors explored temperature optimization for GPT-3 using 11 temperature values (0.0 to 1.0 in 0.1 increments) and found 0.1 to be optimal. Negative samples were generated by pairing non-interacting proteins in the same sentences. Performance was evaluated using precision, recall, and F1-score metrics.

## Key Results
- BERT-based models achieved the best overall performance, with BioBERT achieving highest recall (91.95%) and F1-score (86.84%), while PubMedBERT achieved highest precision (85.25%)
- GPT-4 demonstrated comparable performance to top BERT models when provided with protein names, achieving precision of 83.34%, recall of 76.57%, and F1-score of 79.18%
- Temperature optimization significantly impacted GPT-3 performance, with 0.1 achieving the highest overall performance
- BioM-ALBERT-xxlarge achieved the highest recall (91.95%) among all models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models can effectively detect PPIs when provided with entity names, achieving comparable performance to BERT models.
- Mechanism: GPT models leverage zero-shot learning capabilities to infer relationships between entities from contextual clues in biomedical text, with performance improving significantly when given explicit protein names as references.
- Core assumption: The model's pre-training on diverse text corpora provides sufficient semantic understanding to generalize to biomedical entity relationships.
- Evidence anchors:
  - [abstract]: "GPT-4 demonstrated comparable performance to top BERT models when provided with protein names, achieving precision of 83.34%, recall of 76.57%, and F1-score of 79.18%"
  - [section]: "when provided with protein names, the precision of the GPT models improved significantly, approaching that of the best-performing PubMedBERT model, which achieved 85.17% precision. Specifically, the GPT-4 model with protein names provided achieved 83.71% precision."
  - [corpus]: Weak evidence - no direct corpus analysis of GPT's training data coverage of biomedical terms.
- Break condition: Performance degrades significantly if protein names are ambiguous, misspelled, or not present in the model's training data.

### Mechanism 2
- Claim: BERT-based models achieve superior overall performance due to fine-tuning on domain-specific biomedical corpora.
- Mechanism: Fine-tuning BERT on PubMed abstracts and PMC full-text articles allows the model to learn specialized biomedical language patterns and entity relationships that general-purpose models lack.
- Core assumption: Domain-specific pre-training data is essential for capturing the nuanced language and terminology of biomedical literature.
- Evidence anchors:
  - [abstract]: "BERT-based models achieved the best overall performance, with BioBERT achieving the highest recall (91.95%) and F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%)"
  - [section]: "BERT-based models demonstrate impressive performance, they require fine-tuning with supervised learning, which takes considerable time and technical expertise."
  - [corpus]: Weak evidence - no detailed analysis of the specific biomedical corpora used for fine-tuning.
- Break condition: Performance drops if fine-tuning data is insufficient, biased, or not representative of the target biomedical domain.

### Mechanism 3
- Claim: The temperature parameter in GPT models significantly impacts PPI identification performance.
- Mechanism: Adjusting the temperature parameter controls the model's creativity vs. precision, with lower temperatures producing more deterministic and accurate outputs for structured tasks like PPI extraction.
- Core assumption: PPI identification is a structured task requiring precise outputs rather than creative text generation.
- Evidence anchors:
  - [section]: "We explored the impact of this parameter in PPI identification using OpenAI API and 11 temperatures (minimum=0, maximum=1, increment=0.1). A temperature of 0.1 demonstrated the highest overall performance of GPT-3, thus used in the present study."
  - [section]: "The parameter ranges between 0 (the least creative) and 1 (the most creative)."
  - [corpus]: No corpus evidence available for this mechanism.
- Break condition: Temperature optimization may not generalize across different biomedical tasks or datasets.

## Foundational Learning

- Concept: Protein-Protein Interaction (PPI) identification in biomedical text
  - Why needed here: Understanding what constitutes a PPI and how they are typically represented in scientific literature is essential for evaluating model performance and designing appropriate evaluation metrics.
  - Quick check question: What are the key features that distinguish interacting protein pairs from non-interacting pairs in biomedical sentences?

- Concept: Transformer architecture and its variants (BERT vs GPT)
  - Why needed here: Different transformer architectures (bidirectional vs autoregressive) have distinct strengths that impact their performance on relation extraction tasks.
  - Quick check question: How do the masked language modeling objective in BERT and the autoregressive objective in GPT influence their ability to capture entity relationships?

- Concept: Zero-shot vs. fine-tuned learning paradigms
  - Why needed here: The study compares zero-shot GPT models with fine-tuned BERT models, highlighting the trade-offs between accessibility and performance.
  - Quick check question: What are the advantages and limitations of zero-shot learning compared to fine-tuned models for specialized domains like biomedicine?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (entity normalization, keyword replacement)
  - GPT models (3 versions, each with variations)
  - BERT-based models (7 variants)
  - Evaluation framework (10-fold cross-validation)
  - Temperature optimization module (for GPT-3)
  - Protein name dictionary (for GPT experiments)

- Critical path:
  1. Load and preprocess LLL corpus
  2. Generate negative samples for BERT training
  3. Fine-tune BERT models with document-level 10-fold cross-validation
  4. Run GPT models with optimized temperature parameter
  5. Evaluate and compare performance metrics

- Design tradeoffs:
  - GPT models offer accessibility without fine-tuning but require careful prompt engineering
  - BERT models provide superior performance but need extensive fine-tuning and domain expertise
  - Document-level folding prevents data leakage but reduces training data per fold

- Failure signatures:
  - GPT models produce inconsistent outputs across runs (indicates temperature issues)
  - BERT models show overfitting (high training performance but low test performance)
  - Both model types exhibit low recall (suggests entity recognition issues)

- First 3 experiments:
  1. Test GPT-3 with temperature parameter sweep (0.0 to 1.0 in 0.1 increments) on a small subset of the data
  2. Fine-tune BioBERT on a single fold to verify the training pipeline works
  3. Compare GPT-4 performance with and without protein name dictionary on 5 sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ontology-based methods like Interaction Network Ontology (INO) affect GPT model performance in PPI identification compared to BERT models?
- Basis in paper: [explicit] The authors note "One potential area of future research is exploring the use of ontology to improve literature mining for PPI identification" and mention previous work using INO for similar tasks
- Why unresolved: The study did not incorporate any ontology-based methods and only compared GPT and BERT models using standard approaches
- What evidence would resolve it: A direct comparison of GPT and BERT models when enhanced with ontology-based methods like INO, measuring performance differences in precision, recall, and F1-scores

### Open Question 2
- Question: What specific improvements could be achieved by fine-tuning GPT-4 on biomedical corpora like PubMed and PMC for PPI identification?
- Basis in paper: [explicit] The authors state "improving GPT-4 with biomedical corpora like PubMed and PMC for PPI identification is warranted" and note that BERT models required fine-tuning while GPT models did not
- Why unresolved: The study used only zero-shot and few-shot learning approaches with GPT models, without any domain-specific fine-tuning
- What evidence would resolve it: Performance comparison of fine-tuned GPT-4 versus zero-shot GPT-4 and fine-tuned BERT models on the same PPI datasets, with detailed analysis of which biomedical corpora yield the best improvements

### Open Question 3
- Question: How does the tokenization technique difference between BERT (WordPiece) and ALBERT (SentencePiece) contribute to their performance differences in PPI identification?
- Basis in paper: [inferred] The authors note that "BERT models use the WordPiece tokenization technique, whereas ALBERT uses the SentencePiece tokenization technique" and observed performance differences between these models
- Why unresolved: The study did not conduct controlled experiments isolating the effect of tokenization techniques from other architectural differences
- What evidence would resolve it: A controlled experiment comparing BERT and ALBERT models using identical tokenization techniques, or implementing both tokenization methods in the same model architecture to isolate their effects on PPI identification performance

## Limitations
- Small evaluation corpus (77 sentences, 164 PPIs) may limit generalizability of results to larger, more diverse biomedical literature
- Temperature parameter optimization was only performed for GPT-3, leaving uncertainty about generalization to GPT-3.5 and GPT-4
- Prompt engineering approaches for GPT models may not represent optimal formulations for this task

## Confidence
- **High Confidence**: BERT models achieving superior overall performance metrics (precision, recall, F1-score) due to extensive fine-tuning on biomedical corpora
- **Medium Confidence**: GPT-4's comparable performance when provided with protein names, as this result depends heavily on prompt formulation and entity recognition accuracy
- **Low Confidence**: Generalization of temperature optimization results from GPT-3 to other GPT model variants

## Next Checks
1. Expand Corpus Testing: Validate model performance on larger, more diverse biomedical corpora beyond the LLL dataset to assess generalizability
2. Prompt Engineering Optimization: Systematically explore alternative prompt formulations for GPT models, including few-shot learning approaches and different entity representation strategies
3. Cross-Domain Transfer: Test whether models trained on one biomedical subdomain (e.g., yeast biology in LLL) maintain performance when applied to other domains (e.g., human protein interactions)