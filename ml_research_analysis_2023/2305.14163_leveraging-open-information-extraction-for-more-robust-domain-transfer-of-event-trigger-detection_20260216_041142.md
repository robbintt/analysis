---
ver: rpa2
title: Leveraging Open Information Extraction for More Robust Domain Transfer of Event
  Trigger Detection
arxiv_id: '2305.14163'
source_url: https://arxiv.org/abs/2305.14163
tags:
- training
- transfer
- domain
- target
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of negative transfer in event
  trigger detection (TD) when moving from a high-resource source domain (Wikipedia)
  to low-resource target domains (news). The authors propose leveraging open information
  extraction (OIE) systems to couple triggers across domains by injecting subject-object
  relations into multi-task training designs.
---

# Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection

## Quick Facts
- **arXiv ID**: 2305.14163
- **Source URL**: https://arxiv.org/abs/2305.14163
- **Reference count**: 14
- **Primary result**: OIE relations improve zero- and few-shot TD domain transfer from Wikipedia to news domains

## Executive Summary
This paper addresses negative transfer in event trigger detection when moving from high-resource source domains (Wikipedia) to low-resource target domains (news). The authors propose leveraging open information extraction (OIE) systems to couple triggers across domains by injecting subject-object relations into multi-task training designs. Experiments demonstrate that using OIE relations with implicit multi-task models and sequential transfer training improves zero- and few-shot TD performance compared to vanilla approaches. Further gains are achieved by combining OIE relations with masked language modeling (MLM) on target data. The benefits are shown to be robust across different OIE systems.

## Method Summary
The method uses RoBERTa-base transformer with vanilla, implicit multi-task, and explicit multi-task model designs for event trigger detection. OIE relations extracted via MinIE or Stanford OIE are incorporated through relation embeddings or multi-task objectives. Training regimes include zero-shot, joint training/transfer, sequential transfer, and in-domain training with optional MLM auxiliary objective. The approach is evaluated on MA VEN (Wikipedia) as source dataset and ACE 2005, EDNYT, and EVEXTRA as target datasets using micro F1 score on token classification with IOB2 tagging.

## Key Results
- OIE relations improve zero-shot TD performance on ACE 2005 by up to 2.6 F1 points
- Sequential transfer with implicit multi-task and OIE relations achieves best few-shot results across all shot settings
- Combining OIE relations with MLM on target data provides additional gains of 0.4-1.0 F1 points
- Results are robust across MinIE and Stanford OIE systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open Information Extraction (OIE) relations act as mediators between trigger spans across domains by providing more general predicate-argument structures that overlap with event triggers.
- Mechanism: The implicit multi-task model concatenates token representations with relation embeddings derived from OIE triples. This forces the model to learn shared features between relations and triggers, stabilizing transfer when moving from a high-resource source (Wikipedia) to a low-resource target (news).
- Core assumption: OIE relations and event triggers have sufficient semantic overlap to serve as cross-domain mediators, even if not perfectly aligned.
- Evidence anchors:
  - [abstract]: "We address the problem of negative transfer for TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system."
  - [section 2]: "relations injected through multi-task training can act as mediators between triggers in different domains"
  - [corpus]: No direct experimental results for this mechanism; inferred from multi-task model design.
- Break condition: If OIE relation extraction quality drops significantly, or if triggers in the target domain are too domain-specific to share structure with OIE relations, the mediating effect disappears.

### Mechanism 2
- Claim: Sequential transfer training with OIE relations improves zero- and few-shot TD performance by first adapting to the source domain, then leveraging relations to bridge the distribution gap.
- Mechanism: The model is first fine-tuned on the source domain (Wikipedia) with both trigger and relation labels, then further fine-tuned on the target domain (news) using the same relation embeddings to maintain consistency across domains.
- Core assumption: Training with relation labels in the source domain creates a representation space that generalizes better to the target domain when combined with few-shot target examples.
- Evidence anchors:
  - [abstract]: "We demonstrate that relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer"
  - [section 3.2]: "Sequential Transfer... entails starting from the fine-tuned RoBERTa-base for TD on the source domain data with vanilla, implicit, or explicit model designs and further fine-tuning on all available few-shot examples"
  - [corpus]: No direct experimental comparison without OIE relations; inferred from reported gains over vanilla models.
- Break condition: If the sequential training schedule is too short or too long, or if the few-shot target data is too sparse, the benefits from OIE relations may not materialize.

### Mechanism 3
- Claim: Combining OIE relations with masked language modeling (MLM) on the target domain further reduces distribution shift by adapting the language model to target domain syntax while maintaining trigger detection capability.
- Mechanism: The model is trained in alternating epochs: one epoch on target MLM loss and source TD loss, the next on target TD loss only. This allows the model to adapt to target language patterns without forgetting trigger detection.
- Core assumption: Target domain MLM loss helps the model adjust to domain-specific language without harming trigger detection performance.
- Evidence anchors:
  - [abstract]: "we combine the extracted relations with masked language modeling on the target domain and obtain further TD performance gains"
  - [section 3.3]: "we used a token-level masking probability of15%, and the masking procedure was inherited from... The model's parameters are updated for in-domain training in an alternate fashion inside each epoch: first, based on target training data MLM loss, and then on target TD loss"
  - [corpus]: No ablation study showing MLM alone vs. MLM+relations; benefit inferred from combined result.
- Break condition: If the target domain is too small, MLM may overfit or degrade trigger detection; if the masking probability is too high, it may interfere with trigger detection learning.

## Foundational Learning

- Concept: Token classification with IOB2 tagging
  - Why needed here: Event trigger detection is framed as a sequence labeling task where each token is classified as part of a trigger span or not.
  - Quick check question: What does the IOB2 scheme represent for a multi-token trigger like "broke into two parts"?

- Concept: Multi-task learning with auxiliary objectives
  - Why needed here: The model jointly optimizes trigger detection and relation detection (or uses relation embeddings as input) to leverage shared semantic structure between triggers and relations.
  - Quick check question: In the implicit multi-task design, what is updated during training vs. inference for relation embeddings?

- Concept: Transfer learning regimes (joint vs. sequential)
  - Why needed here: Different fine-tuning strategies are compared to mitigate negative transfer; sequential transfer is found more effective than joint training in this context.
  - Quick check question: Why does joint training with mixed batches perform worse than sequential transfer in this setup?

## Architecture Onboarding

- Component map:
  - RoBERTa-base transformer (shared encoder) -> Token classification layer(s) (trigger detection head, optional relation detection head) -> Relation label embedding matrix (implicit model only) -> OIE system (MinIE or Stanford OIE) for relation extraction -> MLM head (when MLM is used as auxiliary task)

- Critical path:
  1. Preprocess source and target datasets with OIE to extract relations
  2. Fine-tune RoBERTa on source domain with TD (and RD) labels
  3. Sequentially fine-tune on target domain with few-shot examples
  4. (Optional) Add MLM objective during target fine-tuning

- Design tradeoffs:
  - Using OIE relations adds robustness but introduces dependency on OIE system quality
  - Implicit multi-task is simpler and more effective than explicit multi-task
  - Sequential transfer is more resource-efficient than joint training despite lower performance in language transfer settings

- Failure signatures:
  - Negative transfer persists: OIE relations are too noisy or misaligned with triggers
  - Overfitting on source: Few-shot target data is insufficient to override source bias
  - MLM harms performance: Target domain is too small or too different for MLM to help

- First 3 experiments:
  1. Compare zero-shot TD performance of vanilla vs. implicit multi-task model on ACE2005
  2. Test sequential transfer with 50-shot target data using implicit multi-task with MinIE relations
  3. Add MLM objective to sequential transfer with 50-shot data and compare to baseline

## Open Questions the Paper Calls Out
- How do the performance gains from OIE relations compare across different event detection tasks beyond trigger detection, such as event argument detection or event coreference resolution?
- How do different OIE systems with varying levels of precision and recall affect the stability and magnitude of performance improvements in trigger detection domain transfer?
- What is the optimal balance between the number of target domain examples used for MLM pre-training versus trigger detection fine-tuning to maximize domain adaptation performance?

## Limitations
- Heavy reliance on OIE system quality, with no rigorous validation of relation-trigger alignment
- Limited theoretical analysis of why sequential transfer outperforms joint training
- Lack of systematic ablation studies on MLM contribution and relation quality degradation effects

## Confidence

**High confidence**: The empirical demonstration that OIE relations improve domain transfer performance compared to vanilla approaches. The experimental setup is well-controlled with multiple datasets, shot settings, and OIE systems, and the reported improvements are statistically significant across multiple runs.

**Medium confidence**: The claim that implicit multi-task is superior to explicit multi-task design. While the paper reports better performance for implicit multi-task, the difference is relatively small (e.g., 0.4 F1 points on EDNYT) and could be sensitive to hyperparameter choices or implementation details.

**Low confidence**: The assertion that sequential transfer is universally better than joint training for this task. The paper provides limited diagnostic analysis of why joint training fails, and the conclusion may not generalize to other domain pairs or task configurations where joint training has shown benefits.

## Next Checks

1. **Ablation study on OIE relation quality**: Systematically degrade OIE extraction quality (e.g., by filtering out relations with low confidence scores or increasing post-processing constraints) and measure the impact on TD performance to quantify the sensitivity of the approach to OIE system reliability.

2. **Diagnostic analysis of joint vs. sequential transfer**: Conduct gradient analysis or representation similarity studies comparing joint and sequential training to identify specific failure modes in joint training (e.g., gradient interference, catastrophic forgetting) and validate whether these patterns are consistent across different domain pairs.

3. **MLM ablations with controlled conditions**: Compare sequential transfer with and without MLM on the same few-shot datasets, test different masking probabilities (not just the fixed 15%), and evaluate whether MLM benefits persist when using only target domain data without OIE relations to isolate the source of improvement.