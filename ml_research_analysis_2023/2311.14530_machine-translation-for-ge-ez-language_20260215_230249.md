---
ver: rpa2
title: Machine Translation for Ge'ez Language
arxiv_id: '2311.14530'
source_url: https://arxiv.org/abs/2311.14530
tags:
- translation
- languages
- language
- machine
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first machine translation system for Ge'ez,
  an ancient language no longer spoken natively, addressing the challenge of low-resource
  machine translation. The authors develop a multilingual neural machine translation
  model leveraging related languages (English, Amharic, Tigrinya) to improve Ge'ez
  translation quality.
---

# Machine Translation for Ge'ez Language

## Quick Facts
- arXiv ID: 2311.14530
- Source URL: https://arxiv.org/abs/2311.14530
- Reference count: 3
- Primary result: First machine translation system for Ge'ez achieves BLEU scores up to 15.2 using multilingual neural machine translation

## Executive Summary
This work presents the first machine translation system for Ge'ez, an ancient language no longer spoken natively, addressing the challenge of low-resource machine translation. The authors develop a multilingual neural machine translation model leveraging related languages (English, Amharic, Tigrinya) to improve Ge'ez translation quality. The approach uses transfer learning from related languages, shared vocabulary optimization, and byte-pair encoding. Results show the multilingual model achieves a BLEU score of 15.2, outperforming bilingual models by an average of 4 BLEU points.

## Method Summary
The study employs Transformer-based NMT models with shared vocabulary and BPE tokenization, trained on parallel corpora from Opus and AAU Ethiopian Languages corpus. The approach combines bilingual models (en-gez, amh-gez) with a multilingual model incorporating Ge'ez, English, Amharic, and Tigrinya. The multilingual model uses artificial target language tokens and optimized shared vocabulary to leverage transfer learning from related languages. The system also experiments with GPT-3.5 for few-shot translation using embedding-based retrieval and attempts to fine-tune NLLB-200 with limited training data.

## Key Results
- Multilingual model achieves BLEU score of 15.2, outperforming bilingual models by 4 BLEU points
- GPT-3.5 few-shot translation achieves 9.2 BLEU without prior Ge'ez knowledge
- NLLB-200 fine-tuning with 4k samples fails (0.2-3.8 BLEU), demonstrating limits of fine-tuning large models with minimal data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from related languages improves Ge'ez translation by leveraging shared linguistic features.
- Mechanism: Related languages (Amharic, Tigrinya, English) share geographic proximity, script, and morphological similarities with Ge'ez, allowing knowledge transfer during model training.
- Core assumption: Shared linguistic features between source and target languages are sufficient to improve translation quality in low-resource settings.
- Evidence anchors:
  - [abstract]: "Multilingual model achieves a BLEU score of 15.2, outperforming bilingual models by an average of 4 BLEU points."
  - [section]: "We hypothesize that these techniques can enhance the quality and efficiency of Ge'ez MT by exploiting the similarities and differences between Ge'ez and other languages."
- Break condition: If the related languages are too distant in linguistic structure or script, transfer learning gains may not materialize.

### Mechanism 2
- Claim: Shared vocabulary and byte-pair encoding (BPE) reduce out-of-vocabulary (OOV) issues in Ge'ez translation.
- Mechanism: Using a common subword vocabulary across languages reduces vocabulary size and sparsity, helping the model handle rare or unseen words.
- Core assumption: Subword segmentation can adequately represent Ge'ez words using shared units from related languages.
- Evidence anchors:
  - [abstract]: "optimizing shared vocabulary and token segmentation approaches" as part of the methods.
  - [section]: "We used a shared vocabulary instead of separate vocabularies for the source and target languages."
- Break condition: If Ge'ez contains many unique morphemes not present in related languages, BPE may still fail to cover important vocabulary.

### Mechanism 3
- Claim: Few-shot translation with LLMs leverages contextual retrieval to improve translation for Ge'ez despite no initial model knowledge.
- Mechanism: GPT-3.5 uses embedding similarity to retrieve relevant sentence pairs from a parallel corpus, providing context for translating unseen Ge'ez sentences.
- Core assumption: Contextual examples retrieved via fuzzy matching are relevant and helpful enough to guide LLM generation.
- Evidence anchors:
  - [abstract]: "GPT-3.5 achieves a remarkable BLEU score of 9.2 with no initial knowledge of Ge'ez."
  - [section]: "We use embedding similarity-based retrieval to find up to 10 similar source sentences from a parallel corpus of this language and English."
- Break condition: If retrieved examples are irrelevant or too sparse, the LLM cannot generate accurate translations.

## Foundational Learning

- Concept: Multilingual neural machine translation (MNMT)
  - Why needed here: MNMT allows simultaneous training across multiple languages, enabling transfer learning from related languages to Ge'ez.
  - Quick check question: How does adding a target language token at the start of the input sentence help the model learn multilingual translation?

- Concept: Byte-pair encoding (BPE)
  - Why needed here: BPE reduces vocabulary size and handles rare words by breaking them into subword units, crucial for low-resource languages like Ge'ez.
  - Quick check question: What happens to OOV rates when using BPE versus word-level tokenization in a low-resource setting?

- Concept: Transfer learning in NLP
  - Why needed here: Leverages existing linguistic knowledge from high-resource related languages to compensate for limited Ge'ez data.
  - Quick check question: Why might transfer learning be more effective when source and target languages share script or morphology?

## Architecture Onboarding

- Component map: Bilingual NMT models (en-gez, amh-gez) -> Multilingual NMT model (en, amh, gez, ti) -> LLM-based few-shot translation with GPT-3.5
- Critical path: Train bilingual models → train multilingual model with shared vocabulary and BPE → evaluate using BLEU → optionally fine-tune NLLB-200 or use GPT-3.5 for few-shot