---
ver: rpa2
title: 'DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration'
arxiv_id: '2307.09931'
source_url: https://arxiv.org/abs/2307.09931
tags:
- registration
- image
- similarity
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DISA (DIfferentiable Similarity Approximation),
  a novel framework for multimodal image registration. The key idea is to approximate
  complex, non-differentiable similarity metrics with a dot product in the feature
  space of a small CNN.
---

# DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration

## Quick Facts
- arXiv ID: 2307.09931
- Source URL: https://arxiv.org/abs/2307.09931
- Reference count: 28
- Key outcome: DISA achieves comparable accuracy to LC2 while being significantly faster and having a larger capture range, converging to correct solutions in less than two seconds even from distant initializations.

## Executive Summary
DISA (DIfferentiable Similarity Approximation) is a novel framework for multimodal image registration that approximates complex, non-differentiable similarity metrics using a dot product in the feature space of a small CNN. The key innovation is training the CNN on unregistered image pairs to learn an approximation of the LC2 similarity metric without requiring ground truth registration data. This approach combines classical multimodal registration techniques with machine learning, avoiding common limitations of purely data-driven methods. DISA is evaluated on three distinct tasks: brain US-MR affine registration, abdominal MR-CT deformable registration, and abdominal US-CT/US-MR deformable registration, demonstrating state-of-the-art performance in terms of accuracy, speed, and capture range.

## Method Summary
DISA approximates the LC2 similarity metric by training a small 3D CNN to map image patches to feature vectors such that their dot product approximates LC2. The CNN architecture consists of 10 layers with residual blocks, LeakyReLU activations, and BlurPool downsampling (total stride 4), producing 16-channel output. The network is trained on 510,000 patch pairs from unregistered MR-CT data using L2 loss for 35 epochs with batch size 256. During registration, pre-computed feature maps enable fast similarity evaluation via dot product, and global optimization (BFGS or BOBYQA) finds the optimal transformation. The method is evaluated using Fiducial Registration Error (FRE) and Dice Similarity Coefficient (DSC) on three distinct datasets: RESECT brain US-MR, Learn2Reg abdominal MR-CT, and abdominal US-CT/US-MR with landmarks.

## Key Results
- Achieves comparable accuracy to LC2 similarity metric while being significantly faster
- Demonstrates large capture range, converging from distant initializations in under two seconds
- Outperforms competing methods (MIND-SSC, MIRaGe, VoxelMorph) on abdominal US registration tasks
- Maintains accuracy across diverse anatomies (brain, abdomen) and modalities (US, MR, CT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DISA approximates complex, non-differentiable similarity metrics with a dot product in the feature space of a small CNN.
- Mechanism: The CNN learns a mapping from image patches to feature vectors such that the dot product of these vectors approximates the original similarity metric (e.g., LC2).
- Core assumption: The similarity metric can be expressed as a function of local patch similarities that can be learned by a small CNN.
- Evidence anchors:
  - [abstract] "We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN)"
  - [section] "The core idea of our method is to approximate the similarity metric s(P1, P2) of two image patches with a dot product ⟨ϕ(P1), ϕ(P2)⟩ where ϕ(·) is a function that extracts a feature vector"
- Break condition: If the similarity metric involves global context or long-range dependencies that cannot be captured by the small receptive field of the CNN.

### Mechanism 2
- Claim: DISA enables differentiable registration without requiring ground truth data.
- Mechanism: The CNN is trained using patches from unregistered image pairs, minimizing the difference between the approximated and original similarity metrics.
- Core assumption: The CNN can learn to approximate the similarity metric without access to ground truth registrations.
- Evidence anchors:
  - [abstract] "Our method does not necessitate to evaluate the CNN at every optimizer iteration. This approach combines ML and classical multimodal image registration techniques in a novel way, avoiding the common limitations of ML approaches: ground truth registration is not required"
  - [section] "Crucially, our method does not necessitate to evaluate the CNN at every optimizer iteration. This approach combines ML and classical multimodal image registration techniques in a novel way, avoiding the common limitations of ML approaches: ground truth registration is not required"
- Break condition: If the similarity metric is too complex to be approximated by a small CNN without ground truth data.

### Mechanism 3
- Claim: DISA achieves comparable accuracy to LC2 while being significantly faster and having a larger capture range.
- Mechanism: The pre-computed feature maps enable fast registration using a simple dot product similarity, and the global optimization strategy allows for a larger capture range.
- Core assumption: The approximated similarity metric is sufficiently accurate for registration, and the global optimization strategy can find the correct solution.
- Evidence anchors:
  - [abstract] "DISA achieves comparable accuracy to LC2 while being significantly faster and having a larger capture range. Specifically, on abdominal US registration, DISA converges to the correct solution with pose and deformation parameters in less than two seconds, even from distant initializations"
  - [section] "Our method exhibits a large capture range and can converge over a wide range of rotations and deformations"
- Break condition: If the approximated similarity metric is not accurate enough for the specific registration task or the global optimization strategy fails to find the correct solution.

## Foundational Learning

- Concept: Multimodal image registration
  - Why needed here: DISA is a framework for multimodal image registration, so understanding the challenges and existing approaches is crucial.
  - Quick check question: What are the main challenges in multimodal image registration, and how do existing methods like LC2 and MIND-SSC address them?

- Concept: Convolutional neural networks (CNNs)
  - Why needed here: DISA uses a small CNN to approximate the similarity metric, so understanding how CNNs work and their limitations is important.
  - Quick check question: What are the key components of a CNN, and how does the receptive field size affect its ability to capture spatial information?

- Concept: Optimization in image registration
  - Why needed here: DISA converts the registration problem into an optimization problem using the approximated similarity metric, so understanding optimization techniques is necessary.
  - Quick check question: What are the main optimization techniques used in image registration, and how do they differ in terms of convergence speed and robustness?

## Architecture Onboarding

- Component map: Input patches -> 3D CNN with residual blocks and BlurPool -> 16-channel feature vectors -> Dot product similarity -> Global optimization (BFGS/BOBYQA)

- Critical path:
  1. Pre-compute feature maps using the CNN
  2. Perform global optimization using the approximated similarity metric
  3. Refine the registration result using local optimization if needed

- Design tradeoffs:
  - Small CNN vs. large CNN: Smaller CNNs are faster and more generalizable but may not capture complex similarity metrics as well.
  - Global vs. local optimization: Global optimization has a larger capture range but may be slower and less accurate than local optimization.

- Failure signatures:
  - If the CNN fails to learn an accurate approximation of the similarity metric, the registration results will be poor.
  - If the global optimization strategy gets stuck in local minima, the registration may not converge to the correct solution.

- First 3 experiments:
  1. Train the CNN on a small dataset and evaluate its ability to approximate the LC2 similarity metric on a held-out set.
  2. Perform registration on a simple multimodal pair (e.g., MR-CT) using the pre-computed feature maps and evaluate the accuracy and speed compared to LC2.
  3. Evaluate the capture range of the registration algorithm by initializing it from distant poses and measuring the convergence rate and final accuracy.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the approach suggests several areas for future research, including extending DISA to other similarity metrics beyond LC2, exploring different CNN architectures for improved generalization, and applying DISA to 4D registration problems.

## Limitations
- Reliance on local patch similarity may not capture global spatial relationships effectively
- Performance depends on the ability of a small CNN to accurately approximate complex similarity metrics
- Limited evaluation on extreme pose variations beyond the training distribution

## Confidence
- Speed improvement: High
- Capture range: High  
- Generalization across anatomies: Medium
- Avoidance of ground truth requirements: High for training, Medium for deployment

## Next Checks
1. Test DISA on extreme pose variations beyond the training distribution to evaluate robustness
2. Evaluate performance on cross-domain datasets not seen during training to assess generalization
3. Benchmark against state-of-the-art deep learning registration methods under identical conditions to validate competitive performance