---
ver: rpa2
title: 'MindGames: Targeting Theory of Mind in Large Language Models with Dynamic
  Epistemic Modal Logic'
arxiv_id: '2305.03353'
source_url: https://arxiv.org/abs/2305.03353
tags:
- language
- logic
- muddy
- epistemic
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new dataset and method to test language models
  on epistemic logic reasoning, an important component of Theory of Mind. The authors
  generate controlled logic problems with varying agent numbers, predicates, and observations,
  then verbalize them into natural language premise-hypothesis pairs.
---

# MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic

## Quick Facts
- arXiv ID: 2305.03353
- Source URL: https://arxiv.org/abs/2305.03353
- Reference count: 14
- Primary result: Standard LLM scaling fails to improve epistemic reasoning beyond random chance

## Executive Summary
This paper introduces MindGames, a novel dataset designed to test large language models on epistemic logic reasoning tasks that require Theory of Mind capabilities. The authors generate controlled logic problems using dynamic epistemic logic (DEL) that force models to track evolving knowledge states across multiple agents and time steps. These formal logic problems are then verbalized into natural language premise-hypothesis pairs. The study finds that even large language models (up to 174B parameters) perform poorly on these tasks, with performance hovering around random chance, while GPT-4 shows better but still imperfect results. The work suggests that current LLMs fundamentally struggle with complex multi-agent reasoning tasks even when scaling parameters.

## Method Summary
The authors create the MindGames dataset by generating DEL problems with varying numbers of agents (2-3), predicates (2-4), and observations, then verbalizing them into natural language NLI format. They use the SMCDEL model checker to determine whether hypotheses are entailed by premises. The evaluation tests pre-trained language models (Pythia family and OpenAI GPT models) on zero-shot and 5-shot settings using perplexity-based scoring. The dataset contains 400 problems total, with 8 types of problems varying in complexity. No model training occurs - the study evaluates existing models on the generated tasks.

## Key Results
- Standard LLM scaling (70M to 174B parameters) shows no consistent improvement in epistemic reasoning beyond random chance
- GPT-4 performs better than smaller models but still imperfectly on the tasks
- All models struggle with problems requiring tracking of second-order beliefs and multiple agent knowledge updates
- Performance degrades significantly as problem complexity increases with more agents and predicates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic epistemic logic problems force models to track evolving knowledge states across multiple agents and time steps
- Mechanism: The DEL framework encodes multi-agent knowledge updates as formal logic problems, where agents' beliefs change sequentially based on public announcements. This creates structured reasoning chains that must be maintained across multiple inference steps.
- Core assumption: Language models can leverage their parametric knowledge to represent and update multi-agent belief states when presented in structured DEL format
- Evidence anchors:
  - [abstract] "dynamic epistemic logic to generate more intricate problems" and "track multiple agents' beliefs and reasoning about higher-order beliefs"
  - [section] "Dynamic epistemic logic is a type of modal logic that facilitates reasoning about agents' knowledge of facts or other agents' knowledge"
  - [corpus] Weak evidence - related work focuses on LLM ToM but doesn't directly support this specific mechanism
- Break condition: If models cannot maintain consistent belief states across multiple announcement steps, performance will degrade exponentially with problem complexity

### Mechanism 2
- Claim: Verbalization transforms formal logic into natural language while preserving reasoning structure
- Mechanism: The verbalization templates convert DEL problem components (agents, predicates, observabilities, announcements) into coherent English narratives that preserve the logical relationships needed for reasoning
- Core assumption: Natural language processing capabilities can be directed toward logical inference when problems are framed appropriately
- Evidence anchors:
  - [section] "introduce novel verbalization techniques to express these problems using natural language" and template structure preserving logical relationships
  - [section] "utilize the SMCDEL model checker... to determine whether a hypothesis is entailed by the premise"
  - [corpus] Weak evidence - verbalization effectiveness not directly validated in related work
- Break condition: If verbalization introduces ambiguity or loses logical precision, model performance will collapse regardless of underlying reasoning capability

### Mechanism 3
- Claim: Multi-agent epistemic reasoning requires tracking second-order beliefs (beliefs about beliefs)
- Mechanism: The problems require models to infer not just what agents know, but what agents know about what other agents know, creating recursive reasoning chains
- Core assumption: Language models can represent and manipulate nested belief structures
- Evidence anchors:
  - [abstract] "problems can necessitate tracking multiple agents' beliefs and reasoning about higher-order beliefs"
  - [section] "For example, Anne's belief about Sally's belief about Anne's belief about Mary's belief"
  - [corpus] Moderate evidence - related work on ToM benchmarks shows this is a known challenging aspect
- Break condition: If models cannot represent nested belief structures, they will fail on problems requiring second-order or higher reasoning

## Foundational Learning

- Concept: Modal logic and epistemic operators
  - Why needed here: Understanding how knowledge operators work is essential for interpreting DEL problems and their verbalizations
  - Quick check question: What is the difference between "knows that" and "can know that" in epistemic logic?

- Concept: Multi-agent belief state tracking
  - Why needed here: The core challenge involves maintaining consistent mental models of what each agent knows at each step
  - Quick check question: How would you represent the belief state of three agents where each can see the others' foreheads but not their own?

- Concept: Natural language inference framing
  - Why needed here: The premise-hypothesis format requires understanding entailment relationships in context
  - Quick check question: What makes a hypothesis "entailed" by a premise in NLI terms versus merely "plausible"?

## Architecture Onboarding

- Component map: DEL problem generator → Verbalization engine → Model evaluation pipeline → SMCDEL model checker
- Critical path: Problem generation → Verbalization → Model inference → Answer validation
  - Bottleneck: Verbalization quality directly impacts model performance
- Design tradeoffs:
  - Complexity vs. solvability: More agents/predicates increase difficulty but may exceed model capacity
  - Generality vs. specificity: Broad verbalization templates may lose logical precision
  - Randomization vs. control: Diverse problems improve generalization but make debugging harder
- Failure signatures:
  - Consistent random guessing across model sizes suggests fundamental task difficulty
  - Performance gaps between zero-shot and few-shot indicate task unfamiliarity
  - Setup-specific failures reveal verbalization issues
- First 3 experiments:
  1. Generate single-agent problems and verify perfect performance baseline
  2. Test two-agent problems with simple announcements to establish minimum viable complexity
  3. Compare model performance on logically equivalent problems with different verbalizations to isolate verbalization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more complex multi-agent epistemic logic problems (beyond 3 agents) be effectively generated and solved by current large language models?
- Basis in paper: [explicit] The authors note that "When future models can solve Mindgames for 2-3 agents, the difficulty can be easily scaled up by with more agents."
- Why unresolved: The current study only tested problems with 2-3 agents, so the scalability to larger agent numbers remains untested.
- What evidence would resolve it: Generating and testing larger datasets with 4+ agents and evaluating model performance on these would provide empirical evidence.

### Open Question 2
- Question: What specific aspects of epistemic reasoning cause language models to fail on MindGames tasks?
- Basis in paper: [inferred] The paper shows models perform poorly on epistemic reasoning tasks but doesn't analyze what specific reasoning patterns are most problematic.
- Why unresolved: The study focused on overall performance rather than analyzing error patterns to identify specific reasoning deficits.
- What evidence would resolve it: Detailed error analysis categorizing the types of mistakes made by models would reveal specific reasoning challenges.

### Open Question 3
- Question: Does fine-tuning language models on MindGames data improve their performance on other downstream tasks requiring Theory of Mind?
- Basis in paper: [explicit] The authors suggest "further investigation can examine the impact of fine-tuning on other downstream tasks."
- Why unresolved: The study only tested zero-shot and few-shot performance without exploring fine-tuning approaches.
- What evidence would resolve it: Experiments comparing models fine-tuned on MindGames data versus control models on various ToM-relevant downstream tasks would provide answers.

## Limitations

- The study relies on verbalization templates that may introduce ambiguity or lose logical precision
- Evaluation uses perplexity-based scoring which may not capture nuanced reasoning failures
- The comparison with GPT-4 may reflect instruction-following capabilities rather than true epistemic reasoning
- No validation that verbalization preserves logical relationships across all problem types

## Confidence

- High confidence: LLMs struggle with multi-agent epistemic reasoning beyond simple cases
- Medium confidence: Standard scaling laws do not improve epistemic reasoning performance
- Low confidence: The MindGames dataset represents a true measure of epistemic reasoning capability

## Next Checks

1. **Ablation study on verbalization templates**: Systematically remove or modify different template components to quantify how much performance loss is attributable to verbalization quality versus reasoning capability. Test the same logical problems with multiple verbalization styles.

2. **Cross-validation with symbolic reasoning systems**: Run the generated DEL problems through pure symbolic model checkers (without verbalization) and compare against LLM performance to establish a theoretical upper bound on achievable accuracy.

3. **Intermediate complexity scaling**: Generate problems with controlled complexity increments (e.g., 2 agents × 2 predicates → 2 agents × 3 predicates → 3 agents × 2 predicates) to identify the exact point where performance collapses, rather than observing only the final outcome.