---
ver: rpa2
title: Evaluating ChatGPT text-mining of clinical records for obesity monitoring
arxiv_id: '2308.01666'
source_url: https://arxiv.org/abs/2308.01666
tags:
- chatgpt
- overweight
- veterinary
- clinical
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared a large language model (ChatGPT) and a regular
  expression system (RegexT) for identifying overweight body condition scores (BCS)
  in veterinary clinical narratives. ChatGPT achieved higher recall (100%) than RegexT
  (72.6%) in detecting overweight BCS values from 4,415 anonymized clinical records.
---

# Evaluating ChatGPT text-mining of clinical records for obesity monitoring

## Quick Facts
- arXiv ID: 2308.01666
- Source URL: https://arxiv.org/abs/2308.01666
- Authors: 
- Reference count: 18
- Key outcome: ChatGPT achieved 100% recall vs RegexT's 72.6% for detecting overweight BCS values, but had lower precision (89.3% vs 100%) due to false positives from other numerical scores

## Executive Summary
This study compared a large language model (ChatGPT) and a regular expression system (RegexT) for identifying overweight body condition scores (BCS) in veterinary clinical narratives. ChatGPT achieved higher recall (100%) than RegexT (72.6%) in detecting overweight BCS values from 4,415 anonymized clinical records. However, ChatGPT had lower precision (89.3%) compared to RegexT (100%) due to some false positives from misclassifying other numerical scores. ChatGPT also identified overweight animals without recorded BCS values, which RegexT could not detect. While effective, ChatGPT required careful prompt engineering to improve output structure and reduce irrelevant responses.

## Method Summary
The study evaluated ChatGPT (GPT-3.5 Turbo) against a regex-based system (RegexT) using 4,415 anonymized veterinary clinical narratives from the SAVSNET database. Researchers compared the systems' ability to identify overweight BCS values using manual review by domain experts as the reference standard. ChatGPT was prompted with four basic rules to extract BCS information, while RegexT used pattern matching for numerical BCS values. Performance was measured using precision and recall metrics with 95% confidence intervals.

## Key Results
- ChatGPT achieved 100% recall (CI: 96.8%-100%) vs RegexT's 72.6% recall (CI: 62.4%-81.6%) in detecting overweight BCS values
- ChatGPT had lower precision (89.3%) compared to RegexT's 100% due to false positives from other numerical scores
- ChatGPT uniquely identified overweight animals without recorded BCS values through contextual descriptions

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT achieves 100% recall in detecting overweight BCS values due to its ability to generalize from diverse text patterns beyond fixed regex rules. The LLM processes free-text narratives using contextual understanding rather than strict pattern matching, allowing it to recognize overweight BCS values in various formats (e.g., "6/9", "overweight", or contextual descriptions like "would benefit from further weight loss").

### Mechanism 2
ChatGPT's lower precision (89.3%) compared to RegexT (100%) results from extracting numerical scores from unrelated clinical contexts. The model identifies any numerical values formatted like BCS scores (e.g., "6/10") even when they refer to different measurements like lameness scoring or body weight, leading to false positives.

### Mechanism 3
ChatGPT uniquely identifies overweight animals without recorded BCS values by recognizing contextual descriptions. The model interprets free-text descriptions like "she is overweight" or "normal on clinical exam apart from overweight" as indicators of overweight status, even when no BCS score is present.

## Foundational Learning

- Concept: Regular expressions and pattern matching
  - Why needed here: Understanding regex limitations explains why ChatGPT outperforms it - regex requires predefined patterns while ChatGPT generalizes from context
  - Quick check question: Why did RegexT miss 32 overweight BCS values that ChatGPT detected?

- Concept: Prompt engineering for LLMs
  - Why needed here: The study shows ChatGPT's performance depends heavily on prompt design to constrain outputs and reduce irrelevant responses
  - Quick check question: What specific prompt modifications could reduce ChatGPT's false positives from lameness scores?

- Concept: Precision vs. recall tradeoff
  - Why needed here: The study demonstrates ChatGPT's higher recall comes at the cost of lower precision, requiring understanding of when each metric matters
  - Quick check question: In what clinical scenarios would higher recall be more valuable than higher precision for obesity monitoring?

## Architecture Onboarding

- Component map: Data input → Processing (RegexT/ChatGPT) → Manual validation → Metrics calculation
- Critical path: Clinical narratives → Pattern matching/LLM processing → Expert manual review → Precision/recall computation
- Design tradeoffs: ChatGPT offers higher recall but requires careful prompt engineering and manual interpretation; RegexT provides perfect precision but misses novel formats
- Failure signatures: ChatGPT outputs irrelevant text ("Hello! How can I assist you today?"), misclassifies other numerical scores, or fails to return appropriate answers for short narratives
- First 3 experiments:
  1. Test ChatGPT with constrained prompts excluding common non-BCS numerical scores (lameness, heart murmur)
  2. Compare ChatGPT performance on narratives above vs. below 23 characters to identify length-related failures
  3. Evaluate whether tabular output formatting reduces manual interpretation requirements

## Open Questions the Paper Calls Out

### Open Question 1
What specific prompt engineering techniques could reduce false positives from non-BCS numerical scores (like lameness scores or body weights)?
Basis in paper: The authors note that ChatGPT falsely identified BCSs in 14 records due to extraction of other similarly formatted clinical information such as lameness scoring and body weight information
Why unresolved: The paper only suggests a general approach ("by prompting: 'exclude lameness scores normally recorded out of ten and heart murmur scores, normally recorded out of six'") without testing or validating this solution

### Open Question 2
How does ChatGPT's performance compare to RegexT when applied to clinical narratives from different veterinary practice management systems with varying text formats?
Basis in paper: The RegexT system was specifically designed for veterinary clinical narratives but missed 32 overweight BCS values due to format variants not captured by the regex syntax
Why unresolved: The study only used a single dataset of 4,415 anonymized clinical narratives, limiting generalizability to other practice management systems

### Open Question 3
What is the optimal balance between ChatGPT's recall and precision for practical obesity monitoring applications in veterinary medicine?
Basis in paper: ChatGPT achieved perfect recall (100%) but lower precision (89.3%) compared to RegexT's perfect precision (100%) but lower recall (72.6%)
Why unresolved: The paper doesn't explore the clinical significance of false positives or whether the additional identifications of overweight animals without recorded BCS values outweigh the precision trade-off

## Limitations

- Data quality and representativeness limited to single UK-based veterinary surveillance network
- Prompt engineering dependency with unspecified iterative process affecting performance metrics
- Reference standard ambiguity regarding whether false positives are truly incorrect or missed by manual reviewers

## Confidence

- High Confidence: ChatGPT achieves 100% recall in detecting overweight BCS values compared to RegexT's 72.6%
- Medium Confidence: ChatGPT uniquely identifies overweight animals without recorded BCS values through contextual descriptions
- Medium Confidence: ChatGPT's lower precision (89.3%) compared to RegexT's 100% results from misclassifying other numerical scores

## Next Checks

1. Systematically vary the ChatGPT prompt structure (e.g., adding explicit exclusion criteria for non-BCS numerical scores, changing output format requirements) and measure performance degradation to quantify prompt sensitivity

2. Apply the same ChatGPT and RegexT methods to veterinary clinical narratives from different geographic regions or practice types to assess generalizability beyond the SAVSNET dataset

3. Have three independent veterinary domain experts review a random sample of 100 records where ChatGPT identified overweight status but no BCS was recorded, to determine if these are true positives or false positives