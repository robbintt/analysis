---
ver: rpa2
title: Testing the General Deductive Reasoning Capacity of Large Language Models Using
  OOD Examples
arxiv_id: '2305.15269'
source_url: https://arxiv.org/abs/2305.15269
tags:
- proof
- examples
- alex
- deduction
- elimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRONTO QA-OOD, a synthetic dataset for evaluating
  the general deductive reasoning capacity of large language models (LLMs). The dataset
  features controllable proof depth, width, and compositional proofs using various
  deduction rules.
---

# Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples

## Quick Facts
- arXiv ID: 2305.15269
- Source URL: https://arxiv.org/abs/2305.15269
- Authors: 
- Reference count: 40
- LLMs can generalize to compositional proofs using multiple deduction rules when given simpler demonstrations

## Executive Summary
This paper introduces PRONTO QA-OOD, a synthetic dataset for evaluating the general deductive reasoning capacity of large language models (LLMs). The dataset features controllable proof depth, width, and compositional proofs using various deduction rules. Experiments with four LLMs (GPT-3.5, PaLM, LLaMA, and FLAN-T5) show that they can generalize to compositional and longer proofs given simpler demonstrations, though they struggle with longer proofs and require explicit demonstrations for certain deduction rules like proof by cases and proof by contradiction. Model size does not strongly correlate with performance. The study also highlights differences between in-context learning and supervised learning, suggesting that simpler examples or diverse demonstrations may be more effective for in-context learning.

## Method Summary
The evaluation framework uses PRONTO QA-OOD, a synthetic dataset of deductive reasoning proofs. The dataset allows controlled variation of proof depth, width, and compositional complexity using natural deduction rules. LLMs are evaluated using 8-shot chain-of-thought prompting, testing their ability to generalize to proofs more complex than their demonstrations. Performance is measured by formal evaluation of the generated proofs, with particular attention to out-of-distribution generalization to longer proofs, compositional proofs, and proofs using unseen deduction rules.

## Key Results
- LLMs can generalize to compositional proofs using multiple deduction rules when given simpler demonstrations
- Model size does not strongly correlate with performance on deductive reasoning tasks
- Simpler in-context examples can sometimes be more effective than matched-complexity examples for in-context learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generalize to compositional proofs using multiple deduction rules when given simpler demonstrations.
- Mechanism: In-context learning allows the model to infer the structure of more complex proofs from simpler examples, even when the test examples contain deduction rules not present in the demonstrations.
- Core assumption: The model's internal representation captures the abstract structure of deductive reasoning across different rules.
- Evidence anchors:
  - [abstract] "Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs."
  - [section 4.2.2] "The gap in proof accuracy between the ID and OOD settings is quite small, indicating that the models are able to generalize compositionally to an extent."
  - [corpus] Weak - corpus neighbors discuss deductive reasoning but not compositional generalization from simpler demonstrations.

### Mechanism 2
- Claim: Simpler examples can sometimes be more effective for in-context learning than examples from the same distribution as the test case.
- Mechanism: ICL may benefit from diverse demonstrations that highlight different aspects of the reasoning task, rather than matching the test example's complexity.
- Core assumption: The model's learning process in ICL is not simply mimicking the demonstrations but extracting abstract patterns.
- Evidence anchors:
  - [abstract] "We find numerous examples where it is strictly worse to provide in-context examples from the same distribution as the test example."
  - [section 4.2.2] "PALM, LLAMA, and FLAN-T5 sometimes perform better in the OOD setting than in the ID setting, showing that, in ICL, it is not always best to provide demonstrations from the same distribution as the test example."
  - [corpus] Weak - corpus neighbors do not discuss the effectiveness of simpler vs. matched demonstrations in ICL.

### Mechanism 3
- Claim: Distractors in the input do not significantly impact the model's ability to generalize OOD for most deduction rules.
- Mechanism: The model focuses on the logical structure of the proof rather than being misled by irrelevant information, except for specific rules like implication elimination and disjunction elimination in GPT-3.5.
- Core assumption: The model's attention mechanism can effectively filter out distractors when reasoning through the proof.
- Evidence anchors:
  - [section 4.3] "The models' performance is largely unaffected, with the exception of a few rules for specific models."
  - [section 4.3] "It seems only GPT-3.5 acquires these heuristics in implication and disjunction elimination."
  - [corpus] Weak - corpus neighbors do not discuss the impact of distractors on OOD generalization.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: COT prompting elicits reasoning steps from the model, allowing evaluation of its deductive reasoning ability.
  - Quick check question: What is the difference between standard prompting and chain-of-thought prompting in the context of deductive reasoning tasks?

- Concept: Out-of-distribution generalization
  - Why needed here: The study evaluates whether LLMs can handle proofs more complex than their demonstrations, testing their ability to generalize beyond their training distribution.
  - Quick check question: How does out-of-distribution generalization differ from in-distribution generalization in the context of deductive reasoning?

- Concept: Natural deduction rules
  - Why needed here: The study uses a complete set of deduction rules from propositional natural deduction to comprehensively test the model's reasoning ability.
  - Quick check question: What are the key differences between implication elimination and conjunction introduction in natural deduction?

## Architecture Onboarding

- Component map: Data generation -> Prompt generation -> LLM inference -> Proof evaluation
- Critical path: Data generation → Prompt generation → LLM inference → Proof evaluation
- Design tradeoffs: Using synthetic data allows for controlled complexity but may not capture all real-world reasoning scenarios. The choice of deduction rules and proof structures impacts the generalizability of the findings.
- Failure signatures: Poor performance on longer proofs or proofs with unseen deduction rules indicates limitations in the model's reasoning ability. Inconsistent performance across different models suggests that factors other than model size (e.g., training objectives) may be important.
- First 3 experiments:
  1. Test the model's ability to use each deduction rule individually with in-context examples from the same distribution as the test example.
  2. Evaluate the model's ability to generalize to unseen deduction rules by providing in-context examples with different rules than the test example.
  3. Assess the model's ability to handle compositional proofs by comparing performance when given compositional vs. individual rule demonstrations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do simpler in-context examples lead to better generalization than more complex ones for LLMs?
- Basis in paper: [inferred] The authors note that LLMs sometimes perform better in the OOD setting than in the ID setting, suggesting that simpler examples may be more effective for in-context learning.
- Why unresolved: The paper does not systematically compare the effects of simpler vs. more complex in-context examples on generalization.
- What evidence would resolve it: Experiments comparing LLM performance on OOD examples when given in-context examples of varying complexity.

### Open Question 2
- Question: Is there an optimal diversity of in-context examples for compositional generalization in LLMs?
- Basis in paper: [inferred] The authors mention that diverse demonstrations may be beneficial for in-context learning, but do not explore this systematically.
- Why unresolved: The paper does not test the effects of varying the diversity of in-context examples on compositional generalization.
- What evidence would resolve it: Experiments comparing LLM performance on compositional proofs when given in-context examples with varying levels of diversity.

### Open Question 3
- Question: Can LLMs learn to use implication introduction for deductive reasoning?
- Basis in paper: [explicit] The authors state that implication introduction is omitted from the deduction rules because it is unclear how to construct examples where its difficulty can be controlled.
- Why unresolved: The paper does not explore whether LLMs can learn this deduction rule given appropriate in-context examples.
- What evidence would resolve it: Experiments testing whether LLMs can use implication introduction when provided with suitable in-context examples.

## Limitations

- The synthetic nature of the dataset may not capture all real-world reasoning scenarios
- Limited evaluation on models larger than those tested, making it unclear if results generalize to frontier models
- Relatively small sample size for some experimental conditions (particularly for longer proofs)

## Confidence

- LLMs can generalize to compositional proofs from simpler demonstrations: Medium
- Simpler examples can outperform matched-complexity examples in ICL: Medium
- Distractors generally don't impact OOD generalization: Low-Medium

## Next Checks

1. Test the compositional generalization findings on frontier models (GPT-4, Claude, PaLM-2) to determine if the lack of model size correlation holds at larger scales.

2. Evaluate whether models that succeed on synthetic proofs can transfer this reasoning ability to natural language logical reasoning tasks with comparable structure.

3. Systematically compare deductive reasoning performance across models with different training objectives (causal vs. encoder-decoder vs. instruction-tuned) while controlling for model size.