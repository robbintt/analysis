---
ver: rpa2
title: Position Bias Estimation with Item Embedding for Sparse Dataset
arxiv_id: '2305.13931'
source_url: https://arxiv.org/abs/2305.13931
tags:
- position
- bias
- item
- embedding
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses position bias estimation in Learning to Rank,\
  \ particularly when logged data is sparse and skewed due to fixed item placement\
  \ by marketers. It introduces item embedding\u2014using Latent Semantic Indexing\
  \ (LSI) and Variational Autoencoder (VAE)\u2014to enrich the (item, position) matrix\
  \ and improve position bias estimation."
---

# Position Bias Estimation with Item Embedding for Sparse Dataset

## Quick Facts
- arXiv ID: 2305.13931
- Source URL: https://arxiv.org/abs/2305.13931
- Reference count: 28
- Key outcome: Item embedding (LSI and VAE) improves position bias estimation in sparse datasets, reducing RMSE by up to 33.4% (LSI) and 10.3% (VAE) on a real carousel advertisement dataset.

## Executive Summary
This paper addresses the challenge of estimating position bias in Learning to Rank when logged data is sparse and skewed due to fixed item placement by marketers. The authors propose using item embeddingsâ€”via Latent Semantic Indexing (LSI) and Variational Autoencoder (VAE)â€”to enrich the sparse (item, position) matrix, enabling more accurate position bias estimation. Synthetic experiments validate that higher sparsity and KL divergence increase RMSE, while real-world results show significant improvements using the proposed embedding methods.

## Method Summary
The method introduces item embeddings to mitigate data sparsity in position bias estimation. First, item features are mapped to dense embedding vectors using LSI or VAE. Each item is then assigned a probability distribution over embeddings, allowing synthetic (embedding, position) pairs to be sampled even when the original (item, position) pair is missing. A Regression EM algorithm operates on this augmented dataset to estimate position bias, leveraging the richer, less sparse embedding-based data to improve accuracy.

## Key Results
- Synthetic experiments show RMSE increases with higher sparsity ratio and KL divergence, validating the sparsity problem.
- On a real carousel advertisement dataset, the proposed methods reduce RMSE by 33.4% (LSI) and 10.3% (VAE) compared to baseline.
- The improvement demonstrates that item embedding effectively mitigates data sparsity and improves position bias estimation accuracy.

## Why This Works (Mechanism)

### Mechanism 1
Using item embeddings reduces the sparsity of the (item, position) matrix by mapping each item to a dense embedding vector and sampling additional (embedding, position) pairs. LSI and VAE compress high-dimensional sparse item features into lower-dimensional dense embedding vectors. The embedding probability function p(e|i) distributes each item's probability mass across embeddings, allowing reconstruction of item-position relationships even when the original pair is missing. This works under the assumption that similar items share similar embedding distributions, so estimating position bias for one item helps estimate it for another. Evidence: [abstract] "Using a public dataset and internal carousel advertisement click dataset, we empirically show that item embedding with Latent Semantic Indexing (LSI) and Variational autoencoder (VAE) improves the estimation of position bias." [section] "When the (item ð‘‹ , position 1) entry is missing, we assume that we can predict position bias from a similar item ð‘Œ ." Break condition: If embeddings fail to capture semantic similarity (e.g., random initialization, poor training data), the distributional assumption collapses and bias estimation degrades.

### Mechanism 2
Converting sparse (item, position) tuples into dense (embedding, position) tuples via sampling allows standard Regression EM to operate on richer, less sparse data. The algorithm samples reward w from the observed click c weighted by p(e|i), creating a new dataset D_e of (embedding, position, reward). This dataset has many more unique pairs than the original sparse D, enabling more stable position bias estimation. This works under the assumption that the transformation preserves the causal structure: P(w=1|e,u,k) = P(e|i)P(C=1|i,u,k) under the position-based click model. Evidence: [section] "We then transform a sparse tuple (ð‘–, ð‘˜) into a dense tuple (ð‘’, ð‘˜). In Section 4, we show this leads to the improvement of position bias estimation." [section] "We can now have a tuple for both positions (ð‘’0, ð‘˜0) and (ð‘’0, ð‘˜1) for each embedding vector, leading to alleviate the problem of data sparsity." Break condition: If p(e|i) is highly peaked (most mass on one embedding), the synthetic data D_e does not gain sparsity benefits.

### Mechanism 3
The regression-EM algorithm with embeddings improves position bias RMSE by reducing variance in bias estimates caused by limited (item, position) observations. The EM step infers latent variables (E=1,R=1|w,e,k) using current bias and relevance estimates; the M step updates these parameters from the augmented (embedding, position) dataset, smoothing over item similarities captured by embeddings. This works under the assumption that the regression model inside EM can generalize from embedding features to position bias even when direct (item, position) pairs are absent. Evidence: [abstract] "Our result shows that the Regression EM algorithm with VAE improves Root Mean Squared Error (RMSE) relatively by 10.3% and the Regression EM with LSI improves RMSE relatively by 33.4%." [section] "We then represent the original sparse matrix with a dense (embedding vector, position) matrix." Break condition: If the regression model is too simple or overfitting, it cannot exploit embedding similarity, and RMSE gains vanish.

## Foundational Learning

- Concept: Position-based click model (PBM)
  - Why needed here: The entire estimation problem assumes clicks decompose into examination probability P(E=1|k) and relevance P(R=1|i,u); understanding this factorization is key to grasping why position bias must be estimated separately.
  - Quick check question: In PBM, if an item is not examined, can it still be clicked? (Answer: No.)

- Concept: Kullback-Leibler divergence as a sparsity/skewness metric
  - Why needed here: KL divergence between logging policy and uniform distribution quantifies how far the data distribution deviates from ideal uniform coverage; this directly correlates with estimation error.
  - Quick check question: If KL divergence is zero, what does that say about the logging policy? (Answer: It is uniform.)

- Concept: Variational Autoencoder (VAE) for tabular data
  - Why needed here: VAE is used to generate dense embeddings from sparse item features; understanding its encoder-decoder structure and probabilistic nature is essential to implement the embedding step correctly.
  - Quick check question: What is the role of the latent variable z in a VAE? (Answer: It captures the compressed representation used to reconstruct the input.)

## Architecture Onboarding

- Component map: Data ingestion (user, item, click, position) -> Embedding generator (LSI or VAE) -> Synthetic reward sampler -> Regression-EM engine -> Evaluation module (RMSE)
- Critical path: 1. Compute item embeddings and p(e|i) 2. Sample synthetic dataset D_e 3. Run Regression-EM on D_e 4. Output position bias estimates
- Design tradeoffs: LSI: deterministic, faster, may capture linear relationships only; VAE: probabilistic, slower, can model non-linearities but needs careful tuning; Number of embeddings m: larger m â†’ less sparsity but more parameters and risk of overfitting; Softmax temperature in p(e|i): controls concentration; too high â†’ uniform, too low â†’ peaked
- Failure signatures: Embedding probabilities p(e|i) are almost all mass on one embedding â†’ no sparsity gain; Regression-EM fails to converge â†’ check initialization or data quality; RMSE does not improve over baseline â†’ embeddings may not capture item similarity
- First 3 experiments: 1. Generate synthetic data with known position bias, apply baseline Regression-EM, record RMSE; 2. Apply LSI embedding + synthetic sampling, run Regression-EM, compare RMSE to baseline; 3. Repeat with VAE embedding, vary embedding dimension m, plot RMSE vs. m

## Open Questions the Paper Calls Out

- Question: How does the performance of item embedding methods (LSI vs VAE) scale with different types of e-commerce datasets beyond carousel advertisements?
  - Basis in paper: [explicit] The paper shows LSI outperforms VAE on their carousel dataset, but only tests these two methods
  - Why unresolved: The paper only tests on one real-world dataset and two synthetic datasets, limiting generalizability
  - What evidence would resolve it: Comparative experiments across diverse e-commerce domains (search, recommendations, different ad formats) with multiple embedding methods

- Question: What is the theoretical limit of position bias estimation accuracy when using item embeddings to address data sparsity?
  - Basis in paper: [inferred] The paper shows improvements but doesn't establish bounds on estimation accuracy
  - Why unresolved: No theoretical analysis of the trade-off between embedding quality and position bias estimation error
  - What evidence would resolve it: Mathematical proofs or extensive empirical studies showing estimation error as a function of embedding quality, sparsity ratio, and KL divergence

- Question: How does the soft-max conversion method for creating p(e|i) probabilities compare to alternative approaches for handling sparse tabular data?
  - Basis in paper: [explicit] The paper uses soft-max conversion but acknowledges it as one approach among potential alternatives
  - Why unresolved: No comparison with other probability conversion methods or baseline approaches for tabular data
  - What evidence would resolve it: Comparative experiments using alternative methods like TabNet, neural network classifiers, or direct regression approaches for p(e|i) estimation

## Limitations

- The method relies heavily on the quality of item embeddings; if LSI or VAE fail to capture meaningful item similarity, the synthetic (embedding, position) pairs will not improve estimation and may even degrade it.
- No explicit comparison is provided against alternative position bias estimation methods for sparse data (e.g., randomized interventions, IPS-based approaches).
- The VAE method is described as slower and requiring careful hyperparameter tuning; scalability and robustness to hyperparameter choices are not extensively validated.

## Confidence

- High confidence in the theoretical framework (Regression EM, PBM) and synthetic experiments showing sparsity and KL divergence increase RMSE.
- Medium confidence in the practical benefit of embeddings, as improvements are shown on one real dataset and embedding quality is not independently validated.
- Medium confidence in the mechanism claims; the paper provides clear reasoning but lacks extensive ablation studies or alternative embedding comparisons.

## Next Checks

1. Test the method on a dataset where embeddings are intentionally degraded (e.g., random embeddings or poor training data) to confirm that RMSE does not improve and to establish the embedding quality dependency.
2. Compare against an IPS (Inverse Propensity Scoring) or randomized intervention baseline on the same real dataset to quantify relative advantage and assess robustness to different logging policies.
3. Perform an ablation study varying the number of embeddings m and the softmax temperature in p(e|i) to identify the optimal configuration and verify that improvements are not due to overfitting or arbitrary parameter choices.