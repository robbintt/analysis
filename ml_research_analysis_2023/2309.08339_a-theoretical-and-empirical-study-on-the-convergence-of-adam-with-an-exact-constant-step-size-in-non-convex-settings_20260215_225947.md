---
ver: rpa2
title: A Theoretical and Empirical Study on the Convergence of Adam with an "Exact"
  Constant Step Size in Non-Convex Settings
arxiv_id: '2309.08339'
source_url: https://arxiv.org/abs/2309.08339
tags:
- step
- size
- adam
- convergence
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical proof and empirical evidence
  that deterministic Adam with a constant step size converges to a critical point
  for non-convex objectives. The key insight is that using a fixed step size ensures
  convergence to a saddle point with gradient norms approaching zero, unlike time-dependent
  step sizes where the product of step size and gradient norm may vanish even if the
  gradient norm does not.
---

# A Theoretical and Empirical Study on the Convergence of Adam with an "Exact" Constant Step Size in Non-Convex Settings

## Quick Facts
- arXiv ID: 2309.08339
- Source URL: https://arxiv.org/abs/2309.08339
- Reference count: 40
- Primary result: First theoretical proof and empirical evidence that deterministic Adam with constant step size converges to a critical point for non-convex objectives.

## Executive Summary
This paper provides the first theoretical proof and empirical evidence that deterministic Adam with a constant step size converges to a critical point for non-convex objectives. The key insight is that using a fixed step size ensures convergence to a saddle point with gradient norms approaching zero, unlike time-dependent step sizes where the product of step size and gradient norm may vanish even if the gradient norm does not. The authors derive sufficient conditions for a constant step size to achieve asymptotic convergence of gradients to zero, with runtime bounds for deterministic Adam reaching approximate criticality in smooth non-convex functions. Empirically, their constant step size strategy shows faster convergence and higher validation accuracy compared to adaptive step size schedulers on a classification task, often converging to a saddle point near a local minimum.

## Method Summary
The paper proposes using Adam optimizer with a constant step size α ≈ sqrt(2L(w0)/(KT)), where K is the Lipschitz constant estimated from maximum singular values of trained layers, L(w0) is the initial loss, and T is the number of iterations. The method is tested on a handwritten digits classification task using a two-layer neural network with ReLU activations. Convergence is validated by tracking gradient norm reduction to near zero, while performance is compared against exponential decay, inverse square root decay, and cosine annealing schedules through training loss reduction speed and final validation accuracy.

## Key Results
- Deterministic Adam with constant step size converges to a critical point (saddle point) with gradient norms approaching zero for non-convex objectives.
- The convergence rate is O(1/T^(1/4)) for smooth non-convex functions when using the optimal constant step size.
- Empirically, the constant step size strategy achieves faster convergence and higher validation accuracy compared to adaptive step size schedulers on a classification task.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a constant step size forces the product α∇L(wt) to go to zero, which guarantees the gradient norm itself goes to zero and thus ensures convergence to a saddle point.
- Mechanism: With a fixed step size α, the update rule wt+1 = wt - α∇L(wt) means that if the sequence wt converges, then α∇L(wt) must converge to zero. This forces ∇L(wt) → 0.
- Core assumption: The loss function L(w) is K-Lipschitz smooth and has bounded gradients (σ-bounded).
- Evidence anchors:
  - [abstract] "using a fixed step size ensures convergence to a saddle point with gradient norms approaching zero, unlike time-dependent step sizes where the product of step size and gradient norm may vanish even if the gradient norm does not."
  - [section] "With a fixed step size αt = α > 0, if wt converges to wT, the L2 norm of gradient (∥∇L(wt)∥2) must also approach 0. This ensures convergence to a saddle point of L(w) with a fixed step size, an assurance not available with an iteration-dependent step size."
- Break condition: If the gradient is unbounded or the function is not K-Lipschitz smooth, the proof does not hold.

### Mechanism 2
- Claim: The Adam algorithm with constant step size converges to approximate criticality with rate O(1/T^1/4) for smooth non-convex functions.
- Mechanism: The proof bounds the gradient norm at each iteration using Lipschitz continuity and properties of the adaptive moment estimates. By summing these bounds and using the fact that the product αc + Kαδ^2/(2c) is finite, the minimum gradient norm over T iterations is shown to be O(1/T^1/4).
- Core assumption: The step size α is chosen appropriately as α = sqrt(2(L(w0) - L(w*))/Kδ^2T).
- Evidence anchors:
  - [abstract] "The authors derive sufficient conditions for a constant step size to achieve asymptotic convergence of gradients to zero, with runtime bounds for deterministic Adam reaching approximate criticality in smooth non-convex functions."
  - [section] "Theorem (1) with contradiction... min ∥∇L(wt)∥2 ≤ (2Kδ^2/T(L(w0) - L(w*)))^(1/4)"
- Break condition: If the step size is not chosen as specified, or if the function is not smooth and non-convex, the convergence rate may not hold.

### Mechanism 3
- Claim: Empirically, Adam with constant step size converges faster and achieves higher validation accuracy compared to adaptive step size schedulers on classification tasks.
- Mechanism: The constant step size maintains a more aggressive optimization throughout training, leading to faster reduction in gradient norms and better generalization as measured by validation accuracy. The constant step size avoids the sub-optimal outcomes caused by rapidly decaying step sizes in adaptive schedulers.
- Core assumption: The Lipschitz constant K can be estimated from the trained model, and the initial loss L(w0) is known.
- Evidence anchors:
  - [abstract] "Empirically, their constant step size strategy shows faster convergence and higher validation accuracy compared to adaptive step size schedulers on a classification task, often converging to a saddle point near a local minimum."
  - [section] "Exp. 2: Faster convergence towards good saddle point. Fig. 2(a) shows that our step size results in faster convergence compared to other methods."
- Break condition: If the Lipschitz constant K is not well estimated, or if the model architecture significantly changes, the empirical performance may degrade.

## Foundational Learning

- Concept: Lipschitz continuity and smoothness of functions
  - Why needed here: The convergence proof relies on the loss function being K-Lipschitz smooth to bound the change in loss between iterations.
  - Quick check question: What is the definition of a K-Lipschitz smooth function, and why is it important for gradient descent convergence?

- Concept: Adaptive gradient methods (e.g., Adam, RMSProp)
  - Why needed here: The paper analyzes the convergence properties of Adam with constant step size, so understanding how Adam works is crucial.
  - Quick check question: How does Adam differ from standard gradient descent, and what role do the moment estimates mt and vt play in the update rule?

- Concept: Saddle points and critical points in non-convex optimization
  - Why needed here: The paper proves convergence to a saddle point (where gradient norm approaches zero) rather than a local minimum, which is typical in non-convex landscapes.
  - Quick check question: What is the difference between a saddle point and a local minimum in non-convex optimization, and why is convergence to a saddle point still considered meaningful?

## Architecture Onboarding

- Component map:
  - Loss function L(w) -> Adam optimizer -> Model parameters wt+1 = wt - α * (At * mt)

- Critical path:
  1. Initialize model parameters w0 and Adam optimizer
  2. Compute gradients ∇L(wt) at each iteration
  3. Update moment estimates mt and vt
  4. Compute At from vt
  5. Update parameters: wt+1 = wt - α * (At * mt)
  6. Repeat until convergence or max iterations

- Design tradeoffs:
  - Constant step size vs. adaptive step size: Constant step size ensures gradient norm convergence but may require careful tuning; adaptive step size can be more robust but may not guarantee gradient norm convergence.
  - Estimating Lipschitz constant K: Accurate estimation is important for choosing the optimal step size, but can be challenging for complex models.

- Failure signatures:
  - Divergence: If the step size is too large or the Lipschitz constant is underestimated, the optimization may diverge.
  - Slow convergence: If the step size is too small or the Lipschitz constant is overestimated, convergence may be slow.
  - Getting stuck in poor saddle points: Adam with constant step size may converge to saddle points that are not close to good local minima.

- First 3 experiments:
  1. Verify convergence on a simple non-convex function (e.g., a quadratic with a saddle point) with known Lipschitz constant.
  2. Compare convergence speed and final gradient norms of Adam with constant step size vs. Adam with adaptive step size schedulers on a benchmark dataset (e.g., MNIST).
  3. Investigate the effect of different Lipschitz constant estimates on the convergence and final performance of Adam with constant step size.

## Open Questions the Paper Calls Out

- What is the precise relationship between the constant step size parameter and the convergence rate in non-convex optimization?
  - Basis in paper: [explicit] The paper states that deterministic ADAM with a constant step size converges to a critical point with a rate of O(1/T^(1/4)), but acknowledges that tightening the analysis and obtaining faster convergence rates remains an open problem.
  - Why unresolved: The current analysis provides a convergence rate bound, but it is not clear if this is the optimal rate or if there are ways to improve it.
  - What evidence would resolve it: A rigorous proof showing the tightest possible convergence rate for deterministic ADAM with a constant step size in non-convex settings, or empirical evidence demonstrating faster convergence than O(1/T^(1/4)).

- How does the choice of the Lipschitz constant K affect the optimal constant step size and the resulting performance of ADAM?
  - Basis in paper: [inferred] The paper mentions that the optimal constant step size depends on factors like the network's Lipschitz constant K, but does not provide a detailed analysis of how different values of K impact the step size and performance.
  - Why unresolved: The paper only briefly mentions the dependence on K and does not explore the sensitivity of ADAM's performance to variations in K.
  - What evidence would resolve it: Empirical studies showing the relationship between K, the constant step size, and the convergence rate and validation accuracy of ADAM for different neural network architectures and datasets.

- What are the theoretical guarantees for ADAM with a constant step size in the presence of noise or stochastic gradients?
  - Basis in paper: [explicit] The paper focuses on the deterministic case and mentions that the stochastic version of ADAM is also analyzed, but does not provide convergence guarantees for the stochastic case.
  - Why unresolved: The analysis is limited to the deterministic setting, and the behavior of ADAM with a constant step size under stochastic gradients is not fully characterized.
  - What evidence would resolve it: Theoretical results proving convergence rates or other guarantees for ADAM with a constant step size in the presence of stochastic gradients, along with empirical validation on real-world noisy datasets.

## Limitations

- The convergence proof critically depends on estimating the Lipschitz constant K accurately, but the paper provides only a heuristic approach based on maximum singular values of trained layers.
- While the paper proves convergence to a saddle point, it does not guarantee convergence to a local minimum, which may be a limitation for applications requiring global or local optimality.
- The empirical validation is limited to a single classification task with a simple two-layer network, raising questions about generalizability to deeper architectures and other problem domains.

## Confidence

- **High Confidence**: The theoretical mechanism that constant step size forces gradient norm to zero is well-established mathematically, given the Lipschitz smoothness assumptions. The core proof structure following from contradiction arguments is sound.
- **Medium Confidence**: The empirical claims about faster convergence and higher validation accuracy are supported by the presented results, but the single experiment setup limits generalizability. The comparison against multiple adaptive schedulers strengthens these claims.
- **Low Confidence**: The practical estimation method for the Lipschitz constant K is heuristic and not rigorously validated across different architectures. The omission of δ²'s exact form in the step size formula creates uncertainty in implementation.

## Next Checks

1. **Lipschitz Estimation Validation**: Systematically evaluate how sensitive the convergence rate and final performance are to different methods of estimating K across various network architectures (CNNs, ResNets, Transformers). Compare singular value-based estimation against alternative approaches like back-substitution or random sampling.

2. **Saddle Point Characterization**: Design experiments to verify whether the algorithm converges to high-quality saddle points near local minima versus poor saddle points in flat regions. This could involve visualizing the loss landscape around converged points or measuring second-order information.

3. **Architecture Transferability**: Test the constant step size Adam strategy on non-vision tasks such as language modeling or reinforcement learning to assess whether the theoretical convergence guarantees and empirical benefits extend beyond simple classification tasks.