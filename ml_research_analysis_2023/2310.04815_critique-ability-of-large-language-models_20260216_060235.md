---
ver: rpa2
title: Critique Ability of Large Language Models
arxiv_id: '2310.04815'
source_url: https://arxiv.org/abs/2310.04815
tags:
- answer
- critique
- accuracy
- tasks
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRITIC BENCH, a benchmark for evaluating
  the critique abilities of large language models (LLMs) across diverse tasks. The
  benchmark consists of 3K high-quality query-response-judgment triplets covering
  math problem-solving, code completion, and question answering.
---

# Critique Ability of Large Language Models

## Quick Facts
- arXiv ID: 2310.04815
- Source URL: https://arxiv.org/abs/2310.04815
- Reference count: 40
- Primary result: Critique is an emergent ability in large language models, with self-critique particularly challenging even for top models

## Executive Summary
This paper introduces CRITIC BENCH, a benchmark designed to evaluate the critique abilities of large language models across math problem-solving, code completion, and question answering tasks. The authors find that critique ability is generally difficult for most LLMs and only emerges in larger models. Self-critique proves especially challenging, even for top-performing models. To address this, they propose a self-check method that leverages self-critique to improve task performance, achieving an average 9.55% error reduction rate on math word problems.

## Method Summary
The authors constructed CRITIC BENCH with 3K query-response-judgment triplets from GSM8K, HumanEval, and TruthfulQA datasets. They evaluated multiple LLM families (PaLM-2, LLaMA, LLaMA-2, ChatGPT) using few-shot chain-of-thought prompting to assess critique accuracy. The self-check method was implemented by adding self-critique filtering to self-consistency evaluation. The minimum viable reproduction involves collecting query-response pairs, evaluating critique accuracy across models, and implementing the self-check method with varying certainty thresholds.

## Key Results
- Critique ability is an emergent property appearing only in larger language models
- Self-critique remains particularly difficult even for top-performing models
- The self-check method achieves an average 9.55% error reduction rate on math word problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critique ability in LLMs is an emergent capability that only appears in larger models
- Mechanism: Larger models develop more complex reasoning and error-detection processes through increased parameter count and training data exposure
- Core assumption: Model size directly correlates with the ability to perform multi-step reasoning and identify logical flaws
- Evidence anchors:
  - [abstract] "Critique is generally challenging for most LLMs, and this capability often emerges only when models are sufficiently large"
  - [section] "Only large-scale models exhibit performance with a notable difference from a random guess baseline"
  - [corpus] Weak evidence - related papers focus on critique applications rather than emergence properties

### Mechanism 2
- Claim: Self-critique is particularly difficult because models struggle to recognize their own reasoning errors
- Mechanism: Models tend to have blind spots about their own reasoning process, making it hard to identify flaws in outputs they generated
- Core assumption: A model that produces an incorrect answer is likely to have flawed reasoning that it cannot easily detect
- Evidence anchors:
  - [abstract] "self-critique is especially difficult. Even top-performing LLMs struggle to achieve satisfactory performance"
  - [section] "models tend to have lower critique accuracy on problems where they are most uncertain"
  - [corpus] Weak evidence - related work mentions self-critique but doesn't deeply analyze why it's difficult

### Mechanism 3
- Claim: The self-check method improves performance by having models verify their answers before final submission
- Mechanism: By prompting models to critique their own responses, they can identify and correct errors before presenting final answers
- Core assumption: Models can successfully detect flaws in their own work when explicitly prompted to do so
- Evidence anchors:
  - [abstract] "we introduce a simple yet effective baseline named self-check, which leverages self-critique to improve task performance for various models"
  - [section] "The method consistently enhances the baseline performance... achieving an average of 9.55% error reduction rate on math word problems"
  - [corpus] Moderate evidence - related papers discuss self-critique pipelines but focus on different applications

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: The paper relies heavily on chain-of-thought analysis for both generating responses and critiques
  - Quick check question: Can you explain how chain-of-thought prompting differs from direct question answering?

- Concept: Emergent abilities in LLMs
  - Why needed here: The paper argues that critique ability is an emergent property that appears only in larger models
  - Quick check question: What other abilities have been identified as emergent in LLMs, and how do they compare to critique ability?

- Concept: Self-consistency methods
  - Why needed here: The self-check method builds on self-consistency by adding a verification step
  - Quick check question: How does self-consistency improve over simple majority voting, and what role does critique play in this improvement?

## Architecture Onboarding

- Component map: Query → Response Generation → Certainty Score Calculation → Critique Evaluation → Self-Check Filtering → Final Answer
- Critical path: Query → Response Generation → Certainty Score Calculation → Critique Evaluation → Self-Check Filtering → Final Answer
- Design tradeoffs: Using pretrained models vs. fine-tuned models, computational cost of multiple critiques vs. accuracy gains, breadth of task coverage vs. depth of evaluation
- Failure signatures: Random guessing performance, inability to detect subtle errors, over-confidence in incorrect answers, failure to improve with self-check
- First 3 experiments:
  1. Evaluate different model sizes on CriticBench to confirm scaling law
  2. Test self-check method on GSM8K with varying certainty thresholds
  3. Compare critique accuracy on high-certainty vs. low-certainty questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the critique ability of smaller language models be enhanced to match that of larger models?
- Basis in paper: [explicit] The paper discusses that critique ability is an emergent ability that typically emerges only in larger-scale models, and smaller models generally underperform in critique tasks.
- Why unresolved: While the paper suggests that fine-tuning might be considered for smaller models, it does not explore specific methods or techniques to enhance their critique abilities.
- What evidence would resolve it: Research experiments comparing the critique performance of smaller models before and after applying various enhancement techniques, such as fine-tuning, few-shot learning, or specialized training datasets.

### Open Question 2
- Question: Can the self-check method be extended to other tasks beyond math word problems to achieve similar error reduction rates?
- Basis in paper: [explicit] The paper introduces the self-check method, which leverages self-critique to improve task performance on math word problems, achieving an average of 9.55% error reduction rate.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the self-check method on math word problems, but does not explore its applicability to other tasks or domains.
- What evidence would resolve it: Experiments applying the self-check method to a variety of tasks, such as code completion, question answering, and text summarization, and comparing the resulting error reduction rates to those achieved on math word problems.

### Open Question 3
- Question: How can the critique ability of language models be used to improve their overall performance and generalization capabilities?
- Basis in paper: [inferred] The paper discusses the potential of self-critique for autonomous self-improvement and suggests that critique ability could serve as a source of supervised signals for model tuning.
- Why unresolved: While the paper proposes the self-check method as a simple baseline, it does not explore more advanced techniques for leveraging critique ability to enhance model performance and generalization.
- What evidence would resolve it: Research experiments investigating the impact of incorporating critique-based feedback into the training process of language models, and evaluating the resulting improvements in performance and generalization across various tasks and domains.

## Limitations

- The benchmark focuses primarily on correctness judgments rather than nuanced aspects of critique quality
- Only three task types are covered, potentially missing critique challenges in other domains
- Reliance on model-generated responses may introduce bias toward specific model capabilities

## Confidence

**High confidence**: Critique being an emergent capability requiring sufficient model scale
**Medium confidence**: Specific effectiveness metrics of the self-check method (9.55% error reduction)
**Low confidence**: Generalizability of critique difficulty patterns to other domains

## Next Checks

1. Evaluate the self-check method across additional task types beyond math word problems to verify the claimed 9.55% average error reduction generalizes across domains.

2. Conduct ablation studies testing whether critique performance improves with different prompt engineering approaches, including zero-shot vs. few-shot prompting and varying chain-of-thought complexity.

3. Test the scaling behavior claims by evaluating additional model families at intermediate sizes between the current smallest and largest models.