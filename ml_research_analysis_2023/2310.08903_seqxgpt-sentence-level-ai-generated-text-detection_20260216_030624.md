---
ver: rpa2
title: 'SeqXGPT: Sentence-Level AI-Generated Text Detection'
arxiv_id: '2310.08903'
source_url: https://arxiv.org/abs/2310.08903
tags:
- detection
- aigt
- sentence-level
- seqxgpt
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a sentence-level AI-generated text (AIGT)
  detection challenge, where documents contain both human-written and AI-generated
  sentences. To address this, the authors propose SeqXGPT, which uses log probability
  lists from white-box language models as features.
---

# SeqXGPT: Sentence-Level AI-Generated Text Detection

## Quick Facts
- arXiv ID: 2310.08903
- Source URL: https://arxiv.org/abs/2310.08903
- Reference count: 13
- Primary result: Introduces SeqXGPT for sentence-level AIGT detection, outperforming existing methods on sentence and document-level tasks

## Executive Summary
This paper addresses the challenge of detecting AI-generated text at the sentence level, where documents contain mixed human-written and AI-generated content. The authors propose SeqXGPT, which extracts log probability lists from multiple white-box language models as foundational features, processes them through convolutional and self-attention networks, and achieves state-of-the-art performance on sentence-level AIGT detection. The method is evaluated on a newly constructed dataset (SeqXGPT-Bench) and demonstrates strong generalization capabilities on out-of-distribution data.

## Method Summary
SeqXGPT uses log probability distributions from white-box language models (GPT-2, GPT-Neo, GPT-J, LLaMA) as foundational features for sentence-level AIGT detection. These probability lists are treated as temporal features and processed through a CNN-Transformer architecture. The model uses sequence labeling to classify each word, then applies majority voting to determine sentence categories. This approach captures contextual information across sentences and handles fine-grained detection in mixed-content documents. The method is evaluated on a newly constructed dataset and compared against existing approaches like DetectGPT, Sniffer, and RoBERTa-based methods.

## Key Results
- SeqXGPT outperforms existing methods like DetectGPT, Sniffer, and RoBERTa-based approaches in both sentence and document-level AIGT detection
- The model demonstrates strong generalization capabilities on out-of-distribution data
- CNN layers are essential for extracting local patterns from sparse probability features before self-attention processing
- The approach effectively handles fine-grained detection in documents with mixed human and AI-generated content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log probability features from white-box models capture distinguishing patterns between human and AI-generated text
- Mechanism: Extracts word-wise log probabilities from multiple white-box models and processes them through CNN and self-attention networks
- Core assumption: Probability distributions differ systematically between human and AI text, preserved in log probability lists
- Evidence anchors: [abstract] features cannot be studied by LLMs; [section] average per-token log probability contributes to detection
- Break condition: If AI models learn to mimic human probability distributions

### Mechanism 2
- Claim: Sequence labeling with contextualized features outperforms sentence classification
- Mechanism: Treats entire document as input, labels each word, determines sentence category by majority vote
- Core assumption: Context from surrounding text improves discrimination between human and AI-generated sentences
- Evidence anchors: [section] self-attention captures long-range dependencies; [section] RoBERTa-based methods slightly inferior
- Break condition: If document-level context becomes less relevant or majority vote fails for balanced content

### Mechanism 3
- Claim: CNN layers are essential for extracting local patterns from sparse, correlated probability features
- Mechanism: Convolutional networks with kernel sizes (5,3,3,3,3) extract local features before self-attention
- Core assumption: Foundational features are too sparse for transformers alone to learn effectively
- Evidence anchors: [section] CNNs handle temporal features well; [section] high correlation and sparsity make transformer learning difficult
- Break condition: If probability features become less sparse or transformers evolve to handle such features better

## Foundational Learning

- Concept: Log probability distributions and perplexity
  - Why needed here: Method relies on comparing log probability lists as distinguishing features
  - Quick check question: Why would AI-generated text typically have different log probability patterns compared to human-written text?

- Concept: Sequence labeling vs classification tasks
  - Why needed here: Uses sequence labeling with majority voting rather than independent sentence classification
  - Quick check question: What advantage does sequence labeling with majority voting provide over classifying each sentence independently in mixed-content documents?

- Concept: Convolutional neural networks for temporal feature extraction
  - Why needed here: CNNs process wave-like probability features before self-attention layers
  - Quick check question: How do convolutional layers help extract useful features from sparse, correlated temporal data like log probability lists?

## Architecture Onboarding

- Component map: Text → aligned tokens → log probability lists → CNN → transformer → linear classifier → word labels → sentence labels
- Critical path: Text → aligned tokens → log probability lists → CNN → transformer → linear classifier → word labels → sentence labels
- Design tradeoffs:
  - Multiple white-box models provide richer features but increase computation
  - Sequence labeling captures context but requires more memory than sentence classification
  - CNN preprocessing helps with sparse features but adds complexity
  - Majority voting is simple but may fail for balanced documents
- Failure signatures:
  - Poor performance on out-of-distribution data suggests overfitting
  - Random predictions indicate issues with feature extraction or model capacity
  - Systematic bias toward one category suggests class imbalance or labeling issues
- First 3 experiments:
  1. Test on simple dataset with clearly distinguishable human vs AI text to verify basic functionality
  2. Compare performance with and without CNN layers to validate their importance
  3. Evaluate on out-of-distribution data to assess generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary when incorporating semantic features for sentence recognition?
- Basis in paper: [inferred] Current model performs well without semantic features, which could potentially assist further
- Why unresolved: Authors explicitly state this as limitation and leave semantic feature incorporation for future work
- What evidence would resolve it: Experiments incorporating semantic features and comparing results with current model performance

### Open Question 2
- Question: What is the impact of using more diversified instructions on GPT-3.5-turbo performance?
- Basis in paper: [explicit] Authors did not extensively investigate impact of more diversified instructions during dataset construction
- Why unresolved: Focus was on studying detection without analyzing instruction mechanisms and better usage
- What evidence would resolve it: Experimenting with various instructions and analyzing how they affect detectability by SeqXGPT

### Open Question 3
- Question: How would performance change with documents containing sentences from multiple sources?
- Basis in paper: [explicit] Current samples only consist of two distinct sources due to model context length limitations
- Why unresolved: Study restricted by context length and generation patterns, aim to explore more complex scenarios in future
- What evidence would resolve it: Testing datasets with documents containing sentences from three or more distinct sources

## Limitations

- Opaque alignment mechanism between token-wise and word-wise log probabilities not fully specified
- Dependence on multiple white-box language models creates practical deployment constraints
- Majority voting mechanism could fail when documents contain roughly equal proportions of human and AI-generated content

## Confidence

- High confidence: Log probability features from white-box models can distinguish AIGT from human text, supported by consistent performance improvements
- Medium confidence: Generalization claims supported by out-of-distribution testing but limited scope of tested domains and models
- Low confidence: CNN + transformer architecture optimality lacks direct comparison to alternative architectures

## Next Checks

1. **Alignment mechanism verification**: Implement and test the word-token alignment method independently to confirm it preserves discriminative properties across different tokenization schemes

2. **Domain generalization stress test**: Evaluate SeqXGPT on completely unseen domains (legal, medical, technical) and with adversarial human editing of AI-generated text to probe robustness boundaries

3. **Architectural alternative comparison**: Compare CNN + transformer approach against simpler architectures (pure transformer, CNN-only, attention-only) on the same probability features to validate current architecture necessity