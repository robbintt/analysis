---
ver: rpa2
title: How well ChatGPT understand Malaysian English? An Evaluation on Named Entity
  Recognition and Relation Extraction
arxiv_id: '2311.11583'
source_url: https://arxiv.org/abs/2311.11583
tags:
- chatgpt
- docred
- entity
- relation
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ChatGPT's ability to perform named entity
  recognition and relation extraction on Malaysian English news articles. A novel
  three-step "educate-predict-evaluate" methodology is proposed, using in-context
  learning with annotation guidelines, few-shot samples, and self-consistent prompting
  techniques across 18 prompt settings.
---

# How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction

## Quick Facts
- arXiv ID: 2311.11583
- Source URL: https://arxiv.org/abs/2311.11583
- Reference count: 40
- ChatGPT achieved F1-Score of 0.497 for Malaysian English entity extraction

## Executive Summary
This study evaluates ChatGPT's ability to perform named entity recognition and relation extraction on Malaysian English news articles using a novel three-step "educate-predict-evaluate" methodology. The researchers developed 18 prompt settings incorporating annotation guidelines, few-shot samples, and self-consistent prompting techniques. While ChatGPT struggled with entity extraction in Malaysian English (F1-Score 0.497), achieving modest performance, it performed reasonably well on relation extraction (average F1-Score 0.64) using the DocRED dataset. The study identifies morphosyntactic adaptations in Malaysian English—including loan words, compound blends, and derived words—as key factors limiting ChatGPT's performance on named entity recognition tasks.

## Method Summary
The researchers employed an "educate-predict-evaluate" methodology using in-context learning with ChatGPT's web interface. They created 18 unique prompt settings combining zero-shot, few-shot with annotation guidelines, and self-consistent prompting techniques. The evaluation used the MEN-Dataset containing 200 Malaysian English news articles with human annotations, plus the DocRED dataset for relation extraction. Performance was measured by calculating F1-Scores comparing ChatGPT's outputs against human annotations. The study specifically tested how different prompt engineering approaches affected ChatGPT's ability to handle morphosyntactic adaptations unique to Malaysian English.

## Key Results
- ChatGPT achieved highest F1-Score of 0.497 for named entity extraction on Malaysian English
- Morphosyntactic adaptations (loan words, compound blends, derived words) significantly impacted entity recognition performance
- Relation extraction achieved average F1-Score of 0.64, showing less sensitivity to Malaysian English linguistic features
- Self-consistent prompting technique did not improve ChatGPT's entity extraction performance for Malaysian English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing annotation guidelines during in-context learning significantly improves ChatGPT's entity extraction performance in Malaysian English.
- Mechanism: Guidelines act as structured context that informs ChatGPT about task-specific entity labels and their definitions, reducing ambiguity in morphosyntactically adapted Malaysian English.
- Core assumption: ChatGPT's in-context learning is sensitive to explicit task definitions and can integrate structured guidelines into its reasoning process.
- Evidence anchors:
  - [abstract] "The performance of ChatGPT is assessed using F1-Score across 18 unique prompt settings, which were carefully engineered for a comprehensive review."
  - [section 3] "The idea behind this is to teach ChatGPT how to extract entity and relation from Malaysian English texts. To accomplish this, we provided ChatGPT with the annotation guideline prepared while developing MEN-Dataset."
  - [corpus] Weak - no direct citation; inferred from experimental setup.

### Mechanism 2
- Claim: Self-consistent prompting techniques do not significantly improve ChatGPT's performance on Malaysian English entity extraction.
- Mechanism: Self-consistency relies on multiple sampling to select the most frequent answer, but Malaysian English's morphosyntactic complexity may lead to consistent but incorrect predictions across samples.
- Core assumption: Consistency in outputs implies correctness, which may not hold for language varieties with high linguistic variation.
- Evidence anchors:
  - [section 5.1] "Self-consistent technique is not effective in ensuring quality output by ChatGPT... this technique helps to ensure the consistency of the outcome."
  - [section 5.2] "ChatGPT did not work well in extracting entity mentions with Loan Words, Compound Blend, and Derived Words."
  - [corpus] Weak - inference from experimental results without explicit corpus citation.

### Mechanism 3
- Claim: Morphosyntactic adaptations in Malaysian English (loan words, compound blends, derived words) negatively impact ChatGPT's named entity recognition but not relation extraction.
- Mechanism: NER requires precise entity boundary detection, which is disrupted by non-standard morphological forms, while RE focuses on relationship patterns that are less sensitive to entity form variations.
- Core assumption: Entity extraction is more sensitive to surface form variations than relation extraction, which relies more on contextual understanding.
- Evidence anchors:
  - [abstract] "Further analysis shows that the morphosyntactic adaptation in Malaysian English caused the limitation. However, interestingly, this morphosyntactic adaptation does not impact the performance of ChatGPT for relation extraction."
  - [section 5.2] "ChatGPT did not work well in extracting entity mentions with Loan Words, Compound Blend, and Derived Words."
  - [corpus] Weak - supported by paper findings but no direct corpus citation.

## Foundational Learning

- Concept: Morphosyntactic adaptation in Malaysian English
  - Why needed here: Understanding how loan words, compound blends, and derived words affect NLP model performance is crucial for interpreting ChatGPT's limitations.
  - Quick check question: What are the three main types of morphosyntactic adaptations identified in Malaysian English that impacted ChatGPT's performance?

- Concept: In-context learning (ICL)
  - Why needed here: The methodology relies heavily on providing contextual information to ChatGPT to improve its task performance.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches for language models?

- Concept: F1-Score calculation and interpretation
  - Why needed here: The evaluation metric used to assess ChatGPT's performance requires understanding of precision, recall, and their harmonic mean.
  - Quick check question: If ChatGPT has high precision but low recall for entity extraction, what does this tell us about its performance?

## Architecture Onboarding

- Component map: MEN-Dataset -> Annotation guidelines -> Few-shot samples -> Self-consistent prompting -> ChatGPT predictions -> F1-score comparison with human annotations
- Critical path: Malaysian English News (MEN) dataset → Annotation guidelines → Few-shot samples → Self-consistent prompting → ChatGPT predictions → F1-score comparison with human annotations
- Design tradeoffs: Using ChatGPT web interface vs API (context retention vs cost/resource efficiency)
- Failure signatures: Consistent misidentification of entities with loan words/compound blends; high proportion of NO_RELATION predictions; lower performance on PERSON, ORGANIZATION, LOCATION entities compared to standard English
- First 3 experiments:
  1. Replicate zero-shot performance without any guidelines to establish baseline
  2. Test with annotation guidelines only to isolate guideline impact
  3. Implement self-consistent prompting with 3 iterations to test consistency vs accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance on NER for Malaysian English compare to other non-standard English variants like Indian English or Singaporean English?
- Basis in paper: [explicit] The paper evaluates ChatGPT's performance on Malaysian English but does not compare it to other English variants.
- Why unresolved: The study focuses solely on Malaysian English, leaving a gap in understanding how ChatGPT handles other regional English varieties.
- What evidence would resolve it: Conducting similar evaluations on other English variants and comparing F1-scores across different datasets.

### Open Question 2
- Question: What is the impact of increasing the dataset size on ChatGPT's NER performance for Malaysian English?
- Basis in paper: [inferred] The paper mentions that the MEN-Dataset consists of only 200 news articles, which may limit the generalizability of the results.
- Why unresolved: The study uses a relatively small dataset, and it is unclear how performance scales with more data.
- What evidence would resolve it: Expanding the dataset and re-evaluating ChatGPT's performance to see if F1-scores improve significantly.

### Open Question 3
- Question: How does ChatGPT handle dynamic changes in language use, such as the introduction of new loan words or slang in Malaysian English over time?
- Basis in paper: [explicit] The paper notes that morphosyntactic adaptations in Malaysian English, like loan words, impact ChatGPT's performance.
- Why unresolved: The study does not address how ChatGPT adapts to evolving language trends or new vocabulary.
- What evidence would resolve it: Testing ChatGPT on datasets that include recent or newly coined terms and comparing performance over time.

## Limitations
- Limited dataset size (200 articles) may affect generalizability of results
- Web interface interactions introduce variability compared to controlled API-based experiments
- Evaluation on DocRED dataset (standard English) may not accurately reflect ChatGPT's relation extraction performance on Malaysian English

## Confidence

- **High confidence**: The finding that morphosyntactic adaptations (loan words, compound blends, derived words) negatively impact entity recognition performance. This is directly supported by experimental results and clear analysis in Section 5.2.
- **Medium confidence**: The claim that self-consistent prompting does not improve performance for Malaysian English. While results show this pattern, the explanation about consistency reinforcing systematic errors is somewhat speculative.
- **Low confidence**: The assertion that relation extraction is unaffected by morphosyntactic adaptations. This conclusion may be influenced by the DocRED dataset using standard English, which doesn't adequately test ChatGPT's ability to handle Malaysian English relations.

## Next Checks
1. Replicate the study using ChatGPT API with controlled session parameters to verify the stability of reported F1-Scores across different interface versions.
2. Conduct ablation studies testing the individual impact of annotation guidelines versus few-shot examples to isolate their respective contributions to performance.
3. Expand evaluation to include a Malaysian English-specific relation extraction dataset to properly test whether morphosyntactic adaptations truly don't affect RE performance.