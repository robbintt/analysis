---
ver: rpa2
title: 'The Curse of Recursion: Training on Generated Data Makes Models Forget'
arxiv_id: '2305.17493'
source_url: https://arxiv.org/abs/2305.17493
tags:
- data
- generation
- dementia
- original
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a fundamental problem called "model dementia"
  where generative models trained on data produced by previous model generations progressively
  forget the original data distribution, especially rare events. The core issue arises
  from two compounding errors: statistical approximation error (from finite sampling)
  and functional approximation error (from imperfect model expressivity).'
---

# The Curse of Recursion: Training on Generated Data Makes Models Forget

## Quick Facts
- arXiv ID: 2305.17493
- Source URL: https://arxiv.org/abs/2305.17493
- Authors: 
- Reference count: 22
- One-line primary result: Recursive training on model-generated data causes progressive loss of tail information and convergence to simple distributions, requiring access to genuine human data to prevent irreversible degradation.

## Executive Summary
This paper identifies a fundamental problem called "model dementia" where generative models trained on data produced by previous model generations progressively forget the original data distribution, especially rare events. The core issue arises from two compounding errors: statistical approximation error (from finite sampling) and functional approximation error (from imperfect model expressivity). Theoretical analysis shows that without superlinear growth in training data size, the Wasserstein distance between successive model generations and the original distribution diverges over time. Empirical validation across Gaussian Mixture Models, Variational Autoencoders, and fine-tuned language models demonstrates this phenomenon: models progressively converge to simpler distributions with reduced variance, losing ability to represent rare events. For language models, this manifests as increased perplexity on original test data and production of sequences the original model would assign very low probability.

## Method Summary
The paper investigates model dementia through recursive training experiments across three model types: Gaussian Mixture Models using expectation-maximization, Variational Autoencoders with latent Gaussian distributions, and fine-tuned OPT-125m language models on wikitext2. The recursive training pipeline involves training a model on original data, generating new data from the trained model, then training the next generation on this generated data. Experiments track Wasserstein distance between distributions, perplexity of generated sequences when evaluated by the original model, and visual quality of generated samples. The study examines how model performance degrades across multiple generations and under different data scaling conditions.

## Key Results
- Without superlinear growth in training data size, the Wasserstein distance between successive model generations and the original distribution diverges over time
- Models progressively converge to simpler distributions with reduced variance, losing ability to represent rare events
- For language models, increased perplexity on original test data and production of sequences the original model would assign very low probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive training on model-generated data causes progressive loss of tail information in the data distribution
- Mechanism: Each generation samples from an imperfect model approximation, losing rare events that have low probability of being sampled (statistical approximation error). Over multiple generations, this compounds into irreversible distribution shift
- Core assumption: Sampling error at each generation is non-zero and accumulates over time
- Evidence anchors:
  - [abstract] "use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear"
  - [section] "low probability events...get cut off...we lose information about them"
  - [corpus] Weak evidence - neighbor papers focus on unlearning/adaptation but not recursive training dynamics

### Mechanism 2
- Claim: Model convergence to simple distributions with reduced variance over generations
- Mechanism: As tails disappear and only high-probability events survive resampling, models converge to delta functions with minimal variance, losing ability to represent complex multimodal distributions
- Core assumption: Model architecture can represent simple distributions even when original data is complex
- Evidence anchors:
  - [abstract] "tails of the original content distribution disappear" and "converge to a point estimate with very small variance"
  - [section] "over time learned behaviours start converging to a point estimate with very small variance"
  - [corpus] Moderate evidence - neighbor paper on "GOLD" discusses limitations of LLM-generated data for distillation

### Mechanism 3
- Claim: Wasserstein distance between model generations and original distribution diverges without superlinear data growth
- Mechanism: Theoretical analysis shows that without superlinear increase in sample size per generation, the statistical approximation error causes the Wasserstein-2 distance to grow unbounded over time
- Core assumption: Models approximate distributions with finite samples at each generation
- Evidence anchors:
  - [abstract] "without superlinear growth in training data size, the Wasserstein distance...diverges over time"
  - [section] "the risk that occurs in this model ends up diverging for a constant sample size at each generation"
  - [corpus] Weak evidence - no direct corpus support for Wasserstein divergence analysis

## Foundational Learning

- Concept: Statistical approximation error from finite sampling
  - Why needed here: Core mechanism causing model dementia - rare events get lost due to low sampling probability
  - Quick check question: If an event has probability 1/1000 in a dataset of 1000 samples, what's the probability it appears at least once?

- Concept: Markov chain convergence to absorbing states
  - Why needed here: Explains why distribution must converge to delta function over generations
  - Quick check question: In a discrete distribution, what type of state does a Markov chain converge to when sampling error eliminates variation?

- Concept: Wasserstein distance as distributional similarity metric
  - Why needed here: Quantifies how model approximations diverge from original distribution over generations
  - Quick check question: What property of Wasserstein distance makes it suitable for measuring distribution shift in this recursive training scenario?

## Architecture Onboarding

- Component map: Training pipeline → Data generation → Model approximation → Resampling → Next generation
- Critical path: Data generation → Model training → Evaluation of distribution shift
- Design tradeoffs: Sample size vs computational cost; model expressiveness vs overfitting risk
- Failure signatures: Increasing perplexity on original test data; loss of rare event representation; convergence to simple distributions
- First 3 experiments:
  1. Implement single-dimensional Gaussian recursion with varying sample sizes to observe variance divergence
  2. Test GMM training on generated data across multiple generations to visualize distribution collapse
  3. Fine-tune OPT-125m on wikitext2 for 5 epochs, generate equal-sized dataset from trained model, fine-tune new model on generated data, repeat for 5+ generations, measure perplexity of each generation's output when evaluated by original model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum superlinear scaling factor required for training data to prevent model dementia in language models?
- Basis in paper: [inferred] The paper shows that without superlinear growth in training data size, the Wasserstein distance between successive model generations and the original distribution diverges over time. It mentions that "superlinear scaling would be required to minimise the lower bound on model dementia" but doesn't specify the exact factor needed.
- Why unresolved: The paper provides theoretical bounds showing that superlinear scaling is necessary but doesn't calculate the precise scaling factor required in practice for different model types and data distributions.
- What evidence would resolve it: Empirical experiments measuring model performance degradation across different scaling factors (quadratic, cubic, etc.) while training successive generations on generated data.

### Open Question 2
- Question: Can architectural modifications to language models (such as regularization techniques or model capacity increases) mitigate model dementia without requiring exponentially more training data?
- Basis in paper: [inferred] The paper demonstrates that model dementia occurs across various model types (VAEs, GMMs, LLMs) and attributes it to statistical and functional approximation errors, but doesn't explore whether architectural changes could address these errors differently.
- Why unresolved: The experiments focus on standard model architectures and training procedures, leaving open whether alternative architectures or training methods could be more resilient to the compounding errors that cause model dementia.
- What evidence would resolve it: Comparative studies training multiple generations of models with different architectures (varying depth, width, regularization methods) on generated data to measure relative resistance to performance degradation.

### Open Question 3
- Question: How does model dementia affect specialized domains with inherently skewed or multimodal distributions (e.g., medical imaging, legal text) compared to general-purpose datasets?
- Basis in paper: [explicit] The paper shows that model dementia causes loss of tail information and convergence to simpler distributions, but all experiments use relatively simple or general datasets (Gaussian mixtures, general web text).
- Why unresolved: The theoretical analysis and experiments focus on generic distributions, but specialized domains may have unique characteristics that could either exacerbate or mitigate the dementia effect.
- What evidence would resolve it: Controlled experiments training successive generations of models on domain-specific datasets with known multimodal or skewed distributions, measuring preservation of rare but important patterns over generations.

## Limitations

- The theoretical framework primarily focuses on simple distributions (Gaussians, GMMs) and may not fully generalize to high-dimensional, multimodal distributions typical of real-world data
- The paper doesn't clearly delineate practical thresholds where model dementia becomes problematic in real-world applications, creating a gap between theory and practice
- The analysis focuses on specific model architectures and doesn't fully explore how architectural choices might mitigate or exacerbate the model dementia phenomenon

## Confidence

**High Confidence Claims**:
- The fundamental mechanism of model dementia through recursive training on generated data is well-supported by both theoretical analysis and empirical validation
- The role of statistical approximation error in losing rare events is clearly demonstrated and theoretically grounded
- The convergence to simpler distributions with reduced variance is consistently observed across multiple model types

**Medium Confidence Claims**:
- The specific mathematical bounds on Wasserstein distance divergence require additional validation across broader distribution families
- The superlinear data growth requirement for prevention is theoretically sound but needs empirical verification at scale
- The practical thresholds for when model dementia becomes problematic in real-world applications need further investigation

**Low Confidence Claims**:
- The exact architectural factors that influence susceptibility to model dementia across different model families
- The potential for architectural modifications or training procedures to mitigate the effects
- The long-term implications for model training practices and data curation strategies

## Next Checks

1. **Scale-Invariant Validation**: Test the model dementia phenomenon across multiple orders of magnitude in data size and model capacity to verify the superlinear growth requirement. This would involve systematic experiments scaling both training data and model size while tracking distribution divergence.

2. **Architecture Sensitivity Analysis**: Compare the rate of model dementia across different model architectures (RNNs, transformers, diffusion models) trained on the same recursive data pipeline. This would identify architectural factors that influence susceptibility and potential mitigation strategies.

3. **Hybrid Training Protocol Validation**: Design and test hybrid training protocols that mix genuine human-generated data with model-generated data in varying ratios. Determine the minimum fraction of genuine data needed to prevent model dementia while maintaining practical training efficiency.