---
ver: rpa2
title: Towards Measuring Representational Similarity of Large Language Models
arxiv_id: '2312.02730'
source_url: https://arxiv.org/abs/2312.02730
tags:
- similarity
- representations
- representational
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies representational similarity of 7B-parameter
  large language models (LLMs) to understand how similar their internal representations
  are across different tasks. It applies several similarity measures that respect
  key invariances (orthogonal transformations, isotropic scaling, translation) to
  the last-layer representations of multiple LLMs on two tasks: commonsense reasoning
  (Winogrande) and code generation (HumanEval).'
---

# Towards Measuring Representational Similarity of Large Language Models

## Quick Facts
- arXiv ID: 2312.02730
- Source URL: https://arxiv.org/abs/2312.02730
- Reference count: 18
- One-line primary result: Applying representational similarity measures to 7B-parameter LLMs reveals non-uniform similarities across tasks and measures, suggesting representations are not universal.

## Executive Summary
This paper investigates the representational similarity of 7B-parameter large language models across two tasks: commonsense reasoning (Winogrande) and code generation (HumanEval). Using last-layer representations and multiple similarity measures invariant to orthogonal transformations, isotropic scaling, and translation, the study finds that some models are consistently more dissimilar to others. The similarity patterns differ across tasks and measures, indicating that no single measure fully captures representational alignment. The results suggest that model representations are task-dependent and not universally similar, highlighting challenges in interpreting similarity scores and calling for careful use of representational similarity measures in LLM analysis.

## Method Summary
The study applies representational similarity measures to the last-layer representations of 11 base 7B-parameter LLMs on two benchmark tasks. Representations are extracted for the final token of each input sequence using zero-shot prompting. Multiple similarity measures—Orthogonal Procrustes, Aligned Cosine Similarity, Norm RSM-Diff, and Jaccard—are computed under different invariances (orthogonal transformations, isotropic scaling, translation). The resulting similarity matrices are analyzed to compare model pairs, with heatmaps and statistical summaries generated to assess consistency and divergence across measures and tasks.

## Key Results
- Some models (e.g., Falcon) show consistent dissimilarity across tasks and measures.
- Different similarity measures produce divergent rankings, indicating no single measure captures full representational alignment.
- Representational similarity patterns are task-dependent, with models showing different similarity rankings on Winogrande versus HumanEval.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representational similarity measures reveal non-uniform differences between LLMs, with some models showing consistent dissimilarity across tasks.
- **Mechanism:** By applying measures invariant to orthogonal transformations, isotropic scaling, and translation, the study isolates structural alignment of representations independent of basis or scaling artifacts. Models with low similarity scores across multiple measures and tasks are inferred to have fundamentally different internal representations.
- **Core assumption:** Last-layer representations before the classifier are sufficiently rich to capture model-level functional divergence, and task-specific prompts produce deterministic representations.
- **Evidence anchors:**
  - [abstract]: "some models are significantly more dissimilar to others"
  - [section]: "Falcon stands out for a relatively low similarity by Orthogonal Procrustes and Jaccard similarity"
  - [corpus]: Weak - no direct corpus mention of this mechanism, but related work discusses pointwise representational similarity.
- **Break Condition:** If tokenization differences produce non-aligned final token representations, or if stochastic generation is not properly controlled, similarity measures may reflect artifacts rather than true representational divergence.

### Mechanism 2
- **Claim:** Different representational similarity measures produce divergent rankings, indicating no single measure captures the full story.
- **Mechanism:** Measures like Orthogonal Procrustes and Aligned Cosine Similarity align representations before comparison, while Norm RSM-Diff and Jaccard operate on pairwise similarity structures. These different operational principles expose different facets of representational alignment, leading to inconsistent similarity patterns across measures.
- **Core assumption:** Measures sharing the same invariance set (OT, IS, TR) still capture fundamentally different aspects of representation geometry.
- **Evidence anchors:**
  - [abstract]: "Some measures show notable discrepancies in rankings, indicating that no single measure fully captures representational similarity"
  - [section]: "Falcon on Winogrande... stands out as dissimilar only with two of the four similarity measures"
  - [corpus]: Related work (e.g., Ding et al., 2021) notes similar issues between CKA and Orthogonal Procrustes, but this study extends it to a broader set.
- **Break Condition:** If measures are dominated by a few high-variance dimensions, they may all converge on the same ranking despite different operational definitions.

### Mechanism 3
- **Claim:** Representational similarity is task-dependent, with models showing different similarity patterns across datasets.
- **Mechanism:** Models trained on different data distributions develop representations specialized for their training domains. When evaluated on Winogrande (commonsense reasoning) versus HumanEval (code generation), the relative similarity rankings shift, reflecting data-driven representational specialization.
- **Core assumption:** Training data composition strongly influences representation geometry, and similarity measures capture this specialization.
- **Evidence anchors:**
  - [abstract]: "the pattern of similarity differs across tasks"
  - [section]: "GPT-J has a similar Orthogonal Procrustes score to the other models on Winogrande, it stands out as dissimilar on HumanEval"
  - [corpus]: Weak - no direct corpus mention, but general literature on data-dependent generalization supports this.
- **Break Condition:** If representations are largely domain-agnostic or if the datasets are too small to reveal true specialization, task-dependent patterns may not emerge.

## Foundational Learning

- **Concept:** Invariances in similarity measures (orthogonal transformations, isotropic scaling, translation)
  - **Why needed here:** LLMs can represent the same information with rotated, scaled, or shifted representations; measures must account for these symmetries to compare true representational alignment.
  - **Quick check question:** If two representations differ only by a rotation, what should a similarity measure invariant to orthogonal transformations report?
- **Concept:** Representational similarity matrices (RSMs) and their role in similarity computation
  - **Why needed here:** RSMs capture pairwise similarities within a representation, enabling comparison of internal representational geometry rather than direct vector alignment.
  - **Quick check question:** What does the Frobenius norm of the difference between two RSMs measure?
- **Concept:** Limitations of zero-shot prompting and tokenization alignment
  - **Why needed here:** LLMs produce different tokenization sequences for the same text, so comparing representations requires fixing to deterministic final tokens to ensure valid alignment.
  - **Quick check question:** Why does the study restrict comparison to the final token's representation in the last layer?

## Architecture Onboarding

- **Component map:**
  - Data loader: Winogrande and HumanEval datasets with fixed prompt templates
  - Model loader: 11 base LLMs (7B params) from HuggingFace, plus code-specific variants
  - Representation extractor: Forward pass to obtain last-layer representations for final token
  - Preprocessing: Centering, normalization, zero-padding for dimension alignment
  - Similarity engine: Multiple measures (Orthogonal Procrustes, Aligned Cosine, Norm RSM-Diff, Jaccard, RSA, CKA)
  - Analysis pipeline: Heatmap generation, Spearman correlation between measures, task comparison

- **Critical path:**
  1. Load model and dataset
  2. Generate representations for all inputs
  3. Preprocess representations for desired invariances
  4. Compute pairwise similarity scores between all model pairs
  5. Aggregate into heatmaps and statistical summaries

- **Design tradeoffs:**
  - Using only final token representations simplifies alignment but loses fine-grained sequence-level information
  - Applying multiple measures increases robustness but adds computational cost and interpretive complexity
  - Zero-shot prompting avoids label dependence but may not fully probe task-relevant representations

- **Failure signatures:**
  - Uniform similarity scores across all pairs → preprocessing may have collapsed representations
  - Extreme dissimilarity for all pairs → representations may be too sparse or improperly aligned
  - Inconsistent patterns across measures with same invariances → implementation bugs or data issues

- **First 3 experiments:**
  1. Verify preprocessing correctly centers and normalizes representations by checking mean and norm statistics
  2. Test similarity measures on synthetic data with known transformations to confirm invariance properties
  3. Run similarity computation on a small subset of models and datasets to validate pipeline output format and scaling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do representational similarity measures scale when applied to larger models or datasets beyond the 7B parameter models and limited datasets used in this study?
- **Basis in paper:** [inferred] The paper notes that the datasets used are limited in size, which may allow overfitting of alignment-based measures like Orthogonal Procrustes and Aligned Cosine Similarity.
- **Why unresolved:** The study's limited dataset size may not accurately reflect the measures' performance on larger datasets or models, potentially leading to overestimated scores.
- **What evidence would resolve it:** Conducting experiments with larger datasets or models to compare the consistency of similarity scores across different scales would provide insight into the scalability and reliability of these measures.

### Open Question 2
- **Question:** How does the choice of prompting format influence the representational similarity scores across different LLMs?
- **Basis in paper:** [explicit] The paper acknowledges that the specific prompting format used may influence model similarity, citing prior work that demonstrates the importance of prompts for model outputs.
- **Why unresolved:** The study uses a specific prompting format, and it is unclear how different formats might alter the similarity patterns observed.
- **What evidence would resolve it:** Experimenting with various prompting styles and comparing the resulting similarity scores would reveal the impact of prompt design on representational similarity.

### Open Question 3
- **Question:** Are there inherent differences in the representational similarity patterns between models trained on diverse datasets versus those with similar training data?
- **Basis in paper:** [inferred] The paper discusses how similarity is application-dependent and suggests that models trained on different amounts of code may show varying similarity patterns.
- **Why unresolved:** The study highlights that task-specific factors influence similarity, but does not explore how the diversity of training data affects representational similarity across models.
- **What evidence would resolve it:** Analyzing models with known differences in training data composition and observing how their representational similarity scores vary would clarify the influence of training data diversity on model similarity.

## Limitations

- The study relies on last-layer final token representations, potentially missing task-relevant structure distributed across earlier layers or tokens.
- Small sample sizes relative to representation dimensionality may lead to statistical overfitting in alignment-based similarity measures.
- Tokenization differences across models, even when restricted to final tokens, may introduce alignment artifacts not fully controlled for.

## Confidence

- **High Confidence:** The observation that similarity patterns differ across tasks and measures is robust, supported by consistent qualitative patterns across multiple comparisons and aligned with established literature on data-dependent generalization.
- **Medium Confidence:** The claim that no single measure captures full representational similarity is supported by divergent rankings, but the practical implications for model selection or interpretation remain unclear without additional validation on held-out tasks.
- **Low Confidence:** The inference that some models are "fundamentally more dissimilar" based on low similarity scores across measures is tentative, as the measures may be capturing artifacts of preprocessing or tokenization rather than true representational divergence.

## Next Checks

1. **Layer-wise analysis:** Compute similarity measures across multiple layers (not just last) to determine if representational divergence is consistent or layer-dependent.
2. **Synthetic control:** Generate synthetic representations with known transformations (rotations, scalings, translations) to validate that each measure correctly identifies invariance properties and detects structural differences.
3. **Sample size sensitivity:** Systematically vary the number of input samples and measure the stability of similarity scores to assess statistical robustness and potential overfitting.