---
ver: rpa2
title: 'IRFL: Image Recognition of Figurative Language'
arxiv_id: '2303.15445'
source_url: https://arxiv.org/abs/2303.15445
tags:
- images
- gurative
- literal
- language
- figurative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IRFL, a multimodal dataset for understanding
  figurative language (metaphors, similes, idioms) in images. It develops an automatic
  pipeline to find figurative and literal images for idioms, and collects metaphors
  and similes images through manual search.
---

# IRFL: Image Recognition of Figurative Language

## Quick Facts
- arXiv ID: 2303.15445
- Source URL: https://arxiv.org/abs/2303.15445
- Reference count: 26
- The paper introduces IRFL, a multimodal dataset for understanding figurative language (metaphors, similes, idioms) in images.

## Executive Summary
IRFL is a multimodal dataset designed to evaluate vision-and-language models' ability to understand figurative language in images. It includes idioms, metaphors, and similes paired with figurative and literal images, annotated for their relationship to the phrases. The dataset introduces two novel tasks to benchmark models' understanding of figurative language, revealing that state-of-the-art models perform substantially worse than humans, often defaulting to literal interpretations over figurative ones.

## Method Summary
The IRFL dataset is constructed using an automatic pipeline that extracts idioms from the MAGPIE corpus, scrapes definitions from Wiktionary/Oxford, constructs search queries, and retrieves images from Google Images. OCR filters and literal thresholds are applied to rank candidates. Metaphors and similes are collected manually. Human annotators label images using a priority-ordered category system, with majority voting ensuring consistency. Models are evaluated on two tasks: a "mixed" understanding task and a preference task, using zero-shot and fine-tuned approaches.

## Key Results
- State-of-the-art vision-and-language models perform substantially worse than humans on IRFL tasks.
- Models often default to partially literal images over truly figurative ones, indicating a literal bias.
- Fine-tuning improves model performance on figurative language tasks but does not close the gap with human understanding.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** IRFL uses a pipeline that combines textual definitions with visual search to automatically pair figurative and literal images for idioms, reducing manual annotation workload.
- **Mechanism:** For each idiom, the system extracts figurative and idiomatic definitions from Wiktionary/Oxford, parses them into search queries, runs Google image searches, filters out OCR-heavy or text-heavy images, and ranks candidates by matching scores. This yields sets of "figurative" and "literal" candidates for annotation.
- **Core assumption:** Google image search results for an idiom's definitions will contain both literal depictions of the words and figurative visual interpretations.
- **Evidence anchors:**
  - [section] "To collect figurative and literal images for idioms, we developed an automatic pipeline that takes a list of idioms as input and outputs figurative and literal candidate images."
  - [section] "To find figurative images for our search queries, we searched Google images taking up to 20 images per search query...We calculated the matching score of each image with its phrase and search query."
  - [corpus] Weak: no direct corpus mention of pipeline output quality, but neighbor papers discuss similar retrieval tasks.
- **Break condition:** If search queries yield mostly literal or irrelevant images, the candidate pool becomes too noisy for effective annotation.

### Mechanism 2
- **Claim:** Human annotation with prioritized relation categories ensures consistent labeling of image-idiom relationships, enabling a multi-label dataset that captures figurative, literal, and partial matches.
- **Mechanism:** Annotators choose from six categories (Figurative, Literal, Caption, Partial Objects, None) in priority order, guided by a decision tree. Majority voting across five annotators per image determines the final label, with inter-annotator agreement measured.
- **Core assumption:** The priority ordering of categories reduces ambiguity and enforces consistent labeling across annotators.
- **Evidence anchors:**
  - [section] "We hired Amazon Mechanical Turk workers to annotate the relation between each idiom and its candidate images...We split the annotation process into batches..."
  - [section] "We provide further discussion about this aspect of the task in (Appendix A.4). Despite the subjective aspect of the task and its complexity in distinguishing between the various categories, in 94% of the instances, there was a majority of 3 workers or more..."
  - [corpus] Moderate: neighbor papers on multimodal annotation support prioritization for consistency.
- **Break condition:** If annotators disagree widely, the dataset loses reliability for benchmarking model understanding.

### Mechanism 3
- **Claim:** Zero-shot vision-and-language models perform poorly on IRFL tasks because they default to literal associations over figurative ones, revealing a gap in multimodal figurative reasoning.
- **Mechanism:** Models encode phrase-image pairs and select the image with highest matching score. The tasks reveal models often pick partially literal distractors over truly figurative images, indicating weak abstraction of metaphor/simile/idiom mappings.
- **Core assumption:** Poor performance is due to literal bias rather than lack of semantic similarity between phrase and figurative image.
- **Evidence anchors:**
  - [section] "We conduct a fine-grained analysis to examine if models failed the 'mixed' understanding task because they do not see any connection to the figurative images or rather because they prioritize 'weak' literal connections over figurative ones."
  - [section] "We analyzed the models' choices...in all models...a partially literal distractor was selected in 92% - 100% of the instances where the models failed..."
  - [corpus] Weak: no corpus evidence directly comparing literal vs figurative matching scores.
- **Break condition:** If models achieve high accuracy by chance or through overfitting, the conclusion about literal bias is invalid.

## Foundational Learning

- **Concept:** Figurative language types (metaphor, simile, idiom) and their visual representations.
  - Why needed here: IRFL's tasks require distinguishing literal from figurative visual matches for each figure type.
  - Quick check question: Can you give an example of a literal vs figurative image for the idiom "raining cats and dogs"?

- **Concept:** Vision-and-language model matching scores and how they reflect semantic alignment.
  - Why needed here: Understanding why models choose certain images reveals whether they grasp figurative intent.
  - Quick check question: If a model scores an image of a cat higher for "raining cats and dogs" than an image of heavy rain, what does that say about its understanding?

- **Concept:** Inter-annotator agreement metrics and their interpretation.
  - Why needed here: Evaluating dataset quality depends on measuring how consistently humans label figurative vs literal images.
  - Quick check question: What does 94% majority agreement across five annotators indicate about the task's subjectivity?

## Architecture Onboarding

- **Component map:** Idiom extraction (MAGPIE corpus) -> Definition scraping (Wiktionary/Oxford) -> Search query construction -> Image retrieval (Google Images API) -> OCR filtering and literal threshold scoring -> Candidate ranking and selection -> Human annotation (AMT UI) -> Dataset -> Task creation -> Model benchmarking
- **Critical path:** Idiom → Definition → Query → Images → Filter/Score → Annotate → Dataset → Task creation → Model benchmarking
- **Design tradeoffs:** Automated pipeline reduces cost but introduces noise; human annotation ensures quality but is slower; multi-label categories increase dataset richness but require careful UI design.
- **Failure signatures:** Low inter-annotator agreement, candidate images lacking figurative relevance, model scores correlating more with literal content than semantic match.
- **First 3 experiments:**
  1. Run the pipeline on a small idiom subset, inspect candidate images manually, adjust OCR thresholds.
  2. Annotate a batch of candidates, compute agreement statistics, refine UI or instructions if needed.
  3. Evaluate CLIP zero-shot on the small dataset, analyze misclassifications to confirm literal bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different vision-language model architectures (e.g., transformer-based vs. CNN-based) compare in their ability to understand multimodal figurative language?
- Basis in paper: [inferred] The paper compares several state-of-the-art models (CLIP, CLIP-ViL, ViLT) but does not explicitly compare different architectural approaches or analyze why certain architectures might perform better or worse.
- Why unresolved: The paper focuses on evaluating existing models but doesn't investigate the underlying reasons for architectural differences in figurative language understanding.
- What evidence would resolve it: A systematic comparison of models with different architectural backbones (e.g., CNN-based, transformer-based) trained on the same figurative language tasks, with analysis of attention patterns or feature representations that correlate with understanding performance.

### Open Question 2
- Question: Can fine-tuning vision-language models on figurative language data improve their performance on other multimodal reasoning tasks that require abstract thinking?
- Basis in paper: [inferred] The paper shows that fine-tuning improves figurative language understanding but doesn't explore whether this generalizes to other abstract reasoning tasks.
- Why unresolved: The paper focuses on task-specific fine-tuning effects without investigating broader transfer learning implications.
- What evidence would resolve it: Experiments fine-tuning models on figurative language tasks and then evaluating performance on diverse multimodal reasoning benchmarks (e.g., visual question answering, visual reasoning) compared to models fine-tuned on literal language data.

### Open Question 3
- Question: What specific aspects of figurative language (e.g., cultural knowledge, abstract mappings, emotional connotations) pose the greatest challenges for vision-language models?
- Basis in paper: [explicit] The paper notes that idioms require "profound language and cultural knowledge" while metaphors/similes require "abstraction" and "commonsense," but doesn't systematically analyze which aspects are most problematic.
- Why unresolved: The paper provides a general framework but doesn't conduct detailed error analysis to identify specific failure modes.
- What evidence would resolve it: Detailed error analysis categorizing model failures by type of figurative language challenge (cultural references, abstract mappings, emotional cues, etc.) with statistical analysis of which categories most frequently cause errors.

## Limitations

- The automatic pipeline's reliance on Google image search results may introduce noise if queries yield irrelevant images.
- Human annotation's subjective nature could lead to inconsistencies despite high inter-annotator agreement.
- The dataset's focus on English idioms, metaphors, and similes limits generalizability to other languages or figurative forms.

## Confidence

- **High Confidence:** The dataset's construction process, including the use of human annotation and the introduction of novel tasks, is well-documented and reproducible. The observation that models struggle with figurative understanding is supported by experimental results.
- **Medium Confidence:** The automatic pipeline's ability to retrieve relevant figurative and literal images is plausible but depends on the quality of search results and filtering heuristics, which are not fully detailed.
- **Low Confidence:** The claim that models' poor performance is solely due to literal bias is not definitively proven, as other factors like semantic similarity or task complexity could contribute.

## Next Checks

1. **Pipeline Output Quality:** Manually inspect a sample of candidate images from the automatic pipeline to assess the relevance and quality of figurative and literal matches for a subset of idioms.
2. **Annotation Consistency:** Analyze inter-annotator agreement across different relation categories to identify potential ambiguities or inconsistencies in the labeling process.
3. **Model Behavior Analysis:** Conduct a deeper analysis of model misclassifications to determine whether literal bias or other factors (e.g., semantic similarity) drive poor performance on figurative understanding tasks.