---
ver: rpa2
title: 'Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven
  Optimization'
arxiv_id: '2310.12103'
source_url: https://arxiv.org/abs/2310.12103
tags:
- diversity
- human
- qdhf
- metrics
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Quality Diversity through Human Feedback
  (QDHF), a method that leverages human feedback to learn diversity metrics for quality
  diversity (QD) algorithms. QDHF employs contrastive learning to align latent projections
  with human notions of similarity, enabling QD algorithms to optimize for diverse
  and high-quality solutions without relying on manually crafted diversity metrics.
---

# Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization

## Quick Facts
- arXiv ID: 2310.12103
- Source URL: https://arxiv.org/abs/2310.12103
- Reference count: 16
- Key outcome: Introduces QDHF, a method using human feedback and contrastive learning to discover diverse, high-quality solutions in quality diversity optimization, outperforming existing methods.

## Executive Summary
This paper presents Quality Diversity through Human Feedback (QDHF), a novel approach that leverages human feedback to learn diversity metrics for quality diversity algorithms. By employing contrastive learning to align latent projections with human notions of similarity, QDHF enables optimization of diverse and high-quality solutions without relying on manually crafted diversity metrics. The method is evaluated on benchmark tasks in robotics, reinforcement learning, and computer vision, demonstrating significant improvements in diversity discovery and search capabilities compared to existing methods.

## Method Summary
QDHF learns diversity metrics from human feedback using contrastive learning with triplet loss to align latent projections with human notions of similarity. The method involves feature extraction and dimensionality reduction to transform descriptive data into a semantically meaningful latent space. Human judgments on solution similarity are collected through a Two Alternative Forced Choice (2AFC) mechanism. The learned diversity metrics are then integrated into quality diversity algorithms like MAP-Elites to discover diverse, high-quality solutions. QDHF employs an active learning approach with exponential decay in feedback frequency to balance annotation efficiency and model robustness.

## Key Results
- QDHF significantly outperforms state-of-the-art methods in automatic diversity discovery
- Matches the efficacy of quality diversity with manually crafted diversity metrics
- Enhances diversity of images generated by diffusion models in latent space illumination tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns latent projections with human notions of similarity, enabling QDHF to discover diverse solutions that reflect human preferences.
- Mechanism: Minimizes distances between similar solutions and maximizes distances between dissimilar ones in the latent space using triplet loss.
- Core assumption: Human judgments of similarity can be effectively captured through triplet comparisons and encoded into a continuous latent space.
- Break condition: If human similarity judgments are inconsistent or noisy, the learned latent space may not reflect meaningful diversity.

### Mechanism 2
- Claim: Quality diversity algorithms with learned diversity metrics from human feedback can match or exceed the performance of algorithms using manually crafted diversity metrics.
- Mechanism: QDHF learns diversity metrics more aligned with human intuition than predefined metrics, allowing more effective exploration of the search space.
- Core assumption: Human-derived diversity metrics are at least as effective as expert-designed metrics for guiding quality diversity optimization.
- Break condition: If human feedback is limited or biased, the learned metrics may underperform compared to carefully engineered diversity measures.

### Mechanism 3
- Claim: Active learning with exponential decay in feedback frequency allows QDHF to efficiently scale to complex tasks without overwhelming human annotators.
- Mechanism: Initial rapid learning from frequent human feedback is followed by slower refinement as diversity metrics become more robust.
- Core assumption: Early diversity metrics learned from human feedback provide sufficient guidance for effective optimization, with later refinements yielding diminishing returns.
- Break condition: If the exponential decay schedule is too aggressive, the model may stop improving before reaching optimal diversity metrics.

## Foundational Learning

- Concept: Contrastive learning and triplet loss
  - Why needed here: Forms the core mechanism for aligning latent projections with human notions of similarity
  - Quick check question: How does triplet loss encourage the model to separate dissimilar solutions while keeping similar ones close?

- Concept: Quality diversity algorithms and MAP-Elites
  - Why needed here: Provides the optimization framework that uses learned diversity metrics to discover diverse, high-quality solutions
  - Quick check question: How does discretizing the measurement space into cells enable practical implementation of quality diversity objectives?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: Provides the conceptual foundation for learning reward/diversity models from human preferences rather than predefined metrics
  - Quick check question: What distinguishes QDHF's use of human feedback from traditional RLHF approaches?

## Architecture Onboarding

- Component map: Feature extractor → Dimensionality reduction (latent projection) → Contrastive learning module → MAP-Elites optimizer → Archive
- Critical path: Human feedback → Contrastive learning → Latent projection update → Solution evaluation → Archive update
- Design tradeoffs: Online vs offline learning balances annotation efficiency against model stability; simpler vs more complex feature extractors trade-off expressiveness against generalization
- Failure signatures: Poor coverage in archive suggests misalignment between learned and ground truth diversity; low QD score indicates ineffective exploration despite diversity
- First 3 experiments:
  1. Verify contrastive learning correctly orders triplets according to human judgments using a small validation set
  2. Test QDHF on a simple robotic arm task with ground truth diversity to establish baseline performance
  3. Compare online vs offline learning strategies on a mid-complexity task to determine optimal feedback schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample efficiency of QDHF compare to other semi-supervised learning methods when scaling to more complex tasks?
- Basis in paper: The paper states "determining the necessary sample size of human feedback for optimal performance remains a critical question" and provides analysis on robotic arm task with varying sample sizes.
- Why unresolved: The paper only analyzes sample efficiency on the robotic arm task, not on more complex tasks or compared to other semi-supervised methods.
- What evidence would resolve it: Comparative studies of QDHF's sample efficiency on various complex tasks against other semi-supervised learning methods.

### Open Question 2
- Question: Can QDHF effectively handle continuous diversity metrics in high-dimensional spaces without discretization?
- Basis in paper: The paper discusses discretizing the measurement space into an archive of cells for QD algorithms, but does not explore continuous diversity metrics in high-dimensional spaces.
- Why unresolved: The paper does not explore the potential of QDHF to handle continuous diversity metrics in high-dimensional spaces without discretization.
- What evidence would resolve it: Experiments demonstrating QDHF's performance on tasks with continuous diversity metrics in high-dimensional spaces without discretization.

### Open Question 3
- Question: How does the quality of diversity metrics learned by QDHF compare to manually crafted diversity metrics in terms of optimizing diverse and high-quality solutions?
- Basis in paper: The paper states that QDHF's performance is closely tied to the accuracy of latent projection in reflecting human judgment and that the quality of learned diversity metrics is a key aspect of QDHF's effectiveness.
- Why unresolved: The paper does not provide a direct comparison of the quality of diversity metrics learned by QDHF to manually crafted diversity metrics in terms of optimizing diverse and high-quality solutions.
- What evidence would resolve it: Comparative studies of the quality of diversity metrics learned by QDHF versus manually crafted diversity metrics in terms of optimizing diverse and high-quality solutions.

## Limitations
- Reliance on human feedback introduces scalability challenges and potential bias
- Quality of learned metrics directly depends on representativeness and consistency of human judgments
- Exponential decay schedule for feedback frequency lacks empirical validation for different task complexities

## Confidence
- **Mechanism 1 (Contrastive Learning)**: Medium confidence - The theoretical foundation is sound, but the paper lacks ablation studies isolating the impact of contrastive learning from other components.
- **Mechanism 2 (Human vs Manual Metrics)**: High confidence - Demonstrated through quantitative comparisons showing QDHF matching or exceeding performance of manually crafted metrics across multiple benchmark tasks.
- **Mechanism 3 (Active Learning Schedule)**: Low confidence - The exponential decay approach is proposed but not thoroughly validated; optimal decay rates for different problem domains remain unexplored.

## Next Checks
1. Conduct ablation studies removing the contrastive learning component to isolate its contribution to diversity discovery performance.
2. Test QDHF with varying feedback decay schedules (linear, quadratic, logarithmic) to determine optimal annotation efficiency across different task complexities.
3. Evaluate QDHF's performance when human feedback contains intentional bias or inconsistency to assess robustness to noisy annotations.