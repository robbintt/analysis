---
ver: rpa2
title: Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated
  Deepfake Texts?
arxiv_id: '2304.01002'
source_url: https://arxiv.org/abs/2304.01002
tags:
- deepfake
- texts
- participants
- human
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates human factors in detecting deepfake texts,
  comparing non-experts (AMT) and experts (Upwork). A novel task is introduced: identifying
  the machine-generated paragraph in a three-paragraph article with mixed authorship.'
---

# Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?

## Quick Facts
- **arXiv ID**: 2304.01002
- **Source URL**: https://arxiv.org/abs/2304.01002
- **Reference count**: 40
- **Primary result**: Experts significantly outperform non-experts in deepfake text detection, with synchronous collaboration further enhancing expert performance

## Executive Summary
This study investigates human factors in detecting deepfake texts by comparing non-experts (AMT) and experts (Upwork) in identifying machine-generated paragraphs within mixed-authorship articles. A novel task is introduced: participants must identify which of three paragraphs was generated by a large language model. Results show experts significantly outperform non-experts, with collaboration further enhancing detection accuracy for experts through synchronous information exchange. Non-experts show minimal improvement with collaboration, relying more on low-level errors. The findings highlight the importance of expertise and collaboration in detecting deepfake texts, offering insights for future tool and framework designs.

## Method Summary
The study used 50 articles, each containing three paragraphs (two human-written, one GPT-2-generated). Participants from Amazon Mechanical Turk (non-experts) and Upwork (experts) were tasked with identifying the machine-generated paragraph and providing justifications. Two justification formats were tested: select (predefined error types) and write (open-ended). The study compared individual versus collaborative performance across both groups, analyzing accuracy and justification patterns.

## Key Results
- Experts significantly outperformed non-experts in detecting deepfake texts (p < 0.05)
- Synchronous collaboration enhanced expert performance through real-time exchange of linguistic insights
- Non-experts showed minimal improvement with collaboration, relying primarily on low-level errors
- Hybrid-level justifications (combining form and content errors) were most effective for accurate detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experts improve deepfake detection accuracy significantly when collaborating synchronously.
- Mechanism: Synchronous collaboration enables real-time exchange of linguistic insights and shared deductive reasoning strategies that leverage advanced writing expertise.
- Core assumption: Experts possess superior linguistic skills that allow them to detect subtle coherence and consistency errors more effectively than non-experts.
- Evidence anchors:
  - [abstract] "Experts' performance benefits from synchronous collaboration, focusing on coherence, logical fallacies, and self-contradiction errors."
  - [section] "Three categories ('grammar', 'common sense' and 'coherence') had the most increase in use from Individual to Collaboration, but only a 9.11% increase for grammar error type and a 6.23% increase for common sense was found to be significant (p = 0.016, p = 0.036)."
  - [corpus] Weak - corpus does not contain direct evidence for synchronous collaboration benefits.
- Break condition: If participants lack sufficient writing expertise, synchronous collaboration will not yield significant accuracy gains.

### Mechanism 2
- Claim: Non-experts show minimal improvement with collaboration because they rely more on low-level errors.
- Mechanism: Non-experts lack the linguistic sophistication to engage in high-level reasoning about coherence and logical structure, limiting their collaborative gains.
- Core assumption: Low-level errors (e.g., grammar, repetition) are easier to detect individually but do not significantly improve through collaboration.
- Evidence anchors:
  - [abstract] "Non-experts show minimal improvement with collaboration, relying more on low-level errors."
  - [section] "Non-experts' usage of three error levels did not differ between individual and collaborative scenario."
  - [corpus] Weak - corpus lacks direct evidence about error-level reliance differences.
- Break condition: If non-experts receive targeted training on high-level error detection, collaboration benefits may emerge.

### Mechanism 3
- Claim: Hybrid-level justifications (combining form and content errors) are most effective for accurate detection.
- Mechanism: Combining low-level and high-level error detection provides a more comprehensive analysis of text authenticity.
- Core assumption: Effective deepfake detection requires attention to both grammatical form and semantic coherence.
- Evidence anchors:
  - [abstract] "Experts' performance benefits from synchronous collaboration, focusing on coherence, logical fallacies, and self-contradiction errors."
  - [section] "The prevalence of hybrid-level errors, on the other hand, is more than doubled for correct responses between individual and collaborative settings of experts."
  - [corpus] Weak - corpus does not provide evidence for hybrid-level effectiveness.
- Break condition: If either low-level or high-level error detection becomes unreliable, hybrid approaches will lose effectiveness.

## Foundational Learning

- Concept: Natural Language Generation (NLG) and large language models
  - Why needed here: Understanding how deepfake texts are generated is crucial for developing effective detection strategies.
  - Quick check question: What distinguishes NLG-generated text from human-written text in terms of coherence and logical consistency?

- Concept: Human factors in collaborative problem-solving
  - Why needed here: The study investigates how collaboration affects detection performance, requiring understanding of group dynamics.
  - Quick check question: How does synchronous collaboration differ from asynchronous collaboration in terms of information exchange and decision-making?

- Concept: Error categorization in text analysis
  - Why needed here: The study uses multiple error categories to evaluate detection effectiveness.
  - Quick check question: What distinguishes low-level errors (grammar, repetition) from high-level errors (coherence, logical fallacies)?

## Architecture Onboarding

- Component map:
  Data generation pipeline -> Participant recruitment system -> Experimental interface -> Analysis framework

- Critical path:
  1. Generate mixed-author articles
  2. Recruit and qualify participants
  3. Administer detection tasks
  4. Collect and categorize justifications
  5. Analyze performance differences

- Design tradeoffs:
  - Synchronous vs. asynchronous collaboration: Real-time discussion enables deeper analysis but is harder to scale
  - Select vs. write justification: Select is faster but may limit insight; write is richer but requires more effort
  - Expert vs. non-expert recruitment: Experts provide better insights but are more expensive and harder to scale

- Failure signatures:
  - Low accuracy across all groups: Indicates task is too difficult or data generation is flawed
  - No difference between individual and collaboration: Suggests collaboration format is ineffective
  - Non-experts outperforming experts: Suggests task design favors low-level error detection

- First 3 experiments:
  1. Test data generation pipeline with a small sample to verify GPT-2 paragraphs are convincingly human-like
  2. Run pilot with 5-10 participants from each group to validate interface and task clarity
  3. Conduct analysis on pilot data to refine justification categorization scheme

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of human collaboration in detecting deepfake texts change when the texts are generated by newer language models like GPT-3 or GPT-4 compared to GPT-2?
- Basis in paper: [explicit] The paper notes that newer NTGs (Neural Text Generators) are harder to detect, which can sometimes make older detectors obsolete.
- Why unresolved: The study primarily used GPT-2 for generating deepfake texts. It does not provide data on how well humans perform with texts generated by more advanced models.
- What evidence would resolve it: Experiments using deepfake texts generated by newer models like GPT-3 or GPT-4 and comparing the detection accuracy of humans in both individual and collaborative settings.

### Open Question 2
- Question: What is the impact of different types of asynchronous collaboration on the detection of deepfake texts?
- Basis in paper: [inferred] The study compared synchronous collaboration (Upwork) with asynchronous collaboration (AMT) and found that synchronous collaboration was more effective for experts.
- Why unresolved: The study does not explore various methods of asynchronous collaboration, such as structured communication tools or time-delayed discussions.
- What evidence would resolve it: Experiments testing multiple forms of asynchronous collaboration, such as structured forums or delayed feedback systems, and measuring their impact on detection accuracy.

### Open Question 3
- Question: How does the inclusion of more complex or nuanced error types, such as cultural references or domain-specific knowledge, affect the detection of deepfake texts?
- Basis in paper: [inferred] The study used seven pre-defined error types but did not explore more complex or domain-specific errors.
- Why unresolved: The study focused on general error types like grammar and coherence but did not delve into errors that require deeper cultural or domain-specific knowledge.
- What evidence would resolve it: Experiments introducing more complex error types, such as cultural references or specialized terminology, and evaluating whether humans can detect deepfake texts more effectively with these additional cues.

## Limitations

- Expert sample size (n=24) is substantially smaller than non-expert group (n=40), potentially limiting statistical power
- GPT-2 XL model used for generation is less sophisticated than current models, potentially underestimating modern detection challenges
- Study focuses on news articles, limiting generalizability to other text genres like creative writing or technical documentation

## Confidence

**High Confidence:** The core finding that experts outperform non-experts in deepfake detection is well-supported by statistically significant differences in accuracy (p < 0.05) across multiple metrics.

**Medium Confidence:** The claim about synchronous collaboration benefits for experts is supported but shows mixed statistical significance. While overall accuracy improvements are clear, only 2 of 8 error categories showed significant increases in usage during collaboration.

**Low Confidence:** The assertion that non-experts show minimal improvement with collaboration is based on non-significant trends rather than clear evidence. The study reports no statistical difference in error usage between individual and collaborative settings for non-experts, but lacks power analysis to confirm this is not a Type II error.

## Next Checks

1. Replicate with larger expert sample size (n â‰¥ 50): Current n=24 limits statistical power for detecting expert-specific effects.

2. Test with contemporary LLMs (GPT-4, Claude): The use of GPT-2 XL may not reflect current deepfake text quality.

3. Conduct genre-specific validation: Test the same methodology across different text types to assess generalizability beyond news articles.