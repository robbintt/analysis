---
ver: rpa2
title: 'ZipLM: Inference-Aware Structured Pruning of Language Models'
arxiv_id: '2302.04089'
source_url: https://arxiv.org/abs/2302.04089
tags:
- pruning
- ziplm
- structured
- speedup
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZipLM, a structured pruning approach for
  language models that achieves state-of-the-art accuracy-vs-speedup while guaranteeing
  to match target runtime speedups on any given hardware. ZipLM combines three saliency
  criteria (weight magnitude, activation impact, and removing linearly-redundant structures)
  in a unified framework and adds hardware-awareness to ensure desired latency or
  throughput.
---

# ZipLM: Inference-Aware Structured Pruning of Language Models

## Quick Facts
- **arXiv ID**: 2302.04089
- **Source URL**: https://arxiv.org/abs/2302.04089
- **Reference count**: 40
- **Primary result**: State-of-the-art accuracy-vs-speedup for BERT and GPT2 compression with hardware-aware guarantees

## Executive Summary
ZipLM introduces a structured pruning framework for language models that achieves state-of-the-art accuracy-speedup tradeoffs while guaranteeing target runtime speedups on specific hardware. The method uniquely combines three saliency criteria—weight magnitude, activation impact, and redundancy removal—using Hessian-based analysis in a unified framework. ZipLM also introduces hardware-aware pruning by building latency tables for different layer configurations, ensuring pruned models meet desired speedup targets. The approach demonstrates superior performance compared to prior BERT compression techniques and matches heavily optimized MobileBERT by simply pruning BERT-large.

## Method Summary
ZipLM uses a principled one-at-a-time structured pruning algorithm that leverages Hessian-based sensitivity analysis to determine which attention heads and feed-forward layer components to remove. The method builds hardware-specific latency tables by benchmarking different sparsity configurations on the target device, then prunes structures to meet speedup constraints while maintaining accuracy. A key innovation is layer-wise token distillation that transfers knowledge across the entire model without requiring manual layer matching. The algorithm iteratively prunes, fine-tunes, and distills to produce a sequence of compressed models matching various speedup targets in a single run.

## Key Results
- Achieves 2-15× speedup on BERT-base with minimal accuracy loss on GLUE and SQuAD benchmarks
- Outperforms prior BERT compression methods (CoFi, MiniLM, TinyBERT) and matches MobileBERT performance
- Compresses GPT2 to 60% of DistilGPT2's size while being 30% faster with comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZipLM unifies three saliency criteria—weight magnitude, activation impact, and removing linearly-redundant structures—into a principled framework.
- Mechanism: Combines Hessian-based sensitivity analysis with one-at-a-time pruning to balance output quality preservation and redundancy removal.
- Core assumption: Structured compression can be modeled as sparse linear regression where Hessian captures both weight saliency and activation influence.
- Evidence anchors: [abstract] "unifies all the saliency criteria investigated by prior work"; [section 3.1] formal Hessian extension formulas; [corpus] Weak evidence—no direct support for Hessian-based structured pruning unification.
- Break condition: If Hessian becomes ill-conditioned due to aggressive pruning, saliency scores lose meaning and important structures may be pruned.

### Mechanism 2
- Claim: ZipLM achieves hardware-aware pruning by benchmarking layer shapes on target device and building latency tables.
- Mechanism: Prunes structures based on pre-built latency table mapping different head counts and intermediate sizes to actual runtimes.
- Core assumption: Runtime of transformer layer can be approximated by table lookup for fixed batch size and sequence length.
- Evidence anchors: [abstract] "matching a set of desired target runtime speedups on any given target hardware"; [section 3.2] latency recording methodology; [corpus] No direct evidence—neighbors focus on newer pruning techniques.
- Break condition: If target hardware changes, latency table becomes invalid and pruning decisions may not meet speedup targets.

### Mechanism 3
- Claim: ZipLM uses principled layer-wise token distillation that doesn't require manual layer matching.
- Mechanism: Distills intermediate token representations across all layers by minimizing Euclidean distance between corresponding hidden states.
- Core assumption: Token representations in hidden space are sufficient for transferring knowledge without explicit layer-to-layer mapping.
- Evidence anchors: [abstract] "technique for distillation of token representations across the entire model, which does not require manual layer matching"; [section 3.3] Euclidean distance loss definition; [corpus] Weak evidence—contains newer distillation approaches but not layer-agnostic token distillation.
- Break condition: If hidden dimensions differ between teacher and student, Euclidean distance becomes meaningless and distillation fails.

## Foundational Learning

- **Concept: Structured pruning**
  - Why needed here: ZipLM's effectiveness relies on removing entire subcomponents (e.g., attention heads, FC columns) rather than individual weights.
  - Quick check question: What is the difference between structured and unstructured pruning in terms of computational savings on hardware?

- **Concept: Hessian-based sensitivity analysis**
  - Why needed here: Method uses Hessian to determine which structures are least important to output, combining weight magnitude and activation influence.
  - Quick check question: How does the Hessian matrix relate to the importance of weight structures in a neural network?

- **Concept: Latency table construction**
  - Why needed here: ZipLM's hardware awareness depends on accurately predicting runtime for different layer shapes on target device.
  - Quick check question: Why is it important to build a latency table for different sparsity configurations before pruning?

## Architecture Onboarding

- **Component map**: Structured pruning algorithm (with Hessian-based saliency scoring) -> Latency table builder -> Layer-wise token distillation module
- **Critical path**: (1) Build latency table for target hardware, (2) Perform structured pruning using ZipLM algorithm with latency constraints, (3) Apply layer-wise token distillation, (4) Fine-tune pruned model
- **Design tradeoffs**: Structured pruning offers speedup guarantees but can lead to larger accuracy drops compared to unstructured pruning. One-at-a-time pruning is more accurate but slower than batch pruning. Layer-wise token distillation avoids manual mapping but may be less effective if hidden dimensions differ.
- **Failure signatures**: If speedup targets not met, latency table may be inaccurate or pruning algorithm too conservative. If accuracy drops significantly, Hessian-based saliency scoring may be removing important structures or distillation may not be transferring knowledge effectively.
- **First 3 experiments**:
  1. Build latency table for small transformer model on target hardware and verify runtime predictions match actual measurements.
  2. Apply ZipLM pruning to single layer of transformer and verify output remains close to original using Hessian-based saliency scores.
  3. Compare accuracy of ZipLM pruned model with and without layer-wise token distillation on small task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the linear scaling law of accuracy vs. speedup factor generalize across different model architectures and tasks beyond BERT and GPT?
- **Basis in paper**: [explicit] Paper observes linear relationship F1large≈ 92.1− 0.3× speeduplarge and F1base≈ 90.3− 0.6× speedupbase for BERT models on SQuADv1.1.
- **Why unresolved**: Scaling law derived from experiments on BERT models for specific tasks (SQuADv1.1 and GLUE subset). Unclear whether linear relationship holds for other architectures (RoBERTa, T5) or different NLP tasks.
- **What evidence would resolve it**: Conduct experiments applying ZipLM to various model architectures and diverse NLP tasks, then analyze whether accuracy-speedup relationship follows linear pattern across different settings.

### Open Question 2
- **Question**: What is the impact of different Hessian matrix computation methods on accuracy and efficiency of ZipLM structured pruning?
- **Basis in paper**: [inferred] ZipLM uses inverse Hessian matrix (H−1 = (2 XX⊤ +λI)−1) for saliency scoring. Paper mentions Hessian computation can be done in O(d3 col) time but optimized to O(|MS|· d2 col) using Gaussian elimination.
- **Why unresolved**: Paper describes Hessian-based approach but doesn't compare performance with alternative methods for computing saliency scores, such as Fisher information matrix or gradient-based methods.
- **What evidence would resolve it**: Compare ZipLM's pruning results with alternative methods using different saliency computation techniques (Fisher pruning, gradient-based pruning) on same model-task combinations and measure both accuracy and computational efficiency.

### Open Question 3
- **Question**: How does ZipLM's performance change when applied to multilingual or cross-lingual language models?
- **Basis in paper**: [inferred] ZipLM demonstrated on English-language models (BERT and GPT2) and tasks. Method's effectiveness on multilingual models not explored.
- **Why unresolved**: Paper focuses on monolingual English models and tasks. Unclear whether structured pruning approach generalizes to models trained on multiple languages or designed for cross-lingual transfer.
- **What evidence would resolve it**: Apply ZipLM to multilingual models like mBERT or XLM-R and evaluate performance across multiple languages and cross-lingual tasks, comparing results to monolingual pruning.

### Open Question 4
- **Question**: What are the long-term effects of ZipLM pruning on model robustness and generalization to out-of-distribution data?
- **Basis in paper**: [inferred] ZipLM evaluated on standard benchmarks shows state-of-the-art performance in accuracy vs. speedup. Paper doesn't investigate how pruning affects model robustness to adversarial examples or generalization to data outside training distribution.
- **Why unresolved**: Evaluation focuses on in-distribution performance on standard benchmarks. No analysis of how structured pruning impacts model's robustness properties or ability to generalize to unseen data distributions.
- **What evidence would resolve it**: Conduct extensive robustness testing of ZipLM-pruned models, including adversarial attack evaluations, out-of-distribution detection, and transfer learning experiments to assess how pruning affects these properties compared to unpruned models.

## Limitations
- **Hardware Dependency**: Method's effectiveness critically depends on accurate latency tables for target hardware; portability across different deployment environments is limited.
- **Hessian Sensitivity**: Hessian-based approach may become ill-conditioned with aggressive pruning, potentially leading to suboptimal pruning decisions.
- **Computational Overhead**: Iterative one-at-a-time pruning strategy and latency table construction add computational overhead compared to simpler methods.

## Confidence
- **High Confidence**: ZipLM achieves state-of-the-art accuracy-vs-speedup ratios for BERT and GPT2 compression on specific hardware configurations (supported by direct experimental comparisons).
- **Medium Confidence**: Hessian-based saliency scoring framework effectively combines weight magnitude, activation impact, and redundancy removal (theoretical framework sound but practical implementation details introduce uncertainty).
- **Medium Confidence**: Hardware-aware pruning guarantees matching target runtime speedups on any given hardware (latency table construction well-specified but accuracy depends on target hardware characteristics).

## Next Checks
1. **Hardware Generalization Test**: Validate ZipLM's speedup guarantees on different hardware platform (e.g., Intel CPU or AMD GPU) to assess portability of latency tables and hardware-aware pruning mechanism.
2. **Hessian Sensitivity Analysis**: Conduct ablation studies to quantify impact of regularization parameter λ on pruning decisions and final model accuracy; explore sensitivity to different Hessian computation approaches.
3. **Token Distillation Robustness**: Evaluate effectiveness of layer-wise token distillation when teacher and student models have different hidden dimensions; compare performance with and without explicit layer matching.