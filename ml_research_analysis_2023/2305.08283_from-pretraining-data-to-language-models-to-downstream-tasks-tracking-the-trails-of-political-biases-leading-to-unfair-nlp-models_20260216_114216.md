---
ver: rpa2
title: 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the
  Trails of Political Biases Leading to Unfair NLP Models'
arxiv_id: '2305.08283'
source_url: https://arxiv.org/abs/2305.08283
tags:
- political
- language
- social
- pretraining
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a new method to measure political biases in
  language models and evaluate their fairness on downstream tasks. It proposes using
  prompts grounded in political spectrum theories to assess language models' ideological
  positions on social and economic issues.
---

# From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models

## Quick Facts
- arXiv ID: 2305.08283
- Source URL: https://arxiv.org/abs/2305.08283
- Reference count: 40
- Primary result: Language models exhibit measurable political biases that propagate to downstream tasks, leading to unfair outcomes across identity groups and partisan media sources.

## Executive Summary
This paper develops a new method to measure political biases in language models and evaluate their fairness on downstream tasks. The authors propose using prompts grounded in political spectrum theories to assess language models' ideological positions on social and economic issues. Through experiments across 14 language models and two downstream tasks (hate speech and misinformation detection), they find that models do exhibit political leanings that correlate with their pretraining data composition. These biases lead to systematic differences in performance across identity groups and partisan media sources, raising concerns about fairness in high-stakes NLP applications.

## Method Summary
The authors develop a political compass-based probing method to measure language models' social and economic political leanings. For encoder models, they use mask-filling prompts with political statements; for decoder models, they generate completions and apply stance detection. They evaluate 14 language models (BERT variants, GPT variants, and others) on this political compass test, then fine-tune each model on hate speech and misinformation detection tasks. The study analyzes both overall performance metrics (BACC, F1) and per-category fairness across different identity groups and partisan sources.

## Key Results
- BERT variants show more social conservatism compared to GPT variants, correlating with pretraining corpus composition
- Left-leaning models perform better at detecting hate speech against minority groups, while right-leaning models perform better at detecting hate speech against dominant groups
- Political biases in pretraining data propagate to downstream tasks, affecting fairness across identity groups and partisan media sources

## Why This Works (Mechanism)

### Mechanism 1
Pretraining data containing politically biased content leads to language models inheriting those biases, which then propagate into downstream tasks. Large language models learn representations from pretraining corpora that include politically polarized viewpoints. When fine-tuned on downstream tasks, these learned biases influence predictions, leading to unfair outcomes for certain groups or viewpoints. This mechanism assumes pretraining data reflects real-world political polarization and models are sensitive enough to capture these nuances.

### Mechanism 2
Language models exhibit different ideological leanings on social and economic axes, with social issues showing greater polarization. The political compass test framework measures model positions on liberal-conservative (social) and left-right (economic) axes. Models' responses to political statements reflect biases present in pretraining data. This mechanism assumes the political compass test validly measures political ideology and language models can be effectively probed using this framework.

### Mechanism 3
Fine-tuning language models with different political leanings on downstream tasks leads to different performance patterns across identity groups and partisan media sources. Models with varying political leanings, when fine-tuned on tasks like hate speech and misinformation detection, show systematic differences in fairness metrics. This mechanism assumes the fine-tuning process does not eliminate pretraining biases and downstream tasks are sensitive enough to reveal these biases.

## Foundational Learning

**Political spectrum theories and the political compass test**: Provides framework for measuring and categorizing political leanings of language models. *Why needed*: To systematically assess model ideological positions. *Quick check*: What are the two main axes used in the political compass test, and what do they represent?

**Pretraining and fine-tuning of language models**: Understanding how biases in pretraining data can propagate into downstream tasks. *Why needed*: To trace the path from data to model behavior. *Quick check*: What is the difference between pretraining and fine-tuning, and how do they contribute to final model performance?

**Fairness in machine learning**: Understanding implications of biased language models on downstream tasks. *Why needed*: To evaluate and mitigate unfairness in NLP applications. *Quick check*: What are common metrics used to measure fairness in machine learning models, and how can they be applied to language models?

## Architecture Onboarding

**Component map**: Pretraining data (news, forums, books, encyclopedias) → Language models (BERT, RoBERTa, GPT-2/3, etc.) → Political compass test (framework for measuring leanings) → Downstream tasks (hate speech, misinformation detection) → Evaluation metrics (BACC, F1, per-category performance)

**Critical path**: 1) Collect and preprocess pretraining data 2) Pretrain language models 3) Evaluate political leanings using political compass test 4) Fine-tune models on downstream tasks 5) Evaluate fairness of models on downstream tasks

**Design tradeoffs**: Larger, more diverse pretraining data improves performance but may increase bias inheritance; more sophisticated probing techniques provide nuanced understanding but increase computational cost; ensemble methods improve performance but increase complexity.

**Failure signatures**: Models show significantly different downstream performance based on political leanings; models exhibit biased predictions toward certain identity groups or partisan sources; models fail to generalize to new data or tasks.

**First 3 experiments**: 1) Pretrain a language model on politically diverse dataset and evaluate political leanings using political compass test 2) Fine-tune pretrained model on hate speech detection and evaluate performance across identity groups 3) Fine-tune pretrained model on misinformation detection and evaluate performance across partisan media sources

## Open Questions the Paper Calls Out

**Open Question 1**: Can fine-tuning language models on more balanced datasets mitigate observed political biases? The paper discusses political biases and mentions left-leaning LMs slightly outperform right-leaning ones, but does not explore whether balanced datasets can reduce these biases.

**Open Question 2**: How do different model architectures affect magnitude and direction of political biases? The paper evaluates biases across encoder-only, decoder-only, and encoder-decoder models but does not analyze how architecture influences political biases.

**Open Question 3**: Can adversarial training techniques effectively reduce political biases in language models? The paper mentions ensemble methods for mitigating biases but does not investigate adversarial training effectiveness.

## Limitations

- Limited analysis of pretraining data political diversity, with focus on specific partisan corpora rather than comprehensive characterization of common pretraining datasets
- Political compass test sensitivity to prompt variations raises concerns about measurement reliability and whether method captures genuine ideological positions
- Findings may not generalize beyond hate speech and misinformation detection to other high-stakes NLP applications

## Confidence

**High Confidence**: Core observation that language models exhibit measurable political leanings on social and economic axes, well-supported by experimental results across multiple model architectures.

**Medium Confidence**: Claim that political biases propagate to downstream task performance, with strong supporting evidence but limited by specific tasks and datasets studied.

**Low Confidence**: Assertion that pretraining data political composition directly causes model political leanings, lacking comprehensive evidence as study does not systematically vary pretraining data diversity.

## Next Checks

1. **Dataset Composition Analysis**: Systematically analyze political diversity in common pretraining datasets (C4, WebText, etc.) to establish relationship between pretraining data composition and model political leanings.

2. **Prompt Robustness Testing**: Design experiments testing sensitivity of political leaning measurements to prompt variations, paraphrasing, and different probing strategies to validate measurement reliability.

3. **Cross-Task Generalization Study**: Evaluate model political biases and downstream fairness implications across broader range of tasks (content moderation, sentiment analysis, political stance detection) and datasets to assess generalizability of findings.