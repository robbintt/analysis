---
ver: rpa2
title: On the Learnability of Watermarks for Language Models
arxiv_id: '2312.04469'
source_url: https://arxiv.org/abs/2312.04469
tags:
- watermark
- watermarking
- text
- e-04
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can directly learn
  to generate watermarked text, rather than relying on decoding-time watermarking
  algorithms. The authors propose watermark distillation, training a student model
  to imitate a teacher model that uses decoding-based watermarking.
---

# On the Learnability of Watermarks for Language Models

## Quick Facts
- arXiv ID: 2312.04469
- Source URL: https://arxiv.org/abs/2312.04469
- Authors: 
- Reference count: 30
- Key outcome: Models can learn to generate watermarked text through distillation, with watermark detectability depending on distortion levels

## Executive Summary
This paper investigates whether language models can directly learn to generate watermarked text through distillation, rather than relying on decoding-time watermarking algorithms. The authors propose watermark distillation, training a student model to imitate a teacher model that uses decoding-based watermarking. They test three watermarking strategies (KGW, Aar, KTH) and find that models can successfully learn to generate watermarked text with high detectability, though lower-distortion watermarks are harder to learn. The learned watermarks are robust to changes in decoding algorithms but can be removed through fine-tuning on normal text. The paper also demonstrates a proof-of-concept watermark spoofing attack.

## Method Summary
The authors propose watermark distillation, where a student model learns to generate watermarked text by imitating a teacher model that uses decoding-based watermarking. Two approaches are explored: logits-based distillation (matching next token distributions) and sampling-based distillation (fine-tuning on watermarked samples). The experiments use Llama 2 7B as the teacher and test on datasets including OpenWebText, C4, Wikipedia, and arXiv papers. Three watermarking strategies are evaluated: KGW, Aar, and KTH, with various hyperparameter settings to control watermark distortion.

## Key Results
- Models can successfully learn to generate watermarked text through distillation, with AUROC values consistently above 0.8
- Higher-distortion watermark settings are easier to learn, while lower-distortion settings require more training samples
- Learned watermarks remain effective across different decoding algorithms but are lost when fine-tuning on normal text
- A proof-of-concept watermark spoofing attack demonstrates that harmful text can be generated to falsely appear as coming from a victim model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can learn to generate watermarked text through distillation from a teacher model that uses decoding-based watermarking.
- Mechanism: In logits-based watermark distillation, the student model is trained to match the next token distribution outputted by the teacher model using decoding-based watermarking strategy fw. This causes the student to internalize the watermarking behavior.
- Core assumption: The student model can approximate the teacher's next token distribution well enough to learn the watermark signal.
- Evidence anchors:
  - [abstract]: "we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking"
  - [section 3.2]: "the student model is trained to match the next token distribution outputted by the teacher model using decoding-based watermarking strategy fw"
- Break condition: If the student cannot approximate the teacher's distribution well enough, or if the watermark signal is too subtle in the teacher's output distribution.

### Mechanism 2
- Claim: Watermark distillation works because it transfers the knowledge of how to generate watermarked text from the teacher to the student.
- Mechanism: In sampling-based watermark distillation, the teacher generates watermarked samples which the student then learns to reproduce through fine-tuning. This directly teaches the student what watermarked text looks like.
- Core assumption: The student can learn to generate similar watermarked text by fine-tuning on teacher-generated samples.
- Evidence anchors:
  - [section 3.3]: "we generate watermarked text from teacher model pLM with decoding-based watermarking strategy fw applied using key ξ. Then, we fine-tune the student model pθ on this watermarked text"
- Break condition: If the student cannot generalize from the teacher's samples, or if the watermark signal is lost during fine-tuning.

### Mechanism 3
- Claim: Watermarks are learnable because the watermark signal is embedded in the probability distributions that the models learn to predict.
- Mechanism: The watermarking strategies modify the token probability distributions in ways that are detectable through statistical tests. By learning to predict these modified distributions, the student learns to generate watermarked text.
- Core assumption: The watermark signal is sufficiently strong and consistent in the teacher's output distributions to be learnable.
- Evidence anchors:
  - [section 2.3]: "At detection time, the basic test statistic is ϕ(x, ξ) = Plen(x)t=1 − log(1 − ξ(t)xt)"
  - [abstract]: "models can learn to generate watermarked text with high detectability"
- Break condition: If the watermark signal is too weak or inconsistent to be statistically detectable in the student's output.

## Foundational Learning

- Concept: Language modeling and next token prediction
  - Why needed here: The core task of language models is predicting the next token in a sequence, which is what watermarking strategies modify.
  - Quick check question: What is the difference between a language model's probability distribution over tokens and the actual token it generates?

- Concept: Knowledge distillation
  - Why needed here: Watermark distillation is a form of knowledge distillation where the student learns to mimic the teacher's behavior.
  - Quick check question: How does logits-based distillation differ from sampling-based distillation in terms of what information is transferred from teacher to student?

- Concept: Statistical hypothesis testing
  - Why needed here: Watermark detection relies on statistical tests to determine if text contains a watermark signal.
  - Quick check question: What is the null hypothesis in watermark detection, and what does rejecting it mean?

## Architecture Onboarding

- Component map: Teacher model with watermarking -> Distillation process -> Student model with learned watermarking -> Detection algorithm

- Critical path: Teacher model with watermarking → Distillation process → Student model with learned watermarking → Detection algorithm

- Design tradeoffs:
  - Logits-based vs sampling-based distillation: Logits-based is faster but requires white-box access and shared vocabulary; sampling-based is slower but more flexible.
  - Higher vs lower distortion watermarks: Higher distortion is easier to learn but impacts text quality more; lower distortion is harder to learn but less noticeable.

- Failure signatures:
  - Student model fails to generate watermarked text: Check if the distillation process is working correctly, if the watermark signal is strong enough, or if the student has enough capacity.
  - Watermark detection fails on student-generated text: Check if the student has learned the watermark correctly, if the detection algorithm is appropriate, or if the watermark signal is too weak.

- First 3 experiments:
  1. Test logits-based distillation with a simple watermarking strategy (e.g., KGW with high δ) and verify that the student generates watermarked text detectable by the detection algorithm.
  2. Test sampling-based distillation with the same watermarking strategy and compare results to logits-based distillation.
  3. Test learning a lower-distortion watermark (e.g., KGW with lower δ or Aar with higher k) and measure how much more difficult it is to learn.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity of learning different watermarking strategies (KGW, Aar, KTH) compare when using various hyperparameter settings?
- Basis in paper: [explicit] The paper states that "lower-distortion watermarks and hyperparameter settings are more challenging and less sample efficient to learn, but not unlearnable, as the p-values are still noticeably smaller than random chance." It also mentions that "more training samples and steps lead to smaller p-values, with no sign of convergence at 640k samples."
- Why unresolved: The paper does not provide a detailed comparison of sample complexity across different watermarking strategies and their hyperparameter settings. It only mentions that lower-distortion settings are harder to learn but doesn't quantify this difference or compare across strategies.
- What evidence would resolve it: A comprehensive study comparing the number of training samples needed to achieve a certain level of watermark detectability (e.g., a specific AUROC score) for each watermarking strategy and hyperparameter setting. This could be presented as a graph or table showing sample complexity as a function of watermark strength for each strategy.

### Open Question 2
- Question: What is the relationship between watermark distortion and its learnability through watermark distillation?
- Basis in paper: [explicit] The paper discusses that "higher-distortion watermark hyperparam-eter settings (larger δ for KGW, smaller k for Aar, and smaller s for KTH) are successfully learned with small p-values and high detectability," while "lower-distortion watermarks are harder to learn, but they are still learnable, just less efficiently."
- Why unresolved: While the paper establishes a correlation between distortion and learnability, it does not provide a detailed analysis of this relationship. The exact nature of how distortion affects learnability (e.g., is it linear, logarithmic, etc.?) and whether there's a threshold beyond which watermarks become unlearnable is not explored.
- What evidence would resolve it: A systematic study varying the distortion level of watermarks (e.g., by interpolating between high and low distortion settings) and measuring the corresponding learnability. This could be visualized as a curve showing watermark detectability as a function of distortion, potentially revealing the learnability threshold.

### Open Question 3
- Question: How does the robustness of weights-based watermarking to fine-tuning on normal text vary with different watermarking strategies and hyperparameter settings?
- Basis in paper: [explicit] The paper mentions that "weights-based watermarking obtained from watermark distillation is not robust to further fine-tuning on normal, non-watermarked text" and shows this in Figure 4. However, it doesn't explore how this robustness varies across different strategies or settings.
- Why unresolved: The paper demonstrates that fine-tuning removes watermarking capabilities but doesn't investigate the rate at which this happens for different strategies or settings. It's unclear whether some watermarks are more robust to fine-tuning than others.
- What evidence would resolve it: An experiment varying the amount of fine-tuning (e.g., number of steps or proportion of normal text data) and measuring the corresponding decrease in watermark detectability for each watermarking strategy and hyperparameter setting. This could reveal which strategies or settings are more robust to fine-tuning and potentially lead to methods for improving this robustness.

## Limitations
- Vocabulary compatibility constraint: Logits-based distillation requires exact vocabulary matching between teacher and student models, limiting practical applicability.
- Dataset specificity: Experiments primarily use OpenWebText and C4 datasets, leaving effectiveness across different domains untested.
- Single teacher model limitation: All experiments use Llama 2 7B, making generalization to other architectures uncertain.

## Confidence
- High Confidence: The core claim that language models can learn to generate watermarked text through distillation is well-supported by experimental results with strong statistical evidence (AUROC > 0.8).
- Medium Confidence: The claim about watermark robustness to decoding algorithm changes is supported but limited to temperature and nucleus sampling variations.
- Medium Confidence: The watermark spoofing attack demonstration is conceptually sound but depends on real-world deployment scenarios not fully explored.

## Next Checks
1. **Cross-vocabulary distillation test**: Design an experiment where the student model has a different vocabulary than the teacher (e.g., using different tokenizer configurations) to test the robustness of the learning process beyond exact vocabulary matching.

2. **Multi-teacher distillation**: Train a student model using multiple teacher models with different watermarking strategies simultaneously, then test if the student can maintain multiple watermark signals or if they interfere with each other.

3. **Adversarial fine-tuning resistance**: Conduct experiments where the student model is fine-tuned on adversarially crafted normal text (designed to remove watermark signals) to determine the minimum amount of data needed to break the learned watermark.