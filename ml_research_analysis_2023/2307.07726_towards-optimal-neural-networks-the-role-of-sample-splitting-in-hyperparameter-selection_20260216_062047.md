---
ver: rpa2
title: 'Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter
  Selection'
arxiv_id: '2307.07726'
source_url: https://arxiv.org/abs/2307.07726
tags:
- neural
- networks
- sample
- assumption
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of sample splitting in hyperparameter
  selection for neural networks. The authors develop a theoretical framework that
  demonstrates sample splitting enables asymptotically optimal hyperparameter selection,
  minimizing prediction risk.
---

# Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection

## Quick Facts
- arXiv ID: 2307.07726
- Source URL: https://arxiv.org/abs/2307.07726
- Reference count: 12
- Key outcome: Sample splitting enables asymptotically optimal hyperparameter selection for neural networks, minimizing prediction risk across diverse architectures.

## Executive Summary
This paper establishes theoretical foundations for why sample splitting is crucial in neural network hyperparameter selection. The authors prove that under mild assumptions, the hyperparameter chosen by minimizing validation loss is asymptotically optimal, achieving prediction risk equivalent to the best possible network. The theory is validated through extensive experiments across multiple architectures including multilayer perceptrons, convolutional neural networks, and recurrent neural networks, demonstrating convergence of the performance ratio to 1 as sample size increases.

## Method Summary
The method involves splitting data into training and validation sets, then training multiple neural networks with different hyperparameter configurations on the training set. For each configuration, validation loss is computed on the validation set, and the hyperparameter minimizing this loss is selected. The selected model is then evaluated on a held-out test set. The theoretical framework decomposes prediction error into model error and estimation error, proving that sample splitting enables optimal selection by providing unbiased validation loss estimates that converge to true risk.

## Key Results
- Sample splitting enables asymptotically optimal hyperparameter selection by providing unbiased validation loss estimates
- The ratio of test loss using selected hyperparameter to minimum possible loss converges to 1 as sample size increases
- Theory validated across MLP, CNN, and RNN architectures on diverse problem domains including regression, classification, and time series forecasting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample splitting enables asymptotically optimal hyperparameter selection by providing unbiased validation loss estimates
- Mechanism: The validation loss Ln2(λ) computed on independent validation data provides an unbiased estimate of the prediction risk R0(λ). By minimizing this unbiased estimate across hyperparameters, the algorithm converges to the hyperparameter that minimizes true prediction risk as sample size increases.
- Core assumption: The validation and training sets are independent and identically distributed (Assumption 2), and the validation loss has bounded variance (Assumption 5).
- Evidence anchors:
  - [abstract] "sample splitting enables asymptotically optimal hyperparameter selection, minimizing prediction risk"
  - [section] "L(λ; X, Y ) measures the squared loss between the conditional expectation E(Y |X) and the network output fλ(·, ˆwn(λ)) with input X"
  - [corpus] Weak evidence - no direct citations found, but the concept aligns with standard ML validation practices
- Break condition: If validation and training sets are not independent or identically distributed, the unbiasedness of validation loss breaks down, leading to suboptimal hyperparameter selection.

### Mechanism 2
- Claim: Asymptotic optimality is achieved through error decomposition between model error and estimation error
- Mechanism: The total prediction error decomposes into model error (gap between true function and best possible network for given architecture) and estimation error (gap between trained parameters and optimal parameters). Sample splitting enables optimal hyperparameter selection by accounting for both error components.
- Core assumption: The trained parameters ˆwn(λ) converge to optimal parameters w∗(λ) at rate Op(n−1/2) (Assumption 1), and the model error decreases with appropriate hyperparameter choice.
- Evidence anchors:
  - [section] "E(Y |X) − fλ(X, ˆwn(λ)) = {E(Y |X) − fλ(X, w∗(λ))} + {fλ(X, w∗(λ)) − fλ(X, ˆwn(λ))}"
  - [section] "Our work contemplates both model error and estimation error from the perspective of error decomposition"
  - [corpus] Weak evidence - no direct citations found, but the error decomposition concept is standard in statistical learning theory
- Break condition: If the convergence rate of ˆwn(λ) to w∗(λ) is slower than Op(n−1/2) or the model error doesn't decrease appropriately, asymptotic optimality may not hold.

### Mechanism 3
- Claim: The finite hyperparameter space assumption ensures existence of optimal hyperparameters and enables uniform convergence
- Mechanism: By assuming |Λ| < ∞, the authors can apply uniform convergence results across all hyperparameters, ensuring that the validation loss uniformly converges to the true risk and the optimal hyperparameter can be consistently selected.
- Core assumption: The hyperparameter space Λ is finite (|Λ| < ∞), which is justified by practical constraints on continuous hyperparameters and the finite nature of discrete/categorical choices.
- Evidence anchors:
  - [section] "we assume Λ to be a finite set, i.e. |Λ| < ∞" and "it is reasonable to develop our theory under the assumption that |Λ| < ∞"
  - [section] "The existence of ˆλ can be guaranteed by |Λ| < ∞"
  - [corpus] Moderate evidence - aligns with practical hyperparameter optimization practices like grid search and random search
- Break condition: If the hyperparameter space is infinite or diverges with sample size, uniform convergence may fail and asymptotic optimality may not hold.

## Foundational Learning

- Concept: Asymptotic analysis and consistency
  - Why needed here: The paper's main results rely on understanding how estimators behave as sample size n approaches infinity, specifically that the selected hyperparameter converges to the optimal one in the limit.
  - Quick check question: If n1 = n2 = n and we double the sample size, how does this affect the consistency of the hyperparameter selection?

- Concept: Error decomposition in statistical learning
  - Why needed here: Understanding the distinction between approximation error (model error) and estimation error is crucial for interpreting why sample splitting leads to optimal hyperparameter selection.
  - Quick check question: In the decomposition E(Y|X) - fλ(X, ˆwn(λ)), which term represents the model error and which represents the estimation error?

- Concept: Uniform convergence and concentration inequalities
  - Why needed here: The proofs rely on showing that the validation loss uniformly converges to the true risk across all hyperparameters, which requires understanding concentration inequalities and uniform convergence concepts.
  - Quick check question: What role does the assumption |Λ| < ∞ play in establishing uniform convergence of the validation loss?

## Architecture Onboarding

- Component map:
  - Data splitting module: Divides data into training and validation sets
  - Training pipeline: Trains multiple networks with different hyperparameters
  - Validation module: Computes validation loss for each hyperparameter configuration
  - Selection module: Chooses hyperparameter with minimum validation loss
  - Testing module: Evaluates final model on held-out test data

- Critical path: Data splitting → Multiple training runs with different hyperparameters → Validation loss computation → Hyperparameter selection → Final training with selected hyperparameters → Test evaluation

- Design tradeoffs:
  - Smaller validation sets: More training data but noisier validation estimates
  - Larger validation sets: More reliable validation but less training data
  - Finer hyperparameter grid: Better coverage but exponential increase in training time
  - Coarser hyperparameter grid: Faster computation but risk of missing optimal configuration

- Failure signatures:
  - High variance in validation loss across different validation set splits indicates insufficient validation set size
  - Validation loss consistently underestimates true risk suggests overfitting to validation set
  - No clear validation loss minimum across hyperparameter grid suggests inadequate hyperparameter coverage

- First 3 experiments:
  1. Linear regression with varying R² values (σ² = 1/3, 1, 3) to verify convergence of E{L0(ˆλ)/inf λ∈Λ L0(λ)} to 1
  2. Binary classification on Fashion-MNIST with varying CNN architectures to test theory on image data
  3. Time series forecasting with RNNs to verify theory extends beyond i.i.d. assumptions (despite theoretical violation)

## Open Questions the Paper Calls Out
- How does the theory extend to cases where the hyperparameter space Λ is not finite but potentially infinite?
- Can the optimality of sample splitting be proven for neural networks trained on correlated or dependent data, such as time series data?
- What is the impact of the optimization algorithm used to train the neural network on the optimality of the selected hyperparameters?

## Limitations
- The theory assumes finite hyperparameter space, which may not hold with continuous hyperparameters in practice
- The i.i.d. assumption for data splits is technically violated in time series settings where the theory is still claimed to apply
- The theory doesn't explicitly address potential overfitting to the validation set or provide guidance on optimal validation set size

## Confidence
- Theoretical claims: High confidence due to rigorous mathematical proofs under clearly stated assumptions
- Experimental validation: Medium confidence in practical applicability, though limited in scale
- Practical implementation: Low confidence due to unspecified implementation details and hyperparameter ranges

## Next Checks
1. Test the theory with continuous hyperparameter optimization methods (Bayesian optimization, random search) to verify that asymptotic optimality holds beyond finite grid search
2. Evaluate the impact of validation set size on the bias-variance tradeoff in hyperparameter selection through systematic experiments with varying split ratios
3. Verify the theory's predictions on datasets with strong temporal or spatial dependencies where i.i.d. assumptions are violated