---
ver: rpa2
title: What's left can't be right -- The remaining positional incompetence of contrastive
  vision-language models
arxiv_id: '2311.11477'
source_url: https://arxiv.org/abs/2311.11477
tags:
- left
- right
- clip
- relations
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIP and similar models struggle with left-right positional understanding
  despite success on standard vision-language tasks. We analyze why this occurs, showing
  that current datasets and embedding spaces fail to distinguish left-right relations,
  even with large-scale models.
---

# What's left can't be right -- The remaining positional incompetence of contrastive vision-language models

## Quick Facts
- arXiv ID: 2311.11477
- Source URL: https://arxiv.org/abs/2311.11477
- Reference count: 15
- CLIP struggles with left-right positional understanding despite success on standard vision-language tasks

## Executive Summary
Current vision-language models like CLIP perform well on standard tasks but struggle with left-right positional understanding. This paper analyzes why this occurs, showing that existing datasets and embedding spaces fail to distinguish left-right relations. The authors address this by creating synthetic training data that explicitly contrasts left-right positioning using automatically generated image-text pairs. Fine-tuning CLIP on this data significantly improves left-right relation accuracy on the Visual Genome Relations benchmark (from ~50% to ~57%) and better aligns the embedding space to reflect semantic opposites.

## Method Summary
The authors generate synthetic contrastive pairs by extracting nouns from VG-Relations, generating images for each noun using Stable Diffusion, and pairing random nouns in left-right configurations with corresponding captions. CLIP ViT-B-32 is then fine-tuned on this synthetic dataset for 15 epochs using OpenCLIP with a batch size of 512, including 64 contrastive pairs per batch. The fine-tuned model is evaluated on the left-right relations subset of the Visual Genome Relations benchmark by computing cosine similarity between image embeddings and captions.

## Key Results
- CLIP baseline shows ~50% accuracy on left-right relations (random chance)
- Fine-tuning on synthetic contrastive data improves accuracy to ~57%
- Left and right embeddings converge closer in baseline CLIP (cosine similarity ~0.85) but separate after fine-tuning
- Improvement generalizes from synthetic to natural images on VG-Relations benchmark

## Why This Works (Mechanism)

### Mechanism 1
CLIP's default embedding space fails to encode left-right positional semantics because these relations appear too frequently as syntactic variations rather than distinct semantic concepts in training data. The contrastive loss treats syntactic similarity as evidence of semantic similarity, ignoring fine-grained positional distinctions.

### Mechanism 2
Synthetic data generation creates explicit left-right contrasts that force the model to learn distinct embeddings for opposite relations. By generating paired images where only the relative position of two objects changes and placing these pairs in the same training batch, the model is compelled to differentiate the embeddings to minimize contrastive loss.

### Mechanism 3
The improvement generalizes to natural images because the learned distinction captures the underlying semantic meaning of left-right relations rather than memorizing specific synthetic examples. Once the model learns to separate left and right embeddings, this understanding transfers to real-world scenarios where objects have natural left-right relationships.

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: Understanding how CLIP's training process pulls together matching image-text pairs while pushing apart non-matching pairs is crucial for understanding why left-right relations are poorly learned by default.
  - Quick check question: In CLIP's contrastive loss, what happens to the embeddings of an image and its matching caption versus a non-matching caption?

- Concept: Embedding space geometry
  - Why needed here: The spatial relationships between concepts in CLIP's embedding space (like how close "left" and "right" are) directly impact the model's ability to distinguish between them.
  - Quick check question: If "horse" and "tree" are far apart in embedding space but "left" and "right" are close, what does this tell us about how CLIP understands spatial relations?

- Concept: Synthetic data generation for training
  - Why needed here: The paper's key innovation is creating synthetic training data that explicitly teaches left-right distinctions, so understanding how to generate such data is essential.
  - Quick check question: Why does the paper generate two images that only differ in the positioning of objects, rather than just one image with different captions?

## Architecture Onboarding

- Component map: Data pipeline (extract nouns -> generate synthetic images -> create contrastive pairs) -> Model (CLIP ViT-B-32 fine-tuned with OpenCLIP) -> Training loop (MSCOCO + synthetic pairs) -> Evaluation (VG-Relations benchmark with cosine similarity)

- Critical path: Data generation → Model fine-tuning → Embedding space analysis → Evaluation on VG-Relations

- Design tradeoffs:
  - Synthetic vs natural data: Synthetic data allows precise control over left-right relations but may lack real-world complexity
  - Batch composition: Including both MSCOCO and synthetic pairs balances general language understanding with spatial relation learning
  - Model choice: Using CLIP ViT-B-32 enables fair comparison with NegCLIP but may limit maximum performance

- Failure signatures:
  - If left-right accuracy remains at ~50%, the synthetic data may not be forcing the model to distinguish relations
  - If synthetic data quality is poor, the model may learn incorrect associations
  - If batch sampling doesn't ensure opposing pairs are together, the contrastive signal is weakened

- First 3 experiments:
  1. Verify that "left" and "right" embeddings are close in baseline CLIP by computing cosine similarity
  2. Test that synthetic data generation produces visually distinct images with only position changes
  3. Confirm that fine-tuning with synthetic data improves left-right accuracy on a small subset of VG-Relations before full training

## Open Questions the Paper Calls Out

1. How does increasing the diversity of synthetic training data (beyond just left-right positioning) impact CLIP's spatial understanding capabilities?
2. What is the optimal balance between synthetic data and real-world data for training vision-language models to achieve spatial understanding?
3. Can the embedding space alignment techniques used for left-right relations be applied to improve spatial understanding in generative models like DALL-E 2 or Stable Diffusion?

## Limitations
- The modest accuracy gain from ~50% to ~57% suggests the approach may not fully resolve fundamental limitations in CLIP's spatial reasoning capabilities.
- The synthetic data generation process requires 10 image generations per noun using Stable Diffusion, creating significant computational overhead.
- While the paper claims generalization to natural images, the evaluation is limited to the Visual Genome Relations benchmark.

## Confidence
- High Confidence: The experimental methodology for fine-tuning CLIP on synthetic data is well-defined and reproducible.
- Medium Confidence: The claim that synthetic data explicitly contrasting left-right positions improves spatial understanding is supported by the empirical results.
- Low Confidence: The assertion that the learned distinctions capture "underlying semantic meaning" rather than memorizing synthetic examples is plausible but not rigorously proven.

## Next Checks
1. Compute and visualize the cosine distances between "left" and "right" embeddings in the fine-tuned model compared to baseline CLIP, and extend this analysis to other spatial relations (above/below, front/behind).
2. Evaluate the fine-tuned model on alternative spatial reasoning benchmarks (e.g., NLVR2, GQA) to assess whether improvements in left-right understanding transfer to other spatial relation tasks.
3. Systematically vary the amount and quality of synthetic data used during fine-tuning to determine the minimum effective data requirements and identify potential overfitting.