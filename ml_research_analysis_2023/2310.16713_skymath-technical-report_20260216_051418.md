---
ver: rpa2
title: 'SkyMath: Technical Report'
arxiv_id: '2310.16713'
source_url: https://arxiv.org/abs/2310.16713
tags: []
core_contribution: This paper introduces SkyMath, a 13-billion parameter large language
  model for mathematical reasoning. The model applies self-compare fine-tuning to
  enhance reasoning abilities, combining data augmentation techniques with supervised
  fine-tuning.
---

# SkyMath: Technical Report

## Quick Facts
- arXiv ID: 2310.16713
- Source URL: https://arxiv.org/abs/2310.16713
- Reference count: 11
- 13-billion parameter model achieving state-of-the-art performance on GSM8K benchmark

## Executive Summary
This paper introduces SkyMath, a 13-billion parameter large language model for mathematical reasoning. The model applies self-compare fine-tuning to enhance reasoning abilities, combining data augmentation techniques with supervised fine-tuning. By constructing high-complexity math datasets and using progressive hint prompting, SkyMath improves upon standard fine-tuning. The approach iteratively refines the model by having it compare its own answers to ground truth. Experimental results show that SkyMath outperforms all known open-source models of similar size on GSM8K, establishing a new state-of-the-art performance.

## Method Summary
SkyMath applies self-compare fine-tuning to enhance mathematical reasoning abilities of SkyWork-13B-Base. The method combines instruction boosting data augmentation with progressive hint prompting. The model is trained on GSM8K, MATH, and CMath datasets using a three-step process: instruction boosting, self-compare fine-tuning, and iterative refinement. The approach constructs high-complexity math datasets and uses the model's previous answers as hints for subsequent reasoning steps.

## Key Results
- Outperforms all known open-source models of similar size on GSM8K benchmark
- Achieves high accuracy on challenging MATH dataset
- Demonstrates strong generalization to out-of-domain Chinese math problems (CMath)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-compare fine-tuning enables iterative error correction through internal feedback
- Mechanism: The model generates its own answers, then compares them to ground truth, identifying specific error patterns and refining its reasoning through repeated cycles
- Core assumption: The model's ability to accurately compare its outputs against correct answers will lead to meaningful performance improvements
- Evidence anchors:
  - [abstract] "Experimental results show that SkyMath outperforms many open-source models in similar sizes on two mathematical benchmarks"
  - [section] "We hope the model can compare its previous answers with ground truth and correct its specific errors through training"
  - [corpus] "Found 25 related papers" - weak evidence for self-compare specifically, though similar techniques exist in other works
- Break condition: If the model cannot reliably identify its own errors or if the comparison process introduces noise that outweighs the benefits

### Mechanism 2
- Claim: Progressive hint prompting guides reasoning through stepwise refinement
- Mechanism: Previous answers are used as hints for subsequent reasoning steps, creating a progressive refinement loop that helps the model approach correct solutions
- Core assumption: Stepwise guidance using previous outputs will help the model maintain reasoning coherence and avoid compounding errors
- Evidence anchors:
  - [section] "Progressive-Hint Prompting (PHP) enables multiple interactions between users and LLMs by using previously generated answers as hints"
  - [section] "we believe introducing previous answers to the training process also has an effect"
  - [corpus] "Found 25 related papers" - weak evidence for progressive hint prompting specifically
- Break condition: If the hints become misleading or if the model overfits to its own potentially flawed reasoning patterns

### Mechanism 3
- Claim: High-complexity data augmentation expands the model's reasoning capabilities
- Mechanism: Instruction boosting techniques (concretizing, adding constraints, deepening, rephrasing) create more challenging problems that force the model to develop sophisticated reasoning strategies
- Core assumption: Training on more complex, varied mathematical problems will improve generalization to unseen problems
- Evidence anchors:
  - [section] "we adapt instruction boosting...to the question augmentation process"
  - [section] "Experimental results show that SkyMath outperforms many open-source models"
  - [corpus] "KwaiYiiMath: Technical Report" - related work exists but specific evidence for this mechanism is limited
- Break condition: If the augmented data introduces unrealistic problems or if the complexity overwhelms the model's learning capacity

## Foundational Learning

- Concept: Supervised fine-tuning on domain-specific data
  - Why needed here: Standard pre-training doesn't provide sufficient mathematical reasoning capability
  - Quick check question: What's the difference between pre-training and fine-tuning in the context of LLMs?

- Concept: Instruction following and chain-of-thought reasoning
  - Why needed here: Mathematical problems require step-by-step reasoning that must be explicitly modeled
  - Quick check question: How does chain-of-thought prompting improve mathematical reasoning performance?

- Concept: Data augmentation techniques
  - Why needed here: Limited availability of high-quality mathematical reasoning datasets requires synthetic data generation
  - Quick check question: What are the risks of using LLM-generated data for training other LLMs?

## Architecture Onboarding

- Component map: SkyWork-13B -> Data augmentation pipeline -> Self-compare fine-tuning -> Progressive hint prompting -> Evaluation
- Critical path: Data preparation -> Fine-tuning -> Evaluation -> Iteration
- Design tradeoffs: Self-compare vs external verifier, data augmentation complexity vs training stability, computational cost vs performance gains
- Failure signatures: Overfitting to training data, degradation on out-of-domain problems, sensitivity to prompt variations
- First 3 experiments:
  1. Run baseline evaluation on GSM8K and MATH with unmodified SkyWork-13B
  2. Implement self-compare fine-tuning on a small subset and measure performance impact
  3. Test progressive hint prompting on a few example problems to verify reasoning improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-compare fine-tuning method perform when applied to different base model architectures beyond SkyWork-13B-Base?
- Basis in paper: [explicit] The paper states "we apply self-compare fine-tuning to enhance mathematical reasoning abilities of Skywork-13B-Base" and demonstrates results specifically for this model
- Why unresolved: The study only tested the method on SkyWork-13B-Base, leaving open whether the approach would generalize to other architectures or sizes
- What evidence would resolve it: Testing self-compare fine-tuning on various base models (e.g., LLaMA, Mistral, or different sizes) and comparing performance gains across architectures

### Open Question 2
- Question: What is the optimal number of self-compare iterations for different difficulty levels of mathematical problems?
- Basis in paper: [explicit] The paper mentions "in practice we divide the origin dataset into several sub-datasets thus we can repeat self-compare fine-tuning more than once" but doesn't specify optimal stopping criteria
- Why unresolved: The paper doesn't explore how the number of iterations affects performance on different complexity levels or when diminishing returns occur
- What evidence would resolve it: Systematic experiments varying the number of self-compare iterations and measuring performance across different problem complexities

### Open Question 3
- Question: How does SkyMath's performance scale with model size when applying the same self-compare fine-tuning approach?
- Basis in paper: [inferred] While SkyMath uses a 13B parameter model, the paper notes it "outperforms all known open-source models of similar size" suggesting potential for larger models
- Why unresolved: The study focuses on a single model size, leaving unclear how the method would perform with larger or smaller parameter counts
- What evidence would resolve it: Training and evaluating SkyMath-sized models (e.g., 7B, 34B) with identical fine-tuning procedures to establish scaling trends

## Limitations

- Implementation details of self-compare fine-tuning are not fully disclosed
- Limited information on how progressive hint prompting affects training dynamics
- Unclear whether performance gains are primarily from data augmentation or the fine-tuning method
- Potential for overfitting to GSM8K-style problems without broader validation

## Confidence

The claims about SkyMath's performance improvements through self-compare fine-tuning have **medium confidence** due to limited transparency in the implementation details. While the paper demonstrates superior performance on GSM8K and MATH benchmarks, the exact mechanisms of how self-compare fine-tuning and progressive hint prompting are implemented remain underspecified, making independent verification challenging.

The claim that SkyMath achieves "state-of-the-art" performance among open-source models of similar size is **high confidence** for GSM8K but **medium confidence** for MATH, as the paper doesn't provide comprehensive comparisons with all recent models, particularly those from the same time period.

The generalizability claim to Chinese math problems (CMath dataset) has **low confidence** because the dataset size is relatively small (1.7k problems) and the paper doesn't provide detailed analysis of failure cases or error patterns across different language domains.

## Next Checks

1. **Independent replication test**: Implement the self-compare fine-tuning procedure on a different base model (e.g., Llama-13B) using the same data augmentation pipeline to verify that the approach generalizes beyond SkyWork-13B

2. **Ablation study**: Conduct controlled experiments removing either the data augmentation component or the self-compare fine-tuning component to quantify their individual contributions to the performance gains

3. **Out-of-distribution robustness test**: Evaluate SkyMath on diverse mathematical reasoning datasets beyond GSM8K and MATH, including problems requiring different mathematical domains and reasoning patterns, to assess true generalization capability