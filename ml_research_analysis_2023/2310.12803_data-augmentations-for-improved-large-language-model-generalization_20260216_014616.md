---
ver: rpa2
title: Data Augmentations for Improved (Large) Language Model Generalization
arxiv_id: '2310.12803'
source_url: https://arxiv.org/abs/2310.12803
tags:
- data
- counterfactual
- clinical
- causal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of language model generalization
  in the presence of spurious correlations, a key challenge for safety-critical applications
  like healthcare. The authors propose a causal-structure-driven data augmentation
  approach (CATO) that leverages auxiliary data and domain knowledge to generate counterfactual
  training examples.
---

# Data Augmentations for Improved (Large) Language Model Generalization

## Quick Facts
- arXiv ID: 2310.12803
- Source URL: https://arxiv.org/abs/2310.12803
- Reference count: 40
- Key outcome: CATO improves out-of-distribution generalization in clinical NLP by generating counterfactual training examples that break spurious correlations

## Executive Summary
This paper tackles the problem of language model generalization in the presence of spurious correlations, a key challenge for safety-critical applications like healthcare. The authors propose a causal-structure-driven data augmentation approach (CATO) that leverages auxiliary data and domain knowledge to generate counterfactual training examples. Specifically, CATO matches examples using auxiliary data and prompts a large language model to rewrite text in the style of the matched examples, effectively de-correlating the target label from the spurious attribute. The authors show that CATO outperforms several strong baselines, including reweighting and invariance penalties, on clinical NLP tasks and semi-synthetic data. The key takeaway is that incorporating causal knowledge into data augmentation can significantly improve out-of-distribution generalization.

## Method Summary
CATO (Causal-structure Driven Augmentations for Text OOD Generalization) uses auxiliary data to match examples and prompts an LLM to rewrite text in the style of matched examples, creating counterfactual instances where the writing style is decoupled from the clinical outcome. The method operates under a causal graph where C (caregiver ID) is a spurious attribute spuriously correlated with label Y, while X* represents the underlying clinical information. By generating counterfactuals where C is changed while holding X* constant, CATO aims to break this spurious correlation. The approach is applied to clinical NLP tasks using MIMIC-III and i2b2 datasets, with PubMED BERT as the base model.

## Key Results
- CATO outperforms baseline methods including invariance penalties and reweighting on clinical condition prediction tasks
- Conditional augmentation (CATO) shows significant improvements over naive augmentation on restaurant review datasets
- The method demonstrates favorable sample complexity compared to importance reweighting under accurate counterfactual generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Counterfactual data augmentation breaks spurious correlations between attribute C and label Y by simulating interventions on C while holding other variables constant.
- **Mechanism**: The approach uses auxiliary data M to match examples and prompts an LLM to rewrite text in the style of matched examples, effectively creating counterfactual instances where the writing style (C) is decoupled from the clinical outcome (Y).
- **Core assumption**: Strong ignorability holds - X(c) ⊥ ⊥ C | M, meaning that after conditioning on auxiliary data M, the writing style is independent of the text content.
- **Evidence anchors**:
  - [abstract]: "We propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features"
  - [section]: "Under this assumption, we can rewrite the counterfactual distribution with the observed distribution, P(X(c)) = ∫ P(X(c) | M = m)P(M = m)dm"
- **Break condition**: If the auxiliary data M doesn't fully capture the confounding between C and X, the counterfactuals will remain biased and the spurious correlation won't be broken.

### Mechanism 2
- **Claim**: Counterfactual augmentation has favorable sample complexity compared to importance re-weighting for learning invariant predictors.
- **Mechanism**: The method learns a distribution over augmented notes that approximates the unconfounded distribution P/⊙◇⊞, where C is independent of Y. This allows learning a hypothesis that minimizes risk across all distributions in P.
- **Core assumption**: The counterfactual approximation τc is accurate enough that the divergence between τc,∗(Ptrain(X, M)) and P(X(c)) is small.
- **Evidence anchors**:
  - [abstract]: "we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting"
  - [section]: "divergences other than total-variation can be used, resulting in tighter bounds"
- **Break condition**: If the counterfactual generation is poor (high divergence), the method loses its sample complexity advantage and may perform worse than re-weighting.

### Mechanism 3
- **Claim**: Using LLMs with domain knowledge creates plausible counterfactuals that improve OOD generalization.
- **Mechanism**: The LLM models the conditional probability distribution of text given auxiliary data, and is prompted to rewrite text in the style of matched examples, creating counterfactuals that are both fluent and domain-appropriate.
- **Core assumption**: The LLM can accurately model P(X | C, M) and generate text that preserves the underlying information X* while changing the spurious attribute C.
- **Evidence anchors**:
  - [abstract]: "we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text"
  - [section]: "CATO (Causal-structure Driven Augmentations for Text OOD Generalization) involves the use of an LLM to model the conditional probability distribution of text"
- **Break condition**: If the LLM fails to preserve the underlying information X* while changing the style, the counterfactuals will be misleading and harm model performance.

## Foundational Learning

- **Concept: Counterfactual reasoning in causal inference**
  - Why needed here: The entire approach relies on generating and reasoning about counterfactual instances - what the text would look like under different writing styles while keeping the clinical information constant.
  - Quick check question: What is the key assumption that allows us to identify counterfactual distributions from observational data?

- **Concept: Domain adaptation and distribution shift**
  - Why needed here: The motivation is that models trained on one hospital's data (where certain writing styles correlate with diagnoses) fail on new hospitals where this correlation doesn't hold.
  - Quick check question: Why is it important that the label Y is spuriously correlated with attribute C in the training data?

- **Concept: Sample complexity and generalization bounds**
  - Why needed here: The paper compares the sample complexity of counterfactual augmentation versus re-weighting, showing that augmentation can be more sample-efficient under certain conditions.
  - Quick check question: How does the accuracy of counterfactual generation affect the sample complexity advantage of augmentation over re-weighting?

## Architecture Onboarding

- **Component map**: MIMIC-III/i2b2 datasets → Preprocessing → Matching → Counterfactual generation → PubMED BERT training → Evaluation

- **Critical path**:
  1. Match examples using auxiliary data (medications, lab results, vitals)
  2. Generate counterfactuals by prompting LLM to rewrite in matched style
  3. Train model on original + counterfactual data
  4. Evaluate on held-out hospital data

- **Design tradeoffs**:
  - Matching quality vs. computational cost: More precise matching yields better counterfactuals but is slower
  - LLM choice vs. cost: Larger models may generate better counterfactuals but are more expensive
  - Augmentation ratio vs. overfitting: Too many counterfactuals may cause the model to overfit to synthetic patterns

- **Failure signatures**:
  - Poor matching leads to counterfactuals that don't preserve clinical information → Accuracy drops on ID data
  - LLM generates incoherent text → Model learns to ignore counterfactuals
  - Spurious correlation too strong → Even perfect counterfactuals can't fully break the correlation

- **First 3 experiments**:
  1. Run CATO (A) on clinical condition prediction task - verify that it improves OOD F1 score over baseline
  2. Test CATO (B) on restaurant reviews - confirm that conditional augmentation outperforms naive augmentation
  3. Vary the quality of counterfactuals in synthetic data - measure the impact on sample complexity advantage over re-weighting

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the limitations section highlights several areas for future research, including exploring the method's performance on languages other than English, analyzing the impact of auxiliary data quality on performance, and investigating how performance scales with dataset size.

## Limitations

- The method's effectiveness depends heavily on the quality of counterfactual generation, which may vary across different domains and LLM architectures
- The approach assumes access to high-quality auxiliary data that captures the confounding between spurious attributes and labels
- The theoretical sample complexity advantages assume accurate counterfactual generation, which may not hold in practice

## Confidence

The paper's claims about counterfactual data augmentation's superiority over re-weighting methods rely on strong assumptions about ignorability and accurate counterfactual generation. Our confidence in these claims is **Medium** given that the paper provides theoretical bounds but limited empirical validation across diverse datasets and LLM architectures.

The assertion that LLMs can accurately model conditional distributions P(X|C,M) for counterfactual generation is particularly uncertain, as it depends heavily on prompt engineering quality and domain specificity. This represents a **Low** confidence claim area requiring further empirical validation.

The sample complexity advantages claimed for augmentation over re-weighting assume the counterfactual approximation is sufficiently accurate - a critical assumption that isn't thoroughly tested across different levels of approximation error.

## Next Checks

1. Systematically vary the quality of LLM-generated counterfactuals (e.g., by using different prompt templates or model sizes) and measure the impact on both sample complexity and OOD performance
2. Test the method on non-clinical text classification tasks with different types of spurious correlations to assess generalizability
3. Compare the empirical sample complexity empirically by training models with varying amounts of original vs. counterfactual data