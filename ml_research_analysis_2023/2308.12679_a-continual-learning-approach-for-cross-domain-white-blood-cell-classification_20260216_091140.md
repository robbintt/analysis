---
ver: rpa2
title: A Continual Learning Approach for Cross-Domain White Blood Cell Classification
arxiv_id: '2308.12679'
source_url: https://arxiv.org/abs/2308.12679
tags:
- learning
- class
- incremental
- continual
- blood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present an uncertainty-aware continual learning method for white
  blood cell classification across different data sources. Our approach uses exemplar-based
  rehearsal with uncertainty-driven sample selection to mitigate catastrophic forgetting
  in cross-domain settings.
---

# A Continual Learning Approach for Cross-Domain White Blood Cell Classification

## Quick Facts
- arXiv ID: 2308.12679
- Source URL: https://arxiv.org/abs/2308.12679
- Reference count: 31
- We present an uncertainty-aware continual learning method for white blood cell classification across different data sources.

## Executive Summary
This paper introduces an uncertainty-aware continual learning approach for white blood cell classification across multiple data sources. The method addresses catastrophic forgetting by using exemplar-based rehearsal with uncertainty-driven sample selection. The approach employs epistemic uncertainty estimation via MC dropout to identify informative samples for the exemplar set, combined with knowledge distillation to preserve previously learned information. The authors demonstrate superior performance compared to established baselines including iCaRL and EWC on three real-world white blood cell datasets.

## Method Summary
The method uses exemplar-based rehearsal with uncertainty-driven sample selection for continual learning across domains. It employs epistemic uncertainty estimation using MC dropout to select informative samples for the exemplar set, balancing between confident samples near class means and challenging samples. A distillation loss minimizes disagreement between current and previous model parameters to preserve knowledge. The approach uses ResNeXt-50 architecture with dropout layer placement for uncertainty estimation, and implements equal sampling across classes to compensate for dataset imbalance.

## Key Results
- Achieves higher average accuracy compared to established baselines (iCaRL and EWC) on three white blood cell datasets
- Demonstrates lower forgetting rates in both class incremental and domain incremental scenarios
- Outperforms upper bound in domain class incremental experiment due to effective handling of unbalanced data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty-based sampling selects informative samples that better represent class distributions across domains
- Mechanism: The method estimates epistemic uncertainty using MC dropout to identify challenging samples and samples near class means, ensuring exemplar sets capture both class structure and variability
- Core assumption: Uncertainty estimation accurately reflects the model's knowledge gaps, and these samples are more representative for preventing forgetting
- Evidence anchors:
  - [abstract] "We employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model."
  - [section] "We use epistemic uncertainty estimation [20] to select the remaining m/2 images... For every image, the model predicts |Ct| probabilities resulting into a matrix RT ×|Ct|. Thus for every image xi, the uncertainty is calculated as Ui = sum over classes of variance across T inferences."
  - [corpus] Weak evidence - no direct comparison of uncertainty-based vs random sampling in corpus papers

### Mechanism 2
- Claim: Distillation loss preserves previously learned knowledge when training on new tasks
- Mechanism: Binary cross-entropy distillation loss minimizes disagreement between current and previous model parameters, preventing catastrophic forgetting
- Core assumption: The previous model's predictions contain valuable information about class boundaries that should be preserved
- Evidence anchors:
  - [abstract] "We use a distillation loss to preserve the previously learned information"
  - [section] "The distillation loss is defined between the current state of the model with θt parameters and the previous state with learned θt−1 parameters... Using binary cross-entropy loss, we try to minimize the disagreement between the current model and the model before training"
  - [corpus] Weak evidence - corpus papers don't explicitly discuss distillation-based continual learning for WBC classification

### Mechanism 3
- Claim: Equal sampling across classes compensates for dataset imbalance in long-term learning
- Mechanism: The sampling strategy ensures m/2 samples are selected based on class mean proximity and m/2 based on uncertainty, providing balanced representation
- Core assumption: Balanced exemplar sets are more effective than imbalanced ones for maintaining performance across all classes
- Evidence anchors:
  - [section] "With the UACL sampling strategy, classes are equally sampled, and uncertain cases in underrepresented classes are very informative to preserve the latent structure of these classes."
  - [section] "Interestingly, we are out-performing the upper bound in the domain class incremental experiment, which can be due to the unbalanced data"
  - [corpus] No direct evidence in corpus papers about balanced sampling effects

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses forgetting when models are fine-tuned on new data streams, which is central to continual learning
  - Quick check question: What happens to a neural network's performance on old tasks when trained only on new data?

- Concept: Uncertainty estimation (epistemic vs aleatoric)
  - Why needed here: The method uses epistemic uncertainty via MC dropout to identify challenging samples for exemplar selection
  - Quick check question: How does epistemic uncertainty differ from aleatoric uncertainty in deep learning?

- Concept: Knowledge distillation
  - Why needed here: The approach uses distillation loss to preserve previously learned information between model versions
  - Quick check question: What is the purpose of distillation loss in continual learning?

## Architecture Onboarding

- Component map: ResNeXt-50 backbone → dropout layer (between conv2 and conv3) → final classification layer → uncertainty estimation module → exemplar selection module → distillation loss computation
- Critical path: Training loop → forward pass with dropout sampling → uncertainty calculation → exemplar selection → combined loss computation → parameter update
- Design tradeoffs: Memory budget K vs. exemplar set quality, distillation weight γ vs. adaptation capability, dropout sampling iterations T vs. computational cost
- Failure signatures: Rapid accuracy drop on previous tasks, exemplar sets becoming dominated by one class, uncertainty estimates becoming uninformative
- First 3 experiments:
  1. Single dataset class incremental (Matek-19 only) to verify basic functionality
  2. Two-dataset domain incremental (Matek-19 → INT-20) to test domain shift handling
  3. Full three-dataset domain and class incremental to evaluate long-term learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the uncertainty-aware sampling strategy perform compared to other uncertainty estimation methods like entropy-based sampling or variance-based sampling without dropout?
- Basis in paper: [inferred] The paper mentions using MC dropout for epistemic uncertainty estimation but doesn't compare it to alternative uncertainty methods
- Why unresolved: The paper only implements and evaluates one uncertainty estimation approach without benchmarking against alternatives
- What evidence would resolve it: Direct comparison experiments showing performance differences between MC dropout uncertainty, entropy-based sampling, and other uncertainty estimation methods on the same datasets

### Open Question 2
- Question: What is the optimal memory budget (K) for different class distributions and dataset sizes in continual learning scenarios?
- Basis in paper: [explicit] The paper mentions K=1000 for multi-dataset experiments but doesn't systematically study how optimal K varies with dataset characteristics
- Why unresolved: The paper only tests a few exemplar set sizes (125 and 1000) without exploring the full parameter space
- What evidence would resolve it: Systematic experiments varying K across different ranges and dataset compositions to identify optimal memory allocation strategies

### Open Question 3
- Question: How does the proposed method scale to longer task sequences and more classes in real-world clinical settings?
- Basis in paper: [inferred] The longest experiment tested 12 tasks, but clinical applications may require many more tasks over time
- Why unresolved: The paper doesn't test scalability beyond 12 tasks or explore performance degradation with longer task sequences
- What evidence would resolve it: Extended experiments with 50+ tasks and performance tracking over time to identify degradation patterns and potential mitigation strategies

## Limitations
- Dataset Specificity: Results are demonstrated only on three white blood cell datasets with specific class distributions and image characteristics
- Computational Overhead: MC dropout uncertainty estimation with multiple forward passes increases inference time significantly
- Memory Budget Sensitivity: Performance depends heavily on the exemplar set size K, but optimal values are not systematically explored

## Confidence
- High Confidence: The mechanism of using distillation loss to preserve previous knowledge is well-established in continual learning literature
- Medium Confidence: The uncertainty-driven exemplar selection strategy shows promise, but comparative ablation studies are lacking
- Low Confidence: Claims about equal sampling compensating for dataset imbalance need more rigorous validation

## Next Checks
1. Implement a version of the method with random exemplar selection to quantify the actual contribution of uncertainty-driven sampling to overall performance gains
2. Systematically vary the exemplar set size K across different task sequences to identify the minimum memory requirement for maintaining performance
3. Test the method on a non-medical image classification dataset (e.g., CIFAR-100 with domain shifts) to verify the approach's broader applicability beyond white blood cell classification