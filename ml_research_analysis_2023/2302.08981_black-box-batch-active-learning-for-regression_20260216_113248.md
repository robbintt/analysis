---
ver: rpa2
title: Black-Box Batch Active Learning for Regression
arxiv_id: '2302.08981'
source_url: https://arxiv.org/abs/2302.08981
tags:
- learning
- active
- methods
- batch
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a black-box batch active learning approach
  for regression tasks, addressing the limitation of existing white-box methods that
  rely on model gradients or embeddings. The key idea is to use empirical predictive
  covariance kernels based on model predictions from a small ensemble, enabling compatibility
  with a wide range of models including non-differentiable ones like random forests.
---

# Black-Box Batch Active Learning for Regression

## Quick Facts
- **arXiv ID:** 2302.08981
- **Source URL:** https://arxiv.org/abs/2302.08981
- **Reference count:** 39
- **Key outcome:** Proposes black-box batch active learning for regression using predictive covariance kernels from model ensembles

## Executive Summary
This paper introduces a black-box batch active learning approach for regression tasks that overcomes limitations of existing white-box methods requiring model gradients or embeddings. The key innovation is using empirical predictive covariance kernels based on predictions from a small ensemble, enabling compatibility with both differentiable models (like deep neural networks) and non-differentiable models (like random forests and gradient-boosted trees). The method extends state-of-the-art white-box techniques including BADGE, BAIT, and LCMD to black-box settings by leveraging Bayesian principles and kernel-based approaches.

## Method Summary
The method uses empirical predictive covariance kernels computed from model predictions of a small ensemble to enable batch active learning without requiring gradients or model embeddings. For differentiable models, the predictive covariance approximates the gradient kernel via first-order Taylor expansion around the maximum likelihood estimate. For non-differentiable models, a Bayesian hypothesis space formulation treats ensemble members as hypotheses with a multinomial distribution over them. The approach extends white-box batch active learning methods (BADGE, BAIT, LCMD) to black-box models by replacing gradient-based kernels with prediction-based kernels. Experiments use 15 regression datasets with ensemble sizes of 10 for deep learning models, default 100-tree random forests, and virtual ensembles of up to 20 members for gradient-boosted trees.

## Key Results
- Black-box methods perform competitively with or better than white-box approaches for deep learning models
- The approach works well for non-differentiable models like random forests and gradient-boosted trees
- LCMD and BADGE acquisition functions show particularly strong performance
- Results averaged over 20 trials on 15 UCI and OpenML regression datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The predictive covariance kernel approximates the gradient kernel for differentiable models via first-order Taylor expansion
- Mechanism: Using a Taylor expansion around the maximum likelihood estimate, the covariance between predictions approximates the gradient kernel which encodes Fisher information
- Core assumption: The model is differentiable and we can compute gradients around a maximum likelihood estimate
- Evidence anchors:
  - [abstract] "we show that the well-known gradient kernel can be seen as an approximation of this predictive covariance kernel"
  - [section 3.2.3] "The posterior gradient kernel kgrad→post(Dtrain)(xi; xj| ω∗) is an approximation of the predictive covariance kernel kpred(xi; xj)"
  - [corpus] Weak - no direct citations, but related work on kernel methods exists
- Break condition: If the model is highly non-linear or the MLE is far from the true parameter, the Taylor approximation breaks down

### Mechanism 2
- Claim: For non-differentiable models, a Bayesian hypothesis space formulation using ensemble members as hypotheses enables kernel-based active learning
- Mechanism: By treating each ensemble member as a hypothesis and using a multinomial distribution over them, we can compute predictive covariances that serve as kernels
- Core assumption: The ensemble members provide diverse enough predictions to capture uncertainty
- Evidence anchors:
  - [abstract] "This approach is compatible with a wide range of machine learning models including regular and Bayesian deep learning models and non-differentiable models such as random forests"
  - [section 3.2.4] "We now examine this model and its kernels" showing the derivation for non-differentiable models
  - [corpus] Weak - no direct citations, but query-by-committee literature exists
- Break condition: If ensemble members are too similar, predictive covariances will be small and active learning won't work

### Mechanism 3
- Claim: Batch active learning methods can be extended to black-box settings by replacing gradient-based kernels with prediction-based kernels
- Mechanism: Methods like BADGE, BAIT, and LCMD rely on kernels that measure similarity/diversity; replacing gradient kernels with predictive covariance kernels maintains their functionality while removing differentiability requirements
- Core assumption: The prediction-based kernel preserves the key properties (e.g., positive semi-definiteness) needed for the active learning method
- Evidence anchors:
  - [abstract] "This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE, BAIT, LCMD) to black-box models"
  - [section 3] "We extend the work of Houlsby et al. (2011) and Holzmüller et al. (2022) by combining the prediction-based approach with a kernel-based formulation"
  - [corpus] Weak - no direct citations, but kernel-based active learning literature exists
- Break condition: If the prediction kernel doesn't capture the same structure as the gradient kernel for a specific method, performance may degrade

## Foundational Learning

- Concept: Kernel methods and Gaussian processes
  - Why needed here: The paper builds on kernel-based formulations of active learning methods, using predictive covariances as kernels
  - Quick check question: What is the relationship between a Gaussian process and a kernel function?

- Concept: Bayesian model averaging and ensembles
  - Why needed here: The method relies on sampling from a distribution over models (either through Bayesian inference or ensembles) to estimate predictive covariances
  - Quick check question: How does Bayesian model averaging differ from simply training an ensemble?

- Concept: Information theory and mutual information
  - Why needed here: Active learning methods like BALD are based on maximizing mutual information between parameters and predictions
  - Quick check question: What is the difference between entropy and mutual information?

## Architecture Onboarding

- Component map: Ensemble predictions -> Predictive covariance computation -> Kernel-based acquisition function -> Greedy batch selection -> Label acquisition -> Retraining
- Critical path: Sample predictions → Compute predictive covariance kernel → Apply acquisition function → Select batch → Acquire labels → Retrain
- Design tradeoffs: Ensemble size vs. computational cost; prediction accuracy vs. kernel quality; batch size vs. diversity
- Failure signatures: Poor performance on datasets with low feature dimensionality; failure with virtual ensembles; degradation when ensemble members are too similar
- First 3 experiments:
  1. Compare uniform vs. active learning on a simple regression dataset with a small ensemble
  2. Test different ensemble sizes (5, 10, 20) on the same dataset to find the sweet spot
  3. Try the method on a non-differentiable model (e.g., random forest) to verify black-box capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of black-box batch active learning methods scale with the number of ensemble members, and is there an optimal ensemble size?
- Basis in paper: [explicit] The paper mentions using a small ensemble of 10 models for deep learning and states that this ensemble size is "sufficient to achieve good performance," but does not explore how performance scales with ensemble size
- Why unresolved: The paper does not provide experiments or analysis of how varying the ensemble size affects the performance of the black-box active learning methods
- What evidence would resolve it: Experiments showing the performance of black-box active learning methods across a range of ensemble sizes, ideally identifying an optimal ensemble size for different types of models and datasets

### Open Question 2
- Question: Can the black-box batch active learning approach be extended to handle heteroscedastic noise, and how would this affect its performance?
- Basis in paper: [explicit] The paper mentions that their approach could be extended to heteroscedastic noise by substituting a function for the noise variance, but does not explore this extension
- Why unresolved: The paper only evaluates the black-box approach on homoscedastic noise and does not investigate how it would perform or need to be modified for heteroscedastic noise
- What evidence would resolve it: Experiments comparing the performance of the black-box approach on both homoscedastic and heteroscedastic noise settings, potentially with modifications to handle the latter

### Open Question 3
- Question: How does the black-box batch active learning approach perform on high-dimensional feature spaces, and what are the computational implications?
- Basis in paper: [inferred] The paper mentions that the method scales in the number of drawn predictions rather than in the parameter space, which suggests potential advantages in high-dimensional settings, but does not provide experiments or analysis in this context
- Why unresolved: The paper does not explore the behavior of the black-box approach on high-dimensional datasets or discuss the computational implications of scaling with the number of predictions
- What evidence would resolve it: Experiments on high-dimensional datasets comparing the performance and computational efficiency of the black-box approach to other methods, particularly in terms of how it scales with the number of predictions and feature space dimensionality

## Limitations

- The theoretical connection between predictive covariance kernels and gradient kernels relies on a first-order Taylor approximation that may break down for highly non-linear models
- The method's effectiveness for non-differentiable models depends heavily on ensemble diversity, which is not guaranteed and may require careful tuning
- Virtual ensemble approach for gradient-boosted trees showed limitations when early stopping produces insufficient trees

## Confidence

- **High confidence**: The empirical results showing competitive performance with white-box methods on differentiable models (DNNs), supported by multiple datasets and trials
- **Medium confidence**: The extension to non-differentiable models (random forests, GBDTs), as the results are promising but the virtual ensemble approach for GBDTs showed limitations with insufficient trees
- **Medium confidence**: The theoretical claims about kernel equivalence, as the derivations are sound but the practical implications depend on specific model characteristics

## Next Checks

1. **Taylor approximation robustness test**: Systematically evaluate performance degradation across models with increasing non-linearity to quantify the limits of the predictive covariance approximation to gradient kernels

2. **Ensemble diversity quantification**: Measure and report the diversity of predictions across ensemble members for each dataset and model type, correlating this with active learning performance

3. **Virtual ensemble failure analysis**: For GBDTs specifically, analyze cases where early stopping produces insufficient trees and determine whether this systematically affects certain dataset characteristics or model configurations