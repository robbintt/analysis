---
ver: rpa2
title: 'CLIPood: Generalizing CLIP to Out-of-Distributions'
arxiv_id: '2302.00864'
source_url: https://arxiv.org/abs/2302.00864
tags:
- clip
- generalization
- classes
- domain
- clipood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CLIPood, a fine-tuning method to improve the
  out-of-distribution (OOD) generalization of CLIP models on downstream tasks. It
  introduces Margin Metric Softmax (MMS) with class adaptive margins to exploit semantic
  relations from text modality during fine-tuning.
---

# CLIPood: Generalizing CLIP to Out-of-Distributions

## Quick Facts
- arXiv ID: 2302.00864
- Source URL: https://arxiv.org/abs/2302.00864
- Reference count: 11
- Primary result: CLIPood fine-tuning with MMS and BMA improves CLIP OOD generalization across domain shifts and open class scenarios

## Executive Summary
CLIPood addresses the challenge of out-of-distribution (OOD) generalization for CLIP models by introducing two key innovations: Margin Metric Softmax (MMS) and Beta Moving Average (BMA). MMS incorporates semantic relations between classes from the text modality during fine-tuning by adding class-adaptive margins to the softmax objective. BMA maintains a temporal ensemble of models along the training trajectory, balancing pre-trained and fine-tuned knowledge. The method consistently outperforms existing generalization techniques across diverse datasets with domain shift and open class scenarios.

## Method Summary
CLIPood fine-tunes CLIP models for OOD generalization using a two-pronged approach. First, it employs Margin Metric Softmax (MMS), which adds class-adaptive margins to the metric softmax objective based on semantic distances between text embeddings of class names. This preserves the semantic structure learned during pre-training. Second, it uses Beta Moving Average (BMA) to maintain a temporal ensemble of models throughout training, weighted by a Beta distribution to balance zero-shot generalization and task-specific adaptation. The method freezes the text encoder during fine-tuning to prevent representation collapse while adapting the image encoder.

## Key Results
- CLIPood achieves higher accuracy than zero-shot CLIP and state-of-the-art generalization methods (COOP, COCOOP) on PACS, VLCS, OfficeHome, DomainNet, and ImageNet variants
- On domain-shifted data, CLIPood improves accuracy by 1.3% to 8.5% over baselines
- For open class scenarios, CLIPood maintains strong performance on both base and new classes, achieving higher harmonic mean scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMS preserves semantic relations between classes from the text modality by adding adaptive margins to the softmax objective.
- Mechanism: During fine-tuning, MMS adjusts softmax logits so that similarity to correct text embedding is pushed higher than incorrect embeddings by a margin proportional to semantic distance between classes.
- Core assumption: Pre-trained text encoder contains rich semantic relationships between class names that can be preserved and exploited during fine-tuning.
- Evidence anchors: Abstract states MMS introduces "class adaptive margins for fine-tuning" and section 3.3 explains "D(Ty, Tc) represents the distance between text embeddings... serves as an adaptive margin."
- Break condition: If text embeddings don't capture meaningful semantic relationships, margin terms may harm performance.

### Mechanism 2
- Claim: BMA balances pre-trained and fine-tuned knowledge by temporally ensembling models along training trajectory with Beta(β,β) prior.
- Mechanism: BMA maintains moving average of all previous checkpoints, weighted so models near pre-trained and fine-tuned endpoints receive higher importance.
- Core assumption: Pre-trained model has strong zero-shot generalization and fine-tuned model has task-specific knowledge; combining them yields better OOD performance.
- Evidence anchors: Abstract mentions BMA maintains "temporal ensemble according to Beta distribution" and section 3.4 explains Beta Temporal Ensemble with Beta(β,β) weighting.
- Break condition: If fine-tuning trajectory is noisy, ensembling could average over harmful checkpoints; low β may revert too much toward pre-trained weights.

### Mechanism 3
- Claim: Metric softmax preserves open-class nature of CLIP during fine-tuning.
- Mechanism: Comparing image embeddings to text embeddings of class names rather than learning separate classifier maintains alignment between modalities and allows natural generalization to unseen classes.
- Core assumption: CLIP embedding space is structured so nearest-neighbor classification in text embedding space is meaningful for both seen and unseen classes.
- Evidence anchors: Section 3.2 explains "we propose to perform vision-language fine-tuning on CLIP" and "freeze text encoder to avoid representation collapse."
- Break condition: If text encoder embeddings become misaligned during fine-tuning or prompts don't generalize to new classes, metric softmax may fail.

## Foundational Learning

- Concept: Contrastive learning in vision-language pre-training
  - Why needed here: CLIPood builds on CLIP contrastive objective; understanding image-text embedding alignment is essential.
  - Quick check question: What is the role of temperature τ in softmax similarity calculation, and how does it affect training dynamics?

- Concept: Domain generalization vs. open-set recognition
  - Why needed here: Paper tackles both distribution shifts and unseen classes, so distinguishing these scenarios is critical.
  - Quick check question: How does CLIPood handle test sample from completely new class versus known class in new domain?

- Concept: Temporal ensembling and moving averages in optimization
  - Why needed here: BMA is novel temporal ensembling scheme; knowing how EMA works and Beta weighting differs is key to tuning.
  - Quick check question: What is the effect of changing β in Beta distribution used for BMA weights?

## Architecture Onboarding

- Component map:
  - Image encoder (ViT-B/16, fine-tuned) -> Text encoder (frozen, generates class embeddings) -> MMS loss (adaptive margins based on text distances) -> BMA (temporal ensemble of checkpoints)

- Critical path:
  1. Load pre-trained CLIP model
  2. Freeze text encoder; set up prompt templates for each class
  3. At each training step: compute MMS loss, update image encoder, update BMA model
  4. After training, use BMA model for inference

- Design tradeoffs:
  - Freezing text encoder preserves semantic relations but prevents task-specific text adaptation
  - BMA adds negligible computation but requires storing one extra model; EMA would focus only on training end
  - MMS introduces hyperparameter λ; tuning may be dataset-dependent

- Failure signatures:
  - Degraded performance on base classes: BMA overemphasizing pre-trained weights or MMS margins too aggressive
  - Poor generalization to new classes: Text encoder embeddings collapsed or prompts not general enough
  - Overfitting to training domain: Insufficient BMA influence or MMS margins not preserving semantic structure

- First 3 experiments:
  1. Train CLIPood on PACS dataset with default settings; verify accuracy improves over zero-shot
  2. Run ablation: remove MMS, observe drop in OOD performance; confirm MMS is contributing
  3. Sweep β in BMA (0.1, 0.5, 0.9) and observe trade-off between base and new class accuracy

## Open Questions the Paper Calls Out
- The paper acknowledges that freezing the text encoder during fine-tuning may limit adaptation to task-specific text patterns, but doesn't explore when text encoder adaptation might be beneficial
- The impact of prompt engineering on OOD generalization is not systematically studied, though prompts are critical for metric softmax performance
- The method's effectiveness on non-classification tasks like object detection or segmentation remains unexplored

## Limitations
- Implementation details for Beta Moving Average computation and sampling procedure are underspecified
- Evaluation limited to classification tasks; performance on other vision tasks remains unknown
- Method's robustness to extreme distribution shifts or noisy text prompts not thoroughly tested

## Confidence
- High confidence in core mechanisms (MMS and BMA) and theoretical motivation
- Medium confidence in empirical results given controlled experimental setup
- Medium confidence in claims about semantic relation preservation, as this is difficult to verify directly

## Next Checks
1. **Ablation on MMS implementation:** Remove class-adaptive margin component from MMS and retrain on PACS dataset to confirm semantic margin is responsible for OOD gains
2. **BMA hyperparameter sensitivity:** Systematically sweep β values (0.1, 0.5, 0.9) and measure trade-off between base and new class accuracy to identify optimal settings
3. **Extreme domain shift test:** Evaluate CLIPood on challenging OOD scenario (synthetic-to-real shift or artistic style transfer) to probe limits of semantic margin preservation and BMA effectiveness