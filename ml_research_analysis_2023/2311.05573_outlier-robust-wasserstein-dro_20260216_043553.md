---
ver: rpa2
title: Outlier-Robust Wasserstein DRO
arxiv_id: '2311.05573'
source_url: https://arxiv.org/abs/2311.05573
tags:
- risk
- have
- wdro
- robust
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces outlier-robust Wasserstein distributionally\
  \ robust optimization (OR-WDRO), a framework that handles both geometric (Wasserstein)\
  \ perturbations and non-geometric (total variation) contamination in data. The method\
  \ uses a robust Wasserstein ball that accounts for both perturbation types, enabling\
  \ effective decision-making even when an \u03B5-fraction of data is arbitrarily\
  \ corrupted."
---

# Outlier-Robust Wasserstein DRO

## Quick Facts
- arXiv ID: 2311.05573
- Source URL: https://arxiv.org/abs/2311.05573
- Authors: 
- Reference count: 40
- Primary result: Framework handles both geometric (Wasserstein) and non-geometric (TV) contamination with minimax optimal excess risk bounds

## Executive Summary
This paper introduces Outlier-Robust Wasserstein Distributionally Robust Optimization (OR-WDRO), a framework that simultaneously addresses geometric perturbations (modeled by Wasserstein distance) and non-geometric contamination (modeled by total variation distance). The method uses a robust Wasserstein ball that filters out the ε-fraction of mass from the contaminated distribution that contributed most to the transportation cost, then intersects with moment assumptions on the uncorrupted data. The framework provides explicit excess risk bounds that capture both Wasserstein and TV risks, along with tractable convex reformulations through a strong duality theorem.

## Method Summary
OR-WDRO replaces the standard Wasserstein ambiguity set with one based on the outlier-robust Wasserstein distance Wε_p, which filters out the ε-fraction of mass from the contaminated distribution that contributed most to transportation cost. The ambiguity set is defined as the intersection of this robust Wasserstein ball and a set encoding moment assumptions (bounded covariance) on the uncorrupted data distribution. Under convexity conditions on the loss function, the framework admits a tractable convex reformulation that can be efficiently computed using YALMIP and Gurobi solvers.

## Key Results
- Provides minimax optimal excess risk bounds that explicitly capture both Wasserstein and TV risks
- Derives strong duality theorem enabling tractable convex reformulations
- Demonstrates superior performance over classical WDRO on standard regression and classification tasks when observations are subject to adversarial contamination
- Shows bounds improve when loss function depends only on low-dimensional features of data

## Why This Works (Mechanism)

### Mechanism 1
The outlier-robust WDRO framework enables effective decision-making under both geometric and non-geometric perturbations by using a robust Wasserstein ball that accounts for both perturbation types. The framework uses a robust Wasserstein distance that filters out the ε-fraction of mass from the contaminated distribution that contributed most to the transportation cost, then measures the Wp distance post-filtering. This is intersected with a set encoding moment assumptions on the uncorrupted data distribution. The core assumption is that the true data distribution belongs to a class G encoding distributional assumptions (e.g., bounded covariance), and the observed data distribution is within Wε_p distance ρ of the true distribution.

### Mechanism 2
The framework provides minimax optimal excess risk bounds that explicitly capture the effects of both Wasserstein and TV risks. The excess risk bounds are derived using the Wp regularizer Rµ,p(ρ; ℓ) = supν′∈P(Z):Wp(ν′,ν)≤ρ Eν′[ℓ] − Eν[ℓ], which controls the gap between the expected loss under the true distribution and the optimal loss. The bounds decompose into Wasserstein and TV components. The core assumption is that the loss function ℓ⋆ = argminℓ∈L Eµ[ℓ] has low variational complexity (e.g., Lipschitz or Sobolev seminorms).

### Mechanism 3
The framework admits tractable convex reformulations and efficient computation through a strong duality theorem. The inner maximization problem of the outlier-robust WDRO problem can be simplified to a minimization problem involving only two scalars (λ1, λ2) and a decision variable α, using a strong duality result. Under additional convexity conditions on the loss, this leads to an efficiently-computable, finite-dimensional, convex reformulation. The core assumption is that the loss function satisfies certain convexity conditions (e.g., pointwise maximum of finitely many concave functions).

## Foundational Learning

- Concept: Wasserstein distance and its properties
  - Why needed here: The framework is based on the Wasserstein distance, which captures geometric uncertainties due to sampling or localized perturbations of data points. Understanding its properties is crucial for deriving the excess risk bounds and the strong duality result.
  - Quick check question: What are the key properties of the Wasserstein distance that make it suitable for modeling geometric uncertainties in data?

- Concept: Total variation (TV) distance and contamination models
  - Why needed here: The framework also accounts for non-geometric perturbations such as adversarial outliers, which are modeled using the TV distance. Understanding the TV distance and contamination models is essential for deriving the excess risk bounds and the outlier-robust Wasserstein distance.
  - Quick check question: How does the TV contamination model differ from geometric perturbations, and why is it important to consider both in robust optimization?

- Concept: Distributionally robust optimization (DRO) and its variants
  - Why needed here: The framework is a variant of DRO that seeks to learn a model that performs uniformly well over an ambiguity set centered around the observed data distribution. Understanding the DRO framework and its variants is crucial for deriving the excess risk bounds and the strong duality result.
  - Quick check question: What is the main idea behind DRO, and how does it differ from classical optimization approaches?

## Architecture Onboarding

- Component map: Contaminated data → Outlier-robust Wasserstein distance Wε_p → Ambiguity set (Wε_p ball ∩ moment constraints) → Strong duality → Convex reformulation → Optimal decision

- Critical path:
  1. Preprocess the observed data to obtain a robust estimate of the mean (e.g., using coordinate-wise trimmed mean)
  2. Define the ambiguity set using the outlier-robust Wasserstein ball and the moment assumptions
  3. Compute the excess risk bounds using the Wp regularizer and the distributional assumptions
  4. Apply the strong duality theorem to obtain a tractable convex reformulation
  5. Solve the convex reformulation to obtain the optimal decision

- Design tradeoffs:
  - Choice of p: The order of the Wasserstein distance (p) affects the excess risk bounds and the computational complexity. Higher p may lead to tighter bounds but increased computational cost.
  - Choice of G: The class G encoding distributional assumptions affects the excess risk bounds and the feasibility of the ambiguity set. Stronger assumptions may lead to tighter bounds but may be harder to verify in practice.
  - Choice of convexity conditions: The convexity conditions on the loss function affect the tractability of the convex reformulation. Stronger conditions may lead to more efficient computation but may be harder to satisfy in practice.

- Failure signatures:
  - Excess risk bounds are loose or violated: This may indicate that the distributional assumptions encoded in G are violated, or that the loss function has high variational complexity.
  - Convex reformulation is infeasible or computationally expensive: This may indicate that the convexity conditions on the loss function are not satisfied, or that the dimensionality is too high for efficient computation.
  - Outlier-robust Wasserstein distance is not well-defined or ill-behaved: This may indicate that the contamination fraction ε is too large, or that the Wasserstein distance is not well-suited for the problem at hand.

- First 3 experiments:
  1. Implement the outlier-robust WDRO framework for a simple linear regression problem with synthetic data, and compare the excess risk bounds to those of standard WDRO.
  2. Apply the framework to a real-world dataset (e.g., from UCI Machine Learning Repository) with known outliers, and evaluate the performance of the learned model on a held-out test set.
  3. Experiment with different choices of p, G, and convexity conditions, and analyze their impact on the excess risk bounds and computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
Can the excess risk bounds for outlier-robust WDRO be improved under additional distributional assumptions beyond bounded covariance, such as sub-Gaussianity or strong log-concavity?
- Basis in paper: Explicit - The paper mentions that "Distinct trade-offs are observed under stronger tail bounds like sub-Gaussianity" and discusses GsubG class, but focuses primarily on Gcov for theoretical results.
- Why unresolved: The paper primarily develops theory for Gcov class and only briefly mentions stronger tail assumptions without providing detailed analysis or improved bounds for these cases.
- What evidence would resolve it: Formal proofs showing improved excess risk bounds under T1 or log-Sobolev inequalities, or experimental validation showing better performance when such assumptions hold.

### Open Question 2
How does the choice of Wasserstein radius ρ impact the excess risk in practice, and is there an efficient method to select this parameter without prior knowledge of the contamination level?
- Basis in paper: Explicit - The paper discusses parameter tuning in Appendix F and mentions that "in practice, ε, ρ0, and the relevant tail bound may be unknown" but provides only theoretical bounds for parameter selection.
- Why unresolved: While the paper provides theoretical guidance for parameter selection, it doesn't offer a practical algorithm or experimental validation of parameter tuning strategies in real-world scenarios.
- What evidence would resolve it: Empirical studies comparing different parameter selection methods, or a theoretically justified adaptive algorithm for choosing ρ based on the data.

### Open Question 3
Can the outlier-robust WDRO framework be extended to handle more complex contamination models beyond the Huber contamination model, such as asymmetric contamination or contamination that depends on the feature values?
- Basis in paper: Explicit - The paper states "we allow both localized Wasserstein perturbations, that map µ to some µ′ with Wp(µ, µ′) ≤ ρ, and TV ε-contamination that takes µ′ to ˜µ with ∥˜µ − µ′∥TV ≤ ε" but focuses on symmetric TV contamination.
- Why unresolved: The paper's theoretical analysis and experiments are limited to the standard Huber contamination model, without exploring more complex or realistic contamination scenarios.
- What evidence would resolve it: Theoretical analysis of excess risk bounds under asymmetric contamination, or experiments showing the framework's performance under different contamination models.

## Limitations
- Theoretical guarantees depend critically on distributional assumptions encoded in class G being satisfied by the true data distribution
- Computational complexity scales poorly with dimensionality, requiring solving non-smooth convex programs
- Performance critically depends on appropriate selection of robustness radius ρ and contamination level ε, which are typically unknown in practice

## Confidence
- High: Theoretical derivations for outlier-robust Wasserstein distance, strong duality theorem, and excess risk bounds under stated assumptions
- Medium: Practical applicability to real-world datasets given sensitivity to parameter selection and assumption validity
- Low: Scalability claims for high-dimensional problems due to limited empirical evidence

## Next Checks
1. Test the framework on synthetic data where the distributional assumptions encoded in G are intentionally violated to assess sensitivity
2. Develop and evaluate systematic approaches for selecting ρ and ε when these values are unknown in practice
3. Conduct systematic experiments across varying dimensionalities to quantify the computational burden and identify practical limits of the approach