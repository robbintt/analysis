---
ver: rpa2
title: 'DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification'
arxiv_id: '2307.09813'
source_url: https://arxiv.org/abs/2307.09813
tags:
- event
- prompt
- causal
- causality
- daprompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Event Causality Identification
  (ECI), which aims to determine whether there is a causal relation between two event
  mentions in text. The authors propose a novel prompt learning approach called DAPrompt
  that differs from conventional methods by first making a deterministic assumption
  about the existence of a causal relation and then evaluating its rationality, rather
  than directly predicting an answer word.
---

# DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification

## Quick Facts
- **arXiv ID**: 2307.09813
- **Source URL**: https://arxiv.org/abs/2307.09813
- **Reference count**: 17
- **Primary result**: Achieves F1 scores of 62.1% and 65.9% on EventStoryLine and Causal-TimeBank datasets respectively

## Executive Summary
This paper addresses the Event Causality Identification (ECI) task, which determines whether a causal relation exists between two event mentions in text. The authors propose DAPrompt, a novel prompt learning approach that differs from conventional methods by first making a deterministic assumption about the existence of a causal relation and then evaluating its rationality. Instead of directly predicting answer words, the model uses a simple template with two mask tokens to predict the input events, and uses the probabilities of correct predictions to decide whether to accept or reject the causal assumption. Experiments on benchmark datasets show that DAPrompt significantly outperforms state-of-the-art algorithms.

## Method Summary
DAPrompt is a deterministic assumption prompt learning approach for Event Causality Identification. The method uses a simple template with two mask tokens to represent event mentions, allowing for consistent mask prediction regardless of event mention length or vocabulary differences. The model first assumes a causal relation exists between two events, then evaluates this assumption by predicting the input events themselves using mask tokens. The probability of correctly predicting the events determines whether to accept or reject the assumption. A joint decision threshold on the sum of two event prediction probabilities provides a flexible and robust acceptance criterion. The model is trained using AdamW optimizer with L2 regularization, mini-batch size of 16, and learning rate of 1e-5.

## Key Results
- Achieves F1 scores of 62.1% on EventStoryLine corpus, representing a substantial improvement over previous methods
- Achieves F1 scores of 65.9% on Causal-TimeBank corpus, demonstrating strong performance across datasets
- Outperforms state-of-the-art algorithms by significant margins in both precision and recall metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing the traditional prompt template answer prediction with a deterministic assumption and rationality evaluation improves ECI performance.
- **Mechanism:** The model first assumes a causal relation exists between two events, then evaluates this assumption by predicting the input events themselves using mask tokens. The probability of correctly predicting the events determines whether to accept or reject the assumption.
- **Core assumption:** Predicting the input events directly from a deterministic assumption template can better utilize the PLM's encyclopedic knowledge than traditional answer word prediction.
- **Evidence anchors:**
  - [abstract] "Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption."
  - [section 1] "We argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption."

### Mechanism 2
- **Claim:** Using virtual event tokens (<E1>, <E2>) to represent event mentions allows for consistent mask prediction regardless of event mention length or vocabulary differences.
- **Mechanism:** Instead of predicting different words for different event mentions, the model uses two virtual tokens that can be predicted from any event mention context, normalizing the representation.
- **Core assumption:** A simple virtual token can adequately represent diverse event mentions for mask prediction purposes.
- **Evidence anchors:**
  - [section 3.1] "The design consideration is from the fact that the event mentions in different raw sentences usually consist of much different vocabulary words, not to mention having different lengths. We need to simplify and regulate their representations."
  - [section 5.2] "This validates our design of using virtual event tokens of <E1> and <E2> to for events' representations."

### Mechanism 3
- **Claim:** The joint decision threshold on the sum of two event prediction probabilities provides a flexible and robust acceptance criterion for causal assumptions.
- **Mechanism:** Instead of requiring both events to be predicted with high confidence individually, the model accepts the causal assumption if their combined probability exceeds a threshold, allowing for compensation between events.
- **Core assumption:** The sum of individual event prediction probabilities is a meaningful metric for evaluating the overall rationality of the causal assumption.
- **Evidence anchors:**
  - [section 3.3] "We use the sum of P1 and P2 as a joint decision variable for rationality evaluation of the deterministic assumption"
  - [section 5.1] "Fig. 3 (b) compares the overall performance... It can be observed that DAPrompt achieves nearly the same F1 score within a large range of the decision threshold"

## Foundational Learning

- **Concept: Masked Language Model (MLM) probability estimation**
  - Why needed here: The model relies on MLM classifiers to estimate the probability of mask tokens being the virtual event tokens, which is the core mechanism for rationality evaluation.
  - Quick check question: How does the MLM classifier estimate the probability of a mask token being a specific vocabulary word?

- **Concept: Pre-trained Language Model (PLM) fine-tuning strategies**
  - Why needed here: The model fine-tunes the PLM parameters along with MLM classifiers during training, requiring understanding of how to adapt pre-trained models to new tasks.
  - Quick check question: What are the key considerations when fine-tuning a PLM for a new task like event causality identification?

- **Concept: Prompt learning paradigm and template design**
  - Why needed here: The model uses a novel deterministic assumption template design, requiring understanding of how prompt templates influence model behavior and task transformation.
  - Quick check question: How does the choice of prompt template words and structure affect the PLM's output for a given task?

## Architecture Onboarding

- **Component map:**
  - Event mention extraction → Virtual token insertion → Template construction → PLM processing → MLM probability estimation → Joint probability calculation → Threshold comparison → Causality decision

- **Critical path:** Event mention → Virtual token insertion → Template construction → PLM processing → MLM probability estimation → Joint probability calculation → Threshold comparison → Causality decision

- **Design tradeoffs:**
  - Using virtual tokens simplifies mask prediction but may lose fine-grained event information
  - Joint threshold provides flexibility but requires careful threshold tuning
  - Deterministic assumption design is novel but may not generalize to all ECI scenarios

- **Failure signatures:**
  - Low precision: Threshold too low, accepting too many false causal assumptions
  - Low recall: Threshold too high, rejecting too many true causal assumptions
  - Inconsistent performance: Virtual tokens not learning meaningful representations across diverse event contexts

- **First 3 experiments:**
  1. Test different decision thresholds (0.2 to 1.8) to find optimal balance between precision and recall
  2. Compare performance using individual vs. joint decision thresholds to validate the compensation mechanism
  3. Evaluate different PLM backbones (BERT, RoBERTa, DeBERTa) to assess the impact of pre-training strategies on ECI performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions, but several important questions arise from the work:

### Open Question 1
- Question: How does the DAPrompt model handle event causality identification in cases where the events are not explicitly mentioned in the text but are implied or inferred from context?
- Basis in paper: [inferred] The paper discusses the use of a deterministic assumption template and virtual event tokens to represent events, but does not explicitly address cases where events are not directly mentioned in the text.
- Why unresolved: The model's effectiveness in handling implicit or inferred events is not evaluated or discussed in the paper.
- What evidence would resolve it: Experiments comparing the model's performance on datasets containing both explicit and implicit events, or modifications to the model to explicitly handle implied events.

### Open Question 2
- Question: Can the DAPrompt approach be extended to identify causal relationships between more than two events, such as in complex causal chains or networks?
- Basis in paper: [inferred] The paper focuses on identifying causal relations between pairs of events, but does not explore the model's capability in handling more complex causal structures involving multiple events.
- Why unresolved: The model's architecture and evaluation are designed for pairwise event causality, and there is no discussion of its potential extension to multi-event scenarios.
- What evidence would resolve it: Modifications to the model to handle multiple events and experiments demonstrating its performance on datasets with complex causal structures.

### Open Question 3
- Question: How does the DAPrompt model's performance compare to traditional machine learning approaches that use handcrafted features or domain-specific knowledge for event causality identification?
- Basis in paper: [explicit] The paper compares DAPrompt to several recent deep learning-based methods but does not include comparisons with traditional machine learning approaches that rely on handcrafted features or domain-specific knowledge.
- Why unresolved: The paper focuses on comparing with recent deep learning methods but does not explore how DAPrompt performs against traditional approaches that might use different feature engineering or domain expertise.
- What evidence would resolve it: Experiments comparing DAPrompt to traditional machine learning models using handcrafted features or domain-specific knowledge on the same datasets.

## Limitations
- The deterministic assumption approach may not generalize well to all ECI scenarios, particularly those involving complex or ambiguous event pairs
- The use of virtual tokens to represent diverse event mentions may oversimplify the task and potentially lose important semantic information
- The joint decision threshold mechanism introduces sensitivity to threshold selection, requiring additional tuning in practical applications

## Confidence

- **High Confidence:** The core mechanism of using deterministic assumptions and rationality evaluation for ECI is well-supported by the experimental results and ablation studies. The significant improvements in F1 scores on both benchmark datasets provide strong evidence for the method's effectiveness.

- **Medium Confidence:** The design choices for virtual event tokens and joint decision thresholds are well-motivated by the experimental results, but their generalizability to other domains or languages remains to be fully validated.

- **Low Confidence:** The long-term stability and scalability of the method when applied to larger, more diverse corpora or real-world applications has not been thoroughly evaluated.

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate DAPrompt on additional ECI datasets from different domains (e.g., biomedical, financial) to assess its performance and robustness across varied event types and contexts.

2. **Threshold Sensitivity Analysis:** Conduct a comprehensive analysis of the joint decision threshold's impact on precision-recall trade-offs across multiple datasets to establish guidelines for optimal threshold selection in different scenarios.

3. **Error Analysis and Case Studies:** Perform detailed error analysis on the model's predictions to identify specific failure modes and edge cases, providing insights for potential improvements and extensions of the deterministic assumption approach.