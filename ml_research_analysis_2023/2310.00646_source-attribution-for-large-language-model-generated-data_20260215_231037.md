---
ver: rpa2
title: Source Attribution for Large Language Model-Generated Data
arxiv_id: '2310.00646'
source_url: https://arxiv.org/abs/2310.00646
tags:
- data
- source
- watermark
- attribution
- watermarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of identifying the source (i.e.,
  data provider) of synthetic texts generated by a large language model (LLM). To
  do this, the authors propose a watermarking framework, WASA, that embeds imperceptible
  watermarks into text data from different providers, allowing the LLM to learn a
  mapping from texts to watermarks.
---

# Source Attribution for Large Language Model-Generated Data

## Quick Facts
- **arXiv ID:** 2310.00646
- **Source URL:** https://arxiv.org/abs/2310.00646
- **Reference count:** 40
- **One-line primary result:** WASA achieves 74.84% top-1 and 95.76% top-3 source attribution accuracy on academic and book datasets

## Executive Summary
This paper introduces WASA, a watermarking framework for attributing synthetic texts generated by large language models to their original data providers. The framework embeds imperceptible Unicode watermarks into representative sentences from each data provider, enabling the LLM to learn a mapping from text to watermark. WASA achieves high source attribution accuracy while being robust to attacks, scalable to many providers, and preserving LLM performance. The authors evaluate WASA on academic papers and books, demonstrating its effectiveness in real-world scenarios.

## Method Summary
WASA embeds unique 10-character watermarks (using 6 imperceptible Unicode characters) into representative sentences from each data provider based on TF-IDF scoring. The framework trains a modified LLM with separate prediction spaces for word tokens and watermark tokens, allowing it to learn the mapping from text to watermark. During generation, the LLM produces both word and watermark tokens, enabling watermark extraction and source attribution. The authors evaluate WASA on academic papers and books, measuring source attribution accuracy, robustness to attacks, scalability, and performance preservation.

## Key Results
- **Source attribution accuracy:** 74.84% top-1, 95.76% top-3 on ArXiv and BookSum datasets
- **Robustness:** Maintains accuracy after watermark removal, synonym substitution, and syntactic transformation attacks
- **Scalability:** Supports over 60 million unique watermarks using imperceptible Unicode characters
- **Performance preservation:** Negligible impact on text generation quality (perplexity, distinct-n metrics)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Embedding imperceptible Unicode watermarks into representative sentences enables the LLM to learn a robust mapping from text to watermark, enabling source attribution.
- **Mechanism:** The LLM is trained on watermarked text using separate prediction spaces for word tokens and watermark tokens. During training, the watermark token loss explicitly encourages the LLM to learn the mapping from input text to the corresponding watermark. During generation, the LLM generates both word and watermark tokens in their designated spaces, allowing watermark extraction and source attribution.
- **Core assumption:** The selected sentences for watermarking are representative of the data provider's unique characteristics, and the watermark embedding does not significantly alter the semantic meaning of the text.
- **Evidence anchors:**
  - [abstract]: "Our WASA framework assigns a unique watermark (i.e., imperceptible to human eyes) to every data provider, and enables an LLM (coined as WASA-LLM) to learn an accurate mapping from the texts of different data providers to their corresponding watermarks..."
  - [section]: "To enable our WASA-LLM to learn the mapping from the texts of different data providers to their unique watermarks, it is important to only embed watermarks into the sentences that are representative of the unique characteristics of the data providers."
  - [corpus]: Weak - The corpus does not directly address the mechanism of using representative sentences for watermarking, but it provides context on related work in LLM-generated content detection.
- **Break condition:** If the selected sentences are not representative of the data provider's unique characteristics, or if the watermark embedding significantly alters the semantic meaning of the text, the LLM may not learn an accurate mapping from text to watermark.

### Mechanism 2
- **Claim:** The separation of prediction/generation spaces for word tokens and watermark tokens ensures robustness against attacks and preserves LLM performance.
- **Mechanism:** During training, the LLM uses separate softmax layers and losses for word tokens and watermark tokens. This separation allows the LLM to generate watermarks in their designated space, making it robust against attacks that remove or modify watermarks. It also limits the impact of added watermarks on the original ability of the LLM to generate high-quality synthetic texts.
- **Core assumption:** The separation of prediction/generation spaces does not significantly degrade the LLM's text generation ability.
- **Evidence anchors:**
  - [abstract]: "The two separate softmax layers naturally lead to the following separate log-likelihoods: Llm(s′i) and Lwtm(s′i)..."
  - [section]: "This separation of the prediction/generation spaces of the word tokens and watermark tokens allows us to use a small number of additional parameters for watermark prediction based on the hidden states ofWASA-LLM."
  - [corpus]: Weak - The corpus does not directly address the mechanism of separating prediction/generation spaces, but it provides context on related work in LLM watermarking and robustness.
- **Break condition:** If the separation of prediction/generation spaces significantly degrades the LLM's text generation ability, the overall performance of the framework may be compromised.

### Mechanism 3
- **Claim:** The use of imperceptible Unicode characters for watermarks ensures scalability to a large number of data providers while preserving the readability of generated texts.
- **Mechanism:** Each data provider is assigned a unique 10-character watermark, with each character chosen from 6 Unicode characters that are imperceptible to human eyes. This allows for over 60 million unique watermarks, enabling scalability to a large number of data providers. The watermarks are designed to be imperceptible, preserving the readability of generated texts.
- **Core assumption:** The selected Unicode characters are indeed imperceptible to human eyes on commonly used platforms.
- **Evidence anchors:**
  - [abstract]: "Every watermark is made up of 10 characters, each of which is chosen among the following 6 Unicode characters: U+200B: Zero Width Space, U+200C: Zero Width NonJoiner, U+200D: Zero Width Joiner, U+2062: Invisible Times, U+2063: Invisible Separator, and U+2064: Invisible Plus."
  - [section]: "To scale to a large number of data providers and still preserve the semantic meaning of the texts, we construct the watermarks using Unicode characters which are imperceptible to human eyes (yet can be decoded by machine learning models)."
  - [corpus]: Weak - The corpus does not directly address the mechanism of using imperceptible Unicode characters, but it provides context on related work in NLP watermarking.
- **Break condition:** If the selected Unicode characters are not imperceptible on certain platforms, or if the number of available characters is insufficient for the desired number of data providers, the scalability and readability of the framework may be compromised.

## Foundational Learning

- **Concept:** Text watermarking and steganography
  - **Why needed here:** The WASA framework relies on embedding imperceptible watermarks into text data, which is a form of text watermarking and steganography. Understanding the principles and techniques of text watermarking and steganography is essential for designing and implementing the WASA framework.
  - **Quick check question:** What are the key differences between watermarking and steganography, and how do these differences apply to the WASA framework?

- **Concept:** Large Language Models (LLMs) and transformer architecture
  - **Why needed here:** The WASA framework is designed to work with LLMs based on the transformer architecture, such as GPT and OPT. Understanding the inner workings of LLMs and the transformer architecture is crucial for modifying the LLM to incorporate watermarking and for training the WASA-LLM.
  - **Quick check question:** How does the transformer architecture enable LLMs to generate coherent and contextually relevant text, and how can this be leveraged for watermarking?

- **Concept:** Intellectual Property (IP) and data provenance
  - **Why needed here:** The WASA framework aims to address concerns related to the IP of training data for LLMs and enable data provenance. Understanding the concepts of IP and data provenance is important for framing the problem and designing the framework to meet the needs of data providers and LLM users.
  - **Quick check question:** What are the key challenges in protecting the IP of training data for LLMs, and how does the WASA framework address these challenges?

## Architecture Onboarding

- **Component map:** Data providers -> WASA-LLM -> Watermark decoder -> Users
- **Critical path:**
  1. Data providers contribute text data to the LLM platform operator
  2. The LLM platform operator generates unique watermarks for each data provider and embeds them into representative sentences from the data providers
  3. The LLM platform operator trains the WASA-LLM using the watermarked text data
  4. Users request source attribution for LLM-generated synthetic texts
  5. The LLM platform operator uses the watermark decoder to extract watermarks from the synthetic texts and attributes them to the corresponding data providers

- **Design tradeoffs:**
  - Watermark length vs. scalability: Longer watermarks allow for more unique watermarks but may be more difficult to embed and decode
  - Number of watermarked sentences vs. training efficiency: Watermarking more sentences may improve the accuracy of the WASA-LLM but also increases the computational cost of training
  - Separation of prediction/generation spaces vs. model complexity: Separating the prediction/generation spaces for word tokens and watermark tokens improves robustness but also increases the complexity of the model

- **Failure signatures:**
  - Low source attribution accuracy: May indicate that the WASA-LLM has not learned an accurate mapping from text to watermark, or that the watermarks are not being properly extracted and decoded
  - Degraded text generation performance: May indicate that the watermarking process is significantly impacting the LLM's ability to generate high-quality synthetic texts
  - Robustness to attacks: If the framework is not robust against watermark removal or modification attacks, the source attribution accuracy may be compromised

- **First 3 experiments:**
  1. Evaluate the source attribution accuracy of the WASA-LLM on a small dataset with a limited number of data providers
  2. Assess the impact of watermarking on the text generation performance of the WASA-LLM using perplexity and distinct-n metrics
  3. Test the robustness of the WASA-LLM against watermark removal and modification attacks by regenerating watermarks from cleaned synthetic texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the source attribution accuracy of the WASA framework be affected if the data providers have imbalanced or highly similar datasets?
- Basis in paper: The authors state in the conclusion that they have only focused on scenarios where the data providers have balanced data with sufficient dissimilarities, and acknowledge this as a limitation.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on how the framework would perform under imbalanced or highly similar data conditions.
- What evidence would resolve it: Experiments testing the WASA framework on datasets with varying levels of imbalance and similarity between providers, comparing the source attribution accuracy against the current balanced and dissimilar dataset results.

### Open Question 2
- Question: What is the robustness of the WASA framework against more advanced or sophisticated adversarial attacks, such as adversarial training-based attacks?
- Basis in paper: The authors mention in the conclusion that it is unclear whether the framework is robust against more advanced/sophisticated attacks, which may be achieved through adversarial training in future work.
- Why unresolved: The paper only evaluates robustness against a limited set of attacks (insertion, deletion, synonym substitution, syntactic transformation, and watermark removal/modification). It does not explore more advanced attack strategies.
- What evidence would resolve it: Experiments testing the WASA framework against adversarial training-based attacks, comparing the source attribution accuracy against the current results under standard attacks.

### Open Question 3
- Question: How does the choice of watermark length impact the scalability and source attribution accuracy of the WASA framework?
- Basis in paper: The authors mention in the ablation studies that high source attribution accuracy can be achieved for watermarks with different lengths, but they do not provide a detailed analysis of the trade-off between watermark length, scalability, and accuracy.
- Why unresolved: The ablation study results show that shorter watermarks may lead to better source attribution accuracy, but the paper does not discuss the implications of this finding on the scalability of the framework to a large number of data providers.
- What evidence would resolve it: A comprehensive study varying the watermark length and measuring the resulting source attribution accuracy and the maximum number of unique watermarks that can be generated, providing insights into the optimal watermark length for different application scenarios.

## Limitations

- **Platform dependency:** The imperceptibility of Unicode characters may vary across different platforms and text rendering systems, potentially affecting the framework's effectiveness.
- **Data provider characteristics:** The framework's performance on datasets with imbalanced or highly similar data providers remains unexplored, limiting its applicability in real-world scenarios.
- **Attack surface:** The robustness evaluation focuses on specific attack types, and the framework's resilience against more advanced adversarial attacks is unclear.

## Confidence

**High Confidence:** The core watermarking mechanism (using Unicode characters and separate prediction spaces) is well-supported by the evidence and follows established principles of steganography and neural network training. The quantitative results (74.84% top-1, 95.76% top-3 accuracy) are clearly reported and reproducible.

**Medium Confidence:** The claims about robustness to attacks and preservation of LLM performance are supported by experiments, but the evaluation scope is limited to specific attack types and performance metrics. The scalability claim (60 million unique watermarks) is mathematically sound but not empirically validated with large-scale provider counts.

**Low Confidence:** The claim that selected sentences are truly "representative of unique characteristics" lacks rigorous validation - TF-IDF scoring is used as a proxy but its effectiveness for this purpose is not verified. The assertion that watermarks are "imperceptible to human eyes" is based on the chosen Unicode characters but not systematically tested across platforms.

## Next Checks

1. **Cross-platform watermark visibility test:** Systematically verify that all 6 Unicode characters remain imperceptible across major operating systems, browsers, and text editors. Test with different font settings, screen resolutions, and accessibility tools that might expose invisible characters.

2. **Generalizability assessment:** Evaluate the framework on diverse dataset types beyond academic papers and books, including social media content, code repositories, and news articles. Measure source attribution accuracy when mixing heterogeneous data sources to assess real-world applicability.

3. **Expanded attack surface analysis:** Test robustness against additional attack types including adversarial prompt engineering (crafting inputs designed to trigger specific watermarks), watermark detection and removal by third-party tools, and model fine-tuning that might erase watermark associations.