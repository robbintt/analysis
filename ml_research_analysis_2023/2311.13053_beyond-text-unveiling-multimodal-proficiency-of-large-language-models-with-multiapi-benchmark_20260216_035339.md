---
ver: rpa2
title: 'Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with
  MultiAPI Benchmark'
arxiv_id: '2311.13053'
source_url: https://arxiv.org/abs/2311.13053
tags:
- function
- llms
- multimodal
- tasks
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiAPI, a novel benchmark dataset designed
  to evaluate the multimodal capabilities of Large Language Models (LLMs). The dataset
  comprises 235 diverse API functions and 2,038 contextual prompts, covering nine
  domains such as text-to-image, object detection, and image classification.
---

# Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark

## Quick Facts
- arXiv ID: 2311.13053
- Source URL: https://arxiv.org/abs/2311.13053
- Authors: [Not specified in input]
- Reference count: 5
- Primary result: MultiAPI benchmark reveals LLMs struggle with multimodal domain identification, function selection, and argument generation despite strong API call decision-making abilities

## Executive Summary
This paper introduces MultiAPI, a novel benchmark designed to evaluate the multimodal capabilities of Large Language Models (LLMs) through API function invocation tasks. The dataset comprises 235 diverse API functions and 2,038 contextual prompts across nine domains including text-to-image, object detection, and image classification. Through comprehensive experiments, the study reveals that while LLMs demonstrate proficiency in deciding which APIs to call, they face significant challenges in domain identification, function selection, and argument generation. Surprisingly, the inclusion of auxiliary context can impair rather than improve performance. To address these limitations, the authors propose a solution involving domain description prompting and argument revision, which significantly improves model performance.

## Method Summary
The MultiAPI benchmark was created by curating API functions from platforms like OpenAI, HuggingFace, and ML-Space, then manually refining function descriptions and arguments for clarity. The dataset was encapsulated to remove duplicate functionalities and standardized for consistent argument formatting. Experiments were conducted on both API-based models (GPT-3.5) and open-source models (Llama2-13B) using various prompt configurations including in-context learning, chain-of-thought, and function calling approaches. Performance was evaluated across four levels: invocation accuracy, domain match, function match, and argument match (using exact string matching for file paths and semantic similarity metrics for prompts).

## Key Results
- LLMs demonstrate strong API call decision-making but struggle with multimodal domain identification and function selection
- Argument generation presents significant challenges, with exact-match argument success rates falling below 50%
- Surprisingly, auxiliary context and in-context learning tend to impair rather than improve performance
- Domain description prompting and argument revision significantly improve model performance across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle to differentiate multimodal task domains due to insufficient domain-specific training signals
- Mechanism: The model's domain identification relies on recognizing semantic cues in the instruction text. Without explicit domain descriptions or fine-tuning on domain-specific tasks, the model conflates similar domains and defaults to more common patterns
- Core assumption: Domain boundaries are not clearly represented in the model's learned representations, leading to systematic misclassification errors
- Evidence anchors:
  - [abstract] "they face challenges in domain identification, function selection, and argument generation"
  - [section] "LLMs face challenges in multimodal domain selection... while LLMs possess robust common-sense knowledge, they still struggle with accurately comprehending the nuances and definitions unique to each domain of multimodal tasks"
  - [corpus] Weak evidence - corpus papers focus on code generation and privacy, not domain confusion in multimodal tasks

### Mechanism 2
- Claim: Argument generation fails because exact-match arguments require precise path information while concept-match arguments need semantic understanding that LLMs handle inconsistently
- Mechanism: Exact-match arguments (file paths) need verbatim replication, which LLMs struggle with due to path generation complexity and variability. Concept-match arguments (prompts) allow semantic flexibility but require understanding the instruction's intent
- Core assumption: The model treats all arguments similarly in generation approach, not recognizing that different argument types need different handling strategies
- Evidence anchors:
  - [section] "The success rate for matching exact-match arguments falls below 50%, and the semantic similarity of the generated concept-match arguments is similarly subpar"
  - [abstract] "they face challenges in... argument generation"
  - [corpus] Weak evidence - corpus papers don't specifically address argument generation challenges in multimodal contexts

### Mechanism 3
- Claim: In-context learning impairs multimodal function invocation by introducing noise and complexity that disrupts the model's reasoning
- Mechanism: Adding exemplar pairs to the prompt increases context length and introduces potentially irrelevant patterns. The model may try to match instruction patterns to examples rather than understanding the core multimodal task requirements
- Core assumption: The benefits of in-context learning in other domains don't transfer to multimodal function calling due to the unique nature of multimodal reasoning and tool selection
- Evidence anchors:
  - [section] "the incorporation of contextual elements tends to negatively impact performance, a trend that is especially pronounced with the introduction of in-context learning"
  - [abstract] "we surprisingly notice that auxiliary context can actually impair the performance"
  - [corpus] No direct evidence in corpus papers about in-context learning effects on multimodal function calling

## Foundational Learning

- Concept: Multimodal task formulation and API function calling
  - Why needed here: Understanding how to map natural language instructions to structured API calls is the core task being evaluated
  - Quick check question: How would you formulate the instruction "Generate an image of a church" as an API call with appropriate arguments?

- Concept: Domain-specific prompt engineering
  - Why needed here: The effectiveness of different prompt configurations (in-context learning, chain-of-thought, function calling) varies significantly in this multimodal context
  - Quick check question: What are the key differences between using in-context learning vs chain-of-thought prompting for multimodal function selection?

- Concept: Evaluation metrics for structured output generation
  - Why needed here: The evaluation framework uses domain match, function match, and argument match accuracy rather than traditional language generation metrics
  - Quick check question: Why is exact string matching used for file paths but semantic similarity (ROUGE, cosine) used for prompts?

## Architecture Onboarding

- Component map: Data collection → Human refinement → Model encapsulation → Argument standardization → Ground truth transformation → Evaluation framework → Error analysis → Improvement framework
- Critical path: Data preparation (human refinement of descriptions and functions) → Prompt configuration testing → Error analysis → Improvement implementation
- Design tradeoffs: Large dataset with human refinement vs automated generation (accuracy vs scalability), comprehensive evaluation vs computational cost
- Failure signatures: Low domain accuracy indicates confusion between similar multimodal tasks; low argument accuracy suggests generation issues; performance drops with in-context learning indicate noise sensitivity
- First 3 experiments:
  1. Run baseline GPT-3.5 on MultiAPI without any prompt modifications to establish domain, function, and argument accuracy
  2. Test the effect of adding domain descriptions to the system prompt on all accuracy metrics
  3. Implement the argument revision pipeline and measure improvement specifically in argument generation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do auxiliary context and in-context learning specifically impair the performance of LLMs in multimodal function invocation tasks?
- Basis in paper: [explicit] The paper states that "incorporation of contextual elements tends to negatively impact performance" and "a prominent observation is that the incorporation of contextual elements tends to negatively impact performance, a trend that is especially pronounced with the introduction of in-context learning."
- Why unresolved: The paper identifies this as a counterintuitive finding and suggests a need for deeper inquiry into how and why the incorporation of contextual elements in LLMs affects their function invocation capabilities, but does not provide a detailed explanation or mechanism for this impairment.
- What evidence would resolve it: Detailed experiments isolating the effects of different types of context and in-context learning on LLM performance, along with qualitative analysis of the model's reasoning process, could help elucidate the underlying causes of this impairment.

### Open Question 2
- Question: How can LLMs be improved to better differentiate between multimodal task domains, particularly in discerning whether the analysis should encompass the entire image or focus on specific content within the image?
- Basis in paper: [explicit] The paper notes that LLMs struggle with domain identification, especially in multimodal tasks, and that "LLM bias towards local rather than global image analysis" is observed.
- Why unresolved: While the paper identifies this as a challenge and proposes domain description prompting as a potential solution, it does not explore other strategies or provide a comprehensive framework for improving domain differentiation in multimodal tasks.
- What evidence would resolve it: Comparative studies evaluating different approaches to enhancing domain differentiation, such as fine-tuning on multimodal datasets, incorporating domain-specific knowledge, or using hybrid models, could provide insights into the most effective strategies for improving LLM performance in this area.

### Open Question 3
- Question: What are the limitations and potential improvements for argument generation in LLMs when handling multimodal tasks, particularly in generating concept-match arguments?
- Basis in paper: [explicit] The paper states that "GPT models in accurately generating both exact-match and concept-match arguments based on user instructions" and notes that "the inclusion of additional context appears to positively impact the generation of concept-match arguments."
- Why unresolved: The paper identifies argument generation as a significant bottleneck but does not provide a detailed analysis of the specific challenges or potential solutions for improving concept-match argument generation.
- What evidence would resolve it: Experiments evaluating different techniques for enhancing concept-match argument generation, such as fine-tuning on task-specific datasets, incorporating semantic understanding, or using external knowledge bases, could help identify the most effective strategies for improving LLM performance in this area.

## Limitations

- The analysis doesn't fully explain why auxiliary context impairs performance, which contradicts common wisdom about few-shot prompting
- Potential selection bias in the human-refined dataset could limit generalizability of the findings
- The proposed improvement framework's magnitude and generalizability are not thoroughly validated across different model architectures

## Confidence

- **High confidence**: The observation that LLMs struggle with multimodal domain identification is well-supported by multiple experiments showing consistent domain match failures across different models and prompt configurations
- **Medium confidence**: The claim about argument generation challenges shows mixed results, with exact-match arguments performing poorly but semantic similarity metrics for concept-match arguments being less conclusive
- **Medium confidence**: The proposed solution (domain description prompting and argument revision) shows improvements, but the magnitude and generalizability of these improvements are not thoroughly validated

## Next Checks

1. **Cross-domain generalization test**: Apply the proposed improvement framework to a held-out set of API functions from domains not present in the original MultiAPI dataset to verify that improvements aren't overfitting to the specific domains in the benchmark

2. **Prompt engineering ablation study**: Systematically vary the amount and relevance of in-context examples to determine the precise conditions under which auxiliary context becomes harmful versus beneficial, isolating whether the issue is noise, length, or example quality

3. **Alternative evaluation framework validation**: Compare the MultiAPI evaluation metrics against a human judgment baseline where domain experts rate the appropriateness of generated API calls, verifying that the automated metrics capture the true quality of multimodal function invocation