---
ver: rpa2
title: Multi-Source Soft Pseudo-Label Learning with Domain Similarity-based Weighting
  for Semantic Segmentation
arxiv_id: '2303.00979'
source_url: https://arxiv.org/abs/2303.00979
tags:
- source
- domain
- datasets
- target
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of domain adaptive training
  for semantic segmentation using multiple source datasets that are not necessarily
  relevant to the target dataset. The authors propose a soft pseudo-label generation
  method that integrates predicted object probabilities from multiple source models,
  weighting the prediction of each source model based on the estimated domain similarity
  between the source and the target datasets.
---

# Multi-Source Soft Pseudo-Label Learning with Domain Similarity-based Weighting for Semantic Segmentation

## Quick Facts
- arXiv ID: 2303.00979
- Source URL: https://arxiv.org/abs/2303.00979
- Reference count: 25
- Primary result: Proposed method achieves comparative or better performance than previous multi-source domain adaptation approaches for semantic segmentation

## Executive Summary
This paper addresses the challenge of domain adaptive training for semantic segmentation using multiple source datasets that may not be relevant to the target dataset. The authors propose a soft pseudo-label generation method that integrates predicted object probabilities from multiple source models, weighting each source's prediction based on estimated domain similarity between the source and target datasets. This approach emphasizes contributions from models trained on sources more similar to the target while suppressing less relevant sources. The method also uses entropy-based loss weighting to suppress uncertain pseudo-labels during training.

## Method Summary
The proposed method trains segmentation models on multiple source datasets, then generates soft pseudo-labels for the unlabeled target domain by integrating source model predictions weighted by domain similarity. Domain similarity is estimated using entropy of predictions, with lower entropy indicating higher similarity. The soft pseudo-labels are then used to train the target model with a weighted symmetric cross-entropy loss, where pixel-wise weights are inversely proportional to label entropy. This approach enables flexible class learning even when some source datasets lack certain target classes, and experiments show improved performance over existing multi-source domain adaptation methods.

## Key Results
- Proposed method achieves comparative or better performance than the authors' previous work (MSDA CL [15]) and another existing multi-source domain adaptation method (MSPL [6])
- Soft pseudo-labels enable learning classes present in only some source datasets, unlike hard pseudo-label approaches requiring unanimous agreement
- Entropy-based weighting of pseudo-labels during training effectively suppresses influence of uncertain predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain similarity weighting improves pseudo-label quality by emphasizing predictions from source models trained on datasets more relevant to the target domain.
- Mechanism: The method computes domain gap as entropy of predictions normalized by class count, then inverts this to obtain domain similarity weights. These weights are applied to integrate source predictions into soft pseudo-labels, emphasizing more relevant sources.
- Core assumption: Entropy of predictions correlates with domain gap, and higher domain similarity implies more reliable predictions for the target domain.
- Evidence anchors:
  - [abstract]: "The prediction of each source model is weighted based on the estimated domain similarity between the source and the target datasets"
  - [section]: "We employ a method by Liu et al. [20] that uses entropy of predictions as a measure of the domain gap"
  - [corpus]: Weak evidence; related works focus on multi-source DA but not specifically on domain similarity weighting for pseudo-label generation.
- Break condition: If domain gap estimation fails to correlate with actual domain similarity, weighting could emphasize irrelevant sources.

### Mechanism 2
- Claim: Entropy-based loss weighting suppresses the influence of uncertain pseudo-labels during training.
- Mechanism: The method calculates entropy of soft pseudo-labels and uses its inverse as pixel-wise weight in the classification loss, giving higher weight to pixels with lower entropy (more certain predictions).
- Core assumption: Lower entropy in soft pseudo-labels indicates higher confidence and reliability, making these pixels more valuable for training.
- Evidence anchors:
  - [abstract]: "We also propose a training method using the soft pseudo-labels considering their entropy"
  - [section]: "Based on the observation in IV-B, we consider weighting loss values on each pixel with the inverse value of label entropy"
  - [corpus]: Weak evidence; related works discuss entropy-based methods but not specifically for pseudo-label training weight.
- Break condition: If entropy doesn't accurately reflect label reliability, this weighting could suppress valuable learning signals.

### Mechanism 3
- Claim: Soft pseudo-labels enable flexible class learning even when some source datasets lack certain target classes.
- Mechanism: Unlike hard pseudo-labels that require unanimity, soft pseudo-labels integrate probability distributions across sources, allowing classes present in only some sources to still appear in the final pseudo-labels.
- Core assumption: Class probability distributions from multiple sources can be meaningfully combined even when class sets differ across sources.
- Evidence anchors:
  - [abstract]: "It can also involve labels not present in some source datasets"
  - [section]: "Unlike the previous method which excludes from pseudo-labels the pixels on which the source models did not agree with each other, our soft pseudo-labeling allowed for learning classes induced by only part of source datasets"
  - [corpus]: Weak evidence; related works discuss multi-source DA but not the specific advantage of soft pseudo-labels for handling missing classes.
- Break condition: If source predictions for missing classes are unreliable, this could introduce noise into training.

## Foundational Learning

- Concept: Domain adaptation and domain gap
  - Why needed here: The method relies on quantifying domain similarity between source and target datasets to weight source contributions appropriately.
  - Quick check question: How does entropy of predictions relate to domain gap, and why is this relationship useful for domain adaptation?

- Concept: Soft pseudo-labels vs. hard pseudo-labels
  - Why needed here: The method generates soft pseudo-labels (probability distributions) rather than hard labels (one-hot), enabling more nuanced training.
  - Quick check question: What advantages do soft pseudo-labels provide over hard pseudo-labels in multi-source domain adaptation?

- Concept: Loss weighting based on uncertainty
  - Why needed here: The method uses label entropy to weight training loss, suppressing uncertain predictions.
  - Quick check question: Why would weighting loss by inverse entropy help suppress the influence of potentially misclassified pixels?

## Architecture Onboarding

- Component map: Input → Source model predictions → Domain similarity calculation → Soft pseudo-label generation → Loss calculation (with entropy weighting) → Target model training
- Critical path: Pseudo-label generation → Loss weighting → Model training
- Design tradeoffs: Soft pseudo-labels provide flexibility but may introduce noise; entropy weighting reduces noise but may suppress useful signals
- Failure signatures: Poor performance when domain gap estimation fails, when source models disagree significantly, or when entropy doesn't reflect true uncertainty
- First 3 experiments:
  1. Verify domain similarity calculation by testing on synthetic datasets with known domain relationships
  2. Compare soft vs. hard pseudo-label performance on a simple multi-source adaptation task
  3. Test entropy-based loss weighting by training with and without this component on a controlled dataset

## Open Questions the Paper Calls Out

- Question: How does the proposed soft pseudo-label generation method perform compared to other state-of-the-art domain adaptation techniques for semantic segmentation beyond MSDA CL [15] and MSPL [6]?
  - Basis in paper: [explicit] The paper compares the proposed method to MSDA CL [15] and MSPL [6] but does not mention other state-of-the-art methods.
  - Why unresolved: The paper only provides a comparison with two specific baseline methods, leaving the performance of the proposed method against other techniques unknown.
  - What evidence would resolve it: Experimental results comparing the proposed method to other state-of-the-art domain adaptation techniques for semantic segmentation.

- Question: How sensitive is the proposed method to the choice of hyperparameters, such as the scaling parameter λscale in the loss function?
  - Basis in paper: [inferred] The paper mentions the use of hyperparameters in the loss function but does not provide a sensitivity analysis.
  - Why unresolved: The paper does not explore the impact of different hyperparameter values on the performance of the proposed method.
  - What evidence would resolve it: A sensitivity analysis showing the performance of the proposed method with different hyperparameter values.

- Question: How does the proposed method perform on datasets with a larger number of object classes or more complex scenes?
  - Basis in paper: [inferred] The paper evaluates the proposed method on datasets with a limited number of object classes and relatively simple scenes.
  - Why unresolved: The paper does not explore the performance of the proposed method on datasets with a larger number of object classes or more complex scenes.
  - What evidence would resolve it: Experimental results showing the performance of the proposed method on datasets with a larger number of object classes or more complex scenes.

## Limitations

- The method requires multiple source datasets, which may not always be available in practice
- Domain similarity estimation relies on entropy-based metrics that may not accurately capture complex domain relationships in all scenarios
- Performance gains are evaluated primarily on road/drivable area detection tasks, limiting generalizability to other semantic segmentation domains

## Confidence

- **High Confidence**: The core mechanism of entropy-weighted soft pseudo-label training is well-supported by the experimental results and theoretical framework
- **Medium Confidence**: The domain similarity weighting approach shows promise but depends on the quality of entropy-based domain gap estimation, which may vary across different dataset pairs
- **Medium Confidence**: The soft pseudo-label generation method provides flexibility for handling missing classes across sources, though this advantage is primarily demonstrated through ablation studies rather than direct comparisons

## Next Checks

1. **Domain Similarity Robustness**: Test the entropy-based domain gap estimation on synthetic datasets with controlled domain relationships to verify its correlation with actual domain similarity.

2. **Generalization Across Tasks**: Evaluate the method on diverse semantic segmentation tasks beyond road/drivable area detection to assess generalizability.

3. **Ablation of Soft vs Hard Labels**: Conduct controlled experiments comparing soft pseudo-label performance against hard pseudo-label approaches on datasets where all classes are present across all sources, to isolate the benefits of soft label integration.