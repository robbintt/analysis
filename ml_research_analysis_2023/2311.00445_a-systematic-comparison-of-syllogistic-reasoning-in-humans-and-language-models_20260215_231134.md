---
ver: rpa2
title: A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models
arxiv_id: '2311.00445'
source_url: https://arxiv.org/abs/2311.00445
tags:
- baker
- artist
- chemist
- reasoning
- artists
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the syllogistic reasoning abilities of language
  models and humans. It finds that larger language models are more logical than smaller
  ones, and also more logical than humans, though even the largest models make systematic
  errors.
---

# A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models

## Quick Facts
- arXiv ID: 2311.00445
- Source URL: https://arxiv.org/abs/2311.00445
- Authors: 
- Reference count: 39
- Primary result: Larger language models achieve higher syllogistic reasoning accuracy than smaller models and humans, while replicating some human reasoning biases.

## Executive Summary
This study systematically compares syllogistic reasoning in humans and language models using the PaLM 2 family of transformer models. The research finds that larger models are more logical than smaller ones and also more logical than humans, though even the largest models make systematic errors. The models replicate some human reasoning biases, such as sensitivity to the ordering of variables in premises and drawing confident but incorrect inferences from particular syllogisms. Using the Mental Models theory, the study shows that larger models exhibit signatures of more deliberative reasoning regardless of their accuracy.

## Method Summary
The study uses zero-shot chain-of-thought prompting with PaLM 2 models (XXS, XS, S, L sizes) to elicit reasoning on 27 syllogisms. Responses are generated with temperature 0.5 and repeated 30 times per syllogism-content triple combination. The research compares model accuracy to human data from Ragni et al. (2019) and analyzes systematic errors and biases. Mental Models theory is applied through the mReasoner cognitive model to map responses to four parameters (LEN, BROAD, SYSTM2, WEAKEN) that capture different aspects of reasoning behavior.

## Key Results
- Larger language models achieve higher syllogistic reasoning accuracy than smaller models and humans
- Language models replicate human reasoning biases including variable ordering sensitivity and overconfident incorrect inferences
- Larger models show signatures of more deliberative reasoning according to Mental Models theory, regardless of accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger language models achieve higher syllogistic reasoning accuracy due to increased model capacity enabling more sophisticated reasoning patterns.
- Mechanism: As model size increases, the models develop more nuanced understanding of logical relationships, allowing them to overcome human-like biases present in the training data.
- Core assumption: The training data contains sufficient signal to guide models toward logical reasoning despite human biases.
- Evidence anchors: [abstract] "larger models are more logical than smaller ones, and also more logical than humans"; [section] "When averaged across all syllogisms, LM accuracy improves with scale, with the largest model exceeding human performance"

### Mechanism 2
- Claim: Language models replicate human reasoning biases because they learn from human-generated text that contains these biases.
- Mechanism: The self-supervised training objective optimizes for predicting human text patterns, including systematic errors and biases documented in cognitive psychology.
- Core assumption: The language modeling objective does not inherently prioritize logical correctness over human-like patterns.
- Evidence anchors: [abstract] "Do language models, which are trained on text generated by humans, replicate such human biases, or are they able to overcome them?"; [section] "Overall, we find that language models often mimic the human biases included in their training data"

### Mechanism 3
- Claim: Mental Models theory provides an interpretable framework for understanding LLM reasoning behavior, with larger models showing signatures of more deliberative reasoning.
- Mechanism: The four parameters of mReasoner (LEN, BROAD, SYSTM2, WEAKEN) map to dimensions of reasoning behavior that correlate with model size, with larger models behaving like mReasoner instances with higher deliberative parameters.
- Core assumption: LLM reasoning follows similar heuristic strategies as humans, making cognitive models applicable to their behavior.
- Evidence anchors: [abstract] "We use the Mental Models theory to show larger LMs show signatures of being more deliberative in reasoning, irrespective of their accuracy on the syllogisms"; [section] "We find that PaLM 2's responses move upward along PC 1 and PC 2 as models grow larger... they show a stronger behavioral signature of deliberative reasoning"

## Foundational Learning

- Concept: Syllogistic reasoning
  - Why needed here: Understanding the task domain is essential for interpreting model performance and comparing to human reasoning patterns
  - Quick check question: What are the four possible moods (quantifiers) in syllogisms and how do they combine to form valid conclusions?

- Concept: Mental Models theory
  - Why needed here: Provides the theoretical framework for interpreting LLM reasoning behavior and identifying deliberative reasoning signatures
  - Quick check question: How do the four mReasoner parameters (LEN, BROAD, SYSTM2, WEAKEN) relate to different aspects of reasoning behavior?

- Concept: Chain-of-thought prompting
  - Why needed here: The evaluation method used to elicit reasoning from LLMs, which may affect the quality and interpretability of responses
  - Quick check question: How does the "Let's think this through, step by step" prompt influence the reasoning traces produced by LLMs?

## Architecture Onboarding

- Component map: Syllogism generation -> Chain-of-thought prompt construction -> LLM inference with temperature sampling -> Response aggregation and filtering -> Accuracy calculation and bias analysis -> Cognitive model mapping for behavioral interpretation

- Critical path: Stimulus generation → Chain-of-thought prompt construction → LLM inference with temperature sampling → Response aggregation and filtering → Accuracy calculation and bias analysis → Cognitive model mapping for behavioral interpretation

- Design tradeoffs: The study prioritizes controlled experimental conditions over ecological validity by generating custom syllogisms to avoid training data contamination. This provides cleaner causal inference about model capabilities but may limit generalizability to natural reasoning scenarios.

- Failure signatures: Low accuracy on specific syllogism types despite high model scale suggests fundamental limitations in logical reasoning. Systematic biases matching human errors indicate over-reliance on training data patterns. Failure to produce "nothing follows" responses reveals limitations in handling negations or absence of conclusions.

- First 3 experiments:
  1. Reproduce the main accuracy comparison between PaLM 2 sizes on the 27 syllogisms with valid conclusions, using the same chain-of-thought prompt and decoding parameters.
  2. Test the sensitivity to variable ordering by comparing A-B,B-C versus B-A,C-B presentations for each syllogism type.
  3. Apply the mReasoner mapping to the LLM responses to verify the deliberative reasoning signature increases with model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the PaLM 2 models' errors change as model size increases on the syllogisms where humans reason very poorly?
- Basis in paper: [explicit] The paper notes that larger models overcome human biases and reason correctly on some syllogisms where humans reason very poorly.
- Why unresolved: The paper does not provide a detailed breakdown of the specific syllogisms where larger models improve accuracy compared to smaller models and humans.
- What evidence would resolve it: A systematic analysis of accuracy on each syllogism type across model sizes, comparing to human accuracy on the same syllogisms.

### Open Question 2
- Question: What is the impact of the composition of the LM's training corpus on syllogistic reasoning accuracy?
- Basis in paper: [inferred] The paper suggests that the PaLM 2 models' training corpus includes source code, which may teach models to reason more effectively than text alone.
- Why unresolved: The paper does not compare the reasoning abilities of models trained on different corpora.
- What evidence would resolve it: Training models with different proportions of source code vs. natural language text and evaluating their syllogistic reasoning accuracy.

### Open Question 3
- Question: How do the PaLM 2 models' reasoning strategies change as model size increases?
- Basis in paper: [explicit] The paper uses the Mental Models theory to show that larger LMs show signatures of being more deliberative in reasoning, regardless of their accuracy.
- Why unresolved: The paper does not provide a detailed analysis of the specific reasoning strategies employed by the models.
- What evidence would resolve it: A systematic analysis of the models' responses to each syllogism, identifying the reasoning strategies used and how they change with model size.

## Limitations

- The comparison to human reasoning is limited by potential confounders in the human dataset and differences between tested syllogisms
- The study only tests PaLM 2 models up to the L size, leaving open questions about whether improvements continue at larger scales
- The observed human-like biases in LLMs may be partially attributable to the prompting strategy rather than inherent model capabilities

## Confidence

**High confidence**: The finding that LLM accuracy improves with model size on syllogisms is well-supported by experimental results and aligns with scaling laws literature.

**Medium confidence**: The claim that larger models are "more logical than humans" requires cautious interpretation due to potential confounders in the human comparison dataset and limited model size range tested.

**Medium confidence**: The observation that LLMs replicate human biases is supported by results, but the extent to which this reflects training data influence versus inherent reasoning limitations requires further investigation.

**Low confidence**: The interpretation of LLM responses through Mental Models theory and claims about "deliberative reasoning signatures" rest on the strong assumption that LLM reasoning follows human cognitive strategies, which has not been independently validated.

## Next Checks

1. **Cross-dataset validation**: Replicate the human-LLM comparison using multiple independent human reasoning datasets to verify that the observed accuracy advantage for larger models is robust across different participant populations and syllogism sets.

2. **Prompt ablation study**: Systematically vary the chain-of-thought prompt structure (different phrasings, inclusion/exclusion of "step by step" language, temperature variations) to quantify how sensitive the reasoning accuracy and bias patterns are to prompting strategy.

3. **Alternative cognitive modeling**: Apply multiple cognitive models beyond Mental Models theory (e.g., formal logic verification, other reasoning frameworks) to the LLM responses to test whether the observed patterns are specific to Mental Models or reflect more general reasoning signatures.