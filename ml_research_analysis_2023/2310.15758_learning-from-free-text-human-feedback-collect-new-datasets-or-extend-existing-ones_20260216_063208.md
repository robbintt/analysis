---
ver: rpa2
title: Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing
  Ones?
arxiv_id: '2310.15758'
source_url: https://arxiv.org/abs/2310.15758
tags:
- user
- error
- dialogs
- system
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the types and frequency of errors in system
  utterances and subsequent user responses in six dialog datasets (MultiWoZ, SGD,
  BABI, PersonaChat, Wizards-of-Wikipedia, and the Self-Feeding Chatbot) to assess
  their potential for learning from free-text human feedback. The authors manually
  annotate a subset of dialogs to identify error types and user response patterns,
  then propose new taxonomies for both.
---

# Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?

## Quick Facts
- **arXiv ID**: 2310.15758
- **Source URL**: https://arxiv.org/abs/2310.15758
- **Reference count**: 40
- **Key outcome**: Analyzes error types and user feedback patterns in six dialog datasets, showing that including user responses to system errors improves response generation performance across SOTA models.

## Executive Summary
This paper investigates the potential of existing dialog datasets for learning from free-text human feedback by analyzing error types and subsequent user responses. The authors manually annotate a subset of six dialog datasets (MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and Self-Feeding Chatbot) to identify error patterns and user feedback, then propose new taxonomies for both. They find that human-bot dialogs contain more errors and user responses that include free-text feedback, especially in open-domain and knowledge-grounded dialogs. The paper demonstrates that incorporating errors and user responses as additional input signals improves response generation performance for three SOTA models (GPT-2, LLAMA, Flan-T5).

## Method Summary
The authors first identify dialogs containing errors and subsequent user responses by using SentenceTransformer embeddings to find semantically similar dialogs to known error-indicating phrases. They then manually annotate 1,200 dialogs (555 filtered and 600 random) with error and user response types using a modified Integrated Error Taxonomy. Finally, they train baseline response generation models (GPT-2, LLAMA, Flan-T5) on the remaining 967 non-annotated dialogs and retrain them with the 188 annotated dialogs as additional input signals, evaluating performance improvements using F1-score and BLEU metrics.

## Key Results
- Human-bot dialogs contain more errors and user responses that include free-text feedback compared to human-human dialogs
- Including user responses as additional input signals improves response generation performance for Flan-T5 and GPT-2 models
- The modified error taxonomy improves inter-annotator agreement over the original Integrated Error Taxonomy in all cases
- Automatic filtering using error-indicating sentences achieves an approximated recall of 0.72 while improving annotation efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Automatic filtering using error-indicating sentences significantly improves annotation efficiency for identifying dialogs with errors and subsequent user feedback.
- **Mechanism**: SentenceTransformer embeddings enable semantic similarity search between user responses and known error-indicating phrases, filtering dialogs to those most likely containing actionable feedback.
- **Core assumption**: Error-indicating phrases capture the semantic essence of user dissatisfaction, and similarity in embedding space correlates with meaningful user-system interaction patterns.
- **Evidence anchors**: [section] "We find that our procedure for automatic filtering itself has no negative impact on the results of our analysis, but rather improved annotation efficiency. An approximated recall of 0.72 supports this assumption."
- **Break condition**: If error-indicating phrases fail to capture the full range of user dissatisfaction expressions, the recall will drop and filtering will become ineffective.

### Mechanism 2
- **Claim**: Including user responses to system errors as additional input signals improves response generation performance across multiple SOTA models.
- **Mechanism**: User responses provide corrective context and implicit feedback that helps models generate more appropriate responses by learning from real user corrections.
- **Core assumption**: User responses contain actionable information that can be effectively incorporated into the model's input representation to guide response generation.
- **Evidence anchors**: [section] "Including user responses as an additional input signal improves the results over the other configurations, including None, for both Flan-T5 and GPT-2."
- **Break condition**: If user responses are too sparse, noisy, or fail to provide clear corrective signals, the performance improvement will diminish or reverse.

### Mechanism 3
- **Claim**: The modified error taxonomy improves inter-annotator agreement compared to the original Integrated Error Taxonomy.
- **Mechanism**: Simplifying and condensing error types while adding missing categories reduces ambiguity and better captures the error patterns observed in the datasets.
- **Core assumption**: The original taxonomy's abstract and overly specialized error types create confusion among annotators, and a more streamlined taxonomy with concrete error categories improves consistency.
- **Evidence anchors**: [section] "Using our modified Integrated Error Taxonomy improves IAA over the original one in all cases. This is most obvious in the case of the human-human filtered dialogs, where it improves IAA by 0.14 points."
- **Break condition**: If the modified taxonomy oversimplifies or fails to capture nuanced error distinctions, agreement may not improve and could potentially decrease.

## Foundational Learning

- **Concept: Semantic similarity in embedding space**
  - Why needed here: Understanding how SentenceTransformer embeddings capture semantic relationships is crucial for implementing and interpreting the automatic filtering mechanism.
  - Quick check question: How does cosine similarity in a 768-dimensional embedding space relate to semantic similarity between user responses and error-indicating phrases?

- **Concept: Error taxonomy design principles**
  - Why needed here: Knowing how to create effective taxonomies that balance granularity and usability is essential for both understanding the modified taxonomy and potentially extending it.
  - Quick check question: What are the key trade-offs between having many specific error types versus fewer general error types in a taxonomy?

- **Concept: User response analysis in conversational AI**
  - Why needed here: Understanding how users typically respond to system errors helps interpret the user response taxonomy and the patterns observed in the datasets.
  - Quick check question: What are the main patterns in how users typically respond to system errors in dialog systems?

## Architecture Onboarding

- **Component map**: Data filtering pipeline (SentenceTransformer similarity search) -> Manual annotation workflow (error and user response types) -> Response generation models (GPT-2, LLAMA, Flan-T5) with error and feedback input integration -> Evaluation framework (F1-score and BLEU metrics)

- **Critical path**: 1) Filter dialogs using SentenceTransformer similarity search, 2) Manually annotate filtered and random dialogs with error and user response types, 3) Train baseline models on clean dialogs, 4) Fine-tune models with annotated error and user response data as additional inputs, 5) Evaluate performance improvements.

- **Design tradeoffs**: The system trades annotation efficiency (via automatic filtering) against potential recall loss (0.72 approximated recall). The simplified taxonomy trades granularity for improved inter-annotator agreement. The error feedback incorporation trades model complexity for potential performance gains.

- **Failure signatures**: Low inter-annotator agreement despite modified taxonomy, minimal performance improvement when incorporating user responses, poor recall in automatic filtering (below 0.5), or high variance in model performance across different error types.

- **First 3 experiments**:
  1. Implement and test the SentenceTransformer filtering pipeline on a small subset to verify similarity scores and filtering effectiveness.
  2. Conduct a small-scale annotation study using both the original and modified taxonomies to measure inter-annotator agreement differences.
  3. Train a simple GPT-2 model on clean dialogs and then fine-tune it with a small set of annotated dialogs with user responses to observe performance changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of response generation models change if the error and user response annotations were used as multi-turn context rather than just additional input signals?
- Basis in paper: [inferred] The paper investigates the impact of including errors and user responses as additional input signals, suggesting a potential extension to using them as multi-turn context.
- Why unresolved: The paper focuses on the immediate impact of including these annotations as input signals, but does not explore their potential as multi-turn context.
- What evidence would resolve it: Conducting experiments where errors and user responses are used as multi-turn context in response generation models and comparing the results with the current approach.

### Open Question 2
- Question: Would the error and user response type taxonomies proposed in the paper be effective in identifying and categorizing errors and user responses in other domains beyond dialog systems, such as text summarization or machine translation?
- Basis in paper: [explicit] The paper proposes new taxonomies for error and user response types, but does not test their applicability beyond dialog systems.
- Why unresolved: The taxonomies are developed and evaluated specifically for dialog systems, leaving their generalizability to other domains unexplored.
- What evidence would resolve it: Applying the taxonomies to error and user response data from other domains and assessing their effectiveness in categorization and identification.

### Open Question 3
- Question: How does the effectiveness of the proposed automatic filtering method for identifying potentially relevant dialogs compare to other methods, such as using pre-trained models or human annotation?
- Basis in paper: [explicit] The paper introduces an automatic filtering method using Sentence-Transformer and compares its effectiveness to random selection.
- Why unresolved: The paper does not compare the proposed method to other existing methods for identifying relevant dialogs.
- What evidence would resolve it: Comparing the performance of the proposed automatic filtering method with other methods, such as using pre-trained models or human annotation, in terms of recall and precision.

### Open Question 4
- Question: How would the inclusion of error and user response annotations impact the performance of dialog systems in real-world scenarios, where the distribution of errors and user responses may differ from the annotated datasets?
- Basis in paper: [inferred] The paper demonstrates the positive impact of including annotations in response generation, but does not address their effectiveness in real-world scenarios.
- Why unresolved: The paper focuses on controlled experiments using annotated datasets, without considering the potential differences in error and user response distributions in real-world scenarios.
- What evidence would resolve it: Conducting experiments with dialog systems trained on annotated datasets and evaluating their performance in real-world scenarios, where the distribution of errors and user responses may differ.

## Limitations

- **Annotation reliability**: The paper reports improved inter-annotator agreement with the modified taxonomy, but the absolute IAA scores (especially for human-human dialogs) are not provided, making it difficult to assess the practical reliability of the annotations.
- **Generalizability of filtering**: The automatic filtering approach relies on a fixed set of error-indicating phrases, which may not capture the full diversity of user feedback expressions across different domains and languages.
- **Model complexity**: The paper evaluates only three SOTA models (GPT-2, LLAMA, Flan-T5) with a relatively simple incorporation of user responses as additional input signals.

## Confidence

- **High confidence**: The observation that human-bot dialogs contain more errors and user responses that include free-text feedback is well-supported by the data analysis and aligns with prior research on human-bot interaction patterns.
- **Medium confidence**: The claim that including user responses as additional input signals improves response generation performance is supported by the experimental results, but the effect sizes and generalizability across different models and datasets require further validation.
- **Low confidence**: The assertion that the modified error taxonomy improves inter-annotator agreement is based on relative improvements, but the absolute IAA scores and the impact on downstream tasks are not clearly established.

## Next Checks

1. **Replicate IAA scores**: Conduct a small-scale annotation study with multiple annotators to measure inter-annotator agreement using both the original and modified taxonomies, and compare the absolute scores and relative improvements.
2. **Expand model evaluation**: Evaluate the impact of incorporating user responses on a wider range of response generation models, including more recent and diverse architectures, to assess the generalizability of the findings.
3. **Analyze filtering effectiveness**: Perform an error analysis on the filtered dialogs to quantify the types and frequencies of missed errors, and assess the impact of the 0.72 recall on the downstream experiments.