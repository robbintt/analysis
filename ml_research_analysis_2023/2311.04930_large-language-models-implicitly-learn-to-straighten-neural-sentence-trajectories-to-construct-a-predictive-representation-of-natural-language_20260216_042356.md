---
ver: rpa2
title: Large language models implicitly learn to straighten neural sentence trajectories
  to construct a predictive representation of natural language
arxiv_id: '2311.04930'
source_url: https://arxiv.org/abs/2311.04930
tags:
- curvature
- layers
- sentences
- layer
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how autoregressive transformer models,
  such as GPT-2, construct predictive representations of language through a mechanism
  called "representation straightening." The hypothesis suggests that as sentences
  pass through the network layers, their neural trajectories become progressively
  straighter, facilitating next-word prediction via linear extrapolation. The researchers
  quantified this straightening using a 1D curvature metric and found four key results:
  i) trained models show a decrease in curvature from early to middle layers, while
  untrained models do not, ii) larger models and those trained on more data exhibit
  greater curvature reduction, iii) model-generated sentences have straighter trajectories
  than natural sentences, and iv) there is a positive correlation between sentence
  curvature and surprisal in deeper layers.'
---

# Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language

## Quick Facts
- arXiv ID: 2311.04930
- Source URL: https://arxiv.org/abs/2311.04930
- Authors: 
- Reference count: 30
- This study investigates how autoregressive transformer models, such as GPT-2, construct predictive representations of language through a mechanism called "representation straightening."

## Executive Summary
This study investigates how autoregressive transformer models, such as GPT-2, construct predictive representations of language through a mechanism called "representation straightening." The hypothesis suggests that as sentences pass through the network layers, their neural trajectories become progressively straighter, facilitating next-word prediction via linear extrapolation. The researchers quantified this straightening using a 1D curvature metric and found four key results: i) trained models show a decrease in curvature from early to middle layers, while untrained models do not, ii) larger models and those trained on more data exhibit greater curvature reduction, iii) model-generated sentences have straighter trajectories than natural sentences, and iv) there is a positive correlation between sentence curvature and surprisal in deeper layers. These findings suggest that representation straightening is a critical mechanism for efficient language modeling and prediction.

## Method Summary
The researchers analyzed sentence representations from GPT-2 models by computing curvature metrics across layers. They used 8,408 English sentences from the Universal Dependencies corpus, filtered to 100K most common nouns. For each sentence, they extracted layer-by-layer representations and calculated curvature as the angle between adjacent state vectors using arccosine of normalized dot products. They compared curvature changes in trained vs. untrained models, across different model sizes, and with varying training data volumes. Additionally, they generated model continuations for 5,815 sentences and compared curvature patterns with ground truth. The study also correlated curvature with 3-gram word surprisal to examine relationships between geometric properties and prediction difficulty.

## Key Results
- Trained GPT-2 models show decreasing curvature from early to middle layers, while untrained models do not exhibit this pattern
- Larger models and those trained on more data demonstrate greater curvature reduction, following a logarithmic relationship
- Model-generated sentences have straighter trajectories than natural sentences, suggesting the model internally favors straight paths for prediction
- There is a positive correlation between sentence curvature and surprisal in deeper layers, linking geometric properties to prediction difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory straightening in deeper layers enables more accurate next-word prediction by allowing linear extrapolation of future states.
- Mechanism: As sentence representations pass through transformer layers, their curvature decreases, creating straighter trajectories that can be extrapolated linearly to predict the next state.
- Core assumption: The curvature metric accurately captures the non-linearity of trajectories in high-dimensional representation space.
- Evidence anchors:
  - [abstract] "The key insight is that straighter trajectories should facilitate prediction via linear extrapolation."
  - [section 2.2] "We calculated curvature as the angle between these vectors, namely: cp k = arccos[vp k+1·vp k / (||vp k+1|| ||vp k||)]"
  - [corpus] Weak evidence; the paper assumes the curvature metric is valid without extensive validation on synthetic trajectories.
- Break condition: If curvature does not correlate with prediction accuracy, or if model performance does not scale with curvature reduction.

### Mechanism 2
- Claim: Larger models and models trained on more data exhibit greater curvature reduction, which drives better language modeling performance.
- Mechanism: Increased model capacity and exposure to more training data allow the model to learn more effective straightening transformations in deeper layers.
- Core assumption: The relationship between model size/training data and curvature reduction is causal, not just correlational.
- Evidence anchors:
  - [abstract] "Models that perform better on the next-word prediction objective exhibit greater decreases in curvature."
  - [section 3.2] "The average change in curvature shows a logarithmic relationship with model size" and "models trained on more data exhibit greater curvature reduction."
  - [corpus] Limited evidence; the paper shows correlation but doesn't establish causation definitively.
- Break condition: If model performance improves without corresponding curvature reduction, or if curvature reduction plateaus without performance gains.

### Mechanism 3
- Claim: Model-generated sentences have straighter trajectories than natural sentences because the model internally favors straight paths for prediction.
- Mechanism: During generation, the model selects tokens that lead to straighter trajectories in its internal representation space, optimizing for predictability.
- Core assumption: The model's generation process is influenced by the straightening mechanism learned during training.
- Evidence anchors:
  - [abstract] "Given the same linguistic context, the sequences that are generated by the model have lower curvature than the actual continuations observed in a language corpus."
  - [section 3.3] "Model-generated sentences show a greater drop in curvature reduction relative to the ground-truth sentences."
  - [corpus] Evidence is based on a single model (GPT2-XL) and may not generalize to all generation strategies.
- Break condition: If model-generated sentences do not consistently show lower curvature, or if different generation strategies produce different curvature patterns.

## Foundational Learning

- Concept: Neural trajectory analysis in high-dimensional spaces
  - Why needed here: Understanding how sentences evolve through transformer layers requires analyzing their paths in representation space.
  - Quick check question: How would you compute the curvature of a trajectory in a 1600-dimensional space using only 1D curvature metrics?

- Concept: Next-word prediction as a predictive objective
  - Why needed here: The straightening mechanism is hypothesized to emerge specifically from training on next-word prediction.
  - Quick check question: Why might models trained on different objectives (e.g., masked language modeling) show different trajectory geometries?

- Concept: Relationship between geometric properties and functional performance
  - Why needed here: The paper claims that geometric properties (curvature) directly influence functional outcomes (prediction accuracy).
  - Quick check question: What alternative explanations could account for the correlation between curvature and surprisal?

## Architecture Onboarding

- Component map:
  Input layer → Layer normalization → Self-attention → Layer normalization → Feed-forward → Output layer
  Each layer produces contextualized representations that form trajectory points
  Curvature is computed between consecutive layer representations

- Critical path:
  Sentence → Tokenization → Model input → Layer 0 activation → Layer 1 activation → ... → Final layer activation
  For each sentence, compute curvature at each layer, then analyze curvature changes across layers

- Design tradeoffs:
  Using 1D curvature metric simplifies analysis but may miss important geometric features
  Greedy generation for comparison is fast but may not represent optimal model behavior
  Comparing to untrained models controls for architecture but doesn't account for initialization effects

- Failure signatures:
  No curvature reduction in trained models would suggest the straightening mechanism doesn't exist
  Curvature reduction without performance improvement would indicate the mechanism isn't functionally relevant
  Inconsistent curvature patterns across sentences would suggest the metric isn't capturing meaningful structure

- First 3 experiments:
  1. Verify curvature computation on synthetic straight and curved trajectories
  2. Compare curvature patterns in trained vs. untrained models on a small sentence set
  3. Test whether model-generated continuations consistently have lower curvature than natural continuations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does representation straightening emerge as a result of the prediction objective itself, or as a consequence of the model's need to process longer-range contextual dependencies in language?
- Basis in paper: [inferred] The paper discusses two possibilities: (i) that predicting future inputs intrinsically gives rise to implicit next-state prediction, thus directly favoring linear state dynamics, or (ii) that predicting future inputs in domains like language benefits from slowly-changing contextual information, thus indirectly favoring slower (and more approximately linear) state dynamics.
- Why unresolved: The paper acknowledges this distinction but does not provide direct evidence to distinguish between these two possibilities. They suggest that future experiments using artificially created datasets with varying context lengths could help differentiate these mechanisms.
- What evidence would resolve it: Training models on artificially created datasets that systematically vary in the length of context that affects the predictability of an incoming element. If straightening effects obtain across all context lengths, that would support the first possibility. If effects only hold for models trained on data with relatively long predictive contexts, that would support the second possibility.

### Open Question 2
- Question: Is representation straightening specific to the next-word prediction objective, or does it emerge with other training objectives or fine-tuning for downstream tasks?
- Basis in paper: [explicit] The paper states "We have not evaluated the effects on sentence curvature of other training objectives or fine-tuning for downstream tasks. Doing so can help understand the selectivity of the observed effects (i.e., do sentence representations get straightened in the middle layers only under the pressure of the next-word prediction objective?) and their robustness to adding other objectives to a pre-trained model."
- Why unresolved: The current study only examined autoregressive models trained on next-word prediction, leaving open whether this mechanism is specific to this objective or more general.
- What evidence would resolve it: Testing curvature changes across layers in models trained with different objectives (masked language modeling, causal language modeling with different objectives, supervised tasks) and examining whether curvature reduction patterns change or disappear with different training regimes.

### Open Question 3
- Question: Is representation straightening a causal mechanism for improved next-word prediction, or merely a correlate of better performance?
- Basis in paper: [explicit] The paper acknowledges "We have also not causally tested the straightening hypothesis, which would require ablating the model in such a way that only curvature is affected, and testing how next-word prediction behavior changes."
- Why unresolved: The paper establishes correlations between curvature and surprisal, and between curvature reduction and model performance, but does not directly test whether curvature reduction causes better prediction or is merely correlated with it.
- What evidence would resolve it: Ablation studies that specifically manipulate curvature while holding other model properties constant, or interventions that artificially increase curvature and measure the impact on prediction accuracy. If curvature manipulation directly affects prediction performance, this would demonstrate causation.

## Limitations

- The 1D curvature metric may oversimplify the complex geometry of neural trajectories in high-dimensional representation spaces
- The study focuses exclusively on GPT-2 models and English sentences from Universal Dependencies, limiting generalizability
- The paper establishes correlations but does not demonstrate causation between curvature reduction and prediction performance

## Confidence

**High Confidence**: The finding that trained GPT-2 models show decreasing curvature from early to middle layers while untrained models do not. This is directly observable from the data and the experimental design controls for architecture effects.

**Medium Confidence**: The relationship between model size/training data and curvature reduction. While the correlation is demonstrated, the causal mechanism is not definitively established, and alternative explanations cannot be ruled out.

**Low Confidence**: The claim that representation straightening is a critical mechanism for efficient language modeling. The paper shows correlations between curvature and surprisal, and between curvature reduction and model performance, but does not directly test whether curvature reduction causes better prediction or is merely correlated with it.

## Next Checks

1. **Synthetic trajectory validation**: Test the curvature metric on synthetic trajectories with known geometric properties (straight lines, curves of varying radii) to verify it accurately captures trajectory straightness across different dimensionalities and compare results with alternative geometric measures like total variation or fractal dimension.

2. **Ablation of curvature components**: Remove specific transformer components (e.g., layer normalization, feed-forward networks, or attention mechanisms) and measure how this affects curvature reduction patterns to identify which architectural elements are essential for straightening.

3. **Cross-architectural comparison**: Apply the same curvature analysis to other transformer variants (BERT, RoBERTa, T5) and non-transformer architectures (LSTMs, ConvNets) to determine whether straightening is unique to GPT-2 or a more general phenomenon in language models.