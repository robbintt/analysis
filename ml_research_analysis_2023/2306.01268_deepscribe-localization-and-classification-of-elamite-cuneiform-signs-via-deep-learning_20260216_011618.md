---
ver: rpa2
title: 'DeepScribe: Localization and Classification of Elamite Cuneiform Signs Via
  Deep Learning'
arxiv_id: '2306.01268'
source_url: https://arxiv.org/abs/2306.01268
tags:
- sign
- cuneiform
- signs
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a computer vision pipeline for localizing and
  classifying cuneiform signs on tablet images. The system leverages a large, richly
  annotated dataset from the Persepolis Fortification Archive to train a RetinaNet
  object detector (achieving 0.78 mAP@50 for localization) and a ResNet classifier
  (achieving 0.89 top-5 accuracy).
---

# DeepScribe: Localization and Classification of Elamite Cuneiform Signs Via Deep Learning

## Quick Facts
- arXiv ID: 2306.01268
- Source URL: https://arxiv.org/abs/2306.01268
- Reference count: 40
- Primary result: End-to-end pipeline achieves 0.80 top-5 classification accuracy for cuneiform sign recognition

## Executive Summary
This work presents a computer vision pipeline for localizing and classifying cuneiform signs on tablet images. The system leverages a large, richly annotated dataset from the Persepolis Fortification Archive to train a RetinaNet object detector (achieving 0.78 mAP@50 for localization) and a ResNet classifier (achieving 0.89 top-5 accuracy). The end-to-end pipeline achieves 0.80 top-5 classification accuracy. A novel contribution is the use of the PFA dataset to train computer vision models directly on cuneiform signs. The work also explores how the learned sign representations differ from traditional sign lists, finding that the model groups signs by graphic similarity rather than traditional categorization.

## Method Summary
The pipeline employs a two-stage approach: first, a RetinaNet object detector identifies sign bounding boxes treating all signs as a single class; second, a ResNet classifier identifies the specific sign within each detected region. The detector is trained on tablet images with hotspot annotations, while the classifier is trained on the detected regions resized to 50x50 pixels. Both models use Adam optimization with specific learning rate schedules and data augmentation. The pipeline also includes a Sequential RANSAC algorithm for line detection and character error rate calculation for evaluation.

## Key Results
- RetinaNet object detector achieves 0.78 mAP@50 for cuneiform sign localization
- ResNet classifier achieves 0.89 top-5 accuracy for sign classification
- End-to-end pipeline achieves 0.80 top-5 classification accuracy
- Learned sign representations group by graphic similarity rather than traditional categorization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The object detector achieves 0.78 mAP@50 because it learns to recognize cuneiform signs as generic geometric shapes rather than specific characters.
- Mechanism: The RetinaNet is trained as a single-class detector where all cuneiform signs are collapsed into one category. This allows the network to focus on learning general visual features of wedge-shaped impressions in clay rather than memorizing individual sign identities.
- Core assumption: Geometric patterns of cuneiform signs are consistent enough across different signs to be captured by a generic detector.
- Evidence anchors:
  - [abstract] "finding that a RetinaNet object detector can achieve a localization mAP of 0.78"
  - [section 4.2] "We define cuneiform sign localization as an object detection problem - i.e. selecting rectangular bounding boxes that contain a single cuneiform character. All boxes are assigned to an identical class"
- Break condition: If cuneiform signs vary too much in geometric structure (e.g., some are highly irregular or three-dimensional), the generic detector will fail to generalize.

### Mechanism 2
- Claim: The classifier achieves 0.89 top-5 accuracy because it learns semantic groupings of signs based on visual similarity rather than traditional sign lists.
- Mechanism: The ResNet classifier maps sign images to 141-dimensional logit vectors that encode visual similarity. During inference, signs with similar visual features are grouped together in the embedding space, allowing the model to suggest correct signs even when the top prediction is wrong.
- Core assumption: Visual similarity between signs correlates with correct classification more than traditional categorical organization.
- Evidence anchors:
  - [abstract] "finding that a ResNet classifier can achieve a top-5 sign classification accuracy of 0.89"
  - [section 5.2.4] "We perform qualitative analysis of sign clusters to understand how the learned representations relate... signs are clearly organized by graphic similarity"
- Break condition: If visual similarity does not correlate with correct classification (e.g., signs with similar appearance have different meanings), the model's suggestions will be misleading.

### Mechanism 3
- Claim: The end-to-end pipeline achieves 0.80 top-5 accuracy because the modular design isolates localization and classification tasks, allowing each to be optimized independently.
- Mechanism: By training the detector and classifier separately on ground-truth data, each component can be optimized for its specific task without interference. The detector focuses on finding sign regions, while the classifier focuses on identifying signs within those regions.
- Core assumption: The errors from each component are independent and do not compound catastrophically when combined.
- Evidence anchors:
  - [abstract] "The end-to-end pipeline achieves a top-5 classification accuracy of 0.80"
  - [section 4.1] "We decided to split the problem into two independent modeling steps... Each stage in the pipeline is trained separately using ground-truth annotations"
- Break condition: If errors from the detector (false positives/negatives) systematically bias the classifier's performance, the modular approach will underperform a joint model.

## Foundational Learning

- Concept: Object detection fundamentals (bounding box regression, anchor generation, non-max suppression)
  - Why needed here: The RetinaNet must learn to localize cuneiform signs within complex tablet images
  - Quick check question: What is the difference between object detection and image classification, and why does this matter for cuneiform sign localization?

- Concept: Image classification with class imbalance handling
  - Why needed here: The 141 cuneiform sign classes have highly imbalanced frequencies, requiring special handling during training
  - Quick check question: How does class imbalance affect model training, and what techniques can mitigate its impact on low-frequency classes?

- Concept: Dimensionality reduction and visualization (PCA, t-SNE)
  - Why needed here: Analyzing the learned sign representations requires reducing high-dimensional embeddings to 2D for visualization and interpretation
  - Quick check question: What is the difference between PCA and t-SNE, and when would you use each for analyzing model embeddings?

## Architecture Onboarding

- Component map: RetinaNet detector → ResNet classifier → Sequential RANSAC line detector → Character Error Rate evaluation
- Critical path: Detector predictions → Classifier predictions → Line detection → CER calculation
- Design tradeoffs: Modular design enables independent optimization but may miss synergies; single-class detector simplifies localization but loses sign-specific information
- Failure signatures: High false positive rate from detector, poor performance on low-frequency signs from classifier, incorrect line ordering from RANSAC
- First 3 experiments:
  1. Train RetinaNet on full dataset and evaluate mAP@50 on held-out tablets to establish baseline localization performance
  2. Train ResNet classifier on ground-truth hotspots and evaluate top-1/top-5 accuracy to understand recognition difficulty
  3. Run end-to-end pipeline on test set and calculate CER to assess complete system performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would the pipeline generalize to cuneiform scripts from different time periods or languages?
- Basis in paper: [inferred] The authors mention applying the detector to non-Elamite tablets as future work and provide preliminary results on Ur III Sumerian tablets.
- Why unresolved: The paper only provides qualitative results on a small number of tablets from a different time period and language. A systematic evaluation on a larger and more diverse set of cuneiform scripts would be needed to determine the generalization capabilities of the pipeline.
- What evidence would resolve it: A study evaluating the pipeline's performance on a large and diverse set of cuneiform scripts from different time periods and languages, comparing the results to the performance on the Elamite dataset.

### Open Question 2
- Question: Would incorporating linguistic context improve the performance of the hotspot detector and sign classifier?
- Basis in paper: [explicit] The authors discuss the possibility of incorporating linguistic context into the detection and classification stages as future work, and note that the current pipeline's end-to-end performance is limited by the noisy outputs of the sign classifier.
- Why unresolved: The paper does not experiment with incorporating linguistic context into the pipeline, so it is unclear how much of an impact it would have on performance.
- What evidence would resolve it: An experiment comparing the performance of the current pipeline to a pipeline that incorporates linguistic context, using the same dataset and evaluation metrics.

### Open Question 3
- Question: How does the learned sign representation in the pipeline compare to traditional sign lists and human understanding of cuneiform signs?
- Basis in paper: [explicit] The authors analyze the learned sign representations using t-SNE and find that the model groups signs by graphic similarity, which differs from traditional sign lists organized by hierarchical analysis of wedge elements.
- Why unresolved: The analysis is qualitative and limited to a visualization of the learned representations. A more rigorous comparison to traditional sign lists and an exploration of how the learned representations relate to human understanding of cuneiform signs would be needed to fully answer this question.
- What evidence would resolve it: A study comparing the learned sign representations to traditional sign lists and human judgments of sign similarity, using quantitative metrics and a larger set of signs.

## Limitations
- The PFA dataset may not fully represent the diversity of Elamite cuneiform writing, potentially limiting generalizability to other cuneiform corpora.
- Sign bounding box annotations are inherited from the project's OCR pipeline, which may contain systematic errors.
- Collapsing all cuneiform signs into one detection class may oversimplify the localization task.

## Confidence
- Localization Performance (mAP@50 0.78): Medium confidence
- Classification Performance (Top-5 0.89): Medium confidence
- End-to-End Pipeline (Top-5 0.80): Medium confidence
- Visual Similarity Grouping: Low confidence

## Next Checks
1. Test the trained models on cuneiform tablet images from different archives (e.g., Ur III tablets) to assess generalization beyond the PFA dataset.
2. Perform manual verification of a random sample of bounding box annotations to quantify potential ground-truth errors and their impact on model performance.
3. Implement and evaluate a multi-class RetinaNet variant to determine if sign-specific localization improves overall performance compared to the single-class approach.