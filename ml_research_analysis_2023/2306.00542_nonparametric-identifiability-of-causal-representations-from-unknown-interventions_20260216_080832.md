---
ver: rpa2
title: Nonparametric Identifiability of Causal Representations from Unknown Interventions
arxiv_id: '2306.00542'
source_url: https://arxiv.org/abs/2306.00542
tags:
- causal
- learning
- cited
- page
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies causal representation learning, the problem
  of inferring latent causal variables and their causal relations from high-dimensional
  observations. Prior work relied on weak supervision, restrictive assumptions, or
  partial knowledge of the generative process.
---

# Nonparametric Identifiability of Causal Representations from Unknown Interventions

## Quick Facts
- arXiv ID: 2306.00542
- Source URL: https://arxiv.org/abs/2306.00542
- Reference count: 40
- One-line primary result: Identifies causal latents and graph up to isomorphism using unknown interventions, with one perfect intervention per node sufficient for two variables under genericity.

## Executive Summary
This paper establishes nonparametric identifiability of causal representations from high-dimensional observations when only interventional data from unknown interventions is available. Unlike prior work requiring weak supervision or restrictive assumptions, this approach works with unconstrained causal models and mixing functions. The key insight is that interventions break symmetries in the observational distribution, enabling recovery of latent causal variables and their causal graph up to a set of natural ambiguities. For two variables, observational data plus one perfect intervention per node suffices under a genericity condition; for arbitrary numbers of variables, two distinct paired perfect interventions per node guarantee identifiability.

## Method Summary
The method involves learning causal representations by fitting generative models (e.g., normalizing flows) to multi-environment interventional data, where each environment corresponds to either observational data or an intervention on one of the latent variables. The approach can work with unknown intervention targets by evaluating candidate models with different assumed intervention targets and selecting the best via log-likelihood. For autoencoder-based approaches, conditional independence tests enforce sparsity of interventions. The theoretical framework shows that under certain conditions (genericity for two variables, paired distinct interventions for arbitrary variables), the ground truth latents and causal graph can be identified up to element-wise reparameterizations and permutations.

## Key Results
- One perfect intervention per node plus observational data suffices to identify causal graph for two variables under genericity condition
- Two distinct paired perfect interventions per node guarantee identifiability for any number of variables
- Causal influence strengths are preserved across all equivalent solutions, enabling causal conclusions from learned representations
- The observational distribution and interventional data together break symmetries that prevent identification from observational data alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** One perfect intervention per node, plus observational data, suffices to identify the causal graph up to isomorphism when mixing is diffeomorphic and latents are nonparametric.
- **Mechanism:** Interventions break symmetries in the observational distribution. By comparing ratios of intervened vs. non-intervened mechanisms, partial derivatives of the unmixing function can be forced to zero, which pins down the causal ordering and structure.
- **Core assumption:** The intervened and non-intervened mechanisms are "generically" distinct; that is, no fine-tuned cancellations exist between them (Assumption 4.2).
- **Break condition:** If the intervened mechanism is too similar to the base mechanism (violating genericity), the partial derivative equations no longer force zeros, and the solution space becomes ambiguous.

### Mechanism 2
- **Claim:** Two distinct paired interventions per node guarantee identifiability for any number of variables.
- **Mechanism:** With two distinct interventions per node, the unmixing function must be element-wise (no mixing across variables) because the ratio of two distinct intervened distributions forces the unmixing map to be constant in all but one coordinate. This, combined with faithfulness, pins down both the variables and the causal graph.
- **Core assumption:** Each node receives two distinct interventions whose ratio is strictly monotonic (Assumption 4.10).
- **Break condition:** If the two interventions per node are too similar, the ratio may not be strictly monotonic, and the element-wise constraint fails.

### Mechanism 3
- **Claim:** Causal influence strengths are preserved across all equivalent solutions under ∼CRL, making the learned representation causally interpretable.
- **Mechanism:** The KL-based causal influence measure depends only on interventional distributions, which are invariant under element-wise reparameterizations and permutations. Thus, even though the exact latents may not be recovered, the strength of causal relationships is.
- **Break condition:** If the interventions do not capture the true causal mechanisms (e.g., soft instead of perfect), the KL-based measure may not be invariant.

## Foundational Learning

- **Concept:** Diffeomorphic mixing and invertibility
  - **Why needed here:** Ensures that the latent causal variables can be uniquely recovered from observations; without invertibility, multiple latent configurations could generate the same observations.
  - **Quick check question:** If f is not invertible, can you uniquely recover V from X? (Answer: No, because multiple V could map to the same X.)

- **Concept:** Faithfulness assumption
  - **Why needed here:** Guarantees a one-to-one correspondence between conditional independences in the data and graphical separations in the causal graph; without it, causal structure cannot be uniquely inferred.
  - **Quick check question:** What could happen if faithfulness is violated? (Answer: Observed independences may not reflect the true causal graph, leading to incorrect structure learning.)

- **Concept:** Intervention vs. observational data
  - **Why needed here:** Observational data alone cannot distinguish between causal and anticausal directions when mixing is nonlinear; interventions break these symmetries by altering mechanisms.
  - **Quick check question:** Why can't we identify the causal graph from i.i.d. observational data in the nonlinear mixing case? (Answer: The mixing function can create spurious independences that mask the true causal structure.)

## Architecture Onboarding

- **Component map:** Multi-environment data loader -> Intervention target inference module -> Unmixing function estimator (e.g., normalizing flows) -> Causal graph structure learner -> Model selection based on log-likelihood
- **Critical path:** 1. Load heterogeneous interventional datasets. 2. Fit candidate models with different assumed graphs and intervention targets. 3. Compare log-likelihoods to select the best-fitting model. 4. Verify identifiability via MCC between inferred and ground truth latents.
- **Design tradeoffs:** More interventions per node → stronger identifiability guarantees but higher experimental cost. Parametric vs. nonparametric mixing → parametric is easier to fit but may miss true structure; nonparametric is more flexible but requires more data. Known vs. unknown intervention targets → unknown is more realistic but complicates learning and requires more diverse environments.
- **Failure signatures:** Low MCC even with highest log-likelihood → likely model misspecification or violation of genericity. Similar log-likelihoods across different graphs → insufficient intervention diversity or weak faithfulness. Unstable training of normalizing flows → may need more layers or better initialization.
- **First 3 experiments:** 1. **Synthetic linear Gaussian test:** Use linear Gaussian latents with known graph; verify MCC ≈ 1 for correctly specified model. 2. **Varying intervention strength:** Gradually reduce intervention magnitude to see at what point genericity breaks down. 3. **Unknown target recovery:** Randomly permute intervention labels and test if model still recovers correct graph via log-likelihood selection.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the identifiability results for two causal variables be generalized to n > 2 variables?
- **Basis in paper:** The paper states: "We conjecture that Thm. 4.1 can be generalized to n > 2, subject to a suitably adjusted set of genericity conditions involving several intervened and base mechanisms, akin to (4.2) for the bivariate case."
- **Why unresolved:** The proof for the bivariate case does not easily extend to higher dimensions due to combinatorial challenges in aligning intervention targets across multiple environments.
- **What evidence would resolve it:** A formal proof demonstrating that the identifiability conditions for n > 2 causal variables are satisfied, or a counterexample showing that the conjecture is false.

### Open Question 2
- **Question:** Are two paired perfect interventions per node necessary for identifiability, or can it be achieved with fewer interventions?
- **Basis in paper:** The paper states: "While two single-node perfect interventions are sufficient, we do not believe this to be necessary."
- **Why unresolved:** The current proof relies on having two paired interventions per node, but it is unclear if this is the minimal requirement for identifiability.
- **What evidence would resolve it:** An identifiability proof for the case where only one intervention per node is available, or a counterexample showing that fewer interventions are insufficient.

### Open Question 3
- **Question:** How does the assumption of known n (number of latent causal variables) affect the applicability of the results?
- **Basis in paper:** The paper discusses this in the context of Markovianity and states: "Extensions of our analysis to unknown n, non-Markovian, or non-invertible models constitute an interesting direction for future investigations."
- **Why unresolved:** The current results assume that the number of latent causal variables is known, which may not always be the case in real-world applications.
- **What evidence would resolve it:** A method for estimating the number of latent causal variables from data, or an extension of the identifiability results to the case where n is unknown.

## Limitations
- Genericity condition (4.2) is mathematically defined but lacks practical implementation guidelines
- Requires perfect interventions, which may be unrealistic in many real-world scenarios
- Assumes known number of latent causal variables (n), limiting applicability to cases where this is unknown

## Confidence
- **Mechanism 1 (Single intervention identifiability):** Medium confidence. The theoretical framework is rigorous, but the genericity condition's practical applicability remains uncertain.
- **Mechanism 2 (Paired interventions for arbitrary variables):** High confidence. The mathematical derivation is complete and the conditions are clearly specified.
- **Mechanism 3 (Causal influence preservation):** High confidence. This follows directly from the invariance of KL-divergence under element-wise reparameterizations.

## Next Checks
1. **Genericity condition test:** Systematically vary the intervention strength and measure at what point the MCC between learned and ground truth latents drops below 0.9, providing empirical boundaries for the genericity assumption.
2. **Soft intervention robustness:** Replace perfect interventions with Gaussian-shifted soft interventions and measure the degradation in identifiability metrics, testing the practical applicability of the theory.
3. **Unknown target recovery stress test:** Create synthetic datasets with randomized intervention labels and evaluate whether the log-likelihood-based model selection consistently recovers the correct intervention targets across different graph structures.