---
ver: rpa2
title: 'Size Matters: Large Graph Generation with HiGGs'
arxiv_id: '2306.11412'
source_url: https://arxiv.org/abs/2306.11412
tags:
- graphs
- graph
- higgs
- bter
- higg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HIGGS (Hierarchical Generation of Graphs),
  a framework for generating large attributed graphs using GNNs. The key insight is
  to decompose graph generation into a hierarchy: first generate a coarse-grained
  representation, then generate detailed subgraphs conditioned on this representation,
  and finally predict edges between subgraphs.'
---

# Size Matters: Large Graph Generation with HiGGs

## Quick Facts
- arXiv ID: 2306.11412
- Source URL: https://arxiv.org/abs/2306.11412
- Reference count: 35
- Primary result: First deep learning method to generate graphs with tens of thousands of nodes and categorical attributes

## Executive Summary
This paper presents HIGGS (Hierarchical Generation of Graphs), a framework for generating large attributed graphs using GNNs. The key insight is to decompose graph generation into a hierarchy: first generate a coarse-grained representation, then generate detailed subgraphs conditioned on this representation, and finally predict edges between subgraphs. This approach extends the scale of graphs that can be generated by quadratic order compared to existing GNN methods. The authors implement HIGGS using a novel edge-predictive variant of the DiGress diffusion model. Experiments on datasets ranging from 40 to 22,000 nodes show that HIGGS outperforms the rule-based BTER model on local graph structure metrics while approaching BTER's performance on global metrics.

## Method Summary
HIGGS uses hierarchical graph generation where large graphs are decomposed into communities (h1 subgraphs) and inter-community connections (h2 graph). The framework trains DiGress diffusion models to generate the h2 meta-graph, generate each h1 subgraph conditioned on its corresponding h2 node, and predict edges between h1 subgraphs using a novel edge-DiGress model. The approach enables generation of graphs with tens of thousands of nodes by avoiding quadratic complexity through parallel generation of independent components.

## Key Results
- HIGGS extends graph generation scale by quadratic order compared to existing GNN methods
- Outperforms BTER on local graph structure metrics while approaching BTER on global metrics
- Successfully generates graphs with up to 22,000 nodes and categorical attributes
- First deep learning method to achieve this scale for attributed graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HIGGS extends the scale of graph generation by quadratic order by breaking the problem into hierarchical sub-tasks.
- Mechanism: Instead of generating a large graph in one pass, HIGGS generates a low-resolution graph (h2), then conditions generation of many smaller subgraphs (h1) on this coarse structure, and finally predicts edges between them. This avoids quadratic complexity in node count for any single stage.
- Core assumption: The hierarchical decomposition preserves enough global structure for the generated graph to remain realistic, and the inter-h1 edge prediction can be conditioned without prior global knowledge.
- Evidence anchors:
  - [abstract] "HIGGS has the capacity to extend the scale of generated graphs from a given GNN model by quadratic order"
  - [section] "The order of DiGress is O(N 2), which is costly. Fortunately HIGGS does not need to scale beyond a few hundred nodes in any given sampling process."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.434, average citations=0.1. Top related titles: The Evolution of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey.
- Break condition: If Louvain partitioning produces partitions that are too uneven in size, or if edge prediction between h1 subgraphs requires knowledge of other subgraphs, the model's performance degrades.

### Mechanism 2
- Claim: Conditional generation in DiGress allows HIGGS to encode community structure and attributes at each hierarchical level.
- Mechanism: DiGress supports discrete denoising diffusion conditioned on node, edge, or graph attributes. In HIGGS, each h1 subgraph is generated conditioned on the class/category of its corresponding h2 node, and inter-h1 edges can be conditioned on edge attributes in h2.
- Core assumption: The conditioning signal (e.g., majority node class in an h1 subgraph) is sufficiently informative to preserve realistic structure in the generated subgraphs.
- Evidence anchors:
  - [section] "For the two node-categorised datasets, Cora and Facebook, we categorise h1 graphs by their most common node category. The sampling of h1 graphs is conditioned on that node class from h2."
  - [section] "The version of DiGress used in this HIGGS implementation is from the pre-print version of Vignac et al. [23], and so does not use their updated regressive conditioning method."
  - [corpus] Average neighbor citations=0.1, indicating moderate relatedness to the core topic.
- Break condition: If the conditioning signal is too weak (e.g., uniform majority class) or the dataset lacks meaningful community structure, the generated subgraphs will not reflect realistic patterns.

### Mechanism 3
- Claim: Parallelizing the h1 generation and edge prediction steps removes the bottleneck of sequential graph construction.
- Mechanism: In HIGGS, all h1 subgraphs and inter-h1 edge predictions are independent once h2 is fixed, enabling trivial parallelization. This reduces wall-clock time and memory pressure.
- Core assumption: Each h1 subgraph and edge pair can be generated independently without requiring global coordination or information from other parts of the graph.
- Evidence anchors:
  - [section] "Crucially, and unlike block-generative or motif-based models, stages two and three consist of many independent jobs, making them trivially paralellisable."
  - [section] "This means that the complexities of HIGGS... are O(N 2 G) in the segmentation stage... O(N 2 GN2) (time) and O((NG/N2)2) (memory) for Stage Two (h1), and O(N 2 G) and O((NG/N2)2) for Stage Three (h1,i ↔ h1,j)."
  - [corpus] No explicit parallel processing evidence in neighbors; this is an inference from the described design.
- Break condition: If the edge prediction stage requires knowledge of edges outside the pair being processed (e.g., global degree constraints), parallelization fails and memory/compute cost increases.

## Foundational Learning

- Concept: Diffusion models for graph generation
  - Why needed here: HIGGS relies on DiGress, a diffusion-based graph generator, to produce both coarse (h2) and fine (h1) structures. Understanding denoising diffusion helps reason about how conditioning is applied and how the model handles discrete graph attributes.
  - Quick check question: What is the forward and reverse process in diffusion models, and how does conditioning (e.g., on node class) modify the reverse denoising steps?

- Concept: Graph partitioning and community detection (Louvain)
  - Why needed here: HIGGS uses Louvain segmentation to create the hierarchical decomposition. Knowing how resolution parameter affects the number/size of partitions is key to tuning HIGGS.
  - Quick check question: How does the Louvain resolution parameter r control the number and size of partitions, and why does r=10 vs r=2 lead to different trade-offs in edge prediction memory?

- Concept: Graph metrics for evaluation (degree, clustering, spectral features)
  - Why needed here: The paper compares generated graphs to originals and benchmarks using MMD and QQ plots. Understanding these metrics clarifies how realism is measured at both global and local scales.
  - Quick check question: What does MMD measure in the context of graph generation, and why might BTER outperform HIGGS on whole-graph metrics but underperform on community-level metrics?

## Architecture Onboarding

- Component map: Large graph dataset -> Louvain partitioning -> h1 subgraphs and h2 meta-graph -> DiGress models (h2, h1, edge-DiGress) -> Generated large graph with attributes

- Critical path:
  1. Segment original graph into h1 subgraphs and h2 meta-graph using Louvain partitioning
  2. Train DiGress on h2, h1 subgraphs, and inter-h1 edge pairs
  3. Generate h2 -> for each node, generate h1 -> for each edge, generate inter-h1 edges

- Design tradeoffs:
  - High Louvain resolution → many small h1s → smaller memory per subgraph but more edge pairs to predict
  - Low resolution → large h1s → fewer edge pairs but memory may exceed edge-DiGress limits
  - Conditioning depth → richer attributes → more realistic subgraphs but more complex model

- Failure signatures:
  - Unrealistic degree distribution → check if h2 connectivity or conditioning signal is too sparse
  - Low clustering in generated subgraphs → verify DiGress was trained with clustering features; check if edge-DiGress masks edges incorrectly
  - Excessive number of connected components → likely due to too sparse h2 connectivity; try lowering resolution or adjusting edge probability

- First 3 experiments:
  1. Generate Cora at r=2: verify h1 sizes <500 nodes; measure memory usage in edge-DiGress stage
  2. Generate SBM with varying resolution: compare clustering coefficient distributions across resolutions
  3. Parallelize h1 generation: measure speedup vs. sequential generation; confirm correctness of output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would alternative graph partitioning algorithms affect the quality of generated graphs in HIGGS?
- Basis in paper: [explicit] The paper discusses using Louvain segmentation for partitioning but notes "A different partitioning algorithm during training data creation could also have beneficial results, particularly one that aims to produce partitions of a near-uniform size"
- Why unresolved: The current implementation only tests Louvain partitioning, and the authors suggest that different algorithms could yield improvements but do not explore this
- What evidence would resolve it: Comparative experiments using different partitioning algorithms (e.g., Leiden, METIS) with HIGGS across multiple graph datasets and evaluation metrics

### Open Question 2
- Question: Can HIGGS be extended to generate geometric or quasi-geometric graphs like molecules and road networks?
- Basis in paper: [explicit] The authors state "Additionally in this work, we do not apply HIGGS to any geometric or quasi-geometric graphs such as molecules or road networks" and note these graphs would need "a different edge sampling process, which considers prior connections"
- Why unresolved: The current implementation focuses on social network graphs and does not address the unique structural constraints of geometric graphs
- What evidence would resolve it: Implementation and evaluation of HIGGS on molecular datasets or road networks, with appropriate modifications to handle geometric constraints

### Open Question 3
- Question: How would edge-predictive models that incorporate information from prior edge samplings improve HIGGS performance?
- Basis in paper: [explicit] The authors note "One core assumption of our HIGGS implementation here is that the edges between h1 graph pairs can be reasonably sampled without representation of where other h1 graphs have been connected" and suggest "Passing of information from prior edge samplings, perhaps through node embeddings or similar, may improve performance"
- Why unresolved: The current implementation treats edge prediction between h1 graphs as independent tasks without leveraging information from previously sampled edges
- What evidence would resolve it: Experiments comparing the current independent edge sampling approach with variants that incorporate information from previously sampled edges, measuring improvements in graph realism metrics

## Limitations

- The conditioning approach for inter-h1 edge prediction lacks clarity on how edge attributes from h2 are propagated to h1 nodes
- The claim that HIGGS "extends the scale by quadratic order" assumes edge-DiGress can be applied without exceeding its memory limits, but the paper does not specify what those limits are
- The framework has only been tested on social network graphs and has not been extended to geometric or quasi-geometric graphs like molecules or road networks

## Confidence

- **High confidence**: The hierarchical decomposition mechanism is clearly described and theoretically sound
- **Medium confidence**: The parallelizability claim is supported by the independence of h1 generation, but actual implementation details and performance gains are not provided
- **Low confidence**: The edge-DiGress conditioning mechanism is underspecified, making it difficult to assess whether the approach can generalize beyond the datasets tested

## Next Checks

1. **Edge-DiGress conditioning verification**: Test whether conditioning on h2 edge attributes alone is sufficient to generate realistic inter-h1 edges by comparing against ablations with random or node-class-only conditioning
2. **Resolution parameter sweep**: Systematically vary Louvain resolution on Facebook and measure memory usage and edge prediction quality to identify practical scalability limits
3. **Generalization test**: Apply HIGGS to a dataset with known community structure but no categorical attributes to verify whether the framework can generate realistic graphs without explicit node class conditioning