---
ver: rpa2
title: Does Localization Inform Editing? Surprising Differences in Causality-Based
  Localization vs. Knowledge Editing in Language Models
arxiv_id: '2301.04213'
source_url: https://arxiv.org/abs/2301.04213
tags:
- layer
- tracing
- editing
- edit
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether localization results from Causal
  Tracing can inform model editing in language models. Surprisingly, it finds that
  editing success is essentially unrelated to where factual information is stored,
  as measured by Causal Tracing.
---

# Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models

## Quick Facts
- **arXiv ID:** 2301.04213
- **Source URL:** https://arxiv.org/abs/2301.04213
- **Reference count:** 40
- **Key outcome:** Editing success is essentially unrelated to where factual information is stored, as measured by Causal Tracing

## Executive Summary
This paper investigates whether localization results from Causal Tracing can inform model editing in language models. Surprisingly, it finds that editing success is essentially unrelated to where factual information is stored, as measured by Causal Tracing. Even when attempting to reconnect localization with editing through four variants of the editing problem, tracing effects explain only a small fraction of the variance in editing performance. The choice of edit layer is a far better predictor of success. These results suggest that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.

## Method Summary
The study uses GPT-J (6 billion parameter autoregressive language model) and the CounterFact dataset containing subject-relation-object facts. Causal Tracing identifies MLP layer representations containing factual information by adding Gaussian noise to subject token embeddings and measuring how restoring specific representations changes the probability of the true target object. Various editing methods (ROME, MEMIT, constrained finetuning) are then applied at different MLP layers to inject new facts or manipulate existing ones. The success of these edits is measured through rewrite scores, paraphrase generalization, and neighborhood preservation metrics.

## Key Results
- Editing success is essentially unrelated to where factual information is stored, as measured by Causal Tracing
- Layer choice explains 94.7% of variance in editing success versus 0.1% for tracing effect
- Information in Transformer forward passes accumulates gradually across layers, allowing edits at one layer to override information stored elsewhere

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal Tracing identifies MLP layer representations that contain factual information about a target fact.
- **Mechanism:** The method adds Gaussian noise to subject token embeddings and measures how restoring a specific representation changes the probability of the true target object. The larger the change, the more information about the fact is stored in that representation.
- **Core assumption:** The strength of a representation's effect on the true target probability reflects the degree to which that representation stores the fact.

### Mechanism 2
- **Claim:** Model editing success is unrelated to where facts are localized by Causal Tracing.
- **Mechanism:** Editing methods like ROME optimize a rank-one update to an MLP layer's down-projection matrix to maximize the probability of a new target object. This edit changes the model's behavior regardless of whether the original fact was stored in that layer according to tracing results.
- **Core assumption:** Editing a layer that doesn't contain the original fact can still successfully override the model's output by changing the accumulated information flow through the network.

### Mechanism 3
- **Claim:** Information in Transformer forward passes accumulates gradually across layers, allowing edits at one layer to override information stored elsewhere.
- **Mechanism:** Each layer's output builds on previous layers' representations. An edit at layer k can change the information flow such that downstream layers produce the desired output, even if the original fact was stored in a different layer ℓ.
- **Core assumption:** The network's information flow is mutable enough that a single edit can override accumulated representations across multiple layers.

## Foundational Learning

- **Concept:** Autoregressive language models generate text one token at a time, conditioning each prediction on previous tokens.
  - **Why needed here:** Understanding this helps explain why Causal Tracing measures information content by seeing how restoring a representation affects the probability of the true target object.
  - **Quick check question:** In an autoregressive model, what determines the probability distribution for the next token?

- **Concept:** MLP layers in Transformers apply a linear transformation followed by a non-linear activation to the input representation.
  - **Why needed here:** ROME and MEMIT specifically edit MLP down-projection matrices, so understanding their structure is crucial for grasping how these methods work.
  - **Quick check question:** What are the two main components of an MLP layer's computation in a Transformer?

- **Concept:** Representation denoising measures information content by comparing outputs with and without a specific representation.
  - **Why needed here:** Causal Tracing uses this principle to identify which representations contain information about a target fact by measuring how much restoring them changes the output.
  - **Quick check question:** How does representation denoising estimate the information content of a hidden representation?

## Architecture Onboarding

- **Component map:** GPT-J (28-layer Transformer) -> CounterFact dataset (subject-relation-object facts) -> Causal Tracing (localization method) -> Editing methods (ROME, MEMIT, constrained finetuning)
- **Critical path:** For tracing: forward pass → noise subject → measure probability change when restoring representation → identify layers with high effects. For editing: select layer → apply rank-one update or finetuning → measure rewrite/paraphrase/neighborhood scores.
- **Design tradeoffs:** Tracing uses a window size parameter that balances localization precision vs. noise; editing methods trade off between single-layer edits (ROME) vs. multi-layer edits (MEMIT) and between preserving other knowledge vs. maximizing rewrite success.
- **Failure signatures:** Low rewrite scores across all layers suggest the model didn't learn the fact well; high paraphrase scores but low neighborhood scores suggest overfitting to the specific prompt; negative correlations between tracing and editing success indicate the disconnect described in the paper.
- **First 3 experiments:**
  1. Run Causal Tracing on a CounterFact datapoint to visualize where the fact appears to be stored, then apply ROME at layer 6 and measure edit success.
  2. Repeat experiment 1 but edit at the layer where tracing shows the maximum effect, then compare rewrite scores.
  3. Apply Fact Forcing editing (maximize probability of true target with noised input) and measure whether tracing effects now correlate with edit success.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed disconnect between localization and editing generalize to other forms of knowledge beyond simple subject-relation-object facts in CounterFact?
- **Basis in paper:** The authors note that CounterFact uses a "basic form of factual knowledge" and acknowledge that conclusions may not generalize to other forms of knowledge or relations.
- **Why unresolved:** The paper only tests on CounterFact data. No experiments were conducted with different knowledge types or relation structures.
- **What evidence would resolve it:** Experiments showing whether localization-editing disconnects persist when editing more complex knowledge types (e.g., numerical reasoning, procedural knowledge, multi-hop facts).

### Open Question 2
- **Question:** Would larger language models (beyond GPT-J's 6B parameters) show stronger relationships between localization and editing due to their more refined knowledge representations?
- **Basis in paper:** The authors explicitly state their conclusions may not generalize to models larger than GPT-J and note that larger models exhibit phase changes in behavior under prompting.
- **Why unresolved:** All experiments were conducted on a single 6B parameter model. No scaling analysis was performed.
- **What evidence would resolve it:** Comparative studies measuring localization-editing correlations across models of increasing scale (e.g., GPT-3, GPT-4) to identify any scaling effects on the relationship.

### Open Question 3
- **Question:** Does the disconnect between localization and editing persist when using localization methods that target different model components (e.g., neurons, directions in latent space) rather than MLP layers?
- **Basis in paper:** The authors focus specifically on MLP layers following evidence of their role in factual association, but acknowledge other localization methods exist targeting different components.
- **Why unresolved:** The paper only uses Causal Tracing on MLP layers. No experiments tested alternative localization methods or target components.
- **What evidence would resolve it:** Experiments applying different localization methods (e.g., neuron-level analysis, concept activation vectors) and measuring their predictive power for editing success across various model components.

## Limitations
- The study focuses only on MLP layers, not exploring whether similar disconnects exist for other architectural components like attention mechanisms
- Results may be specific to the editing methods tested (ROME, MEMIT, constrained finetuning) and may not generalize to all possible editing approaches
- Conclusions are based on a single 6B parameter model, with no analysis of how results scale with model size

## Confidence

**High confidence:** The empirical finding that layer choice explains far more variance in editing success than tracing effects (94.7% vs 0.1%). This is directly supported by regression analysis and is robust across multiple editing problems.

**Medium confidence:** The broader claim that better mechanistic understanding of pretrained LMs may not translate to editing insights. While the data supports this for the specific methods tested, it may not generalize to all possible editing approaches.

**Medium confidence:** The assertion that information accumulation across layers allows edits to override stored facts. This mechanism is plausible but not directly tested - the paper shows correlations but doesn't establish causal pathways.

## Next Checks
1. Test whether editing methods that explicitly target representations identified by tracing (rather than random layer selection) show improved performance, which would challenge the paper's central claim about the localization-editing disconnect.
2. Extend the analysis to attention weight parameters and other architectural components beyond MLP layers to determine if the disconnect is specific to MLPs or more general across Transformer architectures.
3. Conduct ablation studies where different components of the editing methods are modified (e.g., using different regularization schemes or optimization objectives) to identify whether the lack of correlation stems from the editing approach itself or represents a more fundamental property of information flow in Transformers.