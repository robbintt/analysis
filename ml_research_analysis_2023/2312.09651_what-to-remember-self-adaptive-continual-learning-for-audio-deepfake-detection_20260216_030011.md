---
ver: rpa2
title: 'What to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection'
arxiv_id: '2312.09651'
source_url: https://arxiv.org/abs/2312.09651
tags:
- learning
- audio
- deepfake
- detection
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of audio deepfake detection under
  continual learning, where models must adapt to new attack types without forgetting
  previous knowledge. The proposed Radian Weight Modification (RWM) method categorizes
  classes based on feature similarity across tasks, using in-class cosine distance
  to distinguish compact (e.g., genuine audio) from dispersed (e.g., various fake
  audio) distributions.
---

# What to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection

## Quick Facts
- arXiv ID: 2312.09651
- Source URL: https://arxiv.org/abs/2312.09651
- Reference count: 8
- Primary result: Radian Weight Modification (RWM) outperforms mainstream continual learning methods on audio deepfake detection and image recognition tasks.

## Executive Summary
This paper introduces Radian Weight Modification (RWM), a novel continual learning method for audio deepfake detection that adapts to new attack types without forgetting previous knowledge. RWM categorizes classes based on feature similarity using in-class cosine distance, applying different gradient modification strategies for compact (e.g., genuine audio) versus dispersed (e.g., various fake audio) distributions. The method employs a self-attention mechanism to learn optimal gradient modification directions, achieving superior performance on both audio deepfake detection benchmarks and image recognition tasks compared to existing continual learning approaches.

## Method Summary
RWM operates by first categorizing classes into compact and dispersed feature distribution groups based on in-class cosine distance calculations. For each batch of input data, a self-attention mechanism computes attention scores that determine how much to modify gradients through learned rotated radians (LRR). The algorithm applies orthogonal gradient modifications for dissimilar features to preserve learned knowledge, while using minimal modifications for similar features to enable adaptation. This adaptive approach allows the model to maintain performance on previous tasks while effectively learning new attack patterns.

## Key Results
- RWM outperforms mainstream continual learning methods (EWC, LwF, OWM, DFWF) on ASVspoof datasets with significant improvements in Equal Error Rate
- Achieves strong performance even with limited training samples, demonstrating robustness to data scarcity
- Shows potential for broader machine learning applications, validated on the CLEAR benchmark image recognition dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RWM categorizes classes based on feature similarity across tasks using in-class cosine distance, enabling different gradient modification strategies for compact vs dispersed distributions.
- Mechanism: Classes with compact feature distributions (e.g., genuine audio) receive minimal gradient modification to preserve learned knowledge, while classes with dispersed distributions (e.g., various fake audio) receive orthogonal gradient modifications to prevent interference with previous tasks.
- Core assumption: The in-class cosine distance reliably captures the compactness of feature distributions across different datasets, and this property remains consistent when new attack types are introduced.
- Evidence anchors:
  - [abstract]: "The fundamental concept underlying RWM involves categorizing all classes into two groups: those with compact feature distributions across tasks, such as genuine audio, and those with more spread-out distributions, like various types of fake audio."
  - [section]: "We first consider a feed-forward network... We compute the compactness of all categories by the average cosine distance between each two samples across all tasks, as shown in Eq 2"

### Mechanism 2
- Claim: The self-attention mechanism learns optimal gradient modification directions based on current input batch, enabling adaptive continual learning.
- Mechanism: Attention scores are calculated for each sample in a batch, normalized, and converted to learned rotated radians (LRR). These LRR values determine how much to rotate the gradient modification direction between the orthogonal direction P and the identity direction Q.
- Core assumption: The self-attention mechanism can effectively learn to distinguish between samples that should preserve knowledge versus those that should adapt to new information based on their feature similarity patterns.
- Evidence anchors:
  - [abstract]: "RWM employs a self-attention mechanism to learn optimal gradient modification directions for different data types"
  - [section]: "To make the model learn the adaptive modification direction automatically, a self-attention (SA) mechanism is then introduced before the classifier to obtain the attention score for each sample in a batch."

### Mechanism 3
- Claim: RWM achieves knowledge preservation by modifying gradients in directions that are either orthogonal to previous data planes (for dissimilar features) or aligned with previous data planes (for similar features).
- Mechanism: For dissimilar features, gradients are modified in the direction orthogonal to the previous task's data plane (using direction P), preserving previously learned knowledge. For similar features, gradients are modified minimally by aligning with the previous data plane (using direction Q), minimizing interference.
- Core assumption: Orthogonal modifications for dissimilar features effectively preserve learned knowledge while allowing adaptation to new attack types, and this preservation property holds across different deepfake detection scenarios.
- Evidence anchors:
  - [abstract]: "By categorizing classes into two groups—those with compact feature distributions across tasks and those with more disparate distributions—we employ distinct strategies. When confronted with data featuring distinct characteristics across tasks... the algorithm guides the model to adopt a direction orthogonal to the previous data plane, ensuring preservation of learned knowledge"
  - [section]: "For those samples that have large differences in features across different tasks... the algorithm guides the model to adopt a direction orthogonal to the previous data plane, ensuring preservation of learned knowledge during adaptation"

## Foundational Learning

- Concept: Cosine distance and its relationship to feature distribution compactness
  - Why needed here: RWM relies on in-class cosine distance to categorize classes into compact vs dispersed groups, which determines the gradient modification strategy
  - Quick check question: If you have three samples A, B, and C with cosine distances of 0.1 between A-B and A-C, and 0.9 between B-C, what does this suggest about the feature distribution compactness?

- Concept: Orthogonal projection in high-dimensional spaces
  - Why needed here: RWM uses orthogonal projections to calculate gradient modification directions that preserve learned knowledge while allowing adaptation
  - Quick check question: If you have two vectors v1 and v2 in 3D space, how do you compute the projection of v1 onto the plane orthogonal to v2?

- Concept: Self-attention mechanisms and their training dynamics
  - Why needed here: RWM uses self-attention to learn attention scores that determine gradient modification directions, requiring understanding of how attention weights are learned and normalized
  - Quick check question: Given attention scores [2, 1, 0.5] for three samples, what are the normalized attention weights after applying softmax?

## Architecture Onboarding

- Component map: Wav2vec 2.0 feature extractor → S-CNN classifier (3 Conv layers, self-attention layer, 2 FC layers) → RWM gradient modification layer → Loss computation
- Critical path: Input audio → Wav2vec 2.0 features → S-CNN forward pass → Self-attention scores → LRR calculation → Gradient modification direction R → Backpropagation with modified gradients
- Design tradeoffs: RWM trades computational complexity (additional attention mechanism and projection calculations) for better continual learning performance versus simpler approaches like EWC or LwF
- Failure signatures:
  - Training instability or divergence due to numerical issues in projection calculations
  - Poor performance on new tasks indicating the attention mechanism isn't learning useful modification directions
  - Catastrophic forgetting despite RWM implementation suggesting incorrect categorization of compact vs dispersed features
- First 3 experiments:
  1. Implement RWM on a single dataset with known compact (genuine) and dispersed (fake) classes to verify the categorization works as expected
  2. Test RWM on two datasets with similar acoustic properties to verify knowledge preservation works before testing on more challenging scenarios
  3. Conduct ablation studies removing the self-attention mechanism to verify it's providing meaningful gradient direction learning versus using fixed directions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RWM method perform when applied to real-world audio deepfake detection scenarios with varying levels of acoustic environment differences?
- Basis in paper: [explicit] The paper mentions that the method is evaluated on datasets with significant acoustic environment differences, but does not provide specific results for real-world scenarios.
- Why unresolved: The paper focuses on controlled experiments and does not explore the method's performance in diverse, real-world acoustic environments.
- What evidence would resolve it: Conducting experiments with real-world audio data from various sources and environments, comparing the method's performance to other continual learning approaches.

### Open Question 2
- Question: What is the optimal hyperparameter rs for the RWM method in different machine learning domains, such as image recognition?
- Basis in paper: [explicit] The paper mentions that the hyperparameter rs is used to split classes into two groups, but does not provide a systematic approach to determine its optimal value for different domains.
- Why unresolved: The paper only provides a general guideline for choosing rs and does not explore its impact on performance across various machine learning tasks.
- What evidence would resolve it: Conducting a systematic study to determine the optimal rs value for different domains, considering factors such as feature distribution and task complexity.

### Open Question 3
- Question: How can the RWM method be extended to address other related problems in machine learning, such as domain adaptation, transfer learning, and multi-task learning?
- Basis in paper: [explicit] The paper suggests that the method can be extended to other machine learning domains, but does not provide specific details on how it can be adapted for these tasks.
- Why unresolved: The paper focuses on audio deepfake detection and image recognition, and does not explore the method's potential for other machine learning problems.
- What evidence would resolve it: Developing and testing the RWM method on various machine learning tasks, such as domain adaptation, transfer learning, and multi-task learning, and comparing its performance to existing approaches.

## Limitations

- The core assumption that in-class cosine distance reliably captures feature distribution compactness may not hold for all deepfake scenarios, particularly novel attack types with sophisticated feature distributions.
- Computational overhead from self-attention and orthogonal projection calculations may limit scalability to larger datasets or real-time applications.
- Performance depends heavily on careful hyperparameter tuning, particularly for rs, which may need dataset-specific optimization.

## Confidence

- High confidence: The fundamental approach of using feature similarity for gradient modification direction selection is well-grounded in continual learning literature.
- Medium confidence: The effectiveness of the self-attention mechanism in learning optimal modification directions across different audio deepfake scenarios.
- Low confidence: The generalizability of the compact/dispersed categorization assumption to all types of deepfake attacks, particularly novel attack methods.

## Next Checks

1. **Cross-dataset validation study**: Test RWM on three additional audio deepfake detection datasets with varying attack types and feature distributions to verify the compact/dispersed categorization assumption holds across diverse scenarios.

2. **Ablation study on self-attention mechanism**: Remove the self-attention component and use fixed gradient modification directions based on predetermined thresholds of in-class cosine distance. Compare performance to validate whether the learned attention scores provide significant advantages.

3. **Numerical stability analysis**: Conduct systematic testing of the orthogonal projector calculations across different batch sizes and feature dimensionalities to identify conditions under which numerical instability occurs and implement regularization techniques.