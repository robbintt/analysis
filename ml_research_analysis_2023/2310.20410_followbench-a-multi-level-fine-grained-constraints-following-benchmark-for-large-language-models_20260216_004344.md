---
ver: rpa2
title: 'FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for
  Large Language Models'
arxiv_id: '2310.20410'
source_url: https://arxiv.org/abs/2310.20410
tags:
- constraints
- instruction
- llms
- constraint
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FollowBench is a novel multi-level fine-grained constraints following
  benchmark for evaluating instruction-following capabilities of large language models.
  It includes five types of constraints (Content, Scenario, Style, Format, and Example)
  and introduces a multi-level mechanism that incrementally adds constraints to initial
  instructions.
---

# FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2310.20410
- Source URL: https://arxiv.org/abs/2310.20410
- Reference count: 40
- Key outcome: FollowBench is a novel multi-level fine-grained constraints following benchmark for evaluating instruction-following capabilities of large language models

## Executive Summary
FollowBench introduces a comprehensive benchmark for evaluating instruction-following capabilities of large language models through a multi-level fine-grained constraints approach. The benchmark covers five constraint types (Content, Scenario, Style, Format, and Example) and incrementally increases difficulty across five levels. Using strong LLMs as judges with evolution path prompts, the evaluation reveals that performance declines as constraint complexity increases, with proprietary models like GPT-4 significantly outperforming open-source alternatives. The study establishes instruction-following ability as a distinct dimension for assessing LLM proficiency beyond knowledge and reasoning.

## Method Summary
The method involves constructing a multi-level instruction dataset where each level adds exactly one constraint to the initial instruction, creating five difficulty levels. Five constraint categories are systematically evaluated: Content, Scenario, Style, Format, and Example constraints. Evaluation uses strong LLMs (GPT-4, GPT-3.5) as judges with a multi-level-aware prompt template that shows the evolution path from level 1 to the current level. Three metrics assess performance: Hard Satisfaction Rate (HSR) for complete instruction satisfaction, Soft Satisfaction Rate (SSR) for individual constraint satisfaction, and Consistent Satisfaction Levels (CSL) for consecutive level performance. The evaluation compares nine popular LLMs across all constraint types and levels.

## Key Results
- Proprietary models (GPT-4, GPT-3.5) significantly outperform open-source models across all metrics
- Performance declines as the number of constraints increases, with GPT-4 achieving 73.0% HSR while Vicuna-13B achieves only 52.6% HSR
- Instruction-following ability serves as an additional dimension for comprehensively assessing LLM proficiency beyond knowledge and reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-level mechanism incrementally increases constraint difficulty to precisely estimate instruction-following capability
- Mechanism: Starting from simple instructions (level 1), each subsequent level adds exactly one new constraint
- Core assumption: Constraint addition follows a predictable pattern where each new constraint builds upon previous ones
- Evidence anchors: [abstract] "To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism" and [section 3.1] "Different from previous benchmarks, we introduce a Multi-level mechanism"

### Mechanism 2
- Claim: Strong LLM judges with evolution path prompts provide more accurate constraint satisfaction assessment
- Mechanism: Showing judges the complete evolution from level 1 to current level helps identify specific constraint violations
- Core assumption: LLMs can parse and understand incremental constraint evolution when explicitly shown
- Evidence anchors: [abstract] "To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths"

### Mechanism 3
- Claim: The combination of HSR, SSR, and CSL metrics provides comprehensive evaluation across different aspects
- Mechanism: HSR measures complete instruction satisfaction, SSR measures constraint-level satisfaction, CSL identifies maximum consecutive levels satisfied
- Core assumption: These three metrics capture orthogonal aspects of instruction-following that together provide complete characterization
- Evidence anchors: [section 3.2] "we propose three novel metrics to evaluate the instruction-following abilities of LLMs"

## Foundational Learning

- Concept: Constraint types and their characteristics
  - Why needed here: Understanding Content, Scenario, Style, Format, and Example constraints is essential for constructing valid test cases
  - Quick check question: What distinguishes a Scenario constraint from a Content constraint?

- Concept: Multi-level instruction construction
  - Why needed here: The core innovation requires understanding how to systematically add constraints while maintaining instruction coherence
  - Quick check question: How do you ensure that adding a new constraint at level n doesn't invalidate the instruction from level n-1?

- Concept: LLM-based evaluation methodology
  - Why needed here: The evaluation protocol relies on using strong LLMs as judges with specific prompting strategies
  - Quick check question: Why does showing the evolution path improve judge accuracy compared to direct instruction-response pairs?

## Architecture Onboarding

- Component map:
  - Instruction collection and categorization -> Multi-level constraint addition -> Model response generation -> LLM judge evaluation with evolution paths -> Metric computation and analysis

- Critical path:
  1. Instruction collection and categorization
  2. Multi-level constraint addition
  3. Model response generation
  4. LLM judge evaluation with evolution paths
  5. Metric computation and analysis

- Design tradeoffs:
  - Open-ended vs. rule-based evaluation: Chose LLM judges for open-ended cases despite potential bias
  - Manual vs. automated constraint addition: Used GPT-4 for most categories to scale while maintaining quality
  - Metric selection: Chose three metrics to capture different aspects rather than relying on a single score

- Failure signatures:
  - Low CSL but high SSR: Model satisfies individual constraints but fails at combining them
  - High HSR but low SSR: Model succeeds on easy instructions but struggles with complex ones
  - Performance drop between consecutive levels: Indicates specific constraint difficulty

- First 3 experiments:
  1. Test constraint addition logic by having engineers manually verify that level n constraints properly build on level n-1
  2. Validate LLM judge accuracy by comparing their evaluations against human annotations on a sample set
  3. Benchmark the full pipeline on a small subset to identify performance bottlenecks before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of content constraints (e.g., narrowing down the topic vs. setting a higher standard) impact LLM performance differently?
- Basis in paper: Explicit - The paper discusses different types of content constraints and their construction process
- Why unresolved: The paper does not analyze the relative difficulty or impact of different content constraint types on LLM performance
- What evidence would resolve it: Comparative analysis of LLM performance across different content constraint types

### Open Question 2
- Question: What is the relationship between model size and instruction-following capability across different constraint categories?
- Basis in paper: Explicit - The paper compares models of different sizes and discusses performance differences
- Why unresolved: While the paper notes size differences, it doesn't systematically analyze how model size affects different constraint categories
- What evidence would resolve it: Detailed analysis of how model size correlates with performance on each constraint category

### Open Question 3
- Question: How do instruction-following capabilities relate to other LLM abilities like reasoning and knowledge?
- Basis in paper: Explicit - The paper includes a section comparing instruction-following with overall quality, knowledge, and reasoning abilities
- Why unresolved: The paper provides initial comparisons but doesn't explore the relationship between these abilities in depth
- What evidence would resolve it: Comprehensive analysis of correlations between instruction-following performance and performance on reasoning/knowledge benchmarks

## Limitations

- Dependency on powerful LLMs as judges creates potential bias that may not generalize across different domains
- The multi-level constraint addition mechanism assumes additive constraint interactions, but real-world instructions may exhibit non-linear interactions
- Scalability issues with manual data collection process (5-6 hours per constraint category)

## Confidence

- LLM judge methodology: Medium confidence - shows strong performance but potential bias toward specific model families
- Multi-level constraint mechanism: Medium confidence - effective for additive constraints but may fail with non-linear interactions
- Evaluation metrics: High confidence - three metrics capture orthogonal aspects of instruction-following ability

## Next Checks

1. Conduct ablation studies comparing LLM judge accuracy with and without evolution path prompts across different instruction domains to validate the prompting strategy's effectiveness
2. Test the multi-level constraint addition mechanism with non-linear constraint interactions to identify failure conditions where the additive assumption breaks down
3. Evaluate judge bias by having multiple LLM judges assess the same responses and measuring inter-judge agreement rates across different model families