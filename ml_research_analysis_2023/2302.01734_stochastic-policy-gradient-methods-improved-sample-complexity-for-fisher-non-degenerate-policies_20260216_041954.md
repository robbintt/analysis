---
ver: rpa2
title: 'Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate
  Policies'
arxiv_id: '2302.01734'
source_url: https://arxiv.org/abs/2302.01734
tags:
- policy
- gradient
- lemma
- methods
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic policy gradient methods for finding
  globally optimal policies in reinforcement learning. The key problem addressed is
  improving sample complexity beyond the previously known $\tilde{\mathcal{O}}(\varepsilon^{-3})$
  bound for Fisher-non-degenerate parameterized policies.
---

# Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies

## Quick Facts
- arXiv ID: 2302.01734
- Source URL: https://arxiv.org/abs/2302.01734
- Reference count: 40
- Primary result: Achieves $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity with N-PG-IGT and $\tilde{\mathcal{O}}(\varepsilon^{-2})$ with (N)-HARPG

## Executive Summary
This paper addresses the sample complexity bottleneck in stochastic policy gradient methods for finding globally optimal policies. The authors propose two momentum-based algorithms, N-PG-IGT and (N)-HARPG, that improve upon the previous $\tilde{\mathcal{O}}(\varepsilon^{-3})$ bound for Fisher-non-degenerate parameterized policies. The key innovation lies in combining normalization, momentum, and careful variance reduction analysis. N-PG-IGT achieves $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity without requiring importance sampling, while (N)-HARPG further improves this to $\tilde{\mathcal{O}}(\varepsilon^{-2})$ by using Hessian-aided variance reduction. Both algorithms are computationally efficient, requiring at most two trajectory samples per iteration.

## Method Summary
The paper proposes two stochastic policy gradient algorithms: N-PG-IGT (Normalized Policy Gradient with Implicit Gradient Transport) and (N)-HARPG (Normalized/Hessian-Aided Recursive Policy Gradient). N-PG-IGT combines normalization of the gradient direction with momentum and implicit gradient transport to achieve $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity. (N)-HARPG extends this by incorporating Hessian-aided variance reduction, achieving $\tilde{\mathcal{O}}(\varepsilon^{-2})$ sample complexity. Both algorithms leverage the relaxed weak gradient dominance inequality and require Fisher-non-degenerate policy parameterizations. The methods use compatible function approximation and maintain computational efficiency with O(Hd) complexity per iteration.

## Key Results
- N-PG-IGT achieves $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity, improving over the best known $\tilde{\mathcal{O}}(\varepsilon^{-3})$ bound by a factor of $\mathcal{O}(\varepsilon^{-0.5})$
- (N)-HARPG further improves sample complexity to $\tilde{\mathcal{O}}(\varepsilon^{-2})$ while sampling at most two trajectories per iteration
- Both algorithms demonstrate significant performance improvements over Vanilla PG in MuJoCo continuous control tasks (Humanoid, Reacher, Walker2d, Hopper, Halfcheetah)
- Theoretical analysis establishes global convergence guarantees under Fisher-non-degeneracy and compatible function approximation assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The N-PG-IGT algorithm achieves a $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity by combining normalization, momentum, and implicit gradient transport.
- Mechanism: The normalized update direction prevents gradient estimate errors from dominating the true gradient, while the implicit gradient transport mechanism (using the extrapolated parameter $\tilde{\theta}_t$) corrects for distribution shifts without requiring importance sampling. The momentum term reduces variance adaptively.
- Core assumption: The policy parameterization satisfies Assumptions 1-4 (bounded score function, Fisher-non-degeneracy, and compatible function approximation).
- Evidence anchors:
  - [abstract] "N-PG-IGT achieves a $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity... does not require importance sampling or second-order information and samples only one trajectory per iteration"
  - [section 4.2] "N-PG-IGT improves over the best known sample complexity... by a factor of $\mathcal{O}(\varepsilon^{-0.5})$"
  - [corpus] Weak - no direct citations found for this specific mechanism combination
- Break condition: If the Fisher information matrix becomes ill-conditioned (violating Assumption 3), the normalization may amplify numerical errors rather than reduce variance.

### Mechanism 2
- Claim: The (N)-HARPG algorithms achieve $\tilde{\mathcal{O}}(\varepsilon^{-2})$ sample complexity by using Hessian-aided variance reduction without requiring importance sampling.
- Mechanism: The algorithm samples trajectories from both current and previous parameter distributions to construct an unbiased estimator of the gradient difference. This variance reduction technique, combined with momentum and optional normalization, enables faster convergence while maintaining computational efficiency.
- Core assumption: The policy parameterization allows efficient Hessian-vector product computation (Assumption 1-(ii)) and satisfies Fisher-non-degeneracy.
- Evidence anchors:
  - [abstract] "N-HARPG further improves this complexity to $\tilde{\mathcal{O}}(\varepsilon^{-2})$... sample at most two trajectories per iteration"
  - [section 3.2] "the Hessian-vector product... can be easily and efficiently implemented in $\mathcal{O}(H d)$ computational complexity"
  - [corpus] Weak - no direct citations found for this specific variance reduction technique in RL context
- Break condition: If the Hessian-vector product computation becomes unstable due to high variance in the stochastic Hessian estimate, the variance reduction benefit may be negated.

### Mechanism 3
- Claim: The relaxed weak gradient dominance inequality enables global convergence analysis beyond first-order stationarity.
- Mechanism: The inequality $||\nabla J(\theta)|| \geq \sqrt{2\mu(J^* - J(\theta))}$ (up to bias terms) guarantees that any decrease in function value must be accompanied by sufficient gradient norm reduction, creating a geometric convergence pathway that can be exploited by normalized gradient methods.
- Core assumption: The policy parameterization satisfies Assumptions 3 and 4, enabling the derivation of this structural property.
- Evidence anchors:
  - [section 4.2] "Lemma 2 (Relaxed weak gradient domination, [Ding et al., 2022]). Let Assumptions 1-(i), 3 and 4 hold. Then $\forall \theta \in \mathbb{R}^d$, $\varepsilon' + ||\nabla J(\theta)|| \geq \sqrt{2\mu(J^* - J(\theta))}$"
  - [section C] "This structural property of the objective function is one of the key tools of our analysis"
  - [corpus] Weak - no direct citations found for this specific relaxation technique in RL
- Break condition: If the compatible function approximation error $\varepsilon_{bias}$ becomes too large, the constant $\varepsilon'$ may dominate and prevent meaningful progress toward global optimality.

## Foundational Learning

- Concept: Fisher-non-degenerate parameterization
  - Why needed here: Ensures the Fisher information matrix is positive definite, which is crucial for the normalization step and the gradient dominance analysis
  - Quick check question: Why does the Gaussian policy with linear mean parametrization satisfy Fisher-non-degeneracy when the feature map has full row rank?

- Concept: Compatible function approximation
  - Why needed here: Provides the theoretical framework to bound the approximation error when using parameterized policies, which appears as the $\varepsilon_{bias}$ term in convergence guarantees
  - Quick check question: How does the transfer error in Assumption 4 relate to the expressiveness of the policy class?

- Concept: Variance reduction in non-stationary distributions
  - Why needed here: Unlike standard stochastic optimization, RL has non-stationary sampling distributions due to policy updates, requiring specialized techniques like Hessian-aided correction
  - Quick check question: Why does the HARPG algorithm sample from both current and previous parameter distributions?

## Architecture Onboarding

- Component map: Policy parameterization -> Trajectory sampler -> Score function estimator -> Hessian-vector product engine -> Normalization module -> Momentum accumulator -> Parameter update engine

- Critical path: Parameter update ‚Üí Trajectory sampling ‚Üí Gradient estimation ‚Üí Variance reduction ‚Üí Normalization ‚Üí Next parameter
- Design tradeoffs:
  - N-PG-IGT: Simpler implementation, single trajectory sampling, but requires second-order smoothness assumption
  - HARPG: Better sample complexity, uses Hessian information, but needs two trajectory samples per iteration
  - N-HARPG: Combines benefits of both, but normalization may be numerically unstable near deterministic policies
- Failure signatures:
  - Exploding gradient norms during normalization ‚Üí Fisher information matrix becoming singular
  - Poor convergence despite theoretical guarantees ‚Üí High variance in Hessian-vector products
  - Oscillatory behavior ‚Üí Inappropriate step-size schedule
- First 3 experiments:
  1. Implement N-PG-IGT with diagonal Gaussian policy on CartPole-v1 to verify basic functionality
  2. Compare convergence rates of N-PG-IGT vs Vanilla PG on HalfCheetah-v2
  3. Test HARPG on Reacher-v2 to validate Hessian-aided variance reduction benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform with heavy-tailed policy parametrizations beyond Gaussian policies?
- Basis in paper: [inferred] The paper mentions heavy-tailed policy parametrizations in the conclusions and discusses Cauchy policy as an example in Appendix B, but does not empirically evaluate their performance.
- Why unresolved: The paper focuses on Gaussian policies and does not explore the robustness of the methods to different policy classes, particularly those with heavier tails.
- What evidence would resolve it: Empirical results comparing the proposed methods on environments using Cauchy or other heavy-tailed policy parametrizations.

### Open Question 2
- Question: What is the impact of non-uniform state visitation distributions on the sample complexity of the proposed methods?
- Basis in paper: [inferred] The paper assumes a general MDP setting but does not explicitly analyze how non-uniform state visitation affects convergence rates. The Fisher-non-degeneracy assumption may implicitly capture some aspects of this.
- Why unresolved: The analysis focuses on the general case without considering specific properties of state visitation distributions that could influence convergence.
- What evidence would resolve it: Theoretical analysis or empirical studies examining how different state visitation distributions (e.g., due to varying initial conditions or policy structure) affect sample complexity.

### Open Question 3
- Question: How do the proposed methods scale with high-dimensional action spaces beyond the ùí™(ùêªùëë) complexity?
- Basis in paper: [explicit] The paper states that the methods have ùí™(ùêªùëë) per-iteration computation cost, where ùëë is the dimension of policy parameters, but does not analyze scalability for very high-dimensional action spaces.
- Why unresolved: The analysis does not address potential computational bottlenecks or memory issues that may arise in high-dimensional settings, such as those encountered in robotics or complex control tasks.
- What evidence would resolve it: Empirical results or theoretical analysis demonstrating the methods' performance and computational requirements as action space dimension increases significantly beyond typical benchmarks.

## Limitations
- The theoretical analysis relies on strong assumptions including Fisher-non-degeneracy and compatible function approximation, which may not hold in practice
- The algorithms require careful tuning of momentum parameters and step-size schedules for optimal performance
- The extension to general exponential family policies beyond Gaussians may face computational challenges in computing gradients and Hessians efficiently

## Confidence
- **High Confidence**: The theoretical framework and proof techniques are sound given the stated assumptions. The sample complexity bounds follow logically from the relaxed gradient dominance inequality and variance reduction analysis.
- **Medium Confidence**: The practical performance improvements depend on implementation details that aren't fully specified (Hessian-vector computation, weight clipping, initialization). The momentum parameters (Œ≤) and step-size schedules need careful tuning for each environment.
- **Low Confidence**: The extension to general exponential family policies beyond Gaussians may face computational challenges in computing gradients and Hessians efficiently.

## Next Checks
1. **Implementation Fidelity Check**: Compare the variance of gradient estimates and Hessian-vector products during training to verify the variance reduction mechanism is working as intended.
2. **Assumption Violation Stress Test**: Systematically test what happens when the Fisher information matrix becomes ill-conditioned or when the compatible function approximation error is large.
3. **Computational Efficiency Benchmark**: Measure actual wall-clock time per iteration for N-PG-IGT vs N-HARPG to verify the claimed O(Hd) complexity advantage of the former.