---
ver: rpa2
title: 'SpecTr: Fast Speculative Decoding via Optimal Transport'
arxiv_id: '2310.15141'
source_url: https://arxiv.org/abs/2310.15141
tags:
- draft
- algorithm
- tokens
- optimal
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpecTr, a new algorithm for accelerating autoregressive
  sampling from large language models. SpecTr extends the speculative decoding framework
  by formulating token-level draft selection as an optimal transport problem with
  membership cost.
---

# SpecTr: Fast Speculative Decoding via Optimal Transport

## Quick Facts
- arXiv ID: 2310.15141
- Source URL: https://arxiv.org/abs/2310.15141
- Reference count: 40
- Primary result: SpecTr achieves 2.13X speedup over autoregressive decoding and 1.37X over speculative decoding on standard benchmarks

## Executive Summary
SpecTr introduces a novel algorithm for accelerating autoregressive sampling from large language models by generalizing speculative decoding to allow multiple candidate drafts per token. The approach formulates token-level draft selection as an optimal transport problem with membership cost, enabling selection of output tokens from k candidate drafts while maintaining the large model's distribution. This leads to improved acceptance probabilities and faster sampling, achieving wall-clock speedups of 2.13X over baseline autoregressive decoding and 1.37X over speculative decoding on standard benchmarks.

## Method Summary
SpecTr extends speculative decoding by allowing k candidate drafts per token, formulated as an optimal transport problem with membership cost. The k-sequential selection algorithm achieves (1-1/e)-optimal multiplicative approximation while being computationally efficient (O(|Ω| log(k)) complexity). The sequence-level selection recursively applies token-level optimal transport to build valid sequences from multiple draft sequences, with the draft model generating K candidate sequences that are filtered based on acceptance probabilities computed using the large model.

## Key Results
- Achieves 2.13X wall-clock speedup over baseline autoregressive decoding
- Provides 1.37X additional speedup over speculative decoding on standard benchmarks
- Demonstrates (1-1/e)-optimal multiplicative approximation in acceptance probability
- Maintains validity of output distribution through recursive sequence-level selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SpecTr improves acceptance probability by allowing multiple draft candidates per token position, reducing the total number of rejected tokens.
- Mechanism: The optimal transport formulation with membership cost enables selecting a single output token from a set of k draft candidates. By allowing k ≥ 1 drafts per token, the probability that at least one candidate matches the large model's distribution increases multiplicatively.
- Core assumption: The draft model's distribution is close to the large model's distribution in total variation distance.
- Evidence anchors:
  - [abstract] "generalize the speculative decoding method to allow for a set of k candidates at the token-level, which leads to an improved optimal membership cost"
  - [section 3] "generalize the speculative decoding method to allow for a set of k candidates at the token-level, which leads to an improved optimal membership cost"
  - [corpus] Weak - no direct comparison to multi-draft approaches in corpus

### Mechanism 2
- Claim: The k-sequential selection algorithm achieves (1-1/e)-optimal multiplicative approximation of the optimal transport plan while being computationally efficient.
- Mechanism: By searching over a single parameter ρ to find the best valid transport plan within a restricted family, the algorithm avoids exponential complexity while maintaining provable guarantees.
- Core assumption: The restricted family of transport plans includes a plan with acceptance probability within (1-1/e) of optimal.
- Evidence anchors:
  - [section 6] "it achieves a (1−1/e)-approximation of the optimal acceptance probability"
  - [section 6] "Moreover, it can be computed in time almost linear with size of domain of a single token"
  - [corpus] Weak - no comparison to other approximate OT algorithms

### Mechanism 3
- Claim: SpecTr's sequence-level selection recursively applies token-level optimal transport to build valid sequences from multiple draft sequences.
- Mechanism: The algorithm treats each token position independently using token-level transport plans, then filters draft sequences based on accepted tokens, ensuring the final output follows the large model's distribution.
- Core assumption: The autoregressive structure allows independent treatment of each token position given the context.
- Evidence anchors:
  - [section 7] "The algorithm proceeds in a recursive fashion"
  - [section 7] "By the property of the token-level selection algorithms and the autoregressive structure of language models, it can be shown that Y is always a valid sample"
  - [corpus] Weak - no empirical validation of recursive correctness

## Foundational Learning

- Concept: Optimal Transport with Membership Cost
  - Why needed here: Provides the mathematical framework to select output tokens from multiple draft candidates while minimizing rejection probability
  - Quick check question: What is the transportation cost function for membership cost when k=3 and the output token appears in exactly 2 of the 3 draft tokens?

- Concept: Total Variation Distance
  - Why needed here: Measures the closeness between draft and large model distributions, directly affecting acceptance probability
  - Quick check question: If two distributions have total variation distance 0.1, what is the maximum possible acceptance probability for a single draft token?

- Concept: Autoregressive Generation and Context Conditioning
  - Why needed here: Ensures that the sequence-level selection maintains validity by conditioning each token on previously accepted tokens
  - Quick check question: Why can we compute conditional probabilities for all draft tokens in parallel during the verification step?

## Architecture Onboarding

- Component map: Draft Model (Ms) -> Token-Level Selection -> Sequence-Level Selection -> Large Model (Mb) -> Verification Engine
- Critical path:
  1. Generate K draft sequences of length L from Ms
  2. Compute Mb(y|xt, zi) for all y, i, z in parallel
  3. Apply sequence-level selection to produce L' accepted tokens
  4. If L' < L, sample correction token from Mb
  5. Use accepted sequence as context for next iteration

- Design tradeoffs:
  - Higher K improves acceptance but increases draft model computation
  - Longer L reduces iteration overhead but may increase correction frequency
  - Larger vocabulary |Ω| increases computational complexity quadratically
  - Multiple candidate selection improves acceptance but adds selection overhead

- Failure signatures:
  - Low block efficiency (< 1.5) suggests draft model too dissimilar from large model
  - High correction frequency (> 30%) indicates poor draft quality or insufficient K
  - Increasing latency with larger K may indicate draft model bottleneck
  - Degraded output quality suggests errors in transport plan implementation

- First 3 experiments:
  1. Measure acceptance probability vs k for simple Bernoulli distributions
  2. Compare block efficiency for K=1,2,4,8 with fixed L=4
  3. Profile wall-clock time breakdown between draft generation, verification, and selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact closed-form expression for the optimal acceptance probability αk(p, q) when k > 1 for general distributions p and q?
- Basis in paper: [explicit] The paper states that while a closed-form expression exists for k = 1 (see Eq. (1)), it is unaware of a general closed-form expression for larger values of k.
- Why unresolved: The paper provides an information-theoretic upper bound (Lemma 3) but does not derive the exact closed-form expression for the optimal acceptance probability when k > 1.
- What evidence would resolve it: A mathematical derivation or proof that provides the exact closed-form expression for αk(p, q) when k > 1 for general distributions p and q.

### Open Question 2
- Question: How does the performance of SpecTr compare to other speculative decoding methods, such as aggressive decoding or retrieval-augmented text, in terms of wall-clock speedup and acceptance probability?
- Basis in paper: [explicit] The paper compares SpecTr to speculative decoding and baseline autoregressive decoding but does not provide a comprehensive comparison with other speculative decoding methods.
- Why unresolved: The paper focuses on the theoretical foundations and experimental results of SpecTr but does not extensively compare its performance to other speculative decoding methods.
- What evidence would resolve it: Experimental results comparing SpecTr to other speculative decoding methods, such as aggressive decoding or retrieval-augmented text, in terms of wall-clock speedup and acceptance probability.

### Open Question 3
- Question: What is the impact of the draft model size on the block efficiency and wall-clock speedup of SpecTr?
- Basis in paper: [explicit] The paper mentions that the size of the draft model might affect the block efficiency and wall-clock speedup, but it does not provide a detailed analysis of this relationship.
- Why unresolved: The paper does not investigate the impact of the draft model size on the performance of SpecTr.
- What evidence would resolve it: Experimental results showing the relationship between the draft model size and the block efficiency and wall-clock speedup of SpecTr.

## Limitations
- Theoretical guarantees may not translate directly to consistent wall-clock speedup across all scenarios
- Computational overhead of generating multiple drafts could negate benefits for very large vocabularies
- Performance heavily depends on similarity between draft and large models
- Hyperparameter sensitivity requires extensive validation for different model pairs

## Confidence
**High confidence:** The optimal transport formulation with membership cost is mathematically sound; the k-sequential algorithm achieves (1-1/e)-approximation under stated assumptions; SpecTr improves upon single-draft speculative decoding empirically

**Medium confidence:** The (1-1/e)-approximation translates to consistent wall-clock speedup across diverse tasks; the computational complexity remains manageable for typical vocabulary sizes; the sequence-level selection algorithm maintains validity in all edge cases

**Low confidence:** Performance generalizes to non-transformer architectures; the approach scales to extreme vocabulary sizes (>100K tokens) without modification; hyperparameters can be easily tuned without extensive validation

## Next Checks
1. **Ablation study on draft quality:** Systematically vary the similarity between draft and large models to quantify the relationship between total variation distance and acceptance probability.

2. **Complexity analysis at scale:** Implement the token-level selection algorithm and measure actual runtime as vocabulary size increases from 10K to 100K+ tokens.

3. **Edge case behavior testing:** Create pathological draft sequences to stress-test the sequence-level selection algorithm and verify validity is maintained in worst-case scenarios.