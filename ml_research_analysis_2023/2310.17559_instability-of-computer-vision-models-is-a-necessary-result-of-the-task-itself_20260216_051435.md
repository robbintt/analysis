---
ver: rpa2
title: Instability of computer vision models is a necessary result of the task itself
arxiv_id: '2310.17559'
source_url: https://arxiv.org/abs/2310.17559
tags:
- image
- space
- vision
- classification
- instability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that adversarial examples in computer vision
  are inevitable due to symmetries in the data (translational invariance), the categorical
  nature of the classification task, and the fundamental discrepancy of classifying
  images as objects themselves. The authors show that instability arises from the
  manifold structure of the input space, the entanglement of features, and the loss
  of context when images are flattened into 2D representations.
---

# Instability of computer vision models is a necessary result of the task itself

## Quick Facts
- arXiv ID: 2310.17559
- Source URL: https://arxiv.org/abs/2310.17559
- Reference count: 16
- Primary result: Adversarial examples in computer vision are inevitable due to symmetries in data, categorical classification tasks, and the fundamental discrepancy of classifying images as objects themselves.

## Executive Summary
This paper presents a theoretical framework demonstrating that adversarial examples in computer vision are not merely a flaw in current algorithms but an inevitable consequence of how computer vision problems are formulated. The authors argue that instabilities arise from symmetries in the input space, the categorical nature of classification tasks, and the fundamental discrepancy between images and the objects they represent. While these instabilities cannot be eliminated, the paper suggests they can be partially alleviated through increased image resolution, contextual information, exhaustive training data labeling, and system access limitations.

## Method Summary
The paper provides a theoretical analysis of adversarial examples in computer vision, examining the topological and geometric properties of image classification problems. The authors analyze how convolutional neural networks process images through feature extraction, how symmetries in the input space create unstable regions, and how the categorical nature of classification tasks contributes to vulnerability. The analysis extends to comparing computer vision with other domains like graph neural networks, showing that the problem is fundamentally tied to the structure of the task rather than implementation details.

## Key Results
- Adversarial examples are inevitable in computer vision due to symmetries in the data and the categorical nature of classification tasks
- The fundamental discrepancy between images and objects themselves creates an underparameterized problem space
- While instability cannot be eliminated, it can be partially mitigated through increasing image resolution, providing contextual information, exhaustive labeling, and limiting system access
- The presence of adversarial examples does not necessitate colloquial instability; rather, it is the ability of hostile actors to find them that causes practical issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples are inevitable in computer vision due to the entanglement of features in convolutional neural networks.
- Mechanism: Max-pooling and non-differentiable activation functions create regions of instability where points of different feature expressions are dense in each other, leading to fragile features that correlate with output labels at specific points but not in neighborhoods.
- Core assumption: The input space of computer vision problems has symmetries (translational invariance) that increase the number of unstable points as image resolution increases.
- Evidence anchors:
  - [section] "Maxpooling will create the exact kind of undifferentiability as generated in the L1 filtering, suggesting an inherent tendency to regions of high instability along any one particular feature axis apply to most current CNNs."
  - [section] "Though, indeed, fragile features can arise from even very simple types of filtering."
  - [corpus] No direct evidence found in corpus, but the concept of feature entanglement is mentioned in the paper.
- Break condition: The effect becomes negligible as image resolution increases and the volume of unstable regions tends to zero relative to the total input space.

### Mechanism 2
- Claim: Instability in computer vision systems is a result of the categorical nature of the classification task and the fundamental discrepancy of classifying images as objects themselves.
- Mechanism: The image of an object is not the object itself, and the classification problem is underparameterized when treating images as direct representations of objects. This leads to fragile features that correlate with labels at specific points but fall off quickly in neighborhoods.
- Core assumption: The problem space has a high degree of permutation invariance, leading to a larger space of classification-invariant transformations.
- Evidence anchors:
  - [section] "One last point to mention, a subtle but important one, is that the image of something is not the thing itself."
  - [section] "Herein lies what you might call a hubris of prediction; that the most likely thing is always the object of belief."
  - [corpus] No direct evidence found in corpus, but the concept of the image-object discrepancy is mentioned in the paper.
- Break condition: The problem becomes less underparameterized as more contextual information is provided, reducing the symmetries in the problem.

### Mechanism 3
- Claim: The existence of adversarial examples is a necessary result of how computer vision problems are formulated, not a flaw in current algorithms.
- Mechanism: The presence of adversarial examples does not necessitate instability in the colloquial sense; rather, it is the ability of hostile actors to find them that causes practical issues. Protection from adversarial attacks can be increased through increasing image resolution, providing contextual information, exhaustive labeling of training data, and preventing attackers from frequent access to the system.
- Core assumption: The problem cannot be eliminated, but through analysis of the causes, it can be partially alleviated.
- Evidence anchors:
  - [abstract] "Therefore we conclude that instability is a necessary result of how the problem of computer vision is currently formulated."
  - [section] "Protection from adversarial attacks can be increased through: i) increasing the resolution of images, ii) providing contextual information for the image, iii) exhaustive labelling of training data, and iv) preventing attackers from frequent access to the system."
  - [corpus] No direct evidence found in corpus, but the concept of adversarial examples as a necessary result is mentioned in the paper.
- Break condition: The problem becomes less severe as the measures to alleviate instability are implemented and the system becomes more robust to adversarial attacks.

## Foundational Learning

- Concept: Topological spaces and continuity
  - Why needed here: Understanding the formal definition of stability and instability in classification problems requires knowledge of topological spaces and continuity.
  - Quick check question: What is the topological definition of continuity, and how does it relate to the stability of a classifier?

- Concept: Manifold learning and feature extraction
  - Why needed here: The paper discusses how feature extraction in convolutional neural networks can lead to fragile features and instability in the classification boundary.
  - Quick check question: How does the manifold structure of the input space affect the stability of a classifier, and what role do feature extraction techniques play in this?

- Concept: Symmetry and invariance in image classification
  - Why needed here: The paper argues that symmetries in the data (translational invariance) contribute to the inevitability of adversarial examples in computer vision.
  - Quick check question: How do symmetries in the input space affect the stability of a classifier, and what are some examples of these symmetries in computer vision problems?

## Architecture Onboarding

- Component map:
  - Input: Images (2D grids of pixels)
  - Feature extraction: Convolutional layers, max-pooling, and activation functions
  - Manifold learning: Dimensionality reduction and statistical parameter learning
  - Classification: Output layer (softmax) and decision boundary
  - Adversarial examples: Small perturbations to input images that cause misclassification

- Critical path:
  1. Input image is fed into the convolutional neural network
  2. Feature extraction layers process the image and learn statistical parameters
  3. The final layer maps the features to a classification decision
  4. Adversarial examples can be generated by finding small perturbations that cause misclassification

- Design tradeoffs:
  - Higher resolution images can reduce the effect of instability but require more computational resources
  - Providing contextual information can improve stability but may increase the complexity of the model
  - Exhaustive labeling of training data can improve robustness but is time-consuming and expensive
  - Limiting system access can prevent adversarial attacks but may reduce the system's usability

- Failure signatures:
  - Fragile features that correlate with output labels at specific points but not in neighborhoods
  - Unstable regions in the feature space where points of different feature expressions are dense in each other
  - Adversarial examples that are small perturbations to input images but cause significant misclassification

- First 3 experiments:
  1. Generate adversarial examples for a simple convolutional neural network trained on a standard dataset (e.g., MNIST or CIFAR-10) and analyze the fragile features that cause misclassification.
  2. Compare the stability of classifiers trained on high-resolution images versus low-resolution images and analyze the effect of resolution on the number and severity of adversarial examples.
  3. Implement a defense mechanism (e.g., adversarial training or input preprocessing) and evaluate its effectiveness in reducing the number and severity of adversarial examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between image resolution and the probability of adversarial examples occurring, and at what resolution does this probability become negligible for practical purposes?
- Basis in paper: [explicit] The paper states that as image resolution increases, the effect of adversarial examples becomes less significant, and that the probability of falling within epsilon of a boundary point tends to zero as the volume of unstable regions shrinks to zero with increasing input dimension.
- Why unresolved: The paper provides a theoretical framework showing that instability becomes less likely with higher resolution, but does not quantify the exact relationship or provide practical thresholds for when adversarial examples become negligible in real-world applications.
- What evidence would resolve it: Empirical studies measuring the frequency of adversarial examples across a range of image resolutions, identifying the point at which the probability of successful attacks drops below a practical threshold (e.g., less than 1% success rate).

### Open Question 2
- Question: How does the presence of contextual information in images affect the stability of computer vision models, and can this be quantified as a reduction in adversarial vulnerability?
- Basis in paper: [explicit] The paper suggests that providing contextual information for images can help alleviate instability, as the lack of context contributes to the fundamental underparameterization of the problem where the image is treated as identical to the object itself.
- Why unresolved: While the paper proposes context as a potential solution, it does not provide concrete evidence or methodology for how much context is needed or how to quantify its impact on reducing adversarial vulnerability.
- What evidence would resolve it: Controlled experiments comparing model performance with and without contextual information, measuring the reduction in successful adversarial attacks and the relationship between amount of context provided and stability.

### Open Question 3
- Question: What is the minimum level of exhaustive labeling required in training data to significantly reduce the occurrence of adversarial examples, and how does this scale with the complexity of the classification task?
- Basis in paper: [explicit] The paper identifies non-exhaustive labeling of training data as a factor that exacerbates instability, suggesting that comprehensive labeling could help, but does not specify what constitutes "exhaustive" labeling or how to achieve it.
- Why unresolved: The concept of exhaustive labeling is introduced but not operationalized, leaving unclear how much additional labeling is needed and whether there are diminishing returns or practical limits to this approach.
- What evidence would resolve it: Studies measuring model stability as a function of labeling completeness, identifying the point at which additional labeling provides minimal improvement in adversarial robustness and establishing guidelines for practical implementation.

### Open Question 4
- Question: How does the symmetry analysis framework presented in the paper apply to other machine learning domains beyond computer vision, such as natural language processing or graph neural networks?
- Basis in paper: [explicit] The paper extends its symmetry analysis to graph neural networks, noting that they have a much larger space of classification-invariant transformations compared to images, but does not explore other domains or provide a general framework for applying this analysis.
- Why unresolved: While the paper demonstrates the concept with computer vision and briefly mentions graph neural networks, it does not provide a comprehensive methodology for analyzing symmetry in other domains or comparing the relative vulnerability across different types of machine learning tasks.
- What evidence would resolve it: Application of the symmetry analysis framework to multiple domains (e.g., NLP, speech recognition, time series analysis), quantifying the relationship between symmetry group size and adversarial vulnerability, and establishing comparative benchmarks across domains.

## Limitations
- The analysis is primarily theoretical with limited empirical validation
- No specific datasets or experimental setups are provided to test the claims
- The proposed mitigations lack concrete implementation details and evaluation metrics

## Confidence
- High confidence: The core claim that symmetries in data and the categorical nature of classification contribute to adversarial vulnerability
- Medium confidence: The specific mechanisms of feature entanglement and manifold structure leading to instability
- Low confidence: The practical effectiveness of proposed mitigation strategies without empirical validation

## Next Checks
1. Generate adversarial examples for a simple CNN on standard datasets (MNIST/CIFAR-10) and analyze whether fragile features exist as described
2. Systematically compare classifier stability across different image resolutions to test if higher resolution reduces adversarial vulnerability as claimed
3. Implement one proposed mitigation (e.g., context provision or adversarial training) and measure its effectiveness against standard attack methods on benchmark datasets