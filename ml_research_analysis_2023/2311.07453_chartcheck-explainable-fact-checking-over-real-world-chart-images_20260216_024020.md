---
ver: rpa2
title: 'ChartCheck: Explainable Fact-Checking over Real-World Chart Images'
arxiv_id: '2311.07453'
source_url: https://arxiv.org/abs/2311.07453
tags:
- chart
- charts
- claim
- reasoning
- chartcheck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChartCheck is a new dataset for fact-checking against real-world
  charts, containing 1.7k charts and 10.5k human-written claims and explanations.
  The dataset addresses the challenge of verifying claims against data visualizations,
  which requires analyzing both text and visual components, as well as applying various
  reasoning types.
---

# ChartCheck: Explainable Fact-Checking over Real-World Chart Images

## Quick Facts
- arXiv ID: 2311.07453
- Source URL: https://arxiv.org/abs/2311.07453
- Authors: 
- Reference count: 28
- Key outcome: ChartCheck dataset with 1.7k charts and 10.5k claims/explanations achieves 73.9% accuracy on fact-checking tasks.

## Executive Summary
ChartCheck is a novel dataset designed to advance fact-checking capabilities against real-world chart images. It consists of 1.7k charts and 10.5k human-written claims and explanations, addressing the challenge of verifying claims against data visualizations. The dataset requires models to analyze both textual and visual components while applying various reasoning types. ChartCheck enables systematic evaluation of state-of-the-art vision-language and chart-to-table models, revealing performance of 73.9% accuracy in fine-tuned settings while highlighting specific challenges in chart reasoning.

## Method Summary
The authors created ChartCheck by collecting chart images from Wikimedia Commons and using crowdsourcing to generate claims and explanations. They evaluated both vision-language models (MatCha, DePlot) and chart-to-table translation approaches on this dataset. The evaluation followed a two-step approach: first converting charts to tables when applicable, then performing claim classification and explanation generation. Models were fine-tuned on the training set and evaluated on separate test sets, with performance measured using accuracy for classification and standard NLG metrics for explanation generation.

## Key Results
- ChartCheck dataset contains 1.7k real-world charts and 10.5k human-written claims and explanations
- Vision-language and chart-to-table models achieve 73.9% accuracy on claim classification in fine-tuned setting
- Models struggle particularly with chart legends, world knowledge, and complex reasoning types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChartCheck leverages vision-language models to extract visual and textual features from chart images, enabling fact-checking claims against data visualizations.
- Mechanism: Vision-language models like MatCha and DePlot are used to process chart images directly or convert them into tables, followed by claim classification and explanation generation.
- Core assumption: Vision-language models can effectively understand and reason about complex chart data, including visual attributes like colors, positions, and orientations.
- Evidence anchors:
  - [abstract]: "ChartCheck is a novel dataset for explainable fact-checking against real-world charts, consisting of 1.7k charts and 10.5k human-written claims and explanations."
  - [section]: "We evaluate state-of-the-art models for chart reasoning and explanation generation in a fine-tuning, few- and zero-shot setting on ChartCheck."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.348, average citations=0.0. Top related titles: Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking, ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild, ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts.
- Break condition: If vision-language models fail to accurately extract and reason about chart data, the fact-checking process will be compromised.

### Mechanism 2
- Claim: ChartCheck includes a diverse set of real-world charts and human-written claims, capturing the complexity of chart reasoning tasks.
- Mechanism: The dataset consists of 1.7k real-world charts and 10.5k human-written claims and explanations, covering various chart types, reasoning types, and visual attributes.
- Core assumption: The diversity of charts and claims in ChartCheck reflects the real-world complexity of chart fact-checking tasks.
- Evidence anchors:
  - [abstract]: "ChartCheck is a novel dataset for explainable fact-checking against real-world charts, consisting of 1.7k charts and 10.5k human-written claims and explanations."
  - [section]: "ChartCheck includes two testsets: testset t1 and t2. While t1 is a testset with a similar distribution as the training and validation sets, for t2 we only included chart images which are not part of the training set."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.348, average citations=0.0. Top related titles: Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking, ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild, ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts.
- Break condition: If the dataset lacks sufficient diversity in charts, claims, or reasoning types, it may not accurately represent the challenges of real-world chart fact-checking.

### Mechanism 3
- Claim: ChartCheck enables the evaluation of chart reasoning models across various dimensions, such as reasoning types, chart attributes, and model architectures.
- Mechanism: The dataset includes annotations for reasoning types, chart attributes, and model performance breakdowns, allowing for a comprehensive analysis of chart reasoning capabilities.
- Core assumption: Evaluating models on multiple dimensions provides a more thorough understanding of their strengths and weaknesses in chart fact-checking tasks.
- Evidence anchors:
  - [abstract]: "We study chart reasoning types and visual attributes that pose a challenge to these models."
  - [section]: "We also annotate a subset of ChartCheck with (i) reasoning types humans apply for chart understanding and (ii) visual attributes common in charts."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.348, average citations=0.0. Top related titles: Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking, ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild, ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts.
- Break condition: If the evaluation framework fails to capture the nuances of chart reasoning or model performance, it may not provide a complete picture of the challenges and opportunities in this domain.

## Foundational Learning

- Concept: Vision-language models
  - Why needed here: Vision-language models are essential for processing chart images and extracting both visual and textual features necessary for fact-checking claims against data visualizations.
  - Quick check question: Can you explain how vision-language models like MatCha and DePlot process chart images and enable fact-checking?

- Concept: Chart reasoning types
  - Why needed here: Understanding the various reasoning types required for chart fact-checking, such as retrieving values, filtering, and calculating sums or averages, is crucial for developing effective models and evaluation frameworks.
  - Quick check question: What are the different reasoning types humans apply when interacting with chart data, and how do they relate to the challenges of chart fact-checking?

- Concept: Chart attributes
  - Why needed here: Analyzing common chart attributes like legends, axis scales, and data point labels is important for understanding the complexity of chart fact-checking tasks and developing models that can handle diverse chart types and features.
  - Quick check question: How do chart attributes like legends, axis scales, and data point labels impact the difficulty of chart fact-checking tasks, and how can models be designed to handle these challenges?

## Architecture Onboarding

- Component map: Chart images -> Vision-language models (MatCha, DePlot) -> Feature extraction -> Claim classification (DeBERTa, TAPAS) -> Explanation generation (FlanT5) -> Evaluation metrics
- Critical path: Chart image processing and feature extraction -> Data extraction and table generation (if applicable) -> Claim classification and veracity prediction -> Explanation generation and evaluation
- Design tradeoffs:
  - Vision-language models vs. chart-to-table translation: Vision-language models can process charts directly but may struggle with complex reasoning, while chart-to-table translation can simplify the reasoning process but may lose important visual information.
  - Multitask learning vs. separate models: Multitask learning can improve explanation generation but may not necessarily benefit claim classification.
- Failure signatures:
  - Inaccurate feature extraction or data extraction from chart images
  - Inability to handle complex reasoning types or chart attributes
  - Poor performance on less common chart types or claims requiring world knowledge
- First 3 experiments:
  1. Evaluate the performance of vision-language models (MatCha) and chart-to-table baselines (DePlot) on the ChartCheck dataset, comparing their accuracy on claim classification and explanation generation.
  2. Analyze the performance of models across different reasoning types and chart attributes, identifying the challenges and opportunities for improvement.
  3. Investigate the impact of multitask learning on claim classification and explanation generation, comparing the performance of multitask models with separate models for each task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific chart types (e.g., heat maps, scatter plots, histograms) are missing from ChartCheck, and how might their inclusion impact the dataset's utility for fact-checking?
- Basis in paper: [explicit] The paper mentions that certain chart types such as heat maps, scatter plots, and histograms are missing from ChartCheck.
- Why unresolved: The paper does not explore the potential benefits or challenges of including these additional chart types in the dataset.
- What evidence would resolve it: Conducting an analysis of how the inclusion of these chart types would affect the performance of fact-checking models on ChartCheck.

### Open Question 2
- Question: How does the performance of fact-checking models on ChartCheck compare to their performance on datasets containing charts from real-world documents (e.g., scientific papers, news articles)?
- Basis in paper: [inferred] The paper discusses the evaluation of models on ChartCheck but does not compare their performance to models tested on charts from real-world documents.
- Why unresolved: The paper focuses on the creation and evaluation of ChartCheck but does not provide a comparative analysis with other datasets.
- What evidence would resolve it: Performing a comparative study of model performance on ChartCheck versus other datasets with real-world charts.

### Open Question 3
- Question: What are the specific challenges and limitations of translating charts to tables using DePlot, and how do these impact the overall accuracy of fact-checking models?
- Basis in paper: [explicit] The paper mentions that DePlot's chart-to-table translation can lead to errors such as hallucinating labels, confusing similar colors, and missing data points.
- Why unresolved: The paper does not provide a detailed analysis of how these translation errors affect the accuracy of fact-checking models.
- What evidence would resolve it: Conducting a detailed error analysis of DePlot's table translations and correlating these errors with the performance of fact-checking models on ChartCheck.

## Limitations

- The dataset lacks certain chart types like heat maps, scatter plots, and histograms which could limit evaluation scope
- Performance varies significantly across different chart types, with models struggling on donut and area charts
- Extracting information from chart legends and handling claims requiring world knowledge remain challenging

## Confidence

- Claim Cluster 1 (ChartCheck dataset and evaluation): High
- Claim Cluster 2 (Model performance and limitations): Medium
- Claim Cluster 3 (Chart reasoning types and attributes): High

## Next Checks

1. Conduct a detailed analysis of the chart-to-table and vision-language models used, including their specific architectures, hyperparameters, and training configurations, to better understand their performance on the ChartCheck dataset.
2. Perform a comprehensive evaluation of the dataset quality, focusing on potential biases or inconsistencies introduced during the crowdsourcing process. This could involve manual inspection of a subset of claims and explanations or comparing the dataset against other chart reasoning benchmarks.
3. Investigate the impact of chart legends and world knowledge on model performance by designing targeted experiments that specifically test the models' ability to extract information from legends and handle claims requiring external knowledge.