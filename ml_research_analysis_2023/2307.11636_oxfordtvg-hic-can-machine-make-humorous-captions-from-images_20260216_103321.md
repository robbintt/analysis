---
ver: rpa2
title: 'OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?'
arxiv_id: '2307.11636'
source_url: https://arxiv.org/abs/2307.11636
tags:
- humour
- image
- captions
- oxfordtvg-hic
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OxfordTVG-HIC is a large-scale dataset for humour captioning, containing
  2.9M image-text pairs with humour scores. It addresses the scarcity of data in humour
  generation tasks by providing diverse, out-of-context examples conducive to humour.
---

# OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?

## Quick Facts
- arXiv ID: 2307.11636
- Source URL: https://arxiv.org/abs/2307.11636
- Reference count: 40
- Large-scale dataset (2.9M pairs) for humor captioning with position-conditioned loss improving diversity preservation

## Executive Summary
OxfordTVG-HIC introduces a large-scale dataset for humor captioning containing 2.9M image-text pairs with humor scores. The dataset addresses the scarcity of data in humor generation tasks by providing diverse, out-of-context examples conducive to humor. A key contribution is the position-conditioned loss function that mitigates diversity collapse during training by adjusting penalty weights based on token position, allowing the model to handle the high semantic diversity of humorous captions. The work also provides comprehensive evaluation metrics and explainability analysis aligned with benign violation theory of humor.

## Method Summary
The method trains image captioning models (ClipCap and BLIP) on OxfordTVG-HIC using a novel position-conditioned loss that adjusts the penalty weight for false positive predictions based on token position. This addresses the diversity collapse problem of standard cross-entropy loss when training on highly diverse captions. The models are evaluated using humor classifier scores, benignity scores, fluency metrics, and semantic diversity measures. Attention visualization is used to identify visual and linguistic cues that influence humor generation, with results aligned with benign violation theory.

## Key Results
- Position-conditioned loss significantly improves humor score and diversity compared to standard cross-entropy training
- Generated captions achieve high fluency while maintaining humor intensity
- Attention analysis reveals models focus on emotionally salient and incongruous image regions
- The dataset enables effective training of humor captioning models that were previously limited by data scarcity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Position-conditioned loss mitigates the diversity collapse problem inherent in standard cross-entropy training on highly diverse caption datasets.
- **Mechanism:** By adjusting the penalty weight for false positive predictions based on token position, the model relaxes the learning signal early in the sequence (when ground truths are highly diverse) and tightens it later (when ground truths converge), preventing the model from averaging towards a "neutral" token.
- **Core assumption:** The semantic diversity of valid captions decreases monotonically with sequence position; tokens early in the sequence have more plausible alternatives than tokens later in the sequence.
- **Evidence anchors:** The abstract states that cross-entropy tends to generate captions close to all ground truths, which is detrimental for training. The method section describes the token-position-conditioned loss based on the assumption that the closer to the start of the caption, the less strict the condition is for the predicted token.
- **Break condition:** If caption diversity is not position-dependent (e.g., equal ambiguity throughout the sequence) or if the model already learns diversity-preserving representations without position conditioning.

### Mechanism 2
- **Claim:** Humor generation quality correlates with both benign violation and semantic diversity, which are captured by the proposed metrics.
- **Mechanism:** The humor classifier learns to detect violations of semantic norms (humor score) while the benign score filters out offensive content, ensuring that generated captions are both humorous and socially acceptable.
- **Core assumption:** Humor can be decomposed into a "violation" component and a "benign" component, and both can be quantified via trained classifiers.
- **Evidence anchors:** The abstract mentions that qualitative analysis shows cues aligned with benign violation theory. The method section describes the approach for evaluating humor intensity based on a classification model trained on OxfordTVG-HIC.
- **Break condition:** If benign violation theory does not fully explain humor in the dataset, or if the classifiers overfit to dataset-specific cues that do not generalize.

### Mechanism 3
- **Claim:** Attention patterns learned on OxfordTVG-HIC focus on emotionally salient and incongruous image regions, reflecting humor-relevant features.
- **Mechanism:** By training on captions that emphasize emotional and abnormal aspects of images, the model learns to prioritize these regions during generation, aligning with benign violation theory.
- **Core assumption:** Humor-relevant image features are also emotionally intense or incongruous, and these features can be learned from caption-image pairs.
- **Evidence anchors:** The abstract mentions that explainability analysis identifies visual and linguistic cues influential for humor prediction. The method section describes visualizing the attention map of the image encoder to examine how it attends to different parts of the image to produce humorous captions.
- **Break condition:** If humor captions do not consistently emphasize emotional/incongruous regions, or if the attention mechanism does not generalize to unseen images.

## Foundational Learning

- **Concept:** Benign violation theory of humor
  - **Why needed here:** Provides theoretical grounding for both the humor evaluation metrics and the interpretation of attention patterns.
  - **Quick check question:** What are the two necessary components of humor according to benign violation theory, and how are they quantified in this work?

- **Concept:** Cross-entropy vs. position-conditioned loss
  - **Why needed here:** Understanding why standard cross-entropy fails on diverse caption datasets and how position conditioning addresses this.
  - **Quick check question:** In what way does position-conditioned loss change the learning signal for early vs. late tokens in the sequence?

- **Concept:** Multimodal representation learning (CLIP, image encoders)
  - **Why needed here:** The backbone models (CLIPCap, BLIP) rely on joint vision-language embeddings to generate contextually relevant captions.
  - **Quick check question:** How does the CLIP text encoder contribute to diversity evaluation in this work?

## Architecture Onboarding

- **Component map:** Data pipeline → Encoder backbone → Loss module → Decoder → Evaluation → Explainability
- **Critical path:** Data → Encoder → Loss → Decoder → Evaluation → Explainability
- **Design tradeoffs:**
  - Cross-entropy is simpler but collapses diversity; position-conditioned loss is more complex but preserves it.
  - Humor evaluation via classifier is proxy-based and may not generalize; linguistic metrics (BLEU, etc.) are inappropriate for humor.
  - Position-conditioned loss introduces hyperparameters (kernel function, scaling factors) that require tuning.
- **Failure signatures:**
  - Low humor score but high fluency → model is fluent but not humorous (overfitting to linguistic patterns).
  - High humor score but low benign score → model generates offensive humor (violation without benignity).
  - No improvement from position-conditioned loss → diversity is not position-dependent or the kernel function is poorly chosen.
- **First 3 experiments:**
  1. Train baseline ClipCap on OxfordTVG-HIC with cross-entropy; evaluate humor score and diversity.
  2. Replace cross-entropy with position-conditioned loss (sigmoid kernel, α=6); compare humor and diversity scores.
  3. Visualize attention maps for both models; check if the position-conditioned model focuses more on emotional/incongruous regions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the proposed position-conditioned loss be adapted to other image captioning datasets with varying levels of caption diversity?
- **Basis in paper:** The paper mentions that the position-conditioned loss is designed to address the diversity issue in OxfordTVG-HIC, but it doesn't discuss its applicability to other datasets.
- **Why unresolved:** The paper focuses on the effectiveness of the position-conditioned loss for OxfordTVG-HIC and doesn't explore its performance on other captioning datasets with different characteristics.
- **What evidence would resolve it:** Experiments comparing the performance of the position-conditioned loss on various captioning datasets with different levels of caption diversity would provide insights into its adaptability and generalizability.

### Open Question 2
- **Question:** What are the specific cultural and linguistic differences in humor that can be explored using the OxfordTVG-HIC dataset?
- **Basis in paper:** The paper mentions that the dataset is collected from different cultural websites and communities, but it doesn't delve into the specific cultural and linguistic differences in humor that can be analyzed.
- **Why unresolved:** The paper highlights the potential for exploring cultural and linguistic differences in humor but doesn't provide a detailed analysis or examples of such differences.
- **What evidence would resolve it:** A comprehensive analysis of the dataset, including a comparison of humor styles and themes across different cultural and linguistic groups, would shed light on the specific differences and their impact on humor generation.

### Open Question 3
- **Question:** How can the explainability analysis of humor models be further improved to provide more insights into the cognitive factors involved in humor perception?
- **Basis in paper:** The paper mentions that the explainability analysis identifies visual and linguistic cues influential for humor prediction, but it doesn't discuss how this analysis can be enhanced to uncover deeper cognitive factors.
- **Why unresolved:** The paper provides a basic understanding of the cues that contribute to humor but doesn't explore how to gain a more comprehensive understanding of the underlying cognitive processes.
- **What evidence would resolve it:** Advanced techniques for explainability analysis, such as incorporating cognitive psychology theories or using more sophisticated visualization methods, could provide a deeper understanding of the cognitive factors involved in humor perception.

## Limitations
- The core empirical claims rest on a single dataset and two backbone models, limiting generalizability
- Benign violation theory grounding relies on proxy classifiers trained on the same dataset, creating potential circularity
- Explainability analysis using attention visualization provides qualitative support but lacks quantitative validation against human judgments

## Confidence
**High Confidence:**
- OxfordTVG-HIC is a genuinely large-scale humor captioning dataset with diversity that challenges standard cross-entropy training
- Position-conditioned loss provides measurable improvements over cross-entropy on this dataset
- The dataset and evaluation framework are technically sound and reproducible

**Medium Confidence:**
- The position-conditioned loss mechanism will generalize to other diverse caption datasets
- Benign violation theory adequately explains humor patterns in the generated captions
- Attention patterns genuinely reflect humor-relevant features rather than dataset artifacts

**Low Confidence:**
- The humor classifier's scores reliably measure human-perceived humor intensity
- The specific kernel function (sigmoid with α=6) is optimal or near-optimal
- The explainability findings would replicate on completely different image domains

## Next Checks
1. **Cross-dataset validation:** Train the same position-conditioned models on HUMORCHAIN or Cartoon Caption datasets and measure whether diversity and humor scores improve relative to cross-entropy baselines.
2. **Human evaluation study:** Conduct a controlled experiment where human annotators rate generated captions for humor, benignity, and fluency, then correlate these ratings with the automated metrics to validate the proxy measures.
3. **Ablation on kernel parameters:** Systematically vary the α parameter and kernel function (linear, gaussian, etc.) in the position-conditioned loss to determine sensitivity and identify whether the sigmoid with α=6 is optimal or merely adequate.