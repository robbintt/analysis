---
ver: rpa2
title: Breast Cancer classification by adaptive weighted average ensemble of previously
  trained models
arxiv_id: '2311.13206'
source_url: https://arxiv.org/abs/2311.13206
tags:
- cancer
- which
- breast
- used
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a novel approach to breast cancer classification
  using adaptive weighted average ensemble of previously trained convolutional neural
  network models. The method combines fully trained models (ResNet-50, DenseNet-201,
  and Inception-V3) by averaging their outputs with weights proportional to their
  individual accuracies.
---

# Breast Cancer classification by adaptive weighted average ensemble of previously trained models

## Quick Facts
- arXiv ID: 2311.13206
- Source URL: https://arxiv.org/abs/2311.13206
- Reference count: 0
- The ensemble approach achieved 98% accuracy on breast cancer histopathology images, improving 1% over individual models

## Executive Summary
This study proposes an adaptive weighted average ensemble method for breast cancer classification using pre-trained convolutional neural networks. The approach combines ResNet-50, DenseNet-201, and Inception-V3 models by averaging their outputs with weights proportional to their individual accuracies. When tested on the BreakHis dataset containing 9,109 histopathological images, the ensemble achieved 98% accuracy, representing a 1% improvement over the best individual model. The method differs from traditional ensemble approaches by applying weighted averaging after independent training of each model.

## Method Summary
The method uses transfer learning with three pre-trained CNN architectures (ResNet-50, DenseNet-201, and Inception-V3) trained independently on the BreakHis dataset of breast cancer histopathology images. After individual training, the models are combined using adaptive weighted averaging where each model's prediction is multiplied by its accuracy, summed, and divided by the total accuracy sum. The ensemble approach aims to leverage the complementary strengths of different architectures while avoiding interference between training processes.

## Key Results
- Achieved 98% classification accuracy on BreakHis dataset
- 1% improvement over best individual model (97% accuracy)
- Reduced false positives and false negatives compared to individual models
- Improved evaluation metrics including precision, recall, and F1-score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted averaging after independent training improves ensemble performance by leveraging model strengths
- Mechanism: Each model's prediction is multiplied by its accuracy, then summed and divided by total accuracy sum
- Core assumption: Individual model accuracies remain stable across validation/test sets
- Evidence anchors:
  - [abstract] "It averages the outputs of every trained model, and every model will have weight according to its accuracy"
  - [section] "if the output is less than .5 that means it belongs to the first class and if it is bigger than .5, that means it belongs to the second class. We sum the last outputs of models and multiply every model by its accuracy, then we divide by sum of the models accuracies"
  - [corpus] Weak evidence for effectiveness of post-training weighted ensembles specifically

### Mechanism 2
- Claim: Combining multiple CNN architectures captures complementary feature representations
- Mechanism: ResNet-50, DenseNet-201, and Inception-V3 each extract different types of features (residual connections, dense connections, inception modules)
- Core assumption: Architectural diversity leads to uncorrelated errors
- Evidence anchors:
  - [section] "Gathering those models altogether to use advantages of every model, surely every model will be better at detecting specific lines or edge, and may this model will have defects in detecting and classifying a kind of other edges"
  - [section] "ResNet-50... DenseNet-201... Inception-v3" all have distinct architectural principles
  - [corpus] No direct corpus support; related works focus on single architecture fine-tuning

### Mechanism 3
- Claim: Post-training ensemble avoids interference between model training processes
- Mechanism: Models are trained independently to convergence, then combined
- Core assumption: Independent training allows each model to optimize its own representation
- Evidence anchors:
  - [abstract] "Our approach is different because it used adaptive average ensemble after training which has increased the performance of evaluation metrics"
  - [section] "this is different from the literature which uses average ensemble before training and the average ensemble is trained simultaneously"
  - [corpus] Weak support; ensemble methods in literature mostly focus on simultaneous training

## Foundational Learning

- Concept: Transfer learning with pre-trained CNNs
  - Why needed here: Enables effective feature extraction from limited histopathological data by leveraging ImageNet-trained weights
  - Quick check question: Why is fine-tuning preferred over training from scratch for medical imaging datasets?

- Concept: Ensemble averaging with weighted voting
  - Why needed here: Improves classification robustness by combining multiple models' predictions based on their reliability
  - Quick check question: How does weighting by accuracy differ from simple majority voting in ensemble methods?

- Concept: Confusion matrix interpretation
  - Why needed here: Essential for understanding false positive/negative rates and evaluating medical diagnostic performance
  - Quick check question: In breast cancer classification, which error type (FP or FN) is typically more critical to minimize?

## Architecture Onboarding

- Component map: Input images → Three parallel CNN models (ResNet-50, DenseNet-201, Inception-V3) → Individual predictions (probabilities) → Weighted averaging (weights = model accuracies) → Final classification output
- Critical path: Image preprocessing → Independent model inference → Weighted probability aggregation → Sigmoid thresholding
- Design tradeoffs: Post-training ensemble avoids training complexity but requires all models to be trained independently; weighting by accuracy assumes accuracies generalize to test set
- Failure signatures: Degraded performance if model accuracies vary between validation and test sets; poor results if models learn highly correlated features
- First 3 experiments:
  1. Train each CNN independently on BreakHis, record individual accuracies and confusion matrices
  2. Implement simple unweighted averaging ensemble, compare metrics to individual models
  3. Implement weighted averaging using validation accuracies as weights, evaluate on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive weighted ensemble approach perform compared to traditional ensemble methods (e.g., majority voting, unweighted averaging) on breast cancer histopathology classification?
- Basis in paper: [explicit] The paper mentions that using unweighted averaging resulted in a 1% decrease in F1-score compared to the weighted average method
- Why unresolved: While the paper shows that the weighted approach outperforms the best individual model, it does not provide a direct comparison with other ensemble methods like majority voting or simple averaging
- What evidence would resolve it: A head-to-head comparison of weighted averaging, unweighted averaging, and majority voting on the same dataset with identical preprocessing and model architectures

### Open Question 2
- Question: Can the adaptive weighted ensemble approach be generalized to other medical imaging tasks beyond breast cancer histopathology?
- Basis in paper: [inferred] The paper focuses solely on breast cancer classification and does not explore the approach's applicability to other medical imaging domains
- Why unresolved: The effectiveness of the ensemble method is demonstrated only on one specific dataset and task, leaving its generalizability uncertain
- What evidence would resolve it: Applying the same ensemble approach to other medical imaging datasets (e.g., lung cancer CT scans, skin cancer dermoscopy images) and comparing performance to baseline models

### Open Question 3
- Question: How does the performance of the ensemble change when using different combinations of CNN architectures or varying the number of models in the ensemble?
- Basis in paper: [explicit] The paper uses three specific models (ResNet-50, DenseNet-201, Inception-V3) but does not explore the impact of different combinations or ensemble sizes
- Why unresolved: The choice of models and ensemble size may significantly impact performance, but this is not systematically investigated
- What evidence would resolve it: Systematic experiments varying the number of models (e.g., 2, 3, 5 models) and testing different combinations of architectures to identify optimal ensemble configurations

## Limitations

- The BreakHis dataset contains significant class imbalance (benign vs malignant), but the paper does not explicitly address how this was handled during training or evaluation
- No cross-validation or multiple random seeds were reported, making it difficult to assess the statistical significance of the claimed 1% improvement
- The method assumes individual model accuracies remain stable between validation and test sets, but this assumption is not validated experimentally

## Confidence

- **High confidence**: The basic ensemble methodology (weighted averaging of independent model outputs) is technically sound and correctly implemented based on the description
- **Medium confidence**: The reported 98% accuracy improvement over individual models is plausible given the ensemble approach, but lacks statistical validation through multiple runs or cross-validation
- **Low confidence**: The claim that this approach is "novel" compared to literature is questionable, as similar weighted ensemble methods have been explored in other domains

## Next Checks

1. **Statistical significance test**: Run the complete experiment with 5-fold cross-validation and report mean±std for all metrics across folds to establish whether the 1% improvement is statistically significant
2. **Robustness analysis**: Test the ensemble performance when individual model accuracies are perturbed (e.g., ±5% variation) to assess sensitivity to weight estimation errors
3. **Class imbalance correction**: Repeat experiments using stratified sampling and class-balanced loss functions to determine if the reported accuracy improvement persists under balanced evaluation