---
ver: rpa2
title: Integrating Image Features with Convolutional Sequence-to-sequence Network
  for Multilingual Visual Question Answering
arxiv_id: '2303.12671'
source_url: https://arxiv.org/abs/2303.12671
tags:
- image
- question
- features
- answers
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The VLSP2022-EVJVQA challenge introduces a new multilingual Visual
  Question Answering dataset in English, Vietnamese, and Japanese. We propose a solution
  that combines the Convolutional Sequence-to-Sequence network with image features
  and hints extracted from state-of-the-art Vision-and-Language Transformer models.
---

# Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering

## Quick Facts
- arXiv ID: 2303.12671
- Source URL: https://arxiv.org/abs/2303.12671
- Reference count: 38
- Achieved 3rd place in VLSP2022-EVJVQA challenge with 0.4210 F1 on private test set

## Executive Summary
This paper presents a solution for the VLSP2022-EVJVQA challenge, a multilingual Visual Question Answering task involving English, Vietnamese, and Japanese. The proposed approach combines Convolutional Sequence-to-Sequence (ConvS2S) networks with image features and hints extracted from pre-trained vision-language models (OFA and ViLT). The ensemble method achieved strong performance, ranking 3rd in the competition, demonstrating the effectiveness of multimodal integration for multilingual VQA while highlighting ongoing challenges in cross-lingual answer generation.

## Method Summary
The approach uses ConvS2S to generate answers from sequences combining questions, hints from pre-trained VQA models, and image features. For non-English inputs, questions are translated to English, processed through OFA and ViLT models to extract answer hints, then back-translated to the target language. ViT-B/16 extracts image features as patch embeddings. The final sequence concatenates question tokens, repeated hints (based on probability scores), and image features, which ConvS2S processes to generate the answer in the original language.

## Key Results
- ConvS2S + ViT-B/16 + OFA large ensemble achieved 0.3390 F1 on public test set and 0.4210 F1 on private test set
- Adding ViLT hints improved F1 from 0.1183 to 0.1317 (English) and from 0.2270 to 0.3662 (Japanese)
- Integrating ViT image features consistently improved performance across all model configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convolutional Sequence-to-Sequence networks enable parallelization over sequence elements, accelerating training compared to recurrent architectures.
- Mechanism: ConvS2S replaces recurrent layers with convolutional layers that apply multiple filters with fixed width across the sequence, allowing parallel computation of each element's representation during training.
- Core assumption: GPU hardware optimization is sufficient to offset the increased parameter count from convolutional filters compared to RNNs.
- Evidence anchors:
  - [section] "ConvS2S has significant capabilities to accelerate training progress and reduce our computational resource limitations due to its efficiency in terms of GPU hardware optimization and parallel computation."
  - [corpus] Weak - the corpus mentions "cross-modal feature graphing" but provides no direct evidence about ConvS2S training efficiency.
- Break condition: If sequence lengths become very long, convolutional layers may require excessive memory or fail to capture long-range dependencies effectively.

### Mechanism 2
- Claim: Integrating pre-trained vision-language model hints improves VQA performance by providing contextual clues about image-question relationships.
- Mechanism: Pre-trained models (OFA, ViLT) generate answer candidates that serve as hints, which are then incorporated into the input sequence to guide the ConvS2S model toward correct answers.
- Core assumption: The semantic relevance of generated hints correlates with improved answer quality, even if the hints themselves are not perfectly accurate.
- Evidence anchors:
  - [abstract] "Our best ensemble, ConvS2S + ViT-B/16 + OFA large, achieved 0.3390 F1 on the public test set and 0.4210 F1 on the private test set"
  - [section] "the OFA model with unified Seq2Seq structure outperforms ViLT with F1 0.1902, while ViLT achieves the best performance with F1 0.1317 using 2 keyword answers"
- Break condition: If hint generation becomes too noisy or irrelevant to the specific image-question pair, it may introduce harmful bias rather than helpful context.

### Mechanism 3
- Claim: Vision Transformer image features provide complementary spatial information that enhances answer generation beyond text-only context.
- Mechanism: ViT extracts patch embeddings from images, which are concatenated with text embeddings along the sequence dimension to form a unified multimodal representation.
- Core assumption: The 196x768 visual feature vector captures sufficient visual information to improve answer accuracy when combined with textual hints.
- Evidence anchors:
  - [section] "After adding image feature from ViT-B/16, the performance of previous models tend to improve"
  - [corpus] Weak - corpus mentions "cross-modal feature graphing" but doesn't provide evidence about ViT's specific contribution to performance.
- Break condition: If the visual features don't align well with the textual information or if the concatenation disrupts the sequence learning process.

## Foundational Learning

- Concept: Sequence-to-sequence learning fundamentals
  - Why needed here: The task transforms images and questions into answers, requiring mapping between different sequence domains
  - Quick check question: Can you explain the difference between encoder-only, decoder-only, and encoder-decoder architectures in sequence modeling?

- Concept: Vision-language model pretraining and transfer learning
  - Why needed here: The approach relies on pre-trained models (OFA, ViLT) to generate hints without fine-tuning on the target dataset
  - Quick check question: What are the key differences between contrastive pretraining objectives and generative pretraining objectives for vision-language models?

- Concept: Attention mechanisms in multimodal contexts
  - Why needed here: The ConvS2S architecture uses attention to align question elements with answer elements across modalities
  - Quick check question: How does multi-head attention differ from single-head attention in capturing cross-modal relationships?

## Architecture Onboarding

- Component map:
  Image + Question (text) → Translation → Hint models → Hints → Concatenation → ConvS2S → Answer
  Image + Question (text) → ViT → Features → Concatenation → ConvS2S → Answer

- Critical path: Image → ViT → Features → Concatenation → ConvS2S → Answer
  Question → Translation → Hint models → Hints → Concatenation → ConvS2S → Answer

- Design tradeoffs:
  - Hint frequency vs. noise: Higher hint repetition improves visibility but may introduce redundancy
  - Image feature resolution vs. computational cost: Higher patch counts provide more detail but increase memory usage
  - Model complexity vs. training time: Deeper ConvS2S networks capture more patterns but require longer training

- Failure signatures:
  - BLEU much lower than F1 indicates the model generates plausible but not exact answers
  - Attention weights concentrated on single hint tokens suggest overfitting to specific patterns
  - Performance drop when switching languages indicates translation quality issues

- First 3 experiments:
  1. Train ConvS2S with question only (baseline) to establish performance without multimodal features
  2. Add ViLT hints to sequence and compare F1 improvement to baseline
  3. Add ViT image features to the best hint-performing model and measure impact on both metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the extracted hints from pre-trained VQA models impact the final answer quality compared to using image features alone?
- Basis in paper: [explicit] The paper states that integrating hints from OFA and ViLT models improved the F1 score, but it does not quantify the individual contributions of hints versus image features.
- Why unresolved: The paper combines hints and image features in the experiments, making it difficult to isolate the impact of hints alone on the final performance.
- What evidence would resolve it: A controlled experiment comparing ConvS2S performance with hints only, image features only, and the combination would clarify the relative contributions.

### Open Question 2
- Question: How does the model's performance vary across different question types (e.g., counting, color recognition, object identification) in the multilingual VQA task?
- Basis in paper: [inferred] The paper mentions that the UIT-EVJVQA dataset contains various types of questions, but it does not analyze the model's performance on specific question types.
- Why unresolved: The paper evaluates overall performance using F1 and BLEU scores, but does not provide insights into how the model handles different question types.
- What evidence would resolve it: A detailed analysis of the model's performance on different question types, including examples of correct and incorrect responses, would provide insights into its strengths and weaknesses.

### Open Question 3
- Question: How do the attention mechanisms in the ConvS2S model differ when processing multilingual questions compared to monolingual questions?
- Basis in paper: [explicit] The paper mentions that the model generates answers in the same language as the input question, but it does not analyze the attention mechanisms for different languages.
- Why unresolved: The paper does not provide a detailed analysis of the attention mechanisms for multilingual questions, making it difficult to understand how the model processes different languages.
- What evidence would resolve it: A comparative analysis of the attention weights for multilingual and monolingual questions, along with examples of attention alignment, would clarify how the model processes different languages.

## Limitations
- The approach depends heavily on translation quality for non-English inputs without evaluating translation error propagation
- The hint integration methodology lacks systematic ablation studies to quantify individual contributions of different hint sources
- The substantial parameter count (1.4 billion) raises concerns about practical deployment and scalability

## Confidence
- High confidence: The baseline ConvS2S architecture implementation and general multimodal integration approach
- Medium confidence: The specific hint integration methodology and its quantitative impact on performance
- Low confidence: Claims about multilingual robustness and the generalizability of findings to other VQA datasets

## Next Checks
1. Conduct ablation studies systematically varying hint repetition frequency and selection criteria to determine optimal hint integration parameters
2. Perform cross-lingual transfer experiments to assess whether improvements generalize across all three languages or are language-specific
3. Implement attention visualization to analyze how the model distributes focus between question tokens, hints, and image features during inference