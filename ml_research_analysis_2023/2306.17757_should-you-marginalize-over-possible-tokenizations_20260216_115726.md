---
ver: rpa2
title: Should you marginalize over possible tokenizations?
arxiv_id: '2306.17757'
source_url: https://arxiv.org/abs/2306.17757
tags:
- tokenizations
- blocks
- data
- tokenization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates whether language models should account for
  all possible tokenizations of text strings. Using a novel importance sampling approach,
  it estimates marginal probabilities across multiple tokenizations for GPT-2 and
  BLOOM on diverse datasets.
---

# Should you marginalize over possible tokenizations?

## Quick Facts
- arXiv ID: 2306.17757
- Source URL: https://arxiv.org/abs/2306.17757
- Reference count: 31
- Primary result: Marginalizing over possible tokenizations typically changes log-likelihood by <0.5%, rising to 1-2% for complex words or distribution shift.

## Executive Summary
This paper investigates whether language models should account for all possible tokenizations of text strings when computing probabilities. Using a novel importance sampling approach, the authors estimate marginal probabilities across multiple tokenizations for GPT-2 and BLOOM on diverse datasets. The results show that the gap in log-likelihood between default and marginalized scoring is typically under 0.5%, though it becomes more pronounced (1-2%) for data with long complex words or under distribution shift. The authors release code to enable others to assess the effect in their settings.

## Method Summary
The paper proposes a novel importance-sampling-based algorithm to compute estimates of marginal probabilities over all possible tokenizations of a text string. The algorithm splits sequences into blocks, samples tokenizations within each block from a proposal distribution, and reweights them by P(T,S)/Q(T|S) to produce an unbiased estimate of P(S). The method is validated against exact marginalization on short sentences and applied to GPT-2 and BLOOM models on diverse datasets including Wikipedia, Twitter, news, speeches, multilingual text, and code.

## Key Results
- Marginalization changes log-likelihood by <0.5% in most cases
- Gaps become 1-2% for data with long complex words
- Results hold across English, multilingual, and code datasets
- Algorithm validated against exact marginalization on short sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The importance sampling estimate approximates the true marginal probability over all tokenizations by averaging scores from samples drawn from a proposal distribution.
- Mechanism: The algorithm splits text into blocks, samples tokenizations within each block from a proposal distribution Q, and reweights them by P(T,S)/Q(T|S) to produce an unbiased estimate of P(S).
- Core assumption: The proposal distribution Q(T|S) is sufficiently close to the true posterior P(T|S) so that the variance of the importance sampling estimate remains low.
- Evidence anchors:
  - [abstract] "Our importance sampling estimates show that in practice marginal-ization does not influence log-likelihood much"
  - [section 2.2] "the closer the proposal Q(T|S) is to the true posterior distribution P(T|S), the smaller the variance of the unbiased estimate (2) tends to be"
- Break condition: If the proposal Q is far from the posterior, variance becomes too high and the estimate becomes unreliable.

### Mechanism 2
- Claim: The gap between default tokenization likelihood and marginalized likelihood is small for common text but increases for rare, complex, or distribution-shifted words.
- Mechanism: Common words have default tokenizations with high probability under both the model and proposal, so marginalizing changes little. Rare or complex words have multiple plausible tokenizations with non-negligible probability, so marginalizing can increase the likelihood.
- Core assumption: Tokenization probability mass is concentrated in the default tokenization for frequent, simple words, but spread across multiple tokenizations for rare, complex words.
- Evidence anchors:
  - [abstract] "Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words"
- Break condition: If evaluation data is dominated by rare or complex words, or if there is significant domain shift, the default tokenization will underestimate the true marginal probability.

### Mechanism 3
- Claim: The proposed sampling algorithm can approximate the exact marginalization for short sentences but scales to long sequences by block decomposition.
- Mechanism: For short sentences, all possible tokenizations can be enumerated and the exact marginalization computed. The sampling algorithm is validated against this ground truth. For long sequences, the sentence is split into blocks, each sampled independently, and the product of block probabilities yields the sequence probability.
- Core assumption: The independence assumption across blocks is valid when blocks are short enough that tokenizations do not span block boundaries.
- Evidence anchors:
  - [section 2.2] "in the extreme case of splitting S into a single block B1 = S, our proposal Q(T|S) turns into the true posterior P(T|S), allowing to compute the exact marginalization with a single sample"
- Break condition: If block size is too small, the number of type-2 blocks increases, preventing sampling of the default tokenization and degrading the estimate.

## Foundational Learning

- Concept: Importance sampling for estimating expectations under intractable distributions.
  - Why needed here: The true marginal probability over all tokenizations is intractable due to exponential growth, so importance sampling provides a practical unbiased estimator.
  - Quick check question: If the proposal distribution Q is exactly equal to the target distribution P, what is the variance of the importance sampling estimator?
    - Answer: Zero variance; one sample suffices.

- Concept: Block decomposition to manage combinatorial explosion.
  - Why needed here: The number of possible tokenizations for a long string is exponential; splitting into blocks makes the problem tractable by limiting the number of tokenizations per block.
  - Quick check question: What happens if a word is split into type-2 blocks? Can the default tokenization still be sampled?
    - Answer: No; type-2 blocks prevent the default tokenization from being formed, so the estimate becomes biased.

- Concept: Tokenizer design and its effect on model probability.
  - Why needed here: The tokenization scheme determines which token sequences are possible and their probabilities; different tokenizers can assign different probabilities to the same string.
  - Quick check question: If a tokenizer produces only one tokenization for a string, is marginalization necessary?
    - Answer: No; marginalization is only needed when multiple tokenizations are possible.

## Architecture Onboarding

- Component map:
  - Tokenizer: Converts strings to token sequences (BPE, WordPiece, etc.)
  - Language Model: Computes P(T|S) for a given tokenization
  - Proposal Sampler: Implements Algorithm 1 to sample tokenizations from Q(T|S)
  - Estimator: Computes the importance sampling average P(S) ≈ (1/K) Σ P(Tk,S)/Q(Tk|S)
  - Validator: Optionally enumerates all tokenizations for short sentences to check accuracy

- Critical path:
  1. Input string S
  2. Split into blocks at whitespace/newlines, limiting block length to L
  3. For each block, enumerate top-M tokenizations by number of subtokens
  4. Score each tokenization with LM to get proposal probabilities
  5. Sample K tokenizations per block from Q
  6. Concatenate block tokenizations to form full sequence tokenizations
  7. Compute P(T,S)/Q(T|S) for each sampled tokenization
  8. Average over K samples to get estimate of P(S)

- Design tradeoffs:
  - Block size L: Larger L reduces the number of blocks but increases the number of possible tokenizations per block; smaller L increases independence but may create type-2 blocks
  - M (top tokenizations per block): Larger M improves coverage of high-probability tokenizations but increases LM evaluations
  - K (samples per sequence): Larger K reduces variance but increases computation time
  - Tokenizer choice: Affects the number of possible tokenizations and their probabilities

- Failure signatures:
  - High variance in BPC gap estimates: Indicates Q is far from P, likely due to inappropriate L or M
  - Negative or near-zero BPC gap: May indicate type-2 blocks are preventing sampling of the default tokenization
  - Slow runtime: Could be due to too large L, M, or K; or due to blocks with many possible tokenizations

- First 3 experiments:
  1. Validate the algorithm on short sentences (≤25 chars) by comparing the sampling estimate to exact marginalization
  2. Test the effect of varying L on a dataset with complex words (e.g., Twitter) to find the optimal block size
  3. Compare BPC gap across different tokenizers (BPE, WordPiece, UnigramLM) on the same dataset to see how tokenizer choice affects the need for marginalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed importance sampling algorithm's performance scale with sequence length and vocabulary size? 
- Basis in paper: [explicit] The paper mentions that the algorithm requires O(|B| × M ) evaluations of the LM per sample, where |B| is the number of blocks, and discusses how the number of possible tokenizations grows exponentially with sequence length.
- Why unresolved: The paper does not provide empirical data on how the algorithm's performance degrades with very long sequences or extremely large vocabularies. It only validates the algorithm on sequences up to 800 tokens.
- What evidence would resolve it: Empirical results showing BPC gaps and computational time for the algorithm on sequences of varying lengths (e.g., 1000, 2000, 5000 tokens) and on models with different vocabulary sizes would clarify the scalability limitations.

### Open Question 2
- Question: How sensitive is the BPC gap to the choice of the maximum block size parameter L, and is there an optimal strategy for selecting L that minimizes the BPC gap while maintaining computational efficiency?
- Basis in paper: [explicit] The paper discusses the effect of L on the proportion of T1 and T2 blocks and its impact on the BPC gap, but does not provide a systematic analysis of the optimal L value.
- Why unresolved: The paper only tests a few values of L and does not explore the trade-off between BPC gap reduction and computational cost across a wider range of L values.
- What evidence would resolve it: A comprehensive study varying L across a wider range (e.g., from 5 to 50 tokens) and measuring the corresponding BPC gaps and computational times would identify the optimal L value for different datasets and model sizes.

### Open Question 3
- Question: Does the BPC gap vary significantly across different tokenization algorithms (e.g., BPE, WordPiece, Unigram), and if so, which tokenization method is most robust to the marginalization effect?
- Basis in paper: [inferred] The paper mentions that the proposed algorithm can be applied to any tokenizer, but only tests GPT-2 (BPE) and BLOOM (SentencePiece). The paper also references Cao and Rimell (2021) who used the Unigram tokenizer.
- Why unresolved: The paper does not directly compare the BPC gap across different tokenization algorithms, leaving the question of which method is most robust to the marginalization effect unanswered.
- What evidence would resolve it: Experiments applying the importance sampling algorithm to language models using different tokenization methods (BPE, WordPiece, Unigram) on the same datasets would reveal which tokenization is most robust to the marginalization effect.

## Limitations
- Computational cost remains high despite importance sampling, limiting practical adoption
- Hyperparameter sensitivity to block size L and number of tokenizations M may affect results
- Proposal distribution quality is critical but heuristic-based, potentially inaccurate for complex cases

## Confidence

- **High Confidence**: The claim that marginalized scoring rarely changes log-likelihood by more than 0.5% for typical text is well-supported by extensive experiments across multiple datasets and languages.
- **Medium Confidence**: The finding that gaps become more pronounced (1-2%) for data with long complex words or under distribution shift is supported by experimental results but partly driven by dataset composition.
- **Low Confidence**: The claim that block decomposition provides an effective trade-off between accuracy and tractability lacks thorough validation across different hyperparameter settings.

## Next Checks
1. Perform a systematic hyperparameter sweep across L ∈ {10, 20, 30, 40, 50} and M ∈ {32, 64, 128, 256} on Twitter and Flores-200 datasets to quantify how block size and tokenization limit affect BPC gap and variance.
2. Implement and compare alternative proposal distributions beyond the simple subtoken-product heuristic, such as incorporating token frequency statistics or using a learned proposal distribution.
3. Conduct a controlled experiment on synthetic data where the true marginal probability is known analytically, generating strings with varying numbers of possible tokenizations to measure estimation accuracy as a function of K, L, and M.