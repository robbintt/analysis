---
ver: rpa2
title: Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models
arxiv_id: '2308.02587'
source_url: https://arxiv.org/abs/2308.02587
tags:
- diffusion
- data
- tool
- cataract
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data scarcity in cataract surgery
  video analysis, where certain surgical phases and tool combinations are underrepresented.
  The authors propose a conditional generative model based on Denoising Diffusion
  Implicit Models (DDIM) and Classifier-Free Guidance (CFG) to synthesize diverse,
  high-quality examples for these rare cases.
---

# Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models

## Quick Facts
- arXiv ID: 2308.02587
- Source URL: https://arxiv.org/abs/2308.02587
- Reference count: 31
- One-line primary result: Conditional generative model synthesizes realistic cataract surgery images for rare tool-phase combinations, improving downstream tool classifier performance by up to 10% for underrepresented cases

## Executive Summary
This paper addresses data scarcity in cataract surgery video analysis by proposing a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). The model generates diverse, high-quality images conditioned on surgical phases and tool combinations, particularly focusing on rare cases that are underrepresented in the training data. Clinical experts found the synthesized images difficult to distinguish from real images, and the synthetically extended data improved downstream tool classifier performance by up to 10% for rare cases.

## Method Summary
The authors propose a conditional generative model using DDIM and Classifier-Free Guidance to synthesize cataract surgery images. The model conditions on surgical phases and tool combinations, learning to generate realistic images for both common and rare cases. Classifier-Free Guidance eliminates the need for a separate classifier by using a weighted difference between conditional and unconditional noise estimates. The model generates images that are difficult to distinguish from real ones, even by clinical experts, and these synthetic samples improve the performance of a downstream tool classifier when used to augment training data for underrepresented surgical phases and tool combinations.

## Key Results
- Generated images are difficult to distinguish from real images, even for clinical experts with 5+ years of experience
- Downstream tool classifier performance improved by up to 10% for rare cases when trained on synthetic data
- Outperforms state-of-the-art baselines in image quality metrics (FID, KID, IS, CF1, LPIPS) while preserving tool representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifier-Free Guidance (CFG) enables the model to generate high-quality images for rare tool-phase combinations without needing a separate classifier
- Mechanism: CFG uses a weighted difference between conditional and unconditional noise estimates, allowing the model to balance fidelity to the conditioning and diversity of the output
- Core assumption: The UNet architecture can learn to encode both the unconditional and conditional distributions effectively, and the weighting parameter w can control the trade-off between adherence to conditioning and sample diversity
- Evidence anchors:
  - [abstract]: "Instead, Classifier-Free Guidance (CFG) [8] yields a simple trick to achieve class-constrained generative results with diffusion models."
  - [section]: "The weighted noise ¯ϵθ(xt, t, y(p), y(s)) can simply replace ϵ in Equation 1."

### Mechanism 2
- Claim: The model's ability to generate realistic images improves the performance of a downstream tool classifier by providing more diverse training examples for rare cases
- Mechanism: By generating synthetic images that are difficult to distinguish from real images, the model increases the number of training examples for rare tool-phase combinations, allowing the classifier to learn better representations for these cases
- Core assumption: The generated images are realistic enough to be useful for training the classifier, and the classifier can effectively learn from the additional examples
- Evidence anchors:
  - [abstract]: "The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases."
  - [section]: "We then leverage the conditional denoising diffusion model to generate unseen samples for these phases."

### Mechanism 3
- Claim: The model's ability to generate diverse examples based on complex multi-class multi-label conditions allows it to address the data sparsity problem in cataract surgery video analysis
- Mechanism: By conditioning the model on surgical phases and combinations of surgical tools, it can generate a wide range of examples that cover rare cases, effectively increasing the size and diversity of the training data
- Core assumption: The model can effectively learn the complex relationships between surgical phases, tools, and image content, and generate diverse examples that cover the full range of rare cases
- Evidence anchors:
  - [abstract]: "Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools."
  - [section]: "To automatise this process, we compute the joint probabilities pϕ(ys, yp) from the available CATARACTS annotations."

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: DDPMs provide the foundation for the generative model used in this work, allowing it to learn the reverse process of gradually adding Gaussian noise to an image
  - Quick check question: What is the key idea behind DDPMs, and how do they differ from traditional generative models?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: CFG enables the model to generate class-constrained images without requiring a separate classifier, simplifying the training process and allowing for more flexible conditioning
  - Quick check question: How does CFG work, and what are its advantages over traditional classifier-based guidance methods?

- Concept: Multi-class Multi-label Conditioning
  - Why needed here: The ability to condition the model on multiple surgical phases and tools allows it to generate diverse examples that cover rare cases, addressing the data sparsity problem in cataract surgery video analysis
  - Quick check question: What are the challenges of multi-class multi-label conditioning, and how does the model in this work address them?

## Architecture Onboarding

- Component map: CATARACTS Data -> Embedding Modules -> UNet Architecture -> Classifier-Free Guidance -> Generated Images -> Tool Classifier
- Critical path: Data → Embedding → UNet → CFG → Generated Image → Tool Classifier
- Design tradeoffs:
  - Balancing the weighting parameter in CFG for optimal sample quality and diversity
  - Choosing the appropriate number of denoising steps for efficient inference
  - Deciding on the level of conditioning complexity based on the available data and task requirements
- Failure signatures:
  - Generated images lack realism or fail to adhere to the desired conditioning
  - The model struggles to generate diverse examples for rare cases
  - The downstream tool classifier does not show significant performance improvement
- First 3 experiments:
  1. Train the model on a subset of the data and evaluate the quality of generated images using quantitative metrics (FID, KID, IS, CF1, LPIPS)
  2. Generate synthetic images for rare tool-phase combinations and assess their realism using a user study with clinical experts
  3. Retrain the tool classifier on the combined original and synthetic data and evaluate its performance on the test set, focusing on the previously worst-performing phases

## Open Questions the Paper Calls Out
None explicitly called out in the provided material.

## Limitations
- The evaluation is conducted on a single surgical dataset (CATARACTS), raising questions about generalizability to other surgical domains
- The user study with clinical experts (N=5) provides qualitative validation but lacks statistical power for definitive conclusions about image realism
- Computational cost of the diffusion model during inference, particularly for real-time surgical applications, is not discussed in detail

## Confidence
- High Confidence: The core methodology of using DDIM with CFG for rare case generation is well-established and the technical implementation appears sound
- Medium Confidence: The quantitative improvements in tool classification (up to 10% for rare cases) are supported by metrics, though the real-world clinical impact requires further validation
- Medium Confidence: The claim that synthesized images are "difficult to distinguish from real images" is supported by expert opinion but lacks rigorous quantitative validation through proper statistical testing

## Next Checks
1. **Cross-dataset validation:** Test the model's generalization capability by training on CATARACTS and evaluating on a different cataract surgery dataset or even a different surgical domain to assess transferability
2. **Statistical validation of realism:** Conduct a larger-scale blinded study with statistical analysis (e.g., two-alternative forced choice, area under ROC curve) comparing expert ability to distinguish real vs synthetic images
3. **Computational efficiency analysis:** Measure inference time and resource requirements for the diffusion model in practical surgical settings, comparing against real-time constraints for clinical deployment