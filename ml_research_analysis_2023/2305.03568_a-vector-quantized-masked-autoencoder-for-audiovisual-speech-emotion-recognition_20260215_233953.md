---
ver: rpa2
title: A vector quantized masked autoencoder for audiovisual speech emotion recognition
arxiv_id: '2305.03568'
source_url: https://arxiv.org/abs/2305.03568
tags:
- vq-mae-a
- tokens
- recognition
- emotion
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VQ-MAE-AV, a self-supervised multimodal model
  for audiovisual speech emotion recognition. The model leverages masked autoencoders
  and vector quantized variational autoencoders to learn representations of audiovisual
  speech without labels.
---

# A vector quantized masked autoencoder for audiovisual speech emotion recognition

## Quick Facts
- arXiv ID: 2305.03568
- Source URL: https://arxiv.org/abs/2305.03568
- Reference count: 40
- State-of-the-art emotion recognition results across multiple datasets

## Executive Summary
This paper introduces VQ-MAE-AV, a self-supervised multimodal model for audiovisual speech emotion recognition. The model leverages masked autoencoders and vector quantized variational autoencoders to learn representations of audiovisual speech without labels. By compressing raw audio and visual speech data into discrete tokens, VQ-MAE-AV enables efficient processing of long sequences while maintaining semantic information. The approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.

## Method Summary
VQ-MAE-AV uses VQ-VAEs to compress raw audio and visual inputs into discrete tokens, which are then processed by a multimodal masked autoencoder with joint or cross-attention encoder architectures. During self-supervised pre-training, the model reconstructs randomly masked audiovisual speech tokens and uses contrastive learning on global tokens to align corresponding audio-visual pairs. In the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for emotion recognition. The method is pre-trained on VoxCeleb2 and fine-tuned on RAVDESS-Speech, RAVDESS-Song, and CREMA-D datasets.

## Key Results
- Achieves state-of-the-art emotion recognition accuracy across multiple datasets
- Demonstrates effective cross-modal alignment through contrastive learning on global tokens
- Shows robust performance in both controlled and in-the-wild conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrete audiovisual tokens enable multimodal masking without exploding parameter count.
- **Mechanism:** The model splits raw audio and visual inputs into spatio-temporal and spectro-temporal patches, replacing each with discrete codebook indices. This compresses data dimensionality, allowing longer sequences to be processed while keeping the number of trainable parameters manageable.
- **Core assumption:** Discrete quantization preserves semantic information relevant for emotion recognition while reducing dimensionality sufficiently for efficient masking.
- **Evidence anchors:** [abstract] "compresses raw audio and visual speech data into discrete tokens"; [section] "By dividing the audiovisual data into spatio-spectro-temporal patches, the method could learn to relate audio tokens to visual tokens and capture correlations between them"

### Mechanism 2
- **Claim:** Cross-attention fusion between modalities during encoding captures complementary emotion cues.
- **Mechanism:** Two encoder architectures are explored: a joint-encoder that concatenates audio and visual tokens, and a cross-encoder that processes each modality separately then applies cross-attention to fuse them. Cross-attention allows each modality to attend over the other, aligning audio prosody with visual expressions.
- **Core assumption:** Emotion-relevant information is distributed across modalities such that cross-attention can effectively align cues.
- **Evidence anchors:** [abstract] "model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations"; [section] "This layer is implemented as follows: z(v) = Attention (Qz(v),V z,K z) ; z(a) = Attention (Qz(a),V z,K z)"

### Mechanism 3
- **Claim:** Contrastive learning on global tokens improves emotion recognition alignment.
- **Mechanism:** Global tokens (one per modality) are extracted and trained using Noise Contrastive Estimation (NCE) to align corresponding audio-visual pairs while pushing apart mismatched pairs. This encourages the model to learn shared semantic representations useful for emotion recognition.
- **Core assumption:** Global tokens capture sufficient global context for emotion classification, and NCE alignment improves cross-modal consistency.
- **Evidence anchors:** [abstract] "reconstructing randomly masked audiovisual speech tokens and using a contrastive learning strategy"; [section] "Building upon the approaches presented in [23, 20], the global tokens are learned using Noise Contrastive Estimation (NCE)"

## Foundational Learning

- **Masked Autoencoders**
  - Why needed here: MAE enables self-supervised pre-training on unlabeled audiovisual speech by reconstructing randomly masked tokens, forcing the encoder to learn rich representations.
  - Quick check question: In MAE, what proportion of tokens are typically masked for images vs. text? (Answer: ~75% for images/audio, ~15% for text.)

- **Vector Quantization (VQ-VAE)**
  - Why needed here: VQ-VAE compresses raw spectrograms and image sequences into discrete codebook indices, reducing computational load and providing a shared discrete representation space for both modalities.
  - Quick check question: What is the dimensionality reduction achieved by VQ-VAE-audio from the original spectrogram? (Answer: From D=513 to D'=64.)

- **Cross-Attention in Transformers**
  - Why needed here: Cross-attention allows the audio and visual streams to interact, aligning prosodic cues with facial expressions for better emotion recognition.
  - Quick check question: In cross-attention, which modality serves as the query and which as the key/value? (Answer: One modality is query, the other is key/value.)

## Architecture Onboarding

- **Component Map:**
  - Raw audio/video -> VQ-VAE encoders -> discrete tokens
  - Masking -> VQ-MAE-AV encoder (joint/cross) -> latent codes + global tokens
  - Decoder reconstructs masked tokens -> VQ-VAE decoders -> reconstructed data
  - Pre-training loss: reconstruction + contrastive
  - Fine-tuning: freeze encoder, train classification head

- **Critical Path:**
  1. Raw audio/video → VQ-VAE encoders → discrete tokens
  2. Masking → VQ-MAE-AV encoder (joint/cross) → latent codes + global tokens
  3. Decoder reconstructs masked tokens → VQ-VAE decoders → reconstructed data
  4. Pre-training loss: reconstruction + contrastive
  5. Fine-tuning: freeze encoder, train classification head

- **Design Tradeoffs:**
  - Joint vs. Cross encoder: Joint simpler but may mix modalities early; Cross preserves modality-specific processing before fusion.
  - Masking ratio: Higher masking forces stronger cross-modal inference but risks reconstruction collapse.
  - Codebook size: Larger improves expressiveness but increases memory and training time.

- **Failure Signatures:**
  - Poor emotion accuracy → Check masking ratio balance, cross-attention alignment, or codebook capacity.
  - High reconstruction error → Inspect masking strategy, token quantization quality, or decoder depth.
  - Slow convergence → Reduce model depth, lower batch size, or adjust learning rate.

- **First 3 Experiments:**
  1. Train VQ-MAE-AV with joint-encoder only (no cross-attention) and compare emotion accuracy.
  2. Vary masking ratio (e.g., 50% vs 80%) and measure impact on reconstruction quality and downstream emotion recognition.
  3. Disable contrastive loss and observe effect on global token quality and emotion classification performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VQ-MAE-AV compare to other state-of-the-art self-supervised models for multimodal emotion recognition, such as those based on contrastive learning or generative models?
- Basis in paper: [inferred] The paper mentions that VQ-MAE-AV outperforms state-of-the-art supervised methods, but it does not provide a direct comparison with other self-supervised models.
- Why unresolved: The paper does not include a direct comparison with other self-supervised models, so it is unclear how VQ-MAE-AV's performance stacks up against these approaches.
- What evidence would resolve it: A direct comparison of VQ-MAE-AV's performance with other self-supervised models on the same datasets and evaluation metrics.

### Open Question 2
- Question: How does the choice of hyperparameters, such as the number of attention blocks, the size of discrete tokens, and the masking ratio, affect the performance of VQ-MAE-AV on emotion recognition tasks?
- Basis in paper: [explicit] The paper conducts an ablation study to evaluate the impact of various hyperparameters and model designs on the emotion recognition performance of VQ-MAE-AV.
- Why unresolved: While the paper provides insights into the impact of some hyperparameters, it does not explore all possible combinations or provide a comprehensive analysis of how different hyperparameter choices affect the model's performance.
- What evidence would resolve it: A more extensive ablation study that systematically explores the impact of various hyperparameter choices on VQ-MAE-AV's performance, including different combinations and ranges of values.

### Open Question 3
- Question: Can VQ-MAE-AV be effectively applied to other multimodal tasks beyond emotion recognition, such as audiovisual event classification or video action recognition?
- Basis in paper: [inferred] The paper focuses on the application of VQ-MAE-AV to emotion recognition, but it mentions that the model could be extended to other multimodal sequential data.
- Why unresolved: The paper does not provide any evidence or experiments demonstrating the effectiveness of VQ-MAE-AV on other multimodal tasks.
- What evidence would resolve it: Experiments evaluating the performance of VQ-MAE-AV on other multimodal tasks, such as audiovisual event classification or video action recognition, using appropriate datasets and evaluation metrics.

## Limitations
- The paper lacks detailed architectural specifications and hyperparameter settings, particularly for the VQ-VAE pre-training stage and the fine-tuning classification heads.
- The model relies on VoxCeleb2 for pre-training, which primarily contains spoken content rather than emotional speech, raising questions about the quality of emotion-relevant representations learned during self-supervised training.
- The evaluation focuses on acted emotional speech datasets (RAVDESS, CREMA-D) rather than spontaneous emotional interactions, limiting claims about real-world applicability.

## Confidence
- **High Confidence**: The core architectural contributions (discrete token representation, masked autoencoder framework, and contrastive learning strategy) are technically sound and well-motivated.
- **Medium Confidence**: The reported performance improvements over baselines, while impressive, are difficult to fully verify without access to exact implementation details and hyperparameter configurations.
- **Low Confidence**: Claims about the model's effectiveness in "in-the-wild" conditions are not substantiated by evaluation on naturalistic emotional speech datasets.

## Next Checks
1. Conduct an ablation study on masking ratios (50%, 70%, 90%) to determine optimal masking strategy for both reconstruction quality and downstream emotion recognition accuracy.
2. Perform cross-modal dependency analysis by systematically degrading or removing one modality during fine-tuning to quantify the model's reliance on each modality for emotion recognition.
3. Test the pre-trained VQ-MAE-AV encoder on spontaneous emotional speech datasets not seen during pre-training or fine-tuning to assess generalization beyond acted emotional expressions.