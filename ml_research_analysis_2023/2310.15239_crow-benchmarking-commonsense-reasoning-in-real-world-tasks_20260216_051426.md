---
ver: rpa2
title: 'CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks'
arxiv_id: '2310.15239'
source_url: https://arxiv.org/abs/2310.15239
tags:
- commonsense
- knowledge
- answer
- dialogue
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CROW, a multi-task benchmark for evaluating
  commonsense reasoning in real-world natural language processing tasks. CROW is constructed
  using a novel data collection pipeline that applies commonsense-violating perturbations
  to existing task datasets, creating Winograd-style examples across six tasks: machine
  translation, dialogue generation, dialogue summarization, intent detection, stance
  classification, and safety detection.'
---

# CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks

## Quick Facts
- arXiv ID: 2310.15239
- Source URL: https://arxiv.org/abs/2310.15239
- Reference count: 31
- State-of-the-art models perform significantly worse than humans on commonsense reasoning tasks

## Executive Summary
This paper introduces CROW, a multi-task benchmark designed to evaluate commonsense reasoning in real-world natural language processing tasks. The benchmark uses a novel data collection pipeline that applies commonsense-violating perturbations to existing task datasets, creating Winograd-style examples across six NLP tasks. The authors categorize commonsense knowledge into six dimensions (temporal, causal, attribution, comparison, physical, and social) to enable fine-grained analysis of model performance. When evaluated on CROW, state-of-the-art models including GPT-4 show significant performance gaps compared to humans, demonstrating that commonsense reasoning remains a challenging problem in real-world NLP task settings.

## Method Summary
CROW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. The pipeline consists of Commonsense Knowledge Annotation (CKA) and Winograd-style Schema Generation (WSG) stages, each followed by validation. The benchmark categorizes commonsense violations across six dimensions to enable fine-grained analysis. The evaluation uses binary classification tasks with Macro-F1 and Situational Accuracy metrics to measure both individual example performance and context-level reasoning capabilities.

## Key Results
- GPT-4 performs approximately 18% worse than humans on individual examples
- GPT-4 performs approximately 37% worse than humans on situational accuracy
- Models show significant performance gaps across all six commonsense dimensions
- Even state-of-the-art models struggle with temporal, causal, and social reasoning in real-world tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-stage data collection pipeline improves data quality by separating commonsense knowledge annotation from Winograd schema generation.
- Mechanism: By breaking down the schema construction into two independent stages (CKA and WSG), each followed by validation, the pipeline ensures that commonsense violations are grounded to specific dimensions and reduces noise in the final dataset.
- Core assumption: Crowdworkers can effectively identify and validate implicit commonsense knowledge when given clear instructions and validation steps.
- Evidence anchors:
  - [abstract] "CROW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations."
  - [section] "We design a data collection pipeline that breaks down the schema construction into two independent stages: Commonsense Knowledge Annotation (CKA) and Winograd-style Schema Generation (WSG), each of which is followed by a complementary validation stage."

### Mechanism 2
- Claim: Categorizing commonsense violations across six dimensions enables fine-grained analysis of model performance.
- Mechanism: By annotating examples with specific commonsense dimensions (temporal, causal, attribution, comparison, physical, and social), researchers can analyze where models succeed or fail in different types of reasoning.
- Core assumption: The six commonsense dimensions capture the most relevant types of reasoning needed for real-world NLP tasks.
- Evidence anchors:
  - [abstract] "We use CROW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning."
  - [section] "Inspired by Winograd schemas (Levesque et al., 2011), we build our benchmark by applying commonsense-based minimal perturbations on examples from existing datasets for each task."

### Mechanism 3
- Claim: Using Winograd-style minimal perturbations creates challenging test cases that require deep commonsense understanding.
- Mechanism: By creating examples that differ by only a few words but require different commonsense knowledge to solve correctly, the benchmark tests whether models can handle subtle reasoning differences.
- Core assumption: Minimal perturbations that flip the correct answer based on commonsense knowledge create meaningful test cases for evaluating reasoning abilities.
- Evidence anchors:
  - [abstract] "Inspired by Winograd schemas (Levesque et al., 2011), we build our benchmark by applying commonsense-based minimal perturbations on examples from existing datasets for each task."
  - [section] "The Winograd Schema Challenge (Levesque et al., 2011), an often-used benchmark to measure commonsense reasoning abilities, tests whether models can distinguish the meaning of pairs of sentences with commonsense-based minimal perturbations that flip their meaning."

## Foundational Learning

- Concept: Winograd schemas
  - Why needed here: Understanding Winograd schemas is crucial for grasping the benchmark's methodology of using minimal perturbations to test commonsense reasoning.
  - Quick check question: What is the key feature that makes Winograd schemas challenging for models to solve?

- Concept: Commonsense knowledge dimensions
  - Why needed here: The benchmark categorizes commonsense knowledge into six dimensions, so understanding what each dimension represents is essential for analyzing results.
  - Quick check question: Which commonsense dimension would be most relevant for understanding why "water shortages" is an effect of climate change?

- Concept: Minimal perturbation methodology
  - Why needed here: The benchmark relies on creating examples that differ by only a few words, so understanding this approach is key to evaluating the benchmark's effectiveness.
  - Quick check question: Why might changing "effects" to "causes" in an intent statement create a valid Winograd-style perturbation?

## Architecture Onboarding

- Component map: Data collection pipeline (CKA → validation → WSG → validation) → Dataset construction → Model evaluation → Analysis across commonsense dimensions
- Critical path: Data collection → Dataset construction → Model evaluation → Analysis of performance across commonsense dimensions
- Design tradeoffs: The multi-stage pipeline increases data quality but also increases collection time and cost; focusing on real-world tasks makes the benchmark more practical but may limit the scope of commonsense reasoning tested.
- Failure signatures: Models performing well on individual examples but poorly on situational accuracy indicates lack of robust commonsense understanding; consistent failure across specific commonsense dimensions reveals particular reasoning weaknesses.
- First 3 experiments:
  1. Evaluate a simple baseline (majority/random) on each task to establish performance floors
  2. Test GPT-4 with and without chain-of-thought prompting to measure the impact of reasoning traces
  3. Analyze model performance across different commonsense dimensions to identify specific reasoning strengths/weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of prompt engineering and tuning on commonsense reasoning performance in real-world tasks?
- Basis in paper: [inferred] The paper notes that they do not perform prompt tuning, but GPT-3/4 have been found to be sensitive to prompt construction, and performance may vary when using other prompts for the same task.
- Why unresolved: The paper does not explore the impact of different prompts on model performance, which could potentially improve results.
- What evidence would resolve it: Evaluating models with various prompt engineering techniques and comparing performance to determine if prompts can significantly impact commonsense reasoning.

### Open Question 2
- Question: How do the limitations of the six commonsense knowledge dimensions affect the benchmark's ability to evaluate reasoning in real-world tasks?
- Basis in paper: [explicit] The paper acknowledges that commonsense knowledge has many dimensions, and they only consider six core ones (temporal, causal, attribution, comparison, physical, and social) as a basis for their commonsense knowledge annotation stage.
- Why unresolved: The paper does not explore how these limitations might impact the evaluation of reasoning in real-world tasks, or if other dimensions should be considered.
- What evidence would resolve it: Analyzing the impact of including additional commonsense knowledge dimensions on model performance and the benchmark's ability to evaluate reasoning in real-world tasks.

### Open Question 3
- Question: What are the potential data quality issues, annotation artifacts, and biases introduced by using crowdsourcing for generating Winograd schemas?
- Basis in paper: [explicit] The paper mentions that the benchmark is susceptible to data quality issues, annotation artifacts, and biases due to the use of crowdsourcing for generating Winograd schemas.
- Why unresolved: The paper does not explore the specific data quality issues, annotation artifacts, and biases that may arise from using crowdsourcing for generating Winograd schemas.
- What evidence would resolve it: Conducting a thorough analysis of the data generated through crowdsourcing to identify potential issues, artifacts, and biases, and developing strategies to mitigate them.

## Limitations
- The benchmark may create artificially challenging examples that don't reflect naturally occurring edge cases in production systems
- Binary classification formulation may oversimplify complex NLP tasks like machine translation
- Model comparison fairness may be affected by differences in model architectures and training approaches not adequately controlled for

## Confidence
**High Confidence Claims:**
- CROW successfully creates a challenging benchmark for commonsense reasoning across multiple NLP tasks
- State-of-the-art models show significant performance gaps compared to humans on CROW
- The multi-stage data collection pipeline produces high-quality Winograd-style examples

**Medium Confidence Claims:**
- The six commonsense dimensions comprehensively capture relevant reasoning types
- Minimal perturbations effectively test deep commonsense understanding
- Binary classification formulation adequately represents the original NLP tasks

## Next Checks
1. Cross-dataset validation: Test whether models that perform well on CROW also show improved robustness on naturally occurring edge cases in production NLP systems.
2. Task complexity analysis: Systematically vary the complexity of perturbations within each commonsense dimension to determine the threshold where models begin to fail consistently.
3. Human baseline expansion: Conduct larger-scale human evaluations with diverse populations to better understand the upper bound of performance and potential cultural/linguistic biases in the benchmark.