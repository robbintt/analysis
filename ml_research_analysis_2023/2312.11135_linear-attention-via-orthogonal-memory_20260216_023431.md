---
ver: rpa2
title: Linear Attention via Orthogonal Memory
arxiv_id: '2312.11135'
source_url: https://arxiv.org/abs/2312.11135
tags:
- attention
- context
- lavo
- language
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Linear Attention Via Orthogonal memory (LAVO),
  a novel approach to address efficiency degradation in causal linear attention mechanisms.
  LAVO employs orthogonal decomposition to compress a context into a fixed-size orthogonal
  memory while minimizing redundancy.
---

# Linear Attention via Orthogonal Memory

## Quick Facts
- arXiv ID: 2312.11135
- Source URL: https://arxiv.org/abs/2312.11135
- Reference count: 27
- The paper introduces LAVO, a novel approach to address efficiency degradation in causal linear attention mechanisms, achieving strong performance while maintaining linear complexity and scaling context length to 128K for unbounded language modeling.

## Executive Summary
This paper introduces Linear Attention Via Orthogonal memory (LAVO), a novel approach to address efficiency degradation in causal linear attention mechanisms. LAVO employs orthogonal decomposition to compress a context into a fixed-size orthogonal memory while minimizing redundancy. It further dissects the context to incorporate fine-grained local information and embeds relative position encoding to improve extrapolation ability. Experimental results show that LAVO outperforms other efficient baselines in causal language modeling, achieving strong performance while maintaining linear complexity. It successfully scales context length to 128K for unbounded language modeling.

## Method Summary
LAVO addresses efficiency degradation in causal linear attention mechanisms by employing orthogonal decomposition to compress context into a fixed-size orthogonal memory, minimizing redundancy. The method further dissects the context to incorporate fine-grained local information through context dissecting, and embeds relative position encoding to improve extrapolation ability. The approach uses linear complexity O(rn) where r is the number of orthogonal bases. The method is tested across various tasks including language modeling, text-to-speech, summarization, point cloud completion, and time-series forecasting, demonstrating strong performance while maintaining linear complexity and successfully scaling context length to 128K for unbounded language modeling.

## Key Results
- LAVO outperforms other efficient baselines in causal language modeling.
- Achieves strong performance while maintaining linear complexity.
- Successfully scales context length to 128K for unbounded language modeling.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal decomposition compresses the context into a fixed-size memory while maximizing feature distinguishability.
- Mechanism: The method initializes orthogonal bases B ∈ Rr×d and projects each token xt into these bases, computing H = (1/n)∑(B·xt) and the orthogonal memory eX = B ⊙ H. This ensures each basis captures independent components of the context, minimizing redundancy.
- Core assumption: Orthogonal bases effectively span the feature space so that averaging projections captures global context without losing distinguishing information.
- Evidence anchors:
  - [abstract] "LAVO employs orthogonal decomposition to compress a context into a fixed-size orthogonal memory while effectively minimizing redundancy within the context."
  - [section] "The orthogonal spaces maximize the feature distinguishability in the bounded memory."
  - [corpus] Weak. No directly comparable orthogonal-memory papers cited, so empirical support is limited.
- Break condition: If orthogonal bases do not span relevant subspaces, the compressed memory loses critical context information, degrading performance.

### Mechanism 2
- Claim: Context dissecting amplifies fine-grained local information by attending to windowed segments.
- Mechanism: The input is split into windows of size w. Each query attends to its local window C⌊t/w⌋w:t for local features Flocal, while global features Fglobal are obtained from compressed global context via LAVO. Final output is averaged Flocal and Fglobal.
- Core assumption: Local attention within windows captures fine-grained patterns that orthogonal memory, which aggregates globally, cannot preserve.
- Evidence anchors:
  - [abstract] "Given that orthogonal memory compresses global information, we further dissect the context to amplify fine-grained local information."
  - [section] "Although orthogonal memory covers the information of the whole context, it cannot produce fine-grained local content."
  - [corpus] Moderate. Local attention is common (e.g., Linformer, BigBird) but not combined with orthogonal compression in the cited corpus.
- Break condition: If window size w is too small, local context is insufficient; if too large, local detail is lost.

### Mechanism 3
- Claim: Embedded position encoding improves extrapolation by modeling relative positions within windows without O(n²) cost.
- Mechanism: Relative position embeddings P ∈ R2w−1 are added to attention scores based on distance within a window. This gives O(wn) time and O(w) space complexity.
- Core assumption: Relative positions within a small window suffice for extrapolation ability without global positional encoding.
- Evidence anchors:
  - [abstract] "we embed the relative position encoding into LAVO to improve the extrapolation ability."
  - [section] "We embed the position encoding into local attention of LAVO... the time and space complexities of embedded position encoding are O(wn) and O(w), respectively."
  - [corpus] Weak. Most cited works (cosFormer, Performer) use global relative embeddings with O(n²) complexity; this is a novel localized approach.
- Break condition: If extrapolation requires longer-range positional dependencies, local embeddings may fail.

## Foundational Learning

- Concept: Orthogonal decomposition and projection into basis spaces.
  - Why needed here: To compress unbounded context into fixed-size memory without redundancy.
  - Quick check question: If B has rank less than d, what happens to the span of the projected context?

- Concept: Local vs global context attention.
  - Why needed here: To combine coarse global encoding with fine-grained local patterns.
  - Quick check question: Why is averaging local and global features beneficial rather than concatenating?

- Concept: Relative position encoding within windows.
  - Why needed here: To model sequence order for extrapolation without incurring O(n²) cost.
  - Quick check question: How does window size affect the range of relative positions that can be modeled?

## Architecture Onboarding

- Component map: Orthogonal bases B -> projection -> averaged H -> element-wise multiplication -> orthogonal memory eX. Local windowing (size w) -> local attention -> local features Flocal. Relative position embeddings P -> added to local attention scores. Final output = average(Flocal, Fglobal).
- Critical path: Context -> orthogonal compression + local windows -> attention -> output.
- Design tradeoffs:
  - Window size w: Larger improves local detail but increases O(wn) cost.
  - Number of bases r: Larger captures more context but increases O(rn) cost.
  - Orthogonal bases: Fixed vs learnable impacts expressiveness vs efficiency.
- Failure signatures:
  - Poor perplexity or ROUGE: likely insufficient local context or weak orthogonal compression.
  - Memory blow-up: check r or w scaling incorrectly.
  - Slow inference: verify w and r are constants, not growing with n.
- First 3 experiments:
  1. Ablation: run LAVO without context dissecting; compare perplexity to baseline.
  2. Ablation: run LAVO without embedded position encoding; check extrapolation on 16K context.
  3. Scaling test: increase w and r; measure speedup vs vanilla attention on 8K sequence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the orthogonal decomposition in LAVO compare to other compression techniques in terms of information retention and distinguishability?
- Basis in paper: [explicit] The paper introduces orthogonal decomposition to compress context into a fixed-size orthogonal memory, claiming it maximizes distinguishability among memory units.
- Why unresolved: While the paper claims this approach improves information entropy, it doesn't provide a direct comparison with other compression techniques like learned weight matrices or dual attention mechanisms.
- What evidence would resolve it: A comprehensive ablation study comparing LAVO's orthogonal decomposition with other compression methods on various tasks and sequence lengths would clarify its advantages and limitations.

### Open Question 2
- Question: What is the optimal balance between global information (orthogonal memory) and local context in LAVO for different task types?
- Basis in paper: [explicit] The paper introduces context dissecting to incorporate fine-grained local information, but doesn't extensively explore the trade-offs between global and local context.
- Why unresolved: The impact of varying the window size and the ratio of local to global attention on different tasks is not thoroughly investigated.
- What evidence would resolve it: Experiments varying the window size and the proportion of local to global attention across different tasks would reveal the optimal balance for various applications.

### Open Question 3
- Question: How does LAVO's performance scale with extremely long sequences beyond 128K tokens?
- Basis in paper: [explicit] The paper demonstrates LAVO's ability to handle 128K context length, but doesn't explore its limits or behavior with even longer sequences.
- Why unresolved: The paper focuses on the 128K context length as a significant achievement but doesn't investigate how LAVO performs with sequences longer than this.
- What evidence would resolve it: Testing LAVO on tasks with context lengths exceeding 128K tokens would reveal its scalability limits and potential performance degradation.

## Limitations

- The choice of orthogonal basis dimension r is not thoroughly explored, with sensitivity to this hyperparameter and its impact on performance across different tasks unclear.
- The context dissecting window size w is fixed at 16 without exploring whether this is optimal across domains.
- The claim of improved extrapolation ability is primarily demonstrated through a single experiment, lacking systematic evaluation across different extrapolation scenarios.

## Confidence

- **High confidence**: The core mechanism of orthogonal decomposition for context compression is well-founded mathematically and the resulting efficiency gains are demonstrated across multiple tasks.
- **Medium confidence**: The effectiveness of context dissecting for preserving local information, as the ablation studies are limited and the choice of window size appears somewhat arbitrary.
- **Low confidence**: The claim that embedded position encoding is essential for extrapolation, as the ablation study only removes this component without exploring alternatives or providing deeper analysis of why it works.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the orthogonal basis dimension r (e.g., 32, 64, 128) and window size w (e.g., 8, 16, 32) to identify optimal settings and understand their impact on different tasks.

2. **Runtime complexity validation**: Measure actual wall-clock time and memory usage of LAVO versus baselines (Performer, Linear Transformer, Nyströmformer) on sequences of varying lengths (1K, 4K, 16K, 128K) to empirically verify the claimed O(rn) complexity advantage.

3. **Extrapolation robustness testing**: Evaluate LAVO's extrapolation ability by training on multiple context lengths (4K, 8K, 16K) and testing on sequences 2x-8x longer, comparing against baselines to determine whether the improvement is consistent across scales and whether it generalizes beyond the PG-19 dataset.