---
ver: rpa2
title: 'PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer'
arxiv_id: '2308.04791'
source_url: https://arxiv.org/abs/2308.04791
tags:
- performance
- transformer
- ltsf
- petformer
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of Transformer models
  for long-term time series forecasting (LTSF), addressing three key challenges: temporal
  continuity, information density, and multi-channel relationships. The authors propose
  a novel model called PETformer, which incorporates three key designs: Placeholder
  Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel
  Separation and Interaction (MSI).'
---

# PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer

## Quick Facts
- arXiv ID: 2308.04791
- Source URL: https://arxiv.org/abs/2308.04791
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on 8 public time series datasets, improving MSE by 4.7% and MAE by 3.7% compared to PatchTST

## Executive Summary
This paper addresses long-term time series forecasting (LTSF) challenges using Transformer models, proposing PETformer with three key innovations: Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI). The model tackles temporal continuity, information density, and multi-channel relationship issues that plague existing Transformer approaches for LTSF. PETformer demonstrates significant performance improvements across eight diverse datasets, outperforming both Transformer-based and non-Transformer-based state-of-the-art methods.

## Method Summary
PETformer employs a placeholder-enhanced Transformer encoder that takes independent channel token sequences as input, using learnable placeholders for future data concatenated with historical data. The model divides sequences into sub-sequences of length up to 48 to increase semantic richness per token while reducing overall token count. Channels are processed separately before applying inter-channel attention or channel identifiers to model relationships. The architecture uses 4 layers of Transformer encoder with 512 hidden dimensions, 8 multi-head attention, and 0.5 dropout rate, trained with Smooth L1 loss on datasets split in chronological order (6:2:2 for ETT datasets, 7:1:2 for others).

## Key Results
- Achieves state-of-the-art performance on all 8 evaluated datasets
- Improves MSE by 4.7% and MAE by 3.7% compared to PatchTST
- Sub-sequence division reduces computational complexity while improving performance
- Channel separation prevents interference during temporal dependency learning

## Why This Works (Mechanism)

### Mechanism 1: Temporal Continuity via PET
- Claim: PETformer addresses temporal continuity by using placeholders that allow future and past data to interact at the same attention level.
- Mechanism: The Placeholder Enhancement Technique (PET) replicates learnable placeholders for future data and concatenates them with historical data before encoding, enabling bidirectional attention between past and future segments.
- Core assumption: Direct interaction between historical and future data preserves temporal dependencies better than encoder-decoder cross-attention.
- Evidence anchors:
  - [abstract] "PETformer, which incorporates three key designs: Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI)."
  - [section] "The placeholder-enhanced Transformer encoder takes an independent channel token sequence as input... The encoder outputs the last m tokens, which contain learned future prediction information."

### Mechanism 2: Information Density via LSD
- Claim: Long Sub-sequence Division (LSD) improves information density by reducing token count while increasing semantic richness per token.
- Mechanism: Instead of using single time points as tokens, LSD partitions sequences into sub-sequences of length w (up to 48), reducing the number of tokens and increasing the amount of information each token carries.
- Core assumption: Larger sub-sequences provide richer semantic context than individual points, leading to better forecasting performance.
- Evidence anchors:
  - [section] "We delve into longer sub-sequences, up to a length of 48, to leverage richer semantic information and achieve further improvements."
  - [section] "This method addresses the considerable time complexity issue arising from the use of single points as inputs in prevalent Transformer-based models, simultaneously incorporating richer sub-sequence semantics to boost LTSF performance."

### Mechanism 3: Multi-channel Relationships via MSI
- Claim: Multi-channel Separation and Interaction (MSI) improves forecasting by first extracting channel-specific temporal features before modeling inter-channel dependencies.
- Mechanism: MSI separates channels before feature extraction, processes them independently, then applies inter-channel attention or channel identifiers to model relationships between channels.
- Core assumption: Separating channels prevents interference during temporal dependency learning and allows more focused modeling of inter-channel relationships.
- Evidence anchors:
  - [section] "We delve deeper into diverse strategies for Multi-channel Separation and Interaction (MSI), including inter-channel attention and channel identifiers."
  - [section] "By employing channel interaction strategies after the independent extraction of time sequence features, interference between irrelevant channels can be substantially diminished during the model's learning of sequence temporal dependencies."

## Foundational Learning

- Concept: Attention mechanism in Transformers
  - Why needed here: Understanding how PETformer uses attention between historical and future data, and between channels
  - Quick check question: What's the difference between self-attention and cross-attention in the context of LTSF?

- Concept: Sequence tokenization and patch techniques
  - Why needed here: Understanding how LSD divides sequences into sub-sequences and why this reduces complexity
  - Quick check question: How does increasing the sub-sequence window length from 16 to 48 affect the number of tokens and computational complexity?

- Concept: Channel separation vs. channel mixing in multivariate time series
  - Why needed here: Understanding why MSI separates channels before processing and how this differs from direct channel mixing
  - Quick check question: In what scenarios might direct channel mixing actually perform better than channel separation?

## Architecture Onboarding

- Component map:
  Input Embedding → Placeholder-Enhanced Transformer Encoder → Inter-Channel Interaction → Token-Wise Predictor

- Critical path:
  1. Separate channels and apply sub-sequence division
  2. Create token embeddings with placeholders
  3. Apply placeholder-enhanced encoder
  4. Perform inter-channel interaction
  5. Predict future values token by token

- Design tradeoffs:
  - Larger sub-sequence window (w) reduces complexity but may lose fine-grained temporal patterns
  - Channel separation increases sample count but may lose immediate inter-channel dependencies
  - Full attention vs. restricted attention modes trade off performance vs. computational efficiency

- Failure signatures:
  - Out-of-memory errors: Likely due to too large sub-sequence window or too many channels
  - Poor performance on highly correlated channels: May indicate insufficient inter-channel interaction
  - Slow training: May indicate need to reduce sub-sequence window or attention modes

- First 3 experiments:
  1. Compare PET vs. Flattening vs. Feature Head on a small dataset to validate temporal continuity improvement
  2. Test different sub-sequence window lengths (w=16, 24, 48) on ETTh1 to find optimal information density
  3. Compare MSI strategies (NCI, SA, CA) on Electricity dataset to validate multi-channel relationship modeling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal attention mode for the Placeholder Enhancement Technique (PET) in PETformer?
- Basis in paper: [explicit] The paper compares four attention modes (Full Attention, No Inter-future Attention, No Inter-history Attention, Only Future Focuses on History) and finds that Full Attention outperforms the others, but does not explore other potential attention mechanisms.
- Why unresolved: The comparison is limited to these four modes, and there might be other attention mechanisms that could further improve performance.
- What evidence would resolve it: Experiments comparing PETformer with additional attention mechanisms, such as sparse attention or adaptive attention, could determine the optimal attention mode.

### Open Question 2
- Question: How does the performance of PETformer scale with increasing dataset size and dimensionality?
- Basis in paper: [inferred] The paper shows PETformer outperforms baselines on datasets with dimensions ranging from 7 to 862, but does not explore performance on datasets with significantly larger dimensions or longer sequences.
- Why unresolved: The scalability of PETformer to larger and more complex datasets is not thoroughly investigated.
- What evidence would resolve it: Testing PETformer on datasets with dimensions and sequence lengths exceeding those in the current study would reveal its scalability and potential limitations.

### Open Question 3
- Question: What is the impact of different loss functions on PETformer's performance in long-term time series forecasting?
- Basis in paper: [explicit] The paper uses Smooth L1 loss as the default loss function, but does not explore the impact of other loss functions on PETformer's performance.
- Why unresolved: The choice of loss function could significantly affect PETformer's performance, and the optimal loss function for LTSF tasks remains unexplored.
- What evidence would resolve it: Comparing PETformer's performance using different loss functions, such as Mean Squared Error or Huber loss, on various LTSF datasets would determine the impact of the loss function on its performance.

## Limitations

- **Computation Complexity**: Requires 4.3x more computation than PatchTST while achieving only 1.7x better performance, raising efficiency concerns.
- **Scalability Concerns**: Performance on datasets with very high channel counts (862 channels) may be limited by quadratic complexity of self-attention.
- **Generalization Across Domains**: Results may not generalize to all time series forecasting scenarios beyond the eight evaluated datasets.

## Confidence

**High Confidence Claims**:
- PETformer achieves state-of-the-art performance on the eight evaluated datasets
- The three proposed designs (PET, LSD, MSI) effectively address the identified challenges in LTSF
- Sub-sequence division reduces computational complexity while improving performance

**Medium Confidence Claims**:
- The specific improvements in MSE (4.7%) and MAE (3.7%) compared to PatchTST
- The relative effectiveness of different MSI strategies (NCI, SA, CA) across all datasets
- The optimal sub-sequence window length of 48 for all datasets

**Low Confidence Claims**:
- Claims about the model's performance on datasets not included in the evaluation
- The generalizability of the model to real-world industrial applications with different characteristics
- The long-term stability and robustness of the model in production environments

## Next Checks

1. **Cross-dataset Validation**: Test PETformer on additional time series datasets beyond the eight evaluated, particularly those with different characteristics (e.g., different channel counts, varying periodicity, non-stationary patterns) to assess generalization capabilities.

2. **Computational Efficiency Analysis**: Conduct a detailed analysis of the trade-off between computational cost and performance improvement, including memory usage and training time comparisons with other state-of-the-art models across different hardware configurations.

3. **Ablation Studies with Hyperparameter Sensitivity**: Perform comprehensive ablation studies to quantify the individual contributions of each component (PET, LSD, MSI) and analyze the model's sensitivity to key hyperparameters like sub-sequence window length, look-back window, and prediction horizon across different dataset types.