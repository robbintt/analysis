---
ver: rpa2
title: 'LLM-Pruner: On the Structural Pruning of Large Language Models'
arxiv_id: '2305.11627'
source_url: https://arxiv.org/abs/2305.11627
tags:
- pruning
- arxiv
- language
- performance
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel structured pruning method for compressing
  large language models (LLMs) without task-specific fine-tuning. The method automatically
  identifies interdependent structures within LLMs and estimates their importance
  using a combination of first-order gradients and approximated second-order information.
---

# LLM-Pruner: On the Structural Pruning of Large Language Models

## Quick Facts
- arXiv ID: 2305.11627
- Source URL: https://arxiv.org/abs/2305.11627
- Reference count: 40
- This paper proposes a novel structured pruning method for compressing large language models (LLMs) without task-specific fine-tuning

## Executive Summary
This paper introduces LLM-Pruner, a structured pruning approach that automatically identifies and removes non-critical coupled structures in large language models while preserving their multi-task capabilities. The method combines dependency discovery, gradient-based importance estimation with Fisher approximation, and LoRA-based recovery to achieve significant parameter reduction (20-50%) with minimal performance loss (90.7-93.6% retention). Experiments on LLaMA-7B, Vicuna-7B, and ChatGLM-6B demonstrate effective compression across different architectures.

## Method Summary
LLM-Pruner operates through a three-stage pipeline: first, it discovers coupled structures in the LLM by traversing dependency graphs and identifying interdependent components that must be pruned together; second, it estimates the importance of each structure using first-order gradients combined with Fisher information matrix approximation for second-order effects; finally, it prunes low-importance structures and recovers performance through LoRA fine-tuning on a small external dataset. The approach avoids task-specific fine-tuning by leveraging the inherent multi-task nature of LLMs.

## Key Results
- Achieves 20-50% parameter reduction while retaining 90.7-93.6% of original performance
- Zero-shot classification performance remains strong across 7 datasets after pruning
- LoRA recovery requires only ~3 hours and 50K samples to restore pruned model capabilities
- Maintains hardware efficiency through structured pruning compared to unstructured alternatives

## Why This Works (Mechanism)

### Mechanism 1
Coupling structure discovery enables task-agnostic pruning without catastrophic loss. The algorithm traverses the model's dependency graph starting from each neuron, recursively identifying coupled structures (e.g., attention heads, MLP blocks, embeddings) that must be pruned together to preserve intermediate representations. Core assumption: Neuron dependencies in LLMs follow the directional pruning rules where outputs of degree-1 neurons must be pruned with their sole input.

### Mechanism 2
First-order gradient importance with approximated second-order correction works without full Hessian computation. Importance is estimated by measuring loss change when weights are zeroed, using the gradient term and an approximated Fisher information matrix for diagonal Hessian elements. Core assumption: The diagonal Hessian approximation captures sufficient curvature information for importance ranking even without full second-order computation.

### Mechanism 3
LoRA-based recovery enables efficient post-training with minimal data. After pruning, remaining weights are frozen and LoRA adapters (P, Q matrices) are trained on small external dataset, reducing parameter count and training time. Core assumption: The low-rank approximation can effectively recover pruned functionality without accessing original training data.

## Foundational Learning

- Concept: Neural network pruning theory and structured vs unstructured pruning
  - Why needed here: Understanding pruning fundamentals is essential for grasping why structural pruning preserves hardware efficiency while maintaining model capabilities
  - Quick check question: What's the key difference between structured pruning (removing entire filters) and unstructured pruning (removing individual weights)?

- Concept: Gradient-based importance estimation and Hessian approximation methods
  - Why needed here: The paper relies on gradient information and Fisher-based Hessian approximation for importance scoring without full second-order computation
  - Quick check question: Why does the paper use Fisher information matrix instead of computing the full Hessian for importance estimation?

- Concept: Low-rank adaptation (LoRA) and its mathematical formulation
  - Why needed here: LoRA is the recovery mechanism that enables efficient post-training with minimal data and parameters
  - Quick check question: How does decomposing weight updates into PQ matrices reduce the number of parameters that need training?

## Architecture Onboarding

- Component map: Dependency discovery module -> Importance estimation module -> Pruning selection module -> LoRA recovery module -> Evaluation pipeline
- Critical path: Dependency discovery → Importance estimation → Pruning selection → LoRA recovery → Evaluation
- Design tradeoffs: Structural pruning vs unstructured (hardware efficiency vs parameter granularity); First-order vs second-order importance (computational efficiency vs accuracy); LoRA vs full fine-tuning (recovery speed vs parameter efficiency)
- Failure signatures: Dependency discovery fails (pruned groups contain unrelated structures, causing performance collapse); Importance estimation fails (important parameters pruned, requiring extensive recovery); LoRA recovery insufficient (performance plateaus below acceptable threshold)
- First 3 experiments:
  1. Run dependency discovery on LLaMA-7B and verify coupled structures match expected attention heads, MLPs, and embeddings
  2. Test importance estimation on a small subset with ground truth importance (via iterative pruning) to validate ranking quality
  3. Apply 10% pruning with LoRA recovery and measure zero-shot performance degradation on BoolQ and PIQA datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLM-Pruner scale with different pruning ratios beyond 50%, and what is the theoretical limit of effective pruning while maintaining reasonable task performance? The paper only tested up to 50% pruning ratio and explicitly states this is a limitation.

### Open Question 2
How does the choice of external dataset for LoRA fine-tuning affect the final model performance, and can we identify optimal characteristics for such datasets? The paper uses a single external dataset without exploring how different datasets impact recovery performance.

### Open Question 3
Why does parameter-wise importance estimation fail on ChatGLM-6B while succeeding on LLaMA-7B and Vicuna-7B, and what architectural differences cause this discrepancy? The paper observes this difference but does not investigate the underlying causes.

### Open Question 4
How does LLM-Pruner's structured pruning approach compare to unstructured pruning methods in terms of efficiency gains and accuracy retention for LLMs? The paper focuses exclusively on structured pruning without direct comparison to unstructured alternatives.

## Limitations
- Dependency discovery rules may not capture all functional dependencies in complex LLM architectures
- Fisher information matrix approximation may provide insufficient importance estimation accuracy for critical pruning decisions
- LoRA recovery may fail for highly pruned models where essential information is not recoverable through low-rank adaptation

## Confidence
**High Confidence**: Structured pruning reduces parameters by 20-50% while maintaining hardware efficiency; LoRA-based recovery enables efficient post-training with minimal data and parameters; The general framework combining dependency discovery, importance estimation, and LoRA recovery is sound

**Medium Confidence**: First-order gradient importance with Fisher approximation works without full Hessian computation; 90.7-93.6% performance retention is achievable across different LLM architectures

**Low Confidence**: Dependency rules capture all necessary functional couplings for arbitrary LLM architectures

## Next Checks
- Run the dependency detection algorithm on LLaMA-7B using 10 Bookcorpus samples and verify that the identified coupled structures match expected architectural patterns (attention heads, MLP blocks, embeddings)
- Test the first-order gradient + Fisher approximation method on a small subset of parameters where ground truth importance can be established through iterative pruning experiments
- Apply the full pruning pipeline (20% reduction) on Vicuna-7B and systematically vary the recovery dataset size (25K, 50K, 100K samples) and training duration to identify sufficient recovery thresholds