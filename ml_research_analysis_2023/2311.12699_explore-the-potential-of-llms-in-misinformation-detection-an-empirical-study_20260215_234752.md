---
ver: rpa2
title: 'Explore the Potential of LLMs in Misinformation Detection: An Empirical Study'
arxiv_id: '2311.12699'
source_url: https://arxiv.org/abs/2311.12699
tags:
- llms
- detection
- news
- misinformation
- propagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have demonstrated impressive performance
  in various natural language processing tasks, but their effectiveness in misinformation
  detection remains underexplored. This study presents a comprehensive empirical analysis
  of LLMs on eight misinformation detection datasets, focusing on both content-based
  and propagation-based approaches.
---

# Explore the Potential of LLMs in Misinformation Detection: An Empirical Study

## Quick Facts
- arXiv ID: 2311.12699
- Source URL: https://arxiv.org/abs/2311.12699
- Reference count: 19
- Primary result: LLMs achieve comparable performance to fine-tuned small models in text-based misinformation detection but exhibit limited capabilities in understanding propagation structures

## Executive Summary
This study presents a comprehensive empirical analysis of Large Language Models (LLMs) for misinformation detection across eight datasets, examining both content-based and propagation-based approaches. The researchers found that while LLMs perform comparably to fine-tuned small models on text-based tasks, they struggle with propagation structure understanding. The study introduces four instruction-tuned strategies - alternating sample learning, hard sample learning, format graph input, and refining structure - that significantly improve LLM performance in misinformation detection tasks.

## Method Summary
The study evaluates multiple LLM variants (GPT-3.5-turbo, GLM-6B, GLM-130B, LLaMa2-7B) against fine-tuned small models (DeBERTa, Bi-GCN) across eight misinformation detection datasets containing both content and propagation data. Researchers test various prompting strategies (vanilla, task, chain-of-thought) and few-shot learning settings, then implement four instruction-tuned strategies to enhance LLM understanding of content and propagation. The evaluation focuses on accuracy metrics across different experimental conditions and dataset types.

## Key Results
- LLMs achieve comparable performance to fine-tuned small models in text-based misinformation detection
- LLMs exhibit limited capabilities in understanding propagation structures
- Alternating sample learning and hard sample learning strategies significantly improve detection accuracy
- Format graph input enhances LLM understanding of propagation structures
- One-shot learning sometimes performs worse than zero-shot due to potential "hallucination" effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Alternating Sample Learning mitigates LLMs' hallucination by exposing the model to alternating fake and real news samples, enabling better capture of classification boundaries.
- **Mechanism**: The transformer architecture's self-attention is sensitive to input sequence order. By alternating positive and negative samples, LLMs continuously adjust classification boundaries and learn key features from both categories simultaneously.
- **Core assumption**: LLMs can leverage self-attention to effectively learn from alternating sample patterns in the context window.
- **Evidence anchors**:
  - [abstract]: "The former adjusts the position of sample inputs to learn potential interactions from limited samples."
  - [section]: "By alternately inputting positive and negative samples, LLMs can better capture the key features of each category during the continuous adjustment of classification boundaries, thereby enhancing classification accuracy."
- **Break condition**: If the number of samples exceeds the context window size, alternating patterns may be lost and the benefit diminishes.

### Mechanism 2
- **Claim**: Hard Sample Learning enhances LLMs' ability to detect misinformation by focusing on samples that are consistently misclassified, forcing the model to learn difficult-to-distinguish features.
- **Mechanism**: By identifying samples where the LLM's predicted labels consistently differ from ground truth across multiple prompts, and then training on these "hard" samples, the model is forced to confront and resolve its classification weaknesses.
- **Core assumption**: LLMs possess meta-learning capabilities that allow them to quickly adapt and learn from mistakes when presented with hard samples.
- **Evidence anchors**:
  - [abstract]: "The latter enables LLMs to learn more task-relevant features from hard samples."
  - [section]: "By incorporating hard samples, we anticipate that LLMs can swiftly learn key features of fake news from previous errors in judgment."
- **Break condition**: If hard samples are not truly representative of the task's difficulty (e.g., mislabeled or out-of-distribution), the model may learn incorrect patterns.

### Mechanism 3
- **Claim**: Format Graph Input improves LLMs' understanding of propagation structures by providing a systematic, formal description using graph terminology (nodes, edges, root nodes) instead of free-form text.
- **Mechanism**: By translating propagation structures into a formal graph representation, LLMs can better parse and reason about the relationships between tweets and replies, focusing on structural patterns rather than semantic noise.
- **Core assumption**: LLMs can effectively interpret and reason about structured graph representations when provided in a consistent, formal format.
- **Evidence anchors**:
  - [abstract]: "The other strategy defines propagation description languages to adapt to LLMs."
  - [section]: "We attempt to provide a more systematic description of the propagation structure using formal graph terminology (nodes, edges, and root nodes) to provide a suitable graph input format for LLMs."
- **Break condition**: If the graph representation is too complex or exceeds the model's reasoning capacity, the benefit may be negated or reversed.

## Foundational Learning

- **Concept**: Prompt engineering for LLMs
  - **Why needed here**: Different prompt formats (vanilla, task, chain-of-thought) significantly impact LLM performance on misinformation detection tasks.
  - **Quick check question**: How does adding "think step by step" to a prompt affect zero-shot detection accuracy compared to a vanilla prompt?

- **Concept**: Few-shot learning with LLMs
  - **Why needed here**: The number of samples (0, 1, 2, 3, 5) in few-shot learning affects the model's ability to generalize and avoid hallucinations in misinformation detection.
  - **Quick check question**: Why might one-shot learning perform worse than zero-shot learning for some LLMs on misinformation detection tasks?

- **Concept**: Graph neural networks vs. LLMs for propagation-based tasks
  - **Why needed here**: The paper compares LLMs to Bi-GCN, a graph neural network, to highlight LLMs' limitations in understanding propagation structures.
  - **Quick check question**: What structural information do graph neural networks capture that LLMs struggle with in propagation-based misinformation detection?

## Architecture Onboarding

- **Component map**: Data pipeline -> Prompt generator -> LLM interface -> Evaluation module -> Hard sample selector -> Graph refiner
- **Critical path**: Prompt → LLM → Classification → Evaluation → Strategy adjustment (e.g., switch to ASL/HSL/FGI)
- **Design tradeoffs**: 
  - Using multiple LLMs vs. specializing on one (broader coverage vs. deeper tuning)
  - Including propagation structure vs. text-only (richer context vs. complexity and noise)
  - Zero-shot vs. few-shot learning (speed and simplicity vs. accuracy and adaptation)
- **Failure signatures**: 
  - Accuracy drops when propagation structure is added (indicates poor graph understanding)
  - One-shot learning performs worse than zero-shot (suggests insufficient context)
  - Open-source LLMs (LLaMa2) underperform proprietary models (indicates data/scale limitations)
- **First 3 experiments**:
  1. Run all LLMs on FakeNewsNet with vanilla zero-shot prompts to establish baseline text-based detection performance.
  2. Test the impact of adding task descriptions to prompts on LTCR dataset to validate cross-lingual sensitivity.
  3. Evaluate one-shot vs. zero-shot performance on FND23 to observe the "hallucination" effect in few-shot learning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can LLMs be further improved to better understand propagation structures in misinformation detection?
- **Basis in paper**: [explicit] The paper discusses LLMs' limited capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection.
- **Why unresolved**: While the paper proposes instruction-tuned strategies like Format Graph Input and Refining Structure, it acknowledges that LLMs' understanding of propagation structure remains a crucial factor and further enhancement is needed.
- **What evidence would resolve it**: Experimental results showing improved performance of LLMs in propagation-based misinformation detection tasks after implementing additional strategies or fine-tuning approaches.

### Open Question 2
- **Question**: What are the specific factors contributing to the degradation in LLMs' performance under one-shot learning settings?
- **Basis in paper**: [explicit] The paper mentions that most LLMs show poor detection performance under one-shot learning compared to zero-shot settings, indicating that insufficient instances fail to make the model fully understand the task.
- **Why unresolved**: The paper identifies the issue but does not delve into the specific factors causing this degradation, leaving room for further investigation into the underlying mechanisms.
- **What evidence would resolve it**: Detailed analysis of LLM behavior during one-shot learning, identifying the specific stages or components where performance drops, and correlating these with model architecture or training data characteristics.

### Open Question 3
- **Question**: How does the inclusion of propagation structure affect LLMs' ability to detect misinformation across different languages and domains?
- **Basis in paper**: [explicit] The paper observes that incorporating propagation structure typically leads to a decline in accuracy for LLMs under zero-shot setting, and performance varies across different languages and domains.
- **Why unresolved**: While the paper provides initial observations, it does not explore the nuanced relationship between propagation structure, language, and domain in depth, leaving open questions about generalizability and effectiveness.
- **What evidence would resolve it**: Comparative experiments across multiple languages and domains, analyzing the impact of propagation structure on detection performance and identifying language/domain-specific challenges or opportunities.

## Limitations

- The effectiveness of instruction-tuned strategies may be dataset-dependent, as most datasets contain relatively clean propagation structures without significant noise
- The study focuses primarily on English and Chinese datasets, limiting generalizability to other languages
- The choice of specific hyperparameters for hard sample learning (e.g., threshold for identifying "hard" samples) is not thoroughly explored

## Confidence

**High confidence**: The claim that LLMs achieve comparable performance to fine-tuned small models in text-based misinformation detection is well-supported by extensive experimental results across multiple datasets and LLM variants.

**Medium confidence**: The assertion that LLMs exhibit limited capabilities in understanding propagation structures is supported by experimental evidence, though the specific reasons for this limitation (e.g., format complexity, lack of structural reasoning) require further investigation.

**Medium confidence**: The claim that alternating sample learning mitigates hallucination is plausible given the proposed mechanism, but the experimental evidence is limited to specific datasets and may not generalize to all scenarios.

## Next Checks

1. **Validate the hallucination effect**: Conduct additional experiments systematically varying the number of samples (0, 1, 2, 3, 5) in few-shot learning across multiple datasets to confirm the non-monotonic relationship between sample count and accuracy, and identify the optimal number of samples for different LLM variants.

2. **Test instruction-tuned strategies on noisy data**: Apply the proposed instruction-tuned strategies (ASL, HSL, FGI, RS) to datasets with artificially injected propagation noise to assess their robustness and effectiveness in real-world scenarios where propagation structures are often incomplete or noisy.

3. **Compare against advanced baselines**: Evaluate the proposed strategies against state-of-the-art graph neural networks and other specialized misinformation detection models on large-scale datasets to establish the relative strengths and weaknesses of LLMs in both content-based and propagation-based detection tasks.