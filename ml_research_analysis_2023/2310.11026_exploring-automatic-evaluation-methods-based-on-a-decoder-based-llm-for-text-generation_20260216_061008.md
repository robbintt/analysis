---
ver: rpa2
title: Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text
  Generation
arxiv_id: '2310.11026'
source_url: https://arxiv.org/abs/2310.11026
tags:
- text
- evaluation
- methods
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares various automatic evaluation methods for text
  generation, including tuned encoder-based models and large language models (LLMs)
  under equal conditions, on machine translation evaluation and semantic textual similarity
  tasks in Japanese and English. The key findings are: 1) Tuned decoder-based models
  perform poorly compared to tuned encoder-based models, likely because they focus
  on surface word sequences rather than capturing meaning.'
---

# Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation

## Quick Facts
- arXiv ID: 2310.11026
- Source URL: https://arxiv.org/abs/2310.11026
- Reference count: 8
- Primary result: Tuned decoder-based models perform poorly compared to tuned encoder-based models for semantic evaluation tasks, while in-context learning with large LLMs like ChatGPT struggles to identify fine-grained semantic differences.

## Executive Summary
This paper investigates the effectiveness of decoder-based large language models (LLMs) for automatic text generation evaluation compared to traditional encoder-based models. Through extensive experiments on machine translation and semantic textual similarity tasks in both English and Japanese, the authors demonstrate that tuned encoder-based models like RoBERTa significantly outperform tuned decoder-based models. The study reveals that decoder-based models focus on surface-level word sequences rather than semantic meaning, and that in-context learning with very large LLMs faces challenges in capturing fine-grained semantic distinctions. The research also shows that while tuning LLMs improves performance, accuracy gains plateau beyond a certain model size.

## Method Summary
The authors conducted experiments comparing encoder-based models (RoBERTa) with decoder-based models (Cerebras-GPT and Rinna-gpt) on machine translation evaluation and semantic textual similarity tasks. They employed LoRA-tuning for LLMs and fine-tuning for RoBERTa, evaluating performance using Kendall's correlation against human-labeled similarity scores. The study included datasets from WMT20, WMT21, STS-B, SICK, and JSTS, testing various model sizes and configurations. They also compared these tuned models against baselines including BERTScore, BLEU, and ChatGPT in both zero-shot and few-shot settings.

## Key Results
- Tuned encoder-based models significantly outperform tuned decoder-based models on semantic evaluation tasks
- In-context learning with large LLMs like ChatGPT struggles to identify fine-grained semantic differences
- Tuned LLM accuracy increases with model size up to a point, then plateaus
- Decoder-based models focus on surface word sequences rather than semantic meaning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tuned decoder-based models perform poorly compared to tuned encoder-based models because they focus on surface word sequences rather than capturing meaning.
- **Mechanism:** Decoder-based models like GPT and Llama use unidirectional attention, which limits their ability to model bidirectional context. This causes them to rely more heavily on surface-level token similarity, leading to poorer performance on tasks requiring semantic understanding.
- **Core assumption:** Bidirectional attention is necessary for capturing semantic similarity beyond surface-level word overlap.
- **Evidence anchors:**
  - [abstract] "The analysis of the causes for the poor performance of the tuned decoder-based models suggests that they focus on surface word sequences and do not capture meaning."
  - [section] "The most significant difference between the two models is that RoBERTa, an encoder-based model, has bidirectional attention, while an LLM has unidirectional attention. Here, we hypothesized that unidirectional attention focuses more on surface word sequences as opposed to bidirectional attention."
- **Break condition:** If a decoder-based model is explicitly trained to capture semantic similarity (e.g., through contrastive learning), the performance gap may narrow.

### Mechanism 2
- **Claim:** In-context learning with very large decoder-based models like ChatGPT makes it difficult to identify fine-grained semantic differences.
- **Mechanism:** Large decoder-based models trained for in-context learning tend to output scores with limited granularity (e.g., rounding to nearest 5 or 10), which obscures subtle semantic distinctions needed for tasks like machine translation evaluation.
- **Core assumption:** Fine-grained scoring is required for accurate semantic evaluation.
- **Evidence anchors:**
  - [abstract] "It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences."
  - [section] "However, in the output scores, there were many cases where the last digit was 0 or 5 in both zero-shot and few-shot settings."
- **Break condition:** If the evaluation task has inherently coarse-grained labels, the granularity limitation may not significantly impact performance.

### Mechanism 3
- **Claim:** Tuned LLM accuracy increases with model size up to a point, then plateaus.
- **Mechanism:** Larger decoder-based models have more capacity to learn complex patterns, but once they reach a certain size, additional parameters provide diminishing returns for the task of semantic evaluation.
- **Core assumption:** Model size correlates with learning capacity for semantic tasks.
- **Evidence anchors:**
  - [abstract] "When a decoder-based model is tuned, accuracy increases with model size up to a point, but then plateaus."
  - [section] "For LoRA-tuned LLMs, there is a tendency for the accuracy to be proportional to the model size up to a certain model size, but it reaches a ceiling."
- **Break condition:** If the evaluation task becomes more complex (e.g., cross-lingual semantic similarity), the plateau may occur at a larger model size.

## Foundational Learning

- **Concept:** Bidirectional vs unidirectional attention in transformer models
  - **Why needed here:** Understanding the architectural difference between encoder-based and decoder-based models is crucial for explaining their performance differences in semantic evaluation tasks.
  - **Quick check question:** How does bidirectional attention in encoder models differ from unidirectional attention in decoder models, and why does this matter for semantic understanding?

- **Concept:** Fine-tuning vs in-context learning
  - **Why needed here:** The paper compares tuned models with in-context learning approaches, so understanding the strengths and limitations of each is essential for interpreting the results.
  - **Quick check question:** What are the key differences between fine-tuning a model on a specific task versus using in-context learning, and how might these affect performance on semantic evaluation tasks?

- **Concept:** Correlation metrics for evaluation
  - **Why needed here:** The paper uses Kendall's correlation to measure the alignment between model predictions and human judgments, so understanding this metric is important for interpreting the results.
  - **Quick check question:** What does Kendall's correlation measure, and why is it appropriate for evaluating the performance of semantic evaluation models?

## Architecture Onboarding

- **Component map:** Input text pair -> Model (encoder-based or decoder-based) -> Similarity score calculation -> Evaluation metric (Kendall's correlation)
- **Critical path:** Input text pair → Model processing → Similarity score calculation → Evaluation metric (Kendall's correlation)
- **Design tradeoffs:**
  - Encoder-based models have bidirectional attention, better for semantic understanding, but are smaller in size
  - Decoder-based models have unidirectional attention, focus on surface-level patterns, but can be much larger
  - Fine-tuning provides task-specific adaptation, while in-context learning avoids additional training but may lack granularity
- **Failure signatures:**
  - Poor performance on semantic tasks despite high parameter count (decoder-based models)
  - Limited score granularity in in-context learning outputs
  - Plateau in performance gains as model size increases
- **First 3 experiments:**
  1. Compare RoBERTa-large (fine-tuned) vs Cerebras-GPT (LoRA-tuned) on WMT20 MQM to verify the performance gap between encoder-based and decoder-based models
  2. Test ChatGPT's in-context learning on STS-B to confirm the granularity limitation in score outputs
  3. Vary the size of Cerebras-GPT from 111M to 6.7B parameters to observe the accuracy plateau as described in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural differences between encoder-based and decoder-based models contribute most to the performance gap in fine-tuned evaluation tasks?
- **Basis in paper:** [explicit] The paper notes that tuned decoder-based models perform poorly compared to tuned encoder-based models, and the analysis suggests that decoder-based models focus on surface word sequences rather than capturing meaning.
- **Why unresolved:** While the paper hypothesizes that unidirectional attention in decoder-based models is a key factor, it doesn't definitively prove this is the primary cause or rule out other architectural differences.
- **What evidence would resolve it:** Comparative experiments isolating attention mechanisms (e.g., bidirectional vs unidirectional) while keeping other model components constant would help identify the key architectural difference responsible for the performance gap.

### Open Question 2
- **Question:** What is the optimal model size for tuned decoder-based models beyond which additional parameters no longer improve evaluation accuracy?
- **Basis in paper:** [explicit] The paper observes that accuracy increases with model size up to a point but then plateaus, but doesn't specify the exact threshold for each task/language combination.
- **Why unresolved:** The paper provides general observations about model size effects but doesn't give precise optimal sizes for different scenarios, likely due to computational constraints in testing extremely large models.
- **What evidence would resolve it:** Systematic testing of tuned decoder-based models across a wider range of sizes (e.g., 10B, 20B, 50B+ parameters) on each task/language combination would reveal the point of diminishing returns for each scenario.

### Open Question 3
- **Question:** How can in-context learning methods be improved for translation evaluation tasks to better identify fine-grained semantic differences?
- **Basis in paper:** [explicit] The paper reveals that ChatGPT's in-context learning struggles with fine-grained semantic differences in translation evaluation, particularly when label distributions are skewed.
- **Why unresolved:** The paper identifies the problem but doesn't propose or test solutions for improving in-context learning on skewed label distributions or fine-grained distinctions.
- **What evidence would resolve it:** Experiments testing modified prompts, few-shot examples specifically designed for fine-grained distinctions, or alternative in-context learning strategies on translation tasks would show whether the issue can be mitigated.

## Limitations

- The study focuses primarily on comparison between encoder-based and decoder-based models without exploring hybrid approaches that might combine their strengths
- Limited to specific model sizes due to computational constraints, potentially missing the optimal size for some tasks
- The findings are based on specific languages (English and Japanese) and may not generalize to all languages

## Confidence

- **Mechanistic explanations for decoder-based model performance**: Medium - The claims about unidirectional attention focusing on surface sequences are inferred from performance patterns rather than directly measured
- **Observations about model size plateau**: Medium - Based on experimental conditions that may not capture the full range of possible model sizes
- **Generalizability to other languages**: Low - The study is limited to English and Japanese tasks

## Next Checks

1. Test whether explicit semantic training (e.g., contrastive learning) can reduce the performance gap between encoder-based and decoder-based models
2. Investigate if larger decoder-based models can overcome the granularity limitation in in-context learning by using regression tasks instead of classification
3. Examine whether the accuracy plateau holds for more complex semantic tasks, such as cross-lingual similarity evaluation