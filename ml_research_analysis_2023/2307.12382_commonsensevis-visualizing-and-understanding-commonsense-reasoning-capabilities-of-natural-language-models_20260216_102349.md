---
ver: rpa2
title: 'CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities
  of Natural Language Models'
arxiv_id: '2307.12382'
source_url: https://arxiv.org/abs/2307.12382
tags:
- concepts
- commonsense
- knowledge
- question
- view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CommonsenseVIS, a visual analytics system
  that contextualizes and explains model behavior on commonsense reasoning tasks.
  It uses an external knowledge base to extract relevant commonsense knowledge from
  data as concept-relation triplets.
---

# CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models

## Quick Facts
- **arXiv ID:** 2307.12382
- **Source URL:** https://arxiv.org/abs/2307.12382
- **Reference count:** 40
- **Primary result:** Visual analytics system for contextualizing and explaining commonsense reasoning in NLP models

## Executive Summary
CommonsenseVIS is a visual analytics system designed to contextualize and explain the commonsense reasoning capabilities of natural language models. It extracts relevant commonsense knowledge from QA inputs as concept-relation triplets using an external knowledge base (ConceptNet) and aligns model behavior with human reasoning through multi-level visualizations. The system was evaluated through a user study with 10 NLP experts who found it effective for understanding and diagnosing commonsense reasoning capabilities.

## Method Summary
CommonsenseVIS uses ConceptNet to extract concept-relation triplets from QA data, aligns model behavior with these concepts through feature attribution (SHAP) and linear transformation of embeddings, and presents the results through three visualization levels: Global View (UMAP projections summarizing performance), Subset View (clustering instances and comparing model vs ConceptNet concepts), and Instance View (detailed explanations with SHAP values and interactive probing/editing).

## Key Results
- Evaluated by 10 NLP experts who found the system effective for understanding and diagnosing commonsense reasoning capabilities
- Extracts relevant commonsense knowledge from inputs as concept-relation triplets using ConceptNet
- Features multi-level visualization and interactive model probing and editing for different concepts and relations
- Aligns model behavior with human knowledge through external knowledge base integration

## Why This Works (Mechanism)

### Mechanism 1
CommonsenseVIS uses ConceptNet to extract implicit commonsense knowledge from QA inputs as concept-relation triplets, enabling alignment of model behavior with human reasoning. The system tokenizes question stems into n-grams, matches them with ConceptNet concepts, builds a subgraph within two hops, and derives relational paths between question concepts and target concepts. This contextualizes model behavior for visual alignment.

### Mechanism 2
Multi-level visualization (global, subset, instance) enables scalable and systematic analysis of model behavior across concepts and relations. Global View uses UMAP projections and relation accuracy bars to summarize overall performance; Subset View clusters instances and compares model vs ConceptNet concepts; Instance View provides SHAP-based explanations and interactive probing/editing.

### Mechanism 3
Feature attribution (SHAP) quantifies input concept importance, and linear transformation of embeddings reveals learned relations between question and target concepts. SHAP computes importance scores for input words; linear least-squares transform aligns question concept embeddings with target concept embeddings for correct instances, indicating relational learning.

## Foundational Learning

- **Commonsense knowledge representation as concept-relation triplets in knowledge graphs**
  - Why needed: The system relies on extracting such triplets from ConceptNet to contextualize model behavior and align it with human reasoning
  - Quick check: Can you explain why "take an umbrella when it rains" is represented as umbrella → is used for → protection from rain in ConceptNet?

- **Feature attribution methods (e.g., SHAP) for quantifying model input importance**
  - Why needed: SHAP scores identify which input words the model relies on, enabling comparison with ConceptNet concepts for alignment analysis
  - Quick check: How does SHAP differ from LIME in terms of theoretical foundation and application to NLP models?

- **Dimensionality reduction (UMAP) for summarizing high-dimensional embeddings in scatter plots**
  - Why needed: UMAP projects model embeddings into 2D for global performance overview, preserving both local and global structure for cluster analysis
  - Quick check: Why might UMAP be preferred over PCA for visualizing model embedding clusters in this context?

## Architecture Onboarding

- **Component map:** Input (CSQA dataset, UnifiedQA model, ConceptNet) → Processing (N-gram tokenization → ConceptNet matching → Subgraph extraction → SHAP feature attribution → Linear transformation) → Visualization (Global View → Subset View → Instance View) → Output (Interactive exploration interface)
- **Critical path:** Data → Knowledge extraction → Model behavior contextualization → Multi-level visualization → Interactive exploration/editing
- **Design tradeoffs:** ConceptNet coverage vs. computational cost; N-gram size vs. phrase granularity; Linear transformation vs. non-linear relations
- **Failure signatures:** Low overlap between ConceptNet and model concepts; Misaligned clusters in Global View; High model accuracy but low ConceptNet concept overlap
- **First 3 experiments:** 1) Load CSQA and UnifiedQA, verify subgraph extraction for sample questions; 2) Compute SHAP scores for batch instances, compare top model concepts with ConceptNet; 3) Run full pipeline on small subset, explore all three views

## Open Questions the Paper Calls Out

- How does integration of multiple commonsense knowledge bases (ATOMIC, GLUCOSE) affect scalability and accuracy compared to ConceptNet alone?
- What are the computational trade-offs of using faster feature attribution methods (CXplain) versus SHAP in terms of explanation quality and scalability?
- How do the system's visualizations and interactive features impact the efficiency and accuracy of NLP experts' model diagnosis compared to traditional methods?

## Limitations

- Effectiveness depends on ConceptNet coverage for CSQA questions, which is unverified
- SHAP-based feature attribution may not capture non-linear reasoning patterns in large language models
- Small user study sample size (n=10) limits generalizability of usability claims

## Confidence

- Mechanism 1 (ConceptNet alignment): **Medium** - Core assumption depends on external knowledge base coverage
- Mechanism 2 (Multi-level visualization): **High** - Well-established visualization patterns with proven effectiveness
- Mechanism 3 (SHAP and linear transformation): **Medium** - Standard techniques but may not capture complex model behavior

## Next Checks

1. Test ConceptNet coverage on 100 random CSQA questions to quantify knowledge gap rates
2. Compare SHAP explanations with LIME and integrated gradients to assess robustness of feature attribution
3. Conduct user study with stratified sampling (different expertise levels) to validate usability across user types