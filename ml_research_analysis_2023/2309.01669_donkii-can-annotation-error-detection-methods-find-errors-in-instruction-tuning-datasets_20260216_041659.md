---
ver: rpa2
title: 'Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning
  Datasets?'
arxiv_id: '2309.01669'
source_url: https://arxiv.org/abs/2309.01669
tags:
- errors
- instances
- output
- instruction
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DONKII, a novel benchmark for evaluating
  Annotation Error Detection (AED) methods on instruction-tuning datasets. It addresses
  the challenge of applying AED methods, traditionally used in classification, to
  generative language tasks.
---

# Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?

## Quick Facts
- arXiv ID: 2309.01669
- Source URL: https://arxiv.org/abs/2309.01669
- Authors: 
- Reference count: 23
- Primary result: Average probability (Pµ) is the best-performing AED method for instruction-tuning datasets

## Executive Summary
This paper introduces DONKII, a novel benchmark for evaluating Annotation Error Detection (AED) methods on instruction-tuning datasets. The authors created three instruction-tuning datasets enriched with expert-annotated errors, developed a taxonomy of error types, and proposed four AED baselines based on training dynamics. Experiments showed that average probability (Pµ) is the best-performing AED method, with model size being crucial for effectiveness. The case study revealed that errors in instruction-tuning datasets have minimal quantitative impact on downstream performance but can qualitatively affect model behavior.

## Method Summary
The authors created three instruction-tuning datasets (P3-Donkii, SNI-Donkii, ADC-Donkii) enriched with expert-annotated errors, then trained T5 models of varying sizes on these datasets. They computed four AED scores (PPL, Pµ, Pmin, AUM) during training and evaluated performance using Average Precision. The study also included a case study examining the impact of AED-cleaned datasets on downstream performance.

## Key Results
- Pµ (average probability) consistently outperforms other AED methods across all model sizes
- Model size significantly impacts AED effectiveness, with different sizes performing best on different datasets
- AED methods successfully identify annotation errors in instruction-tuning datasets
- Case study shows minimal quantitative impact on downstream performance but qualitative effects on model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pµ (average probability) is the best-performing AED method for instruction-tuning data because it captures overall token confidence trends across epochs.
- Mechanism: By averaging the probability assigned to each token in the output sequence over all training epochs, Pµ creates a stable signal that correlates with annotation quality.
- Core assumption: Lower average token probabilities indicate higher likelihood of annotation errors.
- Evidence anchors:
  - [abstract]: "Experiments showed that average probability (Pµ) is the best-performing AED method"
  - [section]: "On average, Pµ (average probability) performs the best across all model sizes"
- Break condition: If the relationship between token probabilities and annotation quality is not monotonic, Pµ performance would degrade.

### Mechanism 2
- Claim: Training dynamics-based methods work for generative tasks when adapted from classification by computing scores over output sequences.
- Mechanism: Instead of class probabilities, these methods use token-level probabilities from the language model's output distribution, averaged or otherwise aggregated.
- Core assumption: The same statistical patterns that indicate errors in classification (e.g., low confidence) also indicate errors in generation.
- Evidence anchors:
  - [abstract]: "We propose four AED baselines for the generative setting and evaluate them extensively on the newly introduced dataset"
  - [section]: "We propose the following measures: (1) Perplexity, (2) The (negative) average probability, (3) The (negative) minimum probability, (4) The (negative) Area-under-the-Margin score"
- Break condition: If generative tasks have fundamentally different error patterns than classification, the adapted methods may not generalize.

### Mechanism 3
- Claim: Model size is crucial for AED effectiveness because larger models have more stable training dynamics and better token probability calibration.
- Mechanism: Larger models produce more reliable probability distributions, making error signals clearer and more discriminative.
- Core assumption: The relationship between model capacity and probability calibration is strong enough to impact AED performance.
- Evidence anchors:
  - [abstract]: "model size being crucial for effectiveness"
  - [section]: "Regarding model size, small performs best for P3 and ADC, while large is the best for SNI. On average, base and large achieve the best performance"
- Break condition: If model size effects are task-dependent or if smaller models are sufficiently calibrated, the relationship may not hold.

## Foundational Learning

- Concept: Training dynamics
  - Why needed here: AED methods rely on statistics collected during model training to identify annotation errors.
  - Quick check question: What are the four training dynamics measures proposed for AED in generative settings?

- Concept: Instruction-tuning datasets
  - Why needed here: The paper introduces a novel benchmark for AED specifically on instruction-tuning data, which has different characteristics than traditional classification datasets.
  - Quick check question: What are the three types of instruction-tuning datasets distinguished in the paper?

- Concept: Annotation error detection (AED)
  - Why needed here: The paper evaluates whether AED methods, traditionally used for classification, can be adapted to find errors in instruction-tuning datasets.
  - Quick check question: What is the difference between flaggers and scorers in AED?

## Architecture Onboarding

- Component map:
  - Donkii datasets (P3-Donkii, SNI-Donkii, ADC-Donkii) -> AED baselines (PPL, Pµ, Pmin, AUM) -> Evaluation protocol (ranking-based with AP metric) -> T5-based models of varying sizes -> Case study evaluation pipeline

- Critical path:
  1. Create synthetic or extract real errors from source datasets
  2. Train T5 models on instruction-tuning data
  3. Compute AED scores during training
  4. Evaluate AED performance using AP metric
  5. Analyze impact of errors on downstream performance

- Design tradeoffs:
  - Synthetic vs real errors: Synthetic errors (P3-Donkii) offer control but may not reflect real error patterns
  - Model size: Larger models may perform better but at higher computational cost
  - Epoch aggregation: Averaging scores over epochs improves stability but increases computation time

- Failure signatures:
  - Random performance: AED methods not distinguishing between error and non-error instances
  - Inverted ranking: Higher scores assigned to non-error instances
  - High variance across seeds: Unstable AED performance indicating method sensitivity

- First 3 experiments:
  1. Evaluate all four AED methods on P3-Donkii with different model sizes
  2. Compare instance-level vs task-level AED performance on SNI-Donkii
  3. Analyze AED performance per error category on ADC-Donkii

## Open Questions the Paper Calls Out
- How does the performance of AED methods vary across different instruction-tuning dataset domains (e.g., general NLP tasks vs. specialized domains like biomedical text)?
- What is the long-term impact of undetected errors in instruction-tuning datasets on model behavior beyond immediate performance metrics?
- How effective are AED methods in detecting errors in instruction-tuning datasets created by different generation approaches (e.g., human-authored vs. LLM-authored vs. mixed methods)?

## Limitations
- Donkii datasets are relatively small compared to full instruction-tuning datasets
- Synthetic errors may not capture full diversity of real-world annotation errors
- Limited set of downstream tasks used in case study

## Confidence
- High Confidence: Pµ is best-performing AED method
- Medium Confidence: Model size is crucial for AED effectiveness
- Medium Confidence: AED can find errors in instruction-tuning datasets

## Next Checks
1. Evaluate best-performing AED methods from Donkii on other instruction-tuning datasets (e.g., OASST1, OpenHermes) to test generalization
2. Conduct human-in-the-loop validation where annotators review top-ranked instances identified by AED methods
3. Test impact of AED-cleaned datasets on a broader range of downstream tasks, including non-language tasks