---
ver: rpa2
title: 'Looking Inside Out: Anticipating Driver Intent From Videos'
arxiv_id: '2312.01444'
source_url: https://arxiv.org/abs/2312.01444
tags:
- driver
- data
- prediction
- features
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a driver intent prediction system that combines
  in-cabin and external camera data using handcrafted features. Their approach extracts
  driver gaze and head pose from interior camera data, and object detections and road
  features from exterior camera data.
---

# Looking Inside Out: Anticipating Driver Intent From Videos

## Quick Facts
- arXiv ID: 2312.01444
- Source URL: https://arxiv.org/abs/2312.01444
- Reference count: 22
- Primary result: 87.5% accuracy with 4.35s average prediction time on Brains4Cars dataset

## Executive Summary
This paper presents a driver intent prediction system that combines in-cabin and external camera data using handcrafted features. The approach extracts driver gaze and head pose from interior camera data, and object detections and road features from exterior camera data. These features are fed into both LSTM and transformer-based architectures, achieving state-of-the-art performance on the Brains4Cars dataset with 87.5% accuracy and an average prediction time of 4.35 seconds before the maneuver occurs. The method significantly outperforms prior approaches that either used only in-cabin data or learned end-to-end features from external video.

## Method Summary
The method uses handcrafted features from both interior and exterior cameras to predict driver intent across five maneuvers (straight, left/right turn, left/right lane change). Interior data is processed to extract gaze and head pose using MediaPipe, while exterior data is analyzed using Grounding Dino for object detection and lane information. Two architectures are proposed: F-LSTM uses separate LSTMs for each feature type (gaze/head pose, objects, lanes) before combining them in an MLP, while F-TF uses a transformer with self-attention on concatenated feature streams. Both models are trained using cross-entropy loss with 10-fold cross-validation on the Brains4Cars dataset.

## Key Results
- Achieves 87.5% accuracy on Brains4Cars dataset, outperforming state-of-the-art methods
- Average prediction time of 4.35 seconds before maneuver occurs
- Ablative experiments confirm handcrafted exterior features are critical for performance
- Transformer architecture shows promise but requires more data to significantly outperform LSTM baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Handcrafted object and road features from external cameras provide critical contextual information that learned features from raw video lack.
- Mechanism: Explicit extraction of object detections (CAR, BICYCLE, PERSON, TRAFFIC SIGN, TRAFFIC LIGHT) and lane/intersection data gives the model interpretable, semantically meaningful inputs that correlate strongly with driver intentions.
- Core assumption: The relationship between surrounding objects/lane configuration and driver intent is sufficiently stable and predictable to be captured by handcrafted features.
- Evidence anchors:
  - [abstract] "Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention."
  - [section] "We leverage Grounding Dino [15], a SOTA zero-shot 2D object detector to detect the following object classes: CAR, BICYCLE, PERSON, TRAFFIC SIGN, TRAFFIC LIGHT, and DATE."

### Mechanism 2
- Claim: Fusing multiple LSTM streams allows each modality to specialize in interpreting its own type of data while sharing temporal context.
- Mechanism: Separate LSTMs for gaze/head pose, object detections, and lane information process their respective inputs, then their outputs are flattened and combined in an MLP, allowing specialized temporal modeling per modality.
- Core assumption: Different data modalities benefit from specialized processing rather than a single shared LSTM layer.
- Evidence anchors:
  - [section] "Figure 4 shows the architecture of the F-LSTM. Separate LSTMs are used to accept the inputs for head pose and gaze, vehicle objects, and lane detections."
  - [section] "LSTM 1 accepts gaze and head pose information and has a hidden dimension of 10. LSTM 2 accepts lane information and has a hidden dimension of 5. LSTM 3 accepts surrounding vehicle information and has a hidden dimension of 10."

### Mechanism 3
- Claim: The transformer's self-attention mechanism can learn long-range temporal dependencies that LSTMs struggle with, especially when sufficient data is available.
- Mechanism: By using self-attention across all time steps in the sequence, the transformer can directly model relationships between distant frames without the path length limitations of LSTMs.
- Core assumption: The dataset is large enough for the transformer to learn meaningful long-range dependencies.
- Evidence anchors:
  - [abstract] "The transformer architecture shows promise for learning long-range temporal dependencies but requires more data to significantly outperform the LSTM baseline."
  - [section] "While LSTM-based architectures have proven successful for sequential tasks, prior works [16] demonstrate that they are adversely affected by long-range time dependencies due to increasing path length for signals."

## Foundational Learning

- Concept: Object detection and tracking
  - Why needed here: The system relies on accurate detection of surrounding vehicles, pedestrians, and traffic infrastructure to understand the driving context.
  - Quick check question: Can you explain how zero-shot object detection works and why it was chosen over a dataset-specific detector?

- Concept: Head pose and gaze estimation from 2D images
  - Why needed here: Driver intent prediction relies heavily on understanding where the driver is looking and facing, which requires converting 2D facial landmarks to 3D gaze vectors.
  - Quick check question: What are the limitations of using MediaPipe for face landmark detection in varying lighting and occlusion conditions?

- Concept: LSTM and transformer architectures for sequential data
  - Why needed here: Both architectures are used to process temporal sequences of driver behavior and environmental data, with LSTMs being more data-efficient and transformers potentially better at long-range dependencies.
  - Quick check question: What are the key differences in how LSTMs and transformers handle long-term temporal dependencies, and when would each be preferred?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Feature extraction -> LSTM/transformer processing -> Classification -> Evaluation
  MediaPipe/MediaPipe -> 4D gaze/head pose vector
  Grounding Dino -> 25D object detection vector
  Lane labels -> 3D lane configuration vector
  Three separate LSTMs -> Flattened features -> MLP -> Classification

- Critical path: Data preprocessing → feature vector construction → LSTM/transformer processing → classification → evaluation

- Design tradeoffs:
  - Handcrafted vs. learned features: Handcrafted provides interpretability and domain knowledge but may miss novel patterns; learned features are more flexible but require more data
  - Separate vs. joint LSTMs: Separate allows specialization but may miss cross-modal interactions; joint could capture interactions but may be harder to train
  - LSTM vs. transformer: LSTM is more data-efficient and established; transformer can model longer dependencies but needs more data

- Failure signatures:
  - Low accuracy with high variance: Likely overfitting due to limited data, especially for transformer
  - Poor performance on specific maneuvers: May indicate missing features for those scenarios (e.g., no detection of cyclists for left turns)
  - Degradation with earlier prediction times: Suggests temporal modeling is insufficient for longer horizons

- First 3 experiments:
  1. Ablation study: Train F-LSTM and F-TF without external features to quantify their contribution
  2. Data augmentation: Apply temporal shifts, horizontal flips, and brightness adjustments to increase training data diversity
  3. Cross-dataset validation: Test models on a different driver intent dataset to evaluate generalization beyond Brains4Cars

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed transformer architecture scale in performance if trained on significantly larger datasets, such as those containing millions of driving maneuvers instead of the current Brains4Cars dataset?
- Basis in paper: [explicit] The paper states "We postulate that our F-TF architecture would scale more effectively than other methods if provided with far more training data" and "It is well-understood that computer vision transformer architectures [18] require internet-scale amounts of data to significantly surpass convolutional neural network (CNN) architectures."
- Why unresolved: The authors explicitly acknowledge that the transformer requires more data to demonstrate its full potential, and they only had access to the Brains4Cars dataset for evaluation.
- What evidence would resolve it: Training the same F-TF architecture on a much larger dataset (e.g., 10x or 100x the size of Brains4Cars) and comparing its performance metrics against the current results and the LSTM baseline would provide concrete evidence.

### Open Question 2
- Question: Would incorporating additional driver behavior features, such as hand position on the steering wheel or foot pedal pressure, further improve prediction accuracy and time-to-maneuver?
- Basis in paper: [inferred] The authors note that their approach uses "hand-crafted features" and achieves state-of-the-art results, but the feature space is limited to gaze, head pose, object detections, and road features. The discussion mentions that "the number of vehicles being driven is continuously increasing, but less than half of all drivers follow even basic safety conduct like turning on a blinker before performing a lane change."
- Why unresolved: The current model relies on visual data and doesn't incorporate direct vehicle control inputs, which could provide additional predictive signals.
- What evidence would resolve it: Extending the model to include additional sensor data from the vehicle's CAN bus (steering angle, pedal positions, turn signal status) and evaluating whether this additional information improves accuracy or TUM compared to the current approach.

### Open Question 3
- Question: How would the model performance change if tested on datasets from different geographic regions with varying driving cultures, road designs, and traffic patterns?
- Basis in paper: [inferred] The evaluation is performed solely on the Brains4Cars dataset, which likely represents a specific geographic region. The paper's discussion of "predicting driver intentions across 5 driving maneuvers" assumes a particular driving context.
- Why unresolved: The current evaluation doesn't address the model's generalizability to different driving environments and cultures, which could significantly impact the relevance of the learned features.
- What evidence would resolve it: Testing the trained model on datasets collected from different countries or regions with distinct driving patterns (e.g., datasets from European, Asian, or African driving contexts) and measuring performance degradation or the need for fine-tuning.

## Limitations
- Small dataset size (634 videos) limits generalizability and transformer performance
- Handcrafted features assume stable relationships between objects/road configurations and driver intent across environments
- Performance evaluation limited to single geographic region without cross-cultural validation

## Confidence
- High confidence: F-LSTM architecture's performance metrics and ablative experiments are well-supported
- Medium confidence: Comparative advantage over end-to-end learned features is reasonably demonstrated
- Low confidence: Transformer architecture's claimed superiority is not fully substantiated due to data limitations

## Next Checks
1. Cross-dataset validation: Test models on a different driver intent prediction dataset to evaluate generalization beyond Brains4Cars
2. Feature importance analysis: Conduct detailed ablation study removing individual feature categories to quantify their contributions
3. Transformer scaling study: Train F-TF with varying amounts of augmented data to identify data threshold for outperforming LSTM baseline