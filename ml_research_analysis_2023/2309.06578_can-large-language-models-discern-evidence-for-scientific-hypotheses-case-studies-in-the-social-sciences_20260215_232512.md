---
ver: rpa2
title: Can Large Language Models Discern Evidence for Scientific Hypotheses? Case
  Studies in the Social Sciences
arxiv_id: '2309.06578'
source_url: https://arxiv.org/abs/2309.06578
tags:
- hypothesis
- dataset
- language
- prompt
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of scientific hypothesis evidencing
  (SHE), which aims to determine whether a scientific abstract provides evidence supporting
  or refuting a given hypothesis. The authors create a novel dataset by annotating
  abstracts from 12 collaborative literature reviews in the social sciences, containing
  69 hypotheses and 602 articles with corresponding entailment, contradiction, or
  inconclusive labels.
---

# Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences

## Quick Facts
- arXiv ID: 2309.06578
- Source URL: https://arxiv.org/abs/2309.06578
- Reference count: 40
- Primary result: SHE task is challenging for NLU models, with best supervised model achieving 0.615 macro-F1; LLMs perform competitively in zero-shot but don't exceed traditional models

## Executive Summary
This paper introduces the Scientific Hypothesis Evidencing (SHE) task, which aims to determine whether scientific abstracts provide evidence supporting or refuting given hypotheses. The authors create a novel dataset from 12 collaborative literature reviews in social sciences, containing 69 hypotheses and 602 article-abstract pairs with entailment, contradiction, or inconclusive labels. They evaluate multiple model families including supervised classifiers, transfer learning models, and large language models. Results show that SHE is a challenging task for current NLU models, with the best supervised embedding model achieving a macro-F1 score of 0.615. While LLMs perform competitively in zero-shot settings, their performance does not exceed traditional models, highlighting the need for more specialized approaches.

## Method Summary
The authors created the CoRe dataset by annotating abstracts from 12 collaborative literature reviews in social sciences, generating 69 hypotheses and 602 article-abstract pairs with corresponding entailment, contradiction, or inconclusive labels. They evaluated three model families: supervised classifiers using pre-trained embeddings (Longformer, text-embedding-ada-002), transfer learning models (ESIM, MT-DNN), and large language models (ChatGPT, PaLM 2) in zero-shot and few-shot settings. The CoRe dataset was split into training (70%), validation (15%), and held-out (15%) sets. Models were evaluated using macro-F1 score and accuracy metrics, with LLMs tested across five different prompt styles to assess the impact of prompt engineering on performance.

## Key Results
- SHE task is challenging for NLU models, with best supervised embedding model achieving 0.615 macro-F1 score
- LLMs perform competitively with traditional models in zero-shot settings but don't exceed their performance
- Performance varies significantly across different prompt styles for LLMs (0.337 to 0.479 macro-F1 for gpt-3.5-turbo)
- Models trained on SNLI dataset show significantly lower performance compared to those trained on CoRe dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHE task is difficult because it requires latent evidence extraction from abstracts to support abstract-level hypotheses
- Mechanism: Hypotheses in CoRe are expressed at a higher level of abstraction than the evidence within abstracts. Models must perform contextual reasoning to link specific behaviors or outcomes mentioned in abstracts to broader conceptual hypotheses
- Core assumption: Abstracts contain sufficient evidence to test high-level hypotheses through inference, even when explicit terminology differs
- Evidence anchors:
  - [abstract]: "In SHE, the hypotheses or research questions are typically expressed at a higher level of abstraction than the evidence provided within the abstract."
  - [section]: "For example, in Table 1, identifying the relationship between the abstract and hypothesis provided requires the model to reason that depressive disorder is a bad mental health outcome, usage of Twitter, Facebook, Instagram, Snapchat is use of social media leading to higher likelihood of major depressive disorder and hence the relationship is entailment."
- Break condition: If abstracts consistently lack the necessary contextual clues or if terminology gaps are too wide for inference

### Mechanism 2
- Claim: Domain-specific knowledge is crucial for SHE task performance
- Mechanism: Scientific abstracts contain domain-specific terminology and numerical data that differ from general NLI datasets. Models need specialized understanding of social science concepts to accurately map evidence to hypotheses
- Core assumption: Social science domain knowledge is transferable from pre-training data and can be leveraged for hypothesis-evidencing
- Evidence anchors:
  - [abstract]: "The language used in scientific publications contains domain-specific terminology which is different from the premise-hypothesis pairs in general domains (e.g., SNLI [5]). Furthermore, abstracts from scientific articles contain numerical data that is often not present in traditional NLP datasets."
  - [section]: "As anticipated, ESIM and MT-DNN models when trained and fine-tuned on the SNLI dataset respectively, exhibited significantly lower performance compared to training, fine-tuning on the CoRe dataset. This can be attributed to the differences in the characteristics of hypotheses and premises in the datasets."
- Break condition: If domain-specific terminology becomes too specialized or if numerical data interpretation is critical for hypothesis evaluation

### Mechanism 3
- Claim: Prompt engineering significantly impacts LLM performance on SHE task
- Mechanism: Different prompt styles guide LLMs to approach the hypothesis-evidencing task differently, affecting their ability to extract and map evidence from abstracts to hypotheses
- Core assumption: LLMs can be effectively guided through prompt engineering to perform complex reasoning tasks when given clear instructions
- Evidence anchors:
  - [abstract]: "For instance, in the zero-shot setting, gpt-3.5-turbo recorded the lowest mean macro-f1-score of 0.337 when prompted with P4 whereas it achieved highest mean macro-f1-score of 0.479 when using prompt P3."
  - [section]: "While variations in performance were observed across individual runs, for a given prompt, the temperature setting did not have a major influence on the average performance of LLMs."
- Break condition: If prompts become too complex or if LLM's reasoning capabilities are insufficient for the task complexity

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: SHE task is fundamentally an NLI problem where models must determine if abstract text entails, contradicts, or is neutral to a given hypothesis
  - Quick check question: What are the three possible relationships between a premise and hypothesis in NLI tasks?

- Concept: Transfer learning
  - Why needed here: Models are fine-tuned from pre-trained language models on the CoRe dataset to adapt to the specific characteristics of scientific hypothesis-evidencing
  - Quick check question: Why might models trained on general NLI datasets like SNLI perform worse on the CoRe dataset?

- Concept: Prompt engineering
  - Why needed here: Different prompt styles can significantly impact LLM performance on the SHE task, requiring careful design of instructions and output formats
  - Quick check question: How does the choice of labels in prompts (e.g., true/false/neutral vs. entail/contradict/neutral) affect LLM performance?

## Architecture Onboarding

- Component map: Collaborative review documents -> hypothesis-abstract-label triplets -> embedding models (Longformer, text-embedding-ada-002) -> transfer learning models (ESIM, MT-DNN) -> LLM models (ChatGPT, PaLM 2) -> evaluation framework (Macro-F1 score calculation)

- Critical path:
  1. Load hypothesis-abstract pairs from CoRe dataset
  2. Generate embeddings or apply transfer learning models
  3. Make predictions on entailment, contradiction, or inconclusive
  4. Calculate macro-F1 score for evaluation

- Design tradeoffs:
  - Embedding-based models vs. transfer learning: Faster inference vs. potentially better performance
  - Zero-shot vs. few-shot LLM: No training data needed vs. better performance with examples
  - Prompt simplicity vs. specificity: Easier to implement vs. potentially better guidance

- Failure signatures:
  - Low macro-F1 scores across all models (indicating task difficulty)
  - Inconsistent performance across different prompt styles
  - Poor generalization from SNLI to CoRe dataset

- First 3 experiments:
  1. Compare performance of embedding-based models (Longformer vs. text-embedding-ada-002) on the CoRe dataset
  2. Test different prompt styles with ChatGPT in zero-shot setting to identify most effective prompt template
  3. Evaluate the impact of few-shot examples on LLM performance across different temperature settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning GPT-3.5-turbo on the CoRe dataset compare to the performance of embedding-based models?
- Basis in paper: [explicit] The paper states that OpenAI recently introduced fine-tuning functionality for gpt-3.5-turbo, suggesting future work should investigate its performance after fine-tuning on the CoRe dataset.
- Why unresolved: The authors did not have access to fine-tuned GPT-3.5-turbo models for their experiments and only tested the zero-shot and few-shot capabilities of existing LLMs.
- What evidence would resolve it: Training and evaluating a fine-tuned GPT-3.5-turbo model on the CoRe dataset and comparing its performance to embedding-based models in terms of macro-F1-score and accuracy.

### Open Question 2
- Question: What is the impact of prompt tuning on the performance of LLMs for the SHE task?
- Basis in paper: [inferred] The authors suggest exploring prompt tuning in future work, as discrete human-generated prompts have limitations and prompt tuning has shown promise in other NLP tasks.
- Why unresolved: The paper only tested five pre-defined prompts and did not explore prompt tuning techniques that could potentially improve LLM performance.
- What evidence would resolve it: Implementing and evaluating different prompt tuning strategies on LLMs for the SHE task and comparing their performance to fixed prompts and other model families.

### Open Question 3
- Question: How does the performance of LLMs on the SHE task generalize to other scientific domains beyond the social sciences?
- Basis in paper: [explicit] The authors focused on the social sciences due to the availability of high-quality datasets annotated by domain experts, but acknowledge that the SHE task could be applied to other scientific domains.
- Why unresolved: The CoRe dataset used in the paper is specific to the social sciences, and the performance of LLMs on SHE may vary depending on the domain-specific terminology, writing styles, and evidence presentation in other fields.
- What evidence would resolve it: Creating and evaluating SHE datasets from other scientific domains (e.g., biomedical, physics, computer science) and testing the performance of LLMs and other model families on these datasets to assess generalizability.

## Limitations

- Manual annotation process for mapping study outputs to entailment/contradiction/inconclusive labels may introduce inconsistencies and subjective interpretation
- Zero-shot performance variation across different LLM prompts indicates high sensitivity to prompt engineering, but the extent of this sensitivity remains unclear
- Lack of human performance baselines makes it difficult to assess whether observed model performance represents reasonable capability given task complexity

## Confidence

- High confidence: Dataset creation methodology and annotation scheme are well-documented and reproducible; task definition is clearly specified with concrete examples
- Medium confidence: Observed performance differences between model families are consistent, but absolute performance levels may be influenced by annotation inconsistencies and prompt sensitivity
- Low confidence: Generalizability of results to other scientific domains beyond social sciences; extent to which observed performance limitations reflect model capabilities versus dataset annotation quality

## Next Checks

1. **Human baseline evaluation**: Have domain experts in social sciences evaluate a random subset of 100 hypothesis-abstract pairs to establish human performance benchmarks and assess annotation consistency in the CoRe dataset.

2. **Prompt sensitivity analysis**: Systematically vary prompt templates, labels, and formatting for the same LLM across 50+ hypothesis-abstract pairs to quantify the impact of prompt engineering on performance variance.

3. **Cross-domain generalization test**: Apply the best-performing models to a held-out dataset of scientific abstracts from natural sciences or biomedical domains to evaluate domain transferability of the learned hypothesis-evidencing capabilities.