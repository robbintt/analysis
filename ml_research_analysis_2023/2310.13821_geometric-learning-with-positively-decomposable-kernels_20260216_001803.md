---
ver: rpa2
title: Geometric Learning with Positively Decomposable Kernels
arxiv_id: '2310.13821'
source_url: https://arxiv.org/abs/2310.13821
tags:
- kernels
- kernel
- theorem
- space
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning with non-positive
  definite (PD) kernels on non-Euclidean data spaces, where constructing PD kernels
  is challenging. The authors propose using reproducing kernel Krein spaces (RKKS)
  based methods, which require only kernels that admit a positive (PD) decomposition.
---

# Geometric Learning with Positively Decomposable Kernels

## Quick Facts
- arXiv ID: 2310.13821
- Source URL: https://arxiv.org/abs/2310.13821
- Authors: 
- Reference count: 39
- Key outcome: Proposes using reproducing kernel Krein spaces (RKKS) for learning with non-positive definite kernels on non-Euclidean data spaces, requiring only kernels that admit positive decompositions without needing access to the decomposition.

## Executive Summary
This paper addresses the challenge of learning with non-positive definite kernels on non-Euclidean data spaces, where constructing positive definite kernels is difficult. The authors propose a framework using reproducing kernel Krein spaces (RKKS) that only requires kernels admitting a positive decomposition, without needing explicit access to the decomposition. They show that invariant kernels on homogeneous spaces admit positive decompositions under tractable regularity assumptions, making them easier to construct than positive definite kernels. This provides a route for learning with kernels for non-Euclidean data while maintaining theoretical foundations similar to classical kernel methods.

## Method Summary
The paper uses harmonic analysis on double coset spaces H\G/H to characterize positive decomposable invariant kernels. By mapping invariant kernels to Hermitian functions on H\G/H and leveraging the Fourier-Stieltjes algebra, they show that smooth functions with appropriate decay at infinity admit positive decompositions. The framework relies on RKKS theory, which generalizes RKHS to handle non-positive definite kernels. The representer theorem for RKKS demonstrates that learning solutions are finite linear combinations of kernel sections when the regularizer is linear in the squared indefinite inner product, and crucially, only the existence of a positive decomposition is needed, not explicit knowledge of it.

## Key Results
- Invariant kernels on homogeneous spaces admit positive decompositions under regularity assumptions, making them easier to construct than positive definite kernels
- Gaussian kernels admit positive decompositions on many non-Euclidean spaces despite not being positive definite
- Learning with non-positive definite kernels in RKKS requires only the existence of a positive decomposition, not explicit knowledge of it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Invariant kernels on homogeneous spaces admit positive decompositions under regularity assumptions.
- Mechanism: The paper uses harmonic analysis on double coset spaces H\G/H to characterize PD decomposable invariant kernels. By mapping invariant kernels to Hermitian functions on H\G/H, they leverage results from Fourier-Stieltjes algebras to show that smooth functions with appropriate decay at infinity admit PD decompositions.
- Core assumption: The kernel is invariant under a transitive group action, and the underlying space has sufficient symmetry structure.
- Evidence anchors:
  - [abstract]: "invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions"
  - [section 3]: "We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions."
- Break condition: If the kernel is not invariant or the space lacks sufficient symmetry structure, the method fails.

### Mechanism 2
- Claim: Gaussian kernels admit positive decompositions on many non-Euclidean spaces despite not being positive definite.
- Mechanism: By showing that the Gaussian kernel is smooth with exponential decay of derivatives at infinity, it satisfies the regularity conditions for belonging to BC(H\G/H), which guarantees a positive decomposition.
- Core assumption: The Gaussian kernel is smooth and its derivatives decay exponentially at infinity on the given space.
- Evidence anchors:
  - [section 4]: "we show that the Gaussian kernel, despite failing to be PD for instance on the torus Tn, has a PD decomposition on any Abelian Lie group."
  - [section 5.2]: "if g ∈ C∞([0, ∞)) is such that it and its derivatives decay exponentially at infinity... then f ∈ BC(H\G/H). In particular, the Gaussian function exp(−λd(eH, ·)2) ∈ BR(H\G/H)"
- Break condition: If the space lacks the required smoothness or the Gaussian kernel's derivatives do not decay exponentially at infinity, the method fails.

### Mechanism 3
- Claim: Learning with non-positive definite kernels in RKKS requires only the existence of a positive decomposition, not explicit knowledge of it.
- Mechanism: The representer theorem for RKKS shows that solutions to learning problems are finite linear combinations of kernel sections when the regularizer is linear in the squared indefinite inner product. Crucially, one only needs to know that a positive decomposition exists, not have access to it explicitly.
- Core assumption: The kernel admits a positive decomposition and the learning problem's regularizer is linear in the squared indefinite inner product.
- Evidence anchors:
  - [section 2.2]: "Crucially, we have shown in Theorem 10, that in the case where g is multiplication by a constant... one does not need access to the feature map Φ, nor the RKKS K and its indefinite inner product ⟨·, ·⟩K. So, in this case, one does not need access to the PD decomposition of k to apply Theorem 10; one only needs to know of its existence."
- Break condition: If the regularizer is not linear in the squared indefinite inner product, explicit knowledge of the decomposition may be required.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: RKHS theory provides the foundation for understanding positive definite kernels and classical kernel methods in machine learning.
  - Quick check question: What property must a kernel have to give rise to an RKHS, and what does this property ensure about the associated space of functions?

- Concept: Krein spaces and reproducing kernel Krein spaces (RKKS)
  - Why needed here: RKKS generalize RKHS to handle non-positive definite kernels, allowing the use of a broader class of kernels in machine learning while maintaining a similar theoretical framework.
  - Quick check question: How does a Krein space differ from a Hilbert space, and what property must a kernel have to give rise to an RKKS?

- Concept: Harmonic analysis on locally compact groups
  - Why needed here: The paper uses harmonic analysis, particularly the Fourier-Stieltjes algebra, to characterize which functions on homogeneous spaces admit positive decompositions, which is crucial for understanding when non-positive definite kernels can be used in learning.
  - Quick check question: What is the relationship between positive definite functions on a locally compact group and finite positive measures on its dual group?

## Architecture Onboarding

- Component map: Harmonic analysis on H\G/H -> Characterization of positive decomposable invariant kernels -> RKKS framework for learning -> Application to non-Euclidean spaces
- Critical path: Understand the need for RKKS-based methods -> Characterize positive decompositions for invariant kernels -> Apply to specific non-Euclidean spaces -> Verify theoretical results with examples
- Design tradeoffs: The main tradeoff is between the generality of the approach (working with invariant kernels on homogeneous spaces) and the computational complexity of working with Krein spaces versus Hilbert spaces. Another tradeoff is between the strength of the regularity assumptions required and the class of kernels that can be handled.
- Failure signatures: Key failure modes include: attempting to use a kernel that does not admit a positive decomposition, applying the theory to spaces that lack sufficient symmetry structure, or using kernels with insufficient smoothness or decay properties.
- First 3 experiments:
  1. Verify that the Gaussian kernel on the circle S1 has a positive decomposition by checking its smoothness and decay properties.
  2. Test the representer theorem for RKKS on a simple learning problem using a known positive decomposable kernel.
  3. Apply the theory to a non-Euclidean space of interest (e.g., a sphere or hyperbolic space) by checking if the kernel is invariant and has the required regularity properties.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the Gaussian kernel admit a positive definite (PD) decomposition on non-Euclidean spaces beyond the examples provided (e.g., compact Lie groups, non-compact symmetric spaces)?
- Basis in paper: [explicit] The paper shows the Gaussian kernel has a PD decomposition on Abelian Lie groups and non-compact symmetric spaces, but notes this is an open question for more general non-Euclidean spaces.
- Why unresolved: The authors have not fully characterized the necessary and sufficient conditions for the Gaussian kernel to have a PD decomposition on arbitrary non-Euclidean spaces. They only provide sufficient conditions for specific classes of spaces.
- What evidence would resolve it: A complete characterization of the conditions under which the Gaussian kernel admits a PD decomposition on arbitrary non-Euclidean spaces, potentially involving a combination of geometric and analytic properties of the space.

### Open Question 2
- Question: Can the correspondence between Hermitian functions on double coset spaces and invariant kernels be extended to non-locally compact groups or more general homogeneous spaces?
- Basis in paper: [explicit] The paper focuses on locally compact groups and their homogeneous spaces, but does not explore whether the results can be generalized to non-locally compact groups or more general homogeneous spaces.
- Why unresolved: The authors have not investigated the applicability of their results to more general settings, such as non-locally compact groups or more general homogeneous spaces, which could be of interest in certain applications.
- What evidence would resolve it: A proof or counterexample showing whether the correspondence between Hermitian functions on double coset spaces and invariant kernels holds for non-locally compact groups or more general homogeneous spaces.

### Open Question 3
- Question: Are there efficient algorithms for computing the PD decomposition of a given kernel on a non-Euclidean space, especially for spaces where the decomposition is known to exist?
- Basis in paper: [inferred] The paper establishes the existence of PD decompositions for certain classes of kernels on non-Euclidean spaces but does not provide explicit algorithms for computing these decompositions.
- Why unresolved: The authors have not addressed the computational aspects of finding PD decompositions, which is crucial for practical applications of RKKS-based methods.
- What evidence would resolve it: The development and analysis of algorithms for computing PD decompositions of kernels on non-Euclidean spaces, along with their computational complexity and numerical stability.

## Limitations
- The paper assumes the underlying space is a homogeneous space G/H with a transitive group action, which may not always hold for practical datasets
- The regularity conditions for positive decomposability (smoothness and decay at infinity) may be difficult to verify for complex non-Euclidean spaces
- The paper does not provide explicit constructions of positive decompositions for specific non-Euclidean geometries, only sufficient conditions for their existence

## Confidence

**High confidence**: The theoretical framework for RKKS-based learning methods, including the representer theorem and conditions for positive decomposability of invariant kernels.

**Medium confidence**: The application of these results to specific non-Euclidean spaces like tori and spheres, as the regularity conditions need to be verified for each case.

**Low confidence**: The practical implementation and computational aspects of learning with RKKS, as the paper focuses on theoretical foundations rather than algorithms.

## Next Checks
1. Verify Gaussian kernel on S1: Check that the Gaussian kernel on the circle S1 is smooth and has exponentially decaying derivatives at infinity, confirming it admits a positive decomposition.
2. Representer theorem for RKKS: Implement a simple learning problem using a known positive decomposable kernel on a non-Euclidean space and verify that the solution is a finite linear combination of kernel sections.
3. Positive decomposability on a sphere: Apply the theory to the 2-sphere S2, checking if the Gaussian kernel is invariant under SO(3) and satisfies the smoothness and decay conditions for positive decomposability.