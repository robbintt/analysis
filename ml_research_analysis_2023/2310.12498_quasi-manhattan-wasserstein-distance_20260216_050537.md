---
ver: rpa2
title: Quasi Manhattan Wasserstein Distance
arxiv_id: '2310.12498'
source_url: https://arxiv.org/abs/2310.12498
tags:
- distance
- wasserstein
- qmwd
- manhattan
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Quasi Manhattan Wasserstein Distance (QMWD),
  a metric designed to quantify the dissimilarity between two matrices by combining
  elements of the Wasserstein Distance with specific transformations. QMWD offers
  improved time and space complexity compared to the Manhattan Wasserstein Distance
  (MWD) while maintaining accuracy.
---

# Quasi Manhattan Wasserstein Distance

## Quick Facts
- arXiv ID: 2310.12498
- Source URL: https://arxiv.org/abs/2310.12498
- Reference count: 5
- Primary result: QMWD reduces computation time by ~80% for 30×30 matrices while maintaining <5% relative error compared to MWD

## Executive Summary
This paper introduces the Quasi Manhattan Wasserstein Distance (QMWD), a metric that approximates the dissimilarity between two matrices by decomposing the 2D optimal transport problem into three 1D problems. QMWD achieves significant computational efficiency improvements over the Manhattan Wasserstein Distance (MWD) while maintaining comparable accuracy. The method is particularly valuable for large datasets or scenarios with limited computational resources, offering O(mn) complexity versus MWD's O(m²n²) complexity.

## Method Summary
QMWD computes the Wasserstein distance between two matrices by vectorizing them, rotating one matrix by 90 degrees, and transposing one matrix, then calculating the Wasserstein distance for each transformation and taking the maximum. This approach avoids the expensive cost matrix computation required by MWD while capturing essential transport features through directional transformations. The method is specifically designed for matrices with equal sum of elements and positive integer values.

## Key Results
- QMWD reduces computation time by approximately 80% for 30×30 matrices compared to MWD
- Maintains relative error of less than 5% compared to MWD
- Achieves O(mn) time complexity versus MWD's O(m²n²) complexity
- Combines three 1D Wasserstein distance calculations through vectorization, rotation, and transposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QMWD achieves comparable accuracy to MWD while reducing computation time by ~80% for 30×30 matrices.
- Mechanism: QMWD approximates the 2D optimal transport problem by decomposing it into three 1D optimal transport problems (vectorized, rotated, transposed) and taking their maximum. This avoids the O(m²n²) cost matrix computation required by MWD.
- Core assumption: The maximum of the three 1D Wasserstein distances captures the essential dissimilarity between matrices as well as the full 2D transportation plan.
- Evidence anchors:
  - [abstract]: "Simulation results show that QMWD achieves comparable accuracy to MWD while significantly reducing computational time. Specifically, for matrices of size 30×30, QMWD reduces the computation time by approximately 80% compared to MWD, while maintaining a relative error of less than 5%."
  - [section]: "The dominant factor in the overall complexity of MWD is the computation of the cost matrix, resulting in a final time complexity of O(m²n²) + O(T). ... The overall time complexity of QMWD is dominated by the three WD calculations. Therefore, the final time complexity of QMWD is 3×(O(mn) + O(T)) = O(3mn) + O(3T) = O(mn) + O(T)."
  - [corpus]: Weak correlation with corpus papers on Wasserstein variants, suggesting limited external validation of this specific approximation claim.
- Break condition: If the maximum of three 1D distances fails to capture critical 2D transport features, accuracy degrades beyond the claimed <5% relative error.

### Mechanism 2
- Claim: QMWD maintains accuracy by incorporating transformations (rotation and transpose) that capture directional information lost in simple vectorization.
- Mechanism: By computing WD on vec(P), vec(R(P)), and vec(Pᵀ), QMWD captures horizontal, vertical, and cross-directional transport costs, compensating for the overpenalization of vertical divergences that occurs in standard 1D vectorization.
- Core assumption: The combination of these three directional views provides sufficient coverage of the 2D structure to approximate the full transportation plan.
- Evidence anchors:
  - [abstract]: "It offers improved time and space complexity compared to the Manhattan Wasserstein Distance (MWD) while maintaining accuracy."
  - [section]: "QMWD combines the WD between vectorized matrices, the WD between rotated matrices, and the WD between transposed matrices, each scaled by the dimensions n and m, and modified by the modulo operation."
  - [corpus]: No direct corpus support for this specific transformation-based accuracy preservation mechanism.
- Break condition: If matrix structures have complex patterns not captured by these three transformations, the approximation may fail to maintain accuracy.

### Mechanism 3
- Claim: The modulo operation in QMWD scaling prevents overflow and normalizes contributions from different dimensions.
- Mechanism: The terms WD/scale + WD mod scale ensure that both the average transport cost and the remainder contribute to the final metric, balancing dimensional effects.
- Core assumption: This dual-component scaling appropriately weights contributions from matrices of different aspect ratios.
- Evidence anchors:
  - [section]: "QMWD combines the WD between vectorized matrices, the WD between rotated matrices, and the WD between transposed matrices, each scaled by the dimensions n and m, and modified by the modulo operation."
  - [abstract]: No explicit mention of the modulo mechanism's role.
  - [corpus]: No corpus evidence supporting this specific scaling approach.
- Break condition: If the modulo operation creates artificial discontinuities or fails to properly normalize for certain matrix distributions, the metric may become unreliable.

## Foundational Learning

- Concept: Optimal Transport Theory and Wasserstein Distance
  - Why needed here: QMWD is fundamentally built on Wasserstein distance principles, and understanding how optimal transport works is essential to grasp why the approximation is valid.
  - Quick check question: What is the primary difference between 1-Wasserstein (Earth Mover's) and 2-Wasserstein distances in terms of their cost functions?

- Concept: Computational Complexity Analysis
  - Why needed here: The paper's main contribution is reducing O(m²n²) to O(mn) complexity, which requires understanding how to analyze nested loops and linear programming costs.
  - Quick check question: Given four nested loops each running m times, what is the time complexity, and how does it compare to three nested loops?

- Concept: Linear Programming and Transportation Problems
  - Why needed here: Both MWD and QMWD involve solving transportation problems, with MWD requiring explicit cost matrix construction and QMWD using pre-existing solvers.
  - Quick check question: In a transportation problem, what constraints must be satisfied for the solution to be valid (mass conservation)?

## Architecture Onboarding

- Component map: Matrix transformation -> Three parallel WD computations -> Maximum aggregation -> Output
- Critical path: Matrix transformation → Three parallel WD computations → Maximum aggregation → Output
- Design tradeoffs: QMWD trades the exact 2D transportation plan for computational efficiency; this is acceptable when only the final distance metric matters, not the transport details.
- Failure signatures: Accuracy degradation beyond 5% relative error, especially for matrices with complex structural patterns; computational time not improving as expected (suggesting solver inefficiencies).
- First 3 experiments:
  1. Verify the 80% time reduction claim by timing MWD vs QMWD on random 30×30 matrices with 20 repetitions
  2. Test accuracy preservation by computing relative errors across matrices with varying structural complexity (stripes, blocks, random)
  3. Evaluate robustness by testing matrices with different aspect ratios (2×30, 15×15, 30×2) to check dimensional scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QMWD perform compared to other approximate methods for computing Wasserstein distance on large-scale datasets?
- Basis in paper: [explicit] The paper compares QMWD only with MWD and WD, but does not evaluate its performance against other approximation methods.
- Why unresolved: The paper lacks comparative analysis with alternative approximation techniques like Sinkhorn distance or sliced Wasserstein distance.
- What evidence would resolve it: Empirical studies comparing QMWD's accuracy and computational efficiency against other approximation methods on large-scale datasets.

### Open Question 2
- Question: What is the impact of matrix sparsity on QMWD's performance and accuracy?
- Basis in paper: [inferred] The complexity analysis assumes dense matrices, but real-world data often exhibits sparsity.
- Why unresolved: The paper does not investigate how sparse matrices affect QMWD's computational complexity or accuracy.
- What evidence would resolve it: Empirical studies examining QMWD's performance on sparse matrices with varying levels of sparsity.

### Open Question 3
- Question: How does QMWD generalize to higher-dimensional tensors beyond 2D matrices?
- Basis in paper: [inferred] The paper only considers 2D matrices, but higher-dimensional data is common in applications like image processing and deep learning.
- Why unresolved: The paper does not explore extending QMWD to higher-dimensional tensors or discuss potential challenges.
- What evidence would resolve it: Mathematical formulation and empirical validation of QMWD for higher-dimensional tensors.

## Limitations
- Limited external validation beyond simulation results
- No analysis of edge cases like highly imbalanced distributions
- Modulo operation's effectiveness not empirically verified

## Confidence

**High Confidence**: The computational complexity reduction from O(m²n²) to O(mn) is mathematically sound and follows standard complexity analysis principles.

**Medium Confidence**: The ~80% time reduction and <5% relative error claims are supported by the authors' simulations but lack independent verification.

**Low Confidence**: The theoretical foundation for why the maximum of three 1D Wasserstein distances accurately approximates the 2D transport plan, and the role of the modulo operation in maintaining accuracy.

## Next Checks
1. Cross-validation on diverse datasets: Test QMWD against MWD on real-world matrices from domains like image processing, economics, and genomics to verify the <5% relative error claim holds across different data structures.
2. Solver dependency analysis: Implement QMWD using multiple linear programming solvers (e.g., simplex, interior-point) to confirm that computational time improvements are consistent and not solver-specific.
3. Edge case robustness testing: Evaluate QMWD performance on matrices with extreme aspect ratios (e.g., 2×100, 100×2), highly skewed distributions, and cases where transport plans have multiple optimal solutions to identify potential failure modes.