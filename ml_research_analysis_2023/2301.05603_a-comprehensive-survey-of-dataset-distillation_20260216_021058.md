---
ver: rpa2
title: A Comprehensive Survey of Dataset Distillation
arxiv_id: '2301.05603'
source_url: https://arxiv.org/abs/2301.05603
tags:
- dataset
- distillation
- data
- match
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews dataset distillation, a technique
  for synthesizing small, informative datasets from large original datasets to accelerate
  deep learning training. The paper categorizes existing methods into meta-learning
  and data matching frameworks, with the former treating distilled data as hyperparameters
  optimized via validation loss, and the latter matching gradients, trajectories,
  or distributions between synthetic and target data.
---

# A Comprehensive Survey of Dataset Distillation

## Quick Facts
- arXiv ID: 2301.05603
- Source URL: https://arxiv.org/abs/2301.05603
- Reference count: 21
- Key outcome: Dataset distillation synthesizes small, informative datasets that can train models with performance approaching that of the original large datasets

## Executive Summary
Dataset distillation is a powerful technique for accelerating deep learning training by synthesizing compact datasets that preserve the generalization performance of much larger original datasets. This survey comprehensively reviews over 15 algorithms across meta-learning and data matching frameworks, demonstrating that methods like KFS and RFAD can achieve high accuracy (e.g., 92%+ on CIFAR-10 with IPC=10) compared to traditional approaches. The paper explores applications in continual learning, neural architecture search, privacy preservation, federated learning, and adversarial robustness, while identifying key challenges including theoretical foundations, consistent knowledge definitions, and synthetic data form.

## Method Summary
The survey categorizes dataset distillation methods into two primary frameworks: meta-learning approaches that treat synthetic data as hyperparameters optimized via validation loss, and data matching approaches that align gradients, trajectories, or distributions between synthetic and target data. Disentangled dataset distillation improves performance by optimizing latent codes and decoders to generate synthetic data, reducing information redundancy. The methods are evaluated across multiple benchmark datasets including MNIST, CIFAR-10/100, SVHN, and Tiny ImageNet, with performance measured by classification accuracy when training models on distilled data compared to original datasets and baseline methods.

## Key Results
- Dataset distillation methods achieve 92%+ accuracy on CIFAR-10 with IPC=10, significantly outperforming random selection and coreset approaches
- Disentangled dataset distillation methods reduce information redundancy by optimizing latent codes and decoders rather than direct synthetic examples
- Kernel Ridge Regression approaches bypass computational inefficiency of back-propagation through time by using closed-form solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset distillation reduces training cost by synthesizing a small dataset that preserves the generalization performance of the original large dataset.
- Mechanism: By optimizing synthetic data points to match gradients, trajectories, or distributions of the original dataset during model training, the distilled data encodes essential information needed for effective model learning.
- Core assumption: The synthetic data, when used for training, induces similar parameter updates and generalization behavior as the original data.
- Evidence anchors:
  - [abstract] "Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community."
  - [section] "Different from coreset selection, dataset distillation removes the restriction of uneditable elements and carefully modifies a small number of examples to preserve more information, as shown in Figure 4 for synthetic examples."
  - [corpus] Weak evidence; no direct corpus citations support this mechanism.
- Break condition: If the synthetic data fails to capture the distribution or discriminative features of the original data, the distilled dataset will not preserve generalization performance.

### Mechanism 2
- Claim: Disentangled dataset distillation improves performance by learning latent codes and decoders to generate synthetic data, reducing information redundancy.
- Mechanism: By optimizing both the latent codes and corresponding decoders, the synthetic data generation process captures intra-class information and allows for more compact representation compared to directly optimizing synthetic examples.
- Core assumption: The disentangled representation can encode the essential information of the original dataset more efficiently than non-disentangled approaches.
- Evidence anchors:
  - [abstract] "Disentangled dataset distillation further improves performance by optimizing latent codes and decoders to generate synthetic data."
  - [section] "Through generating synthetic datasets with the combination of codes and decoders, the compression ratio can be further decreased, and information redundancy in distilled images is also reduced."
  - [corpus] Weak evidence; no direct corpus citations support this mechanism.
- Break condition: If the latent space does not adequately represent the original data's variability or if the decoder fails to reconstruct meaningful samples, the performance gain will not materialize.

### Mechanism 3
- Claim: Kernel Ridge Regression (KRR) approach in dataset distillation bypasses the computational inefficiency of back-propagation through time by using a closed-form solution.
- Mechanism: By replacing neural networks with a kernel model in the inner loop, the synthetic data can be updated via meta-gradient without unrolling the recursive computation graph, significantly improving efficiency.
- Core assumption: The kernel model's training dynamic is equivalent to that of infinite-width neural networks, ensuring that the distilled data remains effective when transferred to neural network training.
- Evidence anchors:
  - [abstract] "Considering the existence of closed-form solutions in kernel regression regime, Nguyen et al. [2020] replace the neural network in the inner loop with a kernel model, which bypasses the recursive back-propagation of meta-gradient."
  - [section] "Due to the closed-form solution in KRR, Î¸ does not require an iterative update and the backward pass of gradient thus bypasses the recursive computation graph, as shown in Figure 5(b)."
  - [corpus] Weak evidence; no direct corpus citations support this mechanism.
- Break condition: If the kernel model fails to capture the complex patterns in the data or if the equivalence to neural networks does not hold for finite-width networks, the distilled data may not transfer well.

## Foundational Learning

- Concept: Backpropagation Through Time (BPTT)
  - Why needed here: Understanding BPTT is crucial for grasping the computational challenges in meta-learning framework dataset distillation methods.
  - Quick check question: What is the primary computational drawback of using BPTT in dataset distillation?

- Concept: Kernel Ridge Regression (KRR)
  - Why needed here: KRR provides a computationally efficient alternative to BPTT by offering a closed-form solution for updating synthetic data.
  - Quick check question: How does KRR bypass the need for iterative parameter updates in dataset distillation?

- Concept: Gradient Matching
  - Why needed here: Gradient matching is a key strategy in data match framework for aligning the training dynamics of synthetic and original datasets.
  - Quick check question: What is the main objective of gradient matching in dataset distillation?

## Architecture Onboarding

- Component map:
  - Meta-learning framework: Outer loop (optimizing synthetic data) and inner loop (training model on synthetic data)
  - Data match framework: Gradient match, trajectory match, and distribution match strategies
  - Disentangled dataset distillation: Latent codes and decoders for synthetic data generation
  - Kernel Ridge Regression: Kernel model replacement for efficiency

- Critical path:
  1. Define the target dataset and distillation budget (IPC)
  2. Choose a distillation framework (meta-learning or data match)
  3. Implement the chosen framework with appropriate algorithms
  4. Optimize synthetic data to match the target dataset's performance
  5. Evaluate the distilled dataset's effectiveness on downstream tasks

- Design tradeoffs:
  - Computational efficiency vs. performance: KRR is faster but may underperform BPTT
  - Direct optimization vs. disentangled representation: Disentangled methods are more efficient but require additional design choices
  - Matching granularity: Gradient match is simpler but may miss long-term dynamics captured by trajectory match

- Failure signatures:
  - Performance degradation when using distilled dataset compared to original
  - Overfitting to the synthetic data leading to poor generalization
  - Computational inefficiency in the distillation process

- First 3 experiments:
  1. Implement a basic meta-learning framework with BPTT on MNIST to verify the core concept
  2. Compare KRR-based distillation with BPTT on CIFAR-10 to assess efficiency gains
  3. Apply gradient matching on SVHN to evaluate its effectiveness against random selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we theoretically characterize the relationship between the distillation budget (IPC) and the generalization error of models trained on distilled datasets?
- Basis in paper: [explicit] The paper states "the theoretical correlation between the distillation budget (the size of synthetic datasets) and performance can provide an upper bound on the test error" and notes this as an important area for future work.
- Why unresolved: Current dataset distillation methods lack rigorous theoretical foundations, and there are no established theoretical bounds connecting the size of synthetic datasets to generalization performance.
- What evidence would resolve it: Mathematical proofs or rigorous theoretical analysis showing how generalization error scales with IPC, and empirical validation across diverse datasets and model architectures.

### Open Question 2
- Question: What is the most effective pre-processing strategy for target datasets that improves dataset distillation performance?
- Basis in paper: [explicit] The paper identifies "elaborate pre-processing" of target datasets as a promising direction and mentions that ZCA whitening has shown some empirical success but lacks comprehensive analysis.
- Why unresolved: The paper notes that "only a few works discuss the influence of pre-processing on final distillation performance" and suggests that more investigation is needed.
- What evidence would resolve it: Systematic empirical studies comparing various pre-processing techniques (normalization, whitening, augmentation, etc.) across multiple distillation algorithms and datasets, showing consistent improvements.

### Open Question 3
- Question: How can we design effective initialization augmentation strategies that improve the robustness of distilled datasets to different model architectures and parameter initializations?
- Basis in paper: [explicit] The paper identifies "initialisation augmentation" as a promising direction and notes that synthetic datasets are "sensitive to the parameter initialisation and network architecture adopted in the distillation process."
- Why unresolved: Current approaches rely on "trivial repeated distillation w.r.t. different initialisations" which is inefficient and lacks sophisticated strategies for generating diverse initializations.
- What evidence would resolve it: Development and empirical validation of initialization augmentation techniques (such as early-stage models or weight perturbation methods) that consistently improve cross-architecture generalization without requiring excessive computational resources.

## Limitations
- The survey relies on reported results from various papers without independent reproduction of all methods
- Significant variations in experimental setups across studies make direct comparisons difficult
- Theoretical foundations of dataset distillation remain underdeveloped with most methods relying on empirical validation

## Confidence
- High confidence: The categorization of dataset distillation methods into meta-learning and data matching frameworks, as well as the general effectiveness of distillation approaches compared to random selection or coreset methods.
- Medium confidence: Specific performance numbers across different datasets and methods, given the acknowledged variations in experimental conditions.
- Low confidence: Theoretical claims about why dataset distillation works, as the survey notes the field lacks comprehensive theoretical understanding.

## Next Checks
1. Reproduce the KFS method on CIFAR-10 with IPC=10 using the exact experimental protocol specified in the original paper to verify the reported ~92% accuracy.
2. Implement a controlled comparison between meta-learning and data matching frameworks on SVHN using identical network architectures and preprocessing to isolate the impact of each approach.
3. Evaluate the transfer performance of distilled datasets when applied to a different architecture than the one used during distillation to assess the generalization capability of the synthetic data.