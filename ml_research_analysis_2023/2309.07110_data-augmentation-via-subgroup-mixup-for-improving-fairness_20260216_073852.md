---
ver: rpa2
title: Data Augmentation via Subgroup Mixup for Improving Fairness
arxiv_id: '2309.07110'
source_url: https://arxiv.org/abs/2309.07110
tags:
- mixup
- data
- fairness
- samples
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel data augmentation technique, Subgroup
  Mixup, to improve group fairness in machine learning models. The method involves
  interpolating between samples from different subgroups (defined by class and group
  membership) to generate synthetic data that encourages fair and accurate decision
  boundaries.
---

# Data Augmentation via Subgroup Mixup for Improving Fairness

## Quick Facts
- arXiv ID: 2309.07110
- Source URL: https://arxiv.org/abs/2309.07110
- Reference count: 0
- Primary result: Novel Subgroup Mixup data augmentation improves fairness while maintaining or improving accuracy, especially for underrepresented subgroups.

## Executive Summary
This paper introduces Subgroup Mixup, a data augmentation technique designed to improve group fairness in machine learning models. The method generates synthetic data by interpolating between samples from different subgroups (defined by class and group membership), encouraging fair and accurate decision boundaries. The approach is adaptable to various bias scenarios including unbalanced subgroups and subgroup distribution shifts. Experiments on synthetic data and a real-world benchmark demonstrate that Subgroup Mixup achieves higher fairness than existing methods while maintaining or improving accuracy.

## Method Summary
Subgroup Mixup is a data augmentation technique that improves group fairness by interpolating between samples from different subgroups. Given labeled data with features, binary labels, and binary protected attributes, the method selects source and target subgroups based on imbalance or distribution shifts, then generates synthetic samples via pairwise linear mixup using λ sampled from a Beta distribution. The augmented dataset is used to train classifiers. The approach addresses bias from unbalanced subgroups, unbalanced classes, and underrepresented subgroups by strategically choosing which subgroups to mix.

## Key Results
- Subgroup Mixup achieves higher fairness (lower DP gap) than existing methods on synthetic simulations and real-world datasets
- The method maintains or improves accuracy while improving fairness, particularly for underrepresented subgroups
- Different choices of source and target subgroups effectively address specific types of bias scenarios

## Why This Works (Mechanism)

### Mechanism 1
Mixing samples from underrepresented subgroups with target subgroups improves fairness by encouraging smoother decision boundaries between groups. Interpolation generates synthetic data that fills gaps in feature space, leading classifiers to treat groups more similarly.

### Mechanism 2
Choosing underrepresented subgroups as sources and balancing target subgroups improves fairness by directly addressing subgroup imbalance. Sampling from the smallest subgroup and interpolating with nearest neighbors from target subgroups oversamples the underrepresented group.

### Mechanism 3
Mixing within the same class but across groups reduces bias from subgroup distribution shifts by encouraging consistent predictions across groups. When distribution shifts occur, interpolating samples from different groups within the same class promotes model invariance.

## Foundational Learning

- **Group fairness definitions (e.g., Demographic Parity)**: The method explicitly aims to achieve demographic parity by equalizing prediction distributions across groups. Quick check: What is the mathematical condition for demographic parity to be satisfied?

- **Mixup data augmentation**: The core of the method is interpolating between samples to generate synthetic data that encourages fair decision boundaries. Quick check: How does mixup differ from traditional data augmentation methods like SMOTE?

- **Subgroup imbalance and distribution shifts**: The method is designed to address both unbalanced subgroup proportions and distribution shifts between subgroups. Quick check: How do unbalanced subgroups and distribution shifts contribute to bias in machine learning models?

## Architecture Onboarding

- **Component map**: Load dataset → Partition into subgroups (Y, Z) → Choose source/target subgroups → Generate synthetic samples via mixup → Train classifier on augmented dataset → Evaluate fairness and accuracy

- **Critical path**: 
  1. Load dataset and identify subgroups
  2. Choose source and target subgroups based on imbalance or distribution shifts
  3. Generate synthetic samples via mixup
  4. Train classifier on augmented dataset
  5. Evaluate fairness and accuracy

- **Design tradeoffs**: 
  - Balancing subgroups vs. maintaining original data distribution
  - Interpolation strength (α parameter) vs. overfitting to synthetic data
  - Computational cost of nearest neighbor search vs. quality of generated samples

- **Failure signatures**:
  - High DP gap despite augmentation: Subgroups may be too far apart in feature space for interpolation to be effective
  - Decreased accuracy: Overfitting to synthetic data or inappropriate choice of source/target subgroups
  - No improvement in fairness: Underlying bias may be too strong or not addressable through interpolation

- **First 3 experiments**:
  1. Synthetic data with known subgroup imbalance: Verify that the method can balance subgroup proportions and improve fairness
  2. Real-world dataset with known bias: Evaluate the method's effectiveness on a benchmark fairness dataset
  3. Ablation study: Test the impact of different source/target subgroup choices on fairness and accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several areas remain unexplored:
- How does the choice of mixup parameter α affect the balance between fairness improvement and accuracy maintenance across different bias scenarios?
- How does Subgroup Mixup perform on non-binary group fairness definitions beyond Demographic Parity?
- What is the theoretical relationship between the neighborhood size K and the trade-off between fairness improvement and potential overfitting?

## Limitations
- The method is currently limited to binary protected attributes and labels, restricting its applicability to more complex fairness scenarios
- The effectiveness of interpolation relies on subgroups being sufficiently close in feature space, which may not hold in high-dimensional or highly complex datasets
- The choice of mixup parameter α requires empirical tuning and may not generalize across different datasets or bias scenarios

## Confidence
- **High**: The method's ability to improve fairness on binary classification tasks with binary protected attributes, as demonstrated by experiments on synthetic data and a real-world dataset
- **Medium**: The method's effectiveness in addressing subgroup imbalance and distribution shifts, as the choice of source and target subgroups is based on reasonable assumptions but may not always lead to optimal results
- **Low**: The method's applicability to non-binary protected attributes and multi-class labels, as the current formulation is limited to binary Y and Z

## Next Checks
1. Evaluate the method's performance on a dataset with multi-class labels and non-binary protected attributes to assess its generalizability
2. Conduct a sensitivity analysis on the choice of mixup parameter α to determine its impact on fairness and accuracy
3. Investigate the method's effectiveness on high-dimensional datasets to assess its robustness to complex feature spaces