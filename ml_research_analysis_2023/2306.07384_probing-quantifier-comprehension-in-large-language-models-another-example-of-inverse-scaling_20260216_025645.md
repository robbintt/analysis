---
ver: rpa2
title: 'Probing Quantifier Comprehension in Large Language Models: Another Example
  of Inverse Scaling'
arxiv_id: '2306.07384'
source_url: https://arxiv.org/abs/2306.07384
tags:
- quantifier
- word
- llms
- language
- quantifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how well large language models (LLMs) understand
  quantifiers like "most" and "few". Previous work suggested LLMs exhibit inverse
  scaling - getting worse at few-type quantifiers as they grow larger.
---

# Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling

## Quick Facts
- arXiv ID: 2306.07384
- Source URL: https://arxiv.org/abs/2306.07384
- Reference count: 4
- Large language models improve at distinguishing few-type from most-type quantifiers as they scale, but show inverse scaling for most-type quantifier comprehension

## Executive Summary
This paper investigates how well large language models understand quantifiers like "most" and "few". The authors argue that previous work on inverse scaling in quantifier comprehension resulted from inappropriate testing methodology that measured typicality rather than actual quantifier understanding. They propose new evaluation methods that keep critical words constant while varying quantifiers, finding that LLMs do improve at distinguishing quantifier types as they scale. However, when evaluating whether models incorporate quantifier meaning, they find that few-type quantifier comprehension improves with model size but most-type quantifier comprehension exhibits inverse scaling. The authors conclude that LLMs still struggle with quantifier understanding, suggesting statistical word co-occurrence dominates over actual meaning.

## Method Summary
The authors evaluate quantifier comprehension by calculating surprisal (negative log-probability) of critical words in contexts with different quantifiers. They use two evaluation approaches: Experiment-1 compares surprisal across quantifier types for fixed critical words, and Experiment-2 measures how adding quantifiers affects surprisal compared to no quantifier baseline. They test this across multiple LLM families (GPT2, GPT-Neo, OPT, GPT-3, GPT3.5) ranging from 125M to 175B parameters using a dataset of 960 sentences derived from psycholinguistic studies.

## Key Results
- LLMs improve at distinguishing between few-type and most-type quantifiers as model size increases
- Few-type quantifier comprehension improves with model size
- Most-type quantifier comprehension exhibits inverse scaling, getting worse as models grow larger
- The original evaluation method by Michaelov and Bergen (2022) inadvertently measured typicality rather than quantifier comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evaluation method in Michaelov and Bergen (2022) inadvertently measured typicality rather than quantifier comprehension.
- Mechanism: By keeping both the quantifier and critical word constant while varying model size, the method effectively measured how well models distinguish typical from atypical words, not how they interpret quantifier meaning.
- Core assumption: The typical/atypical word pairs used in the dataset are not uniquely determined by the quantifier meaning but reflect human judgments of typicality.
- Evidence anchors:
  - [abstract] "method used to measure quantifier comprehension in LLMs by (Michaelov and Bergen, 2022) was inadvertently measuring something else"
  - [section 3.2.1] "This shows that the typical token for humans is not necessarily typical for language models"
  - [corpus] Weak evidence - no corpus neighbors directly support this mechanism
- Break condition: If typical/atypical word pairs were uniquely determined by quantifier meaning rather than general typicality, this mechanism would break.

### Mechanism 2
- Claim: LLMs improve at distinguishing between most-type and few-type quantifiers as they scale.
- Mechanism: Larger models develop better statistical representations that capture the distributional differences between contexts where different quantifiers typically appear.
- Core assumption: The distributional patterns of words following most-type versus few-type quantifiers become more distinct in larger models' learned representations.
- Evidence anchors:
  - [abstract] "LLMs are able to better understand the difference between the meaning of few-type and most-type quantifiers as their size increases"
  - [section 4.1] "It shows that LLMs get increasingly better at understanding the meaning of quantifier"
  - [corpus] Weak evidence - corpus neighbors don't directly address this mechanism
- Break condition: If quantifier meaning were primarily compositional rather than distributional, scaling wouldn't improve comprehension.

### Mechanism 3
- Claim: Most-type quantifier comprehension exhibits inverse scaling while few-type quantifier comprehension improves with scale.
- Mechanism: The distributional patterns for few-type quantifiers may be more robust to model scaling, while those for most-type quantifiers may become increasingly dominated by statistical co-occurrence patterns that override semantic meaning.
- Core assumption: Different quantifier types have different distributional properties that interact differently with model scaling effects.
- Evidence anchors:
  - [abstract] "we also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments"
  - [section 4.2] "we find that the models get increasingly worse at taking into account quantifier meaning, thus showing an inverse-scaling in most-type quantifier comprehension"
  - [corpus] Weak evidence - no corpus neighbors directly support this mechanism
- Break condition: If both quantifier types were equally affected by scaling, or if neither showed inverse scaling, this mechanism would break.

## Foundational Learning

- Concept: Surprisal and its relationship to probability
  - Why needed here: The paper uses surprisal (negative log-probability) as the primary metric for evaluating model behavior with quantifiers
  - Quick check question: If a word has probability 0.1 in a context, what is its surprisal value?

- Concept: Inverse scaling laws
  - Why needed here: The paper discusses how model performance can decrease as model size increases for certain tasks
  - Quick check question: If performance metric X decreases from 0.7 to 0.5 as model size increases from 1B to 10B parameters, is this inverse scaling?

- Concept: Subword tokenization and its effects on probability distributions
  - Why needed here: The paper discusses how critical words being split into multiple subwords affects probability calculations
  - Quick check question: If a critical word is split into 3 subwords, how does this affect its overall probability compared to a word split into 1 subword?

## Architecture Onboarding

- Component map: Dataset -> Quantifier Application -> Model Inference -> Critical Word Probability -> Surprisal Calculation -> Accuracy Evaluation -> Scaling Analysis
- Critical path: Backbone phrase → Quantifier application → Model inference → Critical word probability → Surprisal calculation → Accuracy evaluation → Scaling analysis
- Design tradeoffs:
  - Using fixed critical words versus variable critical words affects whether the evaluation measures typicality or quantifier comprehension
  - Normalizing surprisal by subword length versus not affects comparability across different critical words
  - Measuring absolute surprisal differences versus relative changes affects sensitivity to context
- Failure signatures:
  - If accuracy plateaus below 50%, the model isn't capturing quantifier meaning at all
  - If accuracy decreases with scale for both quantifier types, the methodology may be flawed
  - If results vary dramatically across different model families, there may be implementation issues
- First 3 experiments:
  1. Verify that typical words have lower surprisal than atypical words without quantifiers present
  2. Test if adding quantifiers changes surprisal in the expected direction (most-type lowers surprisal for typical words, few-type raises it)
  3. Compare scaling patterns between most-type and few-type quantifiers to confirm inverse scaling for most-type quantifiers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models truly understand quantifiers or are they merely recognizing statistical patterns in word co-occurrence?
- Basis in paper: [explicit] The paper discusses how language models struggle with quantifier comprehension and suggests that statistical co-occurrence of words might dominate over actual word meaning.
- Why unresolved: The paper demonstrates that language models show inverse scaling in most-type quantifier comprehension and only moderate accuracy in understanding both quantifier types. However, it does not definitively prove whether this is due to lack of true understanding or limitations in the evaluation methodology.
- What evidence would resolve it: A study comparing language model behavior on quantifiers to human comprehension, using methods that control for statistical patterns, could help determine if language models are truly understanding quantifiers or just recognizing patterns.

### Open Question 2
- Question: Is quantifier comprehension necessary for effective language understanding in large language models?
- Basis in paper: [explicit] The paper suggests that despite language models' lack of quantifier comprehension, they still improve across various benchmarks, implying that quantifier comprehension might not be as crucial for language understanding as previously thought.
- Why unresolved: The paper raises this question but does not provide a definitive answer, leaving open the possibility that quantifier comprehension might be important for certain tasks or contexts that were not tested.
- What evidence would resolve it: A comprehensive study examining the performance of language models on a wide range of language tasks with and without explicit quantifier comprehension capabilities would help determine the necessity of quantifier understanding for effective language processing.

### Open Question 3
- Question: What are the underlying reasons for the inverse scaling law observed in most-type quantifier comprehension?
- Basis in paper: [explicit] The paper observes that most-type quantifier comprehension gets worse as the model size increases, contrary to human psycho-linguistic experiments and previous work.
- Why unresolved: The paper does not provide a clear explanation for this phenomenon, leaving open questions about whether it is due to model architecture, training data, or other factors.
- What evidence would resolve it: A detailed analysis of the training data, model architecture, and internal representations of quantifiers in large language models could help identify the root causes of the inverse scaling law in most-type quantifier comprehension.

## Limitations
- The evaluation methodology still relies on human-selected typical/atypical word pairs, which may introduce confounds
- Surprisal-based evaluation may not fully capture the complex semantic understanding required for quantifier comprehension
- The relationship between typicality and quantifier meaning remains unclear and may affect results

## Confidence
- **High confidence**: The finding that the original evaluation method measured typicality rather than quantifier comprehension
- **Medium confidence**: The observation of inverse scaling for most-type quantifier comprehension
- **Medium confidence**: The conclusion that LLMs struggle with quantifier understanding

## Next Checks
1. **Cross-linguistic validation**: Test the evaluation methodology with quantifiers from languages other than English to determine if the observed patterns are language-specific or reflect more general limitations in LLM comprehension.
2. **Alternative evaluation metrics**: Implement semantic similarity-based evaluation alongside surprisal measures to determine if the inverse scaling pattern persists when using different assessment approaches.
3. **Fine-tuning intervention**: Conduct experiments where models are fine-tuned on quantifier-typicality pairs to determine if the inverse scaling pattern can be mitigated through targeted training.