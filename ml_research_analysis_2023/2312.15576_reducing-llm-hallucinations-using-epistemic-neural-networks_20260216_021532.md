---
ver: rpa2
title: Reducing LLM Hallucinations using Epistemic Neural Networks
arxiv_id: '2312.15576'
source_url: https://arxiv.org/abs/2312.15576
tags:
- training
- dataset
- epistemic
- neural
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes integrating epistemic neural networks (ENNs)
  into pre-trained large language models to reduce hallucinations in text generation.
  The authors train an ENN on top of a frozen Llama-2 7B model, using its latent embeddings
  as features and contrastive decoding to improve factualness.
---

# Reducing LLM Hallucinations using Epistemic Neural Networks

## Quick Facts
- arXiv ID: 2312.15576
- Source URL: https://arxiv.org/abs/2312.15576
- Reference count: 8
- Key outcome: ENN-DoLa combination degrades TruthfulQA performance to 46.8% MC1 accuracy vs DoLa baseline of 50.5%

## Executive Summary
This paper proposes integrating epistemic neural networks (ENNs) into pre-trained large language models to reduce hallucinations in text generation. The authors train an ENN on top of a frozen Llama-2 7B model using contrastive decoding (DoLa) to improve factualness. However, evaluation on the TruthfulQA benchmark shows that incorporating the ENN leads to reduced performance compared to the baseline DoLa method, likely due to overfitting on the limited C4 training data. While the authors successfully demonstrate that ENNs can be trained for next-token prediction, their application for hallucination reduction requires further development, particularly in selecting a more diverse training dataset.

## Method Summary
The method integrates epistemic neural networks (ENNs) with a frozen Llama-2 7B model using contrastive decoding (DoLa). The ENN takes latent embeddings from the LLM and produces an additive correction to output logits. It consists of a prior network (randomized, frozen) and a learnable network trained on next-token prediction using the C4 dataset. The ENN processes concatenated mature and premature hidden features plus epistemic samples, producing logits that are combined with DoLa's contrastive logits before the final softmax. The combined approach attempts to leverage both epistemic uncertainty modeling and layer-wise semantic emphasis.

## Key Results
- ENN-DoLa achieves 46.8% accuracy on TruthfulQA MC1 task vs DoLa baseline of 50.5%
- ENN training converges on C4 dataset but shows signs of overfitting
- ENN successfully trains for next-token prediction despite performance degradation
- Combination of ENN with DoLa requires further development for hallucination reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epistemic neural networks (ENNs) reduce LLM hallucinations by modeling epistemic uncertainty through joint prediction distributions.
- Mechanism: ENNs take latent embeddings from a frozen LLM and use a prior network (randomized, frozen) and a learnable network to produce an additive correction to the LLM's output logits. The learnable network is trained to minimize cross-entropy loss on next-token prediction, while the prior network provides epistemic variation via sampled indices.
- Core assumption: Epistemic uncertainty in LLM outputs can be captured by perturbing the latent representation space and learning a correction function that improves joint probability calibration.
- Evidence anchors:
  - [abstract]: "Epistemic neural networks have recently been proposed to improve output joint distributions for large pre-trained models."
  - [section]: "Epistemic neural networks are small networks attached to large, frozen models to improve the model's joint distributions and uncertainty estimates."
  - [corpus]: No direct corpus evidence of ENN reducing LLM hallucinations; related papers focus on epistemic uncertainty quantification but not hallucination reduction.
- Break condition: If the learned correction function overfits to the training corpus, the ENN will not generalize and may degrade performance, as observed in the TruthfulQA experiments.

### Mechanism 2
- Claim: Contrastive decoding (DoLa) improves factualness by emphasizing later transformer layers that encode higher-level semantic information while downplaying earlier layers.
- Mechanism: DoLa selects a "premature" layer dynamically by maximizing Jensen-Shannon divergence with the final layer's output. Logits from the final layer are amplified while the premature layer's logits are suppressed using a contrastive ratio.
- Core assumption: Factual knowledge is localized to later transformer layers, and amplifying these while suppressing earlier layers increases the factualness of generated text.
- Evidence anchors:
  - [section]: "DoLa... proposes to exploit the fact that transformer LMs have been loosely shown to encode 'lower-level' information... in the earlier layers and more 'semantic information' in the later layers."
  - [section]: "The difference in logits obtained from a higher layer versus a lower layer would emphasize on the knowledge from higher layers and at the same time downplay the lower or intermediate layer knowledge, potentially making LMs more factual."
  - [corpus]: No corpus evidence specifically for DoLa reducing hallucinations; related papers discuss uncertainty quantification but not contrastive layer decoding.
- Break condition: If the optimal premature layer changes dynamically across inputs, static or mis-specified layer selection will degrade performance.

### Mechanism 3
- Claim: Combining ENN correction with DoLa contrastive decoding yields improved joint probability estimates that reduce hallucination.
- Mechanism: The ENN processes concatenated mature and premature hidden features plus epistemic samples, producing logits that are added to DoLa's contrastive logits before the final softmax. This combination attempts to leverage both epistemic uncertainty modeling and layer-wise semantic emphasis.
- Core assumption: The combination of epistemic uncertainty modeling and contrastive layer emphasis is synergistic and leads to better calibrated outputs.
- Evidence anchors:
  - [section]: "We attempt to combine the advantages of DoLa and Epinet into one single unified framework."
  - [section]: "The combined output of the prior and learnable networks goes through Llama2's vocab head and is added to the DoLa logits before the softmax function."
  - [corpus]: No corpus evidence of this combined approach reducing hallucinations; the paper's own results show degradation in TruthfulQA performance.
- Break condition: If either component (ENN or DoLa) degrades output quality or introduces conflicting gradients, the combined approach will perform worse than either alone.

## Foundational Learning

- Concept: Joint probability distributions and epistemic uncertainty
  - Why needed here: ENNs are designed to improve joint probability estimates and capture epistemic uncertainty in LLM outputs.
  - Quick check question: How does modeling joint distributions differ from marginal distributions in terms of uncertainty quantification?

- Concept: Transformer layer semantics and knowledge localization
  - Why needed here: DoLa relies on the assumption that different transformer layers encode different types of information (lower-level vs semantic).
  - Quick check question: What evidence supports the claim that factual knowledge is localized to later transformer layers?

- Concept: Contrastive decoding and logit manipulation
  - Why needed here: DoLa's hallucination reduction relies on amplifying/de-emphasizing logits from different layers.
  - Quick check question: How does the contrastive ratio between mature and premature layer logits affect the final token distribution?

## Architecture Onboarding

- Component map: Input → Llama-2 → DoLa features + mature layer → ENN (prior+learnable) → combined logits → vocab head → output
- Critical path: Input → Llama-2 → DoLa features + mature layer → ENN (prior+learnable) → combined logits → vocab head → output
- Design tradeoffs:
  - Training ENN on limited C4 data vs. full Llama-2 pretraining data
  - Small ENN capacity (22M params) vs. potential underfitting
  - JAX-based ENN vs. PyTorch DoLa requires online tensor conversion
- Failure signatures:
  - ENN training loss plateaus above 0.1 → underfitting or poor feature representation
  - ENN training loss decreases but TruthfulQA performance degrades → overfitting to C4 data
  - Dynamic premature layer selection unstable → inconsistent DoLa gains across inputs
- First 3 experiments:
  1. Train ENN on random toy dataset (100 samples) to verify convergence and hyperparameter sensitivity
  2. Train ENN on C4 subset (5K tokens) with varying epistemic sample counts and learning rates
  3. Evaluate ENN checkpoints on TruthfulQA to identify overfitting and performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can epistemic neural networks be effectively trained for next token prediction in large language models?
- Basis in paper: [explicit] The paper states that training ENNs for next token prediction has never been attempted before in prior literature.
- Why unresolved: The authors observed overfitting issues when training ENNs on limited C4 dataset, suggesting challenges in data selection and training methodology.
- What evidence would resolve it: Successful training of ENNs on diverse, large-scale datasets with improved performance on downstream tasks like TruthfulQA.

### Open Question 2
- Question: What is the optimal combination strategy for integrating ENN logits with base model logits to reduce hallucinations?
- Basis in paper: [inferred] The authors experimented with different strategies for combining ENN and DoLa logits but found that applying softmax on ENN logits increased training epochs.
- Why unresolved: The paper does not provide a definitive optimal strategy for logit combination that effectively reduces hallucinations.
- What evidence would resolve it: Comparative analysis of various logit combination methods showing clear performance improvements on hallucination benchmarks.

### Open Question 3
- Question: How does the integration of ENNs during the pre-training stage of LLMs affect uncertainty estimation and hallucination reduction?
- Basis in paper: [explicit] The authors suggest future work to incorporate ENN architecture during LLM pre-training to improve uncertainty estimation.
- Why unresolved: This approach has not been explored or validated in the paper, leaving its potential benefits untested.
- What evidence would resolve it: Experimental results comparing pre-trained LLMs with and without integrated ENNs on uncertainty estimation metrics and hallucination benchmarks.

## Limitations
- Significant performance degradation observed on TruthfulQA benchmark (ENN-DoLa: 46.8% vs DoLa baseline: 50.5%)
- ENN training likely overfitting to limited C4 dataset (67K tokens) rather than learning generalizable patterns
- Insufficient architectural detail about ENN implementation, particularly prior network structure and hyperparameters

## Confidence

- **Low confidence** in the core claim that ENNs reduce LLM hallucinations: The experimental results directly contradict this, showing worse performance than the baseline DoLa method.
- **Medium confidence** in the technical feasibility of training ENNs for next-token prediction: The paper demonstrates that ENN training converges on C4 data, though with potential overfitting concerns.
- **Low confidence** in the proposed combination of ENN with DoLa: While technically implemented, the synergistic benefits are not demonstrated, and the combination appears to degrade performance.

## Next Checks

1. **Dataset diversity validation**: Evaluate the trained ENN on multiple datasets beyond C4 (e.g., WikiText, RealNews) to determine if performance degradation is specific to C4's domain or indicates fundamental overfitting issues.

2. **Architecture ablation study**: Train simplified versions of the ENN (removing the prior network, reducing latent space dimensions) to identify which components contribute to overfitting and performance degradation.

3. **Uncertainty calibration testing**: Measure whether the ENN actually produces better-calibrated uncertainty estimates using metrics like expected calibration error (ECE) or negative log-likelihood, independent of TruthfulQA performance.