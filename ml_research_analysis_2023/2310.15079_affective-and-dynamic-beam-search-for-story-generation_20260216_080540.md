---
ver: rpa2
title: Affective and Dynamic Beam Search for Story Generation
arxiv_id: '2310.15079'
source_url: https://arxiv.org/abs/2310.15079
tags:
- story
- affgen
- stories
- beam
- interesting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of generating interesting stories.
  The authors propose AFFGEN, a language model that uses a novel contextual bandit-based
  decoding mechanism.
---

# Affective and Dynamic Beam Search for Story Generation

## Quick Facts
- arXiv ID: 2310.15079
- Source URL: https://arxiv.org/abs/2310.15079
- Authors: 
- Reference count: 29
- Key outcome: AFFGEN uses contextual bandit-based decoding with dynamic beam sizing and affective reranking to generate more interesting and emotionally engaging stories while maintaining coherence.

## Executive Summary
This paper introduces AFFGEN, a language model for story generation that balances coherence and interestingness through a novel contextual bandit-based decoding mechanism. The model dynamically explores different beam sizes during generation and re-ranks candidates based on affective quality. Automatic and human evaluations demonstrate that AFFGEN generates stories that are both more interesting and maintain better coherence than baseline approaches.

## Method Summary
AFFGEN fine-tunes GPT-2 or GPT-3 on the ROCStories dataset and employs a two-phase decoding strategy. First, it samples the position for an intriguing twist using a learned distribution based on sentence embedding distances. For sentences before the twist, it uses standard decoding. For the twist sentence, it applies Dynamic Beam Sizing using a contextual bandit (LinUCB) to select beam sizes based on features like arousal score, event trigger likelihood, sequence length, and perplexity. Finally, it applies Affective Reranking to select candidates based on arousal scores and valence contrast with preceding sentences.

## Key Results
- AFFGEN-3 achieved the highest human evaluation scores for emotional engagement (2.74), empathy (2.64), and interestingness (2.73) compared to GPT-3 baseline
- The model maintained strong coherence scores (UNION 0.482, RUBER 0.508) while improving affective quality (Arousal 1.23)
- Ablation studies confirmed both Dynamic Beam Sizing and Affective Reranking contributed to improved story quality

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Beam Sizing
- Claim: Enables balance between coherence and interestingness by exploring different beam sizes
- Mechanism: Uses contextual bandit (LinUCB) to dynamically select beam sizes based on arousal score, event trigger likelihood, sequence length, and perplexity
- Core assumption: Tradeoff exists between coherence (small beams) and interestingness (large beams), optimal balance depends on context
- Evidence anchors: Abstract mentions "Dynamic Beam Sizing encourages less predictable, more captivating word choices"; section 4.3 discusses balancing coherence and interestingness through beam size variation
- Break condition: If contextual features fail to capture what makes a word "interesting," bandit may not select useful beam sizes

### Mechanism 2: Affective Reranking
- Claim: Prioritizes sentence candidates based on affect intensity to increase emotional engagement
- Mechanism: Reranks candidates using combination of arousal score and affective contrast (valence difference between candidate and preceding story)
- Core assumption: Higher affective contrast and arousal produce more interesting, emotionally engaging stories
- Evidence anchors: Abstract states "Affective Reranking prioritizes sentence candidates based on affect intensity"; section 4.3 specifies using high arousal and affective contrast
- Break condition: If affective lexicon is not representative of story domain, reranking may not improve perceived interestingness

### Mechanism 3: Position Sampling for Intriguing Twist
- Claim: Identifies optimal position for intriguing twist using data-driven approach based on sentence embeddings
- Mechanism: Samples from learned distribution D(n) over sentence positions, estimated from WritingPrompts dataset using embedding distance as proxy for unexpectedness
- Core assumption: Position of intriguing twist in human-written stories correlates with where it should be placed in generated stories
- Evidence anchors: Section 4.1 describes data-driven approach using embedding distance to determine unexpectedness
- Break condition: If embedding-based distance metric does not capture "interestingness" well, sampled position may not enhance story

## Foundational Learning

- **Concept**: Contextual multi-arm bandits
  - Why needed here: Dynamically choose beam sizes during decoding based on context without requiring labeled data for each choice
  - Quick check question: How does LinUCB use context to balance exploration and exploitation when selecting beam sizes?

- **Concept**: Affective lexicons (NRC Word-Emotion Association, NRC-VAD)
  - Why needed here: Quantify emotional dimensions (arousal, valence) of words and sentences for reranking and feature extraction
  - Quick check question: How is the arousal score of a sentence computed from individual token scores?

- **Concept**: Beam search decoding
  - Why needed here: Generate text by maintaining candidate sequences and expanding iteratively, baseline against which Dynamic Beam Sizing improves
  - Quick check question: What is the difference between standard beam search and nucleus sampling?

## Architecture Onboarding

- **Component map**: Prompt → Position Selector → For each sentence: if not intriguing twist, use base decoder; else use Dynamic Beam Sizer + Affective Reranker → Output story

- **Critical path**: Prompt → Position Selector → For each sentence: if not intriguing twist, use base decoder; else use Dynamic Beam Sizer + Affective Reranker → Output story

- **Design tradeoffs**:
  - Fixed vs. dynamic beam sizes: Fixed sizes simpler but cannot adapt to context; dynamic sizes require bandit training but improve flexibility
  - Affect vs. likelihood: Prioritizing affect may reduce fluency; model balances via payoff function
  - Position sampling vs. planning: Sampling simpler but less controlled than explicit narrative planning

- **Failure signatures**:
  - Low coherence scores (UNION/RUBER) despite high arousal: Bandit over-explores, losing coherence
  - Repetitive or incoherent intriguing twists: Contextual features or payoff function mis-specified
  - No improvement over baseline: Position sampling or reranking not effective for dataset

- **First 3 experiments**:
  1. Ablation: Compare AFFGEN with static beam sizes (10, 30, 60) to validate benefit of dynamic sizing
  2. Ablation: Compare AFFGEN with and without Affective Reranking to measure contribution to interestingness
  3. Human evaluation: Compare stories generated by AFFGEN-3 vs. GPT-3 on coherence, emotional engagement, empathy, interestingness, and overall preference

## Open Questions the Paper Calls Out

- **Open Question 1**: How would AFFGEN perform on generating longer stories (e.g., more than 5 sentences) while maintaining coherence and interestingness?
  - Basis in paper: [inferred] Paper tested on short stories and acknowledges future work could investigate advanced planning strategies for composing longer stories
  - Why unresolved: Paper focuses on short stories (5 sentences) and does not provide results for longer narratives
  - What evidence would resolve it: Conducting experiments with AFFGEN on longer stories (e.g., 10-20 sentences) and evaluating for coherence, interestingness, and overall quality

- **Open Question 2**: How would performance change if trained on more diverse dataset including stories from different genres and cultures?
  - Basis in paper: [explicit] Mentions ROCStories dataset contains gender-related biases and uses "commonsense" everyday stories
  - Why unresolved: Paper does not explore impact of using more diverse dataset on model's performance
  - What evidence would resolve it: Training AFFGEN on more diverse dataset and comparing performance to current model

- **Open Question 3**: How sensitive is AFFGEN to choice of hyperparameters α, β, and λ in Dynamic Beam Sizing algorithm?
  - Basis in paper: [explicit] Mentions specific values for α, β, and λ (0.00015, 0.0003, and 1.5) set based on validation set performance
  - Why unresolved: Paper does not provide systematic analysis of sensitivity to variations in these hyperparameters
  - What evidence would resolve it: Conducting sensitivity analysis by varying values of α, β, and λ and evaluating impact on generated stories' quality

## Limitations

- Contextual features may not effectively capture what makes a word "interesting," limiting bandit's ability to select useful beam sizes
- Affective lexicons may not generalize well to all story domains, particularly creative writing with non-standard language
- Position sampling method assumes embedding distance correlates with narrative interestingness, which may not hold across different story genres

## Confidence

- **High Confidence**: Core architecture combining contextual bandit beam search with affective reranking is technically sound; ablation studies clearly show individual contributions
- **Medium Confidence**: Human evaluation results showing AFFGEN-3 outperforming baselines are compelling but limited by small sample size (50 stories per condition)
- **Low Confidence**: Claim that Dynamic Beam Sizing "encourages less predictable, more captivating word choices" relies heavily on contextual features being meaningful predictors of interestingness without empirical validation

## Next Checks

1. **Feature Ablation Study**: Systematically remove individual contextual features (arousal, event trigger, sequence length, perplexity) from bandit model to quantify individual contributions and identify redundancies

2. **Affective Lexicon Coverage Analysis**: Analyze how well NRC lexicons cover vocabulary used in ROCStories and WritingPrompts datasets, identifying gaps where affective scoring may be unreliable

3. **Cross-Domain Evaluation**: Test AFFGEN on story generation tasks from different domains (news articles, fairy tales, science fiction) to evaluate generalization beyond training data distribution