---
ver: rpa2
title: Are cascade dialogue state tracking models speaking out of turn in spoken dialogues?
arxiv_id: '2311.04922'
source_url: https://arxiv.org/abs/2311.04922
tags:
- dialogue
- spoken
- state
- slots
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spoken dialogue systems often struggle with noisy speech transcriptions,
  especially for non-categorical slot values. We present an analysis of a cascade
  system where ASR errors propagate to a dialogue state tracker (DST), degrading performance.
---

# Are cascade dialogue state tracking models speaking out of turn in spoken dialogues?

## Quick Facts
- arXiv ID: 2311.04922
- Source URL: https://arxiv.org/abs/2311.04922
- Reference count: 0
- Primary result: Non-categorical slot values suffer ~18% precision drop in spoken dialogue state tracking compared to text-based systems

## Executive Summary
Spoken dialogue systems often struggle with noisy speech transcriptions, especially for non-categorical slot values. This analysis examines a cascade system where ASR errors propagate to a dialogue state tracker (DST), degrading performance. The key finding is that non-categorical slot values suffer the most, with a ~18% drop in precision compared to text-based DST. Error analysis shows that ASR errors in slot values are often not corrected by the DST model, and the main error types are confusions and hallucinations in the generative DST. Potential solutions include improving the correction capacity of the DST and better utilizing dialogue context. Data augmentation strategies such as injecting rule-based or TTS-ASR errors into the training data slightly improve DST corrections. Overall, the paper highlights the importance of jointly addressing ASR and DST errors to bridge the gap between spoken and text-based dialogue systems.

## Method Summary
The study uses the Spoken MultiWOZ dataset with TTS and human speech for dev/test, ASR transcriptions from Whisper-Medium, and ground-truth transcriptions. A T5-small generative DST model is fine-tuned on MultiWOZ with various data augmentation strategies including rule-based ASR error injection and TTS-ASR pipeline, plus post-processing for normalization and NER correction. Evaluation metrics include Joint-Goal Accuracy (JGA), Slot Type Accuracy (STA), and Slot-type value Precision (SP). The analysis compares performance between oracle (ground-truth) and baseline (ASR) conditions, examining error propagation and correction mechanisms.

## Key Results
- Non-categorical slot values show an 18% macro-averaged SP decrease compared to text-based DST
- Using ground-truth transcriptions in previous context brings a +6 JGA increase
- Data augmentation with rule-based or TTS-ASR errors improves DST correction capacity by 0.2-0.6% JGA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR transcription errors propagate to DST and degrade non-categorical slot value predictions more than categorical ones.
- Mechanism: Errors in ASR output become input to DST; generative DST struggles to correct non-categorical values because they are open-set and often hallucinate plausible but incorrect values.
- Core assumption: The DST model is trained on clean text but tested on noisy ASR output.
- Evidence anchors:
  - [abstract] "non-categorical slot values suffer the most, with a ~18% drop in precision compared to text-based DST."
  - [section 4.1] "Non-categorical slots and time slots explain most of this performance gap... an 18% macro-averaged SP decrease for non-categorical slots."
- Break condition: If the DST model is trained with ASR-like noise or if the model architecture can reliably correct transcription errors.

### Mechanism 2
- Claim: Contextual information from previous turns is critical for DST correction of noisy transcriptions.
- Mechanism: DST leverages dialogue history to infer missing or corrupted slot values; without clean context, correction fails.
- Core assumption: DST is contextually aware and uses dialogue history as input.
- Evidence anchors:
  - [section 3.3] "The encoder is inputted the entire dialogue history at turn t..."
  - [section 5.2] "using the ground-truth transcriptions in the previous context brings a +6 JGA increase."
- Break condition: If the model input window is too short or context is truncated.

### Mechanism 3
- Claim: Data augmentation with rule-based or TTS-ASR errors improves DST's ability to correct transcription errors.
- Mechanism: Injecting synthetic noise during training teaches DST to handle similar errors at inference.
- Core assumption: Training data diversity improves robustness to ASR noise.
- Evidence anchors:
  - [section 4.2] "both the Rule Error and TTS-ASR training regimes for DST slightly improves this correction capacity."
  - [section 3.4.2] "Our first approach consists in simulating ASR errors by injecting three types of character-level errors..."
- Break condition: If injected noise does not match real ASR error patterns or if the model overfits to synthetic noise.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR) and its error types
  - Why needed here: Understanding ASR errors is essential to diagnosing DST degradation.
  - Quick check question: What are the most common ASR error types in spoken dialogue (substitutions, deletions, insertions)?

- Concept: Generative vs extractive dialogue state tracking
  - Why needed here: The paper uses a generative DST model that can hallucinate or correct values; knowing the differences informs error analysis.
  - Quick check question: How does a generative DST model output differ from an extractive one in handling unseen slot values?

- Concept: Dialogue context and turn-level dependencies
  - Why needed here: DST relies on full dialogue history; understanding context propagation explains correction limits.
  - Quick check question: Why does truncating dialogue history hurt DST performance more for non-categorical slots?

## Architecture Onboarding

- Component map: ASR module → transcribes speech to text → DST module → processes full dialogue history + current utterance → outputs dialogue state → Post-processing → normalizes time, corrects NER errors

- Critical path: Speech input → ASR → ASR output + dialogue history → DST → DST output → dialogue state → Post-processing (normalization, NER correction)

- Design tradeoffs:
  - Larger ASR models improve transcription but increase latency.
  - Generative DST is more flexible but prone to hallucination.
  - Training with augmented data improves robustness but risks overfitting to synthetic noise.

- Failure signatures:
  - Low JGA on non-categorical slots indicates ASR → DST error propagation.
  - High Levenshtein distance between ASR and gold values with no DST correction suggests insufficient correction capacity.
  - Low STA with high SP indicates slot identification is fine but value extraction fails.

- First 3 experiments:
  1. Measure Levenshtein distance between ASR and gold values for non-categorical slots; correlate with DST correction success.
  2. Compare JGA with and without dialogue history (ablation on context).
  3. Evaluate JGA after injecting synthetic errors into training data at different noise rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the correction capacity of DST models be improved to better handle ASR errors in non-categorical slot values?
- Basis in paper: [explicit] The paper highlights that the DST model can correct some mistranscribed values, but not all. It suggests that improving the correction capacity of the DST model is a potential avenue for improvement.
- Why unresolved: The paper does not provide a concrete solution for improving the correction capacity of DST models. It only suggests that investigating the mechanisms behind corrections might help.
- What evidence would resolve it: Developing and testing new techniques or models that can effectively correct a higher percentage of ASR errors in non-categorical slot values.

### Open Question 2
- Question: How can the importance of dialogue context be leveraged to improve spoken dialogue state tracking?
- Basis in paper: [explicit] The paper demonstrates that using ground-truth transcriptions in the previous context brings a +6 JGA increase, highlighting the importance of context.
- Why unresolved: The paper does not explore how to effectively leverage dialogue context to improve spoken dialogue state tracking. It only shows the importance of context.
- What evidence would resolve it: Developing and testing methods that can effectively utilize dialogue context to improve the accuracy of spoken dialogue state tracking.

### Open Question 3
- Question: How can the issue of open-set values be addressed in a complete dialogue system scenario?
- Basis in paper: [explicit] The paper suggests that the issue of open-set values should be investigated in a complete dialogue system scenario with a realistic database of potential entities of interest.
- Why unresolved: The paper does not explore how to address the issue of open-set values in a complete dialogue system scenario. It only suggests that this is an important area for future research.
- What evidence would resolve it: Developing and testing methods that can effectively handle open-set values in a complete dialogue system scenario, including the use of a realistic database of potential entities of interest.

## Limitations

- Single architecture and dataset: Results rely on T5-small DST and MultiWOZ dataset, limiting generalizability
- Modest augmentation improvements: Data augmentation yields only 0.2-0.6% JGA improvements
- ASR error simulation: Synthetic noise may not capture all real-world ASR error patterns

## Confidence

- High: Non-categorical slot value degradation (~18% precision drop) due to ASR errors
- Medium: Effectiveness of dialogue context for correction (+6 JGA with ground-truth context)
- Medium: Data augmentation benefits (+0.2-0.6% JGA improvements)
- Low: Generalization to other DST architectures and spoken domains

## Next Checks

1. **Error Pattern Analysis**: Characterize ASR error types (substitutions, deletions, insertions) specifically for non-categorical slot values and measure their correlation with DST correction success rates.

2. **Context Sensitivity Test**: Conduct an ablation study systematically varying dialogue history length and content, measuring JGA impact specifically for non-categorical vs categorical slots.

3. **Cross-Dataset Verification**: Replicate the core findings (18% precision drop, context correction benefits) on at least one additional spoken dialogue dataset (e.g., SGD, M2M-9) to assess generalizability beyond MultiWOZ.