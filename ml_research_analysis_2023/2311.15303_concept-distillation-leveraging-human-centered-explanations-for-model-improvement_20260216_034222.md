---
ver: rpa2
title: 'Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement'
arxiv_id: '2311.15303'
source_url: https://arxiv.org/abs/2311.15303
tags:
- concept
- teacher
- loss
- concepts
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the concept activation vector (CAV) method from
  post-hoc analysis to ante-hoc training, enabling the reduction of model bias through
  fine-tuning with an additional concept loss. The authors generalize CAVs to intermediate
  layers using class prototypes and introduce concept distillation to create richer
  concepts using a pre-trained model as the teacher.
---

# Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement

## Quick Facts
- arXiv ID: 2311.15303
- Source URL: https://arxiv.org/abs/2311.15303
- Reference count: 40
- This paper extends concept activation vectors (CAVs) from post-hoc analysis to ante-hoc training, enabling reduction of model bias through fine-tuning with concept loss and introducing concept distillation using pre-trained models as teachers.

## Executive Summary
This paper presents a novel framework for improving model interpretability and reducing bias by extending concept activation vectors (CAVs) from post-hoc analysis to ante-hoc training. The authors introduce concept distillation, which leverages a pre-trained teacher model to create richer concept representations that are mapped to a student model's activation space. Their method can both sensitize and desensitize models to specific concepts, enabling debiasing of classification problems and induction of prior knowledge into reconstruction tasks. The approach is validated across multiple datasets showing improved interpretability, reduced biases, and better incorporation of domain knowledge.

## Method Summary
The authors extend CAV methodology to intermediate layers using class prototypes, enabling more localized concept sensitivity measurement. They introduce concept distillation where a pre-trained teacher model provides concept knowledge that is mapped to the student's activation space through an autoencoder. The combined loss function includes original task loss, concept loss based on CAV directions, and prototype loss. This framework allows fine-tuning models to be either more or less sensitive to specific concepts, enabling both bias reduction and prior knowledge induction. The method is evaluated on ColorMNIST, TextureMNIST, DecoyMNIST for debiasing, and Intrinsic Image Decomposition for prior knowledge induction.

## Key Results
- Successfully reduced color bias in ColorMNIST and texture bias in TextureMNIST through concept desensitization
- Improved MSE and SSIM scores in Intrinsic Image Decomposition by inducing reflectance prior knowledge
- Demonstrated concept distillation's effectiveness in transferring concept knowledge from teacher to student models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAVs from intermediate layers can better capture class-specific concepts than final layer CAVs.
- Mechanism: Using prototypes in intermediate layers allows capturing more localized, abstract features that are class-specific and less entangled with global predictions.
- Core assumption: Intermediate layers contain more informative and class-specific activations than the final layer for concept representation.
- Evidence anchors: The paper states intermediate layers facilitate class learning in the last convolution layer, which is known to be most informative.

### Mechanism 2
- Claim: Concept distillation via a pre-trained teacher improves concept learning quality over learning from a potentially biased student model.
- Mechanism: A large, pre-trained teacher model provides more accurate and generalized concept representations, which are then mapped to the student's activation space and used to guide training through concept loss.
- Core assumption: A large, pre-trained model has seen enough diverse data to have learned correct concept associations, unlike smaller, potentially biased student models.
- Evidence anchors: The paper notes that base models may have wrong concept associations due to training bias or limited exposure to concepts.

### Mechanism 3
- Claim: Concept loss can be used to either sensitize or desensitize a model to specific concepts, improving generalization and reducing unintended biases.
- Mechanism: By minimizing the cosine similarity between the loss gradient and the CAV direction, the model's gradients are nudged away from or towards the concept, effectively reducing or increasing sensitivity.
- Core assumption: The direction of the loss gradient is the primary driver of weight updates, and perturbing it can change the model's sensitivity to concepts.
- Evidence anchors: The paper demonstrates the method can sensitize or desensitize a model towards concepts through gradient perturbation.

## Foundational Learning

- Concept: Concept Activation Vectors (CAVs)
  - Why needed here: CAVs are the core representation of concepts used to quantify and manipulate model sensitivity to those concepts.
  - Quick check question: What is a CAV and how is it calculated in the context of this paper?

- Concept: Prototypes for class representations
  - Why needed here: Prototypes are used to define losses in intermediate layers where no ground truth is available, enabling concept sensitivity measurement and manipulation in those layers.
  - Quick check question: How are prototypes calculated and used in the context of concept distillation?

- Concept: Teacher-student distillation framework
  - Why needed here: This framework allows leveraging the knowledge of a large pre-trained model to improve concept learning in a smaller, potentially biased student model.
  - Quick check question: What are the key components of the teacher-student distillation framework used in this paper?

## Architecture Onboarding

- Component map:
  Teacher model (DINO ViT-B/8) -> Mapping module (autoencoder) -> Student model -> CAV learning module -> Prototype calculation -> Loss functions (original + concept + prototype)

- Critical path:
  1. Learn CAVs in teacher space
  2. Map teacher activations to student space
  3. Calculate prototypes in student space
  4. Compute concept loss using CAVs and prototypes
  5. Backpropagate combined loss to update student weights

- Design tradeoffs:
  - Using intermediate layers vs. final layer for concept sensitivity
  - Fixed vs. varying CAVs during training
  - Choice of teacher model size and architecture
  - Complexity of mapping module

- Failure signatures:
  - Low accuracy improvement despite concept distillation
  - Instability in concept loss during training
  - Poor mapping between teacher and student activation spaces
  - CAVs that do not accurately represent the intended concepts

- First 3 experiments:
  1. ColorMNIST debiasing: Test basic concept distillation on a simple biased dataset
  2. TextureMNIST debiasing: Test on a more challenging dataset with complex biases
  3. IID prior induction: Test application to a non-classification problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the method at handling abstract concepts with ambiguous definitions beyond color and texture?
- Basis in paper: The paper mentions the method works with "abstract concept sets" but doesn't extensively test it on more complex, ambiguous concepts like 'shading' and 'reflectance'.
- Why unresolved: The experiments focus on concrete concepts like color and texture. The effectiveness on truly abstract, ambiguous concepts remains untested.
- What evidence would resolve it: Testing the method on datasets requiring understanding of complex concepts like artistic style, emotion, or abstract reasoning, and comparing results with existing methods.

### Open Question 2
- Question: What is the optimal balance between using teacher knowledge and learning from the base model itself?
- Basis in paper: The paper discusses the tradeoff between using a knowledgeable teacher and relying on the base model, but doesn't provide a clear methodology for determining the optimal balance.
- Why unresolved: The paper uses a fixed teacher for all experiments. The impact of varying teacher knowledge and base model capabilities on final performance is unknown.
- What evidence would resolve it: Systematic experiments varying teacher knowledge (size, pre-training, bias) and base model capabilities, and analyzing the resulting performance trade-offs.

### Open Question 3
- Question: How does the method scale to very large datasets and complex models?
- Basis in paper: The paper mentions computational efficiency for smaller datasets and models, but doesn't explore scalability to real-world, large-scale scenarios.
- Why unresolved: The experiments use relatively small datasets and simple models. The computational cost and performance on large datasets and complex architectures are unknown.
- What evidence would resolve it: Benchmarking the method on large-scale image datasets like ImageNet and complex architectures like ResNet or Vision Transformers, measuring computational cost and performance gains.

## Limitations
- The method requires training an additional autoencoder for mapping between teacher and student spaces, significantly increasing training time.
- The paper does not extensively explore how CAVs learned from one architecture transfer to others, which is crucial for practical adoption.
- The assumption that intermediate layers contain more class-specific information than final layers, while theoretically sound, lacks comprehensive empirical validation across diverse model architectures.

## Confidence

- **High Confidence**: The core mechanism of concept distillation and its application to debiasing (supported by quantitative results on multiple datasets)
- **Medium Confidence**: The claim that intermediate layer CAVs are more informative than final layer CAVs (supported by theoretical arguments but limited empirical comparison)
- **Medium Confidence**: The effectiveness of concept distillation for prior knowledge induction (demonstrated on a single reconstruction task)

## Next Checks

1. **Architecture Transferability Test**: Evaluate CAV effectiveness when teacher and student models have different architectures to validate the mapping module's robustness
2. **Ablation Study on Intermediate Layers**: Systematically compare concept sensitivity across different intermediate layers to validate the claim about layer-specific concept representation
3. **Scalability Assessment**: Measure the computational overhead and training time increase when applying concept distillation to larger-scale models and datasets