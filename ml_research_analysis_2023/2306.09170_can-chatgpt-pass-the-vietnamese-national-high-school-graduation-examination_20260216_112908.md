---
ver: rpa2
title: Can ChatGPT pass the Vietnamese National High School Graduation Examination?
arxiv_id: '2306.09170'
source_url: https://arxiv.org/abs/2306.09170
tags:
- chatgpt
- performance
- education
- vnhsge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined ChatGPT performance on the Vietnamese National
  High School Graduation Examination. The model was tested on a dataset of 30 literature
  essays and 1,700 multiple-choice questions across subjects including mathematics,
  English, physics, chemistry, biology, history, geography, and civic education.
---

# Can ChatGPT pass the Vietnamese National High School Graduation Examination?

## Quick Facts
- arXiv ID: 2306.09170
- Source URL: https://arxiv.org/abs/2306.09170
- Authors: 
- Reference count: 13
- ChatGPT achieved an average score of 6-7 on the Vietnamese National High School Graduation Examination, demonstrating sufficient proficiency to pass.

## Executive Summary
This study evaluated ChatGPT performance on the Vietnamese National High School Graduation Examination (VNHSGE) using a dataset of 30 literature essays and 1,700 multiple-choice questions across eight subjects. ChatGPT achieved an average score of 6-7, meeting the passing threshold for the examination. The model performed particularly well in knowledge and comprehension-based questions, excelling in literature, English, and history. Performance was less consistent in subjects requiring visual interpretation or complex problem-solving, with chemistry being the weakest subject at an average score of 4.8.

## Method Summary
The study employed zero-shot prompting of ChatGPT with VNHSGE questions from 2019-2023 exam sets, systematically recording responses and scoring them against official answer keys. The evaluation covered 30 literature essays and 1,700 multiple-choice questions across mathematics, English, physics, chemistry, biology, history, geography, and civic education. Scores were calculated per subject and for various subject combinations, comparing ChatGPT performance against Vietnamese student averages and distributions. The study used direct prompting without fine-tuning on VNHSGE data.

## Key Results
- ChatGPT achieved an average passing score of 6-7 across the VNHSGE examination
- The model excelled in knowledge and comprehension-based questions, particularly in literature, English, and history
- Performance was weakest in chemistry (average score 4.8) and subjects requiring visual interpretation like geography

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's success on VNHSGE is due to strong performance on knowledge and comprehension-level questions (~70% of exam content)
- Mechanism: Pre-training on large text corpora enables effective retrieval and synthesis of factual information and basic explanations
- Core assumption: Exam content is largely static and fact-based, aligning with training data distribution
- Evidence anchors: [abstract] "performed well in knowledge and comprehension-based questions"; [section] "number of correct answers in each subject remains relatively stable"; [corpus] Found 25 related papers, average neighbor FMR=0.397
- Break condition: Performance would degrade significantly if exam emphasizes higher-order thinking (application, analysis, synthesis)

### Mechanism 2
- Claim: ChatGPT achieves comparable scores across natural and social subject combinations due to handling both factual recall and interpretive tasks well
- Mechanism: Transformer architecture processes structured information (natural sciences) and narrative content (social sciences) using similar attention mechanisms
- Core assumption: Both subject areas contain sufficient overlap in question types for generalist approach to work equally well
- Evidence anchors: [abstract] "pass the examination with an average score of 6-7"; [section] "consistently performs well in both natural and social combinations"
- Break condition: Performance would break down if either area introduces more domain-specific reasoning requirements

### Mechanism 3
- Claim: ChatGPT's superior English performance stems from access to vast English text corpus during training
- Mechanism: Extensive English language material in training data developed stronger language processing capabilities than typical Vietnamese students
- Core assumption: Quality and quantity of training data directly correlates with performance on language tasks
- Evidence anchors: [section] "achieved a score for the English subject that ranged from 7.8 to 8.6"; [abstract] "performed well in literature, English, and history"
- Break condition: Advantage would diminish if Vietnamese students receive significantly more immersive English instruction

## Foundational Learning

- Concept: Tokenization and context window limitations
  - Why needed here: Understanding how ChatGPT processes Vietnamese text and whether character-based vs word-based tokenization affects performance on a non-Latin script language
  - Quick check question: How does ChatGPT's tokenizer handle Vietnamese diacritical marks and compound words compared to English text?

- Concept: Zero-shot vs few-shot learning capabilities
  - Why needed here: Study used zero-shot prompting without fine-tuning; understanding these limitations helps interpret performance bounds
  - Quick check question: What's the difference in performance when ChatGPT receives examples of similar exam questions before answering?

- Concept: Evaluation metrics for educational assessments
  - Why needed here: Scoring formula combines subject scores with grade averages; understanding this weighting helps interpret what "passing" means
  - Quick check question: How does the Vietnamese Ministry of Education's scoring formula weight core subjects versus elective combinations?

## Architecture Onboarding

- Component map:
  Input processing (Tokenizer) -> Core model (Transformer-based GPT-3.5) -> Output generation (Autoregressive text completion) -> Evaluation (Comparison against answer keys) -> Dataset (VNHSGE questions categorized by subject and difficulty)

- Critical path:
  1. Load VNHSGE question dataset
  2. Format question for ChatGPT API
  3. Send prompt and receive response
  4. Parse response to extract answer
  5. Compare against correct answer key
  6. Calculate scores per subject and overall

- Design tradeoffs:
  - Zero-shot evaluation provides general capability assessment but misses domain-specific optimization
  - Multiple-choice questions are easier to evaluate than essay questions, potentially biasing results
  - Vietnamese language support may be less robust than English due to training data distribution

- Failure signatures:
  - Consistently wrong answers on visual interpretation questions (geography maps, charts)
  - Degradation on complex problem-solving requiring multi-step reasoning
  - Variable performance across years suggesting sensitivity to question phrasing

- First 3 experiments:
  1. Test ChatGPT on a subset of VNHSGE questions with different prompt formulations to measure sensitivity to prompt engineering
  2. Compare zero-shot performance against few-shot performance using similar question examples
  3. Evaluate performance breakdown by difficulty level (knowledge vs comprehension vs application vs high application) to identify capability boundaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT performance vary across different question difficulty levels (knowledge, comprehension, application, high application) in the VNHSGE?
- Basis in paper: [explicit] The study categorized questions into four distinct difficulty levels and found ChatGPT performed well in knowledge and comprehension-based questions.
- Why unresolved: Paper only provides general performance data but doesn't break down performance by difficulty level for each subject
- What evidence would resolve it: Detailed performance metrics showing accuracy rates for each difficulty level across all subjects

### Open Question 2
- Question: What specific factors contribute to ChatGPT lower performance in chemistry compared to other subjects?
- Basis in paper: [explicit] Chemistry had the lowest average score (4.8) and was identified as ChatGPT weakest subject
- Why unresolved: Paper mentions lower performance but doesn't investigate underlying causes or compare content complexity
- What evidence would resolve it: Comparative analysis of chemistry question types, content coverage, and ChatGPT training data relevance for chemistry

### Open Question 3
- Question: How would ChatGPT performance change when using GPT-4 instead of GPT-3.5, particularly for visual interpretation tasks?
- Basis in paper: [explicit] Study notes GPT-3.5 limitations in reading images and charts, particularly affecting geography performance
- Why unresolved: Study only used GPT-3.5 and speculates about GPT-4 potential without empirical testing
- What evidence would resolve it: Direct comparison of GPT-3.5 vs GPT-4 performance on the same VNHSGE questions, especially those requiring visual interpretation

## Limitations
- Study relied on zero-shot prompting without domain-specific fine-tuning, potentially underestimating ChatGPT's true potential
- Performance evaluation on essay questions lacks clear methodology or rubric specification
- Comparison with Vietnamese student performance doesn't control for test-taking conditions or resource access

## Confidence
- **High confidence**: ChatGPT can achieve passing scores on VNHSGE using knowledge and comprehension-level questions
- **Medium confidence**: Performance differences between natural and social science combinations are negligible at the knowledge/comprehension level
- **Low confidence**: ChatGPT's performance on visual interpretation tasks and complex problem-solving in any subject area

## Next Checks
1. **Multi-shot evaluation**: Test ChatGPT performance using few-shot prompting with examples of similar VNHSGE questions to establish baseline improvements from minimal fine-tuning
2. **Difficulty stratification analysis**: Break down performance by cognitive level (knowledge vs comprehension vs application vs high application) to identify precise capability boundaries and potential failure modes
3. **Cross-linguistic comparison**: Evaluate the same VNHSGE questions using a ChatGPT model fine-tuned specifically on Vietnamese educational content to isolate the impact of language-specific training