---
ver: rpa2
title: Efficient Vision Transformer for Human Pose Estimation via Patch Selection
arxiv_id: '2306.04225'
source_url: https://arxiv.org/abs/2306.04225
tags:
- patches
- pose
- patch
- estimation
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational inefficiency of Vision Transformers
  (ViTs) in human pose estimation due to their quadratic complexity with high-resolution
  inputs. The authors propose three methods to reduce this complexity by selecting
  and processing only the most informative image patches based on predictions from
  a lightweight pose estimator.
---

# Efficient Vision Transformer for Human Pose Estimation via Patch Selection

## Quick Facts
- arXiv ID: 2306.04225
- Source URL: https://arxiv.org/abs/2306.04225
- Reference count: 28
- Key outcome: 30-44% reduction in GFLOPs with only 0-3.5% accuracy drop using patch selection methods for ViT-based pose estimation

## Executive Summary
This paper addresses the computational inefficiency of Vision Transformers for human pose estimation by proposing three patch selection methods that reduce the number of processed image patches. The methods use predictions from a lightweight pose estimator to identify and process only the most informative patches around keypoints, skeletal lines, or through learnable joint tokens. Experiments demonstrate significant computational savings (30-44% GFLOPs reduction) with minimal accuracy loss (0-3.5% AP drop) across COCO, MPII, and OCHuman benchmarks.

## Method Summary
The authors propose reducing ViT computational complexity by selecting and processing only the most informative image patches rather than the entire image. They use a lightweight pose estimator to predict keypoint locations, then apply three patch selection strategies: (1) selecting neighboring patches around each keypoint using BFS, (2) selecting patches along skeletal lines between joint pairs using Bresenham's algorithm, and (3) using learnable joint tokens. The selected patches are processed by a ViT encoder while unselected patches are zeroed out, significantly reducing self-attention computation while maintaining pose estimation accuracy.

## Key Results
- ViTransPose-B with skeleton patch selection achieved 74.3% AP on COCO vs 76.9% for full model
- GFLOPs reduced from 17.9 to 13.3 (25.7% reduction) with only 2.6% AP drop
- Overall 30-44% GFLOPs reduction across benchmarks with 0-3.5% accuracy drop
- Method scales quadratically better than full ViT as input resolution increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting only patches near predicted keypoints reduces ViT computational complexity without significant accuracy loss.
- Mechanism: A lightweight pose estimator predicts keypoint locations; patches around these keypoints are selected for ViT processing while others are filled with zeros, reducing self-attention computation.
- Core assumption: Keypoints and their immediate surroundings contain most of the relevant information for pose estimation, and distant patches contribute little to the final heatmap.
- Evidence anchors: [abstract] "We propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others." [section 3.1.1] "we employ a Breadth-First Search (BFS) algorithm... for every keypoint located at 2D patch location (x, y) ∈ B, we identify the nearest four neighboring patches... and store them in a queue." [corpus] Weak evidence; neighboring papers focus on different patch selection methods (e.g., text-relevant for V&L), not pose estimation-specific patches.
- Break condition: If the lightweight pose estimator is inaccurate, selected patches may miss critical pose information, leading to degraded accuracy beyond the reported 0-3.5% drop.

### Mechanism 2
- Claim: Selecting patches along skeletal lines between joint pairs captures long-range dependencies efficiently.
- Mechanism: Bresenham's algorithm is used to select patches that form the lines between pairs of predicted keypoints, ensuring coverage of the skeletal structure without processing the entire image.
- Core assumption: The spatial relationships along the skeletal structure (lines between joints) are important for pose estimation and can be captured by processing only these patches.
- Evidence anchors: [section 3.1.2] "we adopt a different strategy by identifying the patches where the line formed by the body joint pair crosses, based on Bresenham's algorithm." [section 2.2] "Recent research [25] indicates that the long-range dependencies between predicted keypoints are mostly restricted to the body part regions." [corpus] No direct evidence; related papers do not mention Bresenham-based skeletal patch selection for pose estimation.
- Break condition: If joint predictions are inaccurate or the skeletal structure is complex (e.g., occluded limbs), the selected patches may not adequately represent the pose, causing accuracy to drop beyond acceptable levels.

### Mechanism 3
- Claim: Using learnable joint tokens allows the model to focus on the most important body joint information.
- Mechanism: A set of learnable tokens is introduced to represent joint positions, ensuring that the selected patches contain the most relevant information for pose estimation.
- Core assumption: Learnable tokens can adaptively capture the most informative features for each joint, improving efficiency without sacrificing accuracy.
- Evidence anchors: [abstract] "the third method utilizes a set of learnable joint tokens to ensure that the selected patches contain the most important information about body joints." [section 3] "we propose a standard Vision Transformer-based HPE with patch selection... The first two methods leverage a lightweight pose estimation network to guide the patch selection process, while the third method utilizes a set of learnable joint tokens..." [corpus] Weak evidence; related papers do not discuss learnable joint tokens in the context of patch selection for pose estimation.
- Break condition: If the learnable tokens fail to converge or do not effectively capture joint-specific information, the model's accuracy may degrade significantly.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and self-attention mechanism.
  - Why needed here: Understanding how ViT processes image patches via self-attention is crucial to grasp why reducing the number of patches reduces computational complexity.
  - Quick check question: How does the computational complexity of self-attention scale with the number of input tokens, and why does this make ViTs expensive for high-resolution images?

- Concept: Human pose estimation (HPE) and heatmap-based approaches.
  - Why needed here: Knowing how pose estimation works—specifically, predicting 2D Gaussian heatmaps centered at body joints—explains why patches outside the body contribute little and can be discarded.
  - Quick check question: In heatmap-based pose estimation, what does each pixel value in the output heatmap represent, and how is it related to keypoint locations?

- Concept: Breadth-First Search (BFS) and Bresenham's line algorithm.
  - Why needed here: These algorithms are used to select neighboring patches and skeletal patches, respectively; understanding them is key to implementing the patch selection methods.
  - Quick check question: How does Bresenham's algorithm determine which pixels (or patches) to select when drawing a line between two points on a grid?

## Architecture Onboarding

- Component map: Input image → Patch embedding (16x16 patches) → Patch selection module → ViT encoder (with L transformer layers) → CNN heatmap decoder → Output heatmaps. Lightweight pose estimator (external, for guiding patch selection). Patch selection methods: BFS-based neighboring patches, Bresenham-based skeletal patches, learnable joint tokens.

- Critical path: 1. Input image is divided into patches. 2. Lightweight pose estimator predicts keypoints. 3. Patch selection module selects relevant patches (neighboring or skeletal) or applies learnable tokens. 4. Selected patches are processed by ViT encoder. 5. Feature map is passed to CNN decoder to generate heatmaps. 6. Heatmaps are post-processed (e.g., via UDP) to obtain final keypoint predictions.

- Design tradeoffs: Accuracy vs. efficiency: More patches selected → higher accuracy but lower efficiency; fewer patches → faster but potentially less accurate. Patch selection strategy: Neighboring patches are simpler but may miss skeletal structure; skeletal patches capture structure but require accurate joint predictions. Dependency on lightweight estimator: Accuracy of patch selection depends on the quality of the lightweight pose estimator; errors propagate to final predictions.

- Failure signatures: Accuracy drops significantly (>3.5%) compared to full ViT model. Selected patches consistently miss body parts (e.g., limbs are cut off or ignored). Model fails on occluded or complex poses where the lightweight estimator is inaccurate. Computational savings are minimal (GFLOPs reduction << 30%) due to inefficient patch selection.

- First 3 experiments: 1. Implement BFS-based neighboring patch selection and compare GFLOPs and accuracy (AP) on COCO val set with the full ViTransPose model. 2. Implement Bresenham-based skeletal patch selection and evaluate trade-off between number of neighboring patches and accuracy on MPII. 3. Integrate learnable joint tokens and assess their impact on both efficiency and accuracy across all three benchmarks (COCO, MPII, OCHuman).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed patch selection method scale to even higher resolution images or long videos beyond what was tested?
- Basis in paper: [explicit] The authors state "the computational cost of ViTs scales quadratically as the number of input tokens increases, making them intractable for practical use" and mention this is a limitation for "processing high-resolution images and long videos"
- Why unresolved: The experiments only tested on standard benchmarks with moderate resolutions (256×192), not truly high-resolution or video inputs
- What evidence would resolve it: Experiments showing patch selection performance and computational gains on 4K images, video sequences, or other high-resolution datasets would clarify scalability limits

### Open Question 2
- Question: What is the optimal number of neighboring patches (n) to select for balancing accuracy and computational efficiency across different datasets?
- Basis in paper: [explicit] The authors mention "We can also control the drop in accuracy by changing the number of neighboring patches" and show Figure 3 with varying n values, but don't provide a principled method for choosing n
- Why unresolved: The choice of n appears to be heuristic and dataset-dependent based on the visual trade-off curves, with no systematic analysis of how to optimize this parameter
- What evidence would resolve it: A comprehensive study analyzing the relationship between n, dataset characteristics, and the accuracy/complexity trade-off, potentially with a learned selection criterion

### Open Question 3
- Question: How would alternative lightweight pose estimators affect the patch selection quality and overall performance compared to LiteHRNet?
- Basis in paper: [explicit] The authors state "we employ a lightweight pose estimation network to guide the patch selection process" and specifically used LiteHRNet, but don't explore alternatives
- Why unresolved: Only one lightweight pose estimator was tested, leaving uncertainty about whether LiteHRNet is optimal or if other models might provide better guidance for patch selection
- What evidence would resolve it: Experiments comparing patch selection performance using different lightweight pose estimators (e.g., other HRNet variants, MobileNet-based models, or even older CNN-based pose estimators) would reveal the sensitivity to this choice

## Limitations
- The method relies heavily on the accuracy of the lightweight pose estimator, which is not evaluated for its own performance characteristics
- The assumption that patches outside selected regions contribute negligible information is not rigorously validated across diverse pose scenarios
- Bresenham-based skeletal patch selection may fail for complex poses with occluded or highly articulated limbs where skeletal lines cross non-body regions

## Confidence
- High confidence: The computational complexity reduction mechanism (quadratic scaling with patch count) and the basic feasibility of patch selection for efficiency gains are well-established concepts.
- Medium confidence: The specific patch selection strategies (BFS neighboring patches and Bresenham skeletal patches) show reasonable results but lack extensive ablation studies on their individual contributions.
- Low confidence: The learnable joint tokens mechanism is described superficially without sufficient detail on implementation or thorough evaluation of its effectiveness.

## Next Checks
1. **Ablation on Patch Selection Strategy**: Systematically evaluate the contribution of each patch selection method (neighboring vs. skeletal) by training models using only one method at a time, and measure both accuracy and computational efficiency independently.

2. **Lightweight Estimator Performance Impact**: Quantify how errors in the lightweight pose estimator propagate through the patch selection process by intentionally degrading its accuracy and measuring the corresponding impact on final pose estimation performance.

3. **Cross-dataset Generalization**: Test the patch selection methods on datasets not seen during training (e.g., using a model trained on COCO and evaluated on LSP or CrowdPose) to assess whether the selected patch regions generalize beyond the training distribution.