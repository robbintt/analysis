---
ver: rpa2
title: Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification
arxiv_id: '2304.07022'
source_url: https://arxiv.org/abs/2304.07022
tags:
- labels
- classi
- multi-label
- cation
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LD-SPN, a novel approach for multi-label
  text classification that treats the problem as a set prediction task rather than
  sequence generation. The method employs Graph Convolutional Networks (GCN) to model
  label dependencies through an adjacency matrix constructed from statistical relations
  between labels.
---

# Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification

## Quick Facts
- arXiv ID: 2304.07022
- Source URL: https://arxiv.org/abs/2304.07022
- Reference count: 40
- Key outcome: Introduces LD-SPN that achieves superior performance on multi-label text classification through set prediction with label dependency modeling

## Executive Summary
This paper presents LD-SPN, a novel approach for multi-label text classification that treats the problem as a set prediction task rather than sequence generation. The method employs Graph Convolutional Networks (GCN) to model label dependencies through an adjacency matrix constructed from statistical relations between labels. Additionally, Bhattacharyya distance is applied to the output distributions of the set prediction networks to enhance recall ability. The model is evaluated on four multi-label datasets (MixSNIPS, MixATIS, AAPD, and RCV1-V2) and demonstrates superior performance compared to previous baselines, achieving significant improvements in F1 score and hamming loss.

## Method Summary
LD-SPN addresses multi-label text classification by treating labels as unordered sets and predicting them simultaneously using a non-autoregressive decoder. The model uses a BERT encoder to obtain text representations, then applies Graph Convolutional Networks to model label dependencies through a statistically constructed adjacency matrix. A set prediction network with fixed-size label queries generates label predictions, and bipartite matching loss ensures correct label-to-prediction assignment. The Bhattacharyya distance between output distributions is used as regularization to improve recall by enforcing diversity in the predicted label probabilities.

## Key Results
- LD-SPN achieves significant improvements in F1 score and hamming loss across four multi-label datasets compared to previous baselines
- Ablation studies confirm the effectiveness of each component, particularly the label dependency modeling through GCN and the Bhattacharyya distance module for improving recall
- The model demonstrates superior performance in balancing precision and recall through the diversity regularization mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bipartite matching loss ensures correct label-to-prediction assignment regardless of order
- Mechanism: The loss function solves an optimal assignment problem between ground truth labels and predictions using permutations
- Core assumption: Labels are unordered sets, so any permutation of predictions should be evaluated against ground truth
- Evidence anchors:
  - [section]: "Since the predicted labels are unordered and loss function should be invariant to any permutation of predictions, we adopt bipartite matching loss to measure the distance between the ground truths and predictions"
  - [section]: "The matching solution can be derived from the following optimization: π∗ = argminπ∈Π(m) ∑Cmatch(Yi, ˆYπ(i))"
- Break condition: If label order matters in the task or if there are duplicate labels that break the uniqueness assumption

### Mechanism 2
- Claim: Graph Convolutional Networks model label dependencies through statistical co-occurrence
- Mechanism: An adjacency matrix built from conditional probabilities between labels is used as graph structure for GCN to propagate label information
- Core assumption: Label co-occurrence statistics capture meaningful semantic relationships between labels
- Evidence anchors:
  - [section]: "Since labels are correlated, we model the label dependency via a graph, which is effective in capturing the relationship among labels"
  - [section]: "We count the number of occurrence of label pairs in the training data and get a matrix C∈RK×K, where K is the number of distinct labels"
- Break condition: If label relationships are not well-captured by co-occurrence statistics or if the label space is too sparse

### Mechanism 3
- Claim: Bhattacharyya distance between output distributions increases recall by enforcing diversity
- Mechanism: A regularization term maximizes the distance between predicted label probability distributions to prevent them from overlapping
- Core assumption: Overlapping output distributions lead to poor recall performance in set prediction
- Evidence anchors:
  - [section]: "The output distributions of the queries may overlap, which deteriorate the recall performance for label generation"
  - [section]: "In order to improve the recall ability of the model, we use the Bhattacharyya distance to calculate the distance between the output label distribution, making the distribution more diverse and uniform"
- Break condition: If the trade-off parameter λ is poorly tuned or if the model overfits to the diversity constraint

## Foundational Learning

- Concept: Bipartite matching and assignment problems
  - Why needed here: Understanding how the optimal permutation is computed between predictions and ground truth
  - Quick check question: What optimization problem does the bipartite matching loss solve?

- Concept: Graph Convolutional Networks and message passing
  - Why needed here: Understanding how GCN propagates information between label nodes based on the adjacency matrix
  - Quick check question: How does the normalized symmetric adjacency matrix affect information flow in GCN?

- Concept: Bhattacharyya distance and probability distribution similarity
  - Why needed here: Understanding how this metric measures overlap between distributions and why minimizing it helps
  - Quick check question: Why does maximizing Bhattacharyya distance lead to more diverse output distributions?

## Architecture Onboarding

- Component map: BERT encoder → GCN for label dependencies