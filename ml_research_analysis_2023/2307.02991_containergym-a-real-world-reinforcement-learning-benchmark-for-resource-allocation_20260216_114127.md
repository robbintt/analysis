---
ver: rpa2
title: 'ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation'
arxiv_id: '2307.02991'
source_url: https://arxiv.org/abs/2307.02991
tags:
- emptying
- environment
- learning
- containers
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ContainerGym is a reinforcement learning benchmark environment
  derived from a real-world industrial waste sorting process. It simulates a resource
  allocation problem involving stochastic filling of containers and limited processing
  units.
---

# ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation

## Quick Facts
- arXiv ID: 2307.02991
- Source URL: https://arxiv.org/abs/2307.02991
- Reference count: 20
- Primary result: Standard RL algorithms struggle to learn optimal resource allocation policies in stochastic industrial environments, with PPO showing the most stable performance.

## Executive Summary
ContainerGym is a reinforcement learning benchmark environment derived from a real-world industrial waste sorting process. It simulates a resource allocation problem involving stochastic filling of containers and limited processing units. The environment is customizable in complexity and provides tools for analyzing agent behavior beyond reward curves. Standard deep RL algorithms (PPO, TRPO, DQN) were evaluated on eight different configurations varying in container count, processing unit count, and timestep length. PPO showed the most stable performance, particularly with longer timesteps. However, none of the algorithms learned optimal policies, especially in cases where the number of processing units equaled the number of containers. The benchmark highlights challenges in learning long-term dependencies and optimal resource allocation in stochastic environments. ContainerGym provides a valuable platform for developing and testing reinforcement learning algorithms for real-world industrial applications.

## Method Summary
The authors implemented ContainerGym as a customizable OpenAI Gym environment that simulates a waste sorting facility with stochastic container filling and limited processing units. They trained PPO, TRPO, and DQN algorithms on 8 different configurations varying in container count, processing unit count, and timestep length. The agents were evaluated based on their cumulative reward over episodes, with additional analysis of emptying volumes and action frequencies to understand learned behaviors.

## Key Results
- PPO outperformed TRPO and DQN on ContainerGym, particularly with longer timesteps (δ=120 vs δ=60).
- None of the algorithms learned optimal policies when the number of processing units equaled the number of containers.
- Longer timesteps led to better performance by reducing action frequency and allowing agents to make more informed decisions aligned with material accumulation dynamics.

## Why This Works (Mechanism)

### Mechanism 1
ContainerGym succeeds as a benchmark because it captures real-world stochasticity and long-term planning challenges in a minimal, yet non-trivial model. The environment models stochastic material filling via a random walk with drift, constrained PUs with limited availability, and rewards structured around multiple optimal emptying volumes. This combination forces agents to learn timing, coordination, and uncertainty management rather than memorization.

### Mechanism 2
PPO outperforms other RL algorithms on ContainerGym due to its stability in high-variance, sparse-action environments. PPO's clipped surrogate objective limits policy updates when reward variance is high, preventing destructive large updates in sparse-reward settings. This matches ContainerGym's rare but high-impact emptying actions.

### Mechanism 3
Longer timesteps improve agent performance because they reduce action frequency and align decision points with material accumulation dynamics. By increasing δ from 60 to 120 seconds, agents make fewer but more informed decisions, reducing premature emptying and better matching PU availability cycles.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: ContainerGym is explicitly formulated as an MDP; understanding states, actions, transitions, and rewards is essential to interpret the benchmark and design agents.
  - Quick check question: In the state definition st = ({vi,t}^n_i=1, {pj,t}^m_j=1), what does each component represent and why is it important for the allocation problem?

- **Concept: Random Walk with Drift**
  - Why needed here: The container fill dynamics follow a random walk with drift; understanding this stochastic process explains why timing and coordination are non-trivial.
  - Quick check question: How does the noise term ϵi,t in fi(vi,t) = max(0, αi + vi,t + ϵi,t) affect the predictability of when a container will reach its optimal emptying volume?

- **Concept: Reward Shaping and Multi-Modal Objectives**
  - Why needed here: The reward function uses Gaussian peaks around multiple optimal volumes, requiring agents to learn nuanced behavior rather than simply maximizing reward per action.
  - Quick check question: Why does the reward function include both a penalty for overflow and Gaussian peaks at multiple optimal volumes, and how does this shape agent behavior?

## Architecture Onboarding

- **Component map**: Gym environment wrapper -> step(), reset(), render() -> State (volumes per container + PU timers) -> Action (do nothing or empty specific container) -> Reward (overflow penalty, Gaussian-shaped reward, small penalty) -> Dynamics (stochastic volume increment, PU processing time, state update rules)

- **Critical path**: 
  1. Agent observes state st
  2. Agent selects action at
  3. Environment computes st+1 and reward rt via defined dynamics
  4. Episode ends on overflow or T timesteps

- **Design tradeoffs**:
  - Simplicity vs. realism: simplified PU and material flow but retains stochasticity and coordination challenge
  - Timestep granularity: shorter timesteps increase action frequency but may cause premature emptying; longer timesteps reduce actions but risk overflow
  - Reward complexity: multi-modal Gaussian rewards encourage nuanced policies but increase learning difficulty

- **Failure signatures**:
  - High negative rewards from overflow indicate poor anticipation of PU availability
  - Frequent small penalties (rpen) suggest agents are emptying at non-optimal volumes
  - Low variance in emptying volumes across episodes indicates overfitting or lack of exploration

- **First 3 experiments**:
  1. Run PPO with δ=60 and δ=120 on n=5, m=2; compare reward curves and ECDF plots of emptying volumes
  2. Test PPO vs. a rule-based controller (empty when volume within 1 unit of global optimum) on n=5, m=2
  3. Vary n and m (e.g., n=11, m=2 vs. n=11, m=11) to assess scalability and PPO's ability to coordinate under scarcity

## Open Questions the Paper Calls Out

### Open Question 1
What specific algorithmic improvements are needed to enable RL agents to learn optimal policies in the ContainerGym environment, particularly when the number of processing units equals the number of containers? The authors state that none of the benchmarked algorithms manage to learn the optimal policy when there are as many PUs as containers, independently of the environment dimensionality and the timestep length. This remains unresolved as the paper demonstrates failure but does not propose specific solutions.

### Open Question 2
How does the performance of RL agents in ContainerGym scale with increasing numbers of containers and processing units, and what are the fundamental limitations of current algorithms in handling high-dimensional resource allocation problems? While the paper provides initial benchmark results on 8 configurations, it does not systematically explore how agent performance degrades or improves as the environment complexity increases.

### Open Question 3
What is the impact of different reward function designs on the learning behavior and final performance of RL agents in ContainerGym, and how can reward shaping be used to encourage more desirable agent behaviors? The paper uses a specific reward function based on Gaussian peaks around optimal emptying volumes but does not explore alternative reward structures or their effects on agent learning and behavior.

## Limitations

- The random walk parameters for container fill rates and processing unit characteristics are not fully specified, affecting reproducibility and generalizability.
- The comparison to existing benchmarks is limited, as no direct comparisons are made with other industrial RL benchmarks.
- The assertion that ContainerGym captures the essential hardness of real-world resource allocation problems is based on a simplified model and lacks direct validation against industrial data.

## Confidence

- **High confidence**: PPO's superior stability and performance on ContainerGym configurations, as evidenced by lower standard deviations and smaller differences between best and median performances.
- **Medium confidence**: The claim that longer timesteps improve agent performance, as it is supported by experimental results but lacks a strong theoretical justification.
- **Low confidence**: The assertion that ContainerGym captures the essential hardness of real-world resource allocation problems, as it is based on a simplified model and lacks direct validation against industrial data.

## Next Checks

1. **Parameter sensitivity analysis**: Conduct experiments to determine the impact of varying random walk parameters (αi, σi) and processing unit characteristics (bi, βi, λi) on agent performance and benchmark difficulty.

2. **Comparison with rule-based controllers**: Implement and compare PPO policies against rule-based controllers (e.g., empty when volume within 1 unit of global optimum) to assess the learned policies' optimality and robustness.

3. **Transfer to real-world data**: Validate ContainerGym's effectiveness by transferring learned policies to real-world waste sorting facility data, if available, to assess the benchmark's ability to capture real-world dynamics and inform industrial applications.