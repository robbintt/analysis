---
ver: rpa2
title: Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution
  Detection
arxiv_id: '2303.14961'
source_url: https://arxiv.org/abs/2303.14961
tags:
- certi
- adversarial
- detection
- robustness
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of certifying the robustness\
  \ of Out-Of-Distribution (OOD) detection within a \u21132-norm around the input,\
  \ and improving techniques for detecting adversarial attacks on OOD samples. The\
  \ authors propose a novel method called DISTRO, which combines a diffusion denoiser,\
  \ a standard OOD detector, and a certified binary discriminator."
---

# Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection

## Quick Facts
- arXiv ID: 2303.14961
- Source URL: https://arxiv.org/abs/2303.14961
- Reference count: 32
- This paper addresses certifying the robustness of Out-Of-Distribution (OOD) detection within an ℓ2-norm around the input, and improving techniques for detecting adversarial attacks on OOD samples.

## Executive Summary
This paper proposes DISTRO, a novel method that combines a diffusion denoiser, a standard OOD detector, and a certified binary discriminator to achieve certified and adversarial robust OOD detection. The method shows significant improvements over previous approaches, achieving an average increase of ~13%/5% in OOD detection metrics on CIFAR10/100 datasets. DISTRO provides high levels of certified and adversarial robustness on both in-distribution (ID) and out-of-distribution (OOD) data without requiring special training or modifications to the network architecture.

## Method Summary
DISTRO combines a pre-trained diffusion denoiser with a standard OOD detector (such as Outlier Exposure) and a certified binary discriminator (trained via Interval Bound Propagation). The diffusion denoiser operates on a noisy version of the input to remove adversarial perturbations, the OOD detector distinguishes between ID and OOD samples, and the binary discriminator ensures low confidence for samples far from the training distribution. The method leverages randomized smoothing to compute provable ℓ2-norm bounds for OOD detection without requiring special training of the base classifier.

## Key Results
- DISTRO achieves an average increase of ~13%/5% in OOD detection metrics (AUC, AUPR, FPR) compared to previous approaches on CIFAR10/100 datasets
- The method provides high levels of certified and adversarial robustness on both ID and OOD data
- DISTRO successfully detects adversarial attacks on OOD samples while maintaining good performance on ID data
- The method does not require special training or modifications to the network architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diffusion denoising before classification reduces adversarial noise in the input, improving both adversarial and certified robustness.
- **Mechanism**: The denoiser operates on a noisy version of the input (Gaussian perturbation) and recovers a cleaner image that is more representative of the true sample. This makes the subsequent classifier less sensitive to adversarial perturbations.
- **Core assumption**: The one-step diffusion denoiser introduces minimal variance and preserves the semantic content of the input while removing adversarial noise.
- **Evidence anchors**: Supported by prior work on diffusion denoising for adversarial robustness (Carlini et al., 2023) and empirical results showing improved metrics.

### Mechanism 2
- **Claim**: Randomized smoothing makes the classifier locally Lipschitz continuous, enabling provable upper bounds on confidence for OOD samples within an ℓ2-norm.
- **Mechanism**: By convolving the classifier with Gaussian noise, the smoothed function G has bounded gradients. This Lipschitz continuity ensures that confidence scores for OOD samples cannot increase beyond a calculable radius, enabling guaranteed OOD detection.
- **Core assumption**: The classifier's smoothed version assigns probability > 0.5 to the predicted class, so the Lipschitz constant applies.
- **Evidence anchors**: Theorem 4.1 provides a novel way of calculating the upper bound of any classifier; empirical validation through binomial proportion confidence tests.

### Mechanism 3
- **Claim**: Combining a certified binary discriminator with a smoothed classifier ensures asymptotic underconfidence for far-OOD samples.
- **Mechanism**: The binary discriminator P(i|x) → 0 as the input moves far from the training distribution. Combined with the smoothed classifier's confidence, this yields low overall confidence for far-OOD samples, independent of the main classifier's behavior.
- **Core assumption**: The binary discriminator is trained in a certifiable manner (e.g., via IBP) and its output P(i|x) decreases to zero for far-OOD inputs.
- **Evidence anchors**: Empirical results show improved OOD detection when using a certified binary discriminator trained with IBP on OOD data.

## Foundational Learning

- **Concept**: Randomized smoothing and Lipschitz continuity
  - Why needed here: Enables provable robustness bounds without modifying the base classifier or requiring special training.
  - Quick check question: What is the radius R in randomized smoothing, and how is it computed from the smoothed classifier's confidence p?

- **Concept**: Diffusion denoising and score-based generative models
  - Why needed here: Provides a principled way to remove adversarial noise before classification without retraining.
  - Quick check question: How does the one-step denoising in DISTRO differ from iterative denoising in terms of information preservation?

- **Concept**: Out-of-distribution detection metrics (AUROC, AUPR, FPR)
  - Why needed here: These metrics quantify how well the method distinguishes ID from OOD samples under clean, adversarial, and certified settings.
  - Quick check question: Why is it important to evaluate both adversarial and guaranteed versions of these metrics?

## Architecture Onboarding

- **Component map**: Input → Gaussian noise addition → Diffusion denoiser (one-step) → Main classifier (e.g., OE-trained ResNet) → Confidence output; Parallel branch: Input → Binary discriminator (IBP-trained) → P(i|x) output; Final confidence: P(y|x) = P(y|x,i)P(i|x) + (1/K)(1 - P(i|x))

- **Critical path**: Gaussian noise → Denoiser → Classifier → Confidence. The discriminator runs in parallel and merges at the final confidence computation.

- **Design tradeoffs**:
  - One-step denoiser vs. iterative: Faster, less hallucination, but potentially less noise removal
  - Using MSP vs. Energy for OOD scoring: MSP is better for certified ℓ2 bounds; Energy gives asymptotic underconfidence without certification
  - Classifier choice: OE-trained models work well with DISTRO; other OOD-aware models may need retraining

- **Failure signatures**:
  - Clean accuracy drops significantly → Denoiser oversmooths
  - ℓ∞ robustness fails → Discriminator or denoiser not effective against ∞-norm attacks
  - ℓ2 certified bounds are zero → Smoothed classifier confidence ≤ 0.5

- **First 3 experiments**:
  1. Run DISTRO on clean CIFAR10 test set with Gaussian noise σ=0.12; verify clean accuracy matches baseline
  2. Evaluate ℓ2 certified AUC on CIFAR10 vs. CIFAR100 with R>0; confirm non-zero GAUC
  3. Apply AutoAttack (ℓ∞) on CIFAR10 test set; measure AAUC and compare to baseline OE model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of using a diffusion denoiser on the computational cost of robust OOD detection compared to standard OOD detection methods?
- Basis in paper: The paper states that using a diffusion denoiser increases computational costs compared to previous approaches
- Why unresolved: The paper does not provide specific metrics or comparisons of computational costs between DISTRO and other methods
- What evidence would resolve it: A detailed analysis of the computational cost (e.g., time complexity, memory usage) of DISTRO versus standard OOD detection methods on the same hardware and datasets

### Open Question 2
- Question: How does the performance of DISTRO vary with different threat models (e.g., ℓ∞ vs ℓ2 norms) for adversarial attacks on OOD data?
- Basis in paper: The paper discusses the performance of DISTRO under different threat models but does not provide a comprehensive comparison
- Why unresolved: The paper focuses on ℓ2-norm robustness but does not extensively explore the performance under other threat models
- What evidence would resolve it: Empirical results comparing DISTRO's performance under various threat models (e.g., ℓ∞, ℓ2, ℓ1 norms) on multiple datasets

### Open Question 3
- Question: Can the diffusion denoiser in DISTRO be further optimized to improve its robustness without compromising clean accuracy?
- Basis in paper: The paper suggests that the diffusion denoiser improves robustness but does not explore potential optimizations
- Why unresolved: The paper does not investigate alternative denoising techniques or parameter tuning for the diffusion denoiser
- What evidence would resolve it: Experiments comparing different denoising techniques and parameter settings to identify optimal configurations for DISTRO

### Open Question 4
- Question: How does DISTRO perform on larger and more complex datasets, such as ImageNet, compared to its performance on CIFAR-10/100?
- Basis in paper: The paper evaluates DISTRO on CIFAR-10/100 datasets but does not test it on larger datasets
- Why unresolved: The scalability of DISTRO to larger datasets is not addressed in the paper
- What evidence would resolve it: Empirical results of DISTRO's performance on larger datasets, such as ImageNet, to assess its scalability and generalization capabilities

## Limitations
- DISTRO's performance on datasets beyond CIFAR10/100 remains unverified
- The computational overhead of combining diffusion denoising with randomized smoothing may limit practical deployment
- The certified ℓ2 bound computation depends on unstated conditions that may not always be satisfied in practice

## Confidence
- Mechanism 1 (Diffusion denoising benefits): Medium confidence - supported by prior work but limited ablation in this paper
- Mechanism 2 (Certified ℓ2 bounds): Low confidence - the theoretical bound depends on unstated conditions
- Mechanism 3 (Asymptotic underconfidence): Medium confidence - empirically demonstrated but theoretical guarantees not fully established

## Next Checks
1. Verify the certified ℓ2 bound computation on a simple classifier where p > 0.5 is guaranteed
2. Test DISTRO on ImageNet-1K to assess scalability to larger datasets
3. Measure the computational overhead (latency and memory) compared to baseline OE models