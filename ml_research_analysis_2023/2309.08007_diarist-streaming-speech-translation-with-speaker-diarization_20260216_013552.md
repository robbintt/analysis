---
ver: rpa2
title: 'DiariST: Streaming Speech Translation with Speaker Diarization'
arxiv_id: '2309.08007'
source_url: https://arxiv.org/abs/2309.08007
tags:
- speech
- speaker
- streaming
- system
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiariST, the first streaming speech translation
  (ST) and speaker diarization (SD) system. It addresses the challenge of translating
  overlapping speech in conversations with unknown word time stamps.
---

# DiariST: Streaming Speech Translation with Speaker Diarization

## Quick Facts
- **arXiv ID**: 2309.08007
- **Source URL**: https://arxiv.org/abs/2309.08007
- **Reference count**: 0
- **Key outcome**: DiariST is the first streaming ST and SD system that achieves strong performance on both tasks compared to offline baselines while handling overlapping speech.

## Executive Summary
This paper introduces DiariST, the first streaming speech translation (ST) and speaker diarization (SD) system. It addresses the challenge of translating overlapping speech in conversations with unknown word time stamps by combining a neural transducer-based ST model with token-level serialized output training (t-SOT) and token-level speaker embeddings (t-vector). The system achieves strong ST and SD performance compared to offline baselines based on Whisper, while maintaining streaming inference capability. It outperforms the offline ST-then-SD baseline in BLEU score by 2.2 points and the SD-then-ST baseline by 2.7 points on speaker-attributed BLEU (SAtBLEU), while effectively handling overlapping speech. The paper also introduces a new evaluation dataset and metrics to measure ST quality with SD accuracy.

## Method Summary
DiariST combines a neural transducer-based streaming ST system with token-level serialized output training (t-SOT) and token-level speaker embeddings (t-vector). The t-SOT framework generates transcripts in chronological order using a special separator token ⟨cc⟩ to distinguish speakers, while t-vector provides token-level speaker representations for online clustering. The system uses model-based token emission times from Viterbi alignment as a proxy for word-level timestamps. A new evaluation dataset (DiariST-AliMeeting) and metrics (SAgBLEU and SAtBLEU) are introduced to assess ST quality while accounting for SD accuracy. The system is pre-trained on multilingual data and fine-tuned on the DiariST-AliMeeting dataset.

## Key Results
- Achieved SAgBLEU of 15.5 on IHM-CA T condition of DiariST-AliMeeting dataset
- Outperformed Whisper-based offline ST-then-SD baseline by 2.2 BLEU points and SD-then-ST baseline by 2.7 points on SAtBLEU
- Effectively handled overlapping speech scenarios in IHM-MIX and SDM conditions
- Demonstrated streaming capability while maintaining competitive accuracy with offline systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level serialized output training (t-SOT) enables streaming multi-talker speech recognition by generating transcripts in chronological order using a special separator token ⟨cc⟩ to distinguish speakers.
- Mechanism: The model is trained to output all tokens in order of their end times, inserting ⟨cc⟩ whenever the speaker changes. During inference, the model deserializes the output by splitting on ⟨cc⟩ to recover individual speaker transcripts.
- Core assumption: Even though word-level timestamps are not available for speech translation, model-based token emission times obtained via Viterbi alignment from a pre-trained single-talker ST model can serve as a proxy for chronological ordering.
- Evidence anchors:
  - [abstract] "It is built upon a neural transducer-based streaming ST system and integrates token-level serialized output training and t-vector, which were originally developed for multi-talker speech recognition."
  - [section] "In the t-SOT framework, given audio with multiple active speakers, the ASR model is trained to generate the transcriptions of all speakers in chronological order based on the end time of each token. To distinguish the transcriptions of different speakers in the regions of overlapping speech, a special separator symbol ⟨cc⟩ is inserted when the adjacent tokens belong to different speakers."
  - [corpus] No direct corpus evidence for t-SOT effectiveness in ST; however, related papers mention t-SOT in multi-talker ASR context.
- Break condition: If the model-based token emission times from Viterbi alignment are inaccurate or inconsistent across speakers, the chronological ordering will be incorrect, leading to garbled transcripts.

### Mechanism 2
- Claim: Token-level speaker embeddings (t-vector) enable streaming speaker diarization by generating speaker representations at the token level, which can be clustered online without requiring word-level timestamps.
- Mechanism: A speaker encoder processes frame-level features in parallel with the ASR encoder, while a speaker decoder generates a t-vector embedding for each token based on the encoder output and target token embedding. These t-vectors are then fed into an online clustering algorithm for diarization.
- Core assumption: The token-level speaker embeddings capture sufficient speaker identity information even in overlapping speech regions, and the online clustering can adapt to streaming conditions.
- Evidence anchors:
  - [abstract] "It is built upon a neural transducer-based streaming ST system and integrates token-level serialized output training and t-vector, which were originally developed for multi-talker speech recognition."
  - [section] "The t-vector model comprises a speaker encoder and a speaker decoder... During inference, t-vectors are estimated in a streaming fashion, in parallel with token estimation. These t-vectors are then fed into the online clustering algorithm for speaker diarization."
  - [corpus] Related papers mention t-vector in streaming multi-talker ASR context, supporting its applicability to ST.
- Break condition: If the t-vector embeddings are not discriminative enough to distinguish speakers, or if the online clustering algorithm cannot adapt quickly enough to speaker changes, diarization errors will occur.

### Mechanism 3
- Claim: The proposed evaluation metrics, speaker-agnostic BLEU (SAgBLEU) and speaker-attributed BLEU (SAtBLEU), provide a comprehensive assessment of speech translation quality while accounting for speaker diarization errors.
- Mechanism: SAgBLEU computes BLEU score without considering speaker identity, simply concatenating all reference and hypothesis utterances. SAtBLEU computes BLEU score while considering speaker identity, finding the speaker permutation that maximizes the BLEU score between speaker-wise hypotheses and references.
- Core assumption: These metrics can effectively capture the impact of speaker diarization errors on translation quality, and the speaker permutation search can handle cases where the number of predicted and reference speakers do not match.
- Evidence anchors:
  - [abstract] "We also propose new metrics, called speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality while taking SD accuracy into account."
  - [section] "SAgBLEU computes the BLEU score without considering the SD errors... SAtBLEU computes the BLEU score while considering the SD errors."
  - [corpus] No direct corpus evidence for the effectiveness of these metrics; however, the paper reports results using them.
- Break condition: If the speaker permutation search is computationally expensive or fails to find the optimal permutation, SAtBLEU may not accurately reflect the impact of diarization errors on translation quality.

## Foundational Learning

- Concept: Neural transducer architecture
  - Why needed here: The neural transducer is the backbone of the streaming ST system, enabling low-latency inference while naturally handling word reordering.
  - Quick check question: How does the neural transducer differ from attention-based encoder-decoder models in terms of latency and word reordering capabilities?

- Concept: Token-level speaker embeddings
  - Why needed here: T-vector provides a way to generate speaker representations at the token level, which is crucial for streaming speaker diarization without requiring word-level timestamps.
  - Quick check question: What are the key components of the t-vector model, and how do they interact with the ASR encoder during inference?

- Concept: Serialized output training
  - Why needed here: T-SOT enables the model to handle overlapping speech by generating transcripts in chronological order using a special separator token.
  - Quick check question: How does the model obtain model-based token emission times for training when word-level timestamps are not available in ST?

## Architecture Onboarding

- Component map: Feature extraction -> ST model (Conformer transducer) -> T-vector model (Res2Net encoder + LSTM decoder) -> Online clustering -> Output translation with speaker attribution

- Critical path: Feature extraction → ST model inference → T-vector model inference → Online clustering → Output translation with speaker attribution

- Design tradeoffs: Streaming vs. offline: Streaming allows for real-time processing but may sacrifice some accuracy compared to offline systems. Token-level vs. utterance-level diarization: Token-level diarization provides finer granularity but may be more sensitive to errors in the t-vector embeddings.

- Failure signatures: High SAtBLEU but low SAgBLEU: Indicates that the model is generating accurate translations but struggling with speaker diarization. Low SAtBLEU and low SAgBLEU: Indicates that the model is struggling with both translation and diarization.

- First 3 experiments:
  1. Evaluate the streaming ST model on the IHM-CA T condition of the DiariST-AliMeeting dataset using SAgBLEU to establish a baseline.
  2. Evaluate the t-SOT ST model on the IHM-MIX condition using SAgBLEU to assess the impact of handling overlapping speech.
  3. Evaluate the t-SOT ST + t-vector system on the SDM condition using SAtBLEU to assess the overall system performance in the most challenging scenario.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed streaming ST and SD system compare to offline ST-then-SD systems in terms of SAtBLEU when handling overlapping speech with more than two speakers simultaneously?
- Basis in paper: [explicit] The paper mentions that t-SOT ST achieved significantly better SAgBLEU for IHM-MIX and SDM conditions which contain overlapping speech, but does not specifically test scenarios with more than two simultaneous speakers.
- Why unresolved: The evaluation dataset and experiments focused on up to two active speakers at the same time frame, leaving the performance on more complex overlapping scenarios untested.
- What evidence would resolve it: Additional experiments testing the system with artificially created or naturally occurring recordings containing three or more simultaneous speakers, comparing SAtBLEU scores with offline ST-then-SD systems.

### Open Question 2
- Question: What is the impact of using different speaker embedding extraction methods (d-vector vs t-vector) on the streaming SD performance when combined with 1spk ST versus t-SOT ST models?
- Basis in paper: [explicit] The paper compares d-vector and t-vector combinations with both 1spk ST and t-SOT ST, showing t-vector is more effective with t-SOT ST, but does not explore why this combination works better or test additional embedding methods.
- Why unresolved: While the paper demonstrates performance differences, it does not investigate the underlying reasons for the effectiveness of t-vector with t-SOT ST or compare with other potential speaker embedding approaches.
- What evidence would resolve it: Detailed ablation studies analyzing the contribution of different embedding methods to SD accuracy, including comparison with alternative speaker embedding architectures and analysis of their interaction with different ST model types.

### Open Question 3
- Question: How would the streaming ST and SD system perform on languages other than Mandarin Chinese and English, particularly those with different phonetic and grammatical structures?
- Basis in paper: [inferred] The paper mentions multilingual pre-training data containing 14 languages, but all evaluation was performed on Mandarin Chinese to English translation, leaving performance on other language pairs unexplored.
- Why unresolved: The effectiveness of the t-SOT and t-vector approach on different language pairs is unknown, particularly for languages with significant structural differences from Chinese and English.
- What evidence would resolve it: Extensive experiments testing the system on various language pairs from the multilingual pre-training set, including languages with different word orders, morphologies, and phonetic characteristics, with evaluation using appropriate metrics for each language pair.

## Limitations

- Limited evaluation to Mandarin-Chinese to English translation, restricting generalizability to other language pairs
- Reliance on model-based token emission times introduces potential cascading errors from ST model alignment inaccuracies
- Computational requirements for real-time processing with t-vector model are substantial and not fully characterized

## Confidence

**High Confidence**: The core mechanism of combining neural transducer architecture with t-SOT for streaming multi-talker ST is well-established in the literature, with clear implementation details provided. The evaluation metrics (SAgBLEU and SAtBLEU) are logically sound and appropriately designed to measure both translation quality and diarization accuracy.

**Medium Confidence**: The effectiveness of model-based token emission times as a proxy for word-level timestamps is plausible but relies on the assumption that the pre-trained ST model's alignments are sufficiently accurate. The t-vector model's performance in streaming conditions is supported by related work, but specific implementation details for online clustering could affect real-world performance.

**Low Confidence**: The generalization of results to other language pairs and domains beyond Mandarin-English is uncertain, as the evaluation is limited to a single dataset. The computational efficiency of the full system under strict real-time constraints is not thoroughly characterized.

## Next Checks

1. **Error Propagation Analysis**: Conduct an ablation study to quantify how errors in model-based token emission times propagate through the t-SOT framework and affect final translation quality. Compare against ground-truth timestamped data when available.

2. **Cross-Lingual Generalization**: Implement and evaluate the DiariST system on an additional language pair (e.g., English-Spanish) using a publicly available conversational dataset to assess cross-lingual performance and identify potential language-specific challenges.

3. **Real-Time Processing Evaluation**: Measure the system's end-to-end latency and computational requirements under various streaming conditions, including different numbers of speakers and levels of overlapping speech, to verify its practical applicability in real-world scenarios.