---
ver: rpa2
title: On genuine invariance learning without weight-tying
arxiv_id: '2308.03904'
source_url: https://arxiv.org/abs/2308.03904
tags:
- invariance
- group
- learning
- learned
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how neural networks learn invariance to transformations
  without architectural weight-tying. The authors show that networks can achieve good
  accuracy and low invariance error without genuinely invariant decision-making, leading
  to unreliable performance under distribution shift.
---

# On genuine invariance learning without weight-tying

## Quick Facts
- arXiv ID: 2308.03904
- Source URL: https://arxiv.org/abs/2308.03904
- Reference count: 6
- Primary result: Networks can achieve high accuracy with spurious invariance without genuine invariance, leading to unreliable performance under distribution shift.

## Executive Summary
This paper investigates how neural networks learn invariance to transformations without explicit architectural weight-tying. The authors demonstrate that data augmentation alone often results in spurious invariance where the network achieves good accuracy but does not genuinely learn invariant decision-making. This leads to poor performance under distribution shift. The paper proposes metrics to quantify genuine invariance (logit invariance error, saliency invariance similarity) and a regularization method to align learned invariance with genuine invariance. Experiments on MNIST and FMNIST with rotation and translation groups show that invariance regularization improves invariance quality but reduces accuracy due to a spectral decay effect, where the network reduces overall sensitivity to input perturbations.

## Method Summary
The paper proposes a 5-layer MLP with ReLU activations and 128 hidden units to study invariance learning. The group-invariant variant uses weight-tying only in the first layer with pooling over the group. The authors introduce three metrics to quantify invariance: predictive distribution invariance error (DIf) using KL divergence between softmax outputs under transformations, logit invariance error (LIf) using squared L2 distance between raw logits, and saliency invariance similarity (SIf) using cosine similarity between saliency maps. Invariance regularization is implemented by minimizing the logit invariance error with a tuned weighting to achieve SIf≥0.95. The models are trained with Adam (lr=0.0008, batch=512) for 300 epochs on MNIST and FMNIST with R2_4 rotation and T2_3 translation groups.

## Key Results
- Data-augmented models achieve high accuracy but fail to learn genuine invariance, showing high LIf and low SIf.
- Invariance regularization improves invariance quality but reduces accuracy due to spectral decay effect.
- Models with invariance regularization exhibit reduced overall sensitivity to input perturbations as indicated by smaller σmax(J).
- The trade-off between learning genuine invariance and downstream task performance highlights limitations of data-driven invariance learning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Networks learn spurious invariance by mapping different orientations to different latent points that still fall within the correct class boundary.
- Mechanism: During training with data augmentation, the network learns separate feature mappings for each transformation, as long as the final classification decision remains correct.
- Core assumption: The classification loss alone does not incentivize genuine invariance unless explicitly regularized.
- Evidence anchors:
  - [abstract] "the underlying decision-making in such a model does not attain genuine invariance"
  - [section 4.2] "We observe that the models trained with data augmentation fail to learn genuine group invariance as indicated by high logit invariance error LIf and lower saliency similarity score SIf"
- Break condition: If the data distribution shifts such that the class boundaries learned for different orientations no longer align, the spurious invariance fails.

### Mechanism 2
- Claim: Invariance regularization reduces overall network sensitivity to any input perturbation, not just the targeted transformation group.
- Mechanism: Minimizing the logit invariance error implicitly constrains the spectral norm of the weight matrices, reducing the maximum singular value.
- Core assumption: The gradient flow analysis showing non-increasing spectral norm holds for the network architectures used.
- Evidence anchors:
  - [section 3.3] "Logit invariance error minimization implies σmax(W (t)) ≤ σmax(W (0)) when t → ∞"
  - [section 4.4] "Models trained with invariance regularization come with overall reduced sensitivity to input perturbations as indicated by considerably smaller σmax(J)"
- Break condition: If the regularization weight is too low, the spectral decay effect is insufficient to achieve genuine invariance; if too high, task performance collapses.

### Mechanism 3
- Claim: Saliency maps reveal the decision-making process and show that regularized models make genuinely invariant decisions, while data-augmented models use transformation-dependent features.
- Mechanism: By comparing saliency maps across transformations, one can quantify whether the network focuses on the same features regardless of orientation.
- Core assumption: Saliency maps accurately reflect the network's decision-making process and are comparable across transformations.
- Evidence anchors:
  - [abstract] "saliency invariance similarity to measure the consistency of network's decisions under group transformations"
  - [section 4.2] "saliency maps of the model with invariance regularization are well-aligned over the group orbit"
- Break condition: If saliency attribution methods are unreliable for the network architecture, the metric may not reflect true invariance.

## Foundational Learning

- Concept: Group theory and group actions
  - Why needed here: The paper frames invariance learning in terms of group symmetry, so understanding groups, orbits, and group actions is essential to follow the analysis.
  - Quick check question: What is the group orbit of an element x under a group action G?

- Concept: Invariance vs equivariance
  - Why needed here: The paper distinguishes between genuine invariance (weight-tying) and learned invariance, and discusses equivariant networks as a baseline.
  - Quick check question: How does equivariance differ from invariance in the context of neural networks?

- Concept: Spectral norm and singular values
  - Why needed here: The spectral decay phenomenon is analyzed using the maximum singular value of weight matrices as a measure of sensitivity.
  - Quick check question: Why does minimizing the logit invariance error constrain the spectral norm of the weight matrices?

## Architecture Onboarding

- Component map: Input → Group transformation → Network (with or without weight-tying) → Logits → Softmax → Classification
- Critical path: The input image undergoes group transformation, passes through the 5-layer MLP, and produces logits that are converted to probabilities via softmax for classification.
- Design tradeoffs: Weight-tying enforces genuine invariance but is computationally expensive and requires architectural changes. Learning invariance from data is cheaper but may result in spurious invariance. Regularization can promote genuine invariance but reduces task performance.
- Failure signatures: High logit invariance error despite good classification accuracy indicates spurious invariance. Low saliency invariance similarity suggests transformation-dependent decision-making.
- First 3 experiments:
  1. Train a baseline model with data augmentation and evaluate logit invariance error, saliency invariance similarity, and accuracy on both in-distribution and out-of-distribution data.
  2. Train a model with invariance regularization and compare the same metrics to the baseline.
  3. Analyze the spectral norm of the weight matrices during training for both models to observe the spectral decay effect.

## Open Questions the Paper Calls Out

- Question: How can we learn genuine invariance without requiring prior knowledge of the transformation group?
  - Basis in paper: [explicit] The paper mentions this as a limitation in the Discussion section, stating that "the way we design invariance regularization assumes a known group of transformations, which may not always be accessible in practice."
  - Why unresolved: Current methods for promoting genuine invariance rely on knowing the specific transformation group in advance. Without this prior knowledge, it's unclear how to design effective regularization or architectural modifications.
  - What evidence would resolve it: A method that can learn invariance to unknown transformations without requiring architectural modifications or prior knowledge of the transformation group. This could be demonstrated on datasets with unknown or complex symmetries.

- Question: What is the relationship between learned invariance and other inductive biases like disentanglement or causal structure?
  - Basis in paper: [inferred] The paper discusses learned invariance as a key inductive bias, but doesn't explore its relationship to other forms of inductive bias learning.
  - Why unresolved: While the paper focuses specifically on invariance, neural networks learn multiple forms of inductive bias simultaneously. The interplay between invariance learning and other inductive biases remains unexplored.
  - What evidence would resolve it: Studies examining how invariance learning interacts with other forms of inductive bias learning, such as disentanglement or causal structure learning. This could involve analyzing the relationship between invariance metrics and metrics for other inductive biases.

- Question: How does the accuracy-invariance trade-off change with different network architectures and training regimes?
  - Basis in paper: [explicit] The paper identifies this trade-off in the Discussion section, stating "our results also highlight a trade-off between learning the genuine invariance and the downstream task performance."
  - Why unresolved: The paper only examines this trade-off in relatively simple networks (5-layer perceptrons). Different architectures (like ResNets or Transformers) or training regimes (like self-supervised learning) might exhibit different trade-offs.
  - What evidence would resolve it: Empirical studies comparing the accuracy-invariance trade-off across different network architectures and training regimes. This could include analyzing how the trade-off scales with model size or changes with different optimization methods.

## Limitations
- The conclusions about the limitations of data-driven invariance learning are based on specific experimental conditions (MNIST/FMNIST with limited transformation groups) and may not generalize to more complex datasets or transformation sets.
- The saliency map analysis assumes that these attributions accurately reflect the network's decision-making process, but the validity of this assumption depends on the specific saliency method and network architecture used.
- The spectral decay effect, while effective for promoting invariance, simultaneously reduces the network's overall sensitivity, limiting its representational capacity.

## Confidence
- High confidence: The mathematical framework for quantifying invariance (DIf, LIf, SIf metrics) and the spectral norm analysis are theoretically sound and well-supported by the evidence.
- Medium confidence: The empirical demonstration of spurious invariance in data-augmented models and the effectiveness of invariance regularization are convincing within the experimental scope but may not generalize to more complex scenarios.
- Low confidence: The broader claims about the limitations of data-driven invariance learning and the necessity of architectural solutions (like weight-tying) for achieving genuine invariance require further validation on more diverse datasets and transformation groups.

## Next Checks
1. **Generalization to complex datasets**: Validate the findings on CIFAR-10/100 or ImageNet with a wider range of transformations (e.g., affine, projective) to assess the robustness of the conclusions.
2. **Alternative regularization methods**: Explore regularization techniques that promote invariance without causing significant spectral decay, such as group-equivariant convolutions or adaptive regularization schemes.
3. **Invariance under realistic distribution shifts**: Evaluate the models' performance under realistic distribution shifts (e.g., style transfer, occlusion, adversarial examples) to better understand the practical implications of spurious invariance.