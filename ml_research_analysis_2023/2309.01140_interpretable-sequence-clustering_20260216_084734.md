---
ver: rpa2
title: Interpretable Sequence Clustering
arxiv_id: '2309.01140'
source_url: https://arxiv.org/abs/2309.01140
tags:
- clustering
- tree
- data
- sequence
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an interpretable clustering method for categorical
  sequences called Interpretable Sequence Clustering Tree (ISCT). The main problem
  addressed is the lack of interpretability in existing sequence clustering algorithms,
  which makes it difficult to understand why sequences are grouped into particular
  clusters.
---

# Interpretable Sequence Clustering

## Quick Facts
- arXiv ID: 2309.01140
- Source URL: https://arxiv.org/abs/2309.01140
- Reference count: 40
- Primary result: ISCT achieves comparable or better clustering performance than state-of-the-art methods while providing interpretable tree structures

## Executive Summary
This paper introduces ISCT (Interpretable Sequence Clustering Tree), a method that addresses the interpretability gap in sequence clustering algorithms. Traditional clustering methods often produce cluster assignments without explaining why sequences are grouped together. ISCT solves this by constructing a decision tree where each node splits based on the presence or absence of discriminative sequential patterns. The method uses random projection of sequences into subspaces, followed by k-means clustering, and then builds an interpretable tree by mining and selecting top discriminative patterns. Experiments on 14 real-world datasets demonstrate that ISCT achieves competitive performance in purity, NMI, and F1-score while providing clear, human-understandable tree structures.

## Method Summary
ISCT combines random subspace projection with k-means clustering to obtain initial cluster assignments, then constructs a decision tree using discriminative sequential patterns mined from each cluster. The method first projects sequences into random subspaces using randomly generated sequential patterns, applies LCS similarity to transform sequences into feature vectors, and performs dimensionality reduction with PCA. Initial k-means clustering provides the foundation for tree construction, where the algorithm iteratively mines the top-1 discriminative pattern for each cluster based on relative risk and internal similarity measures. A boosting strategy is employed during tree construction, where each node re-partitions the current sequence subset using the random projection approach to improve consistency with the original clustering results.

## Key Results
- ISCT achieves comparable or better clustering performance than state-of-the-art methods across 14 real-world datasets
- The method provides interpretable tree structures where each path corresponds to a sequence of pattern presence/absence decisions
- The boosting strategy during tree construction improves consistency with original clustering results compared to direct construction methods

## Why This Works (Mechanism)

### Mechanism 1
Random projection of sequences into lower-dimensional subspaces enables efficient k-means clustering by converting categorical sequences into feature vectors. The method uses randomly generated sequential patterns to transform sequences via LCS similarity, applies PCA for dimensionality reduction, then performs k-means on the transformed data. The core assumption is that random projection captures sufficient discriminative information for effective clustering.

### Mechanism 2
The boosting strategy during tree construction improves consistency between the interpretable tree and original clustering results. At each node, the algorithm re-partitions the current sequence subset using random projection + k-means before mining the top-1 discriminative pattern. This re-clustering helps correct inconsistencies that arise when using a single global clustering to guide the entire tree construction.

### Mechanism 3
Using discriminative sequential patterns as splitting criteria creates an interpretable tree structure that humans can understand. The algorithm mines top-1 discriminative patterns based on relative risk and internal similarity measures. Sequences containing the pattern go to one branch, those without to the other, creating a binary tree where each path represents pattern presence/absence decisions.

## Foundational Learning

- Concept: Longest Common Subsequence (LCS) similarity measure
  - Why needed here: Used to compute distances between random patterns and sequences during random projection step
  - Quick check question: How does LCS similarity differ from edit distance when comparing two sequences?

- Concept: Relative risk as a discriminative measure
  - Why needed here: Used to identify the top-1 discriminative pattern for each cluster during tree construction
  - Quick check question: Why is relative risk preferred over simple frequency when measuring pattern discriminative power?

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: Applied after random projection to reduce dimensionality of feature vectors before k-means clustering
  - Quick check question: What is the main benefit of applying PCA after random projection in this context?

## Architecture Onboarding

- Component map: Random pattern generator -> LCS similarity calculator -> PCA module -> k-means clusterer -> Pattern miner -> Top-1 pattern selector -> Tree builder
- Critical path: Random projection → k-means clustering → Pattern mining → Top-1 pattern selection → Tree construction (with boosting)
- Design tradeoffs:
  - Random pattern generation vs. frequent pattern mining: Random patterns provide diversity but may miss important patterns; frequent patterns are more targeted but may be redundant
  - Number of random patterns: More patterns capture more information but increase computational cost
  - Pattern length: Longer patterns are more discriminative but less frequent and harder to mine
  - Re-clustering frequency: More frequent re-clustering improves consistency but increases computational cost
- Failure signatures:
  - Poor initial clustering quality → Inconsistent tree structure
  - Too many similar patterns → Overfitting to noise
  - Patterns that don't generalize → Poor performance on test data
  - Tree depth too large → Loss of interpretability
- First 3 experiments:
  1. Run ISCT on a small synthetic dataset with known cluster structure to verify tree interpretability
  2. Compare ISCT performance with and without the boosting strategy on a medium-sized dataset
  3. Test different numbers of random patterns (e.g., 512, 1024, 2048) on clustering quality and runtime

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ISCT compare to other interpretable clustering methods for non-sequential data when applied to sequential data? The paper focuses on interpretable clustering for sequential data and compares ISCT to other sequence clustering methods, but does not explicitly compare to interpretable clustering methods for non-sequential data when applied to sequential data.

### Open Question 2
How does the choice of the number of random projections and the length of the subsequences used in the random projection step affect the performance of ISCT? The paper mentions that the number of random projections and the maximum pattern length are set to default values (2048 and 5, respectively) but does not explore the impact of varying these parameters.

### Open Question 3
Can the interpretability of ISCT be further improved by incorporating additional information, such as domain knowledge or feature importance, into the tree construction process? The paper focuses on constructing an interpretable tree based on discriminative sequential patterns but does not explore the potential benefits of incorporating additional information into the tree construction process.

## Limitations
- Computational scalability may be limited by the LCS-based random projection approach, which can be expensive for large datasets
- The boosting strategy's effectiveness depends heavily on the quality of re-clustering at each node, which may introduce instability
- Interpretability assumes users can meaningfully understand and use pattern-based splits, which may not hold for very long or complex patterns

## Confidence
- High confidence: The core mechanism of using discriminative patterns for interpretable tree construction is well-supported by experimental results showing competitive clustering performance across 14 datasets
- Medium confidence: The boosting strategy's improvement over direct construction is demonstrated but could benefit from more rigorous ablation studies
- Medium confidence: The interpretability claim is supported by the tree structure but lacks user studies validating actual human comprehension

## Next Checks
1. Conduct runtime complexity analysis on datasets of increasing size to determine scalability limits of the LCS-based random projection step
2. Perform ablation studies comparing ISCT with and without the boosting strategy across all 14 datasets to quantify its contribution
3. Design a small user study where participants interpret sample trees generated by ISCT and assess their ability to explain cluster assignments using the pattern splits