---
ver: rpa2
title: Context-faithful Prompting for Large Language Models
arxiv_id: '2303.11315'
source_url: https://arxiv.org/abs/2303.11315
tags:
- llms
- knowledge
- context
- prompts
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of context-faithfulness in large
  language models (LLMs), where models tend to rely on their parametric knowledge
  rather than the provided context, leading to incorrect predictions. The authors
  propose two methods to improve context faithfulness: opinion-based prompts and counterfactual
  demonstrations.'
---

# Context-faithful Prompting for Large Language Models

## Quick Facts
- arXiv ID: 2303.11315
- Source URL: https://arxiv.org/abs/2303.11315
- Reference count: 26
- Primary result: Proposed methods reduce memorization ratio of text-davinci-003 from 35.2% to 3.0% on natural questions

## Executive Summary
This paper addresses the problem of context-faithfulness in large language models, where models tend to rely on their parametric knowledge rather than provided context, leading to incorrect predictions. The authors propose two methods to improve context faithfulness: opinion-based prompts and counterfactual demonstrations. Opinion-based prompts reframe questions as opinion-seeking rather than fact-seeking, forcing models to prioritize context. Counterfactual demonstrations use instances with false facts to improve faithfulness in knowledge conflict situations. The methods are evaluated on three datasets and show significant improvements in faithfulness to contexts, with the combination of both methods being most effective.

## Method Summary
The paper proposes two methods to improve context faithfulness in large language models without additional training. Opinion-based prompts transform fact-seeking questions into opinion-seeking questions by framing the context as a narrator's statement and asking about the narrator's opinion. Counterfactual demonstrations involve using instances containing false facts as demonstrations to improve faithfulness in knowledge conflict situations. The authors evaluate these methods on three datasets using memorization ratio, exact match, F1 score, accuracy, and Brier score as metrics.

## Key Results
- Memorization ratio of text-davinci-003 reduced from 35.2% to 3.0% on natural questions
- Opinion-based prompts combined with counterfactual demonstrations show the most significant improvements
- Methods effectively reduce reliance on parametric knowledge while maintaining or improving performance on answerable questions
- Significant improvements observed across all three tested datasets (Re-TACRED, natural questions, RealTime QA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Opinion-based prompts increase context faithfulness by reframing questions from fact-seeking to opinion-seeking, forcing LLMs to prioritize the provided context over their parametric knowledge.
- Core assumption: LLMs are sensitive to question framing and will adjust reliance on parametric knowledge based on whether the question seeks fact or opinion.
- Evidence: "Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions" [abstract]

### Mechanism 2
- Claim: Counterfactual demonstrations improve faithfulness in knowledge conflict scenarios by providing examples that explicitly conflict with the model's parametric knowledge.
- Core assumption: LLMs can learn to adjust predictions based on in-context examples without additional training.
- Evidence: "Counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations" [abstract]

### Mechanism 3
- Claim: Instructed prompts with natural language instructions improve context faithfulness by explicitly directing the model to use the provided context.
- Core assumption: LLMs are responsive to explicit instructions in prompts and will adjust behavior accordingly.
- Evidence: "We also explicitly instruct LLMs to read context by natural language" [abstract]

## Foundational Learning

- Concept: Knowledge Conflict
  - Why needed here: Understanding knowledge conflict is crucial because it's the main scenario where LLMs fail to be context-faithful.
  - Quick check question: What is knowledge conflict in LLMs, and why is it a problem for context faithfulness?

- Concept: Prompt Engineering
  - Why needed here: The paper relies heavily on different prompting strategies to improve context faithfulness.
  - Quick check question: What are the different prompting strategies proposed, and how do they aim to improve context faithfulness?

- Concept: In-Context Learning
  - Why needed here: The paper uses in-context learning (demonstrations) to improve faithfulness without additional training.
  - Quick check question: How do in-context demonstrations work, and why are counterfactual demonstrations more effective than factual ones in knowledge conflict scenarios?

## Architecture Onboarding

- Component map: Prompt templates (opinion-based, instructed, counterfactual) -> LLMs (text-davinci-003) -> Datasets (Re-TACRED, natural questions, RealTime QA) -> Evaluation metrics (MR, EM, F1, accuracy, Brier score)

- Critical path: 1. Design prompt templates, 2. Generate counterfactual instances, 3. Evaluate LLMs on datasets, 4. Analyze results

- Design tradeoffs: Opinion-based vs. instructed prompts (effectiveness vs. phrasing difficulty); counterfactual vs. factual demonstrations (effectiveness in conflict vs. data generation requirements)

- Failure signatures: High memorization ratio despite opinion-based prompts; low accuracy on answerable questions

- First 3 experiments:
  1. Compare memorization ratios using base prompts vs. opinion-based prompts on natural questions
  2. Evaluate counterfactual vs. factual demonstrations on Re-TACRED
  3. Test combination of opinion-based prompts and counterfactual demonstrations on RealTime QA for prediction with abstention

## Open Questions the Paper Calls Out
- How do the proposed methods perform on a broader range of NLP tasks beyond machine reading comprehension and relation extraction?
- How do the proposed methods compare to other techniques for improving faithfulness in large language models?
- How do the proposed methods affect performance on instances without knowledge conflicts?

## Limitations
- Implementation details and code are not provided, making exact replication challenging
- Evaluation focuses on three datasets that may not generalize to all knowledge conflict scenarios
- Does not address potential performance degradation when parametric knowledge is actually correct

## Confidence
- High confidence: Core observation that LLMs struggle with context faithfulness in knowledge conflict scenarios is well-established
- Medium confidence: Effectiveness of proposed methods appears promising but lacks independent verification
- Low confidence: Claimed mechanisms are largely intuitive without rigorous empirical validation

## Next Checks
1. Conduct ablation study of opinion-based prompting to identify specific linguistic cues driving improvements
2. Apply methods to datasets outside tested domains to determine generalizability
3. Measure ground truth accuracy in knowledge conflict cases to quantify tradeoff between faithfulness and overall accuracy