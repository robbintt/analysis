---
ver: rpa2
title: 'Large Language Models on Wikipedia-Style Survey Generation: an Evaluation
  in NLP Concepts'
arxiv_id: '2308.10410'
source_url: https://arxiv.org/abs/2308.10410
tags:
- survey
- gpt-4
- information
- section
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the performance of Large Language Models
  (LLMs), specifically GPT-3.5 and GPT-4, on generating Wikipedia-style survey articles
  in the field of Natural Language Processing (NLP). The study compares zero-shot,
  one-shot, and prompt-enhanced settings using automated metrics (ROUGE) and human
  evaluation across six dimensions: readability, relevancy, redundancy, hallucination,
  completeness, and factuality.'
---

# Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts

## Quick Facts
- arXiv ID: 2308.10410
- Source URL: https://arxiv.org/abs/2308.10410
- Reference count: 1
- GPT-4 outperforms GPT-3.5 in generating Wikipedia-style survey articles with prompt-enhanced one-shot setting achieving best results

## Executive Summary
This paper evaluates the performance of Large Language Models (LLMs), specifically GPT-3.5 and GPT-4, on generating Wikipedia-style survey articles in the field of Natural Language Processing (NLP). The study compares zero-shot, one-shot, and prompt-enhanced settings using automated metrics (ROUGE) and human evaluation across six dimensions: readability, relevancy, redundancy, hallucination, completeness, and factuality. GPT-4 outperformed GPT-3.5 and achieved best results with the prompt-enhanced one-shot setting, demonstrating strong performance in readability, relevancy, redundancy, and hallucination detection, but showed weaknesses in completeness and factuality.

## Method Summary
The study uses the Surfer100 dataset containing 100 manually written surveys on 99 NLP concepts, with 20 selected concepts for evaluation. The researchers compare GPT-3.5 and GPT-4 across three prompt settings: zero-shot, one-shot, and prompt-enhanced. They generate survey articles for each concept and evaluate them using ROUGE scores against ground truth and human evaluation across six dimensions. The prompt-enhanced setting provides detailed descriptions for each section to guide the model's output structure.

## Key Results
- GPT-4 surpasses GPT-3.5 in automated ROUGE metrics by 2-20% across all settings
- Prompt-enhanced one-shot setting yields best results for GPT-4
- GPT-4-generated surveys can be more contemporary and informative than human-authored ones
- GPT-4 shows strong performance in readability, relevancy, redundancy, and hallucination detection but weaknesses in completeness and factuality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 in generating Wikipedia-style survey articles when evaluated against human-written ground truth.
- Mechanism: GPT-4's superior performance stems from its enhanced language understanding and generation capabilities, which allow it to produce more relevant, readable, and less redundant content compared to GPT-3.5.
- Core assumption: The evaluation metrics (ROUGE scores and human evaluation) accurately capture the quality differences between the two models' outputs.
- Evidence anchors:
  - [abstract]: "Automated benchmarks reveal that GPT-4 surpasses its predecessors, including GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth."
  - [section]: "In the first group, we compare GPT-3.5 and 4 in zero- and one-shot settings. One may notice that GPT-4 is, in general, better than GPT-3.5."
- Break condition: If the evaluation metrics or human judgment criteria are flawed or biased, the claimed superiority of GPT-4 might not hold.

### Mechanism 2
- Claim: Prompt-enhanced one-shot setting yields the best results for GPT-4 in generating survey articles.
- Mechanism: Providing a detailed description for each section as the instruction helps GPT-4 understand the specific requirements of each section, leading to more structured and relevant content.
- Core assumption: The prompt-enhanced setting effectively guides the model to produce content that aligns with the desired format and quality.
- Evidence anchors:
  - [abstract]: "GPT-4 outperformed GPT-3.5 and achieved best results with the prompt-enhanced one-shot setting."
  - [section]: "We add the description prompt and compare the effect of the one-shot instruction. With these descriptions, GPT-4 is not sensitive to the one-shot setting."
- Break condition: If the prompt-enhanced setting does not provide clear enough guidance, or if the model fails to interpret the instructions correctly, the expected improvement may not materialize.

### Mechanism 3
- Claim: GPT-4-generated surveys can be more contemporary and informative than human-authored ones.
- Mechanism: GPT-4 has access to a broader range of up-to-date information and can synthesize this information more effectively, potentially leading to more current and comprehensive survey articles.
- Core assumption: The information available to GPT-4 is more current and relevant than the information used by human authors when creating the ground truth.
- Evidence anchors:
  - [abstract]: "Notably, GPT-4 sometimes generated content that was more contemporary and informative than the ground truth."
  - [section]: "In the second example, Hidden Markov Models (HMMs), the GPT response is more precise and more complete than the ground truth."
- Break condition: If GPT-4's training data is not significantly more up-to-date or comprehensive than the sources used by human authors, this advantage may not hold.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding LLMs is crucial as the paper evaluates their performance in generating survey articles.
  - Quick check question: What are the key differences between GPT-3.5 and GPT-4 in terms of architecture and capabilities?
- Concept: Natural Language Processing (NLP)
  - Why needed here: The study focuses on generating survey articles in the NLP domain, requiring knowledge of NLP concepts and terminology.
  - Quick check question: What are some common NLP tasks and how do they relate to the survey generation task?
- Concept: Evaluation Metrics (ROUGE scores, Human Evaluation)
  - Why needed here: The paper uses various metrics to assess the quality of generated survey articles, necessitating an understanding of these evaluation methods.
  - Quick check question: How do ROUGE scores measure the quality of generated text, and what are their limitations?

## Architecture Onboarding

- Component map: GPT-3.5 -> GPT-4 -> Prompt Settings (zero-shot, one-shot, prompt-enhanced) -> Survey Generation -> Evaluation (ROUGE + Human Evaluation)
- Critical path: Select model and prompt setting → Generate survey article → Compute ROUGE scores → Conduct human evaluation → Analyze results
- Design tradeoffs: Choice between GPT-3.5 and GPT-4 involves balancing performance with computational resources; prompt setting selection balances guidance needs with output diversity
- Failure signatures: Incomplete or inaccurate information in generated surveys; readability or relevance issues; hallucination or factual errors
- First 3 experiments:
  1. Compare GPT-3.5 and GPT-4 performance in zero-shot settings to establish baseline
  2. Test impact of one-shot and prompt-enhanced settings on GPT-4's performance
  3. Conduct detailed analysis of strengths and weaknesses of GPT-4-generated surveys compared to human-authored ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt strategies affect the factual accuracy of LLM-generated survey articles?
- Basis in paper: [explicit] The paper compares zero-shot, one-shot, and prompt-enhanced settings, noting that GPT-4 showed weaknesses in completeness and factuality
- Why unresolved: While the paper identifies that GPT-4 has issues with factuality, it doesn't provide a detailed analysis of how specific prompt strategies impact the frequency and severity of factual errors across different types of NLP concepts
- What evidence would resolve it: A systematic analysis comparing the factual accuracy of survey articles generated using different prompt strategies across various NLP topics, including error classification and frequency analysis

### Open Question 2
- Question: What is the optimal balance between conciseness and completeness in LLM-generated survey articles?
- Basis in paper: [inferred] The paper notes that GPT-4 performed well in readability, relevancy, and redundancy but showed weaknesses in completeness and factuality
- Why unresolved: The study doesn't explore whether there's an optimal trade-off between keeping survey articles concise (50-150 words per section) and ensuring they contain all necessary information
- What evidence would resolve it: Comparative analysis of survey articles with varying levels of detail, measuring expert satisfaction and information retention across different length and detail levels

### Open Question 3
- Question: How do human evaluators' biases affect the assessment of LLM-generated content compared to ground truth?
- Basis in paper: [explicit] The paper mentions that they found "systematic bias in using GPT evaluation" and discusses differences between human and GPT-based evaluation scores
- Why unresolved: The paper identifies bias in evaluation but doesn't provide a detailed analysis of how human evaluators' preconceptions about LLM-generated content might influence their scoring
- What evidence would resolve it: A controlled study comparing evaluation scores when evaluators are told the source of content versus when they are blind to the source, along with analysis of evaluation patterns

## Limitations
- Evaluation methodology lacks full disclosure of human evaluation protocol details including expert selection and scoring rubric
- Dataset characteristics and potential biases of ground truth articles are not thoroughly discussed
- Prompt template specifics across different settings are not fully specified, affecting reproducibility

## Confidence

- High Confidence: GPT-4 outperforms GPT-3.5 in automated ROUGE metrics across all settings
- Medium Confidence: GPT-4's superiority in human evaluation dimensions (readability, relevancy, redundancy, hallucination detection) is supported but limited by incomplete methodology details
- Medium Confidence: The claim about GPT-4 generating more contemporary content than ground truth is based on specific examples but lacks systematic analysis

## Next Checks
1. Obtain and test the exact prompt templates used in zero-shot, one-shot, and prompt-enhanced settings to verify reported performance differences
2. Replicate the human evaluation process with independent experts using the same six dimensions to confirm reliability of qualitative assessments
3. Systematically compare temporal coverage and currency of information between GPT-4-generated surveys and ground truth across all 20 concepts, not just selected examples