---
ver: rpa2
title: 'pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable
  3D Reconstruction'
arxiv_id: '2312.12337'
source_url: https://arxiv.org/abs/2312.12337
tags:
- gaussian
- depth
- neural
- view
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents pixelSplat, a method for generalizable novel
  view synthesis from stereo image pairs. The key idea is to predict a 3D Gaussian
  splatting representation of the scene in a single forward pass.
---

# pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction

## Quick Facts
- arXiv ID: 2312.12337
- Source URL: https://arxiv.org/abs/2312.12337
- Reference count: 40
- Key outcome: Novel view synthesis from stereo pairs using 3D Gaussian splatting with differentiable probabilistic sampling and epipolar scale inference

## Executive Summary
This paper introduces pixelSplat, a method for generalizable novel view synthesis from stereo image pairs. The key innovation is predicting a 3D Gaussian splatting representation in a single forward pass, enabling fast reconstruction and real-time rendering. To handle the inherent scale ambiguity in real-world datasets, pixelSplat uses an epipolar transformer to infer per-scene scale factors. Additionally, it employs a probabilistic depth prediction scheme with differentiable sampling to avoid local minima during Gaussian parameter optimization. Experiments demonstrate that pixelSplat outperforms state-of-the-art light field transformers in reconstruction quality while being significantly faster and more memory-efficient.

## Method Summary
pixelSplat predicts 3D Gaussian primitives from stereo image pairs by first encoding both views into feature volumes, then using an epipolar transformer to establish cross-view correspondences and infer scale. For each pixel, it predicts a discrete probability distribution over depth buckets and samples Gaussian means from this distribution using a reparameterization trick that makes the sampling differentiable. The sampled Gaussians are rendered using 3D splatting, producing novel views. The method is trained end-to-end with a combination of MSE and LPIPS losses, gradually increasing the inter-frame distance during training.

## Key Results
- Outperforms state-of-the-art light field transformers on RealEstate10k and ACID datasets in PSNR, SSIM, and LPIPS metrics
- 2.5 orders of magnitude faster at rendering compared to optimization-based methods
- Significantly lower memory usage while maintaining high-quality reconstructions
- Produces an interpretable and editable 3D radiance field represented by Gaussian primitives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The epipolar transformer resolves per-scene scale ambiguity in real-world datasets.
- Mechanism: By comparing features along corresponding epipolar lines between two views and encoding their depths, the model learns the arbitrary scale factor inherent to structure-from-motion camera poses.
- Core assumption: Cross-view correspondences can be reliably established via epipolar attention, and depth positional encoding provides sufficient information to recover the scale.
- Evidence anchors:
  - [abstract]: "To handle scale ambiguity in real-world datasets, the method uses an epipolar transformer to infer the per-scene scale factor."
  - [section 4.1]: "Our encoder similarly has to predict the geometry of the scene, chiefly via the position of each Gaussian primitive, which depends on the per-scene scale si."
  - [corpus]: Weak - corpus neighbors focus on compression, few-shot synthesis, and delivery, not scale ambiguity resolution.
- Break condition: If epipolar correspondences are unreliable (e.g., low-texture regions), the model cannot accurately infer depth and thus scale.

### Mechanism 2
- Claim: Probabilistic depth prediction via reparameterization sampling avoids local minima in Gaussian parameter optimization.
- Mechanism: Instead of directly regressing Gaussian means, the model predicts a discrete probability distribution over depth buckets. Sampling from this distribution is made differentiable by setting Gaussian opacity equal to the sampled bucket's probability, allowing gradients to flow into the probability parameters.
- Core assumption: The reparameterization trick effectively couples the likelihood of sampling a Gaussian at a location with its opacity, enabling gradient-based optimization to avoid local minima.
- Evidence anchors:
  - [abstract]: "To avoid local minima in optimizing Gaussian parameters, it predicts a dense probability distribution over 3D space and samples Gaussian means from this distribution, making the sampling differentiable via a reparameterization trick."
  - [section 4.2]: "We make the sampling operation differentiable via a reparameterization trick that couples the density of a sampled Gaussian primitive to the probability of that location."
  - [corpus]: Weak - corpus neighbors do not discuss differentiable sampling or local minima avoidance in Gaussian splatting.
- Break condition: If the number of depth buckets is too coarse, the model cannot accurately represent fine depth variations, leading to poor reconstructions.

### Mechanism 3
- Claim: Pixel-aligned Gaussian prediction with shared per-pixel parameters enables efficient and coherent scene reconstruction.
- Mechanism: For each pixel, a neural network predicts the parameters (covariance, spherical harmonics coefficients) of multiple Gaussian primitives, with their locations sampled from the predicted depth distribution. This ties the Gaussian representation to image features, ensuring coherence.
- Core assumption: Pixel-aligned features contain sufficient information to predict the parameters of Gaussian primitives that accurately represent the underlying scene geometry and appearance.
- Evidence anchors:
  - [abstract]: "Our method features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time."
  - [section 4.2]: "In the generalizable case, we need to back-propagate gradients through the representation and thus cannot rely on any non-differentiable operations that spawn or delete Gaussian primitives."
  - [corpus]: Weak - corpus neighbors focus on compression, few-shot synthesis, and delivery, not pixel-aligned Gaussian prediction.
- Break condition: If the pixel features are not sufficiently discriminative, the model cannot accurately predict Gaussian parameters, leading to poor reconstructions.

## Foundational Learning

- Concept: Structure-from-motion (SfM) and scale ambiguity
  - Why needed here: The model operates on real-world datasets where camera poses are reconstructed up to an arbitrary scale factor. Understanding SfM and scale ambiguity is crucial for grasping why the epipolar transformer is necessary.
  - Quick check question: What is the main limitation of camera poses reconstructed by SfM software, and why does this pose a problem for 3D reconstruction from images?

- Concept: Differentiable rendering and local minima in optimization
  - Why needed here: The model uses differentiable rendering to optimize Gaussian parameters. Understanding the concept of local minima and why they are problematic in this context is essential for appreciating the need for the probabilistic sampling approach.
  - Quick check question: What is the main challenge in optimizing Gaussian parameters via gradient descent, and how does the probabilistic sampling approach address this challenge?

- Concept: Reparameterization trick in variational autoencoders
  - Why needed here: The model uses a reparameterization trick to make the sampling of Gaussian means differentiable. Understanding this concept is crucial for grasping how gradients can flow through the sampling operation.
  - Quick check question: How does the reparameterization trick enable gradients to flow through a sampling operation, and why is this important for training the model?

## Architecture Onboarding

- Component map: Two-view image encoder with epipolar transformer -> Pixel-aligned Gaussian prediction module -> 3D Gaussian splatting renderer
- Critical path:
  1. Encode reference views into feature volumes using per-image encoders.
  2. Perform epipolar cross-attention to establish cross-view correspondences and encode depth information.
  3. Predict Gaussian parameters (means, covariances, opacities, spherical harmonics coefficients) from pixel-aligned features.
  4. Sample Gaussian means from the predicted depth distributions.
  5. Render novel views using 3D Gaussian splatting.
- Design tradeoffs:
  - Using a two-view encoder vs. a single-view encoder: The two-view encoder resolves scale ambiguity but requires more computation and memory.
  - Predicting Gaussian parameters probabilistically vs. directly: The probabilistic approach avoids local minima but requires additional complexity in the model and training process.
  - Using pixel-aligned features vs. global features: Pixel-aligned features provide more precise control over Gaussian parameters but may require more parameters and computation.
- Failure signatures:
  - Scale ambiguity not resolved: Ghosting or motion blur artifacts in rendered images.
  - Local minima not avoided: Speckling or missing geometry in rendered images.
  - Pixel features not discriminative enough: Poor reconstruction quality or missing details.
- First 3 experiments:
  1. Train the model without the epipolar transformer and compare the reconstruction quality to the full model.
  2. Train the model without the probabilistic sampling approach and compare the reconstruction quality to the full model.
  3. Train the model with different numbers of depth buckets and analyze the impact on reconstruction quality and training stability.

## Open Questions the Paper Calls Out
- No explicit open questions were identified in the paper.

## Limitations
- Scale ambiguity resolution and local minima avoidance claims lack rigorous ablation studies
- Probabilistic sampling approach adds complexity without comparison to deterministic alternatives
- Limited exploration of depth bucket resolution impact on quality and efficiency

## Confidence
- **High Confidence**: Real-time rendering speed and memory-efficiency claims (directly measured and compared against baselines).
- **Medium Confidence**: PSNR/SSIM/LPIPS improvements over LFT (results shown but without extensive ablations on individual design choices).
- **Low Confidence**: Claims about local-minima avoidance and scale ambiguity resolution (supported by mechanism description but not rigorously isolated or validated).

## Next Checks
1. **Ablation of the epipolar transformer**: Train an otherwise identical model without epipolar cross-attention and measure per-scene scale error and reconstruction quality to directly test scale ambiguity resolution.
2. **Sampling vs. deterministic placement**: Replace the probabilistic depth sampling with a deterministic depth regression head and compare training stability, final PSNR, and visual artifacts to verify local-minima avoidance.
3. **Depth bucket resolution sweep**: Systematically vary the number of depth buckets in the discrete distribution and measure reconstruction quality and training convergence to identify the minimal sufficient resolution.