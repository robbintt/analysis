---
ver: rpa2
title: An energy-based comparative analysis of common approaches to text classification
  in the Legal domain
arxiv_id: '2311.01256'
source_url: https://arxiv.org/abs/2311.01256
tags:
- energy
- consumption
- performance
- classification
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative analysis of Large Language Models
  (LLMs) and traditional approaches (e.g., SVM) for text classification in the legal
  domain, considering both performance and eco-friendly metrics. The authors use the
  LexGLUE benchmark and evaluate models in terms of F1 score, energy consumption,
  costs, and carbon footprint.
---

# An energy-based comparative analysis of common approaches to text classification in the Legal domain

## Quick Facts
- arXiv ID: 2311.01256
- Source URL: https://arxiv.org/abs/2311.01256
- Reference count: 27
- Primary result: SVM-based approaches achieve performance very close to large LLMs but with significantly lower power consumption and resource demands

## Executive Summary
This paper presents a comparative analysis of Large Language Models (LLMs) and traditional approaches for text classification in the legal domain, considering both performance and eco-friendly metrics. The authors use the LexGLUE benchmark and evaluate models in terms of F1 score, energy consumption, costs, and carbon footprint. They simulate both R&D and production scenarios to assess the models' performance and energy requirements. The results indicate that SVM-based approaches, particularly SVMnlp, achieve performance very close to that of large LLMs but with significantly lower power consumption and resource demands. The study suggests that companies should consider additional evaluations beyond performance when selecting ML solutions, taking into account factors such as energy consumption and carbon footprint.

## Method Summary
The study compares five text classification models (BERT, LegalBERT, DistilBERT, SVMbow, and SVMnlp) on seven legal datasets from the LexGLUE benchmark. Models are evaluated using F1 score (both micro and macro averaging), energy consumption (measured via codecarbon), costs, and carbon footprint. The comparison includes both R&D and production scenarios to assess total lifecycle costs. SVMnlp uses enriched linguistic features (lemmas, POS tags, concepts) while SVMbow uses standard bag-of-words features.

## Key Results
- SVMnlp achieves F1 scores very close to BERT-based models across multiple legal datasets
- BERT-based models consume up to 4000 times more energy than SVMbow
- The performance gaps between approaches are small, but energy and cost differences are substantial
- SVMnlp offers an excellent balance of performance and eco-compatibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simpler models (SVM with linguistic features) can match or nearly match LLM performance on legal text classification while drastically reducing energy consumption.
- Mechanism: SVM models leverage structured linguistic features (lemmas, POS tags, concepts) to create a more informative feature space, allowing them to capture domain-specific patterns without the computational overhead of large neural architectures.
- Core assumption: Legal text contains enough linguistic regularities that can be effectively encoded in handcrafted features, making massive parameter models unnecessary for competitive performance.
- Evidence anchors:
  - [abstract] "very often, the simplest algorithms achieve performance very close to that of large LLMs but with very low power consumption and lower resource demands."
  - [section] "SVMnlp model seems to be an excellent ML solution, perfectly balancing performance (F1 scores very close to BERT-based models) and cost and eco-compatibility, with significant energy savings."
  - [corpus] Found related works evaluating legal classification baselines and legal multi-label classification, supporting relevance of domain-specific approaches.
- Break condition: If legal text exhibits highly non-linear or context-dependent patterns that cannot be captured by linguistic features alone, the performance gap between SVMs and LLMs would widen.

### Mechanism 2
- Claim: The energy consumption and carbon footprint of LLM-based models are orders of magnitude higher than traditional models, making them economically and environmentally unsustainable for many use cases.
- Mechanism: Large neural models require extensive matrix multiplications and memory transfers during both training and inference, leading to high energy draw; this is compounded when deployed at scale.
- Core assumption: Energy costs scale linearly with model size and inference frequency, and current energy pricing and carbon intensity make high consumption prohibitive.
- Evidence anchors:
  - [section] "BERT-based models have reported extremely high energy consumption values, in some cases even reaching a factor of 4000 times those of a standard SVMbow."
  - [abstract] "gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration."
  - [corpus] Works like "Energy and policy considerations for deep learning in NLP" quantify LLM carbon emissions, corroborating the large-scale consumption issue.
- Break condition: If renewable energy becomes dominant or if hardware efficiency improves dramatically, the relative cost and footprint disadvantage of LLMs could shrink.

### Mechanism 3
- Claim: Including eco-friendly metrics (energy, cost, CO2) in model selection can lead to better real-world decisions without sacrificing much accuracy.
- Mechanism: By quantifying total lifecycle costs, decision makers can weigh the marginal performance gains of LLMs against their environmental and economic impact, often finding that simpler models suffice.
- Core assumption: Stakeholders value total cost of ownership and environmental impact alongside accuracy, and these factors are measurable and comparable.
- Evidence anchors:
  - [abstract] "companies should consider additional evaluations beyond performance when selecting ML solutions, taking into account factors such as energy consumption and carbon footprint."
  - [section] "results obtained could suggest companies to include additional evaluations in the choice of Machine Learning (ML) solutions."
  - [corpus] Survey papers on NLP for legal domain highlight tasks, datasets, and challenges, implying practical evaluation contexts where such tradeoffs matter.
- Break condition: If performance requirements are extremely high and no simpler model meets them, eco-friendly metrics may be secondary to accuracy.

## Foundational Learning

- Concept: Text classification pipeline (preprocessing → feature extraction → model training → evaluation)
  - Why needed here: The paper compares different feature extraction methods (BoW vs enriched linguistic features) and models (BERT vs SVM), so understanding the full pipeline is essential to interpret results.
  - Quick check question: What are the main steps in a supervised text classification workflow, and where do feature extraction and model training fit in?

- Concept: Multi-label classification metrics (micro/macro F1)
  - Why needed here: LexGLUE datasets involve multi-label tasks, and the paper reports both micro and macro F1 scores to evaluate performance comprehensively.
  - Quick check question: How do micro and macro F1 differ in multi-label classification, and why might both be reported?

- Concept: Energy measurement in ML (e.g., codecarbon library)
  - Why needed here: The study uses codecarbon to estimate energy consumption and carbon footprint, so understanding how these metrics are obtained is crucial for interpreting the results.
  - Quick check question: What does codecarbon measure, and what factors influence its estimates of energy use and CO2 emissions?

## Architecture Onboarding

- Component map: Data ingestion → LexGLUE dataset loader → Preprocessing (tokenization, feature extraction) → BoW (SVMbow) vs enriched linguistic features (SVMnlp) → Model training → SVM with linear kernel (SVMbow, SVMnlp) vs BERT / LegalBERT / DistilBERT (LLM group) → Evaluation → F1 (micro/macro) and Energy measurement (codecarbon) → Reporting → Performance vs energy/cost/CO2 tables

- Critical path: 1. Load and preprocess dataset 2. Extract features (BoW or linguistic) 3. Train SVM or fine-tune BERT 4. Evaluate F1 scores 5. Measure energy consumption and cost 6. Compare results across models and datasets

- Design tradeoffs:
  - Feature richness vs model complexity: SVMnlp adds linguistic features but remains lightweight; LLMs are feature-rich but heavy.
  - Accuracy vs sustainability: Slight F1 gains with LLMs may not justify large energy increases.
  - Training vs inference cost: LLMs are expensive to train and run; SVMs are cheap at both stages.

- Failure signatures:
  - Poor F1 scores could indicate inadequate feature representation or model mismatch.
  - Anomalously high energy readings may signal inefficient code or hardware misconfiguration.
  - Disproportionate cost/CO2 for marginal F1 gains suggests model selection should be revisited.

- First 3 experiments:
  1. Reproduce SVMbow baseline on a small LexGLUE dataset (e.g., Unfair ToS) to verify F1 and energy metrics.
  2. Implement SVMnlp with enriched features and compare F1 and energy to SVMbow on the same dataset.
  3. Run a small BERT model (e.g., DistilBERT) on the same data and record F1, energy, and inference time to quantify the trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different text classification approaches (LLM vs. SVM) compare in terms of energy consumption and performance in the legal domain?
- Basis in paper: [explicit] The paper presents a comparative analysis of LLM and SVM approaches for text classification in the legal domain, considering both performance (F1 score) and eco-friendly metrics (energy consumption, costs, carbon footprint).
- Why unresolved: The paper provides a detailed analysis of the two approaches, but it does not explicitly state which approach is more energy-efficient and performs better overall. The results show that SVM-based approaches achieve performance very close to that of large LLMs but with significantly lower power consumption and resource demands.
- What evidence would resolve it: A comprehensive comparison of the energy consumption, costs, and performance of LLM and SVM approaches in the legal domain would provide a clear answer to this question.

### Open Question 2
- Question: How can companies balance the need for high-performance text classification models with the desire to reduce energy consumption and carbon footprint?
- Basis in paper: [explicit] The paper suggests that companies should consider additional evaluations beyond performance when selecting ML solutions, taking into account factors such as energy consumption and carbon footprint.
- Why unresolved: The paper does not provide specific guidelines or recommendations for companies to balance performance and eco-friendliness in their ML solutions. It only suggests that companies should consider additional evaluations beyond performance.
- What evidence would resolve it: Case studies or examples of companies that have successfully implemented high-performance, energy-efficient text classification models in the legal domain would provide valuable insights for other companies.

### Open Question 3
- Question: How can the legal domain benefit from the use of lightweight ML models that achieve performance close to that of large LLMs but with significantly lower power consumption and resource demands?
- Basis in paper: [explicit] The paper presents the results of experiments using the LexGLUE benchmark, which indicate that SVM-based approaches, particularly SVMnlp, achieve performance very close to that of large LLMs but with significantly lower power consumption and resource demands.
- Why unresolved: The paper does not discuss the specific benefits that the legal domain can gain from using lightweight ML models, such as cost savings, improved accessibility, or reduced environmental impact.
- What evidence would resolve it: A detailed analysis of the potential benefits and challenges of using lightweight ML models in the legal domain, including cost savings, improved accessibility, and reduced environmental impact, would provide valuable insights for legal professionals and organizations.

## Limitations

- Reproducibility challenges with SVMnlp's linguistic feature extraction using proprietary tools
- Energy measurement variability across different hardware configurations
- Uncertainty about generalizability beyond the seven LexGLUE datasets

## Confidence

- **High confidence**: The comparative energy consumption ratios between SVM and LLM models (claims about SVM being orders of magnitude more energy-efficient)
- **Medium confidence**: The near-parity F1 score performance between SVMnlp and BERT-based models on the specific LexGLUE datasets
- **Low confidence**: The economic cost calculations, as these depend heavily on specific cloud pricing models and regional electricity rates that may not be universally applicable

## Next Checks

1. **Feature Engineering Replication**: Implement an alternative linguistic feature extraction pipeline (e.g., spaCy or NLTK-based) to verify that SVMnlp's performance advantages can be reproduced without proprietary tools.

2. **Hardware Sensitivity Analysis**: Run energy measurements across different GPU/CPU configurations (e.g., RTX 3080 vs RTX 4090, different CPU generations) to establish the stability and generalizability of consumption metrics.

3. **Cross-Dataset Generalization**: Apply the five model approaches to at least two additional legal text classification datasets not included in LexGLUE to test whether the observed SVM-LLM performance gap persists across different legal domains.