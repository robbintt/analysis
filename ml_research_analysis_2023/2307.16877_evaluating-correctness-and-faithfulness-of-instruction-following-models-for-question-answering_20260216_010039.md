---
ver: rpa2
title: Evaluating Correctness and Faithfulness of Instruction-Following Models for
  Question Answering
arxiv_id: '2307.16877'
source_url: https://arxiv.org/abs/2307.16877
tags:
- metrics
- question
- answer
- evaluation
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates retrieval-augmented instruction-following models
  across three QA tasks. Human annotations show that traditional QA metrics like EM
  and F1 are poorly aligned with verbose model responses.
---

# Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering

## Quick Facts
- arXiv ID: 2307.16877
- Source URL: https://arxiv.org/abs/2307.16877
- Reference count: 21
- Primary result: Traditional QA metrics poorly align with verbose model responses; token-level recall and K-Precision best capture correctness and faithfulness respectively.

## Executive Summary
This paper evaluates retrieval-augmented instruction-following models across three question answering tasks (Natural Questions, HotpotQA, TopiOCQA). The authors conduct extensive human annotation studies to assess model responses for correctness (satisfaction of information needs) and faithfulness (grounding in provided knowledge). Their findings reveal that traditional metrics like EM and F1 are poorly aligned with human judgments for verbose responses, while simpler metrics like token-level recall and K-Precision show better correlation. The study also demonstrates that instruction-following models often outperform fine-tuned baselines on correctness but struggle with faithfulness, frequently hallucinating or failing to abstain when irrelevant knowledge is provided.

## Method Summary
The authors evaluate four instruction-following models (GPT-3.5, Llama-2, Flan-T5, Alpaca) on three QA datasets with retrieved passages. Models receive prompts containing instructions, retrieved knowledge snippets, and questions. Human annotators judge 400 instances for correctness and faithfulness. The study compares various automatic metrics including traditional lexical overlap measures (EM, F1), token-level recall, knowledge-grounded metrics (K-F1, K-Precision), and LLM-based evaluation (GPT-4 and GPT-3.5). The evaluation framework distinguishes between correctness (satisfaction of information needs) and faithfulness (grounding in provided knowledge).

## Key Results
- Token-level recall shows the highest correlation with human correctness judgments, outperforming traditional metrics like EM and F1
- K-Precision correlates best with human faithfulness judgments compared to other knowledge-grounded metrics
- GPT-4-based evaluation shows the strongest correlation with human judgments but is expensive and potentially biased
- Instruction-following models demonstrate superior correctness but struggle with faithfulness, often hallucinating or failing to abstain from answering with irrelevant knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level recall correlates better with human correctness judgments than traditional metrics like F1 because it does not penalize verbose responses that contain the reference answer.
- Mechanism: Recall measures the proportion of tokens in the reference answer that are present in the model response. This metric captures whether the core information need is satisfied, regardless of additional context or elaboration in the model's response.
- Core assumption: The reference answer contains the essential information needed to satisfy the user's query, and additional information in the model response does not detract from correctness.
- Evidence anchors:
  - [abstract] "Our findings suggest that, for correctness, recall – the proportion of tokens in the reference answer also present in the model response – exhibits the highest correlation than lexical overlap metrics like EM or F1."
  - [section] "Recall does not penalize verbose model response, as long as the response contains the reference answer tokens."
  - [corpus] Weak evidence - no direct citations in corpus about token-level recall specifically.
- Break condition: If the reference answer is incomplete or the model response contains critical information not in the reference, recall will underestimate correctness.

### Mechanism 2
- Claim: K-Precision correlates better with human faithfulness judgments than K-F1 because it measures the proportion of response tokens found in the knowledge, making it length-invariant and suitable for information-seeking tasks.
- Mechanism: K-Precision evaluates faithfulness by checking what fraction of the model's response can be grounded in the provided knowledge. This is asymmetric - the knowledge can be longer than the response, but the response should be a subset of the knowledge.
- Core assumption: In information-seeking tasks, the model response should be grounded in the provided knowledge, and shorter responses that extract relevant information are acceptable.
- Evidence anchors:
  - [abstract] "For faithfulness, K-Precision – the proportion of model response tokens that appear in the knowledge snippet – correlates better with human judgments than any other token-overlap metric."
  - [section] "We propose K-Precision – the proportion of tokens in the model response u that are present in K. The intuition behind this is that in information-seeking, grounding u in K is inherently an asymmetric task, i.e., u can be a subset of K but K cannot be a subset of u."
  - [corpus] No direct evidence in corpus about K-Precision specifically.
- Break condition: If the model needs to synthesize information from multiple knowledge snippets or perform complex reasoning, K-Precision may underestimate faithfulness.

### Mechanism 3
- Claim: LLM-based evaluation (GPT-4) correlates highest with human judgments because LLMs can understand semantic equivalence and handle verbosity better than lexical metrics.
- Mechanism: GPT-4 uses its language understanding capabilities and world knowledge to evaluate whether responses satisfy information needs or are grounded in knowledge, overcoming the limitations of token overlap metrics.
- Core assumption: GPT-4 has sufficient language understanding and world knowledge to accurately evaluate semantic equivalence and grounding, even in verbose responses.
- Evidence anchors:
  - [abstract] "Among model-based metrics, i.e., using a model to determine the correctness/faithfulness of an answer w.r.t. reference answer/knowledge, GPT-4 correlates the most but it is expensive and prone to systematic biases."
  - [section] "Notably, GPT4-eval has the highest agreement with human judgments, with 70.15 Spearman correlation and 69.36 Kendall correlation, closely followed by GPT3.5-Eval."
  - [corpus] Weak evidence - corpus mentions related works but no direct evidence about GPT-4 evaluation specifically.
- Break condition: If GPT-4 exhibits systematic biases or is sensitive to input instructions, its evaluation may be unreliable or inconsistent.

## Foundational Learning

- Concept: Evaluation metrics correlation with human judgments
  - Why needed here: The paper's central contribution is identifying which automatic metrics best align with human assessments of correctness and faithfulness in instruction-following models.
  - Quick check question: What metric showed the highest correlation with human correctness judgments in the study?

- Concept: Retrieval-augmented generation (RAG) pipeline
  - Why needed here: Understanding how retrievers select relevant passages and generators produce responses is crucial for evaluating both correctness and faithfulness.
  - Quick check question: How many passages were provided to instruction-following models for the open-domain QA task?

- Concept: Faithfulness vs. correctness distinction
  - Why needed here: The paper evaluates models along two dimensions - whether they satisfy information needs (correctness) and whether they ground responses in provided knowledge (faithfulness).
  - Quick check question: What is the key difference between correctness and faithfulness in the context of this paper?

## Architecture Onboarding

- Component map:
  - Retriever (DPR) → Generator (instruction-following models) → Evaluation (human + automatic metrics)

- Critical path: Retriever selects passages → Generator produces responses based on prompt → Evaluation measures performance using human annotations and automatic metrics

- Design tradeoffs:
  - Token overlap metrics (EM, F1) are fast but penalize verbosity
  - Recall and K-Precision are simple but can be "gamed" by copying knowledge
  - LLM-based metrics (GPT-4) are most accurate but expensive and potentially biased
  - Instruction-following models are flexible but struggle with faithfulness

- Failure signatures:
  - Low EM/F1 but high Recall indicates verbose responses with correct information
  - High K-F1 but low K-Precision suggests the model copied extensive knowledge unnecessarily
  - GPT-4-based metrics disagreeing with human judgments may indicate bias or sensitivity to prompts

- First 3 experiments:
  1. Compare EM, F1, and Recall on a sample of model responses to understand how verbosity affects traditional metrics
  2. Evaluate K-F1 vs. K-Precision on the same sample to see how knowledge grounding is assessed differently
  3. Run GPT-4-based evaluation on a subset of responses to establish a "gold standard" for comparison with other metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop metrics that resist being gamed while still being accessible and aligned with human judgment?
- Basis in paper: [explicit] The paper identifies that simple lexical overlap metrics like Recall and K-Precision are easy to hack but human-aligned, while LLM-based metrics are expensive and potentially biased.
- Why unresolved: The paper demonstrates the limitations of current metrics and the trade-offs between accessibility and robustness, but doesn't propose a definitive solution to create metrics that are both resistant to gaming and practical to use.
- What evidence would resolve it: A new metric that maintains high correlation with human judgments, is computationally efficient, and cannot be easily manipulated by models that simply copy all retrieved knowledge.

### Open Question 2
- Question: What prompts or instructions can improve instruction-following models' ability to correctly identify when to answer versus when to abstain?
- Basis in paper: [explicit] The paper shows that models struggle to refuse to answer when given irrelevant knowledge, even with explicit instructions to say "I don't know," suggesting current prompting strategies are insufficient.
- Why unresolved: While the paper tests a basic "I don't know" instruction, it doesn't explore more sophisticated prompting techniques, iterative refinement, or meta-learning approaches that might improve this capability.
- What evidence would resolve it: A prompting strategy or instruction format that significantly increases correct abstention rates on irrelevant knowledge while maintaining answer quality on relevant knowledge.

### Open Question 3
- Question: How do different training paradigms for instruction-following models affect their performance on correctness versus faithfulness in retrieval-augmented QA?
- Basis in paper: [explicit] The paper compares models trained on different data types (public NLP datasets, expert annotations, synthetic data) but doesn't deeply analyze how these training approaches specifically impact the trade-off between correctness and faithfulness.
- Why unresolved: The analysis shows performance differences between models but doesn't isolate how specific training characteristics (data diversity, instruction types, feedback mechanisms) contribute to their strengths and weaknesses in these two dimensions.
- What evidence would resolve it: Controlled experiments varying training data composition, instruction formats, and supervision types to identify which factors most strongly influence correctness versus faithfulness performance.

## Limitations
- Limited human annotation coverage with only 400 instances across three datasets
- Unknown training details and hyperparameters for instruction-following models
- Potential prompt sensitivity in LLM-based evaluation metrics

## Confidence

**High Confidence** (Strong evidence, multiple supporting points):
- Token-level recall is more aligned with human correctness judgments than EM/F1 for verbose responses
- K-Precision better captures faithfulness than K-F1 for information-seeking tasks
- Instruction-following models show superior correctness but struggle with faithfulness compared to fine-tuned baselines

**Medium Confidence** (Reasonable evidence but with limitations):
- GPT-4-based metrics provide the best correlation with human judgments despite being expensive
- The proposed metrics (Recall, K-Precision) effectively address the limitations of traditional metrics
- Models struggle to abstain from answering when irrelevant knowledge is provided

**Low Confidence** (Limited evidence or significant uncertainties):
- The specific nature and extent of GPT-4's systematic biases in evaluation
- Whether the identified correlation patterns hold across different domains beyond QA
- The generalizability of results to other types of instruction-following tasks

## Next Checks

1. **Inter-annotator Agreement Analysis**: Calculate and report Cohen's kappa or similar agreement metrics for the human annotations to establish reliability of the ground truth judgments used for metric evaluation.

2. **Prompt Sensitivity Testing**: Systematically vary the evaluation prompts used for GPT-4-based metrics to characterize the extent and nature of potential biases, following methodologies similar to those used in instruction-tuning studies.

3. **Extended Validation on Additional Datasets**: Apply the proposed evaluation framework to at least one additional domain (e.g., scientific QA or multi-document summarization) to test whether the identified metric correlations generalize beyond the three QA tasks studied.