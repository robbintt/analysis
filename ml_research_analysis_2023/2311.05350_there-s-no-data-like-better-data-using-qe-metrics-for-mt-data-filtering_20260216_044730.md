---
ver: rpa2
title: 'There''s no Data Like Better Data: Using QE Metrics for MT Data Filtering'
arxiv_id: '2311.05350'
source_url: https://arxiv.org/abs/2311.05350
tags:
- data
- translation
- quality
- bleurt
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using Quality Estimation (QE) metrics for filtering
  parallel sentence pairs in neural machine translation (NMT) training data. While
  most filtering methods focus on removing noisy examples, QE metrics are trained
  to discriminate fine-grained quality differences.
---

# There's no Data Like Better Data: Using QE Metrics for MT Data Filtering

## Quick Facts
- **arXiv ID**: 2311.05350
- **Source URL**: https://arxiv.org/abs/2311.05350
- **Reference count**: 31
- **Key outcome**: QE-based filtering improves translation quality by up to 2.8 COMET points while reducing training data by 50%

## Executive Summary
This paper explores using Quality Estimation (QE) metrics for filtering parallel sentence pairs in neural machine translation training data. Unlike traditional noise-focused filtering methods, QE metrics are trained to discriminate fine-grained quality differences, allowing them to detect subtle translation errors, named entity mismatches, and grammatical issues that don't constitute obvious noise but still degrade translation quality. The authors use two state-of-the-art QE metrics, COMET KIWI and BLEURT QE, to score sentence pairs and filter out the lowest quality half.

Experiments on English ↔ German, Japanese ↔ English, and Chinese ↔ English show that this approach improves translation quality measured by COMET 22 by up to 2.8 points compared to using all the data, while retaining only 50% of the training corpus. The QE filtering approach is effective across different domains, with the main improvements coming from removing translations with subtle errors or low quality rather than obvious noise. The authors conclude that QE metrics are a promising method for improving NMT quality through better data filtering.

## Method Summary
The authors apply QE metrics (COMET KIWI and BLEURT QE) to score all sentence pairs in WMT English ↔ German, Japanese ↔ English, and Chinese ↔ English parallel corpora. They select a threshold to retain 50% of the data with the highest quality scores, then train transformer-based NMT models from scratch on the filtered data. Models use 6 encoder layers, 6 decoder layers, model dimension 1024, hidden dimension 8192, and 16 attention heads. Translation quality is evaluated using COMET 22 on WMT and IWSLT test sets.

## Key Results
- QE filtering improves COMET 22 scores by up to 2.8 points compared to using all training data
- Models trained on 50% filtered data outperform those trained on 100% data across multiple language pairs
- QE metrics capture different quality issues than traditional noise detection methods like BICLEANER
- Improvements are consistent across different domains within each language pair

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QE metrics can identify fine-grained quality differences that traditional noise-focused filtering methods miss
- Mechanism: Quality Estimation metrics are trained to predict human judgments of translation quality without reference translations, allowing them to detect subtle translation errors, named entity mismatches, and grammatical issues that don't constitute obvious noise but still degrade translation quality
- Core assumption: The QE metric's training objective (predicting quality scores) aligns with the goal of selecting high-quality training data for NMT systems
- Evidence anchors:
  - [abstract] "QE models are trained to discriminate more fine-grained quality differences"
  - [section 2] "Judging the quality of translations is the focus of the Quality Estimation (QE) field of machine translation"
  - [corpus] Weak evidence - corpus analysis shows QE filtering keeps different sentences than BICLEANER, but direct comparison of quality scores is not provided
- Break condition: If the QE metric is not well-calibrated to the specific domain or language pair, or if it overfits to certain error patterns that don't generalize

### Mechanism 2
- Claim: Reducing training data size while improving quality can improve NMT performance by reducing noise and focusing learning on high-quality examples
- Mechanism: NMT systems have limited capacity and can be negatively affected by memorizing low-quality or erroneous training examples. By removing the lowest-quality 50% of data, the model can focus on learning from cleaner, more reliable examples
- Core assumption: The remaining 50% of data after QE filtering contains proportionally more useful information for learning translation patterns than the original full dataset
- Evidence anchors:
  - [abstract] "We show that by selecting the highest quality sentence pairs in the training data, we can improve translation quality while reducing the training size by half"
  - [section 1] "Neural networks have a great ability of memorizing (parts of) the training data" and "such outliers may have a critical effect on the output of the system"
  - [corpus] Evidence from Table 2 showing 2.8 point COMET improvements across multiple language pairs when using 50% filtered data
- Break condition: If the QE filtering threshold is too aggressive and removes too much diverse data, or if the quality differences between filtered and unfiltered data are not substantial enough to impact learning

### Mechanism 3
- Claim: QE-based filtering and traditional noise detection filtering methods complement each other by capturing different types of problematic data
- Mechanism: Traditional methods like BICLEANER excel at detecting obvious noise (wrong language, untranslated segments, severe misalignments) while QE metrics excel at detecting subtle quality issues (minor mistranslations, grammatical errors, entity mismatches). Different data types are captured by each method
- Core assumption: The combination of both filtering approaches would capture a wider range of data quality issues than either approach alone
- Evidence anchors:
  - [section 5.1] Analysis showing BLEURT QE filters out "Single Entity Mistranslations" and "Low Quality Translations" while BICLEANER keeps them
  - [section 5.2] Table 5 showing BLEURT QE and BICLEANER have different strengths on synthetic noise categories
  - [corpus] Table 3 shows only ~66% overlap between methods, suggesting complementary filtering
- Break condition: If the two methods have significant overlap in what they filter, or if one method's weaknesses are not covered by the other's strengths

## Foundational Learning

- Concept: Quality Estimation (QE) vs traditional evaluation metrics
  - Why needed here: Understanding the difference between QE (evaluates without references) and traditional metrics (requires references) is crucial for grasping why QE can be used for data filtering
  - Quick check question: What is the key difference between how BLEU and BLEURT QE evaluate translation quality?

- Concept: Neural network memorization and sensitivity to noise
  - Why needed here: The paper's premise that NMT systems are sensitive to noisy training data (unlike statistical MT) is fundamental to understanding why data filtering matters
  - Quick check question: How does neural MT's sensitivity to noise differ from statistical MT's robustness?

- Concept: Corpus filtering objectives: noise detection vs quality selection
  - Why needed here: The paper contrasts traditional noise-focused filtering with quality-focused QE filtering, which is the central innovation
  - Quick check question: What is the primary objective of traditional corpus filtering methods compared to QE-based filtering?

## Architecture Onboarding

- Component map: Data → QE Metrics (BLEURT QE, COMET KIWI) → Quality Scores → Threshold Selection (50%) → Filtered Dataset → NMT Training → Translation Output
- Critical path: The sequence from scoring all sentence pairs with QE metrics through threshold selection to final model training
- Design tradeoffs: Computational cost of scoring all data vs quality improvements, threshold selection (50% chosen empirically), direction of QE scoring (source→target only)
- Failure signatures: No improvement over baseline, degradation in specific domains (like IWSLT'23), computational infeasibility for very large corpora
- First 3 experiments:
  1. Run QE scoring on full dataset and plot score distribution to understand data quality landscape
  2. Test different filtering thresholds (25%, 50%, 75%) to find optimal balance between data quantity and quality
  3. Compare QE filtering results with random selection at same data reduction level to validate effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the filtering threshold affect the balance between translation quality improvement and corpus size reduction?
- Basis in paper: [inferred] The paper uses a fixed 50% filtering threshold across all experiments. However, the optimal threshold likely varies depending on the specific dataset characteristics and language pair
- Why unresolved: The paper does not explore different threshold values or their impact on translation quality
- What evidence would resolve it: Experiments comparing translation quality at different filtering thresholds (e.g., 25%, 50%, 75%) would show the optimal trade-off between corpus size and translation quality

### Open Question 2
- Question: Can the QE-based filtering approach be effectively combined with traditional noise detection methods for raw web-crawled data?
- Basis in paper: [explicit] The paper mentions that QE metrics are less effective at detecting certain types of noise like untranslated sentences, while BICLEANER excels at this. However, it only briefly explores combining both methods and finds that it's too aggressive in their setup
- Why unresolved: The paper doesn't explore different combinations of QE and traditional filtering methods or how to adapt thresholds for optimal performance
- What evidence would resolve it: Experiments using different combinations of QE and traditional filtering methods, with tuned thresholds for each combination, would show the optimal approach for raw web-crawled data

### Open Question 3
- Question: How do the improvements from QE-based filtering generalize to other language pairs and domains beyond those tested in the paper?
- Basis in paper: [inferred] The paper shows improvements across three language pairs and various domains within those pairs. However, it doesn't test other language pairs or highly specialized domains
- Why unresolved: The paper's experiments are limited to specific language pairs and don't cover the full range of possible translation scenarios
- What evidence would resolve it: Experiments on additional language pairs, including low-resource languages, and highly specialized domains (e.g., legal, medical) would demonstrate the broader applicability of the approach

## Limitations

- The paper uses a fixed 50% filtering threshold without exploring whether more aggressive filtering might yield better results
- QE metrics' effectiveness on low-resource language pairs and highly specialized domains is not tested
- Computational costs of applying QE metrics to score all training data are not discussed or analyzed

## Confidence

**High Confidence** - The core claim that QE metrics can improve NMT quality while reducing training data by 50% is well-supported by the experimental results across multiple language pairs (up to 2.8 COMET points improvement).

**Medium Confidence** - The claim that QE metrics capture different types of quality issues than traditional noise detection methods is supported by corpus analysis but lacks quantitative comparison of actual quality improvements when combining both approaches.

**Low Confidence** - The paper's assertion that QE filtering would generalize well to other language pairs or domains beyond the tested ones is speculative, as no cross-domain or low-resource experiments are presented.

## Next Checks

1. **Threshold Sensitivity Analysis**: Run experiments with different filtering thresholds (25%, 50%, 75%) on the same datasets to determine whether 50% is truly optimal or if more aggressive filtering yields better quality improvements.

2. **Complementary Filtering Experiment**: Train models using data filtered by both QE metrics and BICLEANER to quantify whether combining both approaches yields multiplicative or additive improvements over either method alone.

3. **Computational Cost Benchmark**: Measure the wall-clock time and GPU hours required to score the full training corpora with QE metrics, then extrapolate to estimate costs for 10x larger datasets to assess practical scalability limits.