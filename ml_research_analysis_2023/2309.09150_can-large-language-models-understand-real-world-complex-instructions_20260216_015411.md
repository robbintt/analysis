---
ver: rpa2
title: Can Large Language Models Understand Real-World Complex Instructions?
arxiv_id: '2309.09150'
source_url: https://arxiv.org/abs/2309.09150
tags:
- complex
- task
- instructions
- arxiv
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CELLO, the first systematic benchmark for
  evaluating LLMs' ability to follow complex real-world instructions. The authors
  define eight key features of complex instructions, construct a dataset of 523 samples
  across nine tasks from real-world scenarios, and propose four evaluation criteria
  (Count Limit, Answer Format, Task-prescribed Phrases, Input-dependent Query) with
  corresponding automated metrics.
---

# Can Large Language Models Understand Real-World Complex Instructions?

## Quick Facts
- arXiv ID: 2309.09150
- Source URL: https://arxiv.org/abs/2309.09150
- Reference count: 26
- Key outcome: CELLO benchmark shows open-source models lag significantly behind GPT-4 in complex instruction following

## Executive Summary
This paper introduces CELLO, the first systematic benchmark for evaluating LLMs' ability to follow complex real-world instructions. The authors define eight key features of complex instructions, construct a dataset of 523 samples across nine tasks from real-world scenarios, and propose four evaluation criteria with corresponding automated metrics. Their two-stage data evolution framework enriches simple instructions by adding constraints and enabling multi-turn interactions. CELLO outperforms existing benchmarks in comprehensiveness and objectivity, showing that open-source models lag significantly behind GPT-4.

## Method Summary
The CELLO benchmark evaluates LLMs through a two-stage data evolution framework that first diversifies complex instructions and then incrementally complicates simple ones. The process involves collecting real-world instructions, applying In-breadth evolution (task description relocation, paraphrasing, and emulation) followed by In-depth evolution (constraint addition and multi-round interaction), then evaluating models using automated metrics across four criteria: Count Limit, Answer Format, Task-prescribed Phrases, and Input-dependent Query.

## Key Results
- Open-source models show significant performance gaps compared to GPT-4 across all evaluation criteria
- CELLO provides more comprehensive and objective assessment than existing benchmarks
- Models demonstrate varying strengths and weaknesses across different tasks and criteria
- The benchmark enables fine-grained analysis of complex instruction following capabilities

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Data Evolution for Complexity Control
- Gradual complexity increase allows models to learn complex instruction patterns without overwhelming them
- In-breadth evolution adds diversity through task description relocation, paraphrasing, and emulation
- In-depth evolution increases complexity via constraint addition and multi-round interaction

### Mechanism 2: Multi-Criteria Automated Evaluation with Fine-Grained Metrics
- Four evaluation criteria (Count Limit, Answer Format, Task-prescribed Phrases, Input-dependent Query) each have multiple sub-scores
- Automated metrics provide comprehensive, objective assessment of complex instruction following
- Breaking down complex instruction following into specific criteria captures essential challenges

### Mechanism 3: Task-Specific Prompt Engineering for Data Diversification
- Carefully designed prompts enable effective data diversification across different task types
- Task-specific prompts guide GPT-3.5-turbo to generate diverse task descriptions and operations
- Well-crafted prompts can effectively simulate real-world instruction complexity

## Foundational Learning

- **Concept**: Instruction Following in LLMs
  - Why needed here: Understanding how LLMs interpret and execute instructions is fundamental to evaluating their complex instruction following ability
  - Quick check question: What are the two main components of an instruction according to the paper, and how do they differ?

- **Concept**: Evaluation Metrics for Open-Ended Tasks
  - Why needed here: Traditional closed-ended evaluation metrics are insufficient for assessing complex, open-ended instructions
  - Quick check question: Why are binary pass rates considered too strict and coarse-grained for evaluating complex instruction following?

- **Concept**: Data Augmentation and Evolution Techniques
  - Why needed here: The two-stage data evolution framework relies on established data augmentation principles to create diverse, complex instructions
  - Quick check question: What are the two main perspectives of the data evolution framework, and how do they differ in their approach to increasing instruction complexity?

## Architecture Onboarding

- **Component map**: Real-world instructions -> Two-stage data evolution (In-breadth + In-depth) -> Dataset construction (523 samples, 9 tasks) -> Evaluation system (4 criteria, automated metrics) -> Model evaluation (34 representative models)

- **Critical path**: 1. Collect real-world instructions, 2. Apply two-stage data evolution, 3. Annotate scoring keywords, 4. Evaluate models using automated metrics, 5. Analyze results by task and criteria

- **Design tradeoffs**: Real-world data vs. synthetic data (authenticity vs. diversity), automated vs. human evaluation (scalability vs. nuance), task coverage vs. depth (breadth vs. depth)

- **Failure signatures**: Models consistently fail on certain criteria despite success on others, evaluation metrics show poor correlation with human judgment, data evolution introduces artifacts or bias

- **First 3 experiments**: 1. Validate evaluation metrics with small model set, 2. Compare automated vs. human evaluation, 3. Test sensitivity to instruction complexity levels

## Open Questions the Paper Calls Out

Key research questions include: how to develop comprehensive evaluation metrics beyond binary pass/fail, optimal methods for constructing real-world complex instruction datasets, extent of language transfer for complex instruction understanding, impact of different model architectures and training approaches, optimal context length for different instruction types, systematic characterization of instruction complexity, minimizing bias in automated evaluation, cross-task generalization capabilities, scalability of different models, and optimal balance between automated data evolution and human oversight.

## Limitations

- Automated evaluation metrics may miss nuanced aspects of instruction compliance
- Dataset construction relies heavily on GPT-3.5-turbo, potentially introducing bias
- Focus on specific task types may not represent all real-world complex instructions

## Confidence

**High Confidence**: Overall benchmark framework and methodology are well-defined with clear evaluation criteria and systematic data evolution approach. Performance gaps between GPT-4 and open-source models are substantial and consistent.

**Medium Confidence**: Automated evaluation metrics are reliable for coarse assessment but may miss subtle compliance issues. Two-stage data evolution framework is sound but exact implementation details are not fully specified.

**Low Confidence**: Generalizability to completely unseen instruction types and real-world applications remains uncertain. Sensitivity of model performance to instruction complexity levels is not thoroughly explored.

## Next Checks

1. Conduct human evaluation study on CELLO instructions to validate automated metrics correlation
2. Test same models on CELLO and other complex instruction benchmarks to assess consistency
3. Design experiment to systematically vary instruction complexity and measure model performance degradation