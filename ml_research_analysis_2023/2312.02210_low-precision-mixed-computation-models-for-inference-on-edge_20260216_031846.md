---
ver: rpa2
title: Low-Precision Mixed-Computation Models for Inference on Edge
arxiv_id: '2312.02210'
source_url: https://arxiv.org/abs/2312.02210
tags:
- posit
- quantization
- neural
- mixed-computation
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mixed-computation approach for neural network
  inference on edge devices, using 4-bit Posit for sensitive weights and 4-bit fixed-point
  for others. A sensitivity metric based on gradient magnitudes and quantization errors
  is introduced to assign number systems per layer.
---

# Low-Precision Mixed-Computation Models for Inference on Edge

## Quick Facts
- arXiv ID: 2312.02210
- Source URL: https://arxiv.org/abs/2312.02210
- Reference count: 32
- Key outcome: Mixed 4-bit Posit and 4-bit fixed-point inference improves accuracy by ~1.5% over fixed-point with only 0.19% energy overhead by applying Posit to <10% of parameters.

## Executive Summary
This paper introduces a mixed-precision inference framework for edge devices that selectively applies 4-bit Posit to the most sensitive layers while using 4-bit fixed-point elsewhere. A sensitivity metric based on gradient magnitudes and quantization errors guides the assignment of number systems per layer. The approach includes scaled Posit variants to better match DNN weight distributions and a custom tanh-based gradient approximation for training. Evaluation across vision and language models shows that the mixed approach significantly improves accuracy over fixed-point with minimal energy overhead, making it well-suited for energy-constrained edge inference.

## Method Summary
The method employs a two-stage process: first, a sensitivity metric combining gradient magnitudes and quantization error gaps between fixed-point and Posit is computed for each layer to identify which should receive Posit4. Second, these sensitive layers are quantized using scaled Posit4 (Posit4sc,4 or Posit4sc,8) while others use 4-bit fixed-point, with activations fixed to fixed-point for efficiency. A custom tanh-based gradient approximation is used during training to handle the non-uniform quantization of Posit, and a specialized Posit/FixP MAC hardware unit is proposed to minimize energy overhead.

## Key Results
- Mixed Posit4/FixP4 improves accuracy by ~1.5% over pure FixP4 on vision and language models
- Energy overhead is only 0.19% due to limiting Posit to <10% of parameters
- Scaled Posit variants (Posit4sc,4 and Posit4sc,8) better match DNN weight distributions than unscaled Posit
- The proposed Posit/FixP MAC unit maintains energy efficiency while enabling mixed computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective Posit usage on high-sensitivity layers reduces quantization error without incurring full energy cost.
- Mechanism: The paper identifies layers with high gradient magnitudes and large quantization error gaps between fixed-point and Posit. Posit4 is applied only to these layers, leveraging its higher precision near zero for sensitive weights, while other layers use FixP4. This targeted application limits the expensive Posit arithmetic to a small fraction (<10%) of parameters.
- Core assumption: The sensitivity metric correctly ranks layers by their impact on final accuracy when quantized.
- Evidence anchors:
  - [abstract] "This mixed-computation approach employs 4-bit Posit (Posit4), which has higher precision around zero, for representing weights with high sensitivity, while it uses 4-bit FixP (FixP4) for representing other weights."
  - [section III-B] "To exploit Posit effectively, we should apply it to layers that (i) have higher gradient values, and (ii) the reduction in quantization error is significant (compared to fixed-point)."
- Break condition: If the sensitivity metric fails to correlate with actual accuracy loss, the wrong layers get Posit, degrading accuracy or wasting energy.

### Mechanism 2
- Claim: Scaled Posit variants (Posit4sc,4 and Posit4sc,8) better match DNN weight distributions than unscaled Posit.
- Mechanism: The paper introduces scaled Posit values to align the dynamic range of Posit representations with typical DNN weight distributions (mean near zero, std near one). This scaling is achieved by shifting the raw Posit format, which is computationally cheap since scaling factors are powers of two.
- Core assumption: DNN weight distributions are centered near zero and can be matched by simple scaling.
- Evidence anchors:
  - [section III-A] "To ameliorate this issue, we propose to employ scaled values of the Posit4, divided by 4 (Posit4sc,4) and 8 (Posit4sc,8)."
  - [section III-A] "The integration of these scaling factors into the Posit decoding process is simple, and since the considered scaling factors are the power of 2 numbers, only the decimal point in the raw posit format is moved to the left."
- Break condition: If DNN weight distributions deviate significantly from assumed scaling, quantization error increases despite scaling.

### Mechanism 3
- Claim: A custom gradient approximation for Posit enables effective backpropagation despite non-uniform quantization.
- Mechanism: The paper replaces the standard STE with a tanh-based estimator for Posit weights. This estimator assigns larger gradients near quantization thresholds (where rounding error is maximal) and near-zero gradients near representable values, reflecting the non-uniform nature of Posit quantization.
- Core assumption: The tanh-based gradient approximation adequately captures the error landscape for Posit weights.
- Evidence anchors:
  - [section III-C] "we propose to use the tanh(x) function to approximate the backward gradients in the Posit quantizer."
  - [section III-C] "∂QP osit G ∂x = ∂ ∂x ( 2 αi+1−αi tanh( 5 αi+1−αi (x − 0.5(αi+1 − αi))))"
- Break condition: If the tanh approximation poorly models the true gradient, training may diverge or converge to poor minima.

## Foundational Learning

- Concept: Posit number system and tapered precision
  - Why needed here: Understanding why Posit4 outperforms FixP4 near zero is essential for grasping the mixed-computation strategy.
  - Quick check question: What property of Posit allows higher precision near zero compared to fixed-point?
- Concept: Sensitivity analysis for quantization
  - Why needed here: The mixed-computation approach relies on identifying which layers benefit most from Posit based on gradients and quantization error.
  - Quick check question: What two factors does the sensitivity metric combine to rank layers?
- Concept: Hardware implications of mixed-precision arithmetic
  - Why needed here: The energy overhead trade-off hinges on the relative costs of Posit and FixP MAC operations.
  - Quick check question: Why is the Posit/FixP MAC still more energy-efficient than a full Posit MAC in the mixed approach?

## Architecture Onboarding

- Component map: Sensitivity analysis module -> Layer-wise quantization selector -> Scaled Posit encoder/decoder -> Custom gradient approximator -> Mixed MAC unit -> Retraining pipeline
- Critical path:
  1. Load FP32 model → 2. Compute sensitivity per layer → 3. Assign number system → 4. Quantize weights (entropy-based FixP4, scaled Posit4) → 5. Train with custom gradient estimator → 6. Deploy with mixed MAC
- Design tradeoffs:
  - Posit usage limit (η=10%) vs accuracy vs energy
  - Scaled Posit variants (4x vs 8x) per layer
  - Fixed-point vs Posit for activations (activations fixed to FixP4 for efficiency)
- Failure signatures:
  - Accuracy drop >1.5% vs FixP4 baseline → sensitivity metric or gradient estimator may be flawed
  - Energy overhead >0.25% → too many layers mapped to Posit or inefficient MAC design
  - Training instability → gradient approximation not well-behaved
- First 3 experiments:
  1. Apply sensitivity analysis to a small CNN and verify Posit layers match intuition (high gradient, high quantization error).
  2. Compare accuracy of scaled vs unscaled Posit on a single sensitive layer to validate scaling.
  3. Implement and benchmark the Posit/FixP MAC unit vs full FixP MAC to confirm energy overhead estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed sensitivity metric for layer-specific Posit assignment generalize to more complex neural network architectures beyond those tested (e.g., deeper or recurrent networks)?
- Basis in paper: [explicit] The paper mentions a heuristic sensitivity metric based on gradient magnitudes and quantization errors to assign number systems per layer, but only evaluates it on vision and language models.
- Why unresolved: The evaluation is limited to standard models like ResNet and BERT, so scalability and robustness of the heuristic to other architectures is unclear.
- What evidence would resolve it: Testing the sensitivity metric on diverse and deeper architectures such as transformers with more layers or recurrent networks and comparing accuracy/energy trade-offs.

### Open Question 2
- Question: What is the long-term impact on model accuracy when using the proposed Posit-specific gradient approximation during training compared to using exact Posit gradients?
- Basis in paper: [explicit] The paper introduces a Posit-specific gradient approximation for backpropagation but does not evaluate how it compares to exact gradient methods in terms of final model quality.
- Why unresolved: The approximation may introduce training bias or slow convergence, which are not quantified in the study.
- What evidence would resolve it: Conducting controlled experiments comparing training convergence and final accuracy using exact vs. approximated gradients over multiple runs and datasets.

### Open Question 3
- Question: How does the proposed mixed-computation approach scale when Posit usage exceeds the 10% threshold mentioned, and what is the optimal threshold for different application domains?
- Basis in paper: [inferred] The paper sets a fixed 10% Posit usage threshold to limit energy overhead, but does not explore how varying this threshold affects accuracy and energy consumption across different domains.
- Why unresolved: Different edge applications may have different accuracy/energy priorities, and the fixed threshold may not be optimal for all.
- What evidence would resolve it: Systematic evaluation of accuracy and energy consumption across multiple domains (e.g., real-time vision vs. language tasks) by varying the Posit threshold and analyzing trade-offs.

## Limitations

- The sensitivity metric combines gradient magnitude and quantization error, but the exact weighting between these factors is not specified, creating uncertainty in layer selection.
- The tanh-based gradient approximation for Posit weights is proposed but not extensively validated against alternative approximations or the true gradient landscape.
- The 10% Posit usage threshold is fixed without exploring how varying this threshold affects accuracy and energy consumption across different domains.

## Confidence

- **High Confidence**: The energy overhead measurement (0.19%) is well-defined and measurable. The hardware description for the Posit/FixP MAC unit is sufficiently detailed for implementation verification.
- **Medium Confidence**: The 1.5% accuracy improvement claim is supported by results across multiple datasets, but the layer selection process introduces variability that could affect reproducibility.
- **Low Confidence**: The effectiveness of the tanh-based gradient approximation for training stability and convergence has limited validation, particularly for deeper networks.

## Next Checks

1. **Sensitivity Metric Verification**: Implement the sl metric on a small CNN and verify that layers selected for Posit4 align with intuition (high gradient magnitudes, large quantization error gaps between FixP4 and Posit4). Compare with random selection to quantify impact.
2. **Gradient Approximation Testing**: Train the same network with three gradient estimators: standard STE, the proposed tanh-based estimator, and a linear approximation. Compare training stability, convergence speed, and final accuracy to isolate the impact of the custom estimator.
3. **Energy-Accuracy Pareto Analysis**: Systematically vary η (the Posit usage limit) from 5% to 20% and measure both accuracy and energy overhead. This will reveal whether 10% is truly optimal or if the trade-off curve suggests different operating points for different application requirements.