---
ver: rpa2
title: A Multi-level Distillation based Dense Passage Retrieval Model
arxiv_id: '2312.16821'
source_url: https://arxiv.org/abs/2312.16821
tags:
- retrieval
- distillation
- query
- ranker
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dense passage retrieval,
  where dual-encoder models lack interaction between queries and documents, while
  cross-encoder models are computationally expensive. To overcome these limitations,
  the authors propose a multi-level distillation based dense passage retrieval model
  (MD2PR).
---

# A Multi-level Distillation based Dense Passage Retrieval Model

## Quick Facts
- **arXiv ID**: 2312.16821
- **Source URL**: https://arxiv.org/abs/2312.16821
- **Reference count**: 38
- **Primary result**: MD2PR outperforms 11 baseline models on MS-MARCO and Natural Questions datasets using sentence and word-level distillation from cross-encoder to dual-encoder

## Executive Summary
This paper addresses the computational efficiency challenge in dense passage retrieval by proposing a multi-level distillation approach that transfers knowledge from a computationally expensive cross-encoder ranker to a more efficient dual-encoder retriever. The method distills knowledge at both sentence and word levels, enabling the retriever to capture rich query-document interactions without the high computational cost of the ranker. Additionally, a dynamic filtering method identifies false negatives during training to improve representation space consistency.

## Method Summary
MD2PR uses a cross-encoder ranker as a teacher to guide the training of a dual-encoder retriever through multi-level distillation. At the sentence level, KL divergence aligns the relevance distributions between ranker and retriever. At the word level, MSE loss aligns the attention matrices and hidden states. The method also includes dynamic false negative filtering that masks high-scoring negative samples identified by the ranker during training. The total loss combines contrastive loss, sentence-level distillation, and word-level distillation objectives.

## Key Results
- MD2PR achieves MRR@10 of 0.362 on MS-MARCO Dev set, outperforming baseline models including ADORE, TCT, and ANCE.
- On Natural Questions, MD2PR achieves R@100 of 0.912, showing strong performance on out-of-domain data.
- The combined sentence-level and word-level distillation improves performance compared to using either individually.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level distillation transfers cross-encoder knowledge to dual-encoder at both sentence and word levels, improving retrieval quality without high computational cost.
- Mechanism: The cross-encoder ranker computes attention scores and CLS embeddings that capture rich query-document interactions. These are distilled into the dual-encoder retriever via KL divergence (sentence-level) and MSE loss (word-level) between attention matrices and hidden states.
- Core assumption: The dual-encoder can learn to approximate the cross-encoder's interaction patterns through distillation losses without direct attention computation.
- Evidence anchors:
  - [abstract] "Sentence-level distillation enhances the dual-encoder on capturing the themes and emotions of sentences. Word-level distillation improves the dual-encoder in analysis of word semantics and relationships."
  - [section 3.2] "In the ranker, for each query ùëû of length |ùëû| and each document ùëë of length |ùëë |, the concatenation of the query and document is inputted into the model, resulting in an attention matrix..."
  - [corpus] Weak evidence - no direct citations about multi-level distillation, but related works on knowledge distillation and dense retrieval provide indirect support.
- Break condition: If the cross-encoder ranker's attention patterns cannot be effectively approximated by the dual-encoder's hidden states, or if the distillation loss fails to converge.

### Mechanism 2
- Claim: Dynamic false negative filtering improves representation space consistency by excluding high-relevance negative samples.
- Mechanism: During training, the ranker's relevance scores are used to identify negative samples that score higher than the positive sample. These are masked with negative infinity in the loss calculation, preventing the retriever from pushing away truly relevant samples.
- Core assumption: High-scoring negatives identified by the ranker are more likely to be false negatives (semantically relevant but incorrectly labeled).
- Evidence anchors:
  - [abstract] "Furthermore, we propose a simple dynamic filtering method, which updates the threshold during multiple training iterations to ensure the effective identification of false negatives and thus obtains a more comprehensive semantic representation space."
  - [section 3.3] "We believe that positive samples should be highly matched to the query, and hence, their scores should be higher than those of other documents. As a result, if there are documents with scores higher than the given positive sample, it indicates a high match with the query and should be considered a positive sample rather a false negative."
  - [corpus] No direct evidence - this appears to be an original contribution not covered in related literature.
- Break condition: If the ranker's scores are unreliable or if too many negatives are filtered out, potentially reducing the effective training signal.

### Mechanism 3
- Claim: Combining sentence-level and word-level distillation provides complementary semantic information that enhances retriever performance.
- Mechanism: Sentence-level distillation captures overall semantic similarity through CLS embeddings, while word-level distillation captures fine-grained word relationships through attention matrices. The combined loss ensures both coarse and fine semantic alignment.
- Core assumption: The two levels of distillation capture different aspects of semantic similarity that reinforce each other.
- Evidence anchors:
  - [abstract] "In this model, we distill the knowledge learned from the cross-encoder to the dual-encoder at both the sentence level and word level."
  - [section 3.4] "Consequently, we use both sentence-level output and word-level output for distillation. This allows the retriever to learn the knowledge for text matching more comprehensively and deeply, improving its generalization ability and performance on unseen texts."
  - [section 4.2.2] "The combined use of sentence-level and word-level distillation further improved the model performance compared to using them individually... The interaction between the two levels of distillation can reflect the inherent characteristics of the data more accurately."
- Break condition: If the combined loss causes gradient conflicts or if one level dominates training at the expense of the other.

## Foundational Learning

- Concept: Knowledge distillation in neural networks
  - Why needed here: The entire method relies on transferring knowledge from a complex teacher (cross-encoder) to a simpler student (dual-encoder) model.
  - Quick check question: What are the two main components of the distillation loss in most knowledge distillation setups?

- Concept: Contrastive learning for information retrieval
  - Why needed here: The retrieval model is trained using contrastive loss to push apart query-document pairs with different relevance levels.
  - Quick check question: How does InfoNCE loss differ from standard cross-entropy in retrieval tasks?

- Concept: Attention mechanisms in transformer models
  - Why needed here: The word-level distillation specifically transfers attention patterns from the ranker to the retriever.
  - Quick check question: What is the difference between self-attention and cross-attention in transformer models?

## Architecture Onboarding

- Component map:
  - Cross-encoder ranker (teacher): DeBERTa model that computes attention matrices and CLS embeddings
  - Dual-encoder retriever (student): BERT model with separate query and document encoders
  - Dynamic filtering module: Threshold-based false negative identification
  - Distillation losses: KL divergence (sentence-level) and MSE (word-level)

- Critical path:
  1. Ranker processes query-document pairs to generate scores, attention matrices, and CLS embeddings
  2. Retriever processes queries and documents separately to generate embeddings and hidden states
  3. Dynamic filtering identifies false negatives based on ranker scores
  4. Distillation losses align ranker and retriever outputs at both levels
  5. Total loss combines contrastive loss, distillation losses, and filtering mask

- Design tradeoffs:
  - Ranker complexity vs distillation effectiveness: Larger rankers provide better guidance but increase training time
  - False negative filtering aggressiveness: Too aggressive filtering may remove useful negative samples
  - Distillation weight balance: Sentence-level vs word-level loss weighting affects final performance

- Failure signatures:
  - Ranker overfits to training data, providing poor guidance
  - Retriever fails to approximate attention patterns despite distillation
  - Dynamic filtering removes too many negatives, causing representation collapse
  - Combined losses create gradient conflicts or instability

- First 3 experiments:
  1. Implement basic knowledge distillation from cross-encoder to dual-encoder without filtering
  2. Add dynamic false negative filtering to the basic distillation setup
  3. Compare sentence-level vs word-level distillation effectiveness separately

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of model size for the ranker and retriever impact the performance of MD2PR, and what is the optimal balance between model size and training efficiency?
- Basis in paper: [explicit] The paper discusses the effect of different model sizes on MD2PR's performance, noting that larger models can improve performance but may lead to overfitting or increased training time.
- Why unresolved: While the paper provides some insights into the relationship between model size and performance, it does not establish a clear guideline for determining the optimal balance between model size and training efficiency.
- What evidence would resolve it: Further experiments comparing the performance of MD2PR using different model sizes for the ranker and retriever, while controlling for training time and overfitting, would help determine the optimal balance.

### Open Question 2
- Question: How does the dynamic filtering method for false negatives in MD2PR compare to other methods in terms of effectiveness and computational efficiency?
- Basis in paper: [explicit] The paper introduces a dynamic filtering method for false negatives, but does not provide a comprehensive comparison with other methods in terms of effectiveness and computational efficiency.
- Why unresolved: While the paper suggests that the dynamic filtering method is effective, it does not provide a detailed comparison with other methods to determine its relative performance and efficiency.
- What evidence would resolve it: Experiments comparing the dynamic filtering method to other false negative filtering methods, such as those used in ANCE or SimANS, in terms of effectiveness and computational efficiency, would help determine its relative performance.

### Open Question 3
- Question: How does the word-level distillation in MD2PR impact the model's ability to capture query-document interactions, and what are the potential limitations of this approach?
- Basis in paper: [explicit] The paper discusses the word-level distillation in MD2PR and its role in capturing query-document interactions, but does not provide a detailed analysis of its impact or potential limitations.
- Why unresolved: While the paper suggests that word-level distillation is effective in capturing query-document interactions, it does not provide a comprehensive analysis of its impact or potential limitations, such as the effect of noise in the attention scores or hidden states.
- What evidence would resolve it: Further experiments analyzing the impact of word-level distillation on the model's ability to capture query-document interactions, as well as potential limitations such as noise in the attention scores or hidden states, would help determine its effectiveness and potential drawbacks.

## Limitations

- The effectiveness of MD2PR heavily depends on the reliability of the cross-encoder ranker's attention patterns and relevance scores, which are not thoroughly validated.
- The dynamic false negative filtering method may remove too many negative samples, potentially reducing the effective training signal and causing suboptimal representation learning.
- The paper lacks detailed analysis of the computational overhead introduced by the multi-level distillation process compared to simpler baselines.

## Confidence

- **High Confidence**: The overall experimental design and comparison methodology are sound, with clear metrics and baseline comparisons.
- **Medium Confidence**: The distillation mechanisms (sentence-level and word-level) are well-motivated and theoretically grounded, though the exact implementation details require careful verification.
- **Low Confidence**: The dynamic false negative filtering method's effectiveness depends heavily on the ranker's reliability, which is not thoroughly validated in the paper.

## Next Checks

1. Conduct ablation studies to isolate the contribution of each distillation component (sentence-level, word-level, and dynamic filtering) to verify their individual impact on performance.
2. Evaluate the ranker's reliability by measuring its consistency and accuracy on a held-out validation set before using it for distillation.
3. Test the robustness of the model to different ranker architectures (e.g., smaller or larger models) to ensure the distillation process is not overly dependent on a specific ranker configuration.