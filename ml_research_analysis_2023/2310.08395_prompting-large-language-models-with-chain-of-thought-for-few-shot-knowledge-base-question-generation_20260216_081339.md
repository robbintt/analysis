---
ver: rpa2
title: Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge
  Base Question Generation
arxiv_id: '2310.08395'
source_url: https://arxiv.org/abs/2310.08395
tags:
- logical
- forms
- kqg-cot
- join
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles few-shot Knowledge Base Question Generation
  (KBQG) by leveraging large language models (LLMs) with Chain-of-Thought (CoT) prompting.
  The method, KQG-CoT, treats KBQG as a reasoning problem, breaking down complex question
  generation into a series of sub-question steps guided by CoT prompts.
---

# Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation

## Quick Facts
- arXiv ID: 2310.08395
- Source URL: https://arxiv.org/abs/2310.08395
- Authors: 
- Reference count: 25
- Key outcome: KQG-CoT+ achieves 18.25, 10.72, and 10.18 absolute points improvement on BLEU-4, METEOR, and ROUGE-L over state-of-the-art few-shot results on PathQuestions dataset

## Executive Summary
This paper addresses few-shot Knowledge Base Question Generation (KBQG) by leveraging large language models with Chain-of-Thought (CoT) prompting. The method treats KBQG as a reasoning problem, breaking down complex question generation into stepwise sub-question generation guided by CoT prompts. KQG-CoT+ improves upon the base method by sorting demonstrations by complexity, achieving state-of-the-art few-shot results across three KBQG datasets.

## Method Summary
The KQG-CoT method operates in two stages: first selecting supportive logical forms from an unlabeled data pool using clustering and sampling techniques, then constructing prompts that guide LLMs through a reasoning chain to generate complete questions. The enhanced KQG-CoT+ version sorts logical forms by complexity (number of logical form jumps) for improved performance. The approach uses structured encoding to capture logical form diversity, K-means clustering to group similar structures, and greedy sampling to select maximally diverse demonstrations for prompt construction.

## Key Results
- KQG-CoT+ consistently outperforms other prompting baselines across three KBQG datasets
- Achieves 18.25 absolute BLEU-4 improvement over state-of-the-art few-shot results on PathQuestions
- Demonstrates 10.72 and 10.18 absolute improvements on METEOR and ROUGE-L metrics respectively for PathQuestions
- Shows robustness across different KBQG datasets with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured encoding + clustering selects diverse logical forms that improve prompt quality
- Mechanism: Replaces KB entities/relations with abstract tokens, encodes structures with Sentence-BERT, clusters by similarity, then samples maximally diverse forms within each cluster
- Core assumption: Logical form structure is the dominant signal for prompt diversity; semantic content is secondary for few-shot generalization
- Evidence anchors: [section] describes structure extraction, encoding, clustering, and sampling process; [corpus] shows lower average semantic similarity in selected logical forms

### Mechanism 2
- Claim: Chain-of-Thought prompting decomposes complex question generation into stepwise sub-question generation
- Mechanism: Constructs nested expansion where each step generates a simpler sub-question, gradually building up to the full question
- Core assumption: LLMs can follow multi-step reasoning chains when provided with explicit sub-task delineations
- Evidence anchors: [abstract] states generation is split into series of sub-question generation; [section] describes writing rationales to split complete question generation

### Mechanism 3
- Claim: Sorting demonstrations by complexity improves performance over random or uncertainty-based ordering
- Mechanism: Orders demonstrations from simplest to most complex logical forms (ascending by number of logical form jumps)
- Core assumption: LLMs benefit from curriculum-like ordering of examples, where simpler examples build foundational understanding
- Evidence anchors: [section] extends KQG-CoT into KQG-CoT+ via sorting; [corpus] shows efficacy of proposed approach

## Foundational Learning

- Concept: Knowledge Base Question Generation (KBQG)
  - Why needed here: Understanding KBQG is essential to grasp why few-shot prompting is challenging—logical forms are structured KB queries, not natural language
  - Quick check question: What distinguishes a logical form in KBQG from a typical text generation input?

- Concept: In-Context Learning (ICL) with LLMs
  - Why needed here: The method relies on ICL rather than fine-tuning; knowing how demonstrations influence LLM output is critical
  - Quick check question: How does ICL differ from standard fine-tuning in terms of data requirements and model adaptation?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT is the core reasoning strategy enabling stepwise generation; without it, complex logical forms would overwhelm single-step generation
  - Quick check question: Why does decomposing a generation task into sub-steps help LLMs produce more accurate outputs?

## Architecture Onboarding

- Component map: Logical forms -> Structure Encoder -> Clustering Module -> Sampling Module -> Prompt Constructor -> LLM -> Output questions

- Critical path:
  1. Extract and encode logical form structures
  2. Cluster structures and sample diverse forms
  3. Construct stepwise CoT prompts from sampled forms
  4. Append target logical form to prompt
  5. Generate question via LLM
  6. Extract final sub-question as output

- Design tradeoffs:
  - Structured encoding vs. full semantic encoding: Structure-only encoding improves diversity but loses some semantic nuance
  - Fixed demonstration count (k=12) vs. adaptive: Fixed count simplifies engineering but may under/overwhelm for different dataset complexities
  - Manual CoT template writing vs. automated: Manual ensures quality but introduces human bias and limits scalability

- Failure signatures:
  - Poor diversity in selected demonstrations → overfitting to narrow logical patterns
  - Incoherent sub-questions → CoT reasoning chain breaks
  - Context window overflow → incomplete prompt processing
  - Low BLEU/METEOR scores → mismatch between prompt design and LLM capabilities

- First 3 experiments:
  1. Vary k (number of demonstrations) from 4 to 20 and measure BLEU-4/ROUGE-L to find optimal size
  2. Replace K-means clustering with hierarchical clustering to test impact on demonstration diversity
  3. Compare manual CoT template writing vs. GPT-4-generated templates for sub-question generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of using handcrafted prompts in KQG-CoT, and how can these limitations be mitigated?
- Basis in paper: The paper mentions that handcrafted prompts rely on personal knowledge and experience, which can introduce subjective biases
- Why unresolved: The paper acknowledges the issue but does not provide a detailed analysis of the limitations or potential solutions
- What evidence would resolve it: A study comparing the performance of KQG-CoT with handcrafted prompts against KQG-CoT using automatically generated prompts, along with an analysis of the biases introduced by human-written prompts

### Open Question 2
- Question: How does the performance of KQG-CoT scale with increasing complexity of logical forms, and what is the maximum complexity it can handle effectively?
- Basis in paper: The paper discusses the ability of KQG-CoT to handle complex logical forms with nested structures, but does not provide a detailed analysis of performance across varying levels of complexity
- Why unresolved: The paper focuses on demonstrating the effectiveness of KQG-CoT but does not explore the upper limits of its capability
- What evidence would resolve it: A comprehensive evaluation of KQG-CoT's performance on logical forms with varying levels of complexity, including a detailed analysis of the factors that limit its effectiveness

### Open Question 3
- Question: How does the performance of KQG-CoT compare to other few-shot learning methods that do not rely on large language models, such as meta-learning or data augmentation approaches?
- Basis in paper: The paper compares KQG-CoT to other few-shot learning methods that use large language models but does not compare it to non-LLM-based few-shot learning approaches
- Why unresolved: The paper focuses on demonstrating the effectiveness of KQG-CoT within the context of LLM-based few-shot learning but does not explore how it compares to other few-shot learning paradigms
- What evidence would resolve it: A comparative study of KQG-CoT against non-LLM-based few-shot learning methods on the same KBQG datasets, including an analysis of the strengths and weaknesses of each approach

## Limitations
- Manual prompt engineering introduces potential human bias and scalability concerns
- Complexity sorting heuristic is simplistic and may not capture true difficulty spectrum
- Evaluation focuses on surface metrics without deeper semantic analysis of logical form meaning
- Performance on out-of-distribution logical forms or different KB schemas remains untested

## Confidence
- High confidence: Empirical results showing KQG-CoT+ outperforming baselines on all three datasets, with particularly strong improvements on PathQuestions
- Medium confidence: Structured encoding effectively captures diversity needed for few-shot generalization
- Medium confidence: Effectiveness of complexity-based ordering (KQG-CoT+ vs KQG-CoT)
- Low confidence: Assertion that CoT prompting is the primary driver of performance improvements

## Next Checks
1. Conduct ablation study on CoT necessity by removing reasoning chain while keeping all other components identical
2. Evaluate method on logical forms from KB schemas not seen during demonstration selection to test generalization
3. Perform human evaluation of semantic fidelity comparing whether generated questions capture intended logical form meaning beyond surface-level metrics