---
ver: rpa2
title: Transparency challenges in policy evaluation with causal machine learning --
  improving usability and accountability
arxiv_id: '2310.13240'
source_url: https://arxiv.org/abs/2310.13240
tags:
- causal
- learning
- machine
- treatment
- forest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines transparency challenges in applying causal machine
  learning to policy evaluation, particularly when estimating heterogeneous treatment
  effects. While such methods offer flexibility and power, they rely on black-box
  models that lack global interpretability, complicating accountability to affected
  individuals and usability for analysts and decision-makers.
---

# Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability

## Quick Facts
- arXiv ID: 2310.13240
- Source URL: https://arxiv.org/abs/2310.13240
- Reference count: 16
- The paper finds that existing explainable AI tools are poorly suited for understanding causal machine learning models, and that simplifying these models for interpretability leads to substantial accuracy loss.

## Executive Summary
This paper examines the transparency challenges in applying causal machine learning to policy evaluation, particularly when estimating heterogeneous treatment effects. The authors find that existing explainable AI tools like SHAP values and variable importance measures provide limited global insight and are not well-suited to the causal forest's structure. When they attempt to simplify models for interpretability through single tree extraction or best linear projection, they observe substantial accuracy loss. The study concludes that new transparency tools specifically designed for causal machine learning are needed to properly understand these models and their algorithms.

## Method Summary
The study uses Australian HILDA survey data from 2021-22 (Wave 21) to estimate returns to education using a causal forest model. The analysis focuses on 34 pre-treatment variables identified as valid controls. The authors apply various transparency tools including variable importance measures, SHAP values, and refutation tests to analyze the causal forest. They also test model simplification approaches like extracting a single decision tree and fitting a best linear projection to evaluate the interpretability-accuracy tradeoff.

## Key Results
- Existing XAI tools like SHAP values and variable importance measures provide limited global insight into causal forest models
- Attempts to extract a single tree or fit a best linear projection lead to substantial accuracy loss (mean absolute loss of 144% when moving from 100,000 trees to a single tree)
- Causal machine learning methods' orthogonalization step and multi-model structure make standard interpretability tools poorly suited
- New transparency tools specifically designed for causal machine learning are needed to properly understand these models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal machine learning transparency problems arise because interpretability tools designed for predictive ML fail when applied to causal models due to the orthogonalization step and multi-model structure.
- Mechanism: In causal forests, the nuisance models (treatment propensity and outcome) orthogonalize the data before the heterogeneity model is fit. Standard XAI tools like SHAP assume a single predictive model, so they either cannot properly trace through this pipeline or give misleading explanations that conflate nuisance modeling with causal effect estimation.
- Core assumption: The causal forest's estimation strategy fundamentally changes the relationship between inputs and outputs compared to standard predictive models.
- Evidence anchors:
  - [abstract] "existing tools for understanding black-box predictive models are poorly suited to causal machine learning"
  - [section 3.2.1] "causal machine learning methods generally involve the fitting of several models with different purposes where predictive applications typically involve fitting one"
  - [corpus] Weak - no direct citations in neighbor papers, but mechanism is consistent with transparency challenges discussed in related work.
- Break condition: If a causal ML method uses a single unified model (no nuisance step) or if interpretability tools are specifically adapted to trace through orthogonalization, this mechanism may not hold.

### Mechanism 2
- Claim: Transparency is particularly important in causal ML because these models are used to inform human decision-makers rather than automate decisions directly, creating a "joint decision-making" scenario where both the algorithm and human share responsibility.
- Mechanism: Because causal models provide evidence about data-generating processes to humans who then make policy decisions, the human decision-maker must understand the model to weigh its evidence appropriately and be accountable for decisions. This creates a need for both usability (for the analyst) and accountability (for oversight) that doesn't exist in fully automated predictive systems.
- Core assumption: Causal ML models are primarily decision-support tools rather than autonomous decision-makers in policy contexts.
- Evidence anchors:
  - [abstract] "it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability"
  - [section 3.1] "transparency is not just about understanding models though, it is also about holding human beings accountable for the consequences of these models"
  - [corpus] Weak - neighbor papers don't directly address human-in-the-loop accountability, but mechanism aligns with broader AI governance literature.
- Break condition: If causal ML models are used for fully automated decision-making rather than informing human decisions, this mechanism may not apply.

### Mechanism 3
- Claim: The "Rashomon Curve" effect makes it difficult to create interpretable causal models without substantial accuracy loss because the best interpretable model (like a single decision tree) may be far from the optimal black-box solution for the same problem.
- Mechanism: As a complex model like a causal forest is simplified to increase interpretability (e.g., extracting a single tree), performance degrades significantly because the problem space doesn't contain redundant rules that would allow simplification with minimal loss. This creates an unavoidable tradeoff between interpretability and accuracy.
- Core assumption: The underlying causal estimation problem doesn't have sufficient structure that allows for accurate interpretable approximations.
- Evidence anchors:
  - [section 5.2.1] "the trade-off for moving to a single tree seems to be a poor one... the mean absolute loss for moving from 100,000 trees to a single tree was 144% of the comparison value on average"
  - [section 5.2.1] "there seem to be substantial misestimates with low incomes being grossly overestimated in the smaller models"
  - [corpus] Weak - no direct citations in neighbor papers, but mechanism is consistent with interpretability literature.
- Break condition: If the underlying causal problem has simple, redundant structure that allows for accurate interpretable approximations, this mechanism may not hold.

## Foundational Learning

- Concept: Orthogonalization in causal inference
  - Why needed here: Understanding how causal ML methods like double machine learning and causal forests use orthogonalization to achieve unbiased treatment effect estimates is crucial for understanding why standard interpretability tools fail.
  - Quick check question: What is the purpose of orthogonalizing treatment assignment and outcome in causal machine learning methods?

- Concept: Heterogeneous treatment effects vs. average treatment effects
  - Why needed here: Causal ML methods like causal forests estimate heterogeneous treatment effects (varying by individual characteristics) rather than just average effects, which changes how we need to interpret and explain model outputs.
  - Quick check question: How does estimating heterogeneous treatment effects differ from estimating average treatment effects in terms of what the model is trying to capture?

- Concept: SHAP values and local vs. global interpretability
  - Why needed here: Understanding the difference between local explanations (like SHAP values for individual predictions) and global model understanding is key to evaluating whether transparency tools are adequate for causal ML applications.
  - Quick check question: What is the difference between local and global interpretability, and why might local explanations be insufficient for understanding causal ML models?

## Architecture Onboarding

- Component map:
  - Nuisance models: Two separate models for treatment propensity and outcome prediction
  - Heterogeneity model: Causal forest that estimates conditional average treatment effects
  - Interpretability layer: Various tools (SHAP, variable importance, single tree extraction, BLP) applied to different components
  - Evaluation framework: Refutation tests and performance comparisons

- Critical path:
  1. Data preprocessing and variable selection (pre-treatment only)
  2. Fit nuisance models (treatment propensity and outcome)
  3. Fit causal forest on orthogonalized data
  4. Apply interpretability tools to analyze heterogeneity
  5. Validate with refutation tests and performance checks

- Design tradeoffs:
  - Accuracy vs. interpretability: Complex models give better estimates but are less transparent
  - Local vs. global explanations: SHAP gives detailed individual explanations but may miss global patterns
  - Variable selection: Including too many variables can introduce bias; too few can miss important heterogeneity

- Failure signatures:
  - Poor nuisance model fit indicated by refutation tests showing non-zero effects for randomized treatment/outcome
  - Variable importance measures dominated by post-treatment variables (suggesting collider bias)
  - SHAP values that seem inconsistent across similar individuals
  - Best linear projection that fails to capture key nonlinear relationships

- First 3 experiments:
  1. Run refutation tests (randomized treatment/outcome) to check if nuisance model fit is affecting treatment effect estimates
  2. Compare SHAP values for a few representative individuals to identify if explanations seem reasonable and consistent
  3. Fit a best linear projection of the heterogeneity model and compare its performance to the full causal forest to quantify the interpretability-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using causal machine learning methods on policy decisions when compared to traditional causal inference methods?
- Basis in paper: [inferred] The paper discusses the potential benefits and challenges of using causal machine learning in policy evaluation, but acknowledges a lack of evidence on its actual impact on decision-making.
- Why unresolved: There is limited research on the real-world application of causal machine learning in policy-making, and the extent to which these methods influence decisions is unknown.
- What evidence would resolve it: Studies comparing policy outcomes using causal machine learning versus traditional methods, or surveys of policymakers on their use and perception of these methods.

### Open Question 2
- Question: How can accountability be ensured when using complex causal machine learning models in policy evaluation?
- Basis in paper: [explicit] The paper highlights the importance of accountability in policy evaluation and the challenges posed by the black-box nature of causal machine learning models.
- Why unresolved: The paper suggests that existing transparency tools are insufficient for causal machine learning, and new methods are needed to ensure accountability.
- What evidence would resolve it: Development and testing of new transparency tools specifically designed for causal machine learning, or case studies demonstrating successful accountability mechanisms in practice.

### Open Question 3
- Question: What is the trade-off between model interpretability and predictive accuracy in causal machine learning?
- Basis in paper: [explicit] The paper discusses the limitations of existing interpretability tools and the potential loss of accuracy when simplifying models.
- Why unresolved: The paper provides a case study, but the trade-off may vary depending on the specific application and data.
- What evidence would resolve it: Systematic comparison of interpretability and accuracy across different causal machine learning methods and applications, or development of methods that balance interpretability and accuracy.

## Limitations
- Analysis relies on a single case study using Australian HILDA survey data, which may not generalize to other contexts
- Only tests a subset of available interpretability tools, potentially missing newer methods that may perform differently
- Focuses on understanding model outputs rather than validating the causal identification assumptions underlying the analysis

## Confidence

- **High**: The fundamental challenge that standard XAI tools are poorly suited to causal ML models due to their multi-model structure and orthogonalization steps.
- **Medium**: The specific performance tradeoffs observed when simplifying causal forest models for interpretability.
- **Low**: The generalizability of these findings to other causal ML applications and transparency tools not tested in this study.

## Next Checks
1. Test the same transparency tools on a different causal ML application (e.g., different policy domain or dataset) to assess generalizability.
2. Compare performance of newer interpretability methods designed specifically for causal inference (e.g., causal SHAP variants) against those tested in this study.
3. Conduct a formal user study with policy analysts to evaluate which transparency tools actually improve usability and accountability in practice.