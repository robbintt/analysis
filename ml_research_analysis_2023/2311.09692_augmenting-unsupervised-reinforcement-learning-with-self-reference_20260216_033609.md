---
ver: rpa2
title: Augmenting Unsupervised Reinforcement Learning with Self-Reference
arxiv_id: '2311.09692'
source_url: https://arxiv.org/abs/2311.09692
tags:
- agent
- learning
- query
- reinforcement
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of nonstationarity and forgetting
  in unsupervised reinforcement learning (URL) by introducing the Self-Reference (SR)
  approach. SR is an add-on module that explicitly leverages historical information
  to improve agent performance during pretraining and finetuning.
---

# Augmenting Unsupervised Reinforcement Learning with Self-Reference

## Quick Facts
- **arXiv ID**: 2311.09692
- **Source URL**: https://arxiv.org/abs/2311.09692
- **Reference count**: 40
- **Primary result**: SR achieves state-of-the-art results on URLB with 86% IQM and 16% Optimality Gap reduction

## Executive Summary
This paper introduces Self-Reference (SR), an add-on module that addresses nonstationarity and forgetting in unsupervised reinforcement learning by leveraging historical information. SR uses a query module to retrieve relevant past experiences and aggregates them into a reference vector that aids the agent during both pretraining and finetuning. The method achieves state-of-the-art performance on the Unsupervised Reinforcement Learning Benchmark, with significant improvements in IQM and Optimality Gap metrics.

## Method Summary
Self-Reference is an add-on module that augments existing unsupervised RL algorithms by incorporating historical trajectory information. During pretraining, it mitigates nonstationarity of intrinsic rewards by explicitly modeling state visitation patterns through retrieved reference vectors. During finetuning, it prevents forgetting of exploratory behaviors by maintaining references to valuable past trajectories. The method uses a query network to retrieve top-k nearest neighbors from historical experiences, aggregates them via multi-head attention, and concatenates the resulting reference vector with the agent's state features. The architecture is compatible with any RL algorithm and can be distilled after training to remove the retrieval overhead during deployment.

## Key Results
- SR achieves 86% Interquartile Mean (IQM) on URLB, representing state-of-the-art performance
- Reduces Optimality Gap by 16% compared to previous methods
- Improves existing algorithms by up to 17% IQM and reduces Optimality Gap by 31%
- Increases sample efficiency and addresses forgetting of exploratory behaviors during finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using historical trajectories as references during both pretraining and finetuning reduces nonstationarity in the intrinsic reward function.
- Mechanism: The query module retrieves nearest neighbors from past experiences and aggregates them into a reference vector that is concatenated with the agent's state features, effectively providing the agent with explicit knowledge of state visitation frequencies.
- Core assumption: The agent can benefit from explicit historical information about state visitation patterns during pretraining where intrinsic rewards implicitly depend on these patterns.
- Evidence anchors:
  - [abstract] "During pretraining, an agent's past experiences can be explicitly utilized to mitigate the nonstationarity of intrinsic rewards."
  - [section IV-A] "We argue that the query module should have the flexibility to query neighbors... information on possible future visitations."
  - [corpus] Weak - no direct evidence in related papers about nonstationarity mitigation.
- Break condition: If the historical information becomes stale or irrelevant to current policy, the reference vector may introduce noise rather than helpful context.

### Mechanism 2
- Claim: Explicitly referencing historical exploratory behaviors during finetuning prevents catastrophic forgetting of these behaviors.
- Mechanism: By retrieving and incorporating past exploratory trajectories during finetuning, the agent maintains awareness of previously discovered states and behaviors, reducing unlearning of exploratory skills.
- Core assumption: Exploratory behaviors learned during pretraining are valuable and should be preserved during finetuning for efficient adaptation.
- Evidence anchors:
  - [abstract] "In the finetuning phase, referencing historical trajectories prevents the unlearning of valuable exploratory behaviors."
  - [section IV-C] "we can view some exploratory behaviors as beneficial for quickly learning the new task and as good reference trajectories for the agent to maintain some of its favorable characteristics."
  - [corpus] Weak - while retrieval-augmented RL exists in the corpus, none specifically address catastrophic forgetting in pretrain-finetune scenarios.
- Break condition: If the retrieved trajectories are not representative of valuable exploratory behaviors or are too outdated relative to current task requirements.

### Mechanism 3
- Claim: The query module's flexibility in selecting reference trajectories improves performance over static retrieval methods.
- Mechanism: The learned query module can dynamically select appropriate reference trajectories based on current state and task requirements, rather than using fixed criteria like nearest neighbors only.
- Core assumption: Dynamic selection of reference trajectories based on learned queries is more effective than static retrieval methods.
- Evidence anchors:
  - [section IV-A] "To enhance the system's adaptability, we developed a module capable of determining the optimal queries" and "We hypothesize that expanding the retrieved states to trajectories provides more information."
  - [section VI] "Utilizing the nearest neighbor of the current state or retrieving random trajectories proves somewhat helpful, illustrating how explicitly providing the agent with the history of transitions is beneficial. Moreover, SR achieves the best result, emphasizing the query module's flexibility."
  - [corpus] Moderate - some related papers like "Learning Diverse Policies with Soft Self-Generated Guidance" suggest learned guidance can be beneficial.
- Break condition: If the query module learns suboptimal query strategies or if the learned queries become too complex relative to the benefit gained.

## Foundational Learning

- Concept: Nonstationary Markov Decision Processes
  - Why needed here: Understanding why intrinsic rewards that depend on visitation history create nonstationarity is crucial for appreciating why historical references help.
  - Quick check question: Why does a count-based exploration reward create a nonstationary MDP, and how does explicitly modeling state visitation history address this?

- Concept: Catastrophic Forgetting in Sequential Learning
  - Why needed here: The mechanism preventing unlearning of exploratory behaviors during finetuning relies on understanding how fine-tuning can overwrite previously learned skills.
  - Quick check question: How does the standard pretrain-then-finetune paradigm lead to forgetting of exploratory behaviors, and what evidence supports this claim?

- Concept: Attention Mechanisms and Feature Aggregation
  - Why needed here: The reference vector construction uses multi-head attention to combine retrieved trajectories with current state features, which is central to the SR architecture.
  - Quick check question: How does multi-head attention combine the current state (as query) with retrieved trajectory features (as keys and values) to create a useful reference vector?

## Architecture Onboarding

- Component map: State → Query Module → Faiss Retrieval → MHA Aggregation → DDPG Actor/Critic → Action
- Critical path: State → Query Module → Faiss Retrieval → MHA Aggregation → DDPG Actor/Critic → Action
- Design tradeoffs:
  - Larger k and D provide more information but increase computation and memory
  - Context window size affects how far back in history the agent can reference
  - Query module adds parameters but enables dynamic selection of references
  - Distillation phase removes retrieval overhead at deployment cost of one-time training
- Failure signatures:
  - Degraded performance when reference vector adds noise rather than useful information
  - Slow training due to retrieval and attention computation overhead
  - Query module collapse to identity function or random output
- First 3 experiments:
  1. Run baseline DDPG on URLB tasks to establish performance without SR
  2. Implement SR with k=1, D=1 (minimal retrieval) to verify basic integration works
  3. Scale up k and D incrementally to find optimal balance between performance and computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Self-Reference scale with different numbers of nearest neighbors (k) and neighbor sequence lengths (D)?
- Basis in paper: [explicit] The paper conducts experiments varying k and D to assess their impact on performance, suggesting an optimal balance between computational demands and performance efficiency.
- Why unresolved: The paper provides results for specific values of k and D but does not explore the full range of possible values or their combined effects.
- What evidence would resolve it: Comprehensive experiments varying both k and D across a wide range, and analyzing their combined effects on performance and computational efficiency.

### Open Question 2
- Question: How does the Self-Reference method perform in environments with observational states (e.g., images) compared to environments with direct state access?
- Basis in paper: [inferred] The paper mentions that querying in a compact and meaningful space could significantly extend the method under observational circumstances, implying that the current state space querying might be inefficient for image-based environments.
- Why unresolved: The paper does not provide experimental results for environments with observational states, leaving the effectiveness of the method in such scenarios untested.
- What evidence would resolve it: Experiments applying Self-Reference to image-based environments and comparing its performance to direct state access environments.

### Open Question 3
- Question: What is the impact of using multiple queries instead of a single query in the Self-Reference method?
- Basis in paper: [explicit] The paper suggests that providing more than one query could allow the agent to access multi-modal distributed information, indicating potential improvements in performance.
- Why unresolved: The paper does not implement or test the use of multiple queries, leaving the potential benefits unexplored.
- What evidence would resolve it: Experiments comparing the performance of the Self-Reference method with single and multiple queries, analyzing the differences in exploration and learning efficiency.

## Limitations

- The computational overhead of the retrieval and attention mechanisms during training is not quantified, making it unclear if the performance gains justify the additional complexity
- The distillation phase, while promising for deployment, is not empirically validated for its effectiveness in removing the retrieval dependency while maintaining performance
- The paper demonstrates strong performance on URLB but does not extensively validate whether the benefits transfer to reward-driven RL or real-world applications beyond simulation environments

## Confidence

- **High confidence**: Claims about SR improving IQM and reducing Optimality Gap on URLB tasks are well-supported by experimental results
- **Medium confidence**: Claims about mitigating nonstationarity during pretraining and preventing forgetting during finetuning are mechanistically sound but could benefit from more direct empirical validation
- **Low confidence**: Claims about the distillation phase effectively removing retrieval overhead while maintaining performance are theoretical rather than empirically validated

## Next Checks

1. **Cross-domain validation**: Test SR on reward-driven RL benchmarks (e.g., Atari, DM Control Suite) to verify if the benefits extend beyond unsupervised pretraining scenarios
2. **Computational overhead analysis**: Measure and report the additional wall-clock time and memory requirements introduced by the retrieval and attention mechanisms during training
3. **Distillation phase validation**: Implement and evaluate the distillation phase empirically to confirm that performance can be maintained without online retrieval after a one-time training phase