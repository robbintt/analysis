---
ver: rpa2
title: Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning
  about Defeasible Commonsense Norms
arxiv_id: '2310.10418'
source_url: https://arxiv.org/abs/2310.10418
tags:
- action
- image
- judgment
- judgments
- lens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NORM LENS, a multimodal dataset for visually
  grounded reasoning about defeasible commonsense norms. The dataset consists of 10K
  human annotations covering 2K multimodal situations, each comprising an image, an
  action, and human judgments about the action's moral acceptability.
---

# Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms

## Quick Facts
- arXiv ID: 2310.10418
- Source URL: https://arxiv.org/abs/2310.10418
- Reference count: 30
- Primary result: Introduces NORM LENS dataset for multimodal reasoning about defeasible commonsense norms

## Executive Summary
This paper introduces NORM LENS, a multimodal dataset for visually grounded reasoning about defeasible commonsense norms. The dataset consists of 10K human annotations covering 2K multimodal situations, each comprising an image, an action, and human judgments about the action's moral acceptability. The paper evaluates state-of-the-art models' ability to align with human judgments and provide explanations. Results show that while models struggle to fully align with human judgments, fine-tuning with automatically generated examples improves performance, achieving a 31.5% improvement in judgment task accuracy on the high agreement subset. The paper also highlights the importance of visual grounding and reasoning capabilities in these tasks.

## Method Summary
The paper introduces NORM LENS, a multimodal dataset for visually grounded reasoning about defeasible commonsense norms. The dataset consists of 10K human annotations covering 2K multimodal situations, each comprising an image, an action, and human judgments about the action's moral acceptability. The paper evaluates state-of-the-art models' ability to align with human judgments and provide explanations. Results show that while models struggle to fully align with human judgments, fine-tuning with automatically generated examples improves performance, achieving a 31.5% improvement in judgment task accuracy on the high agreement subset. The paper also highlights the importance of visual grounding and reasoning capabilities in these tasks.

## Key Results
- Fine-tuning with automatically generated data improves alignment scores in most cases
- Visual inputs are important for multimodal reasoning tasks
- Models show low alignment with human judgments, particularly in NORM LENS HA where they score between 34.0% to 41.9%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual grounding improves model alignment with human judgments on commonsense norms
- Mechanism: Converting visual inputs to textual descriptions (Socratic Models) allows language models to leverage their reasoning capabilities while maintaining visual context
- Core assumption: Language models can effectively reason about visually-grounded situations when provided with accurate image descriptions
- Evidence anchors:
  - [abstract] "This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines"
  - [section] "Visual inputs are important. All the SMs clearly outperform their text-only counterparts (LM) except for GPT-3 Davinci"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: When VLM fails to generate accurate image descriptions, leading to degraded reasoning performance

### Mechanism 2
- Claim: Text-only language models can generate useful training data for multimodal norm reasoning
- Mechanism: Large language models can create synthetic examples of multimodal situations with judgments and explanations that capture context-dependent norms
- Core assumption: Generated examples, despite being imperfect, provide sufficient signal to improve model performance
- Evidence anchors:
  - [abstract] "we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models"
  - [section] "The machine-generated data improves alignment scores in most cases"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: When generated examples are systematically misaligned with human norms, leading to model degradation

### Mechanism 3
- Claim: Reasoning capability is crucial for understanding defeasible commonsense norms
- Mechanism: Models with stronger reasoning capabilities can better identify when actions are impossible or context-dependent
- Core assumption: Complex norm reasoning requires sophisticated logical inference beyond simple pattern matching
- Evidence anchors:
  - [abstract] "it necessitates both visual understanding and reasoning about commonsense norms"
  - [section] "All VLMs show a low level of alignment, particularly in NORM LENS HA where they score between 34.0% to 41.9%"
  - [corpus] Weak - no direct corpus evidence for this mechanism
- Break condition: When norm reasoning requires commonsense knowledge not present in model training data

## Foundational Learning

- Concept: Defeasibility of commonsense norms
  - Why needed here: Understanding that norms can be overridden by context is central to the task
  - Quick check question: Why is "reading books" sometimes wrong but sometimes okay?

- Concept: Multimodal reasoning
  - Why needed here: The task requires integrating visual and textual information
  - Quick check question: How does adding visual context change the judgment about an action?

- Concept: Human-AI collaboration in dataset creation
  - Why needed here: The dataset creation process combines automated generation with human validation
  - Quick check question: Why is human annotation still necessary after using language models?

## Architecture Onboarding

- Component map: Image → VLM (for description) → Language Model (for reasoning) → Judgment/Explanation
- Critical path: The VLM description generation step is most critical as it directly impacts reasoning quality
- Design tradeoffs: Text-only generation is cheaper but less accurate than human annotation; VLMs can reason but may lack norm understanding
- Failure signatures: Low performance on "impossible" actions indicates VLM description quality issues; poor "wrong" action performance suggests reasoning limitations
- First 3 experiments:
  1. Test VLM description quality by comparing generated descriptions to ground truth
  2. Measure impact of removing visual grounding by testing text-only vs multimodal models
  3. Evaluate synthetic data quality by comparing generated examples to human annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuning process with automatically generated data affect the models' ability to generalize to unseen situations?
- Basis in paper: [inferred] The paper mentions that fine-tuning with automatically generated examples improves alignment scores, but there is a decrease in scores for the "Okay" class, indicating more conservative model decisions.
- Why unresolved: The paper does not provide a detailed analysis of the models' performance on unseen situations or their ability to generalize beyond the training data.
- What evidence would resolve it: Conducting experiments on a separate test set of unseen situations and comparing the performance of fine-tuned models to non-fine-tuned models would provide evidence for generalization capabilities.

### Open Question 2
- Question: How do different cultural backgrounds and diverse perspectives impact the agreement between model predictions and human judgments in NORM LENS?
- Basis in paper: [explicit] The paper mentions that NORM LENS is manually annotated by English-speaking workers in Canada, UK, and US, and acknowledges that it may not cover all commonsense norms based on different sociocultural backgrounds or diverse perspectives.
- Why unresolved: The paper does not explore the impact of cultural differences on the agreement between model predictions and human judgments, nor does it include annotations from diverse cultural backgrounds.
- What evidence would resolve it: Including annotations from individuals with diverse cultural backgrounds and comparing the agreement between their judgments and model predictions would provide evidence for the impact of cultural differences.

### Open Question 3
- Question: How does the reasoning capability of VLMs compare to the reasoning capability of LMs in the context of visually grounded reasoning about defeasible commonsense norms?
- Basis in paper: [inferred] The paper suggests that visual inputs are important and that reasoning capability is crucial, but it does not directly compare the reasoning capabilities of VLMs and LMs in this specific context.
- Why unresolved: The paper does not provide a direct comparison of the reasoning capabilities of VLMs and LMs when it comes to visually grounded reasoning about defeasible commonsense norms.
- What evidence would resolve it: Conducting experiments that directly compare the reasoning capabilities of VLMs and LMs in the context of visually grounded reasoning about defeasible commonsense norms would provide evidence for their relative performance.

## Limitations

- The paper does not address potential cultural biases in the dataset or how norm reasoning might vary across different demographic groups
- The evaluation framework focuses primarily on judgment accuracy and explanation quality metrics without deeper qualitative analysis of why models succeed or fail on specific norm categories
- The study relies heavily on automated data generation without extensive human validation of synthetic examples, creating potential alignment gaps between model outputs and true human norms

## Confidence

- **Medium confidence**: Visual grounding improves model alignment with human judgments on commonsense norms
  The evidence shows visual inputs generally improve performance, but exceptions exist (e.g., GPT-3 Davinci outperforming its multimodal counterpart)

- **Medium confidence**: Text-only language models can generate useful training data for multimodal norm reasoning
  While the paper demonstrates performance improvements from fine-tuning, the synthetic data quality and potential propagation of biases are not thoroughly examined

- **Low confidence**: Reasoning capability is crucial for understanding defeasible commonsense norms
  The paper asserts this requirement but provides limited evidence showing how reasoning capabilities specifically contribute to norm understanding versus other factors like training data quality

## Next Checks

1. **Human validation of synthetic data quality**: Conduct a small-scale human evaluation comparing a random sample of machine-generated examples to human-annotated examples, measuring alignment with human norms and identifying systematic biases

2. **Cross-cultural norm variation analysis**: Test model performance on norm judgments across different cultural contexts by creating or identifying norm situations that vary culturally, then measuring how well models adapt to these variations

3. **VLM description quality assessment**: Systematically evaluate the impact of VLM-generated image descriptions on reasoning performance by comparing model outputs when using ground-truth descriptions versus VLM-generated ones, identifying specific failure patterns in the VLM component