---
ver: rpa2
title: Affective Natural Language Generation of Event Descriptions through Fine-grained
  Appraisal Conditions
arxiv_id: '2307.14004'
source_url: https://arxiv.org/abs/2307.14004
tags:
- emotion
- appraisal
- text
- generation
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Affective Natural Language Generation of Event Descriptions through
  Fine-grained Appraisal Conditions Problem addressed: Most affective text generation
  models rely on basic emotions or valence/arousal values, which limits their ability
  to generate emotionally connotated event descriptions. This paper aims to improve
  affective text generation by incorporating appraisal variables from psychological
  theories.'
---

# Affective Natural Language Generation of Event Descriptions through Fine-grained Appraisal Conditions

## Quick Facts
- arXiv ID: 2307.14004
- Source URL: https://arxiv.org/abs/2307.14004
- Reference count: 19
- Primary result: Adding appraisal information during training improves emotion accuracy by 10 pp in F1 compared to using only emotion conditions.

## Executive Summary
This paper explores the use of appraisal variables from psychological theories to improve affective text generation of event descriptions. The authors fine-tune transformer-based models (Bart and T5) on a dataset labeled with emotions and appraisals, experimenting with three configurations: emotion-only, emotions and appraisals, and appraisals-only. The results show that incorporating appraisal information during training leads to a 10 pp improvement in emotion accuracy, while also enabling more fine-grained control over the generated text.

## Method Summary
The authors use two transformer-based models (Bart and T5) and fine-tune them on a dataset of event descriptions labeled with emotions and appraisals. They experiment with three model configurations: emotion-only (E), emotions and appraisals (EA), and appraisals-only (A). The models are evaluated using automatic emotion and appraisal classifiers, as well as human evaluation.

## Key Results
1. Adding appraisal information during training improves emotion accuracy by 10 pp in F1 compared to using only emotion conditions.
2. The generated texts with appraisal variables are longer and contain more details, providing a more fine-grained control over the generation process.
3. The T5 model outperforms Bart and the baseline model (ATG) in terms of emotion accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing appraisal variables during training improves emotion accuracy in generated text.
- **Mechanism:** The model learns a more accurate internal representation of emotion concepts by being exposed to both emotion categories and their associated appraisal dimensions. This richer conditioning signal helps the model generate text that better fulfills the target emotion condition.
- **Core assumption:** Appraisal variables provide meaningful, non-redundant information about what makes a specific emotion and what properties it has.
- **Evidence anchors:**
  - [abstract] "adding appraisals during training improves the accurateness of the generated texts by 10 pp in F1"
  - [section 4.2] "T5 shows a 10pp higher F1 with appraisal information than without"
  - [corpus] Weak evidence - corpus doesn't directly test if appraisal dimensions are non-redundant with emotions
- **Break condition:** If appraisal variables are highly correlated with each other or with emotions, they may not provide additional useful information beyond what emotions alone convey.

### Mechanism 2
- **Claim:** Appraisal variables allow for more fine-grained control over generated text.
- **Mechanism:** By specifying appraisal dimensions (e.g., Attention, Control, Pleasantness), users can guide the model to generate event descriptions with specific properties, leading to longer and more detailed texts.
- **Core assumption:** Appraisal variables are independent enough to provide distinct control dimensions over the generated text.
- **Evidence anchors:**
  - [abstract] "The variables of appraisal allow a user to perform a more fine-grained control of the generated text"
  - [section 4.3] "instances obtained with the model trained with appraisal information (EA) are 15 tokens long for T5, while instances of the model trained only with emotion conditions (E) are 9 tokens long"
  - [corpus] Weak evidence - corpus doesn't test if appraisal dimensions are independent enough for distinct control
- **Break condition:** If appraisal variables are highly correlated, specifying multiple appraisals may not lead to significantly different generated texts.

### Mechanism 3
- **Claim:** T5 outperforms Bart in affective text generation with appraisal conditions.
- **Mechanism:** T5's text-to-text framework and pretraining objective may make it more effective at incorporating multiple conditional variables (emotion and appraisals) into the generation process.
- **Core assumption:** T5's architecture is better suited for handling complex conditional inputs than Bart's.
- **Evidence anchors:**
  - [section 4.2] "T5 outperforms Bart and ATG" in terms of emotion accuracy
  - [section 4.2] "the text quality suffers a small drop in quality" for Bart when using appraisal conditions
  - [corpus] Weak evidence - corpus doesn't directly compare T5 and Bart architectures
- **Break condition:** If the performance difference is due to dataset characteristics rather than architectural differences, Bart might perform equally well on different datasets.

## Foundational Learning

- **Concept:** Appraisal theories in psychology
  - Why needed here: Understanding appraisal theories is crucial for grasping how emotions are linked to cognitive evaluations of events, which is the basis for this work's approach to affective text generation.
  - Quick check question: What are the key appraisal dimensions used in this paper, and how do they relate to different emotions?
- **Concept:** Conditional natural language generation (CNLG)
  - Why needed here: The paper builds on CNLG techniques, using conditional variables (emotions and appraisals) to guide text generation, so understanding CNLG is essential.
  - Quick check question: How does the paper's approach to CNLG differ from traditional methods that use only basic emotions or valence/arousal values?
- **Concept:** Transformer-based language models (T5, BART)
  - Why needed here: The paper uses T5 and BART as the base architectures for the affective text generation models, so understanding their capabilities and differences is important.
  - Quick check question: What are the key differences between T5 and BART, and how might these differences affect their performance in affective text generation?

## Architecture Onboarding

- **Component map:** Input prompt construction (emotion + appraisals + trigger phrase) -> Transformer-based encoder-decoder models (T5, BART) -> Fine-tuning on emotion and appraisal labeled dataset -> Inference with beam search -> Evaluation using automatic classifiers and human annotation
- **Critical path:** Input prompt → Model fine-tuning → Inference → Evaluation
- **Design tradeoffs:**
  - Using appraisal variables improves emotion accuracy but may slightly decrease text quality (higher perplexity)
  - T5 outperforms BART but may have different computational requirements
  - Automatic evaluation using classifiers is efficient but may not fully capture human judgment
- **Failure signatures:**
  - Model generates text that doesn't match the target emotion
  - Generated text is too short or lacks detail compared to human-written examples
  - Model struggles to differentiate between conditions and trigger phrases
- **First 3 experiments:**
  1. Fine-tune T5 with emotion-only (E) and emotion+appraisal (EA) configurations on a small subset of data, evaluate on a held-out test set
  2. Compare perplexity scores of E and EA configurations to assess impact on text quality
  3. Conduct a small-scale human evaluation to validate automatic evaluation results and assess text quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different model architectures (e.g., embedding conditions into encoder/decoder) impact text quality when using appraisal conditions?
- Basis in paper: [inferred]
- Why unresolved: The paper relies on prompt-based condition embedding which leads to decreased text quality. They hypothesize other architectures might improve this but don't test them.
- What evidence would resolve it: Experiments comparing T5/BART with prompt-based conditions versus architectures that embed conditions into model components, measuring both emotion accuracy and text quality metrics.

### Open Question 2
- Question: Can automatic appraisal estimation replace manual appraisal labeling in conditional generation?
- Basis in paper: [explicit]
- Why unresolved: The paper trains with annotated appraisals but notes this isn't realistic for deployment. They mention needing to estimate appraisals automatically but don't explore this.
- What evidence would resolve it: Experiments showing how automatic appraisal classifiers perform in place of ground-truth appraisals during training, and how well they work for inference prompts.

### Open Question 3
- Question: How does adding topic or previous dialogue context as additional conditions affect emotion accuracy and text quality?
- Basis in paper: [explicit]
- Why unresolved: The paper acknowledges future work needs to add topic/previous utterance conditions but doesn't explore the interaction between multiple conditions.
- What evidence would resolve it: Experiments conditioning on emotion+appraisal+topic combinations, measuring emotion accuracy and text quality, and analyzing conflicts between conditions.

## Limitations
- The automatic evaluation relies on classifiers trained on the same dataset used for fine-tuning, which may overestimate performance.
- The human evaluation sample size is relatively small (30 instances), limiting the generalizability of the results.
- The text quality slightly decreases when using appraisal conditions, suggesting a tradeoff between control and naturalness.

## Confidence
- **High confidence**: The improvement in emotion accuracy when using appraisal variables (10 pp F1 increase) is well-supported by both automatic and human evaluation results.
- **Medium confidence**: The claim that appraisal variables allow for more fine-grained control over generated text is supported by the results, but the extent of this control and its practical implications need further investigation.
- **Medium confidence**: The superiority of T5 over BART in this task is demonstrated, but the generalizability of this finding to other datasets or tasks is uncertain.

## Next Checks
1. **Cross-dataset evaluation**: Test the models on a held-out dataset from a different source to validate the robustness of the results and ensure the models are not overfitting to the crowd-enVENT dataset.
2. **Ablation study on appraisal dimensions**: Conduct an ablation study to determine the contribution of each appraisal dimension to the improvement in emotion accuracy and text quality. This will help identify the most influential appraisal variables and potentially simplify the model.
3. **Comparison with other generative models**: Compare the performance of the proposed models with other state-of-the-art generative models, such as GPT or CTRL, on the same task. This will provide a broader context for the effectiveness of the appraisal-based approach.