---
ver: rpa2
title: Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social
  Media
arxiv_id: '2307.02313'
source_url: https://arxiv.org/abs/2307.02313
tags:
- data
- depression
- bdi-ii
- generated
- symptoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study focuses on retrieving depression-related content from\
  \ Reddit using ChatGPT-generated data. Researchers designed a prompt to generate\
  \ Reddit-style posts based on Beck\u2019s Depression Inventory (BDI-II) symptoms,\
  \ aiming for semantic diversity and emotional depth."
---

# Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media

## Quick Facts
- arXiv ID: 2307.02313
- Source URL: https://arxiv.org/abs/2307.02313
- Reference count: 35
- Key outcome: MPNet embeddings using original BDI-II responses as queries outperformed synthetic data for depression symptom retrieval from Reddit, achieving 0.104 average precision in majority voting.

## Executive Summary
This study investigates using ChatGPT-generated synthetic Reddit posts to retrieve depression-related content from social media. The researchers generated Reddit-style posts based on Beck's Depression Inventory (BDI-II) symptoms, aiming for semantic diversity and emotional depth. They compared semantic search models using original BDI-II responses and synthetic data as queries, embedding social media posts with MPNet and MentalRoBERTa models. The MPNet-based semantic search using original BDI-II responses achieved the best performance, while synthetic data proved too specific and hindered retrieval effectiveness. The findings suggest promise for future refinements in prompt engineering to improve synthetic data utility.

## Method Summary
The researchers generated synthetic Reddit posts using ChatGPT based on BDI-II symptoms, then performed semantic search by computing cosine similarity between social media posts and both original BDI-II responses and synthetic queries. They used MPNet and MentalRoBERTa models to create sentence embeddings, retrieving the top 50 most relevant sentences per symptom. The study evaluated retrieval effectiveness using average precision, R-precision, precision at 10, and normalized discounted cumulative gain at 1000.

## Key Results
- MPNet embeddings with original BDI-II responses achieved 0.104 average precision (AP) in majority voting and 0.129 AP in unanimity evaluation
- MPNet-based semantic search outperformed MentalRoBERTa for this retrieval task
- Synthetic ChatGPT-generated data was too specific, hindering retrieval effectiveness compared to original BDI-II responses
- Retrieval used cosine similarity on sentence embeddings to rank Reddit sentences by semantic closeness to symptom queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using original BDI-II responses as queries outperforms synthetic Reddit-style posts for semantic search in depression symptom retrieval.
- Mechanism: The semantic search model with MPNet embeddings uses original BDI-II responses as queries and retrieves Reddit sentences by cosine similarity. Original BDI-II responses are standardized, symptom-focused, and avoid overly specific details that can confuse retrieval.
- Core assumption: Semantic search benefits from query data that is semantically aligned with the symptom definitions and avoids overly specific anecdotal content.
- Evidence anchors:
  - [abstract] "The synthetic data was found too specific, with overly detailed posts hindering retrieval effectiveness."
  - [section] "The data generated by ChatGPT is too specific, and future work needs to be done to manipulate the prompt such that data is semantically similar and more diverse than the BDI-II responses, but, at the same time, has fewer specific details."
- Break condition: If synthetic prompts are engineered to match symptom relevance without excessive detail, retrieval performance may improve.

### Mechanism 2
- Claim: MPNet embeddings designed for semantic search outperform MentalRoBERTa embeddings for this retrieval task.
- Mechanism: MPNet (multi-qa-mpnet-base-dot-v1) is fine-tuned for semantic search, directly optimizing for sentence similarity. MentalRoBERTa is pre-trained on mental health data but not optimized for semantic search, leading to lower retrieval effectiveness.
- Core assumption: Embedding models optimized for semantic search yield better cosine similarity rankings than general domain models, even if the latter are task-relevant.
- Evidence anchors:
  - [abstract] "Our results show that an approach using for sentence embeddings a model that is designed for semantic search outperforms the model pre-trained on mental health data."
  - [section] "The semantic search model utilizing the original BDI-II responses as queries performs better than the model using generated data."
- Break condition: If the task shifts to classification or requires mental health domain specificity, MentalRoBERTa may outperform.

### Mechanism 3
- Claim: Semantic search using cosine similarity on sentence embeddings retrieves the most relevant sentences for depression symptoms from Reddit.
- Mechanism: Embeddings are computed for both queries (BDI-II responses or synthetic posts) and Reddit sentences. Cosine similarity ranks sentences by semantic closeness to the symptom queries, retrieving the top 50 per symptom.
- Core assumption: Semantic similarity in embedding space correlates with symptom relevance in natural language text.
- Evidence anchors:
  - [abstract] "We perform semantic search and rank the sentences' relevance to the BDI-II symptoms by cosine similarity."
  - [section] "We performed semantic search and used cosine similarity to get the most relevant social media posts to the original BDI-II responses and generated queries."
- Break condition: If embedding models fail to capture symptom semantics, or if Reddit text is too noisy, cosine similarity may not reflect symptom relevance.

## Foundational Learning

- Concept: Beck Depression Inventory-II (BDI-II)
  - Why needed here: BDI-II defines the 21 depression symptoms used as queries for semantic search; understanding its structure is essential for query design and evaluation.
  - Quick check question: How many items are in the BDI-II, and what type of scale is used for responses?

- Concept: Semantic search and cosine similarity
  - Why needed here: The retrieval method relies on embedding similarity; engineers must understand how cosine similarity ranks semantic closeness in vector space.
  - Quick check question: What does a cosine similarity score of 1.0 indicate about two sentence embeddings?

- Concept: Large Language Models for data generation
  - Why needed here: ChatGPT is used to generate synthetic Reddit posts; understanding prompt engineering and generation limits is crucial for producing useful synthetic queries.
  - Quick check question: What are the risks of using overly specific anecdotes in synthetic data for semantic search?

## Architecture Onboarding

- Component map: Reddit sentences + BDI-II queries → Text preprocessing (URL/non-English removal) → Embedding (MPNet or MentalRoBERTa) → Cosine similarity ranking → Top-k retrieval (k=50) → Evaluation (AP, P@10, NDCG@1000)
- Critical path: Embedding generation → Cosine similarity computation → Top-k filtering → Evaluation metrics
- Design tradeoffs: MPNet optimized for semantic search vs. MentalRoBERTa domain-specific; synthetic query diversity vs. specificity; computational cost of embeddings vs. retrieval accuracy
- Failure signatures: Low AP/P@10 scores; retrieval dominated by irrelevant but semantically similar posts; synthetic queries introducing noise
- First 3 experiments:
  1. Compare retrieval with MPNet vs. MentalRoBERTa embeddings on original BDI-II queries.
  2. Vary synthetic prompt specificity and evaluate retrieval impact.
  3. Test retrieval with top-k=25 vs. top-k=50 to optimize precision/recall tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt modifications could make ChatGPT-generated synthetic data more effective for retrieving depression symptoms from social media posts?
- Basis in paper: [explicit] The paper notes that the generated data was "too specific" and contained "too many details" that were not helpful for semantic search, suggesting the need for prompt manipulation to generate more suitable data.
- Why unresolved: The paper only identified the problem with specificity but did not test or propose specific prompt modifications to address this issue.
- What evidence would resolve it: Experiments testing different prompt structures, constraints, or instructions that balance semantic diversity with specificity, along with quantitative comparisons of retrieval performance using these modified prompts.

### Open Question 2
- Question: How does the performance of semantic search models using BDI-II responses as queries compare to models using other clinical questionnaire-based queries (e.g., PHQ-9, CES-D) for depression symptom retrieval?
- Basis in paper: [inferred] The paper demonstrates that BDI-II responses as queries outperformed generated data but doesn't compare to other clinical questionnaires that have been used in related work.
- Why unresolved: The study only evaluated BDI-II responses and didn't benchmark against other clinical scales that might contain different symptom descriptions or response formats.
- What evidence would resolve it: Direct comparison studies using the same semantic search framework with queries derived from multiple clinical questionnaires, measuring retrieval performance across all.

### Open Question 3
- Question: What is the optimal balance between semantic diversity and specificity in synthetic Reddit posts for effective depression symptom retrieval?
- Basis in paper: [explicit] The authors note their synthetic data was "too specific" and suggest the need for data that is "semantically similar and more diverse than the BDI-II responses, with fewer specific details."
- Why unresolved: The paper identifies this as a problem but doesn't experimentally determine what level of specificity versus diversity yields optimal retrieval performance.
- What evidence would resolve it: Controlled experiments varying the specificity and diversity parameters in synthetic data generation, with systematic evaluation of retrieval performance across these dimensions.

## Limitations

- Synthetic ChatGPT-generated data proved too specific, containing excessive details that hindered retrieval effectiveness compared to original BDI-II responses
- Evaluation relied on a relatively small dataset (100 Reddit sentences), limiting generalizability to larger-scale retrieval scenarios
- The comparison between MPNet and MentalRoBERTa embeddings doesn't account for potential domain adaptation benefits that MentalRoBERTa might offer with further fine-tuning

## Confidence

- **High confidence**: The conclusion that original BDI-II responses outperform synthetic queries for retrieval is well-supported by the empirical results (0.104 vs 0.099 AP in majority voting, 0.129 vs 0.117 AP in unanimity)
- **Medium confidence**: The claim that MPNet outperforms MentalRoBERTa for this task is supported by the data but could vary with different evaluation datasets or retrieval tasks
- **Medium confidence**: The finding that cosine similarity on embeddings effectively retrieves relevant depression symptoms is demonstrated but limited by the small evaluation set

## Next Checks

1. Test prompt engineering variations to generate less specific synthetic data and measure impact on retrieval performance
2. Expand evaluation to larger Reddit datasets (1000+ sentences) to verify scalability of the approach
3. Conduct ablation studies comparing different top-k values (25, 50, 100) to optimize the precision-recall tradeoff