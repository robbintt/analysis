---
ver: rpa2
title: '"According to ...": Prompting Language Models Improves Quoting from Pre-Training
  Data'
arxiv_id: '2305.13252'
source_url: https://arxiv.org/abs/2305.13252
tags:
- grounding
- prompt
- language
- prompts
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve factual accuracy in large
  language models by prompting them to ground responses in their pre-training data.
  The approach, called "according-to prompting," directs models to quote from trusted
  sources like Wikipedia by appending prompts such as "Respond by using information
  from Wikipedia in your response." To measure the effectiveness of this grounding,
  the authors propose QUIP-Score, which quantifies the percentage of generated text
  that exactly matches spans in the pre-training corpus using character n-gram overlap.
---

# "According to ...": Prompting Language Models Improves Quoting from Pre-Training Data

## Quick Facts
- arXiv ID: 2305.13252
- Source URL: https://arxiv.org/abs/2305.13252
- Reference count: 10
- Primary result: Grounding prompts increase QUIP-Score by 5-105% and often improve downstream task performance

## Executive Summary
This paper introduces a method to improve factual accuracy in large language models by prompting them to ground responses in their pre-training data. The approach, called "according-to prompting," directs models to quote from trusted sources like Wikipedia by appending prompts such as "Respond by using information from Wikipedia in your response." To measure the effectiveness of this grounding, the authors propose QUIP-Score, which quantifies the percentage of generated text that exactly matches spans in the pre-training corpus using character n-gram overlap. Experiments across multiple datasets (TriviaQA, Natural Questions, HotpotQA, and ELI5) and models (including ChatGPT, GPT-4, FLAN-T5, and others) show that grounding prompts increase QUIP-Score by 5-105% and often improve downstream task performance. The method is more effective in larger and instruction-tuned models, and its impact correlates with entity popularity in the training data. The study demonstrates that prompting can reliably steer models toward more factual, quoted responses.

## Method Summary
The study uses datasets such as TriviaQA, Natural Questions, HotpotQA, and ELI5, with models including ChatGPT, GPT-4, FLAN-T5, GPT-J, Koala, and others. The pre-training data includes Wikipedia, PubMed, and the U.S. legal tax code. The method involves appending prompts to encourage grounding in specific corpora (e.g., "Respond by using information from Wikipedia in your response"). The study evaluates the impact of these prompts on QUIP-Score and downstream performance across multiple models and datasets. QUIP-Score measures the percentage of generated text that exactly matches spans in the pre-training corpus using character n-gram overlap. Secondary metrics include exact match (EM), F1, and Rouge-L scores for downstream task performance.

## Key Results
- Grounding prompts increase QUIP-Score by 5-105% across multiple models and datasets
- Larger and instruction-tuned models show greater effectiveness with grounding prompts
- QUIP-Score correlates with entity popularity in pre-training data
- Grounding prompts often improve downstream task performance (EM, F1, Rouge-L)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models are more effective at grounding because they have greater capacity to memorize and recall factual spans from pre-training data.
- Mechanism: As model size increases, memorization capacity scales, allowing larger models to store and retrieve exact textual spans from Wikipedia more reliably when prompted.
- Core assumption: Memorization correlates with model scale and improves grounding performance.
- Evidence anchors:
  - [abstract] "Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request."
  - [section 4.2] "Model size vs QUIP-Score performance, with FLAN-T5 models (left) and OpenAI models (right). As model scale increases, so does performance. At smaller model sizes, the grounding prompt is not more effective than the null prompt, but gains efficacy with model size."
- Break condition: If the model's pre-training data doesn't contain the factual information being requested, even large models cannot ground their responses regardless of prompting.

### Mechanism 2
- Claim: Instruction tuning enhances the model's ability to follow grounding prompts because it improves instruction-following capability.
- Mechanism: Instruction-tuned models have been fine-tuned on instruction-following datasets, making them more responsive to natural language prompts that direct them to quote from specific sources.
- Core assumption: Instruction tuning improves the model's ability to understand and execute natural language instructions.
- Evidence anchors:
  - [abstract] "This work explores the intriguing possibility of steering LLMs by prompting them to quote more of the curated sources of information they have memorized during pre-training"
  - [section 4.3] "We find that instruction-tuning does help, as the QUIP-Scores for T5-v1.1-Adapt are similar between grounding and null prompts, while the FLAN-T5 model has a large difference between the null and grounding prompt (roughly 2x better)."
- Break condition: If the instruction-tuned model hasn't seen examples of quoting from sources during fine-tuning, the benefit may be limited.

### Mechanism 3
- Claim: Entity popularity in pre-training data affects grounding because frequently occurring entity relationships are more likely to be memorized and recalled.
- Mechanism: When entities and their relationships appear multiple times in pre-training data, the model is more likely to have memorized these patterns and can recall them more accurately when prompted.
- Core assumption: Memorization is frequency-dependent and entities with higher co-occurrence counts are more likely to be recalled.
- Evidence anchors:
  - [abstract] "Furthermore, we observe the opposite phenomenon – it is possible to discourage LLMs from grounding in Wikipedia via prompts that either discourage grounding or encourage grounding to other corpora."
  - [section 4.4] "We find in Figure 3 that QA entity popularity is positively correlated with QUIP-Score for both grounding and null prompts (although more effective for the grounding prompt) and that the model is better able to recall information from Wikipedia when QA entities frequently co-occur."
- Break condition: If the entity relationship is mentioned only once in the entire pre-training corpus, the model may not have effectively memorized it regardless of popularity trends.

## Foundational Learning

- Concept: Character n-gram overlap for membership testing
  - Why needed here: QUIP-Score relies on efficiently checking whether generated text spans exist in pre-training data, and character n-grams provide tokenization-agnostic matching
  - Quick check question: Why use character-based n-grams instead of token-based n-grams for QUIP-Score calculation?

- Concept: Bloom filters for efficient corpus membership testing
  - Why needed here: The DATA PORTRAIT uses Bloom filters to enable fast membership queries across large corpora like Wikipedia
  - Quick check question: How does a Bloom filter enable sub-millisecond membership queries for QUIP-Score computation?

- Concept: Grounding vs. hallucination distinction
  - Why needed here: Understanding that grounding involves quoting from known sources while hallucination involves fabricating information is crucial for interpreting QUIP-Score results
  - Quick check question: What is the fundamental difference between a grounded response and a hallucinated response in this context?

## Architecture Onboarding

- Component map:
  - Prompt generator → Language model → Text generator → QUIP-Score calculator → DATA PORTRAIT (Bloom filter) → Wikipedia corpus
  - Data flow: Input question → Append grounding prompt → Generate response → Extract character n-grams → Check membership → Calculate QUIP-Score

- Critical path: The time-critical path is from text generation to QUIP-Score calculation, which must happen quickly for large-scale evaluation across multiple datasets and models

- Design tradeoffs: Exact matching (QUIP-Score) vs. semantic similarity - exact matching is faster and more reliable but may miss paraphrased facts; semantic similarity would be more comprehensive but computationally expensive

- Failure signatures: Low QUIP-Score despite correct answers indicates the model may be paraphrasing rather than quoting; high QUIP-Score with incorrect answers suggests the model is quoting irrelevant or outdated information

- First 3 experiments:
  1. Test grounding prompts on a small subset of TriviaQA with a known model to verify QUIP-Score calculation works correctly
  2. Compare QUIP-Score across different n-gram lengths (15, 25, 35 characters) to find optimal balance between precision and recall
  3. Test anti-grounding prompts to confirm they decrease QUIP-Score as expected, validating the bidirectional control mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of according-to prompting vary across different domains beyond Wikipedia, such as legal texts or scientific literature?
- Basis in paper: The paper primarily focuses on Wikipedia as the grounding source but mentions the potential for applying the approach to other domains like law or medicine.
- Why unresolved: The experiments conducted were limited to Wikipedia, PubMed, and the U.S. legal tax code. The effectiveness of the approach in other domains remains unexplored.
- What evidence would resolve it: Conducting experiments using according-to prompting with other domain-specific corpora, such as legal databases or scientific journals, and comparing the QUIP-Score and downstream task performance across these domains.

### Open Question 2
- Question: What is the impact of different n-gram lengths on the accuracy and efficiency of the QUIP-Score metric?
- Basis in paper: The paper uses 25-character n-grams for the DATA PORTRAIT but does not explore the impact of varying n-gram lengths on the metric's performance.
- Why unresolved: The choice of n-gram length could affect the sensitivity and specificity of the QUIP-Score, potentially influencing its ability to accurately measure grounding.
- What evidence would resolve it: Systematically testing the QUIP-Score with different n-gram lengths and analyzing the trade-offs between accuracy, efficiency, and the ability to capture meaningful overlaps with the pre-training corpus.

### Open Question 3
- Question: How does the QUIP-Score correlate with human judgments of factual accuracy and relevance in model-generated text?
- Basis in paper: The QUIP-Score measures the extent of exact matches between generated text and the pre-training corpus, but it does not directly assess the factual accuracy or relevance of the information.
- Why unresolved: While QUIP-Score provides a quantitative measure of grounding, it does not capture the nuances of factual correctness or the relevance of the quoted information to the query.
- What evidence would resolve it: Conducting human evaluations to compare QUIP-Score results with assessments of factual accuracy and relevance, and analyzing the correlation between these measures.

## Limitations

- QUIP-Score may underestimate grounding when models paraphrase factual information rather than quote it verbatim
- Grounding prompts are most effective for entity-based factual questions with limited evaluation on complex reasoning tasks
- Effectiveness depends heavily on the quality and coverage of pre-training data, with potential for outdated or biased responses

## Confidence

**High Confidence**: The core finding that grounding prompts can reliably increase QUIP-Score across multiple models and datasets is well-supported by extensive experiments. The bidirectional control (both increasing and decreasing grounding via prompts) provides strong evidence for the mechanism.

**Medium Confidence**: The claims about model size and instruction tuning improving grounding effectiveness are supported by experiments, but the relationship may be more complex than presented. The study shows correlation but doesn't definitively establish causation, and there may be confounding factors.

**Low Confidence**: The generalizability of QUIP-Score to domains outside of entity-based factual questions remains uncertain. The metric's effectiveness for abstract reasoning, creative tasks, or domains with rapidly evolving knowledge bases has not been established.

## Next Checks

1. **Paraphrase Sensitivity Test**: Design an experiment where models are given questions with known answers in Wikipedia, but measure whether they paraphrase rather than quote. Compare QUIP-Scores when models generate paraphrased vs. quoted responses to the same questions, establishing the false negative rate of the metric.

2. **Cross-Corpus Grounding Evaluation**: Test grounding prompts that direct models to quote from non-Wikipedia sources (e.g., PubMed for medical questions, legal documents for tax questions) and measure whether QUIP-Scores increase specifically for those domains. This would validate whether the grounding mechanism works across different types of pre-training data.

3. **Reasoning Task Grounding Assessment**: Evaluate grounding prompts on datasets requiring multi-step reasoning or synthesis (e.g., DROP, HotpotQA comparison questions) to determine whether QUIP-Score remains meaningful and whether grounding improves or degrades performance on complex reasoning tasks.