---
ver: rpa2
title: 'Towards Answering Health-related Questions from Medical Videos: Datasets and
  Approaches'
arxiv_id: '2309.12224'
source_url: https://arxiv.org/abs/2309.12224
tags:
- visual
- dataset
- medical
- question
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HealthVidQA, a large-scale dataset for medical
  visual answer localization, and proposes monomodal and multimodal approaches for
  the task. The dataset is created using a pipelined approach that includes selecting
  medical instructional videos, detecting visual answer segments, and generating medical
  instructional questions.
---

# Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches
arXiv ID: 2309.12224
Source URL: https://arxiv.org/abs/2309.12224
Reference count: 40
Primary result: Introduces HealthVidQA dataset and achieves substantial performance improvement on medical visual answer localization

## Executive Summary
This paper addresses the task of medical visual answer localization, which involves identifying visual segments in medical instructional videos that answer health-related questions. The authors introduce HealthVidQA, a large-scale dataset created through a pipelined approach that combines video selection, visual answer segment detection, and question generation. They propose monomodal and multimodal approaches, including a Cycle-Consistent Answer Localization (CCAL) method, achieving substantial performance improvements on multiple evaluation metrics for the medical visual answer localization task.

## Method Summary
The approach consists of two main components: dataset creation and model development. For dataset creation, the authors select medical instructional videos from HowTo100M, detect visual answer segments using XLNet-CRF or XLNet-Prompt models, and generate questions using T5-based question generation. The proposed models include a reading comprehension approach and the CCAL approach, which enforces semantic alignment between predicted answer segments and generated questions. Multimodal extensions incorporate visual features from various encoders (ViT, VideoMAE, etc.) through late fusion with the text-based models.

## Key Results
- HealthVidQA-CRF dataset created with 23,434 triplets from 11,708 videos
- HealthVidQA-Prompt dataset created with 52,771 triplets from 13,990 videos
- XLNet-CRF model achieves F1-score of 0.418 on MedVidQA test set
- CCAL approach outperforms existing methods on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XLNet-CRF outperforms BERT variants for visual segment detection
- Mechanism: XLNet's autoregressive bidirectional context modeling better captures segment label dependencies (B-Seg, I-Seg, O) than BERT's masked language modeling
- Core assumption: Visual answer segments follow consistent topic-aware structure
- Evidence: XLNet-CRF achieves F1-score of 0.418 on MedVidQA test set

### Mechanism 2
- Claim: CCAL improves localization through semantic alignment
- Mechanism: Joint optimization of reading comprehension and question generation creates consistency signal
- Core assumption: Semantic similarity between original and generated questions indicates correct localization
- Evidence: CCAL outperforms baseline approaches on benchmark datasets

### Mechanism 3
- Claim: Multimodal fusion with visual features can improve localization
- Mechanism: Visual features provide complementary information to disambiguate verbal vs. visual content
- Core assumption: Visual demonstrations are strong cues for identifying answer segments
- Evidence: Visual features sometimes improve but can also degrade performance depending on encoder

## Foundational Learning

- Concept: Conditional Random Fields (CRFs) for sequence labeling
  - Why needed: To model dependencies between adjacent segment labels and improve boundary detection
  - Quick check: How does a CRF differ from a simple softmax classifier for sequence labeling?

- Concept: Prompt-based fine-tuning with pre-trained language models
  - Why needed: To leverage pre-trained knowledge without extensive retraining
  - Quick check: What is the role of the "verbalizer" in prompt-based fine-tuning?

- Concept: Cycle-consistency in training
  - Why needed: To regularize the model by ensuring predicted answers can generate similar questions
  - Quick check: How does cycle-consistency loss differ from standard cross-entropy loss?

## Architecture Onboarding

- Component map: Video selection -> Subtitle segmentation -> Visual segment detection -> Question generation -> Reading comprehension/CCAL -> Multimodal fusion (optional) -> Evaluation

- Critical path: 1. Filter medical instructional videos 2. Detect visual answer segments 3. Generate questions 4. Train localization model 5. Evaluate on test set

- Design tradeoffs:
  - CRF vs. prompt-based: CRF models label dependencies better but requires more supervision
  - Monomodal vs. multimodal: Monomodal is simpler; multimodal may improve accuracy but adds complexity
  - XLNet vs. other PLMs: XLNet may capture context better for sequence labeling

- Failure signatures:
  - Poor segment detection: Many false positives/negatives in localization
  - Poor question generation: Generated questions are unrelated or grammatically incorrect
  - Poor localization: Low IoU scores, especially at stricter thresholds
  - Multimodal degradation: Performance worse than monomodal with visual features

- First 3 experiments:
  1. Evaluate XLNet-CRF on MedVidQA test set to confirm sequence labeling performance
  2. Train CCAL model and compare to RC baseline on MedVidQA test set
  3. Add visual features to CCAL and evaluate impact on MedVidQA test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can HealthVidQA-Prompt be improved to match HealthVidQA-CRF quality?
- Basis: Paper notes HealthVidQA-Prompt has noise but could be useful for low-resource settings
- Why unresolved: No specific methods provided for quality improvement
- What would resolve: Comparative studies showing improved downstream performance with enhanced HealthVidQA-Prompt

### Open Question 2
- Question: What is the impact of different visual encoders on multimodal approach performance?
- Basis: Authors experimented with multiple encoders but didn't explore reasons for varying performance
- Why unresolved: Lack of detailed analysis of encoder strengths and weaknesses
- What would resolve: Detailed analysis of each visual encoder's performance in medical visual answer localization

### Open Question 3
- Question: How does CCAL compare to other state-of-the-art video QA methods?
- Basis: Authors claim CCAL outperforms existing approaches but lack comprehensive comparison
- Why unresolved: No detailed comparison with other state-of-the-art methods
- What would resolve: Extensive experiments comparing CCAL with other methods on various datasets and metrics

## Limitations

- Results are primarily validated on synthetic MedVidQA dataset rather than real-world HealthVidQA data
- Visual feature contributions are marginal or sometimes negative, suggesting potential issues with feature quality or fusion approach
- Dataset creation relies heavily on automated methods with limited human validation, raising quality concerns for medical content

## Confidence

- High confidence in dataset creation pipeline and its potential to address medical visual QA dataset scarcity
- Medium confidence in monomodal approaches (XLNet-CRF, CCAL) based on reported strong performance metrics
- Low confidence in multimodal approach due to mixed results and negative impacts from certain visual feature encoders

## Next Checks

1. Evaluate CCAL model performance on real-world HealthVidQA test set rather than just synthetic MedVidQA dataset
2. Conduct ablation studies isolating visual feature contributions by comparing monomodal and multimodal models on same test sets
3. Perform human evaluation on random sample of HealthVidQA dataset to verify quality of generated questions and answer segment annotations