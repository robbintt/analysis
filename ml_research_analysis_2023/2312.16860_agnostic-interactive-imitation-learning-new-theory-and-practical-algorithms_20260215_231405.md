---
ver: rpa2
title: 'Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms'
arxiv_id: '2312.16860'
source_url: https://arxiv.org/abs/2312.16860
tags:
- learning
- expert
- imitation
- policy
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies interactive imitation learning, where a learner
  queries an expert for action annotations and aims to learn a policy competitive
  with the expert using as few annotations as possible. The authors focus on the agnostic
  setting where the expert policy may not be in the learner's policy class.
---

# Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms

## Quick Facts
- arXiv ID: 2312.16860
- Source URL: https://arxiv.org/abs/2312.16860
- Reference count: 40
- This paper proposes theoretical guarantees and practical algorithms for agnostic interactive imitation learning, including MFTPL-P and Bootstrap-Dagger.

## Executive Summary
This paper addresses interactive imitation learning where a learner queries an expert for action annotations and aims to learn a policy competitive with the expert using as few annotations as possible. The authors focus on the agnostic setting where the expert policy may not be in the learner's policy class. They propose MFTPL-P, a new oracle-efficient algorithm with provable finite-sample guarantees under an explorative distribution assumption, and Bootstrap-Dagger, a practical variant without additional sample access requirements. Empirically, both algorithms notably surpass online and offline imitation learning baselines in continuous control tasks.

## Method Summary
The paper proposes MFTPL-P (Mixed Follow the Perturbed Leader with Poisson perturbations) for agnostic interactive imitation learning. The algorithm trains an ensemble of base policies, each trained on datasets augmented with Poisson-perturbed examples from a smooth exploration distribution. The ensemble mean approximates an idealized predictor with infinite ensemble size. The authors also introduce Bootstrap-Dagger, which uses ensemble bootstrapping to collect more efficient expert queries by exploring beyond the expert's state distribution support. Both algorithms reduce imitation learning to online learning problems with subsequent online-to-batch conversion.

## Key Results
- MFTPL-P achieves sublinear regret guarantees under smoothness assumptions with finite sample bounds
- Bootstrap-Dagger significantly outperforms DAgger and Behavior Cloning baselines in continuous control tasks
- Both algorithms demonstrate improved sample efficiency compared to existing online and offline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Ensemble-based policies achieve sublinear regret by approximating an idealized predictor with infinite ensemble size.
- **Mechanism**: MFTPL-P trains E base policies independently on datasets augmented with Poisson-perturbed examples from a smooth exploration distribution. The ensemble mean approximates the idealized policy that would be trained on infinite perturbations.
- **Core assumption**: The smooth distribution d0 covers the state visitation distribution of all policies in the class (σ-smoothness).
- **Evidence anchors**:
  - [abstract]: "MFTPL-P (abbreviation for Mixed Follow the Perturbed Leader with Poisson perturbations) with provable finite-sample guarantees, under the assumption that the learner is given access to samples from some 'explorative' distribution over states."
  - [section]: "By setting the size of ensembles E large enough, MFTPL-P approximates its 'ideal' version with infinite ensemble size, which enjoys deterministic regret guarantees."
  - [corpus]: Weak evidence - no directly comparable mechanism found in related papers.
- **Break condition**: If the smooth distribution assumption fails (dπ/d0 not bounded), the ensemble approximation breaks down and regret bounds no longer hold.

### Mechanism 2
- **Claim**: Bootstrap-DAgger collects higher quality data than DAgger by efficiently exploring beyond the expert's state distribution support.
- **Mechanism**: Bootstrapping creates diverse base policies that explore different regions of state space when rolling out, leading to more informative expert queries and faster error correction.
- **Core assumption**: The base policy class includes sufficient capacity to represent policies competitive with the expert.
- **Evidence anchors**:
  - [section]: "BD makes more efficient exploration than DAgger in regions beyond the support of the expert's state distribution."
  - [section]: "Bootstrap-DAgger out-performs DAgger and Behavior Cloning across continuous control tasks."
  - [corpus]: Weak evidence - related papers mention ensemble sampling but not specific to imitation learning data collection quality.
- **Break condition**: If the base policy class is too limited (non-realizable expert), the ensemble cannot explore effectively and performance degrades.

### Mechanism 3
- **Claim**: The online-to-batch conversion preserves competitive performance with the expert policy.
- **Mechanism**: The no-regret sequence of policies {πn} from the online learning game translates to a final policy competitive with the best policy in hindsight via uniform averaging.
- **Core assumption**: The expert policy is µ-recoverable with respect to the loss function used.
- **Evidence anchors**:
  - [abstract]: "our theoretical guarantees hold for any policy class, which is considerably broader than prior state of the art."
  - [section]: "Theorem 2 ([40]). Suppose (M, πexp) is µ-recoverable with respect to ℓ... Then ˆπ, which is by choosing a policy uniformly at random from {πn}N
n=1 and adhering to it satisfies: J(ˆπ)−J(πexp) ≤ µH..."
  - [corpus]: Weak evidence - no directly comparable conversion mechanism found in related papers.
- **Break condition**: If the expert is not µ-recoverable (large mistakes cannot be recovered), the performance guarantee breaks down.

## Foundational Learning

- **Concept**: Online learning regret minimization
  - Why needed here: The algorithm reduces imitation learning to an online learning problem where policies are selected sequentially to minimize cumulative loss
  - Quick check question: What does it mean for an algorithm to have sublinear regret in the context of online learning?

- **Concept**: Martingale concentration inequalities
  - Why needed here: High-probability bounds on the regret require concentration results for martingale difference sequences arising from the online learning process
  - Quick check question: How does Azuma-Hoeffding inequality apply to bounding the approximation term in the regret decomposition?

- **Concept**: Classification oracle model
  - Why needed here: Computational efficiency relies on being able to query a classification oracle that returns the policy minimizing empirical loss on a given dataset
  - Quick check question: What computational assumption allows MFTPL-P to be oracle-efficient?

## Architecture Onboarding

- **Component map**: MDP environment -> Policy class B -> Classification oracle O -> Smooth distribution d0 -> Online learning loop -> Aggregation function

- **Critical path**: Data collection → Oracle query → Ensemble training → Policy update → Repeat
  - The bottleneck is typically oracle queries, which scale with ensemble size E and training iterations

- **Design tradeoffs**:
  - Larger ensemble size E → better approximation of idealized predictor but higher computational cost
  - Smaller K (samples per round) → more rounds needed but potentially faster per-round computation
  - Stronger smooth distribution assumption → tighter regret bounds but may be harder to satisfy

- **Failure signatures**:
  - Poor performance despite many rounds → likely base policy class insufficient or smooth distribution assumption violated
  - High variance in results → ensemble size too small or insufficient oracle calls per round
  - Slow convergence → exploration distribution not covering relevant state space

- **First 3 experiments**:
  1. Run with E=1 (baseline DAgger) vs E=5 to verify ensemble benefit on simple task
  2. Vary K (samples per round) to find sweet spot between data quality and computational efficiency
  3. Test with different smooth distributions d0 to verify sensitivity to exploration distribution choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Bootstrap-DAgger's data collection efficiency compare to DAgger in regions with sparse expert coverage?
- Basis in paper: [explicit] The paper shows Bootstrap-DAgger collects "pertinent expert demonstration data more efficiently" and explores "regions beyond the support of the expert's state distribution." 
- Why unresolved: The paper provides t-SNE visualizations and qualitative comparisons but doesn't quantify the efficiency gain in terms of state coverage or information gain per expert query.
- What evidence would resolve it: A quantitative comparison of state visitation distributions between Bootstrap-DAgger and DAgger, measuring coverage of regions where expert data is sparse.

### Open Question 2
- Question: What is the impact of ensemble size on Bootstrap-DAgger's performance in non-realizable settings with very limited expert data?
- Basis in paper: [explicit] The paper shows BD-5 and BD-25 outperform baselines in non-realizable settings, but doesn't analyze the effect of ensemble size under extreme data scarcity.
- Why unresolved: The paper only tests ensemble sizes up to 25 and doesn't explore how performance scales with ensemble size when expert annotations are severely limited.
- What evidence would resolve it: Empirical results showing performance vs. ensemble size curves for Bootstrap-DAgger with varying levels of expert data availability.

### Open Question 3
- Question: How sensitive is Bootstrap-DAgger's performance to the choice of base policy architecture (e.g., MLP vs. other function approximators)?
- Basis in paper: [inferred] The paper uses MLPs for all experiments but doesn't explore how performance changes with different architectures.
- Why unresolved: The paper assumes MLPs are suitable but doesn't provide ablation studies or comparisons with other architectures like transformers or decision trees.
- What evidence would resolve it: Comparative results showing Bootstrap-DAgger's performance with different base policy architectures across multiple tasks.

## Limitations
- The theoretical guarantees rely heavily on unverifiable smoothness assumptions that may be difficult to satisfy in practice
- Bootstrap-DAgger lacks formal theoretical guarantees beyond intuitive justification
- The online-to-batch conversion assumes µ-recoverability of the expert policy, which may not hold for all problems

## Confidence
- Sublinear regret guarantees: Medium - depends on unverifiable smoothness assumption
- Practical performance improvements: Medium - strong empirical results but limited hyperparameter analysis
- Bootstrap-DAgger theoretical foundation: Low - lacks formal guarantees beyond intuition

## Next Checks
1. Test sensitivity to smooth distribution d0 by varying its support and measuring impact on regret bounds
2. Evaluate ensemble size scaling empirically to verify the approximation to infinite ensembles
3. Benchmark against alternative ensemble methods (bagging, boosting) in the imitation learning setting