---
ver: rpa2
title: Overwriting Pretrained Bias with Finetuning Data
arxiv_id: '2303.06167'
source_url: https://arxiv.org/abs/2303.06167
tags:
- bias
- netuning
- pretrained
- dataset
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how biases in pretrained models transfer
  to downstream tasks during finetuning. The authors examine two types of bias: spurious
  correlations between target labels and sensitive attributes, and underrepresentation
  of certain groups.'
---

# Overwriting Pretrained Bias with Finetuning Data

## Quick Facts
- arXiv ID: 2303.06167
- Source URL: https://arxiv.org/abs/2303.06167
- Reference count: 40
- Primary result: Pretrained model biases can be overwritten by manipulating the finetuning dataset distribution.

## Executive Summary
This paper investigates how biases in pretrained models transfer to downstream tasks during finetuning. The authors examine two types of bias: spurious correlations between target labels and sensitive attributes, and underrepresentation of certain groups. Through extensive experiments on datasets like CelebA and COCO, they find that models finetuned on top of pretrained models can inherit their biases, especially when the correlation level is high, the salience of the bias signal is high relative to the true task signal, and/or the number of finetuning samples is low. However, the authors demonstrate that this bias can be corrected for through relatively minor interventions to the finetuning dataset, often with a negligible impact on performance. For example, by manipulating the strength of the spurious correlation in the finetuning dataset from 20% to 30%, they can retain the same high performance from using a biased pretrained model, but cut by almost half the amount of bias. These findings suggest that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.

## Method Summary
The paper investigates bias transference from pretrained models to downstream tasks through finetuning. Experiments use ResNet50 architecture and various pretrained models (Scratch, TorchVision, MoCo, SimCLR, Places). The authors manipulate finetuning datasets by adjusting correlation levels between target labels and sensitive attributes (e.g., gender), as well as proportions of underrepresented groups. They evaluate performance using AUC and accuracy metrics, and fairness using FPR difference and directional bias amplification. A grid search approach is used for hyperparameter optimization during finetuning.

## Key Results
- Pretrained model biases can be inherited during finetuning, with effect sizes of 2.19 for correlation level and 1.30 for salience (both p < .05)
- Bias transference is more pronounced with fewer finetuning samples
- Manipulating the finetuning dataset can reduce bias while maintaining performance
- For example, increasing spurious correlation from 20% to 30% in the finetuning dataset cuts bias by almost half while retaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained model biases can be overwritten by manipulating the distribution of the finetuning dataset.
- Mechanism: By altering the proportion of data that exhibits the spurious correlation or underrepresentation during finetuning, the model learns to rely less on these biased signals, reducing their impact on downstream performance.
- Core assumption: The finetuning process is flexible enough to override the learned representations from pretraining when given appropriate data signals.
- Evidence anchors:
  - [abstract] "this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance."
  - [section] "By manipulating the strength of the spurious correlation in the finetuning dataset from 20% to 30%, we can retain the same high performance from using a biased pretrained model, but cut by almost half the amount of bias."
  - [corpus] Weak - no direct evidence in corpus about dataset manipulation effectiveness.
- Break condition: If the number of finetuning samples is too low, the model may not have enough data to override the pretrained biases.

### Mechanism 2
- Claim: The effectiveness of bias correction through dataset manipulation depends on the correlation level and salience of the bias in the downstream task.
- Mechanism: High correlation levels between the target task and sensitive attribute make it more likely that pretrained biases will transfer, while low salience of the target task relative to the sensitive attribute makes the bias more influential.
- Core assumption: The model's learning process prioritizes more salient signals when available.
- Evidence anchors:
  - [section] "We find the effect size for correlation level to be 2.19... and salience to be 1.30... both with p < .05."
  - [section] "The positive coefficient for correlation level indicates that the stronger the correlation between the target task and gender... the more it will be affected by a biased pretrained model."
  - [corpus] Weak - no direct evidence in corpus about correlation level and salience effects.
- Break condition: If the downstream task's true signal is more salient than the biased signal, the effect of pretrained bias may be negligible regardless of correlation level.

### Mechanism 3
- Claim: The number of finetuning samples influences the degree to which pretrained biases are inherited.
- Mechanism: With fewer finetuning samples, the model is more likely to rely on the biased representations from pretraining, while with more samples, it can learn to overcome these biases.
- Core assumption: The finetuning process can effectively overwrite pretrained representations given sufficient data.
- Evidence anchors:
  - [section] "While with sufficient finetuning samples all three models converge to the same levels of performance and bias..."
  - [section] "However, when the fairnesses converge, it becomes negligible which pretrained model was used."
  - [corpus] Weak - no direct evidence in corpus about sample size effects.
- Break condition: If the finetuning dataset is too small, even optimal manipulation may not overcome pretrained biases.

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: The paper's core premise is that pretrained models can be leveraged for downstream tasks, but biases may transfer. Understanding how transfer learning works is crucial to understanding why biases can propagate.
  - Quick check question: What are the benefits and potential drawbacks of using transfer learning in machine learning?

- Concept: Spurious Correlations
  - Why needed here: The paper operationalizes bias as spurious correlations between target labels and sensitive attributes. Understanding what spurious correlations are and why they are problematic is essential to grasping the paper's findings.
  - Quick check question: Can you provide an example of a spurious correlation in a real-world dataset?

- Concept: Underrepresentation Bias
  - Why needed here: The paper also considers bias in terms of underrepresentation of certain groups. Understanding how underrepresentation can lead to biased models is key to interpreting the paper's results on this form of bias.
  - Quick check question: How might underrepresentation of certain groups in a training dataset lead to biased model predictions?

## Architecture Onboarding

- Component map: Pretrained model (Scratch, Gendered, Control) -> Finetuning dataset (manipulated correlation levels, underrepresented group proportions) -> Finetuned model
- Critical path: Finetuning pretrained model on manipulated downstream dataset to achieve high performance while minimizing bias
- Design tradeoffs: The main tradeoff is between performance and fairness. Manipulating the finetuning dataset to reduce bias may come at the cost of some performance, although the paper shows that this tradeoff can often be minimized.
- Failure signatures: If the number of finetuning samples is too low, or if the correlation level and salience are not appropriately considered, the model may fail to overcome pretrained biases. Additionally, if the finetuning dataset manipulation is too extreme, it may lead to poor generalization on the test set.
- First 3 experiments:
  1. Finetune a pretrained model on a downstream task with a high correlation level between the target label and sensitive attribute, and observe the degree of bias transfer.
  2. Manipulate the finetuning dataset to have a lower correlation level and finetune the same pretrained model, observing the impact on bias and performance.
  3. Repeat experiment 2 with different levels of dataset manipulation to find the optimal balance between bias reduction and performance retention.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions in the provided text.

## Limitations

- The paper's claims about dataset manipulation effectiveness are supported primarily by experimental results rather than theoretical guarantees. The mechanisms by which finetuning overcomes pretrained biases are not fully explained.
- While the paper demonstrates that bias can be reduced through dataset manipulation, the optimal level of manipulation for balancing fairness and performance is not definitively established.
- The experiments focus on image classification tasks; generalizability to other domains (e.g., NLP) remains uncertain.
- The paper does not address potential long-term consequences of dataset manipulation on model robustness or generalization.

## Confidence

- High confidence in the experimental findings showing that pretrained model biases can transfer to downstream tasks during finetuning, and that these biases can be reduced through dataset manipulation.
- Medium confidence in the generalizability of the findings to other domains and tasks beyond image classification.
- Low confidence in the theoretical underpinnings of why dataset manipulation is effective at reducing bias, as this is not thoroughly explored.

## Next Checks

1. Replicate the experiments with different pretrained models (e.g., BERT, GPT) on text classification tasks to assess generalizability across domains.
2. Investigate the impact of dataset manipulation on model robustness and generalization by testing on out-of-distribution data or adversarial examples.
3. Develop a theoretical framework to explain why dataset manipulation is effective at reducing bias, potentially drawing insights from causal inference or information theory.