---
ver: rpa2
title: Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation
arxiv_id: '2303.01687'
source_url: https://arxiv.org/abs/2303.01687
tags:
- data
- private
- neural
- kernel
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a differentially private method for generating
  synthetic data by minimizing Maximum Mean Discrepancy (MMD) between real and synthetic
  data using neural tangent kernels (NTKs). The key innovation is the use of empirical
  NTK features, which provide a data-independent representation for the MMD loss without
  requiring any public data for pretraining.
---

# Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation

## Quick Facts
- arXiv ID: 2303.01687
- Source URL: https://arxiv.org/abs/2303.01687
- Reference count: 5
- Key outcome: DP-NTK achieves better privacy-accuracy trade-offs than DP-MERF and DP-HP while producing comprehensible and diverse samples

## Executive Summary
This paper introduces DP-NTK, a differentially private method for generating synthetic data by minimizing Maximum Mean Discrepancy (MMD) between real and synthetic data using neural tangent kernels (NTKs). The key innovation is using empirical NTK features from an untrained neural network, which provide a data-independent representation for the MMD loss without requiring public data for pretraining. These features are extracted and normalized, then privatized via the Gaussian mechanism to create a fixed target for training a generator network. Experiments on image datasets (MNIST, FashionMNIST, CelebA, CIFAR10) and tabular data show that DP-NTK achieves competitive or superior performance compared to existing private generative models while maintaining privacy guarantees.

## Method Summary
DP-NTK extracts empirical NTK features from an untrained neural network, computes the mean embedding of real data in this feature space, and privatizes it using the Gaussian mechanism. A generator network is then trained to minimize the MMD between its synthetic data's mean embedding and the privatized real data mean embedding. The method uses a one-shot privatization approach where the privatized mean embedding remains constant during generator training, avoiding additional privacy loss. NTK features are normalized to bound sensitivity, and the generator is trained using gradient-based optimization to match the privatized statistics of real data.

## Key Results
- DP-NTK outperforms DP-MERF and DP-HP on MNIST, FashionMNIST, and tabular datasets in terms of classification accuracy
- Visual samples from DP-NTK show comparable quality to non-private GANs while maintaining strong privacy guarantees
- The method scales to high-dimensional data like CIFAR10 and CelebA while preserving privacy-utility trade-offs
- NTK features from untrained networks provide sufficient expressiveness for generation without public pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using empirical NTK features as a fixed data-independent representation enables one-shot privatization of the mean embedding without additional privacy loss during generator training.
- Mechanism: The empirical NTK features are extracted from an untrained neural network and normalized, providing a fixed feature space. The mean embedding of the training data is computed once and privatized using the Gaussian mechanism. During training, the privatized mean embedding is kept constant while the generator's mean embedding is computed from generated samples and the MMD loss is calculated.
- Core assumption: The NTK features provide sufficient expressiveness to distinguish real from synthetic data distributions, and the normalization bounds the sensitivity of the mean embedding.
- Evidence anchors:
  - [abstract]: "These features are extracted from an untrained neural network and then privatized via the Gaussian mechanism."
  - [section 3]: "The normalization is necessary to bound the sensitivity of the mean embedding."
- Break condition: If the NTK features fail to capture the essential structure of the data distributions, or if the normalization fails to bound sensitivity adequately.

### Mechanism 2
- Claim: The NTK-based MMD loss corresponds to training a GAN with an infinitely wide discriminator, providing strong theoretical motivation for using NTKs in generative modeling.
- Mechanism: As the width of a neural network grows, the empirical NTK converges to a fixed kernel that depends only on the architecture. GANs with discriminators in the infinite-width limit theoretically become MMD minimizers using this NTK. By using NTK features, DP-NTK implicitly approximates this infinite-width behavior.
- Core assumption: The practical NTK features approximate the theoretical infinite-width behavior sufficiently well for effective generative modeling.
- Evidence anchors:
  - [section 2]: "GANs with discriminators in an appropriate infinite limit theoretically become MMD minimizers using the NTK of the discriminator architecture."
  - [section 4]: "If the squared MMD is small, no function with small norm under the kernel k can strongly distinguish P and Q."
- Break condition: If the finite-dimensional NTK features fail to capture the expressiveness of the theoretical infinite-width kernel.

### Mechanism 3
- Claim: The privacy-utility tradeoff improves by using NTK features instead of random Fourier features or Hermite polynomial features, as NTKs provide better expressiveness for the same dimensionality.
- Mechanism: NTK features are extracted from a neural network architecture, providing a learned prior over useful functions for the task. This learned prior makes NTKs more expressive than random Fourier features (which are data-independent) or Hermite polynomials (which are fixed basis functions). Higher expressiveness allows better generation quality at the same privacy level.
- Core assumption: The NTK features provide better expressiveness per dimension than alternative feature choices.
- Evidence anchors:
  - [abstract]: "We find that, perhaps surprisingly, the expressiveness of the untrained e-NTK features is comparable to that of the features taken from pre-trained perceptual features using public data."
  - [section 1]: "NTKs have proved to be powerful general-purpose kernels for small-data classification tasks... They have similarly been shown to work well for the problem of statistically testing whether the MMD between two distributions is nonzero."
- Break condition: If the expressiveness advantage of NTK features over alternatives is minimal or non-existent for the target dataset.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: The method must ensure that the generated data does not reveal information about individual training examples. Understanding DP guarantees and mechanisms is essential for implementing and evaluating the privacy-accuracy tradeoff.
  - Quick check question: What is the difference between (ϵ,0)-DP and (ϵ,δ)-DP, and why might one choose the latter?

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is the core objective function for training the generator. Understanding how it measures distributional distance and how it can be estimated from finite samples is crucial for implementing the method correctly.
  - Quick check question: How does the choice of kernel affect the MMD, and why is this important for generative modeling?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: NTK features are the key innovation that enables the method. Understanding what NTKs are, how they arise from neural network training, and why they might be useful for generative modeling is essential for appreciating the method's contributions.
  - Quick check question: What is the relationship between the empirical NTK and the infinite-width NTK, and why does this matter for practical implementation?

## Architecture Onboarding

- Component map: Untrained NTK network -> NTK feature extraction -> Mean embedding computation -> Gaussian mechanism -> Privatized embedding -> Generator training loop
- Critical path: Generator training loop: noise vectors -> generator -> synthetic data -> NTK feature extraction -> mean embedding computation -> MMD loss calculation -> parameter update
- Design tradeoffs: The method trades off the complexity of the NTK network (which affects expressiveness) against the dimensionality of the feature space (which affects privacy noise). The choice of normalization for the NTK features balances sensitivity bounds against information preservation.
- Failure signatures: If the generated samples are low-quality or lack diversity, this could indicate insufficient expressiveness of the NTK features, inadequate privacy noise calibration, or poor generator architecture. If privacy guarantees are not met, this could indicate incorrect sensitivity calculation or insufficient noise addition.
- First 3 experiments:
  1. Verify NTK feature extraction: Extract NTK features from the untrained network on a small dataset and visualize them to ensure they capture meaningful structure.
  2. Test privatization: Compute the mean embedding of a small dataset, apply the Gaussian mechanism with different noise levels, and verify that the sensitivity bounds are respected.
  3. Validate MMD loss: Implement the MMD loss using the privatized mean embedding and synthetic data, and verify that it decreases during generator training.

## Open Questions the Paper Calls Out

- Open Question 1: How would using the infinite-width neural tangent kernel (NTK) instead of empirical NTK (e-NTK) affect the privacy-accuracy trade-off in DP-NTK?
  - Basis in paper: [inferred] The paper mentions that infinite NTKs tend to have better performance than e-NTKs at initialization, but using them in privacy settings is more challenging.
  - Why unresolved: The paper does not provide empirical results comparing the use of infinite NTK versus e-NTK in DP-NTK, leaving the impact on privacy-accuracy trade-off unclear.
  - What evidence would resolve it: Empirical results showing the performance of DP-NTK using infinite NTK compared to e-NTK on benchmark datasets would clarify the impact on privacy-accuracy trade-off.

- Open Question 2: How can DP-NTK incorporate information from related public datasets for pretraining to improve performance?
  - Basis in paper: [explicit] The paper suggests that incorporating information from related public datasets could improve DP-NTK performance, as done by Harder et al. (2022).
  - Why unresolved: The paper does not provide a detailed strategy or empirical results on how to incorporate related public datasets into DP-NTK.
  - What evidence would resolve it: A detailed methodology and empirical results showing the improvement in DP-NTK performance when incorporating related public datasets would address this question.

- Open Question 3: What are the potential benefits and challenges of using other kernel-based methods, such as Gaussian kernels or random Fourier features, in the context of differentially private data generation?
  - Basis in paper: [explicit] The paper discusses the use of Gaussian kernels and random Fourier features in existing methods like DP-MERF and DP-HP, but does not explore other kernel-based methods.
  - Why unresolved: The paper focuses on NTK features and does not provide a comprehensive comparison of different kernel-based methods in differentially private data generation.
  - What evidence would resolve it: A systematic comparison of various kernel-based methods, including Gaussian kernels and random Fourier features, in the context of differentially private data generation would provide insights into their benefits and challenges.

## Limitations

- The theoretical claims about NTK expressiveness and its connection to infinite-width GANs are asserted but not rigorously tested
- The normalization scheme for NTK features and its impact on sensitivity bounds is not fully specified
- For tabular data, the handling of class imbalance and the specific evaluation protocol require clarification
- The paper lacks quantitative comparisons of NTK feature expressiveness against pre-trained perceptual features

## Confidence

- High confidence: The core privacy mechanism (one-shot privatization of mean embedding) is sound and well-grounded in differential privacy theory
- Medium confidence: The empirical results showing competitive performance on benchmark datasets, though the absence of some standard baselines limits generalizability
- Low confidence: The theoretical claims about NTK expressiveness and its connection to infinite-width GANs, as these are asserted but not rigorously tested

## Next Checks

1. **Reproduce the privacy analysis**: Implement the one-shot privatization of mean embeddings and verify that the sensitivity bounds are correctly calculated for different NTK architectures and normalization schemes.
2. **Benchmark against additional baselines**: Compare DP-NTK against standard differentially private baselines (DP-SGD, DP-GANs) on a held-out dataset to assess relative performance.
3. **Analyze feature expressiveness**: Quantitatively compare the expressiveness of NTK features to random Fourier features and pre-trained perceptual features on a simple classification task to validate the paper's qualitative claims.