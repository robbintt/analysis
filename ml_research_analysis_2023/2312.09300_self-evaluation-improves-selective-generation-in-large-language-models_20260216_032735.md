---
ver: rpa2
title: Self-Evaluation Improves Selective Generation in Large Language Models
arxiv_id: '2312.09300'
source_url: https://arxiv.org/abs/2312.09300
tags:
- answer
- sample
- arxiv
- generation
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of assessing the quality of language
  model (LM) generations for safe deployment. While LM sequence probabilities are
  often used, they are not well-calibrated for quality.
---

# Self-Evaluation Improves Selective Generation in Large Language Models

## Quick Facts
- arXiv ID: 2312.09300
- Source URL: https://arxiv.org/abs/2312.09300
- Authors: 
- Reference count: 11
- The paper demonstrates that self-evaluation based scores improve accuracy and calibration in selective generation compared to sequence likelihood baselines.

## Executive Summary
This paper addresses the challenge of assessing the quality of language model generations for safe deployment. While language models are better calibrated at the token level, their sequence-level probabilities often fail to accurately reflect generation quality. The authors propose reformulating open-ended generation tasks into token-level prediction tasks, where the model self-evaluates its answers using multi-way comparison or point-wise evaluation. By including a "None of the above" option, the method mitigates overconfidence when none of the sampled answers is correct. Experiments with PaLM-2 and GPT-3 on TruthfulQA and TL;DR datasets show that self-evaluation based scores not only improve accuracy but also better correlate with overall generation quality.

## Method Summary
The paper proposes reducing free-form generation tasks into token-level prediction tasks to leverage the superior calibration of large language models at the token level. The method involves instructing an LM to self-evaluate its answers using either multi-way comparison (Sample and Select) or point-wise evaluation (Sample and Eval), with an optional "None of the above" choice to express uncertainty. The authors benchmark various scoring methods based on self-evaluation and evaluate their performance in selective generation using the TruthfulQA and TL;DR datasets. A hybrid approach combines the strengths of both sample and select and sample and eval methods to improve accuracy and calibration.

## Key Results
- Self-evaluation based scores improve both accuracy and calibration-AUC compared to sequence-level likelihood baselines
- Including "None of the above" option reduces overconfidence and improves selective generation performance
- The hybrid approach combining sample-and-select with sample-and-eval achieves overall better performance
- Self-evaluation methods show strong performance on both TruthfulQA and TL;DR benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing sequence-level generation to token-level evaluation leverages better-calibrated token probabilities in LLMs
- Core assumption: Token-level probabilities from LLMs are better calibrated than sequence-level probabilities for quality assessment
- Evidence anchors:
  - [abstract] "LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements."
  - [section] "Inspired by this, we propose to reduce free-form generation to multiple-choice and true/false evaluation tasks, in order to leverage token-level calibration to improve the calibration of free-form generation."
- Break condition: If token-level probabilities are not better calibrated than sequence-level probabilities, the method would not improve quality assessment

### Mechanism 2
- Claim: Including a "None of the above" option mitigates overconfidence when none of the sampled answers is correct
- Core assumption: LLMs can accurately assess their own uncertainty and choose "None of the above" when appropriate
- Evidence anchors:
  - [section] "To mitigate that, we add 'NONE OF THE ABOVE' as an additional candidate answer to give model a chance to reject the sampled answers."
  - [section] "A higher nota score indicates that the selected answer is less likely to be correct. So we use -p(cnota|x, {cy}+nota) as the confidence score of the selected answer."
- Break condition: If the model consistently fails to choose "None of the above" when appropriate, the method would not effectively reduce overconfidence

### Mechanism 3
- Claim: The hybrid approach combines the strengths of both sample and select and sample and eval methods to improve accuracy and calibration
- Core assumption: Combining multi-choice selection with pointwise evaluation results in better overall performance than using either method alone
- Evidence anchors:
  - [section] "Therefore, we combine the best of both: We first use Sample and Select to select the best answer within a given question. The answer with the highest softmax probability score is selected... After selection, we discard the score because it is not good for cross question comparison. We score the selected answer via Sample and Eval p(Yes|x, Å·)."
  - [section] "The hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance."
- Break condition: If either the selection or evaluation component fails to perform well, the hybrid approach would not yield improved results

## Foundational Learning

- Concept: Token-level probability calibration
  - Why needed here: Understanding that LLMs are better calibrated at the token level is crucial for leveraging this property to improve quality assessment in generation tasks
  - Quick check question: Why are token-level probabilities from LLMs considered better calibrated than sequence-level probabilities for quality assessment?

- Concept: Selective generation
  - Why needed here: Selective generation is the process of abstaining from generating poor-quality outputs, which is the primary goal of the proposed method
  - Quick check question: How does the proposed method improve the ability to selectively generate high-quality outputs?

- Concept: Confidence scoring
  - Why needed here: Confidence scoring is essential for determining when to abstain from generating an output based on the model's assessment of its own quality
  - Quick check question: How does the inclusion of a "None of the above" option in the confidence scoring process help mitigate overconfidence?

## Architecture Onboarding

- Component map: Sampling component -> Selection component -> Evaluation component -> Hybrid component
- Critical path: 1. Receive a question 2. Generate multiple candidate answers 3. Apply sample and select to choose the best answer 4. Apply sample and eval to score the selected answer 5. Output the selected answer and its confidence score
- Design tradeoffs:
  - Inference time vs. accuracy: The hybrid approach increases inference time but improves accuracy and calibration
  - Position bias vs. simplicity: Shuffling answers to reduce position bias increases computational complexity but improves fairness
- Failure signatures:
  - Poor accuracy: Indicates issues with the selection component or the quality of candidate answers
  - Low calibration-AUC: Suggests problems with the evaluation component or overconfidence in the model's assessments
- First 3 experiments:
  1. Compare the accuracy and calibration-AUC of sequence likelihood vs. the proposed hybrid method on a benchmark dataset
  2. Evaluate the impact of including the "None of the above" option on the model's ability to express uncertainty
  3. Assess the effect of position bias on the performance of the sample and select method by comparing vanilla and de-biased settings

## Open Questions the Paper Calls Out
1. How can we improve the quality calibration of sequence-level scores during training and fine-tuning of large language models?
2. How does the performance of self-evaluation based scores compare to other methods for selective generation?
3. How does the performance of self-evaluation based scores vary across different types of language generation tasks?

## Limitations
- Lack of detailed prompt specifications makes exact reproduction difficult
- No analysis of computational overhead from generating multiple candidate answers and performing self-evaluation
- Limited exploration of failure cases where the method might underperform

## Confidence
- High confidence: The general mechanism of reducing sequence-level generation to token-level evaluation improves calibration
- Medium confidence: The specific contribution of the "None of the above" option in mitigating overconfidence
- Medium confidence: The superiority of the hybrid approach over individual sample-and-select or sample-and-eval methods

## Next Checks
1. Reproduce the core experimental results using the provided datasets while testing with at least two different prompting strategies for the self-evaluation tasks
2. Conduct an ablation study specifically isolating the impact of the "None of the above" option by comparing performance with and without this option across different model sizes
3. Test the method's robustness on a third, previously unseen dataset with different generation characteristics to assess generalizability beyond the two benchmark datasets used in the paper