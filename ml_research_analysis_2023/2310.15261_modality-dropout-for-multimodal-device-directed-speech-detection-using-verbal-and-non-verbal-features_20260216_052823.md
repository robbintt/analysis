---
ver: rpa2
title: Modality Dropout for Multimodal Device Directed Speech Detection using Verbal
  and Non-Verbal Features
arxiv_id: '2310.15261'
source_url: https://arxiv.org/abs/2310.15261
tags:
- ddsd
- fusion
- speech
- features
- prosody
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a device-directed speech detection (DDSD)
  system that fuses non-verbal prosody features with traditional verbal cues (acoustic,
  text, ASR) to improve performance. The authors demonstrate that prosody features,
  captured via pitch, voicing, jitter, shimmer, and voice activity detection, can
  enhance DDSD accuracy when combined with verbal features using late and intermediate
  fusion schemes.
---

# Modality Dropout for Multimodal Device Directed Speech Detection using Verbal and Non-Verbal Features

## Quick Facts
- arXiv ID: 2310.15261
- Source URL: https://arxiv.org/abs/2310.15261
- Authors: 
- Reference count: 23
- Primary result: Prosody features combined with verbal cues improve DDSD accuracy, with non-linear intermediate fusion and modality dropout reducing FA by up to 8.5% and 7.4% respectively

## Executive Summary
This paper addresses the challenge of detecting device-directed speech (DDSD) by introducing a multimodal fusion approach that incorporates non-verbal prosody features alongside traditional verbal cues. The authors demonstrate that prosody features—including pitch, voicing, jitter, shimmer, and voice activity detection—can significantly enhance DDSD accuracy when combined with acoustic, text, and ASR features. Their approach uses non-linear intermediate fusion of embeddings from single-modality models and employs modality dropout during training to improve robustness to missing data, achieving up to 8.5% reduction in false acceptance rate compared to verbal-only models.

## Method Summary
The study employs a multi-modal dataset of over 19,000 audio recordings from 1,300 participants to train and evaluate DDSD models. The methodology involves extracting prosody features (pitch, voicing, jitter, shimmer, VAD) and training single-modality DDSD models for acoustic, text, ASR, and prosody-based detection. These models are then combined using late fusion (linear and non-linear) and intermediate fusion schemes, where non-linear layers are applied to embeddings from penultimate layers. The authors introduce modality dropout during training, stochastically dropping different modality inputs with predefined probabilities to improve robustness to missing data at inference time. Models are trained using Adam optimizer with learning rate 0.001, gradient clipping, and weighted binary cross-entropy loss for 50 epochs.

## Key Results
- Non-verbal prosody features improve DDSD accuracy when combined with verbal cues
- Non-linear intermediate fusion outperforms late fusion schemes by exploiting richer information in embeddings
- Modality dropout during training improves robustness to missing modalities, reducing FA by 7.4% at 30% missing data
- The proposed approach achieves up to 8.5% reduction in false acceptance rate compared to verbal-only models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-verbal prosody features improve DDSD accuracy by providing complementary information to verbal cues
- Mechanism: Prosody features capture how speech is delivered rather than just what is said, allowing the model to distinguish between device-directed speech (often louder, slower, more articulated) and casual conversation.
- Core assumption: The acoustic characteristics of device-directed speech differ systematically from casual speech in ways that prosody features can detect
- Evidence anchors:
  - [abstract] "prosody features, captured via pitch, voicing, jitter, shimmer, and voice activity detection, can enhance DDSD accuracy when combined with verbal features"
  - [section] "humans tend to talk louder, slower, over-articulate, and avoid having stops or speech fillers... when they issue a command to a VA"
- Break condition: If the acoustic characteristics of device-directed speech don't differ systematically from casual speech, or if the prosody features fail to capture these differences

### Mechanism 2
- Claim: Intermediate fusion outperforms late fusion because embeddings contain richer information than scores
- Mechanism: The penultimate layer embeddings from individual DDSD models contain more detailed information than the final output scores. Non-linear intermediate fusion can exploit this richer information to make better classification decisions.
- Core assumption: Embeddings from penultimate layers contain complementary information that isn't fully captured in the final scores
- Evidence anchors:
  - [abstract] "we apply non-linear layers to the embeddings that are extracted from the penultimate layers"
  - [section] "Intermediate layers of single-modality DNN models usually contain richer information than just the output scores"
- Break condition: If the embeddings don't contain significantly more information than the scores, or if the non-linear fusion doesn't effectively exploit this information

### Mechanism 3
- Claim: Modality dropout during training improves robustness to missing modalities at inference time
- Mechanism: By stochastically dropping different modality inputs during training with predefined probabilities, the fusion model learns to make robust predictions even when some modalities are missing at inference time.
- Core assumption: Training with randomly missing modalities forces the model to develop redundant information pathways that make it robust to missing data
- Evidence anchors:
  - [abstract] "our use of modality dropout techniques improves the performance of these models by 7.4% in terms of FA when evaluated with missing modalities during inference time"
  - [section] "We investigate an input modality dropout technique during training to improve the robustness to missing data for the late and intermediate fusion models"
- Break condition: If the dropout probabilities aren't well-tuned, or if the model becomes too reliant on specific modality combinations

## Foundational Learning

- Concept: Binary classification and evaluation metrics (EER, FA@FR)
  - Why needed here: DDSD is a binary classification task, and understanding evaluation metrics is crucial for interpreting results
  - Quick check question: What does a lower FA@10% FR value indicate about a DDSD model's performance?

- Concept: Feature fusion techniques (early, intermediate, late fusion)
  - Why needed here: The paper compares different fusion schemes for combining prosody and verbal features
  - Quick check question: What's the key difference between intermediate fusion and late fusion in terms of what information is combined?

- Concept: Modality dropout regularization
  - Why needed here: This technique is used to make fusion models robust to missing data
  - Quick check question: How does modality dropout during training help with missing data at inference time?

## Architecture Onboarding

- Component map:
  Prosody feature extraction -> GRU-based DDSD model -> Embeddings extraction
  Acoustic model -> Embeddings extraction
  Text model -> Embeddings extraction
  ASR model -> Embeddings extraction
  Fusion layer (linear/non-linear/intermediate) -> DDSD output

- Critical path:
  1. Extract prosody features from raw audio
  2. Train single-modality DDSD models
  3. Extract embeddings from penultimate layers
  4. Train fusion models with modality dropout
  5. Evaluate on test set with and without missing modalities

- Design tradeoffs:
  - Prosody features vs. computational cost: Adding prosody features improves accuracy but increases computational requirements
  - Intermediate fusion complexity vs. performance: More complex than late fusion but provides better results
  - Modality dropout regularization vs. potential underfitting: Helps with missing data robustness but might slow convergence

- Failure signatures:
  - Poor performance on missing modalities test: Modality dropout probabilities may need adjustment
  - Minimal improvement from prosody features: Feature extraction or model architecture might need revision
  - Overfitting on validation set: Regularization parameters (dropout rates) may need tuning

- First 3 experiments:
  1. Train single-modality DDSD models and evaluate baseline performance
  2. Implement intermediate fusion without modality dropout and compare to late fusion
  3. Add modality dropout to intermediate fusion and evaluate robustness to missing modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of prosody features (e.g., pitch, jitter, shimmer) individually contribute to DDSD performance compared to each other?
- Basis in paper: [explicit] The paper uses multiple prosody features but does not isolate their individual contributions to DDSD performance.
- Why unresolved: The paper combines all prosody features together without examining their individual effectiveness or identifying which features are most informative for DDSD.
- What evidence would resolve it: Experiments testing DDSD performance using each prosody feature independently, or ablation studies showing performance impact when individual features are removed.

### Open Question 2
- Question: How does the DDSD performance generalize across different languages and cultural contexts?
- Basis in paper: [inferred] The dataset is collected from English native speakers only, with no mention of cross-linguistic or cross-cultural validation.
- Why unresolved: The study is limited to English speakers and does not address whether the observed improvements with prosody features would translate to other languages or cultural communication patterns.
- What evidence would resolve it: DDSD performance evaluation using datasets from multiple languages and cultural contexts, with systematic comparison to the English results.

### Open Question 3
- Question: What is the optimal fusion strategy for different missing modality scenarios (random vs. systematic)?
- Basis in paper: [explicit] The paper only tests modality dropout with 30% randomly missing modalities, without exploring systematic patterns of missing data.
- Why unresolved: Real-world scenarios may involve systematic rather than random missing modalities (e.g., text unavailable during privacy mode, audio unavailable during touch interaction), which the current dropout strategy may not address optimally.
- What evidence would resolve it: Comparative studies testing different dropout strategies (random vs. systematic patterns) across various combinations of missing modalities, with corresponding fusion model adaptations.

### Open Question 4
- Question: How does the temporal resolution of prosody features affect DDSD performance?
- Basis in paper: [explicit] Prosody features are extracted at 100 Hz sampling frequency, but the paper does not explore whether this resolution is optimal or how performance changes with different sampling rates.
- Why unresolved: The paper uses a fixed temporal resolution without examining whether higher or lower sampling frequencies would improve DDSD accuracy or computational efficiency.
- What evidence would resolve it: Systematic evaluation of DDSD performance across different prosody feature sampling frequencies, with analysis of the trade-off between temporal resolution and model accuracy.

## Limitations

- The study relies on a proprietary multi-modal dataset, limiting reproducibility and external validation
- The effectiveness of modality dropout depends heavily on specific dropout probabilities chosen during training, with no sensitivity analysis provided
- Simulated random missing modalities may not accurately reflect real-world scenarios where certain modalities are systematically missing

## Confidence

**High Confidence**: The core finding that prosody features can improve DDSD accuracy when combined with verbal cues
**Medium Confidence**: The claim that intermediate fusion outperforms late fusion due to richer information in embeddings
**Medium Confidence**: The assertion that modality dropout improves robustness to missing modalities

## Next Checks

1. Test the prosody feature extraction and fusion approach on publicly available datasets (such as VoxCeleb or Librispeech) to verify generalization beyond the proprietary dataset used in the study

2. Design experiments where modalities are missing in systematic patterns that reflect real-world conditions (e.g., ASR degradation in noisy environments) rather than random dropout to better assess practical robustness

3. Conduct a comprehensive grid search over modality dropout probabilities and fusion layer configurations to identify optimal settings and understand the sensitivity of results to these hyperparameters