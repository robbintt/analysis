---
ver: rpa2
title: 'PubDef: Defending Against Transfer Attacks From Public Models'
arxiv_id: '2310.17645'
source_url: https://arxiv.org/abs/2310.17645
tags:
- adversarial
- attacks
- source
- attack
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PUBDEF, a defense method against transfer
  attacks from public models. It addresses the limitations of existing defenses which
  suffer from a trade-off between clean accuracy and adversarial robustness.
---

# PubDef: Defending Against Transfer Attacks From Public Models

## Quick Facts
- **arXiv ID**: 2310.17645
- **Source URL**: https://arxiv.org/abs/2310.17645
- **Reference count**: 40
- **Primary result**: PUBDEF achieves 62% accuracy under strongest transfer attack on ImageNet vs 36% for best adversarially trained model

## Executive Summary
PUBDEF addresses the trade-off between clean accuracy and adversarial robustness by training a model to resist transfer attacks from publicly available models. The defense leverages a game-theoretic perspective, selecting source models from different robustness categories and combining multiple defense mechanisms. Unlike traditional adversarial training, PUBDEF maintains near-undefended clean accuracy while achieving state-of-the-art robustness against transfer attacks, with only 2% clean accuracy drop compared to undefended models.

## Method Summary
PUBDEF trains a defended model using pre-generated transfer attacks from a selected subset of public source models. The method involves grouping 24 public models into four categories (normal, ℓ∞-adversarial, ℓ2-adversarial, corruption-robust), selecting one model from each group, and generating transfer attacks using 11 different algorithms. The defended model is then adversarially trained with combined clean and adversarial loss, achieving robustness through coverage of diverse adversarial subspaces while preserving clean accuracy.

## Key Results
- PUBDEF achieves 62% accuracy under strongest transfer attack on ImageNet vs 36% for best adversarially trained model
- Clean accuracy is only 2% lower than undefended model (78% vs 80%)
- Generalizes incredibly well to unseen attacks, losing only 1.7-7.6% robustness across datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PUBDEF achieves high adversarial accuracy by training on diverse transfer attacks from public models, exploiting low-dimensional structure of adversarial perturbations
- **Mechanism:** Transfer attacks from models within same robustness category lie in similar subspaces; selecting one from each group covers broad but overlapping adversarial subspaces
- **Core assumption:** Transfer attacks span low-dimensional subspaces allowing generalization from limited training set
- **Evidence anchors:** Outperforms white-box adversarial training with minimal clean accuracy loss; generalizes to unseen attacks with only 1.7-7.6% robustness loss
- **Break condition:** If transfer attacks span high-dimensional disjoint subspaces, PUBDEF would fail to generalize

### Mechanism 2
- **Claim:** PUBDEF maintains clean accuracy close to undefended models by avoiding over-regularization of white-box adversarial training
- **Mechanism:** Uses pre-generated transfer attacks instead of continuous perturbation during training, focusing on clean accuracy while learning to resist constrained attack space
- **Core assumption:** Transfer attacks from public models are less diverse than white-box attacks, avoiding overfitting
- **Evidence anchors:** Only 2% clean accuracy drop vs undefended model; avoids typical adversarial training trade-off
- **Break condition:** If transfer attacks become as diverse as white-box attacks, PUBDEF would suffer clean accuracy loss

### Mechanism 3
- **Claim:** PUBDEF's game-theoretic approach optimizes defender's strategy by balancing robustness across all transfer attack scenarios
- **Mechanism:** Formulates defense as game between attacker (choosing source model/algorithm) and defender (choosing model weights), approximating local equilibrium
- **Core assumption:** Payoff matrix is approximately linear or tractable for heuristic approximation
- **Evidence anchors:** Proposes heuristics for solving complex game without full payoff matrix computation
- **Break condition:** If payoff matrix is highly non-linear, heuristics may fail to find good equilibrium

## Foundational Learning

- **Concept:** Transfer attacks and their transferability across models
  - **Why needed here:** Understanding how adversarial examples transfer between models is crucial for grasping PUBDEF's threat model and defense strategy
  - **Quick check question:** Why do adversarial examples often transfer between models trained on the same task?

- **Concept:** Adversarial training and its limitations
  - **Why needed here:** PUBDEF is compared against white-box adversarial training, so knowing its strengths and weaknesses (e.g., clean accuracy drop) is essential
  - **Quick check question:** What is the main trade-off of standard adversarial training, and how does PUBDEF aim to avoid it?

- **Concept:** Game theory and Nash equilibrium
  - **Why needed here:** PUBDEF's defense strategy is inspired by game-theoretic perspective, so understanding basic concepts helps explain its design
  - **Quick check question:** In two-player zero-sum game, what does Nash equilibrium represent for defender?

## Architecture Onboarding

- **Component map:** Data pipeline -> Source model selection -> Transfer attack generation -> Defended model training -> Evaluation
- **Critical path:** 1) Pre-generate transfer attacks from selected source models 2) Train PUBDEF model with combined clean and adversarial loss 3) Evaluate robustness against all transfer attacks
- **Design tradeoffs:**
  - Model diversity vs training cost: More source models increase robustness but computational cost; diminishing returns beyond one per group
  - Clean accuracy vs adversarial robustness: PUBDEF prioritizes minimal clean accuracy drop, accepting slightly lower worst-case robustness
  - Heuristic vs optimal source selection: Simple grouping heuristic works well but may miss optimal combinations; exhaustive search intractable
- **Failure signatures:**
  - Low adversarial accuracy: Poor source model selection or ineffective attack generation
  - Large clean accuracy drop: Over-regularization or overly aggressive adversarial training
  - Poor generalization to unseen attacks: Higher-dimensional attack subspace than expected or failed heuristic
- **First 3 experiments:**
  1. Sanity check: Train PUBDEF with default settings and verify clean and adversarial accuracy match reported values
  2. Ablation on source models: Remove one source model group at a time and measure impact on adversarial accuracy
  3. Generalization test: Train PUBDEF on subset of attacks and evaluate on unseen attacks to confirm low-dimensional subspace hypothesis

## Open Questions the Paper Calls Out

- **Question:** What specific architectural or training characteristics of source models most contribute to transfer attack success?
- **Question:** Does low-dimensional structure of transfer adversarial examples extend to other threat models beyond TAPM?
- **Question:** What is theoretical basis for effectiveness of "one model per group" heuristic in source model selection?

## Limitations
- Claims about low-dimensional adversarial subspaces rely on empirical observations rather than theoretical guarantees
- Effectiveness depends on specific composition of public model zoo - future models spanning more diverse subspaces could degrade PUBDEF's performance
- Game-theoretic framing provides conceptual motivation but actual implementation uses heuristic source selection rather than solving full game

## Confidence

- **High confidence**: Clean accuracy preservation (directly measured and reported)
- **Medium confidence**: Effectiveness against unseen transfer attacks (strong empirical support but limited theoretical justification)
- **Low confidence**: Claims about low-dimensional adversarial subspaces (observational evidence only, no formal analysis)

## Next Checks
1. **Generalization test**: Train PUBDEF on subset of attacks and evaluate on held-out attacks from same model zoo to verify low-dimensional subspace hypothesis
2. **Model zoo sensitivity**: Systematically vary composition of source models to identify which groups are most critical for robustness
3. **Attack diversity stress test**: Generate transfer attacks from models with different architectures/distributions to test if PUBDEF's defense generalizes beyond original model zoo