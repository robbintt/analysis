---
ver: rpa2
title: 'FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability
  through 5W Question-Answering'
arxiv_id: '2306.05523'
source_url: https://arxiv.org/abs/2306.05523
tags:
- image
- claim
- arxiv
- news
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FACTIFY 3M, the largest dataset and benchmark
  for multimodal fact verification with explainability through 5W question-answering.
  It addresses the growing challenge of disinformation by providing a comprehensive
  dataset of 3 million samples containing textual claims, paraphrased claims, associated
  images, generated images, pixel-level image heatmaps, 5W QA pairs, and adversarial
  fake news stories.
---

# FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering

## Quick Facts
- arXiv ID: 2306.05523
- Source URL: https://arxiv.org/abs/2306.05523
- Reference count: 40
- Key outcome: Multimodal fact verification outperforms text-only models and adversarial attacks degrade performance

## Executive Summary
FACTIFY3M introduces the largest dataset and benchmark for multimodal fact verification with explainability through 5W question-answering. The dataset contains 3 million samples with textual claims, paraphrased claims, associated images, generated images, pixel-level heatmaps, 5W QA pairs, and adversarial fake news stories. The paper addresses the growing challenge of disinformation by providing a comprehensive benchmark that enables research into multimodal fact verification systems that can explain their reasoning through structured 5W questions.

## Method Summary
The method involves automatic claim paraphrasing using GPT-3, visual paraphrasing using Stable Diffusion, 5W semantic role labeling for QA pair generation, and adversarial attack injection using OPT. The core approach generates multimodal fact verification samples by pairing textual claims with relevant images and their generated counterparts, then creating 5W question-answer pairs through semantic role labeling. The dataset includes various entailment classes and is designed to test both multimodal reasoning and explainability capabilities of fact verification systems.

## Key Results
- Multimodal models outperform text-only models, demonstrating the value of visual modality in fact verification
- Adversarial attacks significantly degrade model performance, highlighting robustness gaps in current systems
- The dataset provides comprehensive coverage with 3 million samples including real and generated multimodal content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fact verification outperforms text-only models because images provide additional, complementary signals
- Mechanism: The model integrates visual embeddings from CLIP and textual embeddings from MPNet to jointly reason about the consistency between claim, document, and their associated images
- Core assumption: Visual and textual modalities carry independent and non-redundant information relevant to the claim's veracity
- Evidence anchors: Experiments show multimodal model outperforms text-only models; multimodal model shows distinct improvement in performance

### Mechanism 2
- Claim: 5W semantic role labeling enables explainable fact verification by decomposing claims into verifiable atomic facts
- Mechanism: SRL identifies verb-centric semantic roles, which are then mapped to 5W question-answer pairs; each pair is validated independently against evidence
- Core assumption: Complex claims can be broken down into smaller, verifiable assertions that align with journalistic fact-checking practice
- Evidence anchors: Offering explainability through 5W question-answering; provides explanation of why a particular claim is refutable

### Mechanism 3
- Claim: Adversarial fake news injection degrades model performance, highlighting robustness gaps
- Mechanism: OPT-generated fake news documents are paired with refute claims; during evaluation, models must distinguish real from synthetic adversarial evidence
- Core assumption: Real-world fact verification systems must handle synthetically generated fake news that mimics authentic reporting style
- Evidence anchors: Adversarial attacks significantly degrade performance; F1 score post adversarial attack proves injecting adversarial news can confuse fact-checking

## Foundational Learning

- Concept: Semantic Role Labeling (SRL)
  - Why needed here: SRL extracts structured semantic components (who, what, when, where, why) from claims for 5W QA generation
  - Quick check question: Given the sentence "Magic Johnson visited the hospital last month to donate blood," what is the "when" component extracted by SRL?

- Concept: Text-to-Image Generation (Stable Diffusion)
  - Why needed here: Generates visual paraphrases to enrich dataset diversity and mimic real-world multimodal claims
  - Quick check question: Why does Stable Diffusion require latent space diffusion instead of pixel-space diffusion for efficiency?

- Concept: Adversarial Attack in ML
  - Why needed here: Injects synthetic fake news to test model robustness against realistic misinformation
  - Quick check question: What is the expected impact on F1 score when a model is evaluated against adversarial vs. clean data?

## Architecture Onboarding

- Component map: Raw claims → paraphrasing → image synthesis → SRL → 5W QA generation → adversarial injection
- Models: GPT-3 (paraphrasing), Stable Diffusion (images), ProphetNet (5W QG), T5 (5W QA validation), MPNet (text embeddings), CLIP (visual embeddings)
- Evaluation: Multimodal entailment classifier, 5W QA validation, adversarial attack robustness
- Critical path: Paraphrasing → Image synthesis → Multimodal entailment → 5W QA generation → Adversarial injection → Evaluation
- Design tradeoffs:
  - Using automatic paraphrasing speeds dataset creation but may miss nuanced meaning variations
  - Generating images with Stable Diffusion adds realism but risks low semantic alignment with claims
  - 5W QA decomposition aids explainability but depends heavily on SRL accuracy
- Failure signatures:
  - Low 5W QA correctness → SRL mapping errors or QA model misalignment
  - Multimodal model underperforms text-only → Visual embeddings are noisy or irrelevant
  - No performance drop under adversarial attack → Fake news generation lacks realism
- First 3 experiments:
  1. Evaluate paraphrased claim coverage vs. correctness trade-off using BLEU and entailment scores
  2. Measure CLIP-based relevance score between real and SD-generated images for a sample of claims
  3. Test multimodal entailment F1 with and without adversarial documents to quantify robustness drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current methods at detecting AI-generated content in multimodal fact verification scenarios?
- Basis in paper: The paper discusses adversarial attacks using AI-generated content and mentions that current detection systems have only 22% accuracy on such content
- Why unresolved: The paper acknowledges that AI-generated text detection is still in its infancy and current tools are not reliable enough
- What evidence would resolve it: A comprehensive evaluation of multiple AI detection tools on FACTIFY 3M dataset showing their accuracy rates across different types of adversarial attacks

### Open Question 2
- Question: What is the optimal balance between automatically generated and manually curated 5W QA pairs for effective fact verification?
- Basis in paper: The paper mentions limitations of automatic 5W QA generation and suggests manual generation of few thousand abstract QA pairs as future work
- Why unresolved: The paper acknowledges the limitations of automatic generation but doesn't provide specific guidelines on when manual curation is necessary
- What evidence would resolve it: Empirical comparison of fact verification performance using different ratios of automatic vs manual 5W QA pairs across various domains

### Open Question 3
- Question: How does the performance of multimodal fact verification systems degrade under different types of adversarial attacks?
- Basis in paper: The paper introduces adversarial fake news stories and shows performance degradation, but only provides aggregate results
- Why unresolved: The paper only shows overall performance drop but doesn't analyze how different types of adversarial attacks affect performance differently
- What evidence would resolve it: Detailed analysis of model performance degradation across different categories of adversarial attacks (e.g., image manipulation vs text generation vs combination)

## Limitations

- The core methodology relies heavily on automated generation and SRL accuracy, yet validation of the automatic claim paraphrasing pipeline is limited to aggregate statistics without per-claim quality checks
- The paper does not disclose the exact thresholds for entailment classification, making exact replication challenging
- While adversarial attack degradation is reported, the synthetic nature of the fake news may not fully capture real-world misinformation sophistication

## Confidence

- High Confidence: The multimodal model's performance improvement over text-only baselines, as supported by direct experimental comparisons and consistent results across multiple entailment classes
- Medium Confidence: The explainability through 5W QA generation, as the mechanism is sound but depends heavily on SRL accuracy which is not extensively validated
- Low Confidence: The robustness claims against adversarial attacks, as the fake news generation quality and realism are not independently verified

## Next Checks

1. **SRL Accuracy Validation**: Manually annotate 100 claim samples to verify that 5W QA pairs correctly capture the semantic roles, measuring SRL accuracy as a function of claim complexity
2. **Visual Relevance Analysis**: Compute CLIP-based relevance scores between real and SD-generated images for a stratified sample of claims, identifying thresholds where visual noise begins to degrade multimodal performance
3. **Adversarial Realism Benchmark**: Evaluate model performance against human-generated fake news stories to quantify whether OPT-generated adversarial examples overestimate or underestimate real-world robustness challenges