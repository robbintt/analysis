---
ver: rpa2
title: Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative
  Pose Encoding
arxiv_id: '2310.12970'
source_url: https://arxiv.org/abs/2310.12970
tags:
- hptr
- prediction
- motion
- agents
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HPTR, a hierarchical Transformer framework for
  real-time motion prediction in autonomous driving. It introduces KNARPE, a novel
  attention mechanism that allows Transformers to use pairwise-relative polyline representations.
---

# Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding

## Quick Facts
- **arXiv ID**: 2310.12970
- **Source URL**: https://arxiv.org/abs/2310.12970
- **Reference count**: 40
- **Key outcome**: HPTR achieves state-of-the-art performance on Waymo and Argoverse-2 datasets while reducing memory consumption and inference latency by 80% compared to agent-centric methods, enabling real-time prediction for 64 agents at 40 FPS.

## Executive Summary
This paper introduces HPTR, a hierarchical Transformer framework for real-time motion prediction in autonomous driving. HPTR uses a novel pairwise-relative polyline representation with K-nearest neighbor attention and relative pose encoding (KNARPE) to achieve state-of-the-art performance while significantly reducing computational costs. The method represents HD maps, traffic lights, and agent trajectories as polylines with global poses and local attributes, enabling efficient attention computation and asynchronous token updates during online inference.

## Method Summary
HPTR represents all inputs as pairwise-relative polylines and uses KNARPE attention to process them efficiently. The hierarchical architecture separates intra-class and inter-class attention with a lower triangular matrix, enabling asynchronous token updates during online inference. Static map features are reused while only dynamic tokens (agents, traffic lights) are updated. The model predicts multi-modal futures using an anchor-based decoder with mixture of Gaussians, trained with hard assignment and cross entropy loss for confidences, negative log-likelihood for positions, negative cosine loss for yaw angles, and Huber loss for speeds/velocities.

## Key Results
- Achieves state-of-the-art performance on Waymo and Argoverse-2 datasets
- Reduces memory consumption and inference latency by 80% compared to agent-centric methods
- Predicts multi-modal futures for 64 agents in real-time at 40 frames per second

## Why This Works (Mechanism)

### Mechanism 1
HPTR achieves 80% reduction in memory consumption and inference latency by sharing local attributes among agents and reusing static map features during online inference. The pairwise-relative representation enables efficient attention computation through K-nearest neighbor approximation.

### Mechanism 2
The hierarchical architecture with lower triangular attention matrix enables asynchronous token update by separating intra-class and inter-class attention. Static tokens (maps) can be reused while only dynamic tokens (agents, traffic lights) are updated as needed.

### Mechanism 3
KNARPE attention allows effective processing of pairwise-relative polyline representations by limiting attention to K-nearest neighbors and incorporating relative pose encoding to handle positional information without requiring full pairwise attention.

## Foundational Learning

- **Transformer architecture with self-attention mechanism**: HPTR uses Transformer encoders and decoders as the core neural network architecture to process polyline tokens. *Quick check: How does multi-head self-attention help capture relationships between different types of polylines?*

- **Pairwise-relative representation vs agent-centric vs scene-centric**: HPTR uses pairwise-relative representation to balance the accuracy of agent-centric methods with the efficiency of scene-centric methods. *Quick check: What are the key differences between these representations in terms of viewpoint invariance and scalability?*

- **Relative pose encoding (RPE) and its implementation**: RPE is crucial for HPTR to handle the positional information of pairwise-relative polylines. *Quick check: How does sinusoidal positional encoding work for 2D positions and angles in RPE?*

## Architecture Onboarding

- **Component map**: Polyline-level encoder (PointNet with masked max-pooling) → KNARPE attention processing → Intra-class Transformers (block diagonal attention) → Inter-class Transformers (TL→MP, AG→MP/TL) → All-to-all Transformer (full attention) → Anchor-to-all Transformer → Output heads (confidence and trajectory)

- **Critical path**: Input polyline encoding → KNARPE attention processing → Hierarchical Transformer stages → Output prediction generation

- **Design tradeoffs**: K-nearest neighbor approximation vs full attention (accuracy vs efficiency), lower triangular attention vs full attention (asynchronous updates vs some accuracy), fixed anchors vs dynamic generation (simplicity vs diversity)

- **Failure signatures**: Memory overflow when K or number of agents exceeds GPU capacity, slow inference indicating inefficient KNARPE implementation, poor accuracy suggesting insufficient K value or inadequate anchor diversity

- **First 3 experiments**:
  1. Verify KNARPE attention implementation by comparing outputs with standard attention on synthetic data
  2. Test hierarchical attention structure by comparing diagonal, lower triangular, and full attention variants
  3. Measure online inference efficiency by caching static map features and comparing latency with and without caching

## Open Questions the Paper Calls Out

### Open Question 1
Does KNARPE generalize to 3D motion prediction tasks beyond the 2D scenarios studied? The paper states the 2D poses can be extended to 3D space in the future, but provides no experimental validation or theoretical analysis for this extension.

### Open Question 2
How sensitive is HPTR's performance to the choice of distance metric for K-nearest neighbors? The paper uses only L2 distance without exploring topology-aware metrics that might incorporate map structure or semantic information.

### Open Question 3
What is the impact of different anchor-based decoding strategies on prediction diversity and accuracy compared to advanced techniques like goal-conditioning? The paper uses a basic anchor decoder and acknowledges this limits diversity, but doesn't compare against more sophisticated approaches.

## Limitations

- The 80% reduction in memory consumption and inference latency is stated but not empirically validated against specific agent-centric baselines
- KNARPE's K-nearest neighbor approximation may lose critical long-range dependencies in complex traffic scenarios
- The lower triangular attention matrix assumes hierarchical relationships that may not hold universally in all traffic scenarios

## Confidence

**High Confidence**: Overall framework design and pairwise-relative polyline representation are well-justified and address known limitations of existing approaches.

**Medium Confidence**: Reported state-of-the-art performance on Waymo and Argoverse-2 datasets, though lacks detailed ablation studies isolating individual component contributions.

**Low Confidence**: Claimed 80% reduction in memory consumption and inference latency compared to agent-centric methods, which lacks direct empirical evidence or comparison with specific baselines.

## Next Checks

1. **Empirical latency and memory benchmarking**: Implement direct comparison between HPTR and a representative agent-centric baseline (e.g., TNT or LaneGCN) measuring actual memory consumption and inference latency on the same hardware when predicting for 64 agents.

2. **K-value sensitivity analysis**: Conduct systematic experiments varying the K parameter in KNARPE attention (e.g., 8, 16, 32, 64) on both validation and test sets to identify optimal tradeoff between accuracy and inference speed.

3. **Attention matrix structure ablation**: Compare HPTR's lower triangular attention matrix against full attention, block diagonal matrix, and random sparse attention to quantify the contribution of hierarchical asynchronous update capability to overall performance and efficiency.