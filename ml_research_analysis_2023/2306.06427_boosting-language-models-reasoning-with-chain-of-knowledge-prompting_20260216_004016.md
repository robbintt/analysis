---
ver: rpa2
title: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting
arxiv_id: '2306.06427'
source_url: https://arxiv.org/abs/2306.06427
tags:
- answer
- triples
- evidence
- reasoning
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a Chain-of-Knowledge (CoK) prompting method
  to enhance the reasoning capabilities of large language models (LLMs) by explicitly
  generating structured knowledge evidence in the form of triples. CoK consists of
  two components: evidence triples (CoK-ET) that capture the reasoning evidence and
  explanation hints (CoK-EH) that provide textual explanations.'
---

# Boosting Language Models Reasoning with Chain-of-Knowledge Prompting

## Quick Facts
- **arXiv ID**: 2306.06427
- **Source URL**: https://arxiv.org/abs/2306.06427
- **Reference count**: 40
- **Key outcome**: Chain-of-Knowledge (CoK) prompting with structured knowledge triples and F²-Verification outperforms standard Chain-of-Thought (CoT) prompting on commonsense, factual, arithmetic, and symbolic reasoning tasks.

## Executive Summary
This paper introduces Chain-of-Knowledge (CoK) prompting, a method to enhance large language model (LLM) reasoning by generating structured knowledge evidence in the form of triples. CoK consists of evidence triples (CoK-ET) and explanation hints (CoK-EH), along with an F²-Verification strategy to assess the reliability of reasoning chains. The method also incorporates a rethinking process to correct unreliable responses by injecting ground-truth knowledge. Experiments demonstrate that CoK prompting significantly improves reasoning accuracy over standard CoT prompting and other baselines.

## Method Summary
The method involves constructing Chain-of-Knowledge prompts with selected exemplars, generating textual rationales using zero-shot CoT, and retrieving or manually annotating evidence triples. LLMs are then prompted to generate reasoning chains (evidence triples, explanation hints, answers), followed by F²-Verification to assess factuality and faithfulness. For unreliable responses, a rethinking process injects correct knowledge triples to prompt the LLM to regenerate the answer. The approach is evaluated on commonsense, factual, arithmetic, and symbolic reasoning tasks.

## Key Results
- CoK prompting significantly outperforms standard CoT prompting and other baselines on commonsense, factual, arithmetic, and symbolic reasoning tasks.
- The F²-Verification method effectively estimates the reliability of reasoning chains in terms of factuality and faithfulness.
- The rethinking process with knowledge injection further improves accuracy by correcting unreliable responses.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structured knowledge triples reduce reasoning ambiguity compared to free-form text rationales.
- **Mechanism**: LLMs generate reasoning steps as triples (subject, relation, object), which constrain the output space to explicit, fact-checkable claims.
- **Core assumption**: Explicit triples are easier for models to manipulate and for humans to verify than prose explanations.
- **Evidence anchors**:
  - [abstract] states "we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple."
  - [section 3.1] explains CoK-ET is "a list of structure triples can reflect the overall reasoning evidence from the query towards the answer."
  - [corpus] shows related work on "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources."
- **Break condition**: If the KB does not contain relevant triples, or if triple generation produces noisy or irrelevant triples, the benefit diminishes.

### Mechanism 2
- **Claim**: F²-Verification quantifies both factuality and faithfulness of reasoning chains.
- **Mechanism**: Factuality checks match triples against ground-truth KB triples; faithfulness measures similarity between generated explanation and reconstructed explanation from triples+answer.
- **Core assumption**: Both factual accuracy of triples and logical consistency between explanation and answer are needed for reliable reasoning.
- **Evidence anchors**:
  - [abstract] mentions "F²-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
  - [section 3.3] defines factuality as "matching degree between reasoning evidence and ground-truth knowledge" and faithfulness as "consistency degree between reasoning evidence and the textual explanation with the final answer."
  - [corpus] shows "No Need for Explanations: LLMs can implicitly learn from mistakes in-context" which supports the idea of verification-based improvement.
- **Break condition**: If the KB is incomplete or the similarity measure fails to capture semantic differences, verification accuracy drops.

### Mechanism 3
- **Claim**: Iterative rethinking with injected correct knowledge improves unreliable responses.
- **Mechanism**: When reliability score < threshold, wrong triples are replaced with correct KB triples and the model regenerates the answer.
- **Core assumption**: Providing correct knowledge at the point of error guides the model to a correct final answer without overfitting.
- **Evidence anchors**:
  - [abstract] describes "For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink this problem."
  - [section 3.4] Algorithm 1 details injection of correct knowledge triples into the prompt for the next iteration.
  - [corpus] references "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models" aligning with iterative refinement.
- **Break condition**: If the model fails to incorporate injected knowledge or the threshold is too strict, the rethinking loop may not converge.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: CoT is the baseline that CoK improves upon; understanding its limitations is key to grasping CoK’s value.
  - **Quick check question**: What is the main limitation of CoT prompting according to the paper?
    - **Answer**: Generated rationales often contain mistakes, making unfactual and unfaithful reasoning chains.

- **Concept**: Knowledge Graph Embeddings (e.g., TransR)
  - **Why needed here**: Used for implicit verification of triples not found in the KB.
  - **Quick check question**: Which embedding model is used for implicit factuality verification in the paper?
    - **Answer**: TransR.

- **Concept**: Sentence Similarity (e.g., SimCSE)
  - **Why needed here**: Measures faithfulness by comparing the generated explanation to one reconstructed from triples and answer.
  - **Quick check question**: What model is used to compute faithfulness scores?
    - **Answer**: SimCSE.

## Architecture Onboarding

- **Component map**: Exemplar construction with CoK prompts -> LLM inference with CoK prompt -> F²-Verification -> if unreliable -> rethinking loop -> final answer.
- **Critical path**: Prompt construction → LLM generation → F²-Verification → if unreliable → rethinking → final answer.
- **Design tradeoffs**: Structured triples vs. free-form text: triples are verifiable but may be harder to generate; F²-Verification adds overhead but improves reliability; rethinking loop adds latency but can correct errors.
- **Failure signatures**: Low accuracy despite high triple count (noisy triples), verification scores near threshold but unstable (threshold tuning needed), rethinking loop never exits (threshold too high).
- **First 3 experiments**:
  1. Baseline CoT vs. CoK without verification on a small commonsense dataset.
  2. CoK with F²-Verification but no rethinking on arithmetic reasoning tasks.
  3. Full CoK + F²-Verification + rethinking with varying thresholds on StrategyQA.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the reliability threshold θ impact the accuracy of the CoK model across different reasoning tasks?
- **Basis in paper**: [explicit] The paper mentions the rethinking process with different reliability thresholds θ ∈ {0, 0.2, 0.4, 0.6, 0.8, 1.0} and iteration numbers N ∈ {1, 2, 3, 4, 5}.
- **Why unresolved**: The paper only shows the results for three tasks (CSQA, BoolQ, and Last Letter Connection) with varying θ and N. It does not provide a comprehensive analysis of the impact of θ on different reasoning tasks.
- **What evidence would resolve it**: An analysis of the accuracy of the CoK model across different reasoning tasks (e.g., commonsense, factual, arithmetic, and symbolic) with varying reliability thresholds θ.

### Open Question 2
- **Question**: How does the choice of knowledge base (KB) affect the performance of the CoK model?
- **Basis in paper**: [explicit] The paper mentions using a combination of knowledge bases from different domains (e.g., Wiktionary, ConceptNet, Wikidata5M, ATOMIC, GLUCOSE, ASER, and CausalBank) for different tasks.
- **Why unresolved**: The paper does not provide a detailed analysis of the impact of different knowledge bases on the performance of the CoK model. It is unclear which knowledge bases are more effective for specific reasoning tasks.
- **What evidence would resolve it**: An analysis of the performance of the CoK model using different knowledge bases for various reasoning tasks, and an evaluation of the effectiveness of each knowledge base.

### Open Question 3
- **Question**: How does the CoK model perform when applied to other large language models (LLMs) with different sizes and architectures?
- **Basis in paper**: [explicit] The paper mentions using the GPT-3 model (text-davinci-002) with 175B parameters.
- **Why unresolved**: The paper does not explore the performance of the CoK model on other LLMs with different sizes and architectures. It is unclear whether the CoK model can be generalized to other LLMs.
- **What evidence would resolve it**: An evaluation of the CoK model on various LLMs with different sizes and architectures, and a comparison of the performance across these models.

## Limitations

- The method depends on high-quality knowledge bases and accurate triple annotation, with no detailed specification of KB scale or domain coverage.
- The rethinking process requires careful threshold tuning; incorrect thresholds can lead to non-convergence or over-correction.
- Computational overhead from F²-Verification and multiple LLM calls is not quantified, raising concerns about scalability and cost.

## Confidence

- **High confidence**: The structural advantage of using triples over free-form text for verification purposes is well-supported by the explicit methodology and clear experimental comparisons showing improved accuracy over CoT baselines.
- **Medium confidence**: The effectiveness of the rethinking process with knowledge injection is demonstrated but depends heavily on threshold tuning and the assumption that injected knowledge will be properly incorporated.
- **Low confidence**: Claims about scalability and real-world deployment are not supported, as the paper lacks analysis of performance in low-resource knowledge domains or detailed latency measurements.

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the reliability threshold in the rethinking process across different reasoning tasks to identify optimal values and failure modes at extremes.

2. **Knowledge base dependency test**: Run CoK prompting on tasks where the KB is intentionally restricted or perturbed to measure degradation in accuracy and identify the minimum KB quality required for robust performance.

3. **Latency and cost comparison**: Measure end-to-end inference time and API costs for CoK with F²-Verification and rethinking versus standard CoT across multiple model sizes to quantify the practical overhead.