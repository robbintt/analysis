---
ver: rpa2
title: 'A reading survey on adversarial machine learning: Adversarial attacks and
  their understanding'
arxiv_id: '2308.03363'
source_url: https://arxiv.org/abs/2308.03363
tags:
- adversarial
- attacks
- attack
- neural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of adversarial machine
  learning attacks and defenses. It classifies attacks based on attacker's knowledge,
  attack goals, scope, strategy, optimization methods, and perturbation constraints.
---

# A reading survey on adversarial machine learning: Adversarial attacks and their understanding

## Quick Facts
- arXiv ID: 2308.03363
- Source URL: https://arxiv.org/abs/2308.03363
- Reference count: 40
- Primary result: Comprehensive survey classifying adversarial attacks across eleven orthogonal dimensions and discussing defense strategies and limitations

## Executive Summary
This paper provides a comprehensive survey of adversarial machine learning attacks and defenses, classifying attacks based on attacker's knowledge, attack goals, scope, strategy, optimization methods, and perturbation constraints. The survey covers various attack types including white-box, black-box, targeted, untargeted, perceivable, non-perceivable, digital, physical, individual, and universal attacks. It also discusses defense strategies like adversarial training, input transformations, and detection methods. The paper highlights ongoing challenges in developing robust defenses against adversarial attacks and emphasizes the need for adaptive, proactive measures.

## Method Summary
This paper provides a comprehensive survey of existing adversarial attacks and defenses without proposing new methods. It systematically classifies adversarial attacks from eleven different perspectives including knowledge availability, attack goals, perceivability, scope, strategy, optimization methods, and perturbation constraints. The survey reviews various attack types and defensive approaches while highlighting their limitations and suggesting future research directions in the field.

## Key Results
- Comprehensive classification of adversarial attacks across eleven orthogonal dimensions
- Discussion of defense strategies including adversarial training, input transformations, and detection methods
- Identification of key challenges including black-box model nature, theoretical limitations, and need for proactive defenses
- Timeline analysis showing evolution of attack sophistication from 2013-2020

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's classification framework enables systematic understanding of adversarial attacks by organizing them across eleven orthogonal perspectives.
- Mechanism: By categorizing attacks based on knowledge availability (white/grey/black-box), attack goals (targeted/untargeted), perceivability (visible/invisible), and other dimensions, the survey creates a multidimensional taxonomy that reveals relationships and patterns between different attack types.
- Core assumption: Adversarial attacks can be meaningfully decomposed into distinct dimensions that are largely independent of each other.
- Evidence anchors:
  - [abstract] "We classify the adversarial attacks based on the knowledge available to adversarial attacks, goals of adversarial attack, the scope of the adversarial attack, the strategy employed by the adversarial attack, optimisation used by adversarial attack, and constraints on perturbation imposed by adversarial attacks."
  - [section] "There exist diverse types of adversarial attacks; in order to classify these several types, we employ classification of adversarial attacks from eleven different perspectives as shown in Figure 3."

### Mechanism 2
- Claim: Understanding attack classifications helps identify which defenses are effective against specific attack types.
- Mechanism: By mapping attack characteristics to defense requirements, practitioners can select appropriate defensive strategies. For example, physical attacks require different defenses than digital attacks, and white-box attacks require defenses that don't rely on model secrecy.
- Core assumption: There is a predictable relationship between attack classification and defensive effectiveness.
- Evidence anchors:
  - [abstract] "We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks."
  - [section] "Regarding defensive systems, there are many variations of defenses [19]-[26] which are carefully analysed in [29], [30] and many of their shortcomings are documented."

### Mechanism 3
- Claim: The survey's temporal perspective (timeline from 2013-2020) reveals the evolution of attack sophistication and defensive responses.
- Mechanism: By documenting the progression from simple gradient-based attacks to universal perturbations and physical attacks, the survey helps practitioners anticipate future attack vectors and prioritize defensive research.
- Core assumption: Attack sophistication follows a predictable evolutionary path that can be learned from historical patterns.
- Evidence anchors:
  - [section] "Figure 1. Timeline of adversarial attacks and samples in adversarial machine learning, compared to work on the security of deep networks [10]."

## Foundational Learning

- Concept: Gradient-based optimization in high-dimensional spaces
  - Why needed here: Most adversarial attacks rely on gradient-based optimization to find perturbations that fool neural networks
  - Quick check question: What optimization method would you use to find adversarial examples if you had full access to model gradients?

- Concept: Distance metrics and their properties (L0, L1, L2, L-infinity norms)
  - Why needed here: Adversarial attacks often constrain perturbations using different distance metrics, affecting both attack success and perceptibility
  - Quick check question: How does the choice of distance metric affect the perceptibility of adversarial perturbations?

- Concept: Transferability of adversarial examples between models
  - Why needed here: Understanding when adversarial examples created for one model work on others is crucial for black-box attack scenarios
  - Quick check question: Under what conditions are adversarial examples most likely to transfer between different neural network architectures?

## Architecture Onboarding

- Component map: Attack Classification -> Defense Strategy Selection -> Effectiveness Evaluation -> Implementation
- Critical path: Understanding attack classification → Identifying relevant defensive strategies → Evaluating defense effectiveness → Implementing appropriate protections
- Design tradeoffs: Comprehensive classification vs. practical usability; theoretical understanding vs. real-world applicability; academic rigor vs. actionable guidance
- Failure signatures: Incomplete classification leading to overlooked attack types; overemphasis on white-box attacks limiting real-world relevance; failure to connect classification to defensive strategies
- First 3 experiments:
  1. Implement a simple white-box gradient attack and classify it according to all eleven perspectives from the survey
  2. Create a defense against one specific attack type and evaluate its effectiveness against related attack classifications
  3. Compare the transferability of adversarial examples created using different optimization strategies and classification dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between adversarial robustness and the interpretability of neural networks?
- Basis in paper: [explicit] The paper states that "adversarial attacks are entangled with the interpretability of neural networks as results on adversarial samples can hardly be explained."
- Why unresolved: The paper suggests a connection between adversarial attacks and interpretability but does not provide a clear explanation or methodology to explore this relationship further.
- What evidence would resolve it: Empirical studies demonstrating how improvements in adversarial robustness correlate with increased interpretability of neural network decisions, or theoretical frameworks linking these concepts.

### Open Question 2
- Question: How can we develop proactive measures to handle new adversarial attacks?
- Basis in paper: [explicit] The paper mentions that "a lack of proactive framework remains a challenge to handle the new adversarial attacks."
- Why unresolved: Current adversarial defenses are reactive, focusing on existing attacks. The paper highlights the need for proactive measures but does not provide a concrete approach to develop such frameworks.
- What evidence would resolve it: Development and demonstration of a proactive adversarial defense system that can anticipate and mitigate novel attack strategies before they are widely known.

### Open Question 3
- Question: What are the theoretical limitations of neural networks in understanding and defending against adversarial attacks?
- Basis in paper: [explicit] The paper states that "current neural networks are difficult to analyse theoretically due to their complicated non-convex properties" and that "it is hard to describe the solutions to these complicated optimisation problems."
- Why unresolved: The complexity of neural network architectures and the non-convex nature of adversarial attack optimization problems make it challenging to develop a comprehensive theoretical framework for understanding and defending against these attacks.
- What evidence would resolve it: Advancements in theoretical tools or frameworks that can effectively analyze the non-convex properties of neural networks and provide insights into the nature of adversarial attacks and potential defenses.

## Limitations

- The survey's classification framework assumes orthogonal attack dimensions, but practical attacks often combine multiple characteristics, creating potential overlap in classification
- Limited empirical validation of the claimed relationships between attack classifications and defensive effectiveness
- The survey focuses primarily on computer vision tasks, potentially limiting generalizability to other domains like NLP or tabular data

## Confidence

- High confidence: Basic classification framework (white-box vs black-box, targeted vs untargeted attacks)
- Medium confidence: Relationships between attack types and defensive strategies, temporal evolution patterns
- Low confidence: Claims about future research directions and their expected impact on the field

## Next Checks

1. Implement a systematic experiment testing whether defenses effective against one attack classification remain effective against related but differently classified attacks
2. Conduct a meta-analysis of the surveyed papers to quantify the empirical support for claimed defense limitations and attack transferability
3. Extend the classification framework to non-vision domains and validate whether the same orthogonal dimensions apply across different machine learning applications