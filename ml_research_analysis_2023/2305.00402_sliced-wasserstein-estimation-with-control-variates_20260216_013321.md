---
ver: rpa2
title: Sliced Wasserstein Estimation with Control Variates
arxiv_id: '2305.00402'
source_url: https://arxiv.org/abs/2305.00402
tags:
- control
- distance
- wasserstein
- variate
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high variance in Monte Carlo
  estimation of the sliced Wasserstein distance. The core idea is to introduce control
  variates that leverage closed-form Wasserstein-2 distances between Gaussian approximations
  of projected one-dimensional measures.
---

# Sliced Wasserstein Estimation with Control Variates

## Quick Facts
- arXiv ID: 2305.00402
- Source URL: https://arxiv.org/abs/2305.00402
- Reference count: 40
- Primary result: Proposed control variate estimators reduce variance in Monte Carlo estimation of sliced Wasserstein distance while maintaining computational efficiency

## Executive Summary
This paper addresses the problem of high variance in Monte Carlo estimation of the sliced Wasserstein (SW) distance. The authors introduce control variates that leverage closed-form Wasserstein-2 distances between Gaussian approximations of projected one-dimensional measures. Two computationally efficient control variate estimators are proposed - a lower bound and an upper bound of the Wasserstein-2 distance between fitted Gaussians. These estimators maintain the same computational complexity as the conventional estimator while significantly reducing variance. Empirical results demonstrate improved performance in distribution comparison, gradient flows, and deep generative modeling tasks.

## Method Summary
The method involves projecting input measures to one dimension using random directions, fitting Gaussian approximations to these projected measures via KL divergence minimization, and constructing control variates using the closed-form Wasserstein-2 distance between the fitted Gaussians. The control variate estimators combine these bounds with the Monte Carlo estimator to reduce variance. The approach maintains O(Ldn) time complexity where L is the number of projections, d is dimensionality, and n is the number of supports.

## Key Results
- Control variate estimators achieve lower error when comparing distributions over images and point clouds
- Faster convergence observed in gradient flows between point clouds
- Improved deep generative modeling performance with FID score reductions of up to 14% (CIFAR10) and 10% (CelebA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Control variates reduce variance by leveraging closed-form Wasserstein-2 distance between Gaussian approximations
- Core assumption: Gaussian approximations are close enough to projected measures that their Wasserstein-2 distance correlates with true Wasserstein distance
- Evidence: Paper shows KL divergence minimization for fitting Gaussians and uses their Wasserstein-2 distance as control variates
- Break condition: Poor Gaussian approximations for highly non-Gaussian distributions weaken correlation and reduce variance reduction effect

### Mechanism 2
- Claim: Control variate estimators maintain same computational complexity as conventional estimator
- Core assumption: Additional computations for fitting Gaussians and computing control variates are negligible
- Evidence: Complexity analysis shows O(Ldn) for both projection and Gaussian fitting
- Break condition: Very large L values make control variate computations significant, though still same order complexity

### Mechanism 3
- Claim: Control variate estimators improve downstream task performance
- Core assumption: Variance reduction in SW distance estimation translates to better downstream performance
- Evidence: Empirical results show improved FID scores and convergence in gradient flows
- Break condition: Downstream tasks insensitive to SW distance variance may not benefit significantly

## Foundational Learning

- Concept: Kullback-Leibler divergence
  - Why needed: Used to fit Gaussian approximations to projected one-dimensional measures for control variate construction
  - Quick check: What is the formula for KL divergence between distributions p and q?

- Concept: Monte Carlo integration
  - Why needed: Method used to estimate expectation in sliced Wasserstein distance
  - Quick check: How does Monte Carlo estimator variance change with number of samples?

- Concept: Control variates
  - Why needed: Technique to reduce variance of Monte Carlo estimator for sliced Wasserstein distance
  - Quick check: What condition must hold for a control variate to reduce estimator variance?

## Architecture Onboarding

- Component map: Input measures -> Random projections -> Gaussian fitting -> Control variate computation -> SW distance estimation
- Critical path:
  1. Project input measures to one dimension using random directions
  2. Fit Gaussians to projected measures via KL divergence minimization
  3. Compute control variates using Wasserstein-2 distance between fitted Gaussians
  4. Combine control variates with Monte Carlo estimator to reduce variance
- Design tradeoffs: Gaussian fitting method choice, control variate bounds selection, number of projections L
- Failure signatures: High variance in estimators (poor Gaussian fitting), no downstream performance improvement, excessive computational cost
- First 3 experiments:
  1. Compare variance of control variate estimators vs conventional estimator on synthetic data
  2. Evaluate performance in gradient flow task on toy example
  3. Test in deep generative modeling on small image dataset

## Open Questions the Paper Calls Out

- Question: What is the optimal number of projections (L) balancing computational efficiency and variance reduction for different dimensional data distributions?
  - Basis: Paper tests L up to 10,000 finding diminishing returns
  - Why unresolved: Optimal L depends on dimensionality, distribution complexity, and application requirements
  - Resolution: Experiments varying L across different dimensionalities and complexities with variance/computation metrics

- Question: How do control variate estimators perform on non-Gaussian distributions with significant multimodality or heavy tails?
  - Basis: Paper mentions Gaussian approximations but doesn't test on highly non-Gaussian distributions
  - Why unresolved: Effectiveness may degrade for complex distributions
  - Resolution: Systematic experiments on synthetic multimodal/heavy-tailed distributions and real-world complex datasets

- Question: Can control variate framework extend to other sliced Wasserstein variants beyond uniform projecting directions?
  - Basis: Paper mentions adaptation to distributional sliced Wasserstein but provides no implementations
  - Why unresolved: Practical implementation and performance for alternative slicing distributions unexplored
  - Resolution: Implementations for generalized/hierarchical/spherical sliced Wasserstein with comparative analysis

## Limitations
- Gaussian approximation assumption may break down for highly non-Gaussian distributions
- Computational overhead becomes significant for very large numbers of projections
- Performance improvements in downstream tasks may vary depending on specific task characteristics

## Confidence
- High confidence: Variance reduction mechanism through Gaussian-based control variates is theoretically sound
- Medium confidence: Computational complexity claim holds for practical L values but needs extreme case verification
- Medium confidence: Translation to downstream task improvements is empirically supported but may vary

## Next Checks
1. Test control variate estimators on highly non-Gaussian distributions to evaluate Gaussian approximation robustness and measure variance reduction
2. Perform detailed computational complexity analysis for varying L values, examining when control variate overhead becomes non-negligible
3. Conduct ablation studies on downstream tasks to isolate variance reduction contribution from other performance factors