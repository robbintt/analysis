---
ver: rpa2
title: Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights
  from Cross-Lingual Language Understanding
arxiv_id: '2307.07880'
source_url: https://arxiv.org/abs/2307.07880
tags:
- language
- cross-lingual
- finetuning
- performance
- profit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares prompt-based finetuning with
  vanilla finetuning for cross-lingual language understanding tasks. The authors propose
  the PROFIT pipeline, which reformulates classification tasks into masked language
  modeling prompts using language-specific patterns and verbalizers, enabling direct
  use of multilingual PLMs without architectural changes.
---

# Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding

## Quick Facts
- arXiv ID: 2307.07880
- Source URL: https://arxiv.org/abs/2307.07880
- Authors: 
- Reference count: 40
- Primary result: PROFIT prompt-based finetuning consistently outperforms vanilla finetuning in cross-lingual language understanding across 15 languages

## Executive Summary
This study systematically compares prompt-based finetuning with vanilla finetuning for cross-lingual language understanding tasks. The authors propose PROFIT, a pipeline that reformulates classification tasks into masked language modeling prompts using language-specific patterns and verbalizers. Comprehensive experiments on sentiment analysis, paraphrase identification, and natural language inference across 15 languages demonstrate that PROFIT consistently outperforms vanilla finetuning in full-data settings and shows larger gains in few-shot scenarios. The effectiveness is influenced by language similarity and pretraining data size, with stronger correlations observed when test sets contain more languages.

## Method Summary
PROFIT reformulates classification tasks into masked language modeling prompts by converting inputs into cloze-style questions using task-specific prompt patterns and verbalizers that map class labels to words in the source language vocabulary. The method leverages multilingual pretrained models (mBERT and XLM-R) without architectural changes, applying them directly to the reformulated prompts. The pipeline maintains the original model architecture while providing structured guidance through prompt-based reformulation, enabling zero-shot cross-lingual transfer from English to target languages.

## Key Results
- PROFIT consistently outperforms vanilla finetuning in full-data scenarios, achieving +1.23% accuracy on Amazon Reviews with mBERT
- Prompt-based finetuning exhibits greater advantages in few-shot settings, with performance improvements varying by task type
- Cross-lingual performance is significantly influenced by language similarity and pretraining data size, with stronger correlations when test sets contain more diverse languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based finetuning outperforms vanilla finetuning in full-data scenarios
- Mechanism: Reformulating classification tasks into masked language modeling prompts leverages the prior knowledge encoded in pretraining, utilizing both semantic information from task labels and the MLM pretraining objective
- Core assumption: The pretrained language model has learned representations during pretraining that are beneficial for masked token prediction tasks
- Evidence anchors:
  - [abstract] "Our results reveal the effectiveness and versatility of prompt-based finetuning in cross-lingual language understanding. Our findings indicate that prompt-based finetuning outperforms vanilla finetuning in full-data scenarios"
  - [section] "In prompt-based learning, downstream tasks are reformulated to resemble the types of problems tackled during the PLM's original pretraining by using a textual prompt"
  - [corpus] Weak evidence - corpus shows related work on prompt decomposition but lacks direct comparison in full-data settings
- Break condition: When the task structure does not align well with masked language modeling, or when the pretrained model's knowledge is not relevant to the downstream task

### Mechanism 2
- Claim: Prompt-based finetuning exhibits greater advantages in few-shot scenarios
- Mechanism: With limited training data, prompt-based methods provide more structured guidance by converting inputs into cloze-style questions, reducing the reliance on large amounts of labeled data
- Core assumption: The prompt structure provides sufficient inductive bias to compensate for limited training examples
- Evidence anchors:
  - [abstract] "Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios"
  - [section] "In prompt-based learning, downstream tasks are reformulated to resemble the types of problems tackled during the PLM's original pretraining by using a textual prompt"
  - [corpus] Evidence from corpus shows related work on few-shot learning with prompts but lacks comprehensive comparison across different few-shot settings
- Break condition: When the number of shots becomes sufficiently large, vanilla finetuning can learn task-specific patterns that outweigh the benefits of prompt structure

### Mechanism 3
- Claim: Cross-lingual performance is influenced by language similarity and pretraining data size
- Mechanism: Languages with higher typological similarity to the source language and larger pretraining data sizes benefit more from cross-lingual transfer through shared linguistic features and richer representations
- Core assumption: Multilingual pretrained models learn transferable representations that correlate with linguistic similarity metrics
- Evidence anchors:
  - [abstract] "Additionally, we analyze underlying factors such as language similarity and pretraining data size that impact the cross-lingual performance of prompt-based finetuning"
  - [section] "Based on the obtained language features and experimental results of task performance with PROFIT, we did a correlation analysis"
  - [corpus] Weak evidence - corpus shows related work on language embeddings but lacks direct analysis of pretraining data size effects on prompt-based methods
- Break condition: When the source and target languages are linguistically distant or when pretraining data is insufficient for either language

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: PROFIT reformulates classification tasks into MLM tasks, so understanding MLM is essential for implementing the approach
  - Quick check question: How does the model predict masked tokens during pretraining, and how is this leveraged during finetuning?

- Concept: Cross-lingual transfer learning
  - Why needed here: The study focuses on zero-shot cross-lingual transfer, requiring understanding of how models trained on one language generalize to others
  - Quick check question: What factors influence the success of cross-lingual transfer in multilingual pretrained models?

- Concept: Prompt engineering and verbalizers
  - Why needed here: PROFIT requires designing prompt patterns and verbalizers that map class labels to words in the source language vocabulary
  - Quick check question: How do you design effective prompt patterns and verbalizers for different classification tasks?

## Architecture Onboarding

- Component map: Input → Prompt Pattern → MPLM → Masked Token Probabilities → Verbalizer → Class Prediction
- Critical path: Input → Prompt Pattern → MPLM → Masked Token Probabilities → Verbalizer → Class Prediction
- Design tradeoffs: PROFIT maintains original model architecture (no architectural changes) versus potential gains from specialized cross-lingual architectures
- Failure signatures: Poor performance when prompt patterns are poorly designed, when target languages are linguistically distant from source, or when pretraining data is insufficient
- First 3 experiments:
  1. Implement PROFIT on Amazon Reviews with mBERT using the provided prompt pattern and verbalizer, compare with vanilla finetuning
  2. Test PROFIT in few-shot settings with varying numbers of shots per class, analyze performance trends
  3. Evaluate PROFIT on PAWS-X with different target languages, measure correlation between language similarity and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of prompt-based finetuning vary across different language families in cross-lingual transfer tasks?
- Basis in paper: [explicit] The authors mention analyzing language similarity as an underlying factor affecting cross-lingual performance, noting that the XNLI dataset contains 15 different languages while PAWS-X and Amazon only contain 7 and 6 languages respectively, leading to weaker correlations in the latter tasks.
- Why unresolved: The analysis shows significant correlations between language similarity and performance on XNLI, but the limited language diversity in PAWS-X and Amazon datasets makes it difficult to draw comprehensive conclusions about how prompt-based finetuning effectiveness varies across language families.
- What evidence would resolve it: Comprehensive experiments across a wider range of language families with more diverse test languages would help determine if prompt-based finetuning effectiveness is consistent across different language families or if it varies significantly based on typological characteristics.

### Open Question 2
- Question: Does the optimal number of few-shot training examples (K) for prompt-based finetuning depend on the specific task type and model architecture?
- Basis in paper: [explicit] The authors find that the performance improvement of PROFIT over Vanilla varies with the number of shots, with sentiment analysis showing clearer improvements for smaller numbers of shots while language inference and paraphrase tasks show greater enhancements with larger K values.
- Why unresolved: While the paper identifies task-specific patterns in the relationship between few-shot examples and performance, it doesn't establish whether these patterns are consistent across different model architectures (mBERT vs XLM-R) or if there are optimal K values that maximize prompt-based finetuning effectiveness for each task type.
- What evidence would resolve it: Systematic experiments varying K across multiple task types and model architectures would reveal whether optimal few-shot settings are task-specific, model-specific, or universal.

### Open Question 3
- Question: How does the pretraining data size for target languages interact with language similarity to influence prompt-based finetuning performance?
- Basis in paper: [explicit] The authors find that both the size of pretraining data for target languages and language similarity significantly correlate with cross-lingual performance, with the correlation strength varying between mBERT and XLM-R models.
- Why unresolved: While the paper establishes that both factors independently affect performance, it doesn't explore how these factors interact or whether there are threshold effects where one factor becomes more important than the other depending on the target language characteristics.
- What evidence would resolve it: Detailed analysis of performance across languages with varying combinations of pretraining data size and similarity scores would reveal interaction effects and help identify optimal conditions for prompt-based finetuning.

## Limitations

- The exact prompt patterns and verbalizers used for each task-language combination are not fully specified, making exact reproduction difficult
- The study focuses on only two model architectures (mBERT and XLM-R), limiting generalizability to other multilingual models
- While correlations between language similarity/pretraining data size and performance are established, the causal mechanisms are not fully explored

## Confidence

- **High confidence**: PROFIT consistently outperforms vanilla finetuning in full-data scenarios on Amazon Reviews
- **Medium confidence**: PROFIT shows greater advantages in few-shot scenarios compared to vanilla finetuning
- **Medium confidence**: Language similarity and pretraining data size influence cross-lingual transfer performance

## Next Checks

1. Conduct ablation studies varying prompt patterns and verbalizers to identify optimal designs for different task types
2. Test PROFIT pipeline with additional multilingual models (e.g., mT5, multilingual DeBERTa) to assess architecture dependence
3. Perform deeper analysis of the relationship between language similarity metrics and transfer performance, including more granular linguistic feature comparisons