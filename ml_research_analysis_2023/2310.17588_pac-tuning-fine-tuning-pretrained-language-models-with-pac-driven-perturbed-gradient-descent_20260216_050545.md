---
ver: rpa2
title: PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven Perturbed
  Gradient Descent
arxiv_id: '2310.17588'
source_url: https://arxiv.org/abs/2310.17588
tags:
- training
- pac-tuning
- noise
- fine-tuning
- pac-bayes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PAC-tuning addresses the challenge of fine-tuning large pretrained
  language models (PLMs) for few-shot text classification by directly minimizing the
  PAC-Bayes generalization bound. The method introduces a two-stage approach: first,
  it learns optimal noise variance parameters by minimizing the PAC-Bayes bound; second,
  it performs perturbed gradient descent with the learned noise levels.'
---

# PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent

## Quick Facts
- arXiv ID: 2310.17588
- Source URL: https://arxiv.org/abs/2310.17588
- Reference count: 14
- Outperforms strong baseline methods like data augmentation, LoRA, and prefix-tuning on GLUE benchmark tasks

## Executive Summary
PAC-tuning addresses the challenge of fine-tuning large pretrained language models (PLMs) for few-shot text classification by directly minimizing the PAC-Bayes generalization bound. The method introduces a two-stage approach: first, it learns optimal noise variance parameters by minimizing the PAC-Bayes bound; second, it performs perturbed gradient descent with the learned noise levels. Experiments on 5 GLUE benchmark tasks show that PAC-tuning achieves average accuracy improvements of 4-5% over strong baseline methods, successfully handling the challenges of large model sizes and limited training data.

## Method Summary
PAC-tuning is a two-stage approach for few-shot PLM fine-tuning that directly minimizes the PAC-Bayes generalization bound. Stage 1 learns optimal noise variance parameters by jointly optimizing model parameters and noise levels using the PAC-Bayes objective. Stage 2 applies perturbed gradient descent with the learned noise to fine-tune the model. The method uses different noise levels for pretrained layers and adaptation layers, reflecting their different confidence levels. The PAC-Bayes bound combines training loss with a KL divergence term that regularizes model complexity.

## Key Results
- Achieves average accuracy improvements of 4-5% across 5 GLUE benchmark tasks
- Outperforms data augmentation, LoRA, and prefix-tuning baselines
- Successfully handles large model sizes (BERT-base, GPT-2) with limited training data (100 samples per task)
- Demonstrates that PAC-Bayes training can be effective for PLMs in few-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the PAC-Bayes bound directly optimizes the generalization error instead of just the training loss.
- Mechanism: The PAC-Bayes bound J(θ, Q, P) = L_train + L_PAC combines training loss with a KL divergence term that penalizes overly complex parameter distributions, effectively regularizing the model to avoid overfitting.
- Core assumption: The PAC-Bayes bound is nonvacuous and provides a tight upper bound on true generalization error for large PLMs with few-shot data.
- Evidence anchors: [abstract] "directly minimizes the PAC-Bayes generalization bound to learn proper parameter distribution"; [section 3.2] "minimizing the bound effectively reduces the generalization error"
- Break condition: If KL divergence term dominates objective and prevents fitting, or if bound becomes vacuous due to curse of dimensionality.

### Mechanism 2
- Claim: Two-stage approach learns noise levels reflecting parameter importance and improves generalization.
- Mechanism: Stage 1 learns optimal noise variance parameters ξ and ε by minimizing PAC-Bayes bound, automatically determining parameter importance. Stage 2 uses learned noise in perturbed gradient descent to inject structured noise during training.
- Core assumption: Different noise levels for pretrained layers (θ) and adaptation layers (ω) are necessary due to confidence differences.
- Evidence anchors: [section 3.5] "we are motivated to use different noise levels as well as learning rates for θ and ω"; [section 3.4] "learned noise levels can be used for model interpretation/validation, as they reflect how important each model parameter is"
- Break condition: If Stage 1 fails to converge or learns uniform noise levels across all parameters, reducing structured noise injection effectiveness.

### Mechanism 3
- Claim: Perturbed gradient descent with learned noise helps escape local minima and saddle points.
- Mechanism: Noise injection in PGD implicitly regularizes trace of Hessian matrix, pushing model toward regions with larger flatness in loss landscape, correlating with better generalization.
- Core assumption: Learned noise variance from Stage 1 is appropriate for specific fine-tuning task and model architecture.
- Evidence anchors: [abstract] "modifies the gradient by injecting noise with the variance learned in the first stage into model parameters during training"; [section 3.3] "PGD is shown to effectively help algorithm escape spurious local minima and saddle points"
- Break condition: If noise level is too high causing training loss explosion, or too low providing insufficient regularization.

## Foundational Learning

- Concept: PAC-Bayes bounds and their relationship to generalization error
  - Why needed here: Understanding why minimizing PAC-Bayes bound can directly improve generalization rather than just training performance
  - Quick check question: What is the difference between minimizing training loss and minimizing PAC-Bayes bound, and why does this matter for few-shot learning?

- Concept: KL divergence and its role in regularization
  - Why needed here: Understanding how KL term in PAC-Bayes bound acts as regularizer to prevent overfitting
  - Quick check question: How does KL divergence between posterior and prior distributions control model complexity in PAC-Bayes training?

- Concept: Perturbed gradient descent and its implicit regularization properties
  - Why needed here: Understanding how noise injection can improve optimization landscape exploration and generalization
  - Quick check question: Why does adding noise during gradient descent help avoid poor local minima and saddle points?

## Architecture Onboarding

- Component map:
  - Stage 1: PAC-Bayes bound minimization with noise variance learning
    - Parameters: θ (PLM), ω (classification layer), ξ (noise for θ), ε (noise for ω)
    - Objective: J(D; ξ, ε, θ, ω) = L_train + L_PAC
  - Stage 2: Perturbed gradient descent with fixed learned noise
    - Parameters: θ, ω
    - Objective: L_train only
  - Prior distributions: P_θ = N(θ₀, λI), P_ω = N(ω₀, βI)
  - Posterior distributions: Q_θ = θ + N(0, diag(ξ)), Q_ω = ω + N(0, diag(ε))

- Critical path:
  1. Initialize model parameters and noise variances
  2. Run Stage 1 for T1 epochs to learn optimal noise levels
  3. Validate Stage 1 convergence by checking noise variance changes
  4. Run Stage 2 using learned noise levels with PGD
  5. Evaluate on development set

- Design tradeoffs:
  - Two-stage vs single-stage: Two-stage allows proper noise learning but increases training time
  - Fixed vs adaptive noise: Fixed learned noise is simpler but may not adapt to changing loss landscape
  - Different noise levels for θ vs ω: Reflects confidence difference but requires careful hyperparameter tuning

- Failure signatures:
  - Stage 1 doesn't converge: Noise variance parameters stop changing or oscillate
  - Training loss plateaus early: KL term dominates and prevents fitting
  - Validation performance degrades: Noise level is too high or optimization is unstable
  - No improvement over baselines: PAC-Bayes bound is vacuous or noise learning is ineffective

- First 3 experiments:
  1. Verify Stage 1 noise learning: Run Stage 1 on SST with BERT-base, plot mean noise variance over epochs to confirm it stabilizes
  2. Test noise sensitivity: Run Stage 2 with different noise levels (ξ* ± 10%) to find optimal range
  3. Ablation study: Compare full PAC-tuning vs Stage 1 only, Stage 2 only, and vanilla fine-tuning on CoLA task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PAC-tuning perform when applied to larger language models like GPT-3, GPT-4, or ChatGPT?
- Basis in paper: [explicit] The paper acknowledges that BERT and GPT-2 are relatively small compared to current large language models and recommends further experiments with larger models.
- Why unresolved: The paper only tested PAC-tuning on BERT and GPT-2, which are significantly smaller than modern LLMs.
- What evidence would resolve it: Experimental results showing PAC-tuning performance on GPT-3, GPT-4, or other modern LLMs compared to baseline fine-tuning methods.

### Open Question 2
- Question: Can PAC-tuning be effectively combined with prompt-based methods like ChatGPT and Bard for improved performance?
- Basis in paper: [inferred] The paper suggests comparing PAC-tuning performance with prompt-based techniques but does not conduct such experiments.
- Why unresolved: The paper focuses on parameter-tuning methods and does not explore integration with prompt-based approaches.
- What evidence would resolve it: Experiments comparing PAC-tuning with prompt-based fine-tuning methods on the same tasks and models.

### Open Question 3
- Question: How can the convergence speed of Stage 1 in PAC-tuning be improved without sacrificing performance?
- Basis in paper: [explicit] The paper acknowledges that Stage 1 requires many epochs to learn both the model and noise, and recommends further studies on faster convergence.
- Why unresolved: The paper does not explore optimization techniques for accelerating Stage 1 training.
- What evidence would resolve it: Experiments demonstrating faster convergence of Stage 1 through techniques like learning rate scheduling, adaptive optimizers, or alternative initialization strategies.

## Limitations

- Theoretical assumptions about PAC-Bayes bound tightness may not hold for high-dimensional PLMs with few-shot data, potentially making the bound vacuous.
- Method requires careful tuning of multiple hyperparameters (learning rates, prior variances, noise levels) with no systematic sensitivity analysis provided.
- Stage 1 convergence is critical but not thoroughly analyzed across different tasks, and failure modes are not well characterized.

## Confidence

**High Confidence**: Experimental results showing PAC-tuning outperforms baseline methods (data augmentation, LoRA, prefix-tuning) on GLUE benchmark tasks with average improvements of 4-5%. Methodology is clearly described with sufficient implementation details for reproduction.

**Medium Confidence**: Claim that PAC-tuning handles challenges of large model sizes and limited training data effectively. While results are positive, study only tests on BERT-base and GPT-2 with 100 samples per task, which may not represent full spectrum of few-shot scenarios.

**Low Confidence**: Theoretical claim that minimizing PAC-Bayes bound directly optimizes generalization error for PLMs. Paper provides limited empirical validation that bound is actually tight or informative, and relationship between bound minimization and generalization improvement needs more rigorous analysis.

## Next Checks

1. **Bound Tightness Analysis**: For each task, compute actual generalization gap (train - test error) and compare to PAC-Bayes bound value. This validates whether bound is actually informative rather than vacuous, and whether minimizing it correlates with improved generalization.

2. **Stage 1 Behavior Study**: Run Stage 1 training on SST and CoLA tasks with different initialization schemes and learning rates for noise variances. Plot evolution of mean noise variance parameters over training epochs to verify convergence patterns and identify failure modes.

3. **Ablation on Noise Levels**: Systematically test Stage 2 performance using different noise levels (ξ* ± 20%, ϵ* ± 20%) learned from Stage 1. This validates whether learned noise levels are optimal or if method is sensitive to noise magnitude, and whether different noise levels for θ vs ω are actually beneficial.