---
ver: rpa2
title: 'Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or
  Technical Possibility?'
arxiv_id: '2312.06652'
source_url: https://arxiv.org/abs/2312.06652
tags:
- islamic
- llms
- language
- evaluation
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores building domain-specific Large Language Models
  (LLMs) faithful to the Islamic worldview. It frames the task as a technical problem
  and evaluates multiple approaches: prompt engineering (zero-shot, few-shot, instruction-based),
  Retrieval-Augmented Generation (RAG), and fine-tuning.'
---

# Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?

## Quick Facts
- arXiv ID: 2312.06652
- Source URL: https://arxiv.org/abs/2312.06652
- Reference count: 17
- Key outcome: Few-shot prompting with GPT-4 achieves highest F1 score (0.372) on Islamic Q&A tasks; fine-tuning on Islamic QA dataset further improves performance to F1 0.422

## Executive Summary
This paper investigates the technical feasibility of building domain-specific Large Language Models (LLMs) faithful to the Islamic worldview. The authors frame the challenge as an engineering problem and systematically evaluate three approaches: prompt engineering (zero-shot, few-shot, instruction-based), Retrieval-Augmented Generation (RAG) with Hadith datasets, and fine-tuning on Islamic datasets. Performance is measured on 100 randomly sampled IslamQA questions using BERTScore and embedding distance metrics. The results show that few-shot prompting with GPT-4 achieves the highest F1 score of 0.372, while fine-tuning on Islamic QA datasets further improves performance to F1 0.422. The study highlights the importance of high-quality Islamic datasets, robust evaluation metrics, and interdisciplinary collaboration between machine learning and Islamic scholarship.

## Method Summary
The authors evaluate three technical approaches to building Islamic domain-specific LLMs: prompt engineering techniques (zero-shot, few-shot, instruction-based), Retrieval-Augmented Generation (RAG) using Hadith datasets, and fine-tuning GPT-3.5 Turbo on Islamic datasets. They use 100 randomly sampled questions from IslamQA as the evaluation benchmark, measuring performance with BERTScore and embedding distance (using OpenAI text-embedding-ada-002). The prompt engineering experiments compare GPT-3.5 and GPT-4 across different prompting strategies. The fine-tuning experiments use 400 examples from Islamic QA datasets, and the RAG experiments employ a Hadith dataset of approximately 54,227 hadiths. All approaches are evaluated against reference answers using precision, recall, and F1 scores.

## Key Results
- Few-shot prompting outperforms zero-shot and instruction-based prompting across all models
- GPT-4 achieves the highest F1 score (0.372) in few-shot prompting experiments
- Fine-tuning on Islamic QA dataset alone yields the best performance (F1 0.422) among fine-tuning approaches
- RAG with Hadith datasets does not significantly improve performance due to semantic mismatch issues
- Embedding distance and BERTScore show consistent ranking across different approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting outperforms zero-shot and instruction-based prompting for Islamic domain-specific questions.
- **Mechanism:** Few-shot examples provide the model with explicit patterns of how to structure answers according to Islamic teachings, allowing it to learn the expected format and tone without requiring a full fine-tuning cycle.
- **Core assumption:** The quality and representativeness of the few-shot examples are sufficient to guide the model toward accurate, faith-consistent responses.
- **Evidence anchors:**
  - Table 1 shows that few-shot prompting consistently achieves the highest F1-Score (0.352 for GPT-3.5, 0.372 for GPT-4) and the lowest embedding distances across all models.
  - "Few-shot prompting yields the best results, with GPT-4 achieving the highest F1 score (0.372)."
- **Break Condition:** If the few-shot examples are not representative or contain subtle biases, the model may learn incorrect patterns or overfit to non-generalizable examples.

### Mechanism 2
- **Claim:** Fine-tuning on Islamic QA datasets improves model performance over prompting alone.
- **Mechanism:** Fine-tuning adapts the model's internal representations to the specific vocabulary, style, and theological nuances of Islamic discourse, making it more fluent and accurate in generating answers rooted in Islamic texts.
- **Core assumption:** The fine-tuning dataset is sufficiently large and diverse to cover the breadth of Islamic knowledge and the nuances across different schools of thought.
- **Evidence anchors:**
  - Table 3 shows that fine-tuning on Islamic QA alone achieves the highest F1 score (0.422) compared to fine-tuning on Hadith datasets or mixed datasets.
  - "Fine-tuning on Islamic datasets further improves performance, with the highest F1 of 0.422 when fine-tuning on Islamic QA alone."
- **Break Condition:** If the fine-tuning dataset is too small, biased, or lacks coverage of key Islamic topics, the improvements may be marginal or introduce new errors.

### Mechanism 3
- **Claim:** Retrieval-Augmented Generation (RAG) with Hadith datasets does not significantly improve performance due to semantic mismatch.
- **Mechanism:** RAG relies on retrieving relevant passages to augment the model's knowledge. If the embedding similarity does not align well with the semantic relevance needed for Islamic Q&A, the retrieved context may not improve answer quality.
- **Core assumption:** The embedding model (OpenAI text-embedding-ada-002) captures the semantic relationships relevant to Islamic texts and questions.
- **Evidence anchors:**
  - Table 2 shows that the RAG system with Hadith dataset did not yield significant improvement over few-shot prompting alone.
  - "RAG with Hadith datasets did not yield significant gains."
- **Break Condition:** If a more semantically aligned embedding model or better chunking strategy is used, RAG could become more effective.

## Foundational Learning

- **Concept:** Prompt Engineering Techniques (zero-shot, few-shot, instruction-based)
  - **Why needed here:** Different prompting strategies have varying impacts on model performance in domain-specific tasks, and selecting the right one is critical for initial experimentation.
  - **Quick check question:** What is the main difference between zero-shot and few-shot prompting in terms of model guidance?

- **Concept:** Evaluation Metrics for Semantic Similarity (BERTScore, embedding distance)
  - **Why needed here:** Accurate evaluation of model outputs against expert-generated answers requires metrics that capture semantic equivalence, not just lexical overlap.
  - **Quick check question:** Why might embedding distance be used alongside BERTScore in evaluating Islamic Q&A systems?

- **Concept:** Retrieval-Augmented Generation (RAG) Pipeline
  - **Why needed here:** RAG can potentially ground model responses in authoritative Islamic texts, reducing hallucination and increasing faithfulness to source material.
  - **Quick check question:** What are the key steps in a RAG pipeline from indexing to generation?

## Architecture Onboarding

- **Component map:** Data Ingestion → Embedding Generation → Vector Store → Retrieval Engine → Prompt Engineering Layer → LLM → Evaluation Metrics → Fine-tuning module (optional) → Guardrails module (optional)
- **Critical path:** Prompt Engineering → LLM Inference → Evaluation (BERTScore + embedding distance)
- **Design tradeoffs:**
  - Prompt engineering vs. fine-tuning: Prompting is faster and cheaper but may be less accurate; fine-tuning is more accurate but requires more resources and data.
  - RAG vs. prompting: RAG can ground answers in source texts but may not improve performance if semantic retrieval is weak.
  - Model size vs. latency: Larger models (GPT-4) perform better but are slower and more expensive than smaller models (GPT-3.5).
- **Failure signatures:**
  - Low precision/recall despite high embedding similarity: Possible semantic drift in embeddings or misalignment between prompt style and evaluation criteria.
  - Guardrails blocking valid responses: Overly strict validators or mismatched RAIL specifications.
  - RAG not improving performance: Poor chunking, irrelevant retrieval, or embedding model not capturing Islamic semantics.
- **First 3 experiments:**
  1. Compare zero-shot, few-shot, and instruction-based prompting on a held-out set of IslamQA questions using BERTScore and embedding distance.
  2. Fine-tune a base model on Islamic QA data and evaluate on the same benchmark to measure improvement over prompting.
  3. Implement a basic RAG pipeline with Hadith dataset and evaluate whether retrieval improves answer faithfulness compared to prompting alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific high-quality Islamic datasets would be most effective for fine-tuning LLMs to achieve better performance in answering religious questions?
- Basis in paper: explicit
- Why unresolved: The paper mentions the need for high-quality Islamic datasets but does not specify which datasets would be most effective or provide detailed comparisons of different dataset types.
- What evidence would resolve it: Systematic evaluation of different Islamic datasets (Quran, Hadith, Tafsir, Islamic QA) on model performance metrics, with ablation studies showing which dataset combinations yield the best results.

### Open Question 2
- Question: How can we develop evaluation metrics that better capture the nuanced requirements of Islamic scholarship, such as theological accuracy and contextual appropriateness?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that current metrics like BERTScore and embedding distance have limitations in capturing attributes like relevance, fluency, consistency, and coherence in religious contexts.
- What evidence would resolve it: Development and validation of domain-specific evaluation metrics that incorporate Islamic scholarly principles, with comparative studies showing improved correlation with expert human judgments.

### Open Question 3
- Question: What is the optimal balance between general knowledge and domain-specific fine-tuning for Islamic LLMs to maintain both broad utility and religious accuracy?
- Basis in paper: explicit
- Why unresolved: The paper explores fine-tuning on Islamic datasets but doesn't investigate the trade-offs between maintaining general capabilities versus domain-specific performance.
- What evidence would resolve it: Comparative studies measuring performance on both Islamic and general tasks across different fine-tuning strategies, identifying the sweet spot for optimal performance in both domains.

### Open Question 4
- Question: How can RAG systems be optimized to handle the complex structure and context-dependent nature of Islamic texts, particularly the Quran's sequential vs. revelation order?
- Basis in paper: explicit
- Why unresolved: The paper mentions challenges with Quran structure and varying hadith authenticity but doesn't explore solutions for optimizing RAG retrieval in these contexts.
- What evidence would resolve it: Development of specialized retrieval algorithms or data preprocessing techniques that account for Islamic text structures, with empirical validation showing improved retrieval accuracy for religious queries.

## Limitations

- Evaluation framework relies on BERTScore and embedding distance metrics, which may not fully capture nuanced theological accuracy required for Islamic Q&A
- Study focuses only on GPT models (GPT-3.5 and GPT-4), limiting generalizability to other LLM architectures or open-source alternatives
- Fine-tuning experiments used only 400 examples, which may be insufficient for comprehensive domain adaptation
- 100-question evaluation set may not be representative of the full breadth of Islamic discourse or account for sectarian variations

## Confidence

**High Confidence**: The comparative performance ranking of prompting strategies (few-shot > instruction-based > zero-shot) and the superiority of GPT-4 over GPT-3.5 are well-supported by the presented results. The observation that fine-tuning on Islamic QA alone outperforms mixed datasets is also robust.

**Medium Confidence**: The claim that RAG with Hadith datasets did not yield significant improvements is supported by the data, but the semantic mismatch explanation requires further validation. The effectiveness of few-shot prompting may be sensitive to the quality and representativeness of the examples used.

**Low Confidence**: The generalizability of these findings to other Islamic subdomains (e.g., jurisprudence, theology, history) or to models beyond the GPT family remains uncertain. The long-term stability of fine-tuned models on evolving Islamic discourse is unknown.

## Next Checks

1. **Expand Evaluation Set**: Test the prompting and fine-tuning approaches on a larger, more diverse set of Islamic questions (e.g., 500+ questions spanning multiple Islamic disciplines) to assess robustness and generalizability.

2. **Cross-Model Validation**: Evaluate the same approaches using open-source models (e.g., LLaMA, Falcon) fine-tuned on Islamic datasets to determine if the performance trends hold across different architectures.

3. **RAG Semantic Alignment Study**: Experiment with alternative embedding models (e.g., contrastive models trained on Islamic texts) and chunk sizes to determine if RAG performance can be improved through better semantic alignment.