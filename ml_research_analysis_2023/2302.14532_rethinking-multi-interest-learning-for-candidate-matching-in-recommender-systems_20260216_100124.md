---
ver: rpa2
title: Rethinking Multi-Interest Learning for Candidate Matching in Recommender Systems
arxiv_id: '2302.14532'
source_url: https://arxiv.org/abs/2302.14532
tags:
- multi-interest
- training
- negative
- sampling
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work revisits the training framework of multi-interest candidate
  matching in recommender systems and uncovers two major problems: 1) the uniform
  sampled softmax fails to effectively train discriminative representations due to
  the severe increase in easy negative samples; 2) routing collapse, where each interest
  representation only contains information from a single item. To address these issues,
  the authors propose REMI, a general framework that consists of an Interest-aware
  Hard Negative mining strategy (IHN) and a Routing Regularization (RR) method.'
---

# Rethinking Multi-Interest Learning for Candidate Matching in Recommender Systems

## Quick Facts
- arXiv ID: 2302.14532
- Source URL: https://arxiv.org/abs/2302.14532
- Reference count: 40
- Primary result: REMI improves multi-interest candidate matching by addressing easy negative sampling and routing collapse with interest-aware hard negative mining and routing regularization

## Executive Summary
This work addresses two fundamental problems in multi-interest candidate matching for recommender systems: the failure of uniform sampled softmax to train discriminative representations due to easy negative samples, and routing collapse where interest representations become overly specialized. The authors propose REMI, a framework that combines Interest-aware Hard Negative mining (IHN) with Monte-Carlo importance sampling and Routing Regularization (RR) using variance regularization. Experiments on three real-world datasets demonstrate significant improvements over state-of-the-art methods with minimal computational overhead.

## Method Summary
REMI is a general framework for multi-interest candidate matching that consists of two components: IHN and RR. IHN uses Monte-Carlo importance sampling to approximate an ideal hard negative sampling distribution that assigns higher probabilities to items with larger scores related to the current interest representation. RR introduces a variance regularizer on item-to-interest routing matrices to prevent routing collapse by maintaining diversity in routing weights. The framework is implemented on top of the ComiRec-SA architecture and requires minimal computational overhead while providing substantial performance improvements.

## Key Results
- REMI achieves 38.35% to 65.96% improvement in Hit Rate@50 compared to state-of-the-art methods
- IHN effectively addresses the easy negative problem by prioritizing interest-aware hard negatives
- RR successfully prevents routing collapse, maintaining diverse item-to-interest routing distributions
- The combined framework enhances multi-interest representations from both optimization and composition perspectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interest-aware hard negative mining (IHN) solves the easy negative problem by prioritizing hard negatives related to the currently selected interest concept
- Mechanism: IHN uses Monte-Carlo importance sampling to approximate an ideal distribution that assigns higher sampling probabilities to items with larger scores related to the current interest representation
- Core assumption: Items with higher inner product scores with the selected interest representation are more informative as negative samples for training that specific interest
- Evidence anchors:
  - [abstract] "IHN emphasizes interest-aware hard negatives by proposing an ideal sampling distribution and developing a Monte-Carlo strategy for efficient approximation"
  - [section 4.1] "We propose a negative sampling distribution qβ, which is subjective to the chosen interest concept v_u, and assign the hard negative items with larger scores higher probabilities of being sampled"
  - [corpus] Weak - corpus neighbors focus on different aspects of multi-interest learning rather than negative sampling strategies
- Break condition: If the concentration parameter β is set too high, the method may focus excessively on the hardest samples, potentially including false negatives and harming training stability

### Mechanism 2
- Claim: Routing regularization prevents routing collapse by maintaining diversity in item-to-interest routing weights
- Mechanism: The variance regularizer on the routing weights ensures each interest representation is composed of multiple relevant items rather than collapsing to a single item
- Core assumption: Routing collapse occurs when attention weights become sparse, causing each interest to focus on only one or few items
- Evidence anchors:
  - [abstract] "RR prevents routing collapse by introducing a novel regularization term on the item-to-interest routing matrices"
  - [section 4.2] "we introduce the variance regularizer on the routing weights to eliminate sparsity and effectively address the problem"
  - [section 5.5] "We also visualize the item-to-interest routing for state-of-the-art UMI [3] and PIMI [4] in Figure 4, with and without REMI. Some random samples show that REMI effectively avoids routing collapse"
- Break condition: If the regularization weight λ is too high, it may force routing weights to be uniform rather than capturing the true relevance structure

### Mechanism 3
- Claim: The combination of IHN and RR enhances multi-interest representations from both optimization and composition perspectives
- Mechanism: IHN improves the training objective by providing more informative hard negatives, while RR improves the composition information by preventing routing collapse
- Core assumption: Multi-interest representations can be enhanced by addressing both the quality of negative samples during training and the diversity of items contributing to each interest
- Evidence anchors:
  - [abstract] "These two components enhance the learned multi-interest representations from both the optimization objective and the composition information"
  - [section 5.6.1] "Combined, they boost the performance with a 38.35% to 65.96% improvement in Hit Rate@50 on the three datasets"
  - [corpus] Weak - corpus neighbors discuss various multi-interest approaches but don't specifically address the combined effect of hard negative mining and routing regularization
- Break condition: If either component is missing, the full potential of multi-interest representation learning is not realized, as shown in ablation studies

## Foundational Learning

- Concept: Monte-Carlo importance sampling
  - Why needed here: To approximate the ideal hard negative sampling distribution without computing relevance scores for all items in the corpus
  - Quick check question: How does Monte-Carlo importance sampling reduce computational complexity compared to direct sampling from the ideal distribution?

- Concept: Routing collapse in attention mechanisms
  - Why needed here: To understand why multi-head attention-based routing can fail and lead to information loss in interest representations
  - Quick check question: What happens to the attention weights in routing collapse, and why does this harm the quality of interest representations?

- Concept: Sampled softmax and negative sampling in large corpora
  - Why needed here: To understand why uniform sampling fails in multi-interest scenarios and how hard negative mining addresses this issue
  - Quick check question: Why does the selected-interest-focused training scheme in multi-interest learning make uniform sampling particularly ineffective?

## Architecture Onboarding

- Component map:
  - User behavior sequence → Item representation (embedding + optional enhancement) → Multi-interest routing (attention/capsule) → Interest selection (argmax) → Sampled softmax training with IHN → Routing regularization
  - IHN modifies the loss computation during training; RR adds a regularization term to the routing matrix

- Critical path: User sequence → Interest representations → Interest selection → Negative sampling → Loss computation → Backpropagation
  - IHN affects the loss computation step; RR affects the routing computation step

- Design tradeoffs:
  - IHN: Higher β values increase training effectiveness but risk focusing on false negatives; lower values are safer but less effective
  - RR: Higher λ values prevent routing collapse better but may over-regularize and reduce the model's ability to learn true relevance patterns

- Failure signatures:
  - IHN too aggressive: Training instability, poor convergence, or degraded performance if β is too high
  - RR too strong: Uniform routing weights that fail to capture item-interest relationships, or degraded performance if λ is too high
  - Both components missing: Performance similar to baseline multi-interest models with no improvements

- First 3 experiments:
  1. Compare REMI with and without IHN on a small dataset to verify the easy negative problem is solved
  2. Compare REMI with and without RR to verify routing collapse is prevented
  3. Sweep the concentration parameter β to find the optimal value for your specific dataset characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REMI vary with different dataset characteristics (e.g., sparsity, item diversity, sequence length)?
- Basis in paper: [inferred] The paper mentions that the optimal concentration parameter β differs across datasets, suggesting dataset characteristics influence performance.
- Why unresolved: The paper only tests three datasets with varying properties but does not provide a systematic analysis of how specific characteristics affect REMI's performance.
- What evidence would resolve it: Controlled experiments varying dataset characteristics (e.g., artificially sparsifying data, altering item diversity, changing sequence lengths) while measuring REMI's performance compared to baselines.

### Open Question 2
- Question: What is the impact of REMI on the ranking stage of recommender systems, and can it be effectively integrated with ranking models?
- Basis in paper: [explicit] The paper focuses on candidate matching but mentions that multi-interest learning is also used in the ranking stage in related work (e.g., DMIN, DemiNet, MGNM).
- Why unresolved: The paper only evaluates REMI in the candidate matching stage and does not explore its potential benefits or integration challenges in the ranking stage.
- What evidence would resolve it: Experiments applying REMI to ranking models and comparing performance with and without REMI integration in the ranking stage.

### Open Question 3
- Question: How does REMI's routing regularization compare to other regularization techniques (e.g., dropout, L2 regularization) in preventing routing collapse and improving performance?
- Basis in paper: [explicit] The paper compares routing regularization to Re4 and SINE but does not compare it to other common regularization techniques.
- Why unresolved: The paper only compares routing regularization to a limited set of methods, leaving the question of its relative effectiveness compared to other regularization techniques unanswered.
- What evidence would resolve it: Experiments comparing routing regularization to other regularization techniques (e.g., dropout, L2 regularization) in terms of preventing routing collapse and improving multi-interest model performance.

## Limitations

- The effectiveness of IHN depends critically on the concentration parameter β, which requires careful tuning and may vary across datasets
- The paper doesn't investigate scenarios where multi-interest representations might be unnecessary or even detrimental
- Evaluation focuses on top-K metrics without examining recommendation diversity or how well interests capture different user needs
- The theoretical explanation for routing collapse is incomplete, focusing on symptoms rather than root causes

## Confidence

- **High Confidence**: The identification of the easy negative problem in uniform sampling for multi-interest learning, supported by theoretical analysis and experimental evidence
- **Medium Confidence**: The effectiveness of routing regularization in preventing collapse, based on visualization evidence and performance improvements
- **Medium Confidence**: The combined framework's superiority over state-of-the-art methods, though ablation studies could be more comprehensive

## Next Checks

1. Conduct a systematic ablation study varying the concentration parameter β across multiple orders of magnitude to identify the optimal range and test the robustness of IHN
2. Implement a visualization analysis comparing the distribution of routing weights with and without regularization across different training epochs to verify that routing collapse is indeed prevented throughout training
3. Design an experiment comparing REMI's performance against a single-interest baseline to quantify the actual benefit of multi-interest representations versus just the improved training methodology