---
ver: rpa2
title: Context-aware Adversarial Attack on Named Entity Recognition
arxiv_id: '2309.08999'
source_url: https://arxiv.org/abs/2309.08999
tags:
- adversarial
- words
- methods
- candidate
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial attacks on Named Entity Recognition
  (NER) systems. It proposes context-aware methods that perturb the most informative
  words for recognizing entities, using linguistic features like POS tags, dependency
  parsing, chunking, and gradient attribution.
---

# Context-aware Adversarial Attack on Named Entity Recognition

## Quick Facts
- arXiv ID: 2309.08999
- Source URL: https://arxiv.org/abs/2309.08999
- Reference count: 13
- Key outcome: Context-aware methods that perturb informative words reduce NER model performance by ~20% F1 while maintaining high textual similarity

## Executive Summary
This paper introduces context-aware adversarial attack methods for Named Entity Recognition systems that target the most informative words for entity recognition. The approach uses linguistic features (POS tags, dependency parsing, chunking) and gradient attribution to identify critical words, then replaces them using synonym or masked language model strategies. Experiments on three datasets demonstrate that these attacks effectively reduce model performance while maintaining textual coherence, outperforming random attack baselines.

## Method Summary
The method involves three key stages: first, extracting linguistic features (POS, dependency, chunking) from input text; second, selecting informative words using either linguistic patterns or gradient-based importance scores; third, replacing selected words with either synonyms (from WordNet) or MLM-predicted candidates (using RoBERTa-base). The adversarial examples are evaluated by measuring both textual similarity (using Universal Sentence Encoder) and performance degradation (F1 score drop) on a BERT-base NER model fine-tuned on each dataset.

## Key Results
- Gradient-based word selection outperforms linguistic feature-based selection
- Synonym replacement achieves better textual similarity than MLM replacement
- Both methods reduce NER F1 scores by 10-20% on average across datasets
- Context-aware attacks are significantly more effective than random word selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing informative words causes NER model to misclassify entities
- Mechanism: Words that provide high context for entity recognition have strong gradient signals. Modifying these words disrupts the model's entity boundary inference
- Core assumption: NER models rely heavily on contextual words rather than just entity tokens for type prediction
- Evidence anchors: [abstract] "Specifically, we propose perturbing the most informative words for recognizing entities", [section] "Some words in a sentence are more informative than others in guiding the model to recognize named entities"
- Break condition: If model relies primarily on entity tokens themselves rather than surrounding context

### Mechanism 2
- Claim: Synonym replacement preserves semantic coherence while breaking model predictions
- Mechanism: Replacing words with synonyms maintains human-readable meaning but changes input distribution the model was trained on
- Core assumption: Models trained on specific word distributions fail when encountering semantically equivalent but distributionally different inputs
- Evidence anchors: [abstract] "we investigate different candidate replacement methods to generate natural and plausible adversarial examples", [section] "Using synonyms to replace candidate words as adversarial samples can guarantee the preservation of text semantics"
- Break condition: If model uses semantic understanding beyond surface word patterns

### Mechanism 3
- Claim: Gradient-based word selection identifies most impactful perturbations
- Mechanism: Integrated Hessians calculate feature importance by measuring gradient interactions between non-entity words and entity instances
- Core assumption: Words with high gradient importance scores have disproportionate impact on model outputs
- Evidence anchors: [section] "we use Integrated Hessians (Janizek et al., 2021) to determine the importance of non-entity words based on their feature interactions with entity instances"
- Break condition: If model predictions are distributed across many features rather than concentrated on few key words

## Foundational Learning

- Concept: Named Entity Recognition (NER) task structure
  - Why needed here: Understanding how NER models process context vs entity tokens is fundamental to designing effective attacks
  - Quick check question: What are the typical entity types recognized in NER (e.g., Person, Organization, Location)?

- Concept: Gradient-based interpretability methods
  - Why needed here: The gradient selection method relies on understanding how gradients can identify feature importance in neural networks
  - Quick check question: What does it mean when a word has a high gradient magnitude with respect to model loss?

- Concept: Masked Language Model (MLM) mechanics
  - Why needed here: MLM replacement method requires understanding how models predict masked tokens based on context
  - Quick check question: How does a masked language model determine which words to predict when given a masked input?

## Architecture Onboarding

- Component map: Input text → Linguistic feature extraction (POS, dependency, chunking) → Candidate selection module → Replacement strategy (synonym/MLM) → Adversarial example generation → NER model evaluation
- Critical path: Candidate selection → Replacement → Evaluation
- Design tradeoffs:
  - Synonym replacement: higher similarity, lower attack effectiveness
  - MLM replacement: lower similarity, higher attack effectiveness
  - Random selection: simple but less targeted
  - Gradient selection: computationally expensive but most effective
- Failure signatures:
  - Low textual similarity indicates overly aggressive perturbations
  - Minimal performance decrease suggests ineffective word selection or replacement
  - High computational cost may indicate inefficient gradient calculation
- First 3 experiments:
  1. Baseline: Run victim model on original CoNLL03 test set to establish F1 score
  2. Simple attack: Apply random word selection with synonym replacement, measure F1 drop and similarity
  3. Targeted attack: Apply gradient-based selection with MLM replacement, compare against baseline and simple attack metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed adversarial attack methods perform against more robust NER models like RoBERTa or ALBERT compared to BERTbase?
- Basis in paper: [explicit] The paper uses BERTbase as the victim model and shows performance degradation under adversarial attacks. However, it does not compare results against other more robust PLMs.
- Why unresolved: The study only evaluates attacks on BERTbase without exploring whether more robust models are less susceptible to these attacks.
- What evidence would resolve it: Testing the same attack methods on multiple victim models (RoBERTa, ALBERT, etc.) and comparing performance degradation across models would show if model architecture affects vulnerability.

### Open Question 2
- Question: How do the proposed methods generalize to multilingual NER datasets or low-resource languages where linguistic tools may be limited?
- Basis in paper: [inferred] The limitations section mentions that the methods require linguistic knowledge and tools that may need extension for other languages, especially minority languages.
- Why unresolved: The experiments are conducted only on English datasets, and the paper acknowledges potential limitations for multilingual applications.
- What evidence would resolve it: Applying the attack methods to multilingual NER datasets or simulating low-resource conditions by limiting available linguistic tools would demonstrate generalizability.

### Open Question 3
- Question: What is the relationship between the number of perturbed words and the rate of performance degradation, and is there an optimal perturbation threshold?
- Basis in paper: [explicit] The paper shows that perturbing five words leads to 10-20% F1 drops, but does not systematically explore different perturbation levels or identify an optimal threshold.
- Why unresolved: The experiments use a fixed number of perturbed words (five) without exploring how performance degrades as perturbation increases.
- What evidence would resolve it: Conducting experiments with varying numbers of perturbed words (e.g., 1, 2, 3, 5, 10) and measuring the corresponding performance degradation would reveal the relationship and potential optimal threshold.

## Limitations
- The evaluation relies on Universal Sentence Encoder for textual similarity, which may not fully capture semantic preservation for domain-specific entities
- Gradient-based selection (Integrated Hessians) is computationally expensive and may not scale well to larger models
- The study focuses only on BERT-base as the victim model, limiting generalizability to more advanced architectures

## Confidence

**High Confidence:** The core finding that context-aware word selection improves attack effectiveness over random selection is well-supported by experimental results across three datasets. The observation that gradient-based selection outperforms linguistic features has clear empirical backing.

**Medium Confidence:** The claim that perturbing informative words disrupts entity boundary inference is mechanistically plausible but lacks direct causal evidence. While experiments show correlation between perturbation and performance drop, the exact mechanism remains partially speculative.

**Low Confidence:** The paper's assertion that synonym replacement guarantees semantic preservation while breaking model predictions is problematic. Some synonyms may carry different connotations or domain-specific meanings that affect entity recognition beyond simple distributional shifts.

## Next Checks
1. **Transferability Test:** Evaluate whether adversarial examples generated against BERT-base successfully transfer to other NER architectures (e.g., RoBERTa, DistilBERT) to assess attack robustness beyond the specific victim model.

2. **Human Evaluation:** Conduct human studies to verify that synonym and MLM replacements maintain natural language perception, particularly for domain-specific entities that may have specialized meanings.

3. **Defense Mechanism Analysis:** Test whether standard defense techniques like adversarial training or input denoising can mitigate the proposed attacks, which would validate whether the attacks exploit fundamental model weaknesses or specific implementation details.