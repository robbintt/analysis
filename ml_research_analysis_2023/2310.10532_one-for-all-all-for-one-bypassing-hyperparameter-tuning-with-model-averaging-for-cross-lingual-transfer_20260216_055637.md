---
ver: rpa2
title: 'One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging
  For Cross-Lingual Transfer'
arxiv_id: '2310.10532'
source_url: https://arxiv.org/abs/2310.10532
tags:
- zs-xlt
- performance
- averaging
- validation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model selection based on source-language validation data often
  selects checkpoints with suboptimal zero-shot cross-lingual transfer (ZS-XLT) performance.
  The paper proposes accumulative run-by-run averaging of models trained with different
  hyperparameters as an alternative evaluation protocol that improves ZS-XLT performance
  without requiring target-language validation data.
---

# One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2310.10532
- Source URL: https://arxiv.org/abs/2310.10532
- Reference count: 40
- Primary result: Model averaging outperforms conventional model selection based on source-language validation for zero-shot cross-lingual transfer

## Executive Summary
This paper addresses the challenge of zero-shot cross-lingual transfer (ZS-XLT) where model selection based on source-language validation data often leads to suboptimal performance on target languages. The authors propose an accumulative run-by-run averaging approach that combines models trained with different hyperparameters, eliminating the need for target-language validation data. Across three diverse tasks (NLI, extractive QA, and NER) and multiple languages, this method consistently improves ZS-XLT performance and closely matches the "oracle" performance that would be achieved with target-language validation.

## Method Summary
The proposed method involves fine-tuning XLM-R large with multiple hyperparameter configurations (21 combinations of learning rates and batch sizes, each with 3 random seeds) on source language data. Instead of selecting the single best model based on source-language validation performance, the method accumulatively averages snapshots from different runs into a single model. This averaging is performed iteratively across all hyperparameter configurations, creating a final model that leverages the diversity of different training runs. The approach is evaluated on NLI (XNLI/IndicXNLI), extractive QA (TyDiQA-GoldP), and NER (WikiANN, MasakhaNER, MasakhaNER 2.0) tasks.

## Key Results
- Accumulative averaging consistently outperforms conventional model selection based on source-language validation across all tasks and languages
- The method closely tracks "oracle" ZS-XLT performance selected using target-language validation data
- Averaging reduces performance variance compared to selecting the best source-language validation model
- Results hold across diverse languages and tasks with varying training set sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulative averaging smooths out the noise and instability inherent in individual fine-tuning runs.
- Mechanism: Each training run with different hyperparameters explores a different region of the parameter space. By averaging models across runs, we get a "middle ground" that tends to perform well on both source and target languages.
- Core assumption: The "optimal" model for ZS-XLT is not at the extreme end of any hyperparameter configuration but rather somewhere in between the best-performing runs.
- Evidence anchors:
  - [abstract]: "We find that conventional model selection based on source-language validation quickly plateaus to suboptimal ZS-XLT performance..."
  - [section 4]: "With more runs, source-language validation may even prefer models that are worse at ZS-XLT on TyDiQA and NER."
- Break condition: If all hyperparameter configurations consistently produce models with poor ZS-XLT performance, averaging them will not help.

### Mechanism 2
- Claim: Model averaging leverages the diversity of different hyperparameter configurations to improve generalization.
- Mechanism: Different hyperparameter settings (learning rates, batch sizes) can lead to models that generalize differently. Averaging combines these strengths, compensating for individual weaknesses.
- Core assumption: There is no single hyperparameter configuration that is optimal for both source-language validation and ZS-XLT.
- Evidence anchors:
  - [abstract]: "... we propose to accumulatively average snapshots from different runs into a single model."
  - [section 2]: "The key idea is to accumulatively average snapshots of runs with different hyperparameters: this improves performance over model selection based on source-language validation performance."
- Break condition: If the hyperparameter space is too small or if all configurations lead to similar performance, averaging may not provide additional benefits.

### Mechanism 3
- Claim: Accumulative averaging is more robust and replicable than model selection based on source-language validation.
- Mechanism: By averaging models, we reduce the variance introduced by random seeds and specific hyperparameter choices, leading to more stable ZS-XLT performance.
- Core assumption: The variance in ZS-XLT performance across runs is significant enough that averaging can reduce it.
- Evidence anchors:
  - [abstract]: "... our accumulative run-by-run averaging of models trained with different hyperparameters boosts ZS-XLT performance and closely correlates with 'oracle' ZS-XLT..."
  - [section 4]: "Accumulative averaging stabilizes ZS-XLT and reduces performance variance vis-a-vis Max. SRC -DEV counterparts."
- Break condition: If the variance is minimal or if averaging introduces bias, the robustness advantage may not hold.

## Foundational Learning

- Concept: Zero-shot cross-lingual transfer (ZS-XLT)
  - Why needed here: The paper's main focus is improving ZS-XLT performance without target-language validation data.
  - Quick check question: What is the difference between ZS-XLT and few-shot cross-lingual transfer?

- Concept: Model averaging
  - Why needed here: The proposed solution relies on averaging models trained with different hyperparameters to improve ZS-XLT.
  - Quick check question: How does model averaging differ from ensemble methods?

- Concept: Hyperparameter tuning
  - Why needed here: The paper argues that conventional hyperparameter tuning based on source-language validation is suboptimal for ZS-XLT.
  - Quick check question: What are the common hyperparameters tuned in fine-tuning multilingual language models?

## Architecture Onboarding

- Component map:
  Data pipeline -> Training loop (21 configs Ã— 3 seeds) -> Model averaging -> Evaluation

- Critical path:
  1. Fine-tune model with configuration (l, b) and save checkpoints
  2. Repeat for all hyperparameter configurations and random seeds
  3. Accumulatively average checkpoints from different runs
  4. Evaluate averaged model on target-language test sets

- Design tradeoffs:
  - Tradeoff between exploration (trying different hyperparameters) and exploitation (focusing on the best configuration)
  - Tradeoff between model averaging (which may smooth out good features) and selecting the best model

- Failure signatures:
  - ZS-XLT performance does not improve with more runs
  - Averaging models leads to worse performance than the best individual model
  - High variance in ZS-XLT performance across runs

- First 3 experiments:
  1. Fine-tune with learning rate 1e-5 and batch size 32; save checkpoints every 10% of training steps
  2. Fine-tune with learning rate 2e-5 and batch size 64; save checkpoints every 10% of training steps
  3. Fine-tune with learning rate 5e-5 and batch size 16; save checkpoints every 10% of training steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does accumulative model averaging perform on tasks with significantly larger or smaller training sets than the ones tested?
- Basis in paper: [inferred] The paper mentions that TyDiQA's small training set (3,696 instances) affects the performance of CA, suggesting training set size may impact averaging effectiveness.
- Why unresolved: The paper only tests on three specific tasks with varying but limited training set sizes, not exploring a broader range.
- What evidence would resolve it: Experiments comparing accumulative averaging across tasks with training sets spanning several orders of magnitude (e.g., 100 to 100,000+ instances).

### Open Question 2
- Question: Does the optimal learning rate and batch size configuration for accumulative averaging differ from the optimal configuration for single model selection?
- Basis in paper: [explicit] The paper states "our full results in Table 3 indicate that for each task we obtain maximal (oracle) ZS-XLT performance with a different, task-specific hyperparameter configuration."
- Why unresolved: The paper tests a wide grid but doesn't specifically analyze whether the best hyperparameters for averaging differ systematically from those for single model selection.
- What evidence would resolve it: A detailed analysis comparing optimal hyperparameter configurations between accumulative averaging and single model selection across all tasks and languages tested.

### Open Question 3
- Question: How does accumulative averaging performance change when using more than 10 runs?
- Basis in paper: [explicit] The paper conducts experiments with up to 10 runs and notes consistent improvement, but doesn't explore beyond this limit.
- Why unresolved: The paper stops at 10 runs without investigating whether performance continues to improve or plateaus.
- What evidence would resolve it: Experiments extending to 20, 50, or 100 runs to determine if there's a point of diminishing returns or continued improvement.

## Limitations
- The assumption that target-language validation data is unavailable may not hold in all practical deployment scenarios
- The hyperparameter grid of 21 configurations may not capture the full diversity of models that could benefit from averaging
- The paper doesn't explicitly address checkpoint alignment requirements for effective averaging across all tasks

## Confidence

**High Confidence**:
- The core empirical finding that source-language validation plateaus at suboptimal ZS-XLT performance is well-supported by the presented results across multiple tasks and languages
- The accumulative averaging method consistently improves ZS-XLT performance compared to conventional model selection based on source-language validation
- The claim that accumulative averaging closely tracks "oracle" performance (selected with target-language validation) is empirically validated

**Medium Confidence**:
- The mechanism explaining why averaging works (smoothing noise, leveraging diversity) is plausible but not directly proven
- The claim that accumulative averaging is more robust and replicable than model selection is supported by variance reduction but could benefit from more systematic analysis

**Low Confidence**:
- The generalizability of the findings to other tasks beyond NLI, QA, and NER, or to other multilingual models beyond XLM-R large

## Next Checks

1. **Vary the hyperparameter grid size and composition**: Test whether the averaging benefits persist when using a larger or differently structured hyperparameter grid (e.g., including different optimizers, dropout rates, or layer-wise learning rates). This would validate whether the observed improvements are robust to the specific hyperparameter space explored.

2. **Analyze checkpoint alignment requirements**: Systematically test whether proper alignment of task heads across runs is critical for effective averaging by comparing aligned vs. unaligned averaging approaches across all three tasks.

3. **Test on additional tasks and models**: Validate the method on other cross-lingual tasks (e.g., text classification, semantic parsing) and with different multilingual models (e.g., mT5, mBERT) to assess generalizability beyond the specific experimental setup.