---
ver: rpa2
title: Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration
arxiv_id: '2311.08152'
source_url: https://arxiv.org/abs/2311.08152
tags:
- books
- year
- sold
- agent
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a multi-agent peer review collaboration strategy
  that emulates the academic peer review process to enhance complex reasoning in large
  language models. Each agent independently constructs a solution, provides reviews
  on others' solutions with confidence scores, and revises its initial solution based
  on peer feedback.
---

# Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration

## Quick Facts
- arXiv ID: 2311.08152
- Source URL: https://arxiv.org/abs/2311.08152
- Reference count: 10
- Primary result: Multi-agent peer review collaboration improves complex reasoning in LLMs across mathematical, commonsense, and symbolic reasoning tasks

## Executive Summary
This paper proposes a novel multi-agent peer review collaboration strategy that emulates academic peer review to enhance complex reasoning in large language models. The approach involves three agents independently constructing solutions, providing reviews with confidence scores, and revising their initial solutions based on peer feedback. Extensive experiments on ten benchmark datasets demonstrate consistent improvements over single-agent and basic multi-agent baselines, with confidence scores proving particularly effective for mathematical reasoning tasks.

## Method Summary
The method implements a three-stage iterative process: creation, review, and revision. Three agents independently generate solutions using chain-of-thought reasoning, then exchange peer reviews with confidence scores (1-10) quantifying feedback reliability. Each agent refines their solution based on received feedback, and the final answer is determined through majority voting. The approach is evaluated across ten datasets spanning mathematical (GSM8K, SVAMP, AQuA, MultiArith, AddSub, SingleEq), commonsense (ARC-c, StrategyQA), and symbolic reasoning (Colored Objects, Penguins) tasks.

## Key Results
- Consistently outperforms single-agent and multi-agent baselines across all ten datasets
- Confidence scores significantly improve mathematical reasoning performance
- Cross-critique mechanism enables agents to achieve accurate consensus through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent peer review collaboration improves complex reasoning by leveraging diverse solution generation and mutual feedback
- Mechanism: Each agent independently constructs solutions, reviews peers' solutions with confidence scores, and revises based on feedback, emulating academic peer review
- Core assumption: Different agents can identify unique errors in each other's solutions that the original agent cannot see
- Evidence anchors:
  - [abstract] "Each agent independently constructs its own solution, provides reviews on the solutions of others, and assigns confidence levels to its reviews. Upon receiving peer reviews, agents revise their initial solutions."
  - [section] "Huang et al. (2023) further note that a single LLM struggles to self-correct its response without external feedback"
- Break condition: If agents lack diversity in their initial approaches or if confidence scores are not reliable indicators of review quality

### Mechanism 2
- Claim: Integrating confidence scores in reviews enhances mathematical reasoning performance by providing reliability metrics for feedback
- Mechanism: Agents attach confidence levels (1-10) to their reviews, allowing recipients to weigh feedback appropriately
- Core assumption: Agents can accurately assess their own confidence in identifying errors
- Evidence anchors:
  - [abstract] "The integration of confidence scores in reviews proves effective for mathematical reasoning"
  - [section] "Mirroring such real-life review practices, the agents not only examine the reasoning process step by step but also attach a confidence score to quantify the reliability of their feedback"
- Break condition: If agents systematically over/under-estimate their confidence or if confidence scores don't correlate with actual review accuracy

### Mechanism 3
- Claim: Cross-critique among multiple agents leads to accurate consensus through iterative refinement
- Mechanism: Agents exchange feedback on each other's solutions, allowing each to refine their approach based on multiple perspectives
- Core assumption: The majority vote among agents will converge toward the correct answer through iterative improvement
- Evidence anchors:
  - [abstract] "Extensive experiments on ten datasets across mathematical, commonsense, and symbolic reasoning tasks show that the proposed method consistently outperforms existing single-agent and multi-agent baselines"
  - [section] "In the example shown in Figure 1, our collaboration strategy enables multiple agents to engage in mutual review and correction, leading to an accurate consensus"
- Break condition: If initial agent solutions are too far from correct or if feedback loops reinforce incorrect approaches

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Forms the basis for agents' initial solution construction
  - Quick check question: Can you explain how chain-of-thought prompting differs from direct answer generation?

- Concept: Multi-agent collaboration dynamics
  - Why needed here: Understanding how agents interact and influence each other's outputs
  - Quick check question: What are the key differences between majority voting and debate-based multi-agent approaches?

- Concept: Confidence calibration
  - Why needed here: Agents must accurately express uncertainty in their reviews
  - Quick check question: How would you test whether an LLM's confidence scores are well-calibrated?

## Architecture Onboarding

- Component map: Agent Creation -> Review Module -> Revision Module -> Voting Module
- Critical path:
  1. Question input → Agent Creation (3 agents)
  2. Each agent's solution → Review Module (reviews from other 2 agents)
  3. All reviews → Revision Module (each agent revises)
  4. Revised solutions → Voting Module (majority vote)

- Design tradeoffs:
  - Agent number vs. computational cost: More agents improve diversity but increase cost
  - Review depth vs. performance: More review rounds don't necessarily improve accuracy
  - Confidence scoring vs. simplicity: Confidence scores help but add complexity

- Failure signatures:
  - Low improvement after revision: Indicates agents may not be providing useful feedback
  - Inconsistent majority vote: Suggests agent solutions are too diverse or all incorrect
  - Confidence scores don't correlate with review accuracy: Indicates poor calibration

- First 3 experiments:
  1. Single-agent self-correction baseline to establish performance floor
  2. Multi-agent without confidence scores to measure confidence impact
  3. Varying agent numbers (2, 3, 4) to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of confidence scores in multi-agent peer review vary across different types of reasoning tasks (mathematical, commonsense, symbolic)?
- Basis in paper: [explicit] The authors note that "incorporating confidence assessment can result in unchanged or even reduced performance" for commonsense and symbolic reasoning tasks compared to mathematical reasoning.
- Why unresolved: The paper provides evidence of decreased performance when using confidence scores for non-mathematical tasks, but doesn't fully explore the underlying reasons or potential task-specific adaptations of the confidence mechanism.
- What evidence would resolve it: Detailed analysis of confidence score accuracy and impact across task types, potentially leading to task-specific confidence elicitation strategies.

### Open Question 2
- Question: What is the optimal balance between the number of agents and review rounds for maximizing reasoning performance?
- Basis in paper: [explicit] The authors observe that "an upward trend is observed in the relationship between the accuracy and the agent number" up to 4 agents, but "no significant improvement trend is detected with more review rounds."
- Why unresolved: While the paper provides initial observations on agent count and review rounds, it doesn't determine the optimal configuration or explain the underlying reasons for the observed patterns.
- What evidence would resolve it: Systematic experiments varying both agent numbers and review rounds across different reasoning tasks to identify optimal configurations and their task-specific variations.

### Open Question 3
- Question: How does the multi-agent peer review approach scale to more complex reasoning tasks or longer problem-solving chains?
- Basis in paper: [inferred] The experiments focus on relatively short, well-defined reasoning tasks. The paper doesn't address how the approach might perform on more complex, open-ended problems or tasks requiring extended reasoning chains.
- Why unresolved: The current implementation and evaluation are limited to specific task types and problem lengths. Scaling to more complex scenarios would require addressing potential issues with context length, agent coordination, and maintaining coherent multi-step reasoning.
- What evidence would resolve it: Experiments applying the method to more complex reasoning tasks, including those requiring multiple interconnected reasoning steps or domain-specific knowledge, along with analysis of performance degradation or improvements at scale.

## Limitations
- The paper doesn't provide detailed prompt templates or implementation specifics for baseline methods
- Effectiveness of confidence scores varies significantly across reasoning task types
- Scalability to more complex reasoning tasks and longer problem-solving chains remains unexplored

## Confidence

- **High confidence**: The experimental results showing improved accuracy across ten datasets compared to single-agent and basic multi-agent baselines
- **Medium confidence**: The mechanism by which confidence scores improve mathematical reasoning, as the paper shows effectiveness but doesn't deeply explore why certain types of reasoning benefit more from confidence-weighted feedback
- **Medium confidence**: The generalizability of the approach to other domains beyond the tested mathematical, commonsense, and symbolic reasoning tasks

## Next Checks

1. **Cross-task validation**: Test the multi-agent peer review approach on additional reasoning tasks (e.g., logical reasoning, spatial reasoning) not included in the original ten datasets to assess generalizability and identify potential domain-specific limitations.

2. **Confidence calibration analysis**: Conduct a detailed analysis of how well the confidence scores correlate with actual review accuracy across different reasoning types, including statistical measures of calibration and potential systematic biases in confidence estimation.

3. **Scalability assessment**: Systematically vary the number of agents (2, 3, 4, 5+) and review iterations to identify optimal configurations and determine whether the approach scales effectively with increased computational resources and agent diversity.