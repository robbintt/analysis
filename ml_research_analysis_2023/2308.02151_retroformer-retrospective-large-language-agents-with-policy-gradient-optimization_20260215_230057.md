---
ver: rpa2
title: 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization'
arxiv_id: '2308.02151'
source_url: https://arxiv.org/abs/2308.02151
tags:
- language
- agent
- agents
- retrospective
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Retroformer, a framework for improving large
  language agents through policy gradient optimization of a retrospective model that
  refines actor prompts using environmental feedback. The approach learns to assign
  credit and generate actionable insights from failed attempts, enabling faster learning
  and better task completion.
---

# Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization

## Quick Facts
- arXiv ID: 2308.02151
- Source URL: https://arxiv.org/abs/2308.02151
- Reference count: 8
- This paper introduces Retroformer, a framework for improving large language agents through policy gradient optimization of a retrospective model that refines actor prompts using environmental feedback.

## Executive Summary
Retroformer introduces a novel approach to improving large language agents by learning a retrospective model that generates self-reflections to refine actor prompts based on environmental feedback. The framework uses policy gradient optimization to train the retrospective model offline, treating the frozen actor LLM as part of the environment. This approach achieves 53% success rate on HotPotQA validation tasks within 5 trials, outperforming baselines like Reflexion (50%) and ReAct. The method avoids direct LLM fine-tuning, making it suitable for cloud-based models while generating more precise self-reflections and structured action plans.

## Method Summary
Retroformer employs a two-LLM architecture where a frozen actor LLM generates actions based on refined prompts, while a local retrospective LLM produces self-reflections to improve those prompts. The framework uses policy gradient optimization (PPO) to train the retrospective model offline using differences in episode returns as reward signals. A replay buffer stores instruction-response pairs and returns across tasks, enabling the retrospective model to learn general patterns of effective reflection. The approach generates structured reflection responses with clear sections for analysis and planning, improving comprehension and actionability compared to unstructured feedback.

## Key Results
- Achieves 53% success rate on HotPotQA validation tasks within 5 trials
- Outperforms Reflexion (50%) and ReAct baselines by generating more precise self-reflections
- Generates structured reflection responses (Reflection + New Plan sections) that improve actor comprehension
- Learns effective credit assignment patterns across multiple tasks via replay buffer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy gradient optimization of the retrospective model improves actor performance by refining prompts with actionable feedback.
- Mechanism: The retrospective model generates self-reflections that are rated based on their impact on subsequent trial rewards. Positive ratings (increased returns) reinforce the retrospective model's behavior via PPO, leading to better credit assignment and actionable insights.
- Core assumption: The difference in episode returns between consecutive trials can serve as a reliable reward signal for improving the retrospective model.
- Evidence anchors:
  - [abstract] "Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment."
  - [section 6.2] "We use the differences of episode returns as the ratings of the generated reflection responses."
  - [corpus] Found 25 related papers. Average neighbor FMR=0.442. Weak direct evidence of retrospective model policy gradient optimization.
- Break condition: If the environment provides sparse or noisy rewards, the retrospective model may learn to generate misleading reflections that do not improve actor performance.

### Mechanism 2
- Claim: Structured reflection responses (Reflection + New Plan sections) improve comprehension and actionability compared to unstructured feedback.
- Mechanism: The reinforced retrospective model learns to format responses with clear sections for analysis and planning, making it easier for the actor to understand and act on the feedback.
- Core assumption: Clear formatting of reflections improves the actor's ability to process and apply the feedback.
- Evidence anchors:
  - [section 7.4] "We discover one emergent behavior of the reinforced model that it can automatically paraphrase the original responses into two separate structured sections, namely Reflection section and New plan: section..."
  - [corpus] No direct evidence of structured reflection formatting in related work.
- Break condition: If the actor model cannot effectively parse the structured format, the formatting benefit may be lost.

### Mechanism 3
- Claim: The retrospective model's credit assignment improves by learning from multiple tasks and environments via the replay buffer.
- Mechanism: The replay buffer stores instruction-response pairs and returns across tasks, allowing the retrospective model to learn general patterns of effective reflection rather than task-specific heuristics.
- Core assumption: Credit assignment patterns are transferable across tasks and environments.
- Evidence anchors:
  - [section 6.1] "The long and short-term memory components provide context that is specific to a given task over several failed trials and the replay buffer provides demonstrations of successful reflections across the tasks and environments..."
  - [corpus] Weak evidence of multi-task learning in retrospective models in related work.
- Break condition: If tasks are too diverse, the retrospective model may learn conflicting patterns that reduce performance on specific tasks.

## Foundational Learning

- Concept: Reinforcement learning with policy gradient methods
  - Why needed here: The core mechanism relies on policy gradient optimization to improve the retrospective model based on environmental rewards.
  - Quick check question: What is the difference between on-policy and off-policy reinforcement learning, and which approach does Retroformer use?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The actor model is a frozen LLM that relies on carefully crafted prompts to perform tasks, and the retrospective model must generate effective prompt refinements.
  - Quick check question: How does in-context learning differ from fine-tuning in large language models?

- Concept: Credit assignment in multi-step reasoning
  - Why needed here: The retrospective model must identify which actions in a multi-step trial led to failure to generate effective feedback.
  - Quick check question: What are the challenges of credit assignment in environments with delayed rewards?

## Architecture Onboarding

- Component map:
  Actor LLM (Ma) -> Environment -> Retrospective LLM (Mr) -> Replay Buffer -> PPO Trainer -> Retrospective LLM (Mr)

- Critical path:
  1. Actor interacts with environment and receives reward
  2. Retrospective model generates reflection based on trajectory and reward
  3. Reflection is rated by comparing returns across consecutive trials
  4. Ratings are stored in replay buffer
  5. PPO trainer updates retrospective model offline

- Design tradeoffs:
  - Using a frozen actor LLM avoids expensive fine-tuning but limits direct optimization
  - Offline training of the retrospective model reduces real-time computational overhead
  - Structured reflections improve comprehension but may limit flexibility in feedback generation

- Failure signatures:
  - Actor performance plateaus despite improved retrospective reflections
  - Reflections become repetitive or generic across different tasks
  - Offline training fails to generalize to new environments

- First 3 experiments:
  1. Compare success rates of Retroformer vs. baseline ReAct and Reflexion agents on HotPotQA validation set
  2. Analyze the quality of reflections generated by frozen vs. reinforced retrospective models on failed trials
  3. Evaluate the impact of structured vs. unstructured reflection formats on actor performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Retroformer scale with the size and complexity of the language model used for the actor component?
- Basis in paper: Inferred
- Why unresolved: The paper mentions using GPT-3 (text-davinci-003) as the frozen actor model, but does not explore how performance changes with different model sizes or architectures. It's unclear if larger models would provide better performance or if there's a point of diminishing returns.
- What evidence would resolve it: Systematic experiments comparing Retroformer performance using different actor model sizes and architectures (e.g., GPT-3 variants, GPT-4, Claude) while keeping the retrospective model constant.

### Open Question 2
- Question: What is the impact of varying the temperature setting for the actor model during inference on Retroformer's overall performance and learning dynamics?
- Basis in paper: Explicit - The paper mentions setting temperature to zero to isolate randomness effects, but acknowledges this may not reflect real-world usage where exploration is beneficial.
- Why unresolved: The paper explicitly states that higher temperature could encourage exploration but makes comparisons difficult. The trade-off between exploration (higher temperature) and the impact of reflections (lower temperature) remains unexplored.
- What evidence would resolve it: Comparative experiments varying temperature settings across trials and environments, measuring both task completion rates and learning efficiency.

### Open Question 3
- Question: How does Retroformer perform on tasks requiring long-term planning and memory beyond what can fit in the actor's prompt window?
- Basis in paper: Inferred - The paper mentions prompt length limitations as a challenge for LLM-based agents and that Retroformer avoids this by using the retrospective model, but doesn't test scenarios where even the retrospective model's output might exceed practical limits.
- Why unresolved: The paper demonstrates effectiveness on HotPotQA tasks but doesn't explore scenarios requiring extensive state tracking or very long action sequences where even the refined prompt might become too long.
- What evidence would resolve it: Experiments on tasks requiring 10+ reasoning steps with complex dependencies, or tasks with dynamic state spaces that grow substantially over time.

### Open Question 4
- Question: What is the minimum dataset size required for effective offline RL training of the retrospective model, and how does performance degrade with smaller datasets?
- Basis in paper: Inferred - The paper mentions collecting 3,383 reflection samples but doesn't explore dataset size sensitivity or minimum requirements for effective learning.
- Why unresolved: While the paper demonstrates effectiveness with their collected dataset, it doesn't establish dataset requirements or explore performance on smaller datasets that might be more practical in real-world deployment.
- What evidence would resolve it: Systematic experiments training Retroformer with varying dataset sizes (10%, 25%, 50%, 75%, 100% of original) and measuring performance degradation.

### Open Question 5
- Question: How does Retroformer's performance compare to fine-tuning the actor model directly when the actor model parameters are accessible?
- Basis in paper: Explicit - The paper states it avoids direct LLM fine-tuning by treating the actor as part of the environment, making it a flexible plug-in for cloud-based models.
- Why unresolved: The paper positions this as an advantage for cloud-based models where fine-tuning isn't possible, but doesn't compare against scenarios where direct fine-tuning could be performed. It's unclear if the plug-in approach sacrifices performance compared to direct fine-tuning.
- What evidence would resolve it: Direct comparison experiments where both Retroformer (with frozen actor) and direct fine-tuning approaches are applied to the same task, measuring both final performance and learning efficiency.

## Limitations
- Claims about policy gradient optimization effectiveness rely heavily on empirical results from a single benchmark (HotPotQA)
- Framework depends on quality of frozen actor LLM, which could limit performance improvements
- Generalizability across diverse tasks and environments not thoroughly validated

## Confidence

- **High confidence**: The architectural design and implementation approach are clearly specified and technically sound. The separation of actor and retrospective models with policy gradient optimization is well-defined.
- **Medium confidence**: The empirical results showing improvement over baselines are promising but limited to one dataset and task type. The 53% success rate demonstrates effectiveness but leaves room for improvement.
- **Low confidence**: Claims about generalizability across diverse tasks and environments are not thoroughly validated. The mechanism for credit assignment learning from the replay buffer across different domains lacks empirical support.

## Next Checks

1. **Cross-task generalization test**: Evaluate Retroformer on diverse reasoning tasks (e.g., DROP, QASC) to verify if the retrospective model's credit assignment patterns transfer beyond HotPotQA.

2. **Ablation study on reward signal stability**: Test how sensitive the approach is to noisy reward signals by introducing artificial reward perturbations and measuring performance degradation.

3. **Human evaluation of reflection quality**: Conduct blinded human assessments of the actionable insights and credit assignment quality in reflections generated by frozen vs. reinforced retrospective models.