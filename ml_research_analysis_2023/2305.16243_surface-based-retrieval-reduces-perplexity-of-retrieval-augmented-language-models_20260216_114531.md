---
ver: rpa2
title: Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language
  Models
arxiv_id: '2305.16243'
source_url: https://arxiv.org/abs/2305.16243
tags:
- retrieval
- retro
- language
- neighbors
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies RETRO, a retrieval-augmented language model
  that improves perplexity by retrieving nearest neighbors during generation. The
  authors observe that RETRO's performance gain is better explained by surface-level
  similarities, such as token overlap, rather than semantic similarity.
---

# Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models

## Quick Facts
- **arXiv ID**: 2305.16243
- **Source URL**: https://arxiv.org/abs/2305.16243
- **Reference count**: 9
- **Primary result**: BM25-based surface-level retrieval reduces perplexity by 13.6% compared to semantic retrieval in RETRO

## Executive Summary
This paper investigates why RETRO, a retrieval-augmented language model, achieves perplexity improvements through retrieval. The authors discover that RETRO's performance gains are driven primarily by surface-level similarities (token overlap) rather than semantic similarity captured by dense embeddings. By replacing RETRO's semantic retrieval with BM25-based surface-level retrieval, they achieve a 13.6% reduction in perplexity. They also propose a hybrid approach that combines fast dense retrieval with BM25 reranking, capturing 24.7% of BM25's benefit with minimal overhead.

## Method Summary
The authors replace RETRO's semantic retrieval with BM25-based surface-level retrieval and evaluate perplexity reduction. They use The Pile and RealNews datasets (36M documents, 52B tokens) with a 630M parameter RETRO model trained for 140k steps. The method involves retrieving neighbors based on token overlap using BM25, and comparing results to RETRO's original sentence transformer-based semantic retrieval. A hybrid approach is also explored, using dense retrieval to retrieve approximate neighbors followed by BM25 reranking.

## Key Results
- BM25-based retrieval reduces perplexity by 13.6% compared to semantic retrieval in RETRO
- Token overlap correlates more strongly with perplexity reduction than semantic similarity
- Hybrid approach (dense + BM25 reranking) captures 24.7% of BM25's benefit with minimal overhead

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Surface-level similarity (token overlap) is a stronger predictor of perplexity reduction than semantic similarity in retrieval-augmented language models.
- **Mechanism**: RETRO relies more on exact or near-exact token matches from retrieved neighbors rather than deeper semantic understanding captured by dense embeddings.
- **Core assumption**: The language model benefits primarily from lexical overlap during generation, not semantic generalization.
- **Evidence anchors**: 
  - "the performance gain is better explained by surface-level similarities, such as token overlap"
  - "The results, reported in Table 1, show a considerably stronger correlation between âˆ†PPL and unigram token overlap than negative L2 distance between ST representations."
- **Break condition**: If the language model's performance depends on semantic generalization rather than memorization or lexical copying, this mechanism would fail.

### Mechanism 2
- **Claim**: Replacing semantic retrieval with BM25 yields significant perplexity reduction because BM25 retrieves neighbors with higher surface similarity.
- **Mechanism**: BM25 retrieves chunks that share more unigrams with the query chunk, which RETRO can then leverage to reduce perplexity through lexical copying or near-copying strategies.
- **Core assumption**: Higher token overlap between retrieved neighbors and the query chunk directly translates to lower perplexity.
- **Evidence anchors**:
  - "replace RETRO's semantic retrieval with a surface-level method based on BM25, which significantly reduces perplexity by 13.6%"
  - "Using neighbors with more surface-level similarity to the generated chunk is a solid method for leveraging the retrieval mechanism to reduce the perplexity."
- **Break condition**: If the dataset contains many paraphrases or semantically similar but lexically different passages, BM25 would miss useful neighbors.

### Mechanism 3
- **Claim**: Hybrid retrieval (approximate dense + BM25 reranking) captures much of BM25's benefit with minimal overhead.
- **Mechanism**: Dense retrieval (e.g., FAISS) quickly retrieves approximate neighbors; BM25 then re-ranks the top-k to promote high-token-overlap chunks, balancing speed and quality.
- **Core assumption**: High-quality BM25 neighbors are likely to appear within the top-k dense-retrieved neighbors.
- **Evidence anchors**:
  - "a hybrid approach that first retrieves approximate neighbors using dense representations and then re-ranks them using BM25"
  - "With the proposed reranking, RETRO [ON]-ST+BM25-R could achieve 24.7% of the PPL reduction of RETRO [ON]-BM25 compared to RETRO [ON]-ST."
  - "of top-4 BM25-retrieved neighbors, 17.6% appear in top-100 faiss-retrieved chunks"
- **Break condition**: If the retrieval set is very large or the dense embeddings poorly align with surface similarity, most BM25-quality neighbors would be missed.

## Foundational Learning

- **Concept**: Dense vs sparse retrieval representations
  - **Why needed here**: Understanding why BM25 outperforms semantic retrieval requires knowing how dense embeddings capture semantic similarity but may miss surface overlap.
  - **Quick check question**: What is the key difference between dense (e.g., BERT) and sparse (e.g., BM25) retrieval methods in terms of what they optimize for?

- **Concept**: Perplexity as a language modeling metric
  - **Why needed here**: The paper's evaluation hinges on perplexity reduction; understanding how perplexity relates to token prediction quality is essential.
  - **Quick check question**: How does reducing perplexity relate to the quality of next-token predictions in a language model?

- **Concept**: Token overlap and lexical similarity
  - **Why needed here**: The paper's central claim is that surface-level token overlap, not semantic similarity, drives performance gains in RETRO.
  - **Quick check question**: What is unigram token overlap and why might it be a strong signal for retrieval-augmented language models?

## Architecture Onboarding

- **Component map**: RETRO core (auto-regressive decoder with chunked cross-attention) -> Retrieval module (replaced with BM25 or hybrid BM25+FAISS) -> Tokenization (T5 tokenizer) -> Embedding (sentence transformers for dense retrieval, Lucene for BM25) -> Index (Lucene BM25 index built on training set)
- **Critical path**: 
  1. Generate chunk C_u
  2. Retrieve neighbors using BM25 (or dense+rerank)
  3. Pass neighbors + C_u through encoder
  4. Decoder attends to encoder output for next token prediction
- **Design tradeoffs**:
  - BM25: higher token overlap, but expensive at scale
  - Dense retrieval: fast, but misses surface-level matches
  - Hybrid: balances speed and overlap, but requires tuning k
- **Failure signatures**:
  - Perplexity doesn't improve after switching to BM25: likely token overlap not driving gains
  - Reranking too slow: k too large or BM25 index not optimized
  - Neighbors irrelevant: tokenization mismatch or vocabulary size issue
- **First 3 experiments**:
  1. Measure token overlap correlation with perplexity reduction on validation set
  2. Replace ST retrieval with pure BM25 and measure PPL change
  3. Implement hybrid retrieval (FAISS top-1000 + BM25 rerank) and compare PPL to pure BM25

## Open Questions the Paper Calls Out
- **Open Question 1**: How do the performance gains from BM25-based retrieval compare to semantic retrieval in downstream tasks like question answering? (Basis: explicit; unresolved because paper only evaluates on language modeling; resolution: experiments on downstream tasks)
- **Open Question 2**: How much does the generalization ability of retrieval-augmented language models depend on the retrieval method used? (Basis: explicit; unresolved because paper suggests RETRO relies on surface similarity which may affect generalizability; resolution: experiments comparing generalization of RETRO with different retrieval methods)
- **Open Question 3**: How does the performance of RETRO with BM25 retrieval change as the model size and retrieval set size increase? (Basis: explicit; unresolved because paper only experiments with small model/data; resolution: experiments on larger models and retrieval sets)

## Limitations
- Model-specific findings may not generalize to other retrieval-augmented architectures beyond RETRO
- Results are based on specific datasets (The Pile and RealNews) and may not transfer to other domains
- Perplexity improvements don't necessarily translate to better performance on downstream tasks

## Confidence

**High confidence** (supported by direct experimental evidence):
- BM25 retrieval reduces perplexity more than semantic retrieval in RETRO
- Token overlap correlates more strongly with perplexity reduction than semantic similarity
- Hybrid retrieval captures significant portion of BM25's benefit with minimal overhead

**Medium confidence** (supported by experimental results but with some assumptions):
- The mechanism of surface-level similarity being more beneficial than semantic similarity applies broadly to retrieval-augmented language models
- The hybrid approach scales effectively to larger retrieval sets
- The 24.7% capture rate of BM25's benefit is representative across different chunk sizes and retrieval depths

**Low confidence** (primarily theoretical or based on limited evidence):
- These findings generalize to other retrieval-augmented architectures beyond RETRO
- The same surface-level advantage would hold for multilingual or morphologically rich languages
- The mechanisms would remain consistent when scaling to trillion-parameter models

## Next Checks
1. **Cross-architecture validation**: Implement and test BM25-based retrieval on alternative retrieval-augmented models (e.g., REALM, FiD) to determine if surface-level similarity provides similar advantages across different architectures.
2. **Multilingual evaluation**: Conduct experiments using BM25 retrieval on multilingual datasets with morphologically rich languages (e.g., Finnish, Turkish, Arabic) to assess whether token overlap remains an effective signal when surface forms vary significantly across semantically equivalent expressions.
3. **Downstream task performance**: Evaluate the retrieved models on benchmark tasks (e.g., question answering, summarization) to determine if perplexity improvements translate to better task performance, or if surface-level retrieval creates models that are good at copying but poor at reasoning or generation.