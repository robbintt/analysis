---
ver: rpa2
title: A Survey on Large Language Model based Autonomous Agents
arxiv_id: '2308.11432'
source_url: https://arxiv.org/abs/2308.11432
tags:
- agents
- arxiv
- agent
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of large language model
  (LLM)-based autonomous agents, which are systems that leverage the human-like intelligence
  of LLMs to achieve autonomy and solve complex tasks. The authors propose a unified
  framework for LLM-based autonomous agent construction, encompassing four key modules:
  profiling, memory, planning, and action.'
---

# A Survey on Large Language Model based Autonomous Agents

## Quick Facts
- arXiv ID: 2308.11432
- Source URL: https://arxiv.org/abs/2308.11432
- Reference count: 40
- One-line primary result: Comprehensive survey of LLM-based autonomous agents covering unified construction frameworks, diverse applications, and evaluation strategies.

## Executive Summary
This paper presents a comprehensive survey of large language model (LLM)-based autonomous agents, which are systems that leverage the human-like intelligence of LLMs to achieve autonomy and solve complex tasks. The authors propose a unified framework for LLM-based autonomous agent construction, encompassing four key modules: profiling, memory, planning, and action. They also provide an overview of the diverse applications of LLM-based autonomous agents across social science, natural science, and engineering domains, as well as commonly employed evaluation strategies. The survey highlights the potential of LLM-based autonomous agents to revolutionize various fields, while also identifying key challenges such as role-playing capability, generalized human alignment, prompt robustness, hallucination, knowledge boundary, and efficiency.

## Method Summary
This is a survey paper synthesizing prior work into a unified framework for LLM-based autonomous agent construction. The authors organize findings around four modules: profiling, memory, planning, and action. They review representative models and applications, categorize them, and identify challenges and future directions. The survey draws on 40 references and maintains a living repository for updates.

## Key Results
- Unified framework for LLM-based autonomous agents comprising profiling, memory, planning, and action modules
- Comprehensive categorization of applications across social science, natural science, and engineering domains
- Identification of key challenges: role-playing, human alignment, prompt robustness, hallucination, knowledge boundary, and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-based autonomous agents can achieve human-like decision-making by leveraging the vast knowledge and reasoning capabilities of large language models, structured through a unified profiling-memory-planning-action framework.
- **Mechanism**: The framework integrates four key modules: profiling (defining agent roles), memory (storing and retrieving past experiences), planning (breaking down tasks into subtasks), and action (executing decisions). Each module contributes to agent autonomy by simulating cognitive processes like role assignment, memory recall, strategic thinking, and task execution.
- **Core assumption**: LLMs can simulate human cognitive processes sufficiently to support autonomous decision-making when augmented with structured modules.
- **Evidence anchors**: [abstract]: "a unified framework that encompasses a majority of the previous work" and "leverage the human-like intelligence of LLMs to achieve autonomy." [section]: "the profiling module impacts the memory and planning modules, and collectively, these three modules influence the action module." [corpus]: Corpus signals show average neighbor FMR=0.49, suggesting moderate relatedness but not high certainty—evidence is present but not conclusive.
- **Break condition**: If LLMs fail to generalize beyond training data, the framework cannot simulate human cognition accurately, leading to poor decision-making.

### Mechanism 2
- **Claim**: Memory operations (reading, writing, reflection) enable agents to accumulate experiences and improve decision-making over time, enhancing consistency and effectiveness.
- **Mechanism**: The memory module uses short-term and long-term storage to buffer recent perceptions and consolidate important information. Memory reading extracts relevant past data based on recency, relevance, and importance; writing stores new experiences while managing duplication and overflow; reflection allows self-summarization, verification, and correction.
- **Core assumption**: Agents can reliably extract, store, and reflect on memories in ways that improve future actions, analogous to human learning.
- **Evidence anchors**: [section]: "memories that are more recent, relevant, and important are more likely to be extracted" and "reflection can occur hierarchically, meaning that insights can be generated based on existing insights." [corpus]: Corpus signals average FMR=0.49 indicates moderate evidence for memory mechanisms but no strong proof of their effectiveness.
- **Break condition**: If memory retrieval or reflection fails to select or generate useful information, agents cannot improve over time.

### Mechanism 3
- **Claim**: Planning with feedback (environmental, human, or model) enables agents to iteratively refine their strategies, leading to more robust and adaptable behavior.
- **Mechanism**: Planning modules can operate with or without feedback. With feedback, agents adjust plans based on real-time signals—environmental outcomes, human guidance, or model-generated critiques—closing the loop between action and planning.
- **Core assumption**: Feedback signals are available, interpretable, and actionable by the agent to improve planning.
- **Evidence anchors**: [section]: "environmental feedback serves as a direct indicator of planning success or failure, thereby enhancing the efficiency of closed-loop planning." [corpus]: Corpus signals show moderate relatedness (FMR=0.49), suggesting some support but no definitive proof of feedback effectiveness.
- **Break condition**: If feedback is noisy, delayed, or absent, the planning loop breaks and agents cannot adapt effectively.

## Foundational Learning

- **Concept: Large Language Model (LLM) capabilities**
  - Why needed here: Understanding LLM strengths (language understanding, reasoning, generation) is essential to grasp how agents simulate human-like intelligence.
  - Quick check question: Can an LLM generate coherent plans from natural language instructions without external tools?

- **Concept: Autonomous agent architecture design**
  - Why needed here: Knowing how modules (profiling, memory, planning, action) interact explains the agent’s autonomy and decision flow.
  - Quick check question: How does the memory module influence the planning module in the unified framework?

- **Concept: Feedback-driven learning**
  - Why needed here: Recognizing how agents learn from environment, human, or model feedback is key to understanding adaptive behavior.
  - Quick check question: What types of feedback can an agent use to refine its plans in real time?

## Architecture Onboarding

- **Component map**: Profiling module → Memory module → Planning module → Action module
- **Critical path**: Profile → Memory → Planning → Action → Feedback (loop)
- **Design tradeoffs**:
  - Unified vs. hybrid memory: unified is simpler but less flexible; hybrid allows separation of short- and long-term functions.
  - Planning with vs. without feedback: feedback improves adaptability but requires reliable signals; without feedback is faster but less robust.
  - Tool use vs. self-knowledge: tools expand capabilities but add complexity; self-knowledge is more integrated but limited.
- **Failure signatures**:
  - Profiling errors → agents behave inconsistently or in wrong roles.
  - Memory retrieval failures → poor decision-making due to missing context.
  - Planning breakdowns → inability to decompose tasks or adapt plans.
  - Action execution failures → inability to complete tasks or interact effectively.
- **First 3 experiments**:
  1. **Profile generation test**: Create agents with different profile methods (handcrafting vs. LLM-generated) and compare task consistency.
  2. **Memory operation test**: Implement unified and hybrid memory; measure retrieval accuracy and task completion rates.
  3. **Planning feedback test**: Compare agent performance with and without environmental feedback on a fixed task set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively constrain the utilization of user-unknown knowledge in LLMs for agent-based simulations to ensure realistic human behavior replication?
- Basis in paper: [explicit] The paper discusses the challenge of knowledge boundary in LLM-based agents, specifically in the context of simulating human behavior. It mentions that LLMs are trained on extensive web knowledge, which can lead to unrealistic decision-making when simulating user selection behaviors for movies.
- Why unresolved: Existing LLMs are trained on a vast corpus of web knowledge, making it difficult to ensure that they only utilize user-unknown knowledge for simulations. The paper suggests that implementing appropriate strategies to address this issue is crucial for building believable agent simulation environments.
- What evidence would resolve it: Experimental results comparing the performance of LLM-based agents with and without knowledge constraints in simulating human behavior, demonstrating the impact of knowledge boundary on the realism of the simulations.

### Open Question 2
- Question: How can we develop a unified and robust prompt framework that can be applied to various LLMs for consistent operation and effective communication in autonomous agents?
- Basis in paper: [explicit] The paper discusses the challenge of prompt robustness in constructing autonomous agents. It mentions that prompts for one module can influence others, and prompt frameworks can vary significantly across different LLMs. Developing a unified and robust prompt framework is seen as an important yet unresolved issue.
- Why unresolved: The paper highlights that even minor alterations in prompts can yield substantially different outcomes in LLMs. The complexity of prompt frameworks in autonomous agents, considering all modules, further exacerbates the challenge. Finding a solution that works consistently across different LLMs is difficult.
- What evidence would resolve it: Comparative studies evaluating the performance of autonomous agents using different prompt frameworks, demonstrating the impact of prompt robustness on the agents' behavior and task completion abilities.

### Open Question 3
- Question: How can we effectively address the hallucination problem in LLM-based autonomous agents to ensure accurate and reliable outputs?
- Basis in paper: [explicit] The paper mentions that hallucination is a fundamental challenge for LLMs, where the model erroneously outputs false information confidently. It highlights the potential consequences of hallucination, such as incorrect or misleading code, security risks, and ethical issues.
- Why unresolved: The paper suggests that incorporating human correction feedback within the loop of human-agent interaction could be a possible approach to address hallucination. However, the effectiveness of this approach and the development of more robust methods to mitigate hallucination in LLM-based agents remain open challenges.
- What evidence would resolve it: Comparative studies evaluating the performance of LLM-based agents with and without hallucination mitigation techniques, demonstrating the impact of addressing hallucination on the agents' output accuracy and reliability.

## Limitations
- Survey comprehensiveness depends on completeness of reference coverage; recent developments may be missed.
- Unified framework may not fully encompass emerging hybrid approaches or edge cases in agent design.
- Claims about feedback-driven planning benefits lack strong empirical validation in surveyed works.

## Confidence
- **High confidence**: The survey's organization into profiling, memory, planning, and action modules is well-supported by cited works and represents a common structure in the field.
- **Medium confidence**: Claims about memory operations improving decision-making over time are plausible but lack strong empirical validation in the surveyed works.
- **Low confidence**: The assertion that feedback-driven planning universally leads to more robust behavior is not fully substantiated, as feedback quality and availability vary widely across applications.

## Next Checks
1. Verify the survey's reference list for completeness by cross-checking against recent conference proceedings and preprints in the field.
2. Test the unified framework by applying it to a new LLM-based agent not included in the original survey and assess its fit.
3. Conduct a small-scale experiment comparing agent performance with and without feedback loops in a controlled environment to evaluate the claimed benefits of feedback-driven planning.