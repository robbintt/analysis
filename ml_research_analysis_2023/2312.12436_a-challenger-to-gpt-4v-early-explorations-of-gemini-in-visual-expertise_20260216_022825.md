---
ver: rpa2
title: A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise
arxiv_id: '2312.12436'
source_url: https://arxiv.org/abs/2312.12436
tags:
- image
- gpt-4v
- gemini
- section
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive qualitative and quantitative
  evaluation of Gemini Pro, GPT-4V, and Sphinx, three powerful multi-modal large language
  models (MLLMs). The evaluation covers diverse domains of visual understanding, including
  fundamental perception, advanced cognition, challenging vision tasks, and various
  expert capacities.
---

# A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise

## Quick Facts
- arXiv ID: 2312.12436
- Source URL: https://arxiv.org/abs/2312.12436
- Reference count: 40
- Key outcome: Gemini Pro achieves competitive performance to GPT-4V in multimodal tasks, with balanced capabilities across perception and cognition

## Executive Summary
This paper presents a comprehensive evaluation of three MLLMs—Gemini Pro, GPT-4V, and Sphinx—across fundamental perception, advanced cognition, challenging vision tasks, and expert capacities. The study combines qualitative analysis with quantitative benchmarking using the MME dataset, revealing that Gemini Pro demonstrates superior overall performance while highlighting common limitations in spatial perception, OCR, and logical self-consistency across all models. The findings suggest Gemini Pro is a strong challenger to GPT-4V in multimodal reasoning tasks.

## Method Summary
The study employs both qualitative and quantitative evaluation methods. For qualitative assessment, diverse images and text prompts are collected and evaluated across four domains using various prompting techniques including simple instruction following, visual referring prompts, chain-of-thought prompts, and in-context few-shot learning. The quantitative component uses the MME benchmark, which covers 14 subtasks across perception and cognition domains. The evaluation compares model performance on standard accuracy and enhanced accuracy metrics, though specific training procedures are not detailed as the focus is on evaluation rather than model development.

## Key Results
- Gemini Pro achieves the highest overall MME score, demonstrating balanced performance across perception and cognition tasks
- GPT-4V shows superior performance in advanced cognition tasks but exhibits privacy safeguards that limit certain responses (e.g., celebrity identification)
- Sphinx outperforms other models in perception tasks, likely due to training data alignment with academic benchmarks
- All models struggle with spatial perception limitations, OCR challenges, and logical self-consistency issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gemini achieves competitive performance to GPT-4V in multimodal tasks due to better balance between perception and cognition
- Mechanism: Gemini's architecture is optimized for both perception and cognition tasks, whereas GPT-4V leans more heavily toward detailed reasoning output
- Core assumption: Architectural tuning, not just model size, drives task-specific performance differences
- Evidence anchors:
  - [abstract]: "Gemini prefers to output a direct and concise answer. The quantitative evaluation on the popular MME benchmark also demonstrates the impressive multi-modal understanding performance of Gemini, and its potential to be a strong challenger to GPT-4V."
  - [section]: Figure 106 shows Gemini leading in overall MME score despite GPT-4V's higher cognition sub-scores
  - [corpus]: Weak evidence—no neighbor papers directly compare architectural design choices
- Break condition: If qualitative samples show Gemini consistently weaker in cognition, or MME scores reverse, the balanced-competitor claim fails

### Mechanism 2
- Claim: GPT-4V's refusal to respond to real-person queries (e.g., celebrity identification) limits its MME score but reflects privacy safeguards
- Mechanism: GPT-4V is explicitly trained to avoid identifying real individuals in images, causing zero scores in relevant sub-tasks
- Core assumption: Zero-score sub-task (celebrity recognition) is due to policy enforcement, not inability
- Evidence anchors:
  - [abstract]: "GPT-4V issues a privacy statement and evades the request" (Figure 81)
  - [section]: MME results table shows GPT-4V score of 0.0 for "Cele." sub-task
  - [corpus]: Weak—no neighbor papers discuss privacy refusal in multimodal benchmarks
- Break condition: If GPT-4V could answer but scores zero for other reasons, the privacy-refusal claim is invalid

### Mechanism 3
- Claim: Sphinx's strong perception scores stem from training data distribution aligned with academic benchmarks
- Mechanism: Sphinx was trained on datasets similar to public academic MME perception datasets, giving it an advantage there
- Core assumption: Overlap between training data and benchmark data drives superior perception scores
- Evidence anchors:
  - [abstract]: "Sphinx outperforms other models in most of the perception tasks... this is probably because that Sphinx pay more attention on perception during training, such as object detection"
  - [section]: Figure 106 shows Sphinx leading on perception sub-scores while lagging in cognition
  - [corpus]: Weak—no neighbor papers confirm training data overlap
- Break condition: If perception advantage disappears on different benchmarks, the data-alignment hypothesis is falsified

## Foundational Learning

- Concept: Multimodal alignment between vision and language modalities
  - Why needed here: Core to MLLM performance—models must jointly interpret visual and textual inputs
  - Quick check question: What happens if visual embeddings are poorly aligned with language embeddings in a MLLM?

- Concept: Chain-of-Thought prompting for complex reasoning
  - Why needed here: Enables step-by-step reasoning in tasks like math or physics problems (Figures 42, 54)
  - Quick check question: How does CoT prompting improve reasoning accuracy in GPT-4V and Gemini?

- Concept: Visual hallucination in MLLMs
  - Why needed here: Both models fabricate details when unable to perceive correctly (Figures 43, 59)
  - Quick check question: What prompts or conditions lead to visual hallucination in Gemini vs GPT-4V?

## Architecture Onboarding

- Component map: Vision Encoder → Multimodal Fusion → LLM Decoder → Output Generator
- Critical path: Input image → Vision encoder → Fusion → LLM → Output. Failures in any stage degrade overall quality
- Design tradeoffs: Perception accuracy vs. reasoning depth vs. safety/privacy constraints. Gemini favors balance; GPT-4V favors depth with safety
- Failure signatures: Spatial misperception (position tasks), OCR errors (charts/tables), hallucination (fabricated details), refusal (privacy)
- First 3 experiments:
  1. Run MME perception sub-tasks to quantify Sphinx's data-alignment advantage
  2. Test CoT prompting on mathematical problems to confirm accuracy gains
  3. Vary prompt framing (yes/no vs. open-ended) to measure robustness differences between models

## Open Questions the Paper Calls Out

- Question: How do different prompt engineering techniques impact the performance of Gemini Pro on abstract visual reasoning tasks compared to GPT-4V and Sphinx?
  - Basis in paper: [explicit] The paper discusses various prompt techniques including simple instruction following, visual referring prompts, chain-of-thought prompts, and in-context few-shot learning, and notes that GPT-4V supports a diverse range of prompt techniques
  - Why unresolved: While the paper mentions these techniques, it doesn't provide a detailed comparison of how each technique specifically affects Gemini Pro's performance relative to the other models on abstract visual reasoning tasks
  - What evidence would resolve it: A systematic evaluation of Gemini Pro's performance on abstract visual reasoning tasks using different prompt techniques, with direct comparisons to GPT-4V and Sphinx's performance under the same conditions

- Question: To what extent does the training data distribution influence the performance gap between Sphinx and the closed-source models (Gemini Pro and GPT-4V) on perception tasks?
  - Basis in paper: [explicit] The paper states that Sphinx's superior performance on perception tasks in the MME benchmark may be due to the close alignment between the benchmark's data distribution and Sphinx's training set
  - Why unresolved: The paper suggests this correlation but doesn't provide a detailed analysis of how the training data distribution specifically affects the performance gap across different perception tasks
  - What evidence would resolve it: A detailed analysis of the training data distributions for all three models, with a focus on perception tasks, and a correlation study between data distribution similarity and performance on the MME benchmark

- Question: What are the specific limitations of Gemini Pro's spatial perception capabilities, and how do these limitations manifest in different types of visual tasks?
  - Basis in paper: [explicit] The paper identifies spatial perception limitations as a common issue for Gemini Pro and GPT-4V, citing examples of difficulty with left-right identification and imprecise bounding boxes
  - Why unresolved: While the paper mentions these limitations, it doesn't provide a comprehensive analysis of the extent of these limitations across different types of spatial perception tasks or potential underlying causes
  - What evidence would resolve it: A detailed study of Gemini Pro's performance on various spatial perception tasks, including object detection, spatial relation recognition, and scene understanding, with analysis of error patterns and potential architectural limitations

## Limitations
- Limited sample size and selection bias in qualitative evaluations
- Specific benchmark questions and evaluation criteria for MME remain unspecified
- Claims about privacy refusal and training data alignment rely on limited evidence

## Confidence
- Privacy refusal mechanism: Medium
- Training data alignment hypothesis: Medium
- Architectural balance claim: Medium

## Next Checks
1. Replicate the MME benchmark evaluation using publicly available test subsets to verify score distributions and confirm Sphinx's perception advantage disappears on non-aligned benchmarks
2. Systematically test GPT-4V's refusal behavior across multiple real-person identification prompts to determine if zero scores are consistent privacy policy enforcement versus isolated cases
3. Conduct controlled prompting experiments varying Chain-of-Thought versus direct prompting on mathematical problems to quantify Gemini's reasoning improvements over GPT-4V