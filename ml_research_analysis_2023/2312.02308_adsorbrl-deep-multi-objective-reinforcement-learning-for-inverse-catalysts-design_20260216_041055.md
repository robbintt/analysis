---
ver: rpa2
title: 'AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts
  Design'
arxiv_id: '2312.02308'
source_url: https://arxiv.org/abs/2312.02308
tags:
- energy
- materials
- learning
- adsorption
- catalysts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents AdsorbRL, a Deep Reinforcement Learning approach
  for inverse catalyst design, addressing the challenge of identifying materials with
  optimal multi-objective binding energy profiles for clean energy applications. The
  method uses offline reinforcement learning on sparse datasets from the Open Catalyst
  Project and Materials Project, training Deep Q-Network agents to traverse chemical
  spaces and identify promising catalysts.
---

# AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse Catalysts Design

## Quick Facts
- arXiv ID: 2312.02308
- Source URL: https://arxiv.org/abs/2312.02308
- Reference count: 40
- Primary result: RL agent achieves 4.1 eV improvement in single-objective binding energy and 0.8 eV simultaneous improvement across six target adsorbates

## Executive Summary
This work presents AdsorbRL, a Deep Reinforcement Learning approach for inverse catalyst design that addresses the challenge of identifying materials with optimal multi-objective binding energy profiles for clean energy applications. The method uses offline reinforcement learning on sparse datasets from the Open Catalyst Project and Materials Project, training Deep Q-Network agents to traverse chemical spaces and identify promising catalysts. Key innovations include Random Edge Traversal for action space reduction and Objective Sub-Sampling for multi-objective exploration. Results show the agent achieves significant improvements in both single-objective and multi-objective settings, demonstrating strong potential for deep RL in navigating complex chemical spaces for low-emissions technologies.

## Method Summary
The approach uses offline reinforcement learning with Deep Q-Networks to identify optimal catalyst compositions. The agent operates in a chemical space of ~160,000 possible compounds, using known adsorption energies from the OC20 and Materials Project datasets as sparse rewards. The method employs Random Edge Traversal to reduce the action space from 60 to 5 actions, and Objective Sub-Sampling to handle multi-objective optimization by randomly selecting one objective per training rollout. The agent learns to modify catalyst compositions through add/remove element actions to optimize binding energies for target adsorbates like *OH2, *OH, *CH4, *CH2, *N2, and *NH3.

## Key Results
- Single-objective setting: Agent achieves 4.1 eV average improvement in target binding energy
- Multi-objective setting: Agent achieves 0.8 eV simultaneous improvement across six target adsorbates
- Action space reduction: Random Edge Traversal reduces actions from 60 to 5 while maintaining performance
- Offline learning: Successfully trains on sparse datasets with only 2,000-3,000 known states per adsorbate

## Why This Works (Mechanism)

### Mechanism 1
Random Edge Traversal reduces action space complexity enough to enable learning in sparse reward environments. By replacing element-specific add/remove actions with "add random element" and "remove specific element" actions, the agent only needs to learn 5 actions instead of 60, making Q-learning tractable in the OC20-subgraph. Core assumption: The reduced action space still allows the agent to reach optimal states through exploration.

### Mechanism 2
Objective Sub-Sampling encourages exploration by reducing per-step optimization pressure in multi-objective learning. By randomly sampling one objective per training rollout instead of optimizing all objectives simultaneously, the agent can make progress on individual objectives without being penalized for poor performance on others. Core assumption: The agent can still learn to optimize all objectives effectively despite only experiencing one per rollout during training.

### Mechanism 3
Offline RL on known adsorption energies provides sufficient signal for catalyst design despite sparse rewards. The agent learns from the OC20 dataset where adsorption energies are known for ~2,000-3,000 catalysts per adsorbate, using these as sparse rewards to guide exploration of the ~160,000 possible compounds. Core assumption: The known states in the dataset are representative enough of the full chemical space to guide discovery of optimal catalysts.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The entire RL framework is built on MDP formalism where states, actions, rewards, and transitions define the problem. Quick check: Can you identify the 4-tuple (S, A, R, P) components in the catalyst design problem?

- **Q-Learning and Bellman Equation**: The agent uses Q-learning to learn value functions for state-action pairs, with the Bellman equation providing the recursive update rule. Quick check: What is the difference between the Q-learning update and the Bellman optimality equation?

- **Goal-Conditioned Reinforcement Learning**: Multi-objective learning requires conditioning the policy on different target adsorption energy profiles. Quick check: How does the goal-conditioned Bellman equation differ from the standard version?

## Architecture Onboarding

- **Component map**: State (55-dim one-hot) → Action Selection (5 actions) → Reward Calculation (energy-based) → Q-Value Update → Policy Improvement
- **Critical path**: State → Action Selection → Reward Calculation → Q-Value Update → Policy Improvement
- **Design tradeoffs**: Simplified action space vs. exploration capability; offline vs. online learning; single vs. multi-objective formulations
- **Failure signatures**: Agent converges to invalid states; poor exploration of chemical space; failure to optimize multiple objectives simultaneously
- **First 3 experiments**:
  1. Full state/actions setup with DQN (baseline, expected to fail due to sparse rewards)
  2. Periodic table GridWorld with Q-learning (simplest environment to test basic concepts)
  3. OC20-subgraph with Random Edge Traversal (first successful approach on real data)

## Open Questions the Paper Calls Out

- How does the performance of the RL-based approach compare to traditional optimization methods (e.g., gradient ascent, genetic algorithms) in the multi-objective inverse catalyst design problem?

- How does the performance of the RL agent change when using different adsorbate combinations and objective vectors in the multi-objective setup?

- How does the performance of the RL agent change when using different ML-based adsorption energy estimators as the critic in an actor-critic setup?

## Limitations

- Relies on sparse datasets with only 2,000-3,000 known states per adsorbate out of ~160,000 possible compounds
- Random Edge Traversal may limit discovery of novel catalysts requiring specific element combinations
- Multi-objective improvements (0.9 eV) are smaller than single-objective gains (4.1 eV), suggesting potential tradeoffs

## Confidence

*High Confidence:* The architectural claims around Deep Q-Network implementation and the core MDP formulation are well-established in RL literature and the experimental setup is clearly described.

*Medium Confidence:* The Random Edge Traversal and Objective Sub-Sampling mechanisms show promise but their effectiveness depends heavily on dataset characteristics and implementation details not fully specified in the paper.

*Low Confidence:* The claim that offline RL can effectively navigate the full chemical space with sparse rewards, given that only a small fraction of possible compounds have known adsorption energies.

## Next Checks

1. Test the agent's generalization by evaluating performance on catalyst compositions not present in the training dataset to verify true exploration capabilities.

2. Conduct ablation studies comparing Random Edge Traversal against full action space with different reward shaping strategies to quantify the tradeoff between simplicity and exploration power.

3. Evaluate the multi-objective learning by testing whether improving one target adsorbate consistently degrades performance on others, revealing potential Pareto frontiers in the design space.