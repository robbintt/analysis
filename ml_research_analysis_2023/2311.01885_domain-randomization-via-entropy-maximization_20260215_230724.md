---
ver: rpa2
title: Domain Randomization via Entropy Maximization
arxiv_id: '2311.01885'
source_url: https://arxiv.org/abs/2311.01885
tags:
- distribution
- dynamics
- parameters
- success
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DORAEMON, a novel method for domain randomization
  (DR) in reinforcement learning (RL) that automatically guides the training dynamics
  distribution without requiring real-world data. The core idea is to maximize the
  entropy of the distribution while constraining it by the policy's probability of
  success, enabling the agent to learn robust and generalizable policies.
---

# Domain Randomization via Entropy Maximization

## Quick Facts
- arXiv ID: 2311.01885
- Source URL: https://arxiv.org/abs/2311.01885
- Reference count: 26
- Key outcome: DORAEMON outperforms DR baselines in global success rate and entropy across benchmark tasks, achieving successful zero-shot transfer in robotic manipulation with unknown real-world parameters

## Executive Summary
This paper introduces DORAEMON, a novel method for domain randomization in reinforcement learning that automatically guides the training dynamics distribution without requiring real-world data. The core innovation is maximizing the entropy of the training distribution while constraining it by the policy's probability of success, enabling robust and generalizable policies. The method demonstrates superior performance compared to standard domain randomization approaches across multiple simulation benchmarks and achieves successful zero-shot transfer in a real-world robotic manipulation task.

## Method Summary
DORAEMON is a constrained optimization approach that maximizes the entropy of the training distribution while maintaining a desired probability of success. The method iteratively updates the dynamics distribution over simulator parameters to increase diversity, while ensuring the policy maintains sufficient performance. It uses Soft Actor-Critic (SAC) as the RL subroutine and conditions the critic network with true dynamics parameters. The algorithm only requires a notion of task success defined through domain knowledge, making it flexible for various applications.

## Key Results
- DORAEMON outperforms representative DR baselines in global success rate and entropy of training distribution
- Successful zero-shot transfer achieved in PandaPush robotic manipulation task with unknown real-world parameters
- Demonstrates effective balance between policy convergence and generalization across benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Maximizing entropy of the training distribution gradually increases diversity of sampled environments, enabling better generalization.
- **Mechanism**: The algorithm iteratively increases the entropy of the distribution over dynamics parameters while constraining it by the policy's success rate, allowing the agent to encounter wider range of dynamics during training.
- **Core assumption**: Higher entropy in training distribution leads to better generalization, and policy can maintain sufficient success rate while training on diverse dynamics.
- **Break condition**: If success rate constraint becomes too restrictive, entropy maximization may stall, limiting diversity of encountered dynamics.

### Mechanism 2
- **Claim**: Constraining entropy growth by probability of success prevents performance collapse from excessive randomization.
- **Mechanism**: Uses success indicator function to define successful trajectories, constraining entropy maximization to maintain desired success rate and prevent overly conservative policies.
- **Core assumption**: Success indicator function accurately reflects policy's ability to solve task across different dynamics.
- **Break condition**: If success indicator function is poorly defined, constraint may not effectively prevent performance collapse.

### Mechanism 3
- **Claim**: Method only requires notion of task success, making it flexible and applicable to various tasks.
- **Mechanism**: Allows user to define success indicator function based on domain knowledge (e.g., distance thresholds, task-specific tolerance, lower bound on expected return).
- **Core assumption**: User can define appropriate success indicator function for task at hand.
- **Break condition**: If task lacks clear notion of success or defining suitable indicator is difficult, method may not be directly applicable.

## Foundational Learning

- **Concept: Domain Randomization (DR)**
  - **Why needed here**: Core technique used to bridge reality gap in reinforcement learning by randomizing simulator parameters during training
  - **Quick check question**: What is the main challenge addressed by Domain Randomization in reinforcement learning?

- **Concept: Entropy Maximization**
  - **Why needed here**: Key mechanism used to guide training distribution toward higher diversity, enabling better generalization
  - **Quick check question**: How does maximizing entropy of training distribution contribute to better generalization?

- **Concept: Constrained Optimization**
  - **Why needed here**: Used to balance trade-off between increasing entropy and maintaining policy performance, preventing excessive randomization
  - **Quick check question**: Why is it important to constrain entropy maximization by probability of success?

## Architecture Onboarding

- **Component map**: Policy network -> Dynamics distribution -> Success indicator function -> Entropy maximization algorithm

- **Critical path**:
  1. Initialize dynamics distribution and policy
  2. Sample dynamics parameters from distribution
  3. Collect trajectories by running policy in environment with sampled dynamics
  4. Update policy using RL algorithm
  5. Update dynamics distribution to maximize entropy while maintaining desired success rate
  6. Repeat steps 2-5 for number of iterations

- **Design tradeoffs**:
  - Tradeoff between entropy and success rate: Higher entropy leads to better generalization but may cause performance collapse if not constrained properly
  - Choice of success indicator function: Flexibility allows application to various tasks but requires domain knowledge
  - Computational complexity: Updating dynamics distribution involves optimization, which can be expensive for high-dimensional parameter spaces

- **Failure signatures**:
  - If success rate constraint too restrictive, entropy maximization may stall, limiting diversity
  - If success indicator function not well-defined, constraint may not effectively prevent performance collapse
  - If dynamics space too high-dimensional, updating distribution may become computationally intractable

- **First 3 experiments**:
  1. Implement algorithm on simple task like Cartpole with few dynamics parameters to validate basic functionality
  2. Compare performance with standard domain randomization on complex task like Hopper to assess benefits of entropy maximization
  3. Evaluate sim-to-real transfer capabilities on robotic manipulation task like PandaPush to demonstrate applicability to real-world scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the method be modified to prevent collapse to "easy" modes in optimization landscape when backtracking the optimization problem?
- **Basis in paper**: Authors acknowledge this as limitation, mentioning method may collapse to "easy" modes when backtracking
- **Why unresolved**: Paper only suggests possible future mitigation by adding KL constraint between current optimizing policy and best-performing policy, but doesn't provide concrete solution or empirical results
- **What evidence would resolve it**: Empirical results showing improved performance and robustness when incorporating proposed KL constraint during backtracking

### Open Question 2
- **Question**: Can method be extended to incorporate prior knowledge of dynamics to bias growth of distribution?
- **Basis in paper**: Authors mention incorporating prior knowledge of dynamics to bias growth could be beneficial but would make optimization harder to solve
- **Why unresolved**: Paper doesn't explore this direction experimentally or provide concrete approach for incorporating prior knowledge
- **What evidence would resolve it**: Experimental results comparing performance with and without incorporating prior knowledge of dynamics

### Open Question 3
- **Question**: How does choice of success indicator function impact performance and generalization of method?
- **Basis in paper**: Authors discuss flexibility of defining success indicator function based on domain knowledge and task-specific metrics, investigate impact of different success thresholds
- **Why unresolved**: While authors provide some analysis on impact of success indicator function, more comprehensive study on relationship between different success metrics and overall performance lacking
- **What evidence would resolve it**: Systematic comparison of DORAEMON's performance using various success indicator functions across different tasks

## Limitations

- Method relies on well-defined success indicator function, which may require domain-specific knowledge and can be challenging to define for certain tasks
- Performance is likely sensitive to choice of hyperparameters such as trust region size, success rate threshold, and learning rates
- Computational complexity of updating dynamics distribution increases with number of parameters, limiting scalability to very high-dimensional spaces

## Confidence

- **High confidence**: Core mechanism of maximizing entropy while constraining by success rate is well-explained and supported by theoretical analysis
- **Medium confidence**: Flexibility in defining success notions based on domain knowledge is key strength, but lack of detailed implementation guidelines introduces uncertainty
- **Low confidence**: Scalability to high-dimensional parameter spaces and sensitivity to hyperparameter choices not thoroughly investigated

## Next Checks

1. Conduct sensitivity analysis to evaluate impact of hyperparameters (trust region size, success rate threshold, learning rates) on DORAEMON performance

2. Evaluate scalability to high-dimensional parameter spaces by testing method on tasks with large number of dynamics parameters and measuring computational complexity

3. Investigate failure modes by deliberately introducing challenging scenarios (highly dynamic environments, ambiguous success criteria) and analyzing method behavior