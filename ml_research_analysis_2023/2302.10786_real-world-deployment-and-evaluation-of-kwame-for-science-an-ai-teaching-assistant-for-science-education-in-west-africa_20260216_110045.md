---
ver: rpa2
title: Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant
  for Science Education in West Africa
arxiv_id: '2302.10786'
source_url: https://arxiv.org/abs/2302.10786
tags:
- questions
- science
- kwame
- answers
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the development and real-world deployment
  of Kwame for Science, an AI teaching assistant for science education in West Africa.
  The system provides answers to students' science questions using a knowledge base
  of curated passages and past national exam questions, with results filtered by year,
  question type, and automatically detected topics (91% accuracy).
---

# Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa

## Quick Facts
- arXiv ID: 2302.10786
- Source URL: https://arxiv.org/abs/2302.10786
- Reference count: 19
- 87.2% top-3 accuracy for science question answering

## Executive Summary
Kwame for Science is an AI teaching assistant deployed as a web app to support science education in West Africa. The system retrieves answers from a curated knowledge base of passages and past national exam questions using SBERT embeddings and cosine similarity. Over 8 months, it served 750 users across 32 countries with 1.5K questions asked. The system achieved 87.2% top-3 accuracy, meaning at least one useful answer appeared among the top 3 results for most questions. The paper also identifies challenges around content licensing, user trust, and technical limitations that future deployments should address.

## Method Summary
The system preprocesses science education content into 3-sentence passages, then uses a pre-trained SBERT model to embed user questions and compute cosine similarity with passage embeddings. Top 3 passages are retrieved along with 5 related past exam questions filtered by automatically detected syllabus topics (91% accuracy). The knowledge base combines CK-12 textbook content, Simple Wikipedia passages, and manually answered past exam questions from 1993-2021. The web interface allows filtering by topic, year, and question type.

## Key Results
- 87.2% top-3 accuracy on human-evaluated questions
- 91% unweighted average recall for automatic topic classification
- 750 users across 32 countries during 8-month deployment
- 1.5K questions asked by students

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kwame for Science achieves high answer relevance by using cosine similarity between SBERT embeddings of user questions and pre-indexed knowledge passages.
- Mechanism: Questions are encoded using a pre-trained SBERT model, and similarity scores are computed against embeddings of knowledge passages (3-sentence chunks). Top 3 passages are returned, increasing the probability of at least one useful answer.
- Core assumption: SBERT embeddings preserve semantic similarity for science education content and questions.
- Evidence anchors:
  - [abstract] "Kwame for Science has a high chance of giving at least one useful answer among the 3 displayed" and "87.2% top 3 accuracy"
  - [section] "When a user types a question... our system computes an embedding of the question using the SBERT model... computes cosine similarity scores with a bank of answers... retrieves and returns the top 3 answers"
- Break condition: If the knowledge base lacks relevant passages or if SBERT embeddings fail to capture domain-specific semantics (e.g., scientific terms, equations).

### Mechanism 2
- Claim: Integration of past exam questions with expert-verified answers improves contextual relevance and usability.
- Mechanism: After retrieving passages, Kwame for Science also retrieves top 5 related past exam questions and their answers, giving students both explanatory content and curriculum-aligned examples.
- Core assumption: Students benefit from seeing both explanatory passages and specific past exam questions tied to the same topic.
- Evidence anchors:
  - [abstract] "provides passages from well-curated knowledge sources and related past national exam questions as answers"
  - [section] "We then hired 4 subject-matter experts to provide answers to exam questions... This process results in 3.5K question-answer pairs"
- Break condition: If past exam questions are not well-aligned with the student's question or if expert answers are incomplete or inaccurate.

### Mechanism 3
- Claim: Automatic topic detection enables filtered access to past exam questions, improving discoverability.
- Mechanism: A trained SVM classifier maps exam questions to 48 syllabus topics using SBERT embeddings, enabling students to filter by topic, year, and question type.
- Core assumption: Classifying questions by topic improves usability and allows targeted study.
- Evidence anchors:
  - [abstract] "students can view past national exam questions... and filter by... topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall)"
  - [section] "We trained a machine-learning topic detection model... Our baseline approach had a UAR of 83.1% and our main approach had a UAR of 91%"
- Break condition: If topic classification accuracy drops below usability threshold or if topic labels do not match user mental models.

## Foundational Learning

- Concept: Cosine similarity in vector space models
  - Why needed here: Kwame for Science uses cosine similarity between SBERT embeddings to rank relevant passages.
  - Quick check question: If two embeddings have a cosine similarity of 0.9, what does that indicate about their semantic relationship?

- Concept: Topic classification using SVM with TF-IDF/SBERT features
  - Why needed here: The system automatically categorizes past exam questions into syllabus topics to enable filtering.
  - Quick check question: What is the main difference between using TF-IDF vs SBERT embeddings for topic classification?

- Concept: Question-answering system evaluation metrics (top-1 vs top-3 accuracy)
  - Why needed here: The paper evaluates Kwame for Science using top-1 and top-3 accuracy to measure answer usefulness.
  - Quick check question: Why might top-3 accuracy be more informative than top-1 accuracy for a QA system returning multiple answers?

## Architecture Onboarding

- Component map: Frontend (web app) -> SBERT encoder -> ElasticSearch index (passages & past questions) -> SVM topic classifier -> UI display
- Critical path: User question -> SBERT encoding -> cosine similarity search -> retrieve passages + past questions -> display to user
- Design tradeoffs: Used pre-computed SBERT embeddings for speed vs real-time encoding; used SBERT similarity vs fine-tuned BERT for re-ranking
- Failure signatures: Low similarity scores -> "no answer" message; poor topic classification -> incorrect filtering; OCR errors -> incorrect past question parsing
- First 3 experiments:
  1. Test SBERT similarity ranking on a small set of hand-crafted science questions vs ground truth passages.
  2. Evaluate SVM topic classifier on held-out exam questions to verify 91% UAR claim.
  3. Simulate user query flow end-to-end to measure latency from question to answer display.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Kwame for Science's accuracy compare to other AI teaching assistants in science education contexts?
- Basis in paper: [explicit] The paper states Kwame for Science has 87.2% top-3 accuracy, but no comparison is made to other systems.
- Why unresolved: The paper only evaluates Kwame for Science against itself and does not benchmark against other AI teaching assistants for science education.
- What evidence would resolve it: Comparative evaluation studies between Kwame for Science and other AI teaching assistants like Curio SmartChat using the same test set of science questions.

### Open Question 2
- Question: How can the copyright barriers to using local educational content be overcome for AI teaching assistants in Africa?
- Basis in paper: [explicit] The authors discuss copyright issues as a major challenge preventing use of local textbooks.
- Why unresolved: The paper only describes the problem but does not propose solutions or explore alternative licensing models.
- What evidence would resolve it: Case studies of successful partnerships between AI education companies and African publishers, or alternative content sourcing strategies that bypass copyright issues.

### Open Question 3
- Question: What is the impact of Kwame for Science on actual learning outcomes and academic performance?
- Basis in paper: [explicit] The authors state they plan to run randomized controlled trials but have not yet done so.
- Why unresolved: The paper only evaluates technical accuracy, not educational impact on student learning or exam performance.
- What evidence would resolve it: Randomized controlled trial results comparing student performance between groups using Kwame for Science versus traditional learning methods.

## Limitations
- Real-world deployment limited to 750 users over 8 months, which may not capture full range of question types and user demographics
- 87.2% top-3 accuracy based on human evaluation of a subset of questions without explicit definition of "useful" answers
- Knowledge base may have gaps in coverage for certain topics or question types, particularly specialized science concepts

## Confidence
- High confidence: The deployment architecture and integration of SBERT embeddings with cosine similarity for passage retrieval is technically sound and well-documented.
- Medium confidence: The 91% topic detection accuracy is based on the authors' evaluation but lacks external validation or comparison to alternative approaches.
- Medium confidence: The 87.2% top-3 accuracy metric is supported by human evaluation, but the sample size and evaluation criteria need further scrutiny.

## Next Checks
1. Conduct a larger-scale evaluation with diverse user groups and question types to validate the 87.2% top-3 accuracy claim and assess usability across different contexts.
2. Perform an ablation study to quantify the impact of topic filtering on answer discovery and user engagement, testing whether students actually use this feature effectively.
3. Test the system's performance on questions outside the current knowledge base scope to identify limitations and opportunities for knowledge expansion.