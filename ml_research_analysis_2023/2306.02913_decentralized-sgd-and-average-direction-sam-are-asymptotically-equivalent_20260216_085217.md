---
ver: rpa2
title: Decentralized SGD and Average-direction SAM are Asymptotically Equivalent
arxiv_id: '2306.02913'
source_url: https://arxiv.org/abs/2306.02913
tags:
- d-sgd
- learning
- decentralized
- gradient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization gap between decentralized
  and centralized stochastic gradient descent (D-SGD and C-SGD) in deep learning.
  Existing theories claim decentralization invariably undermines generalization, but
  empirical observations show D-SGD can outperform C-SGD in large-batch settings.
---

# Decentralized SGD and Average-direction SAM are Asymptotically Equivalent

## Quick Facts
- arXiv ID: 2306.02913
- Source URL: https://arxiv.org/abs/2306.02913
- Reference count: 40
- Key outcome: Proves D-SGD asymptotically minimizes average-direction SAM loss, providing regularization benefits in large-batch settings

## Executive Summary
This paper bridges the theoretical gap between decentralized and centralized SGD by proving that D-SGD asymptotically performs sharpness-aware minimization (SAM) through gradient diversity. While existing theories suggest decentralization harms generalization, empirical observations show D-SGD can outperform C-SGD in large-batch scenarios. The authors establish that the consensus distance in D-SGD acts as an implicit sharpness regularizer, providing three key advantages: free uncertainty evaluation, gradient smoothing, and batch-size-independent regularization. This theoretical framework explains why D-SGD consistently learns flatter minima than C-SGD in large-batch settings.

## Method Summary
The study compares D-SGD with C-SGD across CIFAR-10 and Tiny ImageNet datasets using AlexNet, ResNet-18, and DenseNet-121 architectures with 16 distributed workers. The method involves implementing D-SGD with ring, grid, and exponential communication topologies, training with varying batch sizes (1024, 8192), and analyzing validation accuracy. The theoretical analysis derives the asymptotic equivalence between D-SGD and average-direction SAM through Taylor expansion of gradient diversity terms, examining how consensus distance acts as sharpness regularization. Loss landscape visualization tools are used to assess minima flatness, and the relationship between spectral gap, batch size, and generalization is empirically validated.

## Key Results
- D-SGD asymptotically minimizes the loss function of an average-direction SAM algorithm
- Decentralization introduces gradient diversity that acts as implicit sharpness regularization
- The sharpness regularization effect of D-SGD does not decrease as total batch size increases
- Sparse communication topologies provide stronger regularization but slower convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: D-SGD asymptotically minimizes the loss function of an average-direction SAM algorithm, providing a regularization-optimization tradeoff.
- **Mechanism**: Decentralization introduces gradient diversity among local workers. This diversity induces a Hessian-consensus dependent noise that directs optimization toward flatter minima. The noise magnitude is controlled by the consensus distance (weight diversity matrix Ξ(t)), which acts as a sharpness regularizer.
- **Core assumption**: The gradient diversity term can be approximated using second-order Taylor expansion, and the residual terms become negligible as local models approach consensus.
- **Evidence anchors**:
  - [abstract] "We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-β-smooth settings."
  - [section 4.1] "D-SGD asymptotically minimizes the loss function of an average-direction sharpness-aware minimization algorithm with zero additional computation."
  - [corpus] Weak - no direct corpus evidence for this specific asymptotic equivalence claim.
- **Break condition**: If the spectral gap of the communication topology is too small, consensus distance becomes large, slowing optimization convergence and potentially preventing the asymptotic equivalence from holding.

### Mechanism 2
- **Claim**: Decentralization introduces a gradient smoothing effect that improves training stability.
- **Mechanism**: The noise introduced by decentralization makes the gradient Lipschitz continuous with a smaller constant. This smoothing effect stabilizes optimization, particularly in large-batch settings.
- **Core assumption**: The loss function Lw is α-Lipschitz continuous and its gradient is β-Lipschitz continuous, allowing the noise to reduce the effective Lipschitz constant of the smoothed gradient.
- **Evidence anchors**:
  - [section 4.3] "Decentralization can be interpreted as the injection of Gaussian noise into gradient. There arises a natural question that whether or not the noise introduced from decentralization, which is not necessarily isotropic, would smooth the gradient."
  - [section 4.3] "Corollary 2 implies that if the lower bound of noise magnitude satisfies σmin ≥ √2α/β, then the noise ϵ can make the Lipschitz constant of gradients smaller, therefore leading to gradient smoothing."
  - [corpus] Weak - no direct corpus evidence for this specific gradient smoothing claim.
- **Break condition**: If the smallest eigenvalue of Ξ(t) is below √2α/β, the noise may not sufficiently smooth the gradient, and the regularization effect could be insufficient.

### Mechanism 3
- **Claim**: The sharpness regularization effect of D-SGD does not decrease as total batch size increases, unlike C-SGD.
- **Mechanism**: Decentralization introduces additional noise that compensates for the reduced gradient variance in large-batch scenarios. This noise maintains the sharpness regularization effect even as batch size grows.
- **Core assumption**: The additional noise from decentralization remains effective even when the total batch size becomes very large, and the ratio κ = η·B·(N-B)/(N-1) does not dominate the regularization effect.
- **Evidence anchors**:
  - [abstract] "the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios."
  - [section 4.4] "Theorem 3 proves that the sharpness regularization terms in D-SGD do not decrease as the total batch size increases, unlike in C-SGD."
  - [corpus] Weak - no direct corpus evidence for this specific claim about batch size independence.
- **Break condition**: If the learning rate η is not scaled appropriately with batch size, or if the spectral gap becomes too small, the regularization effect may diminish as batch size increases.

## Foundational Learning

- **Concept**: Doubly Stochastic Matrix
  - Why needed here: Defines the communication topology in decentralized learning, ensuring proper information diffusion among workers.
  - Quick check question: What properties must a gossip matrix P satisfy to be considered doubly stochastic?

- **Concept**: Spectral Gap
  - Why needed here: Measures the connectivity of the communication topology, affecting both convergence speed and the magnitude of consensus distance.
  - Quick check question: How does the spectral gap of a gossip matrix relate to the sparsity of the communication topology?

- **Concept**: Sharpness-aware minimization (SAM)
  - Why needed here: Provides the theoretical foundation for understanding how D-SGD implicitly performs sharpness regularization through its gradient diversity.
  - Quick check question: How does the loss function of average-direction SAM differ from vanilla SAM?

## Architecture Onboarding

- **Component map**: Communication topology -> Local workers -> Consensus mechanism -> Sharpness regularization

- **Critical path**:
  1. Initialize local models and communication topology
  2. Compute local gradients on mini-batches
  3. Aggregate gradients and update local models
  4. Calculate weight diversity matrix Ξ(t)
  5. Use Ξ(t) to implicitly perform sharpness regularization

- **Design tradeoffs**:
  - Sparse vs. dense communication topologies: Sparse topologies provide stronger regularization but slower convergence
  - Batch size vs. regularization: Larger batches reduce gradient variance but decentralization compensates with additional noise
  - Learning rate scaling: Must balance between optimization speed and maintaining regularization effect

- **Failure signatures**:
  - Slow convergence: Indicates poor connectivity (small spectral gap) or excessive consensus distance
  - Poor generalization: Suggests insufficient sharpness regularization or improper learning rate scaling
  - Unstable training: May indicate gradient smoothing effect is insufficient or noise magnitude is inappropriate

- **First 3 experiments**:
  1. Compare D-SGD with C-SGD on simple convex problems to verify asymptotic equivalence
  2. Test gradient smoothing effect by varying spectral gap and measuring Lipschitz constant
  3. Evaluate sharpness regularization by training with different batch sizes and measuring generalization gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a general scaling strategy for learning rate as a function of batch size and spectral gap in decentralized deep learning to maintain generalizability in large-batch settings?
- Basis in paper: [explicit] The paper states that existing tricks for hyperparameter tuning are tailored for centralized SGD and asks if a general scaling strategy can be designed.
- Why unresolved: The paper conjectures that D-SGD could be more tolerant to hyperparameters like learning rate than C-SGD but does not provide a concrete scaling strategy.
- What evidence would resolve it: Experiments comparing different learning rate scaling strategies for D-SGD and C-SGD with varying batch sizes and spectral gaps.

### Open Question 2
- Question: Does continuously reducing the spectral gap improve the validation performance of D-SGD?
- Basis in paper: [inferred] The paper discusses the regularization-optimization trade-off in D-SGD and mentions that a sparse topology with a small spectral gap leads to a large consensus distance, which can hamper optimization.
- Why unresolved: The paper does not provide a definitive answer on whether continuously reducing the spectral gap would improve validation performance.
- What evidence would resolve it: Experiments showing the impact of varying spectral gaps on the validation performance of D-SGD.

### Open Question 3
- Question: Can we provide rigorous adversarial robustness guarantees for decentralized learning algorithms?
- Basis in paper: [explicit] The paper mentions that Cao et al. (2023) find that decentralized stochastic gradient algorithms are more adversarial robust to their centralized counterpart in certain scenarios and asks if rigorous adversarial robustness guarantees can be provided.
- Why unresolved: The paper does not provide any adversarial robustness guarantees for decentralized learning algorithms.
- What evidence would resolve it: Theoretical analysis and empirical results demonstrating the adversarial robustness of decentralized learning algorithms compared to centralized algorithms.

### Open Question 4
- Question: Does D-SGD share the properties of SAM beyond generalizability, such as better interpretability and transferability?
- Basis in paper: [explicit] The paper asks if D-SGD shares the properties of SAM, beyond generalizability, including better interpretability and transferability.
- Why unresolved: The paper does not provide any evidence on whether D-SGD shares the properties of SAM beyond generalizability.
- What evidence would resolve it: Experiments and theoretical analysis comparing the interpretability and transferability of D-SGD and SAM.

### Open Question 5
- Question: Can the insights gained from SAM be utilized to design more effective decentralized algorithms?
- Basis in paper: [explicit] The paper asks if the insights gained from SAM can be utilized to design more effective decentralized algorithms.
- Why unresolved: The paper does not provide any examples of using SAM insights to design more effective decentralized algorithms.
- What evidence would resolve it: Experiments and theoretical analysis showing the effectiveness of decentralized algorithms designed using SAM insights compared to existing decentralized algorithms.

## Limitations

- Theoretical claims rely heavily on asymptotic analysis that may not capture finite-time behavior in practical deep learning scenarios
- Gradient smoothing mechanism depends on specific conditions for noise magnitude that may not hold across all learning rates and batch sizes
- Equivalence between D-SGD and average-direction SAM assumes well-behaved loss landscapes that may not hold for complex neural network architectures

## Confidence

- **High confidence**: The sharpness regularization claim (Mechanism 1) is well-supported by the asymptotic analysis and aligns with established SAM theory.
- **Medium confidence**: The gradient smoothing effect (Mechanism 2) is theoretically sound but lacks empirical validation and depends on specific parameter conditions.
- **Medium confidence**: The batch size independence claim (Mechanism 3) follows logically from the asymptotic analysis but requires empirical verification across diverse scenarios.

## Next Checks

1. **Empirical validation of asymptotic regime**: Run experiments tracking the convergence of D-SGD models to centralized models as training progresses, measuring the consensus distance Ξ(t) and gradient diversity terms over time.
2. **Gradient smoothing verification**: Systematically vary the spectral gap of communication topologies and measure the effective Lipschitz constant of gradients, comparing against theoretical predictions.
3. **Batch size scaling experiments**: Train with exponentially increasing batch sizes (1024 → 8192 → 65536) while monitoring both training loss and validation accuracy to verify the sharpness regularization effect remains consistent.