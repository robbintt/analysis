---
ver: rpa2
title: A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding
arxiv_id: '2305.03668'
source_url: https://arxiv.org/abs/2305.03668
tags:
- section
- page
- image
- text
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces WikiWeb2M, a dataset of 2M Wikipedia pages\
  \ containing all text, images, and structure data. It proposes three generative\
  \ tasks\u2014page description generation, section summarization, and contextual\
  \ image captioning\u2014to evaluate multimodal webpage understanding."
---

# A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding

## Quick Facts
- arXiv ID: 2305.03668
- Source URL: https://arxiv.org/abs/2305.03668
- Reference count: 31
- Primary result: Introduces WikiWeb2M dataset and three generative tasks for multimodal webpage understanding, with Prefix Global attention outperforming full attention at longer sequences

## Executive Summary
This paper introduces WikiWeb2M, a dataset of 2M Wikipedia pages with text, images, and structure data, and proposes three generative tasks to evaluate multimodal webpage understanding: page description generation, section summarization, and contextual image captioning. The authors design a novel attention mechanism called Prefix Global that uses page structure to select the most relevant tokens as global tokens, achieving better performance than full attention with lower computational complexity. Experiments show that images consistently improve performance across all tasks, and the WikiWeb2M data provides additional gains compared to prior datasets.

## Method Summary
The authors utilize the T5 encoder-decoder framework with a ViT image encoder for modeling WikiWeb2M tasks. They implement a novel Prefix Global attention mechanism that selects the most salient text and images as global tokens in the prefix of the input sequence. The model is trained and evaluated on task-specific datasets created from WikiWeb2M, using BLEU-4, ROUGE-L, and CIDEr scores for evaluation. The minimum viable reproduction plan involves downloading the WikiWeb2M dataset, preparing input sequences according to task specifications, and training the T5 model with Prefix Global attention for 218 steps using Adafactor optimizer, constant learning rate 1e-3, and batch size 128.

## Key Results
- Prefix Global attention outperforms full attention, especially at longer sequence lengths (1K-2K tokens)
- Images consistently improve performance across all three webpage understanding tasks
- WikiWeb2M data provides additional gains compared to prior datasets
- Prefix Global reduces FLOPs by roughly half compared to full attention at 2K tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix Global attention improves performance by designating a fixed set of global tokens that attend to all other tokens, allowing the model to prioritize salient content.
- Mechanism: Prefix Global defines the most relevant image and text content (e.g., target section's first sentences, images, and captions) as global tokens in the prefix of the input sequence. These global tokens attend to all other tokens, while the remaining tokens have local attention. This allows the model to focus on the most important information for the task.
- Core assumption: Separating out the most relevant inputs via structure or known biases like leading sentence bias is beneficial for webpage understanding tasks.
- Evidence anchors:
  - [abstract]: "We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context."
  - [section]: "While the full attention in T5 is performant, it results in a quadratic computational complexity with respect to the input sequence length and does not make use of the structured webpage content. We define a new mixture of local-global attention, Prefix Global, which uses our structured data to select the most salient text and images as global tokens in the prefix of our input sequence."
- Break condition: If the assumption that earlier content in a body of text is more important does not hold for the specific webpage understanding tasks.

### Mechanism 2
- Claim: Longer input sequences improve performance for Prefix Global, even when the number of global tokens is fixed.
- Mechanism: Prefix Global can handle longer input sequences than full attention due to its lower computational complexity. The global tokens can attend to all other tokens, allowing the model to use the additional context from the longer sequence. This is especially beneficial for tasks like page description generation where the entire page's content is relevant.
- Core assumption: The additional context from longer input sequences is beneficial for webpage understanding tasks.
- Evidence anchors:
  - [abstract]: "Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work."
  - [section]: "We utilize the T5 (Raffel et al., 2020) framework for modeling WikiWeb2M tasks. While the full attention in T5 is performant, it results in a quadratic computational complexity with respect to the input sequence length and does not make use of the structured webpage content."
- Break condition: If the additional context from longer input sequences is not beneficial for the specific webpage understanding tasks, or if the computational complexity becomes too high.

### Mechanism 3
- Claim: Including images improves performance for all three webpage understanding tasks.
- Mechanism: The Prefix Global attention mechanism allows the model to attend to images as global tokens, giving them the same importance as text content. This is especially important for tasks like contextual image captioning where the image is the primary focus. Even for tasks like section summarization and page description generation, the images provide additional context that improves performance.
- Core assumption: Images are a valuable source of information for webpage understanding tasks.
- Evidence anchors:
  - [abstract]: "Experiments show that the new annotations from WikiWeb2M improve task performance compared to data from prior work."
  - [section]: "We utilize the T5 (Raffel et al., 2020) framework for modeling WikiWeb2M tasks. While the full attention in T5 is performant, it results in a quadratic computational complexity with respect to the input sequence length and does not make use of the structured webpage content."
- Break condition: If the images do not provide relevant information for the specific webpage understanding tasks, or if the computational cost of processing the images outweighs the benefits.

## Foundational Learning

- Concept: Transformers and attention mechanisms
  - Why needed here: The paper uses a Transformer-based model (T5) with a custom attention mechanism (Prefix Global) to process multimodal webpage data.
  - Quick check question: What is the difference between full attention and local-global attention in Transformers?

- Concept: Multimodal learning
  - Why needed here: The paper combines text and image data to understand webpages, requiring knowledge of how to process and integrate different modalities.
  - Quick check question: What are some common approaches for integrating text and image data in multimodal models?

- Concept: Webpage structure and HTML
  - Why needed here: The paper leverages the structured nature of webpages to select relevant content for the Prefix Global attention mechanism.
  - Quick check question: What are some common HTML elements used to structure webpage content?

## Architecture Onboarding

- Component map: T5 encoder-decoder model with ViT image encoder -> Prefix Global attention mechanism -> WikiWeb2M dataset with multimodal webpage data -> Evaluation metrics (BLEU, ROUGE, CIDEr)

- Critical path:
  1. Preprocess WikiWeb2M data into task-specific formats
  2. Initialize T5 and ViT models with appropriate checkpoints
  3. Implement Prefix Global attention mechanism
  4. Train and evaluate model on task-specific datasets
  5. Analyze results and iterate on model architecture or data preprocessing

- Design tradeoffs:
  - Prefix Global vs. full attention: Prefix Global has lower computational complexity but may miss some long-range dependencies.
  - Number of global tokens: More global tokens allow for more salient content but increase computational cost.
  - Input sequence length: Longer sequences provide more context but increase computational cost and may require more memory.

- Failure signatures:
  - Low performance on specific tasks: May indicate issues with data preprocessing, model initialization, or task formulation.
  - High computational cost: May indicate issues with attention mechanism or input sequence length.
  - Overfitting or underfitting: May indicate issues with model architecture, regularization, or data size.

- First 3 experiments:
  1. Train and evaluate baseline T5 model with full attention on WikiWeb2M data to establish performance benchmarks.
  2. Implement and evaluate Prefix Global attention mechanism with fixed number of global tokens on WikiWeb2M data to compare with baseline.
  3. Vary the number of global tokens and input sequence length for Prefix Global attention to find optimal configuration for each task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of global tokens for Prefix Global attention across different tasks?
- Basis in paper: [explicit] The paper mentions that for page description and section summarization, using 512 global tokens performs well, while for image captioning, using 256 global tokens outperforms 512. However, the authors did not exhaustively ablate the number of global tokens.
- Why unresolved: The paper does not provide a comprehensive study on how the optimal number of global tokens varies across different tasks and sequence lengths. This could impact the performance and efficiency of the model.
- What evidence would resolve it: A systematic study varying the number of global tokens for each task and sequence length, reporting performance metrics like BLEU-4, ROUGE-L, and CIDEr scores.

### Open Question 2
- Question: How does the performance of Prefix Global compare to other local-global attention mechanisms like Longformer or BigBird on multimodal webpage understanding tasks?
- Basis in paper: [inferred] The paper compares Prefix Global to full attention and TGlobal, but does not compare it to other local-global attention mechanisms like Longformer or BigBird.
- Why unresolved: It is unclear how Prefix Global performs relative to other established local-global attention mechanisms on the proposed tasks. This could impact the choice of attention mechanism for future work.
- What evidence would resolve it: Experiments comparing Prefix Global to Longformer and BigBird on the same tasks, using the same model sizes and training setups.

### Open Question 3
- Question: What is the impact of using different pretrained checkpoints (e.g., T5, ViT) on the performance of the models on the proposed tasks?
- Basis in paper: [explicit] The paper performs ablations on pretrained checkpoints, comparing ViT models pretrained on ImageNet vs. JFT, and T5 models of different sizes. However, it does not explore other pretrained checkpoints or architectures.
- Why unresolved: The choice of pretrained checkpoint can significantly impact the performance of downstream tasks. It is unclear how other checkpoints like BERT, RoBERTa, or CLIP would perform on these tasks.
- What evidence would resolve it: Experiments comparing different pretrained checkpoints (e.g., BERT, RoBERTa, CLIP) on the same tasks, using the same model sizes and training setups.

### Open Question 4
- Question: How does the performance of the models on the proposed tasks generalize to other domains beyond Wikipedia webpages?
- Basis in paper: [inferred] The paper focuses on Wikipedia webpages and does not evaluate the models on other domains like news articles, instructional websites, or mobile apps.
- Why unresolved: The proposed tasks and models may not generalize well to other domains with different structures and content. It is unclear how the models would perform on other types of webpages or multimodal documents.
- What evidence would resolve it: Experiments evaluating the models on other domains like news articles, instructional websites, or mobile apps, using the same tasks and metrics.

### Open Question 5
- Question: What is the impact of using different image features (e.g., ResNet, CLIP) on the performance of the models on the proposed tasks?
- Basis in paper: [explicit] The paper uses ViT features for images and does not explore other image feature extractors like ResNet or CLIP.
- Why unresolved: The choice of image feature extractor can impact the performance of multimodal models. It is unclear how other feature extractors like ResNet or CLIP would perform on these tasks.
- What evidence would resolve it: Experiments comparing different image feature extractors (e.g., ResNet, CLIP) on the same tasks, using the same model sizes and training setups.

## Limitations

- Dataset-specific performance claims: The performance improvements attributed to WikiWeb2M data versus prior datasets are not independently verified through ablation studies comparing different dataset sizes or compositions.
- Computational complexity analysis: The analysis does not account for the additional overhead of image processing through the ViT encoder, and actual wall-clock time and memory usage differences are not reported.
- Generalizability of attention mechanism: The Prefix Global approach relies on the assumption that leading content in webpages is most salient, which may not generalize to webpages with different structural conventions or domains outside Wikipedia.

## Confidence

- High Confidence: The experimental results showing Prefix Global outperforms other attention schemes at longer sequence lengths (1K-2K tokens) are well-supported by the data.
- Medium Confidence: The claim that WikiWeb2M data provides additional gains compared to prior datasets is supported by the experimental results, but the lack of ablation studies on dataset composition reduces confidence in the magnitude of this improvement.
- Low Confidence: The mechanism explanation for why Prefix Global works relies heavily on assumptions about webpage structure and content importance that are not rigorously tested.

## Next Checks

1. **Dataset ablation study**: Create controlled experiments comparing performance using only text from WikiWeb2M versus using text from multiple sources, while keeping the model architecture constant.

2. **Cross-domain generalization test**: Evaluate the Prefix Global model on webpages from different domains (news articles, academic papers, e-commerce pages) to assess whether the attention mechanism generalizes beyond Wikipedia's structural conventions.

3. **Comprehensive computational analysis**: Measure actual wall-clock training and inference times, memory usage, and throughput for full attention versus Prefix Global across different sequence lengths and batch sizes, including the overhead from image processing.