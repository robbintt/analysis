---
ver: rpa2
title: The Expressive Power of Tuning Only the Normalization Layers
arxiv_id: '2302.07937'
source_url: https://arxiv.org/abs/2302.07937
tags:
- matrix
- layers
- then
- network
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the expressive power of training only normalization
  layers (e.g., BatchNorm, LayerNorm) of a neural network, while keeping all other
  weights frozen. The authors show that for random ReLU networks, tuning only the
  normalization layers can functionally reconstruct any target network that is $O(\sqrt{\text{width}})$
  times smaller, even when the random network is sparsely connected.
---

# The Expressive Power of Tuning Only the Normalization Layers

## Quick Facts
- arXiv ID: 2302.07937
- Source URL: https://arxiv.org/abs/2302.07937
- Reference count: 40
- Key outcome: Training only normalization layers can functionally reconstruct any target network that is O(√width) times smaller, even for sparsely connected random networks.

## Executive Summary
This work investigates the expressive power of training only normalization layers (e.g., BatchNorm, LayerNorm) while keeping all other weights frozen in a neural network. The authors prove that for random ReLU networks, tuning only normalization parameters can functionally reconstruct any target network that is O(√width) times smaller. This surprising result is achieved through leveraging the invertibility of Khatri-Rao products of random matrices. The findings are supported by theoretical proofs for wide, deep, and sparse network cases, along with numerical experiments demonstrating the reconstruction capability.

## Method Summary
The paper theoretically proves that tuning only normalization layers can reconstruct smaller target networks by leveraging the invertibility of Khatri-Rao products of random matrices. For wide networks, the method uses frozen random weights with trainable normalization parameters to exactly match smaller target networks. For deep networks, skip connections allow width to be traded for depth. For sparse networks, the authors show that with appropriate sparsity probability, reconstruction remains possible. The proofs involve setting normalization parameters to linearize ReLU activations and solving for these parameters using the pseudo-inverse of Khatri-Rao products.

## Key Results
- Training only normalization layers can reconstruct any target network that is O(√width) times smaller
- Skip connections enable depth to compensate for reduced width in reconstruction
- Random sparsification with probability p = Θ(√log d/d) retains expressive power while reducing parameters
- Theoretical proofs rely on the invertibility of Khatri-Rao products of random matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training only normalization parameters can functionally reconstruct any target network that is O(√width) times smaller.
- Mechanism: The scaling and shifting transformations of normalization layers, when applied to random features from a frozen network, can approximate any smaller target network through the invertibility of Khatri-Rao products of random matrices.
- Core assumption: Random matrices with continuous distributions have full rank Khatri-Rao products with probability 1.
- Evidence anchors:
  - [abstract]: "tuning only the normalization layers can functionally reconstruct any target network that is O(√width) times smaller"
  - [section]: "The proofs of the theorems rely crucially on the invertibility of Khatri-Rao products of random (possibly sparse) matrices"
  - [corpus]: Weak evidence - no direct support for this specific claim in related papers
- Break condition: If the random matrices are not full rank or if the target network is too large relative to the width of the random network.

### Mechanism 2
- Claim: Skip connections allow depth to compensate for reduced width in reconstruction.
- Mechanism: By partitioning the input and using skip connections, each layer of the target network can be reconstructed by a sequence of layers with width dk, where k is the partition size, effectively trading width for depth.
- Core assumption: The target network's weight matrix can be decomposed into blocks that correspond to the partitioned inputs.
- Evidence anchors:
  - [abstract]: "if the random network is sufficiently wide (or deep with skip connections)"
  - [section]: "if skip connections are allowed, we can reconstruct the target network by a much narrower yet deeper frozen random network"
  - [corpus]: Weak evidence - no direct support for this specific claim in related papers
- Break condition: If the partition size k is too small relative to the input dimension d, or if the target network has dependencies that cannot be captured by the partitioned approach.

### Mechanism 3
- Claim: Random sparsification of weight matrices can retain expressive power with fewer parameters.
- Mechanism: By sparsifying the random weight matrices with a probability p = Θ(√log d/d), the Khatri-Rao product remains invertible with high probability, reducing the total number of parameters while maintaining reconstruction capability.
- Core assumption: The invertibility of the Khatri-Rao product depends primarily on the sparsity pattern rather than the specific values of the non-zero elements.
- Evidence anchors:
  - [abstract]: "This is achieved by leveraging the invertibility of Khatri-Rao products of random matrices"
  - [section]: "consider that the random matrices of each of the layers are sparsified with probability p = Θ(√log d/d)"
  - [corpus]: Weak evidence - no direct support for this specific claim in related papers
- Break condition: If the sparsity is too high (p too small) or if the depth is not polynomial in the input dimension.

## Foundational Learning

- Concept: Khatri-Rao product and its properties
  - Why needed here: The proof of the main theorems relies on the invertibility of Khatri-Rao products of random matrices.
  - Quick check question: What is the relationship between the rank of a Khatri-Rao product and the ranks of its constituent matrices?

- Concept: ReLU activation function and its properties
  - Why needed here: The random network uses ReLU activations, and the proof involves linearizing these activations by setting the bias parameters appropriately.
  - Quick check question: How can you ensure that all ReLU activations are "on" in a given layer?

- Concept: Skip connections and their impact on network architecture
  - Why needed here: The second theorem shows that skip connections allow for depth to compensate for reduced width in the reconstruction.
  - Quick check question: How does the use of skip connections affect the flow of information through the network?

## Architecture Onboarding

- Component map:
  - Random weight matrices (frozen) -> Normalization layers (BatchNorm/LayerNorm) with trainable scaling and shifting parameters -> ReLU activation functions -> Skip connections (for depth-over-width reconstruction)

- Critical path:
  1. Initialize random weight matrices and normalization layers
  2. Set normalization layer parameters to linearize ReLU activations
  3. Use Khatri-Rao product invertibility to solve for normalization parameters that reconstruct the target network

- Design tradeoffs:
  - Width vs. depth: Increasing width allows for shallower networks, while increasing depth with skip connections allows for narrower networks.
  - Sparsity vs. parameter count: Sparsifying weight matrices reduces the total number of parameters but may require a larger width to maintain expressive power.

- Failure signatures:
  - The Khatri-Rao product of the random matrices is not invertible, indicating that the random network cannot reconstruct the target network.
  - The normalization layer parameters cannot be set to linearize the ReLU activations, indicating a problem with the initialization or the choice of parameters.

- First 3 experiments:
  1. Verify that a randomly initialized network with trainable normalization layers can reconstruct a small target network (e.g., one layer with width 5).
  2. Test the impact of skip connections on the width-depth tradeoff by varying the partition size k and measuring the required width for reconstruction.
  3. Measure the effect of sparsifying the random weight matrices on the reconstruction error and the total number of parameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sparsity level for random matrices in normalization-only training to balance expressive power and parameter efficiency?
- Basis in paper: [explicit] Theorem 3 states that with sparsity probability p = Θ(√log d/d), the network can still functionally reconstruct the target, but the authors conjecture this bound can be improved to p = Θ(log d/d).
- Why unresolved: The current bound requires polynomial depth (l = poly(d)) and the authors acknowledge their proof technique may not be tight. The exact threshold where expressive power breaks down with increasing sparsity remains unknown.
- What evidence would resolve it: Numerical experiments varying sparsity levels systematically while measuring reconstruction accuracy, combined with tighter theoretical bounds on the singularity probability of Khatri-Rao products of sparse random matrices.

### Open Question 2
- Question: Can the width-overparameterization requirement of O(d) be reduced when training only normalization layers?
- Basis in paper: [inferred] The current construction requires width d' = O(d²) for exact reconstruction, which seems inefficient compared to training all parameters. The authors mention this as a limitation.
- Why unresolved: The proof relies on constructing matrices that need to be invertible via Khatri-Rao products, which inherently requires this overparameterization. Alternative constructions that achieve the same expressiveness with fewer parameters remain unknown.
- What evidence would resolve it: Lower bounds proving that O(d) width is necessary for exact reconstruction, or new constructive algorithms achieving the same expressiveness with width closer to d.

### Open Question 3
- Question: How well do gradient-based optimization methods (like SGD) leverage the full expressive power of normalization-only training compared to the constructive algorithms?
- Basis in paper: [explicit] The experiments show that the constructive algorithms outperform SGD baseline, but the paper doesn't explore why SGD fails to find the optimal normalization parameters.
- Why unresolved: The authors only compare SGD to their constructive methods without analyzing the optimization landscape or exploring alternative optimization strategies for normalization parameters.
- What evidence would resolve it: Analysis of the loss landscape for normalization-only training, experiments with alternative optimization methods (e.g., second-order methods), and investigation of initialization strategies that help SGD find better solutions.

## Limitations
- The theoretical results depend critically on continuous random weight distributions and may not hold for practical finite-width networks with discrete initializations
- Numerical experiments are limited to small-scale demonstrations and don't fully explore scaling behavior
- The sparsity results rely on specific sparsity patterns and may not generalize to all network architectures

## Confidence
- **High Confidence**: The theoretical framework for wide network reconstruction is sound, with proofs relying on well-established properties of random matrices and Khatri-Rao products
- **Medium Confidence**: The extension to deep networks with skip connections is plausible but depends on the ability to partition the input and the target network's weights appropriately
- **Low Confidence**: The claim about sparse network reconstruction is the least certain, as it relies on specific sparsity patterns and may not hold for all network architectures or sparsity levels

## Next Checks
1. **Scalability Test**: Verify the theoretical predictions by scaling up the numerical experiments to larger networks and different depths, particularly focusing on the depth-over-width tradeoff with skip connections
2. **Initialization Robustness**: Assess the impact of different weight initialization schemes (e.g., Gaussian vs. uniform) on the invertibility of the Khatri-Rao products and the overall reconstruction capability
3. **Sparse Network Validation**: Conduct a thorough analysis of the sparse network case, varying the sparsity level and measuring the reconstruction error, to determine the practical limits of the proposed method