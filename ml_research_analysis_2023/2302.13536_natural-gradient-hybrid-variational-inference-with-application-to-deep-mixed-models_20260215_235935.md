---
ver: rpa2
title: Natural Gradient Hybrid Variational Inference with Application to Deep Mixed
  Models
arxiv_id: '2302.13536'
source_url: https://arxiv.org/abs/2302.13536
tags:
- gradient
- natural
- variational
- matrix
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fast and accurate variational inference method
  for models with global parameters and latent variables by combining hybrid VI with
  natural gradient optimization. The key innovation is using the natural gradient
  for the global parameters while generating latent variables from their conditional
  posterior, resulting in a scalable algorithm where computational cost depends only
  on the dimension of global parameters.
---

# Natural Gradient Hybrid Variational Inference with Application to Deep Mixed Models

## Quick Facts
- arXiv ID: 2302.13536
- Source URL: https://arxiv.org/abs/2302.13536
- Authors: 
- Reference count: 40
- Key outcome: Proposes a fast and accurate variational inference method combining hybrid VI with natural gradient optimization for models with global parameters and latent variables.

## Executive Summary
This paper introduces a natural gradient hybrid variational inference (NG-HVI) method that combines hybrid variational inference with natural gradient optimization for models containing global parameters and latent variables. The key innovation is applying natural gradients to update global parameters while generating latent variables from their conditional posterior, resulting in a scalable algorithm where computational cost depends only on the dimension of global parameters. Applied to deep mixed models for asset pricing, the approach demonstrates substantial improvements in predictive accuracy compared to traditional models while maintaining computational efficiency.

## Method Summary
The method employs a variational approximation with Gaussian factor covariance matrix for the marginal of global parameters θ and the exact conditional posterior of latent variables z|θ. Natural gradient updates are computed using a Tikhonov damped Fisher information matrix, with the re-parameterization trick ensuring stable updates. At each iteration, global parameters are updated using the natural gradient, and latent variables are generated from their conditional posterior. The approach leverages the equality between the ELBO of the joint posterior and the marginal posterior of θ with z integrated out exactly.

## Key Results
- NG-HVI outperforms ordinary gradient hybrid VI and existing natural gradient methods in both speed and accuracy
- Deep mixed models using NG-HVI show substantial improvements in predictive accuracy compared to traditional models
- Computational cost depends only on the dimension of global parameters, not latent variables
- Tikhonov damping with δ = 10 provides strong results, though tuning may further improve performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using natural gradient for global parameters reduces latent variable draws needed for convergence.
- **Mechanism**: Natural gradient follows steepest direction on information geometry manifold, allowing faster convergence of variational parameters for global parameters, thus requiring fewer latent variable draws.
- **Core assumption**: Fisher information matrix for marginal approximation of global parameters is tractable and efficiently computable.
- **Evidence anchors**: Abstract and section 3.1 establish theoretical framework.
- **Break condition**: If FIM becomes singular or computational cost of inversion outweighs savings.

### Mechanism 2
- **Claim**: Hybrid VI framework allows exact integration of latent variables, avoiding cumulative approximation error.
- **Mechanism**: Variational approximation q(ψ) = p(z|θ, y)q₀(θ) means ELBO for joint posterior equals ELBO for marginal posterior of θ with z integrated out exactly.
- **Core assumption**: Conditional posterior p(z|θ, y) can be sampled from exactly or approximately.
- **Evidence anchors**: Abstract and section 2.3 describe the mathematical framework.
- **Break condition**: If sampling from p(z|θ, y) becomes computationally infeasible.

### Mechanism 3
- **Claim**: Tikhonov damping combined with adaptive learning rates stabilizes natural gradient updates in high-curvature regions.
- **Mechanism**: Damping adds scaled identity matrix to FIM, moving it away from unstable values while taking into account scale; adaptive learning rates improve convergence.
- **Core assumption**: Damped FIM is positive definite and invertible; adaptive learning rate method is effective.
- **Evidence anchors**: Section 3.2 describes damping implementation and ADADELTA usage.
- **Break condition**: If damping parameter is not properly tuned or adaptive method doesn't converge.

## Foundational Learning

- **Concept**: Fisher Information Matrix (FIM)
  - Why needed here: Central to computing natural gradient, the key innovation; captures curvature of log-likelihood and preconditions gradient updates.
  - Quick check question: How is FIM defined for variational approximation q(θ) in terms of expectations over q(θ)?

- **Concept**: Variational Inference (VI) and Evidence Lower Bound (ELBO)
  - Why needed here: Method built on VI framework that approximates posterior by minimizing KL divergence, equivalent to maximizing ELBO.
  - Quick check question: What is relationship between KL divergence and ELBO in variational inference?

- **Concept**: Natural Gradient vs. Ordinary Gradient
  - Why needed here: Method replaces ordinary gradients with natural gradients for updating global parameters; understanding difference is key to method's efficiency.
  - Quick check question: How does natural gradient differ from ordinary gradient in terms of direction in parameter space?

## Architecture Onboarding

- **Component map**: Data y, model structure (θ, z) -> Variational Approximation q(ψ) = p(z|θ, y)q₀(θ) -> Optimization (Natural gradient for θ, sampling for z) -> Fitted variational parameters λ, predictive distributions

- **Critical path**:
  1. Initialize λ
  2. Generate θ(t) ~ q₀(θ)
  3. Generate z(t) ~ p(z|θ(t), y)
  4. Compute natural gradient using Tikhonov damped FIM
  5. Update λ(t+1) = λ(t) + ρ(t) ◦ ∇λL(λ(t))
  6. Repeat until convergence

- **Design tradeoffs**:
  - Natural gradient vs. ordinary gradient: Faster convergence but higher per-step cost due to FIM computation
  - Exact vs. approximate sampling from p(z|θ, y): Exact is more accurate but may be slower; approximate is faster but introduces error
  - Tikhonov damping parameter δ: Larger δ increases stability but may slow convergence; smaller δ risks instability

- **Failure signatures**:
  - Slow convergence or divergence: Check if FIM is singular or damping parameter too small
  - Poor predictive accuracy: Check if sampling from p(z|θ, y) is adequate or variational approximation is misspecified
  - High computational cost: Check if FIM inversion is bottleneck or latent variable dimension is too high

- **First 3 experiments**:
  1. Implement linear random effects model with natural gradient and ordinary gradient updates; compare convergence speed and accuracy
  2. Test effect of different Tikhonov damping parameters (δ) on convergence and stability
  3. Compare predictive accuracy of fitted model on held-out data using different architectures (e.g., one vs. three hidden layers in DMM)

## Open Questions the Paper Calls Out

- **Question**: How does choice of damping factor δ affect convergence and stability of NG-HVI method, and what is optimal value for different model types?
  - Basis in paper: Paper mentions δ = 10 produces strong results but suggests performance may improve by tuning δ as in George et al. (2018) and Osawa et al. (2019)
  - Why unresolved: Paper doesn't provide systematic study of different δ values across various model types
  - What evidence would resolve it: Empirical studies comparing performance with different δ values across range of models

- **Question**: How does performance of NG-HVI method compare to other state-of-the-art VI methods for high-dimensional latent variables, such as normalizing flows?
  - Basis in paper: Paper compares to benchmark natural gradient VI methods but doesn't compare to other advanced VI methods
  - Why unresolved: Paper focuses on specific benchmarks but doesn't explore broader landscape of advanced VI techniques
  - What evidence would resolve it: Systematic empirical comparisons with normalizing flows, implicit copula variational approximations, and other cutting-edge techniques

- **Question**: What are theoretical guarantees on convergence rate and accuracy of NG-HVI method, and how do they compare to other VI methods?
  - Basis in paper: Paper demonstrates effectiveness through empirical studies but doesn't provide theoretical analysis
  - Why unresolved: Paper focuses on empirical performance without addressing theoretical underpinnings
  - What evidence would resolve it: Rigorous theoretical analysis of convergence rate and accuracy, including comparison to other VI methods

## Limitations

- The computational cost analysis assumes Fisher information matrix is tractable and invertible, which may not hold for all model structures
- Specific architectural choices for deep mixed model (number of hidden layers, activation functions) and their impact on predictive accuracy are not thoroughly explored
- The claim that natural gradient significantly reduces latent variable draws needed for convergence is supported by theory but not rigorously tested against alternatives

## Confidence

- **High confidence**: Theoretical framework of hybrid VI and integration of latent variables is sound and well-established
- **Medium confidence**: Implementation of natural gradient updates with Tikhonov damping is technically correct but practical benefits require more extensive validation
- **Low confidence**: Specific architectural choices for deep mixed model and their impact on predictive accuracy are not thoroughly explored

## Next Checks

1. **Robustness to FIM singularity**: Test method on models where Fisher information matrix becomes near-singular, quantifying how different damping parameters affect convergence and stability

2. **Comparison of sampling methods**: Implement and compare method using exact sampling from p(z|θ, y) versus approximate methods (e.g., MCMC), measuring trade-off between accuracy and computational cost

3. **Architectural sensitivity analysis**: Systematically vary deep mixed model architecture (number of layers, activation functions) and evaluate impact on predictive accuracy and computational efficiency