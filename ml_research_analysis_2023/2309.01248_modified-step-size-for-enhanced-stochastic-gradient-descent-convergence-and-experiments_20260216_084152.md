---
ver: rpa2
title: 'Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and
  Experiments'
arxiv_id: '2309.01248'
source_url: https://arxiv.org/abs/2309.01248
tags:
- step
- size
- convergence
- stochastic
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitations of the standard 1/\u221A\
  t step size in stochastic gradient descent (SGD), particularly its inability to\
  \ adequately decrease in later iterations for deep neural networks. The authors\
  \ propose a novel modified step size that incorporates a logarithmic term, ln(t),\
  \ alongside the traditional 1/\u221At term."
---

# Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and Experiments

## Quick Facts
- **arXiv ID:** 2309.01248
- **Source URL:** https://arxiv.org/abs/2309.01248
- **Reference count:** 28
- **Primary result:** Proposed modified step size η_t = η_0/(√t + ln t) improves convergence rate to O(ln(T)/√T) and increases classification accuracy by 0.5-1.4% over standard 1/√t step size

## Executive Summary
This paper addresses the limitation of standard 1/√t step sizes in later optimization stages by introducing a modified step size that incorporates both 1/√t and ln(t) terms. The new formulation provides a more controlled decrease in step size throughout training, particularly beneficial for deep neural network optimization. The authors establish a convergence rate of O(ln(T)/√T) for smooth non-convex functions without requiring the Polyak-Łojasiewicz condition, and demonstrate practical improvements through image classification experiments on FashionMNIST (0.5% accuracy gain) and CIFAR10 (1.4% accuracy gain).

## Method Summary
The proposed method modifies the standard SGD step size by adding a logarithmic term to create η_t = η_0/(√t + ln t). This modification ensures the step size decreases more rapidly in later iterations compared to 1/√t alone. The method is combined with a warm restart algorithm that periodically resets the step size. Implementation is available in a public GitHub repository with scripts for both image classification (FashionMNIST, CIFAR10) and binary classification (a1a, a2a, mushrooms, rcv1, w1a) tasks.

## Key Results
- Convergence rate of O(ln(T)/√T) established for smooth non-convex functions without PL condition
- 0.5% accuracy improvement on FashionMNIST dataset compared to standard 1/√t step size
- 1.4% accuracy improvement on CIFAR10 dataset
- Consistent improvements in accuracy and loss across multiple binary classification datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new step size η_t = η_0/(√t + ln t) improves convergence by decreasing more rapidly in later iterations than 1/√t alone
- Mechanism: The addition of ln t to the denominator makes it grow faster, forcing the step size to shrink more quickly in later iterations
- Core assumption: ln t < √t for all t ≥ 1, ensuring the new step size is always smaller than 1/√t
- Evidence anchors:
  - Abstract: The proposed step size integrates a logarithmic term, leading to the selection of smaller values in the final iterations
  - Section 2.2: The motivation behind incorporating the ln t function lies in its gradual growth pattern
  - Corpus: Found 25 related papers with average neighbor FMR=0.474
- Break condition: If f is not smooth (violating Assumption A1), convergence guarantees may not hold

### Mechanism 2
- Claim: The convergence rate of O(ln T/√T) is maintained for smooth non-convex functions without the PL condition
- Mechanism: Analysis leverages smoothness of f (Assumption A1) and bounded variance of stochastic gradients (Assumption A2)
- Core assumption: f is L-smooth and stochastic gradient has bounded variance
- Evidence anchors:
  - Section 3: We demonstrate the convergence rate of O(ln T/√T) for smooth non-convex functions without the PL condition
  - Section 2.1: Assumption A1 and A2 on smoothness and bounded variance
- Break condition: If f violates L-smoothness condition or variance bound on stochastic gradients

### Mechanism 3
- Claim: The warm restart algorithm with the new step size improves accuracy on image classification tasks
- Mechanism: Warm restart technique, combined with modified step size, allows algorithm to escape local minima
- Core assumption: Warm restart algorithm is effective in improving optimization performance for deep neural networks
- Evidence anchors:
  - Abstract: Numerical experiments demonstrated effectiveness with 0.5% and 1.4% accuracy improvements
  - Section 2.3: Warm restart Algorithm 1 employed with same number of epochs in inner loop
- Break condition: If warm restart schedule is not properly tuned, algorithm may not achieve expected improvements

## Foundational Learning

- **Concept: Smoothness of functions**
  - Why needed here: Convergence analysis relies on L-smoothness of objective function f to bound error terms
  - Quick check question: What is the definition of an L-smooth function, and how does it relate to convergence of gradient-based methods?

- **Concept: Stochastic gradient descent (SGD)**
  - Why needed here: Proposed step size is specifically designed to enhance performance of SGD in training deep neural networks
  - Quick check question: How does choice of step size affect convergence behavior of SGD, and what are common strategies for selecting step sizes?

- **Concept: Polyak-Łojasiewicz (PL) condition**
  - Why needed here: Paper establishes convergence rate without requiring PL condition, a common assumption in non-convex optimization
  - Quick check question: What is the PL condition, and how does it simplify convergence analysis of non-convex optimization algorithms?

## Architecture Onboarding

- **Component map:** Input (η₀, x₀, l, T) -> Modified step size calculation (η_t = η₀/(√t + ln t)) -> SGD update -> Warm restart -> Output (x, convergence rate, accuracy metrics)

- **Critical path:**
  1. Initialize parameters and hyperparameters
  2. For each outer iteration:
     - For each inner iteration:
       - Calculate modified step size
       - Perform SGD update using modified step size
     - Reset step size at beginning of each outer iteration
  3. Return optimized parameters and performance metrics

- **Design tradeoffs:**
  - Modified step size provides better convergence but may require more computational overhead due to logarithmic term
  - Warm restart technique can improve accuracy but may increase number of iterations required for convergence

- **Failure signatures:**
  - Step size decreases too rapidly, algorithm gets stuck in local minima
  - Improperly tuned warm restart schedule fails to achieve expected improvements

- **First 3 experiments:**
  1. Implement modified step size and compare convergence behavior with 1/√t on simple convex function
  2. Apply modified step size to non-convex function and verify O(ln T/√T) convergence rate
  3. Integrate warm restart mechanism and evaluate impact on FashionMNIST and CIFAR10 accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several natural questions emerge:

### Open Question 1
- Question: How does the proposed modified step size perform on other types of deep learning architectures beyond CNNs and ResNets, such as Transformers or LSTMs?
- Basis in paper: Paper only tests modified step size on CNNs and ResNets for image classification tasks
- Why unresolved: Authors did not explore effectiveness on other popular deep learning architectures used for different data types
- What evidence would resolve it: Empirical results comparing performance across various deep learning architectures for tasks beyond image classification

### Open Question 2
- Question: What is the theoretical justification for choice of logarithmic term (ln t) in modified step size formula?
- Basis in paper: Authors state logarithmic term chosen for gradual growth pattern but provide no theoretical justification
- Why unresolved: Authors do not explore impact of different functions or provide mathematical proof for why ln t is optimal choice
- What evidence would resolve it: Theoretical analysis comparing convergence properties with different functions or proof demonstrating optimality of ln t

### Open Question 3
- Question: How does modified step size perform on non-convex optimization problems with multiple local minima?
- Basis in paper: Paper establishes convergence rate for smooth non-convex functions but does not explore performance on problems with multiple local minima
- Why unresolved: Authors do not provide empirical evidence or theoretical analysis of modified step size's ability to escape local minima
- What evidence would resolve it: Experimental results comparing performance on various non-convex optimization problems with multiple local minima

## Limitations
- Theoretical analysis limited to smooth non-convex functions without PL condition, restricting applicability to complex optimization landscapes
- Warm restart mechanism lacks theoretical justification for interaction with modified step size
- Convergence proof relies heavily on smoothness assumption and bounded variance of stochastic gradients

## Confidence
- **High Confidence:** Basic mechanism of adding ln(t) to denominator and its mathematical validity for t ≥ 1
- **Medium Confidence:** Theoretical convergence rate O(ln(T)/√T) given assumptions hold, following established stochastic optimization frameworks
- **Medium Confidence:** Experimental improvements on image classification tasks, though magnitude may be sensitive to implementation details

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically evaluate how initial step size η₀ and warm restart schedule affect performance across different datasets
2. **Comparative Analysis with Alternative Step Sizes:** Benchmark proposed step size against other logarithmic and adaptive step sizes on identical architectures and datasets
3. **Theoretical Gap Analysis:** Investigate whether convergence guarantees extend to non-smooth functions or functions with higher-order curvature properties commonly encountered in deep learning