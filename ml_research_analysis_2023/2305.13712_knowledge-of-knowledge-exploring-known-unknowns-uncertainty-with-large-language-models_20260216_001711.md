---
ver: rpa2
title: 'Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language
  Models'
arxiv_id: '2305.13712'
source_url: https://arxiv.org/abs/2305.13712
tags:
- questions
- question
- unknown
- uncertainty
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates Large Language Models' (LLMs) ability\
  \ to handle known-unknown questions\u2014queries with high uncertainty due to the\
  \ absence of definitive answers. The authors create a new dataset, KUQ, containing\
  \ such questions and categorize them into six uncertainty types (Future Unknown,\
  \ Unsolved Problem/Mystery, Controversial/Debatable, False Assumption, Counterfactual,\
  \ Underspecified)."
---

# Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models

## Quick Facts
- arXiv ID: 2305.13712
- Source URL: https://arxiv.org/abs/2305.13712
- Reference count: 31
- Key outcome: LLMs struggle to distinguish known from unknown questions, achieving only 71.1% accuracy (random baseline: 50%), with minimal uncertainty disparity in generated answers.

## Executive Summary
This paper investigates Large Language Models' ability to handle known-unknown questionsâ€”queries with high uncertainty due to the absence of definitive answers. The authors create a new dataset, KUQ, containing such questions and categorize them into six uncertainty types (Future Unknown, Unsolved Problem/Mystery, Controversial/Debatable, False Assumption, Counterfactual, Underspecified). They evaluate open-source LLMs on three tasks: classifying known vs. unknown questions, categorizing unknown questions, and assessing answer uncertainty. Results show models perform poorly at distinguishing known from unknown questions, struggle with certain categories, and exhibit minimal uncertainty disparity in generated answers. The authors conclude that while LLMs can partially identify and express uncertainty, significant improvements are needed in classification and uncertainty calibration to reduce overconfidence and hallucinations.

## Method Summary
The study collects a KUQ dataset with 12,726 questions (6,363 known from ELI5, 6,363 unknown from web, crowd-sourcing, and GPT-generated). Questions are categorized into six uncertainty types. The authors fine-tune open-source LLMs (Alpaca-7B, Vicuna-13B) and evaluate GPT-4, GPT-3.5-turbo, and davinci-003 on binary classification (known vs. unknown), multi-class categorization of unknown questions, and answer uncertainty assessment using semantic features (subjectivity, hedges, sentence uncertainty, uncertainty aspect). The evaluation uses zero-shot, few-shot, and self-ask prompting strategies to compare model performance across different uncertainty tasks.

## Key Results
- Models achieve 71.1% accuracy in distinguishing known from unknown questions, barely above random baseline of 50%
- Best models reach ~66% accuracy for categorizing unknown questions into six uncertainty types
- Minimal uncertainty disparity between answers to known and unknown questions, indicating models struggle to express appropriate uncertainty
- Fine-tuned open-source models perform substantially worse than GPT-4, with self-ask approach actually degrading performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorization improves uncertainty identification
- Mechanism: By dividing unknown questions into six specific uncertainty types, the model can learn to recognize linguistic patterns associated with each category. This structured approach allows the model to develop specialized uncertainty representations for different types of unanswerable questions.
- Core assumption: Each uncertainty category has distinct linguistic features that can be learned by the model
- Evidence anchors: [abstract] "we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework"; [section 4.2] "we have identified a series of reasons why a question may be unknown"
- Break condition: If uncertainty categories overlap significantly or share similar linguistic features

### Mechanism 2
- Claim: Semantic uncertainty features capture answer uncertainty
- Mechanism: The model extracts multiple semantic features from generated answers to quantify the difference between known and unknown responses. This multi-dimensional approach provides a more comprehensive uncertainty measurement than single-feature methods.
- Core assumption: Different semantic features can effectively capture different dimensions of uncertainty in generated text
- Evidence anchors: [section 5.4] "We compute the average of the differences of several semantic features that capture the uncertainty disparity"
- Break condition: If semantic features don't correlate with human perception of uncertainty

### Mechanism 3
- Claim: Self-ask approach reveals model confidence calibration
- Mechanism: By having the model first generate an answer and then classify the question as known or unknown based on its own answer, the model's confidence calibration can be evaluated.
- Core assumption: The model's answer generation process influences its subsequent classification of question certainty
- Evidence anchors: [section 5.1] "Inspired by the work from Self-Ask, we conduct a similar experiment where the model is first asked to provide the answer to the question"
- Break condition: If the model's answer generation doesn't significantly influence its uncertainty classification

## Foundational Learning

- Concept: Metacognition in language models
  - Why needed here: Understanding how LLMs recognize their own knowledge limitations is crucial for building systems that can express appropriate uncertainty
  - Quick check question: Can you explain the difference between a model knowing it doesn't know something versus simply not knowing something?

- Concept: Uncertainty quantification in text
  - Why needed here: The paper relies on multiple uncertainty measurement techniques to evaluate how well models distinguish between known and unknown questions
  - Quick check question: What are the key differences between sentence-level and aspect-level uncertainty measurement?

- Concept: Fine-tuning vs zero-shot learning
  - Why needed here: The experiments compare different training approaches to understand their impact on uncertainty classification performance
  - Quick check question: How does few-shot learning typically differ from zero-shot learning in terms of model performance and data requirements?

## Architecture Onboarding

- Component map: Data collection -> Categorization framework -> Classification models -> Uncertainty measurement -> Evaluation framework
- Critical path: 1) Collect and categorize unknown questions, 2) Train classification models, 3) Implement multi-class categorization, 4) Generate answers, 5) Measure uncertainty disparity, 6) Evaluate performance
- Design tradeoffs: Manual categorization vs automated clustering (consistency vs scalability), multiple semantic features vs single measure (comprehensive vs complex), self-ask vs direct classification (confidence calibration vs potential bias)
- Failure signatures: High accuracy but low uncertainty disparity indicates classification without appropriate uncertainty expression; poor performance on specific categories suggests category-specific challenges; minimal difference between few-shot and zero-shot suggests limited benefit from example-based learning
- First 3 experiments: 1) Implement binary classification using different prompting strategies, 2) Test multi-class categorization performance across all uncertainty types, 3) Measure uncertainty disparity using subjectivity and hedges features on generated answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key linguistic features that distinguish future unknown questions from other types of unknown questions?
- Basis in paper: Explicit
- Why unresolved: While the paper notes that future unknown questions contain linguistic features that make them easier to identify, it does not specify what these features are or how they differ from features in other categories.
- What evidence would resolve it: Detailed linguistic analysis of question phrasing, tense usage, modal verbs, and other syntactic patterns specific to future unknown questions compared to other categories.

### Open Question 2
- Question: How does the inclusion of uncertainty expression in model prompts affect the quality and accuracy of answers to known-unknown questions?
- Basis in paper: Inferred
- Why unresolved: The paper evaluates models on uncertainty disparity when given explicit knowledge about whether a question is known or unknown, but does not explore how incorporating uncertainty expression directly into prompts affects model performance.
- What evidence would resolve it: Experiments comparing model outputs when prompted to express uncertainty versus when not prompted, measuring both answer quality and confidence calibration.

### Open Question 3
- Question: What is the relationship between model size and performance on known-unknown question classification across different uncertainty categories?
- Basis in paper: Explicit
- Why unresolved: While the paper shows that larger models perform better overall, it does not provide a detailed analysis of how performance varies by model size within each uncertainty category.
- What evidence would resolve it: Systematic evaluation of multiple model sizes across all six uncertainty categories, revealing whether performance gains are uniform or category-specific.

## Limitations
- Models achieve only 71.1% accuracy in distinguishing known from unknown questions, barely above random baseline
- Minimal uncertainty disparity between answers to known and unknown questions, indicating models struggle to express appropriate uncertainty
- Self-ask approach actually degraded performance, suggesting answer generation may reinforce overconfidence

## Confidence

- **High confidence**: Dataset construction methodology and basic classification performance metrics are well-documented and reproducible
- **Medium confidence**: Category-specific performance patterns are supported by data but may benefit from additional validation
- **Medium confidence**: The six-category framework effectively captures different uncertainty types, though some categories may overlap in practice
- **Low confidence**: The semantic features used for uncertainty measurement may not fully capture human perceptions of uncertainty in model responses

## Next Checks

1. Conduct human evaluation studies to validate whether the identified semantic features (subjectivity, hedges, etc.) align with human judgments of uncertainty in model-generated answers

2. Test model performance on additional uncertainty categories not covered in the current framework to assess framework completeness

3. Implement and evaluate alternative uncertainty quantification methods, such as confidence intervals or probabilistic reasoning frameworks, to compare against the current semantic feature approach