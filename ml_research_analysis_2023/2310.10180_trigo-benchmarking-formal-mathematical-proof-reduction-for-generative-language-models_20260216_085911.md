---
ver: rpa2
title: 'TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language
  Models'
arxiv_id: '2310.10180'
source_url: https://arxiv.org/abs/2310.10180
tags:
- have
- trigo
- proof
- tactics
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces TRIGO, a new benchmark for formal mathematical\
  \ proof reduction using generative language models. Unlike existing benchmarks focused\
  \ on symbolic inference, TRIGO evaluates a model\u2019s ability to manipulate, group,\
  \ and factor number terms in trigonometric expressions."
---

# TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models

## Quick Facts
- arXiv ID: 2310.10180
- Source URL: https://arxiv.org/abs/2310.10180
- Reference count: 40
- Key outcome: Introduces TRIGO benchmark showing current LMs struggle with numerical reasoning in formal trigonometric proofs

## Executive Summary
TRIGO is a new benchmark for evaluating generative language models on formal mathematical proof reduction, specifically focusing on trigonometric expression manipulation. Unlike existing theorem proving benchmarks that emphasize symbolic inference, TRIGO requires numerical reasoning skills including term grouping, factorization, and equivalent substitution. The benchmark consists of real-world problems collected from high school exercises and automatically generated samples, all formalized in Lean. Experiments demonstrate that current state-of-the-art models, including GPT-4, struggle significantly on TRIGO, particularly with numerical reasoning tasks and out-of-distribution generalization.

## Method Summary
The TRIGO benchmark is constructed by collecting trigonometric reduction problems from online sources, manually annotating reduction steps, and translating them into Lean formal language. The dataset includes both real-world problems (TRIGO-real) and automatically generated samples with controlled difficulty levels (TRIGO-gen). Models are fine-tuned on these datasets and evaluated using a BFS search algorithm that interacts with the Lean-Gym environment to generate proof steps. The primary evaluation metric is pass rate - the percentage of problems where the model outputs a correct proof within a maximum number of search steps.

## Key Results
- GPT-2L achieves only 0.72% pass rate on TRIGO-real test set despite strong performance on other formal reasoning tasks
- Models show significant performance gap between TRIGO-real and TRIGO-web test sets, indicating distribution mismatch
- GPT-4 exhibits an 8.45% gap in EM@8 between "all" tactics and tactics without "have" tactics on TRIGO-real
- Out-of-distribution test sets with larger angle values result in 0% pass rates across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TRIGO poses a new challenge for generative LMs because it requires numerical reasoning within formal proofs.
- Mechanism: Current ATP benchmarks focus on symbolic inference without complex number combination reasoning. TRIGO introduces problems requiring term grouping, factorization, and equivalent substitution, which involve understanding of numerical elements.
- Core assumption: Language models trained on symbolic reasoning benchmarks will struggle with tasks requiring numerical manipulation in formal proofs.
- Evidence anchors:
  - [abstract]: "However, current ATP benchmarks mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning."
  - [introduction]: "Current ATP benchmarks (Wu et al., 2021a; Han et al., 2021; Zheng et al., 2022) mainly focus on symbolic inference but rarely involve the understanding of complex number combination reasoning, such as term grouping, term factorization and equivalent substitution."

### Mechanism 2
- Claim: GPT-4's performance gap on "have" tactics reveals limitations in complex number combination reasoning.
- Mechanism: The "have" tactic requires generating sub-goal equations that involve manipulating numbers and formulas. GPT-4 struggles with this compared to simpler tactics, showing it lacks strong numerical reasoning abilities.
- Core assumption: Difficulty with "have" tactics specifically indicates a gap in numerical reasoning rather than general formal proof capabilities.
- Evidence anchors:
  - [section 6.3]: "However, the prediction of 'have' tactics poses a significant challenge in overall proof generation, especially in the TRIGO -real dataset where there is an 8.45% gap in EM@8 between 'all' tactics and tactics without the 'have' tactics."

### Mechanism 3
- Claim: Generating data from real-world problems bridges the distribution gap and improves performance.
- Mechanism: The TG-E dataset, generated starting with expressions in TRIGO -real, shows better performance than purely synthetic data, suggesting real-world problem structure helps model learning.
- Core assumption: The structure and complexity of real-world trigonometric problems provides valuable inductive biases that purely synthetic generation misses.
- Evidence anchors:
  - [section 6.2]: "To explore the gap between TRIGO -real and TRIGO -web, we train the GPT-2L-PACT on TG-E whose samples are generated start with expression in TRIGO -real."

## Foundational Learning

- Concept: Lean formal theorem proving environment
  - Why needed here: The entire benchmark is built on Lean as the formal verification system, and all generated proofs must be verifiable in Lean-Gym.
  - Quick check question: Can you explain how Lean-Gym interacts with GPT-2 to generate proof steps and verify them?

- Concept: Automated theorem proving (ATP) benchmarks
  - Why needed here: Understanding existing ATP benchmarks helps contextualize why TRIGO is novel and what gaps it fills in the landscape.
  - Quick check question: What are the key differences between TRIGO and other ATP benchmarks like LeanStep or MiniF2F?

- Concept: Trigonometric expression reduction
  - Why needed here: The core task involves reducing complex trigonometric expressions using identities and transformations, which requires both mathematical and formal reasoning.
  - Quick check question: Can you describe the process of reducing sin(13π/6) to sin(π/6) using appropriate trigonometric identities?

## Architecture Onboarding

- Component map: Data Collection → Interactive Annotation → Lean Formalization → Automatic Generation → Model Training → Proof Search
- Critical path: Data collection → Interactive annotation → Lean formalization → Model training
  - The Lean formalization step is critical because errors here propagate through the entire system and affect model training quality.
- Design tradeoffs:
  - Manual vs automatic data generation: Manual annotation ensures correctness but is time-consuming; automatic generation scales but may miss real-world complexity.
  - BFS vs MCTS for proof search: BFS is simpler and more effective for this dataset, while MCTS requires better value functions that weren't developed.
- Failure signatures:
  - Low pass rates on TRIGO -web despite good performance on TRIGO -gen indicate distribution mismatch.
  - Models failing on "have" tactics specifically suggests numerical reasoning limitations.
  - 0% pass rates on OOD test sets with larger angle values indicates lack of numerical generalization.
- First 3 experiments:
  1. Train GPT-2L on TRIGO -real training set and evaluate on both TRIGO -real and TRIGO -web test sets to measure distribution gap.
  2. Train GPT-2L-PACT on TG-E (generated from real expressions) and compare performance against GPT-2L-PACT trained on TRIGO -real.
  3. Evaluate GPT-4's performance on "have" tactics specifically to assess numerical reasoning capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of "have" tactics specifically impact model performance on trigonometric reduction proofs?
- Basis in paper: [explicit] The paper shows a significant performance gap between "all" tactics and tactics without "have" tactics, with an 8.45% gap in EM@8 between "all" tactics and tactics without the "have" tactics in TRIGO-real.
- Why unresolved: The paper does not investigate why "have" tactics are particularly challenging or how their complexity compares to other tactics.
- What evidence would resolve it: Detailed analysis of "have" tactic patterns and their complexity metrics compared to other tactics, along with ablation studies removing "have" tactics from training data.

### Open Question 2
- Question: What is the optimal temperature setting for balancing tactic diversity and validity in proof generation?
- Basis in paper: [explicit] The paper tests temperatures 1.0, 1.25, and 1.5, finding that temperature 1.25 reduces illegal characters while improving pass rates, but does not explore the full temperature spectrum.
- Why unresolved: The paper only tests three temperature values and does not systematically explore the temperature-performance tradeoff.
- What evidence would resolve it: Comprehensive temperature sweep experiments showing the relationship between temperature, tactic validity rate, and proof success rate across different model sizes.

### Open Question 3
- Question: Why does Monte Carlo Tree Search perform worse than Breadth-First Search on TRIGO-gen despite being effective in other theorem proving domains?
- Basis in paper: [explicit] The paper reports that MCTS performs significantly worse than BFS on TRIGO-gen, attributing it to the lack of a well-developed value function.
- Why unresolved: The paper does not explore alternative MCTS implementations or investigate whether the problem lies in the value function design or the search space structure.
- What evidence would resolve it: Experiments with different MCTS value function designs, including learned value functions from proof data, and comparison with other search algorithms like A* or weighted A*.

## Limitations

- Dataset construction relies heavily on manual annotation for real-world problems, which is time-consuming and may introduce human bias
- Significant performance gap between real-world and web-collected test sets suggests potential distribution mismatch issues
- Evaluation metric based on fixed-step BFS search may not fully capture model capabilities for proofs requiring more steps

## Confidence

- High Confidence: Current state-of-the-art models struggle with numerical reasoning in formal proofs
- Medium Confidence: Real-world problem structure provides valuable inductive biases for model learning
- Low Confidence: "Have" tactic performance specifically reveals numerical reasoning limitations

## Next Checks

1. Conduct a systematic study of annotation consistency across multiple annotators for the TRIGO-real dataset to quantify annotation reliability
2. Create synthetic variants of TRIGO-web with controlled modifications to isolate whether the distribution gap is due to numerical range, problem complexity, or other factors
3. Compare BFS performance against other search algorithms (e.g., depth-limited search, heuristic-guided search) to determine if the fixed-step limitation artificially constrains model evaluation