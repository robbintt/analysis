---
ver: rpa2
title: Benchmarking common uncertainty estimation methods with histopathological images
  under domain shift and label noise
arxiv_id: '2301.01054'
source_url: https://arxiv.org/abs/2301.01054
tags:
- ensemble
- uncertainty
- methods
- data
- mcdo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares uncertainty estimation methods (SVI, MCDO, Deep
  Ensembles, TTA, and their ensembles) for histopathological image classification
  under domain shift and label noise. The methods are evaluated on tile-level using
  the Camelyon17 breast cancer dataset, which includes slides from five different
  clinics, inducing domain shifts.
---

# Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise

## Quick Facts
- arXiv ID: 2301.01054
- Source URL: https://arxiv.org/abs/2301.01054
- Reference count: 20
- Primary result: Ensembles and TTA are most effective for uncertainty estimation in histopathology under domain shift and label noise

## Executive Summary
This paper benchmarks uncertainty estimation methods (SVI, MCDO, Deep Ensembles, TTA, and their ensembles) for histopathological image classification using the Camelyon17 breast cancer dataset. The study evaluates these methods under domain shift across five different clinics and varying levels of label noise. Results show that ensemble methods generally provide better accuracy, calibration, and robustness compared to single models. Test-time data augmentation (TTA) emerges as a computationally efficient alternative that performs competitively with ensembles. The study demonstrates that rejecting uncertain predictions significantly improves classification accuracy across all methods.

## Method Summary
The paper evaluates five uncertainty estimation methods on the Camelyon17 dataset: Stochastic Variational Inference (SVI), Monte Carlo Dropout (MCDO), Deep Ensembles, Test-Time Data Augmentation (TTA), and their ensemble combinations. The dataset contains 50 whole-slide images from five different clinics, creating domain shift between centers. Methods are tested on tile-level classification with 256×256 pixel patches, and experiments include simulated label noise at varying percentages. Performance is measured using accuracy, balanced accuracy, expected calibration error (ECE), and AUARC metrics across in-distribution and out-of-distribution data splits.

## Key Results
- Ensemble methods consistently outperform single models in accuracy, calibration, and robustness across domain shifts
- TTA and ensemble methods excel at detecting misclassifications while maintaining computational efficiency
- Under label noise, TTA and ensembles show greater robustness than SVI and MCDO
- Rejecting uncertain tiles significantly improves accuracy across all uncertainty estimation methods
- SVI and MCDO show inconsistent performance and are less reliable under label noise conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensembles improve accuracy and calibration under domain shift
- Mechanism: Combining multiple models trained from different initializations reduces overfitting to specific data distribution and smooths prediction variance
- Core assumption: Domain shift causes individual models to specialize differently; averaging reduces variance while preserving signal
- Evidence anchors:
  - [abstract] "ensembles generally lead to better accuracy, calibration, and robustness compared to single models"
  - [section 4.1] "As expected, the ensemble variants outperform their non-ensemble counterparts"
  - [corpus] Weak; no direct ensemble-specific papers found
- Break condition: If domain shift is too extreme, ensemble averaging may suppress useful specialized signals

### Mechanism 2
- Claim: Test-Time Data Augmentation (TTA) provides robustness without architectural changes
- Mechanism: Augmenting inputs at inference time approximates model ensemble behavior while requiring only one trained model
- Core assumption: The set of augmentations captures the variability of domain shift; averaging predictions over augmented versions stabilizes outputs
- Evidence anchors:
  - [abstract] "TTA and ensembles outperform others in detecting misclassifications, with TTA being computationally efficient"
  - [section 2.1.4] "TTA uses the same model multiple times by augmenting the input in different ways during inference"
  - [corpus] Weak; no direct TTA-specific papers found
- Break condition: If augmentation set is too narrow or too broad, TTA may under- or over-regularize

### Mechanism 3
- Claim: Rejecting uncertain tiles improves classification accuracy significantly
- Mechanism: Low-confidence predictions correlate with misclassification; thresholding them out improves overall accuracy
- Core assumption: Uncertainty estimates (confidence, entropy, variance) are well-calibrated and correlate with actual prediction correctness
- Evidence anchors:
  - [abstract] "Across methods, rejecting uncertain tiles significantly improves accuracy"
  - [section 4.2] "a mostly linear increase of balanced accuracy when rejecting a fraction of the most uncertain tiles"
  - [section 4.3] "lower tumor confidences at the border of annotated tumor regions"
- Break condition: If uncertainty estimates are poorly calibrated, rejection may discard correct predictions

## Foundational Learning

- Concept: Bayesian Model Averaging
  - Why needed here: Understanding how SVI, MCDO, and Deep Ensembles approximate the posterior predictive distribution
  - Quick check question: What is the mathematical form of the posterior predictive distribution and why is it intractable for neural networks?

- Concept: Domain Shift and Out-of-Distribution Detection
  - Why needed here: Camelyon17 has domain shift between clinics; methods must generalize across scanners and staining protocols
  - Quick check question: How does the Camelyon17 dataset structure create domain shift between in-distribution and out-of-distribution centers?

- Concept: Calibration and Expected Calibration Error (ECE)
  - Why needed here: Methods must not only be accurate but also well-calibrated for clinical trust
  - Quick check question: How is ECE computed and what does it measure about a model's confidence predictions?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Inference with uncertainty estimation -> Uncertainty metric computation -> Rejection threshold -> Evaluation
- Critical path:
  1. Data preprocessing and tile extraction with tumor coverage filtering
  2. Model training with appropriate uncertainty method
  3. Inference with multiple samples (SVI/MCDO) or augmentations (TTA)
  4. Uncertainty metric computation and rejection
  5. Evaluation on in-distribution and out-of-distribution data
- Design tradeoffs:
  - SVI: More complex training, better uncertainty estimates but inconsistent performance
  - MCDO: Easy to implement, requires dropout tuning, moderate performance
  - Deep Ensemble: Best performance but highest computational cost (5× training)
  - TTA: No architectural changes, good performance, computationally efficient at inference
- Failure signatures:
  - SVI: High variability in results, poor performance under label noise
  - MCDO: No improvement over baseline, may need dropout probability tuning
  - Deep Ensemble: Computational bottleneck, diminishing returns beyond 5 members
  - TTA: Poor calibration, may need careful augmentation selection
- First 3 experiments:
  1. Baseline ResNet-34 on ID data to establish performance floor
  2. TTA with standard augmentations to verify computational efficiency claims
  3. Deep Ensemble vs. TTA Ensemble comparison on OOD center 2 to test domain robustness

## Open Questions the Paper Calls Out
1. How do uncertainty estimation methods perform on histopathological multi-class classification tasks?
2. What is the impact of label noise on uncertainty estimation methods in more realistic medical annotation scenarios?
3. How do uncertainty estimation methods perform on whole-slide image (WSI) level predictions rather than tile-level?

## Limitations
- Limited clinical validation with real-world deployment testing and pathologist workflows
- Simulation-based label noise may not reflect true inter-observer variability in medical annotations
- Results are based exclusively on breast cancer histopathology, limiting generalizability
- Computational cost trade-offs lack detailed timing comparisons across hardware configurations

## Confidence
- High confidence: Ensembles generally outperform single models
- Medium confidence: TTA provides competitive performance with computational efficiency
- Medium confidence: Label noise disproportionately affects SVI and MCDO
- Low confidence: Rejection of uncertain tiles consistently improves accuracy

## Next Checks
1. Cross-tissue validation: Test uncertainty methods on histopathology datasets from other organs (lung, prostate, colon) to assess generalizability
2. Real-world label noise analysis: Apply methods to datasets with known annotation inconsistencies between pathologists
3. Clinical workflow integration: Conduct user study with pathologists to evaluate how uncertainty estimates influence diagnostic decision-making