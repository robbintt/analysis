---
ver: rpa2
title: Composing Parameter-Efficient Modules with Arithmetic Operations
arxiv_id: '2306.14870'
source_url: https://arxiv.org/abs/2306.14870
tags:
- lora
- pems
- language
- toxic
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free method to compose parameter-efficient
  modules (PEMs) trained on different data distributions or tasks through linear arithmetic
  operations in the weight space. The key idea is to define addition and negation
  operators for PEMs as basic building blocks, and then combine them to perform flexible
  arithmetic operations such as addition, subtraction, and weighted combinations.
---

# Composing Parameter-Efficient Modules with Arithmetic Operations

## Quick Facts
- arXiv ID: 2306.14870
- Source URL: https://arxiv.org/abs/2306.14870
- Reference count: 40
- Key outcome: Training-free method to compose PEMs through linear arithmetic operations achieves significant performance gains across distribution generalization, multi-tasking, unlearning, and domain transfer tasks

## Executive Summary
This paper introduces a novel approach to composing parameter-efficient modules (PEMs) trained on different data distributions or tasks through linear arithmetic operations in the weight space. The key innovation is defining addition and negation operators for PEMs as basic building blocks, enabling flexible arithmetic operations such as addition, subtraction, and weighted combinations without additional training. The approach allows merging PEMs to integrate their capabilities, transfer skills across domains, or unlearn certain abilities. Experiments demonstrate that composed PEMs significantly outperform individual ones across diverse settings, including distribution generalization, multi-tasking, unlearning, and domain transfer, with the method also extended to detoxify instruction-tuned large language models.

## Method Summary
The method defines two fundamental operators for PEMs: addition (⊕) and negation (⊖). For LoRA, addition combines the A and B matrices from different PEMs, while negation simply negates the B matrix. For (IA)3, addition combines the l vectors, and negation computes lneg = 2 - l. These operators enable training-free composition of PEMs through weighted combinations using a hyperparameter λ. The approach leverages the hypothesis that PEM parameters lie in the same error basin as base model parameters, allowing their effects to be linearly combined. The method is applied across various settings including combining PEMs trained on different subsets of tasks, unlearning specific capabilities through negation, and transferring domain knowledge through analogy operations.

## Key Results
- Composed PEMs outperform individual PEMs across all tested settings including distribution generalization, multi-tasking, unlearning, and domain transfer
- The method successfully reduces toxicity in instruction-tuned models by composing toxic and non-toxic PEMs
- Zero-shot domain transfer is achieved through analogy operations, with LoRA showing significant improvements on 3 out of 4 transfer settings
- The training-free approach matches or exceeds performance of individually trained PEMs while requiring no additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEM addition works because PEM parameters lie in the same error basin as the base model parameters
- Mechanism: PEMs trained from the same base model initialization make small, linear modifications to hidden states that can be added together to combine their effects
- Core assumption: PEM parameters are linearly connected in parameter space, allowing simple addition to approximate their combined effect
- Evidence anchors: Abstract mentions linear arithmetic operations in weight space; section 3 proposes testing this hypothesis across settings; corpus provides only weak evidence mentioning related work
- Break condition: If PEMs from different initializations fall into different loss basins, their parameters may not combine linearly

### Mechanism 2
- Claim: PEM negation works by reversing the modification to hidden states that the PEM applies
- Mechanism: Each PEM modifies hidden states by a specific delta (∆h), and negation reverses this by negating appropriate parameters (B for LoRA, computing lneg = 2 - l for (IA)3)
- Core assumption: PEM modifications can be cleanly separated and reversed without affecting base model capabilities
- Evidence anchors: Section 3 derives lneg = 2 - l for (IA)3; section 4.4 shows toxicity reduction with negated-LoRA; corpus lacks direct evidence for this mechanism
- Break condition: If PEM modifications interact with base model parameters in non-linear ways, simple negation may cause collateral damage

### Mechanism 3
- Claim: PEM composition enables zero-shot domain transfer through analogy operations
- Mechanism: PEMs trained on source domain tasks can be combined with related language modeling PEMs to transfer capabilities through subtraction and addition operations
- Core assumption: Language modeling PEMs capture domain-specific features transferable to classification tasks
- Evidence anchors: Section 4.5 shows LoRA improving vanilla transfer on 3/4 settings; section 3 references word embedding analogy example; corpus provides weak evidence
- Break condition: If domain features aren't transferable or subtraction removes essential capabilities rather than domain-specific features

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The entire paper builds on understanding how PEFT methods like LoRA and (IA)3 work differently from full fine-tuning
  - Quick check question: What distinguishes LoRA from traditional adapter methods in terms of how they modify hidden states?

- Concept: Linear connectivity of neural network parameters
  - Why needed here: The core hypothesis is that PEM parameters lie in the same error basin and can be linearly combined
  - Quick check question: Under what conditions do neural network parameters exhibit linear connectivity that allows direct addition?

- Concept: Domain adaptation and transfer learning
  - Why needed here: The domain transfer experiments rely on understanding how features learned in one domain can be applied to another
  - Quick check question: What is the key difference between domain adaptation and domain transfer in the context of PEM composition?

## Architecture Onboarding

- Component map:
  - Base model (frozen during PEFT)
  - PEM architecture (LoRA or (IA)3)
  - PEM parameters (A, B for LoRA; l vectors for (IA)3)
  - Composition operators (⊕ for addition, ⊖ for negation)
  - Weight hyperparameter λ for interpolation

- Critical path:
  1. Train PEMs on source tasks
  2. Apply composition operators to PEM parameters
  3. Integrate composed PEM into base model
  4. Evaluate on target tasks

- Design tradeoffs:
  - Simplicity vs. expressiveness: Using only addition and negation limits flexibility but ensures training-free composition
  - Initialization sensitivity: PEMs trained from different initializations may not compose as well
  - Task compatibility: Not all PEM combinations may be meaningful (e.g., combining toxic and non-toxic PEMs)

- Failure signatures:
  - Performance worse than individual PEMs
  - Catastrophic forgetting of base model capabilities
  - Mode collapse in generation tasks

- First 3 experiments:
  1. Combine two LoRA PEMs trained on different subsets of the same task (MNLI) and evaluate on validation set
  2. Negate a LoRA PEM trained on toxic data and evaluate toxicity reduction while maintaining language modeling ability
  3. Combine LoRA PEMs for sentiment classification and language modeling across Amazon and Yelp domains to test zero-shot transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of PEM architecture (LoRA vs. (IA)3) affect the effectiveness of the composition method across different settings?
- Basis in paper: [explicit] The paper conducts experiments with both LoRA and (IA)3 architectures across multiple settings with varying performance differences
- Why unresolved: The paper lacks theoretical explanation for architecture-specific performance differences
- What evidence would resolve it: Detailed analysis comparing internal mechanisms of LoRA and (IA)3 with controlled experiments isolating architecture-specific impacts

### Open Question 2
- Question: What is the optimal initialization strategy for PEMs when composing them through arithmetic operations?
- Basis in paper: [explicit] The paper acknowledges that different initialization affects merging performance but only briefly explores this with random seeds
- Why unresolved: The paper only scratches the surface of initialization effects without comprehensive guidelines
- What evidence would resolve it: Systematic experiments varying multiple aspects of initialization across different PEM architectures and settings

### Open Question 3
- Question: Can the weight hyperparameter λ be automatically determined rather than manually tuned?
- Basis in paper: [explicit] The paper notes λ is the only tunable hyperparameter and mentions automatic methods as future work
- Why unresolved: The paper uses manual grid search without exploring sophisticated methods despite acknowledging this limitation
- What evidence would resolve it: Development and validation of automatic methods for determining λ that outperform manual tuning

## Limitations

- The core hypothesis of linear connectivity lacks rigorous theoretical justification despite empirical support
- The mechanism for PEM negation may not generalize well across all PEM architectures
- Domain transfer via analogy operations lacks strong theoretical grounding

## Confidence

- **High Confidence**: Empirical results showing performance improvements from PEM composition are well-supported by statistically significant experiments across multiple tasks
- **Medium Confidence**: The hypothesis that PEMs can be linearly combined in weight space is plausible but lacks complete theoretical justification
- **Low Confidence**: The domain transfer mechanism via analogy operations lacks strong theoretical grounding despite promising results

## Next Checks

1. **Loss Basin Analysis**: Conduct empirical validation to confirm PEM parameters from different initializations fall into the same loss basin by measuring loss when interpolating between PEM parameters

2. **Generalization Across PEM Architectures**: Test the composition approach with additional PEM architectures beyond LoRA and (IA)3, such as prefix tuning or soft prompt tuning

3. **Ablation of Linear Connectivity**: Design experiments to isolate the effect of linear connectivity from other explanations by comparing linear composition to non-linear alternatives while controlling for model capacity