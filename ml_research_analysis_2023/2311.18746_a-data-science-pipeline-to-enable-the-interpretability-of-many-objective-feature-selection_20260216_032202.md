---
ver: rpa2
title: A data-science pipeline to enable the Interpretability of Many-Objective Feature
  Selection
arxiv_id: '2311.18746'
source_url: https://arxiv.org/abs/2311.18746
tags:
- solutions
- objectives
- feature
- features
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a methodology for the interpretation of
  many-objective feature selection (MOFS) results by providing a systematic framework
  to analyze and compare the set of non-dominated solutions across three perspectives:
  objectives, solutions, and individual features. The methodology combines post-processing
  and visualization techniques, including clustering, dimensionality reduction, ranking
  with TOPSIS, and feature frequency/contribution analysis.'
---

# A data-science pipeline to enable the Interpretability of Many-Objective Feature Selection

## Quick Facts
- arXiv ID: 2311.18746
- Source URL: https://arxiv.org/abs/2311.18746
- Reference count: 39
- Primary result: Introduces a three-level methodology to interpret MOFS results, combining clustering, TOPSIS ranking, and SHAP analysis to improve feature subset selection decisions.

## Executive Summary
This paper presents a systematic methodology for interpreting many-objective feature selection (MOFS) results by decomposing the analysis into three hierarchical perspectives: objectives, solutions, and individual features. The approach uses clustering to identify similar solutions, TOPSIS to rank them based on weighted objectives, and combines feature frequency with SHAP values to assess feature relevance. Experiments on two datasets demonstrate that including fairness metrics leads to more balanced and interpretable solutions compared to baselines, with the methodology showing robustness across different objective weight schemes.

## Method Summary
The methodology processes MOFS solution sets through three sequential pipelines: (1) similarity detection using K-Means clustering and PCA visualization to identify representative solutions, (2) objective sorting and solution ranking using variance-based weights and TOPSIS multi-criteria decision making, and (3) feature analysis combining frequency counting with SHAP-based contribution scores. The approach was validated using NSGA-III with Naive Bayes and Logistic Regression classifiers on Diabetes and German credit datasets, optimizing six objectives including accuracy, fairness metrics, and feature subset characteristics.

## Key Results
- The three-level decomposition (objectives, solutions, features) provides interpretable decision-making framework for MOFS outcomes
- Including fairness objectives (statistical parity, equalised odds) produces more balanced solutions compared to accuracy-only baselines
- TOPSIS rankings show sensitivity to weight schemes, with entropy weights producing more stable results than equal or variance-based weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The methodology's three-level decomposition (objectives, solutions, features) enables interpretable decision-making by aligning the analysis scope with the hierarchical structure of MOFS outcomes.
- Mechanism: By partitioning the analysis into objective-centric, solution-centric, and feature-centric views, the methodology allows users to progressively refine their understanding from high-level trade-offs down to individual feature relevance.
- Core assumption: Users can interpret and act on information at each level independently without requiring simultaneous multi-dimensional reasoning.
- Evidence anchors:
  - [abstract] The methodology provides "high-level information at three different levels: objectives, solutions, and individual features."
  - [section] "The methodology supports the data scientist in the selection of an optimal feature subset by providing her with high-level information at three different levels: objectives, solutions, and individual features."
- Break condition: If the feature-level analysis does not correlate meaningfully with the solution-level rankings, the hierarchical structure breaks down and loses interpretability value.

### Mechanism 2
- Claim: Clustering non-dominated solutions and ranking them using TOPSIS enables the identification of representative trade-off points and prioritizes solutions based on multi-objective importance weights.
- Mechanism: K-Means clustering reduces the solution space to interpretable groups, while TOPSIS aggregates weighted objective values to rank solutions relative to ideal and nadir points.
- Core assumption: The clustering and ranking methods capture the essential trade-offs without losing critical solution diversity.
- Evidence anchors:
  - [section] "We project the solutions into 2-D using PCA to visualise the clusters as shown in Figure 4... we rank the solutions using the TOPSIS method."
- Break condition: If clustering produces overlapping or degenerate groups, or if TOPSIS weights are poorly chosen, the ranking becomes uninformative.

### Mechanism 3
- Claim: Feature frequency and SHAP-based contribution scores provide complementary perspectives on feature relevance, enabling both selection frequency analysis and discriminative power assessment.
- Mechanism: Frequency counts show how often a feature appears across solutions, while SHAP values quantify each feature's contribution to classification performance across classes.
- Core assumption: Features that are both frequently selected and highly discriminative are the most relevant for the task.
- Evidence anchors:
  - [section] "The frequency of the features is computed by counting the occurrence of each feature in the set of solutions... we use SHAP to measure the global contribution of each selected feature."
- Break condition: If SHAP values are unstable or if feature frequency does not correlate with solution quality, the feature-level analysis loses reliability.

## Foundational Learning

- Concept: Many-objective optimization (MOO) and Pareto optimality
  - Why needed here: MOFS generates multiple non-dominated solutions; understanding Pareto optimality is essential to interpret trade-offs.
  - Quick check question: What is the difference between a dominated and non-dominated solution in multi-objective optimization?

- Concept: Feature selection objectives and fairness metrics
  - Why needed here: The methodology includes fairness objectives (statistical parity, equalised odds) alongside predictive accuracy; understanding these metrics is crucial for balanced interpretation.
  - Quick check question: How do statistical parity and equalised odds differ in measuring fairness?

- Concept: SHAP (SHapley Additive exPlanations) values
  - Why needed here: SHAP quantifies individual feature contributions to model predictions, enabling interpretability at the feature level.
  - Quick check question: What does a high absolute SHAP value indicate about a feature's influence on a prediction?

## Architecture Onboarding

- Component map: MOFS solutions -> Clustering + PCA visualization -> Objective weight computation -> TOPSIS ranking -> Feature frequency counting + SHAP contribution -> Final solution selection

- Critical path: The critical path for final solution selection is: detect similarity → sort objectives → rank solutions → compute feature frequency/contribution → choose final solution. Delays in any step directly delay the decision-making outcome.

- Design tradeoffs: Using K-Means for clustering trades exact Pareto front structure for interpretability; TOPSIS weights heavily influence solution ranking; SHAP values require model-specific computation and may not generalize across classifiers.

- Failure signatures: If clustering yields a single cluster, similarity detection fails; if TOPSIS ranks are unstable across weight schemes, the ranking is unreliable; if feature frequencies are uniformly distributed, the feature analysis provides no discriminative insight.

- First 3 experiments:
  1. Run MOFS with 4 objectives on a small synthetic dataset; apply the methodology and verify that the three-level decomposition produces distinct, non-overlapping insights.
  2. Vary TOPSIS weights systematically (equal, entropy, r/sigma) on the Diabetes dataset; compare solution rankings and identify weight sensitivity.
  3. Replace SHAP with Gini importance on the German credit dataset; assess correlation between the two feature contribution measures and impact on final solution choice.

## Open Questions the Paper Calls Out

- Question: How does the methodology perform when applied to datasets with more than six objectives or different objective types beyond the ones used in the experiments?
- Basis in paper: [explicit] The paper mentions that they applied the methodology to eight more datasets with four objectives but did not discuss the results due to space constraints.
- Why unresolved: The paper only presents results for two datasets with six objectives, leaving the generalizability to other objective configurations unexplored.

- Question: What is the impact of different similarity detection methods (e.g., clustering vs. MDS vs. pairwise visualizations) on the final solution selection and overall interpretability?
- Basis in paper: [explicit] The paper discusses multiple approaches for detecting similar/outlier solutions including clustering, MDS, and pairwise visualizations, but does not compare their relative effectiveness.
- Why unresolved: The paper presents the methodology as using clustering for similarity detection without exploring or comparing alternative approaches.

- Question: How do users perceive and interact with the methodology compared to traditional tabular presentations or parallel coordinate plots?
- Basis in paper: [inferred] The paper emphasizes the interpretability advantages of the methodology over current practices but does not include any user study or usability assessment.
- Why unresolved: The paper provides theoretical arguments for improved interpretability but lacks empirical evidence from actual users.

## Limitations
- The methodology's generalizability across different problem domains and classifier types remains untested
- TOPSIS ranking sensitivity to weight schemes suggests potential instability in solution selection
- Computational overhead of SHAP values may limit scalability to larger feature sets

## Confidence
- Three-level decomposition interpretability: Medium - supported by methodology description but lacks comparative validation
- Clustering + TOPSIS ranking effectiveness: Low-Medium - theoretical justification present but empirical robustness testing is limited
- Feature frequency + SHAP combination: Medium - mechanism is sound but correlation between measures is not quantified

## Next Checks
1. Test the methodology on a third dataset with different characteristics (higher dimensionality, more classes) to assess generalizability
2. Compare K-Means clustering results with alternative clustering methods (DBSCAN, hierarchical clustering) to evaluate solution space representation quality
3. Measure the correlation between feature frequency and SHAP values across all solutions to validate their complementary nature and identify potential redundancy