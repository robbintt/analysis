---
ver: rpa2
title: 'Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking
  in Large Language Models'
arxiv_id: '2310.07712'
source_url: https://arxiv.org/abs/2310.07712
tags:
- ranking
- rankings
- gpt-3
- self-consistency
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses positional bias in large language models when
  performing listwise ranking tasks. The authors propose permutation self-consistency,
  a decoding strategy that mitigates positional bias by aggregating multiple rankings
  generated from randomly permuted input lists.
---

# Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models

## Quick Facts
- arXiv ID: 2310.07712
- Source URL: https://arxiv.org/abs/2310.07712
- Reference count: 6
- Key outcome: Permutation self-consistency improves listwise ranking in LLMs by up to 18% relative score increases for GPT-3.5 and 16% for LLaMA v2 (70B) on sorting and passage reranking tasks.

## Executive Summary
This paper introduces permutation self-consistency, a decoding strategy that mitigates positional bias in large language models during listwise ranking tasks. The method works by generating multiple rankings from randomly permuted input lists and aggregating them using Kemeny-Young optimal ranking. Theoretical analysis shows convergence to true rankings under certain noise conditions, and empirical results demonstrate consistent improvements across five datasets, including new state-of-the-art results in passage reranking.

## Method Summary
Permutation self-consistency addresses positional bias in LLMs by marginalizing out different list orders in the prompt. For each ranking task, the method generates multiple permutations of the input list, passes each through the LLM to produce rankings, and aggregates these rankings using Kemeny-Young optimal ranking (which minimizes Kendall tau distances to all sampled rankings). This approach breaks the association between input list order and output rankings, reducing positional bias while maintaining or improving ranking quality.

## Key Results
- Up to 18% relative score improvements for GPT-3.5 and 16% for LLaMA v2 (70B) on sorting tasks
- New state-of-the-art results in passage reranking on TREC-DL19 and TREC-DL20 datasets
- Higher gains observed on lower-quality models, with performance improvements inversely proportional to baseline quality
- Effectiveness demonstrated across both sorting tasks (MathSort, WordSort, GSM8KSort) and passage reranking tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation self-consistency improves ranking by marginalizing out positional biases through repeated list shuffling and rank aggregation.
- Mechanism: The method repeatedly shuffles the input list and generates rankings, then aggregates these rankings using Kemeny-Young optimal ranking, which minimizes Kendall tau distances to all sampled rankings. This process effectively breaks the association between individual list order and output rankings.
- Core assumption: Positional biases in LLMs are sensitive to input list order, and these biases vary across different permutations of the same list.
- Evidence anchors:
  - [abstract]: "Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias."
  - [section]: "With it, we propose permutation self-consistency: for the first sample step, we randomly shuffle the list in the prompt to curate a diverse set of rankings, each with different position biases. For the next aggregate step, we compute the central ranking closest in Kendall tau distance to all the sampled rankings..."
  - [corpus]: Weak evidence. No direct corpus evidence found for this specific mechanism, though related work on positional bias exists.
- Break condition: If the LLM's positional biases are not sensitive to list order or if the aggregation method fails to converge to a meaningful central ranking.

### Mechanism 2
- Claim: The Kemeny-Young optimal ranking provides theoretical robustness by converging to the true ranking under arbitrary noise distributions when at least one correctly ordered pair exists in each observation.
- Mechanism: The Kemeny-Young ranking minimizes the sum of Kendall tau distances to all sampled rankings, which, under certain conditions, converges to the true ranking as the number of observations increases.
- Core assumption: There exists at least one correctly ordered pair in each noisy ranking observation, and enough observations are provided.
- Evidence anchors:
  - [abstract]: "Theoretically, we prove the robustness of our method, showing convergence to the true ranking in the presence of random perturbations."
  - [section]: "Proposition 2.1. Let there be a true ranking σ and a sequence of noisy rankings. Suppose each noisy ranking has a uniformly random, nonempty concordant subset S′ with σ... Then the Kemeny–Young ranking of converges in probability to σ..."
  - [corpus]: No direct corpus evidence found for this specific theoretical proof, though related work on Kemeny-Young ranking exists.
- Break condition: If the noise distribution is such that no correctly ordered pair exists in any observation, or if the number of observations is insufficient for convergence.

### Mechanism 3
- Claim: The method is effective for both sorting tasks and passage reranking, with higher gains observed on lower-quality models.
- Mechanism: The method improves ranking quality by aggregating multiple rankings generated from different permutations of the input list, which helps mitigate positional biases that affect both sorting and reranking tasks.
- Core assumption: Positional biases affect both sorting and reranking tasks, and the method's effectiveness is inversely proportional to the baseline quality of the model.
- Evidence anchors:
  - [abstract]: "Empirically, on five list-ranking datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 7–18% for GPT-3.5 and 8–16% for LLaMA v2 (70B)..."
  - [section]: "We conduct experiments on sorting and passage ranking, which constitute two distinct types of problems in listwise ranking."
  - [corpus]: Weak evidence. No direct corpus evidence found for this specific claim, though related work on LLM ranking exists.
- Break condition: If the method's effectiveness is not inversely proportional to the baseline quality of the model, or if positional biases do not significantly affect both sorting and reranking tasks.

## Foundational Learning

- Concept: Kendall tau distance and correlation
  - Why needed here: The method uses Kendall tau distance to measure dissimilarity between rankings and Kendall tau correlation to quantify list order similarity.
  - Quick check question: What is the relationship between Kendall tau distance and Kendall tau correlation, and how are they used in the context of rank aggregation?

- Concept: Kemeny-Young optimal ranking
  - Why needed here: The method uses Kemeny-Young optimal ranking as the aggregation method, which minimizes the sum of Kendall tau distances to all sampled rankings.
  - Quick check question: How does the Kemeny-Young optimal ranking method work, and what are its theoretical properties in the context of rank aggregation?

- Concept: Positional bias in language models
  - Why needed here: The method aims to mitigate positional biases in LLMs, which are sensitivities to input list order that can affect ranking quality.
  - Quick check question: What are positional biases in language models, and how do they affect ranking quality in listwise ranking tasks?

## Architecture Onboarding

- Component map: Input list -> Shuffling module -> LLM -> Multiple rankings -> Kemeny-Young aggregation -> Aggregated ranking
- Critical path:
  1. Generate random permutations of the input list
  2. Pass each permuted list through the LLM to generate rankings
  3. Aggregate the generated rankings using Kemeny-Young optimal ranking
  4. Return the aggregated ranking as the final output
- Design tradeoffs:
  - Number of permutations vs. computational cost: More permutations lead to better aggregation but increase computational cost
  - Choice of aggregation method: Kemeny-Young optimal ranking is computationally expensive but theoretically robust
  - LLM quality vs. positional bias: Higher-quality LLMs may have less positional bias, reducing the need for aggregation
- Failure signatures:
  - Poor aggregation quality: The aggregated ranking does not significantly improve upon individual rankings
  - High computational cost: The method becomes too expensive for practical use with large input lists or many permutations
  - Sensitivity to noise: The method fails to converge to the true ranking under high levels of noise or when no correctly ordered pairs exist in observations
- First 3 experiments:
  1. Evaluate the method's effectiveness on a simple sorting task with a known ground truth ranking
  2. Compare the method's performance with and without the aggregation step to assess the importance of rank aggregation
  3. Vary the number of permutations and assess the trade-off between aggregation quality and computational cost

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored:
- The theoretical limit of the number of permutations needed for optimal performance
- How the method performs on other types of listwise ranking tasks beyond sorting and passage reranking
- The method's effectiveness when applied to non-language models or other machine learning models that generate rankings
- The impact of different aggregation methods (e.g., majority voting, weighted averages) on performance

## Limitations
- Theoretical robustness claims rely on specific noise distribution assumptions that may not hold in real-world LLM ranking scenarios
- Kemeny-Young aggregation has factorial complexity, raising scalability concerns for longer lists
- Limited comparison with other state-of-the-art ranking methods, particularly for passage reranking tasks

## Confidence
- Theoretical claims about convergence: Medium confidence - the proof assumes uniform random concordant subsets which may be overly optimistic for actual LLM behavior
- Empirical results on sorting tasks: High confidence - improvements are consistent and substantial across multiple datasets
- Empirical results on passage reranking: Medium confidence - results are positive but limited to two datasets with no comparison to recent specialized reranking methods

## Next Checks
1. Test scalability limits by evaluating performance degradation as list length increases beyond the paper's tested range
2. Compare permutation self-consistency against direct fine-tuning approaches for ranking tasks on the same datasets
3. Analyze the distribution of positional biases across different list positions to verify the uniformity assumption in the theoretical proof