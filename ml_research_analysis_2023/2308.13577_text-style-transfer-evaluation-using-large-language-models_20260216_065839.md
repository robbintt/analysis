---
ver: rpa2
title: Text Style Transfer Evaluation Using Large Language Models
arxiv_id: '2308.13577'
source_url: https://arxiv.org/abs/2308.13577
tags:
- evaluation
- style
- human
- sentence
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of Large Language Models (LLMs) for
  evaluating Text Style Transfer (TST), focusing on style transfer accuracy, content
  preservation, and fluency. The authors compare the performance of multiple LLMs,
  including InstructGPT, OPT, BLOOM, and GPT3, using various zero-shot prompts and
  prompt ensembling.
---

# Text Style Transfer Evaluation Using Large Language Models

## Quick Facts
- arXiv ID: 2308.13577
- Source URL: https://arxiv.org/abs/2308.13577
- Reference count: 40
- Key outcome: InstructGPT correlates strongly with human evaluations and often surpasses traditional automated metrics for TST assessment

## Executive Summary
This study explores using Large Language Models (LLMs) to evaluate Text Style Transfer (TST) tasks across three dimensions: style transfer accuracy, content preservation, and fluency. The authors compare multiple LLMs including InstructGPT, OPT, BLOOM, and GPT3 using zero-shot prompting and prompt ensembling techniques. Results demonstrate that InstructGPT, particularly with ensembled prompts, achieves the highest correlation with human evaluations and often outperforms traditional automated metrics. The study establishes LLMs as a promising unified evaluation method for TST while highlighting limitations like unparsable answers and out-of-range scores.

## Method Summary
The study evaluates TST model outputs (CAAE, ARAE, DAR) from the Yelp dataset using human evaluation scores from Mir et al. (2019) as ground truth. For each TST model output, 11 prompts per evaluation aspect are used to generate scores from multiple LLMs through zero-shot prompting. Scores are normalized and ensembled through uniform averaging across prompts. Spearman rank correlation is computed between LLM-generated scores and human evaluations for each aspect and model combination. The evaluation covers style transfer accuracy, content preservation, and fluency across four LLM models.

## Key Results
- InstructGPT achieves the highest correlation with human evaluations (ρ = 0.85 for style accuracy)
- Prompt ensembling improves robustness over single prompts
- InstructGPT produces fewer unparsable answers and out-of-range scores than other models
- Zero-shot prompting performs comparably to or better than traditional automated metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with RLHF fine-tuning (e.g., InstructGPT) align better with human evaluations than non-RLHF models.
- Mechanism: RLHF alignment teaches the model to better interpret human evaluation intents in prompts, yielding more consistent and parsable responses.
- Core assumption: RLHF improves understanding of human intent in evaluation tasks.
- Evidence anchors:
  - [abstract] "Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks."
  - [section] "We attribute the superior performance of InstructGPT to the further alignment achieved through RLHF fine-tuning."
  - [corpus] Weak – only mentions general LLMs; no corpus-specific RLHF evidence.
- Break condition: If RLHF alignment degrades for more nuanced or domain-specific evaluation tasks.

### Mechanism 2
- Claim: Prompt ensembling increases robustness by averaging across multiple prompt formulations.
- Mechanism: Averaging normalizes out prompt-specific biases, yielding scores less sensitive to wording changes.
- Core assumption: Different prompts capture slightly different aspects of the same underlying evaluation score.
- Evidence anchors:
  - [abstract] "Furthermore, we introduce the concept of prompt ensembling, demonstrating its ability to enhance the robustness of TST evaluation."
  - [section] "We normalize the scores and afterward apply a uniform averaging across the prompts per aspect."
  - [corpus] Weak – no corpus evidence of prompt ensembling directly applied to TST.
- Break condition: If prompts are too divergent in intent, averaging could blur distinct aspects.

### Mechanism 3
- Claim: Zero-shot prompting with InstructGPT yields scores more correlated with human judgments than automated metrics like BLEU or perplexity.
- Mechanism: InstructGPT’s pretrained knowledge of language quality aligns with human perceptual standards.
- Core assumption: InstructGPT’s training corpus includes enough human-written examples for style transfer evaluation.
- Evidence anchors:
  - [abstract] "Our findings indicate that (even zero-shot) prompting correlates strongly with human evaluation and often surpasses the performance of (other) automated metrics."
  - [section] "InstructGPT has the highest correlation to human evaluations compared to the other automated style transfer accuracy metrics."
  - [corpus] Weak – only general references to LLMs in corpus.
- Break condition: If task requires specialized domain knowledge not covered in pretraining.

## Foundational Learning

- Concept: Spearman rank correlation
  - Why needed here: Evaluating how well LLM scores match human rankings across TST models.
  - Quick check question: What is the difference between Pearson and Spearman correlation in this context?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Explains why InstructGPT outperforms non-RLHF models.
  - Quick check question: How does RLHF change the model’s behavior compared to standard pretraining?

- Concept: Prompt engineering and ensembling
  - Why needed here: Designing prompts that extract consistent numerical scores from LLMs.
  - Quick check question: What are the trade-offs between using many diverse prompts vs. a single optimized prompt?

## Architecture Onboarding

- Component map: LLM prompt interface → parsing regex → score normalization → ensembling module → correlation calculation with human scores
- Critical path: Prompt → LLM → Parse → Validate → Normalize → Average → Compare to human evaluation
- Design tradeoffs: Larger models give better parsability but higher cost; ensembling adds robustness but increases latency
- Failure signatures: Unparsable answers → prompt format issue; out-of-range scores → prompt constraint mismatch; low correlation → model mismatch
- First 3 experiments:
  1. Test single prompt on InstructGPT vs. GPT3 for style accuracy
  2. Add ensembling and measure correlation improvement
  3. Vary prompt temperature and observe effect on answer consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM sizes affect the reliability and correlation with human evaluations for TST tasks?
- Basis in paper: [inferred] The paper discusses the use of various LLM sizes, including OPT models ranging from 125m to 175b parameters, and observes that larger models tend to have higher parsable rates and lower out-of-range scores.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of LLM size on TST evaluation reliability and correlation with human evaluations.
- What evidence would resolve it: A detailed study comparing the performance of different LLM sizes on TST tasks, including their correlation with human evaluations and their ability to produce reliable scores.

### Open Question 2
- Question: How does the inclusion of explanations in LLM evaluations affect the interpretability and usefulness of the results?
- Basis in paper: [explicit] The paper mentions that LLM evaluations can potentially provide more explainable results by asking the LLM to add an explanation to the score, but it also notes that this can make parsing the results more difficult.
- Why unresolved: The paper does not explore the impact of including explanations in LLM evaluations on the interpretability and usefulness of the results.
- What evidence would resolve it: An analysis of the benefits and drawbacks of including explanations in LLM evaluations, including their impact on interpretability, reproducibility, and the ability to identify failure cases.

### Open Question 3
- Question: How do LLMs perform in evaluating TST tasks beyond sentiment transfer, such as formality transfer or politeness transfer?
- Basis in paper: [explicit] The paper focuses on sentiment transfer as the TST task and mentions that future work could apply the approach to other TST tasks like formality transfer or politeness transfer.
- Why unresolved: The paper does not evaluate the performance of LLMs on TST tasks other than sentiment transfer.
- What evidence would resolve it: A study comparing the performance of LLMs on different TST tasks, including their correlation with human evaluations and their ability to handle the specific challenges of each task.

## Limitations
- Limited dataset scope (Yelp only), raising questions about generalizability to other domains
- Small sample size (three TST models, one dataset) for evaluating evaluation methods
- No cross-linguistic validation across different languages or writing styles

## Confidence
**High confidence claims:**
- InstructGPT correlates more strongly with human evaluations than non-RLHF models (ρ = 0.85 vs. ~0.75-0.8 for others)
- Prompt ensembling improves robustness over single prompts
- InstructGPT produces fewer unparsable answers than other models
- Zero-shot prompting works reasonably well without task-specific fine-tuning

**Medium confidence claims:**
- InstructGPT "often surpasses" traditional automated metrics (BLEU, perplexity, etc.)
- RLHF alignment is the primary reason for InstructGPT's superior performance
- The improvements generalize beyond the Yelp dataset and tested models

**Low confidence claims:**
- No claims are explicitly low confidence, but generalizability statements lack supporting evidence

## Next Checks
- Test the proposed evaluation method on a different TST dataset (e.g., formality transfer, simplification, or sentiment modification) to verify if the correlation patterns hold across domains
- Systematically vary prompt wording, temperature, and format to identify which prompt features most strongly influence LLM evaluation scores and correlation with human judgments
- Design adversarial TST examples (mixed styles, highly creative transfers, domain-specific terminology) to determine where LLM evaluation breaks down and identify early warning signs of unreliable assessments