---
ver: rpa2
title: 'More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually
  Diverse Images of Parsons Problems'
arxiv_id: '2311.04926'
source_url: https://arxiv.org/abs/2311.04926
tags:
- problems
- parsons
- visual
- code
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are capable of solving a wide range of problems,
  including programming problems, which has raised concerns about academic integrity.
  To address these concerns, researchers have suggested using visual programming problems
  as a potential barrier to cheating.
---

# More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems

## Quick Facts
- arXiv ID: 2311.04926
- Source URL: https://arxiv.org/abs/2311.04926
- Reference count: 40
- Primary result: GPT-4V solved 96.7% of visual Parsons problems across diverse formats

## Executive Summary
Large multimodal models (LMMs) can effectively solve visually diverse images of Parsons problems, a type of programming puzzle where code snippets must be arranged in the correct logical sequence. This study evaluated GPT-4V and Bard on Parsons problems presented in four different visual formats, finding that GPT-4V achieved a 96.7% success rate while Bard solved 69.2% of problems. The results demonstrate that visual representations alone may not provide adequate protection against AI-assisted cheating in programming assessments, as LMMs can process and solve these problems regardless of visual complexity.

## Method Summary
The study used six Parsons problems from existing literature, converting each into four visual representations using common Parsons problem generation tools (JS-Parsons, Runestone, Epplets, EvoParsons). Researchers tested GPT-4V and Bard on these problems using a structured prompt that guided the models through image extraction, problem identification, and code reordering. Each problem-representation pair was attempted five times without regeneration, yielding 120 responses per model. The evaluation focused on correctness rates and identified patterns in model performance across different visual formats.

## Key Results
- GPT-4V solved 96.7% of visual Parsons problems with minimal difficulty across all formats
- Bard achieved a 69.2% success rate, with significant performance variation across visual representations
- Bard struggled particularly with the Epplets format, solving only 50% of problems due to visual complexity and UI clutter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can accurately parse and solve visual Parsons problems across diverse formats.
- Mechanism: GPT-4V uses multimodal processing to extract text from images, identify code blocks, and reorder them logically, bypassing traditional copy-paste barriers.
- Core assumption: GPT-4V's vision-language model can reliably extract code from varied visual layouts without semantic loss.
- Evidence anchors:
  - GPT-4V successfully solved 116 out of 120 attempts, struggling minimally with a single Parsons problem
  - Performance was consistent across all four visual representations
- Break condition: Visual complexity overwhelms OCR accuracy or semantic parsing

### Mechanism 2
- Claim: Visual complexity and UI clutter degrade Bard's performance more than GPT-4V's.
- Mechanism: Bard's multimodal processing is more sensitive to extraneous visual elements that interfere with text extraction and code block recognition.
- Core assumption: Bard's vision-language integration has lower robustness to visual noise compared to GPT-4V.
- Evidence anchors:
  - Bard solved only 50% of Epplets problems due to small font size and visual stimuli
  - 7.5% refusal rate when visual complexity exceeded processing capacity
- Break condition: Simplified visual layouts restore Bard's performance

### Mechanism 3
- Claim: Prompt engineering significantly influences LMM performance on visual programming tasks.
- Mechanism: Explicit, step-by-step instructions guide models to correctly interpret Parsons problems and avoid refusals or hallucinations.
- Core assumption: LMMs require precise prompting to leverage their multimodal capabilities effectively.
- Evidence anchors:
  - Initial generic prompts yielded only 50% performance for GPT-4V
  - Structured prompts with explicit instructions improved success rates significantly
- Break condition: Generic or ambiguous prompts lead to high refusal rates

## Foundational Learning

- **Multimodal large language models (LMMs)**: Understanding how GPT-4V and Bard process text and image data together is crucial to interpreting their success on visual Parsons problems. *Quick check*: What are the two main data types that LMMs can process simultaneously in this study?
- **Parsons problems**: Knowing the structure and purpose of Parsons problems clarifies why visual representations pose a barrier to traditional LLMs and how LMMs overcome it. *Quick check*: What is the primary task students must perform when solving a Parsons problem?
- **Prompt engineering**: Recognizing the role of prompt design explains the variation in model performance and guides future experiments. *Quick check*: What are the three key components of the effective prompt used in this study?

## Architecture Onboarding

- **Component map**: Image → Text extraction → Code block identification → Logical reordering → Output generation
- **Critical path**: Vision module extracts text from images → Language module understands code structure → Reordering logic arranges code blocks → Output generation produces solution
- **Design tradeoffs**: High visual fidelity vs. model robustness to clutter; explicit prompts vs. model autonomy
- **Failure signatures**: High refusal rates, hallucinations (rewriting code blocks), incorrect sequencing, inability to extract text from complex visuals
- **First 3 experiments**:
  1. Test GPT-4V and Bard on a simple Parsons problem with minimal visual noise; compare success rates
  2. Gradually increase visual complexity (add UI elements, different fonts) and measure performance degradation
  3. Vary prompt specificity (generic vs. explicit step-by-step) and observe impact on refusal rates and correctness

## Open Questions the Paper Calls Out

### Open Question 1
What specific aspects of the Epplets visual representation contribute to Bard's reduced performance compared to other formats? The paper suggests complexity, small font size, and additional text input windows may be factors, but does not definitively identify the specific elements causing the issue. Systematic comparison of different visual elements within Epplets could resolve this.

### Open Question 2
How does the performance of other large multimodal models compare to GPT-4V and Bard on visual programming problems? The paper evaluated only two LMMs, limiting generalizability. Testing a wider range of LMMs would provide more comprehensive insights into their relative strengths and weaknesses.

### Open Question 3
How do student prompting behaviors influence the performance of large multimodal models on visual programming problems? The paper suggests effectiveness varies with prompting quality and raises concerns about unequal benefits if some students are better at prompting than others. Analyzing student prompting behaviors and correlating them with LMM performance would provide insights into this dynamic.

## Limitations
- Dataset consists of only six Parsons problems, raising questions about generalizability to broader problem types and difficulty levels
- Study does not explore the full parameter space of prompt variations or investigate optimal prompting strategies
- Findings are limited to Python-focused problems and do not test cross-language generalization

## Confidence
- **High confidence**: GPT-4V's superior performance across all visual formats is well-supported by the 96.7% success rate and consistent results
- **Medium confidence**: Mechanism explanations for Bard's sensitivity to visual complexity are plausible but not definitively proven through controlled experiments
- **Low confidence**: Broader pedagogical implications extend beyond empirical findings and lack evidence from real classroom settings

## Next Checks
1. **Prompt Optimization Study**: Systematically vary prompt structure and specificity across a larger problem set to establish optimal strategies and quantify upper bounds of LMM performance
2. **Visual Complexity Gradient**: Create controlled series of Parsons problems with incrementally increasing visual complexity to measure performance degradation curves and establish robustness thresholds
3. **Cross-Language Generalization**: Test both models on Parsons problems across multiple programming languages and complexity levels to determine whether performance patterns hold beyond Python-focused problems