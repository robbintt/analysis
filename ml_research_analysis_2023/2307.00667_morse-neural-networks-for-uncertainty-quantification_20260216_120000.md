---
ver: rpa2
title: Morse Neural Networks for Uncertainty Quantification
arxiv_id: '2307.00667'
source_url: https://arxiv.org/abs/2307.00667
tags:
- morse
- network
- kernel
- neural
- modes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Morse neural networks are introduced as a flexible deep generative
  model for uncertainty quantification. They generalize Gaussian densities to have
  modes on high-dimensional submanifolds rather than discrete points, enabling more
  expressive density estimation.
---

# Morse Neural Networks for Uncertainty Quantification

## Quick Facts
- arXiv ID: 2307.00667
- Source URL: https://arxiv.org/abs/2307.00667
- Reference count: 16
- Key outcome: Morse neural networks achieve AUROCs of 0.998 and 0.969 for OOD detection on MNIST/FashionMNIST and CIFAR10/100 respectively

## Executive Summary
Morse neural networks are introduced as a flexible deep generative model for uncertainty quantification that generalizes Gaussian densities to have modes on high-dimensional submanifolds rather than discrete points. By using a neural network to map inputs to a latent space and applying Morse kernels, the method creates flexible density estimates that concentrate on level sets of the neural network. The model unifies unsupervised and supervised techniques for OOD detection, calibration, and generative sampling, with experimental results demonstrating strong OOD detection performance on standard benchmarks.

## Method Summary
The method uses a neural network ϕθ to map inputs to a latent space Z, then applies a Morse kernel K to create a density µθ(x) = K(ϕθ(x), a) that concentrates on level sets of dimension d-k. Training is performed via KL divergence minimization between the data distribution and the Morse network density, with uniform sampling regularization. The approach yields multiple uncertainty quantification tools: OOD detection via s(x) = 1-µθ(x), calibration temperature T(x) = 1/µθ(x), and generative sampling through gradient flow. Supervised variants incorporate labels to learn per-class modes using joint densities.

## Key Results
- AUROC of 0.998 for MNIST vs FashionMNIST OOD detection
- AUROC of 0.969 for CIFAR10 vs CIFAR100 OOD detection
- Unsupervised Morse networks produce OOD detectors and distance-aware calibration temperatures from a single KL divergence fit
- Supervised Morse networks with Laplace kernel yield classifiers matching entropic OOD detection methods

## Why This Works (Mechanism)

### Mechanism 1
The Morse network generalizes Gaussian densities by placing modes on high-dimensional submanifolds rather than discrete points, enabling more expressive density estimation. By using a neural network ϕθ to map inputs to a latent space Z and applying a Morse kernel K(ϕθ(x), a), the density µθ(x) = K(ϕθ(x), a) concentrates on level sets {x : ϕθ(x) = a} of dimension d-k, creating flexible mode submanifolds.

### Mechanism 2
Fitting the Morse network via KL divergence yields multiple uncertainty quantification tools simultaneously: generative density, OOD detector, calibration temperature, and generative sampler. Minimizing KL(p∥µθ) = Ex∼p(-log K(ϕθ(x),a)) + Ex∼uni(K(ϕθ(x),a)) optimizes the network to concentrate density on data modes while regularizing with uniform samples, producing s(x) = 1-µθ(x) as OOD score and T(x) = 1/µθ(x) as calibration temperature.

### Mechanism 3
Supervised Morse networks with Laplace kernel produce distance-aware classifiers that coincide with entropic OOD detection methods. Using joint density µ(x,y) = K(ϕθ(x),T(y)) where T(y) is one-hot encoding, the conditional µ(y|x) = K(ϕθ(x),y)/Σy'K(ϕθ(x),y') creates a classifier whose entropy naturally measures distance from training distribution modes.

## Foundational Learning

- **Concept: Morse theory and Morse-Bott condition**
  - Why needed here: The Morse network density is based on level sets of ϕθ, and the Morse-Bott condition ensures the negative log density Vθ(x) = -log µθ(x) is locally a squared distance from the mode submanifold
  - Quick check question: What property must a kernel K satisfy to be called a Morse kernel, and why is this important for the network's density structure?

- **Concept: KL divergence for unnormalized densities**
  - Why needed here: The training objective minimizes KL(p∥µθ) for unnormalized densities, requiring understanding of how to handle the partition function terms
  - Quick check question: In the empirical loss L(θ) = -1/n Σx∈D log K(ϕθ(x),a) + 1/n Σx∈Duni K(ϕθ(x),a), what role does the uniform sampling term play?

- **Concept: Bregman divergences and exponential family connections**
  - Why needed here: The paper shows that Morse networks with exponential family Morse kernels (K(z1,z2) = exp(-λD(z1,z2))) are unnormalized versions of exponential family densities
  - Quick check question: How does choosing A(z) = 1/2 z² in the Bregman divergence framework recover the Gaussian case in the Morse network?

## Architecture Onboarding

- **Component map**: Input feature space X = Rd -> Neural network ϕθ: X → Z -> Morse kernel K: Z × Z → [0,1] -> Density µθ(x) = K(ϕθ(x), a) -> Applications (OOD detector, calibration temperature, generative sampler)
- **Critical path**: Input → ϕθ → K(ϕθ(x), a) → µθ(x) → applications
- **Design tradeoffs**:
  - Kernel choice: Gaussian (smooth, local) vs Laplace (sharper boundaries) vs Cauchy (heavy-tailed)
  - Output dimension of ϕθ: k < d gives mode submanifolds of dimension d-k, k > d enables mixture models
  - Regularization strength from uniform sampling term in loss
- **Failure signatures**:
  - Poor OOD detection: µθ(x) not concentrated enough on data modes
  - Overconfident calibration: Temperature T(x) = 1/µθ(x) not varying sufficiently with distance
  - Unstable training: Loss not decreasing or gradients exploding
- **First 3 experiments**:
  1. Train unsupervised Morse on 2D synthetic data (e.g., two moons) with Gaussian kernel, visualize µθ(x) heatmap and verify modes match data structure
  2. Use trained Morse network to create OOD detector, test on in-distribution vs out-of-distribution points, compute AUROC
  3. Apply Morse temperature T(x) = 1/µθ(x) to calibrate classifier logits, verify increased uncertainty for OOD inputs

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of kernel (e.g., Gaussian vs Laplace vs Cauchy) affect the topology of the learned mode submanifolds and the resulting OOD detection performance? While the paper provides theoretical insights into how different kernels relate to various density models, it does not empirically investigate how these choices affect the learned mode topology or OOD detection in practice.

### Open Question 2
Can the Morse neural network be effectively extended to handle high-dimensional data with complex, disconnected mode structures without relying on pre-trained embeddings? The paper demonstrates strong OOD detection performance using pre-trained vision transformer embeddings, but notes that training directly on CIFAR10 images yields lower performance, suggesting challenges in learning complex modes directly from high-dimensional data.

### Open Question 3
What is the precise relationship between the negative log density Vθ(x) = -log µθ(x) and the topological invariants of the data distribution's mode submanifolds? While the theoretical framework suggests a connection to topology, the paper does not develop practical methods to compute or leverage topological invariants from the learned Vθ(x).

## Limitations
- The method's effectiveness on non-vision domains or raw input features requires further validation
- Training stability and computational efficiency for very large datasets are not thoroughly examined
- The assumption that neural network level sets can approximate complex data modes as smooth submanifolds is computationally challenging

## Confidence
- **Medium** in the theoretical claims connecting Morse theory to deep generative modeling
- **Medium** in the experimental validation on standard benchmarks
- **Low** in the scalability claims for very large datasets

## Next Checks
1. Evaluate Morse networks on non-vision domains (e.g., tabular data, time series, or text embeddings) to assess whether the OOD detection performance generalizes beyond image datasets.

2. Test training stability and runtime efficiency on larger datasets (e.g., ImageNet-scale) to verify the method's practical viability for real-world applications.

3. Implement a systematic study of how kernel choice (Gaussian, Laplace, Cauchy) and bandwidth selection affect the Morse network's ability to capture complex data modes, comparing against theoretical predictions about mode submanifold regularity.