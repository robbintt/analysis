---
ver: rpa2
title: 'PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning'
arxiv_id: '2311.08711'
source_url: https://arxiv.org/abs/2311.08711
tags:
- language
- response
- pivot
- plug
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PLUG, a simple yet effective approach for instruction
  tuning of large language models in lower-resource languages. PLUG uses a high-resource
  pivot language like English as an intermediary step - the model first interprets
  the instruction in the pivot language and generates a pivot response, before producing
  the final target language response.
---

# PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning

## Quick Facts
- **arXiv ID**: 2311.08711
- **Source URL**: https://arxiv.org/abs/2311.08711
- **Reference count**: 17
- **Primary result**: PLUG achieves 32% improvement for LLaMA-2 and 28% for PolyLM on cross-lingual instruction following tasks

## Executive Summary
PLUG introduces a simple yet effective approach for instruction tuning of large language models in lower-resource languages by leveraging high-resource pivot languages like English. The method involves first interpreting the instruction in the pivot language and generating a pivot response, before producing the final target language response. Experiments on four target languages demonstrate significant performance improvements over monolingual response training, with average 32% gains for LLaMA-2 and 28% for PolyLM on open-ended instruction following tasks. The approach also improves truthfulness on the TruthfulQA benchmark.

## Method Summary
PLUG trains models to generate pivot instruction, pivot response, then target response in one pass. The training format combines pivot language data with PLUG-formatted target language data, where the model first interprets instructions in the target language, formulates responses in the pivot language, and then generates final target language responses. The approach uses a 4-epoch training schedule with batch size 64 and learning rate 5e-6, evaluated using GPT-4 pairwise comparison with position-swapped responses to calculate win-loss differentials.

## Key Results
- PLUG achieves 32% improvement for LLaMA-2 and 28% for PolyLM on open-ended instruction following tasks
- The method significantly outperforms monolingual response training across all four tested languages
- PLUG also improves truthfulness scores on the TruthfulQA benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pivot language processing reduces comprehension difficulty for low-resource languages.
- Mechanism: The model first interprets the instruction in a high-resource pivot language, leveraging its stronger foundational abilities in that language before generating the target response.
- Core assumption: LLMs have significantly better instruction comprehension abilities in high-resource languages compared to low-resource ones due to pre-training data imbalance.
- Evidence anchors: [abstract] "The straightforward strategy entails training LLMs to perform monolingual response generation... However, this endeavor is fraught with challenges... the response quality frequently falls short when compared to those produced for similar instructions in a high-resource language."

### Mechanism 2
- Claim: Pivot response quality improves target response quality through contextual guidance.
- Mechanism: The generated pivot response provides a high-quality intermediate representation that guides the subsequent generation of the target response.
- Core assumption: The model's ability to generate coherent and relevant responses in the pivot language translates into better guidance for target language generation.
- Evidence anchors: [section 5.3] "The inclusion of the pivot response is a crucial contributor to the models' improvements... the preceding pivot response provides valuable guidance for the subsequent response in the target language."

### Mechanism 3
- Claim: Maintaining pivot language proficiency ensures the model doesn't lose capability in the pivot language while improving target language performance.
- Mechanism: By including pivot language data in training alongside PLUG-formatted target language data, the model preserves its instruction-following abilities in the pivot language.
- Core assumption: Training on pivot language instructions and responses prevents catastrophic forgetting of pivot language capabilities.
- Evidence anchors: [section 4.3] "For each LLM evaluated, we train the model with the following methods... PLUG (our approach). Trained on monolingual response data for the pivot language, and the PLUG-formatted data for the target language."

## Foundational Learning

- **Concept: Zero-shot cross-lingual transfer**
  - Why needed here: Understanding how knowledge from high-resource languages can transfer to low-resource languages without direct supervision
  - Quick check question: What is the key difference between zero-shot cross-lingual transfer and traditional fine-tuning approaches?

- **Concept: Instruction tuning**
  - Why needed here: The paper builds on instruction tuning as the foundation for making LLMs follow human instructions
  - Quick check question: How does instruction tuning differ from standard language model pre-training?

- **Concept: Catastrophic forgetting**
  - Why needed here: Understanding why PLUG includes pivot language data to prevent the model from losing its capabilities in the pivot language
  - Quick check question: What training strategy could prevent a model from forgetting previously learned tasks when learning new ones?

## Architecture Onboarding

- **Component map**: Input layer (target language instruction) → Processing layer (pivot instruction generation → pivot response generation) → Output layer (target response generation)
- **Critical path**: Instruction (target) → Pivot instruction (pivot) → Pivot response (pivot) → Target response (target)
- **Design tradeoffs**: PLUG adds complexity by requiring bilingual generation but improves performance; alternative of using external translation tools would require additional components and inference steps; PLUG-PRO variant removes pivot instruction generation for efficiency at some performance cost
- **Failure signatures**: If pivot instruction generation is poor, target response quality degrades; if pivot and target languages are too dissimilar, guidance transfer becomes ineffective; if training data is imbalanced, pivot language proficiency may degrade
- **First 3 experiments**: 1) Test PLUG vs monolingual response training on a simple translation task to verify the basic mechanism; 2) Test different pivot languages to verify versatility across language families; 3) Test PLUG-PRO variant to understand the contribution of pivot instruction vs pivot response

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PLUG compare to other cross-lingual transfer methods like zero-shot cross-lingual transfer or meta-learning approaches?
- Basis in paper: [explicit] The paper mentions pivot-only training (zero-shot cross-lingual transfer) as a baseline, but does not compare PLUG to other cross-lingual transfer methods.
- Why unresolved: The paper focuses on comparing PLUG to monolingual response training and code-switching, but does not explore other cross-lingual transfer techniques that could potentially outperform PLUG.
- What evidence would resolve it: Experiments comparing PLUG to other cross-lingual transfer methods like meta-learning or multilingual pre-training on the same evaluation benchmarks.

### Open Question 2
- Question: How does the choice of pivot language affect the performance of PLUG for different target languages?
- Basis in paper: [explicit] The paper shows that English is the most effective pivot language for the tested multilingual model, but other languages can also serve as pivots. However, it does not provide a systematic analysis of how different pivot languages impact performance across various target languages.
- Why unresolved: The paper only tests a few pivot languages and target languages, leaving the broader question of pivot language selection unexplored.
- What evidence would resolve it: Comprehensive experiments testing multiple pivot languages for each target language, along with an analysis of linguistic similarities and pre-training data distributions.

### Open Question 3
- Question: How does the length of the pivot instruction and response affect the performance of PLUG?
- Basis in paper: [inferred] The paper mentions that extremely long instructions could be inefficient or exceed length constraints when using PLUG, but does not provide a detailed analysis of how instruction and response length impact performance.
- Why unresolved: The paper does not explore the relationship between instruction/response length and PLUG's effectiveness, leaving the optimal length for different tasks unclear.
- What evidence would resolve it: Experiments varying the length of pivot instructions and responses, measuring the impact on target language response quality across different task types and instruction lengths.

## Limitations

- Limited evaluation to only four target languages, leaving generalizability to truly low-resource languages uncertain
- Reliance on GPT-4 automated evaluation may miss cultural and pragmatic nuances that human evaluation would catch
- Implementation details like prompt formatting and indicator tokens are not fully specified, making exact reproduction challenging

## Confidence

- **High Confidence (8/10)**: PLUG's general mechanism of using pivot languages improves cross-lingual instruction following; PLUG outperforms traditional monolingual response training on the tested benchmarks; maintaining pivot language data in training prevents degradation of pivot language capabilities
- **Medium Confidence (6/10)**: PLUG's effectiveness across different pivot languages beyond English; the 32% and 28% improvement metrics are robust across different evaluation settings; PLUG-PRO variant provides meaningful efficiency improvements without substantial quality loss
- **Low Confidence (4/10)**: Performance guarantees for truly low-resource languages not represented in the evaluation set; scalability of PLUG to handle extremely long instructions in target languages; long-term stability of model capabilities when fine-tuned extensively with PLUG

## Next Checks

1. **Cross-Linguistic Generalization Test**: Implement PLUG for language pairs with minimal structural similarity (e.g., English→Vietnamese or English→Swahili) to verify the mechanism works beyond Indo-European languages.

2. **Ablation Study with Human Evaluation**: Conduct comprehensive human evaluation comparing PLUG, monolingual training, and PLUG-PRO across the four tested languages, focusing on pragmatic aspects like cultural appropriateness, idiomatic naturalness, and instruction comprehension.

3. **Long-Term Stability Analysis**: Train PLUG models for extended periods (20+ epochs) while monitoring pivot language proficiency decay using perplexity measurements and human evaluation to verify catastrophic forgetting prevention holds under intensive training scenarios.