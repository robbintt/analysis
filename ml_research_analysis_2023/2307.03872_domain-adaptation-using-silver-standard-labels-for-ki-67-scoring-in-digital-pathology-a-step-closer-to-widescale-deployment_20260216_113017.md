---
ver: rpa2
title: 'Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital
  Pathology: A Step Closer to Widescale Deployment'
arxiv_id: '2307.03872'
source_url: https://arxiv.org/abs/2307.03872
tags:
- target
- domain
- ki-67
- labels
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of domain adaptation for Ki-67
  scoring in digital pathology, where deep learning models often underperform when
  applied to out-of-domain data from different laboratories. The proposed solution
  is a self-supervised domain adaptation pipeline that generates pseudo (silver standard)
  labels in the target domain using an unsupervised immunohistochemical color histogram
  method.
---

# Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment

## Quick Facts
- arXiv ID: 2307.03872
- Source URL: https://arxiv.org/abs/2307.03872
- Reference count: 12
- Key outcome: Self-supervised domain adaptation using silver standard labels significantly improves Ki-67 scoring accuracy on out-of-domain data, achieving 95.9% proliferation index accuracy.

## Executive Summary
This study addresses the challenge of domain adaptation for Ki-67 scoring in digital pathology, where deep learning models often underperform when applied to out-of-domain data from different laboratories. The proposed solution is a self-supervised domain adaptation pipeline that generates pseudo (silver standard) labels in the target domain using an unsupervised immunohistochemical color histogram method. These pseudo labels are then used to pre-train the model, followed by fine-tuning on gold standard labels from the source domain. Experiments with two Ki-67 scoring architectures (UV-Net and piNET) across five training regimes show that the proposed SS+GS method significantly outperforms the baseline GS Only method, achieving a 95.9% proliferation index accuracy and more consistent results on target data. T-SNE analysis further confirms that the SS+GS model learns features better aligned across source and target domains, improving generalization. This approach enables per-laboratory calibration without manual annotations, facilitating widescale deployment of Ki-67 scoring tools in clinical settings.

## Method Summary
The proposed self-supervised domain adaptation pipeline consists of two main stages: (1) generating silver standard pseudo labels in the target domain using an unsupervised immunohistochemical color histogram (IHCCH) method, and (2) training a deep learning model using a combination of these pseudo labels and gold standard labels from the source domain. The IHCCH algorithm processes target domain tissue microarray images to identify nuclei and classify them as Ki-67 positive or negative without manual annotation. The model is then trained using five different regimes: GS Only (gold standard only), SS Only (silver standard only), Mixed (both types combined), GS+SS (gold standard first, then silver standard), and SS+GS (silver standard first, then gold standard). The SS+GS approach, which pre-trains on target domain pseudo labels before fine-tuning on source domain gold labels, demonstrates the best performance.

## Key Results
- SS+GS training regime achieves 95.9% proliferation index accuracy on target domain data
- SS+GS method significantly outperforms GS Only baseline with more consistent results across cross-validation folds
- t-SNE analysis shows better feature alignment between source and target domains for SS+GS trained models
- SS+GS approach reduces domain gap while maintaining high accuracy through two-stage training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on target domain pseudo labels (SS) allows the model to learn domain-specific low-level features before fine-tuning on high-quality source labels (GS).
- Mechanism: The self-supervised pipeline first generates pseudo labels in the target domain using an unsupervised immunohistochemical color histogram method. These labels, though noisy, capture the unique staining and tissue characteristics of the target domain. By pre-training on this data, the model adapts its parameters to the target distribution before being refined with clean source labels, reducing domain shift.
- Core assumption: Noisy pseudo labels still contain useful structural and feature-level information that can bootstrap domain adaptation.
- Evidence anchors:
  - [abstract]: "pre-trains a network on pseudo labels from the target domain will allow a network to first learn dataset-specific characteristics and low-level features that are task-dependent"
  - [section]: "This enables the model to learn the low-level nuclei features and attain optimal parameter initialization"
  - [corpus]: Weak match; no direct evidence in neighbor papers about Ki-67 or pseudo label pre-training for domain adaptation in pathology.
- Break condition: If pseudo labels are too noisy or systematically incorrect, pre-training may degrade rather than improve model performance.

### Mechanism 2
- Claim: Fine-tuning on clean source domain labels (GS) after pre-training on noisy target labels refines the model's feature extraction while preserving domain alignment.
- Mechanism: The SS+GS training order first exposes the network to target domain characteristics, then uses the clean, accurate source labels to fine-tune the model. This two-stage process combines domain-specific adaptation with high-quality feature learning, leading to better generalization on target data.
- Core assumption: The model can retain domain-adapted features while improving accuracy through fine-tuning with clean labels.
- Evidence anchors:
  - [abstract]: "Fine-tuning with GS (clean) labels from the source domain can then allow more detailed features to be captured by the network"
  - [section]: "This pipeline can be used to calibrate automated deep learning-based medical imaging tools on a per-dataset basis"
  - [corpus]: No direct evidence; corpus papers focus on source-free domain adaptation without pseudo labels.
- Break condition: If fine-tuning overrides domain-adapted features too aggressively, the benefit of pre-training on SS labels may be lost.

### Mechanism 3
- Claim: t-SNE analysis shows that SS+GS training produces more aligned feature representations between source and target domains compared to other training regimes.
- Mechanism: By visualizing high-dimensional features in 2D using t-SNE, the SS+GS method demonstrates overlapping distributions for source and target data, indicating reduced domain gap. This visual evidence supports the quantitative performance gains observed in nuclei detection and proliferation index accuracy.
- Core assumption: t-SNE plots accurately reflect feature alignment relevant to model generalization.
- Evidence anchors:
  - [abstract]: "Analysis of t-SNE plots showed features learned by the SS+GS models are more aligned for source and target data"
  - [section]: "features from the SS+GS model are similar across source and target domains, which likely resulted in improved generalization"
  - [corpus]: No evidence in corpus; t-SNE visualization is not mentioned in neighbor papers.
- Break condition: If t-SNE visualization is sensitive to hyperparameters or does not correlate with actual model performance, it may mislead interpretation.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: Ki-67 scoring models trained on one laboratory's data often fail on data from other laboratories due to variations in staining protocols, tissue processing, and imaging equipment.
  - Quick check question: What is the main challenge that domain adaptation methods aim to solve in medical imaging applications?

- Concept: Self-supervised learning with pseudo labels
  - Why needed here: Generating gold standard labels for target domain data is time-consuming and expensive. Self-supervised methods can generate silver standard labels without manual annotation, enabling scalable adaptation.
  - Quick check question: How do pseudo labels differ from gold standard labels in terms of accuracy and generation cost?

- Concept: Transfer learning with pre-training and fine-tuning
  - Why needed here: The SS+GS method leverages transfer learning by first pre-training on target domain data to learn domain-specific features, then fine-tuning on source domain data to improve accuracy with clean labels.
  - Quick check question: Why might pre-training on noisy pseudo labels followed by fine-tuning on clean labels be more effective than training only on clean labels?

## Architecture Onboarding

- Component map:
  - Unsupervised Ki-67 nuclei detection (IHCCH) → Pseudo label generation
  - Deep learning models (UV-Net, piNET) → Feature extraction and classification
  - Training regimes (GS Only, SS Only, Mixed, GS+SS, SS+GS) → Model optimization
  - t-SNE visualization → Feature alignment analysis

- Critical path:
  1. Generate pseudo labels in target domain using IHCCH
  2. Pre-train model on pseudo labels (SS)
  3. Fine-tune model on clean source labels (GS)
  4. Evaluate on held-out target data

- Design tradeoffs:
  - Using noisy pseudo labels risks propagating errors but enables adaptation without manual annotation
  - SS+GS training order prioritizes domain adaptation first, then accuracy refinement
  - Choice of deep learning architecture (UV-Net vs piNET) affects performance on specific tasks

- Failure signatures:
  - High variance in F1 scores across cross-validation folds indicates instability
  - Large proliferation index error (ΔPI) suggests poor generalization to target domain
  - Non-overlapping t-SNE clusters indicate persistent domain gap

- First 3 experiments:
  1. Train UV-Net with GS Only baseline to establish performance without adaptation
  2. Train UV-Net with SS+GS to test the proposed adaptation pipeline
  3. Compare t-SNE visualizations between GS Only and SS+GS models to assess feature alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for patch selection and augmentation to improve domain adaptation performance?
- Basis in paper: [inferred] The paper mentions that future studies will explore "refining patch selection" and "diversifying patient cohorts" as potential enhancements to improve performance.
- Why unresolved: The current study uses a fixed patch size and limited augmentation techniques (rotation and scaling). The impact of different patch selection strategies and more diverse augmentation methods on domain adaptation performance is not yet explored.
- What evidence would resolve it: Experiments comparing different patch selection strategies (e.g., varying patch size, random vs. strategic selection) and augmentation techniques (e.g., elastic deformations, color jittering) on domain adaptation performance.

### Open Question 2
- Question: How does the source of pseudo labels (SS) affect the domain adaptation performance?
- Basis in paper: [explicit] The paper states that future studies will explore "assessing SS label source domain effects" to understand its impact on domain adaptation.
- Why unresolved: The current study uses pseudo labels generated by the IHCCH method from the target domain. It is unclear how the performance would change if pseudo labels were generated from a different source or using a different method.
- What evidence would resolve it: Experiments comparing domain adaptation performance using pseudo labels from different sources (e.g., other unsupervised methods, semi-supervised methods) and domains (e.g., different laboratories).

### Open Question 3
- Question: How does the proposed SS+GS method compare to other state-of-the-art domain adaptation techniques, such as domain adversarial learning and self-supervised model distillation?
- Basis in paper: [explicit] The paper mentions that future studies will compare the proposed approach to "domain adversarial learning and self-supervised model distillation" to evaluate its effectiveness.
- Why unresolved: The current study only compares the proposed SS+GS method to baseline methods and other training configurations. It does not compare against other advanced domain adaptation techniques that have been proposed in the literature.
- What evidence would resolve it: Experiments comparing the SS+GS method to state-of-the-art domain adaptation techniques (e.g., domain adversarial neural networks, self-supervised model distillation) on the same dataset and evaluation metrics.

## Limitations
- The IHCCH algorithm's specific implementation details and hyperparameters are not fully specified, potentially affecting reproducibility.
- The study's evaluation is limited to two specific Ki-67 scoring architectures (UV-Net and piNET) and a relatively small sample of target domain annotations (10 TMAs).
- Applicability to other pathology tasks beyond Ki-67 scoring remains uncertain as the methodology appears Ki-67 specific.

## Confidence
- SS+GS training regime performance claims: High (supported by quantitative metrics and t-SNE visualizations)
- Generalization across different deep learning architectures: Medium (only two architectures tested)
- Applicability to other pathology tasks beyond Ki-67 scoring: Low (methodology appears Ki-67 specific)

## Next Checks
1. **Ablation study on IHCCH parameters**: Systematically vary key IHCCH algorithm parameters (background subtraction thresholds, adaptive radius settings) to quantify their impact on pseudo label quality and downstream model performance.

2. **Cross-architecture validation**: Test the SS+GS pipeline with additional deep learning architectures beyond UV-Net and piNET to assess generalizability of the approach.

3. **Real-world deployment simulation**: Evaluate model performance on a broader range of laboratory data with varying staining protocols, tissue processing methods, and imaging equipment to assess robustness in clinical deployment scenarios.