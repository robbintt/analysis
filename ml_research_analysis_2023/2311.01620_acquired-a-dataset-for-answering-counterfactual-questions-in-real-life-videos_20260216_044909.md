---
ver: rpa2
title: 'ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos'
arxiv_id: '2311.01620'
source_url: https://arxiv.org/abs/2311.01620
tags:
- video
- reasoning
- dataset
- videos
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACQUIRED, a video question answering dataset
  for evaluating multimodal counterfactual reasoning. The dataset contains 3.9K annotated
  videos covering diverse real-world events from both first and third-person viewpoints.
---

# ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos

## Quick Facts
- arXiv ID: 2311.01620
- Source URL: https://arxiv.org/abs/2311.01620
- Reference count: 20
- Dataset contains 3.9K annotated videos with 11K questions spanning physical, social, and temporal reasoning dimensions

## Executive Summary
This paper introduces ACQUIRED, a video question answering dataset specifically designed for evaluating multimodal counterfactual reasoning. The dataset contains 3.9K annotated videos covering diverse real-world events from both first-person (egocentric) and third-person (exocentric) viewpoints. Each video is annotated with questions targeting three reasoning dimensions: physical, social, and temporal. The questions are curated with a correct and distractor answer pair to evaluate the model's ability to understand subtle differences. State-of-the-art language-only and multimodal models are benchmarked on ACQUIRED, showing a significant performance gap (>13%) compared to humans, highlighting the challenging nature of multimodal counterfactual reasoning and the potential of ACQUIRED as a comprehensive and reliable benchmark for future research.

## Method Summary
The dataset is constructed by curating videos from two existing datasets (Oops! and Ego4D) and annotating them with counterfactual questions. For each video, annotators create questions targeting physical, social, or temporal reasoning dimensions, with each question having one correct and one distractor answer. The distractor is designed to be a minimal contrastive counterpart to the correct answer, forcing models to understand subtle visual differences. The dataset is split into 45% training, 5% development, and 50% test sets. State-of-the-art language-only models (DeBERTa, UnifiedQA, GPT series) and multimodal models (VIOLET, VALOR, VL-Adapter) are fine-tuned and evaluated on this dataset using accuracy metrics.

## Key Results
- State-of-the-art multimodal models achieve only 55-66% accuracy, significantly below human performance (>13% gap)
- Models show particular difficulty with temporal reasoning questions compared to physical and social dimensions
- VL-Adapter with frozen visual encoder achieves 66% accuracy, suggesting visual information is important but not fully leveraged
- Performance is comparable across first-person and third-person viewpoints (66-75% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACQUIRED improves multimodal counterfactual reasoning benchmarking by using manually curated distractor answers.
- Mechanism: Each question has one correct and one distractor answer that is a minimal contrastive counterpart, forcing models to understand subtle differences in visual context.
- Core assumption: Human annotators can reliably create distractor answers that are individually judgeable and closely related to correct answers.
- Evidence anchors:
  - [abstract] "Each question is curated with a correct and distractor answer pair to evaluate the model's ability to understand subtle differences."
  - [section 3.1] "For each question, we collect one correct and one distractor answer (which can be a slightly perturbed version of the correct one), where both of which are individually judge-able by themselves respectively."
  - [corpus] FMR scores show that models are fooled ~50-60% of the time during annotation, suggesting distractors are challenging.

### Mechanism 2
- Claim: ACQUIRED enables evaluation across multiple reasoning dimensions (physical, social, temporal) in real-world videos.
- Mechanism: Questions are categorized along three commonsense dimensions to provide a comprehensive evaluation of counterfactual reasoning abilities.
- Core assumption: The three dimensions capture the major aspects of counterfactual reasoning needed for real-world scenarios.
- Evidence anchors:
  - [abstract] "Each video is annotated with questions that span three distinct dimensions of reasoning, including physical, social, and temporal."
  - [section 3.1] "Based on videos... we crowd-source 11K questions over 3.7K videos targeting physical, temporal, and social counterfactual reasoning."
  - [corpus] Performance breakdown by dimension shows models struggle more with temporal reasoning, validating the dimension coverage.

### Mechanism 3
- Claim: Using both first-person (egocentric) and third-person (exocentric) viewpoints improves model generalization.
- Mechanism: Videos from both Ego4D (first-person) and Oops! (third-person) datasets expose models to diverse visual perspectives.
- Core assumption: Counterfactual reasoning skills transfer across different visual viewpoints.
- Evidence anchors:
  - [abstract] "The dataset contains 3.9K annotated videos... incorporating both first and third-person viewpoints."
  - [section 3.1] "Our dataset concerns three types of commonsense reasoning dimensions: physical, social, and temporal, and encompasses videos from both third-person (upper) and first-person (lower) viewpoints."
  - [corpus] VL-Adapter performance shows comparable accuracy across viewpoints (66-75%), supporting generalization.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: The entire dataset and task is designed to evaluate this cognitive ability in AI systems.
  - Quick check question: What is the key difference between counterfactual reasoning and simple prediction?

- Concept: Commonsense reasoning dimensions
  - Why needed here: Questions are categorized along physical, social, and temporal dimensions to provide comprehensive evaluation.
  - Quick check question: Can you give an example of a question that would fall under each of the three dimensions?

- Concept: Multimodal learning
  - Why needed here: The task requires integrating visual and language information to answer questions about videos.
  - Quick check question: What are the main challenges in combining video and text representations for question answering?

## Architecture Onboarding

- Component map: Video preprocessing → Frame sampling → Visual feature encoding → Text understanding → Cross-modal fusion → Answer prediction
- Critical path: Video → Frame extraction → Visual feature encoding → Language understanding → Cross-modal reasoning → Answer selection
- Design tradeoffs:
  - Frame sampling frequency vs. computational cost
  - Single vs. multiple distractors per question
  - Pretrained model freezing vs. full fine-tuning
- Failure signatures:
  - Low accuracy on temporal dimension questions suggests weak temporal reasoning
  - Similar performance between text-only and multimodal models indicates poor visual grounding
  - Large performance gap between humans and models highlights remaining challenges
- First 3 experiments:
  1. Evaluate baseline text-only models (DeBERTa, UnifiedQA) on ACQUIRED to establish upper bounds for text-only reasoning
  2. Test multimodal models (VIOLET, VALOR) with frozen visual encoders to assess visual contribution
  3. Analyze performance breakdown by commonsense dimension to identify specific weaknesses

## Open Questions the Paper Calls Out

## Open Question 1
- Question: How can we improve multimodal models' performance in counterfactual reasoning tasks, specifically for physical, social, and temporal dimensions?
- Basis in paper: The paper benchmarks several state-of-the-art multimodal models on the ACQUIRED dataset and finds that their performance is significantly below human performance (>13% accuracy gap). The models struggle to effectively utilize video contexts and perform counterfactual reasoning.
- Why unresolved: The paper suggests that there is room for improvement in multimodal models' ability to capture visual contexts and perform counterfactual reasoning. However, it does not provide specific solutions or methods to address this issue.
- What evidence would resolve it: Developing and evaluating new multimodal models or techniques that can effectively incorporate visual contexts and improve counterfactual reasoning performance on the ACQUIRED dataset.

## Open Question 2
- Question: How can we improve language-only models' performance in counterfactual reasoning tasks, specifically for physical, social, and temporal dimensions?
- Basis in paper: The paper evaluates several state-of-the-art language-only models on the ACQUIRED dataset and finds that their performance is significantly below human performance (>13% accuracy gap). The models struggle to effectively reason about counterfactual scenarios based on textual information alone.
- Why unresolved: The paper suggests that language-only models are not as effective as multimodal models in counterfactual reasoning tasks. However, it does not provide specific solutions or methods to improve language-only models' performance.
- What evidence would resolve it: Developing and evaluating new language-only models or techniques that can effectively reason about counterfactual scenarios based on textual information alone, and improve performance on the ACQUIRED dataset.

## Open Question 3
- Question: How can we improve the diversity and coverage of video sources and question types in counterfactual reasoning datasets?
- Basis in paper: The paper introduces the ACQUIRED dataset, which aims to address the limitations of existing counterfactual reasoning datasets by covering diverse real-world events and incorporating multiple reasoning dimensions. However, the dataset is still limited to specific video sources and question types.
- Why unresolved: The paper suggests that existing datasets are limited in terms of video sources and question types, making it difficult to evaluate model performance in diverse real-world settings. However, it does not provide specific solutions or methods to improve the diversity and coverage of video sources and question types.
- What evidence would resolve it: Developing and evaluating new counterfactual reasoning datasets that cover a wider range of video sources and question types, and can effectively evaluate model performance in diverse real-world settings.

## Limitations
- Limited dataset size (3.9K videos) may not capture full diversity of real-world scenarios
- Distractor creation relies heavily on human annotators without systematic post-hoc quality validation
- Performance variations across dimensions suggest certain reasoning types remain challenging

## Confidence

- **High confidence**: ACQUIRED provides a meaningful benchmark for multimodal counterfactual reasoning with significant human-model performance gaps
- **Medium confidence**: The three commonsense dimensions (physical, social, temporal) comprehensively capture counterfactual reasoning requirements
- **Low confidence**: Distractor quality and long-term generalization to unseen scenarios

## Next Checks

1. **Distractor Quality Analysis**: Conduct a systematic post-hoc evaluation of distractor quality by having independent annotators rate distractor difficulty and relatedness to correct answers.

2. **Out-of-Distribution Testing**: Evaluate models on videos from completely unseen event categories or domains (e.g., industrial settings, medical scenarios) to assess generalization beyond the curated dataset.

3. **Viewpoint Transferability Study**: Design experiments that explicitly test viewpoint transfer by training on one viewpoint (first-person) and testing on the other (third-person), or vice versa.