---
ver: rpa2
title: 'First Tragedy, then Parse: History Repeats Itself in the New Era of Large
  Language Models'
arxiv_id: '2311.05020'
source_url: https://arxiv.org/abs/2311.05020
tags:
- language
- translation
- evaluation
- machine
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a historical perspective on the current scale
  crisis in NLP, drawing parallels between the rise of large language models (LLMs)
  in the current era and the earlier era of statistical machine translation (SMT).
  It argues that the current scale crisis, where massive proprietary models dominate,
  is not unprecedented and offers several key lessons.
---

# First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models

## Quick Facts
- arXiv ID: 2311.05020
- Source URL: https://arxiv.org/abs/2311.05020
- Reference count: 39
- This paper provides a historical perspective on the current scale crisis in NLP, drawing parallels between the rise of large language models (LLMs) in the current era and the earlier era of statistical machine translation (SMT).

## Executive Summary
This paper analyzes the current scale crisis in NLP by examining historical parallels with the earlier era of Statistical Machine Translation (SMT). The authors argue that the dominance of massive proprietary models is not unprecedented and offer several key lessons from history. These lessons emphasize the primacy of hardware advancements in enabling scale, the enduring importance of data collection and quality evaluation, and the need for continued research on problems where data, not compute, is the bottleneck. The paper recommends that researchers focus on improving evaluation metrics, pursuing small-scale problems, and anticipating future hardware developments.

## Method Summary
The paper employs historical analysis and synthesis of lessons from the SMT era (2005-2013) to inform current LLM research directions. It reviews SMT literature focusing on scale demonstrations, evaluation metrics like BLEU, and hardware developments such as KenLM and GPUs. The authors compare historical SMT developments with current LLM trends, identifying parallels in scale challenges, evaluation issues, and hardware dependencies. Based on this analysis, they synthesize lessons about data bottlenecks, evaluation metrics, and hardware-driven paradigm shifts to formulate research recommendations for current LLM challenges.

## Key Results
- Scale disparities between research groups are transient due to hardware improvements following Moore's law
- Evaluation quality is a critical bottleneck that significantly impacts model training effectiveness
- Human evaluation cannot provide a universal "gold standard" of quality feedback due to inconsistent preferences and measurement challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale disparities are transient because hardware improvements follow Moore's law
- Mechanism: As hardware capabilities double biannually, tasks requiring massive compute today become accessible to smaller labs within years
- Core assumption: Moore's law continues to hold and compute costs decrease proportionally
- Evidence anchors:
  - [abstract] "Scale is supreme... disparities in scale are transient"
  - [section] "Thanks to Moore's law, a six-decade trend in which computing power has doubled biannually... that moment often arrives within a few years"
  - [corpus] "Found 25 related papers... Top related titles: History repeats Itself: A Baseline for Temporal Knowledge Graph Forecasting" (shows similar theme of transience)
- Break condition: If Moore's law fails or hardware scaling hits physical limits before software optimization catches up

### Mechanism 2
- Claim: Evaluation quality is the bottleneck for effective training
- Mechanism: Poor evaluation metrics create suboptimal training feedback loops, limiting model performance even with abundant data and compute
- Core assumption: Better evaluation metrics directly translate to better trained models
- Evidence anchors:
  - [abstract] "Evaluation is a bottleneck... the quality of evaluation methods makes a substantial difference in the effectiveness of training"
  - [section] "evaluation during training can be based on comparison with a ground truth, as in conventional training... or direct feedback, as provided by RLHF"
  - [corpus] Weak evidence - corpus neighbors don't directly address evaluation bottlenecks
- Break condition: If model architectures become so powerful that evaluation metrics become irrelevant for performance gains

### Mechanism 3
- Claim: Human evaluation cannot provide universal "gold standard" quality feedback
- Mechanism: Human annotators have inconsistent preferences and prioritize different quality dimensions, making aggregated ratings unreliable
- Core assumption: Human judgment can be systematically improved through careful task specification
- Evidence anchors:
  - [abstract] "There is no gold standard... human annotation cannot provide a universal 'gold standard' of quality feedback"
  - [section] "Human evaluation does not intrinsically solve problems with measurement modeling, and raises challenges of its own"
  - [corpus] Weak evidence - corpus neighbors don't address human evaluation challenges
- Break condition: If automated metrics advance to perfectly match human judgment or if individual annotator reliability becomes consistent

## Foundational Learning

- Concept: Moore's Law
  - Why needed here: Explains why scale disparities are transient and why hardware accessibility improves over time
  - Quick check question: What is the approximate doubling period for computing power under Moore's law?

- Concept: Scaling Laws in Machine Learning
  - Why needed here: Understanding why scale becomes the dominant factor in system performance
  - Quick check question: How does system performance typically scale with training data size according to observed scaling laws?

- Concept: Evaluation Metric Design
  - Why needed here: Explains why evaluation quality directly impacts model training effectiveness
  - Quick check question: What is the relationship between training feedback metrics and test-time evaluation metrics?

## Architecture Onboarding

- Component map: Historical analysis module (SMT era patterns) -> Scale impact assessment (hardware, data, compute) -> Evaluation bottleneck analysis (metric quality, human feedback) -> Research direction recommendation engine -> Hardware anticipation module (future tech trends)

- Critical path: 1. Historical pattern recognition (SMT era parallels) 2. Scale impact assessment (current vs past disparities) 3. Bottleneck identification (evaluation, human feedback) 4. Recommendation generation (research directions)

- Design tradeoffs:
  - Depth vs breadth in historical analysis
  - Specific vs general recommendations
  - Focus on hardware vs software solutions
  - Individual vs collaborative research approaches

- Failure signatures:
  - Overreliance on hardware trends without considering software bottlenecks
  - Assuming human evaluation can provide objective quality measures
  - Neglecting small-scale problems in pursuit of large-scale research
  - Failing to anticipate paradigm shifts in hardware capabilities

- First 3 experiments:
  1. Compare SMT era scale disparities with current LLM era using standardized metrics
  2. Test evaluation metric improvements on model training effectiveness with controlled data/compute
  3. Analyze human evaluation consistency across different annotation tasks and dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation metrics should be developed to accurately assess the quality of LLM outputs beyond traditional benchmarks?
- Basis in paper: [explicit] The paper discusses the limitations of current automated metrics like BLEU and the need for improved evaluation methods.
- Why unresolved: Automated metrics often fail to predict human evaluation, and there's a lack of consensus on alternative approaches.
- What evidence would resolve it: Development and validation of new evaluation metrics that show strong correlation with human judgments across diverse LLM tasks.

### Open Question 2
- Question: How can we effectively balance the trade-off between model scale and data quality, especially for low-resource languages?
- Basis in paper: [explicit] The paper highlights the importance of data collection for underserved languages and the limitations of scaling in these contexts.
- Why unresolved: Current models are predominantly trained on English data, and there's a lack of resources for many languages.
- What evidence would resolve it: Successful implementation of language models for low-resource languages that demonstrate comparable performance to high-resource language models.

### Open Question 3
- Question: What are the ethical implications of human evaluation in LLM development, and how can we ensure fairness and inclusivity in this process?
- Basis in paper: [explicit] The paper discusses the challenges of human evaluation, including issues of bias and disagreement among annotators.
- Why unresolved: Human evaluation is inherently subjective and can reflect societal biases, leading to unfair or harmful outcomes.
- What evidence would resolve it: Development of guidelines and frameworks for ethical human evaluation that address issues of bias and ensure diverse perspectives are represented.

## Limitations
- The historical parallel between SMT and LLM eras may overstate the applicability of SMT lessons to current LLM challenges
- The claim about evaluation being the primary bottleneck is supported primarily through historical anecdote rather than empirical evidence specific to current LLM contexts
- The dismissal of human evaluation as incapable of providing "gold standard" feedback lacks nuance about how systematic improvements in annotation protocols could address current limitations

## Confidence
- High Confidence: The assertion that scale disparities are transient due to hardware improvements follows established Moore's law patterns
- Medium Confidence: The characterization of evaluation as a critical bottleneck is well-supported historically but requires more empirical validation in the current LLM context
- Low Confidence: The claim that human evaluation cannot provide universal quality feedback oversimplifies the potential for systematic improvements in annotation protocols

## Next Checks
1. **Empirical validation of SMT-LLM parallels**: Conduct quantitative comparison of scaling laws, hardware requirements, and performance improvements between SMT era (2005-2013) and current LLM developments to assess the validity of historical analogies.

2. **Evaluation bottleneck isolation study**: Design controlled experiments comparing model training effectiveness using different evaluation metrics while holding data and compute constant to quantify the impact of evaluation quality on final model performance.

3. **Human evaluation protocol analysis**: Systematically analyze inter-annotator agreement rates and reliability across different annotation dimensions and task specifications in current LLM contexts to identify specific failure modes and potential improvements.