---
ver: rpa2
title: Data-Free Hard-Label Robustness Stealing Attack
arxiv_id: '2312.05924'
source_url: https://arxiv.org/abs/2312.05924
tags:
- data
- robustness
- target
- attack
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel data-free hard-label robustness stealing
  attack, aiming to extract both the accuracy and robustness of a target model without
  access to natural data or soft labels. The key idea is to introduce high-entropy
  examples (HEE) to better characterize the classification boundaries and train a
  generator to synthesize substitute data.
---

# Data-Free Hard-Label Robustness Stealing Attack

## Quick Facts
- arXiv ID: 2312.05924
- Source URL: https://arxiv.org/abs/2312.05924
- Reference count: 38
- One-line primary result: Achieves 77.86% clean accuracy and 39.51% robust accuracy against AutoAttack, only 4.71% and 8.40% lower than the target model, respectively.

## Executive Summary
This paper proposes a novel data-free hard-label robustness stealing attack that extracts both the accuracy and robustness of a target model without access to natural data or soft labels. The key innovation is the introduction of High-Entropy Examples (HEE) to better characterize the classification boundaries, combined with a two-stage training framework that decouples data synthesis from label querying. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate that the proposed method significantly outperforms existing baselines in both clean and robust accuracy.

## Method Summary
The proposed Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack employs a two-stage framework: (1) a temporary generator synthesizes one batch of synthetic data per epoch guided by the current clone model with label smoothing and standard augmentation, storing all samples in a memory bank; (2) High-Entropy Examples (HEE) are constructed by maximizing prediction entropy of the clone model, then strongly augmented before querying the target model for hard labels, which are used to train the clone model. This approach avoids overfitting to the target model while effectively characterizing the decision boundaries for robustness extraction.

## Key Results
- Achieves 77.86% clean accuracy and 39.51% robust accuracy against AutoAttack on CIFAR-10
- Only 4.71% and 8.40% lower than the target model's clean and robust accuracy, respectively
- Significantly outperforms existing baselines in both accuracy and robustness extraction
- Demonstrates effectiveness across multiple target models trained with different adversarial training strategies (PGD-AT, TRADES, STAT-AWP)

## Why This Works (Mechanism)

### Mechanism 1
High-Entropy Examples (HEE) capture a more complete boundary shape than Uncertain Examples (UE) by exploring adaptively rather than converging to a single junction. HEE maximizes the prediction entropy of the clone model during construction, allowing synthetic samples to roam among similar classes instead of being pinned to a fixed uniform target distribution. This approach better characterizes the complete shape of the classification boundary.

### Mechanism 2
The two-stage training (generator → clone) in DFHL-RS avoids overfitting by decoupling data synthesis from label querying. In stage 1, a temporary generator synthesizes one batch per epoch guided by the current clone model, not the target model; in stage 2, the clone is trained on previously stored synthetic samples with hard labels from the target model. This decoupling prevents the min-max game and overfitting issues that arise when using the target model as a discriminator.

### Mechanism 3
Strong augmentation during HEE construction in stage 2 promotes diversity and prevents the clone from memorizing synthetic artifacts. Before querying the target model, synthetic samples are heavily augmented (crop, noise, flip, rotate), so the resulting hard labels are robust to minor transformations and the clone learns invariances. This approach expands the effective sample distribution while preserving underlying class semantics.

## Foundational Learning

- **Concept:** Black-box hard-label model extraction
  - **Why needed here:** The attack only receives top-1 predictions, so it cannot use gradient-based techniques that require logits.
  - **Quick check question:** How does the method obtain gradients for synthetic data generation without access to soft labels?

- **Concept:** Adversarial robustness distillation
  - **Why needed here:** The goal is to copy not just accuracy but also the target model's adversarial robustness, which requires mimicking decision boundaries under attack.
  - **Quick check question:** What distinguishes robustness stealing from accuracy stealing in terms of query strategy?

- **Concept:** Data-free knowledge distillation
  - **Why needed here:** Without any real training data, the method must synthesize representative samples; understanding prior DFKD helps see why the two-stage design is necessary.
  - **Quick check question:** Why does generating synthetic data guided by the clone (instead of the target) help prevent overfitting?

## Architecture Onboarding

- **Component map:** Generator (temporary) → Memory bank → HEE construction → Target query → Clone model
- **Critical path:** Generator produces synthetic data → Memory bank stores samples → HEE maximizes entropy → Target model provides hard labels → Clone model trains on labeled HEE
- **Design tradeoffs:** Using a temporary generator per epoch reduces mode collapse risk but increases per-epoch overhead; strong augmentation improves robustness but may risk semantic drift if too aggressive; fixed clone iterations balance training stability vs. query budget.
- **Failure signatures:** Clean accuracy drops while robust accuracy stays flat → overfitting to synthetic artifacts; both accuracies plateau early → generator not exploring target data distribution; high query budget with low gains → clone iterations or epochs too low.
- **First 3 experiments:** 1) Verify HEE vs UE by training with and without entropy maximization, measuring boundary coverage via t-SNE; 2) Memory bank ablation by disabling memory bank and retraining clone only on current batch to observe catastrophic forgetting; 3) Augmentation sweep by varying strength of augmentation in stage 2, measuring clean vs robust accuracy trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed High-Entropy Examples (HEE) method compare to other uncertainty-based techniques for model stealing, such as Monte Carlo dropout or entropy-based sampling, in terms of both accuracy and robustness? The paper introduces HEE and compares it to Uncertain Examples (UE) and Adversarial Examples (AE), showing its superiority, but does not compare HEE to other uncertainty-based techniques. A comprehensive comparison would provide a broader understanding of HEE's effectiveness.

### Open Question 2
How does the proposed Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack perform against more advanced adversarial training techniques, such as adversarial training with unlabeled data or adversarial training with generative models? The paper evaluates DFHL-RS against target models trained with PGD-AT, TRADES, and STAT-AWP, but does not evaluate its performance against more advanced adversarial training techniques. A comparison with more advanced techniques would provide a better understanding of DFHL-RS's robustness against state-of-the-art defenses.

### Open Question 3
How does the proposed DFHL-RS attack perform in a more realistic setting, where the target model is deployed in a real-world application and the attacker has limited access to the model? The paper evaluates DFHL-RS in a simulated setting, where the attacker has access to the target model's API and can query it with arbitrary inputs, but does not evaluate its performance in a more realistic setting. A more realistic evaluation would provide insights into its practical applicability and limitations.

## Limitations
- The paper lacks direct empirical evidence for the claimed superiority of High-Entropy Examples over Uncertain Examples in boundary characterization.
- The two-stage training design's critical role in preventing overfitting is asserted but not empirically validated through controlled experiments.
- Strong augmentation's exact implementation details and hyperparameters are not specified, making it difficult to assess whether observed robustness gains are due to augmentation strength or other factors.

## Confidence

**High confidence:** The two-stage framework architecture and the general approach of using HEE for hard-label robustness stealing are well-defined and reproducible.

**Medium confidence:** The claimed improvements over baselines (77.86% clean accuracy, 39.51% robust accuracy) are based on the paper's internal experiments, but external validation is needed to confirm these results are not due to overfitting to the target model.

**Low confidence:** The specific mechanisms by which HEE better characterize classification boundaries and how strong augmentation prevents clone overfitting lack sufficient empirical support in the corpus.

## Next Checks

1. **Boundary Coverage Validation:** Conduct t-SNE visualization comparing HEE samples against UE samples to empirically verify that HEE better covers the target model's decision boundaries.

2. **Two-Stage Necessity Test:** Remove the two-stage separation and train the clone model directly using a single generator-querying loop; compare performance to assess if decoupling is essential.

3. **Augmentation Ablation Study:** Systematically vary augmentation strength in stage 2 and measure its impact on clean vs robust accuracy to determine optimal augmentation parameters.