---
ver: rpa2
title: 'Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models'
arxiv_id: '2305.12827'
source_url: https://arxiv.org/abs/2305.12827
tags:
- task
- arithmetic
- page
- https
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies task arithmetic in vision-language models (CLIP),
  where fine-tuned weights are added/subtracted to improve or forget task performance.
  The authors show that task arithmetic effectiveness depends on weight disentanglement,
  where distinct weight directions correspond to spatially localized function changes.
---

# Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models

## Quick Facts
- **arXiv ID**: 2305.12827
- **Source URL**: https://arxiv.org/abs/2305.12827
- **Reference count**: 40
- **Primary result**: Linearizing models in their tangent space amplifies weight disentanglement, achieving up to 5.8 points more in task addition and 13.1 points less in task negation compared to non-linear models.

## Executive Summary
This paper investigates task arithmetic in vision-language models, specifically CLIP, where fine-tuned weights are added or subtracted to improve or forget task performance. The authors identify weight disentanglement as the crucial factor enabling effective task arithmetic, where distinct weight directions correspond to spatially localized function changes. They demonstrate that linearizing models using the Neural Tangent Kernel (NTK) amplifies weight disentanglement, significantly improving task arithmetic benchmarks. The study also establishes that weight disentanglement is an emergent property of pre-training, absent in randomly initialized models.

## Method Summary
The method involves fine-tuning CLIP ViT models on individual tasks to obtain task vectors, then combining these vectors using arithmetic operations. The key innovation is linearizing the models in their tangent space using the NTK approximation, which enhances weight disentanglement. The linearized models are fine-tuned using Jacobian-vector products for efficient computation. Task addition and negation experiments are conducted by adding/subtracting task vectors to/from pre-trained checkpoints, with performance evaluated on each task.

## Key Results
- Linearized fine-tuning achieves 5.8 points more in task addition compared to non-linear models
- Linearized models show 13.1 points less degradation in task negation
- Weight disentanglement error decreases significantly in linearized models
- Task arithmetic fails on randomly initialized models, confirming disentanglement is emergent from pre-training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weight disentanglement is the necessary condition for effective task arithmetic
- **Mechanism**: Distinct directions in weight space correspond to localized function changes in disjoint input regions, allowing independent manipulation of task-specific components without interference
- **Core assumption**: The neural network's functional decomposition allows additive combination of task-specific components that are spatially localized in input space
- **Evidence anchors**:
  - [abstract]: "weight disentanglement is the crucial factor that makes it effective" and "This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks"
  - [section 4]: Formal definition of weight disentanglement and its equivalence to task arithmetic property, plus experimental evidence showing disentanglement error decreases in linearized models
  - [corpus]: Weak - corpus contains related work on task arithmetic but doesn't directly address the disentanglement mechanism
- **Break condition**: If task vectors are not spatially localized or if they interfere across task domains, weight disentanglement breaks down and task arithmetic fails

### Mechanism 2
- **Claim**: Linearizing models in their tangent space amplifies weight disentanglement
- **Mechanism**: By restricting fine-tuning to the neural tangent kernel (NTK) regime, the model's functional changes become more linearly separable, enhancing the spatial localization of task-specific components
- **Core assumption**: The NTK approximation captures the essential task-relevant directions while suppressing non-local interactions
- **Evidence anchors**:
  - [abstract]: "we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement"
  - [section 5]: Experimental results showing linearized fine-tuning achieves 5.8 points more in task addition and 13.1 points less in task negation compared to non-linear models
  - [section 3]: Evidence that post-hoc linearization of non-linearly fine-tuned models still performs better than non-linear task arithmetic, suggesting linearization helps disentangle tasks
- **Break condition**: When the model's behavior significantly deviates from the NTK approximation (non-linear regime), weight disentanglement degrades

### Mechanism 3
- **Claim**: Weight disentanglement is an emergent property of pre-training, not present at random initialization
- **Mechanism**: Pre-training creates a structured weight space where semantically meaningful tasks correspond to orthogonal weight directions, while random initialization lacks this structure
- **Core assumption**: The pre-training process induces a form of implicit regularization that organizes task-relevant information along separable weight directions
- **Evidence anchors**:
  - [abstract]: "weight disentanglement of semantically meaningful tasks is an emergent property of pre-training, as it is absent at random initialization"
  - [section 6.2]: Experimental results showing task addition fails on randomly initialized ViTs, with Table 3 demonstrating poor multi-task accuracy compared to pre-trained models
  - [corpus]: Weak - corpus contains related work on model editing but doesn't directly address the emergence of disentanglement during pre-training
- **Break condition**: If pre-training is skipped or if the model is randomly re-initialized, weight disentanglement disappears and task arithmetic becomes ineffective

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK) and tangent space linearization
  - **Why needed here**: The paper's core technique relies on understanding when models operate in the linear regime defined by their NTK, and how linearization affects task arithmetic performance
  - **Quick check question**: What is the relationship between the NTK and the first-order Taylor expansion of a neural network around its initialization?

- **Concept**: Weight space vs function space relationship
  - **Why needed here**: The paper fundamentally connects how weight changes (task vectors) map to functional changes in input space, which is essential for understanding weight disentanglement
  - **Quick check question**: How does a small change in weights translate to changes in the network's output function, and what does this imply about task arithmetic?

- **Concept**: Eigenfunction decomposition of kernels
  - **Why needed here**: The paper uses spectral analysis of the NTK to explain why certain models can perform task arithmetic, linking it to the localization properties of eigenfunctions
  - **Quick check question**: How does the representation of a function in terms of kernel eigenfunctions relate to the ability to perform task arithmetic?

## Architecture Onboarding

- **Component map**:
  - CLIP Vision Transformer (ViT) encoder -> Neural Tangent Kernel (NTK) -> Task vectors -> Disentanglement error metric

- **Critical path**:
  1. Fine-tune a pre-trained CLIP model on individual tasks to get task vectors
  2. Linearize the model around initialization using NTK approximation
  3. Fine-tune the linearized model to obtain disentangled task vectors
  4. Combine task vectors using arithmetic operations
  5. Evaluate performance on task addition/negation benchmarks

- **Design tradeoffs**:
  - Linear vs non-linear fine-tuning: Linearization improves task arithmetic but may reduce single-task performance
  - Model size impact: Larger models stay closer to NTK regime, reducing the gap between linear and non-linear performance
  - Computational cost: Linearized models require ~2x memory and computation but offer better task arithmetic

- **Failure signatures**:
  - High disentanglement error indicates tasks interfere with each other
  - Poor task arithmetic performance despite good single-task accuracy
  - Post-hoc linearization of non-linearly fine-tuned models performs worse than non-linear task arithmetic

- **First 3 experiments**:
  1. Fine-tune a pre-trained CLIP ViT on two simple tasks (e.g., MNIST and CIFAR-10) and compute the disentanglement error across different Î± values
  2. Compare task arithmetic performance between linearized and non-linear models on the same task pair
  3. Repeat experiment 2 with randomly initialized ViT to demonstrate that disentanglement is emergent from pre-training

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text provided.

## Limitations
- The effectiveness of task arithmetic relies heavily on the NTK approximation assumption
- The relationship between eigenfunction localization and weight disentanglement is theoretically grounded but the proof sketch leaves several gaps
- The computational overhead of linearized models (~2-3x) may limit practical applications

## Confidence
- **High confidence**: The empirical observation that linearized fine-tuning improves task arithmetic performance (measured by addition and negation benchmarks)
- **Medium confidence**: The theoretical link between NTK eigenfunctions and spatial localization of task-specific changes
- **Medium confidence**: The claim that weight disentanglement is necessary for task arithmetic to work effectively

## Next Checks
1. Test task arithmetic across diverse model architectures (CNNs, MLP-Mixers) to verify that the NTK-based linearization approach generalizes beyond ViTs

2. Measure the evolution of weight disentanglement during different stages of pre-training to identify when and how the emergent property develops

3. Compare the NTK linearization approach with alternative linearization methods (e.g., higher-order Taylor expansions) to assess whether the first-order approximation is sufficient or optimal for task arithmetic