---
ver: rpa2
title: Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models
arxiv_id: '2305.16582'
source_url: https://arxiv.org/abs/2305.16582
tags:
- reasoning
- answer
- thought
- graph
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to modeling human thought
  processes in language models (LMs) by representing them as graphs, rather than simply
  as sequential chains of thoughts. The Graph-of-Thought (GoT) reasoning approach
  uses thought units as nodes and connections between them as edges to capture the
  non-sequential nature of human thinking.
---

# Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models

## Quick Facts
- arXiv ID: 2305.16582
- Source URL: https://arxiv.org/abs/2305.16582
- Authors: 
- Reference count: 40
- Key outcome: Graph-of-Thought reasoning improves accuracy over Chain-of-Thought baseline by 3.41% on GSM8K with T5-base and 5.08% with T5-large

## Executive Summary
This paper introduces Graph-of-Thought (GoT) reasoning as an alternative to Chain-of-Thought (CoT) approaches for language model reasoning. The method represents human thought processes as graphs rather than sequential chains, using thought units as nodes and connections as edges. A two-stage framework with an additional GoT encoder and gated fusion mechanism significantly outperforms CoT baselines on both text-only and multimodal reasoning tasks.

## Method Summary
The approach constructs thought graphs using an Extract-Clustering-Coreference (ECC) process, then encodes them with a Graph Attention Network (GAT). The model generates rationales through a two-stage fine-tuning process: first creating the thought graph representation, then producing final answers by fusing text, thought graph, and optional vision features through a gated fusion mechanism. The method is evaluated on GSM8K (text-only math reasoning) and ScienceQA (multimodal science questions) using T5-base and T5-large architectures.

## Key Results
- GoT improves accuracy over CoT baseline by 3.41% on GSM8K test set with T5-base
- GoT improves accuracy over CoT baseline by 5.08% on GSM8K test set with T5-large
- GoT boosts accuracy from 85.19% to 87.59% over state-of-the-art Multimodal-CoT on ScienceQA test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GoT reasoning captures non-linear human thought processes better than CoT by representing reasoning as a graph rather than a chain
- Mechanism: The Graph-of-Thought (GoT) approach uses thought units as nodes and connections between them as edges to model the non-sequential nature of human thinking, allowing for more realistic modeling of thought processes
- Core assumption: Human thought processes are fundamentally non-linear and can be better represented as graphs rather than chains
- Evidence anchors:
  - [abstract] "human thought processes are often non-linear, rather than simply sequential chains of thoughts"
  - [section] "Human thinking is often characterized by its ability to make sudden leaps and connections between seemingly unrelated ideas"
  - [corpus] "Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems" suggests graph-based approaches are effective for complex reasoning
- Break condition: If human thought processes are actually more linear than assumed, or if the graph representation becomes too complex to be computationally efficient

### Mechanism 2
- Claim: The two-stage framework with gated fusion improves reasoning accuracy by combining multiple modalities of information
- Mechanism: The framework first generates rationales using a GoT encoder, then produces final answers by fusing text, thought graph, and optional vision features through a gated fusion mechanism
- Core assumption: Combining multiple representations of the same reasoning process (text, graph, vision) provides more comprehensive information than any single representation
- Evidence anchors:
  - [abstract] "We employ an additional graph-of-thoughts encoder for GoT representation learning and fuses the GoT representation with the original input representation through a gated fusion mechanism"
  - [section] "We use a single head attention to align the text representation H T with image representation H I and thought graph representation H G, respectively"
  - [corpus] "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning" suggests multi-modal fusion can improve reasoning performance
- Break condition: If the fusion mechanism creates noise or conflicts between modalities that degrade performance more than the additional information helps

### Mechanism 3
- Claim: The Extract-Clustering-Coreference (ECC) process improves reasoning by creating denser, more connected thought graphs
- Mechanism: ECC extracts deductive triplets, clusters nodes that refer to the same mentions, and performs coreference resolution to create a more connected graph structure
- Core assumption: Denser thought graphs with resolved coreferences better capture the relationships between concepts needed for reasoning
- Evidence anchors:
  - [section] "By adopting this technique, our model is better equipped with denser thought graphs and the ability for deductive reasoning"
  - [section] "We apply coreference resolution to the extracted nodes using the Stanford CoreNLP system"
  - [corpus] "Soft-Prompting with Graph-of-Thought for Multi-modal Representation Learning" suggests graph density affects representation quality
- Break condition: If the coreference resolution introduces errors or if the additional graph density doesn't translate to improved reasoning performance

## Foundational Learning

- Concept: Graph Attention Networks (GATs)
  - Why needed here: Used to encode the thought graph structure and learn relationships between thought units
  - Quick check question: How does a GAT differ from a standard attention mechanism in terms of processing graph-structured data?

- Concept: Coreference Resolution
  - Why needed here: Essential for the ECC process to cluster nodes and create more connected thought graphs
  - Quick check question: What are the main challenges in coreference resolution for constructing thought graphs from natural language?

- Concept: Multimodal Fusion
  - Why needed here: Required to combine text, thought graph, and vision features effectively
  - Quick check question: What are the advantages and disadvantages of gated fusion compared to other fusion methods like concatenation or summation?

## Architecture Onboarding

- Component map: Input Encoder -> GoT Encoder -> Cross Attention -> Gated Fusion -> Transformer Decoder -> Output

- Critical path: Input → Text Encoder + Vision Encoder (if applicable) → GoT Encoder → Cross Attention → Gated Fusion → Decoder → Output

- Design tradeoffs:
  - Graph complexity vs. computational efficiency: More complex graphs may improve reasoning but increase computation
  - Number of attention heads in GAT: More heads may capture more relationships but increase parameters
  - Fusion strategy: Gated fusion allows selective combination but requires careful weight initialization

- Failure signatures:
  - Degraded performance with random thought graphs suggests the structure matters
  - Performance loss with single-head attention indicates the multi-head mechanism is important
  - Inconsistent results across different subjects may indicate subject-specific reasoning challenges

- First 3 experiments:
  1. Compare GoT with random thought graphs vs. structured thought graphs to verify the importance of graph structure
  2. Test single-head vs. multi-head attention in the GoT encoder to understand the impact of attention mechanism design
  3. Evaluate performance with and without the gated fusion mechanism to measure the contribution of multimodal integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Graph-of-Thought (GoT) compare to other graph-based reasoning approaches, such as Graph Attention Networks (GATs) or Graph Convolutional Networks (GCNs)?
- Basis in paper: [inferred] The paper proposes a novel approach to modeling human thought processes using a graph structure, but does not compare its performance to other graph-based reasoning methods.
- Why unresolved: The paper focuses on comparing GoT to Chain-of-Thought (CoT) and does not explore its performance relative to other graph-based methods.
- What evidence would resolve it: A comprehensive comparison of GoT with other graph-based reasoning approaches on the same benchmarks (e.g., GSM8K and ScienceQA) would provide insights into its relative performance and effectiveness.

### Open Question 2
- Question: How does the performance of GoT vary with different graph construction methods, such as the Extract-Cluster-Coreference (ECC) process used in the paper, or other methods like random graph generation or pre-trained graph models?
- Basis in paper: [explicit] The paper uses the ECC process for graph construction and mentions that random thought graph construction disrupts the deductive reasoning process.
- Why unresolved: The paper does not explore the impact of different graph construction methods on the performance of GoT.
- What evidence would resolve it: Experiments comparing the performance of GoT using different graph construction methods on the same benchmarks would provide insights into the impact of graph construction on the model's effectiveness.

### Open Question 3
- Question: How does the performance of GoT scale with increasing graph size and complexity?
- Basis in paper: [inferred] The paper uses a fixed number of nodes (150) for the thought graph, but does not explore the impact of graph size on performance.
- Why unresolved: The paper does not investigate the relationship between graph size and the model's performance.
- What evidence would resolve it: Experiments varying the number of nodes in the thought graph and evaluating the performance of GoT on the same benchmarks would provide insights into the scalability of the approach.

### Open Question 4
- Question: How does the performance of GoT compare to other multimodal reasoning approaches, such as those that use image features or captions directly, without the graph structure?
- Basis in paper: [explicit] The paper compares GoT to Multimodal-CoT, which uses image features and captions, but does not explore other multimodal reasoning methods.
- Why unresolved: The paper does not investigate the performance of GoT relative to other multimodal reasoning approaches that do not use a graph structure.
- What evidence would resolve it: A comparison of GoT with other multimodal reasoning methods on the same benchmarks would provide insights into the effectiveness of the graph structure in multimodal reasoning tasks.

## Limitations

- Graph Construction Dependency: The performance heavily depends on the quality of the Extract-Clustering-Coreference process, where errors in triplet extraction or coreference resolution could compromise the entire graph structure.
- Multimodal Integration Complexity: The complexity of integrating text, thought graphs, and vision features creates multiple potential failure points that may be sensitive to specific hyperparameters.
- Generalizability Concerns: The significant improvements on GSM8K and ScienceQA haven't been extensively tested across diverse reasoning tasks beyond mathematics and science.

## Confidence

- High Confidence: The core architectural design (GAT-based GoT encoder + gated fusion) is well-specified and improvements over CoT baselines on tested datasets are clearly demonstrated.
- Medium Confidence: The theoretical justification for why graph representations capture non-linear thought processes better than chains is reasonable but not definitively proven.
- Low Confidence: The exact contribution of each component (ECC process, GAT encoder, gated fusion) to overall performance improvement is difficult to isolate from results presented.

## Next Checks

1. **Component Ablation Study**: Systematically disable each major component (ECC process, GoT encoder, gated fusion) to quantify their individual contributions to performance improvements.

2. **Cross-Domain Testing**: Evaluate the GoT approach on reasoning tasks outside mathematics and science (e.g., legal reasoning, common sense reasoning) to assess generalizability.

3. **Graph Structure Sensitivity**: Test performance with varying graph densities and structures to determine if improvements depend on specific graph characteristics or if the general approach is robust.