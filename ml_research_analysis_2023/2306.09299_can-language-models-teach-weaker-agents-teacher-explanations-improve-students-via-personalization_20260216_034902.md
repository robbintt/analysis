---
ver: rpa2
title: Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students
  via Personalization
arxiv_id: '2306.09299'
source_url: https://arxiv.org/abs/2306.09299
tags:
- student
- teacher
- explanations
- intervention
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  act as effective teachers for weaker agents, focusing on how teacher explanations
  can improve student performance. The study proposes a student-teacher framework
  where a teacher LLM intervenes on a student LLM's reasoning by providing natural
  language explanations, with a fixed communication budget limiting the number of
  interventions.
---

# Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization

## Quick Facts
- arXiv ID: 2306.09299
- Source URL: https://arxiv.org/abs/2306.09299
- Authors: 
- Reference count: 40
- One-line primary result: Teacher LLMs can improve weaker student LLMs by intervening on their reasoning with personalized explanations, outperforming random intervention and unpersonalized approaches.

## Executive Summary
This paper investigates whether large language models can act as effective teachers for weaker agents by providing natural language explanations that improve student performance. The study introduces a student-teacher framework where a teacher LLM intervenes on a student LLM's reasoning using a fixed communication budget. A key innovation is the Theory of Mind approach, where the teacher builds few-shot mental models to personalize explanations based on the student's specific knowledge gaps. Results demonstrate that personalized teacher explanations outperform unpersonalized ones, teacher interventions generalize to future unexplained data, and misaligned teachers can intentionally mislead students.

## Method Summary
The paper proposes a teacher-student framework where a teacher LLM intervenes on a student LLM's reasoning by providing natural language explanations under a fixed communication budget. The teacher builds two mental models of the student: an Intervention Function based on Expected Utility to determine when to intervene, and a personalized explanation model to tailor explanations to the student's needs. The system uses Chain-of-Thought prompting with demonstrations, where the teacher calculates expected utility for each potential intervention by simulating the student's pre- and post-intervention confidence. The approach is evaluated across three reasoning datasets (StrategyQA, GSM8k, and CommonsenseQA) using Flan-T5 and LLaMA models as student and teacher agents.

## Key Results
- Teacher LLMs can successfully improve student performance by intervening on their reasoning with natural language explanations
- Personalized teacher explanations outperform unpersonalized ones, particularly at low communication budgets
- Teacher explanations generalize and improve student performance on future unexplained data through multi-round interactions
- Misaligned teachers can intentionally mislead students, lowering their performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher LLMs can intervene on student reasoning to improve performance by providing natural language explanations.
- Mechanism: The teacher builds a mental model of the student through few-shot demonstrations, predicting pre-intervention and post-intervention confidence. It intervenes only when the expected utility (post-intervention confidence minus pre-intervention confidence) is highest.
- Core assumption: The teacher can accurately estimate the student's confidence with and without intervention based on limited demonstrations.
- Evidence anchors:
  - [abstract]: "We propose building two few-shot mental models of the student... The first model defines an Intervention Function that simulates the utility of an intervention"
  - [section]: "The teacher computes the Expected Utility of intervention by simulating the student's predictions using a mental model of the student"
  - [corpus]: Weak evidence - related papers discuss in-context teaching but not the specific utility-based intervention function.
- Break condition: If the teacher cannot accurately model student confidence, interventions will be suboptimal and may not improve performance.

### Mechanism 2
- Claim: Personalized teacher explanations outperform unpersonalized ones by tailoring to the student's specific knowledge gaps.
- Mechanism: The teacher conditions on demonstrations where human explanations successfully flipped student answers from incorrect to correct, learning to generate explanations that fill the student's specific knowledge gaps.
- Core assumption: Explanations that helped a particular student in the past will continue to help that same student in the future.
- Evidence anchors:
  - [abstract]: "The second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers"
  - [section]: "The teacher conditions on these demonstrations to generate explanations for the student"
  - [corpus]: Weak evidence - related work on explanation-enhanced distillation exists but doesn't specifically address personalization via theory of mind.
- Break condition: If the student's knowledge gaps change over time or vary significantly across different types of questions, the personalized approach may fail.

### Mechanism 3
- Claim: Teacher explanations generalize to improve student performance on future unexplained data through multi-round interactions.
- Mechanism: The teacher explains selected data points that are added to the student's prompt. The student then conditions on these teacher explanations as in-context demonstrations to perform better on future unexplained samples.
- Core assumption: Teacher explanations capture general reasoning principles that transfer to new problems, not just memorized solutions to specific examples.
- Evidence anchors:
  - [abstract]: "We also demonstrate that in multi-turn interactions, teacher explanations generalize and learning from explained data improves student performance on future unexplained data"
  - [section]: "The student then conditions on these teacher explanations as in-context demonstrations to perform the reasoning task on future unexplained samples"
  - [corpus]: Weak evidence - related work on knowledge distillation exists but specific evidence for generalization through explanations is limited.
- Break condition: If the student overfits to specific teacher explanations rather than learning general reasoning principles, performance on new data will not improve.

## Foundational Learning

- Concept: Theory of Mind (ToM) - the ability to attribute mental states to others and understand their knowledge, beliefs, and intentions.
  - Why needed here: The teacher needs ToM to build mental models of the student's reasoning process and knowledge gaps to intervene effectively and personalize explanations.
  - Quick check question: Can you explain how the teacher uses ToM to decide when and how to intervene?

- Concept: Expected Utility calculation - a decision-making framework that weighs the benefits of an action against its costs.
  - Why needed here: The teacher uses expected utility to decide which data points to intervene on, maximizing performance improvement while minimizing communication cost.
  - Quick check question: How does the teacher calculate expected utility for each potential intervention?

- Concept: Chain-of-Thought reasoning - a prompting technique where models generate intermediate reasoning steps to solve complex problems.
  - Why needed here: Both teacher and student use CoT to generate explanations and predictions, making it the communication medium for teaching.
  - Quick check question: What role does CoT play in the teacher-student interaction?

## Architecture Onboarding

- Component map:
  - Teacher LLM -> Intervention Function -> Theory of Mind Prompt -> Student LLM
  - Student LLM -> Predictions -> Teacher LLM (feedback loop)

- Critical path:
  1. Teacher observes student performance on demonstration samples
  2. Teacher builds mental models for intervention function and ToM
  3. For each test point, teacher calculates expected utility
  4. Teacher intervenes on top-ranked points within budget
  5. Student conditions on explanations and generates predictions
  6. Process repeats for multi-round generalization

- Design tradeoffs:
  - Budget size vs. performance: Larger budgets improve accuracy but increase cost
  - Personalization vs. generalization: Highly tailored explanations may not transfer as well
  - Model size: Larger teachers perform better but are more expensive to run

- Failure signatures:
  - Student performance plateaus despite increased intervention
  - Teacher explanations don't improve student accuracy on explained samples
  - Multi-round interactions show no generalization to unexplained data

- First 3 experiments:
  1. Random intervention baseline: Teacher intervenes randomly at different budgets to establish performance ceiling
  2. Expected Utility vs. baselines: Compare utility-based intervention against random, teacher confidence, and student confidence methods
  3. Personalization effectiveness: Compare personalized ToM explanations against unpersonalized CoT explanations at low budgets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models learn from explanations in a way that generalizes beyond the specific examples they were trained on?
- Basis in paper: [explicit] The paper discusses multi-round interactions where the teacher explains samples to the student, and the student then uses these explanations to make predictions on future unexplained data. The paper shows that teacher explanations do improve student performance on unexplained data, but it does not explore whether the student can generalize the explanations to completely new tasks or domains.
- Why unresolved: The paper focuses on a specific task and dataset, and it does not explore whether the student can generalize the explanations to other tasks or domains.
- What evidence would resolve it: Experiments showing that the student can use explanations learned from one task to improve performance on a completely different task or domain.

### Open Question 2
- Question: How does the size of the teacher model affect the student's ability to learn from explanations?
- Basis in paper: [explicit] The paper compares the performance of students when taught by different-sized teacher models, but it does not explore how the size of the teacher model affects the student's ability to learn from explanations.
- Why unresolved: The paper only considers a limited range of teacher model sizes, and it does not explore the relationship between teacher model size and student learning.
- What evidence would resolve it: Experiments comparing the performance of students when taught by teacher models of different sizes, and analyzing the relationship between teacher model size and student learning.

### Open Question 3
- Question: Can language models learn from explanations that are not generated by other language models?
- Basis in paper: [explicit] The paper focuses on language models teaching other language models, but it does not explore whether language models can learn from explanations generated by other sources, such as humans or rule-based systems.
- Why unresolved: The paper only considers explanations generated by language models, and it does not explore whether language models can learn from explanations generated by other sources.
- What evidence would resolve it: Experiments comparing the performance of students when taught by explanations generated by language models versus explanations generated by other sources, such as humans or rule-based systems.

## Limitations

- The paper lacks precise documentation of demonstration prompts and confidence estimation methods needed for exact reproduction
- Personalization improvements may stem from overfitting to specific demonstration patterns rather than genuine adaptation to individual student weaknesses
- The study doesn't provide sufficient analysis of whether students develop general reasoning principles or merely memorize specific teacher explanations

## Confidence

- Confidence in core framework architecture: Medium - well-specified design but missing critical implementation details
- Confidence in personalization benefits: Medium-Low - results show improvement but evidence quality is limited
- Confidence in generalization claims: Low-Medium - multi-round results promising but lack rigorous transfer analysis
- Confidence in deceptive teacher results: Medium - single adversarial strategy demonstrated but limited exploration of manipulation space

## Next Checks

1. Replicate the expected utility calculation by implementing the mental model simulation with the exact prompt format and verifying that confidence estimates match the paper's reported values across multiple intervention scenarios.

2. Test personalization sensitivity through ablation studies that remove different components of the ToM personalization (demonstration selection, explanation generation, conditioning) to isolate which elements drive the performance gains.

3. Evaluate knowledge transfer by designing experiments that test student performance on novel problem types after multi-round teaching to determine whether learning generalizes beyond the specific examples seen during training.