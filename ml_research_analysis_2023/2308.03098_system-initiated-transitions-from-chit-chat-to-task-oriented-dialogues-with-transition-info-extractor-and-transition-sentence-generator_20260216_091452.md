---
ver: rpa2
title: System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues with
  Transition Info Extractor and Transition Sentence Generator
arxiv_id: '2308.03098'
source_url: https://arxiv.org/abs/2308.03098
tags:
- transition
- slot
- chit-chat
- dialogue
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a system that proactively transitions from chit-chat
  to task-oriented dialogue by detecting hidden user intent and generating appropriate
  transition sentences. The approach combines a Transition Info Extractor (TIE) to
  detect domain/slot/value information from chit-chat context with a Transition Sentence
  Generator (TSG) integrated into a unified NLG model via Adapter tuning and prompt
  learning.
---

# System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues with Transition Info Extractor and Transition Sentence Generator

## Quick Facts
- arXiv ID: 2308.03098
- Source URL: https://arxiv.org/abs/2308.03098
- Reference count: 12
- Key outcome: A system that proactively transitions from chit-chat to task-oriented dialogue by detecting hidden user intent and generating appropriate transition sentences, achieving 82.78% BERTScore F1 on domain-slot-value guided transitions

## Executive Summary
This work addresses the challenge of system-initiated transitions from chit-chat to task-oriented dialogues by proposing a unified framework that detects hidden user intent and generates appropriate transition sentences. The approach combines a Transition Info Extractor (TIE) using RoBERTa with CRF for improved slot filling accuracy, and a Transition Sentence Generator (TSG) integrated into a unified NLG model via Adapter tuning and prompt learning. The system demonstrates that transition prompts significantly improve generation quality and enable smooth, proactive dialogue mode switching while maintaining performance on both chit-chat and task-oriented responses.

## Method Summary
The method employs a two-stage approach: first, TIE uses RoBERTa with CRF to jointly predict domain/slot and extract slot values from chit-chat context through IOB labeling; second, TSG generates transition sentences when TIE detects task-related information, using Adapter tuning and transition prompts to guide generation. The unified GPT-2 model with AdapterHub layers can generate both chit-chat and task-oriented responses, with TIE detection triggering TSG activation at transition turns. The system is trained on the Prepended FusedChat dataset (3255 train, 474 validation, 331 test dialogues) containing chit-chat prepended to MultiWOZ task-oriented dialogues.

## Key Results
- TIE achieves 73.67% semantic accuracy on test data with notable performance drops for specific domain-slot combinations
- TSG generates transition sentences with 96.8% accuracy in including transition prompts
- Combined system achieves 82.78% BERTScore F1 on domain-slot-value guided transitions
- Transition prompts significantly improve generation quality compared to domain-only guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TIE extracts task-related entities from chit-chat context enabling system-initiated transitions
- Mechanism: TIE uses RoBERTa encoder with CRF layer to jointly predict domain/slot and extract slot values via IOB labeling
- Core assumption: Preceding chit-chat contains implicit task-related entities that can be reliably extracted
- Evidence anchors:
  - [abstract] "detecting domain/slot/value information from chit-chat context"
  - [section 4.1] "utilize the pre-trained RoBERTa...for jointly predicting transition domain and corresponding slot, also extracting the specific value through slot filling task"
  - [corpus] Weak evidence - only mentions related works on NLU but no direct TIE evaluation
- Break condition: If chit-chat lacks clear task-related entities or contains ambiguous references, extraction accuracy drops significantly

### Mechanism 2
- Claim: TSG generates transition sentences that bridge chit-chat to task-oriented dialogue
- Mechanism: TSG uses Adapter tuning and transition prompts to generate domain-guided or domain-slot-value guided sentences
- Core assumption: Transition prompts provide sufficient guidance for coherent transition generation
- Evidence anchors:
  - [abstract] "generate appropriate transition sentences...via Adapter tuning and prompt learning"
  - [section 5.2.1] "transition prompt...explicitly guides the transition sentence generation"
  - [corpus] Moderate evidence - related works on adapter tuning but not specifically for transition generation
- Break condition: If transition prompts are too generic or extraction is inaccurate, generated sentences may not smoothly connect dialogues

### Mechanism 3
- Claim: Combined TIE and TSG achieve proactive dialogue transitions with maintained performance
- Mechanism: TIE detection triggers TSG activation in unified NLG, preserving chit-chat and task-oriented capabilities
- Core assumption: Adapter layers can be selectively activated without degrading base model performance
- Evidence anchors:
  - [abstract] "combined system achieves 82.78% BERTScore F1 on domain-slot-value guided transitions"
  - [section 5.2.2] "Adapter tuning...can retain the original capabilities of generating normal chit-chat and task-oriented responses"
  - [corpus] Weak evidence - no direct corpus evidence for combined performance
- Break condition: If adapter tuning causes catastrophic forgetting or interference with base model capabilities

## Foundational Learning

- Concept: Named Entity Recognition and Slot Filling
  - Why needed here: TIE must extract specific domain/slot/value entities from chit-chat
  - Quick check question: What is the difference between named entity recognition and slot filling in dialogue systems?

- Concept: Prompt Learning and Adapter Tuning
  - Why needed here: TSG needs to extend unified NLG without retraining entire model
  - Quick check question: How do adapter layers differ from full fine-tuning in transformer models?

- Concept: Dialogue State Tracking
  - Why needed here: Understanding when and how to transition between dialogue modes
  - Quick check question: What signals indicate a user wants to switch from chit-chat to task-oriented dialogue?

## Architecture Onboarding

- Component map:
  - TIE (RoBERTa + CRF): Detects transition domain/slot/value from chit-chat
  - Unified GPT-2: Generates responses for both dialogue modes
  - TSG (Adapter layers + Prompts): Generates transition sentences when triggered
  - Transition prompt converter: Converts TIE output to prompt format

- Critical path:
  1. Chit-chat input → TIE processing → Domain/slot/value extraction
  2. Extraction success → Prompt generation → TSG activation
  3. Transition sentence generation → Combined with normal response

- Design tradeoffs:
  - Joint vs separate training for TIE components
  - Domain-slot-value vs domain-only prompts for TSG
  - Adapter layer types (Houlsby vs Pfeiffer) for efficiency

- Failure signatures:
  - Low TIE accuracy → No transitions generated
  - Generic TSG output → Poor transition quality
  - Adapter interference → Degraded base model performance

- First 3 experiments:
  1. Evaluate TIE extraction accuracy on held-out chit-chat data
  2. Test TSG generation quality with various prompt types
  3. Measure combined system performance at transition turns only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do users respond to proactive system-initiated transitions from chit-chat to task-oriented dialogue in real-world deployments?
- Basis in paper: [inferred] The paper discusses proactive transitions and mentions that "proactivity is always desired during the development of voice assistants" but does not provide empirical user feedback data from real deployments
- Why unresolved: The paper focuses on technical implementation and evaluation metrics but lacks user studies or real-world deployment data to assess actual user acceptance and experience with proactive transitions
- What evidence would resolve it: User satisfaction surveys, interaction logs showing acceptance rates, or A/B testing comparing user behavior with and without proactive transitions in deployed systems

### Open Question 2
- Question: What are the optimal timing and frequency parameters for system-initiated transitions in multi-turn dialogues?
- Basis in paper: [inferred] The paper mentions that "dialogue interactions could be more sophisticated in real life and it is difficult to accurately define the most appropriate moment to initiate a proactive transition" and notes that "it gets more complicated if there are multiple transitions in one dialogue"
- Why unresolved: While the paper implements a transition detection system, it doesn't investigate how often transitions should occur, when exactly they should be triggered, or how to handle multiple transitions within a single conversation
- What evidence would resolve it: User studies measuring optimal transition points, analysis of dialogue flow with different timing strategies, or experiments varying transition frequency to find sweet spots for user engagement

### Open Question 3
- Question: How does the TIE model perform on out-of-domain or unexpected user intents during chit-chat?
- Basis in paper: [explicit] The paper states that when no explicit user intention is detected, the domain classifier should recognize it as "UNK" but doesn't evaluate how the system handles unexpected or out-of-distribution scenarios
- Why unresolved: The evaluation focuses on in-domain performance with known domains (train, restaurant, attraction, taxi) but doesn't address robustness to novel user intents or domains not present in the training data
- What evidence would resolve it: Experiments testing TIE performance on out-of-domain dialogues, analysis of false positive/negative rates for unexpected user intents, or stress testing with adversarial chit-chat scenarios

## Limitations
- TIE model shows moderate accuracy (73.67% semantic accuracy) with notable performance drops for specific domain-slot combinations
- 3.2% of generated sentences lack appropriate transition prompts, indicating incomplete coverage in challenging scenarios
- Evaluation focuses primarily on transition-specific metrics rather than comprehensive assessment of overall dialogue quality

## Confidence
- High confidence: The fundamental architecture combining TIE and TSG for transition detection and generation is sound and technically feasible
- Medium confidence: The quantitative results are reliable for the specific dataset and evaluation metrics used, but generalizability to other domains or conversation styles requires further validation
- Low confidence: The assumption that all chit-chat contexts contain extractable task-related entities that justify system-initiated transitions has limited empirical support beyond the curated dataset

## Next Checks
1. **Ablation study on TIE components**: Remove CRF layer from TIE and retrain to quantify the contribution of joint prediction to extraction accuracy. Compare performance degradation specifically for the problematic train domain slot confusion.

2. **Human evaluation of transition quality**: Conduct user studies with 50+ participants rating transition sentences on criteria including naturalness, relevance, and appropriateness. Measure whether high automatic metrics correlate with positive human judgments.

3. **Cross-domain generalization test**: Apply the trained TIE and TSG models to an independent task-oriented dataset (e.g., Schema-Guided Dialogue) without fine-tuning. Quantify performance drop and identify which component (entity extraction vs. sentence generation) is most vulnerable to domain shift.