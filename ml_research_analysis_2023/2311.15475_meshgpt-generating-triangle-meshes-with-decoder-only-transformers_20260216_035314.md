---
ver: rpa2
title: 'MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers'
arxiv_id: '2311.15475'
source_url: https://arxiv.org/abs/2311.15475
tags:
- meshes
- mesh
- shape
- shapes
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MeshGPT, a novel method for generating triangle
  meshes using a decoder-only transformer. The approach involves learning a vocabulary
  of geometric embeddings from a collection of 3D object meshes, which are then used
  to train a transformer to predict meshes autoregressively as a sequence of triangles.
---

# MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers

## Quick Facts
- arXiv ID: 2311.15475
- Source URL: https://arxiv.org/abs/2311.15475
- Reference count: 40
- Key outcome: 9% increase in shape coverage and 30-point enhancement in FID scores over state-of-the-art mesh generation methods

## Executive Summary
This paper introduces MeshGPT, a novel method for generating triangle meshes using a decoder-only transformer. The approach learns a vocabulary of geometric embeddings from 3D object meshes via graph convolutions and residual vector quantization, then trains a transformer to predict meshes autoregressively as sequences of triangles. The method demonstrates significant improvements in shape quality metrics compared to existing mesh generation approaches, particularly in coverage and FID scores across multiple categories.

## Method Summary
MeshGPT learns a vocabulary of geometric embeddings from 3D meshes using graph convolutions to capture local topology, then applies residual vector quantization to discretize these features. A GPT-style decoder-only transformer is trained to predict the next embedding in the sequence autoregressively. During generation, the model samples embeddings from the learned codebook and decodes them into triangle meshes through a ResNet-based decoder and vertex merging process.

## Key Results
- 9% increase in shape coverage (COV) compared to state-of-the-art methods
- 30-point enhancement in FID scores across evaluated categories
- Generates compact, sharp-edged meshes that better resemble artist-created models rather than dense iso-surfacing outputs

## Why This Works (Mechanism)

### Mechanism 1
Graph convolutional encoding of triangle neighborhoods preserves local geometric and topological features. By treating each triangle as a node and connecting neighboring triangles with edges, SAGEConv layers aggregate local mesh structure into enriched feature vectors for each triangle. Core assumption: Local mesh topology is sufficient to represent triangle embeddings for generation.

### Mechanism 2
Residual vector quantization (RVQ) with per-vertex aggregation compresses sequence length while maintaining reconstruction quality. Features are split per vertex, aggregated across shared vertices, and quantized with depth D, reducing tokens from 9N to DN while preserving geometric detail. Core assumption: Per-vertex aggregation reduces redundancy without losing critical geometric information.

### Mechanism 3
GPT-style autoregressive prediction over learned embeddings generates coherent triangle sequences that reconstruct into compact, sharp-edged meshes. A decoder-only transformer predicts the next codebook index given previous embeddings, ensuring sequence consistency and producing meshes with realistic triangulation patterns. Core assumption: The learned embedding space is sufficiently smooth for the transformer to model transitions between triangles.

## Foundational Learning

- Concept: Graph convolutional networks on meshes
  - Why needed here: To encode local geometric and topological context into triangle features before quantization
  - Quick check question: How does SAGEConv aggregate neighbor features differently from standard GCN?

- Concept: Vector quantization and residual quantization
  - Why needed here: To discretize continuous geometric features into learnable tokens for autoregressive modeling
  - Quick check question: Why is residual quantization preferred over single-code quantization in this context?

- Concept: Autoregressive transformers for sequence generation
  - Why needed here: To model the conditional probability of triangle sequences given previous triangles
  - Quick check question: What is the role of positional encoding in this mesh generation setup?

## Architecture Onboarding

- Component map: GraphConv encoder → RVQ bottleneck → Transformer decoder → ResNet decoder → Vertex merging
- Critical path: Mesh → GraphConv → RVQ → Transformer → Triangle sequence → ResNet → Final mesh
- Design tradeoffs: Per-vertex quantization reduces sequence length but may lose face-specific detail; transformer context window limits mesh complexity
- Failure signatures: Repeating structures (context overflow), floating triangles (decoder regression vs classification), blocky shapes (poor encoder features)
- First 3 experiments:
  1. Verify GraphConv encoder preserves local topology by visualizing reconstructed meshes from encoder features
  2. Test RVQ with varying depths D to measure sequence length vs reconstruction quality trade-off
  3. Validate transformer next-index prediction accuracy on held-out triangle sequences before full mesh generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational cost of MeshGPT compare to other methods, especially in terms of memory usage and inference speed?
- Basis in paper: The paper mentions that MeshGPT is slower due to its autoregressive nature, with mesh generation times taking 30 to 90 seconds. It also notes that the current computational resources limit the use of a GPT2-medium transformer, which is smaller than more sophisticated models like Llama2.
- Why unresolved: The paper does not provide a detailed comparison of the computational costs with other methods.
- What evidence would resolve it: A detailed analysis comparing the memory usage and inference speed of MeshGPT with other state-of-the-art methods would resolve this question.

### Open Question 2
- Question: How does the quality of the generated meshes scale with the size of the training dataset and the complexity of the shapes?
- Basis in paper: The paper discusses the use of a large dataset (ShapeNetV2) and mentions that the model is pre-trained on all categories and fine-tuned for each evaluated category.
- Why unresolved: The paper does not explicitly discuss how the quality of the generated meshes scales with the size of the training dataset or the complexity of the shapes.
- What evidence would resolve it: A systematic study varying the size of the training dataset and the complexity of the shapes, along with a corresponding analysis of the quality of the generated meshes, would resolve this question.

### Open Question 3
- Question: How does the choice of the transformer architecture (e.g., GPT2-medium vs. Llama2) affect the performance of MeshGPT?
- Basis in paper: The paper mentions that the current computational resources limit the use of a GPT2-medium transformer, which is smaller than more sophisticated models like Llama2.
- Why unresolved: The paper does not provide a direct comparison between the performance of MeshGPT using different transformer architectures.
- What evidence would resolve it: A direct comparison of the performance of MeshGPT using different transformer architectures, such as GPT2-medium and Llama2, would resolve this question.

## Limitations

- The method's performance on highly complex or topologically diverse meshes beyond the ShapeNet categories tested remains unclear
- Critical hyperparameters like RVQ depth and transformer context window size are not extensively explored
- Specific implementation details necessary for reproduction are not fully specified

## Confidence

**High Confidence**: The core architectural approach of using graph convolutions for feature extraction and residual vector quantization for sequence compression is technically sound and well-grounded in existing literature.

**Medium Confidence**: The claimed improvements over state-of-the-art methods are supported by quantitative metrics, but the absolute performance values and their practical significance require careful interpretation.

**Low Confidence**: The specific implementation details critical for reproduction, such as exact encoder-decoder network architecture and transformer optimization hyperparameters, are not fully specified.

## Next Checks

1. Conduct controlled ablation studies removing each key component (graph convolutions, RVQ, transformer) to quantify their individual contributions to final performance metrics

2. Evaluate the method on more diverse 3D datasets beyond ShapeNet, including meshes with higher topological complexity and irregular structures

3. Systematically vary the depth D in residual vector quantization and transformer context window size to understand their impact on generation quality and identify optimal configurations