---
ver: rpa2
title: 'Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation'
arxiv_id: '2307.15337'
source_url: https://arxiv.org/abs/2307.15337
tags:
- answer
- skeleton
- question
- latency
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Skeleton-of-Thought (SoT), a method to accelerate
  the generation latency of large language models (LLMs) by parallelizing the decoding
  process. SoT first prompts the LLM to generate a concise skeleton of the answer,
  then conducts parallel API calls or batched decoding to expand the contents of each
  skeleton point in parallel, and finally aggregates the outputs.
---

# Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation

## Quick Facts
- arXiv ID: 2307.15337
- Source URL: https://arxiv.org/abs/2307.15337
- Reference count: 40
- Key outcome: Introduces Skeleton-of-Thought (SoT) method achieving up to 2.39x speed-up in LLM generation latency by parallelizing decoding after skeleton generation

## Executive Summary
This paper presents Skeleton-of-Thought (SoT), a method to accelerate large language model (LLM) inference by parallelizing the decoding process. SoT first prompts the LLM to generate a concise skeleton of the answer, then conducts parallel API calls or batched decoding to expand the contents of each skeleton point in parallel, and finally aggregates the outputs. The approach achieves significant speed-ups across 11 different LLMs, with up to 2.39x improvement. Additionally, SoT can potentially improve the answer quality on several question categories in terms of diversity and relevance. The paper also discusses future work and open questions, including generalizing SoT to a "Graph-of-Thought" framework and exploring data-centric optimization for efficiency.

## Method Summary
SoT is a two-stage method that first generates a concise answer skeleton (list of key points) using a carefully crafted prompt, then expands each skeleton point in parallel using batch decoding or multiple API calls. The skeleton stage guides the LLM to structure its response, while the parallel expansion stage exploits GPU computation units that are underutilized during sequential decoding. The method uses specific prompt templates for both stages and employs regex-based point extraction to identify individual skeleton points for parallel processing.

## Key Results
- Achieves up to 2.39x speed-up across 11 different LLM models
- Improves answer diversity and relevance on several question categories
- Effective for questions with clear answer structures (knowledge, common-sense, generic)
- Less effective for questions requiring step-by-step reasoning (math, coding)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel API calls or batched decoding can reduce decoding latency by exploiting under-utilized GPU computation units during sequential decoding.
- Mechanism: During sequential decoding, GPU computation units are underutilized because each token generation requires loading all model weights, causing I/O bottlenecks. By generating multiple tokens in parallel, GPU utilization increases nearly linearly with batch size, reducing per-token latency.
- Core assumption: The I/O bottleneck during decoding is the dominant factor in latency, and GPU utilization is the limiting factor for speed-up.
- Evidence anchors: [abstract] "Not only does SoT provide considerable speed-ups across 11 different LLMs, with up to 2.39x improvement." [section] "The utilization is calculated with respect to the FP16 2 tensor core peak performance â€“ 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token is comparable to that of prefilling 128 tokens (40ms)."
- Break condition: If the model's sequential dependencies are too strong (e.g., math problems), parallel decoding fails because later steps depend on earlier results.

### Mechanism 2
- Claim: LLMs can generate structured skeletons that allow independent expansion of each point, enabling parallelization.
- Mechanism: The skeleton prompt guides the LLM to produce a concise outline of the answer. Each point in the outline can be expanded independently without requiring context from other points, allowing parallel generation.
- Core assumption: The LLM can generate coherent skeletons and expand points independently while maintaining answer quality.
- Evidence anchors: [abstract] "SoT first prompts the LLM to generate a concise skeleton of the answer, then conducts parallel API calls or batched decoding to expand the contents of each skeleton point in parallel." [section] "We find that, in most cases, the skeleton responses are in the desired format. Therefore, we can simply use a regular expression to extract point indexes and point skeletons from the skeleton response."
- Break condition: If the question requires step-by-step reasoning where later steps depend on earlier results, the skeleton approach fails.

### Mechanism 3
- Claim: SoT can improve answer quality by encouraging LLMs to think from multiple perspectives before expanding details.
- Mechanism: The skeleton stage forces the LLM to consider multiple aspects of the answer before expanding, leading to more comprehensive and diverse responses. The parallel expansion maintains relevance by focusing on the outlined points.
- Core assumption: Forcing the LLM to structure its response improves the quality of the final answer compared to sequential generation.
- Evidence anchors: [abstract] "SoT can potentially improve the answer quality on several question categories in terms of diversity and relevance." [section] "SoT's answer is structured in a list of points. It is suitable for some questions but not all... SoT encourages LLMs to directly discuss the answers from multiple aspects without filler words."
- Break condition: If the question requires a coherent narrative or transition between points, the list-based structure may reduce coherence and immersion.

## Foundational Learning

- Concept: Autoregressive decoding and its sequential nature
  - Why needed here: Understanding why sequential decoding is slow and how SoT breaks this pattern is fundamental to grasping the paper's contribution.
  - Quick check question: Why does autoregressive decoding create a bottleneck in LLM inference?

- Concept: Prompt engineering and in-context learning
  - Why needed here: SoT relies on carefully crafted prompts to guide the LLM through skeleton generation and point expansion.
  - Quick check question: How does the skeleton prompt template differ from standard instruction-tuned prompts?

- Concept: GPU memory hierarchy and I/O bottlenecks
  - Why needed here: The efficiency gains come from better utilization of GPU resources during parallel decoding.
  - Quick check question: Why is decoding one token comparable in latency to prefilling 128 tokens?

## Architecture Onboarding

- Component map: Skeleton generation -> Point extraction -> Parallel expansion -> Aggregation
- Critical path: Skeleton generation latency + slowest point expansion latency (for API-based models)
- Design tradeoffs: Parallelism vs. coherence - parallel generation may reduce answer coherence for certain question types
- Failure signatures: Increased latency when skeleton points are too interdependent, or when model cannot follow skeleton format
- First 3 experiments:
  1. Profile latency of skeleton generation vs. normal generation for a simple question
  2. Test parallel expansion with different batch sizes to find optimal configuration
  3. Evaluate answer quality degradation for math questions compared to other categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine if a question is suitable for SoT vs. requiring sequential decoding like Chain-of-Thought?
- Basis in paper: [inferred] from the discussion of SoT's limitations on math and coding questions that require step-by-step reasoning
- Why unresolved: The paper suggests questions with clear answer structures are suitable for SoT, but doesn't provide a concrete framework for determining suitability
- What evidence would resolve it: A taxonomy of question types with clear guidelines on when SoT vs. sequential decoding is appropriate

### Open Question 2
- Question: How can SoT be extended to handle sequential dependencies between points in the answer skeleton?
- Basis in paper: [explicit] "The current SoT solution forces a fully parallelized decoding of all points, ignoring the possible sequential dependencies between points"
- Why unresolved: The paper mentions this as a limitation but doesn't propose specific solutions for handling dependencies
- What evidence would resolve it: A working implementation of "Graph-of-Thought" that allows for conditional point expansion based on prior point content

### Open Question 3
- Question: What are the optimal prompt engineering strategies to improve SoT performance across different question categories?
- Basis in paper: [inferred] from the observation that current SoT prompts work well for some categories but poorly for others like math and coding
- Why unresolved: The paper only tests one prompt template per stage and notes performance variations but doesn't explore prompt optimization
- What evidence would resolve it: A comprehensive study comparing different prompt strategies and their effects on SoT performance across question categories

## Limitations
- Limited generalization across question types, particularly for those requiring coherent narratives or step-by-step reasoning
- Quality evaluation relies on LLM judges, introducing subjectivity and potential bias
- Implementation complexity requires two prompt templates, point extraction logic, and parallel decoding implementation

## Confidence
- High confidence: Claims about SoT's speed-up mechanism (parallelizing token generation after skeleton creation) are well-supported by empirical results showing up to 2.39x improvement across 11 models
- Medium confidence: Claims about quality improvements (diversity and relevance) are supported by evaluation but rely on LLM judges, which introduces subjectivity
- Low confidence: Claims about SoT's generalizability to more complex frameworks (Graph-of-Thought) and its effectiveness for highly specialized domains are speculative

## Next Checks
- Validation 1: Cross-dataset generalization test - Apply SoT to a benchmark dataset outside Vicuna-80 (e.g., TruthfulQA or RealToxicityPrompts) and measure both speed-up and quality degradation across different question categories
- Validation 2: Ablation study on prompt design - Systematically test variations of the skeleton and point-expanding prompt templates to determine which components are essential for SoT's effectiveness
- Validation 3: Memory and resource overhead profiling - Measure GPU memory usage and total energy consumption (including API calls) for SoT versus normal generation across different batch sizes and model scales