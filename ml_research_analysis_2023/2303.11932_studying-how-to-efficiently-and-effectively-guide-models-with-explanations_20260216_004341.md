---
ver: rpa2
title: Studying How to Efficiently and Effectively Guide Models with Explanations
arxiv_id: '2303.11932'
source_url: https://arxiv.org/abs/2303.11932
tags:
- energy
- baseline
- score
- dominated
- dominating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates model guidance methods that use attribution
  maps to regularize model explanations, ensuring predictions are "right for the right
  reasons." While prior work focused on simple or synthetic datasets, this study evaluates
  guidance on large-scale datasets (PASCAL VOC 2007, MS COCO 2014) across various
  attribution methods (GradCAM, IxG, IntGrad, B-cos), localization losses (Energy,
  L1, PPCE, RRR), and guidance depths. A novel Energy loss is proposed to optimize
  for on-object localization while avoiding background regions.
---

# Studying How to Efficiently and Effectively Guide Models with Explanations

## Quick Facts
- arXiv ID: 2303.11932
- Source URL: https://arxiv.org/abs/2303.11932
- Reference count: 40
- Key outcome: Energy loss provides best trade-off between localization quality and classification performance, particularly for B-cos models at input layer with only 1% annotated images

## Executive Summary
This work evaluates model guidance methods that use attribution maps to regularize model explanations, ensuring predictions are "right for the right reasons." The study evaluates guidance on large-scale datasets (PASCAL VOC 2007, MS COCO 2014) across various attribution methods, localization losses, and guidance depths. A novel Energy loss is proposed that optimizes for on-object localization while avoiding background regions within bounding boxes. Experiments demonstrate that the Energy loss provides the best trade-off between localization quality and classification performance, particularly for B-cos models at the input layer, while requiring only 1% of images to be annotated with bounding boxes.

## Method Summary
The method involves jointly optimizing classification and localization losses using attribution maps as explanations. Pre-trained ResNet-50 models are fine-tuned using binary cross-entropy for classification and various localization losses (Energy, L1, PPCE, RRR*) that regularize attribution maps to focus on object features within bounding boxes. Attribution methods include GradCAM, IxG, IntGrad, and B-cos, applied at input or final layers. The Energy loss specifically maximizes EPG score (fraction of positive attributions inside bounding box) without enforcing uniform attribution within boxes, allowing models to focus on object-specific features while ignoring background regions.

## Key Results
- Energy loss provides best trade-off between localization quality (EPG, IoU) and classification performance
- B-cos models at input layer achieve highest localization performance gains when guided
- Approach requires only 1% of images annotated with bounding boxes for effective guidance
- Energy loss improves generalization under distribution shifts in synthetic datasets
- Energy loss is robust to annotation noise and better focuses on object-specific features compared to other losses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy loss optimizes for on-object localization while avoiding background regions within bounding boxes
- Mechanism: By maximizing EPG score, Energy loss doesn't enforce uniform attribution within boxes, allowing focus on object-specific features while ignoring background
- Core assumption: Bounding boxes contain both object and background regions; uniform attribution is not desirable
- Evidence anchors: Abstract mentions Energy loss optimizes for on-object localization; section explains Energy loss doesn't impose uniform prior
- Break condition: If bounding boxes perfectly align with objects (no background), advantage disappears

### Mechanism 2
- Claim: Model guidance with limited annotations (1% of images) can significantly improve localization and generalization
- Mechanism: Joint optimization of classification and localization for small annotated subset provides regularization effect for unannotated data
- Core assumption: Limited annotations provide sufficient signal to guide models toward object-specific features
- Evidence anchors: Abstract states approach is cost-effective requiring 1% annotations; section mentions using small fraction of training data
- Break condition: If limited annotations are not representative of dataset, guidance may not generalize

### Mechanism 3
- Claim: B-cos models achieve best localization performance when guided at input layer
- Mechanism: B-cos models provide model-inherent explanations that are more detailed and faithful at input layer, making them more responsive to guidance
- Core assumption: Model-inherent explanations capture more detailed feature contributions than post-hoc methods
- Evidence anchors: Abstract mentions B-cos models at input layer provide best trade-off; section states B-cos achieves highest EPG/IoU performance
- Break condition: If input layer attributions become too noisy, guidance may not be effective

## Foundational Learning

- Concept: Attribution methods (GradCAM, IxG, IntGrad, B-cos)
  - Why needed here: Different attribution methods provide different ways to explain model decisions, affecting how well guidance can be applied
  - Quick check question: Which attribution method is inherently interpretable and provides detailed input-layer explanations?
    - Answer: B-cos

- Concept: Localization losses (L1, PPCE, RRR*, Energy)
  - Why needed here: These losses define how models are guided to focus on relevant features within bounding boxes
  - Quick check question: Which localization loss promotes uniform attribution within bounding box?
    - Answer: L1

- Concept: Pareto-dominant model selection
  - Why needed here: Since we have two objectives (classification and localization), we need way to select models representing optimal trade-offs
  - Quick check question: What metric is used to select models that balance classification and localization performance?
    - Answer: Pareto-dominance across F1, EPG, and IoU scores

## Architecture Onboarding

- Component map: Image data (X) and one-hot encoded labels (y) -> Attribution method (IxG, IntGrad, GradCAM, or B-cos) -> Localization loss (L1, PPCE, RRR*, or Energy) -> Classification loss (binary cross-entropy) -> Guided model with improved localization

- Critical path:
  1. Load image and annotation
  2. Generate attribution map for selected class
  3. Compute localization loss between attribution and bounding box
  4. Compute classification loss
  5. Backpropagate combined loss
  6. Update model parameters

- Design tradeoffs:
  - Input vs. final layer guidance: Input layer provides more detailed attributions but may be noisier; final layer is smoother but less detailed
  - Choice of attribution method: Model-inherent (B-cos) vs. post-hoc (GradCAM, IxG, IntGrad)
  - Localization loss: Energy for on-object focus vs. L1 for uniform coverage

- Failure signatures:
  - Poor localization: Attributions spread throughout bounding box including background (L1 loss effect)
  - Noisy attributions: High-frequency patterns in attribution maps (input layer guidance)
  - No improvement: Model ignores guidance and relies on spurious correlations

- First 3 experiments:
  1. Compare Energy loss vs. L1 loss on B-cos model at input layer using VOC dataset
  2. Test model guidance with 1% vs. 10% vs. 100% annotated images
  3. Evaluate Energy loss robustness by training with increasingly dilated bounding boxes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Energy loss and L1 loss compare in effectiveness at guiding models to focus on object-specific features when using segmentation masks instead of bounding boxes?
- Basis in paper: [inferred] Paper compares Energy and L1 losses using bounding boxes and finds Energy better at focusing on object-specific features. Mentions using segmentation masks to evaluate on-object localization.
- Why unresolved: Paper doesn't directly compare performance of Energy and L1 losses when using segmentation masks for guidance
- What evidence would resolve it: Controlled experiment comparing EPG scores and qualitative attribution maps of models trained with Energy and L1 losses using segmentation masks instead of bounding boxes

### Open Question 2
- Question: How does effectiveness of model guidance via attribution methods compare to other approaches for mitigating spurious correlations, such as data augmentation or adversarial training?
- Basis in paper: [explicit] Paper demonstrates model guidance can improve accuracy under distribution shifts in synthetic datasets (Waterbirds-100) but doesn't compare to other methods for mitigating spurious correlations
- Why unresolved: Paper focuses on evaluating model guidance in isolation and doesn't benchmark against other techniques for addressing spurious correlations
- What evidence would resolve it: Comparative study evaluating performance of model guidance against data augmentation, adversarial training, and other methods for mitigating spurious correlations on datasets with known spurious correlations

### Open Question 3
- Question: How does choice of attribution method (e.g., GradCAM, IxG, IntGrad) impact effectiveness of model guidance at different network depths?
- Basis in paper: [explicit] Paper evaluates different attribution methods at input and final layers, finding B-cos performs best at input layer while all models show improvements at final layer
- Why unresolved: Paper doesn't explore impact of attribution method choice at intermediate layers or provide comprehensive analysis of how attribution method effectiveness varies with network depth
- What evidence would resolve it: Systematic evaluation of different attribution methods at multiple network depths, comparing effectiveness in terms of EPG, IoU, and classification performance

## Limitations
- Focus on multi-label classification with ResNet-50 limits generalizability to other architectures and tasks
- Energy loss mechanism depends critically on assumption that bounding boxes contain both object and background regions
- Claim that B-cos models inherently provide better explanations lacks comparison with other inherently interpretable models

## Confidence

- High confidence: Energy loss effectively optimizes for on-object localization while avoiding background regions, supported by EPG and IoU metrics showing consistent improvements over L1 loss
- Medium confidence: Cost-effectiveness claim (1% annotations) is supported by experimental results but limited to specific datasets and architectures tested
- Medium confidence: B-cos models achieving best localization at input layer is demonstrated but may be architecture-specific rather than universal property

## Next Checks
1. Test Energy loss robustness across different annotation qualities by systematically varying bounding box tightness from tight to dilated annotations on multiple datasets
2. Compare B-cos guidance performance against other inherently interpretable architectures (e.g., attention-based models) to determine if advantage is specific to B-cos or general to model-inherent explanations
3. Evaluate 1% annotation cost-effectiveness claim on dataset with different object characteristics (e.g., fine-grained classification) to test generalizability beyond VOC and COCO