---
ver: rpa2
title: 'Greedy PIG: Adaptive Integrated Gradients'
arxiv_id: '2311.06192'
source_url: https://arxiv.org/abs/2311.06192
tags:
- attribution
- feature
- integrated
- greedy
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Greedy PIG, an adaptive generalization of
  integrated gradients for feature attribution and selection. The authors formulate
  feature attribution as a subset selection problem, identifying the main weakness
  of integrated gradients as sensitivity to feature correlations.
---

# Greedy PIG: Adaptive Integrated Gradients

## Quick Facts
- arXiv ID: 2311.06192
- Source URL: https://arxiv.org/abs/2311.06192
- Reference count: 25
- Key outcome: Greedy PIG achieves 0.8486 AUC for softmax attribution on ImageNet versus 0.2639 for integrated gradients

## Executive Summary
Greedy PIG introduces an adaptive generalization of integrated gradients that formulates feature attribution as a subset selection problem. The method addresses integrated gradients' sensitivity to feature correlations by iteratively applying the base method and conditioning on previously selected features. Through this adaptive approach, Greedy PIG naturally handles correlated features while maintaining the theoretical guarantees of greedy algorithms for submodular optimization.

## Method Summary
Greedy PIG builds on integrated gradients by iteratively applying the base method and selecting top features, then conditioning future gradient computations on the selected features to eliminate correlations. The algorithm works by running integrated gradients multiple times, each time selecting the top-z features from the previous round and fixing their baseline values to their final values. This creates a residual set function where correlations between already-selected features and remaining features are eliminated through conditioning. The method supports different attribution objectives (softmax vs KL divergence) by simply changing the set function G, allowing it to capture different aspects of model behavior while maintaining computational tractability through greedy optimization.

## Key Results
- Achieves 0.8486 AUC for softmax attribution on ImageNet versus 0.2639 for integrated gradients
- Outperforms integrated gradients on graph neural network compression tasks
- Shows superior performance on post-hoc feature selection for tabular data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Greedy PIG improves feature attribution by iteratively selecting top features and conditioning future gradient computations on them, thereby eliminating correlations between selected and unselected features.
- **Mechanism**: The algorithm works by running integrated gradients multiple times, each time selecting the top-z features from the previous round and fixing their baseline values to their final values. This creates a residual set function where correlations between already-selected features and remaining features are eliminated through conditioning.
- **Core assumption**: Feature correlations are a primary weakness of standard integrated gradients, and eliminating these correlations through iterative selection improves attribution quality.
- **Evidence anchors**:
  - [abstract] "We ascribe the main shortcomings of the path integrated gradient (PIG) algorithms to their limited ability to handle feature correlations."
  - [section] "The key advantage of this approach is that we are able to pose attribution as an explicit optimization problem" - This shows the framework allows for better handling of feature relationships.
  - [corpus] Weak - no direct mention of correlation handling in neighbor papers.
- **Break condition**: When feature correlations are minimal or when the set function G is already approximately submodular, the adaptive rounds may not provide significant improvement over standard integrated gradients.

### Mechanism 2
- **Claim**: The subset selection formulation allows feature attribution to be evaluated using AUC-style metrics that measure how well the attribution ordering preserves the model's behavior.
- **Mechanism**: By framing attribution as subset selection, the method can use the attribution quality metric which measures the area under the curve of G({r1,...,rk}) / max|S|=k G(S) for all k. This provides a principled way to evaluate how well the attribution method identifies important features.
- **Core assumption**: Attribution methods should be evaluated based on their ability to identify subsets of features that preserve model behavior, not just produce human-interpretable heatmaps.
- **Evidence anchors**:
  - [section] "We define the attribution quality of r (without reference to k) as the area under the curve defined by the points (k, G({r1, r2, ..., rk}) for all k ∈ [n)." - This directly defines the evaluation metric.
  - [abstract] "Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework" - Shows the connection to AUC metrics.
  - [corpus] Weak - neighbor papers focus on variants of integrated gradients but don't discuss AUC-style evaluation frameworks.
- **Break condition**: When the evaluation metric G doesn't correlate with the actual goal of the attribution (e.g., if G is poorly chosen), the AUC-based evaluation may not reflect true attribution quality.

### Mechanism 3
- **Claim**: Greedy PIG naturally extends to different attribution objectives (softmax vs KL divergence) by simply changing the set function G, allowing it to capture different aspects of model behavior.
- **Mechanism**: The framework allows defining different G functions like GA ttributionSoftmax(S) = fc∗(x S) for top-class attribution or GA ttributionKL(S) = ℓ(f(x), f(x S)) for capturing the full distribution. Greedy PIG can optimize any differentiable G function.
- **Core assumption**: Different attribution tasks require different objectives, and a flexible framework should allow easy switching between these objectives.
- **Evidence anchors**:
  - [section] "We define the set function: GA ttributionKL(S) = ℓ(f(x), f(x S)) = f(x) logf(x S)" - Shows the KL divergence objective formulation.
  - [section] "Specifically, instead of asking which features are most responsible for a specific classification output, we ask: Which features are most responsible for the distribution on output classes?" - Explains why multiple objectives matter.
  - [corpus] Weak - neighbor papers mention variants but don't discuss flexible objective switching in the context of subset selection.
- **Break condition**: When the set function G is non-differentiable or when gradient-based optimization becomes unstable for certain G choices.

## Foundational Learning

- **Concept**: Submodular optimization and greedy algorithms
  - Why needed here: The paper leverages greedy algorithms from submodular optimization theory, which are known to provide constant-factor approximations for maximizing weakly submodular functions. Understanding why greedy works well for subset selection problems is crucial for grasping why Greedy PIG succeeds.
  - Quick check question: Why does the greedy algorithm for subset selection work well when features are correlated? (Answer: Because it conditions on already-selected features, eliminating their correlations with remaining features)

- **Concept**: Path integrated gradients and their limitations
  - Why needed here: The paper builds directly on integrated gradients as a baseline. Understanding how integrated gradients computes attributions via path integrals and why it struggles with feature correlations is essential for understanding the motivation for Greedy PIG.
  - Quick check question: What is the main weakness of standard integrated gradients that Greedy PIG addresses? (Answer: Sensitivity to feature correlations)

- **Concept**: Feature attribution vs feature selection
  - Why needed here: The paper unifies these two tasks under a subset selection framework. Understanding the difference between attributing features for a single example versus selecting features globally for a dataset helps clarify the paper's contribution.
  - Quick check question: How does feature attribution differ from feature selection in terms of scope? (Answer: Attribution considers one example per invocation, while selection considers an entire dataset)

## Architecture Onboarding

- **Component map**: Input preprocessing -> Model loading -> Gradient oracle computation -> Subset selection (Greedy PIG) -> Attribution output -> Evaluation
- **Critical path**: Model → Gradient oracle → Subset selection (Greedy PIG) → Attribution output → Evaluation
  The gradient oracle is the computational bottleneck, especially for large models or high-resolution images.
- **Design tradeoffs**:
  - Computational cost vs attribution quality: More adaptive rounds improve quality but increase computation time
  - Choice of baseline: Different baselines affect integrated gradients results; the paper uses all-zero/gray baselines
  - Objective function selection: Different G functions capture different aspects of model behavior but may have different optimization properties
- **Failure signatures**:
  - Poor performance when features are nearly independent (adaptive rounds provide little benefit)
  - Instability when G functions are non-smooth or have flat regions
  - Overfitting to training data when using post-hoc feature selection with small datasets
- **First 3 experiments**:
  1. Run Greedy PIG on a simple linear regression problem with correlated features to verify it handles correlations better than integrated gradients
  2. Apply Greedy PIG to image attribution on a small subset of ImageNet and compare AUC scores with integrated gradients
  3. Test Greedy PIG on graph edge attribution by compressing a small graph while maintaining GNN performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but mentions that the subset selection framework can be used to find counterfactual explanations.

## Limitations
- The claim that correlations are the primary weakness of integrated gradients remains somewhat idealized without extensive validation on datasets where correlations are minimal
- Comparison with "top performing algorithms" lacks specificity about what methods were actually compared and how they were implemented
- The paper doesn't extensively validate whether the AUC-style evaluation correlates with human interpretability of attribution maps

## Confidence
- **High confidence**: The mathematical formulation and experimental methodology are sound; the AUC-style evaluation framework is clearly defined and reproducible.
- **Medium confidence**: The claim about correlation handling, while supported by results, could benefit from more direct analysis of correlation structures in the tested datasets.
- **Low confidence**: The comparison with "top performing algorithms" lacks specificity about what methods were actually compared and how they were implemented.

## Next Checks
1. **Correlation analysis validation**: Systematically measure feature correlation structures in the tested datasets and quantify how Greedy PIG's performance scales with correlation strength compared to integrated gradients.
2. **Baseline sensitivity analysis**: Test Greedy PIG with different baseline choices (not just all-zero/gray) to understand how baseline selection affects attribution quality and correlation handling.
3. **Computational complexity benchmarking**: Measure wall-clock time for Greedy PIG versus integrated gradients across different model sizes and input dimensions to quantify the practical cost of the adaptive rounds.