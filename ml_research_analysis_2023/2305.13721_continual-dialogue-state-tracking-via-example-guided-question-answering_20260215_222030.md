---
ver: rpa2
title: Continual Dialogue State Tracking via Example-Guided Question Answering
arxiv_id: '2305.13721'
source_url: https://arxiv.org/abs/2305.13721
tags:
- dialogue
- training
- learning
- state
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses continual learning in dialogue state tracking
  (DST) by reformulating DST as a granular example-guided question answering task.
  The method transforms DST into a series of slot-specific questions, enabling the
  model to learn generalizable example-based question answering.
---

# Continual Dialogue State Tracking via Example-Guided Question Answering

## Quick Facts
- arXiv ID: 2305.13721
- Source URL: https://arxiv.org/abs/2305.13721
- Authors: 
- Reference count: 15
- Key outcome: DST reformulated as granular example-guided QA achieves state-of-the-art performance on Schema-Guided Dialogue with 69.3% JGA using only 60M parameters

## Executive Summary
This paper addresses continual learning in dialogue state tracking (DST) by reformulating it as a series of slot-specific question answering tasks. The approach transforms structured slot-filling into consistent QA format across domains, enabling better generalization and reducing task shift. A retriever trained to identify turns with similar dialogue state changes provides in-context examples during training and testing. Combined with dialogue-level memory replay, this method achieves state-of-the-art performance on the Schema-Guided Dialogue dataset while using a compact 60M parameter model.

## Method Summary
The method reformulates DST as DST-EGQA by decomposing structured slot-filling into per-slot questions using domain-agnostic templates. A SentenceBERT-based retriever identifies turns with similar dialogue state changes (∆DS) to provide in-context examples. The model is trained domain-by-domain with these retrieved examples, and dialogue-level memory sampling from previous domains is used for continual learning. The approach significantly improves continual learning metrics without complex regularization or parameter expansion.

## Key Results
- Achieves 69.3% average JGA on Schema-Guided Dialogue with only 60M parameters
- Reduces backward transfer to 5.9% when combined with dialogue-level memory replay
- Outperforms existing continual learning approaches without complex regularization techniques
- Shows that small models (60M parameters) cannot effectively leverage more than one in-context example

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframing DST as granular example-guided question answering reduces task shift across domains, improving continual learning.
- Mechanism: By decomposing structured slot-filling into per-slot questions, the model learns a consistent task format that generalizes across domains rather than memorizing domain-specific output schemas.
- Core assumption: Task consistency across domains is more beneficial for continual learning than domain-specific memorization.
- Evidence anchors:
  - [abstract] "reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services"
  - [section 2.1] "DST can become a significantly more consistent task across domains by simply reformulating the DST dataset as a bundle of example-guided question answering task"

### Mechanism 2
- Claim: Training with in-context examples retrieved based on similar dialogue state changes teaches the model to leverage relevant context and ignore irrelevant examples.
- Mechanism: The retriever identifies turns with similar dialogue state changes (∆DS) as the target sample, providing in-context examples that require similar reasoning, enabling the model to learn when to use and when to ignore examples.
- Core assumption: Similar dialogue state changes imply similar reasoning patterns needed to answer slot questions.
- Evidence anchors:
  - [section 2.3] "The goal of the retrieval system is to find an example turn H′ that requires similar reasoning for answering the target sample Ht"
  - [section 4.2] "it is important for the model to be able to leverage relevant examples and ignore irrelevant ones"

### Mechanism 3
- Claim: Dialogue-level memory sampling is more effective than turn-level sampling for continual learning memory replay.
- Mechanism: Dialogue-level sampling provides more diverse and comprehensive dialogue state updates in memory, covering a wider range of state changes with fewer samples.
- Core assumption: A single dialogue contains more diverse state transitions than a single turn, making it a more efficient memory sample.
- Evidence anchors:
  - [section 4.4] "Dialogue-level sampling achieves a significantly better performance for all equivalent memory budget sizes for turn-level sampling"
  - [section 4.4] "This is likely due to dialogue-sampling leading to a more comprehensive set of samples that cover a wider diversity of dialogue state updates"

## Foundational Learning

- Concept: Task reformulation from structured generation to question answering
  - Why needed here: DST traditionally requires generating domain-specific structured outputs, which introduces significant distribution shift between domains. Reformulating as QA creates a consistent task format.
  - Quick check question: Can you explain why predicting "What is the name of the hotel?" is more generalizable than generating a structured output with domain and slot labels?

- Concept: In-context learning with retrieved examples
  - Why needed here: The model needs to learn when to leverage examples and when to ignore them, especially with imperfect retrievers. This requires training dynamics that expose the model to both relevant and irrelevant examples.
  - Quick check question: Why does the paper find that using the Oracle retriever at validation time (not training or test time) leads to best performance?

- Concept: State change similarity for retrieval
  - Why needed here: Simple dialogue state overlap matching is insufficient. State change similarity (∆DS) identifies examples requiring similar reasoning by focusing on what changed rather than what is currently predicted.
  - Quick check question: How does computing similarity on ∆DS (changes) rather than full DS (current state) improve retrieval relevance?

## Architecture Onboarding

- Component map:
  T5-small base model (60M parameters) -> Slot template mapper (manual, domain-agnostic templates) -> IC-DST-retriever (SentenceBERT-based contrastive learner) -> Memory buffer (dialogue-level sampled from previous domains) -> Training pipeline (domain-by-domain, 10 epochs each)

- Critical path:
  1. Transform target DST sample to per-slot questions using templates
  2. Retrieve top-k examples based on dialogue state change similarity
  3. Format as in-context learning: [examples] + [target] → [answer]
  4. Train with retrieved examples included in context
  5. For continual learning: add dialogue-level memory samples from previous domains

- Design tradeoffs:
  - Granular QA vs. structured generation: QA provides consistency but loses explicit schema information
  - Single example vs. multiple examples: Single example is more robust for small models; multiple examples may confuse the model
  - Oracle vs. learned retriever: Oracle provides upper bound; learned retriever is practical but introduces noise

- Failure signatures:
  - Performance degrades when retrievers return irrelevant examples consistently
  - Catastrophic forgetting occurs if memory sampling is insufficient or at wrong granularity
  - Model fails to leverage examples if trained with too many irrelevant examples

- First 3 experiments:
  1. Compare DST-EGQA with simple TransferQA format (no examples) to verify reformulation benefit
  2. Test different retrievers (BM25, SentenceBERT, GPT embeddings) with fixed DST-EGQA to find best retrieval approach
  3. Compare turn-level vs. dialogue-level memory sampling with fixed retriever and memory budget to verify sampling strategy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of the number of in-context examples that small models (like 60M parameters) can effectively leverage for DST-EGQA?
- Basis in paper: [explicit] The paper states that small models are unable to leverage more than one example when training to do in-context learning, but does not specify the exact upper limit.
- Why unresolved: The paper only tests up to three examples and finds diminishing returns, but does not explore the full range of possible in-context examples to determine the precise limit.
- What evidence would resolve it: Systematic experiments varying the number of in-context examples from 1 to a higher number (e.g., 5 or 10) and measuring performance would help identify the upper limit.

### Open Question 2
- Question: How does the quality of the retrieval method affect the performance of DST-EGQA, and what is the minimum quality required for the approach to be effective?
- Basis in paper: [explicit] The paper mentions that DST-EGQA is sensitive to the quality of the retrieved examples and the training dynamics, but does not quantify the minimum quality required.
- Why unresolved: The paper only compares a few retrieval methods and does not explore the full spectrum of retrieval quality or establish a clear threshold for effectiveness.
- What evidence would resolve it: Experiments varying the quality of the retrieval method (e.g., by adding noise or using different similarity metrics) and measuring the corresponding performance of DST-EGQA would help establish the minimum quality requirement.

### Open Question 3
- Question: How does the performance of DST-EGQA scale with larger models (e.g., 100M or 1B parameters) compared to the 60M parameter model used in the paper?
- Basis in paper: [explicit] The paper only uses a 60M parameter model and does not explore how performance scales with larger models.
- Why unresolved: The paper does not provide any data or analysis on the performance of DST-EGQA with larger models, leaving the question of scalability unanswered.
- What evidence would resolve it: Experiments using larger models (e.g., 100M or 1B parameters) with DST-EGQA and comparing their performance to the 60M parameter model would help understand the scalability of the approach.

## Limitations

- The approach relies on double-dipping the training set as both targets and in-context examples, potentially overestimating real-world generalization
- The SentenceBERT-based retriever may not generalize well to out-of-domain data with different dialogue state shift patterns
- Manual slot template creation could introduce subtle biases if templates are not truly domain-agnostic

## Confidence

- High confidence: The core mechanism of reformulating DST as granular QA (Mechanism 1) is well-supported by the evidence showing consistent task format across domains improves continual learning
- Medium confidence: The in-context learning approach with retrieved examples (Mechanism 2) shows promise but depends heavily on retriever quality, which can vary significantly based on implementation details not fully specified
- Medium confidence: The dialogue-level memory sampling advantage (Mechanism 3) is demonstrated but may be dataset-specific to SGD's characteristics

## Next Checks

1. Test the model's performance when the retriever is trained on a disjoint dataset from the target domains to evaluate true generalization beyond double-dipping
2. Conduct ablation studies varying the number of in-context examples (k) to determine the optimal balance between example utility and noise for the 60M parameter model
3. Evaluate the approach on a more challenging multi-domain dataset where state changes across domains are less predictable to stress-test the state change similarity retrieval mechanism