---
ver: rpa2
title: Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language
  Model
arxiv_id: '2305.15265'
source_url: https://arxiv.org/abs/2305.15265
tags:
- wta-crs
- memory
- lora
- column-row
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory bottleneck in fine-tuning large
  language models by focusing on reducing activation storage, which is the main contributor
  to memory usage during training. The authors propose Winner-Take-All Column-Row
  Sampling (WTA-CRS), a new unbiased estimator for matrix multiplication that reduces
  variance by focusing on high-probability regions of the sampling distribution.
---

# Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model

## Quick Facts
- arXiv ID: 2305.15265
- Source URL: https://arxiv.org/abs/2305.15265
- Reference count: 40
- Key outcome: Achieves up to 2.7× peak memory reduction with minimal accuracy loss during LLM fine-tuning by approximating GEMM operations in the backward pass

## Executive Summary
This paper addresses the memory bottleneck in fine-tuning large language models by focusing on reducing activation storage, which is the main contributor to memory usage during training. The authors propose Winner-Take-All Column-Row Sampling (WTA-CRS), a new unbiased estimator for matrix multiplication that reduces variance by focusing on high-probability regions of the sampling distribution. By replacing GEMM operations in transformers with WTA-CRS, they achieve significant memory savings while maintaining training accuracy and even improving throughput.

## Method Summary
The method replaces standard GEMM operations with Winner-Take-All Column-Row Sampling (WTA-CRS) in the backward pass of linear layers during LLM fine-tuning. WTA-CRS selects column-row pairs based on a probability distribution proportional to the product of norms, explicitly summing the highest-probability pairs while stochastically sampling the remainder. This approach maintains unbiasedness while reducing variance. The method is evaluated on T5 and BERT models using GLUE benchmark datasets, with experiments combining WTA-CRS with LoRA for additional memory savings.

## Key Results
- Achieves up to 2.7× peak memory reduction during fine-tuning
- Enables up to 6.4× larger batch sizes
- Provides 1.2× higher training throughput under same hardware constraints
- Maintains accuracy within 1% degradation for moderate compression ratios

## Why This Works (Mechanism)

### Mechanism 1
Sub-sampling activations via WTA-CRS reduces peak memory usage during fine-tuning while preserving gradient quality. By selecting k column-row pairs based on a probability distribution proportional to the product of norms, and explicitly summing the k-|C| highest-probability pairs while stochastically sampling the remainder, the estimator retains unbiasedness while reducing variance. The gradient estimator remains unbiased as long as the forward pass uses full activations and only the backward pass is approximated.

### Mechanism 2
Larger models have more redundant activations, making them more compressible via WTA-CRS. T5-3B shows smaller accuracy drop (0.4%) than T5-Large (1%) under the same compression ratio (k/|D| = 0.1), indicating that larger models can tolerate higher compression without performance loss. This redundancy scales with model size, allowing aggressive compression in larger models.

### Mechanism 3
Memory savings from WTA-CRS enable larger batch sizes, which in turn improve training throughput and downstream performance. Reduced activation storage frees memory, allowing larger batch sizes; larger batch sizes improve GPU utilization and training speed, creating a positive feedback loop for faster training.

## Foundational Learning

- **Concept**: Unbiased estimators in stochastic optimization
  - Why needed: Ensures that the approximated gradient remains a valid direction for SGD despite compression
  - Quick check: If E[gradient estimator] ≠ true gradient, will SGD still converge? (No—bias can cause divergence)

- **Concept**: Variance reduction in Monte Carlo sampling
  - Why needed: Lower variance in the gradient estimator means more stable training and less noise in updates
  - Quick check: Does averaging more samples always reduce variance? (Yes, if samples are i.i.d. and unbiased)

- **Concept**: Memory hierarchy in deep learning training
  - Why needed: Activations dominate memory usage; understanding this explains why activation compression is more impactful than parameter compression
  - Quick check: If activations take 80% of memory and parameters take 20%, which compression yields larger gains? (Activations)

## Architecture Onboarding

- **Component map**: Forward pass GEMM -> Store activations -> Backward pass WTA-CRS sampling -> Approximate gradient computation -> Weight update
- **Critical path**: 1) Forward pass: Full GEMM (unchanged) → Store activations; 2) Backward pass: Sample column-row pairs → Approximate gradient computation; 3) Weight update: Use approximated gradient → Apply optimizer step
- **Design tradeoffs**: Memory vs. accuracy (higher compression reduces memory but may increase variance); Speed vs. precision (WTA-CRS adds sampling overhead but enables larger batch sizes); Determinism vs. stochasticity (deterministic selection is faster but biased; WTA-CRS balances both)
- **Failure signatures**: Training divergence (may indicate bias in gradient estimator or too aggressive compression); Memory overflow (may indicate insufficient compression ratio or model too large for available GPU); Slow training (may indicate sampling overhead outweighs batch size gains)
- **First 3 experiments**: 1) Replace a single linear layer's backward pass with WTA-CRS and compare gradient norms to full GEMM; 2) Vary k from 0.1|D| to 0.5|D| and measure accuracy drop and memory savings on T5-Base; 3) Combine WTA-CRS with LoRA and measure peak memory vs. baseline LoRA on T5-3B

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal value of |C| (the size of the highest probability group) for WTA-CRS in different transformer architectures and fine-tuning tasks? The paper only provides experimental evidence for one specific model and dataset. Different transformer architectures (BERT, GPT, etc.) and fine-tuning tasks may have different optimal values of |C|.

### Open Question 2
How does the performance of WTA-CRS compare to other approximation methods when combined with advanced optimization techniques like learning rate scheduling and weight decay? The paper shows WTA-CRS performs well with standard AdamW optimizer but doesn't explore its interaction with advanced optimization techniques.

### Open Question 3
What is the theoretical limit of memory reduction achievable with WTA-CRS as model size increases? While the paper observes that larger models are more compressible, it doesn't provide a theoretical framework for understanding the limits of this compressibility.

## Limitations
- The claim that larger models are inherently more compressible relies on a single internal comparison without theoretical justification
- The optimal value of |C| for variance reduction is only tested on one model and dataset
- No ablation studies are provided to validate the specific variance reduction mechanism across different architectures

## Confidence

**High Confidence Claims:**
- Memory reduction through activation compression is achievable (directly measured)
- Peak memory reduction of 2.7× is reproducible with the described implementation
- Larger batch sizes (up to 6.4×) are feasible under the same hardware constraints

**Medium Confidence Claims:**
- WTA-CRS provides unbiased gradient estimates (theoretical claim with referenced proof)
- Accuracy maintenance within 1% for moderate compression ratios (based on GLUE experiments)
- 1.2× higher training throughput is achievable (measured but dependent on specific hardware setup)

**Low Confidence Claims:**
- Larger models are inherently more compressible due to redundant activations (based on single comparison without theoretical basis)
- The specific variance reduction mechanism generalizes across different model architectures (no ablation studies provided)

## Next Checks

1. **Gradient Norm Comparison Test**: Replace a single linear layer's backward pass with WTA-CRS and compare the gradient norms against the full GEMM baseline across multiple training steps to verify unbiasedness in practice.

2. **Variance Sensitivity Analysis**: Systematically vary k from 0.05|D| to 0.5|D| and measure both accuracy degradation and gradient variance on T5-Base to identify the compression threshold where variance becomes problematic.

3. **Architecture Transfer Test**: Apply WTA-CRS to a different transformer architecture (e.g., GPT-2 or DeBERTa) and measure whether the claimed benefits (memory reduction, accuracy maintenance) transfer or if the performance is architecture-specific.