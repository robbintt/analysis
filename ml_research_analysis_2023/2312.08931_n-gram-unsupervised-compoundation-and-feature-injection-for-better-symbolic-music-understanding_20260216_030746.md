---
ver: rpa2
title: N-Gram Unsupervised Compoundation and Feature Injection for Better Symbolic
  Music Understanding
arxiv_id: '2312.08931'
source_url: https://arxiv.org/abs/2312.08931
tags:
- music
- n-gram
- sequence
- sequences
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of symbolic music understanding
  by proposing NG-Midiformer, a novel method that leverages N-gram techniques from
  NLP. The core idea involves transforming music pieces into word-like sequences using
  an unsupervised compoundation approach, followed by an N-gram Transformer encoder
  that incorporates N-gram information to enhance the primary encoder for better music
  sequence understanding.
---

# N-Gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding

## Quick Facts
- arXiv ID: 2312.08931
- Source URL: https://arxiv.org/abs/2312.08931
- Reference count: 15
- Key outcome: Achieves SOTA performance on music understanding tasks with accuracy boosts of +8.22% to +15.08%

## Executive Summary
This paper addresses symbolic music understanding by proposing NG-Midiformer, which applies N-gram techniques from NLP to music sequences. The method transforms music into word-like sequences using unsupervised compoundation (UCW tokenization), then enhances understanding through an N-gram Transformer encoder that injects N-gram information into the primary encoder. Pre-trained on large-scale music datasets, the model achieves state-of-the-art results on six downstream tasks including composer, emotion, genre, and dance classification, velocity prediction, and melody classification.

## Method Summary
The method involves three key stages: (1) transforming MIDI files into UCW sequences using unsupervised compoundation with BPE tokenization, (2) pre-training NG-Midiformer using Masked Language Modeling on large-scale unlabeled music data, and (3) fine-tuning on six downstream music understanding tasks. The core innovation is the N-gram Transformer encoder that extracts N-gram patterns from UCW sequences and injects this information into the primary encoder through position matrix addition, allowing the model to capture repetitive musical patterns at various scales.

## Key Results
- Achieves SOTA performance on six music understanding tasks
- Accuracy improvements: +8.22% (composer classification), +5.67% (emotion), +15.08% (genre), +6.17% (dance), +0.4% (melody)
- Demonstrates effectiveness of N-gram feature injection for symbolic music understanding
- Shows UCW tokenization reduces sequence length while preserving semantic correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised compoundation improves tokenization by grouping frequent adjacent music events into single tokens, reducing sequence length while preserving semantic correlations.
- Mechanism: Uses BPE on grouped "family" tokens (pitch, duration, velocity) to create variable-length compound words reflecting music's co-occurrence patterns.
- Core assumption: Music events from the same family frequently co-occur in semantically meaningful ways, and grouping them captures more context than individual tokens.
- Evidence anchors: Abstract mentions processing music into word-like sequences with unsupervised compoundation; section discusses frequency-driven vocabulary and co-occurrence patterns.
- Break condition: If music events don't exhibit strong co-occurrence patterns within families, or if BPE creates tokens that are too long to capture meaningful semantic units.

### Mechanism 2
- Claim: N-gram Transformer encoder enhances symbolic music understanding by explicitly incorporating N-gram information through position matrix injection.
- Mechanism: Extracts N-grams from UCW sequences, constructs vocabulary, creates position matrices recording N-gram occurrences and frequencies, then injects N-gram representations into primary encoder layers through weighted addition.
- Core assumption: Music contains repetitive patterns at various scales that N-grams can capture, and these patterns are relevant for downstream tasks.
- Evidence anchors: Abstract mentions N-gram information effectively incorporated to enhance primary encoder; section describes primary encoder and N-gram Transformer encoder.
- Break condition: If N-gram patterns don't generalize across different pieces, or if position matrix injection introduces noise that overwhelms primary encoder signal.

### Mechanism 3
- Claim: Pre-training on large-scale unlabeled symbolic music data enables the model to learn N-gram patterns that transfer to downstream tasks.
- Mechanism: Trains NG-Midiformer using MLM on 24,979 UCW sequences, learning to predict masked tokens while simultaneously learning N-gram co-occurrence statistics.
- Core assumption: N-gram patterns learned during pre-training are useful for downstream music understanding tasks like classification and prediction.
- Evidence anchors: Abstract mentions pre-training enables thorough learning of N-gram information contained within music sequences; section describes pre-training on large-scale unlabeled symbolic music dataset.
- Break condition: If pre-training data is too small or too homogeneous, or if downstream tasks require patterns not present in pre-training data.

## Foundational Learning

- Concept: Music tokenization methods (REMI, CP, UCW)
  - Why needed here: Understanding the evolution from raw MIDI to different tokenization approaches is crucial for grasping why UCW was developed and how it differs from existing methods.
  - Quick check question: What are the key differences between REMI, CP, and UCW tokenization in terms of sequence length and semantic preservation?

- Concept: N-gram language models and their application to music
  - Why needed here: N-grams capture local dependencies and patterns, which are essential for understanding how NG-Midiformer leverages this NLP technique for music understanding.
  - Quick check question: How do N-grams help capture repetitive melodic patterns in music, and why is this particularly useful for symbolic music understanding?

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: NG-Midiformer builds upon Transformer, so understanding how self-attention works and its limitations with long sequences is crucial for appreciating the design choices.
  - Quick check question: Why does the standard Transformer struggle with long sequences like those produced by REMI, and how does UCW address this issue?

## Architecture Onboarding

- Component map: MIDI → UCW Tokenizer → Primary Encoder → N-gram Extraction → N-gram Encoder → Feature Injection → Downstream Tasks

- Critical path: The sequence flows from raw MIDI through UCW tokenization, primary transformer encoding, N-gram extraction and encoding, feature injection, and finally to downstream task heads.

- Design tradeoffs:
  - UCW vs CP: UCW uses variable-length tokens and unified embeddings vs CP's fixed-length with multiple embedding layers; tradeoff is between semantic richness and computational efficiency
  - N-gram Encoder depth: Fewer layers in N-gram encoder vs primary encoder; tradeoff is between capturing N-gram patterns and computational cost
  - Vocabulary size: 1000 for UCW vs larger for CP; tradeoff is between token granularity and sequence length

- Failure signatures:
  - Poor performance on melody classification: May indicate loss of crucial melodic information due to UCW tokenization or N-gram processing
  - Degraded performance compared to MIDI-Bert without N-gram pre-training: Suggests N-gram patterns don't transfer well or pre-training is essential
  - High variance across different tasks: May indicate that N-gram patterns are task-specific rather than generalizable

- First 3 experiments:
  1. Ablation study: Remove N-gram encoder and feature injection, compare performance to full NG-Midiformer on composer classification
  2. Tokenization comparison: Convert same dataset using REMI, CP-4, UCW-4, and UCW-7, measure sequence lengths and downstream task performance
  3. N-gram vocabulary size: Vary N-gram vocabulary size (e.g., 100, 200, 500, 1000) and measure impact on pre-training loss and downstream task accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several unresolved issues emerge regarding the method's broader applicability and limitations.

## Limitations
- UCW tokenization may lose crucial melodic information, preserving only ~60% of information compared to CP tokens
- N-gram patterns learned during pre-training may not generalize across diverse downstream tasks
- Pre-training dataset size (24,979 sequences) may be insufficient for capturing full musical diversity

## Confidence
- High Confidence: The core architectural innovations (UCW tokenization and N-gram injection) are clearly described and represent novel contributions
- Medium Confidence: The performance improvements reported on downstream tasks are likely valid, though magnitudes may be influenced by dataset characteristics
- Low Confidence: Claims about why specific improvements occur lack direct supporting evidence through ablation studies

## Next Checks
1. **Ablation Study on N-gram Contribution**: Train a baseline Midiformer without N-gram injection on the same pre-training data, then compare performance across all downstream tasks to isolate the specific contribution of N-gram features versus general pre-training benefits.

2. **Information Preservation Analysis**: Quantitatively compare the information content preserved by UCW versus CP tokenization by measuring melody reconstruction accuracy or motif preservation rates, addressing the concern that UCW loses significant melodic information.

3. **Cross-dataset Generalization Test**: Evaluate the pre-trained NG-Midiformer on an entirely new symbolic music dataset not seen during pre-training or fine-tuning to assess whether the N-gram patterns learned are truly generalizable or overfit to the specific training corpora.