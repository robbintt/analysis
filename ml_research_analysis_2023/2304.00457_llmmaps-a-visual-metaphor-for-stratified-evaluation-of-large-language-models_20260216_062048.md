---
ver: rpa2
title: LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models
arxiv_id: '2304.00457'
source_url: https://arxiv.org/abs/2304.00457
tags:
- knowledge
- research
- llmmaps
- which
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMMaps introduces a novel visualization approach to evaluate and
  compare the performance of large language models (LLMs) across different knowledge
  subfields. By transforming question-answer datasets and LLM responses into a hierarchical
  knowledge structure, LLMMaps provide detailed insights into model performance, highlighting
  strengths and weaknesses in specific subfields.
---

# LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2304.00457
- Source URL: https://arxiv.org/abs/2304.00457
- Reference count: 40
- Primary result: LLMMaps enable stratified, subfield-level evaluation of LLMs, revealing performance patterns and hallucination risks not visible in aggregate metrics.

## Executive Summary
LLMMaps introduce a novel visualization approach to evaluate and compare large language models (LLMs) across different knowledge subfields. By transforming question-answer datasets and LLM responses into a hierarchical knowledge structure, LLMMaps provide detailed insights into model performance, highlighting strengths and weaknesses in specific subfields. The approach uses stratified evaluation to identify subfields where hallucinations are more likely, aiding in risk assessment and model improvement. A simple hallucination score based on self-assessed question difficulty is introduced to further highlight potential risks. LLMMaps are evaluated through comparative analysis of state-of-the-art models (e.g., GPT-3, ChatGPT, BLOOM) on multiple datasets and qualitative user studies. Results demonstrate the utility of LLMMaps in identifying model limitations and supporting transparent, stratified evaluation, with overall accuracy and hallucination scores visualized per subfield.

## Method Summary
LLMMaps transforms question-and-answer datasets into a hierarchical knowledge structure using LLM-based stratification. The method involves obtaining answers from target LLMs, computing accuracy and a hallucination score based on self-assessed difficulty ratings, and visualizing results using bar charts with color-coded models and subfields. The hallucination score quantifies monotonicity between difficulty and accuracy, with higher values indicating more hallucinations. Blue noise dot plots encode question density per subfield, and the visualization supports both single-model and multi-model comparisons with configurable layout parameters.

## Key Results
- LLMMaps successfully identify subfield-specific strengths and weaknesses in state-of-the-art LLMs across biomedical, scientific, medical, and legal domains.
- The hallucination score effectively highlights subfields where models are overconfident on easy questions but underperform on harder ones, indicating potential hallucination.
- User studies confirm that LLMMaps provide more interpretable and actionable insights compared to traditional aggregate accuracy metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMMaps expose LLM hallucinations by stratifying performance per subfield and showing self-assessed difficulty vs accuracy.
- Mechanism: The visual mapping links knowledge hierarchy leaf nodes to accuracy bars, enabling users to detect inconsistencies between perceived difficulty and actual performance. A monotonicity-based hallucination score (sh) is computed from the ratio of average accuracy over self-assessed difficulty levels.
- Core assumption: If an LLM answers easy questions incorrectly, it indicates overconfidence or hallucination.
- Evidence anchors:
  - [abstract] "LLMMaps provide detailed insights into model performance, highlighting strengths and weaknesses in specific subfields... stratified evaluation to identify subfields where hallucinations are more likely."
  - [section] "we consider the ratio function fr of average accuracy over self-assessed difficulty level... normalize sh to lie in the range [0,1], where 0 stands for no hallucination, while 1 stands for maximal hallucination."
  - [corpus] Weak or missing; no directly comparable hallucination quantification in neighbor papers.
- Break condition: If self-assessed difficulty ratings are unreliable (e.g., systematically biased), the hallucination score loses validity.

### Mechanism 2
- Claim: LLMMaps make it easy to compare multiple LLMs across knowledge subfields without aggregating accuracy into a single number.
- Mechanism: The layout splits the knowledge tree into left/right halves, stacking leaf nodes up to a configurable depth hln, and encodes model performance as horizontally segmented bars. Hue encodes model identity; brightness encodes subfield.
- Core assumption: Users can visually parse and compare multiple bar segments per subfield without confusion.
- Evidence anchors:
  - [section] "we have decided to horizontally split the performance bar into k bars, where k is the number of visualized LLMs... we have decided to use hue to visually encode model in our multi-model scenario."
  - [section] "LLMMaps provide detailed insights into LLMsâ€™ knowledge capabilities in different subfields, by transforming Q&A datasets as well as LLM responses into an internal knowledge structure."
  - [corpus] Weak; neighbor papers focus on trust/comparison but not via stratified subfield visualization.
- Break condition: If the number of models or subfields becomes too large, the visualization becomes cluttered and comparisons degrade.

### Mechanism 3
- Claim: LLMMaps enable detection of knowledge gaps and biases in Q&A datasets themselves.
- Mechanism: Empty leaf nodes are visualized (or omitted) to reveal which subfields are underrepresented in the dataset. Blue noise dot plots encode the number of questions per subfield, highlighting data sparsity.
- Core assumption: Sparse representation in a subfield suggests incomplete evaluation coverage.
- Evidence anchors:
  - [section] "we have further decided to horizontally stack leaf nodes in such a way, that we are able to exploit the full page width... omit empty leaf nodes per default from our visualization."
  - [section] "we decided to choose commonly used density visualizations to display it... we have decided to realize our dot plots as an extension to Blue Noise Plots."
  - [corpus] Weak; neighbor papers do not discuss dataset bias visualization in this manner.
- Break condition: If the knowledge hierarchy generation is biased or incomplete, empty leaf nodes may misrepresent actual dataset coverage.

## Foundational Learning

- Concept: Bloom's Taxonomy and its six cognitive levels.
  - Why needed here: To classify Q&A questions by cognitive demand, enabling stratified evaluation per learning dimension.
  - Quick check question: List Bloom's six levels from lowest to highest cognitive complexity.

- Concept: Knowledge hierarchy construction via textbook outline prompting.
  - Why needed here: LLMMaps require a structured subfield tree; generating it from LLM prompts is central to the approach.
  - Quick check question: What are the three recursive steps in deriving the knowledge hierarchy?

- Concept: Blue noise point distributions.
  - Why needed here: Used to encode the number of questions per subfield without clustering artifacts.
  - Quick check question: Why is blue noise preferred over uniform random sampling for density visualization?

## Architecture Onboarding

- Component map:
  Q&A Dataset Importer -> Internal Data Structure -> Knowledge Hierarchy Generator (LLM-based) -> LLM Answer Collector -> LLMMaps Renderer (SVG)

- Critical path:
  1. Parse raw dataset -> 2. Map to internal schema -> 3. Generate hierarchy (prompt LLM) -> 4. Collect answers (prompt LLM) -> 5. Compute accuracy/difficulty/Bloom -> 6. Render SVG

- Design tradeoffs:
  - Hierarchical splitting vs single tree: halves reduce height but split visual context.
  - Bar + number vs number only: dual encoding aids precision but adds visual complexity.
  - Blue noise vs scatter: avoids perceptual bias but requires more computation.

- Failure signatures:
  - Incorrect hierarchy: model accuracy bars misalign with intended subfields.
  - Low hln: excessive vertical stacking, poor readability.
  - Missing context in prompts: LLM produces irrelevant or hallucinated answers.

- First 3 experiments:
  1. Render a single LLMMap for one model on one dataset with default parameters; verify bar accuracy and leaf counts.
  2. Toggle empty leaf nodes on/off; check visual difference and correctness.
  3. Render a multi-model LLMMaps; ensure hue encoding is consistent and distinguishable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge hierarchy generation be further improved to reduce potential hallucination artifacts while maintaining automation?
- Basis in paper: [explicit] The paper mentions that LLM-based knowledge hierarchy generation could be affected by hallucinations and needs careful checking by domain experts, but this is considered acceptable given the time savings compared to manual curation.
- Why unresolved: The paper acknowledges the risk of hallucination in generated hierarchies but doesn't provide a detailed solution or evaluation of mitigation strategies.
- What evidence would resolve it: Comparative analysis of knowledge hierarchies generated using different prompting strategies, human expert review scores, and evaluation of hallucination frequency in generated hierarchies.

### Open Question 2
- Question: What is the optimal knowledge hierarchy depth and breadth for different types of Q&A datasets beyond the 3-level depth with 5-10 top-level subfields used in this study?
- Basis in paper: [explicit] The paper mentions that a depth of 3 with 5-10 top-level subfields worked well for their handled datasets, but this was determined empirically without systematic optimization.
- Why unresolved: The paper doesn't provide systematic analysis of how different hierarchy structures affect the utility of LLMMaps or the interpretability of results.
- What evidence would resolve it: Controlled experiments varying hierarchy depth and breadth across multiple datasets, user studies comparing interpretability, and analysis of how hierarchy structure affects identification of model weaknesses.

### Open Question 3
- Question: How can LLMMaps be extended to evaluate other types of language model outputs beyond question answering, such as text generation or summarization?
- Basis in paper: [inferred] The paper focuses exclusively on Q&A datasets but mentions that other benchmarks exist for language understanding tasks, suggesting potential for broader application.
- Why unresolved: The paper's methodology is specifically designed for Q&A evaluation and doesn't address how it could be adapted for other NLP tasks.
- What evidence would resolve it: Implementation of LLMMaps for different NLP tasks, evaluation of how knowledge stratification strategies need to be modified, and user studies comparing effectiveness across task types.

## Limitations

- The hallucination detection mechanism depends on the quality and consistency of self-assessed difficulty ratings, which may introduce bias if annotators misjudge question complexity.
- The LLM-based knowledge hierarchy generation process lacks sufficient detail for guaranteed reproducibility across different LLM implementations and prompt designs.
- Visualization clarity and performance comparison degrade when applied to very large or highly complex knowledge domains with many subfields and models.

## Confidence

- **High Confidence**: The core visualization mechanics (SVG rendering, hierarchical layout, bar encoding) and accuracy calculation are well-defined and reproducible.
- **Medium Confidence**: The hallucination score methodology is theoretically sound but depends on the quality of self-assessed difficulty ratings and prompt design.
- **Low Confidence**: The LLM-based knowledge hierarchy generation process lacks sufficient detail for guaranteed reproducibility across different LLM implementations.

## Next Checks

1. **Cross-annotator validation**: Have multiple independent annotators rate question difficulty and measure inter-rater reliability to assess the stability of the hallucination score.
2. **Hierarchy robustness test**: Generate knowledge hierarchies using different LLM prompts and implementations to evaluate consistency and sensitivity to prompt design.
3. **Scalability assessment**: Apply LLMMaps to a larger, more complex Q&A dataset with hundreds of subfields to test visualization clarity and performance bar distinguishability.