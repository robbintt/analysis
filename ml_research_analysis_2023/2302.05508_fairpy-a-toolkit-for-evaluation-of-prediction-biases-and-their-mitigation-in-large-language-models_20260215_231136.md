---
ver: rpa2
title: 'FairPy: A Toolkit for Evaluation of Prediction Biases and their Mitigation
  in Large Language Models'
arxiv_id: '2302.05508'
source_url: https://arxiv.org/abs/2302.05508
tags:
- language
- bias
- biases
- arxiv
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fairpy, an open-source toolkit designed to
  evaluate and mitigate social biases in large language models (LLMs) such as BERT
  and GPT-2. The toolkit provides interfaces to apply existing bias detection metrics
  (WEAT, StereoSet, Hellinger distance, etc.) and mitigation techniques (dropout regularization,
  null space projection, counterfactual data augmentation) to both pretrained and
  custom models.
---

# FairPy: A Toolkit for Evaluation of Prediction Biases and their Mitigation in Large Language Models

## Quick Facts
- arXiv ID: 2302.05508
- Source URL: https://arxiv.org/abs/2302.05508
- Authors: [Not specified in source]
- Reference count: 12
- One-line primary result: Open-source toolkit for evaluating and mitigating social biases in large language models through modular interfaces to existing metrics and techniques

## Executive Summary
This paper introduces Fairpy, an open-source toolkit designed to evaluate and mitigate social biases in large language models (LLMs) such as BERT and GPT-2. The toolkit provides interfaces to apply existing bias detection metrics (WEAT, StereoSet, Hellinger distance, etc.) and mitigation techniques (dropout regularization, null space projection, counterfactual data augmentation) to both pretrained and custom models. The authors decoupled these techniques from specific model architectures and social contexts, making them more generalizable. Empirical analysis on HuggingFace models shows that bias detection metrics are sensitive to internal embedding structures, and that retraining with augmented datasets (e.g., Yelp) can reduce gender bias as measured by WEAT scores, though some unintended increases in stereotype prediction were observed.

## Method Summary
The paper presents a Python-based open-source toolkit that provides interfaces to existing bias detection metrics and mitigation techniques for large language models. The toolkit supports pretrained models from HuggingFace, custom models, and various bias evaluation corpora. Key components include bias detection metrics (WEAT, StereoSet, Hellinger distance), mitigation techniques (dropout regularization, null space projection, counterfactual data augmentation), and interfaces for connecting these tools to different model architectures. The empirical analysis involves applying bias detection metrics to baseline models, applying mitigation techniques, retraining with augmented datasets, and re-evaluating bias levels to measure effectiveness.

## Key Results
- Fairpy successfully decouples bias detection metrics from specific model architectures, enabling application across BERT, GPT-2, and custom models
- Counterfactual data augmentation with Yelp dataset reduced gender bias as measured by WEAT scores, though unintended stereotype prediction increases were observed
- Bias detection metrics show sensitivity to internal embedding structures, with different metrics revealing different aspects of model bias
- The toolkit achieves generalization by providing modular interfaces while maintaining computational efficiency for practical use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The toolkit enables generalized bias detection by decoupling metrics from specific model architectures and social contexts.
- Mechanism: Fairpy provides modular interfaces that allow bias detection metrics (WEAT, StereoSet, Hellinger distance) to be applied across different LLM architectures like BERT, GPT-2, etc., without being tied to their internal structures or specific template requirements.
- Core assumption: Bias detection metrics can be made architecture-agnostic through proper interface design.
- Evidence anchors:
  - [abstract] "we decoupled these techniques to make them general purpose and applicable to a wider range of language models"
  - [section] "we present a Python based open-source package Fairpy, that consists of a large set of available bias detection and reduction tools along with interfaces to connect them to many of the open source language models"
  - [corpus] Weak evidence - no direct corpus citations about the decoupling approach, but the toolkit's design supports this claim
- Break condition: If the interface abstractions fail to handle the unique architectural features of different models (e.g., bidirectional vs. unidirectional attention), the generalization will break down.

### Mechanism 2
- Claim: Counterfactual data augmentation reduces bias by retraining models on balanced datasets.
- Mechanism: The toolkit implements CDA by replacing words representing one social class with another in the training corpus, creating a more balanced dataset that mitigates learned biases during retraining.
- Core assumption: Bias in LLMs is primarily learned from imbalanced training data distributions.
- Evidence anchors:
  - [abstract] "retraining with augmented datasets (e.g., Yelp) can reduce gender bias as measured by WEAT scores"
  - [section] "Counter Factual Data Augmentation or CDA, is a method that aims to change what the Language Model learns by retraining it on a debiased dataset"
  - [corpus] Weak evidence - the corpus analysis doesn't provide direct citations about CDA effectiveness, but the empirical results show WEAT score reduction
- Break condition: If the augmented dataset is not sufficiently large or comprehensive, the debiasing effect may be limited or introduce new biases.

### Mechanism 3
- Claim: Null Space Projection removes bias by eliminating linear dependencies between embeddings and protected attributes.
- Mechanism: The toolkit uses Iterative Nullspace Projection to project biased contextual embeddings onto the null space of a classifier trained to predict protected attributes, effectively removing the influence of those attributes.
- Core assumption: Bias can be mathematically characterized as linear dependency between embeddings and protected attributes.
- Evidence anchors:
  - [abstract] "mitigation techniques (dropout regularization, null space projection, counterfactual data augmentation)"
  - [section] "Iterative Nullspace projection is a technique used to debias the embedding... remove linear dependency between the two using a linear guarding function"
  - [corpus] Weak evidence - no direct corpus citations about null space projection, but the mathematical framework is well-established in the literature
- Break condition: If the bias is not linearly separable from the embeddings, or if important non-biased information is also removed during projection.

## Foundational Learning

- Concept: Hellinger distance as a measure of distribution similarity
  - Why needed here: Used to quantify bias by measuring differences between probability distributions of model outputs for different social groups
  - Quick check question: How does Hellinger distance differ from KL divergence when comparing probability distributions?

- Concept: WEAT (Word Embedding Association Test) and its extension SEAT
  - Why needed here: Provides a standardized metric for measuring bias by comparing semantic associations between word embeddings and social attribute classes
  - Quick check question: What are the limitations of WEAT when measuring bias in contextual embeddings versus static embeddings?

- Concept: Counterfactual augmentation and dataset balancing
  - Why needed here: Fundamental technique for mitigating bias by retraining models on more representative data distributions
  - Quick check question: How does the effectiveness of counterfactual augmentation scale with dataset size and diversity?

## Architecture Onboarding

- Component map:
  - bias_detection package containing metrics (WEAT, StereoSet, Hellinger distance, Honest score, Log Likelihood)
  - bias_mitigation package containing techniques (Dropout regularization, Null Space Projection, CDA, Self Debias)
  - interfaces layer for plugging custom models and tokenizers
  - dataset handlers for standard bias evaluation corpora
  - configuration management for metric parameters and social constructs

- Critical path: Model evaluation workflow - load model → select bias metric → prepare corpus → run evaluation → analyze results → apply mitigation if needed → re-evaluate

- Design tradeoffs:
  - Generalization vs. performance: Abstract interfaces make the toolkit widely applicable but may add computational overhead
  - Metric comprehensiveness vs. runtime: Supporting many metrics increases evaluation thoroughness but extends processing time
  - Debiasing strength vs. model capability: Aggressive debiasing may reduce harmful bias but also diminish model performance on legitimate tasks

- Failure signatures:
  - Metrics returning NaN or infinite values due to tokenization issues
  - Mitigation techniques causing catastrophic forgetting of learned capabilities
  - Inconsistent results across different model architectures for the same metric
  - Runtime errors when handling models with non-standard embedding structures

- First 3 experiments:
  1. Run WEAT score evaluation on BERT-base with the default CrowS-Pairs dataset to establish baseline bias levels
  2. Apply Counterfactual Data Augmentation using the Yelp dataset to debias GPT-2, then re-run WEAT to measure improvement
  3. Test Hellinger distance metric on a custom masked language model to verify the interface generalization works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do bias detection metrics that rely on internal embedding structures compare in effectiveness to those that evaluate model behavior on perturbed inputs?
- Basis in paper: [explicit] The paper distinguishes between metrics that calculate biases in encoding (e.g., WEAT, Hellinger distance) and those that evaluate model behavior given perturbed input sentences (e.g., StereoSet, Log Probability).
- Why unresolved: The paper presents empirical analysis but does not directly compare the relative effectiveness or sensitivity of these two categories of metrics.
- What evidence would resolve it: Systematic experiments comparing detection rates, false positives/negatives, and sensitivity to different types of biases across both metric categories.

### Open Question 2
- Question: Can bias mitigation techniques be effectively cascaded or combined to achieve better results than individual methods?
- Basis in paper: [inferred] The paper mentions that the toolkit currently does not support cascading metrics or hybrid methods, and notes this as a limitation.
- Why unresolved: The paper only evaluates individual mitigation techniques (dropout regularization, null space projection, etc.) separately without exploring their combined effects.
- What evidence would resolve it: Experiments applying multiple mitigation techniques sequentially or in parallel and measuring their cumulative impact on bias reduction across various metrics.

### Open Question 3
- Question: How do bias detection and mitigation techniques generalize across different social contexts beyond binary gender classifications?
- Basis in paper: [explicit] The paper discusses extending WEAT metrics to multi-class scenarios and mentions efforts to include non-binary individuals, but notes these methods have not been fully tested.
- Why unresolved: The paper acknowledges attempts to generalize metrics but does not provide comprehensive evaluation results for non-binary or intersectional social contexts.
- What evidence would resolve it: Systematic testing of bias metrics and mitigation techniques across diverse social categories (race, ethnicity, age, sexual orientation) with intersectional analysis.

## Limitations

- The toolkit's effectiveness is constrained by the availability and quality of bias evaluation datasets, with most metrics relying on English-language corpora
- The decoupling approach, while promoting generalization, may sacrifice performance optimization for specific model architectures
- The empirical analysis is limited to a small set of pretrained models and metrics, potentially missing broader applicability patterns

## Confidence

- **High Confidence**: The toolkit's modular architecture and basic functionality for bias detection and mitigation
- **Medium Confidence**: The effectiveness of specific mitigation techniques (CDA, null space projection) across different model types
- **Low Confidence**: Claims about unintended bias increases and the generalizability of results to non-English languages and non-Western social contexts

## Next Checks

1. **Cross-linguistic Validation**: Test the toolkit's metrics and mitigation techniques on multilingual models and non-English bias datasets to assess cultural generalizability
2. **Architecture-specific Benchmarking**: Evaluate performance overhead and accuracy trade-offs when applying the same bias detection pipeline across diverse model architectures (transformer variants, RNNs, etc.)
3. **Long-term Stability Analysis**: Measure the persistence of bias mitigation effects across different fine-tuning scenarios and over extended usage periods to identify potential bias resurgence patterns