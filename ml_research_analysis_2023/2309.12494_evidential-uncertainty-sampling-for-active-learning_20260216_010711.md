---
ver: rpa2
title: Evidential uncertainty sampling for active learning
arxiv_id: '2309.12494'
source_url: https://arxiv.org/abs/2309.12494
tags:
- uncertainty
- epistemic
- evidential
- labels
- uncertainties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two new uncertainty sampling strategies for
  active learning that leverage evidential reasoning to account for both epistemic
  and aleatoric uncertainties, while also incorporating the inherent uncertainty present
  in labels (e.g., from human annotators). The first strategy uses Klir uncertainty,
  combining discord (measure of conflicting information) and non-specificity (measure
  of ignorance) in the model output, with a parameter to balance exploration and exploitation.
---

# Evidential uncertainty sampling for active learning

## Quick Facts
- arXiv ID: 2309.12494
- Source URL: https://arxiv.org/abs/2309.12494
- Reference count: 27
- Primary result: Two new uncertainty sampling strategies for active learning that leverage evidential reasoning to account for both epistemic and aleatoric uncertainties, while also incorporating the inherent uncertainty present in labels (e.g., from human annotators).

## Executive Summary
This paper proposes two novel uncertainty sampling strategies for active learning that leverage evidential reasoning to account for both epistemic and aleatoric uncertainties, while also incorporating the inherent uncertainty present in labels (e.g., from human annotators). The first strategy uses Klir uncertainty, combining discord (measure of conflicting information) and non-specificity (measure of ignorance) in the model output, with a parameter to balance exploration and exploitation. The second strategy extends epistemic uncertainty to the evidential framework, defining evidential epistemic and aleatoric uncertainties that are naturally applicable to multiple classes. Both methods are computationally simpler than existing approaches and experimental results on real-world datasets demonstrate their ability to identify relevant areas of uncertainty for active learning, outperforming classical uncertainty sampling methods.

## Method Summary
The paper proposes two new uncertainty sampling strategies for active learning that leverage evidential reasoning to account for both epistemic and aleatoric uncertainties, while also incorporating the inherent uncertainty present in labels (e.g., from human annotators). The first strategy uses Klir uncertainty, combining discord (measure of conflicting information) and non-specificity (measure of ignorance) in the model output, with a parameter to balance exploration and exploitation. The second strategy extends epistemic uncertainty to the evidential framework, defining evidential epistemic and aleatoric uncertainties that are naturally applicable to multiple classes. Both methods are computationally simpler than existing approaches and experimental results on real-world datasets demonstrate their ability to identify relevant areas of uncertainty for active learning, outperforming classical uncertainty sampling methods.

## Key Results
- Two new uncertainty sampling strategies for active learning that leverage evidential reasoning
- Proposed methods are computationally simpler than existing approaches
- Experimental results on real-world datasets demonstrate ability to outperform classical uncertainty sampling methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Epistemic uncertainty in evidential frameworks can be decomposed into reducible and irreducible components without dependence on observations.
- **Mechanism**: The paper proposes using belief functions to model uncertainty, where epistemic uncertainty is defined as the minimum of plausibility and one minus belief for each class, summed across classes. This avoids direct dependence on observations and simplifies computation.
- **Core assumption**: The evidential framework accurately captures the epistemic uncertainty in a way that is comparable to traditional methods but without observation dependence.
- **Evidence anchors**:
  - [abstract]: "This paper proposes two new uncertainty sampling strategies for active learning that leverage evidential reasoning to account for both epistemic and aleatoric uncertainties..."
  - [section]: "The epistemic uncertainty can be extended to rich labels by using the notion of plausibility within the framework of belief functions... The evidential extension of the epistemic and aleatoric uncertainties for |Ω| ≥ 2 classes is then: Ue(x) = Σ ω∈Ω min[P l(ω|x), 1 − Bel(ω|x)]..."
  - [corpus]: "Weak. No direct mention of evidential frameworks in corpus. Assumed from general knowledge."
- **Break condition**: If the belief functions do not accurately represent the epistemic uncertainty, the decomposition will fail to provide meaningful insights for active learning.

### Mechanism 2
- **Claim**: Klir uncertainty, combining discord and non-specificity, can address the exploration-exploitation dilemma in active learning.
- **Mechanism**: Discord quantifies the amount of conflicting information in the model's prediction, while non-specificity measures the degree of ignorance. By adjusting the parameter λ, the method can prioritize exploration or exploitation.
- **Core assumption**: The combination of discord and non-specificity effectively balances the need to explore uncertain areas and exploit known information.
- **Evidence anchors**:
  - [abstract]: "...sampling by Klir uncertainty, which tackles the exploration-exploitation dilemma..."
  - [section]: "Klir uncertainty is then derived from discord and non-specificity, it is used here for uncertainty sampling by adding the two previous formulas: Um(x) = N(x) + D(x)... With λ = 0.1, more discord is taken into account, with λ = 0.5, discord and non-specificity are used as much and with λ = 0.9, more non-specificity is taken into account."
  - [corpus]: "Weak. No direct mention of Klir uncertainty in corpus. Assumed from general knowledge."
- **Break condition**: If the discord and non-specificity measures do not accurately reflect the model's uncertainty, the exploration-exploitation balance will be incorrect.

### Mechanism 3
- **Claim**: The proposed methods can handle richer labels that include uncertainty and imprecision, improving active learning performance.
- **Mechanism**: By modeling labels as mass functions within the belief functions framework, the methods can represent uncertainty and imprecision in the labels. This allows the active learning algorithm to account for the inherent uncertainty in the labels when selecting instances to label.
- **Core assumption**: The belief functions framework accurately represents the uncertainty and imprecision in the labels, leading to better active learning performance.
- **Evidence anchors**:
  - [abstract]: "...the aim is to simplify the computational process while eliminating the dependence on observations. Crucially, the inherent uncertainty in the labels is considered, the uncertainty of the oracles."
  - [section]: "In the case of supervised classification, several models are now able to handle these uncertain labels... The labeling process is often carried out by humans; without making any difference between a label given by someone who has hesitated for a long time and a label given by someone who has no doubt..."
  - [corpus]: "Moderate. Evidential deep learning mentioned in corpus, but not specifically for handling uncertain labels."
- **Break condition**: If the belief functions framework does not accurately capture the uncertainty and imprecision in the labels, the active learning performance will not improve.

## Foundational Learning

- **Concept: Belief Functions**
  - Why needed here: Belief functions provide a framework for representing uncertainty and imprecision in labels, which is crucial for the proposed methods.
  - Quick check question: What is the difference between a mass function and a probability distribution in the context of belief functions?
- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: Understanding the difference between epistemic (reducible) and aleatoric (irreducible) uncertainty is essential for decomposing model uncertainty and improving active learning.
  - Quick check question: How can you distinguish between epistemic and aleatoric uncertainty in a model's predictions?
- **Concept: Active Learning**
  - Why needed here: Active learning is the context in which the proposed uncertainty sampling strategies are applied, making it essential to understand its principles and goals.
  - Quick check question: What is the main goal of active learning, and how does uncertainty sampling contribute to it?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Model Training -> Uncertainty Calculation -> Active Learning Loop
- **Critical path**: 
  1. Preprocess data to convert labels into mass functions.
  2. Train the model using the Evidential K-Nearest Neighbors algorithm.
  3. Calculate uncertainties using the proposed methods.
  4. Select instances to label based on the calculated uncertainties.
  5. Repeat steps 2-4 until the desired performance is achieved.
- **Design tradeoffs**:
  - Simplicity vs. Accuracy: The proposed methods are simpler than traditional approaches but may sacrifice some accuracy.
  - Exploration vs. Exploitation: The parameter λ in Klir uncertainty allows for balancing exploration and exploitation, but choosing the right value can be challenging.
  - Computational Cost: The belief functions framework may be computationally expensive compared to traditional methods.
- **Failure signatures**:
  - Poor performance: If the belief functions framework does not accurately capture uncertainty, the active learning performance will suffer.
  - Unstable results: If the parameter λ is not chosen correctly, the exploration-exploitation balance may be incorrect, leading to unstable results.
  - High computational cost: If the belief functions framework is too computationally expensive, it may not be practical for large datasets.
- **First 3 experiments**:
  1. Test the proposed methods on a simple dataset with known uncertainties to validate their accuracy.
  2. Compare the performance of the proposed methods with traditional uncertainty sampling methods on a real-world dataset.
  3. Evaluate the impact of the parameter λ on the exploration-exploitation balance and active learning performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of λ in Klir uncertainty affect the performance of active learning in terms of sample efficiency and final model accuracy?
- Basis in paper: [explicit] The paper mentions using different values of λ (0.1, 0.5, 0.9) to balance exploration and exploitation, but does not provide experimental results comparing their effects.
- Why unresolved: The paper does not include experiments comparing the performance of different λ values in an active learning setting.
- What evidence would resolve it: Experiments showing the impact of different λ values on active learning performance metrics like sample efficiency and final model accuracy.

### Open Question 2
- Question: How do the proposed evidential uncertainty methods perform compared to classical uncertainty sampling methods (entropy, least confidence) in active learning with rich labels?
- Basis in paper: [inferred] The paper introduces new evidential uncertainty methods and mentions they can handle rich labels, but does not provide direct comparisons with classical methods in an active learning context.
- Why unresolved: The paper does not include experiments comparing the proposed methods to classical uncertainty sampling methods in active learning scenarios with rich labels.
- What evidence would resolve it: Experiments comparing the performance of proposed evidential methods to classical methods in active learning tasks with rich labels.

### Open Question 3
- Question: How do the proposed evidential uncertainty methods scale to high-dimensional data and complex models like deep neural networks?
- Basis in paper: [inferred] The paper mentions compatibility with probabilistic models but focuses on K-NN and belief function-based models. It does not address scalability to high-dimensional data or complex models.
- Why unresolved: The paper does not provide experiments or analysis of the proposed methods' performance on high-dimensional data or with complex models like deep neural networks.
- What evidence would resolve it: Experiments demonstrating the performance of proposed methods on high-dimensional datasets and with complex models, along with analysis of computational complexity.

## Limitations

- The evidential framework may not be well-established in the broader machine learning community, limiting the adoption of the proposed methods.
- The choice of the parameter λ in Klir uncertainty may significantly impact the results, but the paper does not provide guidance on how to choose the optimal value.
- While the paper claims to handle richer labels that include uncertainty and imprecision, it does not provide a clear explanation of how the belief functions framework accurately represents this uncertainty.

## Confidence

- High confidence: The paper proposes two new uncertainty sampling strategies for active learning that leverage evidential reasoning.
- Medium confidence: The proposed methods are computationally simpler than existing approaches and experimental results on real-world datasets demonstrate their ability to outperform classical uncertainty sampling methods.
- Low confidence: The evidential framework accurately captures the epistemic uncertainty in a way that is comparable to traditional methods, and the combination of discord and non-specificity effectively balances exploration and exploitation.

## Next Checks

1. Validate the accuracy of the evidential framework in capturing epistemic uncertainty by comparing it with traditional methods on a simple dataset with known uncertainties.
2. Evaluate the impact of the parameter λ on the exploration-exploitation balance and active learning performance by conducting a sensitivity analysis.
3. Test the proposed methods on a different real-world dataset with rich labels to verify their ability to handle uncertainty and imprecision in labels and improve active learning performance.