---
ver: rpa2
title: Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye
  Images Under Manhattan World Assumption
arxiv_id: '2303.17166'
source_url: https://arxiv.org/abs/2303.17166
tags:
- adps
- points
- images
- world
- angle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of accurate and robust camera angle
  estimation from fisheye images in Manhattan world scenes. The authors propose a
  novel learning-based calibration method using heatmap regression to detect labeled
  image coordinates and recover camera rotation and remove fisheye distortion.
---

# Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under Manhattan World Assumption

## Quick Facts
- arXiv ID: 2303.17166
- Source URL: https://arxiv.org/abs/2303.17166
- Reference count: 40
- Method outperforms conventional geometry-based and learning-based methods in rotation estimation accuracy and reprojection error for fisheye image calibration

## Executive Summary
This paper addresses the challenge of accurate camera calibration from fisheye images in Manhattan world scenes. The authors propose a novel learning-based approach using heatmap regression to detect vanishing points and auxiliary diagonal points, which enables robust camera rotation estimation and fisheye distortion removal from a single image. The method introduces a new definition of pan angles to remove ambiguity and demonstrates superior performance compared to conventional geometry-based and learning-based calibration methods across multiple datasets and camera types.

## Method Summary
The proposed method uses a two-branch deep neural network architecture. The VP estimator branch employs heatmap regression to detect 13 labeled image coordinates (5 vanishing points and 8 auxiliary diagonal points) using an HRNet backbone. The distortion estimator branch predicts intrinsic parameters (focal length and distortion coefficient) using a modified version of Wakai's calibration network. Camera rotation is estimated by fitting the detected 3D points to orthogonal Manhattan world coordinates. The method introduces auxiliary diagonal points to compensate for insufficient vanishing points in images and proposes a new definition of pan angles based on road directions to remove ambiguity.

## Key Results
- Achieves mean absolute angle errors of 1.4°, 1.5°, and 1.4° for pan, tilt, and roll respectively on the StreetLearn-MH dataset
- Outperforms geometry-based methods (mean absolute error reduction of 40-60% in rotation estimation)
- Demonstrates cross-dataset robustness, maintaining accuracy when training on one dataset and testing on another
- Achieves low reprojection error (1.2 pixels) and high keypoint detection metrics (AP > 0.9, AR > 0.95, PCK > 0.95)

## Why This Works (Mechanism)

### Mechanism 1
Heatmap regression to vanishing points and auxiliary diagonal points enables robust camera rotation estimation from single fisheye images. The network predicts labeled image coordinates as heatmaps for VPs and ADPs, which are backprojected to 3D unit sphere coordinates using intrinsic parameters. Camera rotation is then estimated by fitting these 3D points to orthogonal Manhattan world coordinates. This approach provides pixel-wise accuracy for keypoint detection, avoiding domain-specific degradation seen in regressor-based approaches.

### Mechanism 2
Auxiliary diagonal points solve the problem of insufficient vanishing points in images. ADPs are defined as eight diagonal points indicating directions of cubic corners, forming a regular octahedron arrangement with VPs. This optimal 3D spatial uniformity ensures at least two unique axes for rotation estimation, even when VPs are missing or occluded in the image.

### Mechanism 3
The proposed Manhattan world definition removes four-fold rotational symmetric ambiguity in pan angles. The pan angle origin is defined along the direction of the road with respect to the camera and travel direction, selecting the axis that forms the smallest angle with the Manhattan world axis. This selection removes ambiguity in rotation estimation.

## Foundational Learning

- Concept: Vanishing points and their geometric significance
  - Why needed here: VPs are crucial for understanding the camera's orientation and the scene's geometry
  - Quick check question: What is a vanishing point and how does it relate to parallel lines in 3D space?

- Concept: Camera models and projection geometry
  - Why needed here: Understanding how 3D world coordinates map to 2D image coordinates is essential for calibration
  - Quick check question: How does a fisheye camera model differ from a standard pinhole camera model?

- Concept: Heatmap regression for keypoint detection
  - Why needed here: Heatmap regression is used to predict VP and ADP locations with pixel-wise accuracy
  - Quick check question: How does heatmap regression differ from direct regression for keypoint detection?

## Architecture Onboarding

- Component map: VP estimator (heatmap regression for 13 points) -> 2D VP/ADP coordinates -> 3D VP/ADP coordinates (with intrinsic params) -> rotation estimator -> camera rotation

- Critical path: Input image → VP Estimator → VP/ADP heatmaps → 2D VP/ADP coordinates → 3D VP/ADP coordinates + intrinsic parameters → 3D VP/ADP coordinates → camera rotation

- Design tradeoffs: Heatmap vs. direct regression (accuracy vs. speed), VP vs. ADP usage (completeness vs. complexity), model size (accuracy vs. computation)

- Failure signatures: High reprojection error indicates issues with VP/ADP detection or camera parameter estimation, inconsistent rotation angles suggest problems with VP/ADP arrangement or fitting process, low keypoint detection metrics indicate VP estimator issues

- First 3 experiments: 1) Test VP estimator on synthetic images with known VP locations, 2) Validate rotation estimation using ground truth VP/ADP coordinates, 3) Evaluate calibration accuracy on real fisheye images with known camera parameters

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed method handle scenes that lack sufficient geometric cues like lines, arcs, or vanishing points? While the paper introduces auxiliary diagonal points, it's unclear how effective they are in extremely challenging scenes with minimal geometric structure. Testing the method on datasets specifically designed to have minimal geometric cues would provide evidence of its effectiveness in such scenarios.

### Open Question 2
How does the choice of backbone architecture (e.g., HRNet vs. ResNet) impact the accuracy and efficiency of the proposed method? While the paper provides a comparison showing HRNet achieves better results, it doesn't explore the impact of other architectural choices or the trade-offs between accuracy and computational efficiency.

### Open Question 3
Can the proposed method be extended to handle multiple images or videos for improved calibration accuracy? The paper focuses on single image calibration but mentions that using multiple images or videos could be a promising direction for future work. Developing and testing an extension that can handle multiple images would provide evidence of its potential benefits.

## Limitations

- Method's reliance on Manhattan world assumptions significantly constrains applicability to non-grid city layouts or natural environments
- Heatmap regression approach requires labor-intensive annotation of VP/ADP coordinates
- Evaluation focuses primarily on pan and tilt angles, with limited discussion of roll angle accuracy and its impact on downstream tasks

## Confidence

- **High confidence**: VP estimator's heatmap regression mechanism for detecting vanishing points and auxiliary diagonal points
- **Medium confidence**: Rotation estimation accuracy and reprojection error improvements
- **Low confidence**: Robustness of pan angle definition in complex real-world scenarios

## Next Checks

1. Cross-domain robustness test: Evaluate VP estimator on datasets with significantly different characteristics from StreetLearn to assess domain generalization beyond Manhattan worlds

2. Failure mode analysis: Systematically analyze cases where fewer than 2 unique axes are detected, quantifying failure rate and identifying image characteristics that cause calibration failures

3. Roll angle accuracy assessment: Conduct targeted experiments measuring roll angle estimation accuracy and its impact on downstream tasks like 3D reconstruction or visual localization