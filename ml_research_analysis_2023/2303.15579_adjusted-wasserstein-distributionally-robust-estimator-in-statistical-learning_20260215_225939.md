---
ver: rpa2
title: Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning
arxiv_id: '2303.15579'
source_url: https://arxiv.org/abs/2303.15579
tags:
- estimator
- wdro
- adro
- adjusted
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an adjusted Wasserstein distributionally
  robust (WDRO) estimator to address the asymptotic bias issue in the classic WDRO
  estimator for statistical learning. The proposed adjusted estimator is derived by
  applying a nonlinear transformation to the WDRO estimator, which removes its asymptotic
  bias and reduces its asymptotic mean squared error.
---

# Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning

## Quick Facts
- arXiv ID: 2303.15579
- Source URL: https://arxiv.org/abs/2303.15579
- Reference count: 7
- Primary result: Adjusted WDRO estimator removes asymptotic bias and reduces mean squared error compared to classic WDRO estimator

## Executive Summary
This paper addresses the asymptotic bias issue in the classic Wasserstein distributionally robust (WDRO) estimator by introducing an adjusted WDRO estimator. The adjustment is achieved through a nonlinear transformation that removes the bias term while maintaining the robustness properties of the original estimator. The paper demonstrates that under certain regularity conditions, the adjusted estimator is asymptotically unbiased and has smaller asymptotic mean squared error than the classic WDRO estimator. The technique is applied to generalized linear models including logistic regression, linear regression, and Poisson regression, with numerical experiments showing improved performance even with finite sample sizes.

## Method Summary
The method involves computing the classic WDRO estimator using an iterative algorithm from Blanchet et al. (2022c), then applying a nonlinear transformation to obtain the adjusted estimator. For logistic regression and other GLMs, this requires solving a nonlinear system involving estimated matrices C(β) and H(β). The adjustment depends on the Wasserstein radius τ and sample size n. Synthetic data is generated from multivariate normal distributions for both logistic and linear regression models, with ground truth parameters specified. The method is evaluated using mean squared error between the estimator and ground truth parameter across different sample sizes and τ values.

## Key Results
- The adjusted WDRO estimator is asymptotically unbiased, removing the O(1/n) bias term present in the classic WDRO estimator
- The asymptotic mean squared error of the adjusted estimator is smaller than the classic WDRO estimator, with the bias term eliminated
- The adjusted estimator maintains the out-of-sample performance guarantee of the WDRO estimator through a generalization bound with an additional Lipschitz penalty term
- Numerical experiments demonstrate improved mean squared error performance of the adjusted estimator compared to the classic WDRO estimator, even with finite sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The nonlinear transformation removes asymptotic bias by inverting a known bias term.
- **Mechanism**: The classic WDRO estimator has asymptotic bias $-C^{-1}H(\beta^*)$. The adjusted estimator defines $K_n(z) = z - C(z)^{-1}H(z)/\sqrt{n}$ and applies $K_n^{-1}$ to the WDRO estimate, effectively undoing the bias in the limit.
- **Core assumption**: $K_n(z)$ is invertible for large enough $n$ and $z$ near the WDRO estimate.
- **Evidence anchors**:
  - [abstract] "based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator... this transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased"
  - [section 2.3] "If $K_n(z) = z - C(z)^{-1}H(z)/\sqrt{n}$... is well-defined, and $K_n^{-1}(\cdot)$ (locally) exists, then the adjusted WDRO estimator $\beta_{ADRO}^n$ is defined as $A_n(\beta_{DRO}^n) := K_n^{-1}(\beta_{DRO}^n)$"
  - [corpus] Weak: No direct corpus evidence for invertibility conditions, though cited work discusses asymptotic bias.

### Mechanism 2
- **Claim**: The adjusted estimator has smaller asymptotic mean squared error (aMSE) than the classic WDRO estimator.
- **Mechanism**: The bias term in the classic WDRO estimator contributes $O(1/n)$ to aMSE. The adjusted estimator eliminates this bias, leaving only the variance term and $O(1/n^{3/2})$ correction terms.
- **Core assumption**: The delta method applies to the nonlinear transformation and the inverse function theorem holds for $K_n^{-1}$.
- **Evidence anchors**:
  - [abstract] "our adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error"
  - [section 3.2] "aMSE($\beta_{DRO}^n$) = 1/n tr(D) + 1/n (C^{-1}H(\beta^*))^T C^{-1}H(\beta^*), aMSE($\beta_{ADRO}^n$) = 1/n tr(D) + O(1/n^{3/2})"
  - [corpus] Weak: No direct corpus evidence for the aMSE comparison, though asymptotic theory is standard.

### Mechanism 3
- **Claim**: The adjusted estimator maintains the out-of-sample performance guarantee of the classic WDRO estimator.
- **Mechanism**: The generalization bound for the classic WDRO estimator holds with high probability. Since the adjusted estimator is close to the classic one (difference $O(1/\sqrt{n})$), the bound transfers with an additional Lipschitz penalty term.
- **Core assumption**: The loss function is Lipschitz continuous in the parameter and the Wasserstein radius choice ensures the generalization bound holds.
- **Evidence anchors**:
  - [abstract] "The adjusted estimator maintains the out-of-sample performance guarantee of the WDRO estimator"
  - [section 3.3] "the following inequality, $E_{P^*}[L(f(X,\beta_{ADRO}^n),Y)] \leq \sup_{P \in U_{\rho_n}(P_n)} E_P[L(f(X,\beta_{ADRO}^n),Y)] + 2h\|\beta_{DRO}^n - \beta_{ADRO}^n\|_2 + \epsilon_n$, holds with probability $1-\alpha$"
  - [corpus] Weak: No direct corpus evidence for the generalization bound transfer, though related work discusses Wasserstein DRO bounds.

## Foundational Learning

- **Concept**: Asymptotic distribution theory and delta method
  - **Why needed here**: The core mechanism relies on the asymptotic distribution of the WDRO estimator and applying the delta method to the nonlinear transformation.
  - **Quick check question**: Can you derive the asymptotic distribution of an M-estimator and apply the delta method to a function of the estimator?

- **Concept**: Wasserstein distributionally robust optimization (WDRO)
  - **Why needed here**: The paper builds on WDRO as the base estimator and extends it with the adjustment technique.
  - **Quick check question**: What is the Wasserstein ambiguity set and how does it lead to a tractable reformulation of the DRO problem?

- **Concept**: Generalized linear models (GLMs)
  - **Why needed here**: The paper demonstrates the adjustment technique for logistic, Poisson, and linear regression, which are special cases of GLMs.
  - **Quick check question**: How do the link function and exponential family distribution define a GLM, and what are the MLE equations for logistic and Poisson regression?

## Architecture Onboarding

- **Component map**: Input data $(X_i, Y_i)$ -> Compute WDRO estimator $\beta_{DRO}^n$ -> Estimate $C(\beta)$ and $H(\beta)$ -> Solve nonlinear system for $\beta_{ADRO}^n$ -> Output adjusted estimator

- **Critical path**:
  1. Estimate $\beta_{DRO}^n$ using Blanchet et al. (2022c) algorithm
  2. Approximate $C(\beta)$ and $H(\beta)$ nonparametrically from data
  3. Solve nonlinear system $C(\beta)\beta - H(\beta)\sqrt{n} - C(\beta)\beta_{DRO}^n = 0$ for $\beta_{ADRO}^n$
  4. Return $\beta_{ADRO}^n$

- **Design tradeoffs**:
  - Accuracy vs. computational cost: Solving the nonlinear system requires iterative methods, which may be slow for large $d$
  - Sample size vs. invertibility: The adjustment technique requires large $n$ for $K_n^{-1}$ to exist
  - Radius choice $\tau$ vs. bias-variance tradeoff: Larger $\tau$ increases bias but may improve robustness

- **Failure signatures**:
  - If the nonlinear system fails to converge, the adjustment may not be computable
  - If the sample size is too small, the invertibility condition may not hold
  - If the loss function is not smooth or convex, the WDRO estimator may not exist

- **First 3 experiments**:
  1. Generate synthetic data from logistic regression, compute $\beta_{DRO}^n$ and $\beta_{ADRO}^n$, compare squared errors to true $\beta^*$
  2. Vary the Wasserstein radius $\tau$ and sample size $n$, plot the improvement in aMSE
  3. Test the adjustment technique on a non-linear model (e.g., neural network) to see if it generalizes

## Open Questions the Paper Calls Out
- What is the "optimal" transformation that minimizes the asymptotic mean squared error among all possible transformations for debiasing the WDRO estimator?
- How can the adjusted WDRO estimator be extended to more general statistical learning models beyond the generalized linear model?
- How can the adjusted WDRO estimator be adapted to more practical applications beyond parameter estimation in statistical learning?

## Limitations
- The invertibility condition for the nonlinear transformation K_n(z) is not rigorously verified
- The generalization bound transfer for the adjusted estimator relies on Lipschitz continuity assumptions that may not hold for all loss functions
- The computational cost of solving the nonlinear system for large-dimensional problems is not discussed

## Confidence
- **High confidence**: The existence and asymptotic bias of the classic WDRO estimator (well-established in prior literature)
- **Medium confidence**: The asymptotic mean squared error comparison between WDRO and ADRO estimators (requires careful verification of technical conditions)
- **Medium confidence**: The out-of-sample performance guarantee transfer (depends on Lipschitz assumptions and generalization bound tightness)

## Next Checks
1. Verify the invertibility condition K_n(z) for specific parameter values and sample sizes through numerical experiments
2. Test the adjustment technique on non-convex loss functions to check the Lipschitz continuity assumption
3. Benchmark the computational efficiency of the nonlinear system solver for high-dimensional problems (d > 10)