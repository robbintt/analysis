---
ver: rpa2
title: Trained MT Metrics Learn to Cope with Machine-translated References
arxiv_id: '2312.00536'
source_url: https://arxiv.org/abs/2312.00536
tags:
- prism
- references
- translation
- metrics
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how neural metrics trained on human evaluations
  of machine translation behave when provided with machine-translated references instead
  of human-created ones. Using a controlled experiment, the authors compare a baseline
  metric (Prism) to a fine-tuned version (Prism+FT) trained on human judgments.
---

# Trained MT Metrics Learn to Cope with Machine-translated References

## Quick Facts
- arXiv ID: 2312.00536
- Source URL: https://arxiv.org/abs/2312.00536
- Reference count: 20
- Primary result: Trained neural MT metrics show significantly higher robustness to machine-translated references compared to non-trained metrics

## Executive Summary
This paper investigates how neural MT evaluation metrics trained on human judgments perform when provided with machine-translated references instead of human-created ones. The authors fine-tune Prism, a neural MT metric, on human judgments from WMT 2020 and compare its performance to the baseline Prism and traditional metrics like BLEU and chrF. They find that trained metrics like Prism+FT maintain significantly higher correlation with human judgments and better system-level pairwise accuracy when using machine-translated references, demonstrating that training on human evaluations not only improves overall correlation but also enhances robustness to imperfect references.

## Method Summary
The authors fine-tune the Prism metric using a combination of cross-entropy pre-training and bidirectional pairwise ranking objectives on human judgments from WMT 2020 MQM annotations. They convert MQM scores to relative rankings using intra-annotator pairing, then jointly train on English-German and Chinese-English pairs. The fine-tuned Prism+FT is evaluated against baseline Prism and traditional metrics using both human-created and machine-translated references (error-free system translations from WMT 2021) on WMT 2021 test data. Performance is measured using segment-level Kendall's tau correlation and system-level pairwise accuracy.

## Key Results
- Trained metrics (Prism+FT) maintain higher segment-level correlation (Kendall's tau) with human judgments when using machine-translated references
- Prism+FT shows better system-level pairwise accuracy compared to baseline metrics under machine-translated references
- Traditional metrics like BLEU and chrF experience a sharper decline in performance when switching from human to machine-translated references

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training MT metrics on human judgments improves robustness to machine-translated references
- Mechanism: The fine-tuning process with pairwise ranking and cross-entropy objectives enables the metric to learn patterns in human judgments that are not captured by simple n-gram overlap, making it less sensitive to translationese artifacts in references
- Core assumption: The human judgment data contains sufficient signal about translation quality beyond surface-level n-gram matching
- Evidence anchors:
  - [abstract]: "Prism+FT maintains higher segment-level correlation (Kendall's tau) and system-level pairwise accuracy when using machine-translated references"
  - [section]: "Prism+FT is consistently more robust to machine-translated references than Prism, indicating that the metric learns to cope with such references during the fine-tuning stage"
  - [corpus]: Weak evidence - only 2 of 8 corpus papers mention "machine-translated" in relation to metric robustness

### Mechanism 2
- Claim: Bidirectional pairwise ranking helps metrics learn to compare translation quality effectively
- Mechanism: The forward and backward ranking losses teach the model to distinguish between better and worse translations relative to references, capturing directional quality differences
- Core assumption: Human judgments can be reliably converted to relative rankings between translation pairs
- Evidence anchors:
  - [section]: "We add a second ranking loss for the reverse paraphrasing direction, i.e., for reconstructing the reference from either of the system translations"
  - [abstract]: "We present an approach for fine-tuning Prism on human judgments that significantly improves segment-level correlation on unseen test data"
  - [corpus]: No direct evidence - corpus papers focus on different metric architectures

### Mechanism 3
- Claim: Cross-entropy pre-training on source-reference pairs prevents catastrophic forgetting during fine-tuning
- Mechanism: The cross-entropy term keeps the model familiar with the input segments while fine-tuning on human judgments, maintaining the original translation modeling capabilities
- Core assumption: The pre-trained model's knowledge about translation patterns is valuable and should be preserved
- Evidence anchors:
  - [section]: "Our goal in using this objective is to familiarize Prism with the segments to which the human judgments refer, and to prevent catastrophic forgetting during the fine-tuning stage"
  - [abstract]: "We show that fine-tuning Prism on human judgments makes it more robust to the use of machine-translated references"
  - [corpus]: No direct evidence - corpus papers don't discuss catastrophic forgetting in this context

## Foundational Learning

- Concept: Kendall's tau correlation coefficient
  - Why needed here: Used to measure segment-level correlation between metric scores and human judgments
  - Quick check question: If a metric perfectly ranks all translation pairs in the same order as human judgments, what would its Kendall's tau be?

- Concept: Pairwise accuracy
  - Why needed here: Used to measure system-level performance by comparing how well metrics rank system pairs relative to human judgments
  - Quick check question: If a metric correctly ranks 90 out of 100 system pairs, what is its pairwise accuracy?

- Concept: Cross-entropy in sequence modeling
  - Why needed here: Used as a pre-training objective to maintain translation modeling capabilities during fine-tuning
  - Quick check question: What is the relationship between cross-entropy loss and perplexity in sequence-to-sequence models?

## Architecture Onboarding

- Component map: Prism base model -> Cross-entropy objective -> Forward ranking loss -> Backward ranking loss -> Fine-tuned metric
- Critical path: Reference -> Prism model -> Paraphrasing probability -> Segment score -> Correlation calculation
- Design tradeoffs: Fine-tuning improves robustness but requires labeled human judgment data; cross-entropy maintains capabilities but may slow adaptation
- Failure signatures: Sharp drop in BLEU scores during fine-tuning indicates catastrophic forgetting; poor correlation with human judgments suggests insufficient learning
- First 3 experiments:
  1. Run fine-tuning without cross-entropy term to measure catastrophic forgetting
  2. Test with different cross-entropy weights to find optimal balance
  3. Compare pairwise ranking vs. regression objectives for fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do trained metrics like Prism+FT behave when evaluated on machine-translated references from different types of MT systems (e.g., rule-based vs. neural)?
- Basis in paper: [inferred] The paper mentions that the machine translations used in their analysis come from systems based on similar technology (Transformer architecture) and trained on similar data. This suggests that the results might not generalize to other types of MT systems.
- Why unresolved: The study focuses on a specific type of MT system (neural, Transformer-based), leaving open the question of how robust trained metrics are to references from other types of systems.
- What evidence would resolve it: Testing trained metrics like Prism+FT on machine-translated references from rule-based MT systems and other non-Transformer architectures would provide insights into the generalizability of the findings.

### Open Question 2
- Question: What is the impact of using machine-translated references on the accuracy of trained metrics like Prism+FT when evaluating low-resource language pairs?
- Basis in paper: [inferred] The paper mentions that it is mainly limited by the data used for fine-tuning and evaluating Prism, and that automatic MT evaluation is relevant for many more language pairs, including low-resource settings.
- Why unresolved: The experiments are based on three language pairs, and the study does not explore the behavior of trained metrics in low-resource settings where the quality of MT systems might be lower.
- What evidence would resolve it: Evaluating trained metrics like Prism+FT on machine-translated references from low-resource language pairs would reveal if the robustness observed in high-resource settings extends to low-resource scenarios.

### Open Question 3
- Question: How does the quality of the machine-translated references (e.g., post-edited vs. raw MT output) affect the performance of trained metrics like Prism+FT?
- Basis in paper: [explicit] The paper mentions that they use error-free system translations as a surrogate for real post-edited references, but it does not explore the impact of using different qualities of machine-translated references.
- Why unresolved: The study uses error-free system translations, which may not fully represent the range of quality in real-world machine-translated references.
- What evidence would resolve it: Testing trained metrics like Prism+FT on machine-translated references of varying quality (e.g., raw MT output, post-edited references with different levels of post-editing) would provide insights into how the quality of references affects the performance of trained metrics.

## Limitations
- Limited scope of machine-translated references: Uses error-free system translations as surrogates rather than real machine-translated references with typical errors and artifacts
- Generalizability concerns: Experiments focus on three high-resource language pairs in the news domain, limiting applicability to low-resource languages and specialized domains
- Metric architecture dependence: Findings based on Prism may not generalize to other neural metric architectures like COMET or BLEURT

## Confidence
- **High Confidence**: The finding that trained metrics (Prism+FT) show higher robustness to machine-translated references compared to non-trained metrics (BLEU, chrF) is well-supported by the experimental evidence and aligns with expectations from machine learning theory.
- **Medium Confidence**: The specific mechanism by which bidirectional pairwise ranking and cross-entropy pre-training contribute to robustness is plausible but not fully empirically validated. The study demonstrates the effect but doesn't isolate the individual contributions of each component.
- **Low Confidence**: The assumption that error-free system translations adequately represent the challenges posed by real machine-translated references is questionable and represents a significant limitation in the experimental design.

## Next Checks
1. **Real-World Reference Validation**: Repeat the experiments using actual machine-translated references from low-quality MT systems (rather than error-free translations) to verify whether the robustness advantage of trained metrics persists under more realistic conditions.

2. **Cross-Architecture Comparison**: Apply the same training methodology to other neural metrics like COMET or BLEURT to determine whether the robustness improvements are specific to Prism or generalize across different metric architectures.

3. **Domain Transfer Analysis**: Test the fine-tuned metrics on specialized domains (medical, legal, technical) where reference quality varies significantly and human judgment data is limited, to assess whether the robustness benefits transfer beyond the news domain.