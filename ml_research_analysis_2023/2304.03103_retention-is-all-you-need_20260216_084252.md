---
ver: rpa2
title: Retention Is All You Need
arxiv_id: '2304.03103'
source_url: https://arxiv.org/abs/2304.03103
tags: []
core_contribution: This work introduces HR-DSS, a decision support system that combines
  machine learning with explainable AI to analyze employee attrition. Eight ML models
  are trained on IBM HR data, with XGBoost achieving the highest accuracy of 89.12%.
---

# Retention Is All You Need

## Quick Facts
- arXiv ID: 2304.03103
- Source URL: https://arxiv.org/abs/2304.03103
- Reference count: 35
- Primary result: XGBoost achieves 89.12% accuracy on IBM HR attrition dataset with SHAP-based explanations

## Executive Summary
This work introduces HR-DSS, a decision support system that combines machine learning with explainable AI to analyze employee attrition. Eight ML models are trained on IBM HR data, with XGBoost achieving the highest accuracy of 89.12%. To improve interpretability, the SHAP library generates feature importance rankings and individual-level explanations, revealing that overtime, stock option level, and monthly income are dominant drivers of attrition. An interactive explainer dashboard enables "what-if" scenario analysis, allowing HR to simulate retention strategies by adjusting employee-specific factors. The system demonstrates that targeted adjustments to these features can convert predicted attrition into retention, providing transparent, actionable insights for HR decision-making.

## Method Summary
The HR-DSS pipeline begins with data preprocessing, including categorical encoding and outlier removal via Isolation Forest. Eight ML models are trained and evaluated; XGBoost is selected as the best performer. SHAP values are computed to quantify feature contributions to individual predictions and global importance. An interactive explainer dashboard is built to visualize SHAP outputs and enable "what-if" analysis, where HR can adjust features to explore retention scenarios.

## Key Results
- XGBoost achieves 89.12% accuracy on IBM HR attrition dataset
- Overtime, stock option level, and monthly income are identified as dominant attrition drivers
- SHAP-based dashboard enables actionable "what-if" retention strategies
- Feature engineering (weighted features, outlier detection) improves model accuracy more than class balancing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining machine learning prediction with SHAP-based explainability improves interpretability and trust in attrition models.
- Mechanism: SHAP assigns Shapley values to each feature, quantifying their contribution to individual predictions. By visualizing these values in a dashboard, HR personnel can understand why a specific employee is predicted to leave and what factors could change that outcome.
- Core assumption: SHAP values are a faithful representation of model behavior and can be aggregated to reveal global feature importance.
- Evidence anchors:
  - [abstract] "the SHAP library generates feature importance rankings and individual-level explanations"
  - [section] "SHAP feature importance are measured by the mean of absolute Shapley values"
- Break Condition: If SHAP explanations do not align with domain knowledge or fail to capture nonlinear interactions, the system loses credibility.

### Mechanism 2
- Claim: Adjusting dominant features via "what-if" analysis can convert predicted attrition into retention.
- Mechanism: The explainer dashboard allows interactive manipulation of feature values (e.g., increasing stock option level or reducing overtime). These changes recalculate SHAP values, showing how the prediction shifts, thus enabling targeted retention strategies.
- Core assumption: Changing a single or few features can meaningfully alter the attrition prediction for an individual.
- Evidence anchors:
  - [abstract] "By adjusting the specific dominant features of each individual, employee attrition can turn into employee retention"
  - [section] "With What-if-analysis, HR analyzes the probable retention of a specific employee by increasing and decreasing the causal factors"
- Break Condition: If feature interactions are too complex for simple adjustments, or if organizational constraints prevent changes, the "what-if" model becomes less actionable.

### Mechanism 3
- Claim: Feature engineering (weighted features, outlier detection) improves model accuracy more than class balancing techniques.
- Mechanism: By adding weights to high-impact features like StockOptionLevel and JobLevel, the model emphasizes their importance. Outlier removal via Isolation Forest cleans the data, reducing noise.
- Core assumption: The weighted features are truly predictive and outliers are harmful to model performance.
- Evidence anchors:
  - [section] "weights are added to StockOptionLevel and JobLevel features based on theories"
  - [section] "After the outlier detection and weight enforcement methods, every model shows some improvement"
- Break Condition: If weighting introduces bias or removes genuine variability, model accuracy could degrade.

## Foundational Learning

- SHAP values and game theory
  - Why needed here: SHAP is the core interpretability method used to explain individual predictions and global feature importance.
  - Quick check question: What does a positive SHAP value for a feature indicate about its impact on the prediction?

- Class imbalance handling
  - Why needed here: The dataset is heavily imbalanced (attrition vs. non-attrition), so techniques like SMOTE or weighting are considered to improve minority class detection.
  - Quick check question: What is the main drawback of using SMOTE in this context, according to the paper?

- Feature engineering and weighted features
  - Why needed here: Adding weights to StockOptionLevel and JobLevel helps the model prioritize these drivers of attrition.
  - Quick check question: Why might weighting features be preferable to oversampling in this scenario?

## Architecture Onboarding

- Component map:
  - Data preprocessing → ML model training (8 models) → Best model selection → SHAP explainability → Interactive explainer dashboard (What-if analysis)
  - Dependencies: Preprocessing outputs feed all models; SHAP requires a trained model and test set; dashboard needs SHAP outputs.

- Critical path:
  1. Read and preprocess data
  2. Train 8 ML models and evaluate accuracy
  3. Select best-performing model (XGBoost)
  4. Apply SHAP to best model predictions
  5. Build explainer dashboard with SHAP visualizations and What-if analysis

- Design tradeoffs:
  - Using multiple ML models increases robustness but adds complexity; the best one is chosen based on accuracy and F1 score.
  - SHAP is model-agnostic and interpretable but computationally heavier than simpler feature importance methods.
  - What-if analysis requires a live dashboard, which increases deployment complexity but greatly enhances HR usability.

- Failure signatures:
  - If SHAP values are unstable or inconsistent across runs, check random seed usage and model stability.
  - If What-if adjustments don't change predictions, verify feature ranges and SHAP computation pipeline.
  - If accuracy is low, revisit feature engineering, class imbalance handling, or try alternative models.

- First 3 experiments:
  1. Train all 8 models on raw data; compare accuracy and F1 to identify baseline performance.
  2. Apply outlier detection and weighted features; retrain models and compare performance gains.
  3. Implement SHAP on the best model; generate summary and dependence plots; verify that top features align with domain expectations.

## Open Questions the Paper Calls Out
1. Does the HR-DSS system maintain its performance and interpretability when scaled to larger, more complex datasets beyond the IBM HR Analytics dataset?
2. How does the HR-DSS system handle dynamic changes in employee features over time, and can it provide real-time updates to retention recommendations?
3. Can the HR-DSS system be extended to provide multi-objective optimization for retention strategies, balancing factors like cost, employee satisfaction, and organizational goals?

## Limitations
- Confidence is Medium for model accuracy and interpretability due to fixed dataset size and source
- Exact weighting scheme for key features is not detailed
- No ablation study on individual preprocessing steps
- No external validation on different datasets

## Confidence
- Model accuracy (89.12% for XGBoost): Medium
- SHAP-based interpretability: Medium
- Real-world impact of dashboard: Low
- Feature engineering improvements: Medium

## Next Checks
1. Replicate the full pipeline with explicit hyperparameter and weighting details; verify SHAP stability by running with multiple seeds.
2. Conduct a small user study with HR personnel to assess the usability and perceived usefulness of the explainer dashboard.
3. Test the model and explanations on a holdout or external dataset to evaluate generalizability.