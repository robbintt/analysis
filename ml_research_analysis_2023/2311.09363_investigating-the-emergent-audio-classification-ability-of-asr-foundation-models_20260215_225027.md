---
ver: rpa2
title: Investigating the Emergent Audio Classification Ability of ASR Foundation Models
arxiv_id: '2311.09363'
source_url: https://arxiv.org/abs/2311.09363
tags:
- whisper
- class
- label
- zero-shot
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the zero-shot audio classification ability
  of ASR foundation models. The authors use template-based text prompts at the decoder
  of Whisper and MMS models, then convert decoding probabilities to zero-shot predictions.
---

# Investigating the Emergent Audio Classification Ability of ASR Foundation Models

## Quick Facts
- arXiv ID: 2311.09363
- Source URL: https://arxiv.org/abs/2311.09363
- Reference count: 25
- Primary result: Whisper achieves state-of-the-art zero-shot audio classification on 8 datasets, outperforming baselines by 9.2% on average

## Executive Summary
This paper investigates the zero-shot audio classification ability of ASR foundation models, specifically Whisper and MMS. By using template-based text prompts at the decoder and converting decoding probabilities to predictions, the authors demonstrate that Whisper achieves state-of-the-art performance across 8 audio classification datasets without additional training or parameters. The key findings include the critical importance of debiasing through unsupervised reweighting of class probabilities (yielding up to 25% performance gains) and the observation that performance improves with model size, suggesting emergent abilities scale predictably as ASR models grow larger.

## Method Summary
The method leverages pre-trained ASR models (Whisper and MMS) by using template-based text prompts at the decoder to generate classification predictions. For each audio input, the model decodes text sequences using prompts like "This is a sound of class_label" and converts the resulting decoding probabilities into class probabilities. The authors evaluate three calibration approaches: uncalibrated outputs, prior-matching (reweighting class probabilities for balanced priors), and null-input calibration. Performance is measured using top-1 accuracy across 8 diverse audio classification datasets including environmental sounds, urban sounds, acoustic scenes, vocal sounds, emotions, music genres, and speaker counts.

## Key Results
- Whisper achieves state-of-the-art zero-shot performance on 8 audio classification datasets
- Debiasing through unsupervised reweighting yields significant performance gains up to 25%
- Performance increases continuously with model size from 39M to 1.6B parameters
- Prior-matching calibration consistently outperforms null-input calibration and uncalibrated methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoder's ability to generate text from speech creates a mapping between acoustic patterns and semantic labels, enabling zero-shot classification.
- Mechanism: When prompted with task-specific templates like "This is a sound of class_label", the decoder computes likelihoods for each label sequence. These likelihoods correlate with the acoustic similarity between the input and prototypical examples of each class, even though the model was never trained for classification.
- Core assumption: The model's pre-training on diverse speech data created latent representations that encode semantic information about audio content, not just surface-level transcription.
- Evidence anchors:
  - [abstract]: "We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions."
  - [section]: "We leverage various 'prompts' by considering various templates to represent the label sequences."
- Break condition: If the decoder's attention mechanism doesn't capture semantic-level audio features, or if the prompts don't align with the decoder's learned language patterns.

### Mechanism 2
- Claim: Prior-matching debiasing realigns the model's output distribution to match expected class priors, correcting systematic biases.
- Mechanism: The model's raw output probabilities are reweighted using unsupervised data to ensure each class has equal prior probability. This counteracts biases toward certain label words that appear more frequently in the pre-training data.
- Core assumption: The bias in zero-shot predictions stems from the training data distribution rather than fundamental limitations in the model's understanding.
- Evidence anchors:
  - [abstract]: "One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains."
  - [section]: "one can reweight the output probabilities to ensure that the corresponding output prior is balanced."
- Break condition: If the class priors are inherently imbalanced in the target domain, or if the unsupervised data doesn't represent the test distribution.

### Mechanism 3
- Claim: Larger models develop more robust semantic representations that generalize better to unseen tasks.
- Mechanism: As parameter count increases from 39M to 1.6B, the model's capacity to capture complex acoustic-to-semantic mappings improves, leading to better zero-shot classification performance across all tasks.
- Core assumption: Emergent abilities scale predictably with model size, following similar patterns observed in text and vision foundation models.
- Evidence anchors:
  - [abstract]: "We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance."
  - [section]: "We observe a continuous improvement in performance as the model size increases."
- Break condition: If the scaling relationship breaks down at larger sizes, or if the additional parameters don't capture meaningful semantic information.

## Foundational Learning

- Concept: Probabilistic classification using generative models
  - Why needed here: The system treats the ASR model as a generative classifier, converting log-likelihoods to probabilities for each class.
  - Quick check question: How does the denominator in Equation 1 ensure that the class probabilities sum to one?

- Concept: Prompt engineering and template design
  - Why needed here: Different prompt templates yield significantly different performance, indicating the importance of natural language alignment with the model's learned patterns.
  - Quick check question: Why do natural language prompts outperform class label-only prompts on average?

- Concept: Calibration and bias correction
  - Why needed here: Raw model outputs exhibit strong class biases that must be corrected through prior-matching or null-input calibration to achieve reliable performance.
  - Quick check question: What's the key difference between standard model calibration and the "task calibration" approach used here?

## Architecture Onboarding

- Component map: Whisper model (encoder-decoder transformer) → Template-based prompting → Probability conversion → Debiasing (optional) → Classification output
- Critical path: Audio input → Encoder → Decoder with prompt → Log-likelihood computation → Class probability conversion → (Optional) Debiasing → Final prediction
- Design tradeoffs: Template prompts vs. learned prompts (simple but less optimal vs. complex but requires training), multilingual vs. English-only models (better performance vs. simpler deployment)
- Failure signatures: Strong bias toward certain classes in uncalibrated outputs, poor performance on tasks far from transcription, sensitivity to prompt wording
- First 3 experiments:
  1. Test different prompt templates on a single dataset to identify the most effective prompt style
  2. Compare uncalibrated vs. prior-matched performance to quantify the impact of debiasing
  3. Evaluate performance across model sizes to verify the scaling relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the Whisper model architecture enable its zero-shot audio classification capabilities that are absent in MMS?
- Basis in paper: [explicit] The paper notes that MMS fails for zero-shot audio classification while Whisper succeeds, hypothesizing this is due to MMS being trained with CTC loss which maps acoustic features independently to characters, while Whisper's attention mechanism allows capturing high-level audio information.
- Why unresolved: The paper doesn't experimentally verify the architectural differences' impact or perform controlled ablation studies to isolate the effect of CTC vs attention mechanisms.
- What evidence would resolve it: A controlled experiment comparing Whisper variants with different loss functions (CTC vs attention) while holding other factors constant would isolate the architectural impact.

### Open Question 2
- Question: What is the relationship between model size and emergent abilities across different audio classification tasks?
- Basis in paper: [explicit] The paper observes that performance increases with model size and that multilingual models outperform English-only models beyond 500M parameters, but doesn't analyze which tasks benefit most from scaling.
- Why unresolved: The analysis aggregates performance across all tasks without examining differential scaling effects for specific task categories (e.g., emotion recognition vs sound event classification).
- What evidence would resolve it: A detailed scaling analysis showing performance improvements per task category as model size increases, identifying which tasks show the most dramatic emergent abilities.

### Open Question 3
- Question: What is the mechanism behind the performance gains from prior-matching calibration?
- Basis in paper: [explicit] The paper shows prior-matching significantly improves performance by creating more balanced class distributions, but doesn't explain why the model's uncalibrated outputs are so biased toward certain classes.
- Why unresolved: The paper demonstrates the effect but doesn't investigate the root cause of the bias in the model's learned representations or the relationship between pre-training data distribution and classification bias.
- What evidence would resolve it: An analysis tracing the origin of class bias to specific patterns in the pre-training data or examining how the model's attention patterns correlate with biased predictions.

### Open Question 4
- Question: How does the effectiveness of null-input calibration compare to prior-matching when test data distribution is unknown or highly imbalanced?
- Basis in paper: [inferred] The paper shows null-input calibration provides moderate improvements (average 6.7% and 4.8% gains) compared to prior-matching's substantial gains, but doesn't explore scenarios where prior-matching assumptions break down.
- Why unresolved: The paper only tests null-input calibration on balanced datasets and doesn't examine its performance on imbalanced or unknown distributions where prior-matching assumptions may not hold.
- What evidence would resolve it: Experiments comparing both calibration methods on imbalanced datasets with varying degrees of class imbalance would reveal their relative robustness in realistic deployment scenarios.

## Limitations
- The paper doesn't systematically explore the prompt space or provide robust methods for prompt selection, relying on ChatGPT-generated templates that may not be optimal or reproducible
- The effectiveness of debiasing may be task-specific rather than universally applicable, with the paper not investigating whether biases stem from pre-training data, prompt construction, or model architecture
- The scaling analysis is limited to models up to 1.6B parameters, without addressing whether the performance improvement trend continues at larger scales or if diminishing returns occur

## Confidence
- High: The core finding that Whisper achieves state-of-the-art zero-shot performance on 8 audio classification datasets (9.2% improvement over baselines) is well-supported by empirical results across multiple datasets and model sizes
- Medium: The claim about emergent abilities improving with model size is supported by the presented scaling analysis, but limited to the examined parameter range and specific model families
- Low: The assertion that debiasing is "one important step to unlock the emergent ability" overstates the universality of this finding, as debiasing effectiveness varies significantly across tasks and may depend on factors not fully explored in the paper

## Next Checks
1. **Prompt Robustness Test**: Systematically vary prompt templates across all datasets using a controlled grid search, measuring performance variance to quantify the sensitivity of zero-shot classification to prompt engineering choices

2. **Cross-Domain Generalization**: Evaluate the same zero-shot approach on audio tasks outside the classification domain (e.g., sound event detection, audio segmentation, or speech translation) to test the limits of emergent abilities

3. **Ablation on Pre-training Data**: Train smaller ASR models on subsets of the original pre-training data (e.g., excluding music, environmental sounds, or non-English speech) to determine which aspects of the pre-training corpus are most critical for emergent classification performance