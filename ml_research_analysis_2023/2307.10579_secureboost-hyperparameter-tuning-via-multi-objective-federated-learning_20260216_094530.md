---
ver: rpa2
title: SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning
arxiv_id: '2307.10579'
source_url: https://arxiv.org/abs/2307.10579
tags:
- utility
- privacy
- training
- leakage
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hyperparameter tuning for SecureBoost
  in vertical federated learning, focusing on balancing utility, efficiency, and privacy.
  The authors propose Constrained Multi-Objective SecureBoost (CMOSB), which uses
  multi-objective optimization to find Pareto optimal solutions that minimize utility
  loss, training cost, and privacy leakage simultaneously.
---

# SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning

## Quick Facts
- arXiv ID: 2307.10579
- Source URL: https://arxiv.org/abs/2307.10579
- Reference count: 8
- Key outcome: Proposed CMOSB algorithm finds superior SecureBoost hyperparameters balancing utility loss, training cost, and privacy leakage across four datasets

## Executive Summary
This paper addresses the critical challenge of hyperparameter tuning for SecureBoost in vertical federated learning, where data features are distributed across multiple parties. The authors propose Constrained Multi-Objective SecureBoost (CMOSB), which uses multi-objective optimization to find Pareto optimal solutions that minimize utility loss, training cost, and privacy leakage simultaneously. The approach employs NSGA-II evolutionary algorithm with constraints to focus the search space on feasible regions. Experimental results demonstrate that CMOSB outperforms baseline methods (FATE, Empirical, VF2Boost) in finding optimal tradeoffs between the three conflicting objectives across four datasets.

## Method Summary
CMOSB is a constrained multi-objective optimization algorithm based on NSGA-II that tunes SecureBoost hyperparameters in vertical federated learning settings. The algorithm generates offspring solutions through crossover and mutation, evaluates them using three objective measurements (utility loss, training cost, privacy leakage), applies constraints to focus the search on feasible regions, and uses non-dominated sorting to select top individuals for the next generation. A novel instance clustering attack is proposed to quantify privacy leakage by inferring labels from leaf node distributions. The method includes countermeasures like differential privacy and dynamic updating to protect against the attack. The algorithm was evaluated on four datasets (Synthetic1, Synthetic2, DefaultCredit, Sensorless) and compared against baseline hyperparameter selection methods.

## Key Results
- CMOSB achieves superior Pareto optimal solutions compared to baseline methods across all four datasets
- The instance clustering attack successfully quantifies privacy leakage with high accuracy under controlled conditions
- Adding constraints to the multi-objective optimization enables more efficient discovery of better Pareto solutions
- Optimal hyperparameter configurations vary significantly across datasets, demonstrating the need for automated tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMOSB uses NSGA-II-based multi-objective optimization to find Pareto optimal hyperparameter sets that balance utility loss, training cost, and privacy leakage simultaneously.
- Mechanism: The algorithm generates offspring solutions via crossover and mutation, evaluates them using objective measurements, applies constraints to focus the search on feasible regions, and uses non-dominated sorting to select top individuals for the next generation.
- Core assumption: The three objectives (utility loss, training cost, privacy leakage) are measurable and can be optimized simultaneously without one dominating the others.
- Evidence anchors:
  - [abstract] "propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage."
  - [section] "CMOSB is based on NSGA-II, and it is described in Algo. 3: offspring solutions of the current generation are generated by performing crossover and mutation using the previous solution"
- Break condition: If the constraints eliminate all feasible solutions, the algorithm cannot converge. If the objective measurements are inaccurate, the Pareto front will be suboptimal.

### Mechanism 2
- Claim: Privacy leakage is effectively measured using a novel instance clustering attack that infers labels from leaf node distributions in SecureBoost.
- Mechanism: The attack constructs a similarity matrix from instance distributions across trees, clusters instances based on similarity, and assigns labels to clusters using known labels within each cluster.
- Core assumption: The passive party can access instance distributions of leaf nodes, and these distributions contain sufficient information to cluster instances accurately.
- Evidence anchors:
  - [abstract] "privacy leakage is measured using our proposed instance clustering attack"
  - [section] "The attacker first constructs the similarity matrix based on the instance distribution of each leaf node using Eq. (3)"
- Break condition: If the purity threshold is set too high or local training is extensive, the clustering accuracy degrades significantly, making the attack ineffective.

### Mechanism 3
- Claim: Adding constraints to the multi-objective optimization focuses the search space on feasible regions, enabling more efficient discovery of better Pareto solutions.
- Mechanism: The algorithm applies penalty functions when solutions violate constraints, which modifies the objective values to discourage infeasible solutions from being selected.
- Core assumption: The constraints are meaningful and enforceable, and the penalty coefficient is appropriately calibrated to balance constraint satisfaction with objective optimization.
- Evidence anchors:
  - [abstract] "Each solution is an optimal tradeoff between the three conflicting objectives" with constraints to focus search space
  - [section] "The algorithm adds constraints on the solutions (line 6) to limit the search space of the solutions"
- Break condition: If penalty coefficients are too high, the algorithm may converge to suboptimal feasible solutions. If too low, infeasible solutions may dominate the search.

## Foundational Learning

- Concept: Vertical federated learning (VFL) data partitioning
  - Why needed here: SecureBoost operates in VFL setting where features are split across parties, requiring understanding of data alignment and collaborative training
  - Quick check question: What distinguishes vertical from horizontal federated learning in terms of data partitioning?

- Concept: Homomorphic encryption (HE) and its computational overhead
  - Why needed here: SecureBoost uses HE to protect gradients, and understanding HE operations is crucial for measuring training cost and evaluating efficiency tradeoffs
  - Quick check question: How do encryption, decryption, and addition operations in HE differ in computational complexity?

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The core contribution is finding hyperparameter sets that achieve optimal tradeoffs between three conflicting objectives
  - Quick check question: What defines a Pareto optimal solution in multi-objective optimization?

## Architecture Onboarding

- Component map:
  - NSGA-II evolutionary algorithm core -> SecureBoost training framework integration -> Objective measurement modules (utility, cost, privacy) -> Constraint enforcement system -> Data partitioning and alignment components

- Critical path: Solution generation → SecureBoost training with current hyperparameters → Objective measurement → Constraint evaluation → Selection → Next generation
- Design tradeoffs:
  - Exploration vs exploitation in NSGA-II (population size, mutation rates)
  - Constraint strictness vs solution diversity
  - Privacy protection strength vs utility loss
  - Training cost vs model performance
- Failure signatures:
  - Hypervolume stagnation indicates convergence issues
  - High constraint violation rates suggest infeasible search space
  - Wide Pareto front spread may indicate poorly calibrated objectives
- First 3 experiments:
  1. Run CMOSB on Synthetic1 dataset with default constraints to verify basic functionality and observe initial Pareto front shape
  2. Test privacy leakage measurement accuracy by comparing predicted vs actual labels in controlled attack scenarios
  3. Validate constraint effectiveness by running with varying constraint bounds and measuring impact on solution quality and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CMOSB perform compared to Bayesian optimization for SecureBoost hyperparameter tuning?
- Basis in paper: [explicit] The authors state in the conclusion that they plan to compare CMOSB with more sophisticated hyperparameter search algorithms, such as Bayesian Optimization.
- Why unresolved: The paper only compares CMOSB to baseline methods like FATE and VF2Boost, not to other hyperparameter optimization techniques.
- What evidence would resolve it: Experimental results showing the performance of CMOSB versus Bayesian optimization on the same datasets, with metrics for utility loss, training cost, and privacy leakage.

### Open Question 2
- Question: What is the impact of model complexity as an additional objective in the multi-objective optimization framework?
- Basis in paper: [explicit] The authors mention in the conclusion that they plan to consider model complexity as one of the objectives to find model hyperparameters with higher interpretability.
- Why unresolved: The current CMOSB framework only optimizes for utility loss, training cost, and privacy leakage, not model complexity.
- What evidence would resolve it: Experimental results showing the effect of including model complexity as an objective on the Pareto front and the resulting hyperparameter choices.

### Open Question 3
- Question: How effective is the instance clustering attack in real-world federated learning scenarios with more diverse data distributions?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the instance clustering attack on synthetic and real-world datasets, but these may not fully represent real-world federated learning scenarios.
- Why unresolved: The experiments use datasets with specific characteristics and data distributions, which may not generalize to all real-world scenarios.
- What evidence would resolve it: Empirical studies on diverse real-world datasets with varying data distributions and federated learning configurations to assess the robustness of the instance clustering attack.

## Limitations

- The privacy leakage measurement via instance clustering attack may not generalize well to datasets with extensive local training or poor label distribution
- The effectiveness of the algorithm depends heavily on accurate implementation details of SecureBoost, particularly homomorphic encryption operations
- The constraint penalty mechanism's calibration could significantly impact solution quality but is not thoroughly validated across different parameter settings

## Confidence

- **High Confidence**: The core NSGA-II framework for multi-objective optimization and the three-objective optimization framework are well-established approaches.
- **Medium Confidence**: The specific implementation of objective measurements (utility loss, training cost) appears reasonable but depends on accurate SecureBoost implementation details.
- **Low Confidence**: The privacy leakage measurement via instance clustering attack is novel but its effectiveness across diverse real-world scenarios is uncertain without extensive empirical validation.

## Next Checks

1. Implement the privacy leakage measurement using the instance clustering attack on controlled datasets where ground truth privacy leakage can be computed, validating the attack's accuracy across different parameter settings.
2. Conduct ablation studies on the constraint penalty mechanism by running CMOSB with varying penalty coefficients to quantify the impact on solution quality and convergence behavior.
3. Test CMOSB's robustness by running experiments on additional real-world datasets with different characteristics (data distributions, feature correlations, label imbalance) to assess generalization beyond the four evaluated datasets.