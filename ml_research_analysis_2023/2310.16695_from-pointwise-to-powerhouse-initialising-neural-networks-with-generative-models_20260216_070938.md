---
ver: rpa2
title: 'From Pointwise to Powerhouse: Initialising Neural Networks with Generative
  Models'
arxiv_id: '2310.16695'
source_url: https://arxiv.org/abs/2310.16695
tags:
- initialisation
- neural
- noise
- accuracy
- initialisations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of neural network weight initialization
  by proposing new methods that utilize generative models instead of traditional pointwise
  distributions. The core idea is to employ variational autoencoders (VAEs) for local
  initialization of weight groups and graph hypernetworks (GHNs) for global initialization
  of full weight sets.
---

# From Pointwise to Powerhouse: Initialising Neural Networks with Generative Models

## Quick Facts
- arXiv ID: 2310.16695
- Source URL: https://arxiv.org/abs/2310.16695
- Reference count: 40
- Key outcome: Global initialization via graph hypernetworks leads to faster convergence and higher accuracy compared to traditional pointwise initialization methods.

## Executive Summary
This paper addresses the problem of neural network weight initialization by proposing new methods that utilize generative models instead of traditional pointwise distributions. The authors introduce local initialization using variational autoencoders (VAEs) for weight groups and global initialization using graph hypernetworks (GHNs) for full weight sets. The methods are evaluated on state-of-the-art deep convolutional neural networks, showing that global initializations result in higher accuracy and faster initial convergence. The authors also propose a "noise GHN" modification that introduces diversity into ensemble members, improving out-of-distribution generalization.

## Method Summary
The paper proposes two groups of initialization methods: local initialization using VAEs for small groups of weights and global initialization using GHNs for full weight sets. VAEs learn the distribution of weight slices within each layer, while GHNs generate complete weight sets conditioned on full network architecture. The authors also introduce "noise GHN," which injects noise into the GHN decoder and modifies the loss function to encourage diversity among ensemble members. The methods are evaluated on CIFAR-10 and PatchCamelyon datasets using ResNet-20 architectures, with metrics including accuracy, convergence speed, and ensembling performance.

## Key Results
- Global initialization via GHN leads to faster initial convergence and higher accuracy compared to traditional methods
- Noise GHN improves ensemble diversity and out-of-distribution generalization
- Local VAE initialization shows no significant performance differences compared to standard initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global initialization via GHN leads to faster initial convergence due to synchronized, already-performing weight sets.
- Mechanism: GHN generates complete weight sets conditioned on full network architecture, ensuring that the produced weights are already aligned and performant for the given structure.
- Core assumption: The GHN has been trained on a diverse set of architectures so that its weight generation is meaningful for unseen but similar networks.
- Evidence anchors:
  - [abstract] "Our results show that global initialisations result in higher accuracy and faster initial convergence speed."
  - [section 3.2] "The resulting weights are immediately effective, achieving an impressive 58.6% accuracy on CIFAR-10 for a ResNet-50 architecture that it has never seen before."
  - [corpus] Weak corpus coverage; no direct evidence found for GHN synchronization claim.
- Break condition: If the GHN is not trained on architectures similar to the target, the weights may not be well synchronized, leading to poor performance.

### Mechanism 2
- Claim: Noise GHN introduces diversity into weight sets, improving ensemble generalization.
- Mechanism: By injecting noise into the GHN decoder and modifying the loss to encourage low cosine similarity between predictions, the Noise GHN produces varied weight sets for the same architecture, enhancing ensemble diversity.
- Core assumption: Diversity in ensemble members is beneficial for out-of-distribution generalization, and the noise injection does not destroy the quality of the weights.
- Evidence anchors:
  - [abstract] "We propose a modification called noise graph hypernetwork, which encourages diversity in the produced ensemble members."
  - [section 3.2] "To achieve different weights in every forward pass, we integrate noise into the GHN... the modified loss function encourages low cosine similarity logits."
  - [section 4] "The Noise GHN significantly enhances diversity, leading to a clear impact on the OOD ECE."
- Break condition: If the noise level is too high, it may disrupt the quality of the generated weights, leading to poor performance.

### Mechanism 3
- Claim: Local initialization using VAEs captures layer-specific weight distributions, but does not significantly improve performance over traditional methods.
- Mechanism: VAEs learn the distribution of weight slices within each layer, and weights are sampled from these learned distributions during initialization.
- Core assumption: The weight distributions within each layer are sufficiently consistent across different trained networks to be captured by a VAE.
- Evidence anchors:
  - [section 3.1] "We train a set of variational autoencoders, one for each layer, to learn the unknown local distributions."
  - [section 4] "There are no significant performance differences between local and standard initialisations."
  - [corpus] No relevant corpus evidence found; weak support for VAE-based initialization benefits.
- Break condition: If the weight distributions within layers are too variable or the VAE fails to capture them accurately, the initialization may not be effective.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs are used to learn local distributions of weight slices within each layer for initialization.
  - Quick check question: What is the main objective function used to train a VAE, and what are its two components?

- Concept: Graph Hypernetworks (GHNs)
  - Why needed here: GHNs generate complete weight sets conditioned on the full network architecture, enabling global initialization.
  - Quick check question: How does a GHN represent a neural network architecture, and what is the role of the hypernetwork in this process?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is used to evaluate the calibration of ensemble predictions on out-of-distribution data.
  - Quick check question: How is ECE calculated, and what does a lower ECE value indicate about a model's predictions?

## Architecture Onboarding

- Component map:
  VAE-based local initialization: Separate VAE for each layer → trained on weight slices from similar architectures
  GHN-based global initialization: Graph network encodes architecture → hypernetwork generates weights → optional noise injection for diversity
  Noise GHN: Modification of GHN with noise injection and modified loss for diversity

- Critical path:
  1. Train GHN on diverse architectures to learn weight generation
  2. For a target architecture, use GHN to generate initial weights
  3. Optionally, inject noise and use modified loss for Noise GHN to enhance diversity

- Design tradeoffs:
  - Global vs. local initialization: Global initialization provides synchronized, performant weights but may lack diversity; local initialization is simpler but does not significantly improve performance
  - Noise injection: Improves diversity but may affect weight quality if not tuned properly

- Failure signatures:
  - Poor convergence or accuracy: GHN may not be well-trained on similar architectures
  - Low ensemble diversity: Noise injection may be insufficient or noise level too low
  - Overfitting: Noise injection may be too high, disrupting weight quality

- First 3 experiments:
  1. Train GHN on a diverse set of ResNet architectures and evaluate weight generation on a held-out ResNet variant
  2. Compare convergence speed and accuracy of GHN-initialized networks vs. traditional initialization on a small dataset
  3. Implement Noise GHN and evaluate ensemble diversity and OOD performance compared to standard GHN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Noise GHN compare to other initialization methods when evaluated on different network architectures beyond ResNet-20?
- Basis in paper: [explicit] The authors state "We train and evaluate our methods on two in-distribution datasets, one OOD dataset and one architecture. Additional experiments are needed to verify, if the advantages extend to different datasets and network architectures."
- Why unresolved: The paper only evaluates on ResNet-20 architecture, limiting generalizability.
- What evidence would resolve it: Experiments evaluating Noise GHN initialization on various architectures (e.g., VGG, DenseNet, EfficientNet) across multiple datasets.

### Open Question 2
- Question: What is the computational cost trade-off between training generative models for initialization versus using traditional initialization methods?
- Basis in paper: [explicit] The authors note "In comparison to the training of a single network, this setup of training and employing on the same dataset is very costly. However, the potential savings when applying our methods to various datasets would justify these costs."
- Why unresolved: No quantitative comparison of computational resources required for training generative models versus traditional methods.
- What evidence would resolve it: Detailed analysis of GPU hours, energy consumption, and overall cost for training VAEs/GHNs versus traditional initialization methods.

### Open Question 3
- Question: How does the diversity introduced by Noise GHN affect ensemble performance on in-distribution data?
- Basis in paper: [inferred] The authors focus on Noise GHN's impact on OOD data and mention that deterministic GHN initializations lead to reduced ensemble performance, but do not specifically analyze in-distribution ensemble performance.
- Why unresolved: The paper primarily evaluates ensemble performance on OOD data without examining in-distribution effects.
- What evidence would resolve it: Experiments comparing ensemble accuracy and calibration on in-distribution test sets using different initialization methods.

## Limitations

- The paper demonstrates improved convergence speed for GHN-initialized networks, but long-term performance stability during extended training remains unclear
- The noise GHN modification shows improved ensemble diversity, but the optimal noise level and its sensitivity to architecture variations is not established
- VAE-based local initialization shows no significant performance gains over traditional methods, suggesting the assumption about consistent layer-wise weight distributions may not hold

## Confidence

- Medium confidence: GHN-based global initialization leads to faster convergence and higher initial accuracy. This is supported by experimental results but requires validation across more architectures.
- Medium confidence: Noise GHN improves ensemble diversity and out-of-distribution calibration. Results are promising but depend heavily on noise parameter tuning.
- Low confidence: Local VAE initialization provides meaningful benefits. Experimental evidence shows no significant improvements over standard initialization.

## Next Checks

1. Evaluate GHN initialization stability over extended training epochs (50-100 epochs) to verify sustained performance advantages.
2. Systematically test noise GHN sensitivity by varying noise amplitude and measuring the tradeoff between diversity and weight quality degradation.
3. Analyze the VAE reconstruction loss distribution across different network layers to identify why local initialization fails to capture meaningful weight patterns.