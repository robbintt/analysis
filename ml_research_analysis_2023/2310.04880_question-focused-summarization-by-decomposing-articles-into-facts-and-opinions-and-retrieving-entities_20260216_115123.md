---
ver: rpa2
title: Question-focused Summarization by Decomposing Articles into Facts and Opinions
  and Retrieving Entities
arxiv_id: '2310.04880'
source_url: https://arxiv.org/abs/2310.04880
tags:
- news
- impact
- summary
- could
- potential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research proposes a system that extracts facts from news articles,
  identifies relevant entities (primarily companies), and generates summaries describing
  how the news impacts each entity. The method uses a rule-based model to extract
  factual sentences, Pyserini for entity retrieval, and GPT-3.5 to generate entity-specific
  impact summaries and aggregate them into a final article summary.
---

# Question-focused Summarization by Decomposing Articles into Facts and Opinions and Retrieving Entities

## Quick Facts
- arXiv ID: 2310.04880
- Source URL: https://arxiv.org/abs/2310.04880
- Reference count: 13
- Primary result: Generates entity-specific news summaries with ChatGPT-based evaluation scores around 4-5 for coherence and accuracy

## Executive Summary
This paper proposes a system for generating entity-focused summaries from news articles by first extracting factual sentences using a rule-based model, then retrieving relevant entities (primarily companies) through dense passage retrieval, and finally generating impact summaries for each entity using GPT-3.5. The approach addresses the challenge of summarizing complex news articles in a way that highlights how the information affects specific entities of interest. The system uses linguistic markers to separate facts from opinions, leverages semantic embeddings for entity matching, and employs few-shot prompting with GPT-3.5 to generate concise impact summaries that are aggregated into a final article summary.

## Method Summary
The method involves three main stages: fact extraction using a rule-based model with NLTK/Spacy to identify factual sentences based on linguistic markers like "according to" and "studies show"; entity retrieval using Pyserini with dense passage retrieval techniques to match facts to relevant entities from a corpus including S&P 500 companies; and summary generation using GPT-3.5 Turbo with few-shot prompts to create entity-specific impact summaries that are then aggregated into a final article summary. The system evaluates summaries using ChatGPT-based metrics (factfulness, conciseness, usefulness) and human evaluations for usefulness, with plans to expand entity sets beyond the initial S&P 500 companies tested.

## Key Results
- ChatGPT-based evaluation scores averaged around 4-5 on 1-5 scale for factfulness, conciseness, and usefulness
- Human evaluations rated summaries as relevant and useful for financial analysis
- System successfully identified and summarized impacts on companies mentioned in news articles about international relations and defense
- Generated summaries were found to be coherent and accurate in capturing entity-specific impacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact extraction accuracy is improved by using rule-based linguistic filters for subjectivity markers.
- Mechanism: The system filters out subjective/opinionated sentences using a rule-based model that identifies markers like "I think," "believe," "hope," and factual indicators like "according to," "studies show," "research indicates." This approach ensures only factual content is used for downstream tasks.
- Core assumption: News articles from sources like the Economist are generally accurate in their reporting, so a rule-based model based on linguistic structures can reliably separate facts from opinions.
- Evidence anchors:
  - [section]: "We classify subjective/opinionated sentences as containing subjective words/phrases ("I think", "believe", "hope", etc) and factual sentences as containing words/phrases like "according to", "studies show", and "research indicates"."
  - [abstract]: "The proposed approach includes the identification of salient facts and events from news articles"
- Break condition: If news sources contain high levels of editorializing or if the rule-based model fails to capture nuanced subjective language, fact extraction accuracy would degrade.

### Mechanism 2
- Claim: Entity retrieval accuracy is improved by using dense passage retrieval with contextual embeddings.
- Mechanism: The system creates embeddings for each entity using Hugging Face sentence transformers and for each fact using DPR techniques. This allows matching facts to the most relevant entities based on semantic similarity rather than keyword matching.
- Core assumption: Entity descriptions from sources like Wikipedia contain sufficient context to create meaningful embeddings that can be matched to factual content.
- Evidence anchors:
  - [section]: "We create an embedding for each fact and use Dense Passage Retrieval (DPR) techniques to map the fact embedding to the three most relevant entities in the corpus."
  - [abstract]: "then use these facts to form tuples with entities"
- Break condition: If entity descriptions are too generic or if DPR fails to capture domain-specific relationships, entity retrieval accuracy would decrease.

### Mechanism 3
- Claim: Summary generation quality is improved by using few-shot prompting with contextual examples.
- Mechanism: The system uses GPT-3.5 Turbo with few-shot prompts that provide examples of fact-entity summaries, allowing the model to learn the expected output format and generate more relevant summaries.
- Core assumption: Few-shot prompting with examples helps the LLM understand the task context better than zero-shot prompting alone.
- Evidence anchors:
  - [section]: "We experimented with different models to find the ones which gave the best possible response."
  - [section]: "If that did not work, we planned to use few-shot learning and feed more context to the prompt in order to generate better results."
- Break condition: If the provided examples are not representative of the target domain or if the LLM cannot generalize from the examples, summary quality would suffer.

## Foundational Learning

- Concept: Rule-based text classification using linguistic features
  - Why needed here: To separate factual content from opinionated content before processing
  - Quick check question: What are the key linguistic markers used to identify factual vs. opinionated sentences?

- Concept: Dense passage retrieval and semantic similarity
  - Why needed here: To match extracted facts to relevant entities based on semantic meaning rather than exact keywords
  - Quick check question: How does DPR differ from traditional keyword-based search methods?

- Concept: Few-shot learning with large language models
  - Why needed here: To guide the LLM in generating relevant summaries without extensive fine-tuning
  - Quick check question: What are the advantages of using few-shot prompting over zero-shot prompting?

## Architecture Onboarding

- Component map:
  - News article ingestion → Rule-based fact extraction → Entity embedding generation → DPR-based entity matching → LLM-based impact summary generation → Summary aggregation → Evaluation
  - Key components: NLTK/SpaCy for NLP processing, Pyserini for DPR, GPT-3.5 Turbo for generation, ChatGPT for evaluation

- Critical path:
  - Fact extraction → Entity matching → Impact summary generation → Aggregation
  - The bottleneck is typically the LLM generation step due to API costs and latency

- Design tradeoffs:
  - Rule-based vs. learned fact extraction: Simpler to implement but may miss nuanced cases
  - Few-shot vs. fine-tuning: Faster iteration but potentially lower quality for specialized domains
  - ChatGPT vs. custom evaluation: More flexible but less reliable than domain-specific metrics

- Failure signatures:
  - Low fact extraction recall: Too many subjective sentences being included
  - Poor entity matching: Facts being associated with irrelevant entities
  - Generic summaries: LLM not capturing entity-specific impact details

- First 3 experiments:
  1. Test rule-based fact extraction on a sample of 50 articles to measure precision/recall
  2. Validate DPR entity matching by checking if top-K results are semantically relevant
  3. Compare few-shot vs. zero-shot summary generation quality on 10 sample facts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompting techniques and variations yield the most accurate and useful entity-specific summaries?
- Basis in paper: [explicit] The paper mentions experimenting with different prompting techniques but doesn't detail which specific approaches worked best
- Why unresolved: The paper states they aimed to experiment with improved prompting techniques but only provides high-level results without detailing specific prompt structures
- What evidence would resolve it: Systematic comparison of different prompt structures (zero-shot, few-shot, chain-of-thought, etc.) with quantitative metrics for each approach

### Open Question 2
- Question: How scalable is the entity retrieval system beyond the S&P 500 companies tested?
- Basis in paper: [inferred] The paper mentions plans to expand beyond S&P 500 companies but doesn't report results on larger or different entity sets
- Why unresolved: The evaluation was limited to a small set of companies, and the paper only states intentions to expand the entity corpus
- What evidence would resolve it: Performance metrics when applied to a significantly larger and more diverse entity set (e.g., all publicly traded companies or broader industry categories)

### Open Question 3
- Question: What is the actual impact of the generated summaries on financial decision-making?
- Basis in paper: [explicit] The paper evaluates summaries using ChatGPT and human evaluators but doesn't measure real-world financial impact
- Why unresolved: The paper focuses on abstract quality metrics rather than measuring whether the summaries actually help analysts make better investment decisions
- What evidence would resolve it: A study measuring how summaries affect actual investment decisions or portfolio performance compared to traditional analysis methods

### Open Question 4
- Question: How does the system perform on news articles from different domains or writing styles?
- Basis in paper: [inferred] The paper primarily uses Economist articles and mentions web-scraped financial news, but doesn't test on diverse article types
- Why unresolved: The evaluation is limited to news articles about international relations and defense, without testing on business, technology, or other domains
- What evidence would resolve it: Performance metrics across different news sources and domains (business, technology, politics, etc.) showing consistency or degradation in summary quality

## Limitations
- Rule-based fact extraction may miss nuanced subjective language or incorrectly filter factual content using subjective language
- ChatGPT-based evaluation methodology lacks external validation and may not accurately reflect human judgment
- System performance may not generalize beyond financial news and specific writing styles tested
- Reported scores of 4-5 on 1-5 scale indicate good but not exceptional performance with room for improvement

## Confidence
- Fact extraction mechanism: Medium - well-specified but depends on rule-based approach effectiveness
- DPR entity matching: Medium - theoretically sound but practical effectiveness depends on embedding quality
- Few-shot prompting: Medium - standard practice but results vary significantly with prompt engineering
- Overall system performance: Medium - limited evaluation scope and lack of detailed implementation specifications

## Next Checks
1. Conduct human evaluation of fact extraction precision/recall on 50 sample articles to validate the rule-based model's effectiveness
2. Test entity retrieval accuracy by having annotators verify top-3 DPR matches for 30 randomly selected facts
3. Compare summary quality between few-shot and zero-shot prompting on 20 sample facts to quantify the prompting benefit