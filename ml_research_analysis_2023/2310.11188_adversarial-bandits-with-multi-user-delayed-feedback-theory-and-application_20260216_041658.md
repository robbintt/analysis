---
ver: rpa2
title: 'Adversarial Bandits with Multi-User Delayed Feedback: Theory and Application'
arxiv_id: '2310.11188'
source_url: https://arxiv.org/abs/2310.11188
tags:
- feedback
- adversarial
- bandits
- bandit
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new algorithm MUD-EXP3 to solve the adversarial
  multi-armed bandit problem with multi-user delayed feedback. The key idea is to
  use an importance-weighted estimator to handle the delayed feedback from multiple
  users and make decisions based on the estimated cumulative loss.
---

# Adversarial Bandits with Multi-User Delayed Feedback: Theory and Application

## Quick Facts
- arXiv ID: 2310.11188
- Source URL: https://arxiv.org/abs/2310.11188
- Reference count: 13
- Primary result: Proposed MUD-EXP3 algorithm achieves regret bound of O(sqrt(T*M^2*ln(N)*(N*e + 4*d_max))) for adversarial multi-armed bandits with multi-user delayed feedback

## Executive Summary
This paper addresses the challenging problem of adversarial multi-armed bandits with multi-user delayed feedback. The authors propose MUD-EXP3, which uses importance-weighted estimators to handle arbitrary delays from multiple users while maintaining unbiased cumulative loss estimates. For unknown time horizons, they introduce AMUD-EXP3 with a doubling trick. The algorithms achieve sublinear regret bounds and outperform state-of-the-art baselines in experiments.

## Method Summary
The paper proposes MUD-EXP3, a modified EXP3 algorithm that uses importance-weighted estimators to handle delayed feedback from multiple users. The algorithm maintains a feedback buffer and computes unbiased loss estimates using the formula $\hat{l}_{i}^{j}(s) = \frac{I\{A_s=i\} \cdot l_{i}^{j}(s)}{p_i(s)}$. For unknown time horizons, AMUD-EXP3 is introduced with a doubling trick approach, dynamically adjusting the learning rate based on missing feedback samples. Both algorithms achieve sublinear regret bounds under the adversarial bandit setting.

## Key Results
- MUD-EXP3 achieves regret bound of O(sqrt(T*M^2*ln(N)*(N*e + 4*d_max)))
- AMUD-EXP3 maintains sublinear regret for unknown T using doubling trick
- Proposed algorithms outperform D-UCB, SE, and random baselines in experiments
- Importance-weighted estimators successfully handle arbitrary user delays

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance-weighted estimators enable unbiased cumulative loss estimates despite arbitrary user delays.
- Mechanism: At each round, the algorithm computes $\hat{l}_{i}^{j}(s) = \frac{I\{A_s=i\} \cdot l_{i}^{j}(s)}{p_i(s)}$ for each observed delayed feedback. This estimator is unbiased because $E[\hat{l}_{i}^{j}(s) | F_{s-1}] = l_{i}^{j}(s)$ when the action is chosen according to the distribution $p(s)$.
- Core assumption: The feedback distribution is oblivious and the delays are arbitrary but finite.
- Evidence anchors:
  - [abstract]: "make a decision at each round by considering the importance-weighted estimator of the received feedback from different users"
  - [section 4.1]: The formal definition and use of $\hat{l}_{i}^{j}(s)$ in the MUD-EXP3 algorithm
- Break condition: If the delay distribution has unbounded variance or if feedback arrives after the terminal round, the estimator becomes biased or unusable.

### Mechanism 2
- Claim: The adaptive learning rate $\eta_\varepsilon = \frac{1}{M}\sqrt{\frac{\ln N}{2\varepsilon}}$ ensures sublinear regret without knowing $T$.
- Mechanism: By using a doubling trick with epoch length determined by the number of missing feedback samples, the algorithm adapts the learning rate dynamically. This prevents the learning rate from being too large (which would cause instability) or too small (which would slow convergence).
- Core assumption: The number of missing feedback samples $V_t$ grows predictably with epochs, and the delay upper bound $d_{max}$ remains finite.
- Evidence anchors:
  - [abstract]: "an adaptive algorithm AMUD-EXP3 is proposed with a sublinear regret with respect to $T$"
  - [section 4.2]: The definition of epochs and the dynamic learning rate update
- Break condition: If the delay distribution is heavy-tailed or unbounded, the epoch boundaries become unpredictable, breaking the doubling trick.

### Mechanism 3
- Claim: Softmax probability updates maintain exploration-exploitation balance under delayed feedback.
- Mechanism: The probability $p_i(t+1) = \frac{W_i(t)}{\sum_k W_k(t)}$ where $W_i(t) = \exp(-\eta' \hat{L}_i(t))$. This softmax update ensures that arms with higher estimated cumulative loss are chosen less frequently, while still maintaining non-zero probability for all arms to enable exploration.
- Core assumption: The cumulative estimated loss $\hat{L}_i(t)$ is a reliable indicator of true performance despite delayed and multi-user feedback.
- Evidence anchors:
  - [abstract]: "makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users"
  - [section 4.1]: The update rules for $W_i(t)$ and $p_i(t+1)$
- Break condition: If the importance-weighted estimator has high variance due to extreme delays or rare feedback, the softmax probabilities may become unstable.

## Foundational Learning

- Concept: Importance-weighted estimation
  - Why needed here: To handle delayed and multi-user feedback without biasing the cumulative loss estimates
  - Quick check question: If an arm is chosen with probability 0.1 and receives loss 1 from a user, what is the importance-weighted estimate?
- Concept: Doubling trick for unknown horizons
  - Why needed here: To achieve sublinear regret when the terminal round $T$ is unknown
  - Quick check question: How does the doubling trick relate the number of missing feedback samples to epoch boundaries?
- Concept: Softmax probability updates
  - Why needed here: To balance exploration and exploitation in the adversarial setting
  - Quick check question: What happens to the probability of an arm if its estimated cumulative loss increases while others remain constant?

## Architecture Onboarding

- Component map: Feedback buffer -> Importance-weighted estimator -> Cumulative loss tracker -> Probability updater -> Arm selector
- Critical path: At each round: select arm → observe delayed feedback → update estimators → update cumulative losses → update probabilities → repeat
- Design tradeoffs: 
  - Memory vs accuracy: Larger feedback buffers improve loss estimation but increase memory usage
  - Exploration vs exploitation: Learning rate controls the trade-off, affecting both regret and convergence speed
  - Complexity vs robustness: More sophisticated estimators can handle edge cases but add computational overhead
- Failure signatures:
  - Exploding probabilities: Learning rate too high or feedback variance too large
  - Stuck in suboptimal arms: Insufficient exploration due to overly aggressive exploitation
  - Memory overflow: Excessive delays causing feedback buffer to grow unbounded
- First 3 experiments:
  1. Test with known $T$ and small delays to verify MUD-EXP3 outperforms baselines
  2. Test with unknown $T$ and moderate delays to verify AMUD-EXP3 maintains sublinear regret
  3. Test with extreme delays (approaching $T$) to stress-test the importance-weighted estimator stability

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations identified in the reproduction notes, the following questions are worth exploring:

### Open Question 1
- Question: What is the impact of varying the delay distribution (e.g., heavy-tailed, bursty) on the regret bounds and empirical performance of MUD-EXP3 and AMUD-EXP3?
- Basis in paper: [inferred] The paper assumes arbitrary delays but does not specify or analyze different delay distributions. It would be interesting to see how the algorithms perform under different delay distributions, especially heavy-tailed or bursty delays which are common in real-world scenarios.
- Why unresolved: The paper focuses on arbitrary delays but does not provide theoretical analysis or empirical results for specific delay distributions.
- What evidence would resolve it: Theoretical analysis of regret bounds under different delay distributions (e.g., exponential, heavy-tailed, bursty) and empirical evaluation of MUD-EXP3 and AMUD-EXP3 under these distributions.

### Open Question 2
- Question: How does the performance of MUD-EXP3 and AMUD-EXP3 compare to other algorithms (e.g., Bayesian methods, Thompson Sampling) in the multi-user delayed feedback setting?
- Basis in paper: [explicit] The paper compares MUD-EXP3 and AMUD-EXP3 to state-of-the-art baselines like D-UCB and SE, but does not consider other popular methods like Bayesian approaches or Thompson Sampling.
- Why unresolved: The paper focuses on comparing to UCB-based methods but does not explore the performance of other popular bandit algorithms in the multi-user delayed feedback setting.
- What evidence would resolve it: Empirical evaluation of MUD-EXP3 and AMUD-EXP3 against Bayesian methods and Thompson Sampling in the multi-user delayed feedback setting, comparing regret and other relevant metrics.

### Open Question 3
- Question: Can the MUD-EXP3 and AMUD-EXP3 algorithms be extended to handle contextual information or combinatorial action spaces?
- Basis in paper: [inferred] The paper focuses on the standard multi-armed bandit setting without contextual information or combinatorial actions. However, in many real-world applications, contextual information or combinatorial actions may be available.
- Why unresolved: The paper does not discuss how to extend the algorithms to handle contextual information or combinatorial action spaces, which are common in many real-world applications.
- What evidence would resolve it: Theoretical analysis and empirical evaluation of extended versions of MUD-EXP3 and AMUD-EXP3 that incorporate contextual information or handle combinatorial action spaces, comparing their performance to existing methods in these settings.

## Limitations
- The regret bounds assume a known maximum delay $d_{max}$, which may not be practical in real-world scenarios
- The algorithms require maintaining feedback buffers that could grow unbounded with increasing delays
- The theoretical analysis doesn't account for feedback arriving after the terminal round T
- The experimental section lacks details on hyperparameter tuning and specific adversarial environment configurations

## Confidence

The paper's main theoretical claims about regret bounds have **High confidence** due to the rigorous mathematical derivations provided for both MUD-EXP3 and AMUD-EXP3 algorithms. The importance-weighted estimator mechanism is **Medium confidence** as it relies on the assumption that delays are finite and feedback distributions are oblivious, though the paper doesn't extensively explore edge cases where these assumptions might break down. The experimental validation has **Low confidence** because specific implementation details, such as the exact loss distributions and transition intervals for the adversarial environments, are not provided. This makes direct reproduction challenging without making assumptions about these parameters.

## Next Checks

1. Implement the importance-weighted estimator and test its variance properties under different delay distributions, particularly focusing on heavy-tailed delay distributions
2. Create a controlled experiment with known T and extreme delays (approaching T) to verify the stability of the regret bounds
3. Reproduce the comparison experiments with state-of-the-art baselines, paying particular attention to the transition interval configurations and loss distributions used