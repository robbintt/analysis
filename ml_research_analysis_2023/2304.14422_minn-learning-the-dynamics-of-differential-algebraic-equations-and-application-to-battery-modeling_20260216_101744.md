---
ver: rpa2
title: 'MINN: Learning the dynamics of differential-algebraic equations and application
  to battery modeling'
arxiv_id: '2304.14422'
source_url: https://arxiv.org/abs/2304.14422
tags:
- battery
- minn
- system
- which
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new machine learning architecture called
  Model-Integrated Neural Networks (MINN) for learning the dynamics of general differential-algebraic
  equation (DAE) systems. The key idea is to integrate the physics-based governing
  equations directly into the neural network architecture through recurrent units.
---

# MINN: Learning the dynamics of differential-algebraic equations and application to battery modeling

## Quick Facts
- arXiv ID: 2304.14422
- Source URL: https://arxiv.org/abs/2304.14422
- Reference count: 40
- Primary result: MINN achieves 100x speedup over physics-based models while maintaining comparable accuracy for battery modeling

## Executive Summary
This paper introduces Model-Integrated Neural Networks (MINN), a new architecture that integrates physics-based governing equations directly into neural network recurrent units. By learning the dynamics of differential-algebraic equation (DAE) systems rather than just solution trajectories, MINN achieves exceptional data efficiency and generalization to unseen conditions. The authors demonstrate MINN on lithium-ion battery modeling, where it achieves accuracy comparable to high-fidelity physics-based models while reducing computation time by two orders of magnitude.

The key innovation is combining the approximation power of neural networks with the physical insights and numerical machinery from physics-based models. MINN preserves physical interpretability of hidden states while circumventing the computational bottlenecks of iterative root-finding for algebraic variables. This makes it particularly promising for real-time optimization and control applications where traditional physics-based models are too computationally expensive.

## Method Summary
MINN integrates physics-based DAE equations directly into recurrent neural network architecture. The model consists of physics-based recurrent units that encode the algebraic constraints, a deep neural network (GNN) that approximates algebraic variables from differential states and control inputs, and an ODE solver for time integration. The training procedure uses a loss function combining data-driven output error and physics-constrained algebraic equation residuals, optimized with ADAMW using automatic differentiation through the ODE solver. Training data is generated from high-fidelity P2D model simulations under controlled current profiles.

## Key Results
- MINN achieves two orders of magnitude reduction in solution time compared to high-fidelity P2D physics-based models
- Prediction accuracy comparable to first principle-based models for both system outputs and locally distributed electrochemical behaviors
- MINN is extremely data-efficient and generalizable to previously unseen input data due to underlying physical invariants
- Model preserves physical significance of hidden states while maintaining computational tractability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MINN achieves data efficiency by learning DAE dynamics instead of learning solution trajectories directly
- Mechanism: Architecture integrates physics-based governing equations into recurrent units, enabling extrapolation to unseen initial conditions and control inputs
- Core assumption: Underlying physical invariants captured by DAE structure remain valid across different operating conditions
- Evidence anchors: [abstract] "Owing to its underlying physical invariants"; [section] "MINN is extremely data-efficient to train while being sufficiently generalizable to previously unseen input data, owing to its underlying physical invariants."

### Mechanism 2
- Claim: MINN accelerates computation by avoiding iterative root-finding for algebraic variables
- Mechanism: Neural network learns explicit approximation function GNN that maps differential states and control inputs directly to algebraic variables
- Core assumption: Neural network can approximate algebraic variable function well enough that physical constraints are approximately satisfied
- Evidence anchors: [section] "The function GNN offers a shortcut to solving the implicit algebraic equations of the DAE system."

### Mechanism 3
- Claim: MINN preserves physical interpretability through architecture design
- Mechanism: Recurrent units continuously transform hidden states based on physics-based equations, maintaining physical meaning of internal states
- Core assumption: States in MINN can be directly mapped to physically meaningful electrochemical states
- Evidence anchors: [abstract] "retains the physical significance of hidden states and model parameters"

## Foundational Learning

- Concept: Differential-Algebraic Equations (DAEs)
  - Why needed here: Battery systems naturally lead to DAE formulations due to different time scales for ionic and electron transport
  - Quick check question: What distinguishes a DAE from a pure ODE system?

- Concept: Physics-Informed Neural Networks (PINNs)
  - Why needed here: Understanding PINN limitations helps appreciate why MINN's approach of integrating physics into architecture is superior for control applications
  - Quick check question: Why do PINNs struggle with time-varying inputs compared to MINN?

- Concept: Model Reduction Techniques
  - Why needed here: MINN represents a new paradigm in model reduction that preserves physics while reducing computational cost
  - Quick check question: How does MINN's approach to model reduction differ from traditional reduced-order modeling?

## Architecture Onboarding

- Component map: Input control → Physics-based recurrent unit → GNN approximation → ODE solver → Output computation
- Critical path: Control inputs flow through physics-based recurrent units, algebraic variables are approximated by GNN, ODE solver integrates dynamics, outputs are computed from final states
- Design tradeoffs: More physics integration vs. model flexibility; explicit algebraic variable approximation vs. iterative root-finding accuracy
- Failure signatures: Divergence in output predictions when initial conditions differ from training; NaN outputs due to physical constraint violations
- First 3 experiments:
  1. Train MINN on constant current discharge from 50% SOC, test on same current from 30% SOC
  2. Compare MINN vs. PINN accuracy on voltage prediction for time-varying inputs
  3. Measure computational speedup of MINN vs. full P2D model on dynamic driving cycles

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but identifies several limitations including the need for broader validation across different battery chemistries, more extensive testing under extreme operating conditions, and systematic evaluation of the approximation error in the GNN-based algebraic variable estimation.

## Limitations
- Training data fidelity depends on P2D model accuracy and discretization choices that are not fully specified
- Algebraic approximation via GNN trades computational speed for exactness without quantified error bounds
- Generalization claims are limited to variations within the same operating envelope, not extreme conditions
- Physical interpretability of hidden states asserted but not systematically validated

## Confidence

- High confidence: MINN achieves significant computational speedup (2 orders of magnitude) over P2D models
- Medium confidence: MINN is "extremely data-efficient" and "sufficiently generalizable" to unseen inputs within tested ranges
- Low confidence: MINN's internal states "retain physical significance" without systematic validation

## Next Checks

1. **Degradation simulation test**: Train MINN on fresh battery data, then evaluate performance on aged battery data with known capacity fade and resistance growth to test physical invariant validity under degradation.

2. **Cross-chemistry transferability**: Train MINN on one battery chemistry (e.g., NMC) and evaluate on a different chemistry (e.g., LFP) to assess adaptability beyond specific parameterizations.

3. **Real-world driving cycle validation**: Replace synthetic P2D training data with experimental data from actual vehicle driving cycles to validate robustness to real-world measurement noise and unmodeled dynamics.