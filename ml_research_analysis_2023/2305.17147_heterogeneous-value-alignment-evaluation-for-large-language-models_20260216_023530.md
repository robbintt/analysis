---
ver: rpa2
title: Heterogeneous Value Alignment Evaluation for Large Language Models
arxiv_id: '2305.17147'
source_url: https://arxiv.org/abs/2305.17147
tags:
- value
- values
- llms
- system
- rationality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for evaluating value alignment
  in large language models (LLMs) across heterogeneous value systems. The authors
  propose a concept of "value rationality" to measure the alignment between an agent's
  behavior and a target value, and develop an automated evaluation system (A2EHV)
  that uses the Social Value Orientation (SVO) framework to quantify this alignment.
---

# Heterogeneous Value Alignment Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2305.17147
- Source URL: https://arxiv.org/abs/2305.17147
- Authors: 
- Reference count: 18
- Key outcome: Introduces automated framework using SVO to evaluate LLM value alignment across four value orientations (altruistic, individualistic, prosocial, competitive)

## Executive Summary
This paper introduces a novel framework for evaluating value alignment in large language models (LLMs) across heterogeneous value systems. The authors propose a concept of "value rationality" to measure the alignment between an agent's behavior and a target value, and develop an automated evaluation system (A2EHV) that uses the Social Value Orientation (SVO) framework to quantify this alignment. The method is applied to evaluate eight mainstream LLMs across four distinct value orientations. Results show that LLMs generally exhibit stronger alignment with prosocial and altruistic values compared to individualistic and competitive values, and that larger models like GPT-4 demonstrate higher value rationality across all value types.

## Method Summary
The paper introduces an automated alignment evaluation system (A2EHV) that uses Social Value Orientation (SVO) to quantify agents' value rationality without human intervention. The framework employs a prompting method that induces LLMs to adopt a particular value orientation, then evaluates their behavior through SVO slider measure tasks. The system can also prompt LLMs to generate goals for specific tasks under their aligned value. The SVO framework maps behaviors to a unit circle where specific angles correspond to distinct value orientations (altruistic, prosocial, individualistic, competitive), transforming subjective value alignment into an objective, measurable quantity.

## Key Results
- LLMs exhibit stronger alignment with prosocial and altruistic values compared to individualistic and competitive values
- GPT-4 consistently outperforms other models across all value types, demonstrating higher value rationality
- Goal prompting can enhance value rationality for certain models, particularly those with extensive conversational training
- Smaller models (LLaMA-13B, Alpaca-13B) show varied performance across different value orientations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SVO-based value rationality metric enables automated, bias-minimized evaluation of LLMs across diverse value systems.
- Mechanism: SVO quantifies an agent's attitude toward self and others using a reward allocation task, mapping behaviors to a unit circle where specific angles correspond to distinct value orientations.
- Core assumption: SVO is a valid and stable psychological construct that generalizes from human decision-making to LLM behavior in structured reward allocation tasks.
- Evidence anchors:
  - [abstract] "We utilize social value orientation (SVO), which quantifies how much an agent cares about themselves and others from social psychology"
  - [section] "The SVO-based value rationality measurement system enables us to automatically assess the extent to which agents exhibit value rationality within any given value system"
  - [corpus] Weak evidence for cross-domain generalization; corpus neighbors show interest in pluralistic and cultural alignment

### Mechanism 2
- Claim: Goal prompting improves value rationality by aligning LLM decision-making with explicit self-constructed objectives under a given value system.
- Mechanism: By first prompting the model to generate a description of a person with the target value, then asking it to construct a task-specific goal that satisfies both self and others, the model forms a clearer internal reference frame for decision-making.
- Core assumption: LLMs can coherently reason about their own values and translate that into actionable goals that guide subsequent behavior.
- Evidence anchors:
  - [section] "comprehensive training with long conversations aids language models in understanding the impact of goals on results"
  - [section] "the ability of language models to autonomously identify suitable goals for specific problems becomes crucial"
  - [corpus] Related work on instruction-based alignment suggests goal grounding is an active research area

### Mechanism 3
- Claim: Larger models demonstrate higher value rationality due to greater reasoning and planning capacity.
- Mechanism: GPT-4 consistently outperforms smaller models across all value types, suggesting parameter count and architectural sophistication correlate with the ability to align behavior with abstract value concepts.
- Core assumption: Value rationality is a function of model capability, not just fine-tuning style or training corpus.
- Evidence anchors:
  - [section] "GPT-4 outperforms the other models across all values"
  - [section] "the correlation between value rationality and the model's parameter size may not be consistently positive" (caveat noted)
  - [corpus] Mixed evidence; corpus shows interest in both capability scaling and alignment fidelity

## Foundational Learning

- Concept: Social Value Orientation (SVO)
  - Why needed here: SVO provides the formal mapping from LLM decision behavior to interpretable value orientations, enabling quantitative evaluation without human judgment.
  - Quick check question: In the SVO framework, what angle range corresponds to a prosocial orientation?

- Concept: Value Rationality
  - Why needed here: It operationalizes the abstract notion of "alignment with values" into a concrete, measurable construct that can be optimized and compared across models.
  - Quick check question: How does value rationality differ from conventional economic rationality in the context of LLM evaluation?

- Concept: Prompt Chaining
  - Why needed here: The evaluation pipeline requires multi-stage prompting (value induction → goal construction → decision making) to elicit consistent value-aligned behavior.
  - Quick check question: What is the role of the self-constructed goal prompt in the chain, and why is it not simply replaced with a fixed objective?

## Architecture Onboarding

- Component map: Input -> Value induction module -> Goal construction module -> SVO evaluation module -> Answer extraction module -> Aggregation module
- Critical path: Value induction → Goal construction → Decision making → Answer extraction → SVO scoring
- Design tradeoffs: Web-based interaction vs. direct API use (simplicity vs. control); manual oversight vs. full automation (safety vs. scalability)
- Failure signatures: Inconsistent SVO scores across trials; model refusal to engage with value framing; answer extractor misclassification
- First 3 experiments:
  1. Run SVO evaluation on GPT-4 without goal prompting to establish baseline value rationality per orientation
  2. Enable goal prompting and compare SVO scores to baseline; verify improvement for prosocial and altruistic values
  3. Test same protocol on LLaMA-13B and Alpaca-13B and measure impact of fine-tuning on value rationality alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of parameters in LLMs affect their value rationality across different value orientations?
- Basis in paper: [explicit] The paper mentions that GPT-4 outperforms other models across all values, but the performance gap among the remaining LLMs is not substantial, suggesting that the correlation between value rationality and the model's parameter size may not be consistently positive.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between parameter size and value rationality for different value orientations.
- What evidence would resolve it: A comprehensive study comparing LLMs with varying parameter sizes across multiple value orientations would help establish a clearer relationship between parameter size and value rationality.

### Open Question 2
- Question: How do different fine-tuning methods impact the value rationality of LLMs across various value orientations?
- Basis in paper: [explicit] The paper discusses that Claude, fine-tuned using Constitutional AI, outperforms its competitors across various values, while GPT models, fine-tuned using RLHF, may exhibit a higher incidence of unsafe behavior.
- Why unresolved: The paper does not provide a detailed comparison of the effects of different fine-tuning methods on value rationality across different value orientations.
- What evidence would resolve it: A comparative study of LLMs fine-tuned using different methods (e.g., Constitutional AI, RLHF) across multiple value orientations would help determine the impact of fine-tuning methods on value rationality.

### Open Question 3
- Question: How does the planning and reasoning ability of LLMs affect their value rationality within a heterogeneous value system?
- Basis in paper: [explicit] The paper suggests that nearly all models excel in prosocial tasks while demonstrating weaker performance under competitive and altruistic values, which may stem from the fact that prosocial behavior is relatively straightforward to achieve compared to competitive or altruistic behavior.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between planning and reasoning abilities and value rationality for different value orientations.
- What evidence would resolve it: A study comparing LLMs with varying planning and reasoning capabilities across multiple value orientations would help establish the impact of these abilities on value rationality.

## Limitations

- The evaluation framework relies heavily on the validity of Social Value Orientation (SVO) as a psychological construct when applied to LLM decision-making, which remains unproven for non-human agents.
- The study's sample size of eight models, while covering major commercial and open-source options, may not capture the full diversity of LLM architectures and training approaches.
- The goal prompting mechanism's effectiveness varies significantly across models, suggesting that the approach may not be universally applicable.

## Confidence

- High confidence in the measurement methodology: The SVO-based framework provides a systematic and reproducible approach to quantifying value alignment, with clear mathematical foundations.
- Medium confidence in cross-model comparisons: While GPT-4 consistently shows higher value rationality, the relative performance of other models may be influenced by access methods (API vs. web demo) and specific fine-tuning approaches not fully controlled for.
- Low confidence in the goal prompting mechanism's general applicability: The inconsistent results across models suggest that the goal construction step may not reliably enhance value rationality, particularly for models without extensive conversational training.

## Next Checks

1. **Internal Consistency Validation**: Run multiple trials of the SVO evaluation for each model and value orientation to measure response consistency. Calculate intraclass correlation coefficients to determine if the SVO scores are stable across repeated measurements, which would strengthen confidence in the measurement approach.

2. **Human Judgment Validation**: Have human evaluators independently assess a subset of LLM responses to the SVO slider tasks, comparing human-assigned value orientations against the automated SVO calculations to validate the automated scoring system's accuracy.

3. **Cross-Cultural SVO Validation**: Test the SVO framework's applicability by evaluating models using culturally diverse value scenarios beyond the standard SVO slider tasks, examining whether the measurement approach generalizes across different value expression styles and contexts.