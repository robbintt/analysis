---
ver: rpa2
title: How connectivity structure shapes rich and lazy learning in neural circuits
arxiv_id: '2310.08513'
source_url: https://arxiv.org/abs/2310.08513
tags:
- learning
- neural
- figure
- initial
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how initial weight structure in neural
  circuits, particularly their effective rank, influences the learning regime (rich
  vs. lazy).
---

# How connectivity structure shapes rich and lazy learning in neural circuits

## Quick Facts
- arXiv ID: 2310.08513
- Source URL: https://arxiv.org/abs/2310.08513
- Reference count: 40
- Key outcome: Initial weight rank significantly influences learning regime, with high-rank structures leading to lazier learning and low-rank structures generally promoting richer learning, though task alignment can override this pattern.

## Executive Summary
This study reveals how the effective rank of initial weight matrices fundamentally shapes learning dynamics in neural circuits. Through theoretical analysis and numerical experiments, the research demonstrates that high-rank initializations typically result in minimal weight and representation changes during training (lazy learning), while low-rank initializations generally enable more substantial adaptations (rich learning). The findings show that this relationship holds across various cognitive tasks and architectures, including biologically inspired connectivity patterns. Notably, the study identifies an exception: when low-rank initializations align with task statistics, they can still produce lazy learning despite their low rank.

## Method Summary
The research employs both theoretical analysis of two-layer linear networks and numerical experiments with recurrent neural networks. For RNNs, the study uses a specific update formula with ReLU activation and trains models on cognitive tasks from Neurogym. Initial weight matrices are generated with controlled effective ranks using SVD truncation of Gaussian random matrices, maintaining constant Frobenius norm across conditions. Learning laziness is quantified through three metrics: weight change norm, representation alignment, and tangent kernel alignment. The experiments systematically vary initial weight rank while keeping other factors constant to isolate the effect on learning regime.

## Key Results
- High-rank initializations consistently produce smaller weight and representation changes, indicating lazier learning across all tested conditions
- Low-rank initializations generally bias learning toward richer regimes with greater network changes
- Biologically inspired connectivity structures with low effective rank tend to promote richer learning, aligning with empirical observations of neural plasticity
- Task alignment can override rank-based learning tendencies, with low-rank initializations aligned with task statistics producing lazy learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-rank initializations generally lead to lazier learning by minimizing the change in neural tangent kernel (NTK) during training.
- Mechanism: The NTK measures the sensitivity of network outputs to weight changes. When initial weights have a high effective rank (singular values spread across all dimensions), the NTK is less likely to change significantly during training, resulting in minimal weight and representation updates.
- Core assumption: The relationship between NTK movement and learning laziness holds across different network architectures and tasks.
- Evidence anchors:
  - [abstract]: "high-rank initializations typically yield smaller network changes indicative of lazier learning"
  - [section]: "high-rank initialization results in effectively lazier learning on average across tasks (Theorem 1)"
  - [corpus]: Weak evidence - only tangentially related papers on NTK and lazy learning are present, but none specifically address rank-structure relationships.
- Break condition: If the task statistics align closely with the low-rank structure of the initial weights, this mechanism can be overridden, leading to richer learning despite low rank.

### Mechanism 2
- Claim: Low-rank initializations bias learning towards richer learning by enabling more significant changes in network weights and representations.
- Mechanism: When initial weights have a low effective rank (concentrated singular values), the network has more capacity to adapt its weights and representations to task statistics, resulting in larger changes during training.
- Core assumption: The effective rank of initial weights directly influences the learning regime, independent of other factors like weight magnitude.
- Evidence anchors:
  - [abstract]: "low-rank initialization biases learning towards richer learning"
  - [section]: "low-rank initial weights result in effectively richer learning and greater network changes"
  - [corpus]: Weak evidence - no direct evidence from related papers, but the claim is supported by the theoretical derivation and numerical experiments.
- Break condition: If the low-rank initialization is already aligned with task statistics, this mechanism can be overridden, leading to lazier learning despite low rank.

### Mechanism 3
- Claim: Biologically inspired connectivity structures, which often exhibit low effective rank, can bias learning towards richer learning in neural circuits.
- Mechanism: Many biological neural circuits have connectivity patterns that result in low effective rank, which, according to the findings, biases learning towards richer learning. This suggests that biological circuits might be predisposed to richer learning regimes.
- Core assumption: The effective rank of initial weights is a key factor in determining the learning regime, and this principle applies to both artificial and biological neural networks.
- Evidence anchors:
  - [abstract]: "initial connection weight magnitude can significantly bias learning dynamics, pushing them towards either rich or lazy regimes"
  - [section]: "different low-rank biologically motivated structures can lead to effectively richer learning compared to the standard random Gaussian initialization"
  - [corpus]: Weak evidence - no direct evidence from related papers, but the claim is supported by the theoretical derivation and numerical experiments on RNNs.
- Break condition: If the biological circuit has evolved to preferentially learn specific tasks, the initial low-rank structure might already be aligned with task statistics, potentially leading to lazier learning despite the low rank.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The NTK is a key measure used to quantify the effective laziness of learning. Understanding NTK helps in grasping how initial weight structures influence learning dynamics.
  - Quick check question: What does a stationary NTK during training indicate about the learning regime?

- Concept: Effective Rank of Weight Matrices
  - Why needed here: The effective rank of initial weight matrices is the central focus of the study. It determines whether learning tends to be rich or lazy.
  - Quick check question: How does the distribution of singular values in a weight matrix relate to its effective rank?

- Concept: Rich vs. Lazy Learning Regimes
  - Why needed here: These concepts are fundamental to understanding the study's findings. Rich learning involves significant changes in weights and representations, while lazy learning involves minimal changes.
  - Quick check question: What are the key differences between rich and lazy learning in terms of weight and representation changes?

## Architecture Onboarding

- Component map: Input layer -> Hidden layer(s) with recurrent connections -> Output layer
- Critical path:
  1. Initialize weight matrices with desired effective rank (high for lazy, low for rich)
  2. Feed input data through the network, updating hidden states using recurrent connections
  3. Compute output and loss function
  4. Backpropagate gradients and update weights using gradient descent
  5. Measure effective laziness using NTK alignment, representation alignment, and weight change norm
- Design tradeoffs:
  - High-rank initialization: Less computational cost due to smaller weight changes, but may not adapt as well to specific tasks
  - Low-rank initialization: More computational cost due to larger weight changes, but may adapt better to specific tasks if not already aligned
  - Biologically inspired structures: May provide a balance between adaptability and efficiency, but require careful design and validation
- Failure signatures:
  - Poor learning performance despite appropriate rank initialization
  - Unexpected shifts in learning regime (e.g., lazy learning with low-rank initialization)
  - Instability in training due to improper rank initialization or network architecture
- First 3 experiments:
  1. Replicate the main findings using a simple RNN architecture on a basic cognitive task (e.g., 2AF)
  2. Vary the effective rank of initial weights systematically and observe the impact on learning regime
  3. Test the influence of biologically inspired connectivity structures on learning regime in RNNs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different biologically motivated connectivity structures (beyond those tested) influence learning dynamics?
- Basis in paper: [explicit] The paper tests several biologically motivated structures (cell-type-specific, EM data-derived, Dale's law-obeying, chain motif over-representation) but acknowledges that future work will broaden scope to other weight characteristics as connectivity data becomes available.
- Why unresolved: The paper only tests a limited set of biologically motivated structures, and the full range of possible structures in biological neural circuits is vast and largely unexplored.
- What evidence would resolve it: Systematic testing of learning dynamics across a comprehensive set of biologically realistic connectivity structures, comparing their effective rank and impact on learning regimes.

### Open Question 2
- Question: How does effective learning regime impact learning speed and generalization capabilities?
- Basis in paper: [inferred] The paper mentions that the ramifications of effective learning regimes on learning speed and generalization remain unexplored, despite mixed findings in existing literature.
- Why unresolved: While the paper establishes the impact of initial weight structure on effective learning regimes, it does not investigate how these regimes affect the speed of learning or the network's ability to generalize to new tasks.
- What evidence would resolve it: Empirical studies comparing learning speed and generalization performance across different effective learning regimes, potentially using metrics like convergence rate and transfer learning ability.

### Open Question 3
- Question: How do biologically plausible learning rules affect the relationship between initial weight rank and effective learning regime?
- Basis in paper: [explicit] The paper notes that while it uses gradient descent via backpropagation, the implications when applied to biologically plausible learning rules are yet to be explored.
- Why unresolved: The paper's theoretical and empirical findings are based on backpropagation, which is not biologically plausible. The extent to which these findings hold for biologically plausible learning rules is unknown.
- What evidence would resolve it: Simulations and theoretical analysis of learning dynamics using biologically plausible learning rules (e.g., local learning rules) to determine if the relationship between initial weight rank and effective learning regime persists.

## Limitations

- Theoretical analysis relies on simplifying assumptions about network linearity and task structure that may not hold in more complex settings
- Empirical validation is limited to specific RNN architectures and cognitive tasks, leaving questions about performance in deeper networks or different domains
- The complexity of real neural systems introduces additional factors not captured in the models, limiting direct applicability to biological circuits

## Confidence

- **High**: The core relationship between high-rank initialization and lazy learning (Theorem 1) - supported by both theoretical proof and consistent numerical results across all tested conditions
- **Medium**: The exception for low-rank initializations aligned with task statistics - theoretically sound but requires more diverse task structures for full validation
- **Low**: The direct applicability to biological neural circuits - while biologically inspired structures are tested, the complexity of real neural systems introduces additional factors not captured in the models

## Next Checks

1. Test the rank-learning regime relationship on deeper feedforward networks and transformer architectures to assess architectural generality beyond RNNs
2. Evaluate performance across a broader range of task statistics and alignment scenarios to better characterize the conditions under which the low-rank exception applies
3. Conduct ablation studies on the biological connectivity parameters (α, γ, τchn) to determine their individual contributions to the learning regime outcomes