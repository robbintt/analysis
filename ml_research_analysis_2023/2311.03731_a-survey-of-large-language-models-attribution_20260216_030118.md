---
ver: rpa2
title: A Survey of Large Language Models Attribution
arxiv_id: '2311.03731'
source_url: https://arxiv.org/abs/2311.03731
tags:
- attribution
- language
- answer
- text
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper surveys attribution mechanisms in large language models\
  \ (LLMs) used for open-domain generative systems. Attribution\u2014providing sources\
  \ for model-generated content\u2014addresses the \"hallucination\" problem of fabricated\
  \ or inaccurate responses by enhancing factuality and verifiability."
---

# A Survey of Large Language Models Attribution

## Quick Facts
- arXiv ID: 2311.03731
- Source URL: https://arxiv.org/abs/2311.03731
- Reference count: 18
- Primary result: Comprehensive survey of attribution mechanisms in LLMs addressing hallucination through three approaches: direct model-driven, post-retrieval answering, and post-generation attribution

## Executive Summary
This survey paper examines attribution mechanisms in large language models used for open-domain generative systems, focusing on how to provide sources for model-generated content to address the "hallucination" problem. The authors categorize attribution into three approaches: direct model-driven (where models self-attribute), post-retrieval answering (using retrieved external documents to generate answers), and post-generation attribution (adding citations after answer generation). The paper analyzes sources of attribution including pre-training data and external knowledge, reviews relevant datasets, and discusses evaluation methods such as correctness, precision, and recall. Key limitations identified include ambiguity in when to cite, potential for inaccurate attributions, outdated information, over-attribution, and bias.

## Method Summary
This paper is a survey of existing research on attribution mechanisms in large language models. The authors systematically reviewed literature to identify three main attribution approaches, analyzed their sources and evaluation methods, and synthesized findings on current limitations and future directions. No new experimental methodology or dataset is introduced, as the work focuses on synthesizing and categorizing existing research on how LLMs can provide verifiable sources for their generated content.

## Key Results
- Attribution mechanisms can be categorized into three approaches: direct model-driven, post-retrieval answering, and post-generation attribution
- Attribution sources include both pre-training data and external knowledge retrieved during inference
- Evaluation methods focus on correctness, precision, and recall metrics, with challenges in balancing these competing requirements
- Current limitations include ambiguity in citation timing, potential for inaccurate attributions, and difficulty distinguishing between general and specialized knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-retrieval answering reduces hallucination by explicitly grounding model responses in retrieved external documents.
- Mechanism: The model first retrieves relevant documents using information retrieval, then generates answers conditioned on those retrieved documents rather than relying solely on parametric knowledge.
- Core assumption: Retrieved documents contain relevant, accurate information that can guide the model toward factual responses.
- Evidence anchors:
  - [abstract] "Post-retrieval answering (using retrieved external documents to generate answers)"
  - [section] "Several research papers have investigated the post-retrieval answering approach for attribution (Chen et al., 2017; Lee et al., 2019; Khattab and Zaharia, 2020)."
  - [corpus] Weak evidence - only 5/25 related papers, average neighbor FMR=0.473, indicating moderate relevance.
- Break condition: When retrieved documents are irrelevant, contradictory, or insufficient to answer the query, the model may still hallucinate or provide incomplete answers.

### Mechanism 2
- Claim: Post-generation attribution allows fact verification and editing without accessing model parameters.
- Mechanism: The system first generates an answer, then retrieves documents based on the question and generated answer, using these to verify facts and perform post-editing if necessary.
- Core assumption: Retrieved evidence can identify and correct hallucinated or unsupported content in the initial generation.
- Evidence anchors:
  - [abstract] "Post-generation attribution (adding citations after answer generation)"
  - [section] "RARR (Gao et al., 2023a) autonomously identifies the attribution for the output of any text generation model, and performs post-editing to rectify unsupported content"
  - [corpus] Weak evidence - corpus neighbors show only moderate similarity, suggesting this mechanism may not be well-represented in related literature.
- Break condition: When the initial generation contains fundamental errors or the retrieved evidence is inadequate, post-generation editing may not sufficiently correct the response.

### Mechanism 3
- Claim: Direct model-driven attribution can improve truthfulness by asking models to self-attribute and self-detect hallucinations.
- Mechanism: The model itself provides attribution for its answers, potentially by accessing its own knowledge base or reasoning about which sources informed its response.
- Core assumption: Models can accurately identify which parts of their knowledge base informed specific responses.
- Evidence anchors:
  - [abstract] "Direct model-driven (where models self-attribute)"
  - [section] "Attribution from parametric knowledge can help reduce hallucination and improve the truthfulness of generated text. By asking models to do self-detection and self-attribution..."
  - [corpus] Weak evidence - the corpus shows limited direct discussion of this specific mechanism.
- Break condition: When models cannot distinguish between different sources of knowledge or when parametric knowledge is inaccurate, self-attribution may perpetuate errors rather than correct them.

## Foundational Learning

- Concept: Information retrieval fundamentals
  - Why needed here: Understanding how documents are retrieved and ranked is crucial for evaluating the effectiveness of post-retrieval and post-generation attribution approaches
  - Quick check question: What is the difference between sparse retrieval (e.g., BM25) and dense retrieval (e.g., DPR) and how might each affect attribution quality?

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI models are used to evaluate whether statements are supported by citations, which is central to attribution evaluation
  - Quick check question: How do NLI models determine entailment, contradiction, and neutral relationships between premises and hypotheses?

- Concept: Evaluation metrics for generation tasks
  - Why needed here: Attribution requires understanding various metrics like precision, recall, and F1-score to assess the quality of citations
  - Quick check question: What is the difference between precision and recall in the context of attribution, and why are both important?

## Architecture Onboarding

- Component map: Query processor → Retrieval engine → Answer generator → Attribution module → Evaluation component
  - Additional path: Query processor → Answer generator → Citation verifier → Post-editor

- Critical path:
  - For post-retrieval: Query → Retrieval → Generation → Attribution
  - For post-generation: Query → Generation → Retrieval → Attribution → Post-editing

- Design tradeoffs:
  - Real-time vs. batch retrieval: Real-time provides fresher information but increases latency
  - Granularity of attribution: Sentence-level attribution provides more precision but requires more complex processing
  - Source diversity vs. reliability: Including diverse sources may improve coverage but could introduce unreliable information

- Failure signatures:
  - Low precision in attribution: Model cites irrelevant sources
  - Low recall in attribution: Model fails to cite relevant sources
  - Attribution latency: Post-generation approaches significantly increase response time
  - Citation quality degradation: Over-reliance on retrieval leads to incoherent citations

- First 3 experiments:
  1. Implement basic post-retrieval attribution with a simple IR system and evaluate precision/recall of citations
  2. Add post-generation attribution with a fact-checking component to identify and correct hallucinated content
  3. Compare direct model-driven attribution versus retrieval-augmented approaches on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal evaluation metrics for attribution quality that balance precision and recall requirements?
- Basis in paper: [explicit] The paper discusses "Comprehensive attribution or citation (high recall)" and "Sufficiency attribution or citation (high precision)" as core requirements, and mentions evaluation metrics like correctness, precision, and recall, but notes challenges in measuring these effectively.
- Why unresolved: The paper identifies the need for both high precision and high recall in attribution but doesn't provide concrete guidance on how to balance these competing requirements or which specific metrics should be prioritized in different contexts.
- What evidence would resolve it: Empirical studies comparing different evaluation metrics across various attribution scenarios, demonstrating which combinations best capture both precision and recall requirements while remaining practical to implement.

### Open Question 2
- Question: How can LLMs effectively distinguish between general knowledge that doesn't require attribution and specialized knowledge that should be cited?
- Basis in paper: [explicit] The paper discusses "One primary challenge is discerning when and how to attribute" and mentions the difficulty in differentiating between "general knowledge, which may not require citations, and specialized knowledge, which should ideally be attributed."
- Why unresolved: The paper identifies this as a key limitation but doesn't propose specific methods for determining attribution thresholds or mechanisms for LLMs to make this distinction reliably.
- What evidence would resolve it: A taxonomy of knowledge types with clear attribution criteria, validated through human evaluations and implemented in LLM systems that can consistently apply these rules.

### Open Question 3
- Question: What architectural modifications could enable LLMs to attribute parameter knowledge to its sources?
- Basis in paper: [explicit] The paper states "And LLMs now do not have ability to attribute parameter knowledge of itself" and identifies this as a current limitation of attribution systems.
- Why unresolved: While the paper acknowledges this limitation, it doesn't explore technical approaches for enabling parameter-level attribution or discuss whether this is theoretically feasible given how LLMs store knowledge.
- What evidence would resolve it: Proposed architectural modifications or training techniques that would allow LLMs to maintain and report the provenance of their parameter-derived knowledge, validated through experimental results showing improved attribution accuracy.

## Limitations

- Limited empirical validation: As a survey paper, the work relies on synthesizing existing research rather than presenting new experimental results, with confidence depending on the quality of cited studies
- Generalizability concerns: The analysis may not fully capture the rapidly evolving landscape of LLM systems and could oversimplify nuanced differences between implementations
- Evaluation methodology gaps: While evaluation metrics are discussed, their effectiveness in distinguishing between high-quality and low-quality attribution systems is not comprehensively validated

## Confidence

**High confidence**: The fundamental problem definition (addressing hallucination through attribution) and the basic categorization of attribution approaches are well-established concepts with multiple supporting citations.

**Medium confidence**: Claims about the effectiveness of specific attribution mechanisms and their relative advantages are supported by references but would benefit from systematic comparative studies.

**Low confidence**: Predictions about future directions (continuous model refreshment, balancing creativity with attribution) are speculative and not grounded in current empirical evidence.

## Next Checks

1. **Empirical comparison study**: Conduct systematic experiments comparing all three attribution approaches (direct model-driven, post-retrieval, post-generation) on the same dataset using standardized evaluation metrics to validate the relative effectiveness claims.

2. **Metric validation**: Perform controlled experiments to test whether current evaluation metrics (precision, recall, F1-score, QUIP-Score) actually correlate with human judgments of attribution quality and usefulness in practical applications.

3. **Cross-domain generalization**: Test the attribution approaches across different domains (technical, creative, factual) to validate whether the survey's conclusions about effectiveness and limitations hold across diverse use cases.