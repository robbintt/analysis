---
ver: rpa2
title: 'Taken by Surprise: Contrast effect for Similarity Scores'
arxiv_id: '2308.09765'
source_url: https://arxiv.org/abs/2308.09765
tags:
- score
- similarity
- surprise
- cosine
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the "surprise score," a context-aware similarity
  metric that improves upon traditional pairwise similarity measures like cosine similarity.
  The key idea is to quantify the surprise of observing a given similarity between
  two objects relative to the distribution of similarities across the ensemble they
  belong to.
---

# Taken by Surprise: Contrast effect for Similarity Scores

## Quick Facts
- arXiv ID: 2308.09765
- Source URL: https://arxiv.org/abs/2308.09765
- Authors: 
- Reference count: 25
- Key outcome: Surprise score improves zero- and few-shot text classification by 10-15% over cosine similarity by incorporating context-aware contrast effects.

## Executive Summary
This paper introduces the "surprise score," a context-aware similarity metric that quantifies the surprise of observing a given similarity between two objects relative to the distribution of similarities across their ensemble. The method improves upon traditional pairwise similarity measures like cosine similarity by capturing the contrast effect in human perception. Evaluated on zero- and few-shot text classification tasks and document clustering, the surprise score demonstrates 10-15% better performance compared to raw cosine similarity. The approach is flexible and can be applied to various similarity scores and tasks beyond text.

## Method Summary
The surprise score is computed by estimating the probability that the similarity between a key and a query exceeds that between the query and a random ensemble element. This is achieved by modeling the distribution of pairwise similarities within an ensemble as Gaussian, then calculating the cumulative probability for the observed similarity. For few-shot learning, a focal loss with binary classification is employed to handle label imbalance. The method includes a dynamic interpolation between plain cosine and surprise scores based on ensemble size, using a tanh function to smoothly transition from one to the other.

## Key Results
- 10-15% improvement in zero-shot classification accuracy over cosine similarity
- Surprise score consistently outperforms cosine similarity across all benchmark datasets
- Dynamic interpolation strategy effectively handles small ensemble sizes
- Focal loss with binary classification improves few-shot learning performance

## Why This Works (Mechanism)

### Mechanism 1
The surprise score improves classification by modeling the distribution of pairwise similarities within an ensemble, thereby incorporating context-aware contrast effects. It calculates the probability that the similarity between a key and a query exceeds that between the query and a random ensemble element. This effectively normalizes the similarity by the ensemble's distribution, capturing how unusual or expected a similarity is in context. The core assumption is that the distribution of cosine similarities between queries and ensemble elements is approximately Gaussian.

### Mechanism 2
By dynamically adjusting the interpolation between plain cosine and surprise scores based on ensemble size, the method remains robust for small datasets. The surprise weight w is a function of |E|/Ncross, using a tanh function to smoothly transition from pure cosine (w=0) to pure surprise (w=1) as the ensemble grows. This assumes that larger ensembles provide more reliable estimates of the similarity distribution, justifying full use of the surprise score.

### Mechanism 3
Using a focal loss with binary classification improves few-shot learning by focusing training on hard, misclassified pairs. The focal loss down-weights well-classified examples and emphasizes difficult ones, which is crucial when the number of negative pairs dominates due to large label sets. This assumes that the imbalance between positive and negative pairs in few-shot settings necessitates a modified loss to avoid skewing predictions toward negatives.

## Foundational Learning

- Concept: Gaussian distribution modeling of similarity scores
  - Why needed here: The surprise score relies on estimating mean and standard deviation of similarity scores within an ensemble; assuming Gaussianity enables analytic computation of the probability.
  - Quick check question: If you sample cosine similarities from a large corpus, what shape does the histogram typically approximate?

- Concept: Contrast effect in human perception
  - Why needed here: The motivation for the surprise score is to mimic how humans adjust similarity judgments based on context, which is the contrast effect.
  - Quick check question: In the example with "dog" and "Alsatian," why does "Alsatian" imply "dog" more strongly than vice versa?

- Concept: Focal loss in imbalanced classification
  - Why needed here: Few-shot scenarios often have many more negative than positive pairs; focal loss mitigates this imbalance by focusing learning on hard examples.
  - Quick check question: What happens to the loss contribution of well-classified examples when gamma in focal loss is set to 1?

## Architecture Onboarding

- Component map: Sentence transformer model → cosine similarity computation → surprise score module (distribution estimation + interpolation) → classification head (argmax over labels) → evaluation metrics
- Critical path: Embeddings → cosine similarity → surprise score calculation → label prediction. Bottlenecks occur in distribution estimation for large ensembles.
- Design tradeoffs: Surprise score requires additional computation to model the ensemble distribution, but yields better context-aware classification. Pure cosine is faster but ignores context.
- Failure signatures: Surprise score underperforms if ensemble size is too small (insufficient distribution data) or if similarity distribution is non-Gaussian (e.g., heavy tails).
- First 3 experiments:
  1. Run zero-shot classification on AG News with sentence-t5-base embeddings using both cosine and surprise scores; compare accuracy.
  2. Vary ensemble size from 3 to 2187 and measure the ratio of F1(cosine)/F1(surprise) to observe the crossover point.
  3. Implement focal loss with ϵ=0.05 on few-shot Amazon Reviews; compare F1 against standard cross-entropy.

## Open Questions the Paper Calls Out

### Open Question 1
How does the surprise score perform on the BEIR dataset for document retrieval tasks, given its known imbalance in statistical properties? The authors explicitly mention that the BEIR dataset has highly unbalanced statistical properties, which they expect to negatively affect the surprise score's performance, but they leave this investigation for future research.

### Open Question 2
How does the surprise score's performance vary when using different plain similarity scores (e.g., Euclidean or Manhattan distances) as the base for the surprise calculation? The authors state that they have only studied the surprise similarity score modeled over cosine similarity and suggest that it would be interesting to analyze whether the advantages persist for other plain similarity scores.

### Open Question 3
What is the optimal choice of ensemble size (Ncross) for the surprise score in different types of tasks (e.g., classification vs. clustering)? The authors use Ncross = 1000 for their few-shot experiments and mention that the surprise score reverts to plain similarity for small ensembles, but they do not provide a systematic study of optimal Ncross values.

### Open Question 4
How sensitive is the surprise score to the choice of statistical model for the ensemble distribution (e.g., Gaussian vs. other distributions)? The authors model the ensemble similarity distribution as Gaussian and mention that for small ensembles or non-Gaussian distributions, using p-50 and p-84.14 values might be advantageous, but they do not systematically compare different statistical models.

## Limitations
- Gaussian assumption for similarity distributions may not hold for all datasets
- Implementation details for dynamic interpolation are underspecified
- Evaluation focuses primarily on text classification, limiting generalizability
- No ablation studies for focal loss contribution in few-shot learning

## Confidence

High confidence: Core mathematical formulation of surprise score, clustering and classification improvements over cosine similarity

Medium confidence: Practical implementation details, ensemble size interpolation strategy, focal loss parameters

Low confidence: Generalizability beyond text classification and clustering domains

## Next Checks

1. **Distribution validation**: Sample cosine similarities from the same datasets and empirically test the Gaussian assumption by plotting histograms and conducting normality tests.

2. **Ablation study**: Compare surprise score performance with and without the focal loss component on few-shot tasks to isolate its contribution.

3. **Ensemble size sensitivity**: Systematically vary ensemble sizes below and above the proposed Ncross threshold to quantify the impact on classification accuracy and determine optimal transition points.