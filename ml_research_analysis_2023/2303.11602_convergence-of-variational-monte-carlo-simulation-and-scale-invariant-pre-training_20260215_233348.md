---
ver: rpa2
title: Convergence of variational Monte Carlo simulation and scale-invariant pre-training
arxiv_id: '2303.11602'
source_url: https://arxiv.org/abs/2303.11602
tags:
- gradient
- convergence
- algorithm
- monte
- carlo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the convergence of stochastic gradient descent
  (SGD) for optimizing neural network wave functions in variational Monte Carlo (VMC)
  simulations of quantum systems. The key challenge is that the wave function belongs
  to a high-dimensional sphere parameterized by a neural network, but the normalization
  constant is intractable.
---

# Convergence of variational Monte Carlo simulation and scale-invariant pre-training

## Quick Facts
- arXiv ID: 2303.11602
- Source URL: https://arxiv.org/abs/2303.11602
- Reference count: 40
- One-line primary result: This paper proves convergence guarantees for SGD in variational Monte Carlo simulations by introducing directionally unbiased gradient estimators and scale-invariant losses.

## Executive Summary
This paper addresses the convergence challenges in optimizing neural network wave functions for variational Monte Carlo (VMC) simulations of quantum systems. The key insight is that the wave function belongs to a high-dimensional sphere parameterized by a neural network, but the normalization constant is intractable. The authors propose a new directionally unbiased gradient estimator for supervised learning and prove its convergence rate matches classical SGD. For VMC, they prove convergence of SGD with a new unbiased gradient estimator. Empirically, the scale-invariant loss and directionally unbiased estimator lead to faster convergence compared to non-invariant alternatives.

## Method Summary
The method transforms the optimization problem over normalized functions on a unit sphere into an unconstrained problem using scale-invariant losses. For supervised learning, the directionally unbiased gradient estimator balances the normalization constant across terms to ensure the expected gradient aligns with the true gradient direction. For VMC, the method uses periodic estimation of the normalization constant as a preconditioner, combined with an unbiased gradient estimator based on local energy calculations. Both settings employ SGD with learning rate schedules and leverage Monte Carlo sampling for expectation evaluation.

## Key Results
- First convergence guarantees for SGD in VMC simulations of quantum systems
- Directionally unbiased gradient estimator achieves matching convergence rate to classical SGD
- Scale-invariant loss functions enable optimization on the unit sphere without explicit normalization tracking
- Empirical validation on H4 model demonstrates faster convergence compared to non-invariant alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The directionally unbiased gradient estimator enables SGD convergence by ensuring the expected gradient direction aligns with the true gradient direction, even without knowing the exact normalization constant.
- Mechanism: By balancing the normalization constant in each term of the gradient estimator (Equation 9), the expectation of the gradient estimator becomes a positive multiple of the true gradient. This removes the bias present in naive plug-in estimators.
- Core assumption: The gradient estimator can be constructed such that its expectation is parallel to the true gradient.
- Evidence anchors:
  - [abstract]: "We propose a new directionally unbiased gradient estimator for supervised learning and prove its convergence rate matches classical SGD."
  - [section]: "In fact, Figure 3 demonstrates that when using this gradient estimate in the SGD algorithm, fθ fails to converge to the target. The reason for this failure appears to be the fact it is unbalanced in the sense that each term has a different power of 1/ ˜Z."
  - [corpus]: Weak evidence. No direct mentions of "directionally unbiased gradient estimator" or related terms in the corpus neighbors.
- Break condition: If the normalization constant estimation becomes too inaccurate (ratio of true to estimated norm deviates significantly), the bias could reappear and break convergence.

### Mechanism 2
- Claim: Scale-invariance of the loss function removes the need to explicitly track normalization constants, enabling optimization on the unit sphere.
- Mechanism: By transforming the optimization problem over normalized functions into one over unconstrained functions using scale-invariant loss (Equation 1), the algorithm avoids the need to project back to the sphere or compute normalization constants at each step.
- Core assumption: The loss function is truly scale-invariant (L(λf) = L(f) for any λ > 0).
- Evidence anchors:
  - [abstract]: "We instead transform the optimization problem over the sphere of normalized functions into one where fθ is unconstrained in H. A loss function on the sphere then corresponds to a scale-invariant loss L : H → R, meaning L(λf) = L(f) for any λ > 0."
  - [section]: "We instead transform the optimization problem over the sphere of normalized functions into one where fθ is unconstrained in H. A loss function on the sphere then corresponds to a scale-invariant loss L : H → R, meaning L(λf) = L(f) for any λ > 0."
  - [corpus]: Weak evidence. The corpus neighbors don't discuss scale-invariant loss functions in the context of quantum Monte Carlo.
- Break condition: If the loss function loses its scale-invariance property (e.g., due to numerical errors or approximations), the optimization would no longer correctly target the unit sphere.

### Mechanism 3
- Claim: The Lipschitz continuity of the gradient ensures stable SGD updates and enables convergence guarantees.
- Mechanism: By establishing bounds on the Lipschitz constant of the gradient (Equation 33 in Lemma C.1), the algorithm ensures that gradient updates remain stable and don't explode, allowing the SGD to converge to a stationary point.
- Core assumption: The gradient of the loss function is Lipschitz continuous with a bounded constant.
- Evidence anchors:
  - [section]: "We establish new bounds on the Lipschitz constant of ∇L and the variance of the gradient estimator (see Lemma C.1), which is a key step in proving the convergence of our method."
  - [section]: "We establish new bounds on the Lipschitz constant of ∇L and the variance of the gradient estimator (see Lemma C.1), subject to the constraints specified in Assumption 3.2."
  - [corpus]: No direct evidence in corpus neighbors about Lipschitz continuity in quantum Monte Carlo contexts.
- Break condition: If the gradient becomes too non-smooth (Lipschitz constant becomes unbounded), the SGD updates could become unstable and convergence would fail.

## Foundational Learning

- Concept: Scale-invariant loss functions
  - Why needed here: The core optimization problem involves finding the minimum of a function on the unit sphere, which requires comparing functions that differ only by a normalization constant. Scale-invariant losses allow us to optimize without explicitly tracking these constants.
  - Quick check question: Given f(x) = 2x and g(x) = 3x, would a scale-invariant loss assign them the same value? (Yes)

- Concept: Riemannian optimization on manifolds
  - Why needed here: The optimization occurs on a high-dimensional sphere (the space of normalized functions), which is a Riemannian manifold. Understanding how SGD works on manifolds is crucial for grasping the convergence proofs.
  - Quick check question: What's the key difference between standard SGD and SGD on Riemannian manifolds? (Projection or exponential map to stay on the manifold)

- Concept: Variance reduction in stochastic optimization
  - Why needed here: The paper uses a technique where the normalization constant is estimated once per epoch rather than every step, which is a form of variance reduction that improves convergence.
  - Quick check question: Why might estimating the normalization constant less frequently improve convergence? (Reduces variance in gradient estimates by using a more stable preconditioner)

## Architecture Onboarding

- Component map:
  - Loss function (scale-invariant)
  - Gradient estimator (directionally unbiased)
  - Normalization constant estimator (periodic)
  - SGD optimizer with learning rate schedule
  - Monte Carlo sampling for expectation evaluation

- Critical path: Loss function → Gradient computation → Normalization estimation → SGD update → Sampling for next iteration

- Design tradeoffs:
  - Frequency of normalization constant estimation vs. accuracy of gradient estimates
  - Batch size for gradient estimation vs. computational cost
  - Learning rate schedule (constant vs. decaying) vs. convergence speed

- Failure signatures:
  - Loss plateauing or diverging indicates gradient estimator bias
  - Oscillations in parameter space suggest learning rate too high
  - Extremely slow convergence may indicate insufficient sampling accuracy

- First 3 experiments:
  1. Implement the directionally unbiased gradient estimator and verify it's unbiased by checking E[G] is parallel to ∇L
  2. Test convergence with different frequencies of normalization constant estimation (N parameter)
  3. Compare convergence rates using scale-invariant vs. non-invariant losses on a simple quantum system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of SGD in VMC simulations compare to more sophisticated optimization methods like stochastic reconfiguration or the linear method?
- Basis in paper: [explicit] The authors mention that historically, methods like stochastic reconfiguration and the linear method were preferred for parameter optimization in VMC simulations with smaller parameter sets, but that first-order methods are usually the only practical option for neural quantum states due to their complexity and large number of parameters.
- Why unresolved: The paper focuses on proving convergence for SGD in VMC simulations but does not compare its performance to other optimization methods.
- What evidence would resolve it: Empirical comparison of SGD convergence rate to other VMC optimization methods on benchmark quantum systems.

### Open Question 2
- Question: Can the convergence guarantees for SGD in VMC simulations be extended to excited states of quantum systems?
- Basis in paper: [inferred] The authors mention that generalizing their work from a high dimensional sphere to a Grassmann manifold parameterized by a neural network up to a gauge matrix could be applicable to VMC simulations of excited states.
- Why unresolved: The current analysis focuses on minimizing energy, which corresponds to finding the ground state. Extending to excited states would require optimizing over a different manifold.
- What evidence would resolve it: Extension of the SGD convergence analysis to the Grassmann manifold setting for excited state optimization.

### Open Question 3
- Question: How does the choice of preconditioner for the normalization constant affect the convergence of SGD in the supervised learning setting?
- Basis in paper: [explicit] The authors use an estimate of the normalization constant calculated once per epoch as a preconditioner in their algorithm, and state that this is reasonable only when the preconditioner is not too far from the true value.
- Why unresolved: While the authors prove convergence under certain assumptions on the preconditioner, they do not explore how different choices of preconditioner affect the convergence rate.
- What evidence would resolve it: Empirical study of SGD convergence with different preconditioning strategies for the normalization constant.

## Limitations

- The convergence guarantees rely on technical assumptions about Lipschitz continuity and variance bounds that may be difficult to verify in practice for complex neural network architectures.
- Empirical validation is limited to a single model (H4), raising questions about generalizability to more complex quantum systems.
- The paper doesn't compare the proposed SGD method's performance against other VMC optimization approaches like stochastic reconfiguration or the linear method.

## Confidence

- **High confidence**: The theoretical framework for directionally unbiased gradient estimators is well-established, and the connection between scale-invariant losses and optimization on the unit sphere is mathematically rigorous.
- **Medium confidence**: The convergence proofs rely on several technical assumptions about gradient smoothness and variance bounds that may not hold for all practical implementations.
- **Low confidence**: The empirical results are limited in scope, and the performance advantages of the proposed method over existing approaches in practical VMC simulations remain to be thoroughly demonstrated.

## Next Checks

1. **Convergence robustness test**: Implement the SGD algorithm with the directionally unbiased gradient estimator on multiple quantum systems beyond H4, including both molecular Hamiltonians and lattice models, to verify the convergence guarantees hold across different physical regimes.

2. **Estimator bias verification**: Conduct systematic experiments measuring the actual bias in the gradient estimator by comparing its expectation against the true gradient across different training stages and normalization constant estimation frequencies.

3. **Lipschitz constant monitoring**: Implement runtime monitoring of the empirical Lipschitz constant during training to verify it remains bounded as assumed in the theoretical analysis, and investigate the relationship between observed Lipschitz behavior and convergence stability.