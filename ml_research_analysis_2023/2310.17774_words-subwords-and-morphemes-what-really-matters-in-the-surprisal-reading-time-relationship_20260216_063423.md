---
ver: rpa2
title: 'Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading
  Time Relationship?'
arxiv_id: '2310.17774'
source_url: https://arxiv.org/abs/2310.17774
tags:
- surprisal
- word
- morphological
- words
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether using morphologically accurate
  subwords instead of LLM-style subword tokens affects surprisal-based predictions
  of reading times. We trained 5-gram models on COCA using orthographic, BPE, and
  morphological tokenization, then compared their surprisal estimates against eye-tracking
  and self-paced reading data.
---

# Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?

## Quick Facts
- arXiv ID: 2310.17774
- Source URL: https://arxiv.org/abs/2310.17774
- Authors: Michael Hahn, Kyle Mahowald, Roger Levy
- Reference count: 16
- Primary result: While BPE and morphological tokenization show similar aggregate predictive power for reading times, morphological tokenization produces more cognitively plausible surprisal estimates for morphologically complex words.

## Executive Summary
This study investigates whether using morphologically accurate subwords instead of LLM-style subword tokens affects surprisal-based predictions of reading times. We trained 5-gram models on COCA using orthographic, BPE, and morphological tokenization, then compared their surprisal estimates against eye-tracking and self-paced reading data. While aggregate results showed no difference in predictive power (ΔLogLik ≈ 0.01 for both BPE and morphological models), finer-grained analysis revealed that BPE tokenization splits fewer words and yields less cognitively plausible surprisal estimates, particularly for morphologically complex words. The morphological tokenizer produced more incremental surprisal increases with word complexity and better predictions for segmented words. These findings suggest caution when using LLM-style tokenization for fine-grained cognitive studies, despite its adequacy for aggregate correlations.

## Method Summary
The study trained three 5-gram language models on the COCA corpus using orthographic, BPE, and morphological tokenization. Surprisal was computed for each word in the Dundee eye-tracking corpus and Natural Stories self-paced reading corpus. For BPE and morphological models, surprisal was summed across subword tokens. Regression models predicting reading times from surprisal, word length, and log frequency were fit using 10-fold cross-validation, and ΔLogLik was used to compare predictive power across tokenization methods.

## Key Results
- Aggregate predictive power was equivalent across tokenization methods (ΔLogLik ≈ 0.01)
- BPE tokenization split only 5% of tokens in psycholinguistic corpora vs 25-44% for morphological tokenization
- Morphological tokenization showed more incremental surprisal increases with word complexity
- BPE's predictive power was significantly worse for words that were segmented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological tokenization aligns with human morphological processing, yielding more cognitively plausible surprisal estimates than BPE.
- Mechanism: When words are segmented into morphemes (e.g., "decomposition" → "de", "compos", "(i)tion"), surprisal increases incrementally with word complexity, matching human cognitive effort patterns.
- Core assumption: Human word processing effort correlates with morphological complexity, not arbitrary BPE splits.
- Evidence anchors:
  - [abstract] "BPE tokenization splits fewer words and yields less cognitively plausible surprisal estimates, particularly for morphologically complex words"
  - [section] "The morphological tokenizer produced more incremental surprisal increases with word complexity and better predictions for segmented words"
  - [corpus] Weak evidence - paper doesn't show cross-linguistic consistency
- Break condition: If morphological units don't correspond to human processing stages (e.g., in languages without clear morpheme boundaries).

### Mechanism 2
- Claim: BPE tokenization fails to split many morphologically complex words, limiting its ability to capture cognitive effort.
- Mechanism: BPE only splits 5% of tokens in psycholinguistic corpora vs 25-44% for morphological tokenization, missing opportunities to model incremental processing effort.
- Core assumption: Splitting complex words into subword units allows modeling of incremental cognitive effort during reading.
- Evidence anchors:
  - [section] "the BPE tokenizer only split 5% of the tokens in the psycholinguistic corpora (11% when ignoring stopwords), compared to 25% and 44% respectively for the morphological analyzer"
  - [section] "for BPE tokenization there is a sharp jump from low surprisal with unsplit words to essentially equal surprisal for words split into 2, 3, and 4 tokens"
  - [corpus] No cross-linguistic validation provided
- Break condition: If BPE tokenization improves in capturing morphological boundaries (e.g., through better training data or algorithms).

### Mechanism 3
- Claim: Sum-of-surprisals approach works for both tokenization methods at aggregate level but reveals differences in fine-grained analysis.
- Mechanism: Both BPE and morphological tokenization show similar aggregate predictive power (ΔLogLik ≈ 0.01), but morphological tokenization shows better fine-grained fit for segmented words.
- Core assumption: Aggregate correlation between surprisal and reading times is sufficient for most psycholinguistic applications.
- Evidence anchors:
  - [abstract] "While aggregate results showed no difference in predictive power (ΔLogLik ≈ 0.01 for both BPE and morphological models)"
  - [section] "the predictive power of the BPE-based model is significantly worse for words that do get split by the tokenizer, and this is not true for the morpheme-based model"
  - [corpus] Limited to English corpus; may not generalize
- Break condition: If fine-grained analysis becomes standard in psycholinguistics, making BPE's limitations more critical.

## Foundational Learning

- Concept: Surprisal theory in psycholinguistics
  - Why needed here: Core linking hypothesis connecting language model predictions to human processing effort
  - Quick check question: What is the relationship between surprisal and reading times according to Smith & Levy (2013)?

- Concept: Subword tokenization methods (BPE vs morphological)
  - Why needed here: Different tokenization approaches produce different units that may or may not align with human cognitive processing
  - Quick check question: How does BPE tokenization differ from morphological segmentation in handling "decomposition"?

- Concept: Regression analysis with cross-validation
  - Why needed here: Method for quantifying predictive power of surprisal while controlling for other factors
  - Quick check question: Why is 10-fold cross-validation important when comparing tokenization methods?

## Architecture Onboarding

- Component map: 5-gram language models -> surprisal calculation -> regression against reading times -> comparison across tokenization methods
- Critical path: Tokenization -> surprisal estimation -> regression modeling -> statistical comparison
- Design tradeoffs: n-gram models (simpler, interpretable) vs. neural LMs (more powerful, less transparent); morphological accuracy vs. computational efficiency
- Failure signatures: No significant difference in aggregate predictive power; BPE failing to split morphologically complex words; morphological tokenization producing too many tokens
- First 3 experiments:
  1. Replicate aggregate analysis using orthographic, BPE, and morphological tokenization on COCA-derived 5-gram models
  2. Analyze distribution of surprisal across different numbers of subword tokens for each method
  3. Compare predictive power separately for segmented vs. non-segmented words under each tokenization method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the predictive power difference between BPE and morphological tokenization vary across languages with different morphological structures?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, noting results are only for English and different results might obtain in languages with other morphological structures.
- Why unresolved: The study was limited to English, and the morphological transducer can be trained on other languages, but this remains untested.
- What evidence would resolve it: Testing the same methodology across multiple languages with varying morphological complexity (e.g., Turkish, Finnish, Chinese) and comparing predictive power differences between BPE and morphological tokenization.

### Open Question 2
- Question: Do the problematic effects of BPE tokenization extend beyond morphologically complex words to other word types?
- Basis in paper: [explicit] Appendix B shows examples where even non-morphologically complex words show surprisal differences between BPE and morphological tokenization.
- Why unresolved: The main analysis focused on aggregate results and morphologically complex words, but didn't systematically investigate other word types.
- What evidence would resolve it: A comprehensive analysis of surprisal differences across different word categories (function words, content words, proper nouns, etc.) using both tokenization methods.

### Open Question 3
- Question: Would training LLMs specifically on morphologically segmented units improve cognitive plausibility while maintaining predictive power?
- Basis in paper: [inferred] The authors suggest this as a future direction, noting it would be "a natural fit for further work on morphological prediction" and would facilitate "more cognitively realistic models."
- Why unresolved: The study used n-gram models rather than LLMs, and the authors explicitly call for training LLMs at scale with morphological segmentation.
- What evidence would resolve it: Training and evaluating transformer-based language models using morphological units versus BPE units on the same psycholinguistic datasets, comparing both predictive power and cognitive plausibility metrics.

## Limitations

- Analysis limited to English language, with unknown generalizability to morphologically rich languages
- Uses 5-gram language models which may not capture long-range dependencies as well as neural models
- Morphological transducer segmentation may not perfectly align with human morphological processing

## Confidence

- **High Confidence**: BPE splits significantly fewer words than morphological tokenization (5% vs 25-44%), and aggregate predictive power equivalence is robustly established
- **Medium Confidence**: Morphological tokenization produces more "cognitively plausible" surprisal estimates, but this remains somewhat subjective
- **Low Confidence**: Practical significance for fine-grained cognitive studies may be overstated given aggregate equivalence in predictive power

## Next Checks

1. Cross-linguistic validation: Replicate analysis using morphologically rich languages (Turkish, Finnish, Arabic) to test generalizability
2. Neural LM comparison: Repeat analysis using neural language models to determine if tokenization differences persist
3. Human processing validation: Conduct behavioral experiment measuring reading times for morphologically complex words to directly assess cognitive plausibility against surprisal predictions