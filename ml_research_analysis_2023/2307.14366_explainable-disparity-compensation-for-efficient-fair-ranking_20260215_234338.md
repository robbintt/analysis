---
ver: rpa2
title: Explainable Disparity Compensation for Efficient Fair Ranking
arxiv_id: '2307.14366'
source_url: https://arxiv.org/abs/2307.14366
tags:
- disparity
- bonus
- points
- fairness
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-driven approach to address disparity
  in ranking outcomes by allocating compensatory bonus points to members of underrepresented
  groups. The authors propose an efficient sampling-based algorithm (DCA) that minimizes
  disparity without requiring access to the full dataset.
---

# Explainable Disparity Compensation for Efficient Fair Ranking

## Quick Facts
- arXiv ID: 2307.14366
- Source URL: https://arxiv.org/abs/2307.14366
- Reference count: 40
- Key outcome: Introduces DCA, a sampling-based algorithm that reduces ranking disparity with sub-linear time complexity while maintaining transparency and utility

## Executive Summary
This paper presents a novel approach to address disparity in ranking outcomes through a data-driven compensation mechanism. The Disparity Compensation Algorithm (DCA) uses bonus points allocated to underrepresented groups to reduce statistical parity gaps in ranking results. By leveraging random sampling instead of full dataset processing, DCA achieves computational efficiency while maintaining effectiveness in reducing disparity. The method is designed to be explainable and transparent, allowing stakeholders to understand how fairness adjustments are made to ranking functions.

## Method Summary
DCA addresses disparity in ranking by allocating compensatory bonus points to members of underrepresented groups. The algorithm uses a sample-based approach to estimate disparity over the underlying data distribution, avoiding the need to process the entire dataset. It iteratively adjusts bonus points to minimize disparity using a derivative-free descent method. The approach can handle both known selection percentages (k) and unknown k values through logarithmic discounting. An optional refinement step using Adam optimization can further improve results. The method is applied to score-based ranking functions where bonus points are added directly to the ranking score of objects based on their fairness attributes.

## Key Results
- DCA achieves sub-linear time complexity by using random samples instead of processing full datasets
- Experiments on NYC school admissions and COMPAS datasets show effective disparity reduction while maintaining ranking utility
- Compared to existing fair ranking techniques, DCA provides comparable or better fairness outcomes with greater computational efficiency
- The approach allows flexible adjustment of bonus points to balance fairness and utility based on stakeholder needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCA achieves sub-linear time complexity through sample-based disparity computation
- Mechanism: Uses random samples drawn uniformly from the underlying distribution to estimate disparity instead of processing the full dataset
- Core assumption: A sufficiently large random sample can accurately estimate disparity due to the Central Limit Theorem
- Evidence anchors: Abstract mentions sub-linear time efficiency; section discusses focusing on underlying distribution rather than specific datasets
- Break condition: If sample size is too small relative to the frequency of the least common group, or if the underlying distribution violates Central Limit Theorem assumptions

### Mechanism 2
- Claim: Bonus points effectively address multiple dimensions of disparity through compounding effects
- Mechanism: Adds bonus points to ranking scores based on fairness attributes, with multiple bonuses combining for intersectionality
- Core assumption: Adding bonus points is a simple, transparent way to adjust for disparity that stakeholders can understand
- Evidence anchors: Abstract describes bonus point generation for underrepresented groups; section explains the simplicity and understandability of the approach
- Break condition: If bonus points create reverse disparate impact or are perceived as penalties rather than compensation

### Mechanism 3
- Claim: DCA can optimize for different selection percentages using logarithmic discounting
- Mechanism: Uses logarithmic discounting to assign more importance to objects selected first, allowing optimization across multiple k values
- Core assumption: Logarithmic discounting reflects real-world scenarios where earlier positions have more impact
- Evidence anchors: Abstract mentions comparable or better fairness outcomes; section proposes modification using logarithmic discounting
- Break condition: If logarithmic discounting doesn't align with actual importance of positions in the application context

## Foundational Learning

- Concept: Statistical parity as a fairness metric
  - Why needed here: Used as a measure of disparity that is easily interpretable and doesn't require assumptions about underlying bias sources
  - Quick check question: What does a disparity value of 0.1 mean in terms of low-income student representation in school admissions?

- Concept: Sample-based estimation and the Central Limit Theorem
  - Why needed here: DCA relies on random samples to estimate disparity over the entire distribution, making sub-linear time possible
  - Quick check question: Why does DCA need at least 30 samples per group to apply the Central Limit Theorem?

- Concept: Gradient descent optimization without derivatives
  - Why needed here: The disparity function is non-differentiable due to its step-function nature, requiring a derivative-free method
  - Quick check question: How does DCA adjust bonus points without using gradients of the disparity function?

## Architecture Onboarding

- Component map: Sample selection -> Disparity calculation -> Bonus point adjustment -> Repeat until convergence -> Apply bonus points to ranking function
- Critical path: Sample selection → Disparity calculation → Bonus point adjustment → Repeat until convergence → Apply bonus points to ranking function
- Design tradeoffs: Sample size vs. accuracy (larger samples provide better estimates but increase computation time), number of learning rate passes vs. runtime (more passes can find better solutions but take longer), and bonus point granularity vs. stakeholder interpretability (finer granularity allows more precise adjustments but may be harder to explain)
- Failure signatures: If disparity doesn't converge to near-zero despite multiple iterations, check if sample size is sufficient for all groups. If runtime is unexpectedly high, verify that sample size hasn't grown too large relative to k. If bonus points seem to have reverse effects, check for negative bonus point values.
- First 3 experiments:
  1. Run DCA with default parameters on the school dataset with known k=0.05 and verify that disparity approaches zero while utility (nDCG) remains above 0.95
  2. Test DCA with varying k values (0.1, 0.2, 0.3) and compare the resulting bonus point vectors to see if they scale appropriately
  3. Implement the refinement step and measure its impact on both disparity reduction and runtime compared to core DCA alone

## Open Questions the Paper Calls Out

- Question: How does logarithmic discounting affect DCA's convergence rate and accuracy compared to using it only for a fixed k?
- Basis in paper: Section 4.5 discusses logarithmic discounting for unknown k values
- Why unresolved: The paper mentions this modification but doesn't provide experimental comparison of convergence rate or accuracy between the two approaches
- What evidence would resolve it: Experimental results comparing DCA performance with and without logarithmic discounting across various k values

- Question: What is the optimal sample size for DCA across different distributions and selection percentages?
- Basis in paper: Section 4.4 discusses sample size requirements but doesn't provide specific guidelines for different scenarios
- Why unresolved: The paper suggests a sample size of 500 for their specific case but doesn't explore how this varies with different distributions
- What evidence would resolve it: Systematic experiments varying sample sizes across different datasets and distributions to identify optimal ranges

- Question: How does DCA perform when applied to ranking functions with non-linear scoring mechanisms?
- Basis in paper: The paper focuses on score-based ranking functions but doesn't explore non-linear cases
- Why unresolved: All examples use linear combinations of attributes, but many real-world ranking functions use non-linear transformations
- What evidence would resolve it: Experiments applying DCA to various non-linear ranking functions and comparing results to linear cases

## Limitations

- The sample-based approach introduces uncertainty about the relationship between sample size and accuracy, particularly for highly skewed distributions or small minority groups
- The bonus point mechanism may not capture more complex forms of bias that require non-linear adjustments to ranking scores
- The paper does not address potential feedback loops where compensatory adjustments might create new forms of disparity over time

## Confidence

**High Confidence**: The computational efficiency claims (sub-linear time complexity) are well-supported by the algorithm's design and the use of sampling instead of full dataset processing. The basic mechanism of using bonus points to adjust rankings is clearly explained and theoretically sound.

**Medium Confidence**: The effectiveness of DCA in reducing disparity is demonstrated through experiments, but the results are limited to specific datasets (NYC schools and COMPAS). The generalizability to other domains and ranking scenarios requires further validation.

**Low Confidence**: The paper's claims about stakeholder interpretability and transparency of the bonus point system are not empirically validated. The potential for reverse disparate impact or stakeholder resistance to bonus points is acknowledged but not thoroughly investigated.

## Next Checks

1. **Sample Size Sensitivity Analysis**: Systematically vary sample sizes (10, 30, 100, 500) and measure the impact on disparity reduction accuracy and runtime to identify the optimal tradeoff point for different group size distributions.

2. **Cross-Domain Applicability Test**: Apply DCA to additional ranking datasets from different domains (e.g., job applicant rankings, loan approvals) to assess whether the algorithm maintains its effectiveness across diverse fairness contexts and ranking functions.

3. **Stakeholder Perception Study**: Conduct user studies with domain experts and affected populations to evaluate how well they understand the bonus point mechanism and whether they perceive it as fair compensation rather than reverse discrimination.