---
ver: rpa2
title: Domain-Specific Language Model Post-Training for Indonesian Financial NLP
arxiv_id: '2310.09736'
source_url: https://arxiv.org/abs/2310.09736
tags:
- financial
- language
- dataset
- sentiment
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents a domain-specific language model post-training
  approach for Indonesian financial natural language processing (NLP). The authors
  constructed a self-supervised Indonesian financial corpus and two task-specific
  datasets: financial sentiment analysis (IndoFinSent) and topic classification.'
---

# Domain-Specific Language Model Post-Training for Indonesian Financial NLP

## Quick Facts
- arXiv ID: 2310.09736
- Source URL: https://arxiv.org/abs/2310.09736
- Reference count: 17
- This study demonstrates that domain-specific post-training significantly improves performance on Indonesian financial NLP tasks, especially with limited fine-tuning data.

## Executive Summary
This study presents a domain-specific language model post-training approach for Indonesian financial natural language processing (NLP). The authors constructed a self-supervised Indonesian financial corpus and two task-specific datasets: financial sentiment analysis (IndoFinSent) and topic classification. They performed continual pre-training on IndoBERT using the financial corpus and evaluated the effectiveness of the post-trained models on downstream tasks. Results showed that post-training significantly improved performance on both sentiment analysis and topic classification tasks, especially when fine-tuning with limited training data. The financial news articles dataset proved more effective than financial reports due to language style similarity. This work provides valuable resources and insights for developing specialized NLP models in the Indonesian financial domain.

## Method Summary
The authors constructed a 4.85 MB Indonesian financial corpus from financial news articles and corporate reports, then performed post-training on pre-trained IndoBERT using masked language modeling for 20 epochs. They fine-tuned the resulting models on sentiment analysis (IndoFinSent dataset) and topic classification tasks, evaluating performance with varying amounts of training data (10% to 100%) using F1 score and accuracy metrics.

## Key Results
- Post-training with financial news articles significantly outperformed the baseline IndoBERT model on both sentiment analysis and topic classification tasks
- The post-trained models showed particular effectiveness when fine-tuning with limited training data (10-50%)
- Financial news articles were more effective than corporate reports for post-training due to language style similarity with downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-training with domain-specific financial corpora improves model performance on financial NLP tasks.
- Mechanism: Domain-specific post-training exposes the model to specialized vocabulary and contextual patterns unique to the financial domain, enhancing its ability to understand and process financial language.
- Core assumption: The financial domain has distinct language patterns and vocabulary that differ from general language, and these differences are significant enough to impact model performance on domain-specific tasks.
- Evidence anchors:
  - [abstract]: "We perform post-training on pre-trained IndoBERT for financial domain using a small scale of Indonesian financial corpus."
  - [section]: "Previous studies have explored on specific domains such as biomedical [11]–[13], scientific domain [14], legal domain [15], and financial [3]–[5]. They use domain-specific corpora to pre-train the language model (mainly BERT) and evaluate its effectiveness on various downstream tasks."
  - [corpus]: Weak. The corpus construction section describes the data collection process but does not directly address the language pattern differences between financial and general domains.

### Mechanism 2
- Claim: Post-training is more effective when fine-tuning data is limited.
- Mechanism: By exposing the model to domain-specific language during post-training, it becomes better at understanding financial concepts even with less task-specific training data.
- Core assumption: Models can leverage knowledge gained from post-training to compensate for limited fine-tuning data.
- Evidence anchors:
  - [abstract]: "Results showed that post-training significantly improved performance on both sentiment analysis and topic classification tasks, especially when fine-tuning with limited training data."
  - [section]: "We perform fine-tuning for sentiment analysis given 10% of training data to 100% of training data. This was performed to evaluate the domain-specific post-training effectiveness in transfer learning especially when there is a limited number of annotated dataset."
  - [corpus]: Weak. The corpus construction section describes the data collection process but does not directly address the relationship between post-training and limited fine-tuning data.

### Mechanism 3
- Claim: Language style similarity between post-training corpus and fine-tuning data enhances model performance.
- Mechanism: When the language style of the post-training corpus matches that of the fine-tuning data, the model can more effectively transfer learned patterns and improve performance.
- Core assumption: Language style similarity between post-training and fine-tuning data facilitates knowledge transfer and improves model performance.
- Evidence anchors:
  - [abstract]: "The financial news articles dataset proved more effective than financial reports due to language style similarity."
  - [section]: "It can be noticed that the post-trained models using financial news articles dataset outperform the IndoBERT baseline. It can also be observed that in lower training data percentages, the post-trained models outperform the baseline."
  - [corpus]: Weak. The corpus statistics section provides information about the data sources but does not directly address the language style similarity between post-training and fine-tuning data.

## Foundational Learning

- Concept: Pre-trained language models
  - Why needed here: Understanding how pre-trained models like IndoBERT work is crucial for grasping the concept of post-training and its impact on model performance.
  - Quick check question: What is the main difference between pre-training and fine-tuning in the context of language models?

- Concept: Domain adaptation
  - Why needed here: Domain adaptation is the process of adapting a model trained on one domain to perform well on another domain. In this case, post-training is a form of domain adaptation for the financial domain.
  - Quick check question: How does post-training differ from fine-tuning in terms of its impact on model performance for domain-specific tasks?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the training objective used during post-training, and understanding how it works is essential for grasping the mechanics of the post-training process.
  - Quick check question: How does MLM encourage the model to learn contextual representations of words?

## Architecture Onboarding

- Component map:
  - IndoBERT (base/large architecture) -> Financial corpus (news articles, corporate reports) -> Downstream tasks (sentiment analysis, topic classification) -> Evaluation metrics (F1 score, accuracy)

- Critical path:
  1. Construct financial corpus
  2. Post-train IndoBERT using the corpus
  3. Fine-tune post-trained models on downstream tasks
  4. Evaluate model performance

- Design tradeoffs:
  - Corpus size vs. diversity: Larger corpora may provide more exposure to domain-specific language, but may also introduce noise or irrelevant information.
  - Fine-tuning data size: Using limited fine-tuning data allows for testing the effectiveness of post-training, but may not reflect real-world scenarios where more data is available.

- Failure signatures:
  - Post-trained models perform worse than baseline models: This could indicate issues with the post-training process, such as an inappropriate corpus or training parameters.
  - Post-trained models show no improvement over baseline models: This could suggest that the financial domain does not have distinct enough language patterns to benefit from post-training.

- First 3 experiments:
  1. Post-train IndoBERT using only financial news articles and evaluate on sentiment analysis and topic classification tasks.
  2. Post-train IndoBERT using only corporate financial reports and evaluate on sentiment analysis and topic classification tasks.
  3. Post-train IndoBERT using a combination of financial news articles and corporate financial reports and evaluate on sentiment analysis and topic classification tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would increasing the size of the financial domain corpus beyond 4.85 MB impact the effectiveness of domain-specific post-training for sentiment analysis and topic classification tasks?
- Basis in paper: [explicit] The authors note their corpus is "a small scale of Indonesian financial corpus" at 4.85 MB, compared to IndoBERT's 23.43 GB general domain pre-training data
- Why unresolved: The study only tested with their constructed small corpus; no experiments were conducted with larger financial domain corpora
- What evidence would resolve it: Systematic experiments comparing model performance with incrementally larger financial domain corpora, showing performance trends as corpus size increases

### Open Question 2
- Question: Would from-scratch pre-training on financial domain data outperform continual pre-training (post-training) from pre-trained IndoBERT models?
- Basis in paper: [explicit] The authors state "from-scratch pre-training can be performed for various language models" as future work, suggesting this comparison hasn't been made
- Why unresolved: The study only explored continual pre-training from pre-trained IndoBERT models; no from-scratch training experiments were conducted
- What evidence would resolve it: Direct comparison of from-scratch pre-trained financial models versus post-trained IndoBERT models on the same downstream tasks, controlling for training data size and model architecture

### Open Question 3
- Question: How does the effectiveness of domain-specific post-training vary across different Indonesian financial institutions beyond the one used in IndoFinSent?
- Basis in paper: [explicit] The authors created IndoFinSent "specific to one of the biggest financial institutions in Indonesia" and note this as a limitation
- Why unresolved: The dataset and experiments focused on a single institution, limiting generalizability across the diverse Indonesian financial sector
- What evidence would resolve it: Testing post-trained models on sentiment analysis datasets from multiple Indonesian financial institutions, comparing performance consistency and identifying institution-specific vs. general financial language patterns

### Open Question 4
- Question: What is the optimal ratio between financial news articles and corporate financial reports for effective domain-specific post-training?
- Basis in paper: [explicit] The authors note that "financial news articles dataset proved more effective than financial reports due to language style similarity" but didn't systematically explore different mixing ratios
- Why unresolved: Only three corpus configurations were tested (news only, reports only, and combination), without exploring intermediate ratios or conducting ablation studies
- What evidence would resolve it: Experiments testing multiple ratios of news-to-report content in the post-training corpus, identifying the ratio that maximizes downstream task performance while maintaining domain coverage

## Limitations
- The study uses a relatively small financial corpus (4.85 MB) which may limit exposure to full range of financial domain language patterns
- Evaluation is limited to only two downstream tasks (sentiment analysis and topic classification)
- The approach is tested exclusively on Indonesian language, limiting generalizability to other languages

## Confidence
- **High Confidence**: Post-training with financial news articles outperforms post-training with financial reports
- **Medium Confidence**: Post-training is particularly effective with limited fine-tuning data
- **Low Confidence**: Generalizability of findings to other languages and financial domains

## Next Checks
1. **Cross-lingual validation**: Replicate the post-training experiments with financial corpora from other languages (e.g., English, Mandarin) to test the generalizability of the approach and identify language-specific factors affecting post-training effectiveness.

2. **Corpus size sensitivity analysis**: Systematically vary the size of the financial corpus used for post-training (e.g., 10%, 25%, 50%, 100%) to determine the minimum effective corpus size and identify diminishing returns thresholds.

3. **Multi-task evaluation**: Extend evaluation beyond sentiment analysis and topic classification to include other financial NLP tasks such as named entity recognition, relationship extraction, and financial question answering to comprehensively assess the model's domain-specific capabilities.