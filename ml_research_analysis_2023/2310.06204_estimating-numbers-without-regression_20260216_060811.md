---
ver: rpa2
title: Estimating Numbers without Regression
arxiv_id: '2310.06204'
source_url: https://arxiv.org/abs/2310.06204
tags:
- number
- numbers
- language
- dexp
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares different number representation methods for
  language models, finding that simply tokenizing numbers on a log-scaled number line
  achieves near state-of-the-art performance on masked number prediction without requiring
  architectural changes. Specifically, discretizing the number line into logarithmically
  sized bins and assigning each bin a token (vocabulary change) matches or outperforms
  methods that change the model architecture to directly regress to numbers (architectural
  change).
---

# Estimating Numbers without Regression

## Quick Facts
- arXiv ID: 2310.06204
- Source URL: https://arxiv.org/abs/2310.06204
- Reference count: 18
- Key outcome: Log-scale tokenization of numbers matches or outperforms architectural changes for masked number prediction in language models

## Executive Summary
This paper challenges the assumption that improving number understanding in language models requires architectural modifications like direct regression. Instead, it demonstrates that simply tokenizing numbers on a log-scaled number line - discretizing the number line into logarithmically sized bins and assigning each bin a token - achieves near state-of-the-art performance on masked number prediction tasks. This vocabulary-level change matches or outperforms methods that add regression heads or other architectural modifications, while being simpler to implement and requiring no changes to the base model architecture.

## Method Summary
The paper fine-tunes a 12-layer BERT-base model on masked number prediction tasks using four different number representation strategies: subword tokenization, notation changes (scientific and digit notation), vocabulary changes (log-scale binning), and an architectural change (DExp regression head). The vocabulary change approach discretizes numbers into bins on a log-scaled number line and assigns each bin a token, with static mantissa assignment (arithmetic or geometric mean). The model is trained on three datasets - Financial News Articles, a price-focused subset, and Scientific Articles - using batch size 32 for 10 epochs with early stopping.

## Key Results
- Log-scale vocabulary tokenization matches or outperforms architectural changes (DExp) on masked number prediction tasks
- Static mantissa assignment (arithmetic or geometric mean) performs nearly as well as learned regression heads
- Notation changes (scientific, digit) provide insufficient improvement over baseline subword tokenization
- The approach transfers well to downstream numerical fact estimation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenizing numbers on a log-scaled number line captures magnitude relationships naturally without architectural changes.
- Mechanism: Log-scale binning preserves order-of-magnitude information by collapsing numbers into bins that reflect their relative sizes, avoiding the arbitrary segmentation of subword tokenization that destroys magnitude relationships.
- Core assumption: Natural language number distributions align with log-scale binning due to Benford's Law and recency bias.
- Evidence anchors: Abstract shows vocabulary change outperforms notation change; Section 4 discusses leading digit distributions and mantissa peaks around 2.
- Break condition: Irregular number distributions (cryptographic keys, random IDs) may not align with log-scale bins.

### Mechanism 2
- Claim: Vocabulary-level changes outperform notation changes for number representation in LMs.
- Mechanism: Vocabulary changes directly encode magnitude by creating tokens for number ranges, while notation changes still rely on subword tokenization that fails to capture magnitude relationships.
- Core assumption: The tokenizer can learn to map magnitude tokens to their semantic meaning in context.
- Evidence anchors: Abstract states vocabulary change is a "far better trade-off"; Section 2 explains distance/similarity concepts for numbers.
- Break condition: Tokenizer fails to properly segment and utilize new vocabulary tokens without retraining.

### Mechanism 3
- Claim: Static mantissa assignment is sufficient for accurate number prediction due to predictable number distributions.
- Mechanism: Fixed mantissas (arithmetic or geometric mean) avoid the complexity of learning regression while achieving high accuracy, as natural language distributions align with these static assignments.
- Core assumption: Real-world frequency distributions make static mantissas nearly optimal.
- Evidence anchors: Section 4 discusses Benford's Law and recency bias; Section 9.3 shows similar trends across experiments.
- Break condition: Unusual number distributions may not align with static mantissas, reducing performance.

## Foundational Learning

- Concept: Logarithmic scaling and its application to number representation.
  - Why needed here: Understanding how log-scale binning preserves magnitude relationships and why it's effective for number tokenization.
  - Quick check question: Why does discretizing the number line into logarithmically sized bins help preserve the relationship between numbers like 799 and 800, which subword tokenization fails to capture?

- Concept: Subword tokenization and its limitations for numbers.
  - Why needed here: Recognizing why standard subword tokenization (e.g., BPE, WordPiece) is inadequate for numbers, as it splits them into arbitrary chunks that destroy magnitude relationships.
  - Quick check question: How does subword tokenization of the number 799 into "79" and "##9" prevent the model from understanding its relationship to nearby numbers like 800?

- Concept: Benford's Law and recency bias in natural language.
  - Why needed here: Understanding why static mantissa assignments work well, due to predictable number distributions in real-world text (e.g., leading digits follow Benford's Law, frequent years like 2000-2023).
  - Quick check question: How do Benford's Law and recency bias explain why static mantissas (arithmetic or geometric mean) perform nearly as well as learned regression heads for number prediction?

## Architecture Onboarding

- Component map: Tokenizer (vocabulary modification for number bins) -> Language Model (BERT-base, fine-tuned with new tokens) -> Number Decoder (masked prediction head, no architectural changes)
- Critical path: Vocabulary change -> Tokenizer retraining -> Fine-tuning LM on MNP task -> Evaluation on masked number prediction and downstream tasks
- Design tradeoffs:
  - Pros: Simple implementation, no architectural changes, minimal computational overhead, works with any off-the-shelf LM
  - Cons: Lossy representation (collapses numbers into bins), may not suit exact arithmetic tasks, requires retraining tokenizer
- Failure signatures: Poor performance on numbers outside typical ranges, failure to capture fine-grained magnitude differences, tokenizer segmentation errors for new vocabulary tokens
- First 3 experiments:
  1. Implement vocabulary change with log-scale binning (base 10) and arithmetic mean mantissa; fine-tune BERT-base on MNP task; compare E-Acc and LogMAE against subword baseline
  2. Repeat with geometric mean mantissa; evaluate if it improves performance over arithmetic mean
  3. Test variable-length binning based on corpus frequency; compare against static log-scale bins to assess robustness of the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of number tokenization on a log-scaled number line generalize to other languages and numeral systems beyond English and Hindu-Arabic numerals?
- Basis in paper: The authors note that their findings may not apply beyond English and Hindu-Arabic numerals and encourage follow-up work to consider other systems.
- Why unresolved: The paper only evaluates on English text with Hindu-Arabic numerals. Cross-linguistic studies would be needed to test generalizability.
- What evidence would resolve it: Experiments testing the log-scaled tokenization approach on datasets in other languages (e.g. Chinese, Arabic, Hindi) and numeral systems (e.g. Roman numerals, Chinese numerals) to compare performance to the English results.

### Open Question 2
- Question: How does the performance of log-scaled tokenization compare to architectural changes like DExp on tasks requiring exact numeracy, such as math word problems or numerical reasoning?
- Basis in paper: The authors note that their log-scaled tokenization method is lossy by design and unlikely to be suitable for exact numeracy. They compare only to approximate number prediction tasks.
- Why unresolved: The paper only evaluates on approximate number prediction tasks. Direct comparison on tasks requiring exact numeracy is needed.
- What evidence would resolve it: Experiments directly comparing log-scaled tokenization and architectural changes like DExp on math word problems, numerical reasoning tasks, etc. to see which approach better captures exact numeracy.

### Open Question 3
- Question: How robust is the log-scaled tokenization approach to variations in corpus frequencies and distributions of numbers?
- Basis in paper: The authors experiment with variable sized bins based on corpus frequencies but find no gains over static bins, suggesting robustness to corpus frequencies.
- Why unresolved: The experiments only test a few binning schemes. More extensive experiments varying corpus frequencies and distributions would better characterize robustness.
- What evidence would resolve it: Experiments systematically varying corpus frequencies and distributions of numbers (e.g. changing proportions of small vs large numbers) and testing impact on log-scaled tokenization performance.

## Limitations

- The approach is lossy by design, collapsing continuous number ranges into discrete bins, which may not suit tasks requiring exact numeracy
- The mechanistic explanations rely on assumed corpus statistics (Benford's Law, recency bias) without direct corpus-level validation
- Experiments are limited to masked number prediction on three datasets, with unclear generalization to other numerical reasoning tasks

## Confidence

**High Confidence**: The core empirical finding that vocabulary changes outperform notation changes and match architectural changes on the MNP task is well-supported by direct comparisons across three datasets with clear metrics.

**Medium Confidence**: The claim that vocabulary changes are a "far better trade-off" than architectural changes rests on assumptions about simplicity and computational overhead being primary desiderata, which may not be universally applicable.

**Low Confidence**: The mechanistic explanations depend heavily on assumed corpus statistics without direct corpus-level validation, particularly the claim that static mantissas are "nearly optimal" due to predictable distributions.

## Next Checks

1. **Corpus Distribution Analysis**: Perform systematic analysis of number distributions across the three datasets, measuring actual frequency distributions, leading digit distributions, and mantissa distributions to validate mechanistic claims.

2. **Extreme Value Performance**: Evaluate the vocabulary change approach on datasets containing numbers outside typical ranges (cryptographic keys, astronomical measurements) to determine limits of log-scale binning's effectiveness.

3. **Downstream Task Generalization**: Test the approach on numerical reasoning tasks beyond masked prediction, such as numerical QA and arithmetic reasoning, to clarify whether performance gains transfer to practical applications.