---
ver: rpa2
title: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning
arxiv_id: '2305.14852'
source_url: https://arxiv.org/abs/2305.14852
tags:
- pruning
- sparsity
- neural
- particles
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to neural network pruning
  called Sparse Weight Averaging with Multiple Particles (SWAMP), which leverages
  the linear connectivity of Iterative Magnitude Pruning (IMP) solutions. The method
  trains multiple sparse models with different batch orders from the same matching
  ticket and averages them to produce a single mask, leading to improved performance.
---

# Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning

## Quick Facts
- arXiv ID: 2305.14852
- Source URL: https://arxiv.org/abs/2305.14852
- Reference count: 40
- Multiple particles trained with different batch orders from same matching ticket and averaged to produce single mask, leading to improved performance

## Executive Summary
This paper introduces Sparse Weight Averaging with Multiple Particles (SWAMP), a novel approach to neural network pruning that enhances Iterative Magnitude Pruning (IMP) by training multiple sparse models (particles) with different batch orders from the same matching ticket and then averaging them. The method leverages the linear connectivity of IMP solutions to safely combine multiple optimization trajectories without encountering loss barriers. Through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet with various architectures, SWAMP consistently outperforms existing baselines across different sparsity levels, achieving performance comparable to an ensemble of two IMP solutions while maintaining the same inference cost as a single model.

## Method Summary
SWAMP extends IMP by concurrently training N sparse models (particles) initialized from the same matching ticket but with different random seeds for SGD noise. Each particle undergoes standard IMP training for 150 epochs (40 for ImageNet) with SWA applied over the last 25% of training. The weight-averaged particle is then pruned by retaining 80% of remaining parameters. This process repeats across IMP cycles, with the weight-averaged solution serving as the initialization for the next cycle.

## Key Results
- SWAMP consistently outperforms IMP and other pruning baselines across CIFAR-10, CIFAR-100, and ImageNet
- Achieves performance comparable to an ensemble of two IMP solutions while maintaining single-model inference cost
- Demonstrates matching performance even at extremely sparse levels (95% sparsity) where IMP struggles
- Successfully finds flatter minima with improved generalization as measured by smaller Hessian trace values

## Why This Works (Mechanism)

### Mechanism 1
Multiple particles trained with different SGD noise but the same matching ticket can be weight-averaged without encountering loss barriers because they remain in the same basin of attraction. When all particles start from the same matching ticket initialization, their optimization trajectories stay in the same basin despite different SGD noise, forming a low-loss subspace due to linear mode connectivity.

### Mechanism 2
Weight averaging the particles produces solutions in flatter regions of the loss landscape, improving generalization. Averaging multiple particles converges to a point with lower curvature (higher flatness) in the loss landscape, as measured by smaller Hessian trace values.

### Mechanism 3
SWAMP preserves linear connectivity between consecutive solutions across IMP cycles, which is crucial for IMP success. By using the same matching ticket initialization and weight averaging within each cycle, SWAMP maintains the linear connectivity property between sparse solutions of different sparsity levels.

## Foundational Learning

- **Linear mode connectivity in neural networks**: Understanding when two solutions are linearly connected (no high-loss barrier between them) is crucial for explaining why SWAMP particles can be averaged safely. Quick check: Given two solutions w(0) and w(1), what condition must hold for them to be linearly connected? (Answer: supλ∈[0,1] L((1-λ)w(0) + λw(1)) ≤ max{L(w(0)), L(w(1))} + ε)

- **Lottery ticket hypothesis and matching tickets**: SWAMP relies on using the same matching ticket across multiple particles, so understanding what makes a good matching ticket is crucial. Quick check: What distinguishes a matching ticket from random initialization when doing IMP? (Answer: A matching ticket is an early-stage checkpoint that, when used for rewinding, allows sparse networks to be trained to full accuracy)

- **Hessian trace as a measure of flatness**: The paper uses Hessian trace to quantify whether SWAMP finds flatter minima than IMP. Quick check: What does a smaller Hessian trace indicate about a local minimum? (Answer: Lower curvature and flatter basin, which typically correlates with better generalization)

## Architecture Onboarding

- **Component map**: Initialization → Matching ticket extraction → IMP cycles with SWAMP → Final sparse model
- **Critical path**: Initialize matching ticket → Train N particles with different seeds → Apply SWA over last 25% → Weight average particles → Prune 80% → Repeat for next cycle
- **Design tradeoffs**: Memory vs. performance (N particles require N× memory but improve results), SWA frequency vs. computational cost (more SWA samples give better averaging but cost more), particle count vs. diminishing returns (beyond certain N, gains plateau)
- **Failure signatures**: High loss barrier when averaging particles (linear connectivity broken), performance worse than single IMP (averaging disrupting good solutions), instability across runs (poor matching ticket quality)
- **First 3 experiments**: 1) Verify linear connectivity by training two particles with same matching ticket but different seeds and checking loss barrier when interpolating, 2) Compare flatness by measuring Hessian trace for IMP vs SWAMP at same sparsity levels, 3) Ablation on particle count by running SWAMP with N=1,2,4,8 particles and measuring performance gains

## Open Questions the Paper Calls Out

### Open Question 1
Does SWAMP's effectiveness stem from finding flatter minima or from averaging multiple solutions with different stochastic noise? The paper shows SWAMP finds flatter minima and discusses both weight averaging and particle averaging as key components, but doesn't definitively isolate which factor is more important.

### Open Question 2
Can SWAMP be made computationally efficient by applying it only to certain pruning cycles? The authors note that SWAMP requires N times more memory than IMP and conduct an ablation study on applying SWAMP to different numbers of cycles, but don't explore optimal scheduling strategies or memory-efficient implementations.

### Open Question 3
Is there a theoretical explanation for why the convex hull of SWAMP particles forms a low-loss subspace? The paper demonstrates empirically that SWAMP particles form a low-loss subspace but doesn't provide theoretical justification for this phenomenon.

## Limitations
- Linear connectivity assumption between SWAMP particles may not hold for all architectures or datasets
- Weight averaging benefits depend on the quality of matching tickets, which can vary significantly
- Computational overhead scales linearly with particle count, limiting practical applications

## Confidence
- High confidence: SWAMP improves performance over IMP in most cases
- Medium confidence: Mechanism 1 (linear connectivity enables safe averaging) is supported but not rigorously proven
- Medium confidence: Mechanism 2 (flatter minima improve generalization) is observed empirically but theoretical connection needs strengthening
- Low confidence: Mechanism 3 (preservation of linear connectivity across IMP cycles) relies heavily on external citations

## Next Checks
1. Verify linear connectivity empirically by measuring loss barriers when interpolating between SWAMP particles
2. Systematically vary particle count N to identify optimal tradeoff between performance and computational cost
3. Test SWAMP on architectures beyond those studied to assess generalizability of the approach