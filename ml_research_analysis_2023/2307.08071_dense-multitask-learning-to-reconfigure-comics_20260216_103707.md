---
ver: rpa2
title: Dense Multitask Learning to Reconfigure Comics
arxiv_id: '2307.08071'
source_url: https://arxiv.org/abs/2307.08071
tags:
- comics
- domain
- image
- depth
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task learning (MTL) approach for
  dense prediction in comic panels, enabling the transfer of comics between publication
  channels by assisting authors in reconfiguring their narratives. The method addresses
  the challenge of disparate artistic styles, layouts, and scales in comics by leveraging
  unsupervised image-to-image translation to utilize real-world annotations, followed
  by an MTL network with a vision transformer backbone and domain transferable attention
  module.
---

# Dense Multitask Learning to Reconfigure Comics

## Quick Facts
- arXiv ID: 2307.08071
- Source URL: https://arxiv.org/abs/2307.08071
- Reference count: 40
- Primary result: 33.65% mIoU segmentation and 0.909 RMSE depth on DCM validation set

## Executive Summary
This paper introduces a multi-task learning approach for dense prediction in comic panels, enabling transfer of comics between publication channels. The method addresses the challenge of disparate artistic styles and layouts in comics by leveraging unsupervised image-to-image translation to utilize real-world annotations, followed by an MTL network with vision transformer backbone and domain transferable attention module. The approach jointly performs semantic segmentation and depth estimation, achieving state-of-the-art results on the DCM dataset.

## Method Summary
The method employs DUNIT to translate comics images to the real domain, enabling MTL training on real-world annotations. A Swin Transformer-based MTL model with domain transferable attention (DTA) performs joint semantic segmentation and depth estimation. The model uses weighted sum of cross-entropy loss for segmentation and rotate loss for depth. DTA uses entropy-based transferability scores from a token-level domain discriminator to align cross-domain features.

## Key Results
- Achieves 33.65% mIoU for semantic segmentation on DCM validation set
- Achieves 0.909 RMSE for depth estimation on DCM validation set
- Outperforms single-task and multi-task baselines including CNN UNet, single-task Swin, and MulT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-domain transfer via unsupervised image-to-image translation enables dense prediction on comics without ground-truth annotations
- Mechanism: DUNIT translates comics to real imagery, allowing MTL models trained on real annotations to be applied to comics domain
- Core assumption: DUNIT can generate realistic real-domain equivalents of comics images that preserve semantic content while removing domain-specific style
- Evidence anchors: [abstract] "we leverage a commonly-used strategy known as unsupervised image-to-image translation, which allows us to utilize a large corpus of real-world annotations" and [section 3.2] "we employ the DUNIT model [4] to translate the comics image C to the real domain"

### Mechanism 2
- Claim: Domain Transferable Attention (DTA) enforces cross-domain feature alignment, improving generalization of dense predictions
- Mechanism: DTA uses entropy-based transferability scores from a token-level domain discriminator to weight attention, prioritizing transferable features from real domain
- Core assumption: Features with high transferability entropy are semantically meaningful across domains and can be leveraged for comics prediction
- Evidence anchors: [section 3.3.3] "we employ a token-level domain discriminator Dtoken that matches cross-domain local features" and "T (.) = H (Dtoken (.)) ∈ [0, 1]"

### Mechanism 3
- Claim: Multitask learning with vision transformers improves dense prediction performance over single-task models
- Mechanism: Shared transformer encoder with task-specific decoders jointly optimizes semantic segmentation and depth estimation, leveraging cross-task consistency
- Core assumption: Semantic segmentation and depth estimation are complementary tasks that benefit from shared representations
- Evidence anchors: [abstract] "Our MTL method can successfully identify the semantic units as well as the embedded notion of 3D in comic panels" and [section 4.4] "Our model outperforms the multitask CNN-based baseline [48] as well as the multitask Swin transformer baseline [21]"

## Foundational Learning

- Concept: Domain adaptation via unsupervised image-to-image translation
  - Why needed here: Comics lack dense annotations; real-world annotations are abundant but domains differ significantly
  - Quick check question: What loss terms in DUNIT ensure both image realism and instance consistency?

- Concept: Vision transformer architecture for dense prediction
  - Why needed here: Transformers capture long-range dependencies better than CNNs, crucial for diverse comics layouts
  - Quick check question: How does the Swin transformer's shifted window mechanism improve efficiency?

- Concept: Domain transferable attention for cross-domain feature alignment
  - Why needed here: Direct MTL application fails due to domain shift; DTA bridges this gap
  - Quick check question: What role does the entropy of the domain discriminator output play in DTA?

## Architecture Onboarding

- Component map: Input comics image -> DUNIT translation -> real-domain image -> Swin encoder -> shared features -> DTA-enhanced transformer decoders -> task predictions

- Critical path: 1. Input comics image → DUNIT translation → real-domain image; 2. Real image → Swin encoder → shared features; 3. Shared features → DTA-enhanced transformer decoders → task predictions

- Design tradeoffs:
  - DUNIT vs diffusion-based translation: DUNIT faster but diffusion may yield better quality
  - DTA complexity vs performance gain: added complexity justified by significant mIoU and RMSE improvements
  - Single vs multi-task: MTL improves performance but requires careful loss balancing

- Failure signatures:
  - Segmentation masks contain artifacts or miss small objects
  - Depth maps show incorrect depth ordering or noisy predictions
  - Translation produces unrealistic images or loses semantic content

- First 3 experiments:
  1. Ablation: Remove DTA and compare MTL performance to MulT baseline
  2. Ablation: Remove I2I translation and test MTL directly on comics images
  3. Quantitative: Measure mIoU and RMSE on DCM validation set and compare to all baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the proposed method generalize to other artistic styles beyond comics, such as illustrations or paintings?
- Basis in paper: [inferred] The paper focuses on comics and does not evaluate the method on other artistic styles
- Why unresolved: The paper only tests the method on comics datasets (DCM, Spirou, Tintin) and does not explore its applicability to other artistic domains
- What evidence would resolve it: Testing the method on a diverse set of artistic styles (e.g., illustrations, paintings, sketches) and comparing its performance to state-of-the-art methods in those domains

### Open Question 2
- Question: What is the impact of the domain transferable attention (DTA) mechanism on the overall performance, and can it be further improved?
- Basis in paper: [explicit] The paper mentions the DTA mechanism but does not provide a detailed analysis of its impact on performance or potential improvements
- Why unresolved: The paper only provides a high-level description of the DTA mechanism and its benefits without a thorough analysis of its impact on performance or potential improvements
- What evidence would resolve it: Conducting ablation studies to isolate the impact of the DTA mechanism on performance and exploring potential improvements to the mechanism, such as incorporating additional domain-specific features or using different attention mechanisms

### Open Question 3
- Question: How does the proposed method compare to other state-of-the-art methods for dense prediction in comics, such as those based on generative adversarial networks (GANs) or other deep learning architectures?
- Basis in paper: [inferred] The paper does not compare the proposed method to other state-of-the-art methods for dense prediction in comics
- Why unresolved: The paper only compares the proposed method to a few baselines and does not explore its performance relative to other state-of-the-art methods in the field
- What evidence would resolve it: Conducting a comprehensive comparison of the proposed method to other state-of-the-art methods for dense prediction in comics, including those based on GANs or other deep learning architectures, using standard evaluation metrics and datasets

## Limitations

- Primary limitation is dependence on quality of unsupervised image-to-image translation; artifacts or semantic distortions in DUNIT output directly impact MTL performance
- Domain transferable attention mechanism lacks extensive ablation studies to quantify its individual contribution versus standard MTL approaches
- Method focuses exclusively on comics and does not evaluate generalization to other artistic styles

## Confidence

- **High confidence**: The overall MTL framework architecture and its ability to jointly predict segmentation and depth
- **Medium confidence**: The effectiveness of DUNIT for domain bridging (no direct quantitative comparison to alternatives like diffusion models)
- **Medium confidence**: The contribution of DTA to cross-domain generalization (novel mechanism with limited ablation analysis)

## Next Checks

1. Conduct ablation studies removing DTA to quantify its individual contribution versus standard MTL
2. Compare DUNIT translation quality against alternative translation methods (e.g., diffusion models) using both qualitative and quantitative metrics
3. Test the approach on additional comics datasets (beyond Spirou and Tintin) to evaluate generalization across different artistic styles