---
ver: rpa2
title: Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz
  Kernels
arxiv_id: '2301.11624'
source_url: https://arxiv.org/abs/2301.11624
tags:
- scheme
- neural
- wasserstein
- particle
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes neural network-based methods for approximating
  Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth
  Riesz kernels. Since these flows can involve singular measures, the authors develop
  methods that approximate disintegrations of transport and velocity plans rather
  than just transport maps.
---

# Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels

## Quick Facts
- arXiv ID: 2301.11624
- Source URL: https://arxiv.org/abs/2301.11624
- Reference count: 40
- The paper proposes neural network methods to approximate Wasserstein gradient flows for MMD functionals with non-smooth Riesz kernels, handling singular measures.

## Executive Summary
This paper addresses the challenge of computing Wasserstein gradient flows for maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels, particularly when the initial measure is singular. Traditional approaches relying on Brenier maps fail for singular measures, so the authors propose approximating disintegrations of transport and velocity plans using generative neural networks. They develop both backward (JKO) and forward schemes, deriving analytical formulas for the interaction energy starting at a Dirac measure as ground truth. The neural schemes are benchmarked against particle flows and show competitive performance, especially in high dimensions, and are demonstrated on image data like MNIST.

## Method Summary
The paper introduces neural network-based methods to approximate Wasserstein gradient flows for MMD functionals with Riesz kernels. For backward schemes, they parameterize Markov kernels using generative neural networks and optimize a JKO-like loss. For forward schemes, they similarly parameterize velocity plans and optimize a steepest descent loss. The methods handle singular measures by learning disintegrations of transport/velocity plans rather than relying on absolute continuity. Analytical formulas are derived for flows starting at a Dirac measure for the interaction energy, providing ground truth for evaluation. The neural schemes are compared against particle flows and shown to perform competitively, especially in high dimensions.

## Key Results
- Neural schemes successfully approximate Wasserstein gradient flows for MMD functionals with non-smooth Riesz kernels.
- Analytical formulas derived for JKO and forward schemes starting at Dirac measure, showing convergence as time step τ→0.
- Neural methods outperform particle flows in high-dimensional settings and are demonstrated on MNIST image data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks can approximate disintegrations of transport and velocity plans, enabling Wasserstein gradient flow computation for singular measures.
- Mechanism: The authors parameterize Markov kernels using generative neural networks. For the backward scheme, they learn a transport plan via πx = Tθ(x,·)#PZ where PZ is a standard Gaussian latent distribution. For the forward scheme, they similarly approximate velocity plans. These disintegrations allow handling measures that are not absolutely continuous, unlike traditional Brenier map approaches.
- Core assumption: The functional F can be approximated by samples (e.g., defined as an integral), and the neural network parameterization is expressive enough to capture the Markov kernel structure of the transport/velocity plans.
- Evidence anchors:
  - [abstract]: "we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions"
  - [section 3]: "Now we propose to parameterize the Markov kernel πx by a NN Tθ(x,·): Rd×Rd→Rd via πx = Tθ(x,·)#PZ"
  - [corpus]: Weak - no direct comparison to similar NN parameterization approaches in related works.
- Break condition: If F cannot be efficiently sampled (e.g., requires explicit density evaluation), the neural schemes fail. Also if the network cannot learn the Markov kernel accurately, the approximation breaks down.

### Mechanism 2
- Claim: The Jordan-Kinderlehrer-Otto (JKO) backward scheme and the forward Euler scheme converge to the same Wasserstein gradient flow for the interaction energy starting at a Dirac measure.
- Mechanism: The paper derives analytical formulas for both schemes starting at δ0 for the interaction energy EK. It proves that as the time step τ→0, both numerical schemes converge to the same limit curve γ(t) = ((t(2−r))^(1/(2−r)) Id)#η*, where η* is the proximal mapping of EK at δ0.
- Core assumption: The functional EK with Riesz kernel satisfies conditions for convergence of both schemes, and the measures computed remain orthogonally invariant.
- Evidence anchors:
  - [abstract]: "We provide analytic formulas for Wasserstein schemes starting at a Dirac measure and show their convergence as the time step size tends to zero."
  - [section 5]: Theorem 4 and Theorem 6 explicitly state convergence results for both schemes to the same curve.
  - [section 5]: "By the above theorem, we can represent the curves γτ|((n−1)τ,nτ ] := µnτ and their limit γ as τ→0 by..."
- Break condition: If the kernel is not a Riesz kernel or r∉(0,2), or if the initial measure is not a Dirac, the convergence results may not hold.

### Mechanism 3
- Claim: Particle flows provide a reasonable baseline for approximating Wasserstein gradient flows, though they depend on the initial particle distribution.
- Mechanism: The authors compare their neural schemes to particle flows that approximate the Wasserstein flow by evolving M particles according to the gradient flow of the functional FM(x1,...,xM) = F(1/M∑δxi). While this doesn't capture all Wasserstein flows for singular measures, the mean-field limit M→∞ may provide a meaningful approximation.
- Core assumption: The particle flow approximates the Wasserstein gradient flow of F, though it's technically a flow of a restricted functional FM.
- Evidence anchors:
  - [section 6]: "We compare our neural forward and backward schemes with particle flows. The main idea is to approximate the Wasserstein flow with respect to a function F by the gradient flow with respect to the functional FM"
  - [section 6]: "In particular, for the Riesz kernels, this is no longer true. Instead, we show in Appendix C that the particle flow is a Wasserstein gradient flow but with respect to a restricted functional."
  - [corpus]: Weak - no direct citation or comparison to established particle flow methods for Wasserstein flows with singular measures.
- Break condition: If the initial particle distribution is poorly chosen (e.g., not capturing the singularity structure), the particle flow performs poorly. Also fails for initial Dirac measures unless particles are placed in a tiny neighborhood.

## Foundational Learning

- Concept: Wasserstein space and Wasserstein distance
  - Why needed here: The paper operates entirely within the framework of Wasserstein spaces P2(Rd) with the Wasserstein-2 distance W2. All gradient flows, proximal mappings, and transport plans are defined with respect to this geometry.
  - Quick check question: What is the definition of the Wasserstein-2 distance between two probability measures, and how does it differ from other probability metrics like total variation?

- Concept: Wasserstein gradient flows and the JKO scheme
  - Why needed here: The core contribution involves approximating Wasserstein gradient flows using neural networks. Understanding the JKO backward scheme (proximal mapping iteration) and the forward Euler scheme for steepest descent flows is essential to grasp the proposed methods.
  - Quick check question: How does the JKO scheme approximate a Wasserstein gradient flow, and what role does the Wasserstein proximal operator play in this discretization?

- Concept: Maximum Mean Discrepancy (MMD) and Riesz kernels
  - Why needed here: The functionals being minimized are MMD functionals with Riesz kernels. The specific properties of these kernels (non-smooth, non-λ-convex) are crucial to why traditional methods fail and why the neural approach is needed.
  - Quick check question: What is the definition of MMD with a kernel K, and what makes Riesz kernels K(x,y) = -||x-y||^r with r∈(0,2) particularly challenging for gradient flow computation?

## Architecture Onboarding

- Component map: Neural backward scheme -> Learn transport plans via Markov kernel parameterization Tθ(x,·)#PZ -> Optimize JKO-like loss with W2 distance and functional F. Neural forward scheme -> Learn velocity plans via Markov kernel parameterization Tθ(x,·)#PZ -> Optimize steepest descent loss with forward-mode AD. Particle flow baseline -> Evolve M particles according to gradient flow of FM -> Starting from samples in small neighborhood of initial measure. Analytical ground truth -> Closed-form solutions for JKO and forward schemes starting at δ0 for EK.

- Critical path: For a new measure and functional, the typical workflow is: 1) Choose scheme (backward/forward), 2) Initialize network(s) for Markov kernel, 3) Sample from current measure, 4) Compute loss (transport plan or velocity plan objective), 5) Update network parameters, 6) Generate next measure samples, 7) Repeat until convergence or target reached.

- Design tradeoffs: Backward scheme vs forward scheme - backward is more stable but requires solving a minimization at each step; forward is explicit but may have stability issues. Neural parameterization vs analytical maps - neural is more flexible for singular measures but less precise than known analytical solutions.

- Failure signatures: If the network cannot learn the Markov kernel well, measures may not evolve correctly (e.g., particles clump incorrectly, fail to spread). If the loss is not properly formulated, the network may not approximate the correct plan. If the time step is too large, numerical instability may occur.

- First 3 experiments:
  1. Implement the neural backward scheme for the interaction energy EK starting at δ0 in 2D with r=1. Compare the evolved measures against the analytical formula from Theorem 4.
  2. Implement the neural forward scheme for the same setup and compare convergence to the backward scheme and analytical solution.
  3. Implement the particle flow baseline for EK starting with particles in a small square around 0, and compare the evolution to the neural schemes and analytical solution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the convergence analysis of the neural forward scheme be extended to functionals beyond the interaction energy, particularly for non-λ-convex kernels?
- Basis in paper: [explicit] The paper mentions that while the forward scheme converges nicely in numerical examples, it would be desirable to derive analytic formulas for steepest descent directions and convergence results for (11) for other functions than the interaction energy.
- Why unresolved: The convergence analysis is only provided for the interaction energy starting at a Dirac measure. The extension to other functionals requires new theoretical developments.
- What evidence would resolve it: Deriving analytical formulas for steepest descent directions and proving convergence results for the forward scheme with other functionals, especially those with non-smooth and non-λ-convex kernels.

### Open Question 2
- Question: How does restricting measures to specific supports, such as curves, affect the behavior of Wasserstein gradient flows with non-smooth Riesz kernels?
- Basis in paper: [inferred] The paper suggests that it would be interesting to extend the framework to restrict measures to certain supports, like curves, and examine corresponding flows, but does not provide any analysis.
- Why unresolved: The paper focuses on general measures in Euclidean space and does not explore the effects of restricting measures to lower-dimensional supports.
- What evidence would resolve it: Numerical experiments and theoretical analysis of Wasserstein gradient flows with Riesz kernels where the measures are constrained to lie on specific manifolds or curves.

### Open Question 3
- Question: Can the neural network-based methods for approximating Wasserstein gradient flows be effectively applied to posterior sampling in a Bayesian setting?
- Basis in paper: [explicit] The paper mentions that extending the framework to posterior sampling in a Bayesian setting would be an interesting direction, as a sampling-based approach could be useful for several applications.
- Why unresolved: The paper does not explore the application of the proposed methods to Bayesian inference or posterior sampling.
- What evidence would resolve it: Demonstrating the effectiveness of the neural schemes for approximating Wasserstein gradient flows in Bayesian posterior sampling problems, potentially through numerical experiments on synthetic or real-world datasets.

## Limitations

- The approach's scalability to very high dimensions remains unclear, as the particle flow baseline shows increased variance in high-dimensional settings.
- The computational cost of solving the JKO-like minimization in the backward scheme is not discussed in detail, and may be prohibitive for large-scale problems.
- The sensitivity to network architecture choices and hyperparameters is not extensively explored.

## Confidence

- **High Confidence**: The analytical convergence results for the JKO and forward schemes starting at a Dirac measure (Theorem 4 and Theorem 6). The derivation and proof structure appear sound.
- **Medium Confidence**: The neural network parameterization of Markov kernels for transport and velocity plans. While the concept is well-established in generative modeling, its application to singular measures via disintegrations is novel and would benefit from more empirical validation.
- **Medium Confidence**: The claim that neural schemes outperform particle flows in high dimensions. The evidence is based on comparisons with a limited number of particles and may not generalize to all scenarios.

## Next Checks

1. **Benchmark against alternative NN parameterizations**: Compare the proposed Markov kernel parameterization to other neural network approaches for learning transport maps or disintegrations, such as normalizing flows or conditional GANs.

2. **Ablation study on network architecture**: Systematically vary the neural network architecture (depth, width, activation functions) and training hyperparameters to assess their impact on the quality of the learned transport/velocity plans.

3. **Scalability analysis**: Evaluate the computational cost and approximation quality of the neural schemes as the dimension of the problem increases, comparing against particle flows with increasing numbers of particles.