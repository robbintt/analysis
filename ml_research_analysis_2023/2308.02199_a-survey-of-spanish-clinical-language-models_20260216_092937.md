---
ver: rpa2
title: A Survey of Spanish Clinical Language Models
arxiv_id: '2308.02199'
source_url: https://arxiv.org/abs/2308.02199
tags:
- corpus
- spanish
- clinical
- language
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey benchmarks 17 Spanish clinical language models across
  12 corpora, fine-tuning over 3000 models. Results show RigoBERTa 2 outperforms others,
  winning on 6 corpora.
---

# A Survey of Spanish Clinical Language Models

## Quick Facts
- **arXiv ID:** 2308.02199
- **Source URL:** https://arxiv.org/abs/2308.02199
- **Reference count:** 40
- **Primary result:** Benchmarks 17 Spanish clinical language models across 12 corpora, with RigoBERTa 2 outperforming others on 6 corpora

## Executive Summary
This survey benchmarks 17 Spanish clinical language models across 12 corpora, fine-tuning over 3000 models to evaluate performance on clinical NLP tasks. The study finds that RigoBERTa 2 outperforms other models, winning on 6 of 12 corpora, while multilingual models like XLM-RoBERTa-Large also perform well. Domain-adapted models show mixed results, and GPT models underperform encoder-only models in clinical tasks. The research highlights the need for better Spanish clinical language models and provides a public benchmark for future research.

## Method Summary
The study fine-tunes 17 Spanish clinical language models across 12 Spanish clinical corpora using a standardized pipeline with Hugging Face Transformers. Models are trained for up to 10 epochs with early stopping, using F1 score for NER tasks and micro-averaged F1 for classification. Hyperparameter grids include batch sizes (16/32/64) and learning rates (1e-5 to 4e-5). The evaluation compares model performance across different task types and includes both encoder-only and generative models, with results made publicly available through a Hugging Face Spaces leaderboard.

## Key Results
- RigoBERTa 2 achieves the best overall performance, winning on 6 out of 12 corpora
- Multilingual models like XLM-RoBERTa-Large perform competitively, with the latter being the best open-sourced option
- Domain adaptation of multilingual models does not consistently improve performance when clinical training data is limited
- Encoder-only models significantly outperform generative models for clinical NLP tasks like NER

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain adaptation of multilingual models does not consistently improve clinical Spanish NLP performance when training data is limited.
- **Mechanism:** Pretraining on general domain text followed by fine-tuning on a small clinical corpus can lead to overfitting and degradation of learned linguistic features.
- **Core assumption:** The clinical corpus used for domain adaptation is small and less diverse than the original pretraining corpus.
- **Evidence anchors:**
  - [section]: "Paradoxically, the more specific the model is, the worse that its results are... XLM-R_GalÃ©n uses as base model XML-RoBERTa-Large, the best performing model in the benchmark. Consequently this means that it is not always worth to adapt a model to a certain domain if the corpus for the adaptation is not big or good enough."
  - [corpus]: The clinical corpora in Spanish are relatively small compared to general domain corpora, as indicated by the limited number of samples in many of the corpora reviewed.
- **Break condition:** If a sufficiently large and diverse clinical corpus becomes available for domain adaptation, the performance of adapted models could surpass general models.

### Mechanism 2
- **Claim:** Larger, more general multilingual models outperform smaller, domain-specific Spanish models in clinical tasks.
- **Mechanism:** Multilingual models trained on vast amounts of text capture broader linguistic patterns and transfer knowledge more effectively to downstream tasks.
- **Core assumption:** The tasks in the benchmark are not highly specialized and can benefit from the broader knowledge captured by multilingual models.
- **Evidence anchors:**
  - [abstract]: "Multilingual models like XLM-RoBERTa-Large also perform well..."
  - [section]: "RigoBERTa 2 obtains the best results by far, winning in 6 out of the 12 corpora... XML-RoBERTa-Large obtains also very good results and is the best open-sourced model available."
- **Break condition:** If tasks become highly specialized or require deep domain knowledge not captured by general models, smaller domain-specific models could outperform.

### Mechanism 3
- **Claim:** Generative language models like GPT are not well-suited for encoder-focused clinical NLP tasks such as Named Entity Recognition (NER) without significant modifications.
- **Mechanism:** Generative models are optimized for text generation and lack the architectural components (like token classification heads) needed for tasks like NER.
- **Core assumption:** The OpenAI API does not provide straightforward ways to adapt generative models for token classification tasks.
- **Evidence anchors:**
  - [section]: "While the models more or less understood our queries, there were a lot of formatting errors and most of the times every entity was positioned wrong."
  - [section]: "The OpenAI API does not allow to do it straightforwardly and transforms the task into a text-to-text problem which turned out to be inefficient and error prone."
- **Break condition:** If the API or model architecture is modified to support token classification or if a suitable fine-tuning approach is developed, generative models could become more viable for NER tasks.

## Foundational Learning

- **Concept: Transformer architecture**
  - Why needed here: Understanding how transformer-based models like BERT, RoBERTa, and GPT work is crucial for interpreting the results and limitations of the models tested.
  - Quick check question: What is the key difference between encoder-only and decoder-only transformer architectures, and how does this affect their suitability for different NLP tasks?

- **Concept: Fine-tuning vs. domain adaptation**
  - Why needed here: The study involves both fine-tuning general models on specific tasks and domain adaptation by further pretraining on clinical data. Understanding the differences is important for interpreting the results.
  - Quick check question: How does domain adaptation differ from task-specific fine-tuning, and what are the potential benefits and drawbacks of each approach?

- **Concept: Named Entity Recognition (NER) and multi-label classification**
  - Why needed here: The benchmark includes both NER and multi-label classification tasks. Understanding these task types is necessary to interpret the evaluation metrics and results.
  - Quick check question: What are the key differences between NER and multi-label classification tasks, and how do the evaluation metrics (F1 score vs. micro F1) reflect these differences?

## Architecture Onboarding

- **Component map:**
  Data ingestion -> Hugging Face Datasets -> Model loading -> Hugging Face Transformers -> Fine-tuning pipeline -> Custom training loop with AdamW optimizer, learning rate scheduling, and early stopping -> Evaluation -> Calculating F1 score for NER tasks and micro F1 for classification tasks -> Leaderboard -> Hugging Face Spaces for creating a public benchmark

- **Critical path:**
  1. Load the dataset and preprocess it.
  2. Load the pre-trained model and configure it for the task.
  3. Fine-tune the model on the training data.
  4. Evaluate the model on the validation data.
  5. Use early stopping to prevent overfitting.
  6. Evaluate the final model on the test data and report the metrics.

- **Design tradeoffs:**
  - Using multilingual models vs. Spanish-only models: Multilingual models may capture more general linguistic patterns but might not be as specialized for Spanish clinical language.
  - Domain adaptation vs. no domain adaptation: Domain adaptation could improve performance on clinical tasks but might lead to overfitting if the clinical corpus is small.
  - Encoder-only vs. decoder-only models: Encoder-only models are better suited for tasks like NER, while decoder-only models are better for text generation.

- **Failure signatures:**
  - Overfitting: If the model performs well on the training data but poorly on the validation data, it may be overfitting.
  - Underfitting: If the model performs poorly on both the training and validation data, it may be underfitting.
  - Domain shift: If the model performs well on general text but poorly on clinical text, there may be a domain shift.

- **First 3 experiments:**
  1. Fine-tune BETO on the CANTEMIST corpus for NER and evaluate its performance.
  2. Fine-tune XLM-RoBERTa-Large on the same corpus and compare its performance to BETO.
  3. Fine-tune RigoBERTa 2 on the CANTEMIST corpus and compare its performance to the other models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can clinical language models be effectively evaluated on negation and uncertainty detection tasks?
- Basis in paper: [explicit] The paper mentions NUBes as a corpus focused on negation and uncertainty annotations in Spanish clinical texts, but doesn't evaluate models on this specific task.
- Why unresolved: The paper focuses on benchmarking models on a subset of available corpora, but doesn't include the negation and uncertainty detection task despite mentioning the relevant corpus.
- What evidence would resolve it: Evaluating the best-performing models from the benchmark on the NUBes corpus and comparing their performance on negation and uncertainty detection tasks.

### Open Question 2
- Question: What are the key factors contributing to the superior performance of RigoBERTa 2 compared to other Spanish clinical language models?
- Basis in paper: [explicit] The paper shows that RigoBERTa 2 outperforms other models on 6 out of 12 corpora, but doesn't provide a detailed analysis of why this model performs better.
- Why unresolved: The paper presents benchmark results but doesn't investigate the underlying reasons for RigoBERTa 2's superior performance, such as architectural differences or training data composition.
- What evidence would resolve it: Conducting an ablation study to identify the key components of RigoBERTa 2's architecture or training process that contribute to its improved performance.

### Open Question 3
- Question: How do the performance and cost-effectiveness of fine-tuned encoder models compare to few-shot and fine-tuned generative models for clinical text classification tasks?
- Basis in paper: [explicit] The paper includes a comparison of GPT models (few-shot and fine-tuned) with an encoder model (bsc-bio-ehr-es) for a multi-label classification task, showing that the encoder model performs better at a lower cost.
- Why unresolved: The comparison is limited to a single task and doesn't explore the generalizability of these findings across different clinical text classification tasks or the potential benefits of combining encoder and generative models.
- What evidence would resolve it: Extending the comparison to multiple clinical text classification tasks and exploring hybrid approaches that leverage both encoder and generative models.

## Limitations

- The clinical corpora available for domain adaptation are relatively small, potentially limiting the performance of specialized models.
- The study focuses on a specific set of 12 corpora and 7 language models, which may not represent the full landscape of available resources.
- The use of the OpenAI API for GPT models introduces uncertainty due to limited control over fine-tuning parameters and task formatting.

## Confidence

**High Confidence:**
- The comparative performance ranking of RigoBERTa 2 as the top-performing model
- The general trend that multilingual models like XLM-RoBERTa-Large perform well on clinical Spanish tasks
- The observation that GPT models struggle with token classification tasks without significant modifications

**Medium Confidence:**
- The conclusion that domain adaptation does not consistently improve performance
- The specific performance metrics on individual corpora
- The assertion that encoder-only models are better suited for clinical NLP tasks than generative models

**Low Confidence:**
- The exact magnitude of performance differences between models
- The generalizability of findings to other clinical languages or domains
- The long-term viability of the benchmark as new models and corpora become available

## Next Checks

1. Re-run the benchmark with specified random seeds to verify result reproducibility and calculate confidence intervals for model performance differences.

2. Test domain adaptation with larger clinical corpora to determine if the observed limitation is due to corpus size rather than the adaptation approach itself.

3. Evaluate alternative GPT fine-tuning approaches using local model deployments to test whether the API limitations were the primary factor in poor performance.