---
ver: rpa2
title: The Perils & Promises of Fact-checking with Large Language Models
arxiv_id: '2310.13549'
source_url: https://arxiv.org/abs/2310.13549
tags:
- fact-checking
- language
- gpt-3
- accuracy
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates large language models (LLMs) GPT-3.5 and
  GPT-4 for fact-checking using an original methodology that combines iterative searching
  and agent-based reasoning. The authors test the models on two datasets: a PolitiFact
  dataset of US political claims and a multilingual dataset from Data Commons.'
---

# The Perils & Promises of Fact-checking with Large Language Models

## Quick Facts
- arXiv ID: 2310.13549
- Source URL: https://arxiv.org/abs/2310.13549
- Reference count: 40
- Primary result: GPT-4 outperforms GPT-3.5 in fact-checking, with accuracy improving significantly when contextual information is provided

## Executive Summary
This paper evaluates large language models GPT-3.5 and GPT-4 for fact-checking using an original methodology combining iterative searching and agent-based reasoning. The authors test the models on two datasets: a PolitiFact dataset of US political claims and a multilingual dataset from Data Commons. Results show GPT-4 significantly outperforms GPT-3.5, particularly when provided with contextual information from external sources. The study reveals substantial performance variations across languages, with translated English prompts often yielding better results than original non-English prompts. While LLMs show promise for fact-checking applications, the authors emphasize the need for caution due to inconsistent accuracy, particularly for ambiguous claims, and stress the importance of explainability and human oversight.

## Method Summary
The authors employ a ReAct (reasoning + acting) framework to create fact-checking agents using GPT-3.5 and GPT-4. The agents iteratively search Google (excluding fact-checking websites) to retrieve evidence, evaluate results, and decide whether to continue searching or finalize their verdict. The methodology is tested on two datasets: the PolitiFact dataset (3,000 claims) and a multilingual dataset from Data Commons. Experiments are conducted with four conditions: two models (GPT-3.5/GPT-4) × two context conditions (with/without external evidence). Performance is evaluated across different veracity labels and languages, with particular attention to the impact of translating non-English claims to English.

## Key Results
- GPT-4 achieves 83% accuracy on PolitiFact dataset versus GPT-3.5's 67% accuracy
- Contextual information significantly improves model calibration and reduces false positives
- Translated English prompts outperform original non-English prompts in 17 out of 20 languages tested
- Both models achieve over 90% accuracy for "pants-on-fire" claims but struggle with "half-true" and "mostly-false" verdicts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative search with ReAct enables self-correcting fact-checking
- Mechanism: The agent queries Google iteratively, evaluates results, and decides whether to continue or finalize. This loop mimics human reasoning: observe → reflect → act → observe.
- Core assumption: Google results provide sufficient signal to judge claim veracity and the agent can parse them correctly.
- Evidence anchors:
  - [abstract] "agents explain their reasoning and cite the relevant sources from the retrieved context"
  - [section] "The model is initially prompted with the claim and then decides if it needs to take actions... The model can decide to either return a final answer or retrieve information with a different query."
  - [corpus] Weak: no direct evidence of self-correction in related papers; must be inferred from ReAct usage patterns.
- Break condition: If the agent repeatedly retrieves irrelevant results, it may loop or return an incorrect verdict.

### Mechanism 2
- Claim: English translations boost multilingual fact-checking accuracy
- Mechanism: Translating non-English claims to English increases model understanding and access to relevant evidence, as most search results are in English.
- Core assumption: The translation preserves semantic content accurately enough for the model to reason about the claim.
- Evidence anchors:
  - [abstract] "Performance varies by language, with translated English prompts often yielding better results than original non-English prompts"
  - [section] "In all but three languages... translating increased the F1 score of the model"
  - [corpus] Moderate: several related papers on multilingual LLMs discuss language performance gaps but not specific to fact-checking.
- Break condition: If translation introduces subtle errors or cultural nuance is lost, model accuracy may degrade despite better English performance.

### Mechanism 3
- Claim: Contextual retrieval improves model calibration and reduces false positives
- Mechanism: Providing external evidence helps the model avoid guessing and makes it less likely to claim a verdict when uncertain.
- Core assumption: Models without context tend to over-predict false verdicts due to uncertainty.
- Evidence anchors:
  - [abstract] "accuracy varies based on query language and claim veracity"
  - [section] "In the context condition, GPT-3.5 is significantly better calibrated... Both models achieve an accuracy of over 90% for claims that are labeled with 'pants-on-fire'"
  - [corpus] Moderate: Explainable fact-checking literature supports this but not specifically tested here.
- Break condition: If context is noisy or misleading, the model may be swayed incorrectly.

## Foundational Learning

- Concept: ReAct framework (reasoning + acting)
  - Why needed here: It structures how the LLM interacts with external tools (Google) and decides when to stop searching.
  - Quick check question: What are the two main actions a ReAct agent takes in this paper?
- Concept: Fact-checking ordinal scales (e.g., True, Mostly-True, Half-True, Mostly-False, False, Pants-Fire)
  - Why needed here: The evaluation metrics and dataset labels rely on these categories.
  - Quick check question: How does the paper simplify the PolitiFact labels for binary accuracy computation?
- Concept: Cross-lingual evaluation and translation impact
  - Why needed here: The multilingual experiment hinges on comparing original vs. translated claims.
  - Quick check question: Which language showed the largest accuracy drop when using the original claim instead of English translation?

## Architecture Onboarding

- Component map: LLM (GPT-3.5/GPT-4) → ReAct agent → Google Search API → Filtered results → Verdict + Reasoning
- Critical path: Prompt → Google query → Retrieve 10 results → Parse results → Decide verdict → Cite source
- Design tradeoffs: 10 results per query balances recall and noise; three-query limit prevents infinite loops but may miss evidence.
- Failure signatures: High "uncertain" rate, inconsistent verdicts across languages, low accuracy on "Half-True" claims.
- First 3 experiments:
  1. Run same claim through both GPT-3.5 and GPT-4 with and without context to confirm performance gap.
  2. Translate a non-English claim to English and compare model verdicts.
  3. Remove PolitiFact domains from Google results and test if model still identifies correct verdicts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be fine-tuned to improve their performance on non-English fact-checking tasks while maintaining accuracy across multiple languages?
- Basis in paper: [explicit] The paper notes that GPT-3.5 and GPT-4 perform significantly better when fact-checking English-language claims, even when those claims are translated from other languages, and that performance varies substantially across languages.
- Why unresolved: The paper identifies the problem but does not propose solutions for improving multilingual fact-checking accuracy or address whether fine-tuning strategies could help.
- What evidence would resolve it: Experimental results comparing different fine-tuning approaches for multilingual fact-checking tasks, with detailed analysis of which techniques (e.g., multilingual training data, language-specific adapters) yield the best performance across different language families.

### Open Question 2
- Question: What specific post-training reinforcement learning from human feedback (RLHF) mechanisms contribute to LLMs' knowledge of current events beyond their training cutoff dates?
- Basis in paper: [explicit] The paper notes that both GPT-3.5 and GPT-4 show continued improvement in accuracy over time, suggesting that post-training refinements via RLHF may introduce new knowledge, particularly for more recent events.
- Why unresolved: The paper observes this phenomenon but doesn't investigate which specific RLHF mechanisms are responsible or how they integrate new information into the model's knowledge base.
- What evidence would resolve it: Analysis of model performance patterns before and after specific RLHF updates, combined with controlled experiments testing model knowledge of deliberately introduced information.

### Open Question 3
- Question: How can the explainability mechanisms in LLMs be optimized to help users better verify and trust fact-checking verdicts?
- Basis in paper: [explicit] The paper emphasizes that while LLMs show promise in fact-checking, integrating mechanisms allowing for verification of their verdict and reasoning is paramount, and that future research should explore if critically examining the reasons and references provided by LLMs can enhance their fact-checking ability.
- Why unresolved: The paper identifies the need for better explainability but doesn't explore what specific improvements to explanation mechanisms would be most effective.
- What evidence would resolve it: User studies comparing different explanation formats and detail levels, measuring both user comprehension of LLM reasoning and the impact on verification accuracy.

## Limitations
- Models struggle significantly with ambiguous claims (half-true, mostly-false) where accuracy drops notably
- The translation approach may mask underlying multilingual model limitations rather than solving them
- Evaluation relies on two datasets that may not represent the full diversity of real-world fact-checking scenarios

## Confidence

**High confidence** in GPT-4's superior performance over GPT-3.5 when contextual information is provided, supported by clear numerical comparisons across multiple metrics.

**Medium confidence** in the translation benefit for multilingual fact-checking, as results show consistent improvement but the underlying mechanisms remain unclear.

**Low confidence** in the ReAct framework's self-correction capability, as the paper shows iterative searching but doesn't provide evidence that agents actually correct their initial judgments.

## Next Checks

1. **Ablation test on search query count**: Run experiments with 1, 3, and unlimited search queries to determine if the three-query limit is optimal or if more queries would improve accuracy, particularly for ambiguous claims.

2. **Cross-dataset validation**: Apply the same methodology to a third, independently curated fact-checking dataset (e.g., Snopes or FactCheck.org) to verify if the observed performance patterns hold across different domains and claim types.

3. **Translation quality assessment**: Compare model performance when using professional human translations versus automated translations for non-English claims to isolate whether accuracy improvements come from better translation or better search result access.