---
ver: rpa2
title: Towards General Text Embeddings with Multi-stage Contrastive Learning
arxiv_id: '2308.03281'
source_url: https://arxiv.org/abs/2308.03281
tags:
- text
- data
- pre-training
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage contrastive learning approach
  to develop a general-purpose text embedding model called GTE. The model is trained
  on a diverse mixture of datasets from multiple sources, including web pages, academic
  papers, hyperlinks, social media, knowledge bases, and code repositories.
---

# Towards General Text Embeddings with Multi-stage Contrastive Learning

## Quick Facts
- arXiv ID: 2308.03281
- Source URL: https://arxiv.org/abs/2308.03281
- Authors: 
- Reference count: 29
- One-line primary result: GTEbase (110M parameters) outperforms OpenAI's embedding API and even surpasses 10x larger text embedding models on MTEB

## Executive Summary
This paper presents GTE, a general-purpose text embedding model developed through multi-stage contrastive learning. The model achieves substantial performance gains by pre-training on a diverse mixture of 800M text pairs from web sources and fine-tuning on 3M annotated triples across multiple tasks. Notably, GTEbase with just 110M parameters outperforms both OpenAI's black-box embedding API and significantly larger models on the massive text embedding benchmark (MTEB), while also excelling at code search tasks without task-specific fine-tuning.

## Method Summary
GTE employs a two-stage contrastive learning approach: first pre-training on mined text pairs from diverse web sources using an improved contrastive loss with bidirectional and expanded negative sampling, then supervised fine-tuning on annotated text triples across multiple tasks. The model uses a BERT encoder with mean pooling to generate fixed-size embeddings, trained with a multinomial sampling strategy (α=0.5) to balance representation across data sources. The improved contrastive loss combines variants from previous work to create richer gradient signals through bidirectional comparisons and larger negative pools.

## Key Results
- GTEbase (110M parameters) outperforms OpenAI's black-box embedding API and even surpasses 10x larger text embedding models on MTEB
- Achieves state-of-the-art performance on CodeSearchNet without task-specific fine-tuning for individual programming languages
- Demonstrates strong zero-shot retrieval performance on BEIR benchmark, outperforming many supervised models
- Shows consistent improvements across all 7 MTEB tasks with optimal data mixing parameter α=0.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage contrastive learning improves generalization across heterogeneous tasks
- Mechanism: By pre-training on weak supervision signals from diverse web sources and then fine-tuning on task-specific triples, the model learns both broad semantic understanding and fine-grained relevance distinctions
- Core assumption: Weak supervision signals (like hyperlink structure or title-body pairs) contain meaningful semantic correlations that transfer to downstream tasks
- Evidence anchors: [abstract] performance gains from increased training data; [section 3.2] weak supervision availability; limited corpus evidence

### Mechanism 2
- Claim: Improved contrastive loss with bidirectional and expanded negative sampling enhances discrimination
- Mechanism: The loss function considers both query-to-document and document-to-query directions while incorporating in-batch documents as negatives, creating a richer learning signal than standard NCE
- Core assumption: Including more negative samples and bidirectional comparisons provides better gradient estimates for the contrastive objective
- Evidence anchors: [section 3.4] loss variant combination; [section 5.5] consistent performance improvements; limited related work evidence

### Mechanism 3
- Claim: Large-scale data mixing with controlled sampling improves robustness
- Mechanism: The multinomial sampling with parameter α=0.5 balances representation across diverse data sources, preventing any single domain from dominating the learned representations
- Core assumption: Different data sources provide complementary information, and balancing their contribution prevents overfitting to task-specific shortcuts
- Evidence anchors: [section 3.4] multinomial distribution for batch sampling; [section 5.4] optimal α=0.5 improves results; limited evidence on data mixing strategies

## Foundational Learning

- Concept: Contrastive learning objective and InfoNCE loss
  - Why needed here: The entire training approach relies on learning representations by contrasting positive and negative pairs, making understanding the objective fundamental to grasping the method
  - Quick check question: What is the difference between the standard InfoNCE loss and the improved contrastive loss used in this work?

- Concept: Dual-encoder architecture with mean pooling
  - Why needed here: The model architecture directly impacts how text is converted to embeddings and what kind of representations are learned
  - Quick check question: How does mean pooling across token representations affect the model's ability to capture phrase-level semantics?

- Concept: Supervised fine-tuning with hard negatives
  - Why needed here: Understanding how supervised data is used differently from unsupervised data is crucial for grasping the multi-stage approach
  - Quick check question: Why might hard negatives be more effective than random negatives during fine-tuning?

## Architecture Onboarding

- Component map: BERT encoder → mean pooling → contrastive loss → multi-stage training pipeline
- Critical path: Data collection → pre-training with improved contrastive loss → supervised fine-tuning → evaluation
- Design tradeoffs: Larger batch sizes improve pre-training but increase memory costs; bidirectional loss improves discrimination but adds complexity
- Failure signatures: Poor performance on BEIR suggests issues with zero-shot retrieval; large gap between pre-training and fine-tuning performance indicates catastrophic forgetting
- First 3 experiments:
  1. Train a baseline with standard InfoNCE loss and compare to the improved contrastive loss on a small dataset
  2. Vary the α parameter in data sampling to find the optimal balance between different data sources
  3. Test the impact of different negative sampling strategies (random vs. hard negatives) during fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GTE scale with even larger models (e.g., 1B+ parameters) and more diverse data sources beyond what was used in this study?
- Basis in paper: [inferred] The paper mentions investigating the impact of scaling the number of data sources, batch size, and model parameters on the quality of learned text embeddings. However, the scaling analysis is limited to models up to 330M parameters.
- Why unresolved: The paper does not explore the performance of GTE with models larger than 330M parameters or with additional data sources beyond those used in this study.
- What evidence would resolve it: Evaluating GTE with larger models (e.g., 1B+ parameters) and incorporating more diverse data sources could provide insights into the potential performance gains and limitations of the approach.

### Open Question 2
- Question: How does the performance of GTE compare to other state-of-the-art text embedding models when fine-tuned on specific downstream tasks?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of GTE on various benchmarks and tasks, but it does not provide a direct comparison to other state-of-the-art text embedding models when fine-tuned on specific downstream tasks.
- Why unresolved: The paper focuses on the general performance of GTE across different tasks and benchmarks, but does not evaluate its performance when fine-tuned for specific downstream tasks compared to other models.
- What evidence would resolve it: Conducting a comprehensive comparison of GTE with other state-of-the-art text embedding models, fine-tuned on specific downstream tasks, would provide a clearer understanding of its relative performance and applicability in different scenarios.

### Open Question 3
- Question: How does the choice of pre-training tasks and data sources impact the performance of GTE on different downstream tasks?
- Basis in paper: [explicit] The paper mentions that GTE benefits from a diverse training data mixture, enabling it to achieve good generalization performance for single vector embedding. However, it does not provide a detailed analysis of how the choice of pre-training tasks and data sources affects performance on specific downstream tasks.
- Why unresolved: While the paper demonstrates the effectiveness of GTE on various benchmarks, it does not investigate the impact of different pre-training tasks and data sources on the model's performance on specific downstream tasks.
- What evidence would resolve it: Conducting an ablation study to analyze the impact of different pre-training tasks and data sources on GTE's performance across various downstream tasks would provide insights into the importance of each component and guide the selection of optimal pre-training strategies.

## Limitations

- The exact data sources and training hyperparameters remain unspecified, making it difficult to assess whether performance gains stem from methodology or data scale
- The improved contrastive loss formulation is referenced but not fully detailed, potentially introducing implementation variations that affect reproducibility
- The reliance on weak supervision signals from web sources may lead to learning spurious correlations that don't generalize to downstream tasks

## Confidence

**High Confidence**: GTEbase outperforms OpenAI's embedding API and larger models on MTEB, supported by extensive evaluation across 56 datasets and 7 tasks.

**Medium Confidence**: Multi-stage contrastive learning with improved loss functions provides substantial performance gains, though the exact contribution of each component remains partially unclear.

**Low Confidence**: Zero-shot retrieval performance on BEIR may be inflated due to potential overfitting to MTEB-style tasks during pre-training.

## Next Checks

1. **Ablation on data source contribution**: Systematically remove individual data sources and retrain the model to quantify the marginal contribution of each domain to overall performance.

2. **Transfer robustness evaluation**: Test the model's performance on out-of-distribution tasks not represented in pre-training or fine-tuning datasets to validate general semantic representations.

3. **Reproducibility study with simplified loss**: Implement a baseline model using standard InfoNCE loss without bidirectional and expanded negative sampling to isolate the contribution of the improved contrastive loss.