---
ver: rpa2
title: 'Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture'
arxiv_id: '2310.12109'
source_url: https://arxiv.org/abs/2310.12109
tags:
- matrices
- monarch
- then
- polynomial
- polynomials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Monarch Mixer (M2) is a sub-quadratic neural architecture that\
  \ uses Monarch matrices\u2014a class of structured matrices that generalize FFT\
  \ and achieve hardware efficiency\u2014to mix information along both sequence length\
  \ and model dimension axes. By parameterizing M2 with p-order Monarch matrices,\
  \ it scales sub-quadratically with O(pN(p+1)/p) complexity."
---

# Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture

## Quick Facts
- arXiv ID: 2310.12109
- Source URL: https://arxiv.org/abs/2310.12109
- Authors: Not specified in source
- Reference count: 40
- One-line primary result: M2-BERT matches BERT-large quality with 24% fewer parameters and 9.1× higher throughput at sequence length 4K

## Executive Summary
Monarch Mixer (M2) introduces a sub-quadratic neural architecture that achieves competitive performance to Transformers while using significantly fewer parameters and offering improved hardware efficiency. By leveraging Monarch matrices—structured matrices that generalize FFT and enable efficient GEMM operations—M2 scales sub-quadratically with O(pN^(p+1)/p) complexity. The architecture demonstrates strong empirical results across three domains: BERT-style language modeling, ViT-style image classification, and GPT-style causal language modeling.

## Method Summary
M2 replaces traditional attention and MLP layers with Monarch-based mixing operations that handle both sequence and dimension mixing. The architecture uses parameterized Monarch matrices as products of block-diagonal matrices interleaved with permutations, enabling sub-quadratic complexity while maintaining expressiveness. For causal settings, novel theory based on multivariate polynomial evaluation and interpolation allows M2 to enforce causality without quadratic masking. The model achieves hardware efficiency through structured decomposition that enables GEMM acceleration, with FLOP utilization significantly higher than FFT-based approaches.

## Key Results
- M2-BERT matches BERT-base and BERT-large in GLUE downstream quality with 27% and 24% fewer parameters respectively
- M2-ViT outperforms ViT-b by 1% accuracy with half the parameters on ImageNet
- M2-GPT achieves Transformer-level perplexity without attention or MLPs in causal settings
- 9.1× higher throughput at sequence length 4K compared to standard implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monarch matrices provide sub-quadratic scaling while maintaining hardware efficiency through structured decomposition.
- Mechanism: Monarch matrices are parameterized as products of block-diagonal matrices interleaved with permutations, allowing O(pN^(p+1)/p) complexity versus O(N²) for dense matrices.
- Core assumption: Block-diagonal structure preserves sufficient expressiveness while enabling GEMM acceleration.
- Evidence anchors:
  - [abstract] "Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically."
  - [section 3.1] "Monarch matrices [12] are a sub-quadratic class of structured matrices that are hardware-efficient and expressive. They can represent many linear transforms, including convolutions, Toeplitz-like transforms, low-displacement rank transforms, and orthogonal polynomials."
- Break condition: If block size becomes too small, loses expressiveness; if too large, loses sub-quadratic benefit.

### Mechanism 2
- Claim: M2 achieves both sequence and dimension mixing through alternating Monarch-based operations.
- Mechanism: M2 layers first mix along sequence dimension using M2(K1 ⊙ M1X), then along embedding dimension using M4σ(M3X⊤).
- Core assumption: The same sub-quadratic primitive can effectively mix information in both dimensions.
- Evidence anchors:
  - [abstract] "Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension"
  - [section 3.2] "M2 uses Monarch matrices to mix information along the sequence and model dimension axes."
- Break condition: If mixing along one dimension dominates learning, the other dimension's mixing becomes redundant.

### Mechanism 3
- Claim: Causal parameterization through polynomial degree constraints enables sub-quadratic causal convolutions.
- Mechanism: By restricting polynomial degrees in bivariate basis and using Kronecker substitution, M2 can enforce causality without full masking.
- Core assumption: Controlled polynomial degree growth prevents information leakage in causal setting.
- Evidence anchors:
  - [abstract] "we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic."
  - [section 4] "To alleviate this quadratic bottleneck with Monarch matrices, we develop new theory to characterize which parameterizations of Monarch matrices maintain causality."
- Break condition: If polynomial degree constraints become too restrictive, model expressiveness suffers.

## Foundational Learning

- Concept: Matrix decomposition and structured matrices
  - Why needed here: Understanding how Monarch matrices decompose into block-diagonal factors is crucial for grasping their efficiency properties.
  - Quick check question: How does the block-diagonal structure of Monarch matrices enable GEMM acceleration?

- Concept: Polynomial evaluation and interpolation
  - Why needed here: The theoretical analysis of M2's causal parameterization relies on interpreting matrix multiplication as polynomial operations.
  - Quick check question: How does bivariate polynomial evaluation relate to the block structure of Monarch matrices?

- Concept: Hardware memory hierarchy and FLOP utilization
  - Why needed here: Performance comparisons depend on understanding GPU memory bandwidth versus compute capacity.
  - Quick check question: Why do memory-bound operations like FFT have lower FLOP utilization than compute-bound operations like dense matrix multiplication?

## Architecture Onboarding

- Component map:
  - Sequence mixer: M2(K1 ⊙ M1X) - bidirectional or causal convolution using Monarch matrices
  - Dimension mixer: M4σ(M3X⊤) - MLP-like mixing using Monarch matrices
  - Layer norm and residual connections around each mixer
  - Optional gating and short convolutions for expressivity

- Critical path: Forward pass through M2 layer: input → sequence mixing → dimension mixing → output
- Design tradeoffs:
  - Block size vs expressiveness: Larger blocks increase expressiveness but reduce sub-quadratic benefit
  - Learnable vs fixed Monarch matrices: Fixed (DFT) matrices are more efficient, learnable ones more expressive
  - Depth vs width: Tradeoff between stacking M2 layers and increasing model dimension

- Failure signatures:
  - Low FLOP utilization (< 10%): Indicates memory-bound bottleneck, possibly due to small block size
  - Training instability: May indicate insufficient expressivity from overly constrained Monarch matrices
  - Poor generalization: Could result from improper mixing balance between sequence and dimension

- First 3 experiments:
  1. Implement M2 layer with fixed DFT Monarch matrices and verify it computes correct convolution
  2. Replace one attention block in BERT with M2 layer and check GLUE performance
  3. Compare throughput of M2 vs FlashAttention at sequence length 2K on A100 GPU

## Open Questions the Paper Calls Out

- Can Monarch Mixer architectures be further optimized with systems-level techniques like kernel fusion to improve hardware utilization?
- Can the blowup from n to 2^p · n in causal Monarch convolutions be reduced to n → 2n for p > 2?
- Can Monarch Mixer achieve the same level of hardware efficiency and performance on inference tasks as it does on training tasks?
- Can Monarch Mixer architectures be successfully applied to other domains beyond the three tested (BERT-style language modeling, ViT-style image classification, and GPT-style causal language modeling)?

## Limitations
- The 9.1× throughput improvement at sequence length 4K was measured on a specific A100 GPU configuration and may not generalize to other hardware
- GLUE benchmark results rely on comparison to published Transformer results without independent reproduction
- Causal M2-GPT claims rely on theoretical bounds rather than extensive perplexity measurements across diverse datasets

## Confidence
- **High**: Sub-quadratic complexity analysis and hardware efficiency claims
- **Medium**: Quality retention claims across BERT, ViT, and GPT variants
- **Low**: Causal parameterization theory and empirical validation

## Next Checks
1. Measure M2 throughput on multiple GPU architectures (A100, H100, and lower-end consumer GPUs) to verify the 9.1× improvement is not architecture-specific
2. Implement M2-GPT with varying polynomial degrees and block sizes to empirically validate that information leakage is prevented while maintaining expressiveness
3. Fine-tune M2-BERT on multiple text classification datasets beyond GLUE (e.g., IMDb, AG News) to confirm the 27% parameter reduction consistently maintains performance across domains