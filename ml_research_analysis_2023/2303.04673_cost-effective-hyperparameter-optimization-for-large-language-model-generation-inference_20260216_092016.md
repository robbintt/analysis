---
ver: rpa2
title: Cost-Effective Hyperparameter Optimization for Large Language Model Generation
  Inference
arxiv_id: '2303.04673'
source_url: https://arxiv.org/abs/2303.04673
tags:
- optimization
- inference
- generation
- hyperparameter
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EcoOptiGen, a framework for cost-effective
  hyperparameter optimization of large language model (LLM) generation inference.
  The method leverages economical hyperparameter optimization and cost-based pruning
  to find optimal settings for parameters like number of responses, temperature, and
  max tokens under a given budget constraint.
---

# Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference

## Quick Facts
- arXiv ID: 2303.04673
- Source URL: https://arxiv.org/abs/2303.04673
- Reference count: 24
- Primary result: EcoOptiGen framework finds better LLM inference hyperparameters than default settings through cost-based pruning and probabilistic success metrics

## Executive Summary
This paper presents EcoOptiGen, a framework for cost-effective hyperparameter optimization of large language model (LLM) generation inference. The method leverages economical hyperparameter optimization and cost-based pruning to find optimal settings for parameters like number of responses, temperature, and max tokens under a given budget constraint. Experiments with GPT-3.5/GPT-4 on code generation, math problem solving, and text summarization tasks show that EcoOptiGen significantly outperforms default HELM settings and other baseline methods. The pruning strategy enables searching 2-27x more configurations under the same budget, leading to substantial performance gains.

## Method Summary
EcoOptiGen uses a BlendSearch approach combining Bayesian optimization with local search to optimize LLM inference hyperparameters. The framework implements cost-based pruning that terminates trials early when expected cost exceeds budget, by varying number of responses and data examples during evaluation. It replaces traditional binary success metrics with probabilistic success rate, which estimates the probability of at least one correct response from multiple attempts. The method operates under budget constraints (B_i tokens per instance, B_o total tokens) and evaluates configurations using a utility function specific to each task type.

## Key Results
- EcoOptiGen significantly outperforms default HELM settings and other baseline methods on code generation, math problem solving, and text summarization tasks
- The pruning strategy enables searching 2-27x more configurations under the same budget
- Best model after tuning is not always the commonly recommended one, demonstrating value of holistic optimization
- Probabilistic success rate provides better optimization signal than binary success rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning invalid configurations early reduces total evaluation cost
- Mechanism: Uses cost-based pruning that terminates trials when expected cost exceeds budget, by varying number of responses and data examples during evaluation
- Core assumption: If configuration x1 has equal or higher max tokens and number of responses than x2, then x1 will have equal or higher average token consumption than x2
- Evidence anchors:
  - [abstract]: "The pruning strategy enables searching 2-27x more configurations under the same budget"
  - [section]: "If a configuration x is invalid, it is beneficial to terminate the trial early to save unnecessary cost"
  - [corpus]: Weak - no direct evidence about pruning efficiency in cited papers
- Break condition: Assumption 3.1 occasionally violated when max valid n equals or exceeds min invalid n

### Mechanism 2
- Claim: Probabilistic success rate provides better hyperparameter optimization signal than binary success rate
- Mechanism: Replaces success rate with probabilistic success rate that estimates probability of at least one correct response from n responses
- Core assumption: Generated responses are independent events, allowing probability calculation
- Evidence anchors:
  - [abstract]: "the probabilistic success rate which has a stronger distinguishing power"
  - [section]: "the estimated probability for one response to be correct on this request is m/n"
  - [corpus]: Weak - no corpus evidence about probabilistic success rate metric
- Break condition: Response independence assumption violated if responses are correlated

### Mechanism 3
- Claim: BlendSearch optimization finds better configurations than default settings
- Mechanism: Combines Bayesian optimization with local search, using cost as optimization metric
- Core assumption: Blackbox optimization approach works for inference hyperparameter optimization
- Evidence anchors:
  - [abstract]: "EcoOptiGen significantly outperforms default HELM settings and other baseline methods"
  - [section]: "We chose a method that combines Bayesian optimization and local search, named BlendSearch"
  - [corpus]: Weak - no corpus evidence about BlendSearch specific to inference optimization
- Break condition: Cost metric doesn't properly capture utility for certain tasks

## Foundational Learning

- Concept: Blackbox optimization techniques
  - Why needed here: System uses blackbox optimization approach for hyperparameter tuning where utility functions are complex
  - Quick check question: What distinguishes blackbox optimization from gradient-based methods?

- Concept: Cost-based pruning strategies
  - Why needed here: System implements cost-based pruning to terminate invalid trials early and save resources
  - Quick check question: How does the system determine when to prune a configuration?

- Concept: Probabilistic vs deterministic metrics
  - Why needed here: System uses probabilistic success rate instead of binary success rate for better optimization signal
  - Quick check question: What advantage does probabilistic success rate have over binary success rate?

## Architecture Onboarding

- Component map: Hyperparameter searcher (BlendSearch) → Configuration evaluator (with pruning) → LLM API → Utility/cost calculation → Decision on configuration validity
- Critical path: Configuration proposal → Initial validity check → Progressive evaluation with data subsampling → Utility/cost computation → Return result or prune
- Design tradeoffs: Early pruning saves cost but may miss borderline cases; probabilistic metrics provide better signals but require independence assumption
- Failure signatures: Invalid configurations not pruned early; probabilistic assumptions violated; searcher gets stuck in local optima
- First 3 experiments:
  1. Run with simple search (no pruning) on small dataset to verify basic functionality
  2. Test pruning logic with known valid/invalid configurations to validate Assumption 3.1
  3. Compare probabilistic success rate vs binary success rate on synthetic data to verify improved signal quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., decoder-only vs encoder-decoder) impact the effectiveness of hyperparameter optimization?
- Basis in paper: [inferred] The paper focuses on GPT-3.5 models, but doesn't explore other architectures.
- Why unresolved: The study is limited to GPT-3.5, leaving questions about generalizability to other architectures.
- What evidence would resolve it: Experiments comparing EcoOptiGen's performance across different LLM architectures (e.g., GPT, BERT, T5).

### Open Question 2
- Question: How does the choice of optimization metric (e.g., success rate vs probabilistic success rate) affect the final model performance?
- Basis in paper: [explicit] The paper mentions that probabilistic success rate has a stronger distinguishing power than success rate.
- Why unresolved: The paper doesn't provide a comprehensive comparison of different optimization metrics.
- What evidence would resolve it: A systematic study comparing the impact of different optimization metrics on final model performance.

### Open Question 3
- Question: How can we automatically search for optimal numbers and choices of demonstration examples in prompts?
- Basis in paper: [explicit] The paper mentions that automatically searching for optimal numbers and choices of demonstration examples can potentially result in more effective ways of using the inference budget.
- Why unresolved: The paper doesn't provide a concrete method for automatic prompt optimization.
- What evidence would resolve it: A proposed algorithm or framework for automatic prompt optimization.

## Limitations

- Budget constraint modeling may not capture all cost scenarios, with pruning effectiveness varying across different budget regimes
- Assumption 3.1 (token consumption monotonicity) may not hold universally across diverse task types and prompt sensitivities
- Results primarily demonstrated on APPS, HumanEval, MATH, and XSum datasets, limiting generalizability to other domains

## Confidence

**High confidence**: Core observation that hyperparameter tuning improves LLM generation inference performance, and that cost-effective search strategies can find better configurations than default settings.

**Medium confidence**: Specific pruning strategy effectiveness (2-27x more configurations searched) and probabilistic success rate metric improvement over binary metrics.

**Low confidence**: Generalizability of Assumption 3.1 across diverse task types and optimal parameter ranges suggested in Table 1 for new applications.

## Next Checks

1. **Budget sensitivity analysis**: Systematically vary B_i and B_o parameters across different task types to determine the robustness of the pruning strategy and identify budget thresholds where performance degrades.

2. **Assumption stress testing**: Create controlled experiments with diverse prompt-response patterns to test how often Assumption 3.1 fails, and quantify the impact of incorrect pruning decisions on final results.

3. **Cross-domain generalization**: Apply EcoOptiGen to at least two additional task domains not covered in the paper (e.g., conversational agents and creative writing) to validate the framework's broader applicability and identify domain-specific tuning requirements.