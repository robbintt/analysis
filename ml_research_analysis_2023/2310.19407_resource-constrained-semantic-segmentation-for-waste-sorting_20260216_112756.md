---
ver: rpa2
title: Resource Constrained Semantic Segmentation for Waste Sorting
arxiv_id: '2310.19407'
source_url: https://arxiv.org/abs/2310.19407
tags:
- enet
- segmentation
- icnet
- loss
- bisenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study focuses on developing resource-constrained semantic
  segmentation models for waste sorting in industrial settings. The primary objective
  is to create models that fit within a 10MB memory constraint while maintaining high
  Mean IoU (mIoU) for edge applications.
---

# Resource Constrained Semantic Segmentation for Waste Sorting

## Quick Facts
- arXiv ID: 2310.19407
- Source URL: https://arxiv.org/abs/2310.19407
- Reference count: 18
- Primary result: Three semantic segmentation models (ICNet, BiSeNet, ENet) compressed to fit 10MB constraint while maintaining high mIoU for waste sorting applications

## Executive Summary
This study addresses the challenge of deploying semantic segmentation models for waste sorting in resource-constrained edge environments. The authors evaluate three network architectures and apply quantization and pruning techniques to meet a 10MB memory constraint. A key contribution is the combination of Focal and Lovász loss functions, which addresses class imbalance while directly optimizing the mIoU metric. BiSeNet achieves a 75% size reduction with only 1.53% mIoU decrease, while ICNet shows 52.25% size reduction with 1.09% mIoU decrease. The work demonstrates that effective waste sorting can be achieved with significantly compressed models suitable for edge deployment.

## Method Summary
The study evaluates ICNet, BiSeNet (with Xception39 backbone), and ENet for waste sorting semantic segmentation. Models are trained with Cross-entropy loss initially, then tested with Focal, Dice, Class-balanced Focal, Lovász, and Focal-Lovász losses. Training uses learning rates {5e-4, 5e-5, 5e-6} with various decay schedules for 100-150 epochs. Data augmentation includes scaling, random cropping, and horizontal flipping. Post-training quantization (PTQ) and pruning (L1 unstructured and random structured) are applied to reduce model size. The dataset from Koskinopoulou et al. contains images of recyclable waste with four object classes and complex backgrounds.

## Key Results
- BiSeNet achieves 75% size reduction (from 13.38MB to 3.21MB) with only 1.53% mIoU decrease
- ICNet shows 52.25% size reduction (from 189.96MB to 90.69MB) with 1.09% mIoU decrease
- Focal+Lovász loss combination outperforms individual losses, achieving mIoU of 0.733 for ENet
- Quantization provides up to 4x memory reduction with minimal accuracy loss when applied correctly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining Focal and Lovász loss mitigates class imbalance while directly optimizing mIoU
- Mechanism: Focal loss reduces the influence of well-classified examples, focusing learning on hard or minority-class pixels. Lovász loss is a surrogate for the mIoU metric, directly optimizing the metric that matters for segmentation performance. Their combination provides both robustness to imbalance and metric optimization.
- Core assumption: Class imbalance in waste sorting is severe enough to warrant Focal loss, and mIoU is the right metric to optimize directly
- Evidence anchors: Table 7 shows Focal-Lovász achieving mIoU of 0.733, outperforming other loss functions for ENet
- Break condition: If class distribution becomes balanced, Focal loss may no longer be necessary and could hurt convergence

### Mechanism 2
- Claim: Quantization reduces model size by mapping float32 weights to uint8 with minimal accuracy loss
- Mechanism: Each float32 parameter is represented as (uint8_value - zero_point) * scale, reducing memory footprint by up to 4x. The technique works because the dynamic range of weights is limited and can be approximated with lower precision
- Core assumption: The weight distributions have low entropy and quantization noise doesn't severely impact gradients
- Evidence anchors: Table 9 shows BiSeNet reducing from 13.38MB to 3.21MB with only 1.53% mIoU drop
- Break condition: If weight distributions are highly skewed or require high precision for small gradients, quantization will degrade performance

### Mechanism 3
- Claim: Pruning removes low-magnitude weights, reducing model size while preserving performance
- Mechanism: Unstructured pruning sets individual weights to zero based on L1 norm; structured pruning removes entire channels/filters. The model retains most of its representational capacity because the removed weights contribute minimally to predictions
- Core assumption: The network has redundant parameters that can be removed without harming accuracy
- Evidence anchors: Table 8 shows BiSeNet pruned to 9.38MB with 4.18% mIoU drop
- Break condition: If pruned model is too sparse, performance drops sharply; must balance sparsity with accuracy

## Foundational Learning

- Concept: Loss functions in segmentation (Cross-entropy, Dice, Focal, Lovász)
  - Why needed here: Different losses handle class imbalance and optimize different metrics; choosing the right one is critical for waste sorting where background dominates
  - Quick check question: Why would Dice loss be preferred over Cross-entropy in highly imbalanced segmentation?

- Concept: Quantization and pruning for model compression
  - Why needed here: Edge devices have strict 10MB memory limits; these techniques enable deployment of accurate models within constraints
  - Quick check question: What is the difference between dynamic and static quantization?

- Concept: Semantic segmentation architectures (ICNet, BiSeNet, ENet)
  - Why needed here: Each architecture has different trade-offs between speed, accuracy, and size; selecting the right one is key to meeting constraints
  - Quick check question: How does BiSeNet's two-branch design improve efficiency compared to single-branch networks?

## Architecture Onboarding

- Component map: Input preprocessing -> Backbone (Xception39/BiSeNet custom/encoder-decoder) -> Segmentation head -> Post-processing (quantization/pruning)

- Critical path: 1. Load pre-trained backbone 2. Apply augmentation 3. Forward pass through network 4. Compute loss (Focal+Lovász) 5. Backpropagate 6. Apply quantization/pruning

- Design tradeoffs: ICNet: high accuracy but large size → needs aggressive pruning+quantization; BiSeNet: medium accuracy, medium size → quantization alone sufficient; ENet: low accuracy, small size → no compression needed

- Failure signatures: mIoU drops >5% after quantization → too aggressive precision reduction; Model size >10MB after pruning → need more aggressive pruning or better quantization; Training instability → learning rate too high or loss function mismatch

- First 3 experiments: 1. Train BiSeNet with Cross-entropy loss, measure mIoU and size 2. Apply post-training quantization, verify size reduction and accuracy 3. Train ICNet with Focal+Lovász loss, compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Focal-Lovász loss function compare to other advanced loss functions like Tversky loss or Dice coefficient loss in terms of segmentation performance for waste sorting?
- Basis in paper: The paper evaluates Focal, Class-balanced Focal, Dice, Lovász, and Focal-Lovász losses, but doesn't compare against other advanced loss functions
- Why unresolved: The study focuses on a specific set of loss functions without exploring the full landscape of available options
- What evidence would resolve it: Comparative experiments using Tversky loss and Dice coefficient loss on the same dataset and evaluation metrics

### Open Question 2
- Question: What is the optimal pruning strategy (structured vs. unstructured) for achieving the best trade-off between model size reduction and mIoU performance in resource-constrained semantic segmentation?
- Basis in paper: The paper mentions both structured (L1 and Ln) and unstructured (random and L1) pruning methods but doesn't provide a comprehensive comparison of their effectiveness
- Why unresolved: Different pruning strategies are mentioned, but their relative performance is not thoroughly analyzed
- What evidence would resolve it: Detailed experiments comparing structured and unstructured pruning methods across multiple models and datasets

### Open Question 3
- Question: How does the performance of the proposed resource-constrained models generalize to real-world industrial waste sorting environments with varying lighting conditions, object orientations, and background clutter?
- Basis in paper: The study uses synthetic data augmentation but doesn't explicitly test the models in real-world scenarios
- Why unresolved: The paper focuses on synthetic data and doesn't provide real-world validation of the proposed models
- What evidence would resolve it: Field tests of the models in actual waste sorting facilities with diverse environmental conditions and waste types

## Limitations
- Class imbalance addressed appears moderate (Bottle:23%, Paper:21%, Nylon:28%, Aluminum:28%), making the necessity of Focal loss questionable for this specific dataset
- Compression techniques are applied post-training rather than during training, potentially leaving performance on the table compared to quantization-aware training approaches
- Dataset size and composition are not fully specified, making it difficult to assess generalizability to other waste sorting scenarios

## Confidence
- High confidence: The effectiveness of quantization and pruning techniques for reducing model size
- Medium confidence: The superiority of Focal+Lovász loss combination for addressing class imbalance and optimizing mIoU
- Low confidence: The claim that these specific models and techniques are optimal for industrial waste-sorting applications

## Next Checks
1. Conduct ablation study on loss functions comparing Focal+Lovász loss against using each loss individually and against other state-of-the-art loss combinations to quantify the additive vs synergistic benefits of the proposed combination.

2. Implement quantization-aware training from the beginning and compare performance and size reduction against post-training quantization to determine if training-time quantization provides additional benefits.

3. Deploy the compressed models on actual edge devices (e.g., NVIDIA Jetson, Raspberry Pi) to measure real-world inference time, memory usage, and thermal performance under industrial conditions, validating the 10MB constraint in practice.