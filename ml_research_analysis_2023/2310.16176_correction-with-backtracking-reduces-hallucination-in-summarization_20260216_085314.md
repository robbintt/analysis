---
ver: rpa2
title: Correction with Backtracking Reduces Hallucination in Summarization
arxiv_id: '2310.16176'
source_url: https://arxiv.org/abs/2310.16176
tags:
- arxiv
- coba
- hallucination
- token
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoBa, a simple yet efficient technique to
  reduce hallucination in abstractive summarization. The approach consists of two
  steps: hallucination detection and mitigation.'
---

# Correction with Backtracking Reduces Hallucination in Summarization

## Quick Facts
- **arXiv ID**: 2310.16176
- **Source URL**: https://arxiv.org/abs/2310.16176
- **Reference count**: 16
- **Key outcome**: CoBa reduces hallucination in abstractive summarization through probability-based detection and backtracking mitigation

## Executive Summary
This paper introduces CoBa, a simple yet efficient technique to reduce hallucination in abstractive summarization. The approach consists of two steps: hallucination detection and mitigation. Hallucination is detected by measuring conditional word probabilities and distance to context words. Mitigation is achieved through backtracking, which re-generates the preceding words that led the model into a position without a faithful continuation. CoBa is evaluated on three benchmark datasets for text summarization, demonstrating its effectiveness in reducing hallucination while maintaining high adaptability and flexibility.

## Method Summary
CoBa addresses hallucination in abstractive summarization through a two-stage process. First, it detects potential hallucinations during decoding by monitoring conditional token probabilities and cosine distances between token embeddings and context words. If a token's probability falls below threshold δ or its embedding distance exceeds threshold φ, it is flagged as potentially hallucinated. Second, CoBa mitigates detected hallucinations through backtracking - eliminating the problematic token and re-generating alternative continuations using a depth-first search approach with a step limit L. This process continues until a valid token is found or the step limit is reached, at which point the system falls back to baseline decoding.

## Key Results
- CoBa achieves 6.9% improvement in AlignScore on Flan-T5 XL compared to greedy decoding
- Outperforms Lookahead and CAD methods while being more computationally efficient
- Maintains high summarization quality (ROUGE-L, BERTScore F1, Bleurt) while improving faithfulness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low conditional token probability at the start of a generation step indicates potential hallucination.
- Mechanism: The model computes p(st|C, S<t) for the next token st. If this probability falls below threshold δ, the generation step is flagged as uncertain.
- Core assumption: Token probabilities from autoregressive models are well-calibrated and reflect the model's confidence in the next token given the context.
- Evidence anchors:
  - [abstract] "We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
  - [section 4.1.1] "We validate that token probabilities are effective for identifying hallucinated tokens in summaries by computing probabilities on an annotated hallucination dataset from Maynez et al. (2020b)."
- Break condition: If the model's token probability estimates are poorly calibrated or if the threshold δ is set too high, this mechanism will miss hallucinations or generate false positives.

### Mechanism 2
- Claim: High token-to-context distance indicates potential hallucination.
- Mechanism: For each proposed token v, compute the minimum cosine distance between its embedding and all context token embeddings. If this distance exceeds threshold φ, flag the token as unsupported by context.
- Core assumption: Contextual embeddings capture semantic similarity such that tokens supported by the context will have lower embedding distances to context tokens than hallucinated tokens.
- Evidence anchors:
  - [abstract] "We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
  - [section 4.1.2] "We compute the distance between its embedding and the embeddings of all tokens in the context and flag the token as a potential hallucination if the minimum distance is above a certain threshold."
- Break condition: If the embedding space does not capture semantic similarity well for this task, or if the threshold φ is poorly chosen, this mechanism will fail to detect hallucinations or produce false alarms.

### Mechanism 3
- Claim: Backtracking and re-generation prevents hallucinations by finding alternative token sequences that maintain high probability and context support.
- Mechanism: When a token is flagged as hallucinated, the last generated token is eliminated and the model attempts to generate a different token that passes the detection criteria. This process repeats, potentially backtracking multiple steps, until a valid continuation is found or a maximum step limit L is reached.
- Core assumption: There exists an alternative sequence of tokens that can be generated which both maintains high probability and is supported by the context, preventing the model from being "cornered" into hallucinating.
- Evidence anchors:
  - [abstract] "Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation."
  - [section 4.2] "After detecting potential hallucination during decoding using the techniques described in subsection 4.1, we perform a local intervention to prevent the generation of hallucinated phrases. Specifically, we introduce a process similar to depth first search."
- Break condition: If the model consistently fails to find a valid alternative continuation within the step limit L, or if the context genuinely requires information not present in the source document, backtracking will not resolve the hallucination.

## Foundational Learning

- Concept: Autoregressive language model generation
  - Why needed here: CoBa operates during the decoding process of autoregressive models, detecting and correcting hallucinations step-by-step.
  - Quick check question: What is the probability formula p(S) = ∏t pθ(st|C, S<t) computing, and why is it autoregressive?

- Concept: Token probability calibration
  - Why needed here: The detection mechanism relies on well-calibrated token probabilities to identify low-confidence (potentially hallucinated) tokens.
  - Quick check question: What does it mean for a language model's token probabilities to be "well-calibrated," and why is this important for CoBa's detection mechanism?

- Concept: Cosine similarity in embedding space
  - Why needed here: The similarity-based detection mechanism uses cosine distance between token embeddings to measure how supported a token is by the context.
  - Quick check question: How is cosine similarity computed between two vectors, and why is it used to measure semantic similarity in embedding space?

## Architecture Onboarding

- Component map: Language model Mθ -> Detection module (δ, φ) -> Backtracking module (L) -> Generation controller

- Critical path:
  1. Generate next token st with Mθ
  2. Compute pθ(st|C, S<t) and check against δ
  3. Compute cos_dist(Emb(st), Emb(ci)) for all ci in C and check minimum against φ
  4. If either check fails, eliminate st and backtrack to find alternative
  5. If alternative found, add to summary and continue; else, revert to baseline decoding after L steps

- Design tradeoffs:
  - Probability threshold δ vs. distance threshold φ: Using both provides complementary detection but increases computation; using only one simplifies the system.
  - Step limit L: Higher values allow more thorough backtracking but increase latency; lower values are faster but may miss corrections.
  - Detection frequency: Checking every token provides thorough coverage but is slower; checking less frequently is faster but may miss hallucinations.

- Failure signatures:
  - High false positive rate: Detection thresholds are too sensitive, causing unnecessary backtracking and slower generation.
  - High false negative rate: Detection thresholds are too lenient, allowing hallucinations to pass undetected.
  - Excessive backtracking: Model frequently fails to find valid alternatives, triggering the fallback to baseline decoding.
  - Increased latency: Backtracking process is computationally expensive, slowing down generation significantly.

- First 3 experiments:
  1. Baseline comparison: Run greedy decoding on Newsroom dataset and measure faithfulness metrics (AlignScore, FactCC, BS-Fact, ROUGE-L).
  2. Probability threshold ablation: Run CoBa with varying δ values (0.1, 0.2, 0.3, 0.4) on Newsroom and measure faithfulness and latency.
  3. Distance threshold ablation: Run CoBa-d with varying φ values (0.1, 0.3, 0.5, 0.7, 0.9) on Newsroom and measure faithfulness and latency.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations section and discussion, several important questions emerge:

1. How does CoBa's performance compare to training-time hallucination mitigation techniques when applied to the same summarization models?
2. What is the computational overhead of CoBa compared to other hallucination detection and mitigation methods during large-scale summarization tasks?
3. Can CoBa be effectively extended to other generation tasks beyond summarization, such as long-form question answering or dialogue systems?

## Limitations

- The method's effectiveness depends on well-calibrated token probabilities and embedding spaces that capture semantic similarity, which may not hold for all model architectures.
- Backtracking introduces computational overhead and potential latency issues, with the tradeoff between faithfulness improvement and generation speed not fully characterized.
- The evaluation focuses on three benchmark datasets, leaving open questions about performance on more diverse or specialized domains.

## Confidence

High confidence in: The overall effectiveness of CoBa in reducing hallucination as measured by faithfulness metrics. The paper demonstrates consistent improvements across multiple datasets and models, with AlignScore improvements of 6.9% for Flan-T5 XL and 5.9% for LLaMA.

Medium confidence in: The specific detection mechanisms (probability thresholds and distance metrics). While the paper shows these work well empirically, the theoretical justification for why these particular thresholds are optimal is limited.

Medium confidence in: The backtracking mechanism's effectiveness. The paper shows it improves faithfulness, but the computational overhead and scenarios where it fails are not fully characterized.

Low confidence in: Generalization to domains beyond the evaluated benchmarks. The method's performance on specialized, technical, or multi-domain documents is not explored.

## Next Checks

1. **Cross-dataset threshold validation**: Test whether the detection thresholds (δ and φ) that work well for Newsroom, CNN/Dailymail, and XSUM also perform effectively on out-of-domain datasets. This would validate the method's robustness and identify whether threshold tuning is necessary for different domains.

2. **Human evaluation of subtle hallucinations**: Conduct human evaluations to assess whether CoBa catches subtle factual inconsistencies that automated metrics might miss. This would validate whether the faithfulness improvements translate to human-perceived quality improvements beyond what the metrics capture.

3. **Computational overhead analysis**: Measure the actual latency increase introduced by CoBa across different step limits L and generation lengths. This would quantify the tradeoff between faithfulness improvement and generation speed, helping to identify optimal parameter settings for practical deployment.