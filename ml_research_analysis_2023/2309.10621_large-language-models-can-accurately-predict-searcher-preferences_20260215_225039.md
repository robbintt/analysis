---
ver: rpa2
title: Large language models can accurately predict searcher preferences
arxiv_id: '2309.10621'
source_url: https://arxiv.org/abs/2309.10621
tags:
- labels
- prompt
- query
- these
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study the use of large language models for relevance labelling,
  and find that they can produce labels of comparable quality to human labellers.
  In particular, we find that language models can produce labels that match first-party
  ground truth from real users, better than third-party human labellers.
---

# Large language models can accurately predict searcher preferences

## Quick Facts
- arXiv ID: 2309.10621
- Source URL: https://arxiv.org/abs/2309.10621
- Reference count: 5
- Primary result: LLM-generated relevance labels match first-party ground truth from real users better than third-party human labellers

## Executive Summary
This paper investigates the use of large language models (LLMs) for generating relevance labels in information retrieval tasks. The study finds that LLM-generated labels can achieve accuracy comparable to human labellers while being significantly more cost-effective. Most notably, when evaluated against first-party ground truth from real users, LLM labels outperform those from third-party human assessors. The research demonstrates that systematic prompt engineering, particularly using aspect-based prompts, can significantly improve label quality.

## Method Summary
The study employs GPT-4 to generate relevance labels for document-query pairs from the TREC-Robust 2004 dataset and Bing's proprietary web search data. The researchers develop and test various prompt structures, including overall relevance ratings, aspect-based prompts (topicality, relevance, quality, trustworthiness), and narrative formats. They compare LLM-generated labels against human labels from TREC assessors and first-party ground truth from real Bing users. Label quality is evaluated using metrics including mean absolute error (MAE), Cohen's kappa, AUC for preference judgments, and rank-biased overlap (RBO) for system-level comparisons.

## Key Results
- LLM-generated labels achieve accuracy comparable to human labellers when measured against first-party ground truth
- Aspect-based prompts improve label quality by 0.21 Cohen's kappa compared to simple overall relevance prompts
- LLM labeling is significantly faster and more cost-effective than human labeling while maintaining or improving quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate relevance labels that better match real user preferences than third-party human judges
- Mechanism: The LLM is trained/optimized using a small set of high-quality first-party ground truth labels from real users, and then generates labels that have better agreement with this ground truth than labels from third-party judges
- Core assumption: First-party labels from real users represent the true gold standard for relevance, and LLM prompts can be optimized to match this gold standard
- Evidence anchors:
  - [abstract] "models produce better labels than third-party workers, for a fraction of the cost"
  - [section] "LLMs can be effective, with accuracy as good as human labellers and similar capability to pick the hardest queries, best runs, and best groups"
  - [corpus] "Found 25 related papers" - weak evidence, no direct citations of this claim
- Break condition: If first-party labels are biased or unrepresentative of the general user population, or if LLM prompts cannot be optimized to match the gold standard

### Mechanism 2
- Claim: Prompt engineering significantly impacts LLM label quality, with aspects-based prompts showing the largest improvement
- Mechanism: Including specific aspects like topicality and trustworthiness in the prompt leads the LLM to generate more nuanced and accurate labels compared to simple overall relevance ratings
- Core assumption: Breaking down the relevance task into multiple aspects helps the LLM better understand and capture the nuances of relevance compared to a single overall score
- Evidence anchors:
  - [abstract] "Systematic changes to the prompts make a difference in accuracy, but so too do simple paraphrases"
  - [section] "Aspects (A) give a substantial improvement in ðœ… against TREC assessors, +0.21"
  - [corpus] "Found 25 related papers" - weak evidence, no direct citations of this claim
- Break condition: If the chosen aspects do not align with user preferences or if the LLM cannot effectively process multi-aspect prompts

### Mechanism 3
- Claim: LLM labeling is more cost-effective and scalable than human labeling while maintaining or improving quality
- Mechanism: LLMs can generate labels much faster and cheaper than human judges, while the quality of labels (as measured by agreement with first-party ground truth) is at least as good as trained human assessors
- Core assumption: The cost and speed advantages of LLMs outweigh any potential quality concerns, and the quality can be maintained through proper prompt engineering
- Evidence anchors:
  - [abstract] "for a fraction of the cost" and "notably better rankers"
  - [section] "LLM models give us better accuracy at vastly reduced latency and cost" and "much faster end-to-end than any human judge"
  - [corpus] "Found 25 related papers" - weak evidence, no direct citations of this claim
- Break condition: If the cost of running LLMs increases significantly or if the quality degrades as the system scales to more queries/languages

## Foundational Learning

- Concept: Relevance labeling in information retrieval
  - Why needed here: The entire paper is about using LLMs to generate relevance labels, which are fundamental to evaluating and improving search systems
  - Quick check question: What is the difference between binary, graded, and multi-aspect relevance labels?

- Concept: Prompt engineering for LLMs
  - Why needed here: The paper demonstrates that different prompt structures and phrasings significantly impact the quality of LLM-generated labels
  - Quick check question: How do aspects-based prompts differ from simple overall relevance prompts in terms of the information they elicit from LLMs?

- Concept: Evaluation metrics for label quality (MAE, Cohen's kappa, AUC, RBO)
  - Why needed here: The paper uses various metrics to compare the quality of LLM-generated labels against human labels and first-party ground truth
  - Quick check question: What does Cohen's kappa measure, and why is it useful for comparing label agreement?

## Architecture Onboarding

- Component map: First-party ground truth collection -> LLM prompt engineering and generation -> Label quality evaluation -> Integration with search system evaluation and ranking

- Critical path:
  1. Collect first-party ground truth labels from real users
  2. Engineer and test LLM prompts using the ground truth
  3. Generate LLM labels at scale
  4. Evaluate label quality and integrate into search system

- Design tradeoffs:
  - Cost vs. quality: More sophisticated prompts may improve quality but increase generation cost
  - Speed vs. thoroughness: Faster label generation may sacrifice some quality
  - Coverage vs. depth: Generating labels for more queries vs. more detailed labels per query

- Failure signatures:
  - LLM labels consistently disagree with first-party ground truth
  - Certain query types or document types consistently receive poor labels
  - Quality degrades as the system scales to new languages or domains

- First 3 experiments:
  1. Test different prompt structures (aspects, narrative, role) on a small set of queries with known ground truth
  2. Generate labels for a larger set of queries and compare quality metrics (MAE, kappa) against human labels
  3. Integrate LLM labels into the search system evaluation pipeline and measure impact on ranking quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of LLM-based relevance labeling on the quality and diversity of web content?
- Basis in paper: [explicit] The paper mentions that "whether website designers can take advantage of such biases in LLMs-for-labelling systems to unfairly gain more exposure for their content, or whether large chunks of the web optimising towards what LLMs deem important leads to undesirable shifts in trends and homogenisation of online content, are also important topics for future research."
- Why unresolved: The paper acknowledges this as an important topic for future research, indicating that the long-term impact is not yet fully understood.
- What evidence would resolve it: Longitudinal studies tracking changes in web content quality and diversity over time, correlating with the adoption of LLM-based labeling systems.

### Open Question 2
- Question: How do different LLM architectures and training data affect their performance in relevance labeling tasks?
- Basis in paper: [inferred] The paper mentions that "Liang et al. [2022] saw large differences from model to model over a range of tasks," suggesting that model choice can significantly impact performance.
- Why unresolved: The paper primarily focuses on one specific model (GPT-4) and does not extensively compare different LLM architectures or training approaches.
- What evidence would resolve it: Systematic comparisons of multiple LLM architectures and training approaches on standardized relevance labeling tasks, measuring accuracy, efficiency, and bias.

### Open Question 3
- Question: Can LLM-based relevance labeling be effectively applied to specialized or domain-specific content where LLMs may lack expertise?
- Basis in paper: [inferred] The paper discusses using LLMs for web search labeling, which is a general domain. It mentions that "Using LLMs for labelling suggests new and more difficult applications, for example labelling private corpora where we cannot give human assessors access," implying potential challenges with specialized content.
- Why unresolved: The paper does not provide evidence of LLM performance on specialized or domain-specific content, focusing instead on general web search.
- What evidence would resolve it: Experimental studies comparing LLM labeling performance on general vs. specialized content, potentially involving human experts in the specific domain for evaluation.

## Limitations

- Evaluation relies on proprietary Bing first-party ground truth data that is not publicly available, making independent validation difficult
- The specific prompts used for the Bing web search evaluation are confidential, preventing exact replication
- Results are primarily demonstrated on English web search data, with limited validation across different domains or languages

## Confidence

- High Confidence: LLM labeling can achieve accuracy comparable to human labellers when measured against first-party ground truth
- Medium Confidence: LLM labeling outperforms third-party human labellers in matching real user preferences
- Medium Confidence: Prompt engineering significantly impacts label quality

## Next Checks

1. Conduct blind validation using an independent first-party ground truth dataset from a different search system or domain
2. Test prompt engineering approaches on multiple LLM providers to assess generalizability
3. Perform longitudinal analysis to measure label consistency and quality over time as LLM models evolve