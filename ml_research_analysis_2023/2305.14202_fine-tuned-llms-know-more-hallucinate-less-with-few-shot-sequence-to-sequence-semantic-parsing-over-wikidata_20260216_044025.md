---
ver: rpa2
title: Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
  Semantic Parsing over Wikidata
arxiv_id: '2305.14202'
source_url: https://arxiv.org/abs/2305.14202
tags:
- wikidata
- entity
- knowledge
- questions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fact hallucination in large
  language models (LLMs) by introducing a few-shot sequence-to-sequence semantic parser,
  WikiSP, that retrieves answers from Wikidata. WikiSP handles large, sparse, and
  noisy knowledge graphs by synthesizing high-quality training data using a property
  hierarchy and a modified SPARQL representation with domain and property names.
---

# Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata

## Quick Facts
- arXiv ID: 2305.14202
- Source URL: https://arxiv.org/abs/2305.14202
- Reference count: 22
- Primary result: Achieves 59% answer accuracy and 63% F1 score on WikiWebQuestions, outperforming prior methods by 3.6% F1 on QALD-7

## Executive Summary
This paper addresses the problem of fact hallucination in large language models by introducing WikiSP, a few-shot sequence-to-sequence semantic parser that retrieves answers from Wikidata. The system handles large, sparse, and noisy knowledge graphs by synthesizing high-quality training data using a property hierarchy and a modified SPARQL representation with domain and property names. By pairing WikiSP with GPT-3, the system provides verifiable answers 69% of the time and useful information 97% of the time, significantly reducing GPT-3's error rate while maintaining the ability to handle questions the semantic parser cannot answer.

## Method Summary
The approach involves fine-tuning LLaMA on augmented few-shot training data that combines synthetic examples with entity augmentation. The semantic parser converts natural language questions into SPARQL queries using BART-large, with modifications to use natural language property and domain names instead of IDs. A property hierarchy groups similar properties under super properties to improve learnability. The system uses ReFinED for entity linking and executes queries on Wikidata. When the semantic parser cannot answer, GPT-3 provides guesses. The model is trained for 80K iterations with a transformer learning rate schedule and warmup schedule.

## Key Results
- Achieves 59% answer accuracy and 63% F1 score on WikiWebQuestions
- Outperforms prior methods by 3.6% F1 on QALD-7
- Provides verifiable answers 69% of the time when paired with GPT-3, reducing hallucination while maintaining 97% useful information provision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modifying SPARQL to use domain and property names instead of IDs improves semantic parser performance by increasing learnability.
- Mechanism: Natural language labels allow the neural model to leverage linguistic patterns and contextual cues, reducing the need to memorize the knowledge base schema.
- Core assumption: Neural networks can effectively learn and generalize from natural language property and entity names.
- Evidence anchors: [abstract] "We modify SPARQL to use the unique domain and property names instead of their IDs." [section 4.2] "We introduce the concept of a property hierarchy to group together properties with similar semantics under a super property."
- Break condition: If property names are ambiguous, overly generic, or if the model fails to learn the semantic relationships between names and their corresponding entities/properties.

### Mechanism 2
- Claim: Few-shot training data augmentation with synthetic examples improves model performance on sparse knowledge graphs.
- Mechanism: Synthesizing training data using a property hierarchy and entity augmentation expands the coverage of the knowledge base schema.
- Core assumption: Synthetic data generated from the property hierarchy and entity augmentation is representative of real-world questions.
- Evidence anchors: [abstract] "We fine-tune LLaMA by adding the few-shot training data to that used to fine-tune Alpaca." [section 6.1] "We synthesized 272,474 examples, and we augment the fewshot data by replacing entities and values."
- Break condition: If the synthetic data is not representative of real-world questions, or if the model overfits to the synthetic data.

### Mechanism 3
- Claim: Pairing a semantic parser with GPT-3 improves trustworthiness by providing verifiable answers and reducing hallucination.
- Mechanism: The semantic parser grounds GPT-3's answers in Wikidata, allowing users to have confidence in the parser's answers while benefiting from GPT-3's guesses when the parser cannot answer.
- Core assumption: The semantic parser is accurate enough to provide correct answers most of the time.
- Evidence anchors: [abstract] "By pairing WikiSP with GPT-3, the system provides verifiable answers 69% of the time and useful information 97% of the time, significantly reducing GPT-3â€™s error rate." [section 7.4] "We propose getting the best of both worlds by answering the question with WikiSP if possible."
- Break condition: If the semantic parser's accuracy is too low, or if GPT-3's guesses are consistently wrong or misleading.

## Foundational Learning

- Concept: Knowledge Base Question Answering (KBQA)
  - Why needed here: Understanding the KBQA task and its challenges is crucial for grasping the motivation behind this work.
  - Quick check question: What are the main challenges in KBQA, and how do semantic parsing-based approaches aim to address them?

- Concept: Semantic Parsing
  - Why needed here: Semantic parsing is the core technique used to convert natural language questions into formal logical forms executable on the knowledge base.
  - Quick check question: What is semantic parsing, and how does it differ from other approaches to KBQA like retrieval-based methods?

- Concept: Wikidata
  - Why needed here: Wikidata is the knowledge base used in this work, and understanding its structure is essential for comprehending the challenges faced and solutions proposed.
  - Quick check question: What are the key features of Wikidata, and how does it differ from other knowledge bases like Freebase?

## Architecture Onboarding

- Component map: User Question -> Entity Linker (ReFinED) -> WikiSP Semantic Parser -> SPARQL Query -> Wikidata -> Answer -> GPT-3 (if needed)

- Critical path: 1. User asks a question, 2. Entity linker identifies entities, 3. WikiSP converts question to SPARQL, 4. SPARQL query executes on Wikidata, 5. Answer returns to user, 6. If WikiSP cannot answer, GPT-3 provides a guess

- Design tradeoffs: Using natural language labels vs. IDs in SPARQL, few-shot training vs. fully supervised training, semantic parsing vs. retrieval-based approaches, pairing with GPT-3 vs. standalone semantic parser

- Failure signatures: Low query or answer accuracy, high entity linking error rate, GPT-3 guesses consistently wrong, model overfits to synthetic training data

- First 3 experiments: 1. Evaluate WikiSP on WikiWebQuestions dev set to establish baseline, 2. Perform ablation study to assess impact of natural language labels vs. IDs, 3. Test combined approach on questions where WikiSP cannot provide answer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a property hierarchy be effectively integrated into Wikidata to improve semantic parser performance on large, sparse, and noisy knowledge graphs?
- Basis in paper: [explicit] The paper introduces a property hierarchy to group similar properties under super properties but does not provide empirical evidence on its effectiveness.
- Why unresolved: The paper presents the concept but lacks experiments comparing performance with and without the property hierarchy.
- What evidence would resolve it: Experiments comparing semantic parser performance with and without the property hierarchy on Wikidata using query accuracy, answer accuracy, and F1 score metrics.

### Open Question 2
- Question: How does WikiSP's performance compare to other state-of-the-art semantic parsers on the WikiWebQuestions benchmark?
- Basis in paper: [explicit] The paper presents WikiSP's performance but does not benchmark against other existing semantic parsers on this dataset.
- Why unresolved: The paper focuses on establishing a baseline without direct comparison to other methods.
- What evidence would resolve it: Running experiments comparing WikiSP with other state-of-the-art semantic parsers on WikiWebQuestions using the same evaluation metrics.

### Open Question 3
- Question: How can the entity linking component be improved to reduce errors in the semantic parsing pipeline?
- Basis in paper: [explicit] The paper identifies entity linking as a significant source of errors (30.7% of failed examples) but does not explore improvements beyond using ReFinED.
- Why unresolved: While acknowledging the importance of entity linking, the paper does not investigate potential improvements or alternative approaches.
- What evidence would resolve it: Investigating improvements to entity linking through fine-tuning ReFinED with diverse data, exploring alternative models, or incorporating additional context, then evaluating the impact on overall semantic parsing performance.

## Limitations

- Data synthesis quality uncertainty: The representativeness of synthetic training examples compared to naturally occurring questions is not fully validated.
- Limited generalizability: Performance is primarily evaluated on WikiWebQuestions and QALD-7 datasets, with unknown performance on other knowledge bases.
- Incomplete GPT-3 integration evaluation: The methodology for when to use GPT-3 versus the semantic parser is not clearly specified.

## Confidence

**High confidence**: The claim that modifying SPARQL to use natural language names improves semantic parser performance is well-supported by experimental results and mechanistic explanation.

**Medium confidence**: The claim about few-shot training data augmentation effectiveness is reasonably supported but would benefit from more detailed analysis of synthetic data quality.

**Medium confidence**: The claim about reduced hallucination through the combined WikiSP + GPT-3 approach is supported by reported metrics, but the evaluation methodology could be more rigorous.

## Next Checks

1. **Synthetic data quality audit**: Conduct detailed analysis comparing synthetic training examples to naturally occurring questions, focusing on semantic complexity, entity diversity, and property distribution to validate representativeness.

2. **Cross-dataset generalization test**: Evaluate WikiSP on additional KBQA datasets (e.g., ComplexWebQuestions, GrailQA) to assess generalization beyond WikiWebQuestions and establish broader applicability claims.

3. **Hallucination reduction validation**: Implement blind human evaluation study where annotators assess trustworthiness and hallucination frequency of answers from vanilla GPT-3, WikiSP alone, and the combined WikiSP + GPT-3 system using standardized hallucination detection protocols.