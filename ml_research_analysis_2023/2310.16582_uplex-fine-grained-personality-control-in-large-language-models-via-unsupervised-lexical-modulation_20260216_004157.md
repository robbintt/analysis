---
ver: rpa2
title: 'UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised
  Lexical Modulation'
arxiv_id: '2310.16582'
source_url: https://arxiv.org/abs/2310.16582
tags:
- personality
- llms
- traits
- five
- ubpl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-grained personality
  control in large language models (LLMs). Prior approaches relied on fine-tuning
  models on specific corpora or manually crafted prompts, but these methods are either
  inefficient, costly, or lack fine-grained control.
---

# UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation

## Quick Facts
- arXiv ID: 2310.16582
- Source URL: https://arxiv.org/abs/2310.16582
- Authors: [Not specified in source]
- Reference count: 5
- One-line primary result: A pluggable method for fine-grained personality control in LLMs without fine-tuning or prompt engineering

## Executive Summary
This paper introduces UPLex, a novel method for fine-grained personality control in large language models (LLMs) through unsupervised lexical modulation. Unlike prior approaches that rely on expensive fine-tuning or complex prompt engineering, UPLex dynamically alters word probabilities during decoding using an Unsupervisedly-Built Personalized Lexicon (UPL). The method demonstrates remarkable effectiveness in manipulating Big Five personality traits while maintaining model coherence and can be seamlessly integrated into any pre-trained LLM without parameter updates.

## Method Summary
UPLex constructs a situational judgment test dataset (SJTs4LLM) using GPT-4, then builds an Unsupervisedly-Built Personalized Lexicon (UBPL) that maps vocabulary to personality trait weights. During decoding, the method intercepts top-k candidate words and modifies their probabilities based on the UBPL values before normalization and sampling. This probability modulation approach enables precise control over personality expression without requiring model fine-tuning or elaborate prompt design, making it a pluggable solution that can be applied to any pre-trained LLM.

## Key Results
- Achieves fine-grained personality manipulation through unsupervised lexical modulation
- Demonstrates pluggable nature by working with any pre-trained LLM without parameter updates
- Constructs a new situational judgment test dataset (SJTs4LLM) for more reliable personality assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personality traits can be manipulated by dynamically altering the predicted probability of upcoming words during the decoding phase.
- Mechanism: The method constructs a personality lexicon (UBPL) that assigns weighted values to vocabulary words based on their association with Big Five personality traits. During decoding, these weights are added to the base probability of candidate words, shifting the model's output distribution toward words aligned with the target personality.
- Core assumption: The distribution of vocabulary usage directly influences the perceived personality of generated text, and shifting probabilities toward trait-associated words will produce coherent personality-consistent outputs.
- Evidence anchors:
  - [abstract] "UPL can be constructed from a newly built situational judgment test dataset in an unsupervised fashion, and used to modulate the personality expression of LLMs by dynamically altering their predicted probability of upcoming words"
  - [section] "We transform pi into F(pi), as shown in Equation 1: F(pi) = pi + α (β0 · VALi[0] + β1 · VALi[1] + β2 · VALi[2] + β3 · VALi[3] + β4 · VALi[4])"

### Mechanism 2
- Claim: Personality assessment via situational judgment tests (SJTs) provides more reliable and context-specific evaluation than traditional psychological questionnaires.
- Mechanism: Instead of abstract self-assessment questions, SJTs present concrete scenarios and ask models to respond. The model's responses are then evaluated by other LLMs for personality trait alignment, avoiding the abstract interpretation issues of traditional questionnaires.
- Core assumption: Concrete situational responses are more interpretable and less prone to hallucination or refusal than abstract self-assessment questions about personality.
- Evidence anchors:
  - [section] "Compared with a questionnaire survey, this method of SJTs is more concrete and can evaluate the personality of LLMs more reasonably"
  - [section] "For example, one of the questions is: 'My mind is often filled with vivid images.' The LLM needs to answer how much it fits this description. Obviously, for the LLM, 'my mind' is an abstract concept"

### Mechanism 3
- Claim: The pluggable nature of the method allows personality control without fine-tuning or prompt engineering.
- Mechanism: By intervening in the sampling process during decoding, the method can be applied to any pre-trained LLM without updating parameters. The UBPL lexicon and hyperparameters (α, β values) are the only components that need adjustment.
- Core assumption: The sampling process is a sufficient intervention point to influence personality without requiring architectural changes or extensive retraining.
- Evidence anchors:
  - [abstract] "The method does not require model fine-tuning or elaborate prompt design, and can be seamlessly integrated into other LLMs without updating their parameters"
  - [section] "Our method not only does not need to update the LLMs parameters, but also can be easily adapted to any other language model, and can flexibly switch the personality during the use of LLMs"

## Foundational Learning

- Concept: Big Five personality theory (OCEAN model)
  - Why needed here: The method maps personality traits to specific vocabulary usage patterns, requiring understanding of what traits to target (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism).
  - Quick check question: Can you name all five Big Five personality traits and give one word that might characterize high vs low in each trait?

- Concept: Top-k kernel sampling in language model decoding
  - Why needed here: The mechanism intervenes in the sampling process by modifying word probabilities after initial prediction but before final selection.
  - Quick check question: In top-k sampling, how are candidate words selected and what happens to words with cumulative probability below the threshold?

- Concept: Situational judgment tests (SJTs) methodology
  - Why needed here: The method uses SJTs instead of traditional questionnaires to assess LLM personality in a more context-specific and interpretable way.
  - Quick check question: What's the key difference between asking "How agreeable are you?" versus presenting a scenario and asking "How would you respond?" in terms of LLM personality assessment?

## Architecture Onboarding

- Component map: SJTs4LLM dataset -> UBPL lexicon -> Sampling intervention module -> Hyperparameter controller

- Critical path:
  1. Construct SJTs4LLM dataset using GPT-4 with prompts
  2. Generate personality-annotated responses for each SJT scenario
  3. Build UBPL lexicon by tokenizing responses and assigning personality weights
  4. During decoding, intercept top-k candidates and apply probability modifications
  5. Normalize and sample final word from modified distribution

- Design tradeoffs:
  - Granularity vs coherence: Higher β values give more personality control but risk incoherence
  - Lexicon coverage vs efficiency: More vocabulary in UBPL gives finer control but increases memory and computation
  - SJT diversity vs annotation cost: More diverse scenarios improve assessment but require more LLM interactions

- Failure signatures:
  - Personality change without context preservation (words feel forced or unnatural)
  - No personality change despite high β values (lexicon weights too weak or model too resistant)
  - System instability or errors during sampling (probability modification causing numerical issues)

- First 3 experiments:
  1. Baseline test: Run LLaMA2 on SJT scenarios without UBPL, record personality assessment scores
  2. Single-trait manipulation: Apply UBPL to increase one personality trait (e.g., conscientiousness) and measure score change
  3. Multi-trait balance: Simultaneously adjust two opposing traits (e.g., increase agreeableness while decreasing neuroticism) and evaluate coherence

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations about dataset quality, hyperparameter selection, and cross-model generalizability.

## Limitations
- Dataset quality concerns: The UBPL lexicon quality depends heavily on the unsupervisedly constructed SJTs4LLM dataset, which may introduce LLM generation biases
- Hyperparameter sensitivity: The effectiveness of personality manipulation depends on proper tuning of α and β values, which are not extensively explored
- Limited model validation: While pluggability is claimed, experiments are only demonstrated on LLaMA2-13b-chat, leaving cross-model generalizability unverified

## Confidence
- High confidence: The core mechanism of probability modulation during decoding is well-specified and technically sound
- Medium confidence: The effectiveness of personality manipulation depends heavily on the quality of the UBPL lexicon, which is constructed in an unsupervised manner
- Low confidence: Claims about seamless pluggability across different LLMs are not thoroughly validated beyond LLaMA2 experiments

## Next Checks
1. Cross-model validation: Test UPLex on multiple LLM architectures (e.g., Mistral, Gemma, BLOOM) to verify the claimed pluggability and identify any model-specific limitations in personality modulation effectiveness.

2. Human evaluation study: Conduct blind human evaluations comparing personality traits in outputs from baseline models versus UPLex-modified models across multiple traits, to validate that the LLM-based personality assessments align with human perception.

3. Stress testing lexicon coverage: Systematically evaluate how the method performs as the vocabulary coverage of UBPL varies (e.g., 10k, 50k, 100k words), and identify the point at which additional lexicon entries no longer improve personality control or begin degrading coherence.