---
ver: rpa2
title: Prompt-Engineering and Transformer-based Question Generation and Evaluation
arxiv_id: '2310.18867'
source_url: https://arxiv.org/abs/2310.18867
tags:
- questions
- prompt
- question
- generation
- squad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates question generation for educational use\
  \ by finetuning a distilBERT transformer and by prompt-engineering Meta\u2019s LLaMA\
  \ LLM. The transformer was trained on the SQuAD 1.1 dataset but failed to generate\
  \ questions, yielding a low F1 score of 15.88 due to its strong question-answering\
  \ bias."
---

# Prompt-Engineering and Transformer-based Question Generation and Evaluation

## Quick Facts
- arXiv ID: 2310.18867
- Source URL: https://arxiv.org/abs/2310.18867
- Reference count: 3
- Primary result: LLaMA prompt engineering outperformed fine-tuned transformer for question generation, with Prompt D achieving highest similarity (0.644) to SQuAD baseline

## Executive Summary
This paper investigates question generation for educational use by comparing two approaches: fine-tuning a distilBERT transformer and prompt engineering with Meta's LLaMA LLM. The transformer, trained on SQuAD 1.1, failed to generate questions due to its strong QA bias, yielding an F1 score of 15.88. In contrast, LLaMA successfully generated questions using four hand-crafted prompts, with Prompt D (the most complex) achieving the highest similarity to SQuAD baseline questions. The study demonstrates that well-designed prompts can effectively steer LLMs to produce high-quality, contextually appropriate questions for educational applications.

## Method Summary
The study employed two distinct approaches to question generation. First, a distilBERT transformer was fine-tuned on the SQuAD 1.1 dataset (87,599 training samples, 10,570 validation) with reversed question/answer format for 3 epochs (LR 0.00005, WD 0.01). Second, Meta's LLaMA model was tested with four hand-crafted prompts (A-D), generating 250 questions per prompt for 50 random contexts. Question quality was evaluated using spaCy embeddings to measure similarity to SQuAD baseline questions, with matches counted above a 0.7 threshold. Temperature was fixed at 0.5 for all LLaMA generations.

## Key Results
- DistilBERT fine-tuned on SQuAD achieved only 15.88 F1 score due to QA bias
- LLaMA Prompt D achieved highest average similarity (0.644) to SQuAD questions
- Prompt D produced most matches above 0.7 threshold (81 out of 250 questions)
- Prompt B performed worst (64 matches) due to generating complex, often unanswerable questions
- Simple Prompt A nearly matched complex Prompt D's performance (79 vs 81 matches)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a transformer on SQuAD causes it to behave as a question-answering model rather than a question-generation model. The pre-trained distilBERT was trained on massive Wikipedia and Book Corpus data to perform QA. When fine-tuned on SQuAD, it reinforced this behavior because SQuAD's inputs (context + answer) and outputs (question) match QA format. Core assumption: The model's architecture and pre-training create strong priors for QA behavior that cannot be easily reversed with limited fine-tuning data. Evidence: The transformer generated sentences similar to answers instead of questions, yielding only 15.88 F1 score.

### Mechanism 2
Prompt engineering can effectively steer LLM behavior by providing explicit instructions. The LLaMA model, being a large language model with general knowledge, can be directed through carefully crafted prompts to generate questions rather than answers. Core assumption: LLMs have sufficient general knowledge and flexibility to follow complex instructions when prompted correctly. Evidence: LLaMA successfully generated questions using the created prompts, with Prompt D achieving highest similarity to baseline questions.

### Mechanism 3
Complex prompts that include answer generation constraints produce higher quality questions. Prompt D included "answer the question in the text" which forced the model to generate answerable questions while maintaining similarity to SQuAD baseline. Core assumption: The model's ability to both generate questions and verify their answerability within context improves output quality. Evidence: Prompt D had the highest number of matches, indicating it could generate the highest quality questions.

## Foundational Learning

- **Vector similarity metrics (cosine similarity, spaCy embeddings)**: Used to evaluate question quality by comparing generated questions to SQuAD baseline questions. Quick check: What does a similarity score of 0.7 indicate about the relationship between two questions?

- **F1 score calculation and interpretation**: Used to evaluate the transformer model's performance, though it failed in this case. Quick check: Why might F1 score be inappropriate for evaluating question generation models?

- **Prompt engineering principles (clear instructions, allowing model to "think")**: The core methodology for generating questions using LLaMA. Quick check: How do temperature settings affect the diversity of generated questions?

## Architecture Onboarding

- **Component map**: Data preprocessing (tokenization, context splitting) → Model training (distilBERT fine-tuning) → Prompt engineering (LLaMA generation) → Evaluation (spaCy vector similarity) → Analysis (statistical comparison)

- **Critical path**: Context → Prompt → Question Generation → Similarity Scoring → Evaluation

- **Design tradeoffs**: Fine-tuning required significant compute but failed; prompt engineering was cheaper and successful. Simple vs complex prompts: Simple prompts (A) performed nearly as well as complex ones (D), suggesting diminishing returns on complexity. Similarity threshold: 0.7 threshold established to distinguish "matches" from dissimilar questions.

- **Failure signatures**: Transformer generating answer-like text instead of questions. Prompts producing unanswerable or compound questions. Low similarity scores indicating poor quality generation.

- **First 3 experiments**: 1) Test distilBERT with reversed SQuAD format to see if it can learn generation. 2) Vary temperature settings (0.2, 0.5, 0.8) to study effect on question diversity. 3) Test additional prompt variations focusing on question type (factual vs. analytical) to optimize for different educational needs.

## Open Questions the Paper Calls Out

### Open Question 1
How effective is the finetuned distilBERT model at generating questions compared to using prompt engineering with LLaMA? While the results show that prompt engineering with LLaMA outperformed the finetuned distilBERT model, the paper does not provide a direct comparison of the quality and diversity of questions generated by both approaches. A comprehensive evaluation considering fluency, relevance, and diversity would help determine the relative effectiveness.

### Open Question 2
How do different prompts impact the quality and type of questions generated by LLaMA? The paper tested four prompts and found Prompt D performed best overall, but does not provide detailed analysis of the types of questions generated by each prompt and how they compare to baseline questions in terms of difficulty, creativity, and educational value. A thorough analysis including similarity to baseline questions, difficulty level, and educational applications would help understand prompt impact.

### Open Question 3
How can the question generation model be improved for specific educational subjects beyond reading comprehension? The authors plan to extend their research by combining the question generation model with a classification model for educational subjects, but do not provide details on implementation or potential impact. The paper does not explore challenges of tailoring the model to specific domains like science, history, or mathematics. Developing and evaluating a specialized model for specific subjects would help assess its potential for enhancing educational applications.

## Limitations
- Exclusive reliance on similarity metrics to SQuAD questions may not fully capture educational question quality
- Single fixed temperature setting (0.5) without exploring full parameter space
- Small sample size of 50 contexts limits generalizability
- No testing on specialized educational domains where question generation might be more challenging

## Confidence

**High Confidence**: The core finding that prompt engineering with LLaMA outperforms fine-tuning a transformer on SQuAD for question generation is well-supported by empirical results. The transformer's failure to generate questions (F1 score of 15.88) and clear performance differences between prompts are directly observable.

**Medium Confidence**: The interpretation that the transformer's SQuAD training bias caused its failure is reasonable but not definitively proven. While evidence strongly suggests this mechanism, alternative explanations cannot be completely ruled out.

**Low Confidence**: The claim that Prompt D's complexity specifically caused its superior performance is not robustly established. The nearly equivalent performance of simple Prompt A suggests complexity may not be the determining factor.

## Next Checks
1. **Cross-domain validation**: Test LLaMA prompts on educational domains outside general knowledge (science, mathematics, history) to assess generalizability and identify domain-specific prompt modifications needed.

2. **Human evaluation study**: Conduct expert educational assessment of generated questions to validate whether similarity scores correlate with pedagogical quality, answerability, and educational value.

3. **Parameter sensitivity analysis**: Systematically vary temperature (0.2, 0.5, 0.8) and maximum length parameters to determine optimal settings for different educational question types and quality metrics.