---
ver: rpa2
title: Knowledge-based Refinement of Scientific Publication Knowledge Graphs
arxiv_id: '2309.05681'
source_url: https://arxiv.org/abs/2309.05681
tags:
- advice
- data
- learning
- knowledge
- author
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of authorship identification by
  modeling it as knowledge graph construction and refinement. The approach uses probabilistic
  logic models learned via relational functional gradient boosting, where relational
  regression trees are learned with human guidance in the form of first-order advice
  clauses.
---

# Knowledge-based Refinement of Scientific Publication Knowledge Graphs

## Quick Facts
- arXiv ID: 2309.05681
- Source URL: https://arxiv.org/abs/2309.05681
- Reference count: 40
- This paper addresses authorship identification by modeling it as knowledge graph construction and refinement using probabilistic logic models with advice-based regularization.

## Executive Summary
This paper addresses the problem of authorship identification by modeling it as knowledge graph construction and refinement. The approach uses probabilistic logic models learned via relational functional gradient boosting, where relational regression trees are learned with human guidance in the form of first-order advice clauses. The key method innovation is incorporating advice as a regularization term in the gradient update, allowing domain knowledge to guide model refinement throughout learning rather than just as initial bias. Experiments across seven authorship datasets demonstrate that the advice-based framework significantly improves performance over baselines, particularly in noisy and missing label scenarios.

## Method Summary
The method uses relational functional gradient boosting to learn probabilistic logic models for authorship disambiguation. First-order logic advice clauses about author names, affiliations, venues, and references are incorporated as regularization terms in the gradient updates. The algorithm iteratively learns relational regression trees, combining data gradients with advice gradients weighted by parameter α. The final model is produced by combining boosted trees using the CoTE algorithm, resulting in interpretable decision trees with fewer clauses than non-advised models.

## Key Results
- The advice-based framework achieves 17.9% improvement in AUC ROC and 18.2% improvement in AUC PR over models without advice in noisy label conditions.
- The approach produces more interpretable models with significantly fewer clauses - 5.4 vs 12.1 in average clause length and 0.43 vs 0.83 in average number of clauses.
- Performance remains robust across different advice weight parameters (α), with optimal range of 0.25-0.75, and the framework generalizes well from mixed training data to unseen test datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Advice clauses act as regularization that guides gradient updates throughout learning, not just as initial bias.
- Mechanism: The gradient update formula explicitly adds an advice term proportional to the difference between satisfied positive and negative advice counts. This term is weighted by λ and combined with the data gradient at every boosting iteration, allowing the advice to influence tree structure decisions and leaf value assignments continuously.
- Core assumption: The advice clauses are sufficiently general and applicable across the dataset, and their counts provide meaningful signal relative to the data gradient.
- Evidence anchors:
  - [abstract] "incorporating advice as a regularization term in the gradient update, allowing domain knowledge to guide model refinement throughout learning rather than just as initial bias"
  - [section] "We inject advice into the gradient of each example, which affects not only the estimation of the values of parameters but the learning of the model structure."
- Break condition: If advice clauses are too specific or data overwhelms advice signal (high α, low λ), the regularization effect weakens and model reverts to pure data-driven learning.

### Mechanism 2
- Claim: Advice reduces model complexity by directing the tree learning process toward shorter, more interpretable clauses.
- Mechanism: By providing early guidance on what relations are likely to be true (e.g., same author names → same author), the algorithm avoids exploring irrelevant or noisy splits, resulting in fewer total clauses and shorter average clause lengths in the final combined tree.
- Core assumption: The advice accurately captures domain regularities that the data might miss or misrepresent due to noise.
- Evidence anchors:
  - [abstract] "produces more interpretable models with fewer clauses"
  - [section] "the inclusion of advice leads to a significant reduction in the number of clauses in the combined tree"
- Break condition: If advice is noisy or contradictory, it may mislead the search and increase complexity or degrade performance.

### Mechanism 3
- Claim: Advice improves robustness to noisy and missing labels by reinforcing correct patterns learned from clean data.
- Mechanism: In noisy label scenarios, advice acts as a soft constraint that counteracts mislabeled examples by boosting the gradient for examples that satisfy known good rules, effectively filtering out noise during training.
- Core assumption: The majority of examples satisfying advice clauses are correctly labeled, so the advice signal outweighs the noise.
- Evidence anchors:
  - [abstract] "particularly in noisy and missing label scenarios"
  - [section] "the inclusion of advice enhances the performance of the learned model with no tuning on advice weight"
- Break condition: If noise level is extremely high and advice coverage is low, the advice may not provide sufficient corrective signal.

## Foundational Learning

- Concept: Probabilistic logic models (PLMs) combine first-order logic with probability theory to handle relational and uncertain data.
  - Why needed here: The authorship disambiguation problem is inherently relational (publications, authors, venues) and uncertain (noisy/missing labels), making PLMs a natural fit.
  - Quick check question: Can you express the fact "publication P has author A" as a first-order predicate and explain why a propositional approach would be inefficient?

- Concept: Functional gradient boosting iteratively fits regression functions to the negative gradient of the loss, building an additive model of trees.
  - Why needed here: It allows learning of weighted first-order rules (relational regression trees) that can be combined to form a probabilistic model of authorship.
  - Quick check question: How does the gradient update formula in relational FGB differ from standard gradient descent in terms of the function being optimized?

- Concept: Advice as regularization vs. advice as initial model.
  - Why needed here: Using advice as regularization (throughout learning) is more effective than seeding the initial model, especially under noisy data, because it prevents the model from drifting away from domain knowledge.
  - Quick check question: What is the key difference in how advice is incorporated in the gradient update formula compared to simply using it as the first tree?

## Architecture Onboarding

- Component map:
  Data preprocessor -> Relational regression tree learner -> Advice module -> Gradient updater -> Model combiner

- Critical path:
  1. Preprocess datasets into predicates.
  2. Initialize model with empty RRT set or constant bias.
  3. For each boosting iteration:
     - Compute data gradients from current model.
     - Evaluate advice clauses on each example.
     - Combine gradients with advice weighting.
     - Fit new RRT to combined gradients.
     - Update model parameters additively.
  4. Combine final trees using CoTE.
  5. Evaluate on test data.

- Design tradeoffs:
  - Advice weight α vs. model reliance on data: higher α gives more influence to advice but risks overfitting to potentially incorrect rules; lower α trusts data more but may miss domain insights.
  - Number of boosting iterations vs. overfitting: more trees can capture complex patterns but may overfit noisy data.
  - Clause complexity vs. interpretability: longer clauses can be more precise but harder to understand.

- Failure signatures:
  - Model performance degrades with very high noise and low advice coverage.
  - Excessive clause length or number indicates advice is not effectively guiding learning.
  - Performance insensitive to advice weight suggests advice is too general or data dominates.

- First 3 experiments:
  1. Train on a single clean dataset (e.g., INSPIRE) with and without advice; compare AUC ROC/PR and clause counts.
  2. Introduce 50% random label noise; evaluate impact of advice weight α on performance; identify robust α range.
  3. Train on merged data from multiple datasets; test generalization on unseen datasets; analyze clause interpretability and model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the advice-based framework be extended to handle dynamic knowledge graphs that evolve over time with continuous data streams?
- Basis in paper: [inferred] The paper mentions that knowledge graphs are inherently noisy and change over time, but doesn't address temporal evolution or streaming data scenarios.
- Why unresolved: The current framework is designed for static knowledge graph refinement and doesn't incorporate mechanisms for handling temporal dependencies or continuous learning from evolving data.
- What evidence would resolve it: Experiments showing performance on incrementally updated knowledge graphs, or theoretical analysis of how advice integration scales with continuous data streams.

### Open Question 2
- Question: What is the optimal strategy for combining multiple pieces of conflicting advice when they provide contradictory guidance for the same examples?
- Basis in paper: [explicit] The paper mentions that advice can be incorporated as regularization but doesn't address how to handle conflicts between different advice rules.
- Why unresolved: The current framework treats advice as additive regularization without mechanisms for conflict resolution or prioritization when advice rules disagree.
- What evidence would resolve it: Comparative experiments showing performance degradation with conflicting advice versus different conflict resolution strategies.

### Open Question 3
- Question: How can the framework be adapted to incorporate human feedback in an interactive manner, allowing experts to iteratively refine advice based on model predictions?
- Basis in paper: [inferred] The paper discusses static advice provided apriori but doesn't explore interactive refinement or learning from human feedback during the model training process.
- Why unresolved: The current approach assumes advice is fixed and provided before training, missing opportunities for active learning and iterative improvement through expert interaction.
- What evidence would resolve it: User studies demonstrating improved performance when experts can interactively provide feedback and refine advice during training.

## Limitations
- The exact implementation details of the relational regression tree learning algorithm and how advice constraints are specifically incorporated into the gradient updates are not fully specified.
- The framework assumes static knowledge graphs and doesn't address temporal evolution or streaming data scenarios.
- Performance improvements are reported without error bars or statistical significance testing, limiting confidence in the magnitude of reported gains.

## Confidence

Medium confidence. The evidence is well-supported by the abstract and methodology descriptions, but some implementation details remain unspecified. The performance improvements (17.9% in AUC ROC, 18.2% in AUC PR) are directly stated but lack error bars or statistical significance testing. The advice incorporation mechanism is clearly described in the gradient update formula, but the exact implementation of the CoTE algorithm for model combination is not fully detailed.

## Next Checks

1. **Reproduce baseline results**: Implement the core RRT learning without advice on a single clean dataset to verify baseline AUC scores before testing advice benefits.
2. **Test advice weight sensitivity**: Systematically vary the advice weight parameter α across a broader range (0.1 to 1.0) to identify the optimal range and test sensitivity to parameter choice.
3. **Evaluate model interpretability**: Analyze the final combined trees from both advised and non-advised models to quantify clause reduction and assess whether shorter clauses maintain or improve predictive accuracy.