---
ver: rpa2
title: 'Coop: Memory is not a Commodity'
arxiv_id: '2311.00591'
source_url: https://arxiv.org/abs/2311.00591
tags:
- memory
- tensor
- coop
- tensors
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Coop addresses memory fragmentation in tensor rematerialization
  by co-optimizing memory allocation and tensor eviction. Unlike prior methods that
  assume memory is fungible, Coop ensures contiguous tensor eviction using a sliding
  window algorithm and improves memory layout via cheap tensor partitioning and recomputable
  in-place operations.
---

# Coop: Memory is not a Commodity

## Quick Facts
- **arXiv ID**: 2311.00591
- **Source URL**: https://arxiv.org/abs/2311.00591
- **Reference count**: 40
- **Primary result**: Memory fragmentation in tensor rematerialization addressed via co-optimizing memory allocation and tensor eviction, achieving up to 2× memory savings and <5% fragmentation rates

## Executive Summary
Coop introduces a novel approach to memory management in deep neural network training that recognizes memory is not fungible. By co-optimizing memory allocation and tensor eviction through a sliding window algorithm, cheap tensor partitioning, and recomputable in-place operations, Coop significantly reduces memory fragmentation while maintaining computational efficiency. The method enables training of large models like GPT-3 2.7B under 25% lower memory budgets without sacrificing performance.

## Method Summary
Coop addresses memory fragmentation in tensor rematerialization by co-optimizing memory allocation and tensor eviction. It employs a sliding window algorithm to ensure contiguous tensor eviction, cheap tensor partitioning to cluster tensors by eviction cost density, and recomputable in-place operations to optimize memory layout. The method is implemented in the OneFlow framework and evaluated across eight representative DNNs, demonstrating substantial improvements in memory efficiency and computational overhead reduction compared to state-of-the-art baselines.

## Key Results
- Achieves up to 2× memory savings compared to state-of-the-art baselines
- Reduces compute overhead by up to 5× while maintaining model performance
- Lowers memory fragmentation rates to under 5% across diverse DNN architectures
- Enables GPT-3 2.7B training under 25% lower memory budgets with minimal performance impact

## Why This Works (Mechanism)

### Mechanism 1: Sliding Window Algorithm
- **Claim**: Coop avoids memory fragmentation by ensuring that evicted tensors are contiguous in memory, so the freed space is immediately usable for new allocations.
- **Mechanism**: The sliding window algorithm iterates over a contiguous sequence of tensors in memory order, evicting all tensors in the optimal window so that the resulting free block is large enough for the new allocation.
- **Core assumption**: Tensor eviction should release a single contiguous block rather than scattered fragments, and the cost of recomputation is minimized by selecting the lowest-heuristic contiguous subset.
- **Evidence anchors**:
  - [abstract]: "Coop ensures contiguous tensor eviction using a sliding window algorithm"
  - [section]: "A brute-force approach to solving Equation 1 is to traverse the combinations of all tensors... we used the sliding window algorithm to find the optimal solution and reduced the time complexity from O(2N) to O(N)."
  - [corpus]: No direct evidence; this is the novel contribution of the paper.
- **Break condition**: If the memory allocator reorders allocations arbitrarily or if eviction ordering constraints force non-contiguous blocks, the sliding window no longer guarantees contiguous free space.

### Mechanism 2: Cheap Tensor Partitioning
- **Claim**: Coop reduces recomputation cost by clustering "cheap" tensors together in memory so that evicting them incurs minimal computational overhead.
- **Mechanism**: Cheap tensor partitioning allocates tensors with similar cost density to the same end of the memory pool, reducing projected recomputation cost for evicted sequences.
- **Core assumption**: Most DNN operators can be coarsely classified into two cost density groups (super-linear vs. linear/sub-linear), and clustering by this classification lowers overall eviction cost.
- **Evidence anchors**:
  - [abstract]: "Coop ensures contiguous tensor eviction using a sliding window algorithm and improves memory layout via cheap tensor partitioning"
  - [section]: "We proposed an approach called cheap tensor partitioning... Clustering tensors with the same magnitude reduces the overall heuristic in two ways..."
  - [corpus]: Weak—no corpus examples directly compare cost density clustering to baseline.
- **Break condition**: If the cost density classification fails (e.g., many operators have intermediate densities), the heuristic may no longer guide optimal clustering.

### Mechanism 3: Recomputable In-Place Operations
- **Claim**: Coop prevents fragmentation caused by in-place operations by reusing the memory of the input tensor for the output, avoiding random allocations.
- **Mechanism**: Recomputable in-place allocates the output tensor directly into the input tensor's memory, ensuring that in-place updates do not break contiguity of the memory pool.
- **Core assumption**: In-place operations are mostly used for unevictable parameters, so reusing their memory does not interfere with eviction decisions.
- **Evidence anchors**:
  - [abstract]: "...and reduces memory fragmentation rates to under 5%..."
  - [section]: "Inspired by the functional but in-place paradigm in Perceus [27], Coop proposes a memory-reuse mechanism named recomputable in-place..."
  - [corpus]: No corpus evidence; this is a novel adaptation of the FBIP paradigm.
- **Break condition**: If an in-place operation is applied to an evictable tensor, reusing its memory could corrupt data needed for recomputation.

## Foundational Learning

- **Concept**: Memory allocator behavior in deep learning frameworks
  - **Why needed here**: Coop relies on the assumption that tensors are allocated in memory order and that free chunks are managed via a free list; understanding this is essential to reason about fragmentation.
  - **Quick check question**: Does the allocator return memory to the OS when a tensor is freed, or keep it in the free list?
- **Concept**: Tensor rematerialization cost heuristics
  - **Why needed here**: Coop's sliding window algorithm uses a heuristic combining projected recomputation cost and staleness; knowing how this is computed is key to predicting eviction behavior.
  - **Quick check question**: How does Coop's heuristic differ from DTR's, and why is memory size excluded?
- **Concept**: In-place operation semantics in dynamic computation graphs
  - **Why needed here**: Recomputable in-place replaces DTR's copy-on-write to avoid fragmentation; understanding the original problem is required to see why this helps.
  - **Quick check question**: What happens to backward dependencies if an in-place operation is performed before rematerialization?

## Architecture Onboarding

- **Component map**: Memory pool with free list -> Sliding window searcher (O(N)) -> Tensor allocator with left/right placement -> In-place reuse handler -> Cost density classifier
- **Critical path**:
  1. New allocation request → check free list
  2. If insufficient, run sliding window to find eviction set
  3. Evict chosen tensors (contiguously)
  4. Allocate new tensor at chosen end of memory pool
  5. If in-place, reuse input memory
- **Design tradeoffs**:
  - Sliding window O(N) vs. brute-force O(2^N) for eviction selection
  - Clustering by cost density vs. preserving allocation order (fragmentation vs. locality)
  - In-place reuse vs. copy-on-write (fragmentation vs. safety)
- **Failure signatures**:
  - Memory fragmentation >5% → sliding window or in-place reuse broken
  - OOM errors despite sufficient total free memory → eviction heuristic failing to select enough tensors
  - High compute overhead → cost density clustering misclassifying operators
- **First 3 experiments**:
  1. Benchmark sliding window eviction set size and contiguity vs. DTR baseline.
  2. Measure memory fragmentation rate under in-place heavy workloads.
  3. Profile compute overhead when varying cost density thresholds for clustering.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several natural extensions and research directions emerge from the results presented.

## Limitations

- The sliding window algorithm's performance under extremely high pre-existing memory fragmentation is not extensively evaluated
- The approach is primarily validated on single GPU memory systems, with no discussion of heterogeneous memory scenarios
- Energy efficiency impact of the memory optimization techniques is not measured or discussed
- The cheap tensor partitioning heuristic assumes only two cost density categories, which may not generalize to all operator distributions

## Confidence

- **High confidence**: Memory fragmentation reduction claims (direct measurements shown)
- **Medium confidence**: Compute overhead reduction (dependent on clustering heuristic accuracy)
- **Low confidence**: Search latency improvements (limited comparative analysis)

## Next Checks

1. Benchmark Coop under varying memory pool sizes (10%, 25%, 50% of total memory) to verify consistent performance across different fragmentation levels
2. Conduct ablation studies isolating the impact of cheap tensor partitioning vs. sliding window algorithm to quantify individual contributions
3. Test Coop's safety guarantees by deliberately introducing in-place operations on evictable tensors and verifying no data corruption occurs during rematerialization