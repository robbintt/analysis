---
ver: rpa2
title: A Survey on Causal Discovery Methods for I.I.D. and Time Series Data
arxiv_id: '2303.15027'
source_url: https://arxiv.org/abs/2303.15027
tags:
- causal
- discovery
- data
- graph
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews causal discovery methods for
  both i.i.d. and time series data, covering constraint-based, score-based, functional
  causal model (FCM)-based, continuous optimization-based, prior knowledge-based,
  hybrid, and miscellaneous approaches.
---

# A Survey on Causal Discovery Methods for I.I.D. and Time Series Data

## Quick Facts
- arXiv ID: 2303.15027
- Source URL: https://arxiv.org/abs/2303.15027
- Authors: 
- Reference count: 40
- Key outcome: Comprehensive review of causal discovery methods for both i.i.d. and time series data, benchmarking algorithms on multiple datasets

## Executive Summary
This survey provides an extensive overview of causal discovery methods, systematically categorizing algorithms for both i.i.d. and time series data. It covers constraint-based, score-based, functional causal model-based, continuous optimization-based, prior knowledge-based, hybrid, and miscellaneous approaches. The survey introduces essential terminologies and evaluates algorithm performance using metrics like SHD, TPR, and FDR on benchmark datasets such as ASIA, CHILD, ALARM, and HEPAR2. It highlights practical applications in healthcare, earth science, education, and business while identifying key challenges including causal sufficiency, faithfulness, and scalability.

## Method Summary
The survey systematically reviews causal discovery algorithms by methodological approach and data type. For i.i.d. data, it categorizes methods into constraint-based (PC, FCI, RFCI), score-based (GES, MMHC), FCM-based (LiNGAM, Direct-LiNGAM), continuous optimization (NOTEARS, GOLEM), knowledge-based (C-MCMC, KCRL), hybrid (MMHC), and miscellaneous approaches. For time series data, it covers constraint-based (PCMCI, LiNGAM with time lags), score-based (GES with temporal constraints), and FCM-based methods. The evaluation uses benchmark datasets with known ground-truth causal graphs, computing SHD, TPR, and FDR to compare algorithm performance across different settings.

## Key Results
- GES and Direct-LiNGAM achieve the lowest SHD on the ASIA dataset among i.i.d. methods
- NOTEARS performs best on the CHILD dataset for i.i.d. data
- GraN-DAG shows the lowest FDR across all benchmark datasets
- Most algorithms struggle with scalability, showing significant performance degradation on the HEPAR2 dataset (70 nodes, 123 edges)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The survey works by systematically categorizing causal discovery methods into i.i.d. and time series data approaches, providing a unified framework for understanding the field.
- **Mechanism**: It organizes algorithms by methodological approach (constraint-based, score-based, FCM-based, continuous optimization, knowledge-based, hybrid, and miscellaneous) and contextual setting (i.i.d. vs. time series), enabling readers to map methods to their specific use cases.
- **Core assumption**: Different types of data (i.i.d. vs. time series) require fundamentally different causal discovery approaches due to temporal dependencies and autocorrelation.
- **Evidence anchors**:
  - [abstract]: "present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (I.I.D.) data and time series data"
  - [section 3]: "Causal Discovery Algorithms for I.I.D. Data" with subsections for each methodological category
  - [section 4]: "Causal Discovery Algorithms for Time Series Data" with similar categorization
- **Break condition**: If data characteristics don't fit neatly into i.i.d. or time series categories (e.g., irregularly sampled temporal data), the categorization framework becomes less useful.

### Mechanism 2
- **Claim**: The survey works by providing comprehensive benchmarking across multiple datasets and metrics to evaluate algorithm performance.
- **Mechanism**: It presents empirical results comparing algorithms on benchmark datasets (ASIA, CHILD, ALARM, HEPAR2) using metrics like SHD, TPR, and FDR, giving readers concrete performance guidance.
- **Core assumption**: Algorithm performance can be meaningfully compared across different datasets and that these benchmarks represent real-world scenarios.
- **Evidence anchors**:
  - [section 6]: "Datasets for Causal Discovery" listing benchmark datasets with ground truth graphs
  - [section 7]: "Benchmarking Causal Discovery Algorithms" with tables showing SHD, TPR, and FDR results
  - [abstract]: "evaluate some widely used causal discovery algorithms on multiple benchmark datasets and compare their performances"
- **Break condition**: If benchmark datasets don't represent the diversity of real-world data, the comparisons may not generalize.

### Mechanism 3
- **Claim**: The survey works by bridging theoretical foundations with practical implementation through tool discussions and evaluation metrics.
- **Mechanism**: It combines theoretical explanations of causal discovery concepts with practical guidance on tools, software packages, and evaluation metrics that practitioners can actually use.
- **Core assumption**: Readers benefit from both theoretical understanding and practical implementation guidance to effectively apply causal discovery methods.
- **Evidence anchors**:
  - [section 8]: "Tools for Causal Discovery" listing software packages and repositories
  - [section 5]: "Evaluation Metrics for Causal Discovery" explaining SHD, TPR, FDR, etc.
  - [section 2]: "Preliminaries of Causal Discovery" introducing key concepts and terminologies
- **Break condition**: If the theoretical-practical gap is too large, readers may struggle to translate concepts into working implementations.

## Foundational Learning

- **Concept**: Conditional independence (CI) testing
  - Why needed here: CI testing is the fundamental mechanism underlying most constraint-based causal discovery algorithms, allowing them to determine causal relationships by checking independence between variables given conditioning sets.
  - Quick check question: Can you explain why two variables connected by a directed edge in a causal graph are always dependent, regardless of conditioning?

- **Concept**: d-separation criterion
  - Why needed here: d-separation provides the rules for determining conditional independence in causal graphs, which is essential for understanding how constraint-based algorithms work.
  - Quick check question: In a chain structure X → Y → Z, are X and Z independent given Y? What about given only X?

- **Concept**: Markov equivalence
  - Why needed here: Understanding that multiple DAGs can represent the same set of conditional independencies helps explain why different algorithms might produce different but equally valid causal structures.
  - Quick check question: Can you name two DAG structures that would be Markov equivalent because they have the same skeleton and v-structures?

## Architecture Onboarding

- **Component map**: The survey is structured around three main components: theoretical foundations (sections 1-2), algorithm catalogs (sections 3-4), and practical guidance (sections 5-9). Each component builds on the previous one, moving from concepts to algorithms to implementation.

- **Critical path**: Understanding requires following the progression from causal concepts (section 2) → algorithm categorization (sections 3-4) → evaluation methods (sections 5-6) → tools and applications (sections 8-9). Missing any component creates gaps in comprehension.

- **Design tradeoffs**: The survey prioritizes breadth over depth, covering many algorithms briefly rather than deeply analyzing a few. This allows comprehensive coverage but may leave readers wanting more implementation details for specific algorithms.

- **Failure signatures**: If readers struggle with sections 3-4, it often indicates insufficient understanding of section 2 concepts. If section 7 benchmarking results seem arbitrary, it suggests the reader hasn't grasped the evaluation metrics in section 5.

- **First 3 experiments**:
  1. Implement PC algorithm on the ASIA dataset using the pcalg R package, then verify results using d-separation rules from section 2.2.2
  2. Run NOTEARS on the CHILD dataset using the causalnex Python package, then compare the output graph's SHD against the ground truth using the metric formula from section 5
  3. Apply PCMCI to a simulated time series dataset with known ground truth, then evaluate performance using TPR and FDR metrics, referencing section 4.1.2 for algorithm details

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causal discovery algorithms effectively handle large-scale networks (100-1000 nodes) while maintaining accuracy?
- Basis in paper: [explicit] The survey notes that most existing algorithms struggle with large networks, as evidenced by poor performance on the HEPAR2 dataset (70 nodes, 123 edges), and emphasizes the need for scalable methods.
- Why unresolved: Current algorithms like GES, NOTEARS, and GraN-DAG show significant performance degradation as network size increases, indicating a lack of efficient scaling mechanisms.
- What evidence would resolve it: Development and validation of new algorithms that maintain high accuracy (e.g., SHD < 10, TPR > 0.8) on benchmark datasets with 100-1000 nodes would demonstrate progress.

### Open Question 2
- Question: How can causal discovery algorithms effectively incorporate prior knowledge to improve accuracy and reduce search space?
- Basis in paper: [explicit] The survey discusses knowledge-based approaches like C-MCMC, JCI, and KCRL, but notes that these methods require manual input and may not always be reliable or consistent.
- Why unresolved: Incorporating prior knowledge effectively remains challenging due to the need for accurate, consistent, and comprehensive domain expertise, which may not always be available or reliable.
- What evidence would resolve it: Development of algorithms that can automatically extract and validate prior knowledge from diverse sources (e.g., literature, expert systems) and demonstrate improved performance on benchmark datasets would address this issue.

### Open Question 3
- Question: How can causal discovery algorithms handle latent confounders and hidden variables effectively?
- Basis in paper: [explicit] The survey mentions that latent confounders are a common issue in real-world data, and several algorithms (e.g., FCI, RFCI, PCMCI) are designed to handle them, but their effectiveness is limited.
- Why unresolved: Latent confounders introduce bias and complexity into causal inference, making it difficult to accurately estimate causal effects and identify true causal relationships.
- What evidence would resolve it: Development of algorithms that can accurately identify and account for latent confounders, as demonstrated by improved performance on benchmark datasets with known latent structures, would resolve this issue.

## Limitations
- The survey prioritizes breadth over depth, providing brief overviews of many algorithms rather than detailed implementation guidance for specific methods
- Benchmarking focuses on synthetic and semi-synthetic datasets that may not fully capture real-world data complexities like measurement error and non-stationarity
- Limited discussion of computational complexity and scalability challenges for large-scale applications

## Confidence
- **Algorithmic categorizations**: High - follows established methodological distinctions in the literature
- **Benchmark results**: Medium - potential variations in implementation details across software packages
- **Practical guidance**: High - references well-established software repositories and documented case studies

## Next Checks
1. Replicate the SHD, TPR, and FDR calculations for the ASIA dataset using both PC and GES algorithms to verify the reported performance metrics.

2. Implement the MCSL algorithm on a time series dataset and compare its performance against PCMCI, checking whether the claimed advantages in handling high-dimensional data hold.

3. Test the sensitivity of Direct-LiNGAM's performance to sample size by running it on subsamples of the ALARM dataset ranging from 100 to 10,000 samples, observing how performance metrics change with data volume.