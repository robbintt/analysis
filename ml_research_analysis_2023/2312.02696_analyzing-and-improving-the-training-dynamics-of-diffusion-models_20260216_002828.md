---
ver: rpa2
title: Analyzing and Improving the Training Dynamics of Diffusion Models
arxiv_id: '2312.02696'
source_url: https://arxiv.org/abs/2312.02696
tags:
- training
- weight
- equation
- conv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses several training imbalances
  in diffusion models that lead to suboptimal image synthesis. The authors propose
  a systematic approach to preserve activation, weight, and update magnitudes on expectation
  by redesigning network layers.
---

# Analyzing and Improving the Training Dynamics of Diffusion Models

## Quick Facts
- arXiv ID: 2312.02696
- Source URL: https://arxiv.org/abs/2312.02696
- Reference count: 40
- Primary result: FID improved from 2.41 to 1.81 on ImageNet-512 using deterministic sampling

## Executive Summary
This paper identifies fundamental training imbalances in diffusion models that lead to suboptimal image synthesis quality. The authors systematically redesign network layers to preserve activation, weight, and update magnitudes on expectation, addressing uncontrolled growth in these quantities during training. Key innovations include forced weight normalization, controlling effective learning rate, and removing group normalizations. Additionally, they introduce a method for post-hoc setting of exponential moving average (EMA) parameters, allowing precise tuning after training. These improvements lead to a new state-of-the-art FID of 1.81 on ImageNet-512 using fast deterministic sampling, representing a significant advance in diffusion model training methodology.

## Method Summary
The authors systematically redesign diffusion model layers to address training imbalances by normalizing weight vectors before use and initializing from unit Gaussian distributions. They implement forced weight normalization to constrain weights to fixed magnitude while introducing inverse square root learning rate decay to compensate. Group normalization layers are removed, pixel normalization is added in encoder paths, and fixed-function layers are modified to preserve expected magnitudes. A novel post-hoc EMA parameter setting method allows tuning after training. The approach is validated on ImageNet-512 and ImageNet-64 using both VAE latents and RGB inputs.

## Key Results
- Achieved SOTA FID of 1.81 on ImageNet-512, improving from previous 2.41
- Deterministic sampling at 20 steps produces high-quality images
- Post-hoc EMA parameter tuning enables precise control over image quality
- Reveals complex interactions between EMA length, architecture, training time, and guidance strength

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation magnitude growth is harmful to training convergence and result quality.
- Mechanism: Residual connections in U-Net allow long paths without normalization, leading to unbounded activation magnitude growth that keeps the network in an unconverged state.
- Core assumption: The magnitude of activations directly affects the stability and quality of learning dynamics.
- Evidence anchors:
  - [abstract] "Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training"
  - [section 2.2] "the activation magnitudes grow uncontrollably in CONFIG C as training progresses, despite the use of group normalizations within each block"
  - [corpus] Weak - no direct evidence in corpus papers about activation magnitude growth specifically
- Break condition: If the residual paths are sufficiently normalized or the magnitude growth is bounded, this mechanism may not apply.

### Mechanism 2
- Claim: Weight magnitude growth leads to effective learning rate decay, causing training imbalances.
- Mechanism: Weight normalization projects gradients onto tangent planes, causing weights to grow over time. This makes the effective learning rate decay unevenly across layers.
- Core assumption: The effective learning rate (relative update size) is crucial for balanced training.
- Evidence anchors:
  - [section 2.3] "Normalization of weights before use forces loss gradients to be perpendicular to the weight vector, and taking a step along this direction always lands on a point further away from the origin"
  - [section 2.5] "the relative impact of optimizer updates, i.e., the effective learning rate, can vary uncontrollably across the layers and over time"
  - [corpus] Weak - no direct evidence in corpus papers about weight magnitude growth specifically
- Break condition: If the weight magnitude growth is controlled (e.g., through weight decay or other regularization), this mechanism may not apply.

### Mechanism 3
- Claim: Global normalization (e.g., group normalization) across images is detrimental to learning.
- Mechanism: Global normalization entangles feature representations across the entire image, making it harder for the model to learn localized features and maintain consistency across geometric transformations.
- Core assumption: Local feature representations are important for high-quality image synthesis.
- Evidence anchors:
  - [section 2.4] "we find that we can simply remove all group normalization layers with no obvious downsides" and "removing them improved the overall stability"
  - [section 2.4] "global normalization that operates across the entire image should be used cautiously"
  - [corpus] Weak - no direct evidence in corpus papers about global normalization specifically
- Break condition: If the dataset or task requires global context, this mechanism may not apply.

## Foundational Learning

- Concept: Exponential Moving Average (EMA)
  - Why needed here: EMA is used to stabilize the model during training and improve the quality of generated images.
  - Quick check question: What is the difference between traditional EMA and the power function EMA proposed in this paper?

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: The paper focuses on improving the training dynamics of DDPMs, so understanding the underlying model is crucial.
  - Quick check question: How does the denoising process work in DDPMs, and what is the role of the score network?

- Concept: Residual Networks (ResNets)
  - Why needed here: The U-Net architecture used in this paper is based on ResNets, so understanding the residual connections is important for understanding the activation magnitude growth issue.
  - Quick check question: How do residual connections affect the flow of information and gradients in a neural network?

## Architecture Onboarding

- Component map: U-Net with encoder and decoder blocks connected by skip connections, self-attention layers, group normalization layers, and an embedding network for conditioning
- Critical path: The main path through the encoder and decoder blocks, where the input is progressively downsampled and then upsampled to produce the output
- Design tradeoffs: The main design tradeoff is between the depth of the network (which affects the receptive field and model capacity) and the computational cost
- Failure signatures: If the activation or weight magnitudes grow uncontrollably, it can lead to training instability and poor result quality. If the EMA length is not properly tuned, it can also affect the quality of the generated images.
- First 3 experiments:
  1. Train the baseline model (CONFIG A) and observe the activation and weight magnitude growth over time.
  2. Implement the magnitude-preserving learned layers (CONFIG D) and compare the activation magnitude growth to the baseline.
  3. Implement the forced weight normalization (CONFIG E) and observe the effect on the weight magnitude growth and training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the magnitude-preserving architecture generalize to other diffusion model variants like RIN or DiT, and if so, how much improvement can be expected?
- Basis in paper: [explicit] "It would seem this sort of magnitude-focusing work has attracted relatively little attention outside the specific topic of ImageNet classifiers" and the authors suggest exploring this as future work.
- Why unresolved: The paper only demonstrates improvements on ADM-style U-Net architectures. The effectiveness on different architectures like recurrent or transformer-based diffusion models remains untested.
- What evidence would resolve it: Implementing and evaluating the magnitude-preserving design principles on RIN and DiT architectures, measuring FID/FDDINOv2 improvements and comparing training dynamics.

### Open Question 2
- Question: Can post-hoc EMA be effectively applied to weight tensors on a per-tensor basis, and what would be the optimal strategy for determining individual EMA lengths?
- Basis in paper: [explicit] "While post-hoc EMA allows choosing the EMA length on a per-tensor basis, we have not explored this opportunity outside this experiment."
- Why unresolved: The paper only briefly mentions the possibility but doesn't investigate it. Per-tensor EMA could potentially yield better results than global EMA but would require significant computational resources to determine optimal settings.
- What evidence would resolve it: Implementing per-tensor EMA during training, developing algorithms to determine optimal per-tensor EMA lengths, and evaluating the resulting improvements in image quality metrics.

### Open Question 3
- Question: How do stochastic sampling techniques interact with the magnitude-preserving architecture and post-hoc EMA, and can they further improve results?
- Basis in paper: [explicit] "It is likely that our results could be improved further using stochastic sampling, but we leave that as future work."
- Why unresolved: The paper focuses on deterministic sampling, leaving open the question of whether stochastic sampling could yield additional quality improvements when combined with their architectural changes.
- What evidence would resolve it: Implementing stochastic sampling with the magnitude-preserving architecture, evaluating different noise schedules and step counts, and comparing results against deterministic sampling baselines.

## Limitations

- The empirical improvements are well-documented but lack theoretical grounding about why specific imbalances harm convergence
- Claims about global normalization being detrimental need more diverse testing across different architectures and datasets
- The solutions rely on specific hyperparameter choices that may not generalize to all training scenarios

## Confidence

- **High Confidence**: The empirical improvements in FID scores (2.41 â†’ 1.81) are well-documented and reproducible. The post-hoc EMA parameter setting method is clearly defined and validated.
- **Medium Confidence**: The mechanisms explaining activation and weight magnitude growth are plausible based on empirical evidence, but lack rigorous mathematical proof. The claim that global normalization is generally detrimental needs more diverse testing.
- **Low Confidence**: The assertion that removing group normalization layers universally improves stability requires more extensive ablation studies across different architectures and datasets.

## Next Checks

1. **Cross-Dataset Validation**: Test the proposed magnitude-preserving techniques on non-ImageNet datasets (e.g., CIFAR-10, LSUN) to verify the generalizability of the findings.
2. **Theoretical Analysis**: Conduct a formal analysis of the gradient dynamics in residual networks with and without the proposed normalization techniques to better understand the convergence behavior.
3. **Architecture Ablation**: Systematically test different normalization strategies (e.g., layer normalization, instance normalization) in place of group normalization to determine the optimal normalization approach for diffusion models.