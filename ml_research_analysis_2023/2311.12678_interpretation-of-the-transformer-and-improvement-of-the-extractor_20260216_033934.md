---
ver: rpa2
title: Interpretation of the Transformer and Improvement of the Extractor
arxiv_id: '2311.12678'
source_url: https://arxiv.org/abs/2311.12678
tags:
- transformer
- matrix
- sublayer
- output
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a comprehensive interpretation of the Transformer
  architecture in plain words and proposes an improvement to a high-performance replacement
  for the multi-head self-attention, called the super high-performance Extractor (SHE).
  The core method involves interpreting the Transformer as a matrix function that
  maps an input matrix to an output matrix by driving row vectors layer by layer.
---

# Interpretation of the Transformer and Improvement of the Extractor

## Quick Facts
- arXiv ID: 2311.12678
- Source URL: https://arxiv.org/abs/2311.12678
- Reference count: 2
- Primary result: Improved SHE (iSHE) outperforms multi-head self-attention and original SHE on text generation without extra parameters or positional embeddings

## Executive Summary
This paper provides a comprehensive interpretation of the Transformer architecture as a matrix function mapping input to output matrices layer by layer. The authors propose an improved version of the super high-performance Extractor (iSHE) that addresses variance scaling issues in the original SHE, standardizing variances across sequence lengths by multiplying by 1/sqrt(i). Experiments validate both the interpretation and demonstrate that iSHE outperforms self-attention and SHE without introducing additional trainable parameters or requiring positional embeddings.

## Method Summary
The method involves interpreting the Transformer as a composite matrix function where each layer implements a function mapping input matrices to output matrices. The core innovation is the improved SHE (iSHE), which standardizes variances in the extraction output by scaling with 1/sqrt(i), where i is the sequence length. This addresses an issue in the original SHE where output variances varied with sequence length. The paper trains models on two datasets: a synthetic binary number generation task and English children's books, comparing iSHE against self-attention and the original SHE.

## Key Results
- iSHE eliminates the need for positional embeddings, saving trainable parameters
- iSHE outperforms both multi-head self-attention and original SHE in training cost
- The improvement comes without introducing additional trainable parameters
- Variance standardization via 1/sqrt(i) scaling effectively stabilizes training across sequence lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Transformer core can be interpreted as a matrix function mapping input to output layer by layer.
- Mechanism: Each Transformer layer implements a function f[k]: R^(t×d) → R^(t×d), composing into a composite function f = f[m] ◦ f[m-1] ◦ ... ◦ f[1], driving row vectors toward target vectors.
- Core assumption: The Transformer processes sequences as fixed-length by padding, enabling uniform matrix function treatment.
- Evidence anchors:
  - [abstract] The paper interprets the Transformer as a matrix function mapping input to output matrices.
  - [section 2] Lemma 1 proves the domain and codomain are both R^(t×d), and Proposition 1 states the composite function form.
  - [corpus] Weak. No direct corpus evidence for the matrix function interpretation, though related papers discuss mathematical interpretations of Transformers.
- Break condition: If sequence lengths vary significantly without padding, or if residual connections are removed, the layer-by-layer driving mechanism may fail.

### Mechanism 2
- Claim: Sublayer 1 performs dimensionality reduction by encoding multiple vectors into a single vector.
- Mechanism: Self-attention or Extractor reduces id-vectors or ld-vectors to d-vectors, serving as an encoder that generates dynamic or static weights.
- Core assumption: The input matrix rows represent vectors to be reduced, and the output rows are encoded representations.
- Evidence anchors:
  - [section 2] Proposition 2 explicitly states sublayer 1 reduces dimensionality and can be viewed as encoding.
  - [section 2] Proposition 5 explains how residual connections adjust row vectors based on prior and same-row vectors.
  - [corpus] Weak. No corpus evidence specifically on dimensionality reduction in sublayer 1, though related works mention attention mechanisms.
- Break condition: If the number of input vectors exceeds the model's capacity or if the encoding process is disrupted by noise.

### Mechanism 3
- Claim: The improved SHE (iSHE) standardizes variances by scaling with 1/sqrt(i), improving performance without extra parameters.
- Mechanism: Multiplying the extraction output by 1/sqrt(i) normalizes variance across sequence lengths, stabilizing training and output quality.
- Core assumption: Variance in extraction outputs varies with sequence length, causing performance issues.
- Evidence anchors:
  - [section 3] Describes the iSHE improvement and the variance standardization method.
  - [section 4] Experimental results show iSHE outperforms SHE and self-attention models.
  - [corpus] Weak. No direct corpus evidence for this specific variance scaling technique, though related works discuss normalization in Transformers.
- Break condition: If sequence lengths are very short or if the scaling factor is not properly tuned, the improvement may not hold.

## Foundational Learning

- Concept: Matrix functions and composition
  - Why needed here: Understanding the Transformer as a composite matrix function is key to the interpretation and improvement.
  - Quick check question: Can you express a two-layer Transformer as f = f[2] ◦ f[1] where each f[k] maps R^(t×d) to R^(t×d)?

- Concept: Dimensionality reduction and encoding
  - Why needed here: Sublayer 1's role in reducing and encoding vectors is central to how Transformers process sequences.
  - Quick check question: How does self-attention differ from the Extractor in reducing dimensionality, and what are the trade-offs?

- Concept: Variance stabilization and normalization
  - Why needed here: The iSHE improvement relies on normalizing variance across sequence lengths to stabilize performance.
  - Quick check question: Why does multiplying by 1/sqrt(i) help standardize variances, and what could go wrong if i is very small?

## Architecture Onboarding

- Component map:
  - Input embedding layer → Transformer core (m layers) → Output mapping → Softmax regression
  - Transformer core: sublayer 1 (attention/extractor) → sublayer 2 (FFN) → residual connections

- Critical path:
  1. Token indices → embedding matrix
  2. Each layer: sublayer 1 reduces/encodes, sublayer 2 transforms, residuals adjust
  3. Output matrix → softmax regression → probabilities

- Design tradeoffs:
  - Self-attention vs Extractor: Self-attention offers dynamic weights but higher compute; Extractor is faster but uses static weights
  - iSHE vs SHE: iSHE adds no parameters but requires careful scaling; may be sensitive to sequence length

- Failure signatures:
  - Training instability: Likely due to variance issues in extraction outputs
  - Poor convergence: May indicate residual connections or normalization are not working
  - Suboptimal performance: Could be from improper scaling in iSHE or insufficient capacity

- First 3 experiments:
  1. Train a minimal Transformer (d=2, m=2) on a small dataset; plot row vector trajectories to verify layer-by-layer driving
  2. Compare SHE vs iSHE on a short sequence task; measure variance of extraction outputs
  3. Replace self-attention with Extractor in a standard model; check if positional embeddings are still needed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iSHE perform on tasks beyond text generation, such as image classification or audio processing?
- Basis in paper: [inferred] The paper only evaluates the iSHE on text generation tasks, leaving its performance on other domains unexplored.
- Why unresolved: The authors focused solely on text generation for evaluation, not extending to other domains.
- What evidence would resolve it: Experiments comparing iSHE performance on image classification, audio processing, and other domains to the original SHE and multi-head self-attention.

### Open Question 2
- Question: What is the impact of varying the number of layers (m) on the performance of the iSHE compared to SHE and multi-head self-attention?
- Basis in paper: [inferred] The paper uses a fixed number of layers (m=12) for evaluation, but the impact of varying this hyperparameter is not explored.
- Why unresolved: The authors did not conduct experiments with different numbers of layers to assess the scalability and performance trade-offs of iSHE.
- What evidence would resolve it: A systematic study evaluating iSHE, SHE, and multi-head self-attention with varying numbers of layers, measuring performance and computational efficiency.

### Open Question 3
- Question: How does the iSHE handle longer sequence lengths compared to SHE and multi-head self-attention, and what are the implications for memory usage and computational cost?
- Basis in paper: [inferred] The paper uses a fixed context window length (l=128) for evaluation, but the behavior of iSHE with longer sequences is not explored.
- Why unresolved: The authors did not investigate the performance and resource requirements of iSHE with longer sequence lengths.
- What evidence would resolve it: Experiments evaluating iSHE, SHE, and multi-head self-attention with increasing sequence lengths, measuring performance, memory usage, and computational cost.

## Limitations

- The mathematical interpretation of the Transformer as a matrix function lacks direct empirical validation from corpus studies.
- The variance scaling improvement in iSHE is empirically validated but lacks rigorous theoretical justification for the specific 1/sqrt(i) factor.
- The claim that iSHE eliminates the need for positional embeddings is not fully explored for very long sequences or tasks requiring strong positional awareness.
- Experiments are conducted on relatively small-scale tasks, leaving generalizability to larger, more complex tasks unclear.

## Confidence

- **High Confidence**: The mathematical interpretation of the Transformer as a matrix function is internally consistent and supported by the provided proofs. The variance stabilization mechanism in iSHE is intuitive and empirically validated on the tested tasks.
- **Medium Confidence**: The claim that iSHE outperforms self-attention and SHE is supported by the reported experiments, but the results are based on small-scale tasks. The elimination of positional embeddings is plausible but not fully explored.
- **Low Confidence**: The broader implications of the matrix function interpretation for model understanding or the general applicability of iSHE to all Transformer tasks are not yet established.

## Next Checks

1. **Scale-Up Experiment**: Train iSHE on a larger, more complex task (e.g., machine translation or a standard language modeling benchmark like WikiText or PG-19) to assess its performance relative to self-attention at scale.
2. **Ablation Study on Positional Embeddings**: Systematically test iSHE with and without positional embeddings on tasks of varying sequence lengths to determine the limits of the claim that they are unnecessary.
3. **Theoretical Analysis of Scaling Factor**: Investigate the theoretical basis for the 1/sqrt(i) scaling factor in iSHE, potentially exploring whether other scaling functions could be more effective or if the factor should be task-dependent.