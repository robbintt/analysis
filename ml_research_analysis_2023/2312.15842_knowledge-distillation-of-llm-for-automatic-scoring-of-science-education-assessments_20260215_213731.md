---
ver: rpa2
title: Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
arxiv_id: '2312.15842'
source_url: https://arxiv.org/abs/2312.15842
tags:
- student
- teacher
- arxiv
- education
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a knowledge distillation approach to compress
  fine-tuned large language models (LLMs) for automatic scoring of science education
  assessments. The method trains a compact neural network (student) using soft labels
  from a pre-trained BERT-based LLM (teacher), guided by a specialized loss function
  that balances student accuracy and teacher mimicry.
---

# Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments

## Quick Facts
- arXiv ID: 2312.15842
- Source URL: https://arxiv.org/abs/2312.15842
- Reference count: 40
- Primary result: Student model achieves 90% accuracy with 100x fewer parameters than teacher model

## Executive Summary
This study introduces a knowledge distillation approach to compress fine-tuned large language models for automatic scoring of science education assessments. The method trains a compact neural network (student) using soft labels from a pre-trained BERT-based LLM (teacher), guided by a specialized loss function that balances student accuracy and teacher mimicry. Tested on a large science assessment dataset (7T) and three math datasets, the student model achieved 90% accuracy—comparable to the teacher—while being 100x smaller (0.02M parameters) and 10x faster in inference. The approach outperforms conventional neural networks by 12% and TinyBERT by 2% in accuracy.

## Method Summary
The method trains student neural networks using soft labels (class probabilities) from teacher BERT models. The specialized loss function combines cross-entropy between student and teacher predictions with cross-entropy between student predictions and true labels. Student architectures are LSTM-based networks with varying embedding sizes, LSTM units, and dense layers tailored to each dataset. The approach was tested on a 7T dataset with 6,684 science responses and three math datasets (Falling Weights, Gelatin, Bathtub).

## Key Results
- Student model achieved 90% accuracy—comparable to the teacher model
- Model is 100x smaller (0.02M parameters vs 1.14M-1.10M for teacher)
- Inference is 10x faster while maintaining performance
- Outperforms conventional neural networks by 12% and TinyBERT by 2% in accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The student model achieves high accuracy by mimicking the teacher model's soft label distribution rather than just hard class labels.
- Mechanism: The loss function combines cross-entropy between student predictions and teacher soft labels with cross-entropy between student predictions and true labels, balancing mimicry and accuracy.
- Core assumption: Teacher soft labels contain richer information about class boundaries than one-hot encoded true labels.
- Evidence anchors:
  - [abstract] "training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model"
  - [section] "By minimizing the discrepancy between the probability outputs of the student and teacher models, the student model can effectively adopt the knowledge and insights of the teacher model"
  - [corpus] Weak evidence - no direct corpus papers discussing soft label mimicry in LLM distillation for education
- Break condition: If teacher soft labels are noisy or biased, the student model may amplify these errors rather than correct them.

### Mechanism 2
- Claim: The specialized loss function enables the student model to retain performance while being 100x smaller in parameters.
- Mechanism: The linear combination of teacher-student cross-entropy and student-truth cross-entropy in the loss function allows the smaller model to learn both from the teacher's generalization and the ground truth.
- Core assumption: The balance parameter λ can be tuned to prevent overfitting to teacher predictions while still capturing their generalization patterns.
- Evidence anchors:
  - [abstract] "achieved 90% accuracy—comparable to the teacher—while being 100x smaller"
  - [section] "We present the prediction accuracy on the test set to assess the model's performance" and "Results have shown that the NN and distilled student models have comparable accuracy to the teacher model"
  - [corpus] Weak evidence - corpus papers discuss KD but not specifically the balance parameter in education contexts
- Break condition: If λ is poorly tuned, the student model may either overfit to teacher predictions or fail to capture their generalization benefits.

### Mechanism 3
- Claim: The student model's architecture (embedding, LSTM, dense layers) is specifically designed to handle educational assessment data effectively.
- Mechanism: The combination of embedding layers for text representation, LSTM for sequential pattern learning, and dense layers for classification creates a compact yet powerful architecture for science assessment scoring.
- Core assumption: Educational assessment responses have sequential patterns that LSTM layers can capture effectively.
- Evidence anchors:
  - [section] "For all models, dropout layers are integrated for regularization, and the model is optimized with Adam" and detailed architecture descriptions for each dataset
  - [section] "The student model size ranges from 0.1M to 0.02M, 100 times smaller in terms of parameters and ten times smaller compared with the original output model size"
  - [corpus] Weak evidence - corpus papers discuss KD architectures but not specifically for educational assessment scoring
- Break condition: If the assessment responses don't have meaningful sequential patterns, the LSTM layers may add unnecessary complexity without benefit.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The entire approach relies on transferring knowledge from a large LLM to a smaller neural network model
  - Quick check question: What is the key difference between traditional supervised learning and knowledge distillation?

- Concept: Soft Labels vs Hard Labels
  - Why needed here: The student model learns from teacher soft labels which contain probability distributions rather than single class assignments
  - Quick check question: How do soft labels provide more information than hard labels in classification tasks?

- Concept: Loss Function Design
  - Why needed here: The specialized loss function balances teacher mimicry and ground truth accuracy
  - Quick check question: What happens if you only use teacher soft labels in the loss function without the ground truth term?

## Architecture Onboarding

- Component map: Teacher model (SciEdBERT or BERT_base with 1.14M/1.10M parameters) → Soft label extraction → Student model (embedding → LSTM → dense layers, 0.02M-0.1M parameters) → Loss function → Optimized parameters
- Critical path: Data preprocessing → Teacher model inference for soft labels → Student model training with combined loss → Validation and testing
- Design tradeoffs: Model size vs accuracy (100x reduction in parameters while maintaining 90% accuracy), inference speed vs model complexity (10x faster inference), computational requirements vs deployment flexibility
- Failure signatures: Accuracy drops below 70%, student model overfitting to teacher predictions, training instability due to λ parameter tuning, slow convergence during training
- First 3 experiments:
  1. Train student model with only ground truth cross-entropy (λ=0) to establish baseline performance
  2. Train student model with only teacher soft label cross-entropy (λ=1) to measure maximum knowledge transfer potential
  3. Train student model with varying λ values (0.1, 0.5, 0.9) to find optimal balance between accuracy and teacher mimicry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of knowledge-distilled models vary when applied to more diverse and complex educational content beyond the science and mathematics datasets tested?
- Basis in paper: [inferred] The paper mentions the need for future research to expand application areas beyond automatic scoring, suggesting uncertainty about model performance on different types of educational content.
- Why unresolved: The study focused on specific datasets (science and math), limiting generalizability to other subjects or more complex educational materials.
- What evidence would resolve it: Testing the distilled models on a broader range of subjects and more complex question types, such as essays or open-ended responses, would provide evidence of their versatility and limitations.

### Open Question 2
- Question: What is the impact of teacher model quality and bias on the performance of the distilled student models, and how can this be mitigated?
- Basis in paper: [explicit] The paper discusses the potential for teacher model biases to be transferred to student models and mentions the need for future research on ethical considerations and fairness.
- Why unresolved: The study did not explicitly analyze the impact of teacher model biases on student model performance or propose methods to mitigate this issue.
- What evidence would resolve it: Conducting experiments to measure the influence of teacher model biases on student model outputs and developing techniques to detect and correct these biases would provide insights into this issue.

### Open Question 3
- Question: How can the process of distilling knowledge be optimized to better handle soft labels from teacher models, especially in cases of false positives?
- Basis in paper: [explicit] The paper mentions the limitation of directly using soft labels from the teacher model without processing, which can affect student model performance if the labels are false positives.
- Why unresolved: The study used a basic approach to handle soft labels and did not explore advanced techniques to validate or correct them.
- What evidence would resolve it: Developing and testing sophisticated validation techniques for soft labels, such as confidence thresholding or ensemble methods, would help improve the accuracy of the distilled models.

## Limitations

- The experimental evaluation relies heavily on soft label mimicry without exploring alternative distillation objectives or architecture-specific adaptations
- The λ parameter tuning process is not detailed, raising questions about reproducibility across different educational contexts
- The study uses relatively small datasets (maximum 6,684 samples for 7T) that may not capture the full variability of real-world educational assessments

## Confidence

**High Confidence**: The core finding that knowledge distillation can compress a fine-tuned BERT model into a much smaller neural network while maintaining comparable accuracy is well-supported by the experimental results. The 90% accuracy claim for the student model is directly validated against test sets, and the 100x parameter reduction is mathematically verifiable from the reported model sizes.

**Medium Confidence**: The claim about 10x faster inference is reasonable given the parameter reduction, but actual inference time measurements would strengthen this assertion. The 12% accuracy improvement over conventional neural networks and 2% improvement over TinyBERT are statistically sound but would benefit from more rigorous significance testing across multiple runs.

**Low Confidence**: The mechanism explanation for why soft labels provide superior information transfer is theoretically sound but lacks empirical validation within this study. The specific architectural choices (embedding sizes, LSTM units, layer configurations) are presented as optimal but without ablation studies to justify these particular configurations over alternatives.

## Next Checks

1. **Ablation Study on Loss Function Components**: Conduct experiments systematically varying the λ parameter from 0 to 1 in increments of 0.1 to quantify the individual contributions of teacher mimicry versus ground truth supervision. This would reveal whether the claimed balance is optimal or if one component dominates performance.

2. **Teacher Model Quality Assessment**: Evaluate the teacher model's own accuracy on held-out validation data and analyze the entropy of its soft label distributions. High-entropy soft labels may indicate uncertainty that could degrade student model performance, while low-entropy labels suggest confident predictions that may not capture nuanced scoring patterns.

3. **Cross-Dataset Generalization Test**: Apply the trained student models to unseen assessment datasets from different educational contexts to assess generalization. This would validate whether the knowledge distillation approach captures transferable scoring patterns or merely memorizes dataset-specific features.