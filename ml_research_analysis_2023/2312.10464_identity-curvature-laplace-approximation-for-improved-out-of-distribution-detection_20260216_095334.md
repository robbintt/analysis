---
ver: rpa2
title: Identity Curvature Laplace Approximation for Improved Out-of-Distribution Detection
arxiv_id: '2312.10464'
source_url: https://arxiv.org/abs/2312.10464
tags:
- laplace
- approximation
- neural
- detection
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates Laplace approximation for uncertainty
  estimation in deep neural networks, focusing on its effectiveness for out-of-distribution
  (OOD) detection. The authors identify a key pathology: the standard approach of
  fitting the Hessian matrix in Laplace approximation negatively impacts OOD detection
  performance.'
---

# Identity Curvature Laplace Approximation for Improved Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2312.10464
- Source URL: https://arxiv.org/abs/2312.10464
- Reference count: 8
- This paper proposes Identity Curvature Laplace Approximation (ICLA) that simplifies the posterior covariance to only prior precision, improving OOD detection performance while maintaining calibration.

## Executive Summary
This paper investigates Laplace approximation for uncertainty estimation in deep neural networks, focusing on its effectiveness for out-of-distribution (OOD) detection. The authors identify a key pathology: the standard approach of fitting the Hessian matrix in Laplace approximation negatively impacts OOD detection performance. They propose a novel method, Identity Curvature Laplace Approximation (ICLA), which replaces the Hessian with an identity matrix scaled by optimized prior precision. This simplified approach improves OOD detection on CIFAR-10, CIFAR-100, and ImageNet-200 datasets while maintaining calibration metrics. Experiments show that ICLA achieves higher AUROC scores for OOD detection compared to standard Laplace approximations (GGN, EF, K-FAC) and competitive performance against non-Bayesian methods like ODIN and VIM.

## Method Summary
The method proposes replacing the standard Laplace approximation's Hessian matrix with an identity matrix scaled by optimized prior precision. Standard Laplace approximation computes the posterior as q(θ|D) ∝ exp(log(p(D|θ)) - λ||θ - θ*||²), where the covariance is typically the negative Hessian of the log-likelihood plus λI. ICLA simplifies this to covariance = λI, removing the Hessian term entirely. The prior precision λ is optimized via marginal likelihood. The approach is applied to last-layer Laplace approximation (LLLA) on pre-trained ResNet-18 networks, with training using SGD (momentum 0.9, learning rate 0.1→1e-6 with cosine annealing) for 100 epochs on CIFAR datasets or 90 epochs on ImageNet-200.

## Key Results
- ICLA achieves higher AUROC scores for OOD detection compared to standard Laplace approximations (GGN, EF, K-FAC)
- Maintains calibration metrics (ECE, NLL, Brier score) while improving OOD detection
- Performance improvements attributed to alignment issues between feature embeddings and Fisher information matrix curvature
- Incorporates Fisher penalty or sharpness-aware minimization to enhance uncertainty estimation in standard Laplace approximation

## Why This Works (Mechanism)

### Mechanism 1
The identity curvature approximation simplifies the posterior covariance, reducing gradient noise that impairs OOD detection. Standard Laplace approximation computes Hessian Σ_θ = ∇²_θ log(p(D|θ)) + λI, but in overparameterized models, the Hessian introduces noisy, unstable curvature estimates. Replacing it with Σ_θ = λI (scaled identity) removes this instability.

### Mechanism 2
The identity curvature yields a wider uncertainty surface for OOD inputs, improving entropy estimates. With Σ_θ = λI, the predictive variance becomes λ times the identity, which broadens the uncertainty ellipsoid uniformly. This increases entropy for OOD samples while preserving the MAP prediction, making OOD samples more distinguishable.

### Mechanism 3
Removing Hessian alignment issues between feature embeddings and Fisher curvature improves OOD detection. The Fisher information matrix (used in GGN, EF, K-FAC) measures curvature in the parameter space, but feature embeddings in deep networks may not align well with this geometry. The identity approximation sidesteps this misalignment.

## Foundational Learning

- Concept: Laplace Approximation
  - Why needed here: Provides a tractable Gaussian posterior over network weights, enabling uncertainty estimation without full Bayesian inference
  - Quick check question: What is the mathematical form of the Laplace approximation posterior q(θ|D)?

- Concept: Overparameterization and Gradient Instability
  - Why needed here: Explains why Hessian-based curvature can be noisy in deep nets, motivating the identity approximation
  - Quick check question: Why do overparameterized models exhibit gradient instabilities during training?

- Concept: Out-of-Distribution Detection Metrics (AUROC, ECE)
  - Why needed here: The paper evaluates OOD detection using AUROC and calibration using ECE; understanding these metrics is essential for interpreting results
  - Quick check question: What does a higher AUROC score indicate in OOD detection?

## Architecture Onboarding

- Component map: Pre-trained ResNet-18 -> Last-layer Laplace approximation (LLLA) with identity curvature (ICLA) -> Predictive uncertainty (entropy) for OOD detection

- Critical path:
  1. Train base network (MAP estimate)
  2. Compute last-layer Jacobian at training points
  3. Set covariance Σ = λI (no Hessian)
  4. Tune λ via marginal likelihood
  5. Evaluate OOD detection (AUROC) and calibration (ECE)

- Design tradeoffs:
  - Identity vs. Hessian: Simpler, less noisy, but may underfit local curvature
  - Last-layer only: Computationally efficient, but may miss uncertainty in earlier layers
  - Prior precision tuning: Critical for performance; poor tuning can hurt OOD detection

- Failure signatures:
  - OOD detection AUROC close to random baseline (~50%) → λ too small or model not overparameterized
  - Calibration metrics (ECE) degrade → identity curvature too coarse for the task
  - Training instability → prior precision not well-tuned

- First 3 experiments:
  1. Reproduce baseline LLLA (GGN/EF/K-FAC) on CIFAR-10 and measure AUROC vs ICLA
  2. Sweep prior precision λ on a validation set to find optimal value
  3. Test ICLA on a toy binary classification dataset (e.g., half-moons) and visualize uncertainty surface

## Open Questions the Paper Calls Out

### Open Question 1
Does the identity curvature approximation fundamentally alter the posterior landscape of Bayesian neural networks, or is it merely a computational shortcut? The authors claim that their method preserves the general structure of the predictive distribution while providing improved OOD detection, but they do not fully characterize the mathematical implications of removing the Hessian. Mathematical analysis comparing the exact posterior, standard Laplace approximation, and ICLA in terms of KL divergence or other distributional metrics would resolve this.

### Open Question 2
Are there specific architectural characteristics of neural networks that make them particularly suited to the identity curvature approximation? The authors note that the phenomenon is independent of training dynamics and suggest it relates to the structural aspects of models, but they don't identify which architectural features are responsible. Systematic experiments varying network architectures and identifying which properties correlate with the effectiveness of ICLA would resolve this.

### Open Question 3
Does the identity curvature approximation generalize to other uncertainty estimation tasks beyond OOD detection, such as calibration in the presence of label noise or adversarial robustness? The authors demonstrate that ICLA maintains calibration metrics while improving OOD detection, suggesting it may have broader applicability. Experiments evaluating ICLA on tasks like learning with noisy labels, adversarial example detection, or uncertainty-aware active learning would resolve this.

## Limitations

- The core claim that identity curvature is superior for OOD detection rests on overparameterization assumptions that may not generalize to smaller models or structured feature spaces
- The evidence for "alignment issues" between Fisher curvature and feature embeddings is observational rather than causal, and the proposed mechanism lacks rigorous theoretical grounding
- The marginal likelihood optimization for prior precision λ is presented as straightforward but may be sensitive to implementation details

## Confidence

- Mechanism 1 (identity curvature reduces gradient noise): Medium - supported by empirical results but theoretical justification is limited
- Mechanism 2 (broader uncertainty surface improves OOD separation): Medium - visual evidence provided but quantitative validation is sparse
- Mechanism 3 (Fisher-feature misalignment): Low - largely speculative with minimal direct evidence

## Next Checks

1. Test ICLA on a small, underparameterized network to verify the overparameterization assumption
2. Visualize feature embedding geometry vs Fisher curvature in standard LLLA to directly assess alignment claims
3. Perform ablation study varying λ across several orders of magnitude to understand sensitivity to prior precision selection