---
ver: rpa2
title: Model-free Reinforcement Learning with Stochastic Reward Stabilization for
  Recommender Systems
arxiv_id: '2308.13246'
source_url: https://arxiv.org/abs/2308.13246
tags:
- latexit
- reward
- recommender
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the high stochasticity of rewards in RL-based
  recommender systems, which significantly degrades sample efficiency and performance.
  To address this, the authors propose the Stochastic Reward Stabilization (SRS) framework,
  which replaces stochastic rewards with their conditional expectations estimated
  via a supervised reward estimation model.
---

# Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems

## Quick Facts
- arXiv ID: 2308.13246
- Source URL: https://arxiv.org/abs/2308.13246
- Reference count: 40
- Key outcome: SRS and SRS2 improve sample efficiency and performance in RL-based recommender systems by replacing stochastic rewards with conditional expectations estimated by a supervised reward model.

## Executive Summary
This paper addresses the high stochasticity of rewards in RL-based recommender systems, which degrades sample efficiency and performance. The authors propose the Stochastic Reward Stabilization (SRS) framework, which replaces stochastic rewards with their conditional expectations predicted by a supervised reward estimation model. They further enhance SRS by sharing representations between the RL model and the reward estimator (SRS2), accelerating the training of user and item embeddings. Extensive experiments on a recommendation simulator and a large-scale industrial system demonstrate that SRS and SRS2 substantially improve sample efficiency and final performance across various model-free RL methods.

## Method Summary
The authors propose two frameworks to stabilize stochastic rewards in RL-based recommender systems. SRS replaces the stochastic reward with its conditional expectation predicted by a supervised reward estimation model. SRS2 extends this by sharing representations between the RL and reward estimation models, accelerating the training of user and item embeddings. Both frameworks are model-agnostic and can wrap any model-free RL algorithm. The reward estimator is trained on historical interaction data, and its predictions are used as deterministic rewards in the RL training loop.

## Key Results
- SRS and SRS2 improve sample efficiency and final performance across various model-free RL methods.
- SRS2 achieves at least 2× speedup in training compared to state-of-the-art models in certain cases.
- The proposed frameworks demonstrate substantial improvements in both a recommendation simulator and a large-scale industrial system.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing stochastic rewards with their conditional expectations reduces variance in learning signals.
- Mechanism: Stochastic feedback is replaced by a deterministic estimate (the conditional expectation) derived from a supervised reward estimation model. This deterministic signal provides more stable gradients for RL policy/value updates.
- Core assumption: The conditional expectation E(r|s,a) can be estimated accurately enough to act as a reliable proxy for the true stochastic reward in the RL objective.
- Evidence anchors:
  - [abstract] "replace the stochastic reward by its conditional expectation predicted via a supervised reward estimation model"
  - [section 4.1] "we propose a reward stabilization process that replaces the stochastic reward by its conditional expectation ˆr := E(r|s, a)"
  - [corpus] Weak anchor: "Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems" touches on estimation but not stabilization.
- Break condition: If the reward estimation model is poorly calibrated, the substituted deterministic reward may systematically bias the RL objective away from true user satisfaction.

### Mechanism 2
- Claim: Sharing representation between the reward estimator and the RL model accelerates learning of user/item embeddings.
- Mechanism: The state and action embeddings are trained jointly using gradients from the reward estimation task (which is supervised and stable) and shared with the RL model, improving sample efficiency.
- Core assumption: Gradients from the supervised reward estimation task are useful for learning representations that also benefit the RL task.
- Evidence anchors:
  - [abstract] "Integrating SRS with auxiliary training ... further accelerates the training of user and item representations"
  - [section 4.2] "the reward estimation and the RL models share a common embedding ... we only use gradients of the supervised task to update the embedding layer"
  - [corpus] No strong anchor: most related papers focus on RL or reward learning separately, not joint embedding stabilization.
- Break condition: If the reward estimation and RL objectives diverge too much, shared embeddings may propagate noise from one task into the other.

### Mechanism 3
- Claim: Model-agnostic design allows leveraging any supervised recommendation method to boost RL performance.
- Mechanism: SRS and SRS2 can wrap any model-free RL algorithm by replacing stochastic rewards in its update rule with estimated deterministic rewards.
- Core assumption: The RL algorithm's update equations can be modified without breaking convergence guarantees, as long as the reward estimate is bounded and unbiased.
- Evidence anchors:
  - [abstract] "Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models"
  - [section 4.1] "Our method only uses a reward estimation model, which is different from model-based methods that require estimating the state transition probability"
  - [corpus] Weak anchor: "Adversarial Batch Inverse Reinforcement Learning" discusses learning rewards but not wrapping arbitrary RL methods.
- Break condition: If the RL algorithm depends on stochasticity (e.g., exploration bonuses tied to reward variance), replacing rewards may degrade exploration behavior.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation for recommender systems.
  - Why needed here: The paper models user-item interactions as an MDP where states are user features, actions are item recommendations, and rewards are user feedback. Understanding the MDP setup is essential to see how stochastic rewards arise and how SRS modifies the Bellman equations.
  - Quick check question: In an MDP for recommendation, what are the typical dimensions of the state and action spaces, and why does this affect sample efficiency?

- Concept: Temporal difference (TD) learning and Q-learning.
  - Why needed here: The paper modifies the TD target by replacing stochastic rewards with estimated expectations. Knowing how TD updates work clarifies how reward stabilization changes the learning signal.
  - Quick check question: In vanilla DQN, what is the form of the TD target, and how does SRS alter it?

- Concept: Supervised reward estimation (e.g., DIN, factorization machines).
  - Why needed here: SRS relies on a pre-trained or jointly trained reward estimator. Understanding common reward estimation models explains what kind of inputs and outputs the estimator provides.
  - Quick check question: If a supervised reward model outputs click probability p, what deterministic reward should SRS use to replace a stochastic click indicator?

## Architecture Onboarding

- Component map: User/item features -> Reward estimator -> Deterministic reward -> RL model -> Policy/value update
- Critical path:
  1. Collect interaction data (s,a,r,s')
  2. Feed (s,a) into reward estimator -> r̂
  3. Update reward estimator using (s,a,r) pairs
  4. Update RL model using (s,a,r̂,s') tuples
- Design tradeoffs:
  - Stability vs. bias: deterministic rewards reduce variance but may introduce bias if the estimator is inaccurate.
  - Shared vs. separate embeddings: sharing speeds up representation learning but risks task interference.
  - Complexity of reward estimator: richer models may predict better but require more data and computation.
- Failure signatures:
  - RL performance degrades when reward estimator overfits to training data and misestimates E(r|s,a) for new states.
  - Convergence stalls if the reward estimator is too noisy or under-trained.
  - Embedding collapse if the reward estimator and RL models pull the shared embedding in incompatible directions.
- First 3 experiments:
  1. Ablation: run vanilla DQN vs. DQN + SRS on Virtual Taobao with same random seed; compare convergence curves.
  2. Reward estimator quality: evaluate reward estimator calibration (e.g., reliability diagram) on held-out data; correlate calibration error with RL performance drop.
  3. Embedding analysis: visualize shared embedding space before/after SRS2 training; check for clustering of user/item types and measure similarity to pre-trained embeddings.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions. However, based on the content, some potential open questions include:

- How does the effectiveness of the SRS framework vary across different types of stochastic reward distributions (e.g., Bernoulli, Gaussian, heavy-tailed distributions)?
- Can the SRS framework be effectively integrated with model-based reinforcement learning approaches for recommender systems, and would this combination yield better sample efficiency than model-free methods alone?
- How does the SRS framework perform in scenarios where the reward estimation model has significant prediction error, and what mechanisms could be added to handle model uncertainty?

## Limitations

- The effectiveness of the approach hinges on the quality of the reward estimation model; if poorly calibrated, it may mislead the RL policy.
- The paper does not provide detailed empirical analysis of reward estimator accuracy or calibration, nor does it discuss how reward estimator errors propagate to RL performance.
- The assumption that supervised reward gradients will benefit RL representation learning is not rigorously validated, and there is a risk of task interference if the two objectives diverge.

## Confidence

- **High confidence**: The theoretical motivation for reducing reward variance via conditional expectations is sound, and the use of a simulator (Virtual Taobao) for controlled experiments is appropriate.
- **Medium confidence**: The improvement in sample efficiency and final performance over baseline RL methods is demonstrated, but the robustness of these gains across diverse RL algorithms and reward estimation models is not established.
- **Low confidence**: Claims regarding the benefits of shared embeddings and the model-agnostic nature of SRS/SRS2 are supported only by limited experiments and lack ablation studies or error analysis.

## Next Checks

1. **Reward Estimator Calibration**: Conduct a thorough evaluation of the reward estimation model's calibration (e.g., reliability diagrams, calibration error) on held-out data, and analyze the correlation between estimator miscalibration and RL performance degradation.

2. **Cross-Algorithm Robustness**: Test SRS/SRS2 with a broader set of RL algorithms (e.g., PPO, SAC) and in multiple recommendation simulators or real-world datasets to assess the generality of the performance gains.

3. **Exploration Strategy Analysis**: Investigate the impact of deterministic rewards on exploration by comparing SRS/SRS2 against RL baselines with intrinsic motivation or stochastic exploration bonuses, particularly in sparse-reward environments.