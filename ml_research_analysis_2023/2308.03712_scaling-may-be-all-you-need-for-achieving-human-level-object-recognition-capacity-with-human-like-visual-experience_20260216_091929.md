---
ver: rpa2
title: Scaling may be all you need for achieving human-level object recognition capacity
  with human-like visual experience
arxiv_id: '2308.03712'
source_url: https://arxiv.org/abs/2308.03712
tags:
- data
- size
- imagenet
- scenario
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether current self-supervised learning
  (SSL) algorithms can achieve human-level visual object recognition capacity with
  human-like visual experience. Previous work only considered scaling of data size,
  while this study examines the simultaneous scaling of data size, model size, and
  image resolution.
---

# Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience

## Quick Facts
- arXiv ID: 2308.03712
- Source URL: https://arxiv.org/abs/2308.03712
- Authors: 
- Reference count: 4
- Human-level object recognition achievable with sub-human scales of model size, data size, and image resolution when scaled simultaneously

## Executive Summary
This paper investigates whether current self-supervised learning algorithms can achieve human-level visual object recognition capacity using human-like visual experience. Through a comprehensive scaling experiment with vision transformers trained on up to 5K hours of human-like video data at resolutions up to 476x476 pixels, the authors demonstrate that human-level performance is achievable at sub-human scales when data size, model size, and image resolution are scaled up simultaneously. Using masked autoencoders for their efficiency, the study estimates that a 2.5B parameter model trained with 20K hours of human-like video at 952x952 resolution could reach human-level accuracy on ImageNet, suggesting that generic learning algorithms and architectures without strong inductive biases can achieve human-level competence in visual object recognition.

## Method Summary
The study conducts a scaling experiment using vision transformers (ViT) with masked autoencoders (MAEs) as the self-supervised learning algorithm. Models ranging from 22M to 633M parameters are trained on subsets of 4971 hours of human-like video data (Ego4D, SAYCam, and other datasets) at image resolutions of 224x224, 448x448, and 476x476 pixels. The MAE approach uses 80% masking ratios, allowing efficient training of large models with high-resolution images. Performance is evaluated on ImageNet and out-of-distribution ImageNet benchmarks after fine-tuning with 1-2% of labeled data. A log-polynomial scaling model is used to analyze the relationship between data size, model size, image resolution, and object recognition accuracy.

## Key Results
- Human-level object recognition capacity achievable at sub-human scales when data size, model size, and image resolution are scaled simultaneously
- A 2.5B parameter ViT model trained with 20K hours of human-like video at 952x952 resolution estimated to reach human-level accuracy (>90% top-5) on ImageNet
- Masked autoencoders enable efficient scaling to large models and high resolutions by only processing unmasked patches through the encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-level visual object recognition can be achieved through simultaneous scaling of data size, model size, and image resolution
- Mechanism: The paper demonstrates that current SSL algorithms (specifically MAEs) can reach human-level performance when all three scaling factors are increased together, rather than just scaling data alone. The log-polynomial model shows multiplicative effects where improvements in each dimension compound to reach human-level accuracy.
- Core assumption: The scaling relationships between data, model size, and resolution follow predictable patterns that can be extrapolated to estimate human-level performance
- Evidence anchors:
  - [abstract] "we find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously"
  - [section] "We model the effects of data size, model size, and image resolution on object recognition accuracy as a simple polynomial function in log space"
  - [corpus] Weak - no direct citations found about this specific scaling mechanism
- Break condition: If the scaling relationships are non-linear or if there are fundamental bottlenecks in SSL algorithms that prevent further improvements regardless of scaling

### Mechanism 2
- Claim: Masked autoencoders (MAEs) enable efficient scaling to large models and high resolutions
- Mechanism: MAEs only process unmasked patches through the encoder, allowing training of much larger models with higher resolution images on modest compute budgets. This efficiency makes the simultaneous scaling experiment feasible.
- Core assumption: The computational efficiency gain from MAEs scales proportionally with model and image size
- Evidence anchors:
  - [section] "The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget"
  - [section] "MAEs work well with very high masking ratios (we use a masking ratio of 80%). Because only visible (unmasked) patches are passed through the encoder in MAEs, we can train much bigger models with larger image sizes"
  - [corpus] Weak - no direct citations about MAE efficiency scaling
- Break condition: If computational overhead grows superlinearly with scale or if MAEs hit architectural limits at very large scales

### Mechanism 3
- Claim: Human-like video data provides sufficient perceptual experience for human-level visual learning
- Mechanism: The paper uses long, continuous, mostly egocentric videos that mimic human visual experience. The combination of Ego4D, SAYCam, and other datasets provides diverse, naturalistic visual input similar to what humans experience.
- Core assumption: The statistical properties of human-like video data are sufficient to learn human-level visual representations
- Evidence anchors:
  - [section] "The videos are 'human-like' in two prominent aspects: (i) most of the videos are naturalistic, egocentric headcam videos recorded from the perspective of adult or child camera wearers during the course of their daily lives; (ii) they are temporally extended, continuous videos"
  - [section] "Human-level competence is thus achievable for a fundamental perceptual capability from human-like perceptual experience (human-like in both amount and type)"
  - [corpus] Weak - no direct citations about human-like video data sufficiency
- Break condition: If the video data lacks critical visual experiences or if the distribution differs fundamentally from human visual experience in ways that prevent learning

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: The entire approach relies on SSL algorithms (specifically MAEs) to learn visual representations without human labels, which is essential for scaling to large amounts of human-like video data
  - Quick check question: How does MAE's masking approach differ from contrastive learning methods like SimCLR or MoCo?

- Concept: Vision transformers (ViTs)
  - Why needed here: The paper exclusively uses ViTs as the architecture, and understanding their scaling properties is crucial for interpreting the results and extrapolating to larger models
  - Quick check question: What architectural advantages do ViTs have over convolutional networks when scaling to very large models and high resolutions?

- Concept: Scaling laws in deep learning
  - Why needed here: The paper's core contribution is about how different scaling factors (data, model size, resolution) interact, which requires understanding of how neural networks scale
  - Quick check question: What is the relationship between model parameter count and compute requirements when scaling transformers?

## Architecture Onboarding

- Component map: Vision Transformer (ViT) -> MAE encoder/decoder -> ImageNet/OOD ImageNet evaluation -> few-shot finetuning
- Critical path: Video data -> MAE pretraining -> model scaling -> evaluation benchmarks
- Design tradeoffs: MAEs vs. other SSL methods (efficiency vs. potential performance), different ViT sizes (parameter count vs. compute), resolution scaling (accuracy vs. memory)
- Failure signatures: Plateaus in accuracy despite scaling, overfitting on training data, poor generalization to OOD ImageNet
- First 3 experiments:
  1. Replicate the ViT-S/14 baseline on 5K hours of Ego4D at 224x224 resolution
  2. Test MAE efficiency by comparing training time with and without masking on a fixed model size
  3. Verify the log-polynomial scaling model by training a few intermediate points between the existing data points

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions arise from the work:

### Open Question 1
- Question: What is the precise relationship between scaling data size, model size, and image resolution when attempting to reach human-level object recognition capacity?
- Basis in paper: [explicit] The paper states "We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously."
- Why unresolved: The paper provides estimates based on fits to a log-polynomial model, but the exact scaling relationship is not fully explored or validated with empirical data.
- What evidence would resolve it: Additional scaling experiments with a wider range of data sizes, model sizes, and image resolutions would help to better understand the precise relationship between these factors.

### Open Question 2
- Question: How do different types of visual data (e.g., naturalistic vs. synthetic) affect the scaling of object recognition performance in self-supervised learning algorithms?
- Basis in paper: [inferred] The paper focuses on "human-like" video data, which includes naturalistic, egocentric headcam videos. However, it does not explore the effects of other types of visual data.
- Why unresolved: The paper does not investigate the effects of different types of visual data on the scaling of object recognition performance, leaving open the question of whether the results are specific to human-like data.
- What evidence would resolve it: Scaling experiments with different types of visual data (e.g., synthetic, animal-centric) would help to determine the generality of the results to other types of visual data.

### Open Question 3
- Question: What are the limitations of using vision transformers and masked autoencoders for achieving human-level object recognition capacity?
- Basis in paper: [explicit] The paper uses vision transformers and masked autoencoders as the primary models and algorithms, but does not discuss their limitations.
- Why unresolved: The paper does not explore the limitations of the chosen models and algorithms, leaving open the question of whether they are the most suitable for achieving human-level object recognition capacity.
- What evidence would resolve it: Comparative studies using alternative models and algorithms (e.g., convolutional neural networks, contrastive learning) would help to determine the limitations and advantages of the chosen approach.

## Limitations

- The scaling relationships are extrapolated beyond the experimental range, requiring assumptions about log-polynomial behavior at larger scales
- Results may be specific to MAEs and may not generalize to other self-supervised learning algorithms
- ImageNet and OOD ImageNet performance may not fully capture human-level visual competence across diverse tasks

## Confidence

- **High confidence**: The observation that simultaneous scaling of data, model size, and resolution yields multiplicative performance gains is well-supported by the experimental results within the tested range
- **Medium confidence**: The specific scaling coefficients and the extrapolation to human-level performance are reasonable but rely on assumptions about behavior beyond the experimental data
- **Low confidence**: The claim that human-level competence is achievable "from human-like perceptual experience" is the most speculative, as it assumes ImageNet-level performance equates to human-level visual competence and that the scaling will continue indefinitely

## Next Checks

1. Test the scaling relationships at intermediate points between the current data points to verify the log-polynomial model's accuracy and identify potential inflection points or bottlenecks
2. Compare MAE performance with alternative SSL algorithms (e.g., SimCLR, MoCo) at similar scales to determine if the scaling benefits are specific to MAEs or generalizable across SSL methods
3. Evaluate the models on more diverse visual tasks beyond ImageNet classification (e.g., object detection, segmentation, visual reasoning) to better assess whether the scaling approach produces truly human-like visual competence