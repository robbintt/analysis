---
ver: rpa2
title: 'Construction of Knowledge Graphs: State and Challenges'
arxiv_id: '2302.11509'
source_url: https://arxiv.org/abs/2302.11509
tags:
- data
- knowledge
- entity
- entities
- construction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively investigates knowledge graph (KG) construction
  pipelines and toolsets, focusing on requirements for incremental updates, scalability,
  and automation. The authors analyze ten open KG projects and seven toolsets, evaluating
  their capabilities across data consumption, metadata management, ontology handling,
  knowledge extraction, entity resolution/fusion, quality assurance, and knowledge
  completion.
---

# Construction of Knowledge Graphs: State and Challenges

## Quick Facts
- arXiv ID: 2302.11509
- Source URL: https://arxiv.org/abs/2302.11509
- Reference count: 40
- Primary result: Comprehensive survey analyzing KG construction pipelines and tools, identifying key limitations in incremental updates, metadata support, and entity resolution

## Executive Summary
This survey systematically examines knowledge graph construction pipelines and toolsets, focusing on requirements for incremental updates, scalability, and automation. The authors analyze ten open KG projects and seven toolsets across eight construction phases: data consumption, metadata management, ontology management, knowledge extraction, entity resolution/fusion, quality assurance, and knowledge completion. While existing approaches excel at individual construction tasks, they show significant limitations in supporting incremental KG updates, metadata management, entity resolution/fusion, and quality assurance. The study highlights the need for open-source toolsets, improved extensibility, better metadata management, and comprehensive evaluation frameworks. Most pipelines still rely on batch processing rather than true incremental updates, revealing the need for more sophisticated solutions to handle continuously changing data sources.

## Method Summary
The authors conducted a comprehensive survey of knowledge graph construction approaches through systematic analysis of ten open KG projects (Freebase, DBpedia, Wikidata, YAGO, EventKG, MusicBrainz, WikiData, BabelNet, NELL, Google Knowledge Vault) and seven KG construction toolsets (Karma, E2X, the proposed approaches, Bi-ION, PrediKat, SOFSY, Kraken). The analysis covered eight key construction phases and evaluated each approach's capabilities across requirements including incremental updates, automation, metadata management, and quality assurance. The survey synthesized findings from 40+ references to identify patterns, limitations, and open challenges in current KG construction methodologies.

## Key Results
- Existing KG construction approaches excel in individual tasks but lack support for incremental updates and comprehensive metadata management
- Entity resolution and fusion remain critical bottlenecks with limited support for schema-agnostic approaches and incremental processing
- Most toolsets still rely on batch processing rather than true incremental updates, hindering scalability and data freshness
- No comprehensive evaluation frameworks exist for assessing complete KG construction pipeline performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental KG updates require preserving intermediate match decisions and similarity graphs to avoid redundant recomputation.
- Mechanism: Incremental entity resolution maintains a similarity graph from previous clustering steps, extending it with new entities and their matches instead of recomputing all pairwise similarities.
- Core assumption: The similarity graph retains enough information to enable accurate incremental clustering without full recomputation.
- Evidence anchors:
  - [section]: "While there are many approaches for batch-like entity clustering [162, 163], the incremental maintenance of entity clusters for new entities has received comparatively little attention."
  - [section]: "If the pairwise match relationships between previously integrated KG entities are maintained in a similarity graph spawning the previous clusters, this graph can be extended by the new entities and links to the newly determined mach candidates as an input for incremental clustering [156]."
- Break condition: If similarity graph becomes too sparse or outdated, incremental clustering accuracy degrades and may require full recomputation.

### Mechanism 2
- Claim: Metadata management enables debugging and provenance tracking throughout the KG construction pipeline.
- Mechanism: Fact-level metadata (provenance, timestamps, source identifiers) is collected at each construction step and stored in a metadata repository for later analysis and quality assurance.
- Core assumption: Metadata is captured consistently across all pipeline stages and can be queried to trace errors back to their origin.
- Evidence anchors:
  - [section]: "Fact-level metadata (or annotations) in the KG can be stored either together with the data items (embedded metadata) or in parallel to the data and referenced using unique IDs (associated metadata) [87]."
  - [section]: "Metadata such as provenance information is also important for quality assurance, for example to explain and maintain KG data concerning the context and validity of conﬂicting values [9]."
- Break condition: Inconsistent metadata capture or incomplete coverage prevents effective debugging and error localization.

### Mechanism 3
- Claim: Schema-agnostic blocking enables efficient entity resolution across heterogeneous data sources with varying attribute structures.
- Mechanism: Blocking keys are derived from all attribute values and their components (words/tokens) rather than specific attribute names, allowing comparison across sources with different schemas.
- Core assumption: Entity similarity can be effectively captured through attribute value content rather than relying on schema knowledge.
- Evidence anchors:
  - [section]: "While most blocking approaches rely on domain or schema knowledge, there are also so-called schema-agnostic blocking schemes for highly heterogeneous data where entities of a certain type can have different sets of attributes."
  - [section]: "Hence, schema-agnostic approaches consider most or all attribute values and their components (e.g. words or tokes), regardless of the associated attribute names."
- Break condition: When attribute values lack discriminative power, schema-agnostic blocking produces too many false positives and becomes inefficient.

## Foundational Learning

- Concept: Knowledge Graph Data Models (RDF vs Property Graphs)
  - Why needed here: Understanding the tradeoffs between RDF and property graphs is crucial for selecting appropriate KG construction approaches and tools.
  - Quick check question: What are the key differences in how RDF and property graphs handle metadata and ontology support?

- Concept: Entity Resolution and Fusion
  - Why needed here: Entity resolution is a critical bottleneck in KG construction that directly impacts data quality and scalability.
  - Quick check question: What are the three main phases of entity resolution and how do they work together?

- Concept: Incremental Processing Paradigms
  - Why needed here: Incremental updates are essential for scalability and data freshness but require different architectural approaches than batch processing.
  - Quick check question: How does incremental entity resolution differ from batch entity resolution in terms of data flow and state management?

## Architecture Onboarding

- Component map:
  - Data Acquisition Layer: Source selection, filtering, and acquisition
  - Preprocessing Layer: Transformation, mapping, and cleaning
  - Knowledge Extraction Layer: NER, entity linking, relation extraction
  - Entity Resolution Layer: Blocking, matching, clustering
  - Fusion Layer: Entity/attribute consolidation and value selection
  - Quality Assurance Layer: Validation, repair, and completeness checking
  - Metadata Repository: Central storage for all metadata and provenance
  - Pipeline Orchestrator: Workflow management and execution control

- Critical path: Data Acquisition → Preprocessing → Knowledge Extraction → Entity Resolution → Fusion → Quality Assurance
- Design tradeoffs:
  - Batch vs incremental updates: Batch offers simplicity but poor scalability; incremental requires state management but scales better
  - Schema-based vs schema-agnostic blocking: Schema-based is more precise but requires domain knowledge; schema-agnostic handles heterogeneity but may be less accurate
  - Manual vs automatic quality assurance: Manual provides high quality but doesn't scale; automatic scales but may miss subtle errors
- Failure signatures:
  - Poor entity resolution: Duplicate entities, incorrect entity fusions, broken relationships
  - Metadata gaps: Inability to trace errors, inconsistent provenance information
  - Pipeline bottlenecks: Slow processing, memory issues, cascading failures
- First 3 experiments:
  1. Implement basic pipeline with one structured data source and one unstructured source, focusing on end-to-end data flow
  2. Add incremental entity resolution for new entities while maintaining existing clusters
  3. Integrate metadata collection and provenance tracking across all pipeline stages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific technical challenges in developing toolsets that support both incremental KG updates and streaming data ingestion simultaneously?
- Basis in paper: [explicit] The paper discusses incremental KG construction requirements and mentions that most approaches have either no or unknown support for incremental updates, while only two projects (DBpedia Live and SAGA) support continuous consumption of event streams.
- Why unresolved: While the paper identifies the need for incremental updates and streaming capabilities, it does not detail the specific technical barriers to implementing both features together in a single toolset.
- What evidence would resolve it: A comprehensive technical analysis comparing existing streaming and incremental approaches, identifying specific architectural and algorithmic challenges in combining both capabilities.

### Open Question 2
- Question: How can knowledge extraction from multimodal data (text, images, audio) be effectively integrated into KG construction pipelines while maintaining data quality?
- Basis in paper: [explicit] The paper mentions that multi-modal data are becoming increasingly popular and that there are efforts to use images as information sources for relation extraction, but notes that intra-modal relation extraction and cross-modal entity linking remain largely unresolved problems.
- Why unresolved: While the paper acknowledges the importance of multimodal data, it does not provide specific solutions for integrating these different data types into a unified KG construction process.
- What evidence would resolve it: Empirical studies demonstrating effective methods for combining information from multiple modalities while preserving data quality in the resulting KG.

### Open Question 3
- Question: What evaluation frameworks could effectively measure the performance of complete KG construction pipelines rather than just individual tasks?
- Basis in paper: [explicit] The paper identifies the evaluation of complete KG construction pipelines as an open problem, noting that while there are benchmarks for individual tasks like knowledge extraction and entity resolution, there are no comprehensive benchmarks for entire pipelines.
- Why unresolved: The paper highlights the difficulty of creating such benchmarks due to the need for near-perfect gold standards for both initial KG construction and incremental updates, but does not propose specific solutions.
- What evidence would resolve it: Development and validation of a standardized evaluation platform that includes metrics for assessing entire pipeline performance across multiple KG construction scenarios.

## Limitations

- Limited empirical validation of proposed solutions, with many claims based on literature analysis rather than performance testing
- No comprehensive performance benchmarks for complete KG construction pipelines or incremental update mechanisms
- Gaps in addressing multimodal data integration and real-time streaming requirements

## Confidence

- **High Confidence:** The fundamental challenges in KG construction (entity resolution, quality assurance, metadata management) are well-established and supported by multiple sources
- **Medium Confidence:** The specific limitations of current toolsets and approaches are reasonably supported by literature analysis but lack empirical validation
- **Low Confidence:** Claims about incremental update mechanisms and their effectiveness are largely theoretical without demonstrated performance metrics

## Next Checks

1. Implement a controlled experiment comparing batch vs incremental entity resolution on datasets with known entity overlaps to measure accuracy and performance tradeoffs
2. Conduct a systematic evaluation of metadata capture completeness across multiple KG construction tools to identify gaps in provenance tracking
3. Benchmark the scalability of schema-agnostic blocking approaches against schema-based methods using heterogeneous data sources to quantify the precision-efficiency tradeoff