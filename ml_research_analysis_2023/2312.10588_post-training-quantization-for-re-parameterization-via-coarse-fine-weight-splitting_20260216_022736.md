---
ver: rpa2
title: Post-Training Quantization for Re-parameterization via Coarse & Fine Weight
  Splitting
arxiv_id: '2312.10588'
source_url: https://arxiv.org/abs/2312.10588
tags:
- uni00000013
- quantization
- uni00000014
- uni00000048
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a post-training quantization (PTQ) method
  for reparameterized neural networks, addressing the significant accuracy degradation
  observed when applying quantization directly to these models. The authors identify
  that the large variation in weight distribution across original branches is the
  primary challenge, leading to unfavorable parameter distributions in fused kernels.
---

# Post-Training Quantization for Re-parameterization via Coarse & Fine Weight Splitting

## Quick Facts
- arXiv ID: 2312.10588
- Source URL: https://arxiv.org/abs/2312.10588
- Reference count: 27
- One-line primary result: Achieves only 0.3% accuracy loss on RepVGG-A1 with INT8 quantization

## Executive Summary
This paper introduces a post-training quantization (PTQ) method specifically designed for re-parameterized neural networks, addressing the significant accuracy degradation observed when applying standard quantization techniques to these models. The authors identify that the large variation in weight distribution across original branches is the primary challenge, leading to unfavorable parameter distributions in fused kernels. To address this issue, they propose Coarse & Fine Weight Splitting (CFWS), which separately quantizes center weights and surrounding weights to reduce quantization errors. Additionally, they develop an improved KL metric for determining optimal quantization scales for activation, particularly for handling large activation values. The proposed method achieves competitive quantization accuracy while retaining the advantages of rapid PTQ deployment, demonstrating effectiveness across various vision tasks including image classification and object detection.

## Method Summary
The method involves merging batch normalization parameters with convolutional layers, converting multi-branch structures into single convolutions, and then applying CFWS for weight quantization. CFWS splits merged kernel weights into center and surrounding components, performs coarse quantization on center weights, computes residuals, and then applies fine quantization to the combined residual and surrounding weights. For activation quantization, an improved KL divergence metric with ReLU fusion and transformed logarithmic computation is used to handle large activation outliers. The approach requires only 32 unlabeled calibration samples and no re-training, making it suitable for rapid deployment scenarios.

## Key Results
- Quantized RepVGG-A1 achieves only 0.3% accuracy loss compared to full precision
- Method generalizes well across vision tasks including image classification and object detection
- Competitive performance against existing methods like RepOpt and QARepVGG while maintaining PTQ advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight distribution disparity between original branches causes quantization error in re-parameterized networks.
- Mechanism: Re-parameterization merges multi-branch weights into a single branch; when branches have very different weight ranges (e.g., identity vs. 3x3 conv), the merged weights have a wide dynamic range with most values clustered near zero and few large outliers. Standard uniform quantization over the full range loses precision for the bulk of the values near zero.
- Core assumption: The merged kernel's center weights have significantly higher variance than the surrounding weights, and this disparity drives quantization error.
- Evidence anchors:
  - [abstract] "the primary challenge arises from the large variation in weight distribution across the original branches"
  - [section 3.1] "We have discovered that the wide distribution range of converted conv weights primarily stems from the distribution around the center point."
  - [corpus] No direct evidence; related works focus on LLMs, not reparameterization.
- Break condition: If center weights and surrounding weights have similar variance, CFWS provides no benefit.

### Mechanism 2
- Claim: Coarse & Fine Weight Splitting (CFWS) reduces quantization error by treating center and surrounding weights separately.
- Mechanism: CFWS splits the merged kernel into center weights (Wcenter) and surrounding weights (Wsurround). It performs a coarse quantization on Wcenter, computes the residual, then merges the residual with Wsurround and applies fine quantization. This compensates for the large variance in Wcenter while preventing Wsurround from being quantized with an overly wide range.
- Core assumption: The residual from coarse quantizing Wcenter can be combined with Wsurround without significant interaction error.
- Evidence anchors:
  - [section 3.1] "The overall process can be viewed as: W ≈ W = Wcoarse + Wfine"
  - [section 3.1] "the second step can be viewed as a coarse quantization of W"
  - [corpus] No direct evidence; no CFWS analog in LLM PTQ literature.
- Break condition: If merging residuals introduces instability or if center weights dominate the total energy.

### Mechanism 3
- Claim: Improved KL metric with ReLU fusion and transformed log computation handles large activation outliers better than MSE or vanilla KL.
- Mechanism: Large activation outliers are insensitive to clipping; using KL divergence on ReLU-clipped activations with log(dist_fp) - log(dist_q) avoids numerical instability. This yields a tighter quantization range that preserves accuracy.
- Core assumption: Outliers in activation distribution do not contribute meaningfully to network performance, so clipping them does not hurt accuracy.
- Evidence anchors:
  - [section 3.2] "we find that many large activation values are insensitive to clipping" and "the large values of the activation value distribution do not affect the overall accuracy"
  - [section 3.2] "we propose to transform the computation: log(dist_fp(x)) - log(dist_q(x))"
  - [corpus] No direct evidence; this is a reparameterization-specific observation not mirrored in LLM PTQ works.
- Break condition: If outliers are actually important for accuracy, clipping will hurt performance.

## Foundational Learning

- Concept: Re-parameterization in CNNs (e.g., RepVGG)
  - Why needed here: The method targets post-training quantization of re-parameterized networks; understanding how multi-branch convs collapse into a single conv is essential.
  - Quick check question: In RepVGG, which branch contributes most to the center weights of the merged 3x3 kernel?

- Concept: Symmetric uniform quantization and scale calibration
  - Why needed here: CFWS relies on standard Min-Max scale computation for coarse and fine quantization; knowing how the scale relates to clipping range is critical.
  - Quick check question: What is the formula for the scale factor `s` in symmetric uniform quantization with bit-width `k`?

- Concept: KL divergence calibration metric
  - Why needed here: The activation quantization uses a KL-based metric; understanding how KL divergence measures distribution similarity is required to grasp the improved metric.
  - Quick check question: In KL calibration, what does a lower KL divergence between full-precision and quantized activation distributions indicate?

## Architecture Onboarding

- Component map: Pre-trained reparameterized model -> BN folding -> Branch merging -> CFWS weight quantization -> KL activation calibration -> INT8 model
- Critical path:
  1. BN folding (identity + BNConv parameters)
  2. Branch merging into single conv
  3. CFWS weight quantization
  4. KL activation calibration
  5. Apply quantization to all layers
- Design tradeoffs:
  - Accuracy vs. BOPs: CFWS adds ~10-15% BOPs vs. naive INT8 but gains ~10% accuracy
  - Complexity vs. deployment speed: No re-training needed, but more complex calibration logic
  - Sensitivity: CFWS sensitive to how cleanly residuals separate; KL metric sensitive to calibration sample size
- Failure signatures:
  - Large accuracy drop after CFWS → likely center/surround split too aggressive or residual too large
  - Unstable KL calibration → likely log transform or ReLU fusion not applied; numerical overflow in log
  - Calibration sample too small → inaccurate scale estimation leading to clipping or underflow
- First 3 experiments:
  1. Run vanilla INT8 PTQ on RepVGG-A1 → record accuracy (~61.7%)
  2. Apply CFWS alone on RepVGG-A1 → measure accuracy gain (~+5-7%)
  3. Combine CFWS with KL activation quantization → measure final accuracy (~74.2%)

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Method performance on architectures beyond RepVGG and YOLOv6 remains unverified
- Computational efficiency and memory usage compared to other quantization methods not thoroughly analyzed
- Limited discussion of how method handles different activation functions beyond ReLU

## Confidence
- Weight distribution disparity identification: High
- CFWS effectiveness: Medium
- Improved KL metric: Medium

## Next Checks
1. Implement and test CFWS on a non-ResNet-like architecture (e.g., MobileNetV3) to verify generalization beyond the tested RepVGG and YOLOv6 models
2. Conduct ablation studies isolating the contributions of center/surrounding split versus the improved KL metric to quantify their individual impact on accuracy
3. Evaluate CFWS on a larger calibration dataset (e.g., 1000 samples instead of 32) to determine the sensitivity to sample size and assess potential accuracy improvements