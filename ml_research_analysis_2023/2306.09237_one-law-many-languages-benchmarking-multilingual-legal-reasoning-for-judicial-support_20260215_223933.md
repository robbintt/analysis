---
ver: rpa2
title: 'One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for Judicial
  Support'
arxiv_id: '2306.09237'
source_url: https://arxiv.org/abs/2306.09237
tags:
- legal
- language
- arxiv
- https
- court
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCALE introduces seven multilingual legal NLP datasets covering
  26 Swiss cantons and the Federal Supreme Court across five languages. The benchmark
  includes long documents (up to 50K tokens), domain-specific legal knowledge, multitasking
  (classification, IR, text generation), and reasoning tasks.
---

# One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for Judicial Support

## Quick Facts
- arXiv ID: 2306.09237
- Source URL: https://arxiv.org/abs/2306.09237
- Reference count: 40
- Best model achieves only 48.4 macro F1 on text classification

## Executive Summary
SCALE introduces seven multilingual legal NLP datasets covering 26 Swiss cantons and the Federal Supreme Court across five languages. The benchmark includes long documents (up to 50K tokens), domain-specific legal knowledge, multitasking (classification, IR, text generation), and reasoning tasks. Despite pretraining domain-specific models on 3.3B tokens of Swiss rulings and 182M tokens of legislation, existing multilingual models struggle with most tasks, achieving only 48.4 macro F1 on text classification and producing incoherent outputs for text generation.

## Method Summary
The SCALE benchmark comprises seven datasets created from Swiss court rulings and legislation, covering five languages (German, French, Italian, Romansh, English). The authors pretrain Legal-Swiss-RoBERTaBase and Legal-Swiss-LFBase on 3.3B tokens of Swiss rulings and 182M tokens of legislation, then fine-tune on individual SCALE tasks. They evaluate using standard metrics including macro F1 for classification, BERTScore/BLEU/METEOR/ROUGE for generation, and NDCG/Recall for IR tasks.

## Key Results
- Best model achieves only 48.4 macro F1 on text classification tasks
- Text generation models produce incoherent outputs even with truncated inputs
- Domain-specific pretraining improves performance but doesn't close the gap for complex reasoning tasks
- IR models struggle with multilingual, long documents (average 847 tokens for queries)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multilingual legal document understanding requires joint modeling of long documents, domain-specific knowledge, and multilingual inputs, which current models cannot fully address.
- **Mechanism**: The SCALE benchmark exposes the limitations of existing multilingual models by combining long document processing (up to 50K tokens), domain-specific legal knowledge (embodied in Swiss legislation and rulings), and multilingual understanding (German, French, Italian, Romansh, English). The benchmark's multitask structure (classification, IR, text generation) with reasoning tasks like Court View Generation and Leading Decision Summarization requires models to synthesize complex legal information across languages and document types.
- **Core assumption**: Legal documents contain intricate reasoning chains and domain-specific terminology that cannot be captured by general-purpose multilingual models trained on generic corpora.
- **Evidence anchors**:
  - [abstract]: "Our benchmark contains diverse datasets from the Swiss legal system, allowing for a comprehensive study of the underlying non-English, inherently multilingual legal system."
  - [section]: "The best model achieves only 48.4 macro F1 on text classification, and text generation models produce incoherent outputs."
  - [corpus]: Weak evidence - the corpus description focuses on dataset creation rather than model limitations.

### Mechanism 2
- **Claim**: Pretraining on in-domain legal data improves performance but does not close the performance gap for complex legal reasoning tasks.
- **Mechanism**: Legal-Swiss-RoBERTaBase and Legal-Swiss-LFBase models, pretrained on 3.3B tokens of Swiss rulings and 182M tokens of legislation, outperform general multilingual models on most tasks. However, even these specialized models struggle with the most challenging tasks (CVG and IR), indicating that domain-specific pretraining alone is insufficient for complex legal reasoning.
- **Core assumption**: Domain-specific pretraining provides better initial representations for legal language but does not teach models how to perform complex reasoning across multiple legal documents and languages.
- **Evidence anchors**:
  - [abstract]: "Despite pretraining domain-specific models on 3.3B tokens of Swiss rulings and 182M tokens of legislation, existing multilingual models struggle with most tasks."
  - [section]: "Our pre-trained model, Legal-ch RBase, outperformed XLM-RBase, suggesting that domain-specific pre-training can lead to significant improvements in model performance."
  - [corpus]: Weak evidence - the corpus description focuses on dataset statistics rather than pretraining effectiveness.

### Mechanism 3
- **Claim**: The Swiss legal system's multilingualism and federal structure create unique challenges for legal NLP models that are not present in single-language or single-jurisdiction systems.
- **Mechanism**: Switzerland's 26 cantons with distinct legal frameworks and five official languages create a naturally multilingual and multi-jurisdictional legal system. Models must handle cross-lingual query-corpus pairs in IR tasks, where laws are written in all three official languages, and must reason across different cantonal legal traditions in classification and generation tasks.
- **Core assumption**: The interaction between multilingualism and legal complexity creates compounding difficulties that exceed the sum of individual challenges.
- **Evidence anchors**:
  - [abstract]: "Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system."
  - [section]: "As laws are written in all three official languages, we end up with cross-lingual query-corpus pairs. Those pairs are then logged as qrels. Long document lengths and cross-lingual aspect may pose challenges for retrieval models."
  - [corpus]: Weak evidence - the corpus description focuses on dataset creation rather than the unique challenges of the Swiss system.

## Foundational Learning

- **Concept**: Long document processing in transformer models
  - Why needed here: Legal documents can be up to 50K tokens, exceeding the typical 512-4096 token context window of most transformer models.
  - Quick check question: What architectural modifications (e.g., Longformer, BigBird) are necessary to handle documents longer than the standard transformer context window?

- **Concept**: Domain-specific pretraining vs. fine-tuning
  - Why needed here: The benchmark shows that even extensive in-domain pretraining (3.3B tokens) does not solve the complex reasoning tasks, suggesting a fundamental limitation in current approaches.
  - Quick check question: What is the difference between pretraining a model on domain-specific data versus fine-tuning it on task-specific data, and when is each approach most effective?

- **Concept**: Multilingual model architectures and cross-lingual transfer
  - Why needed here: The benchmark requires models to handle five languages with varying amounts of training data, testing their ability to transfer knowledge across languages.
  - Quick check question: How do multilingual models like mT5 and XLM-R handle languages with limited training data, and what are the limitations of cross-lingual transfer in specialized domains like law?

## Architecture Onboarding

- **Component map**: Document extraction -> Language identification -> Section splitting -> Task-specific preprocessing -> Model training -> Evaluation using task-specific metrics

- **Critical path**: Understanding the document preprocessing pipeline (HTML/PDF extraction, language identification, section splitting) and the evaluation metrics (macro F1 for classification, BERTScore/BLEU/METEOR/ROUGE for generation, NDCG/Recall for IR)

- **Design tradeoffs**: The choice between general multilingual models (mT5, XLM-R) and domain-specific models (Legal-Swiss-RoBERTa) involves a tradeoff between broad language coverage and legal domain expertise. The decision to truncate long documents for text generation tasks versus developing architectures that can handle full-length documents is another key tradeoff.

- **Failure signatures**: Low performance on text classification tasks (48.4 macro F1) indicates models cannot capture the complex reasoning required for legal judgment prediction. Incoherent outputs in Court View Generation suggest models cannot synthesize legal arguments from facts. Poor IR performance indicates models cannot handle long, multilingual documents with cross-lingual query-document pairs.

- **First 3 experiments**:
  1. Reproduce the text classification results on a subset of the LAP dataset using XLM-RBase and Legal-Swiss-RoBERTaBase to verify the performance gap between general and domain-specific models.
  2. Test the IR task using BM25 and SBERT on a small subset of the Doc2Doc IR dataset to understand the impact of document length and multilingualism on retrieval performance.
  3. Generate sample outputs for the Court View Generation task using mT5Large with truncated input to observe the quality of legal reasoning and identify common failure patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multilingual, long-context models achieve acceptable performance on the Court View Generation task with full input length (up to 50K tokens) rather than truncated input?
- Basis in paper: [explicit] The paper shows current models struggle with Court View Generation, even with truncated inputs of 2048 tokens. The task involves generating coherent legal reasoning from facts descriptions averaging 1522 tokens, but current models are limited to 512 output tokens.
- Why unresolved: The paper was constrained by model context length limitations and could only evaluate truncated versions of the task. Full-length Court View Generation remains untested.
- What evidence would resolve it: Evaluating current or future models on the full-length task with metrics like BERTScore, BLEU, METEOR, and ROUGE on complete input-output pairs.

### Open Question 2
- Question: Does in-domain pretraining on Swiss legal data (rulings and legislation) provide consistent performance gains across all tasks in the SCALE benchmark, or are there tasks where it underperforms compared to general multilingual models?
- Basis in paper: [explicit] The paper shows Legal-Swiss-RoBERTaBase outperforms XLM-RBase on aggregated text classification, but Legal-Swiss-RoBERTaLarge underperforms XLMLarge despite more parameters. Performance varies significantly across individual tasks.
- Why unresolved: The paper provides mixed results on pretraining effectiveness, with some tasks showing gains while others show no improvement or degradation.
- What evidence would resolve it: Systematic ablation studies comparing in-domain vs. general pretraining across all SCALE tasks with statistical significance testing.

### Open Question 3
- Question: Can information retrieval models effectively handle multilingual, long legal documents without truncation while maintaining high recall and NDCG scores?
- Basis in paper: [explicit] The paper shows current IR models struggle with multilingual doc-to-doc retrieval on long documents (average 847 tokens for queries). SBERT models lose context after truncation to 512 tokens, and lexical models like BM25 don't optimize well for long documents.
- Why unresolved: The paper identifies this as a key challenge but doesn't explore solutions like sparse attention mechanisms, retrieval-augmented generation, or domain-specific indexing strategies.
- What evidence would resolve it: Benchmarking IR models on full-length documents using techniques like Longformer attention, late interaction models, or document chunking with aggregation strategies.

## Limitations

- The benchmark focuses on the Swiss legal system, which may not generalize to other legal systems with different structures and requirements.
- The evaluation metrics, while standard, may not fully capture the nuanced quality of legal reasoning required for judicial support.
- The study does not explore alternative architectural solutions beyond standard transformer adaptations, leaving open the question of whether different model designs could better address these challenges.

## Confidence

High: The methodology is clearly described, and the results are reproducible with the provided datasets and code.

Medium: The performance gap between general and domain-specific models is well-established, but the exact contribution of each challenge (document length, multilingualism, domain complexity) remains unclear.

Low: The generalizability of results to other legal systems is uncertain due to the focus on the Swiss legal system's unique characteristics.

## Next Checks

1. Verify the text classification results on a subset of the LAP dataset using both XLM-RBase and Legal-Swiss-RoBERTaBase to confirm the reported performance gap.
2. Test IR performance on a small subset of the Doc2Doc IR dataset using BM25 and SBERT to understand the impact of document length and multilingualism on retrieval.
3. Generate sample outputs for the Court View Generation task using mT5Large with truncated input to observe the quality of legal reasoning and identify common failure patterns.