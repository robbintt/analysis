---
ver: rpa2
title: Linearity of Relation Decoding in Transformer Language Models
arxiv_id: '2308.09124'
source_url: https://arxiv.org/abs/2308.09124
tags:
- relations
- relation
- person
- knowledge
- faithfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how transformer language models represent and
  retrieve relational knowledge. The authors propose that for some relations, the
  mapping from subject representations to predicted objects can be well-approximated
  by a linear transformation (linear relational embedding, or LRE).
---

# Linearity of Relation Decoding in Transformer Language Models

## Quick Facts
- arXiv ID: 2308.09124
- Source URL: https://arxiv.org/abs/2308.09124
- Reference count: 40
- Key outcome: 48% of relations show LRE faithfulness >60%; strong correlation between faithfulness and causal edit efficacy across layers and ranks.

## Executive Summary
This paper investigates how transformer language models represent and retrieve relational knowledge, proposing that certain relations can be decoded through approximately linear transformations of subject representations (Linear Relational Embeddings, or LREs). The authors estimate LRE parameters using Jacobian approximations and evaluate their effectiveness across 47 relations in three model architectures. They find that while many relations exhibit strong linear decoding patterns, relations involving named entities like companies and people show significantly lower faithfulness, suggesting more complex non-linear encoding mechanisms. The work also introduces an "attribute lens" visualization technique to examine knowledge representation even when model predictions are incorrect.

## Method Summary
The authors estimate LRE parameters by computing the Jacobian of the mapping from subject representations to object predictions using a first-order Taylor approximation across multiple samples. For each relation, they extract subject representations from a chosen layer, estimate the Jacobian W and bias b by averaging across samples, and apply a scaling factor β to account for layer normalization effects. They then evaluate faithfulness by comparing LRE predictions to the model's actual outputs and test causality by applying inverse transformations to edit subject representations and observe changes in predicted objects. Hyperparameter optimization searches over layer positions and rank values to maximize performance metrics.

## Key Results
- LREs achieve >60% faithfulness for 48% of tested relations
- Strong correlation exists between faithfulness and causal edit efficacy
- Relations involving company or person names show significantly lower LRE performance
- Attribute lens visualization reveals knowledge representation patterns even when final predictions are incorrect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer LMs implement certain relation decoding as approximately linear transformations
- Mechanism: For specific relations, the mapping from subject embeddings to object embeddings is well-approximated by affine functions LRE(s) = βWrs + br
- Core assumption: Underlying computation is near-linear in the region of interest with zero-mean deviations
- Evidence anchors: Abstract states linear approximation works for subset of relations; sections show affine decoding procedure; related work sparse on direct Jacobian estimation evidence
- Break condition: Relations with many possible objects (like person names) show poor faithfulness due to non-linear encoding needs

### Mechanism 2
- Claim: LRE parameters estimated via first-order Taylor approximation of LM Jacobian
- Mechanism: Jacobian W = ∂F/∂s estimated by averaging over multiple samples, assuming zero-mean error
- Core assumption: Error term has zero mean and zero Jacobian in expectation
- Evidence anchors: Section provides Taylor approximation formula; describes mean Jacobian estimation; related work limited on this specific application
- Break condition: Highly non-linear underlying computation causes Taylor approximation to fail

### Mechanism 3
- Claim: LREs enable causal editing through inverse transformations
- Mechanism: Edit direction computed as ∆s = W†(o′ − o) using pseudoinverse of Wr
- Core assumption: LRE approximation accurate enough to guide effective editing
- Evidence anchors: Section discusses modeling causal influence; describes edit procedure; causality experiments limited in related work
- Break condition: Poorly conditioned pseudoinverse or inaccurate LRE approximation prevents reliable editing

## Foundational Learning

- **Concept**: Jacobian matrix and local linear approximation
  - Why needed: Jacobian W = ∂F/∂s captures how subject representation changes affect object predictions, essential for LRE construction
  - Quick check: What does the Jacobian matrix represent for F: Rm → Rn, and how is it used in first-order Taylor approximation?

- **Concept**: Singular value decomposition and pseudoinverse
  - Why needed: Pseudoinverse W† computes edit directions; SVD explains why low-rank approximations improve stability
  - Quick check: How does SVD of W = UΣVT help compute W†, and why use low-rank approximations?

- **Concept**: Layer normalization effects on linear transformations
  - Why needed: Layer normalization affects activation scale, motivating β > 1 to correct magnitude underestimation
  - Quick check: How does layer normalization affect activation scale, and why might this cause linear approximation magnitude underestimation?

## Architecture Onboarding

- **Component map**: Subject representation s (hidden layer) -> Object representation o (final layer) -> Jacobian W (estimated) -> Bias b -> Scaling factor β -> LRE(s) = βWs + b
- **Critical path**: Extract subject representations from samples → Estimate W and b via averaging → Apply scaling β → Evaluate faithfulness by comparing to actual predictions
- **Design tradeoffs**: First-order Taylor approximation is simple but may fail for non-linear relations; pseudoinverse introduces stability but may lose information; layer and rank choices involve accuracy-robustness tradeoff
- **Failure signatures**: Low faithfulness indicates non-linear relation; poor causality suggests inaccurate LRE; high faithfulness but low causality may indicate information leakage
- **First 3 experiments**: 
  1. Estimate LRE for "adjective antonym" relation and evaluate faithfulness on test set
  2. Perform causal edit using estimated LRE and verify prediction change
  3. Visualize attribute lens for "person plays instrument" relation across layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do relations involving company or person names exhibit low LRE faithfulness despite accurate LM predictions?
- Basis in paper: [explicit] LREs achieve <6% faithfulness for "company CEO" and "task done by tool" relations despite 69% and 30% LM accuracy
- Why unresolved: Authors speculate complex non-linear encoding involving multiple layers but lack definitive evidence
- What evidence would resolve it: Analysis of activation and attention patterns across layers using mechanistic interpretability tools to identify non-linear computation mechanisms

### Open Question 2
- Question: What is the precise relationship between layer ℓ_r choice and LRE effectiveness?
- Basis in paper: [explicit] LRE faithfulness varies across layers with peak performance at specific layers declining in later layers
- Why unresolved: Paper observes phenomenon and speculates about "mode switch" but doesn't rigorously test hypothesis
- What evidence would resolve it: Systematic experimentation varying ℓ_r across all layers combined with representational content analysis to identify optimization shifts

### Open Question 3
- Question: How generalizable are LREs across different transformer architectures and model sizes?
- Basis in paper: [explicit] Similar patterns found across GPT-J, GPT-2-XL, and LLaMA-13B but systematic exploration lacking
- Why unresolved: Limited comparison across three specific architectures without exploring full model space
- What evidence would resolve it: Extensive evaluation across wide range of transformer models varying in size, architecture, and training objectives

## Limitations
- Relations involving named entities like companies and people show significantly lower LRE faithfulness
- Causality results may be influenced by information leakage from earlier layers
- Attribute lens visualization technique lacks validation against ground truth knowledge bases

## Confidence
- Linearity hypothesis for relation decoding: Medium-High
- Jacobian estimation methodology: Medium-High
- Causality claims: Medium
- Attribute lens effectiveness: Low-Medium

## Next Checks
1. Test LRE performance on relations with known non-linear structures to characterize failure modes
2. Conduct ablation studies removing information from earlier layers to isolate causality results
3. Compare attribute lens visualizations against ground truth knowledge to quantify accuracy