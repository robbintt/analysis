---
ver: rpa2
title: A Reduction-based Framework for Sequential Decision Making with Delayed Feedback
arxiv_id: '2302.01477'
source_url: https://arxiv.org/abs/2302.01477
tags:
- algorithm
- regret
- feedback
- delayed
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of sequential decision making with
  delayed feedback, which is common in practical applications such as recommendation
  systems and robotics. The authors propose a novel reduction-based framework that
  can convert any multi-batched algorithm for sequential decision making with instantaneous
  feedback into a sample-efficient algorithm that can handle stochastic delays.
---

# A Reduction-based Framework for Sequential Decision Making with Delayed Feedback

## Quick Facts
- **arXiv ID**: 2302.01477
- **Source URL**: https://arxiv.org/abs/2302.01477
- **Reference count**: 40
- **Key outcome**: Novel reduction-based framework that converts multi-batched algorithms for instantaneous feedback into sample-efficient algorithms handling stochastic delays

## Executive Summary
This paper addresses sequential decision making with delayed feedback, a common challenge in recommendation systems and robotics. The authors propose a reduction-based framework that transforms any multi-batched algorithm into one that can handle stochastic delays by waiting for sufficient delayed observations before proceeding to the next batch. The framework provides sharper results across various problems including linear bandits, tabular MDPs, and Markov games, and offers the first studies on delays in sequential decision making with function approximation.

## Method Summary
The paper introduces a reduction framework that converts multi-batched algorithms (which run in discrete policy update phases) into delayed-feedback variants. The key mechanism waits for enough delayed observations to satisfy the stopping criteria of each batch before proceeding. The framework provides theoretical regret bounds that scale with the number of batches and expected delay, not the total sum of delays. By plugging different multi-batched algorithms into this framework, the authors obtain improved results for linear bandits, tabular MDPs, Markov games, and settings with function approximation.

## Key Results
- Provides the first reduction-based framework for (multi-agent) sequential decision making with stochastic delayed feedback
- Achieves sharper regret bounds compared to existing works by decoupling delay effects from algorithm performance
- Establishes complete results for various settings including linear bandits, tabular MDPs, Markov games, and MDPs with function approximation
- Improves existing results and provides novel analyses for delayed feedback in sequential decision making with function approximation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reduction-based framework transforms any multi-batched algorithm into a delayed-feedback variant by waiting for sufficient delayed observations before proceeding to the next batch.
- Mechanism: At each batch, the algorithm executes the underlying multi-batched policy and collects trajectory feedback only when the episode number plus delay equals the current episode. This ensures that after waiting for `dτ(q)` episodes, enough delayed feedback is gathered to satisfy the stopping criteria of the original batch.
- Core assumption: The delays are stochastic, independent, and identically distributed (i.i.d.) with a known or bounded distribution.
- Evidence anchors:
  - [abstract]: "The key idea is to run the original algorithm and wait for enough delayed feedback to satisfy the stopping criteria of each batch."
  - [section]: "In each batch of the algorithm, we run some extra steps until we observe enough feedback so that the stopping criteria is satisfied."
- Break condition: If delays are adversarial or bursty, the waiting time could be unbounded, breaking the framework.

### Mechanism 2
- Claim: The regret bound scales with the number of batches and the expected delay, not the total sum of delays.
- Mechanism: The regret is decomposed into two parts: the regret of the underlying multi-batched algorithm and the extra regret from running extra episodes to wait for delayed feedback. The extra regret is bounded by the product of the number of batches and the expected delay per batch.
- Core assumption: The delays have finite expectation and are subexponential, allowing concentration inequalities to apply.
- Evidence anchors:
  - [abstract]: "By plugging different multi-batched algorithms into our framework, we provide sharper results... compared to existing works."
  - [section]: "The regret in a batch can be divided into two parts: the first part is the regret caused by running the algorithm in the undelayed environment; the second part is caused by running extra episodes to wait for delayed feedback."
- Break condition: If the expected delay is infinite or the number of batches is exponential, the regret bound becomes vacuous.

### Mechanism 3
- Claim: The quantile function `dτ(q)` provides a high-probability bound on the waiting time needed per batch.
- Mechanism: For any quantile `q ∈ (0,1)`, with probability at least `1 - exp(-q·m/8)`, if a batch runs for `m_q + dτ(q)` episodes, it will observe `m` feedbacks. This allows tuning `q` to balance the waiting time and the probability of insufficient feedback.
- Core assumption: The delays are independent and have a well-defined quantile function.
- Evidence anchors:
  - [section]: "Lemma 15 means, if the length of a batch m is larger than log K/q, then with high probability, if we run the batch for m_q + dτ(q) episodes, we will observe m feedbacks, which is enough for the algorithm to finish the batch."
- Break condition: If the delay distribution has heavy tails or infinite variance, the quantile function may not provide a useful bound.

## Foundational Learning

- Concept: Multi-batched algorithms
  - Why needed here: The reduction framework relies on converting a multi-batched algorithm (which runs in discrete policy update phases) into one that can handle delayed feedback. Understanding the structure of batches, stopping criteria, and policy updates is essential to applying the framework.
  - Quick check question: What is the difference between a multi-batched algorithm and an algorithm with low switching cost?

- Concept: Delayed feedback in sequential decision making
  - Why needed here: The problem setting involves stochastic delays in receiving feedback after taking actions. Understanding how delays affect the learning process and regret bounds is crucial for applying the framework.
  - Quick check question: How does the presence of delays change the way an algorithm updates its policy?

- Concept: Regret decomposition and concentration inequalities
  - Why needed here: The analysis of the reduction framework involves decomposing regret into parts and applying concentration inequalities to bound the extra regret from waiting for delayed feedback. Familiarity with martingale inequalities and subexponential random variables is important.
  - Quick check question: What is the difference between the regret of the underlying multi-batched algorithm and the extra regret from delayed feedback?

## Architecture Onboarding

- Component map: Multi-batched algorithm (ALG) -> Reduction framework -> Delay distribution -> Stopping criteria

- Critical path:
  1. Initialize dataset D0 = ∅
  2. For each batch m:
     a. ALG calculates a policy πm and stopping criteria SC using previous data Dm-1
     b. Initialize D = ∅
     c. While SC(D) = 0:
        - Execute πm in episode k
        - Collect trajectory feedback {ot : t + τt = k}
        - Update D = D ∪ {ot : t + τt = k}
     d. Update dataset Dm = Dm-1 ∪ D
  3. Return final policy

- Design tradeoffs:
  - Waiting time vs. feedback quality: Increasing the quantile q reduces the probability of insufficient feedback but increases the waiting time per batch
  - Batch size vs. policy update frequency: Larger batches reduce the number of policy updates but may increase the waiting time for delayed feedback
  - Algorithm choice vs. regret bound: Different multi-batched algorithms may have different regret bounds and batch structures, affecting the overall performance of the reduction

- Failure signatures:
  - Excessive waiting time: If the delays are too large or bursty, the algorithm may spend most of its time waiting for feedback, leading to poor sample efficiency
  - Insufficient feedback: If the quantile q is too low, the algorithm may not collect enough feedback to satisfy the stopping criteria, leading to suboptimal policy updates
  - Regret blowup: If the number of batches is exponential or the expected delay is infinite, the regret bound may become vacuous

- First 3 experiments:
  1. Implement the reduction framework for a simple multi-armed bandit problem with known delays, using a basic multi-batched algorithm like epoch-greedy
  2. Evaluate the impact of the quantile q on the waiting time and feedback quality in a simulated delayed feedback environment
  3. Compare the performance of the reduction framework with the original multi-batched algorithm on a delayed linear bandit problem, using both synthetic and real-world datasets

## Open Questions the Paper Calls Out

- Question: Is the theoretical framework presented in this paper tight for Markov Decision Processes (MDPs) and Markov Games with delayed feedback?
- Basis in paper: [inferred] The paper states "it remains unclear whether the result is tight for MDP and Markov game with delayed feedback" in the conclusion section.
- Why unresolved: The paper provides theoretical bounds for MDPs and Markov Games, but does not prove whether these bounds are optimal or can be improved further.
- What evidence would resolve it: Constructing examples where the theoretical bounds are achieved, or developing new algorithms that outperform the existing bounds, would help determine if the current results are tight.

- Question: Can we achieve a smaller batch number for tabular Markov Games using the proposed multi-batched algorithm?
- Basis in paper: [explicit] The paper mentions "it is possible that we can derive a multi-batched algorithm for tabular Markov game with a smaller batch number" in the conclusion section.
- Why unresolved: The paper uses a doubling trick to control the number of updates, which ensures a low batch number but may not be optimal. Finding a more efficient way to update the policy could potentially reduce the batch number.
- What evidence would resolve it: Developing a new algorithm with a smaller batch number and proving its regret bound would provide evidence on whether the current batch number can be improved.

- Question: How does the proposed reduction-based framework perform in practice for sequential decision making problems with delayed feedback?
- Basis in paper: [inferred] The paper focuses on theoretical analysis and does not provide empirical results or practical implementation details.
- Why unresolved: The paper establishes theoretical guarantees for the reduction-based framework, but it is unclear how well it performs in real-world applications or how sensitive it is to hyperparameters and implementation choices.
- What evidence would resolve it: Conducting empirical experiments on benchmark problems and comparing the performance of the proposed framework with existing methods would provide insights into its practical effectiveness.

## Limitations
- Requires stochastic, i.i.d. delays with finite expectation and subexponential tails, excluding adversarial or bursty delay patterns
- Assumes access to the quantile function of the delay distribution, which may not be available in practice
- The reduction framework's effectiveness depends on strong distributional assumptions about delays that may not hold in real-world systems

## Confidence
- **Mechanism 1**: High - directly builds on established multi-batched algorithm structures
- **Mechanism 2**: Medium - regret bounds depend on strong distributional assumptions about delays
- **Mechanism 3**: Medium - quantile function approach requires well-behaved delay distributions

## Next Checks
1. Empirical validation of the quantile function approach on real-world delay distributions (e.g., network latency data) to verify if subexponential assumptions hold
2. Stress testing the framework with bursty and correlated delay patterns to identify breaking points
3. Benchmarking against state-of-the-art delayed feedback algorithms on common sequential decision making tasks to verify claimed improvements in sample efficiency