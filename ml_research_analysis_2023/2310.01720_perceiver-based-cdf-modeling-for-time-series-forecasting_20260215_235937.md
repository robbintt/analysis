---
ver: rpa2
title: Perceiver-based CDF Modeling for Time Series Forecasting
arxiv_id: '2310.01720'
source_url: https://arxiv.org/abs/2310.01720
tags:
- data
- tactis
- time
- series
- practis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PrACTiS, a perceiver-based architecture combined
  with a copula-based decoder for time series forecasting. By leveraging the perceiver
  as the encoder, the model efficiently transforms high-dimensional multimodal data
  into a compact latent space, significantly reducing computational demands.
---

# Perceiver-based CDF Modeling for Time Series Forecasting

## Quick Facts
- arXiv ID: 2310.01720
- Source URL: https://arxiv.org/abs/2310.01720
- Reference count: 17
- Primary result: 20% improvement over state-of-the-art methods while using less than half the computational resources

## Executive Summary
This paper introduces PrACTiS, a perceiver-based architecture combined with a copula-based decoder for time series forecasting. The approach efficiently transforms high-dimensional multimodal data into a compact latent space using cross-attention between input embeddings and learned latent vectors, significantly reducing computational demands from quadratic to sub-quadratic scaling. By incorporating midpoint inference and local attention mechanisms, the model captures dependencies within imputed samples while maintaining efficiency. An output variance testing mechanism further mitigates error propagation during prediction. Experimental results on both unimodal and multimodal datasets demonstrate superior performance compared to state-of-the-art methods while utilizing less than half the computational resources.

## Method Summary
PrACTiS combines a perceiver-based encoder with a copula-based decoder for time series forecasting. The perceiver encoder uses cross-attention between input embeddings and learned latent vectors to map high-dimensional multimodal data into a compact latent space, reducing computational complexity from O(N²) to O(NK) where K << N. The copula decoder constructs joint distributions of missing data from these latent embeddings. The approach introduces midpoint inference for recursively determining data point depths based on required inferences, and local attention mechanisms that restrict attention to k closest tokens per variable. An output variance testing mechanism samples 10 predictions per imputation, masking those exceeding 4× input variance to prevent error propagation.

## Key Results
- Achieves 20% improvement in prediction performance over state-of-the-art methods
- Utilizes less than half the computational resources compared to baseline approaches
- Demonstrates superior performance on both unimodal (electricity, traffic, fred-md) and multimodal datasets (room occupation, interstate traffic, air quality)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perceiver-based encoder reduces computational complexity from quadratic to sub-quadratic
- Mechanism: Cross-attention between input embeddings and learned latent vectors maps high-dimensional multimodal data into a compact latent space
- Core assumption: Number of latent vectors K << number of data points N
- Evidence anchors:
  - [abstract] "efficiently transforms high-dimensional and multimodal data into a compact latent space, thereby significantly reducing computational demands"
  - [section 4.1] "computational complexity, which scales at O(N K)"
  - [corpus] Weak - no corpus papers directly address perceiver computational complexity
- Break condition: If K approaches N, complexity becomes O(N²)

### Mechanism 2
- Claim: Midpoint inference with local attention captures dependencies efficiently
- Mechanism: Instead of random permutations, recursively infer midpoints within gaps, restricting attention to k closest tokens per variable
- Core assumption: Regular or nearly-regular sampling allows effective midpoint determination
- Evidence anchors:
  - [section 4.2] "determine the depth of each data point based on the number of midpoint inferences required"
  - [section 4.2] "selects a set of conditioning tokens... comprising both past and future windows, consisting of the k closest tokens"
  - [corpus] Weak - no corpus papers directly discuss midpoint inference for time series
- Break condition: If data is highly irregular, midpoint determination fails

### Mechanism 3
- Claim: Output variance testing mitigates error propagation
- Mechanism: For each imputation, sample 10 predictions, compare variance to threshold (4×input variance), mask samples exceeding threshold
- Core assumption: High variance indicates unreliable predictions that could propagate errors
- Evidence anchors:
  - [section 4.3] "perform10 predictions by sampling from the obtained joint distribution... compare the output variance... If the output variance exceeds four times the threshold"
  - [section 4.3] "mask out this predicted data point to prevent it from influencing future imputation processes"
  - [corpus] Weak - no corpus papers directly discuss variance-based error mitigation
- Break condition: If threshold is too low, legitimate predictions get masked

## Foundational Learning

- Concept: Cross-attention vs self-attention
  - Why needed here: Cross-attention between inputs and latent vectors reduces complexity vs self-attention's quadratic scaling
  - Quick check question: What's the computational complexity difference between cross-attention and self-attention?

- Concept: Copula factorization
  - Why needed here: Allows separate modeling of joint distribution and marginals, crucial for handling missing data
  - Quick check question: How does Sklar's theorem enable copula-based missing data modeling?

- Concept: Factor copulas
  - Why needed here: Model high-dimensional dependencies with linear O(n) parameters instead of quadratic
  - Quick check question: Why are factor copulas more suitable for high-dimensional data than full copulas?

## Architecture Onboarding

- Component map: Input Embedding -> Perceiver Encoder -> Copula Decoder -> Output Variance Test
- Critical path: Input → Perceiver Encoder → Copula Decoder → Output Variance Test
- Design tradeoffs:
  - K latent vectors: Smaller K = faster but less expressive
  - Local attention window size: Larger = more context but slower
  - Variance threshold: Higher = fewer false positives but more errors propagate
- Failure signatures:
  - Memory issues: K too large or window size too big
  - Poor predictions: Variance threshold too high or local window too small
  - Slow training: Batch size too large or number of samples per imputation too high
- First 3 experiments:
  1. Vary K (e.g., 64, 128, 256) and measure training speed/memory
  2. Test different local window sizes (e.g., 5, 10, 20) on prediction accuracy
  3. Sweep variance threshold (e.g., 2×, 4×, 8× input variance) and measure error propagation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed midpoint inference mechanism perform on irregularly sampled time series data compared to regularly sampled data?
- Basis in paper: [inferred] The paper states the midpoint inference mechanism is "well-suited for regularly or nearly-regularly sampled time series data" but does not evaluate its performance on irregularly sampled data.
- Why unresolved: The experimental evaluation focuses on regularly sampled datasets (electricity, traffic, fred-md) and multimodal datasets that may have some irregularity but are not explicitly tested for irregular sampling performance.
- What evidence would resolve it: Experimental results comparing midpoint inference performance on irregularly sampled datasets versus regularly sampled datasets, with quantitative metrics showing the degradation or maintenance of performance.

### Open Question 2
- Question: What is the impact of the output variance testing mechanism on overall model performance and prediction accuracy?
- Basis in paper: [explicit] The paper introduces an output variance testing mechanism to mitigate error propagation but does not provide quantitative results showing its impact on final prediction performance metrics.
- Why unresolved: The paper mentions the mechanism but does not compare model performance with and without this feature, nor does it analyze how the threshold choice affects results.
- What evidence would resolve it: Ablation studies showing performance differences (NLL, RMSE-CM, CRPS) with and without the output variance testing mechanism, along with sensitivity analysis of threshold parameters.

### Open Question 3
- Question: How does the computational efficiency of PrACTiS scale with the number of variables in multimodal datasets compared to unimodal datasets?
- Basis in paper: [inferred] The paper demonstrates memory efficiency improvements on multimodal datasets but does not explicitly analyze how computational costs scale with the number of variables across dataset types.
- Why unresolved: While memory usage is compared between unimodal and multimodal datasets, the paper does not provide a detailed scaling analysis of computational complexity as the number of variables increases.
- What evidence would resolve it: Detailed scaling experiments showing memory and computational time as a function of the number of variables for both unimodal and multimodal datasets, with comparison to theoretical complexity bounds.

## Limitations
- The output variance testing mechanism's threshold selection appears arbitrary without sensitivity analysis
- Midpoint inference approach may struggle with highly irregular sampling patterns common in real-world multimodal data
- Claims about computational efficiency improvements rely heavily on perceiver architecture's scaling properties without detailed ablation studies

## Confidence

**High Confidence:** The fundamental claim that perceiver-based architectures reduce computational complexity compared to standard transformers is well-established in the literature and supported by the theoretical analysis provided. The 20% performance improvement claim is supported by experimental results across multiple datasets.

**Medium Confidence:** The integration of copula-based modeling with perceiver architecture is novel but requires more rigorous validation. The effectiveness of local attention windows and midpoint inference depends heavily on data characteristics that aren't fully explored across the tested datasets.

**Low Confidence:** The output variance testing mechanism's threshold selection appears arbitrary without sensitivity analysis. The claim of using "less than half the computational resources" needs clarification on what baseline is being compared against and under what specific conditions.

## Next Checks

1. **Ablation Study on Latent Vector Count (K):** Systematically vary K (e.g., 32, 64, 128, 256) and measure both computational complexity and prediction accuracy across all datasets to establish the optimal tradeoff point.

2. **Variance Threshold Sensitivity Analysis:** Test multiple variance thresholds (2×, 4×, 6×, 8× input variance) and evaluate their impact on error propagation metrics and overall prediction accuracy to validate the chosen 4× threshold.

3. **Irregular Sampling Pattern Stress Test:** Generate synthetic datasets with varying degrees of sampling irregularity and evaluate how midpoint inference and local attention mechanisms perform under increasingly challenging conditions to identify failure modes.