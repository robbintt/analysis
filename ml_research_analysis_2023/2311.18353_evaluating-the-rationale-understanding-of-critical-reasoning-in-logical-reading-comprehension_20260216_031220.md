---
ver: rpa2
title: Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading
  Comprehension
arxiv_id: '2311.18353'
source_url: https://arxiv.org/abs/2311.18353
tags:
- rationale
- question
- answer
- main
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a dataset for evaluating language models' understanding
  of rationales behind critical reasoning in logical reading comprehension. For questions
  taken from an existing dataset, we crowdsource human-written rationales explaining
  why each answer option should be selected or eliminated, resulting in over 3,000
  subquestions associated with 943 main questions.
---

# Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension

## Quick Facts
- arXiv ID: 2311.18353
- Source URL: https://arxiv.org/abs/2311.18353
- Reference count: 40
- Key outcome: Models can answer main questions correctly but struggle to explain rationales behind critical reasoning, particularly for incorrect options.

## Executive Summary
This paper introduces a novel dataset to evaluate language models' understanding of rationales behind critical reasoning in logical reading comprehension. The authors collect human-written rationales explaining why each answer option should be selected or eliminated for questions from the ReClor dataset. They then generate subquestions to test models' understanding of these rationales, resulting in over 3,000 subquestions associated with 943 main questions. Experiments with strong models including large language models show that even if models can answer main questions correctly, they struggle to answer subquestions about the underlying rationales, particularly for incorrect options. This highlights that models lack a comprehensive ability for critical reasoning, as they cannot fully explain why incorrect alternatives should be eliminated.

## Method Summary
The authors first collect human-written rationales for each answer option in ReClor questions, ensuring specificity through an alignment test. They then generate subquestion texts using InstructGPT and validate their human answerability. Finally, they evaluate strong baseline models (including LLMs) on the main questions and subquestions using various settings (fully-finetuned, few-shot, and zero-shot) and calculate accuracy metrics to assess model performance and reasoning capabilities.

## Key Results
- Models perform significantly worse on subquestions (65.7% accuracy) compared to main questions (80.3% accuracy), especially for incorrect options.
- Even with chain-of-thought prompting, models struggle to provide valid eliminative rationales for incorrect options.
- The performance gap is attributed to the complexity of subquestion texts and the difficulty of refuting incorrect options.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models can select correct answers but fail to generate valid eliminative rationales because they lack a structured elimination process.
- **Mechanism:** When answering a multiple-choice question, the model learns a scoring or ranking over options based on superficial features, but does not maintain a separate justification set for why each option is wrong.
- **Core assumption:** The training data and evaluation only require correct answer selection, not explicit justification of incorrect options.
- **Evidence anchors:**
  - [abstract] "we find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions"
  - [section 4.2] "We observe that the highest accuracy is 80.3% on the main questions ... but ... on the subquestions ... 65.7%"
  - [corpus] "neighbor_titles": ["MalAlgoQA: Pedagogical Evaluation of Counterfactual Reasoning in Large Language Models and Implications for AI in Education"] suggests a trend of exploring model reasoning limitations.
- **Break condition:** If the model is explicitly trained or prompted to produce both selective and eliminative rationales for each option, performance on eliminative subquestions should improve.

### Mechanism 2
- **Claim:** The complexity of subquestion texts, especially those involving negation, causes models to fail even when they understand the main rationale.
- **Mechanism:** Models have difficulty parsing negated conditions in questions, leading to misinterpretation of what is being asked.
- **Core assumption:** The model's internal representation of the rationale is correct, but the question format introduces noise that the model cannot resolve.
- **Evidence anchors:**
  - [section 4.2] "Answering the eliminative subquestions is difficult for the models, highlighting the limited capacity of LLMs: Even if the models can choose the correct answer option, they may not understand why incorrect answer options should be refuted."
  - [section 5] "we attribute this discrepancy to two potential reasons. First, the eliminative subquestions are inherently complex because of the negation included in their question text"
  - [corpus] Weak direct evidence; relies on related work on model robustness to negation.
- **Break condition:** If the model is provided with simplified subquestion texts or if the negation is explicitly handled (e.g., through preprocessing), the performance gap should narrow.

### Mechanism 3
- **Claim:** Human-written rationales are more detailed and supportive than model-generated rationales, leading to better subquestion construction and evaluation.
- **Mechanism:** Humans can capture implicit reasoning steps and nuanced context that models miss when generating rationales, resulting in more effective subquestions.
- **Core assumption:** The quality of the rationale directly impacts the difficulty and clarity of the subquestion, affecting model performance.
- **Evidence anchors:**
  - [section 5] "we find that the human-written rationales are likely to be more detailed and supportive than the model-generated ones"
  - [section 3.3] "we use different prompts for the correct and incorrect options to avoid the problem of the model omitting negatives"
  - [corpus] Neighbor titles suggest a broader interest in rationale generation and evaluation.
- **Break condition:** If model-generated rationales are improved through better prompting or fine-tuning, the performance on subquestions using those rationales should improve.

## Foundational Learning

- **Concept:** Logical reasoning and critical thinking
  - **Why needed here:** The dataset is designed to evaluate models' understanding of the rationale behind critical reasoning, which requires distinguishing between correct and incorrect options and understanding why alternatives should be eliminated.
  - **Quick check question:** Can you explain the difference between selecting the correct answer and eliminating incorrect answers in a logical reasoning task?

- **Concept:** Multiple-choice question construction
  - **Why needed here:** The dataset uses a multiple-choice format for both main questions and subquestions, requiring an understanding of how to create and evaluate options that test specific aspects of reasoning.
  - **Quick check question:** How would you design a subquestion to test a model's understanding of why a particular incorrect option should be eliminated?

- **Concept:** Natural language processing and model evaluation
  - **Why needed here:** The study involves evaluating the performance of large language models on a new dataset, requiring knowledge of NLP techniques and evaluation metrics.
  - **Quick check question:** What are some common metrics used to evaluate the performance of language models on reading comprehension tasks?

## Architecture Onboarding

- **Component map:**
  Dataset construction: Crowdsourcing rationales -> Generating subquestions -> Validating human answerability
  Model evaluation: Finetuned, few-shot, and zero-shot settings on ReClor and RULE datasets
  Analysis: Reasoning type annotation -> Rationale alignment task -> Context-ablation analysis

- **Critical path:**
  1. Collect human-written rationales for each answer option in ReClor questions
  2. Validate the specificity of rationales through alignment tests
  3. Generate subquestion texts using a large language model
  4. Validate the answerability of subquestions by humans
  5. Evaluate model performance on both main questions and subquestions
  6. Analyze model behavior and reasoning capabilities

- **Design tradeoffs:**
  - Using human-written rationales vs. model-generated rationales: Human rationales are likely more detailed and supportive but require more resources to collect.
  - Multiple-choice format vs. open-ended questions: Multiple-choice allows for consistent evaluation but may not fully capture the nuances of reasoning.
  - Crowdsourcing vs. expert annotation: Crowdsourcing is more scalable but may introduce variability in rationale quality.

- **Failure signatures:**
  - Low accuracy on eliminative subquestions despite high accuracy on main questions: Indicates a lack of structured elimination process in the model.
  - Poor performance on subquestions with negation: Suggests difficulty in parsing complex question texts.
  - No improvement with chain-of-thought prompting: Implies that the model's reasoning limitations are not addressed by providing rationales.

- **First 3 experiments:**
  1. Evaluate model performance on subquestions generated from model-written rationales to compare with human-written rationales.
  2. Test model performance on simplified subquestion texts (e.g., removing negation) to isolate the impact of question complexity.
  3. Analyze the relationship between the length of questions/options and model accuracy to identify potential confounding factors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the human-written rationales compare to model-generated rationales in terms of specificity and necessity for certifying the validity of the target option?
- Basis in paper: [explicit] The authors state that human-written rationales are likely to be more detailed and supportive than model-generated ones, and that model rationales struggle to capture implicit rationale necessary for certifying validity.
- Why unresolved: While the authors found that human rationales were generally better, they did not conduct a systematic quantitative comparison between human and model-generated rationales across the entire dataset.
- What evidence would resolve it: A comprehensive quantitative analysis comparing human and model-generated rationales on metrics like specificity, necessity, and effectiveness in supporting the correct option.

### Open Question 2
- Question: How does the performance on eliminative subquestions change when the complexity of subquestion texts is reduced or removed?
- Basis in paper: [explicit] The authors find that models perform worse on eliminative subquestions and suggest this could be due to the complexity of subquestion texts or the difficulty of refuting incorrect options.
- Why unresolved: The authors only partially address this by conducting a rationale alignment task without subquestion texts, but do not test the effect of reducing complexity within the actual subquestions.
- What evidence would resolve it: Experiments comparing model performance on eliminative subquestions with varying levels of text complexity or alternative formats that reduce complexity.

### Open Question 3
- Question: Does adding context-ablated settings help models answer subquestions, or does it hinder their performance?
- Basis in paper: [explicit] The authors conduct a context-ablated analysis and find that performance is reduced in the zero-shot setting but not in the five-shot setting, suggesting some dependency on answer options.
- Why unresolved: The authors only test context-ablated settings in a limited way and do not explore the full implications or potential benefits of context removal for model performance.
- What evidence would resolve it: Comprehensive experiments testing context-ablated settings across different model architectures and training paradigms to determine the optimal balance between context and answer options.

## Limitations
- The exact prompt used to generate subquestions with InstructGPT is not provided, making it difficult to reproduce or modify this crucial step.
- The study does not control for potential confounding factors, such as the length of questions/options or the complexity of subquestion texts, which could influence model performance.
- The evaluation is limited to a subset of the ReClor dataset, and it is unclear how well the findings generalize to other logical reasoning tasks or datasets.

## Confidence
- Models struggle with eliminative rationales: High confidence
- Negation in subquestion texts contributes to difficulty: Medium confidence
- Human-written rationales are more detailed and supportive than model-generated ones: Low confidence

## Next Checks
1. **Prompt analysis and ablation:** Conduct an ablation study to isolate the impact of the subquestion generation prompt on model performance. Vary the prompt to include/exclude negation, adjust the level of detail, or provide explicit instructions for eliminative rationales.
2. **Generalization across datasets:** Evaluate the proposed dataset and models on other logical reasoning datasets, such as the RULE dataset mentioned in the paper, to assess the generalizability of the findings.
3. **Controlled experiments on confounding factors:** Design controlled experiments to isolate the impact of potential confounding factors, such as question length, option complexity, or the presence of negation, on model performance. This will help determine the relative contribution of each factor to the observed performance gap.