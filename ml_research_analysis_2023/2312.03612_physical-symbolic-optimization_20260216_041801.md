---
ver: rpa2
title: Physical Symbolic Optimization
arxiv_id: '2312.03612'
source_url: https://arxiv.org/abs/2312.03612
tags:
- symbolic
- dimensional
- physical
- units
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents \u03A6-SO, a framework for symbolic regression\
  \ in physical contexts that leverages dimensional analysis to constrain the search\
  \ space of analytical expressions. By combining a recurrent neural network with\
  \ in situ dimensional analysis rules, \u03A6-SO generates expressions that are both\
  \ accurate and physically meaningful."
---

# Physical Symbolic Optimization

## Quick Facts
- arXiv ID: 2312.03612
- Source URL: https://arxiv.org/abs/2312.03612
- Reference count: 16
- Key outcome: State-of-the-art symbolic regression for physical contexts using dimensional analysis constraints

## Executive Summary
This work presents Φ-SO, a framework for symbolic regression that leverages dimensional analysis to constrain the search space of analytical expressions. By combining a recurrent neural network with in situ dimensional analysis rules, Φ-SO generates expressions that are both accurate and physically meaningful. The method achieves state-of-the-art results on the Feynman benchmark of SRBench, particularly excelling in exact symbolic recovery and resilience to noise when physical units are known.

## Method Summary
Φ-SO builds upon reinforcement learning symbolic regression frameworks by adding an in situ supervised learning component for dimensional analysis. The LSTM-based RNN generates mathematical expressions in prefix notation, with each generation step constrained by local unit constraints that prevent physically inconsistent operations. These constraints are implemented through masking of the RNN's categorical distribution. The framework includes LBFGS optimization for free constants and uses risk-seeking policy gradients for training. The method requires known physical units for all variables and constants, and focuses on generating concise expressions through a length prior that encourages expression termination.

## Key Results
- Achieved state-of-the-art exact symbolic recovery rates on the Feynman benchmark
- Demonstrated superior resilience to noise (0.1%, 1%, 10%) compared to baseline methods
- Generated concise, interpretable expressions with high R² scores (>0.999)

## Why This Works (Mechanism)

### Mechanism 1
Dimensional analysis constraints guide the RNN toward physically meaningful expressions during generation. At each generation step, an in situ algorithm updates local unit constraints for each node in the partial expression tree. These constraints are used to mask the RNN's categorical distribution, preventing the selection of tokens that would violate dimensional analysis rules.

### Mechanism 2
Combining reinforcement learning with supervised learning on dimensional analysis rules improves exploration efficiency. The RNN learns dimensional analysis rules through supervised learning during training, in addition to being guided by reinforcement learning rewards based on expression fit quality.

### Mechanism 3
The length prior helps terminate expression generation while maintaining dimensional consistency. The categorical distribution is masked to favor terminal nodes (variables and constants) before reaching the maximum expression size, encouraging expression termination.

## Foundational Learning

- **Dimensional analysis**: Provides the physical constraints that guide the symbolic expression generation toward meaningful results. Quick check: What are the dimensional analysis rules for addition, multiplication, and exponentiation operations?
- **Reinforcement learning with risk-seeking policy gradients**: Enables exploration of the symbolic expression space while optimizing for expression fit quality. Quick check: How does risk-seeking policy differ from standard policy gradient methods in reinforcement learning?
- **Recurrent neural networks for sequential generation**: Allows the model to generate mathematical expressions token by token in prefix notation. Quick check: Why is prefix notation used instead of infix notation for expression generation in this framework?

## Architecture Onboarding

- **Component map**: LSTM RNN -> In situ dimensional analysis module -> LBFGS optimizer -> Reinforcement learning component
- **Critical path**: Input contextual information to RNN → RNN generates categorical distribution → Dimensional analysis module masks distribution → Token sampled and expression updated → Free constants optimized → Reward computed and RL update applied
- **Design tradeoffs**: Using dimensional analysis constraints limits the search space but requires known physical units; prefix notation simplifies generation but requires conversion for human readability; the length prior prevents infinite generation but may conflict with dimensional constraints
- **Failure signatures**: High discard rate (>90%) in early training epochs indicates RNN hasn't learned dimensional analysis; low exact symbolic recovery rates suggest reinforcement learning isn't finding correct expressions; complex expressions with many free parameters indicate overfitting to noise
- **First 3 experiments**: 1) Run Φ-SO on a simple Feynman problem with known units and no noise to verify basic functionality; 2) Test with varying noise levels (0%, 0.1%, 1%, 10%) to evaluate noise resilience; 3) Compare performance with and without dimensional analysis constraints to measure their impact

## Open Questions the Paper Calls Out
- Can Φ-SO's performance be maintained or improved when applied to datasets without known physical units?
- How does the computational efficiency of Φ-SO scale with increasing expression complexity and dataset size compared to other SR methods?
- Can the in situ dimensional analysis component of Φ-SO be generalized to handle more complex unit systems or derived units?
- How sensitive is Φ-SO's performance to the choice of hyperparameters, and can these be optimized automatically?

## Limitations
- Requires known physical units for all variables and constants, limiting applicability to physical sciences
- Focus on concise expressions may prevent discovery of more complex but potentially more accurate physical laws
- Effectiveness in handling large-scale, high-dimensional problems remains untested

## Confidence

**High Confidence Claims:**
- The dimensional analysis module effectively constrains the search space during expression generation
- Φ-SO achieves state-of-the-art exact symbolic recovery rates on the Feynman benchmark
- The framework shows resilience to noise across multiple noise levels

**Medium Confidence Claims:**
- The combination of supervised learning for dimensional analysis rules and reinforcement learning provides optimal exploration efficiency
- The length prior successfully balances expression termination with dimensional consistency

## Next Checks
1. **Unit Robustness Test**: Evaluate Φ-SO performance when physical units are partially unknown or incorrectly specified for some variables, to quantify the impact of imperfect unit information on discovery accuracy.

2. **Scalability Assessment**: Apply Φ-SO to a synthetic dataset with higher dimensionality (10+ variables) and more complex underlying physical relationships to test scalability limits and computational efficiency.

3. **Generalization to Novel Problems**: Test Φ-SO on physical datasets from domains outside the Feynman benchmark (e.g., fluid dynamics or quantum mechanics) to assess transferability and identify potential domain-specific limitations.