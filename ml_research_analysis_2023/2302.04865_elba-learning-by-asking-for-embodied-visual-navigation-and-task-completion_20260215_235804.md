---
ver: rpa2
title: 'ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion'
arxiv_id: '2302.04865'
source_url: https://arxiv.org/abs/2302.04865
tags:
- elba
- questions
- agent
- question
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Embodied Learning-By-Asking (ELBA), a model
  that learns when and what questions to ask to dynamically acquire additional information
  for completing embodied vision-language tasks. The authors propose a confusion module
  to determine when to ask questions based on entropy or gradient-based measures,
  and introduce a QA generator and evaluator to produce and select relevant question-answer
  pairs.
---

# ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion

## Quick Facts
- arXiv ID: 2302.04865
- Source URL: https://arxiv.org/abs/2302.04865
- Authors: 
- Reference count: 40
- Key outcome: ELBA achieves 15.5% task success rate and 19.4% goal-condition success rate on seen test split of TEACh dataset

## Executive Summary
ELBA introduces an embodied agent that dynamically asks questions to resolve ambiguities in vision-language navigation tasks. The system uses a confusion module to determine when to ask questions based on entropy or gradient-based measures, combined with a QA generator and evaluator to produce relevant question-answer pairs. Experiments on the TEACh dataset show that ELBA outperforms baseline models without question-answering capabilities, demonstrating the effectiveness of learning when and what to ask in embodied environments.

## Method Summary
ELBA is an embodied agent for vision-language navigation that learns when and what questions to ask to acquire additional information for task completion. The model consists of four components: an ACTIONER that predicts actions and includes a confusion module, a PLANNER that generates future sub-goal instructions, a QA GENERATOR that creates candidate question-answer pairs, and a QA EVALUATOR that selects the most suitable pair. The agent uses entropy or gradient-based measures to determine when it is confused and should ask a question, then generates and evaluates questions to clarify ambiguities before selecting the next action.

## Key Results
- ELBA achieves 15.5% task success rate and 19.4% goal-condition success rate on seen test split
- Ablation studies show effectiveness of both oracle and generated question types
- Confusion module robust to threshold settings across different question types
- Agent demonstrates ability to ask relevant questions for navigation and object interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The confusion module enables the agent to dynamically determine when to ask questions based on entropy or gradient-based measures of uncertainty.
- Mechanism: The confusion module measures the entropy of predicted action/object distributions or the gradient magnitude of the model's loss. If the confusion level exceeds a threshold, the agent attempts to ask a question. The agent only asks the question if it results in decreased confusion (lower entropy or gradient magnitude).
- Core assumption: The agent's confusion can be accurately measured by the entropy of its action/object probability distributions or by the gradient magnitude of its loss function.
- Evidence anchors:
  - [abstract] "confusion module to determine when to ask questions based on entropy or gradient-based measures"
  - [section 4.1] "We formalize the agent confusion in two different ways: entropy-based and gradient-based confusion"
  - [corpus] Weak - the corpus mentions related work on uncertainty-aware planning but does not directly support the specific entropy/gradient-based confusion measures used in ELBA
- Break condition: If the confusion thresholds are set too low, the agent will ask too many questions and become inefficient. If set too high, the agent will miss opportunities to ask clarifying questions.

### Mechanism 2
- Claim: The QA generator and evaluator enable the agent to dynamically generate and select relevant question-answer pairs.
- Mechanism: The planner predicts high-level future sub-goal instructions which are used as candidate answers. The QA generator creates a set of candidate question-answer pairs based on these sub-goals. The QA evaluator assigns a score to each QA pair based on its similarity to the current state, and the agent selects the highest-scoring pair.
- Core assumption: Future sub-goal instructions can serve as effective candidate answers for generating relevant questions, and the similarity between state information and QA pairs can be used to rank their suitability.
- Evidence anchors:
  - [abstract] "QA generator and evaluator to produce and select relevant question-answer pairs"
  - [section 4.2-4.4] Detailed description of how the planner, QA generator, and QA evaluator work together to generate and select question-answer pairs
  - [corpus] Weak - the corpus mentions related work on visual question generation but does not directly support the specific approach of using future sub-goals as candidate answers
- Break condition: If the planner's sub-goal predictions are inaccurate, the generated questions may be irrelevant. If the QA evaluator's similarity measure is poor, the agent may select inappropriate questions.

### Mechanism 3
- Claim: Allowing the agent to ask questions in both template-based and free-form formats improves task performance.
- Mechanism: The QA generator can create both oracle (template-based) and model-generated (free-form) question-answer pairs. The agent can choose from either type when deciding what question to ask.
- Core assumption: A combination of template-based and free-form questions will be more effective than either type alone for resolving ambiguities in embodied tasks.
- Evidence anchors:
  - [abstract] "In contrast to prior work, our proposed model can ask questions both in templates and free-form formats"
  - [section 4.3] Description of how the QA generator creates both oracle and model-generated question-answer pairs
  - [section 6.2] Results showing that ELBA with combined QA types outperforms baselines
- Break condition: If the template-based questions are too restrictive or the free-form questions are poorly generated, the combined approach may not improve performance over using only one type.

## Foundational Learning

- Concept: Multimodal Transformers
  - Why needed here: ELBA needs to encode and process multiple modalities (language, vision, actions) simultaneously to understand the environment and make decisions.
  - Quick check question: Can you explain how multimodal transformers differ from single-modality transformers and why they are necessary for embodied vision-language tasks?

- Concept: Question Generation and Evaluation
  - Why needed here: ELBA needs to dynamically generate relevant questions and evaluate their suitability for the current state in order to ask helpful clarifying questions.
  - Quick check question: What are the key challenges in generating and evaluating questions for embodied agents, and how does ELBA address them?

- Concept: Embodied AI and Navigation
  - Why needed here: ELBA operates in embodied environments and needs to navigate and interact with objects to complete tasks, so understanding the fundamentals of embodied AI and navigation is crucial.
  - Quick check question: How do embodied AI tasks like vision-language navigation and task completion differ from traditional AI tasks, and what unique challenges do they present?

## Architecture Onboarding

- Component map:
  ACTIONER -> PLANNER -> QA GENERATOR -> QA EVALUATOR -> ACTIONER

- Critical path:
  1. ACTIONER encodes state and predicts action/object distribution
  2. Confusion module checks if confusion level exceeds threshold
  3. If confused, PLANNER generates sub-goal instructions
  4. QA GENERATOR creates candidate QA pairs from sub-goals
  5. QA EVALUATOR scores and selects the best QA pair
  6. Agent asks the selected question if it decreases confusion
  7. ACTIONER uses augmented state (with QA pair) to select next action

- Design tradeoffs:
  - Template vs. free-form questions: Templates are more structured but less flexible, while free-form questions are more natural but harder to generate.
  - Entropy vs. gradient-based confusion: Entropy is more interpretable but may be less sensitive to certain types of uncertainty, while gradient-based measures can capture more nuanced uncertainty but are harder to compute.
  - Sub-goal granularity: More detailed sub-goals can lead to more specific questions but may be harder to predict accurately.

- Failure signatures:
  - Agent asks too many questions: Confusion thresholds may be too low or QA evaluator may be assigning high scores to irrelevant QA pairs.
  - Agent asks irrelevant questions: PLANNER may be generating poor sub-goals or QA GENERATOR may be creating mismatched question-answer pairs.
  - Agent fails to ask when needed: Confusion thresholds may be too high or ACTIONER may be struggling to encode state information accurately.

- First 3 experiments:
  1. Run ELBA on a simple embodied task and observe the types of questions it asks and whether they help with task completion.
  2. Vary the confusion thresholds and observe how it affects the frequency and relevance of questions asked.
  3. Compare ELBA's performance using only template-based questions, only free-form questions, and a combination of both.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the entropy-based confusion threshold (ϵα, ϵo) affect the performance of ELBA on unseen environments?
- Basis in paper: [explicit] The paper mentions varying the thresholds for action and object distributions in the entropy-based confusion module.
- Why unresolved: The paper only shows performance with a fixed threshold (0.9) and does not explore the impact of different threshold values on unseen environments.
- What evidence would resolve it: An experiment varying the thresholds (e.g., 0.8, 0.9, 1.0) on the unseen test split and reporting task success rates for each threshold value.

### Open Question 2
- Question: How does the gradient-based confusion threshold (ϵ) affect the performance of ELBA on unseen environments?
- Basis in paper: [explicit] The paper mentions varying the gradient norm threshold for the gradient-based confusion method.
- Why unresolved: The paper only shows performance with a fixed threshold (1.2) and does not explore the impact of different threshold values on unseen environments.
- What evidence would resolve it: An experiment varying the thresholds (e.g., 1.0, 1.2, 1.4) on the unseen test split and reporting task success rates for each threshold value.

### Open Question 3
- Question: How does the number of candidate question-answer pairs (K) affect the performance of ELBA?
- Basis in paper: [inferred] The paper mentions generating K candidate question-answer pairs in the QA GENERATOR section but does not explore the impact of different K values.
- Why unresolved: The paper does not provide any experiments or analysis on how the number of candidate question-answer pairs affects the performance of ELBA.
- What evidence would resolve it: An experiment varying the number of candidate question-answer pairs (e.g., K=5, K=10, K=20) and reporting task success rates for each K value.

## Limitations

- Limited evaluation scope on single dataset with only seen environments
- Missing critical implementation details for baseline model and pre-trained weights
- Performance metrics (15.5% task success) indicate significant room for improvement

## Confidence

**High confidence** in the core architectural components (confusion module, QA generator/evaluator) and their theoretical soundness.

**Medium confidence** in the implementation details and training procedures, given the missing specifications for pre-trained weights, hyperparameters, and optimization strategies.

**Medium confidence** in the empirical results, due to the limited evaluation scope (single dataset, seen environments only).

## Next Checks

1. **Baseline reproducibility**: Implement the ACTIONER baseline using publicly available TEACh code and verify that it achieves comparable performance before adding ELBA components.

2. **Generalization testing**: Evaluate ELBA on the unseen test split and a separate validation environment to assess performance beyond the training distribution.

3. **Component ablation**: Systematically remove each ELBA component (confusion module, QA generator, QA evaluator) and measure the impact on task success rate to quantify their individual contributions.