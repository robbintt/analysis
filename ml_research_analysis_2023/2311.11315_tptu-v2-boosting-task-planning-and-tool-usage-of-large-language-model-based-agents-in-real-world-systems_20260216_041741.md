---
ver: rpa2
title: 'TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based
  Agents in Real-world Systems'
arxiv_id: '2311.11315'
source_url: https://arxiv.org/abs/2311.11315
tags:
- apis
- arxiv
- llms
- real-world
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of using LLMs for task planning
  and tool usage in real-world systems with many similar APIs. The authors propose
  a framework with three components: (1) an API Retriever to filter relevant APIs
  using semantic similarity, (2) an LLM Finetuner trained on diverse datasets to improve
  task planning and API calling, and (3) a Demo Selector that retrieves demonstrations
  to help distinguish between similar APIs.'
---

# TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems

## Quick Facts
- **arXiv ID**: 2311.11315
- **Source URL**: https://arxiv.org/abs/2311.11315
- **Reference count**: 40
- **Primary result**: 96.67% execution accuracy on real-world security system using full TPTU-v2 framework

## Executive Summary
TPTU-v2 addresses the challenge of using LLMs for task planning and tool usage in real-world systems with many similar APIs. The framework combines three key components: an API Retriever that filters relevant APIs using semantic similarity, an LLM Finetuner trained on domain-specific datasets, and a Demo Selector that provides demonstrations for in-context learning. Evaluated on both a commercial security system and the ToolBench dataset, the full model achieves state-of-the-art performance with 96.67% execution accuracy in the real-world scenario and 86.7% in the open-source scenario.

## Method Summary
TPTU-v2 is a three-component framework designed to enhance LLM task planning and API calling in systems with numerous similar APIs. The API Retriever uses Sentence-BERT embeddings to compute semantic similarity between user instructions and API descriptions, selecting the most relevant APIs. The LLM Finetuner performs supervised fine-tuning on curated datasets to improve the model's ability to plan tasks and call APIs correctly. The Demo Selector retrieves demonstrations via embedding search when APIs are too similar, providing in-context learning examples to help the LLM distinguish between them. The framework was evaluated on a commercial security system with 45 APIs and the ToolBench dataset with over 16,000 APIs.

## Key Results
- Achieved 96.67% execution accuracy on real-world commercial security system
- Achieved 86.7% execution accuracy on ToolBench academic dataset
- Outperformed base LLMs and variants missing one or more components
- API Retriever achieved high recall@5 and recall@10 metrics for relevant API selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: API Retriever reduces prompt token length by selecting only relevant APIs before sending to LLM.
- Mechanism: Uses Sentence-BERT embeddings to compute semantic similarity between instruction and each API description, then retrieves top-k APIs.
- Core assumption: Semantic similarity in embedding space correlates with task relevance for API selection.
- Evidence anchors:
  - [abstract]: "The API Retriever selects the most pertinent APIs for the user task among the extensive array available"
  - [section]: "The API Retriever navigates through an extensive array of APIs to retrieve the most relevant ones based on the user's task"
- Break condition: If API descriptions are too generic or the instruction contains ambiguous terminology that doesn't match API semantics well.

### Mechanism 2
- Claim: Fine-tuning on domain-specific datasets improves LLM's task planning and API calling abilities beyond base capabilities.
- Mechanism: Supervised fine-tuning (SFT) on curated datasets with diverse prompt structures (single-step, multi-step, varied API orders) to adapt model to real-world API interaction patterns.
- Core assumption: Base LLMs lack sufficient exposure to domain-specific API interaction patterns, which can be remedied through targeted fine-tuning.
- Evidence anchors:
  - [abstract]: "LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling"
  - [section]: "We shift our approach from pioneering new fine-tuning methods to concentrating on the development of a dataset, expressly curated to enhance the fine-tuning process for real-world systems"
- Break condition: If the fine-tuning dataset doesn't adequately represent the complexity and diversity of real-world API interactions.

### Mechanism 3
- Claim: Demo Selector provides context-rich examples that help LLM distinguish between semantically similar APIs through in-context learning.
- Mechanism: Retrieves demonstrations via embedding search when API similarity exceeds threshold, using these as few-shot examples in the prompt.
- Core assumption: In-context learning with relevant demonstrations can effectively teach LLMs to differentiate between similar APIs.
- Evidence anchors:
  - [abstract]: "the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance"
  - [section]: "The Demo Selector dynamically retrieves demonstrations related to hard-to-distinguish APIs, facilitating in-context learning for the LLM"
- Break condition: If the demonstrations are too similar or don't capture the nuanced differences between APIs.

## Foundational Learning

- Concept: Semantic embedding similarity
  - Why needed here: Core to API Retriever's ability to match user instructions with relevant APIs
  - Quick check question: What distance metric does Sentence-BERT typically use for computing similarity between instruction and API embeddings?

- Concept: Supervised fine-tuning (SFT) methodology
  - Why needed here: Fundamental to how LLM Finetuner adapts base model to domain-specific API interactions
  - Quick check question: What are the three key dataset design principles mentioned for effective fine-tuning in this framework?

- Concept: In-context learning mechanics
  - Why needed here: Essential for understanding how Demo Selector's retrieved demonstrations improve API differentiation
  - Quick check question: What happens when no demonstration exceeds the similarity threshold in the Demo Selector's retrieval process?

## Architecture Onboarding

- Component map:
  - User instruction → API Retriever → LLM Finetuner + Demo Selector → API execution → Final answer

- Critical path: User instruction → API Retriever → LLM Finetuner + Demo Selector → API execution → Final answer

- Design tradeoffs:
  - API Retriever recall vs. precision: Higher recall increases API diversity but may introduce noise
  - Fine-tuning dataset size vs. diversity: Larger datasets improve coverage but require more resources
  - Demo Selector threshold tuning: Higher thresholds ensure relevance but may reduce demonstration availability

- Failure signatures:
  - Low API Retriever recall: LLM receives insufficient API options, leading to failed task completion
  - Poor fine-tuning generalization: Model performs well on training data but fails on real-world tasks
  - Ineffective demonstrations: Similar APIs remain indistinguishable despite Demo Selector intervention

- First 3 experiments:
  1. Test API Retriever recall@5 and recall@10 on held-out instructions to validate embedding-based retrieval
  2. Compare base LLM vs. fine-tuned LLM execution accuracy on single-step API tasks
  3. Evaluate Demo Selector's impact by measuring API differentiation accuracy with and without demonstrations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the API Retriever vary with the number of APIs in the real-world system?
- Basis in paper: [inferred] The paper mentions that real-world systems have a vast array of APIs, making it challenging for LLMs to process all API descriptions.
- Why unresolved: The paper does not provide experimental results showing how the API Retriever's performance changes as the number of APIs increases.
- What evidence would resolve it: Experimental results demonstrating the API Retriever's recall and precision metrics across different numbers of APIs in real-world systems.

### Open Question 2
- Question: What is the impact of fine-tuning on the LLM's ability to generalize to unseen APIs and tasks?
- Basis in paper: [explicit] The paper mentions that fine-tuning the LLM on a diverse dataset improves its task planning and API-calling abilities.
- Why unresolved: The paper does not provide a detailed analysis of how well the fine-tuned LLM generalizes to APIs and tasks that were not part of the fine-tuning dataset.
- What evidence would resolve it: Experimental results comparing the fine-tuned LLM's performance on seen versus unseen APIs and tasks.

### Open Question 3
- Question: How does the Demo Selector handle APIs with highly similar functionalities, and what is the threshold for determining when a demo is relevant?
- Basis in paper: [explicit] The paper introduces the Demo Selector to help LLMs distinguish between similar APIs by retrieving demonstrations for in-context learning.
- Why unresolved: The paper does not provide details on how the Demo Selector determines the relevance of demos or how it handles cases where APIs have very similar functionalities.
- What evidence would resolve it: A detailed explanation of the Demo Selector's similarity threshold and examples of its performance on APIs with highly similar functionalities.

## Limitations
- Limited external validation of core mechanisms across diverse domains
- Unclear marginal contribution of individual components versus their combined effect
- Critical implementation details underspecified, hindering faithful reproduction

## Confidence
- **High Confidence**: Overall architecture design and problem statement are well-defined; performance improvements clearly demonstrated
- **Medium Confidence**: Core mechanisms are theoretically sound but empirical validation of specific implementations is limited
- **Low Confidence**: Framework generalizability to other domains and optimal hyperparameter configurations lack sufficient empirical support

## Next Checks
1. Conduct systematic ablation study isolating each component's contribution by evaluating execution accuracy with different component combinations across multiple datasets
2. Test framework on at least two additional domains with different API characteristics to assess generalization beyond the security domain and ToolBench dataset
3. Measure API Retriever's precision and recall on a held-out test set with diverse instructions, analyzing failure cases where relevant APIs are missed