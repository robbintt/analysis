---
ver: rpa2
title: On Continuity of Robust and Accurate Classifiers
arxiv_id: '2309.17048'
source_url: https://arxiv.org/abs/2309.17048
tags:
- hypothesis
- learning
- adversarial
- holomorphic
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the relationship between continuity and robustness
  in machine learning classifiers. The authors propose that continuous hypotheses
  cannot effectively learn optimal robust classifiers, and introduce a framework for
  studying harmonic and holomorphic hypotheses in learning theory.
---

# On Continuity of Robust and Accurate Classifiers

## Quick Facts
- **arXiv ID**: 2309.17048
- **Source URL**: https://arxiv.org/abs/2309.17048
- **Reference count**: 40
- **Primary result**: Continuous hypotheses cannot effectively learn optimal robust classifiers due to fundamental incompatibility between continuity and robustness

## Executive Summary
This paper challenges the prevailing view that robustness in classifiers can be achieved through continuity constraints like Lipschitz continuity. Instead, it argues that continuity itself is incompatible with achieving both robustness and accuracy simultaneously. The authors introduce weakly-harmonic and holomorphic hypothesis spaces as alternative frameworks, showing that continuous hypotheses cannot uniformly converge to discontinuous target functions, leading to poor generalization outside training domains. Empirical evidence on MNIST and FashionMNIST demonstrates that discontinuous hypotheses (holomorphic classifiers) outperform continuous ones (weakly-harmonic classifiers) in detecting adversarial examples while maintaining accuracy.

## Method Summary
The authors propose a convolutional architecture with fixed cosine templates to construct weakly-harmonic and holomorphic classifiers. Weakly-harmonic classifiers minimize Dirichlet energy through gradient norm regularization, while holomorphic classifiers use complex-valued outputs. Adversarial examples are generated using PGD attacks with ℓ∞ radius 0.3 and 40 steps. Three training sets are created: adversarial examples (Sadv), benign examples (Snat), and combined (S). The classifiers are trained separately on each set and evaluated on their ability to detect adversarial examples using analytic polyhedra Π(h). Continuity bias is measured through statistical hypothesis testing comparing classification and regression tasks.

## Key Results
- Discontinuous holomorphic classifiers outperform continuous weakly-harmonic classifiers in adversarial example detection
- Weakly-harmonic classifiers achieve higher precision but lower recall in identifying adversarial examples
- Statistical tests confirm significant continuity bias in classification tasks compared to regression
- The proposed architecture effectively constructs analytic polyhedra that align with adversarial regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous classifiers cannot learn optimal robust hypotheses because discontinuities in the target function prevent uniform convergence of continuous hypotheses.
- **Mechanism**: If the optimal robust classifier is discontinuous, a sequence of continuous hypotheses trained on finite samples will converge pointwise but not uniformly, failing to generalize outside the training domain and creating adversarial vulnerabilities.
- **Core assumption**: The optimal robust classifier for image classification is discontinuous.
- **Evidence anchors**: [abstract] states continuity is incompatible with robustness; [section] explains pointwise vs uniform convergence distinction.
- **Break condition**: If the optimal robust classifier is actually continuous, or if learning algorithms can effectively smooth discontinuities without losing robustness.

### Mechanism 2
- **Claim**: Regularizing gradient norm forces harmonic functions with analytic polyhedra boundaries that align with adversarial regions.
- **Mechanism**: Gradient norm regularization minimizes Dirichlet energy, leading to harmonic functions whose analytic polyhedra (|h_j(z)| < 1) correspond to adversarial regions in the domain.
- **Core assumption**: Regularization effectively minimizes Dirichlet energy and harmonic functions' boundaries capture adversarial examples.
- **Evidence anchors**: [abstract] introduces weakly-harmonic learning rules; [section] connects gradient regularization to Dirichlet energy.
- **Break condition**: If regularization doesn't minimize Dirichlet energy, or if harmonic polyhedra don't align with adversarial regions.

### Mechanism 3
- **Claim**: Training separate continuous classifiers on adversarial and benign examples outperforms single classifier on combined data.
- **Mechanism**: Continuity bias prevents single continuous classifier from representing discontinuities in decision boundary, while separate classifiers can capture different regions better.
- **Core assumption**: Optimal classifier has different continuous hypotheses for different domain regions.
- **Evidence anchors**: [abstract] suggests training different hypotheses for different regions; [section] describes experimental setup with Sadv, Snat, S.
- **Break condition**: If single continuous classifier can effectively learn discontinuities, or if separate training doesn't improve performance.

## Foundational Learning

- **Concept**: PAC (Probably Approximately Correct) Learning
  - **Why needed here**: Defines when a hypothesis is a good approximation of the target function, crucial for theoretical guarantees.
  - **Quick check question**: What are the two parameters (ϵ, δ) in the PAC definition, and what do they represent?

- **Concept**: Modes of Convergence (Pointwise vs Uniform)
  - **Why needed here**: Distinguishes between convergence types, key to understanding why continuous hypotheses cannot learn discontinuous targets effectively.
  - **Quick check question**: What is the difference between pointwise and uniform convergence, and why does this distinction matter for continuity bias?

- **Concept**: Dirichlet Energy and Harmonic Functions
  - **Why needed here**: Used as regularization term leading to harmonic functions; understanding these is crucial for weakly-harmonic hypothesis space.
  - **Quick check question**: What is Dirichlet energy, and how is it related to harmonic functions? How does minimizing Dirichlet energy affect learned hypothesis properties?

## Architecture Onboarding

- **Component map**: Fixed cosine templates (α with ∥α∥∞ ≤ 1) → Dilated convolution (up to 5 pixels) → Weakly-harmonic features (Σ^(-1/2) transformation) → Extra class (constant zero) → Periodic projection (adversarial examples)
- **Critical path**: Construct adversarial examples → Train classifiers on Sadv, Snat, S → Measure continuity bias → Analyze results
- **Design tradeoffs**: Fixed templates simplify implementation but limit expressiveness; larger dilation captures more correlations but increases computational cost; extra class helps force natural samples outside polyhedra but adds complexity
- **Failure signatures**: Low precision in adversarial detection (Π(h) misalignment); similar classifier performance across datasets (insignificant continuity bias); high continuity bias in regression tasks (discontinuity not classification-specific)
- **First 3 experiments**: 1) Generate adversarial examples using PGD with periodic projection; 2) Train holomorphic classifiers on Sadv, Snat, S and compare performance; 3) Measure continuity bias using weakly-harmonic space with statistical hypothesis testing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is there a theoretical proof that continuous hypotheses cannot effectively learn optimal robust classifiers in practice, beyond the empirical evidence provided?
- **Basis in paper**: Explicit
- **Why unresolved**: Paper presents empirical evidence but lacks rigorous theoretical proof of inherent limitations.
- **What evidence would resolve it**: Formal mathematical proof demonstrating continuous hypotheses' limitations, or counterexample showing continuous hypothesis achieving both robustness and accuracy.

### Open Question 2
- **Question**: Can the proposed convolutional architecture be extended to handle more complex data distributions and higher-dimensional feature spaces?
- **Basis in paper**: Explicit
- **Why unresolved**: Effectiveness and scalability to complex data distributions and higher dimensions not fully explored.
- **What evidence would resolve it**: Empirical evaluations on diverse, complex datasets and theoretical analysis of generalization capabilities.

### Open Question 3
- **Question**: How does continuity bias relate to generalization error on unseen data, and can it predict robustness?
- **Basis in paper**: Explicit
- **Why unresolved**: Relationship between continuity bias, generalization error, and predictive power for robustness not fully established.
- **What evidence would resolve it**: Empirical studies correlating continuity bias with generalization error, and theoretical analysis of relationship with robustness metrics.

## Limitations

- Theoretical framework relies on assumption that optimal robust classifier is discontinuous for image classification tasks
- Empirical validation limited to MNIST and FashionMNIST datasets, not tested on more complex vision tasks
- Connection between Dirichlet energy regularization and adversarial robustness requires more extensive empirical validation
- Statistical test for continuity bias depends on specific assumptions about hypothesis space that may not hold in practice

## Confidence

The paper presents a rigorous theoretical framework but empirical validation is limited. **Medium** confidence in main claims:
- Theoretical argument about continuity/robustness incompatibility: **High**
- Empirical evidence on MNIST/FashionMNIST: **Medium**
- Scalability to complex vision tasks: **Low**
- Practical effectiveness against state-of-the-art defenses: **Low**

## Next Checks

1. Test weakly-harmonic and holomorphic classifiers on CIFAR-10/100 with stronger adversarial attacks (e.g., AutoAttack) to verify scalability and robustness claims
2. Conduct ablation studies removing Dirichlet energy regularization to isolate its effect on adversarial robustness
3. Compare proposed approach against state-of-the-art certified defenses to establish practical effectiveness