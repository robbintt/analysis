---
ver: rpa2
title: Knowledge Guided Semi-Supervised Learning for Quality Assessment of User Generated
  Videos
arxiv_id: '2312.15425'
source_url: https://arxiv.org/abs/2312.15425
tags:
- quality
- video
- videos
- ssl-vqa
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of perceptual quality assessment
  for user-generated content (UGC) videos, which requires large-scale human-annotated
  data for training. The authors propose a semi-supervised learning framework (SSL-VQA)
  that leverages limited human-annotated data and a large amount of unlabeled data.
---

# Knowledge Guided Semi-Supervised Learning for Quality Assessment of User Generated Videos

## Quick Facts
- **arXiv ID**: 2312.15425
- **Source URL**: https://arxiv.org/abs/2312.15425
- **Reference count**: 40
- **Key outcome**: Proposed SSL-VQA framework achieves 10% improvement with limited data and 15% with additional unlabeled data compared to existing methods.

## Executive Summary
This paper tackles the challenge of perceptual quality assessment for user-generated content (UGC) videos, which traditionally requires large-scale human-annotated data. The authors propose a semi-supervised learning framework (SSL-VQA) that leverages limited human-annotated data alongside a large amount of unlabeled data. The approach involves two stages: self-supervised learning of spatio-temporal video quality representation (ST-VQRL) using contrastive learning, and semi-supervised learning with knowledge transfer between two models—one based on human annotations and another based on distances to a corpus of pristine videos. The method achieves state-of-the-art performance, improving by around 10% with limited data and 15% with additional unlabeled data compared to existing methods.

## Method Summary
The proposed method employs a two-stage approach for video quality assessment. First, ST-VQRL uses a Video Swin-T backbone with gated relative positional bias to learn quality-aware features through statistical contrastive loss. Second, SSL-VQA implements dual-model learning with regressor-based and distance-based quality models, intra-model consistency loss, and knowledge transfer loss. The framework is trained on the LSVQ dataset using limited labeled (500) and unlabeled (1500) videos out of 2000 total, and evaluated on LSVQ test and cross-database datasets including KoNVid-1K, LIVE VQC, LIVE Qualcomm, and YouTube-UGC.

## Key Results
- SSL-VQRL achieves SROCC of 0.823 on LSVQ dataset, outperforming other SOTA methods
- SSL-VQA improves performance by 10% with limited data and 15% with additional unlabeled data
- Cross-database testing shows consistent improvements over state-of-the-art methods across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
Contrastive self-supervised pretraining with statistical contrastive loss yields quality-aware features even without human labels. The method uses quality consistent sampling to generate augmented clips from the same video, then pulls them together in embedding space using a statistical distance (Mahalanobis-like) between their feature distributions, while pushing apart clips from different videos. The core assumption is that statistical distances between multivariate Gaussian fits of feature embeddings are perceptually relevant to video quality.

### Mechanism 2
Dual-model semi-supervised learning transfers reliable knowledge from a stable model to an unstable one. Two models predict quality—one from features, one from distance to pristine corpus. For unlabeled videos, the model with smaller intra-model consistency error is deemed stable and its predictions are used as pseudo-labels for the other model. The core assumption is that consistency between augmented views is a valid proxy for model reliability in the absence of labels.

### Mechanism 3
Quality consistent sampling (QCS) preserves local quality information while enabling efficient fragment-based training. Each video is divided into a grid; patches are sampled from corresponding grid cells across distorted versions to ensure they share local quality characteristics. The core assumption is that patches sampled from the same spatial location across distortions of the same content have similar local quality.

## Foundational Learning

- **Multivariate Gaussian modeling of feature distributions**: Used to compute statistical distances in contrastive loss and distance-based quality model. Quick check: How do you compute the Mahalanobis-like distance between two sets of features represented as Gaussians?
- **Semi-supervised learning via consistency regularization**: Leverages unlabeled data by enforcing consistent predictions across augmented views. Quick check: What is the difference between pseudo-labeling and consistency regularization in semi-supervised learning?
- **Video Swin Transformer architecture**: Backbone for spatiotemporal feature extraction; efficient due to local self-attention and shifted windows. Quick check: How does the shifted window mechanism in Swin Transformer reduce computational complexity compared to full self-attention?

## Architecture Onboarding

- **Component map**: ST-VQRL (Video Swin-T + statistical contrastive loss) → quality-aware feature backbone → Direct regressor head → scalar quality from features; Distance model (Gaussian fit on pristine corpus + Mahalanobis distance) → scalar quality from feature distance; Quality consistent sampling (QCS) → augmentation module; Knowledge transfer module → selects stable model and transfers pseudo-labels
- **Critical path**: ST-VQRL pretraining → dual-model SSL training (supervised + consistency + knowledge transfer) → inference (average of two model outputs)
- **Design tradeoffs**: Using QCS reduces resolution but preserves quality locality; trade-off between efficiency and global context. Statistical distance is more robust to local variations than point-wise similarity but computationally heavier. Dual-model setup increases complexity but provides complementary quality signals.
- **Failure signatures**: Low correlation on labeled validation set → either ST-VQRL pretrained features are poor or regression head is underfitting. Large gap between model consistency errors → one model is unstable; check QCS or distance model assumptions. No improvement with unlabeled data → knowledge transfer logic may be flawed or both models are equally unreliable.
- **First 3 experiments**: 1) Train ST-VQRL with only cosine similarity loss (no statistics) and evaluate on a small labeled set to confirm benefit of statistical loss. 2) Run dual-model SSL with fixed pseudo-labels (no consistency check) to see if knowledge transfer alone helps. 3) Evaluate single-model SSL (no distance model) to isolate contribution of dual-model design.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SSL-VQA scale with increasing amounts of unlabeled data beyond 1500 videos, and at what point does the benefit plateau? The paper mentions testing with varying amounts of unlabeled videos (500-2500) and observing that performance nearly saturates when the number exceeds 1500, but doesn't explore further. Experiments testing SSL-VQA performance with even larger amounts of unlabeled data (e.g., 5000-10000 videos) would determine if the performance continues to improve or plateaus.

### Open Question 2
How would the proposed SSL-VQA framework perform when applied to other perceptual quality assessment tasks, such as image quality assessment or audio quality assessment? The paper focuses specifically on video quality assessment, but the underlying semi-supervised learning approach and quality-aware feature extraction could potentially be adapted to other modalities. Implementing and evaluating SSL-VQA or a similar framework for image quality assessment and audio quality assessment tasks would compare performance to state-of-the-art methods in those domains.

### Open Question 3
How does the proposed knowledge transfer mechanism in SSL-VQA compare to other semi-supervised learning techniques, such as consistency regularization or pseudo-labeling, when applied to the VQA task? The paper mentions that existing semi-supervised methods like Mean Teacher, FixMatch, and Meta Pseudo-Label are not directly applicable to VQA due to quality variant augmentations, and proposes a knowledge transfer mechanism as an alternative. Implementing and comparing SSL-VQA with other semi-supervised learning techniques adapted for the VQA task would evaluate their relative performance.

## Limitations
- The quality consistent sampling assumption may not hold across all types of distortions commonly found in user-generated content
- The statistical distance between feature distributions as a quality proxy lacks extensive validation in the literature
- The dual-model knowledge transfer mechanism lacks empirical validation of whether consistency-based reliability estimation is truly effective for regression tasks

## Confidence
- **High Confidence**: The overall two-stage framework (ST-VQRL pretraining + dual-model SSL) and its ability to leverage unlabeled data for performance gains
- **Medium Confidence**: The statistical contrastive loss formulation and its impact on feature quality awareness
- **Medium Confidence**: The quality consistent sampling procedure and its effectiveness in preserving local quality information
- **Low Confidence**: The consistency-based reliability estimation for knowledge transfer between models in regression tasks

## Next Checks
1. **Statistical Distance Validation**: Conduct a controlled experiment to verify whether the Mahalanobis-like distance between feature distributions correlates with human-perceived quality differences on a small labeled validation set.
2. **QCS Robustness Analysis**: Test the quality consistent sampling assumption by applying different distortion types to video patches and measuring whether patches from the same grid cell maintain similar quality rankings.
3. **Knowledge Transfer Ablation**: Compare the proposed consistency-based knowledge transfer against simpler baselines (random selection, fixed pseudo-labels) to isolate the contribution of the reliability estimation mechanism.