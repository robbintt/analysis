---
ver: rpa2
title: Exploring Geometry of Blind Spots in Vision Models
arxiv_id: '2310.19889'
source_url: https://arxiv.org/abs/2310.19889
tags:
- image
- images
- confidence
- source
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates under-sensitivity in vision models by developing
  a Level Set Traversal (LST) algorithm that iteratively explores high-confidence
  regions using orthogonal components of local gradients. Given a source and target
  image, LST finds inputs within the source's confidence level set that are perceptually
  similar to the target, revealing star-like substructures in level sets.
---

# Exploring Geometry of Blind Spots in Vision Models

## Quick Facts
- arXiv ID: 2310.19889
- Source URL: https://arxiv.org/abs/2310.19889
- Reference count: 40
- Primary result: LST algorithm reveals extensive under-sensitivity in vision models, with robust models showing even greater susceptibility

## Executive Summary
This paper introduces a Level Set Traversal (LST) algorithm that systematically explores "blind spots" in vision models where large perturbations don't change predictions. The algorithm finds inputs within a source image's high-confidence region that are perceptually similar to arbitrary target images. Experiments reveal that both standard and adversarially robust models exhibit extensive under-sensitivity, with robust models showing superlevel sets extending beyond their threat model. The work quantifies this phenomenon using visual similarity metrics and confidence measures, demonstrating that LST outputs maintain high source-class confidence while being visually similar to targets.

## Method Summary
The Level Set Traversal (LST) algorithm iteratively explores high-confidence regions by projecting the difference between current iterate and target image onto the tangent space orthogonal to the local gradient. This preserves the model's confidence for the source class while moving perceptually toward the target. The process continues until confidence drops below a threshold, yielding an output that is both visually similar to the target and maintains high confidence for the source class. The method is evaluated on ImageNet and CIFAR-10 using both standard CNNs and ViTs, with additional experiments on adversarially robust models.

## Key Results
- LST successfully finds inputs within source confidence level sets that are perceptually similar to arbitrary target images
- Star-like substructures exist in level sets, with linear interpolants maintaining high confidence throughout
- Adversarially robust models exhibit even greater under-sensitivity, with superlevel sets extending beyond their threat model
- Quantitative metrics confirm visual and confidence similarities between LST outputs and targets, with triangular hull confidences reaching 0.96 for robust ResNet-50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LST exploits orthogonal nature of confidence-preserving perturbations to explore high-confidence regions
- Mechanism: Iterative projection of difference vector onto tangent space orthogonal to local gradient preserves source-class confidence while moving toward target perceptually
- Core assumption: Level set of softmax output is a (d-1) dimensional submanifold where orthogonal perturbations preserve confidence
- Break condition: If decision boundary is highly non-smooth or discontinuous, orthogonal projection may lead to confidence loss or class changes

### Mechanism 2
- Claim: Final LST output is linearly connected to source image with high confidence, forming star-like substructure
- Mechanism: Linear path from source to LST output maintains high confidence due to discretized approximation smoothing non-linear flow path
- Core assumption: Level set submanifold has sufficient overlap in orthogonal hyperplanes across tessellated regions allowing linear connectivity
- Break condition: If level set has complex curvature or disconnected components, linear path may exit high-confidence region

### Mechanism 3
- Claim: Adversarially robust models exhibit greater under-sensitivity with superlevel sets extending beyond threat model
- Mechanism: Robust training with ℓp bounded adversaries smooths model, inducing excessive invariance to larger perturbations
- Core assumption: Adversarial training creates models invariant to perturbations beyond original threat model
- Break condition: If robust model's smoothing is insufficient or threat model alignment is better, superlevel sets may not extend as far

## Foundational Learning

- Concept: Gradient-based optimization and projection onto orthogonal complement
  - Why needed here: LST relies on computing gradients of cross-entropy loss and projecting perturbations onto space orthogonal to gradient to preserve confidence
  - Quick check question: Given a gradient vector g and difference vector ∆x, how do you compute the component of ∆x orthogonal to g?

- Concept: Level sets and submanifolds in high-dimensional spaces
  - Why needed here: Understanding that equi-confidence region is a (d-1) dimensional submanifold helps explain why orthogonal perturbations preserve confidence
  - Quick check question: If a function f: R^d → R has a regular level set at value c, what is the dimension of this level set?

- Concept: Convexity and star-shaped sets
  - Why needed here: Observation that linear interpolants between source and LST outputs maintain high confidence implies superlevel set has star-like structure
  - Quick check question: What is the difference between a convex set and a star-shaped set with respect to a point?

## Architecture Onboarding

- Component map: Input pipeline (source image, target image, model) -> LST iterative loop (gradient computation, projection, confidence check) -> Output (LST output image, confidence metrics, visualization tools)

- Critical path:
  1. Load source and target images
  2. Initialize current iterate as source image
  3. For each iteration:
     a. Compute gradient of cross-entropy loss w.r.t. current iterate
     b. Project difference vector onto orthogonal complement of gradient
     c. Add small parallel perturbation to maintain confidence
     d. Update current iterate
     e. Check if confidence drop exceeds threshold; if so, return current iterate
  4. Return final LST output

- Design tradeoffs:
  - Number of iterations vs. computational cost: More iterations yield better visual similarity but increase runtime
  - Step size η vs. convergence: Larger η speeds up traversal but may overshoot or cause confidence drops
  - Confidence threshold δ vs. exploration: Higher δ allows more exploration but risks confidence loss

- Failure signatures:
  - Confidence drops below threshold: Indicates orthogonal projection led to region where model's decision changes
  - Visual dissimilarity to target: Suggests insufficient iterations or inappropriate step sizes
  - Non-linear connectivity: Implies level set has complex geometry not captured by linear interpolation

- First 3 experiments:
  1. Run LST with default parameters on ResNet-18 on CIFAR-10 with source-target pairs; verify outputs are visually similar to targets and maintain high source-class confidence
  2. Vary number of iterations m and observe trade-off between visual similarity and computational cost; identify minimum m for satisfactory results
  3. Test LST on adversarially robust model and compare extent of under-sensitivity (triangular confidence metrics) with normally trained model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the phenomenon of under-sensitivity in vision models be effectively mitigated given its extensive scale and the star-like substructure of level sets?
- Basis in paper: [explicit] The paper discusses the challenge of mitigating under-sensitivity, highlighting the extensive scale and star-like substructure of level sets as factors contributing to the difficulty
- Why unresolved: The paper suggests that simple approaches like contrastive training may be ineffective due to the combinatorial complexity of addressing protuberances towards every other class
- What evidence would resolve it: Evidence would include successful development and demonstration of a mitigation technique that effectively reduces under-sensitivity across a wide range of vision models and datasets

### Open Question 2
- Question: What are the implications of under-sensitivity on the generalization and reliability of vision models in real-world applications?
- Basis in paper: [inferred] The paper highlights the extensive scale of under-sensitivity and its potential impact on model coherence towards human expectations, implying potential issues in real-world scenarios
- Why unresolved: The paper focuses on theoretical and experimental analysis of under-sensitivity but does not delve into practical implications or real-world case studies
- What evidence would resolve it: Evidence would include empirical studies or case studies demonstrating the impact of under-sensitivity on the performance and reliability of vision models in real-world applications

### Open Question 3
- Question: How does the presence of under-sensitivity in adversarially robust models affect their overall robustness and security?
- Basis in paper: [explicit] The paper discusses the exacerbation of under-sensitivity in adversarially robust models and its presence in regions beyond the original threat model
- Why unresolved: The paper identifies the phenomenon but does not explore its implications for the robustness and security of these models
- What evidence would resolve it: Evidence would include analysis of the trade-offs between robustness to adversarial attacks and susceptibility to under-sensitivity, potentially through empirical studies on the security implications of under-sensitivity in robust models

## Limitations

- Hyperparameter settings for LST (η, ϵ, δ, m) are not fully specified, affecting reproducibility
- Exact implementation details of confidence maintenance steps and exponential moving averages are not detailed
- Star-like substructure claims rely on empirical observations without formal proofs for general cases

## Confidence

- **High Confidence:** LST can find perceptually similar inputs while maintaining high source-class confidence
- **Medium Confidence:** Mechanism of orthogonal gradient projections preserving confidence
- **Medium Confidence:** Comparison of under-sensitivity between standard and robust models

## Next Checks

1. **Reproduce with default parameters:** Implement LST with reasonable default hyperparameters on ResNet-18 using CIFAR-10 to verify basic functionality and compare with paper results
2. **Hyperparameter sensitivity analysis:** Systematically vary η, ϵ, and m to understand their impact on visual similarity, confidence maintenance, and computational cost
3. **Robust model comparison:** Train standard and robust ResNet-50 on ImageNet, apply LST to both, and quantitatively compare their triangular confidence metrics and superlevel set extents as claimed in the paper