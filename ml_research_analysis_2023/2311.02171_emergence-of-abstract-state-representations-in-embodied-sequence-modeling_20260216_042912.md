---
ver: rpa2
title: Emergence of Abstract State Representations in Embodied Sequence Modeling
arxiv_id: '2311.02171'
source_url: https://arxiv.org/abs/2311.02171
tags:
- uni00000013
- state
- uni00000011
- sequence
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether embodied sequence modeling leads
  to the emergence of abstract state representations. Using the BabyAI environment,
  a sequence modeling Transformer is trained to predict actions based on language
  instructions, past actions, and environmental observations.
---

# Emergence of Abstract State Representations in Embodied Sequence Modeling

## Quick Facts
- **arXiv ID**: 2311.02171
- **Source URL**: https://arxiv.org/abs/2311.02171
- **Reference count**: 15
- **Key outcome**: Embodied sequence modeling with Transformers leads to emergence of abstract state representations, as demonstrated by reconstruction accuracy of intermediate environmental layouts from model activations.

## Executive Summary
This paper investigates whether embodied sequence modeling leads to the emergence of abstract state representations through a novel "blindfolded" navigation task in the BabyAI environment. A Transformer-based sequence model is trained to predict actions based on language instructions, past actions, and environmental observations, with intermediate state information systematically removed during training. Probing experiments reveal that the model learns internal representations of intermediate environmental layouts that can be reasonably reconstructed from its activations. Language instructions are shown to play a crucial role in enhancing the accuracy of these reconstructions, suggesting that key features of state representations can emerge through embodied sequence modeling.

## Method Summary
The method involves training a Transformer-based sequence model on next-action prediction tasks in the BabyAI environment, where the model processes sequences of language instructions, actions, and (optionally) environmental states. A "blindfolded" setup removes intermediate state observations during training, forcing the model to internally reconstruct environmental states. Probing experiments use 2-layer MLPs trained to reconstruct intermediate states from model activations at various layers. The approach is evaluated on GoToLocal and MiniBossLevel tasks, with systematic ablations of input components to understand their impact on state representation emergence.

## Key Results
- Intermediate environmental layouts can be reasonably reconstructed from the internal activations of a trained sequence model
- Language instructions play a significant role in improving reconstruction accuracy of intermediate states
- State representations emerge even in the "blindfolded" setup where intermediate states are not provided during training
- The model achieves near-perfect action prediction accuracy on held-out test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embodied sequence modeling leads to emergence of abstract state representations through implicit world modeling.
- Mechanism: The model learns to predict next actions based on past actions and language instructions, which requires it to internally reconstruct intermediate environmental states to make accurate predictions.
- Core assumption: The model needs to maintain and update a mental representation of the environment to complete navigation tasks successfully.
- Evidence anchors:
  - [abstract] "Our probing results show that intermediate environmental layouts can be reasonably reconstructed from the internal activations of a trained model"
  - [section] "The probing results demonstrate that a trained model learns internal representations of the intermediate layouts"
  - [corpus] Weak evidence - corpus neighbors focus on communication emergence and robotics but not specifically on abstract state representation emergence in sequence models
- Break condition: If the model could rely solely on surface statistics of action sequences without needing to understand the underlying environmental state

### Mechanism 2
- Claim: Language instructions play a crucial role in enhancing the emergence of abstract state representations.
- Mechanism: Language instructions provide semantic grounding and object attribute information that helps the model form more complete and accurate internal representations of the environment.
- Core assumption: Language contains information about object types, colors, and affordances that cannot be inferred from actions alone
- Evidence anchors:
  - [abstract] "language instructions play a role in the reconstruction accuracy"
  - [section] "language instructions are essential for the emergence of internal state representations"
  - [corpus] Weak evidence - corpus neighbors discuss language in robotics but not specifically how instructions enhance state representation emergence
- Break condition: If the model could achieve similar performance without language instructions or if instructions provided no additional semantic information

### Mechanism 3
- Claim: The "blindfolded" setup demonstrates that models can learn to internally reconstruct states even when explicit state information is absent during training.
- Mechanism: By removing intermediate state observations from the input sequence, the model must develop internal mechanisms to track environmental changes based on actions taken and language context.
- Core assumption: The model must maintain some form of internal state representation to predict next actions accurately when current state is not explicitly provided
- Evidence anchors:
  - [abstract] "We design a 'blindfolded' navigation task, where only the initial environmental layout, the language instruction, and the action sequence to complete the task are available for training"
  - [section] "This agent can follow the instructions by memorizing the action sequences it has seen during pretraining, or by constructing and updating abstract environmental representations"
  - [corpus] Weak evidence - corpus neighbors discuss embodied control but not specifically state reconstruction in blindfolded scenarios
- Break condition: If the model could achieve the same performance by simply memorizing action sequences without internal state representation

## Foundational Learning

- Concept: Sequence modeling with Transformer architectures
  - Why needed here: The model needs to process heterogeneous data (language, actions, states) as sequences and predict next actions based on context
  - Quick check question: How does causal attention in Transformers enable autoregressive action prediction?

- Concept: Probing as a method to understand internal representations
  - Why needed here: We need to verify whether the model has learned internal representations of environmental states, not just memorized action patterns
  - Quick check question: What does it mean if a probe can accurately reconstruct intermediate states from model activations?

- Concept: Language grounding in embodied environments
  - Why needed here: The model must connect language descriptions (e.g., "go to the grey key") to specific objects and their locations in the environment
- Quick check question: How does the model learn to associate "grey key" in the instruction with specific grid cells in the environment?

## Architecture Onboarding

- Component map: Input embeddings → 2-layer CNN state encoder → MLP for state representation → 6-layer Transformer → Linear action prediction decoder → 2-layer MLP probing from intermediate activations
- Critical path: Input → Embedding layer → Transformer → Action prediction → Probing from intermediate activations
- Design tradeoffs:
  - Using symbolic state representation vs. pixel-based observations
  - Fixed vs. learned positional embeddings
  - Complete-state vs. missing-state training setups
- Failure signatures:
  - Random policy performance indicates model failed to learn task completion
  - Poor probe reconstruction suggests lack of internal state representation
  - Overfitting to training trajectories without generalization
- First 3 experiments:
  1. Train Complete-State model and evaluate action prediction performance on held-out test set
  2. Train probes to reconstruct intermediate states from Complete-State model activations
  3. Compare probe reconstruction accuracy between Complete-State and Missing-State models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do abstract state representations emerge in more complex environments beyond BabyAI, such as 3D environments or those with partial observability?
- Basis in paper: [inferred] The paper suggests future work includes evaluating sequence models in "more challenging BabyAI levels or even other environments that simulate real-world settings" where agents need to understand "more diverse text-object correlations and infer more involved intermediate states."
- Why unresolved: The current experiments are limited to grid-world environments with full observability. More complex environments introduce challenges like partial observability, larger state spaces, and more complex dynamics that could affect whether abstract state representations emerge.
- What evidence would resolve it: Testing sequence models on 3D environments like AI2-THOR, Habitat, or other embodied AI platforms while measuring state reconstruction accuracy would provide direct evidence.

### Open Question 2
- Question: What is the minimal amount of intermediate state information needed for the emergence of abstract state representations?
- Basis in paper: [explicit] The paper systematically ablates different input components (language instructions, intermediate states, actions) to understand their impact on state representation emergence, suggesting this is an open research direction.
- Why unresolved: While the paper shows that removing intermediate states entirely still allows state representation emergence, it doesn't explore partial information scenarios like providing every 2nd state or only state changes.
- What evidence would resolve it: Systematic experiments varying the frequency and type of intermediate state observations during training would reveal the information threshold for representation emergence.

### Open Question 3
- Question: How do abstract state representations in sequence models compare to those learned through traditional reinforcement learning approaches?
- Basis in paper: [inferred] The paper discusses sequence modeling as an alternative to reinforcement learning for decision making, but doesn't directly compare the quality or utility of state representations learned through each approach.
- Why unresolved: The paper focuses on whether sequence models learn state representations, not on comparing these representations to those learned through RL methods like value functions or world models.
- What evidence would resolve it: Direct comparison studies measuring state representation quality (through probing, downstream task performance, or generalization) between sequence models and RL agents on identical tasks would provide answers.

## Limitations

- Probing accuracy does not definitively prove abstract state representations exist - could be pattern matching
- Study focuses on relatively simple grid-world environments, limiting generalizability
- Cannot distinguish between genuine internal state tracking and surface-level pattern matching

## Confidence

- **High Confidence**: The empirical finding that probes can reconstruct intermediate states from model activations (direct measurement)
- **Medium Confidence**: The claim that language instructions enhance state representation emergence (supported by data but mechanism not fully elucidated)
- **Medium Confidence**: The conclusion that embodied sequence modeling can lead to abstract state representations (inferred from probe results but not definitively proven)

## Next Checks

1. **Ablation on Language**: Remove language instructions from training and compare probe reconstruction accuracy to determine if language truly enhances state representation emergence
2. **Temporal Generalization Test**: Evaluate probe reconstruction accuracy on sequences longer than those seen during training to test whether the model has learned genuine state tracking or memorized patterns
3. **Intervention Study**: Modify intermediate states during inference and observe whether action predictions change accordingly, providing stronger evidence that the model uses internal state representations for decision-making rather than just achieving reconstruction through probes