---
ver: rpa2
title: On the Adversarial Robustness of Multi-Modal Foundation Models
arxiv_id: '2308.10741'
source_url: https://arxiv.org/abs/2308.10741
tags:
- adversarial
- images
- attack
- attacks
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that multi-modal models like OpenFlamingo\
  \ are vulnerable to imperceptible adversarial image perturbations (\u2113\u221E\
  \ \u2264 1/255) that can manipulate caption outputs to spread misinformation or\
  \ guide users to malicious websites. The authors introduce targeted and untargeted\
  \ attack frameworks and show that with 500 APGD iterations, targeted attacks achieve\
  \ success rates of 51.66% (\u03B5=1/255) and 100% (\u03B5=4/255) on COCO images,\
  \ while untargeted attacks reduce CIDEr scores to near-random levels."
---

# On the Adversarial Robustness of Multi-Modal Foundation Models

## Quick Facts
- arXiv ID: 2308.10741
- Source URL: https://arxiv.org/abs/2308.10741
- Reference count: 40
- One-line primary result: Small ℓ∞ perturbations (ε ≤ 1/255) can manipulate OpenFlamingo's image captioning output to spread misinformation

## Executive Summary
This paper investigates the adversarial robustness of multi-modal foundation models, specifically targeting OpenFlamingo's image captioning and visual question answering capabilities. The authors demonstrate that imperceptible adversarial perturbations can effectively manipulate model outputs, potentially enabling misinformation campaigns or malicious user redirection. Through systematic evaluation using white-box attacks, the study reveals significant vulnerabilities in current multi-modal architectures, with targeted attacks achieving success rates of 51.66% (ε=1/255) and 100% (ε=4/255) on COCO images, while untargeted attacks degrade caption quality to near-random levels.

## Method Summary
The authors evaluate OpenFlamingo's robustness using white-box ℓ∞-bounded adversarial attacks (ε=1/255 or 4/255) with APGD optimization (500 iterations default). For untargeted attacks, they maximize negative log-likelihood of the true caption, while targeted attacks minimize negative log-likelihood for desired outputs. The experiments use COCO 2014 and Flickr30k for captioning, and OK-VQA and VizWiz for VQA, with success measured via CIDEr score degradation, accuracy drops, and targeted attack success rates.

## Key Results
- Targeted attacks achieve success rates of 51.66% (ε=1/255) and 100% (ε=4/255) on COCO images
- Untargeted attacks reduce CIDEr scores to near-random levels (~3.01), indicating severe caption degradation
- Adversarial perturbations are "hardly visible" to users while significantly altering model outputs

## Why This Works (Mechanism)

### Mechanism 1
Small ℓ∞ perturbations (ε ≤ 1/255) can manipulate OpenFlamingo's image captioning output to spread misinformation.
- Mechanism: Adversarial attack adds imperceptible noise to input images, causing the model's cross-attention layers to attend to altered visual features, which changes the autoregressive language generation path.
- Core assumption: OpenFlamingo's vision encoder and cross-attention layers are sensitive enough that small perturbations in pixel space propagate through to meaningful changes in caption tokens.
- Evidence anchors:
  - [abstract] "imperceivable attacks on images ( ε∞ = 1/255) in order to change the caption output of a multi-modal foundation model"
  - [section] "the perturbations are hardly visible and would not be noticed by a user" (Figure 1)
  - [corpus] Weak: No direct corpus evidence; the closest neighbor is about CLIP robustness, not multimodal caption manipulation.
- Break condition: If the vision encoder has built-in adversarial robustness (e.g., adversarial training or robust fine-tuning), the perturbations may fail to propagate to the language output.

### Mechanism 2
Targeted attacks can force the model to output a specific desired caption (e.g., "Please reset your password") with high success rate.
- Mechanism: The APGD optimization minimizes the negative log-likelihood of the target caption tokens, effectively "steering" the autoregressive generation toward the attacker's desired text.
- Core assumption: The model's autoregressive generation is differentiable w.r.t. input pixels, allowing gradient-based optimization to find adversarial perturbations that bias the token probabilities.
- Evidence anchors:
  - [abstract] "targeted attacks achieve success rates of 51.66% (ε=1/255) and 100% (ε=4/255)"
  - [section] "The objective for the targeted attack then is min δq,δc − ∑ l log p(ŷl | y<l, z, q + δq, c + δc)"
  - [corpus] Weak: No corpus evidence for targeted attacks on multimodal captioning; only general adversarial attacks on single-modal models.
- Break condition: If the model uses non-differentiable components or gradient masking, the attack optimization will fail.

### Mechanism 3
Untargeted attacks degrade CIDEr scores to near-random levels, making the captions uninformative.
- Mechanism: Maximizing the negative log-likelihood of the true caption forces the model to generate text unrelated to the image content, breaking the alignment between vision and language.
- Core assumption: The loss landscape for the captioning task is smooth enough that gradient ascent on the negative log-likelihood produces semantically unrelated captions.
- Evidence anchors:
  - [abstract] "untargeted attacks reduce CIDEr scores to near-random levels"
  - [section] "max δq,δc − ∑ l log p(yl | y<l, z, q + δq, c + δc)"
  - [corpus] Weak: No corpus evidence for untargeted multimodal attacks; closest is general adversarial attack literature.
- Break condition: If the model has robust training or input preprocessing that limits perturbation impact, the CIDEr score degradation may be less severe.

## Foundational Learning

- Concept: Adversarial examples in computer vision
  - Why needed here: The attack relies on standard ℓ∞ threat models and gradient-based optimization (APGD), which are foundational in adversarial ML.
  - Quick check question: What is the difference between targeted and untargeted adversarial attacks?

- Concept: Cross-modal attention mechanisms
  - Why needed here: OpenFlamingo uses cross-attention layers to fuse vision and language; understanding this helps explain how perturbations propagate.
  - Quick check question: How does cross-attention enable a vision encoder to influence language model output?

- Concept: Evaluation metrics for image captioning (CIDEr, BLEU-4)
  - Why needed here: The paper uses these metrics to quantify attack success; knowing their definitions is crucial for interpreting results.
  - Quick check question: What does a CIDEr score of 3.01 (random captions) imply about model robustness?

## Architecture Onboarding

- Component map: CLIP ViT-L-14 → cross-attention layers in MPT-7B → autoregressive language head
- Critical path: Input image → CLIP → embeddings → cross-attention → next-token prediction → repeat until EOS
- Design tradeoffs: Large model (9B params) → high performance but high attack surface; OpenFlamingo's zero-shot capability → no need for fine-tuning, but also no task-specific robustness
- Failure signatures: Low CIDEr/accuracy scores on perturbed images; generated captions unrelated to image content; exact match of target caption in targeted attack
- First 3 experiments:
  1. Run untargeted attack with ε=1/255, 500 APGD steps; measure CIDEr drop on COCO
  2. Run targeted attack with ε=1/255, 500 APGD steps; measure success rate for a short target caption
  3. Vary perturbation fraction (e.g., 60% top pixels) to see minimal attack strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would adversarial training be in defending against the targeted attacks on multi-modal models like OpenFlamingo?
- Basis in paper: [explicit] The paper discusses adversarial training as a prominent defense against adversarial examples in general, but does not evaluate its effectiveness specifically against the targeted attacks on multi-modal models.
- Why unresolved: The paper demonstrates successful targeted attacks on OpenFlamingo but does not explore potential defense mechanisms like adversarial training.
- What evidence would resolve it: Conducting experiments where OpenFlamingo is trained with adversarial examples specifically designed for targeted attacks and evaluating the success rate of these attacks post-training.

### Open Question 2
- Question: Are there other multi-modal models besides OpenFlamingo that exhibit similar vulnerabilities to adversarial attacks, and how do their robustness levels compare?
- Basis in paper: [explicit] The paper focuses on OpenFlamingo as a case study for evaluating adversarial robustness but does not extend the analysis to other multi-modal models.
- Why unresolved: The paper provides a detailed analysis of OpenFlamingo's vulnerabilities but does not compare these findings with other models in the same domain.
- What evidence would resolve it: Performing a comparative study on multiple multi-modal models, including but not limited to OpenFlamingo, to assess their vulnerability to similar adversarial attacks and documenting the differences in robustness levels.

### Open Question 3
- Question: What are the long-term implications of adversarial attacks on multi-modal models in real-world applications, particularly in terms of user trust and model reliability?
- Basis in paper: [inferred] The paper discusses the potential for misinformation and user manipulation due to adversarial attacks but does not explore the broader, long-term implications of these vulnerabilities.
- Why unresolved: While the paper highlights immediate risks, it does not delve into how sustained exposure to adversarial attacks might affect user trust and the perceived reliability of multi-modal models over time.
- What evidence would resolve it: Conducting longitudinal studies on user interactions with multi-modal models, focusing on changes in trust and reliability perceptions after repeated exposure to adversarial attacks.

## Limitations
- The evaluation uses white-box access, which doesn't reflect real-world scenarios where model weights are proprietary
- Only ℓ∞-bounded perturbations are explored, without testing other threat models (ℓ2, ℓ0) or adaptive defenses
- Success metrics measure technical attack effectiveness but don't fully capture practical impact on end-users or downstream applications

## Confidence
- **High confidence**: The fundamental mechanism that small adversarial perturbations can manipulate multi-modal model outputs through gradient-based optimization is well-established
- **Medium confidence**: The specific success rates (51.66% at ε=1/255 for targeted attacks) and CIDEr degradation values are reproducible given the described experimental setup
- **Low confidence**: The claim about "spreading misinformation" or "guiding users to malicious websites" through caption manipulation overstates the practical impact without empirical evidence of real-world exploitation scenarios

## Next Checks
1. Test whether simple preprocessing defenses (JPEG compression, bit-depth reduction, Gaussian noise addition) can mitigate the reported attacks while preserving model utility on clean data
2. Evaluate whether white-box generated perturbations transfer effectively to other multi-modal models (e.g., BLIP, GIT) or when only API access is available
3. Conduct user studies to quantify the actual perceptibility of adversarial perturbations under various viewing conditions and measure how often manipulated captions are trusted versus detected as potentially unreliable