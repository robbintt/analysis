---
ver: rpa2
title: 'MLPST: MLP is All You Need for Spatio-Temporal Prediction'
arxiv_id: '2309.13363'
source_url: https://arxiv.org/abs/2309.13363
tags:
- prediction
- mlpst
- temporal
- time
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MLPST, an all-Multi-Layer Perceptron (MLP)
  architecture for spatio-temporal traffic prediction. The method processes spatial
  maps by dividing them into patches, then uses interleaved token-mixing and channel-mixing
  MLPs to capture global spatial dependencies.
---

# MLPST: MLP is All You Need for Spatio-Temporal Prediction

## Quick Facts
- **arXiv ID**: 2309.13363
- **Source URL**: https://arxiv.org/abs/2309.13363
- **Reference count**: 40
- **Primary result**: MLPST achieves state-of-the-art accuracy on NYCBike and NYCTaxi datasets with significantly fewer parameters and lower computational complexity than baseline methods

## Executive Summary
This paper introduces MLPST, an all-Multi-Layer Perceptron architecture for spatio-temporal traffic prediction that challenges the dominance of attention-based and graph convolutional approaches. The method processes spatial maps by dividing them into patches and using interleaved token-mixing and channel-mixing MLPs to capture global spatial dependencies, while temporal dependencies are modeled separately for trend, period, and closeness intervals using identical MLP structures. Experiments demonstrate that MLPST achieves state-of-the-art prediction accuracy while requiring only linear computational complexity and model parameters more than an order of magnitude smaller than baselines.

## Method Summary
MLPST processes spatio-temporal traffic data by first partitioning spatial grid maps into non-overlapping patches, which are then transformed into token embeddings. The SpatialMixer applies alternating token-mixing and channel-mixing MLP layers to enable global spatial communication across patches. Temporal information is separated into three distinct sequences—trend (weekly), period (daily), and closeness (recent)—each processed by identical TemporalMixer MLP modules. A fusion module aggregates the final time steps from each temporal mixer through weighted summation, and an output MLP layer produces the final HxW prediction grid. The entire architecture uses only MLP operations with GELU activation, avoiding attention mechanisms and convolutions entirely.

## Key Results
- MLPST achieves state-of-the-art accuracy on NYCBike and NYCTaxi datasets with R² scores competitive with transformer-based methods
- The model requires only linear computational complexity (O(N)) compared to quadratic complexity (O(N²)) of self-attention mechanisms
- MLPST uses more than an order of magnitude fewer parameters than baseline methods while maintaining superior or comparable prediction performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Patch-based token mixing achieves global spatial dependencies without convolution or attention
- **Mechanism**: Dividing spatial grid maps into non-overlapping patches creates tokens that are processed by interleaved token-mixing and channel-mixing MLPs. Token-mixing MLPs operate on transposed token embeddings, enabling interaction across all spatial locations. Channel-mixing MLPs then refine these interactions within each token, capturing both local and global spatial patterns through fully connected operations rather than convolutions or self-attention.
- **Core assumption**: Flattening patches into tokens and mixing them with fully connected layers preserves spatial structure sufficiently for effective spatial feature extraction.
- **Evidence anchors**: [abstract] "processes spatial maps by dividing them into patches, then uses interleaved token-mixing and channel-mixing MLPs to capture global spatial dependencies" [section] "Since tokens come from different locations on the original spatial map, then communication between tokens realizes the global mixing of spatial information" [corpus] Weak: No direct evidence of patch-based spatial mixing achieving global dependencies without attention mechanisms in the corpus
- **Break condition**: If patch boundaries destroy local spatial continuity (e.g., discontinuous road networks), token mixing cannot reconstruct it without additional inductive bias.

### Mechanism 2
- **Claim**: Separate temporal mixers for trend, period, and closeness intervals capture distinct temporal dependencies more effectively than joint modeling
- **Mechanism**: The framework splits historical observations into three temporal dependency sequences: trend (weekly intervals), period (daily cycles), and closeness (recent intervals). Each sequence is processed by its own TemporalMixer (identical MLP structure) before fusion. This separation allows each mixer to specialize in its temporal scale, avoiding interference between long-term trends and short-term dynamics.
- **Core assumption**: Different temporal scales have sufficiently distinct patterns that joint modeling would introduce harmful interference or require more parameters to disentangle.
- **Evidence anchors**: [abstract] "Temporal dependencies are modeled separately for trend, period, and closeness intervals" [section] "We recognize the historical data sequence with three different kinds of temporal dependencies: trend, period, and closeness" [corpus] Weak: No direct evidence that separating temporal intervals into distinct mixers improves accuracy versus joint modeling in the corpus
- **Break condition**: If temporal patterns are highly interdependent across scales, separation may prevent the model from learning cross-scale interactions.

### Mechanism 3
- **Claim**: Linear complexity MLPs replace quadratic attention/graph convolutions while maintaining or improving accuracy
- **Mechanism**: Both SpatialMixer and TemporalMixer use MLP-Mixer layers where token-mixing MLPs operate on sequences of length N (patches or time steps) with O(N) complexity rather than O(N²) for attention or graph convolutions. The architecture achieves state-of-the-art results with "linear computational complexity, i.e., O(N)" and model parameters "more than an order of magnitude lower than baselines."
- **Core assumption**: Linear-complexity operations can approximate the representational power of quadratic-complexity mechanisms when structured appropriately (patch-based spatial mixing, interval-separated temporal mixing).
- **Evidence anchors**: [abstract] "requiring only linear computational complexity, as well as model parameters that are more than an order of magnitude lower than baselines" [section] "complexity of this part should be O(NP), unlike O(N²P) of self-attention mechanisms" [section] "complexity here is linear with respect to the number of tokens, which is O(t·dT)" [corpus] No direct evidence comparing linear-complexity MLPs to quadratic-complexity alternatives for this specific task in the corpus
- **Break condition**: If the task requires modeling long-range dependencies that cannot be captured through local mixing operations, linear complexity may be insufficient.

## Foundational Learning

- **Concept**: Multi-Layer Perceptron (MLP) architecture and GELU activation
  - **Why needed here**: MLPST is built entirely from MLP-Mixer layers, so understanding how MLPs process input vectors and apply non-linearities is fundamental to grasping the entire architecture.
  - **Quick check question**: What is the computational complexity of a single MLP layer with input dimension d and hidden dimension h, and how does this compare to self-attention with sequence length n?

- **Concept**: Token-mixing vs. channel-mixing operations
  - **Why needed here**: These are the two core operations in MLP-Mixer layers that enable spatial and temporal information mixing; understanding their mathematical formulation is crucial for debugging and extending the architecture.
  - **Quick check question**: In token-mixing MLPs, if the input is a matrix V with shape (tokens, channels), what is the shape of the output and how does this operation enable cross-location communication?

- **Concept**: Spatial-temporal data representation and grid partitioning
  - **Why needed here**: The input data is represented as H×W×d grid maps that must be partitioned into patches; understanding this representation is essential for implementing the patch processing step correctly.
  - **Quick check question**: If a city grid is 10×20 and patches are 2×2, how many patches are created and what is the dimensionality of each patch after flattening?

## Architecture Onboarding

- **Component map**: Input Layer → SpatialMixer (patch → token-mixing → channel-mixing) → TemporalMixer (3x) → Fusion → Output Layer → Prediction

- **Critical path**: Input → SpatialMixer (patch → token-mixing → channel-mixing) → TemporalMixer (3x) → Fusion → Output → Prediction

- **Design tradeoffs**:
  - Patch size: Larger patches reduce patch count (faster processing) but may lose local detail; smaller patches capture more detail but increase computational load
  - Number of channels (CS, CT): Higher channels increase representational capacity but also parameter count and risk overfitting
  - Number of Mixer layers: More layers can capture more complex patterns but increase training time and risk overfitting

- **Failure signatures**:
  - Poor spatial performance: Check if patch size is too large for the grid resolution or if token-mixing is insufficient for the spatial complexity
  - Poor temporal performance: Verify that temporal intervals (trend/period/closeness) are appropriate for the data frequency and patterns
  - Overfitting: Reduce number of channels, layers, or apply regularization if validation performance degrades

- **First 3 experiments**:
  1. Patch size ablation: Test patch sizes [1, 2, 5, 10] on NYCTaxi to find optimal balance between performance and efficiency
  2. Temporal interval sensitivity: Vary (closeness, period, trend) configurations to understand which temporal dependencies are most important for different datasets
  3. Parameter efficiency: Compare model size vs. accuracy trade-offs by varying CS=CT and number of Mixer layers to identify minimal sufficient architecture

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of MLPST compare when applied to different types of spatio-temporal prediction tasks beyond traffic flow, such as air quality prediction or urban energy consumption forecasting?
  - **Basis in paper**: [explicit] The paper mentions that MLPST is promising for multiple types of STDM tasks in daily urban life, such as air quality prediction and urban energy consumption forecasting.
  - **Why unresolved**: The experiments in the paper only validate MLPST on traffic flow datasets (NYCBike and NYCTaxi), so its performance on other types of spatio-temporal data remains untested.
  - **What evidence would resolve it**: Conducting experiments on air quality or energy consumption datasets to measure MLPST's prediction accuracy and efficiency compared to other methods.

- **Open Question 2**: What is the optimal patch size for MLPST when applied to grid maps of different dimensions or resolutions?
  - **Basis in paper**: [explicit] The paper tests different patch sizes and finds that patch size 2 performs best for their specific grid dimensions (10x20), but notes that larger or smaller patch sizes reduce performance.
  - **Why unresolved**: The paper only tests patch sizes on one grid configuration, and the optimal patch size may vary with different grid resolutions or spatial distributions of data.
  - **What evidence would resolve it**: Experiments varying grid dimensions and resolutions to determine the relationship between optimal patch size and input grid characteristics.

- **Open Question 3**: How does the performance of MLPST scale with increasing input sequence length beyond the tested 12 time steps?
  - **Basis in paper**: [explicit] The paper divides 12 time steps into trend, period, and closeness intervals, but doesn't explore how performance changes with longer sequences.
  - **Why unresolved**: The computational complexity analysis suggests linear scaling, but actual performance degradation or improvement with longer sequences hasn't been tested.
  - **What evidence would resolve it**: Testing MLPST with progressively longer input sequences (e.g., 24, 48, 96 time steps) while measuring accuracy, training time, and inference time.

## Limitations

- **Empirical validation gaps**: The paper lacks ablation studies that would isolate the contribution of each design choice, making it unclear whether the claimed mechanisms actually drive the performance improvements.
- **Dataset specificity**: Evaluation focuses exclusively on urban traffic prediction datasets, raising questions about generalizability to other spatio-temporal domains.
- **Theoretical foundations**: Limited theoretical justification for why linear-complexity MLPs can replace quadratic-complexity attention mechanisms for spatio-temporal modeling.

## Confidence

**High confidence**: The computational complexity analysis showing O(N) vs O(N²) for attention mechanisms is mathematically sound and verifiable. The architectural description of MLP-Mixer layers and their application to spatio-temporal data is clearly specified.

**Medium confidence**: The state-of-the-art performance claims on NYCBike and NYCTaxi datasets are supported by reported metrics, but the lack of ablation studies and comparison to more recent transformer-based approaches reduces confidence in whether the improvements stem from the MLP-only design or other factors.

**Low confidence**: The mechanism by which patch-based token mixing achieves global spatial dependencies without attention or convolution is not well-supported by evidence. The claim that separating temporal intervals prevents interference is similarly speculative without empirical validation.

## Next Checks

1. **Ablation study**: Conduct controlled experiments removing each key component (patch processing, temporal separation, MLP-only design) to quantify their individual contributions to performance gains. This would validate whether the claimed mechanisms actually drive the improvements.

2. **Cross-domain generalization**: Test MLPST on non-traffic spatio-temporal datasets (e.g., climate data, financial markets) to assess whether the architectural advantages generalize beyond the evaluated domain or represent over-fitting to traffic patterns.

3. **Long-range dependency analysis**: Systematically evaluate MLPST's ability to capture long-range spatial and temporal dependencies by testing on datasets with known long-range patterns, comparing against attention-based methods to verify that linear complexity does not compromise critical long-range modeling capabilities.