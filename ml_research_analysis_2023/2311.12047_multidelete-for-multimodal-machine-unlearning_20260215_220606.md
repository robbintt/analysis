---
ver: rpa2
title: MultiDelete for Multimodal Machine Unlearning
arxiv_id: '2311.12047'
source_url: https://arxiv.org/abs/2311.12047
tags:
- data
- unlearning
- multimodal
- knowledge
- unimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMUL, the first approach to machine unlearning
  in multimodal settings. MMUL addresses the challenge of removing specific training
  data from a multimodal model while preserving its performance.
---

# MultiDelete for Multimodal Machine Unlearning

## Quick Facts
- arXiv ID: 2311.12047
- Source URL: https://arxiv.org/abs/2311.12047
- Reference count: 40
- One-line primary result: MMUL outperforms baselines by 17.6 points in distinguishing deleted data from remaining data while maintaining only 0.3 point performance gap compared to retraining

## Executive Summary
This paper introduces MMUL, the first approach to machine unlearning in multimodal settings. MMUL addresses the challenge of removing specific training data from a multimodal model while preserving its performance. It formulates multimodal unlearning through three key properties: modality decoupling, which decouples associations between data modalities marked for deletion; unimodal knowledge retention, which preserves the model's understanding of individual modalities; and multimodal knowledge retention, which maintains the model's overall multimodal knowledge. Experiments on vision-language and graph-text datasets demonstrate that MMUL outperforms existing baselines by 17.6 points in distinguishing deleted data from remaining data, while maintaining pre-existing knowledge with only a 0.3 point performance gap compared to retraining from scratch. The approach is also more efficient than retraining and provides better protection against membership inference attacks.

## Method Summary
MMUL formulates multimodal unlearning using three loss functions: modality decoupling (LMD) to separate associations between deleted multimodal pairs, unimodal knowledge retention (LUKR) to preserve individual modality representations, and multimodal knowledge retention (LMKR) to maintain overall model performance. The approach trains multimodal models (ALBEF, BLIP, or GCN+BERT) on vision-language and graph-text datasets, then applies unlearning by optimizing these losses on a randomly sampled deletion set. The method is evaluated on Flickr30K, SNLI-VE, NLVR2, and PGR datasets using AUC metrics to distinguish deleted from remaining data, test set performance, and membership inference robustness.

## Key Results
- MMUL outperforms baselines by 17.6 points in AUC distinguishing deleted data from remaining data
- Maintains only 0.3 point performance gap compared to retraining from scratch
- Provides better protection against membership inference attacks while being more efficient than full retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality decoupling effectively separates the associations between deleted image-text pairs from random pairs in the remaining dataset
- Mechanism: By minimizing the distance between multimodal representations of deleted pairs and random unpaired pairs, the model learns to treat deleted pairs as unrelated
- Core assumption: The multimodal representation space allows for meaningful distance measurements between paired and unpaired data
- Evidence anchors:
  - [abstract] "MMUL formulates the multimodal unlearning task by focusing on three key properties: (a): modality decoupling, which effectively decouples the association between individual unimodal data points within multimodal inputs marked for deletion"
  - [section] "LMD = Dis(⟦f′(Ii, Ti)⟧|(Ii, Ti) ∈ Du, ⟦f(Ip, Tq)⟧|(Ip, Tp) ∈ Dr, (Iq, Tq) ∈ Dr, p ≠ q)"
  - [corpus] Weak evidence - neighboring papers discuss multimodal unlearning but lack specific mechanistic details about modality decoupling
- Break condition: If the distance function cannot meaningfully distinguish between paired and unpaired representations, or if the representation space collapses

### Mechanism 2
- Claim: Unimodal knowledge retention preserves individual modality representations while unlearning multimodal associations
- Mechanism: By minimizing the difference between unimodal representations of deleted data before and after unlearning, the model maintains core feature understanding
- Core assumption: Unimodal representations remain stable and useful even when multimodal associations are removed
- Evidence anchors:
  - [abstract] "MMUL formulates the multimodal unlearning task by focusing on three key properties: (b): unimodal knowledge retention which retains the unimodal representation capability of the model post-unlearning"
  - [section] "LUKR = Dis(n⟦f′I(Ii), f′T(Ti)⟧|(Ii, Ti) ∈ Du, n⟦fI(Ii), fT(Ti)⟧|(Ii, Ti) ∈ Du)"
  - [corpus] Moderate evidence - neighboring papers mention preserving unimodal knowledge but don't detail the specific retention mechanism
- Break condition: If unimodal representations drift significantly during the unlearning process, or if the modality fusion becomes too dependent on learned associations

### Mechanism 3
- Claim: Multimodal knowledge retention maintains overall model performance on remaining data while unlearning specific samples
- Mechanism: By minimizing the difference between multimodal representations of remaining data before and after unlearning, the model preserves general knowledge
- Core assumption: The remaining dataset contains sufficient information to maintain multimodal understanding without the deleted samples
- Evidence anchors:
  - [abstract] "MMUL formulates the multimodal unlearning task by focusing on three key properties: (c): multimodal knowledge retention which retains the multimodal representation capability of the model post-unlearning"
  - [section] "LMKR = Dis(⟦f′(Ir, Tr)⟧|(Ir, Tr) ∈ Dr, ⟦f(Ir, Tr)⟧|(Ir, Tr) ∈ Dr)"
  - [corpus] Weak evidence - neighboring papers discuss knowledge retention but lack specific details about multimodal knowledge preservation
- Break condition: If the remaining dataset is too small or lacks diversity, causing performance degradation on multimodal tasks

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: Understanding how different modalities (vision, text, graph) are encoded and fused is crucial for implementing MMUL's three key properties
  - Quick check question: Can you explain how a vision-language transformer differs from a single-modality transformer in terms of architecture and training objectives?

- Concept: Distance metrics in representation space
  - Why needed here: MMUL relies on distance functions (like mean squared error) to measure differences between representations before and after unlearning
  - Quick check question: What are the advantages and disadvantages of using cosine similarity versus mean squared error for measuring representation distances?

- Concept: Optimization with multiple loss functions
  - Why needed here: MMUL combines three different loss functions (modality decoupling, unimodal retention, multimodal retention) with weighted coefficients
  - Quick check question: How would you approach balancing multiple loss functions with potentially different scales and gradients?

## Architecture Onboarding

- Component map:
  Unimodal encoders (vision, text, graph) -> Modality fusion module -> Loss computation modules -> Parameter update controller

- Critical path:
  1. Forward pass through unimodal encoders
  2. Fusion of unimodal representations
  3. Computation of three loss components
  4. Weighted aggregation of losses
  5. Backward pass and parameter updates

- Design tradeoffs:
  - Updating all parameters vs. only fusion module: Full updates provide better unlearning but risk more performance degradation; fusion-only updates are more stable but may be less effective
  - Choice of distance function: MSE is simple but may not capture semantic differences; more complex metrics may be more meaningful but computationally expensive
  - Weight balancing (α, β, γ): Requires careful tuning to achieve both effective unlearning and knowledge retention

- Failure signatures:
  - High modality decoupling loss but low unimodal retention loss: Model successfully unlearns associations but may lose useful unimodal features
  - Low modality decoupling loss but high multimodal knowledge retention loss: Model fails to unlearn specific samples but maintains general performance
  - High values across all losses: Poor optimization or inappropriate weight settings

- First 3 experiments:
  1. Test modality decoupling alone: Apply only LMD loss and measure AUC improvement in distinguishing deleted vs. remaining data
  2. Test knowledge retention: Apply only LUKR and LMKR losses and measure performance drop on original test set
  3. Full ablation study: Remove each key property (MD, UKR, MKR) individually and measure impact on all metrics to understand contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MMUL approach scale to more than two modalities, and what challenges might arise in extending it to higher-order multimodal data?
- Basis in paper: [explicit] The paper states "MMUL can be applied to diverse multimodal datasets involving more than two modalities and to other types of data modalities, such as combinations involving graph and text data."
- Why unresolved: The experiments only evaluate vision-language and graph-text datasets, leaving open how the approach would perform with more complex, higher-order multimodal data.
- What evidence would resolve it: Experiments testing MMUL on datasets with three or more modalities, comparing performance and computational efficiency against existing baselines.

### Open Question 2
- Question: What are the long-term effects of MMUL on model performance when repeatedly applied to remove different subsets of data over time?
- Basis in paper: [inferred] The paper focuses on single deletion events but doesn't address scenarios where data needs to be removed multiple times from the same model.
- Why unresolved: No experiments are conducted to test the cumulative impact of multiple unlearning operations on model performance and stability.
- What evidence would resolve it: Longitudinal studies applying MMUL multiple times to the same model with different deletion sets, tracking performance degradation or improvement over time.

### Open Question 3
- Question: How does MMUL perform in real-world scenarios where the deleted data distribution significantly differs from the remaining data?
- Basis in paper: [inferred] The paper uses random deletion sets in experiments, which may not reflect real-world scenarios where deleted data might be systematically different (e.g., biased or outdated samples).
- Why unresolved: The experimental setup doesn't test MMUL's robustness to non-random deletion patterns or adversarial deletion strategies.
- What evidence would resolve it: Experiments where deletion sets are constructed to be systematically different from remaining data (e.g., older data, biased samples, or adversarially selected data) and comparing MMUL's performance against these scenarios.

## Limitations
- Lacks specific architectural details about the fusion module and distance function used in loss computations
- Limited exploration of hyperparameter sensitivity for the loss weights (α, β, γ)
- Evaluation focuses primarily on benchmark datasets without addressing real-world deployment scenarios

## Confidence
- High Confidence: The core framework of MMUL with its three key properties is well-justified and the experimental results demonstrating superiority over baselines are compelling
- Medium Confidence: The mechanism descriptions are clear, but the lack of architectural specifics and distance function details creates uncertainty about exact reproducibility
- Medium Confidence: The performance metrics and evaluation methodology are appropriate, though the limited scope of adversarial robustness testing reduces confidence in real-world applicability

## Next Checks
1. **Architectural Verification**: Implement and test multiple fusion module architectures (e.g., simple concatenation, attention-based fusion, transformer-based fusion) to determine the impact on unlearning effectiveness and knowledge retention

2. **Hyperparameter Sensitivity Analysis**: Conduct a systematic grid search over the loss weights (α, β, γ) and evaluate the impact on both unlearning performance (Du|Dr AUC) and knowledge retention (test set accuracy) to identify optimal configurations and potential trade-offs

3. **Extended Adversarial Robustness Testing**: Evaluate MMUL against more sophisticated adversarial attacks beyond membership inference, such as data reconstruction attacks, attribute inference attacks, and model inversion attacks, to assess the security guarantees of the unlearning process