---
ver: rpa2
title: Are Human-generated Demonstrations Necessary for In-context Learning?
arxiv_id: '2309.14681'
source_url: https://arxiv.org/abs/2309.14681
tags:
- answer
- demonstrations
- few-shot
- question
- inches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-contemplation prompting (SEC) demonstrates that large language
  models can generate their own demonstrations for in-context learning without human-crafted
  examples, achieving comparable performance to traditional supervised in-context
  learning across multiple benchmarks. The method involves prompting LLMs to first
  create tailored demonstrations for each test input, then applying standard in-context
  learning.
---

# Are Human-generated Demonstrations Necessary for In-context Learning?

## Quick Facts
- arXiv ID: 2309.14681
- Source URL: https://arxiv.org/abs/2309.14681
- Reference count: 40
- One-line primary result: LLMs can generate their own demonstrations for in-context learning without human-crafted examples, achieving comparable performance to traditional ICL.

## Executive Summary
Self-contemplation prompting (SEC) demonstrates that large language models can generate their own demonstrations for in-context learning without human-crafted examples, achieving comparable performance to traditional supervised in-context learning across multiple benchmarks. The method involves prompting LLMs to first create tailored demonstrations for each test input, then applying standard in-context learning. Experiments on arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation show that SEC significantly outperforms zero-shot learning and matches few-shot learning performance, indicating that supervised training data may not be essential for many tasks given current LLM capabilities.

## Method Summary
SEC is a novel in-context learning approach where LLMs generate their own demonstrations for each test input without access to any training examples or human intervention. The method prompts the LLM to create tailored demonstrations based on the test input, then applies standard in-context learning using these generated demonstrations. The approach is evaluated across six benchmark datasets including GSM8K, MATH, ARC, MMLU, C-Eval, and HumanEval, comparing performance against zero-shot learning and traditional ICL (both vanilla and chain-of-thought variants) using exact match accuracy and functional correctness metrics.

## Key Results
- SEC achieves comparable results to traditional ICL across six diverse benchmarks
- SEC significantly outperforms zero-shot learning on all tested tasks
- SEC demonstrates ability to bridge the gap between zero-shot and few-shot learning without requiring human-crafted demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate high-quality demonstrations tailored to each test input
- Mechanism: By prompting the LLM to create demonstrations based on the test input, the generated examples are naturally aligned with the query, providing relevant context
- Core assumption: LLMs possess sufficient task knowledge to generate appropriate examples without training data
- Evidence anchors: [abstract] "SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated"

### Mechanism 2
- Claim: Eliminating human demonstration crafting removes performance variability from poor example selection
- Mechanism: By generating demonstrations automatically for each test case, SEC avoids the sensitivity to demonstration selection that plagues traditional ICL
- Core assumption: Human demonstrations can introduce noise or bias that degrades ICL performance
- Evidence anchors: [abstract] "the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations"

### Mechanism 3
- Claim: SEC bridges the gap between zero-shot and few-shot learning by leveraging in-context knowledge generation
- Mechanism: SEC uses the LLM's own knowledge to create context, effectively providing supervision without external data
- Core assumption: LLMs contain sufficient task-relevant knowledge to bootstrap their own learning process
- Evidence anchors: [abstract] "With no access to ANY training example or human intervention, SEC achieves comparable results to ICL"

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Understanding ICL is fundamental to grasping why SEC is innovative - it's an alternative to traditional demonstration-based ICL
  - Quick check question: What distinguishes ICL from standard fine-tuning approaches?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: SEC can be adapted to CoT scenarios, making understanding this prompting strategy essential for implementing SEC
  - Quick check question: How does CoT prompting differ from vanilla ICL?

- Concept: Few-shot prompting
  - Why needed here: SEC generates its own few-shot examples, so understanding the standard approach helps appreciate the innovation
  - Quick check question: Why is the number of shots typically limited in few-shot learning?

## Architecture Onboarding

- Component map: LLM API → Prompt generator → Demonstration extractor → ICL executor
- Critical path: Test input → SEC prompt generation → LLM demonstration generation → ICL with generated demonstrations → Output extraction
- Design tradeoffs: Fewer demonstrations may reduce computation but increase variance; more demonstrations increase cost but improve stability
- Failure signatures: Poor demonstration generation quality, format extraction failures, ICL performance degradation
- First 3 experiments:
  1. Test SEC on GSM8K with 2 shots vs 5 shots to verify performance scaling
  2. Compare SEC vs CoT-SEC on MATH to validate CoT adaptation
  3. Test SEC on HumanEval with varying model sizes to establish capability thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound of performance for SEC when the LLM's capacity increases indefinitely?
- Basis in paper: [inferred] The paper shows that current LLMs can generate demonstrations comparable to human-crafted ones, suggesting potential for further improvement with increased model capacity
- Why unresolved: The paper only tests with gpt-3.5-turbo and doesn't explore the performance limits of SEC with larger models
- What evidence would resolve it: Systematic experiments comparing SEC performance across increasingly larger LLMs (gpt-4, future models) on the same benchmarks

### Open Question 2
- Question: Under what specific conditions do SEC demonstrations outperform ICL demonstrations?
- Basis in paper: [explicit] The paper notes that SEC demonstrations are tailored to each test input and mentions specific cases where this leads to better performance
- Why unresolved: The paper doesn't provide a comprehensive analysis of the characteristics that make SEC demonstrations superior
- What evidence would resolve it: Detailed analysis of test cases where SEC outperforms ICL, identifying common patterns in question complexity, domain specificity, or reasoning requirements

### Open Question 3
- Question: How does the quality of SEC demonstrations vary across different domains and reasoning types?
- Basis in paper: [explicit] The paper shows varying performance across different benchmarks but doesn't analyze the quality of the generated demonstrations themselves
- Why unresolved: The paper focuses on final output performance rather than analyzing the intermediate demonstration generation process
- What evidence would resolve it: Systematic evaluation of SEC-generated demonstrations across domains, measuring their correctness, relevance, and reasoning quality

## Limitations

- Demonstration Quality Control: The paper does not extensively address how the quality of automatically generated demonstrations is controlled or evaluated
- Dataset-Specific Dependencies: The performance improvements seem heavily dependent on the specific datasets used and may not generalize equally well to all task types
- Scalability Concerns: Computational overhead of generating demonstrations for each test instance could become prohibitive as the number of required demonstrations increases

## Confidence

**High Confidence**: The core finding that LLMs can generate their own demonstrations for in-context learning is well-supported by the experimental results across multiple benchmarks.

**Medium Confidence**: The claim that SEC eliminates the need for human demonstrations is qualified by the observation that SEC still relies on the LLM's pre-existing knowledge.

**Low Confidence**: The assertion that SEC is universally applicable across all in-context learning tasks is not fully supported.

## Next Checks

1. **Cross-Dataset Generalization**: Test SEC on datasets not included in the original study, particularly those with different characteristics (e.g., highly specialized domains, tasks requiring precise formatting, or those with limited training data for the underlying LLM).

2. **Quality Assessment of Generated Demonstrations**: Develop and apply metrics to systematically evaluate the quality and relevance of automatically generated demonstrations, comparing them against human-crafted examples to identify systematic biases or weaknesses.

3. **Computational Efficiency Analysis**: Conduct a detailed analysis of the computational overhead introduced by SEC, particularly for larger numbers of demonstrations, and explore optimization strategies to make the method more scalable for practical applications.