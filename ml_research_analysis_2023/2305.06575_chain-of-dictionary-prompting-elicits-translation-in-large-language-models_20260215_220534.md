---
ver: rpa2
title: Chain-of-Dictionary Prompting Elicits Translation in Large Language Models
arxiv_id: '2305.06575'
source_url: https://arxiv.org/abs/2305.06575
tags:
- latn
- arab
- translation
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Dictionary (CoD) improves low-resource translation in
  large language models by chaining multilingual dictionaries as prior knowledge,
  achieving up to 13x chrF++ gains on FLORES-200 (e.g., English to Serbian Cyrillic
  improves from 3.08 to 42.63). CoD consistently outperforms few-shot demonstrations
  and non-chained dictionaries, particularly for languages with baseline chrF++ scores
  under 20.
---

# Chain-of-Dictionary Prompting Elicits Translation in Large Language Models

## Quick Facts
- arXiv ID: 2305.06575
- Source URL: https://arxiv.org/abs/2305.06575
- Authors: Not specified
- Reference count: 9
- Primary result: Chain-of-Dictionary (CoD) improves low-resource translation in large language models, achieving up to 13x chrF++ gains on FLORES-200

## Executive Summary
Chain-of-Dictionary (CoD) is a novel prompting method that improves low-resource machine translation in large language models by chaining multilingual dictionary entries as prior knowledge. The approach extracts keywords from source sentences, translates them into auxiliary languages (French, German, Bulgarian), and constructs a chain of dictionary mappings that guide the LLM's translation output. CoD consistently outperforms few-shot demonstrations and non-chained dictionaries, particularly for languages with baseline chrF++ scores under 20, demonstrating up to 13x improvement on English to Serbian Cyrillic translation.

## Method Summary
CoD improves translation by constructing prompts that include chained multilingual dictionary entries as prior hints. The method extracts keywords from source sentences using ChatGPT, translates them to auxiliary languages (French, German, Bulgarian) using NLLB, and chains these translations into a prompt string. This chained dictionary is then appended to the standard translation prompt and sent to the LLM (ChatGPT). The approach leverages multilingual dictionaries created by translating FLORES-200 English words into 197 languages, using chrF++ for evaluation. No model training is required - the improvement comes entirely from prompt engineering.

## Key Results
- CoD achieves up to 13x chrF++ improvement on low-resource languages (English to Serbian Cyrillic: 3.08 to 42.63)
- CoD consistently outperforms few-shot demonstrations for low-resource languages
- Chaining dictionaries is crucial - decomposed dictionaries degrade performance
- CoD enables ChatGPT to produce viable translations in previously near-impossible cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoD improves low-resource translation by providing multilingual dictionary chains as prior hints to guide LLM decision-making
- Mechanism: CoD injects word-level translation hints (e.g., "professor of medicine" → "profesor ke-dokteran" → "Professorin für Med" → "professeur de méde") before the translation prompt, allowing the model to condition its output on explicit cross-lingual mappings
- Core assumption: LLMs can leverage explicit intermediate dictionary steps to improve translation accuracy, analogous to Chain-of-Thought reasoning
- Evidence anchors: Abstract states CoD improves translation using chained multilingual dictionaries as prior knowledge

### Mechanism 2
- Claim: CoD outperforms few-shot demonstrations for low-resource languages because retrieved demonstrations are often irrelevant
- Mechanism: Few-shot demonstrations rely on semantic similarity to retrieve relevant examples, but low-resource languages lack sufficient parallel data, making retrieved examples less useful. CoD instead uses offline dictionaries, which are easier to acquire
- Core assumption: Offline dictionaries are more reliable than retrieved few-shot demonstrations for low-resource languages
- Evidence anchors: Paper demonstrates CoD's superiority to few-shot demonstrations for low-resource languages

### Mechanism 3
- Claim: Chaining dictionaries (not decomposing them) is crucial for CoD's success
- Mechanism: Chained dictionaries create a reasoning path that mirrors Chain-of-Thought, while decomposed dictionaries introduce redundancy and flatten the reasoning structure
- Core assumption: LLMs benefit from structured intermediate steps in prompting, similar to CoT reasoning
- Evidence anchors: Paper demonstrates that using non-chained decomposed multilingual dictionaries instead of CoD degrades results

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoD applies CoT principles to translation by structuring dictionary mappings as intermediate reasoning steps
  - Quick check question: How does CoD's chaining of dictionaries mirror CoT reasoning in mathematical problem-solving?

- Concept: In-context learning with demonstrations
  - Why needed here: CoD is compared against few-shot demonstrations, so understanding how demonstrations work in LLMs is essential
  - Quick check question: Why might few-shot demonstrations be less effective than dictionaries for low-resource languages?

- Concept: Neural Machine Translation (NMT) evaluation metrics
  - Why needed here: CoD's performance is measured using chrF++ on FLORES-200, so familiarity with MT evaluation is required
  - Quick check question: What does chrF++ measure, and why is it suitable for evaluating translation quality?

## Architecture Onboarding

- Component map: Source sentence -> Keyword extraction -> Multilingual dictionary retrieval -> Chained dictionary construction -> CoD prompt -> ChatGPT -> Translated sentence
- Critical path: 1) Extract keywords from source sentence, 2) Retrieve multilingual dictionary entries for each keyword, 3) Construct CoD prompt (chained dictionaries + translation prompt), 4) Send prompt to LLM, 5) Evaluate output using chrF++
- Design tradeoffs: CoD vs few-shot demonstrations (dictionaries easier to acquire but less flexible), Chained vs decomposed dictionaries (chaining provides structure but may introduce complexity), Auxiliary language selection (high-resource languages improve CoD but may bias results)
- Failure signatures: Degraded performance with decomposed dictionaries (redundancy issue), Inconsistent results across LLM versions (version sensitivity), No improvement for high-resource languages (baseline sufficiency)
- First 3 experiments: 1) Test CoD vs baseline ChatGPT on Serbian Cyrillic to verify 13x chrF++ improvement claim, 2) Compare chained vs decomposed dictionaries on same language to confirm chaining importance, 3) Test few-shot demonstrations vs CoD on low-resource language to validate superiority claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoD performance scale with larger multilingual dictionaries and auxiliary languages beyond French, German, and Bulgarian?
- Basis in paper: Paper uses only three auxiliary languages and mentions possibility of tuning this list for specific languages, but doesn't explore impact of adding more diverse auxiliary languages
- Why unresolved: Paper establishes chaining dictionaries is beneficial but doesn't investigate whether adding more auxiliary languages would yield further improvements
- What evidence would resolve it: Systematic experiments varying number and diversity of auxiliary languages, measuring chrF++ gains for different combinations

### Open Question 2
- Question: What is the mechanism by which chained dictionaries improve translation quality compared to bilingual dictionaries?
- Basis in paper: Paper demonstrates chained dictionaries outperform bilingual dictionaries but doesn't explain why chaining approach is more effective
- Why unresolved: While paper shows empirical superiority of chained dictionaries, it doesn't investigate whether this is due to better context preservation, reduced ambiguity, or other linguistic factors
- What evidence would resolve it: Analysis of model attention patterns, error type distribution, or ablation studies on different chaining strategies

### Open Question 3
- Question: How does CoD performance compare to supervised multilingual models on the same low-resource language pairs?
- Basis in paper: Paper acknowledges it doesn't directly compare to supervised models like NLLB and mentions current LLMs still lag behind supervised models, but doesn't provide quantitative comparison
- Why unresolved: Paper establishes CoD's effectiveness relative to baseline LLMs but doesn't benchmark against current state-of-the-art supervised approaches for these specific language pairs
- What evidence would resolve it: Direct comparison of CoD-augmented LLMs against supervised models like NLLB on same FLORES-200 test sets

## Limitations

- Dictionary quality and coverage uncertainties - paper doesn't specify which three languages were excluded from NLLB support or provide quality control validation
- Keyword extraction reliability concerns - paper mentions using ChatGPT for extraction but doesn't specify criteria or handle context-dependent words
- Version sensitivity issues - paper reports different scores across ChatGPT versions but doesn't explore version consistency or provide stability thresholds
- Baseline methodology gaps - paper compares against standard ChatGPT but doesn't specify if baseline uses any optimization

## Confidence

**High Confidence** (supported by strong evidence):
- CoD improves translation for low-resource languages with baseline chrF++ < 20
- Chaining dictionaries is more effective than using decomposed dictionaries
- CoD outperforms few-shot demonstrations for low-resource languages

**Medium Confidence** (supported by moderate evidence with some gaps):
- 13x improvement claim on English to Serbian Cyrillic (3.08 to 42.63 chrF++)
- General superiority of CoD over standard prompting across all tested languages

**Low Confidence** (supported by weak or absent evidence):
- Mechanism claims about why chaining works (CoT analogy)
- Dictionary quality assessment and coverage statistics
- Version stability and sensitivity analysis

## Next Checks

1. **Replication of Core Claims**: Reproduce the English to Serbian Cyrillic translation test to verify the 13x improvement claim (chrF++ from 3.08 to 42.63) using the same FLORES-200 test set and dictionary construction methodology.

2. **Ablation Study on Dictionary Construction**: Test CoD performance using (a) chained dictionaries, (b) decomposed dictionaries, and (c) no dictionaries on a low-resource language to confirm that chaining specifically drives the performance improvement.

3. **Version Sensitivity Analysis**: Compare CoD performance across multiple ChatGPT versions (3.5, 4.0, 5.0) on the same low-resource language pairs to quantify performance stability and identify potential version-dependent effects.