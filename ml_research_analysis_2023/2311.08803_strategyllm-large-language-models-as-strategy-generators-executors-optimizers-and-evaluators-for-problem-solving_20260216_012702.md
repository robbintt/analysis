---
ver: rpa2
title: 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers,
  and Evaluators for Problem Solving'
arxiv_id: '2311.08803'
source_url: https://arxiv.org/abs/2311.08803
tags:
- answer
- strategy
- question
- task
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StrategyLLM introduces a multi-agent LLM framework that generates,
  executes, optimizes, and evaluates generalizable strategies for few-shot prompting.
  By deriving general strategies from specific instances and applying them consistently,
  it improves both generalizability and consistency compared to instance-specific
  solutions.
---

# StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving

## Quick Facts
- arXiv ID: 2311.08803
- Source URL: https://arxiv.org/abs/2311.08803
- Reference count: 40
- Key outcome: Improves math reasoning accuracy from 34.2% to 38.8% and symbolic reasoning from 30.0% to 79.2%

## Executive Summary
StrategyLLM introduces a multi-agent LLM framework that automatically generates, executes, optimizes, and evaluates generalizable strategies for few-shot prompting. The system derives general problem-solving strategies from specific task examples through inductive reasoning, then applies these strategies consistently across instances using deductive reasoning. By iteratively refining strategies based on execution results, StrategyLLM produces strategy-based few-shot prompts that improve both generalizability and consistency compared to instance-specific solutions.

## Method Summary
StrategyLLM employs a four-agent LLM framework: strategy generator creates initial strategies from task examples, strategy executor applies strategies and measures accuracy, strategy optimizer refines strategies based on failures, and strategy evaluator validates candidate strategies on held-out data. The framework automatically generates strategy-based few-shot prompts without human-annotated reasoning processes. It operates on 13 datasets across four reasoning tasks (math, commonsense, algorithmic, and symbolic), using GPT-3.5 or GPT-4 as the backend LLM.

## Key Results
- Math reasoning accuracy improves from 34.2% to 38.8% compared to CoT-SC baseline
- Symbolic reasoning accuracy improves from 30.0% to 79.2%
- StrategyLLM is robust to different LLMs and example groups
- Generated strategies are complementary for complex tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: StrategyLLM's multi-agent collaboration framework improves generalizability by formulating general problem-solving strategies from specific instances.
- Mechanism: The strategy generator uses inductive reasoning to derive general strategies from specific task examples, allowing solutions to be applicable across different instances rather than being instance-specific.
- Core assumption: LLMs can effectively identify patterns across diverse examples and abstract them into general strategies.
- Evidence anchors: [abstract] "StrategyLLM introduces a multi-agent LLM framework that generates, executes, optimizes, and evaluates generalizable strategies for few-shot prompting"; [section] "The inductive reasoning process enhances generalizability by formulating general problem-solving strategies"; [corpus] Weak evidence - no direct citations of similar multi-agent frameworks in corpus
- Break condition: If the strategy generator fails to identify meaningful patterns across examples, or if examples are too diverse to support generalization.

### Mechanism 2
- Claim: StrategyLLM improves consistency by producing consistent solutions using derived strategies across different instances.
- Mechanism: The deductive reasoning process applies general strategies consistently to particular examples, ensuring task-level consistency in reasoning steps across few-shot examples.
- Core assumption: Once a strategy is derived, LLMs can apply it consistently to generate similar solution structures across different problems.
- Evidence anchors: [abstract] "StrategyLLM employs four LLM-based agents... working together to generate, evaluate, and select promising strategies for a given task"; [section] "The deductive reasoning process improves consistency by producing consistent solutions using a given strategy"; [corpus] No direct evidence in corpus of consistency improvements from strategy-based approaches
- Break condition: If the strategy executor fails to apply the strategy correctly, or if the strategy is too complex for consistent application.

### Mechanism 3
- Claim: StrategyLLM's optimization loop refines strategies based on execution results to improve performance.
- Mechanism: The strategy optimizer analyzes execution results to identify why solutions fail, provides suggestions for improvement, and modifies strategies accordingly through iterative refinement.
- Core assumption: LLMs can analyze their own reasoning failures and generate meaningful improvements to strategies.
- Evidence anchors: [section] "The strategy optimizer, represented as O : (E, d, st, Rst) → sto, optimize the strategy st according to its execution result Rst to obtain the updated strategy sto"; [section] "Table 4 presents the optimization process for three distinct datasets... it is evident that the strategy optimizer plays a vital role in obtaining more qualified strategies and superior strategies"; [corpus] No direct evidence in corpus of self-improving optimization loops for LLM strategies
- Break condition: If the optimizer cannot identify meaningful failure patterns, or if iterative improvements plateau.

## Foundational Learning

- Concept: Inductive reasoning - deriving general principles from specific examples
  - Why needed here: StrategyLLM's strategy generator must abstract general problem-solving approaches from specific task instances
  - Quick check question: If given three examples of solving linear equations, what general steps could be abstracted?

- Concept: Deductive reasoning - applying general principles to specific cases
  - Why needed here: StrategyLLM's strategy executor must apply derived strategies consistently to new examples
  - Quick check question: Given a general strategy for solving equations, how would you apply it to a specific system of equations?

- Concept: Multi-agent collaboration systems
  - Why needed here: StrategyLLM coordinates four specialized LLM agents to work together on strategy generation and evaluation
  - Quick check question: What are the responsibilities of each agent in a multi-agent system for strategy development?

## Architecture Onboarding

- Component map: Strategy Generator -> Strategy Executor -> Strategy Optimizer -> Strategy Executor (iterative) -> Cache -> Strategy Evaluator -> Inference
- Critical path: Generator → Executor → Optimizer → Executor (iterative) → Cache → Evaluator → Inference
- Design tradeoffs:
  - Strategy diversity vs. quality (more strategies increase coverage but may reduce average quality)
  - Execution accuracy threshold (too high may limit candidate strategies, too low may include poor strategies)
  - Number of iterations (more iterations improve quality but increase computational cost)
- Failure signatures:
  - No qualified strategies after optimization → likely need lower accuracy threshold or more examples
  - All strategies perform similarly → may indicate examples are too homogeneous or strategies are too similar
  - Optimizer fails to improve strategies → may need better failure analysis or different optimization approach
- First 3 experiments:
  1. Run StrategyLLM with minimal example set (3-4 examples) on a simple task to verify basic functionality
  2. Test strategy optimization loop by intentionally providing poor initial strategies to see if optimizer can improve them
  3. Evaluate transferability by applying optimal strategies from one LLM to another to measure robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does StrategyLLM's performance scale with the number of task examples used for strategy generation, execution, and optimization?
- Basis in paper: [explicit] The paper states "Typically, the number of task examples |E| ranges from 3 to 8" and "The validation set size is 100 for all the datasets"
- Why unresolved: The paper only experiments with a fixed range of 3-8 examples and does not explore the impact of using more or fewer examples on performance.
- What evidence would resolve it: Experiments systematically varying the number of task examples from 1 to 20+ and measuring the resulting performance to identify the optimal number of examples.

### Open Question 2
- Question: How does the performance of StrategyLLM compare to human-designed strategies for the same tasks?
- Basis in paper: [inferred] The paper demonstrates StrategyLLM can generate effective strategies automatically, but does not compare these to strategies designed by human experts.
- Why unresolved: The paper focuses on demonstrating the framework's ability to generate strategies without human intervention, but does not benchmark against human-designed strategies.
- What evidence would resolve it: Comparing StrategyLLM's generated strategies and performance against strategies designed by domain experts for each task.

### Open Question 3
- Question: What is the impact of different optimization strategies for improving unqualified strategies in StrategyLLM?
- Basis in paper: [explicit] The paper mentions "the optimizer will optimize unqualified strategies based on their execution results" but does not detail the specific optimization methods used.
- Why unresolved: The paper only briefly mentions strategy optimization without describing the specific techniques employed or comparing different optimization approaches.
- What evidence would resolve it: Experimenting with different optimization methods (e.g., gradient-based, evolutionary, or rule-based) and comparing their effectiveness in improving unqualified strategies.

## Limitations

- The strategy generator's ability to reliably abstract generalizable strategies from diverse examples remains an open question, as the paper provides limited qualitative analysis of the strategies generated.
- The robustness claims across different LLMs and example groups are based on limited ablation studies without systematic hyperparameter sweeps.
- The optimization mechanism's effectiveness in identifying and correcting specific reasoning failures is not fully validated, as the analysis focuses on final outcomes rather than the optimization process itself.

## Confidence

- **High**: The multi-agent framework architecture is clearly defined and experimentally validated for basic functionality.
- **Medium**: Claims about improved generalizability and consistency are supported by quantitative results but lack detailed qualitative validation.
- **Low**: Claims about the optimizer's ability to meaningfully improve strategies through failure analysis need more rigorous validation.

## Next Checks

1. **Strategy quality audit**: Manually inspect and categorize a sample of generated strategies to verify they capture generalizable patterns rather than being instance-specific or overly vague.
2. **Cross-LLM transferability test**: Systematically evaluate optimal strategies from one LLM (e.g., GPT-4) on different LLMs (e.g., Claude, LLaMA) to quantify the claimed robustness more rigorously.
3. **Optimizer failure analysis**: Track specific reasoning failures identified by the optimizer and verify that suggested improvements directly address these failures, measuring the correlation between failure identification and strategy improvement.