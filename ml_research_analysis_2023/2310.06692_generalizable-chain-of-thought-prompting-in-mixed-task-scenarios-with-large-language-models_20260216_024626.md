---
ver: rpa2
title: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large
  Language Models
arxiv_id: '2310.06692'
source_url: https://arxiv.org/abs/2310.06692
tags:
- coin
- step
- scenario
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a generalizable chain-of-thought prompting method
  for mixed-task scenarios where the input question type is unknown. The method first
  categorizes the question type, then automatically constructs demonstrations from
  the corresponding data pool.
---

# Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models

## Quick Facts
- arXiv ID: 2310.06692
- Source URL: https://arxiv.org/abs/2310.06692
- Authors: 
- Reference count: 40
- Key outcome: Achieves state-of-the-art 93.7% on SVAMP and 93.6% on GSM8K without in-context demonstrations from those datasets

## Executive Summary
This paper proposes Meta-CoT, a generalizable chain-of-thought prompting method for mixed-task scenarios where the input question type is unknown. The method first categorizes questions into <Category, Form> pairs (e.g., <Arithmetic, SAQ>) and then automatically constructs diverse demonstrations from the corresponding data pool. Experiments on 10 public reasoning tasks and 5 out-of-distribution datasets demonstrate superior generalization capabilities compared to previous CoT methods.

## Method Summary
Meta-CoT partitions mixed questions based on both category (arithmetic, commonsense, symbolic) and form (short-answer, multiple-choice, yes-no). For a given input question, the method identifies its scenario, uses k-means clustering on sentence embeddings to select representative demonstrations from the matched category/form group, and generates rationales with Zero-Shot-CoT. The demonstrations are then appended to the prompt for the LLM to derive the final answer. The method maintains a data pool of mixed questions from multiple tasks for ICL demonstrations.

## Key Results
- Achieves state-of-the-art 93.7% accuracy on SVAMP dataset
- Reaches 93.6% accuracy on GSM8K without using any GSM8K demonstrations
- Demonstrates superior generalization to 5 out-of-distribution datasets compared to baseline CoT methods

## Why This Works (Mechanism)

### Mechanism 1
The <Category, Form> partitioning scheme enables Meta-CoT to generalize across unknown question types by capturing the key discriminative features of reasoning tasks. Questions from the same category and form share similar reasoning patterns that can be captured through in-context learning, even without knowing the exact task name.

### Mechanism 2
Diversity-based demonstration selection through k-means clustering improves performance over random or similarity-based sampling by ensuring coverage of the full reasoning space within a category/form. Questions within the same category/form but different clusters represent distinct reasoning patterns that together provide comprehensive guidance.

### Mechanism 3
Using only the question (without CoT or answer) for scenario identification maintains both high accuracy and generality by avoiding introducing task-specific knowledge that might bias the classification. The question text alone contains sufficient discriminative information about category and form.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Meta-CoT builds on the CoT framework to generate intermediate reasoning steps that serve as rationale for deriving answers
  - Quick check question: What is the key difference between General Zero-Shot-CoT and Specific Few-Shot-CoT?

- Concept: In-context learning
  - Why needed here: Meta-CoT relies on ICL demonstrations to guide the model's reasoning without fine-tuning
  - Quick check question: How does Meta-CoT construct its ICL demonstrations differently from Auto-CoT?

- Concept: Clustering for diversity
  - Why needed here: K-means clustering ensures the selected demonstrations cover the full range of reasoning patterns within a category/form
  - Quick check question: Why might random sampling fail to capture the diversity of reasoning patterns?

## Architecture Onboarding

- Component map: Input question -> Scenario identification -> Demonstration selection -> Answer derivation
- Critical path: Input question → Scenario identification → Demonstration selection → Answer derivation
- Design tradeoffs:
  - Partitioning scheme: <Category, Form> vs task-based vs form-only
  - Clustering vs random vs similarity-based demonstration selection
  - Question-only vs question+CoT vs question+answer for scenario identification
- Failure signatures:
  - Low scenario identification accuracy (below 95%)
  - Performance degradation compared to task-specific methods
  - Unstable results across different runs or question types
- First 3 experiments:
  1. Verify scenario identification accuracy with different input formats (Q, Q+A, Q+CoT, Q+CoT+A)
  2. Compare demonstration selection methods (random, similarity-based, clustering-based)
  3. Test end-to-end performance on a small subset of tasks with known vs unknown question types

## Open Questions the Paper Calls Out

### Open Question 1
How does Meta-CoT's performance scale when applied to mixed-task scenarios with 20+ different reasoning tasks compared to the current 10 tasks? The paper focuses on demonstrating effectiveness with current dataset size but doesn't investigate performance changes with significantly larger task pools.

### Open Question 2
What is the theoretical upper bound on accuracy when the gold scenario is always provided to the model? While the paper shows minimal gains with gold scenario, it doesn't explore whether this represents a hard ceiling or if better demonstration construction could yield larger improvements.

### Open Question 3
How does Meta-CoT's generalizability change when the mixed data pool includes tasks from domains completely outside mathematics and commonsense reasoning (e.g., medical diagnosis, legal reasoning)? The current work focuses on arithmetic, commonsense, and symbolic reasoning tasks, but doesn't test truly out-of-distribution domains.

## Limitations

- The paper lacks direct experimental evidence supporting the superiority of the <Category, Form> partitioning scheme over simpler alternatives.
- Claims about diversity-based demonstration selection are weakly supported without direct comparison of k-means clustering against random sampling.
- The 99% scenario identification accuracy may reflect overfitting to the specific datasets rather than true generalization.

## Confidence

**High confidence**: The core methodology is well-specified and reproducible with clearly defined experimental setup and implementation details.

**Medium confidence**: State-of-the-art results appear credible but lack direct comparisons with other CoT methods on the same datasets.

**Low confidence**: Claims about superiority of specific design choices lack adequate support from ablation studies or controlled comparisons.

## Next Checks

1. Conduct ablation study on partitioning scheme by testing Meta-CoT with form-only, category-only, and task-based alternatives to isolate the contribution of the <Category, Form> approach.

2. Compare k-means clustering against random sampling and similarity-based sampling to determine if diversity-based selection truly improves performance.

3. Evaluate Meta-CoT on questions from completely new domains or with novel reasoning patterns not present in the training data to test true generalization capabilities.