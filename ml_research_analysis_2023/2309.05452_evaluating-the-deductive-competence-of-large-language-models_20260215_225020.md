---
ver: rpa2
title: Evaluating the Deductive Competence of Large Language Models
arxiv_id: '2309.05452'
source_url: https://arxiv.org/abs/2309.05452
tags:
- social
- rules
- problems
- performance
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the deductive reasoning capabilities of several
  large language models (LLMs) using the Wason selection task, a classic cognitive
  science paradigm. The researchers examined how different presentation formats (classic,
  front, back, both) and content types (arbitrary, shuffled, realistic social rules,
  realistic non-social rules) affect performance across four models (Guanaco, MPT,
  BLOOM, Falcon).
---

# Evaluating the Deductive Competence of Large Language Models

## Quick Facts
- arXiv ID: 2309.05452
- Source URL: https://arxiv.org/abs/2309.05452
- Authors: 
- Reference count: 13
- Large language models show limited deductive reasoning capabilities on the Wason selection task despite content familiarity or presentation format variations

## Executive Summary
This study evaluates the deductive reasoning capabilities of four large language models (Guanaco, MPT, BLOOM, Falcon) using the Wason selection task, a classic cognitive science paradigm. The researchers systematically varied content types (arbitrary, shuffled, realistic social rules, realistic non-social rules) and presentation formats (classic, front, back, both) across 350 problems. Results demonstrate that while realistic content and social rules provide some performance benefits, overall deductive reasoning remains limited across all conditions. The findings reveal that LLMs exhibit unique reasoning biases not predicted by human performance data, particularly in their tendency to select antecedent cards differently than human subjects. Performance was surprisingly consistent across models despite architectural differences, suggesting fundamental limitations in current LLM deductive reasoning capabilities.

## Method Summary
The study employed the Wason selection task, presenting 350 problems across four LLM architectures (7B-parameter Guanaco, MPT, BLOOM, and Falcon models). Problems varied by content type (arbitrary, shuffled, realistic social rules, realistic non-social rules) and presentation format (classic, front, back, both). Models were evaluated using zero-shot inference on an A100 GPU with Python 3.10. Performance was measured using domain conditional PMI scoring, which calculates P(yi|x)/P(yi|baseline) for each answer choice. Statistical analysis employed mixed-effects modeling with random effects for item sets to account for the hierarchical structure of matched problem sets. The evaluation pipeline included prompt generation, model inference, answer scoring, and statistical analysis.

## Key Results
- Overall performance remains low across all models and conditions, regardless of content type or presentation format
- Realistic social content improves performance compared to arbitrary and non-social rules, but the benefit is smaller than observed in human subjects
- No presentation format consistently improves performance, with only limited interactions between content type and presentation
- LLMs exhibit unique reasoning biases, particularly in selecting antecedent cards at rates different from human subjects
- Performance is surprisingly consistent across four distinct LLM architectures despite differences in training data and model design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with deductive reasoning even when presented with familiar content because their reasoning biases diverge from human expectations.
- Mechanism: LLMs rely on learned statistical patterns from training corpora, which may not capture the logical structure required for deductive tasks. This leads to unique reasoning biases that are only partially predicted by human performance data.
- Core assumption: The training data contains sufficient logical reasoning examples to support deductive tasks.
- Evidence anchors:
  - [abstract] "we find that performance interacts with presentation format and content in unexpected ways that differ from human performance"
  - [section] "Overall low performance is particularly surprising for realistic social and non-social rules as the relationships for solving these problems are plausibly available in the training data of LLMs"
  - [corpus] Weak - neighbor papers focus on similar deductive reasoning benchmarks but do not directly address corpus content sufficiency
- Break condition: If the training corpus lacks sufficient examples of logical reasoning patterns, or if the models are fine-tuned specifically on deductive reasoning tasks.

### Mechanism 2
- Claim: Different presentation formats affect LLM performance in ways that are inconsistent with human reasoning patterns.
- Mechanism: LLMs may be sensitive to surface-level formatting cues rather than the underlying logical structure of the problem, leading to performance variations that do not align with human reasoning.
- Core assumption: LLMs process formatted text in ways that can be manipulated to improve reasoning performance.
- Evidence anchors:
  - [abstract] "we do find performance differences between conditions; however, they do not improve overall performance"
  - [section] "we do find effects for different presentation formats; however, they do not improve performance"
  - [corpus] Weak - corpus neighbors discuss presentation impacts but do not specifically address format inconsistencies with human reasoning
- Break condition: If LLMs are fine-tuned to recognize logical structure independent of presentation format.

### Mechanism 3
- Claim: LLMs exhibit a systematic bias in selecting antecedent cards at rates different from human subjects.
- Mechanism: The model's probability distribution over answer choices may be skewed toward certain card selections based on training data patterns, rather than logical validity.
- Core assumption: The model's output probabilities reflect learned biases rather than deductive reasoning.
- Evidence anchors:
  - [abstract] "LLMs exhibited unique reasoning biases not fully predicted by human performance data, particularly in their tendency to select antecedent cards differently than human subjects"
  - [section] "LLMs do not pick antecedent cards at the same rate that human subjects do"
  - [corpus] Weak - corpus neighbors do not discuss card selection biases specifically
- Break condition: If the model's output probabilities are calibrated to match human-like reasoning patterns.

## Foundational Learning

- Concept: Conditional inference (modus ponens, modus tollens, etc.)
  - Why needed here: The Wason selection task fundamentally tests understanding of conditional logic, which is the basis for evaluating deductive competence.
  - Quick check question: Can you explain the difference between modus ponens and modus tollens in the context of the Wason task?

- Concept: Mixed-effects modeling
  - Why needed here: The experimental design involves repeated measures across different problem sets and conditions, requiring statistical methods that account for hierarchical data structure.
  - Quick check question: Why is mixed-effects modeling preferred over traditional ANOVA for analyzing performance across matched problem sets?

- Concept: Domain conditional PMI scoring
  - Why needed here: This metric is used to evaluate model performance by measuring how much information a particular instruction domain provides about a particular answer, which is crucial for fair comparison across conditions.
  - Quick check question: How does domain conditional PMI differ from simple accuracy scoring, and why is it more appropriate for this type of task?

## Architecture Onboarding

- Component map: Problem generation → Prompt engineering → Model inference (Guanaco, MPT, BLOOM, Falcon) → Domain conditional PMI scoring → Mixed-effects statistical analysis → Result interpretation
- Critical path: Prompt generation → Model inference → Answer scoring (domain conditional PMI) → Statistical analysis → Result interpretation
- Design tradeoffs: Using zero-shot inference preserves the generality of the evaluation but may limit performance compared to fine-tuned models. The choice of scoring metric (domain conditional PMI) provides nuanced evaluation but may be less interpretable than simple accuracy.
- Failure signatures: Consistent performance across models despite architectural differences suggests a fundamental limitation rather than model-specific issues. Unexpected interactions between content type and presentation format indicate reasoning biases not aligned with human performance.
- First 3 experiments:
  1. Test each model on a subset of problems with different content types to verify the basic performance pattern before full-scale evaluation.
  2. Compare domain conditional PMI scoring with simple accuracy scoring on a small sample to validate the chosen metric.
  3. Run a pilot analysis with mixed-effects modeling on a subset of data to ensure the statistical approach is appropriate before full analysis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training differences in LLMs lead to the observed unique reasoning biases that are not fully predicted from human reasoning performance?
- Basis in paper: [explicit] The authors note that despite differences in training datasets and model architectures, the interaction results are consistent across different LLMs and are not consistent with human reasoning performance.
- Why unresolved: The paper identifies the presence of these biases but does not investigate their underlying causes or mechanisms.
- What evidence would resolve it: Systematic analysis of how different architectural choices (attention mechanisms, layer configurations, training objectives) and training data distributions correlate with specific reasoning biases across multiple models.

### Open Question 2
- Question: What specific features of social rules make them easier for humans to solve but only moderately easier for LLMs compared to non-social rules?
- Basis in paper: [explicit] The authors find that while social content does benefit LLM performance compared to non-social content, the magnitude of this benefit is much smaller than in human performance data.
- Why unresolved: The paper demonstrates the performance difference but does not analyze what aspects of social rules (cost-benefit structure, moral implications, social norms) drive the human advantage.
- What evidence would resolve it: Controlled experiments manipulating specific social rule features (e.g., presence of costs/benefits, moral valence, familiarity of social context) while measuring both LLM and human performance.

### Open Question 3
- Question: What presentation format modifications could meaningfully improve LLM performance on deductive reasoning tasks beyond the limited effects observed in this study?
- Basis in paper: [inferred] The authors tested multiple presentation formats but found only limited interactions with content type, suggesting that current format engineering approaches are insufficient.
- Why unresolved: The paper identifies that format matters but does not explore what alternative formatting strategies (beyond the tested variations) might be more effective.
- What evidence would resolve it: Systematic exploration of alternative prompt engineering strategies, including multi-step reasoning prompts, chain-of-thought approaches, or hybrid symbolic-linguistic representations.

## Limitations

- Zero-shot inference approach may underestimate model potential with appropriate training or fine-tuning
- 7B-parameter models tested may not generalize to larger or more capable architectures
- Wason task performance may not fully translate to broader deductive competence in real-world applications
- The study did not exhaustively explore the space of possible formatting approaches that might improve performance

## Confidence

**High confidence**: Overall low performance across all models and conditions, regardless of content type or presentation format.

**Medium confidence**: Claim about unique reasoning biases that differ from human performance, particularly in selecting antecedent cards.

**Low confidence**: Assertion that format engineering cannot improve performance, as the space of possible formatting approaches was not exhaustively explored.

## Next Checks

1. Test the same models on complementary deductive reasoning benchmarks (e.g., syllogism reasoning or logical inference tasks) to determine whether Wason task performance patterns generalize to other forms of deductive reasoning.

2. Train a subset of models on deductive reasoning tasks and re-evaluate performance on the Wason task to assess whether the observed limitations stem from architectural constraints versus insufficient training exposure.

3. Conduct the identical experiment with human subjects using the same problem sets and presentation formats to establish whether the observed LLM reasoning patterns truly diverge from human reasoning or reflect measurement artifacts.