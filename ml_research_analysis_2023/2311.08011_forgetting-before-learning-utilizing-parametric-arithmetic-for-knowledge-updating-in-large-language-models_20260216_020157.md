---
ver: rpa2
title: 'Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge
  Updating in Large Language Models'
arxiv_id: '2311.08011'
source_url: https://arxiv.org/abs/2311.08011
tags:
- knowledge
- fine-tuning
- parameters
- learning
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new paradigm for knowledge updating in large
  language models called F-Learning (Forgetting before Learning). The key idea is
  to first subtract the parameters of a fine-tuned model to forget old knowledge,
  then fine-tune again to learn new knowledge.
---

# Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models

## Quick Facts
- arXiv ID: 2311.08011
- Source URL: https://arxiv.org/abs/2311.08011
- Reference count: 11
- Key outcome: F-Learning (Forgetting before Learning) improves knowledge updating performance by first subtracting parameters of a fine-tuned model to forget old knowledge, then fine-tuning again to learn new knowledge.

## Executive Summary
This paper introduces F-Learning, a novel paradigm for knowledge updating in large language models that addresses the challenge of conflicting old and new information during sequential fine-tuning. The approach employs parametric arithmetic to first "forget" old knowledge by subtracting fine-tuned parameters from the base model, then learn new knowledge through subsequent fine-tuning. Experiments on two datasets demonstrate that F-Learning outperforms direct fine-tuning approaches for both full fine-tuning and LoRA-based fine-tuning, with LoRA-based forgetting achieving comparable results at lower computational cost.

## Method Summary
F-Learning is a two-stage fine-tuning approach that first removes old knowledge representations by subtracting fine-tuned parameters from the base model, then learns new knowledge through additional fine-tuning. The method defines incremental parameters (θ△) as the difference between fine-tuned and original parameters, which are then subtracted from the base model to forget old knowledge. After this subtraction, the model is fine-tuned on new knowledge data. The approach is tested with both full fine-tuning and LoRA (Low-Rank Adaptation) parameter subtraction, showing that LoRA-based forgetting can achieve similar effectiveness to full fine-tuning while being more computationally efficient.

## Key Results
- F-Learning improves knowledge updating performance compared to direct fine-tuning on both ZsRE and COUNTER FACT datasets
- LoRA-based forgetting achieves similar effects to full fine-tuning parameter subtraction with significantly lower computational cost
- The method shows consistent improvements across three evaluation metrics: Reliabilty, Generality, and Locality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subtracting fine-tuned parameters removes old knowledge representations from the model.
- Mechanism: When a model is fine-tuned on new data, parameter changes encode the new task. By subtracting these changes from the original parameters, the corresponding knowledge representation is removed.
- Core assumption: Parameter changes during fine-tuning directly correspond to knowledge representation changes in the model.
- Evidence anchors:
  - [abstract]: "we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge."
  - [section 4.1]: "we define the incremental parameters as knowledge parameters θ△, calculated as follows: θ△ = FT{θ, K} − θ"
  - [corpus]: Weak evidence - no direct corpus support for parameter subtraction as knowledge removal mechanism.
- Break condition: If parameter changes during fine-tuning don't cleanly represent knowledge (e.g., if changes are distributed across many parameters or entangled with other knowledge).

### Mechanism 2
- Claim: Learning new knowledge after forgetting old knowledge reduces interference between conflicting information.
- Mechanism: By first removing old knowledge representations, the model can learn new information without having to resolve conflicts between old and new knowledge.
- Core assumption: Knowledge conflicts during learning impair the model's ability to acquire new information.
- Evidence anchors:
  - [abstract]: "Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge."
  - [introduction]: "If the original cognition and knowledge are forgotten, then the new knowledge to be learnt will not conflict with the original cognition and knowledge, which makes it better to learn and absorb the new knowledge."
  - [corpus]: Moderate evidence - related work on catastrophic forgetting supports this assumption.
- Break condition: If the model can effectively resolve knowledge conflicts during learning, or if the old and new knowledge don't actually conflict.

### Mechanism 3
- Claim: LoRA parameter subtraction can achieve similar forgetting effects to full fine-tuning parameter subtraction with less computational cost.
- Mechanism: LoRA introduces a small number of trainable parameters. Subtracting these LoRA parameters from the original model achieves partial forgetting, which may be sufficient for knowledge updating while being more computationally efficient.
- Core assumption: LoRA parameters capture a significant portion of the knowledge changes during fine-tuning.
- Evidence anchors:
  - [abstract]: "forgetting old knowledge by subtracting the parameters of LoRA can yield a similar effect to subtracting the parameters of full fine-tuning, and occasionally even surpass it significantly."
  - [section 6]: "we find that forgetting by subtracting the parameters of LoRA can achieve the approximate effect of subtracting the parameters of full fine-tuning"
  - [corpus]: Weak evidence - no direct corpus support for LoRA parameter effectiveness in forgetting.
- Break condition: If LoRA parameters don't capture enough of the knowledge changes, or if full fine-tuning is required for sufficient forgetting.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: F-Learning builds on SFT as the mechanism for both forgetting and learning phases.
  - Quick check question: What is the difference between SFT and prompt tuning in terms of parameter modification?

- Concept: Catastrophic forgetting
  - Why needed here: F-Learning directly addresses this problem by explicitly forgetting old knowledge before learning new knowledge.
  - Quick check question: How does catastrophic forgetting manifest in LLMs during sequential task learning?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: LoRA is a PEFT method used in F-Learning to reduce computational cost.
  - Quick check question: What is the key difference between LoRA and full fine-tuning in terms of parameter modification?

## Architecture Onboarding

- Component map: Base LLM (e.g., LLAMA2-7B) → F-Learning wrapper → Output model
- Critical path: Load base model → Fine-tune on old knowledge → Subtract parameters → Fine-tune on new knowledge → Evaluate
- Design tradeoffs: Computational cost vs. forgetting effectiveness (full fine-tuning vs. LoRA)
- Failure signatures: Decreased performance on unrelated tasks (over-forgetting), no improvement in knowledge updating (insufficient forgetting)
- First 3 experiments:
  1. Full fine-tuning baseline on ZsRE dataset
  2. LoRA baseline on ZsRE dataset
  3. F-Learning with LoRA forgetting and full fine-tuning learning on ZsRE dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of F-Learning vary across different model architectures and sizes?
- Basis in paper: [inferred] The paper only experiments with LLAMA2-7B, but the method could be applicable to other models.
- Why unresolved: The paper does not test F-Learning on other model architectures or sizes to determine its generalizability.
- What evidence would resolve it: Conducting experiments with F-Learning on a diverse set of model architectures and sizes, and comparing the results to determine if the method's effectiveness is consistent across different models.

### Open Question 2
- Question: What is the optimal balance between forgetting old knowledge and learning new knowledge in F-Learning?
- Basis in paper: [explicit] The paper mentions a hyperparameter λ to control the rate of forgetting, but does not provide guidance on how to set this parameter optimally.
- Why unresolved: The paper does not explore the impact of different λ values on the performance of F-Learning, nor does it provide a systematic approach to setting this parameter.
- What evidence would resolve it: Conducting experiments with different λ values and analyzing the trade-off between forgetting old knowledge and learning new knowledge to determine the optimal balance.

### Open Question 3
- Question: How does F-Learning perform on tasks beyond knowledge updating, such as domain adaptation or style transfer?
- Basis in paper: [inferred] The paper focuses on knowledge updating, but the method could potentially be applied to other tasks that involve updating or modifying model parameters.
- Why unresolved: The paper does not explore the application of F-Learning to other tasks beyond knowledge updating.
- What evidence would resolve it: Conducting experiments with F-Learning on various tasks that involve updating or modifying model parameters, and comparing the results to determine if the method is effective in these contexts.

## Limitations
- The approach's effectiveness may be limited to specific types of knowledge and may not generalize to all knowledge domains
- The computational benefits of LoRA-based forgetting need more extensive validation across different model sizes and task complexities
- The fundamental assumption that parameter subtraction cleanly removes specific knowledge representations lacks direct empirical support

## Confidence
- **F-Learning improves knowledge updating performance (High)**: The experimental results on two datasets with multiple metrics provide strong evidence for this claim. The consistent improvements across different metrics and datasets increase confidence.
- **LoRA-based forgetting achieves similar effects to full fine-tuning (Medium)**: While the paper shows promising results, the comparison is limited to specific datasets and model configurations. More extensive testing across different scenarios would increase confidence.
- **Parameter subtraction cleanly removes old knowledge (Medium)**: This is a fundamental assumption of the approach, but the paper doesn't provide direct evidence that parameter subtraction actually removes specific knowledge representations rather than just disrupting overall model performance.

## Next Checks
1. **Cross-domain validation**: Test F-Learning on a diverse set of knowledge domains beyond factual knowledge (e.g., procedural knowledge, common sense reasoning) to assess generalizability.

2. **Knowledge persistence analysis**: Conduct ablation studies to determine which types of knowledge are most and least affected by parameter subtraction, and whether certain knowledge representations are more resilient to the forgetting process.

3. **Long-term stability evaluation**: Evaluate the stability of F-Learning updates over multiple knowledge update cycles to ensure that repeated application doesn't lead to catastrophic forgetting or model degradation over time.