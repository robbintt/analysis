---
ver: rpa2
title: Efficient Interpretable Nonlinear Modeling for Multiple Time Series
arxiv_id: '2309.17154'
source_url: https://arxiv.org/abs/2309.17154
tags:
- nonlinear
- time
- series
- linear
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying interpretable nonlinear
  dependencies among multiple time series by proposing a model that assumes the time
  series are generated through a linear vector autoregressive (VAR) process in a latent
  space, followed by component-wise invertible nonlinear mappings. The method combines
  a linear VAR process in a latent space with per-sensor invertible neural networks
  to model nonlinearities, and enforces sparsity on the VAR coefficients to reflect
  parsimonious dependencies.
---

# Efficient Interpretable Nonlinear Modeling for Multiple Time Series

## Quick Facts
- arXiv ID: 2309.17154
- Source URL: https://arxiv.org/abs/2309.17154
- Reference count: 33
- This paper proposes a method for identifying interpretable nonlinear dependencies among multiple time series by combining a linear VAR process in a latent space with per-sensor invertible neural networks to model nonlinearities.

## Executive Summary
This paper addresses the problem of identifying interpretable nonlinear dependencies among multiple time series by proposing a model that assumes the time series are generated through a linear vector autoregressive (VAR) process in a latent space, followed by component-wise invertible nonlinear mappings. The method combines a linear VAR process in a latent space with per-sensor invertible neural networks to model nonlinearities, and enforces sparsity on the VAR coefficients to reflect parsimonious dependencies. Two formulations are presented: one based on explicit inversion of the nonlinear functions (formulation A), and another based on minimizing prediction error in the latent space (formulation B), which is computationally more efficient. Experiments on synthetic and real data show that the proposed algorithms outperform linear VAR and deep learning-based methods in terms of both prediction accuracy (NMSE) and topology identification (AUROC), with formulation B achieving superior performance in identifying the underlying graph structure while maintaining computational efficiency.

## Method Summary
The proposed method models multiple time series as generated by a linear VAR process in a latent space, followed by component-wise invertible nonlinear mappings. It uses two formulations: formulation A explicitly inverts the nonlinear functions, while formulation B minimizes prediction error in the latent space under Lipschitz continuity assumptions. The method enforces sparsity on VAR coefficients via Lasso penalties to yield interpretable dependency graphs. Training alternates between updating VAR coefficients (proximal step) and INN parameters (gradient step), with formulation B offering computational advantages by avoiding explicit inversion.

## Key Results
- The proposed algorithms outperform linear VAR and deep learning-based methods in terms of prediction accuracy (NMSE) and topology identification (AUROC).
- Formulation B achieves superior performance in identifying the underlying graph structure while maintaining computational efficiency.
- Experiments on synthetic and real data demonstrate the effectiveness of the proposed method in modeling nonlinear dependencies among multiple time series.

## Why This Works (Mechanism)

### Mechanism 1
The invertible neural network (INN) preserves a one-to-one mapping between latent and measurement spaces, enabling linear VAR inference in the latent space to reflect nonlinear dependencies in the original space. A bijective, component-wise invertible function maps sensor measurements to a latent space where dependencies are linear. Since the mapping is invertible, the causal structure (support of VAR coefficients) in the latent space directly corresponds to the nonlinear causal structure in the measurement space.

### Mechanism 2
Sparsity-inducing Lasso penalties on VAR coefficients yield parsimonious, interpretable dependency graphs even when the underlying dynamics are nonlinear. By penalizing the l1 norm of the VAR coefficients during optimization, many coefficients are driven to zero, producing a sparse adjacency matrix that highlights the strongest nonlinear causal links.

### Mechanism 3
Minimizing prediction error in the latent space (Formulation B) approximates minimizing it in the measurement space under Lipschitz continuity, enabling a computationally cheaper algorithm without explicit inversion. Theorem 1 establishes that the MSE in the measurement space is bounded by the Lipschitz constant squared times the MSE in the latent space. Thus, optimizing in the latent space indirectly controls the measurement-space error.

## Foundational Learning

- **Concept**: Vector Autoregression (VAR) causality and Granger causality equivalence.
  - **Why needed here**: The method builds on linear VAR causality in a latent space and claims it transfers to nonlinear causality in the measurement space via invertibility.
  - **Quick check question**: Under what conditions is VAR causality equivalent to Granger causality, and how does invertibility preserve this equivalence?

- **Concept**: Invertible neural networks (INNs) and monotonicity constraints.
  - **Why needed here**: The per-sensor nonlinear mapping must be invertible and monotonic to ensure a bijective correspondence between spaces.
  - **Quick check question**: What parameterization ensures invertibility and monotonicity for each component function?

- **Concept**: Proximal gradient descent and stochastic primal-dual updates.
  - **Why needed here**: The optimization problems involve non-differentiable Lasso penalties and constraints; these algorithms handle such terms efficiently.
  - **Quick check question**: How do proximal updates enforce sparsity while maintaining convergence in the presence of constraints?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Time series normalization -> Latent VAR module -> Linear VAR coefficients (sparse) -> INN module -> Component-wise invertible nonlinear mappings -> Training loop -> Two formulations (A: explicit inversion, B: latent-space minimization) -> Evaluation -> NMSE for prediction, AUROC for topology identification
- **Critical path**: 1. Initialize VAR coefficients (linear VAR estimate) and INN parameters (identity-like transform) 2. Alternate between updating VAR coefficients (proximal step) and INN parameters (gradient step) 3. Enforce constraints (sparsity, invertibility, Lipschitz bounds) 4. Monitor NMSE and AUROC on validation set
- **Design tradeoffs**: Formulation A: Higher accuracy but requires expensive inverse computation (bisection). Formulation B: Lower computational cost but relies on Lipschitz bound; may be looser in highly nonlinear regimes. Sparsity vs. expressiveness: Stronger Lasso penalties yield simpler graphs but risk missing weak dependencies.
- **Failure signatures**: Divergence in training -> Check Lipschitz constant or learning rates. Sparse graph missing true edges -> Lasso penalty too strong or data insufficient for sparsity to emerge. High NMSE despite correct topology -> INN not expressive enough or latent space poorly aligned.
- **First 3 experiments**: 1. Synthetic VAR data with known sparse topology; compare learned adjacency to ground truth (AUROC). 2. Synthetic Lorenz-96 data with increasing chaos (F parameter); test robustness of topology identification. 3. Real offshore sensor data; evaluate prediction NMSE and visualize inferred dependency graph.

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's performance scale with increasingly large and high-dimensional time series datasets, particularly in terms of computational efficiency and memory requirements?
- Basis in paper: [explicit] The paper mentions that the second formulation (B) is computationally more efficient and suitable for sequential and big-data or high-dimensional scenarios, requiring constant memory needs for each iteration.
- Why unresolved: While the paper compares the proposed method with state-of-the-art methods on synthetic and real data, it does not provide extensive testing on very large-scale datasets or explicitly discuss scalability challenges.
- What evidence would resolve it: Experiments demonstrating the performance and computational efficiency of the proposed method on datasets with thousands or millions of time series variables, along with memory usage analysis and comparisons with other methods.

### Open Question 2
How robust is the proposed method to noise and missing data in real-world applications, and what strategies can be employed to handle such scenarios effectively?
- Basis in paper: [inferred] The paper mentions using real data from a sensor network in the offshore oil and gas industry, but does not explicitly discuss the impact of noise or missing data on the method's performance.
- Why unresolved: Real-world sensor data often contains noise and missing values, which can affect the accuracy of topology identification and prediction. The paper does not provide insights into how the proposed method handles these challenges.
- What evidence would resolve it: Experiments evaluating the method's performance under different levels of noise and missing data, along with proposed strategies or modifications to the algorithm to handle such scenarios effectively.

### Open Question 3
How does the choice of the activation function and the number of units in the invertible neural networks (INNs) impact the model's expressive power and interpretability, and are there guidelines for selecting these hyperparameters?
- Basis in paper: [explicit] The paper mentions using a sigmoid function as an example of an activation function and states that the nonlinearities are parameterized using INNs with M units.
- Why unresolved: While the paper provides details on the model structure, it does not explore the impact of different activation functions or the number of units on the model's performance and interpretability. Guidelines for selecting these hyperparameters are also not provided.
- What evidence would resolve it: Experiments comparing the performance of the proposed method using different activation functions and varying numbers of units in the INNs, along with an analysis of the impact on the model's expressive power and interpretability. Guidelines or best practices for selecting these hyperparameters could be derived from the experimental results.

## Limitations
- The method's theoretical guarantees depend on strict invertibility and Lipschitz continuity of the nonlinear mappings, but empirical validation of these conditions on real-world data is lacking.
- The comparison with cMLP, cRNN, and cLSTM baselines lacks details on architecture depth, regularization, or training duration, making it difficult to assess whether reported performance gains are robust.
- The claim that Formulation B is computationally superior to Formulation A is supported by asymptotic analysis but not validated through runtime benchmarks on real datasets.

## Confidence

- **High confidence**: The theoretical framework connecting latent-space optimization to measurement-space prediction under Lipschitz continuity (Theorem 1).
- **Medium confidence**: The sparsity-inducing regularization yields interpretable dependency graphs, as sparsity is a standard assumption in VAR models, though not empirically validated for the nonlinear case here.
- **Low confidence**: The generalizability of the method to extremely dense dependency graphs or highly chaotic systems (e.g., Lorenz-96 with high F), as the paper only tests moderate settings.

## Next Checks

1. Test the method on synthetic data with known dense dependency graphs to evaluate whether Lasso regularization still yields interpretable results or incorrectly suppresses edges.
2. Validate the Lipschitz continuity assumption by analyzing the learned nonlinear mappings on real datasets and checking whether the theoretical bound in Theorem 1 holds empirically.
3. Conduct runtime benchmarks comparing Formulation A and Formulation B on large-scale datasets to confirm the claimed computational efficiency of Formulation B.