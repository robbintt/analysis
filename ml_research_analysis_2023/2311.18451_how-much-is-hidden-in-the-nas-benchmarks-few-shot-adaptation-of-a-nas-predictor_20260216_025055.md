---
ver: rpa2
title: How Much Is Hidden in the NAS Benchmarks? Few-Shot Adaptation of a NAS Predictor
arxiv_id: '2311.18451'
source_url: https://arxiv.org/abs/2311.18451
tags:
- search
- tasks
- neural
- predictor
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MP-NAS, a meta-learning approach for neural
  architecture search (NAS) that leverages knowledge from multiple NAS benchmarks
  to quickly adapt to new tasks and search spaces. The key idea is to use a Graph
  Convolutional Network (GCN) as a performance predictor and meta-learn it across
  diverse NAS tasks.
---

# How Much Is Hidden in the NAS Benchmarks? Few-Shot Adaptation of a NAS Predictor

## Quick Facts
- arXiv ID: 2311.18451
- Source URL: https://arxiv.org/abs/2311.18451
- Reference count: 40
- Key outcome: MP-NAS achieves superior or matching performance in cross-validation experiments and successfully extrapolates to new search spaces, outperforming expert models on 4/5 NAS-Bench-360 tasks.

## Executive Summary
This paper introduces MP-NAS, a meta-learning approach that leverages knowledge from multiple NAS benchmarks to quickly adapt to new tasks and search spaces. The method uses a Graph Convolutional Network (GCN) as a performance predictor and meta-learns it across diverse NAS tasks using a Body Only Meta Learning (BOIL) variant of MAML. This enables the predictor to generalize better and transfer knowledge across different datasets and search spaces. MP-NAS is evaluated on 16 NAS settings spanning 6 benchmarks, demonstrating superior or matching performance in cross-validation experiments and successful extrapolation to completely new search spaces.

## Method Summary
MP-NAS employs a GCN-based performance predictor that is meta-learned using BOIL-MAML across multiple NAS benchmarks. The approach unifies different cell-based NAS search spaces into a common 14-operation representation, allowing the predictor to operate on heterogeneous benchmarks. During meta-training, the predictor learns from architecture-performance pairs across correlated tasks, initializing parameters that generalize across tasks. For new tasks, the predictor is fine-tuned using a small number of architecture-performance pairs from the target dataset, enabling few-shot adaptation without extensive retraining.

## Key Results
- MP-NAS shows superior or matching performance in cross-validation experiments compared to baselines
- Successfully extrapolates to completely new search spaces not seen during meta-training
- On NAS-Bench-360 benchmark, outperforms hand-designed expert models on 4 out of 5 tasks and a recent NAS method on 3 out of 5 tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-learning with BOIL allows the GCN predictor to quickly adapt to new tasks by updating all parameters except the final layer in the inner loop, forcing it to learn new graph features instead of reusing old ones.
- Mechanism: The BOIL variant of MAML updates the body of the GCN (ϕ) in the inner loop but keeps the final layer (λ) fixed. This encourages the network to adapt feature extraction layers for the new task while preserving the output layer's learned mapping, leading to faster and more generalizable adaptation.
- Core assumption: The graph feature representations learned across diverse NAS tasks are transferable, and the final layer can generalize across tasks without retraining.
- Evidence anchors:
  - [section] "We use a modified version of MAML called Body Only Meta Learning (BOIL) which updates all GCN parameters except the final layer in the inner loop and all parameters in the outer loop. This enforces the GCN to learn new graph features when trained on architecture-performance pairs on a new dataset instead of re-using the features [38]."
  - [section] "We observe empirically in section 4.2, BOIL performs better than MAML [4] and ANIL [47] and provides better out-of-domain generalization."

### Mechanism 2
- Claim: Unified search space representation enables cross-benchmark meta-learning by mapping all cell-based NAS benchmarks into a common graph operation space.
- Mechanism: By representing each NAS graph using a one-hot encoding over the union of 11 operations from all benchmarks, the predictor can operate on any benchmark's graphs without retraining from scratch. This allows meta-training across heterogeneous datasets and search spaces.
- Core assumption: The underlying graph structures and operation semantics are sufficiently similar across benchmarks that a shared representation is meaningful.
- Evidence anchors:
  - [section] "We represent a DAG as set of node features and adjacency matrix, G = ( V, A) where V is node feature matrix containing one-hot representation of operations happening at a given node and A is the adjacency matrix of the graph. In order for the GCN to operate on graphs of different sizes we need to unify their representations. We do so by representing the node feature matrices under the union of operations across all search spaces."
  - [section] "We observe the transfer ability reduces as we go from TB101 to NB101 and NB-ASR, as they are more out-of-distribution. Nonetheless, we are still better than random initialization (Table 6)."

### Mechanism 3
- Claim: Meta-learning with correlated tasks improves predictor initialization, reducing the number of architectures needed to fine-tune on a new task.
- Mechanism: By sampling architecture-performance pairs from multiple correlated NAS tasks during meta-training, the predictor learns a parameter initialization that generalizes across tasks. This initialization requires fewer fine-tuning examples to achieve high Spearman correlation on a new task.
- Core assumption: Tasks within the meta-training set are sufficiently correlated that knowledge transfer is possible; uncorrelated tasks degrade performance.
- Evidence anchors:
  - [section] "We observe a slight increase in Spearman correlation from 1 meta-training task to 5 meta-training tasks (likely explained by meta-overfitting) but there is no significant change (< ±2%) if we increase the number of meta-training tasks further."
  - [section] "We observe adding these uncorrelated tasks in the meta-learning deteriorates the transfer ability of our method. Having 10x more uncorrelated tasks in meta-training makes the meta-learner under-fit, eventually converging to the performance of a randomly initialized predictor."

## Foundational Learning

- Concept: Meta-learning (MAML and variants)
  - Why needed here: The paper uses meta-learning to adapt a GCN predictor across diverse NAS tasks and search spaces. Understanding how MAML works, including inner-loop and outer-loop updates, is critical to grasp the adaptation mechanism.
  - Quick check question: In MAML, which parameters are updated in the inner loop and which in the outer loop during meta-training?

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: The predictor is implemented as a GCN that operates on neural architecture graphs. Knowing how GCNs aggregate node features and use adjacency matrices is essential to understand how architectures are encoded and compared.
  - Quick check question: How does a GCN use the adjacency matrix and node features to compute embeddings for graph-structured data?

- Concept: Spearman correlation as evaluation metric
  - Why needed here: The paper evaluates predictor transferability using Spearman correlation between predicted and true performance rankings. Understanding rank correlation vs. point-wise error is important for interpreting results.
  - Quick check question: Why might Spearman correlation be a more appropriate metric than MSE for evaluating a NAS predictor's ranking ability?

## Architecture Onboarding

- Component map:
  - Meta-training pipeline -> Unified search space -> Fine-tuning stage -> NAS loop

- Critical path:
  1. Load unified NAS data from multiple benchmarks
  2. Meta-train GCN predictor using BOIL-MAML across tasks
  3. For a new task, sample a few architectures, train them, and collect performance
  4. Fine-tune the meta-learned predictor on these samples
  5. Use the fine-tuned predictor to rank and select architectures for evaluation
  6. Iterate until a satisfactory architecture is found

- Design tradeoffs:
  - BOIL vs full MAML: BOIL reduces inner-loop computation and encourages feature adaptation, but assumes the final layer is transferable
  - Unified search space: Increases expressiveness and transferability but may include operations not present in all benchmarks, leading to sparse one-hot encodings
  - Number of meta-training tasks: More tasks improve robustness but may introduce uncorrelated noise; the paper finds 5 tasks sufficient

- Failure signatures:
  - Low Spearman correlation on cross-dataset transfer: Indicates poor meta-initialization or insufficient correlation between meta-training and target tasks
  - Degraded performance when extending to macro search spaces: Suggests the unified representation is too restrictive for non-cell-based architectures
  - Overfitting during meta-training: Manifests as high variance across runs or degraded performance when increasing meta-training task count

- First 3 experiments:
  1. Run meta-training on NB201 + TB101 data, then evaluate zero-shot and fine-tuned Spearman correlation on held-out CIFAR-100 architectures
  2. Compare BOIL vs MAML vs ANIL on cross-dataset transfer within TB101 using leave-one-out validation
  3. Extend the predictor to the unified search space, then evaluate cross-search space transfer from NB201 to NB101 and NB-ASR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation between meta-training tasks affect the transferability of the NAS predictor?
- Basis in paper: [explicit] The paper discusses how increasing the correlation between meta-training tasks and meta-test tasks leads to better performance.
- Why unresolved: While the paper shows that higher correlation improves transferability, it doesn't provide a quantitative analysis of how different levels of correlation impact performance. The relationship between correlation strength and predictor effectiveness remains unclear.
- What evidence would resolve it: Conducting experiments with varying levels of correlation between tasks and measuring the corresponding predictor performance would provide insights into this relationship.

### Open Question 2
- Question: What is the optimal number of meta-training tasks for achieving the best predictor transferability?
- Basis in paper: [explicit] The paper mentions that increasing the number of meta-training tasks beyond a certain point doesn't significantly affect Spearman correlation.
- Why unresolved: The paper doesn't explore the upper limit of meta-training tasks or identify the optimal number for maximum transferability. There's a need to determine if there's a point of diminishing returns or if more tasks always lead to better performance.
- What evidence would resolve it: Conducting experiments with varying numbers of meta-training tasks and analyzing the corresponding predictor performance would help identify the optimal number of tasks.

### Open Question 3
- Question: How does the inclusion of uncorrelated tasks in the meta-training set affect the predictor's transferability?
- Basis in paper: [explicit] The paper discusses that adding uncorrelated tasks to the meta-training set deteriorates the transfer ability of the method.
- Why unresolved: While the paper shows that uncorrelated tasks negatively impact performance, it doesn't provide a detailed analysis of how the number or type of uncorrelated tasks affects transferability. The threshold for acceptable uncorrelated tasks remains unclear.
- What evidence would resolve it: Conducting experiments with different proportions and types of uncorrelated tasks in the meta-training set and measuring the corresponding predictor performance would provide insights into this aspect.

## Limitations

- The meta-learning framework relies heavily on the assumption that NAS benchmarks share sufficient structural similarity for knowledge transfer, which is only partially validated
- The BOIL modification of MAML lacks theoretical justification for why freezing the final layer during inner-loop updates improves generalization across NAS tasks
- Evaluation focuses primarily on cell-based search spaces, leaving unclear whether the approach extends to macro-architectures or more complex search spaces

## Confidence

**High Confidence**: The claim that MP-NAS improves Spearman correlation on NB360 tasks compared to baselines is well-supported by Table 3, showing consistent improvements across 4/5 tasks with statistically significant margins.

**Medium Confidence**: The claim about cross-benchmark generalization is supported by leave-one-out validation but has notable exceptions, suggesting the approach has clear boundaries not fully characterized.

**Low Confidence**: The scalability claim to macro search spaces and the assertion that 5 meta-training tasks are sufficient for optimal performance are not thoroughly validated and lack strong theoretical backing.

## Next Checks

1. **Cross-Domain Transfer Robustness**: Systematically test transfer from NB201/TB101 to NB-ASR and macro-architecture benchmarks, measuring both Spearman correlation and actual NAS performance. Include benchmarks with different training protocols to isolate the impact of protocol differences on transferability.

2. **BOIL Mechanism Analysis**: Conduct ablation studies comparing BOIL vs. full MAML across varying levels of task similarity in the meta-training set. Measure not just final performance but also adaptation speed and stability across random seeds to better understand when BOIL's assumptions break down.

3. **Search Space Coverage Validation**: Quantify the impact of the unified 14-operation representation on predictor performance by systematically removing operations from the union set and measuring degradation. This would validate whether the chosen representation size is optimal or whether a more compact encoding could suffice.