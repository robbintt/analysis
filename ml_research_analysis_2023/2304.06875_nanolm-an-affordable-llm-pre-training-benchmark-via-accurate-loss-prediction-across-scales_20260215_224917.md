---
ver: rpa2
title: 'nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction
  across Scales'
arxiv_id: '2304.06875'
source_url: https://arxiv.org/abs/2304.06875
tags:
- loss
- scaling
- training
- large
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a method called \xB5Scaling that leverages\
  \ Maximal Update Parametrization (\xB5P) and scaling laws to accurately predict\
  \ the pre-training loss of large language models (LLMs) based solely on the results\
  \ and hyperparameters from smaller models. The key idea is that under \xB5P, the\
  \ loss landscapes of models with different widths are aligned, allowing for accurate\
  \ fitting of scaling laws for hyperparameters near common loss basins."
---

# nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales

## Quick Facts
- arXiv ID: 2304.06875
- Source URL: https://arxiv.org/abs/2304.06875
- Reference count: 11
- Key outcome: nanoLM predicts LLM pre-training loss using only small model experiments, achieving 14% cost savings

## Executive Summary
nanoLM introduces µScaling, a method that leverages Maximal Update Parametrization (µP) and scaling laws to accurately predict the pre-training loss of large language models based solely on smaller model experiments. The key insight is that µP aligns loss landscapes across model widths for µTransferable hyperparameters, enabling direct comparison of different model designs at large scales without expensive large-scale training. The authors demonstrate this approach on GPT-2 models ranging from 128 to 3072 parameters, showing successful loss prediction within loss basins while identifying limitations at basin boundaries.

## Method Summary
The method uses µP to align loss landscapes across model widths for µTransferable hyperparameters (learning rate, initialization variance, multipliers). By training a series of smaller models with µP-scaled hyperparameters and fitting power-law scaling laws to their losses, nanoLM can predict the loss for arbitrary model sizes. The critical path involves grid-searching optimal µTransferable HPs on a small base model, generating and training smaller agent models with scaled HPs, fitting power-law scaling laws to these results, and applying the scaling law to predict losses for target model sizes.

## Key Results
- Successful loss prediction for GPT-2 models with widths from 128 to 3072 parameters
- Prediction accuracy maintained within loss basins for µTransferable hyperparameters
- One-time pre-training cost reduced to approximately 14% through small model experiments
- Method enables direct comparison of different model designs at large scales without training them

## Why This Works (Mechanism)

### Mechanism 1
- Claim: µP aligns loss landscapes across model widths for µTransferable hyperparameters.
- Mechanism: Under Maximal Update Parametrization, the optimal values of certain hyperparameters transfer between models of different widths via a scaling function. This creates aligned loss landscapes where the optimal hyperparameter point for a small model is also optimal for larger models.
- Core assumption: The loss basin around the optimal hyperparameter point is sufficiently wide and smooth for power-law fitting to be accurate.
- Evidence anchors:
  - [abstract] "our observations that Maximal Update Parametrization (µP) enables accurate fitting of scaling laws close to common loss basins in hyperparameter space"
  - [section 2.4] "This ensures that they have mutually aligned loss landscapes (training loss curves w.r.t. aµ-transferable HP, see Figure 2)"
  - [corpus] Weak corpus support - only 1/8 related papers mention µP directly

### Mechanism 2
- Claim: Power-law scaling laws accurately predict training loss for models trained with µP-aligned hyperparameters.
- Mechanism: When models are trained with µTransferable hyperparameters near the loss basin, their training losses follow a power-law relationship with model size. This allows loss prediction for arbitrary widths by fitting the power-law to a small set of training results.
- Core assumption: The power-law relationship holds across the range of model sizes being predicted, not just locally around the training points.
- Evidence anchors:
  - [abstract] "With μScaling, different model designs can be compared on large scales by training only their smaller counterparts"
  - [section 3.2] "We observe the followings: Successful loss prediction for HPs near the loss basin"
  - [corpus] Moderate support - 3/8 related papers discuss scaling laws for loss prediction

### Mechanism 3
- Claim: µP enables hyperparameter transfer without requiring grid search on large models.
- Mechanism: By using the µP function to scale hyperparameters from small to large models, the method eliminates the need for expensive hyperparameter searches on large-scale models. The optimal hyperparameters for a small base model can be directly scaled to predict optimal settings for larger models.
- Core assumption: The µP function accurately captures the relationship between optimal hyperparameters and model width across the range of scales.
- Evidence anchors:
  - [abstract] "This enables direct comparison of different model designs on large scales by training only their smaller counterparts"
  - [section 2.2] "µP allows some optimal HPs of 'wide' models to be directly computed based on grid search on 'narrow' ones"
  - [corpus] Weak support - only 1/8 papers mention parameter transfer or width scaling

## Foundational Learning

- Concept: Maximal Update Parametrization (µP)
  - Why needed here: µP is the theoretical foundation that enables hyperparameter transfer across model widths, which is essential for the loss prediction method to work
  - Quick check question: What are the three classes of hyperparameters that µP claims can be transferred across model widths?

- Concept: Scaling Laws
  - Why needed here: Scaling laws provide the mathematical relationship (power-law) that connects model size to training loss, enabling loss prediction from small model experiments
  - Quick check question: In the power-law formula L(C) = aCb + c, what does the variable C represent?

- Concept: Loss Landscapes and Basins
  - Why needed here: Understanding loss landscapes is crucial for recognizing why µP works (landscape alignment) and why predictions are only reliable near the loss basin
  - Quick check question: What is the visual difference between aligned and misaligned loss landscapes as described in Figure 2?

## Architecture Onboarding

- Component map: Small model training -> µP hyperparameter scaling -> Power-law fitting -> Loss prediction for large models
- Critical path: (1) Grid search for optimal µTransferable HPs on small base model, (2) Generate and train a series of smaller agent models with µP-scaled HPs, (3) Fit power-law scaling law to these results, (4) Apply scaling law to predict loss for target model size
- Design tradeoffs: The method trades computational efficiency (training only small models) for accuracy (predictions are only reliable near the loss basin). It also requires careful initialization and cannot handle non-µTransferable hyperparameters without additional search.
- Failure signatures: Poor fit quality of the power-law (high deviation between predicted and actual losses), divergence of large models when using naively transferred HPs, or systematic underestimation/overestimation of losses for certain model sizes.
- First 3 experiments:
  1. Verify µP alignment by training small models with µP-scaled HPs and checking if they achieve similar loss values
  2. Test power-law fitting by training multiple small models at different widths and checking if their losses follow the predicted relationship
  3. Validate loss prediction by training a medium-sized model and comparing its actual loss to the prediction from smaller models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different embedding dimensions affect the accuracy of loss prediction in nanoLM?
- Basis in paper: [explicit] The authors discuss the importance of counting embedding weights as model parameters for accurate loss prediction.
- Why unresolved: The paper does not explore how varying the embedding dimension specifically impacts the loss prediction accuracy.
- What evidence would resolve it: Experimental results showing loss prediction accuracy for models with varying embedding dimensions, compared to those with fixed dimensions.

### Open Question 2
- Question: What is the impact of dropout and weight decay on the correctness of µP and loss prediction?
- Basis in paper: [explicit] The authors mention that dropout and weight decay may influence the correctness of µP and leave this for future studies.
- Why unresolved: The paper does not investigate the specific effects of these hyperparameters on loss prediction.
- What evidence would resolve it: Experiments comparing loss prediction accuracy with and without dropout and weight decay, under various configurations.

### Open Question 3
- Question: How do different training dataset sizes affect the scalability and accuracy of loss prediction in nanoLM?
- Basis in paper: [inferred] The paper discusses the use of scaling laws and the importance of model size, but does not explicitly address the role of dataset size in loss prediction.
- Why unresolved: The authors focus on model width and hyperparameters, without exploring how dataset size might impact the loss prediction framework.
- What evidence would resolve it: Comparative analysis of loss prediction accuracy across models trained on datasets of varying sizes, while controlling for model width and other hyperparameters.

## Limitations

- Prediction accuracy degrades for hyperparameters outside the loss basin
- µP alignment may break down at extreme scales due to numerical issues like FP16 underflow
- Non-µTransferable hyperparameters require additional grid search, limiting the method's applicability

## Confidence

**High Confidence**: The mechanism of µP alignment and power-law scaling laws is well-established in the literature and supported by the authors' empirical results.

**Medium Confidence**: The claim of achieving "around 14% of one-time pre-training cost" depends on the specific baseline chosen and may vary significantly based on hardware efficiency, parallelization strategies, and dataset characteristics.

**Medium Confidence**: The accuracy of loss predictions (stated as "accurate") is demonstrated only for hyperparameters within the loss basin and for the specific range of model sizes tested.

## Next Checks

1. **Boundary testing**: Train a model at the edge of the predicted loss basin (using a hyperparameter value that's just outside the basin identified in the paper) to empirically verify the "basin boundary" claim and quantify prediction accuracy degradation.

2. **Cross-dataset validation**: Apply µScaling to predict losses for the same model architectures trained on a different dataset (e.g., C4 instead of OpenWebText) to test the method's robustness to dataset variations.

3. **Extreme scale probing**: Train a small set of models at widths beyond 3072 (perhaps 4096 or 8192) to test whether the power-law scaling relationship holds and whether µP alignment breaks down at these scales, as suggested in the Limitations section.