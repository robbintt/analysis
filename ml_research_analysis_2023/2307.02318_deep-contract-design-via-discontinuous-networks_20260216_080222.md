---
ver: rpa2
title: Deep Contract Design via Discontinuous Networks
arxiv_id: '2307.02318'
source_url: https://arxiv.org/abs/2307.02318
tags:
- delu
- network
- contract
- function
- principal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Discontinuous ReLU (DeLU) network for
  automated contract design, where a principal establishes payments to an agent based
  on uncertain outcomes from the agent's actions. The DeLU network models the principal's
  utility as a discontinuous piecewise affine function, with each piece corresponding
  to the agent taking a particular action.
---

# Deep Contract Design via Discontinuous Networks

## Quick Facts
- arXiv ID: 2307.02318
- Source URL: https://arxiv.org/abs/2307.02318
- Authors: 
- Reference count: 40
- Primary result: DeLU networks can approximate the principal's utility function with a small number of training samples and scale to find approximately optimal contracts on problems with a large number of actions and outcomes, significantly outperforming conventional continuous networks.

## Executive Summary
This paper introduces the Discontinuous ReLU (DeLU) network for automated contract design, where a principal establishes payments to an agent based on uncertain outcomes from the agent's actions. The DeLU network models the principal's utility as a discontinuous piecewise affine function, with each piece corresponding to the agent taking a particular action. This allows DeLU networks to implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal. Experiments demonstrate that DeLU networks can approximate the principal's utility function with a small number of training samples and scale to find approximately optimal contracts on problems with a large number of actions and outcomes, significantly outperforming conventional continuous networks.

## Method Summary
The Discontinuous ReLU (DeLU) network architecture consists of a standard ReLU sub-network η for computing pre-activation outputs and a bias network ζ that maps activation patterns to last-layer biases. The network is trained end-to-end using MSE loss with RMSprop optimizer. For inference, the method either solves linear programs for each linear region or uses gradient-based interior-point methods with barrier functions. The approach is evaluated on synthetic contract design problems with varying numbers of actions and outcomes, comparing performance against baseline ReLU networks and direct LP solvers.

## Key Results
- DeLU networks can approximate the principal's utility function with a small number of training samples
- The method scales to find approximately optimal contracts on problems with a large number of actions and outcomes
- DeLU significantly outperforms conventional continuous networks on contract design tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discontinuous ReLU (DeLU) networks can model the discontinuity of the principal's utility function by conditioning piecewise biases on activation patterns.
- Mechanism: A DeLU network splits into two sub-networks: a standard ReLU sub-network η that computes pre-activation outputs, and a bias network ζ that maps activation patterns to last-layer biases. Because activation patterns are constant within each linear region, conditioning biases on them allows discontinuities at region boundaries.
- Core assumption: Activation patterns uniquely identify linear regions of the ReLU sub-network.
- Evidence anchors:
  - [abstract] "which models the principal's utility as a discontinuous piecewise affine function where each piece corresponds to the agent taking a particular action."
  - [section] "Since there are no activation units at the output layer, given an input contract f, we can obtain the activation pattern r(f) by a forward pass of the network up until the last layer."
  - [corpus] Weak – no direct citation of discontinuity modeling in corpus.
- Break condition: If activation patterns do not uniquely determine linear regions, conditioning biases will not create correct discontinuities.

### Mechanism 2
- Claim: Linear regions of the DeLU network implicitly encode incentive compatibility constraints and the principal's utility maximization objective.
- Mechanism: Each linear region corresponds to a set of contracts Q_i where the agent takes action i. Within Q_i, the DeLU network represents a linear function whose coefficients encode the expected value minus expected payment structure. Solving for optimal contracts within each region becomes a linear programming problem with constraints derived from the activation pattern.
- Core assumption: The agent's utility function is convex and piecewise affine, so that the principal's utility is also piecewise affine with region boundaries aligned to agent indifference curves.
- Evidence anchors:
  - [section] "For a contract f on the boundary of two neighboring linear pieces μp_i and μp_j, the agent is indifferent between action a_i and a_j given f."
  - [section] "Contracts in the same linear piece ρ result in the same activation pattern r, and the DeLU network is a linear function on the piece."
  - [corpus] Weak – no explicit statement about implicit constraint encoding in corpus.
- Break condition: If the principal's utility is not piecewise affine or agent indifference boundaries do not align with network linear regions.

### Mechanism 3
- Claim: Gradient-based interior-point inference can efficiently find optimal contracts in parallel by solving barrier problems for each linear region.
- Mechanism: For each activation pattern (linear region), define a barrier function ϕ(f) that enforces feasibility via logarithmic terms. Maximize the objective function with barrier terms using gradient ascent. Forward and backward passes compute both the objective and its gradient, enabling GPU parallelism over multiple starting points.
- Core assumption: The interior-point method converges quickly for the piecewise linear structure, and the barrier function is differentiable with respect to f.
- Evidence anchors:
  - [section] "We adopt the following barrier function: ϕ(f) = -LXl=1nlX i=1 log (Δ(l)i (M(l)i f + z(l)i))."
  - [section] "A forward pass of the DeLU network gives ϕ(k)(f) and a backward pass is sufficient to calculate ∂ϕ(k)(f)/∂f."
  - [corpus] Weak – no corpus evidence for this specific barrier formulation.
- Break condition: If the barrier function is ill-conditioned or the linear region is too large, gradient-based method may converge slowly or to local optima.

## Foundational Learning

- Concept: Piecewise affine functions and linear regions
  - Why needed here: The principal's utility function is piecewise affine with discontinuities at agent indifference boundaries. Understanding linear regions is crucial for designing networks that can represent these structures.
  - Quick check question: Given a ReLU network with one hidden layer of 4 units, how many linear regions can it have at most?

- Concept: Incentive compatibility constraints in principal-agent problems
  - Why needed here: The agent chooses actions to maximize expected payment minus cost. Contracts must satisfy IC constraints so that the chosen action is indeed optimal for the agent.
  - Quick check question: For two actions with different outcome distributions and costs, what condition must hold at the boundary where the agent is indifferent?

- Concept: Interior-point methods for constrained optimization
  - Why needed here: Finding optimal contracts requires maximizing a function subject to inequality constraints. Interior-point methods handle this efficiently with barrier functions.
  - Quick check question: In a barrier method, what happens to the barrier parameter t as iterations progress, and why?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (η) -> Activation pattern extractor -> Bias network (ζ) -> Output layer

- Critical path:
  1. Forward pass through η to compute hidden activations and activation pattern R(f)
  2. Pass R(f) through ζ to get last-layer bias
  3. Compute final output = W^(L+1) · R(L) · h(L)(f) + bias_ζ(R(f))
  4. For inference: solve max over f using LP or gradient-based method on each linear region

- Design tradeoffs:
  - Network size vs. inference speed: Larger networks have more linear regions but slower LP inference
  - Bias network complexity: More complex ζ can better approximate discontinuities but increases parameters
  - Parallelization strategy: LP inference parallelizes across regions, gradient-based parallelizes across starting points

- Failure signatures:
  - Poor optimality: Network fails to capture discontinuities → ReLU baseline performs better
  - Slow training: Large number of linear regions → training loss plateaus early
  - Unstable inference: Barrier method diverges → gradient steps too large or barrier ill-conditioned

- First 3 experiments:
  1. Generate small synthetic problem (n=5 actions, m=2 outcomes) and compare DeLU vs ReLU optimality
  2. Test LP vs gradient-based inference speed on medium problem (n=50, m=16)
  3. Vary number of training samples to find sample efficiency threshold for good optimality

## Open Questions the Paper Calls Out
No specific open questions were identified in the paper.

## Limitations
- The alignment between network linear regions and agent indifference boundaries relies on careful initialization and sufficient training samples
- Gradient-based inference performance depends heavily on barrier function parameters and step sizes
- The approach assumes convex agent utility functions; extensions to non-convex cases remain unclear

## Confidence
- Mechanism 1 (Discontinuity modeling): High - well-defined mathematical framework with clear implementation
- Mechanism 2 (Implicit constraint encoding): Medium - theoretical justification present but empirical validation limited
- Mechanism 3 (Parallel inference): Low-Medium - barrier method implementation details sparse, convergence properties not fully characterized

## Next Checks
1. Test DeLU performance on non-convex agent utility functions where principal utility may have non-aligned discontinuities
2. Benchmark convergence rates of gradient-based vs LP inference across varying problem sizes and linear region complexities
3. Conduct ablation studies on bias network architecture (depth, width) to determine minimum complexity needed for good performance