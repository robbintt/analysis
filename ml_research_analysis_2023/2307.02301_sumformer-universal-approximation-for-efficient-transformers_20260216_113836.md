---
ver: rpa2
title: 'Sumformer: Universal Approximation for Efficient Transformers'
arxiv_id: '2307.02301'
source_url: https://arxiv.org/abs/2307.02301
tags:
- approximation
- sumformer
- attention
- universal
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first universal approximation theorems
  for efficient Transformers (Linformer and Performer) by introducing the Sumformer
  architecture, which can universally approximate continuous equivariant sequence-to-sequence
  functions on compact sets. The authors prove this using both continuous and discontinuous
  methods, with the continuous approach relying on multisymmetric polynomials and
  the discontinuous approach using piecewise constant approximation.
---

# Sumformer: Universal Approximation for Efficient Transformers

## Quick Facts
- arXiv ID: 2307.02301
- Source URL: https://arxiv.org/abs/2307.02301
- Reference count: 28
- One-line primary result: First universal approximation theorems for efficient Transformers (Linformer, Performer) using the Sumformer architecture.

## Executive Summary
This paper introduces Sumformer, a novel architecture that achieves universal approximation for efficient Transformers by mapping tokens to a latent space, summing these representations, and applying a function conditioned on both the original tokens and the sum. The authors prove this works for both continuous and discontinuous equivariant sequence-to-sequence functions on compact sets, with the continuous proof relying on multisymmetric polynomials and the discontinuous approach using piecewise constant approximation. A key contribution is demonstrating that just one attention layer is sufficient for universal approximation, significantly improving upon previous results requiring exponentially many layers.

## Method Summary
The Sumformer architecture consists of two main components: a mapping function ϕ that transforms each token xᵢ to a latent representation, and a function ψ that takes each original token and the sum Σ = Σᵢ ϕ(xᵢ) to produce the output token. The continuous proof uses multisymmetric polynomials to represent any continuous equivariant function, while the discontinuous proof uses piecewise constant approximation. For efficient Transformers, the authors show that Linformer and Performer can implement the required sum operation through their respective attention mechanisms, preserving universal approximation properties.

## Key Results
- Sumformer achieves universal approximation for continuous equivariant sequence-to-sequence functions using multisymmetric polynomials
- Discontinuous approximation allows polynomially bounded latent dimensions but requires exponentially many layers
- Just one attention layer is sufficient for universal approximation, improving upon previous results
- Linformer and Performer can implement the required sum operation, preserving universal approximation properties
- Numerical experiments confirm Sumformers can learn benchmark functions using gradient descent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sumformer achieves universal approximation by representing sequence-to-sequence functions through a two-step process: first mapping each token via ϕ to a latent space, then summing these latent representations, and finally applying ψ conditioned on the original token and the sum.
- Mechanism: The architecture exploits equivariance by ensuring the sum Σ is invariant to token permutations, while ψ restores the necessary permutation structure by conditioning on each original token. This combination allows the model to capture all equivariant continuous sequence-to-sequence functions on compact sets.
- Core assumption: The functions ϕ and ψ can be chosen (or approximated) such that their composition captures any continuous equivariant function when combined with the sum operation.
- Evidence anchors:
  - [abstract] "We introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions."
  - [section] "We show that the Sumformer architecture is able to approximate all continuous equivariant sequence-to-sequence functions on compact sets (Sec. 3)."
- Break condition: If the functions ϕ or ψ cannot be sufficiently approximated by the available network components (e.g., MLPs or multisymmetric polynomials), the universal approximation property fails.

### Mechanism 2
- Claim: The use of multisymmetric polynomials in the continuous proof allows Sumformer to approximate any continuous equivariant function with a fixed number of layers (one attention/sum layer plus two feedforward layers for ψ).
- Mechanism: Multisymmetric polynomials can represent any multisymmetric polynomial via power sums. By mapping tokens through ϕ to power sums and summing, then reconstructing the function via ψ, the architecture captures the full space of continuous equivariant functions.
- Core assumption: The algebra of multisymmetric polynomials is sufficiently rich to represent any continuous equivariant function when combined with the sum operation.
- Evidence anchors:
  - [section] "The real multisymmetric power sums in a sequence of length n with multidegree |α| :“ α1 ` ¨ ¨ ¨ `αd ď n generate all real multisymmetric polynomials"
  - [section] "Because pi is equivariant we can use Theorem 2.9 to represent pi by a semi-invariant polynomial qi : X n ÞÑ R"
- Break condition: If the token dimension d or sequence length n becomes too large, the required latent dimension d1 grows polynomially, making the approach computationally infeasible.

### Mechanism 3
- Claim: For efficient Transformers (Linformer and Performer), universal approximation is preserved by replacing the standard attention mechanism with their respective efficient variants while maintaining the same ϕ and ψ functions.
- Mechanism: Since Linformer and Performer only modify the attention computation (replacing it with linear projections or kernel approximations), and the Sumformer proof relies on the ability to compute the sum Σ, these architectures can still implement the required sum operation with appropriate weight choices.
- Core assumption: The efficient attention mechanisms can be configured to compute the sum of latent representations Σ with sufficient accuracy.
- Evidence anchors:
  - [abstract] "Using this result, we give a new proof for the universality of Transformers and the first universal approximation results for Linformer and Performer."
  - [section] "Because of the structure with X ` FCpX ` AttpXqq, we can set the attention for the first layers to zero. Thereby, we obtain feed-forward layers without attention."
- Break condition: If the linear projections in Linformer or the kernel approximations in Performer introduce errors that prevent accurate computation of Σ, the universal approximation property may not hold.

## Foundational Learning

- Concept: Equivariance
  - Why needed here: The paper focuses on functions that are invariant to permutations of input tokens, which is fundamental to the Sumformer architecture and its proof of universal approximation.
  - Quick check question: What does it mean for a function f: Xⁿ → Y to be equivariant to permutations of its input sequence?

- Concept: Multisymmetric polynomials
  - Why needed here: These polynomials form the mathematical foundation for the continuous proof of universal approximation, allowing the representation of any continuous equivariant function through power sums.
  - Quick check question: How do multisymmetric power sums generate all multisymmetric polynomials according to Theorem 2.12?

- Concept: Universal approximation theorem
  - Why needed here: The paper builds on and extends existing universal approximation results for neural networks to the specific case of efficient Transformers and the Sumformer architecture.
  - Quick check question: What is the key difference between the universal approximation theorem for standard Transformers (Yun et al., 2019) and the new result presented in this paper?

## Architecture Onboarding

- Component map: Token → ϕ → Sum → ψ → Output token
- Critical path: The attention layer is critical for computing the sum efficiently, especially for long sequences.
- Design tradeoffs:
  - Latent dimension d₁: Higher d₁ allows better approximation but increases computational cost
  - Number of layers: Continuous proof requires fewer layers but larger d₁; discontinuous proof allows smaller d₁ but requires exponentially many layers
  - Attention mechanism: Standard attention is exact but O(n²); Linformer and Performer reduce complexity to O(n) at the cost of approximation error
- Failure signatures:
  - Poor approximation: Likely due to insufficient latent dimension d₁ or inadequate approximation of ϕ and ψ
  - Instability during training: May indicate vanishing/exploding gradients or inappropriate learning rates
  - Memory issues: Suggests d₁ is too large for available resources
- First 3 experiments:
  1. Implement a simple Sumformer with d₁=2 and test on a small equivariant function (e.g., sum of squares) to verify basic functionality
  2. Compare performance of standard attention vs. Linformer attention on the same function to understand the approximation trade-off
  3. Vary d₁ systematically (e.g., 2, 4, 8, 16) and measure approximation quality to identify the scaling relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of the continuous Sumformer construction when the required latent dimension d1 grows polynomially with sequence length n?
- Basis in paper: [explicit] The paper states that in the continuous case, the required latent dimension d1 scales with nd for a fixed d, making the overall computational cost polynomial in n
- Why unresolved: While the paper establishes the scaling relationship, it does not provide specific complexity bounds or analyze how this affects practical implementation
- What evidence would resolve it: A detailed computational complexity analysis showing the exact relationship between n, d, and d1, along with empirical measurements of actual computational costs

### Open Question 2
- Question: Can the discontinuous Sumformer construction be modified to reduce the exponential growth in the number of required feed-forward layers?
- Basis in paper: [explicit] The paper states that the discontinuous construction requires exponentially many feed-forward layers in the sequence lengths n and the token size d
- Why unresolved: The paper identifies this as a limitation but does not explore potential modifications or alternative approaches to mitigate this exponential scaling
- What evidence would resolve it: A new construction or proof showing that the number of layers can be reduced to sub-exponential scaling, or experimental results demonstrating improved performance with fewer layers

## Limitations
- The continuous proof requires polynomially large latent dimensions d1 relative to sequence length n, creating computational bottlenecks
- The discontinuous proof requires exponentially many layers, making it impractical for real-world use
- The paper doesn't quantify how approximation errors in efficient attention mechanisms affect the universal approximation property

## Confidence
- Mathematical proofs: High confidence
- Practical implementation and experiments: Medium confidence

## Next Checks
1. Systematically vary latent dimension d1 and sequence length n in controlled experiments to empirically verify the polynomial scaling relationship predicted by the continuous proof, measuring both approximation accuracy and computational cost.

2. Implement the discontinuous approximation approach with a bounded number of layers to quantify the trade-off between approximation quality and layer count, validating whether the exponential layer growth is truly prohibitive.

3. Compare the approximation errors introduced by Linformer and Performer attention mechanisms against the target function approximation errors to determine whether efficient attention variants preserve the universal approximation property in practice.