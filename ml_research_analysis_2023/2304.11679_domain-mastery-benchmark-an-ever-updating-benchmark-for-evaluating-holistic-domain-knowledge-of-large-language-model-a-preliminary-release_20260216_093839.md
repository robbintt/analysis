---
ver: rpa2
title: 'Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic
  Domain Knowledge of Large Language Model--A Preliminary Release'
arxiv_id: '2304.11679'
source_url: https://arxiv.org/abs/2304.11679
tags:
- language
- arxiv
- domain
- questions
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DomMa, a Domain Mastery Benchmark designed
  to evaluate the domain knowledge understanding of large language models (LLMs).
  The benchmark addresses limitations of previous works by providing extensive domain
  coverage, large data volume, and a continually updated dataset based on Chinese
  112 first-level subject classifications.
---

# Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release

## Quick Facts
- arXiv ID: 2304.11679
- Source URL: https://arxiv.org/abs/2304.11679
- Reference count: 2
- This paper introduces DomMa, a comprehensive benchmark for evaluating domain knowledge of large language models with 100,000 questions across 112 subjects.

## Executive Summary
This paper introduces DomMa, a Domain Mastery Benchmark designed to evaluate the domain knowledge understanding of large language models (LLMs). The benchmark addresses limitations of previous works by providing extensive domain coverage, large data volume, and a continually updated dataset based on Chinese 112 first-level subject classifications. DomMa consists of 100,000 questions in both Chinese and English sourced from graduate entrance examinations and undergraduate exams in Chinese colleges. The benchmark employs three distinct evaluation methods tailored to assess the outputs generated by LLMs, including white-box, grey-box, and black-box evaluations.

## Method Summary
DomMa uses graduate entrance examinations and undergraduate exams from Chinese colleges as source data, categorizing questions into 112 first-level subject classifications. The benchmark employs automated data annotation and updating, using manually annotated questions as seed data and periodically updating with new questions through a curriculum learning approach. Three distinct evaluation methods (white-box, grey-box, black-box) are designed to assess LLM outputs, considering the sensitivity of LLMs to prompts and the challenge of generating answers identical to the ground truth.

## Key Results
- DomMa provides comprehensive domain coverage with 100,000 questions across 112 Chinese first-level subject classifications
- The benchmark successfully implements three-tier evaluation (white-box, grey-box, black-box) for assessing LLM outputs
- The auto-updating mechanism maintains benchmark relevance through curriculum learning and periodic manual annotation updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DomMa addresses the model output evaluation challenge by transforming multiple-choice questions into a format where models must select from 100 options, reducing the chance of random correct answers.
- Mechanism: The benchmark uses open-source synonym databases and morphological techniques to construct 100 options per question, making it statistically improbable for a model to guess correctly. This design shifts evaluation from exact string matching to semantic selection.
- Core assumption: LLMs cannot reliably distinguish the correct answer from 100 semantically similar options without genuine domain knowledge.
- Evidence anchors:
  - [abstract]: "we suggests that the model should select the right answer from lots of options, from 100 options for example."
  - [section]: "we suggests that the model should select the right answer from lots of options, from 100 options for example."
  - [corpus]: Weak evidence - corpus contains similar domain benchmarks but no specific mention of 100-option design.
- Break Condition: If LLMs develop capability to process and evaluate 100 options efficiently, or if synonym databases contain unique identifying features that models can exploit.

### Mechanism 2
- Claim: The benchmark maintains relevance through continuous automated updates using curriculum learning principles.
- Mechanism: Seed data from manually annotated questions trains an annotation model, which then labels new questions. The model is periodically fine-tuned on recent manually annotated data, with latest data emphasized in later training epochs to focus on current question patterns.
- Core assumption: Recent question patterns and characteristics can be captured by fine-tuning on manually annotated data, maintaining annotation quality over time.
- Evidence anchors:
  - [section]: "We employ manually annotated questions as seed data... curriculum learning, fine-tune the latest manually updated data in the latter epochs"
  - [abstract]: "continually auto update" based on Chinese 112 first-level subject classifications.
  - [corpus]: Weak evidence - corpus mentions "ever-updating" but lacks specific curriculum learning details.
- Break Condition: If question patterns change faster than the update cycle can capture, or if manual annotation quality degrades over time.

### Mechanism 3
- Claim: Three-tier evaluation (white-box, grey-box, black-box) ensures robust assessment across different model access scenarios.
- Mechanism: White-box uses model parameters for direct probability extraction, grey-box guides models to generate probabilities through instructions when parameters are unavailable, and black-box uses semantic similarity when models cannot generate probabilities.
- Core assumption: Different evaluation methods can capture model capabilities across varying levels of access and model sophistication.
- Evidence anchors:
  - [abstract]: "Three distinct evaluation methods have been developed to address these scenarios"
  - [section]: "Three distinct evaluation methods have been developed to address these scenarios, with the goal of accurately assessing a model's capabilities"
  - [corpus]: Weak evidence - corpus mentions multiple evaluation approaches but lacks specific three-tier detail.
- Break Condition: If semantic similarity measures prove unreliable for black-box evaluation, or if instruction-following degrades across evaluation tiers.

## Foundational Learning

- Concept: Chinese 112 first-level subject classification system
  - Why needed here: Provides hierarchical structure for domain coverage and enables comprehensive benchmarking across diverse academic fields
  - Quick check question: How many first-level subjects exist in the Chinese classification system that DomMa is based on?

- Concept: Curriculum learning principles
  - Why needed here: Enables automated benchmark updates by prioritizing recent data patterns while maintaining annotation quality
  - Quick check question: What is the key principle behind curriculum learning that makes it suitable for benchmark updates?

- Concept: Semantic similarity evaluation
  - Why needed here: Critical for black-box evaluation when models cannot generate probabilities, requiring robust comparison between generated content and options
  - Quick check question: What metric would be most appropriate for comparing semantic similarity between model outputs and answer options?

## Architecture Onboarding

- Component map: Data collection → Manual annotation → Seed model training → Automated annotation → Three-tier evaluation → Result aggregation
- Critical path: Data collection → Annotation → Evaluation → Result validation
- Design tradeoffs: Manual annotation provides high quality but limits scalability; automated updates increase coverage but may reduce precision
- Failure signatures: Poor annotation quality manifests as inconsistent subject labels; evaluation failures appear as systematic bias toward certain answer patterns
- First 3 experiments:
  1. Test 100-option selection mechanism with a small sample to verify statistical improbability of guessing
  2. Validate automated annotation accuracy by comparing seed data annotations with model-generated ones
  3. Benchmark all three evaluation methods on the same model to verify consistency across tiers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the auto-updating mechanism for maintaining benchmark relevance over time?
- Basis in paper: [explicit] The paper discusses an auto-updating mechanism using seed data and curriculum learning to continuously label new questions.
- Why unresolved: The paper does not provide empirical evidence or long-term studies demonstrating the effectiveness of the auto-updating mechanism in maintaining benchmark relevance.
- What evidence would resolve it: Long-term studies comparing the performance of models on the benchmark over time, showing that the auto-updating mechanism successfully incorporates new questions and maintains relevance.

### Open Question 2
- Question: How does the benchmark handle potential biases in the source data (graduate entrance exams and undergraduate exams)?
- Basis in paper: [inferred] The paper mentions using questions from graduate entrance exams and undergraduate exams but does not discuss handling potential biases in these sources.
- Why unresolved: The paper does not address the potential biases in the source data or how they might affect the benchmark's evaluation of LLM performance.
- What evidence would resolve it: Analysis of the benchmark's questions for potential biases and studies showing how these biases might influence LLM evaluation results.

### Open Question 3
- Question: How do the three evaluation methods (white-box, grey-box, black-box) compare in terms of accuracy and reliability for assessing LLM outputs?
- Basis in paper: [explicit] The paper introduces three distinct evaluation methods tailored to assess LLM outputs but does not compare their accuracy and reliability.
- Why unresolved: The paper does not provide a comparative analysis of the three evaluation methods in terms of their effectiveness in assessing LLM outputs.
- What evidence would resolve it: Empirical studies comparing the accuracy and reliability of the three evaluation methods across various LLM models and question types.

## Limitations

- The 100-option selection mechanism's effectiveness in preventing random guessing lacks empirical validation
- The automated annotation system's long-term accuracy and reliability over time remain unverified
- Potential biases in source data (graduate entrance exams and undergraduate exams) are not addressed

## Confidence

- **High Confidence**: The benchmark's comprehensive domain coverage (112 subject classifications) and large scale (100,000 questions) are well-documented and verifiable.
- **Medium Confidence**: The three-tier evaluation approach is logically sound, but its practical effectiveness across different model architectures and access scenarios requires empirical validation.
- **Low Confidence**: The automated update mechanism's long-term reliability and the 100-option selection design's effectiveness in preventing random guessing lack sufficient empirical support.

## Next Checks

1. **Statistical Validation of 100-Option Design**: Conduct controlled experiments comparing model performance on 100-option vs. traditional multiple-choice formats to verify the claimed reduction in random guessing probability.

2. **Annotation Quality Tracking**: Implement longitudinal monitoring of automated annotation accuracy by periodically comparing model-generated annotations against human annotations across all 112 subject categories.

3. **Cross-Evaluation Tier Consistency**: Test the same models using all three evaluation methods (white-box, grey-box, black-box) and analyze correlation coefficients between tier results to verify consistency across access scenarios.