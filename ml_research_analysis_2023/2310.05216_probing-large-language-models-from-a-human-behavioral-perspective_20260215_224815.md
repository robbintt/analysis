---
ver: rpa2
title: Probing Large Language Models from A Human Behavioral Perspective
arxiv_id: '2310.05216'
source_url: https://arxiv.org/abs/2310.05216
tags:
- gate
- human
- llms
- language
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study probes large language models from a human behavioral
  perspective, correlating model predictions with eye-tracking measures to understand
  their internal mechanisms. Using datasets of eye-tracking and EEG recordings during
  natural and task-specific reading, the authors analyze word prediction patterns
  across different language models.
---

# Probing Large Language Models from A Human Behavioral Perspective

## Quick Facts
- arXiv ID: 2310.05216
- Source URL: https://arxiv.org/abs/2310.05216
- Reference count: 7
- Key outcome: This study probes large language models from a human behavioral perspective, correlating model predictions with eye-tracking measures to understand their internal mechanisms. Using datasets of eye-tracking and EEG recordings during natural and task-specific reading, the authors analyze word prediction patterns across different language models. RNN-based models show the highest correlation with human reading patterns, while GPT-2 exhibits a distinct pattern. Feed-forward networks increase linguistic knowledge encoding until layer 8, then shift to comprehension. Multi-head self-attention distributes functions across different heads. In RNNs, gates control information flow - some promoting, others eliminating information. These findings provide insights into the internal workings of LLMs, contributing to their interpretability and potential for more efficient training.

## Executive Summary
This study explores the internal mechanisms of large language models (LLMs) by correlating their predictions with human eye-tracking data during reading. Using datasets of eye-tracking and EEG recordings, the authors analyze how different model architectures (RNNs, GPT-2, RWKV-v4) align with human reading patterns. The research reveals that RNN-based models exhibit the highest correlation with human behavioral data, while GPT-2 shows a distinct but positively correlated pattern. The study also examines how feed-forward networks encode linguistic knowledge and how multi-head self-attention distributes functions across different heads, providing insights into LLM interpretability and potential for more efficient training.

## Method Summary
The study trains RNN-based language models (RNN, GRU, LSTM) on WikiText-103 and uses pre-trained GPT-2 and RWKV-v4 models. Eye-tracking data from the ZuCo 2.0 dataset (730 English sentences, 18 participants) is correlated with model predictions and internal states. Five eye-tracking measures (GD, TRT, FFD, SFD, GPT) are extracted and analyzed. The correlation analysis employs Spearman correlation on log10-transformed LM outputs across model layers and components. Feed-forward networks and multi-head self-attention are examined for their contribution to word prediction patterns.

## Key Results
- RNN-based models show the highest correlation with human reading patterns (over 0.5)
- GPT-2 exhibits a distinct positive correlation pattern compared to other models' negative correlations
- Feed-forward networks encode linguistic knowledge until layer 8, then shift to comprehension
- Multi-head self-attention distributes functions across different heads with increasing correlation coefficients
- Gate mechanisms in RNNs control information flow, with some promoting and others eliminating information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNN-based models exhibit prediction patterns most similar to human reading behavior, while LLMs like GPT-2 show a distinct but positively correlated pattern.
- Mechanism: RNN-based models process sequential information in a way that mirrors human reading patterns, with hidden states directly mapping to eye-tracking measures. GPT-2, despite being more powerful, predicts difficult words with higher probability, showing a different but still correlated pattern.
- Core assumption: Eye-tracking measures are meaningful indicators of human reading processes that can be correlated with model prediction patterns.
- Evidence anchors:
  - [abstract] "RNN-based models show the highest correlation with human reading patterns, while GPT-2 exhibits a distinct pattern"
  - [section 4.4] "RNN-based LMs exhibit an exceptional correlation of over 0.5 with human behavioral data"
  - [corpus] Weak evidence - corpus neighbors focus on FFN, attention mechanisms, and model editing rather than human behavioral correlation
- Break condition: If eye-tracking measures are not valid indicators of reading difficulty or if models process information fundamentally differently from human reading.

### Mechanism 2
- Claim: Feed-forward networks (FFN) in LLMs encode linguistic knowledge and serve as memory units, with their capacity increasing with layer depth until layer 8, after which they shift to focus on comprehension.
- Mechanism: FFN layers progressively build linguistic knowledge encoding, peaking around layer 8, then pivot to comprehension enhancement. Each FFN output provides potential for vocabulary prediction.
- Core assumption: FFN layers can be interpreted as memory units that encode linguistic knowledge in a structured manner.
- Evidence anchors:
  - [abstract] "Feed-forward networks increase linguistic knowledge encoding until layer 8, then shift to comprehension"
  - [section 4.6] "FFN layers appear to serve as memory units that encode linguistic knowledge toward word prediction, with their ability increasing with the layer count until peaking around layer 8"
  - [corpus] Moderate evidence - corpus includes papers on FFN interpretation and model editing, supporting the concept of FFN as memory units
- Break condition: If FFN layers do not follow a progressive encoding pattern or if comprehension does not become the primary focus after layer 8.

### Mechanism 3
- Claim: Multi-head self-attention distributes functions across different heads, with correlation coefficients increasing with layers, and a minority of attention heads in higher layers showing smaller values due to individual bias in eye-tracking experiments.
- Mechanism: Self-attention heads extract various aspects of the sequence, with their capacity deepening with layer increase. The distribution of functions across heads allows for specialized processing of different linguistic features.
- Core assumption: Self-attention heads can be analyzed individually to understand their specific contributions to the overall model function.
- Evidence anchors:
  - [abstract] "Multi-head self-attention distributes functions across different heads"
  - [section 4.6] "multi-head self-attention is found to be distributed. These attention heads work to promote crucial information, their ability also peaking around layer 8"
  - [corpus] Weak evidence - corpus neighbors focus on attention mechanisms but not specifically on distribution of functions across heads or correlation with human behavioral data
- Break condition: If self-attention heads do not show distributed functions or if their correlation with human behavioral data does not increase with layer depth.

## Foundational Learning

- Concept: Eye-tracking measures and their definitions
  - Why needed here: Understanding eye-tracking measures is crucial for interpreting the correlation between model predictions and human reading patterns
  - Quick check question: What does GD (Gaze Duration) measure in eye-tracking studies?
- Concept: RNN gate mechanisms (reset, update, forget, input, output gates)
  - Why needed here: Gate mechanisms control information flow in RNN-based models, which is essential for understanding how these models process sequential information
  - Quick check question: How do the reset and update gates in GRU differ from the forget and input gates in LSTM?
- Concept: Transformer architecture components (FFN, multi-head self-attention)
  - Why needed here: Understanding the core components of LLMs is necessary for interpreting how they construct word predictions and process information
  - Quick check question: What is the primary function of the feed-forward network in a transformer block?

## Architecture Onboarding

- Component map:
  - Input embeddings → Multi-head self-attention → Feed-forward network → Output layer
  - RNN-based models: Input embeddings → RNN layers (with gates) → Output layer
  - Key components to understand: FFN as memory units, self-attention head distribution, gate mechanisms in RNNs
- Critical path: From input embeddings through layers to output prediction, with FFN and self-attention as the primary processing components
- Design tradeoffs:
  - RNNs: Better correlation with human patterns but limited by sequential processing
  - Transformers: More powerful but show different prediction patterns
  - FFN layers: Tradeoff between linguistic knowledge encoding and comprehension
- Failure signatures:
  - Low correlation with human behavioral data
  - Unexpected patterns in gate mechanism correlations
  - Inconsistent layer-wise progression in FFN or self-attention capabilities
- First 3 experiments:
  1. Replicate correlation analysis between model predictions and eye-tracking measures to verify findings
  2. Analyze individual self-attention head contributions to understand function distribution
  3. Examine gate mechanism behavior across different layers and tasks to understand information flow control

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the gate mechanisms in RNN-based models differ in their information flow patterns between GRU and LSTM models when correlated with human behavioral data?
- Basis in paper: [explicit] The paper states that gate mechanisms exhibit a strong correlation with human behavioral data, leading to the conclusion that gates handle contextual processing and information flow. Specifically, it mentions that certain gates promote tokens while others eliminate them.
- Why unresolved: The paper does not provide a detailed comparison of how each specific gate in GRU and LSTM models behaves differently in terms of promoting or eliminating information.
- What evidence would resolve it: A detailed analysis comparing the correlation patterns of each gate (reset, update, forget, input, output) in both GRU and LSTM models with various eye-tracking measures would clarify their distinct roles.

### Open Question 2
- Question: What specific aspects of the input sequence do different attention heads in multi-head self-attention focus on, and how does this vary across layers?
- Basis in paper: [explicit] The paper mentions that multi-head self-attention is designed to model different aspects of the input sequence and that functions are distributed across various attention heads. It also notes that correlation coefficients increase with layers.
- Why unresolved: The paper does not specify what each attention head focuses on and how their focus changes across layers.
- What evidence would resolve it: Analyzing the attention weights and their correlation with specific linguistic features or eye-tracking measures for each head across layers would provide insights into their distinct roles.

### Open Question 3
- Question: How does the peak in correlation coefficients around layer 8 in FFN relate to the transition from encoding linguistic knowledge to enhancing comprehension?
- Basis in paper: [explicit] The paper observes that correlation coefficients increase with layers in FFN until peaking around layer 8, after which the focus shifts to enhancing comprehension.
- Why unresolved: The paper does not explain the mechanisms behind this transition or why layer 8 is the peak.
- What evidence would resolve it: Investigating the changes in FFN output representations and their correlation with linguistic features and comprehension measures before and after layer 8 would elucidate this transition.

### Open Question 4
- Question: Why does the GPT-2 model show a distinct pattern of positive correlation with human behavioral data compared to the negative correlations seen in other models?
- Basis in paper: [explicit] The paper notes that GPT-2 exhibits a positive correlation with human behavioral data, unlike other models that show negative correlations. It suggests this might be due to GPT-2's powerful capabilities and its confidence in predicting challenging words.
- Why unresolved: The paper does not explore the underlying reasons for this distinct pattern or how it relates to GPT-2's architecture or training.
- What evidence would resolve it: Comparing the internal representations and prediction patterns of GPT-2 with other models, particularly focusing on how it handles difficult words, would clarify the reasons for this difference.

## Limitations
- The correlation between model predictions and eye-tracking measures shows considerable variation across model types and measures
- The study does not address potential confounding factors in eye-tracking data, such as individual differences among participants or task-specific effects
- The analysis focuses on English text, limiting generalizability to other languages

## Confidence
- **High Confidence**: The overall correlation methodology and findings that RNN-based models show higher correlation with human reading patterns than GPT-2
- **Medium Confidence**: The specific claims about FFN layer progression (peaking at layer 8) and self-attention head distribution, as these rely on complex internal state analysis with limited validation
- **Low Confidence**: The interpretation of gate mechanisms in RNNs controlling information flow, as the relationship between gate values and eye-tracking measures is not clearly established

## Next Checks
1. Conduct cross-validation using different eye-tracking datasets to verify the robustness of correlation findings across populations and reading tasks
2. Perform ablation studies on FFN layers and attention heads to confirm their specific contributions to the observed patterns
3. Test the correlation analysis with models of varying sizes and architectures to establish whether the observed patterns generalize beyond the specific models studied