---
ver: rpa2
title: Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient
  Black-box Attacks
arxiv_id: '2307.11906'
source_url: https://arxiv.org/abs/2307.11906
tags:
- attack
- learning
- deep
- adversarial
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a black-box attack method targeting interpretable
  deep learning systems (IDLSes), which combine classification models with interpretability
  components. The proposed approach uses a microbial genetic algorithm that requires
  no knowledge of the target model or its interpreter.
---

# Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks

## Quick Facts
- arXiv ID: 2307.11906
- Source URL: https://arxiv.org/abs/2307.11906
- Reference count: 29
- The proposed black-box attack achieves up to 100% success rate on ImageNet while maintaining interpretation map similarity

## Executive Summary
This paper presents a novel black-box attack method targeting Interpretable Deep Learning Systems (IDLSes) that combine classification models with interpretability components. The attack uses a microbial genetic algorithm that requires no knowledge of the target model or its interpreter, combining transfer-based attacks from a source model with score-based optimization. The method achieves high attack success rates while producing adversarial examples with attribution maps nearly indistinguishable from benign samples, requiring an average of 188-210 queries per image.

## Method Summary
The attack framework uses a microbial genetic algorithm (MGA) to generate adversarial examples against black-box IDLSes. The method begins with transfer-based adversarial examples generated against a white-box source model (DenseNet-169) using PGD. These examples initialize the MGA population, which then evolves through selection, crossover, and mutation operations to optimize a fitness function that balances classification loss and interpretation similarity. The attack constrains perturbations to edge regions of the input image that intersect with the interpretation map, maintaining similarity in attribution maps while achieving successful misclassification.

## Key Results
- Attack success rate up to 100% on ImageNet dataset
- Average query efficiency of 188-210 queries per image
- Intersection-over-Union (IoU) scores demonstrating high similarity between adversarial and benign interpretation maps
- Adversarial examples with perturbations limited to edge regions maintain visual imperceptibility

## Why This Works (Mechanism)

### Mechanism 1
The microbial genetic algorithm (MGA) optimizes adversarial perturbations without gradient access by evolving a population of candidate perturbations using a fitness function that measures attack success and interpretation similarity. MGA iteratively applies selection, crossover, and mutation to evolve adversarial examples that simultaneously fool the black-box classifier and preserve interpretation map similarity. The fitness function effectively balances classification loss and interpretation similarity such that evolution converges to samples meeting both criteria.

### Mechanism 2
Transfer-based learning from a white-box source model to a black-box target model enables efficient initialization of adversarial samples that are likely to transfer successfully. PGD-based adversarial examples generated against a source model are used as initial seeds for the MGA population, providing a starting point closer to successful adversarial examples for the target model. Adversarial examples exhibit transferability between different but related models trained on the same dataset, enabling this transfer-based initialization strategy.

### Mechanism 3
The edge-constrained perturbation strategy ensures adversarial changes occur in regions deemed important by the model's interpretation, maintaining interpretation map similarity while enabling successful misclassification. The loss function includes a term that constrains perturbations to edge regions of the input image that intersect with the interpretation map, limiting visible changes while targeting critical features. Adversarial perturbations applied to edge regions that intersect with important features can both fool the classifier and preserve the overall interpretation map structure.

## Foundational Learning

- Concept: Genetic Algorithms and Evolutionary Optimization
  - Why needed here: Understanding how MGA works is critical for implementing and debugging the attack optimization process.
  - Quick check question: How does the microbial genetic algorithm differ from traditional genetic algorithms in its selection process?

- Concept: Transferability of Adversarial Examples
  - Why needed here: The attack relies on successful transfer from white-box to black-box models, requiring understanding of when and why this occurs.
  - Quick check question: What factors influence the success rate of adversarial example transferability between different models?

- Concept: Interpretable Deep Learning Systems and Attribution Maps
  - Why needed here: The attack specifically targets both the classifier and its interpreter, requiring understanding of how attribution maps work and how they can be manipulated.
  - Quick check question: How does Class Activation Mapping (CAM) generate attribution maps, and why are they vulnerable to adversarial attacks?

## Architecture Onboarding

- Component map: Black-box target model (classifier) ← Query interface ← MGA optimizer ← Transfer-based initialization ← Source model (white-box) ← Interpreter (CAM)
- Critical path: Initial transfer-based adversarial examples → MGA population initialization → Iterative evolution (selection, crossover, mutation) → Attack success validation → Interpretation map comparison
- Design tradeoffs: Query efficiency vs. attack success rate; perturbation magnitude vs. visual imperceptibility; complexity of fitness function vs. convergence speed
- Failure signatures: Low attack success rate despite high query count; large discrepancy between adversarial and benign interpretation maps; transfer failure from source to target model
- First 3 experiments:
  1. Verify transfer success by generating PGD examples against source model and testing against target model without MGA
  2. Test MGA optimization with synthetic fitness function to validate convergence behavior
  3. Validate interpretation map similarity preservation by comparing CAM outputs for benign and adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would the proposed attack be against more diverse and complex interpreters beyond CAM, such as those using attention mechanisms or different visualization techniques?
- Basis in paper: The paper primarily evaluates the attack against CAM interpreter, mentioning other interpretability methods like LIME and SHAP only in passing as "post-hoc interpretability."
- Why unresolved: The evaluation is limited to one interpreter type, leaving uncertainty about transferability across different interpretability methods.
- What evidence would resolve it: Systematic testing against multiple interpreter types (attention-based, perturbation-based, gradient-based) with quantitative comparison of attack success rates and IoU scores.

### Open Question 2
- Question: What is the relationship between query efficiency and attack success rate when scaling to larger models or different datasets?
- Basis in paper: The paper reports average queries (188-210) but acknowledges that "the attack is limited to a maximum of 50,000 queries" without exploring scalability.
- Why unresolved: The study uses fixed query limits and specific models without examining how efficiency changes with model complexity or dataset size.
- What evidence would resolve it: Experiments varying dataset size, model complexity, and query budget to establish scaling relationships between efficiency and success rates.

### Open Question 3
- Question: How would the proposed attack perform under real-world defensive mechanisms such as adversarial training or detection systems?
- Basis in paper: The authors note that "our results highlight the need for improved IDLS security" and mention defensive black-box models in evaluation questions, but do not actually test against defenses.
- Why unresolved: The evaluation focuses on undefended models, leaving open whether the attack would maintain effectiveness against practical security measures.
- What evidence would resolve it: Testing the attack against models with adversarial training, detection mechanisms, or other defensive strategies while measuring changes in success rates and query requirements.

## Limitations
- Attack assumes access to an interpretable component (CAM), which may not be present in all deep learning systems
- Results are specific to ImageNet and two particular model architectures (DenseNet-169 and ResNet-50)
- Edge-constrained perturbation strategy may not generalize to all types of input data or interpretation methods

## Confidence
- **High confidence**: Overall attack framework and experimental setup are well-supported by detailed methodology descriptions
- **Medium confidence**: Specific effectiveness of the microbial genetic algorithm implementation and edge-constrained perturbation strategy are not fully specified
- **Low confidence**: Generalizability of results beyond tested dataset and specific model architectures

## Next Checks
1. **Transferability validation**: Test the attack against a third, previously unseen model architecture to verify the robustness of transfer-based initialization across different model families.

2. **Interpreter robustness**: Replace the CAM interpreter with alternative attribution methods (e.g., Grad-CAM, Integrated Gradients) to assess whether the attack remains effective when targeting different interpretability components.

3. **Dataset generalization**: Evaluate the attack on non-ImageNet datasets (e.g., CIFAR-10, medical imaging datasets) to determine if the observed vulnerabilities extend beyond natural images with clear object boundaries.