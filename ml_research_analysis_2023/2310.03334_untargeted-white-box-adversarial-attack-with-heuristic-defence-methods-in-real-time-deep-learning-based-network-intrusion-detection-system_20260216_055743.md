---
ver: rpa2
title: Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time
  Deep Learning based Network Intrusion Detection System
arxiv_id: '2310.03334'
source_url: https://arxiv.org/abs/2310.03334
tags:
- adversarial
- attack
- nids
- attacks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of deep learning-based
  network intrusion detection systems (NIDS) to adversarial attacks and proposes defense
  strategies to enhance their robustness. The authors implement four powerful white-box
  adversarial attack techniques (FGSM, JSMA, PGD, and C&W) on the CICIDS-2017 dataset
  and evaluate their performance using metrics like accuracy, precision, recall, F1-score,
  and AUC.
---

# Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System

## Quick Facts
- arXiv ID: 2310.03334
- Source URL: https://arxiv.org/abs/2310.03334
- Reference count: 40
- Key outcome: This paper investigates the vulnerability of deep learning-based network intrusion detection systems (NIDS) to adversarial attacks and proposes defense strategies to enhance their robustness.

## Executive Summary
This study evaluates the robustness of deep learning-based NIDS against white-box adversarial attacks using the CICIDS-2017 dataset. The authors implement four attack methods (FGSM, JSMA, PGD, C&W) and three defense strategies (Adversarial Training, Gaussian Data Augmentation, High Confidence). Results show significant performance degradation under attacks, with accuracy dropping to 56.81% for PGD attacks. Defense strategies improve performance for most attacks, with accuracy recovering to around 98% for FGSM, JSMA, and PGD, but C&W attacks remain challenging to defend against.

## Method Summary
The study uses the CICIDS-2017 dataset, preprocessing and splitting it into training (102,216 samples), validation (34,072 samples), and testing (90,860 samples) sets. A deep neural network with 4 hidden layers (60-40-20-10 neurons) is trained using ReLU and sigmoid activations. Adversarial examples are generated using FGSM (ε=0.003), JSMA (θ=0.03, γ=0.02), PGD (ε=0.003, step=10, iter=100), and C&W (lr=0.02, max_iter=10) methods. The model is evaluated under these attacks, then defense strategies are implemented and evaluated.

## Key Results
- Base NIDS model achieves 98.54% accuracy on clean test data
- Adversarial attacks cause significant performance degradation, with accuracy dropping to 56.81% for PGD attacks
- Adversarial Training, Gaussian Data Augmentation, and High Confidence defenses improve model robustness against FGSM, JSMA, and PGD attacks
- C&W attacks remain challenging to defend against, with accuracy improvements limited to around 71-74%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial Training improves NIDS robustness against FGSM, JSMA, and PGD attacks by retraining with adversarial examples, but shows limited effectiveness against C&W attacks.
- **Mechanism**: The NIDS model is retrained on a dataset augmented with adversarial examples generated by FGSM, JSMA, PGD, and C&W. This exposure helps the model learn to classify both clean and adversarial inputs correctly.
- **Core assumption**: The adversarial examples used in retraining are representative of the attack space the model will encounter in deployment.
- **Evidence anchors**:
  - [abstract]: "These defenses improve the model's performance under attack scenarios, with accuracy increasing back to around 98% for FGSM, JSMA, and PGD attacks after Adversarial Training."
  - [section]: "The retrained model is then evaluated using the previous adversarial perturbed examples. As a result, the NIDS model is significantly improved in terms of classification report, confusion matrix, and AUC score."
  - [corpus]: Weak - no direct mention of AT effectiveness against specific attacks, only general adversarial defense.
- **Break condition**: If the adversarial examples used for retraining do not cover the full attack space, or if the attack strength exceeds the model's learned robustness, performance will degrade.

### Mechanism 2
- **Claim**: Gaussian Data Augmentation smooths model confidence by exploring multiple perturbation directions, providing better defense against C&W attacks compared to AT and HC.
- **Mechanism**: Gaussian noise is added to the input data during inference, encouraging the model to gradually reduce its confidence when moving away from the input sample. This exploration in multiple directions helps mitigate adversarial attacks.
- **Core assumption**: Gaussian noise distribution effectively covers the perturbation space relevant to the attacks.
- **Evidence anchors**:
  - [abstract]: "Gaussian Data Augmentation... are implemented to improve the NIDS robustness under adversarial attack situations."
  - [section]: "In the case of gaussian data augmentation defence, the model is built by adding the noisy counterpart in the dataset... The build classifier improved the overall performance of the NIDS model under the four types of adversarial attack situations."
  - [corpus]: Weak - no specific mention of GDA effectiveness against C&W attacks, only general adversarial defense.
- **Break condition**: If the Gaussian noise parameters are not tuned correctly, or if the attack space is not well covered by the noise distribution, the defense will be ineffective.

### Mechanism 3
- **Claim**: High Confidence defense filters out low-confidence predictions, providing an additional layer of protection against adversarial attacks.
- **Mechanism**: The model's predictions are only accepted when the model is highly confident in its output. This approach is straightforward to implement and can help mitigate adversarial attacks by rejecting uncertain predictions.
- **Core assumption**: Adversarial attacks often reduce model confidence, so filtering low-confidence predictions can help reject adversarial inputs.
- **Evidence anchors**:
  - [abstract]: "High Confidence (HC)... are implemented to improve the NIDS robustness under adversarial attack situations."
  - [section]: "In the high confidence defence [66], the model is rebuilt with the cutoff parameter of 0.05 and apply_predict set to True."
  - [corpus]: Weak - no direct mention of HC effectiveness against specific attacks, only general adversarial defense.
- **Break condition**: If the confidence threshold is set too high, it may reject legitimate inputs, leading to high false positive rates. If set too low, it may not effectively filter out adversarial examples.

## Foundational Learning

- **Concept**: Adversarial Machine Learning (AML) - Why needed here: Understanding AML is crucial for comprehending the nature of the attacks and defenses implemented in this study.
  - Why needed here: The paper investigates the vulnerability of NIDS to adversarial attacks and proposes defense strategies. A solid understanding of AML is necessary to grasp the context and implications of the research.
  - Quick check question: What are the main categories of adversarial attacks in AML, and how do they differ in their objectives and methods?

- **Concept**: Deep Learning (DL) and Neural Networks - Why needed here: The NIDS model is based on deep learning, and understanding its architecture and functioning is essential for interpreting the results and implications of the study.
  - Why needed here: The paper evaluates the performance of a DL-based NIDS under various adversarial attacks and defenses. Knowledge of DL principles and architectures is necessary to understand the model's behavior and the effectiveness of the proposed defenses.
  - Quick check question: What are the key components of a deep neural network, and how do they contribute to the model's learning and prediction capabilities?

- **Concept**: Performance Metrics - Why needed here: The paper evaluates the NIDS model using various performance metrics, and understanding these metrics is crucial for interpreting the results and comparing the effectiveness of different defenses.
  - Why needed here: The study presents results in terms of accuracy, precision, recall, F1-score, and AUC. Familiarity with these metrics and their interpretation is necessary to assess the model's performance under different attack and defense scenarios.
  - Quick check question: How do accuracy, precision, recall, F1-score, and AUC differ in their interpretation, and what do they tell us about a model's performance?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Model Training -> Adversarial Attack Generation -> Base Model Evaluation -> Defense Implementation -> Defended Model Evaluation
- **Critical path**:
  1. Preprocess the CICIDS-2017 dataset
  2. Train the base NIDS model on the preprocessed data
  3. Generate adversarial examples using FGSM, JSMA, PGD, and C&W
  4. Evaluate the base model's performance under adversarial attacks
  5. Implement and evaluate the three defense strategies (AT, GDA, HC)
  6. Compare the performance of the defended models against the base model and each other

- **Design tradeoffs**:
  - Model complexity vs. robustness: Increasing model capacity may improve robustness but also increase computational cost and overfitting risk.
  - Defense effectiveness vs. false positive rate: Stricter defenses may reject more adversarial examples but also more legitimate inputs.
  - Attack strength vs. imperceptibility: Stronger attacks may be more effective but also more noticeable to human observers.

- **Failure signatures**:
  - Significant drop in accuracy, precision, recall, F1-score, and AUC under adversarial attacks
  - High false positive and false negative rates in the confusion matrix
  - Low confidence scores for both clean and adversarial inputs

- **First 3 experiments**:
  1. Train the base NIDS model on the preprocessed CICIDS-2017 dataset and evaluate its performance on the test set.
  2. Generate adversarial examples using FGSM and evaluate the base model's performance on these examples.
  3. Implement Adversarial Training using FGSM-generated adversarial examples and evaluate the defended model's performance on FGSM and JSMA attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are heuristic defense strategies compared to certified defense methods against adversarial attacks in NIDS?
- Basis in paper: [explicit] The paper mentions that heuristic defenses like Adversarial Training, Gaussian Data Augmentation, and High Confidence are explored, while certified defenses are not used. It suggests that certified defenses could be vulnerable to evolving evasion strategies and are less adaptive compared to heuristic defenses.
- Why unresolved: The paper only implements heuristic defenses and does not compare their effectiveness to certified defense methods, leaving the question of which approach is more effective unanswered.
- What evidence would resolve it: Implementing and comparing the performance of certified defense methods alongside heuristic defenses against adversarial attacks in NIDS would provide evidence to determine which approach is more effective.

### Open Question 2
- Question: How do adversarial attacks impact the performance of NIDS in real-world scenarios with dynamic and evolving network traffic patterns?
- Basis in paper: [inferred] The paper acknowledges that static datasets may not capture the dynamic nature of network traffic and adversarial behaviors, suggesting that real-world NIDS systems operate in more complex environments.
- Why unresolved: The study uses static datasets and does not test the NIDS model's performance in real-world scenarios with evolving network traffic patterns, leaving the impact of adversarial attacks in such environments unexplored.
- What evidence would resolve it: Conducting experiments with real-time, dynamic network traffic data and adversarial attacks in NIDS would provide evidence of the impact of adversarial attacks in real-world scenarios.

### Open Question 3
- Question: What are the potential defense strategies against black-box adversarial attacks in NIDS, and how effective are they?
- Basis in paper: [explicit] The paper conceptually discusses the applicability of black-box attacks in real-world implementations but does not empirically validate these attacks or explore defense strategies against them.
- Why unresolved: The paper focuses on white-box attacks and does not empirically investigate the impact of black-box attacks or potential defense strategies, leaving the effectiveness of such strategies unknown.
- What evidence would resolve it: Conducting empirical studies on black-box attacks in NIDS and evaluating the effectiveness of various defense strategies against them would provide evidence of potential defense approaches.

## Limitations
- The study focuses on untargeted attacks only, leaving the effectiveness of defenses against targeted attacks unknown
- The CICIDS-2017 dataset represents a specific network scenario and may not capture the full diversity of modern network environments
- Computational cost of implementing all four defense strategies simultaneously is not addressed, which could impact real-time deployment feasibility

## Confidence
- Attack impact evaluation (accuracy drops to 56.81% for PGD): High confidence
- Defense effectiveness (98% accuracy recovery for FGSM/JSMA/PRD): Medium confidence
- C&W attack robustness: Low confidence

## Next Checks
1. Evaluate the proposed defense strategies against targeted adversarial attacks to assess their robustness in more sophisticated attack scenarios
2. Test the defended NIDS model on alternative network intrusion detection datasets to verify generalization across different network environments
3. Conduct computational efficiency analysis of the defense strategies to determine their feasibility for real-time NIDS deployment under resource constraints