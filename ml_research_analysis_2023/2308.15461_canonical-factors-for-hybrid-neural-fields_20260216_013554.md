---
ver: rpa2
title: Canonical Factors for Hybrid Neural Fields
arxiv_id: '2308.15461'
source_url: https://arxiv.org/abs/2308.15461
tags:
- have
- urough
- tilted
- where
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and improves factored neural fields by addressing
  biases introduced by axis-aligned feature volume decompositions. The authors theoretically
  prove that axis-aligned low-rank grids become inefficient at representing rotated
  signals, leading to up to 2 PSNR reconstruction degradation.
---

# Canonical Factors for Hybrid Neural Fields

## Quick Facts
- arXiv ID: 2308.15461
- Source URL: https://arxiv.org/abs/2308.15461
- Reference count: 40
- Key outcome: TILTED removes axis-alignment bias in factored neural fields, enabling 2x larger baseline performance with half the memory and 25% faster training

## Executive Summary
This paper addresses a fundamental limitation in factored neural fields where axis-aligned feature volume decompositions introduce structural bias that degrades reconstruction quality for rotated signals. The authors prove theoretically that low-rank grids become inefficient at representing rotated versions of axis-aligned signals, leading to significant reconstruction degradation. They propose TILTED (Transform-Invariant Latent DEcomposition), which learns canonicalizing transformations to align scene structure with latent feature decompositions, effectively removing this bias.

## Method Summary
TILTED modifies existing factorization techniques by learning canonicalizing transformations τ for each latent grid factor. The method parameterizes projections as learnable functions with transformation parameters, allowing the model to align the latent decomposition with the underlying signal structure. This creates a more robust, transform-invariant latent decomposition. The implementation uses Riemannian ADAM for tangent-space optimization of rotations and employs coarse-to-fine optimization strategies including dynamic low-pass filtering and two-phase optimization to improve convergence when optimizing over transformations.

## Key Results
- TILTED removes axis-alignment bias, reducing reconstruction degradation from rotated signals
- Enables 2x larger baseline performance with half the memory and 25% faster training
- Reduces artifacts in reconstructed meshes while maintaining or improving PSNR/IoU metrics across multiple tasks

## Why This Works (Mechanism)

### Mechanism 1
Axis-aligned feature volume decompositions introduce structural bias that degrades reconstruction quality when the underlying signal is rotated relative to the coordinate system. The factorization into axis-aligned grids creates a coordinate frame that aligns poorly with rotated structures, leading to increased rank requirements and reconstruction errors. The number of components needed grows dramatically with image resolution for rotated signals.

### Mechanism 2
Learning canonicalizing transformations jointly with factorization parameters can recover efficient representations for rotated signals. By parameterizing projections as learnable functions with transformation parameters τ, the model can align the latent decomposition with the underlying signal structure. This creates a more robust, transform-invariant latent decomposition.

### Mechanism 3
Coarse-to-fine optimization strategies improve convergence when optimizing over transformations, particularly for high-frequency signals. Dynamic low-pass filtering reduces high-frequency artifacts during early training, while two-phase optimization first learns coarse alignment with a rank-constrained bottleneck before refining with full capacity.

## Foundational Learning

- **Concept**: Low-rank matrix factorization and singular value decomposition
  - Why needed here: Understanding why axis-aligned decompositions fail for rotated signals requires grasping how rank relates to signal structure and reconstruction quality
  - Quick check question: If a rank-1 matrix represents an axis-aligned square, what rank would be needed to represent the same square rotated by 45 degrees?

- **Concept**: Fredholm operators and Schmidt decomposition for continuum images
  - Why needed here: The theoretical analysis uses continuum analogues of SVD to characterize the efficiency of low-rank grids for different signal orientations
  - Quick check question: How does the Schmidt decomposition of a rotated square differ from its SVD, and why does this matter for discrete approximations?

- **Concept**: Lie group structure of rotation matrices and exponential maps
  - Why needed here: Implementing TILTED requires parameterizing rotations as elements of SO(2) or SO(3) and optimizing over these manifolds using Riemannian gradient descent
  - Quick check question: Why can't rotations be optimized using standard first-order methods, and how does the exponential map solve this problem?

## Architecture Onboarding

- **Component map**: Input coordinates → Transformation layer (learned rotations τ) → Projection layer → Interpolation layer → Reduction layer → Latent vector Z → MLP decoder → Output (radiance, SDF, etc.)

- **Critical path**: Coordinate transformation → Feature interpolation → Latent reduction → Neural decoding

- **Design tradeoffs**:
  - Number of learned transformations T vs. representational capacity: More transformations increase robustness but add parameters
  - Coarse-to-fine scheduling: When to introduce high-frequency components affects convergence and final quality
  - Transformation parameterization: Unit quaternions for 3D vs. unit complex numbers for 2D affects implementation complexity

- **Failure signatures**:
  - Axis-aligned bias persists: Learned transformations τ remain close to identity, indicating optimization failure or insufficient signal structure
  - High-frequency artifacts: Poor convergence of transformation optimization, often solved by increasing σ or using two-phase approach
  - Memory bloat: Too many transformations relative to latent capacity, suggesting need to reduce T or increase channel count

- **First 3 experiments**:
  1. 2D image reconstruction with random rotations: Compare axis-aligned vs TILTED decompositions on rotated images to verify robustness improvement
  2. SDF reconstruction with K-Planes/VM: Test IoU improvements across multiple objects and channel counts to validate general applicability
  3. NeRF-SyntheticSO(3) rotation test: Measure PSNR drop from random scene rotation to quantify axis-alignment bias removal

## Open Questions the Paper Calls Out

### Open Question 1
What are the theoretical limits of TILTED's transform learning capability when applied to scenes with non-axis-aligned structures beyond simple rotations, such as scaling or shearing transformations? The paper focuses on rotation-based transformations in both theoretical analysis and practical experiments, but real-world scenes often contain non-rigid deformations and scaling variations.

### Open Question 2
How does TILTED's performance scale with the number of learned transforms (T) in terms of both computational efficiency and reconstruction quality, particularly for high-resolution scenes? While the paper demonstrates improvements with TILTED-4 and TILTED-8 variants, it doesn't systematically study how reconstruction quality and computational cost scale with larger numbers of transforms.

### Open Question 3
What are the convergence properties and optimization landscape characteristics of TILTED's joint transformation and factorization optimization in the overparameterized regime with MLP decoders? Theorem 2 proves convergence for a simplified linear model, but practical TILTED implementations use MLPs which introduce different implicit regularization and optimization dynamics that aren't theoretically characterized.

## Limitations
- Analysis focuses on rotation transformations while real-world scenes often contain more complex geometric transformations
- Empirical validation limited to specific datasets and model architectures without exploring generalization across different neural field formulations
- Paper doesn't investigate scaling behavior for extremely high-resolution signals or scenes with complex topology

## Confidence
- **High confidence**: Axis-alignment bias exists and degrades reconstruction quality for rotated signals
- **Medium confidence**: TILTED successfully removes this bias through learned canonicalizing transformations
- **Medium confidence**: Coarse-to-fine optimization strategies improve convergence for transformation learning

## Next Checks
1. **Ablation study on transformation parameterization**: Test alternative parameterizations for τ (e.g., Euler angles vs quaternions) to verify that the observed improvements aren't tied to a specific implementation choice.
2. **Cross-architecture generalization**: Apply TILTED to additional neural field architectures like NeuS or Plenoctrees to confirm the approach's generality beyond the three tested.
3. **High-frequency stress test**: Systematically evaluate TILTED on progressively higher-frequency signals to identify the point where coarse-to-fine strategies become essential, and whether this aligns with the theoretical predictions.