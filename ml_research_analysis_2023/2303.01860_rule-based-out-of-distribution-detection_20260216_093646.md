---
ver: rpa2
title: Rule-based Out-Of-Distribution Detection
arxiv_id: '2303.01860'
source_url: https://arxiv.org/abs/2303.01860
tags:
- data
- training
- table
- operational
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses out-of-distribution (OoD) detection, a critical
  issue for safe deployment of machine learning models. The authors propose a rule-based
  method using eXplainable Artificial Intelligence (XAI) to identify if operational
  data significantly differs from training data.
---

# Rule-based Out-Of-Distribution Detection

## Quick Facts
- arXiv ID: 2303.01860
- Source URL: https://arxiv.org/abs/2303.01860
- Authors: 
- Reference count: 30
- One-line primary result: Rule-based OoD detection method achieves lower FPR and FNR than canonical supervised methods without distributional assumptions.

## Executive Summary
This paper introduces a rule-based approach to out-of-distribution (OoD) detection using eXplainable AI, specifically the Logic Learning Machine (LLM) to generate interpretable rules from training data. The method compares histograms of rule activations between training and operational data using metrics like weighted mutual information and rule-based information, detecting distributional shifts without relying on parametric assumptions. Validated on three complex scenarios—predictive maintenance, vehicle platooning, and cybersecurity—the approach demonstrates high precision and significantly lower false positive and negative rates compared to traditional supervised methods.

## Method Summary
The proposed method uses LLM to generate a ruleset from training data, then creates histograms by counting rule activations for each data split. At operation, the same histogram creation is performed and compared to training histograms using metrics like weighted mutual information (WµI), rule-based information (RBI), and lp norms. OoD is declared if metrics fall outside baseline ranges established from training data. The approach is non-parametric and distribution assumption free, validated on three complex scenarios: predictive maintenance (RUL), vehicle platooning, and covert channels in cybersecurity.

## Key Results
- Achieves lower false positive rates (FPRs) and false negative rates (FNRs) than canonical supervised methods
- Provides both OoD detection and a measure of the distance between in-distribution and out-of-distribution distributions
- Successfully identifies distributional shifts across three complex real-world scenarios

## Why This Works (Mechanism)

### Mechanism 1
Rule-based histograms detect distributional shifts without parametric assumptions by counting how often each rule in the training ruleset is triggered by data splits. At operation, the same counting is done and compared to the training histogram. If the distribution of rule activations changes significantly, it signals OoD. The core assumption is that rule satisfaction frequency is a stable fingerprint of the underlying data distribution in-distribution. Break condition: If rule set does not capture the relevant data patterns, the histogram fingerprint will be uninformative.

### Mechanism 2
Weighted mutual information captures similarity between training and operational data distributions by computing mutual information between histograms of training and operational data splits, then weighting by the average absolute difference in rule hits to correct for position-insensitive bias. OoD is flagged when the weighted MI falls outside the baseline range. The core assumption is that the order of rule hits encodes meaningful distributional information, and weighting by differences enhances sensitivity. Break condition: If the weighting scheme fails to reflect true distributional differences, the metric will misclassify.

### Mechanism 3
Gaussian-based rule-based information (RBI) provides statistical separation between in- and out-of-distribution data by fitting a Gaussian distribution to the hit frequencies from training splits for each rule. At operation, the probability of each hit under these Gaussians is computed and used to calculate entropy-based similarity. OoD is detected if RBI falls outside baseline. The core assumption is that even if the raw data is non-Gaussian, the hit frequencies across multiple training splits can be approximated by Gaussians for statistical comparison. Break condition: If the Gaussian assumption is too crude for the hit frequency distribution, the RBI metric will lose discriminative power.

## Foundational Learning

- Concept: Mutual Information (MI)
  - Why needed here: MI measures statistical dependence between training and operational histograms, indicating distributional similarity.
  - Quick check question: What does it mean if MI between two datasets is zero?

- Concept: Entropy and Conditional Entropy
  - Why needed here: These are used to compute MI and the rule-based information (RBI) metric.
  - Quick check question: If a variable has high entropy, what does that say about its predictability?

- Concept: Leave-One-Out Cross-Validation
  - Why needed here: Used in the RBI algorithm to build baseline distributions from multiple training splits.
  - Quick check question: Why is leave-one-out useful when you have limited data splits?

## Architecture Onboarding

- Component map: Training data -> LLM ruleset -> Rule hits histograms -> Baseline metrics (WµI, RBI, lp norms) -> Operational data -> Rule hits histograms -> Metric computation -> OoD decision
- Critical path: Compute rule hits -> Build histograms -> Compute metrics -> Compare to baseline -> Flag OoD
- Design tradeoffs: Weighted MI is faster but less robust with few operational samples; RBI is more accurate but computationally heavier. lp norms are simple but may miss subtle shifts.
- Failure signatures: High false positive rate may indicate overly sensitive thresholds; high false negative rate may indicate insufficient rule coverage or too coarse a baseline.
- First 3 experiments:
  1. Run on synthetic data with known distributional shift; verify that WµI baseline excludes shifted data.
  2. Test incremental groupwise detection on RUL dataset with increasing window size; observe drift detection timing.
  3. Compare FPR/FNR of the proposed method against u-KNN on DNS dataset to validate robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do distributional assumptions affect the performance of out-of-distribution detection in machine learning models, and what are the specific challenges in avoiding these assumptions?
- Basis in paper: [explicit] The paper discusses the importance of distributional assumption-free approaches in out-of-distribution detection and mentions that most solutions rely on strong distributional assumptions or suppose training in and out probability density functions.
- Why unresolved: The paper highlights the challenges but does not provide a detailed analysis of how these assumptions impact performance or specific methods to avoid them.
- What evidence would resolve it: Comparative studies showing the performance differences between methods with and without distributional assumptions, along with detailed case studies of assumption-free approaches.

### Open Question 2
- Question: What are the limitations of current rule-based out-of-distribution detection methods in terms of computational efficiency and scalability to larger datasets?
- Basis in paper: [inferred] The paper mentions the use of rule-based models and metrics like weighted mutual information and rule-based information, but does not delve into the computational efficiency or scalability issues.
- Why unresolved: The paper focuses on the effectiveness of the methods but does not address potential computational bottlenecks or scalability concerns.
- What evidence would resolve it: Empirical studies comparing the computational time and resource usage of rule-based methods against other OoD detection techniques, especially as dataset size increases.

### Open Question 3
- Question: How can out-of-distribution detection methods be improved to handle cases where the operational data is only slightly different from the training data, as in the yellow zone of the EASA severity levels?
- Basis in paper: [explicit] The paper discusses the EASA severity levels and mentions that the proposed method can detect different levels of OoD, but does not specifically address improvements for subtle differences.
- Why unresolved: The paper demonstrates the method's effectiveness in detecting OoD but does not explore enhancements for cases where the differences are minimal.
- What evidence would resolve it: Development and testing of enhanced detection algorithms that can accurately identify subtle shifts in data distributions, along with validation on datasets with varying degrees of similarity between training and operational data.

## Limitations
- The method relies on a proprietary Logic Learning Machine (LLM) with unspecified implementation details, making exact reproduction difficult.
- Metric thresholds and baseline selection lack explicit guidelines, raising reproducibility concerns.
- While tested on three complex scenarios, the method's robustness to diverse data types and real-world deployment conditions remains unproven.

## Confidence
- **Claim: Rule-based histograms effectively capture distributional shifts**: Medium confidence; supported by theoretical framing and experimental results, but lacks direct empirical comparison to established methods in the main text.
- **Claim: Weighted mutual information and RBI metrics improve OoD detection over supervised baselines**: High confidence; quantitative FPR/FNR reductions are reported and methodologically sound.
- **Claim: Non-parametric, distribution-free design enhances robustness**: Medium confidence; theoretically justified, but Gaussian assumption in RBI introduces parametric element.

## Next Checks
1. Apply the method to controlled synthetic datasets with known distributional shifts (e.g., gradual mean/variance changes) to validate sensitivity and specificity of WµI and RBI metrics.
2. Replicate experiments on the DNS dataset and directly compare FPR/FNR against at least two canonical OoD methods (e.g., u-KNN, Mahalanobis distance) under identical conditions.
3. For each application scenario, quantify the proportion of operational data instances not covered by any rule and assess impact on detection performance.