---
ver: rpa2
title: Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse
  Table Data Tasks
arxiv_id: '2310.00789'
source_url: https://arxiv.org/abs/2310.00789
tags:
- table
- tasks
- data
- text
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the impact of unified table-specific pretraining
  for diverse table tasks using encoder-decoder models. The authors pretrain T5 and
  Flan-T5 variants (770M to 11B parameters) on large-scale table-text and table-SQL
  data with self-supervised objectives (MLM, MCP, denoising, generation, and completion),
  then evaluate them on structured knowledge grounding tasks (WikiSQL, WikiTQ, TabFact,
  SQA, CoSQL, Sparc, FetaQA) and table classification tasks.
---

# Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks

## Quick Facts
- arXiv ID: 2310.00789
- Source URL: https://arxiv.org/abs/2310.00789
- Reference count: 40
- Unified pretraining on table-text and table-SQL data improves performance across diverse table tasks

## Executive Summary
This work explores unified pretraining for diverse table tasks using encoder-decoder models. The authors pretrain T5 and Flan-T5 variants (770M to 11B parameters) on large-scale table-text and table-SQL data with self-supervised objectives, then evaluate them on structured knowledge grounding and table classification tasks. Results show consistent improvements across all tasks compared to baselines, with multi-task pre-finetuning further boosting generalization in low-data scenarios. The study demonstrates that unified pretraining enhances both generation and classification performance on table tasks and improves cross-task transfer, especially when data is limited.

## Method Summary
The authors pretrain encoder-decoder models (T5 and Flan-T5) on diverse table data using self-supervised objectives including masked language modeling, mask completion prediction, denoising, generation, and completion. They use a unified serialization format with special tokens to handle different table-related input formats (tables only, tables+text, tables+SQL). After pretraining, they apply multi-task pre-finetuning on mixed task data before fine-tuning on specific downstream tasks. The approach is evaluated on seven diverse table tasks including semantic parsing (WikiSQL, CoSQL, Sparc), question answering (WikiTQ, SQA), table classification (TabFact), and table-to-text generation (FetaQA).

## Key Results
- Pretraining improves performance across all evaluated table tasks compared to baselines
- Scaling to 11B parameters yields continued gains with near-constant compute budget
- Multi-task pre-finetuning significantly boosts generalization in low-data scenarios
- Unified serialization and special tokens are critical for maintaining pretraining gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified pretraining on diverse table-text and table-SQL data with self-supervised objectives improves generalization across both generation and classification tasks.
- Mechanism: The model learns shared representations by processing multiple table-related input formats using denoising and generation objectives, enabling cross-task transfer.
- Core assumption: The inductive biases learned from table understanding during pretraining transfer effectively to downstream tasks with limited labeled data.
- Evidence anchors:
  - [abstract] "Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks."
  - [section] "Our results show that we also achieve improvements when scaling to 11B even when our pretraining method keeps the compute budget near constant for the 11B compared to the 3B models."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.343" - indicates related work exists but this specific unified approach is novel.
- Break condition: If downstream tasks have input formats completely outside the pretraining distribution, transfer may fail.

### Mechanism 2
- Claim: Multi-task pre-finetuning with mixed task data improves performance in low-data scenarios by enabling cross-task transfer.
- Mechanism: By combining diverse task datasets in a multi-task setting, the model learns task-agnostic representations that generalize better when fine-tuned on limited data.
- Core assumption: The mixed task data during pre-finetuning creates a more robust representation space than task-specific pre-finetuning alone.
- Evidence anchors:
  - [abstract] "Multi-task pre-finetuning further boosts generalization under low-data scenarios."
  - [section] "For all the tasks where we perform finetuning in a low-data resource scenario with only 30% of the training data, the degradation with PFT is much less when we apply UniTabPT on top of Flan-T5-XL."
  - [corpus] Weak - no direct corpus evidence for this specific multi-task pre-finetuning approach.
- Break condition: If the pre-finetuning tasks are too dissimilar from the downstream tasks, cross-task transfer benefits may be minimal.

### Mechanism 3
- Claim: Special tokens and unified serialization format are critical for maintaining pretraining gains during downstream finetuning.
- Mechanism: Special tokens act as structural anchors that preserve the table and text relationships learned during pretraining, preventing catastrophic forgetting of table-specific knowledge.
- Core assumption: The model relies on these special tokens to maintain learned table-text associations when processing new tasks.
- Evidence anchors:
  - [abstract] "Unified serialization and special tokens are critical for maintaining gains."
  - [section] "The results of the model Flan-XL+UniTabPT+RF in table 2 show clearly that removing the serialization format results in degradation of the performance on the tasks."
  - [corpus] Weak - no direct corpus evidence for this specific serialization approach.
- Break condition: If the downstream tasks can be effectively processed without special tokens (e.g., tasks with very simple table structures), the format may not be essential.

## Foundational Learning

- Concept: Self-supervised learning with denoising and generation objectives
  - Why needed here: Enables the model to learn table-specific patterns without requiring large amounts of labeled data
  - Quick check question: Can the model recover masked table cells and reconstruct table-text relationships during pretraining?

- Concept: Multi-task learning and cross-task transfer
  - Why needed here: Allows the model to leverage knowledge from diverse table tasks to improve performance on new tasks, especially when data is limited
  - Quick check question: Does pre-finetuning on mixed task data improve performance on downstream tasks compared to no pre-finetuning?

- Concept: Model scaling and compute efficiency
  - Why needed here: Understanding how performance scales with model size while keeping compute budget constant is crucial for practical deployment
  - Quick check question: Do performance gains from pretraining diminish as model size increases from 770M to 11B parameters?

## Architecture Onboarding

- Component map: Tokenizer with special tokens -> Pretraining with self-supervised objectives -> Multi-task pre-finetuning -> Task-specific fin-tuning

- Critical path:
  1. Data preprocessing and serialization with special tokens
  2. Pretraining with denoising and generation objectives
  3. Pre-finetuning with mixed task data
  4. Task-specific finetuning with appropriate format

- Design tradeoffs:
  - Unified format vs. task-specific formats: Unified format enables cross-task transfer but may not be optimal for all tasks
  - Model size vs. compute budget: Larger models may yield better performance but at higher cost
  - Pre-finetuning data diversity vs. relevance: More diverse data enables better generalization but may include irrelevant examples

- Failure signatures:
  - Degraded performance on tasks with input formats different from pretraining data
  - Overfitting to pretraining data if not enough task-specific finetuning
  - Loss of pretraining benefits if special tokens are removed during finetuning

- First 3 experiments:
  1. Pretrain on table-text data only and evaluate on WikiTQ and TabFact to measure gains from unified pretraining
  2. Add pre-finetuning with mixed task data and evaluate on low-resource versions of WikiSQL and SQA to measure cross-task transfer benefits
  3. Remove special tokens during finetuning and compare performance to verify their importance in maintaining pretraining gains

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance on table formats or domains not seen in pretraining remains untested
- Individual contribution of self-supervised objectives to overall performance gains is unclear
- Exact compute budget comparison between model sizes lacks full transparency

## Confidence

**High confidence**: The core finding that unified pretraining improves performance across diverse table tasks is well-supported by extensive experiments across 7 downstream tasks.

**Medium confidence**: The claim about multi-task pre-finetuning improving low-data performance is supported by specific experiments but could benefit from more extensive testing.

**Low confidence**: The exact contribution of individual self-supervised objectives to overall performance gains remains unclear due to lack of targeted ablation studies.

## Next Checks
1. Test pretrained models on table datasets with substantially different formats, structures, or domains to assess generalization limits
2. Systematically remove or isolate each self-supervised objective during pretraining to quantify their individual contributions
3. Examine model performance on rare table patterns and edge cases within benchmark tasks to identify potential blind spots