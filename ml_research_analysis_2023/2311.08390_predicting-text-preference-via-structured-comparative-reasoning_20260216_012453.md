---
ver: rpa2
title: Predicting Text Preference Via Structured Comparative Reasoning
arxiv_id: '2311.08390'
source_url: https://arxiv.org/abs/2311.08390
tags:
- comparison
- text
- arxiv
- preference
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SC, a structured comparative reasoning method
  for improving text preference prediction. The method addresses inconsistencies in
  large language models' comparative reasoning by generating structured intermediate
  representations.
---

# Predicting Text Preference Via Structured Comparative Reasoning

## Quick Facts
- arXiv ID: 2311.08390
- Source URL: https://arxiv.org/abs/2311.08390
- Reference count: 11
- Top baseline outperforms bottom baseline by 7.1 points on average

## Executive Summary
This paper introduces SC (Structured Comparative reasoning), a method for improving text preference prediction by addressing inconsistencies in large language models' comparative reasoning. The approach generates structured intermediate representations through aspect-based comparison, then selects the most consistent comparisons using a pairwise consistency comparator. Experiments across multiple NLP tasks show state-of-the-art performance with average gains of 2.9 points over top baselines and 7.1 points over bottom baselines.

## Method Summary
SC is a structured comparative reasoning method that improves text preference prediction by generating structured intermediate representations. The method consists of three main components: an aspect model that proposes comparison aspects, a comparison model that generates textual comparisons under each aspect, and a consistency comparator that selects the most consistent comparisons. A tournament approach is used to efficiently select the best comparison with O(n) complexity. The method leverages pairwise consistency comparisons, which have been observed to be more reliable than direct consistency scoring for LLMs.

## Key Results
- SC achieves state-of-the-art performance across multiple NLP tasks including summarization, retrieval, and automatic rating
- Average gains of 2.9 points over top baselines and 7.1 points over bottom baselines
- Reduces hallucination and improves consistency in comparative reasoning

## Why This Works (Mechanism)

### Mechanism 1
Structured intermediate representations improve consistency in comparative reasoning. By decomposing the comparison task into aspects and aspect-specific comparisons, the model generates more focused and logically consistent outputs. The consistency comparator ensures similarities and differences are clearly distinguished.

### Mechanism 2
Pairwise consistency comparison is more reliable than direct consistency scoring. Instead of directly scoring consistency, the method uses pairwise comparison to determine which of two comparisons is more consistent, leveraging LLMs' strength in relative judgments.

### Mechanism 3
Tournament-based selection efficiently finds the most consistent comparison. The tournament approach reduces O(n²) comparisons to O(n) by eliminating less consistent comparisons in rounds, assuming transitivity in the consistency comparator.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding CoT is crucial as SC builds upon it by adding structure and consistency checks to improve comparative reasoning.
  - Quick check question: What is the main limitation of standard CoT prompting that SC addresses?

- **Concept**: Aspect-based comparison
  - Why needed here: Aspects guide the comparison process by providing focused dimensions for analysis, which is central to SC's approach.
  - Quick check question: How does SC generate aspects, and why is this important for the comparison process?

- **Concept**: Pairwise ranking and comparison
  - Why needed here: SC uses pairwise consistency comparison rather than direct consistency scoring, which is a key design choice.
  - Quick check question: Why might pairwise comparisons be more reliable than direct consistency scoring for LLMs?

## Architecture Onboarding

- **Component map**: Aspect generation -> Comparison generation -> Consistency selection -> Preference prediction
- **Critical path**: Aspect generation → Comparison generation → Consistency selection → Preference prediction
- **Design tradeoffs**:
  - Structured representation vs. direct prompting: Structured approach improves consistency but adds complexity
  - Tournament approach vs. exact search: Tournament is more efficient but may sacrifice some accuracy
  - Pairwise comparison vs. direct scoring: Pairwise is more reliable but requires more comparisons
- **Failure signatures**:
  - Poor aspect generation leads to unfocused comparisons
  - Inconsistent comparisons fail to clearly distinguish similarities and differences
  - Preference model cannot effectively use the structured representation
- **First 3 experiments**:
  1. Test aspect generation with CoT vs. fixed aspects to compare performance
  2. Compare tournament approach with exact search to measure efficiency vs. accuracy tradeoff
  3. Test different values of |C| (number of comparison samples) to find optimal balance between consistency and cost

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of SC vary across different language models and model sizes? The paper uses different LLM backbones but doesn't systematically compare their relative performance.

### Open Question 2
What is the relationship between the number of samples |C| and the consistency of generated comparisons? The paper mentions |C| is an important hyperparameter but doesn't provide detailed analysis of how different values affect quality.

### Open Question 3
How does SC perform on multilingual datasets and across different domains? All experiments were conducted on English datasets only.

### Open Question 4
What is the impact of different consistency evaluation methods on SC's performance? The paper only uses one consistency evaluation method without exploring alternatives.

### Open Question 5
How does SC's structured reasoning approach compare to end-to-end fine-tuning approaches? The paper focuses only on prompting-based approaches and doesn't compare against fine-tuned models.

## Limitations
- Effectiveness depends heavily on quality of aspect generation
- Pairwise consistency comparator reliability across domains not extensively validated
- Tournament approach assumes transitivity in consistency comparisons

## Confidence

**High confidence**: The overall methodology and experimental setup are sound and well-documented
**Medium confidence**: Claims about improved consistency and reduced hallucination are supported by results but could benefit from more extensive ablation studies
**Low confidence**: The assumption of transitivity in the consistency comparator tournament approach lacks direct empirical validation

## Next Checks

1. Conduct an ablation study testing SC performance with different aspect generation strategies (CoT vs. self-consistency vs. fixed aspects)
2. Evaluate the transitivity assumption in the consistency comparator by testing whether A > B and B > C consistently implies A > C
3. Measure the trade-off between tournament efficiency and accuracy by comparing the tournament approach against exact pairwise comparison of all generated comparisons for small |C| values