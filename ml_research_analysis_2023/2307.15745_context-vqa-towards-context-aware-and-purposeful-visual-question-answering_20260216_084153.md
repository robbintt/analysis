---
ver: rpa2
title: 'Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering'
arxiv_id: '2307.15745'
source_url: https://arxiv.org/abs/2307.15745
tags:
- image
- questions
- context
- images
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Context-VQA, a dataset that pairs images
  with specific web contexts (e.g., shopping, travel, social media) to study how context
  affects the types of questions people ask about images. The authors collected questions
  and answers from crowdsourced participants, comparing two conditions: one where
  participants could see the image plus description, and one with only the description.'
---

# Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering

## Quick Facts
- arXiv ID: 2307.15745
- Source URL: https://arxiv.org/abs/2307.15745
- Reference count: 19
- Key outcome: Context-VQA dataset shows that image context (e.g., shopping, travel, social media) systematically affects question types people ask, especially when images are not visible, supporting more accessible VQA models.

## Executive Summary
This paper introduces Context-VQA, a dataset that pairs images with specific web contexts to study how context influences the types of questions people ask about images. The authors collected questions and answers from crowdsourced participants under two conditions: image plus description, and description only. They found that question types vary systematically across contexts—for example, travel images elicited more "Where?" questions, and social media images elicited more "Who?" questions. Context effects were especially strong when participants could not see the image, highlighting the importance of context-aware VQA models for accessibility. The dataset includes 1,032 questions and 568 answers, with average answer length of 11.03 words, longer than typical VQA datasets.

## Method Summary
The authors curated 35 high-quality images from six web contexts (shopping, travel, health, social media, news, science magazines). Each image was paired with an AI-generated description from BLIP-2. Crowdsourced participants generated questions about images under two conditions: image plus description, or description only. A separate group answered these questions based on the images. Questions were labeled by type (what, who, where, when, why, how, is) using GPT-4, and context effects were analyzed using generalized linear models.

## Key Results
- Question types vary systematically across contexts: travel images elicited 2× more "Where?" questions, and social media/news images elicited 2.8× and 1.8× more "Who?" questions, respectively.
- Context effects are strongest when participants only see the image description, not the image itself.
- Average answer length is 11.03 words, longer than typical VQA datasets, indicating more detailed responses.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Context-sensitive VQA datasets improve accessibility by capturing user-specific information needs.
- Mechanism: By pairing images with their web contexts and prompting users to generate questions based on either the image or its description, the dataset captures how different contexts drive different question types. This enables models to learn context-dependent reasoning rather than one-size-fits-all responses.
- Core assumption: Users' questions about an image depend significantly on the context in which the image appears, and this dependency is stronger when the image itself is not visible.
- Evidence anchors:
  - [abstract] "We find that the types of questions vary systematically across contexts... images presented in a travel context garner 2 times more 'Where?' questions, and images on social media and news garner 2.8 and 1.8 times more 'Who?' questions than the average."
  - [section] "We observe clear effects of context, especially in the description-only study... context effects are especially important when participants can't see the image."
- Break condition: If users do not change their question types based on context, or if the context labels are too ambiguous to influence questioning behavior.

### Mechanism 2
- Claim: Description-only prompts yield more context-sensitive questions than image+description prompts.
- Mechanism: When participants only see a description and not the image, they rely more on contextual cues to infer what information is relevant, leading to stronger context-dependent question distributions.
- Core assumption: Lack of direct visual access forces participants to depend on context to guide their questioning, increasing sensitivity to contextual differences.
- Evidence anchors:
  - [section] "We observe clear effects of context, especially in the description-only study... context effects are especially important when participants can't see the image."
  - [section] "We hypothesize this is because the questioners have less information in the description-only study to inform their questions."
- Break condition: If the descriptions are too detailed or image-like, participants may not rely on context as much, weakening the effect.

### Mechanism 3
- Claim: Context-aware VQA datasets enable more naturalistic question distributions reflecting real-world user needs.
- Mechanism: By situating images in diverse web contexts and collecting naturalistic questions and answers, the dataset mirrors the variety of questions users actually ask in different settings, making models more aligned with accessibility goals.
- Core assumption: Real-world image usage involves varied contexts that influence what users want to know, and capturing this variation improves model relevance.
- Evidence anchors:
  - [abstract] "Context-VQA complements VizWiz but is distinct from it, since the images are part of the general public discourse and not directly conditioned on a question."
  - [section] "Current VQA tasks assume a one-size-fits-all approach... However, recent work suggests that different pieces of information become relevant depending on the particular context in which people encountered the image."
- Break condition: If the contexts are too narrowly defined or not representative of actual web usage, the dataset may not capture the full range of user needs.

## Foundational Learning
- Concept: Context dependency in information needs
  - Why needed here: Understanding that the same image can prompt different questions depending on where it appears is central to the dataset's design and purpose.
  - Quick check question: If an image of a person appears in a news article vs. a social media post, what kinds of questions might users ask in each context?
- Concept: Visual question answering (VQA) task formulation
  - Why needed here: Knowing how VQA datasets are typically constructed and how they differ from accessibility-focused ones is key to understanding the novelty of Context-VQA.
  - Quick check question: What is the main difference between traditional VQA datasets and Context-VQA in terms of question generation?
- Concept: Accessibility and assistive technology design
  - Why needed here: The dataset's motivation is rooted in improving image accessibility for blind and low-vision users, so understanding accessibility needs is essential.
  - Quick check question: Why might context-aware image descriptions be more helpful for users who cannot see the image?

## Architecture Onboarding
- Component map:
  Image collection -> Context assignment -> Annotation interface (description-only and image+description) -> Question collection -> Answer collection -> Post-processing (filtering, labeling) -> Dataset analysis
- Critical path:
  1. Source and curate images from diverse web contexts
  2. Conduct context plausibility norming study
  3. Run question generation study (both versions)
  4. Filter and clean collected questions
  5. Run answer generation study
  6. Post-process answers and filter low-quality responses
  7. Analyze question type distributions and context effects
- Design tradeoffs:
  - Balancing context specificity (too broad = less informative; too narrow = less generalizable)
  - Choosing between description-only and image+description versions (realism vs. grounding)
  - Manual vs. automated question type labeling (accuracy vs. scalability)
- Failure signatures:
  - Context effects are weak or inconsistent across images
  - Question and answer quality is low (inappropriate, off-topic, or incomplete)
  - No significant difference between description-only and image+description results
  - Average answer length is too short, indicating superficial responses
- First 3 experiments:
  1. Replicate the context effect analysis: compare question type distributions across contexts for both study versions.
  2. Test model performance: train a context-aware VQA model on Context-VQA and evaluate if it outperforms a context-agnostic baseline.
  3. Analyze answer length and quality: correlate answer length with question type and context, and assess if longer answers provide more useful information.

## Open Questions the Paper Calls Out
- Question: How can VQA models be designed to dynamically adjust question-answering strategies based on context without requiring explicit context labels?
  - Basis in paper: [explicit] The authors argue that VQA models should be context-sensitive but don't propose specific architectural solutions.
  - Why unresolved: The paper identifies the need for context-aware models but does not provide technical implementations.
  - What evidence would resolve it: Developing and testing VQA architectures that can infer context from web page features or user behavior and adjust their question-answering strategies accordingly.

## Limitations
- Sample size is relatively small (35 images, 1,032 questions) limiting generalizability
- Questions are artificially generated based on context labels rather than naturally occurring in real-world contexts
- Dataset may not fully capture the complexity of real web contexts where images appear
- AI-generated descriptions may introduce artifacts that influence question generation

## Confidence
- High confidence in core finding: Context significantly affects question types asked about images
- Medium confidence in practical applicability: While context effects are clear, the extent to which this improves real-world accessibility tools remains to be validated
- Medium confidence in methodology: The two-condition approach (image+description vs description-only) is sound, but the artificial nature of context assignment may limit external validity

## Next Checks
1. Test whether context effects persist with larger, more diverse image sets and naturally occurring context labels from actual web pages
2. Evaluate if context-aware VQA models trained on this dataset show measurable improvements in accessibility scenarios compared to context-agnostic models
3. Conduct user studies with blind and low-vision participants to verify whether context-sensitive questions lead to more useful image descriptions in practice