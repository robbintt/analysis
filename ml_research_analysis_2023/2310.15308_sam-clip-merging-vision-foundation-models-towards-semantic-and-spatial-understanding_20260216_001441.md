---
ver: rpa2
title: 'SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding'
arxiv_id: '2310.15308'
source_url: https://arxiv.org/abs/2310.15308
tags:
- clip
- learning
- segmentation
- image
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to merge vision foundation models
  into a single model that inherits their capabilities. The proposed approach integrates
  multi-task learning, continual learning, and distillation techniques.
---

# SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding

## Quick Facts
- **arXiv ID**: 2310.15308
- **Source URL**: https://arxiv.org/abs/2310.15308
- **Reference count**: 25
- **Key outcome**: Merges SAM and CLIP into a unified vision transformer that achieves new state-of-the-art zero-shot semantic segmentation (+6.8% mIoU on Pascal VOC, +5.9% on COCO-Stuff).

## Executive Summary
This work introduces SAM-CLIP, a method to merge two vision foundation models (SAM and CLIP) into a single vision transformer that inherits their complementary capabilities. The approach combines multi-task learning, continual learning with memory replay, and knowledge distillation to preserve both models' strengths while enabling new synergistic functionalities. By training a shared backbone with both SAM's segmentation head and CLIP's classification head, the model achieves state-of-the-art performance on zero-shot semantic segmentation across 5 benchmarks, outperforming task-specific models by significant margins.

## Method Summary
The method treats model merging as a continual learning problem where SAM's knowledge is preserved while integrating CLIP's capabilities. A two-stage training process is employed: first, a CLIP head is trained on CLIP data with the SAM backbone frozen; second, both heads and the backbone are jointly trained using multi-task distillation with a 1:10 loss weighting (LCLIP:LSAM). The training uses the Merged-41M dataset (40.6M images for CLIP, 0.65M for SAM) with variable resolutions (224/448px for CLIP, 1024px for SAM). Memory replay prevents catastrophic forgetting, enabling the merged model to maintain both spatial segmentation and semantic understanding.

## Key Results
- Establishes new state-of-the-art zero-shot semantic segmentation on 5 benchmarks, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets
- Reduces storage and compute costs compared to deploying SAM and CLIP independently, making it suitable for edge devices
- Maintains CLIP zero-shot classification accuracy above 70% on ImageNet while preserving SAM's instance segmentation mAP above 40 on COCO
- Introduces synergistic functionalities that emerge from merging the two models' capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging SAM and CLIP into a single vision transformer allows the model to inherit complementary capabilities while avoiding catastrophic forgetting.
- Mechanism: The approach uses continual learning via memory replay and multi-task distillation. By fine-tuning the SAM backbone with a combination of SAM and CLIP distillation losses on small subsets of the original datasets, the model retains SAM's spatial segmentation skills and acquires CLIP's semantic understanding without requiring full retraining.
- Core assumption: Distillation from frozen teacher models on replayed data is sufficient to transfer knowledge without full-scale pre-training.
- Evidence anchors: [abstract] "Our method integrates techniques of multi-task learning, continual learning, and distillation techniques."; [section 3] "We treat model merging as a continual learning problem..."
- Break condition: If replayed data is too small or unrepresentative, the distillation may fail to preserve both sets of capabilities.

### Mechanism 2
- Claim: The multi-head architecture enables zero-shot transfer to new tasks like semantic segmentation by combining learned spatial and semantic features.
- Mechanism: The merged model uses a shared backbone for both SAM's instance segmentation head and CLIP's text-to-image classification head. Because the backbone now contains both spatial and semantic representations, it can support a new zero-shot semantic segmentation task that requires both skills.
- Core assumption: The merged backbone truly integrates both feature types, not just one dominating the other.
- Evidence anchors: [abstract] "SAM-CLIP not only retains the foundational strengths of SAM and CLIP but also introduces synergistic functionalities..."; [section 4.2] "We extend our evaluation to (text-prompted) zero-shot semantic segmentation over 5 datasets..."
- Break condition: If one head's gradients dominate during training, the other capability may be degraded or lost.

### Mechanism 3
- Claim: Using a variable input resolution strategy during training improves the model's adaptability for both high-res segmentation and low-res classification tasks.
- Mechanism: The training pipeline applies different resolutions for CLIP distillation (224/448px) and SAM distillation (1024px), allowing each head to specialize optimally while sharing the same backbone.
- Core assumption: Different heads can adapt to different resolutions without conflicting updates to the shared backbone.
- Evidence anchors: [section 4.1] "We employ variable resolutions of 224/448px for the CLIP distillation via the variable batch sampler approach..."; [section 4.2] "SAM-CLIP can benefit from 336px resolution for zero-shot image classification..."
- Break condition: If the backbone cannot generalize across resolutions, performance may drop for one or both tasks.

## Foundational Learning

- **Concept: Continual Learning and Catastrophic Forgetting**
  - Why needed here: Merging two pretrained models risks overwriting the learned parameters of the first model when training on the second task. Continual learning techniques are required to preserve both capabilities.
  - Quick check question: What would happen if you fine-tuned SAM on CLIP data without any replay or distillation? (Expected answer: SAM would lose its segmentation abilities.)

- **Concept: Knowledge Distillation**
  - Why needed here: Instead of retraining from scratch, the approach distills knowledge from frozen teacher models (original SAM and CLIP) into the merged student model using small replay datasets.
  - Quick check question: How does distillation differ from standard fine-tuning in this context? (Expected answer: Distillation uses soft targets from a teacher model rather than just ground truth labels.)

- **Concept: Multi-task Learning with Shared Backbone**
  - Why needed here: Both SAM and CLIP tasks use the same vision transformer backbone, so the model must learn to balance the gradients and losses from both tasks without interference.
  - Quick check question: Why is a shared backbone beneficial for edge deployment? (Expected answer: It reduces memory and compute cost compared to running two separate models.)

## Architecture Onboarding

- **Component map**: Shared ViT-B/16 backbone (initialized from SAM) -> HeadSAM (initialized from SAM mask decoder) + HeadCLIP (3-layer transformer + pooling + MLP, initialized randomly) + Frozen SAM prompt encoder + Frozen CLIP text encoder
- **Critical path**: 1) Load Merged-41M dataset; 2) Train Stage 1: Freeze backbone, train HeadCLIP on CLIP data (20 epochs); 3) Train Stage 2: Unfreeze backbone, jointly train HeadCLIP and HeadSAM on both datasets (16 epochs); 4) Optional Stage 3: Fine-tune HeadCLIP for 1024px resolution; 5) Evaluate on zero-shot classification, instance segmentation, and semantic segmentation benchmarks
- **Design tradeoffs**: Memory vs. performance (smaller replay datasets reduce memory but risk forgetting); Resolution (multiple resolutions increase complexity but improve head specialization); Head complexity (adds latency but enables new tasks)
- **Failure signatures**: HeadCLIP collapse (zero-shot classification accuracy drops sharply); HeadSAM failure (instance segmentation mAP degrades); Backbone forgetting (both tasks suffer, classification degrades first)
- **First 3 experiments**: 1) Ablation: Train without SAM replay data and observe catastrophic forgetting in instance segmentation; 2) Resolution test: Compare 224px vs 336px inference accuracy on zero-shot classification; 3) Multi-head test: Run both heads together on the same image for zero-shot semantic segmentation and measure mIoU improvement

## Open Questions the Paper Calls Out
- How does the performance of SAM-CLIP vary with different ratios of DSAM to DCLIP during training?
- What is the impact of using different image resolutions for the CLIP head during inference on SAM-CLIP's performance?
- How does the performance of SAM-CLIP compare to other model merging techniques, such as weight averaging or parameter importance analysis?

## Limitations
- Dataset composition details are unspecified, particularly how subsets were selected from multiple sources
- Knowledge transfer completeness untested at minimal dataset scales
- Edge deployment benefits are theoretical without empirical hardware validation
- Performance on out-of-distribution datasets not evaluated

## Confidence
- **High Confidence**: Zero-shot semantic segmentation SOTA results (+6.8% on Pascal VOC, +5.9% on COCO-Stuff) with extensive benchmarking
- **Medium Confidence**: Continual learning mechanism preventing catastrophic forgetting supported by ablation studies
- **Low Confidence**: Edge deployment benefits stated but not empirically validated with hardware measurements

## Next Checks
1. **Edge Hardware Validation**: Measure actual latency, memory usage, and power consumption of SAM-CLIP versus separate SAM and CLIP deployments on representative edge devices (e.g., NVIDIA Jetson, mobile CPU)
2. **Dataset Size Sensitivity**: Systematically vary the size of replay datasets (SA-1B subset and CLIP distillation set) from 1% to 100% and measure the impact on zero-shot semantic segmentation performance
3. **Zero-Shot Generalization Stress Test**: Evaluate SAM-CLIP on out-of-distribution datasets not seen during any training (e.g., medical imaging, satellite imagery, fine-grained classification tasks) to assess generalization beyond benchmark datasets