---
ver: rpa2
title: Vulnerability-Aware Instance Reweighting For Adversarial Training
arxiv_id: '2307.07167'
source_url: https://arxiv.org/abs/2307.07167
tags:
- adversarial
- examples
- training
- natural
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel instance-wise reweighting scheme
  for adversarial training (AT) that improves robustness against strong white-box
  and black-box attacks. The method considers both the intrinsic vulnerability of
  each natural example and the information loss on its adversarial counterpart.
---

# Vulnerability-Aware Instance Reweighting For Adversarial Training

## Quick Facts
- arXiv ID: 2307.07167
- Source URL: https://arxiv.org/abs/2307.07167
- Authors: 
- Reference count: 34
- This paper introduces a novel instance-wise reweighting scheme for adversarial training (AT) that improves robustness against strong white-box and black-box attacks.

## Executive Summary
This paper introduces VIR (Vulnerability-aware Instance Reweighting), a novel instance-wise reweighting scheme for adversarial training that improves robustness by focusing on naturally vulnerable examples and their information loss under adversarial attacks. The method combines two components: vulnerability scoring based on model confidence on true labels, and information loss measured by KL divergence between natural and adversarial predictions. When applied to vanilla AT and TRADES, VIR consistently outperforms existing reweighting methods across multiple datasets and architectures, showing significant gains especially against strong attacks like CW, Autoattack, and black-box attacks.

## Method Summary
VIR introduces a novel instance-wise reweighting scheme for adversarial training that assigns higher weights to adversarial examples derived from naturally vulnerable inputs (low confidence in true class) and those exhibiting high information loss under attack. The reweighting function combines a vulnerability score (Sv) based on exponential decay of model confidence with an information loss score (Sd) computed via KL divergence between natural and adversarial predictions. This multiplicative combination, plus a β offset, creates a balanced weighting scheme that outperforms single-factor methods. The approach is evaluated on CIFAR-10, CIFAR-100, SVHN, and TinyImageNet with ResNet and WideResNet architectures.

## Key Results
- VIR improves robust accuracy by 1.1% against PGD-100 attack compared to GAIRAT on CIFAR-10 with ResNet-18
- Against Autoattack, VIR-AT achieves 47.4% robust accuracy vs 44.8% for GAIRAT on CIFAR-10
- VIR shows consistent improvements across multiple datasets including CIFAR-100, SVHN, and TinyImageNet
- Ablation studies confirm both vulnerability scoring and information loss components are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reweighting robust losses based on natural example vulnerability improves adversarial training by focusing model capacity on harder-to-classify examples.
- Mechanism: The method assigns higher weights to adversarial examples derived from naturally vulnerable inputs (low confidence in true class) using the exponential weighting function Sv(xi,yi) = α·e^(-γfθ(xi)yi). This ensures these examples contribute more to gradient updates during training.
- Core assumption: Vulnerable examples are systematically underrepresented in standard adversarial training and their adversarial counterparts are critical for achieving balanced robustness across classes.
- Evidence anchors:
  - [abstract]: "The proposed reweighting function takes consideration of the intrinsic vulnerability of individual examples used for adversarial training"
  - [section 4.1]: "We argue that a natural example being vulnerable is suggestive of the features of that example correlating to another (wrong) but semantically similar class"
  - [corpus]: Weak - no direct citation to existing vulnerability-aware methods in the corpus
- Break condition: If the model's confidence estimate fθ(xi)yi becomes unreliable (e.g., due to poor initialization or training collapse), the vulnerability scoring fails.

### Mechanism 2
- Claim: Incorporating information loss between natural and adversarial examples via KL divergence captures the unique impact of adversarial attacks on each example.
- Mechanism: The method computes Sd(xi,x'i) = KL(fθ(xi)∥fθ(x'i)) to measure discrepancy between predictions on natural and adversarial variants. This discrepancy score captures how much information is lost when an example is attacked.
- Core assumption: Adversarial attacks deplete non-robust features differently across examples, and this depletion is measurable via KL divergence in model predictions.
- Evidence anchors:
  - [section 4.2]: "We propose to score the vulnerability of an example x as a weight for x by the discrepancy between the model's output prediction on x and that on its adversarial variant x'"
  - [section 4.2]: "The depletion of features in the adversarial examples results in different information loss for different examples"
  - [corpus]: Weak - no direct comparison to KL-based weighting methods in the corpus
- Break condition: If the adversarial attack fails to meaningfully change the model's predictions (e.g., very small perturbations), KL divergence becomes uninformative.

### Mechanism 3
- Claim: Combining vulnerability and information loss into a multiplicative weighting function creates a balanced reweighting scheme that outperforms single-factor methods.
- Mechanism: The final weight w(xi,x'i,yi) = Sv(xi,yi)·Sd(xi,x'i) + β combines both vulnerability scoring and KL-based information loss, with β providing a lower bound to prevent weights from vanishing.
- Core assumption: Neither vulnerability scoring nor information loss alone is sufficient; their combination captures complementary aspects of example importance.
- Evidence anchors:
  - [section 4.3]: "The proposed Vulnerability-aware Instance Reweighting (VIR) function for assigning importance to the example xi in a training set is as follows: w(xi, x'i,yi) = Sv(xi,yi)·Sd(xi, x'i) + β"
  - [section 5.6.1]: Ablation studies show that neither Sv nor Sd alone matches the performance of their combination
  - [corpus]: Weak - no direct citation to multiplicative combination approaches in the corpus
- Break condition: If β is poorly tuned, the reweighting either becomes too aggressive (overfitting to vulnerable examples) or too conservative (losing the reweighting effect).

## Foundational Learning

- Concept: Adversarial training and PGD-based attacks
  - Why needed here: The method builds on standard adversarial training framework using PGD to generate adversarial examples, then modifies the weighting scheme
  - Quick check question: What is the purpose of the inner maximization and outer minimization in the saddle-point formulation of adversarial training?

- Concept: KL divergence as a measure of distributional discrepancy
  - Why needed here: KL divergence is used to quantify the information loss between natural and adversarial predictions, which forms one component of the reweighting function
  - Quick check question: How does KL divergence behave when two distributions become identical versus completely different?

- Concept: Instance-wise reweighting in optimization
  - Why needed here: The method assigns different weights to different training examples' losses, which is a form of instance-wise reweighting that affects the gradient updates
  - Quick check question: How does instance-wise reweighting differ from class-wise or batch-wise reweighting in terms of gradient computation?

## Architecture Onboarding

- Component map: Data pipeline -> Model architecture -> PGD Attack -> Score Calculation -> Weight Application -> Loss Computation -> Backpropagation
- Critical path: Data → Model → PGD Attack → Score Calculation → Weight Application → Loss Computation → Backpropagation
- Design tradeoffs: The method adds computational overhead for scoring but potentially reduces the number of epochs needed for convergence by focusing on critical examples
- Failure signatures: Poor robustness against strong attacks (CW, Autoattack) compared to baseline, unstable training dynamics, or convergence to poor local minima
- First 3 experiments:
  1. Run standard adversarial training on CIFAR-10 with ResNet-18 to establish baseline performance
  2. Implement vulnerability scoring component (Sv) and test its impact on class-wise accuracy distribution
  3. Add information loss scoring (Sd) and evaluate the combined effect on robustness against PGD-100 attack

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations
- The paper does not address potential bias introduced by vulnerability-aware weighting (e.g., class imbalance effects)
- Computational overhead of computing KL divergence for every example is not discussed
- Limited ablation on hyperparameter sensitivity, particularly for the β offset

## Confidence

- Mechanism 1 (Vulnerability scoring): High - well-established connection between model confidence and example difficulty
- Mechanism 2 (KL information loss): Medium - intuitive but lacks rigorous theoretical justification for why KL divergence captures adversarial robustness
- Mechanism 3 (Multiplicative combination): Medium - empirical evidence supports the combination, but theoretical analysis of interaction effects is limited

## Next Checks

1. Test VIR's performance with different β values to establish sensitivity and optimal ranges
2. Evaluate class-wise accuracy distributions to verify vulnerability-aware weighting doesn't exacerbate existing class imbalances
3. Compare VIR against other KL-based weighting schemes to isolate the contribution of the information loss component