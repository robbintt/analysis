---
ver: rpa2
title: Unified speech and gesture synthesis using flow matching
arxiv_id: '2310.05181'
source_url: https://arxiv.org/abs/2310.05181
tags:
- speech
- synthesis
- gesture
- motion
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Match-TTSG, a unified architecture for jointly
  synthesising speech acoustics and 3D gesture motion from text using optimal-transport
  conditional flow matching (OT-CFM). The method generates both modalities together
  in a single process, capturing their joint distribution.
---

# Unified speech and gesture synthesis using flow matching

## Quick Facts
- arXiv ID: 2310.05181
- Source URL: https://arxiv.org/abs/2310.05181
- Reference count: 0
- Unified architecture jointly synthesises speech acoustics and 3D gesture motion from text using optimal-transport conditional flow matching

## Executive Summary
This paper introduces Match-TTSG, a unified architecture for jointly synthesising speech acoustics and 3D gesture motion from text using optimal-transport conditional flow matching (OT-CFM). The method generates both modalities together in a single process, capturing their joint distribution. It uses a 1D U-Net decoder with Transformers to better capture long-range dependencies and cross-modal coherence compared to previous work. Objective metrics show the proposed method is more parameter and memory efficient than the previous state of the art (Diff-TTSG), while also running 5x faster at synthesis time. Subjective evaluations demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to Diff-TTSG.

## Method Summary
The proposed architecture consists of a text encoder followed by a duration predictor to upsample the encoded features. A 1D U-Net decoder with Transformers then generates both mel spectrograms and 3D joint rotation features in a unified manner. The model is trained using optimal-transport conditional flow matching (OT-CFM), which enables high-quality synthesis with fewer ODE solver steps compared to diffusion models. During inference, both speech and gesture are generated simultaneously from the same decoder, capturing their joint distribution.

## Key Results
- Match-TTSG achieves 5x faster synthesis (0.18s RTF vs 0.93s) compared to Diff-TTSG
- Unified architecture reduces parameters from 58M to 49M and GPU memory from 3.3GB to 1.7GB
- Subjective evaluations show improved speech naturalness, gesture human-likeness, and cross-modal appropriateness over Diff-TTSG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OT-CFM training enables better synthesis realism in much fewer synthesis steps.
- Mechanism: OT-CFM constructs ODE vector fields that are nearly constant, which simplifies the numerical solution and allows accurate sampling with very few discretisation steps.
- Core assumption: The ODE vector fields learned by OT-CFM are easier to solve numerically than those from diffusion models.
- Evidence anchors:
  - [abstract] "The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before."
  - [section 2.2] "Lipman et al. [13] proposed conditional flow matching (CFM), a new, general framework for learning ODEs that represent distributions... This leads to particularly simple – nearly constant – ODE vector fields for synthesis."
  - [corpus] Weak corpus evidence; no direct mention of OT-CFM's simplicity in ODE fields.
- Break condition: If the ODE solver cannot achieve sufficient accuracy with the reduced number of steps, synthesis quality degrades.

### Mechanism 2
- Claim: The unified architecture samples from the joint distribution of speech and gestures instead of two conditionally independent marginals.
- Mechanism: By using a single decoder that outputs both modalities together, the model learns the joint probability distribution P(acoustics, motion | text) rather than factoring it as P(acoustics | text) · P(motion | text).
- Core assumption: The joint distribution can be effectively modeled by a single decoder with a 1D U-Net followed by Transformers.
- Evidence anchors:
  - [abstract] "The proposed architecture is simpler than the previous state of the art... can capture the joint distribution of speech and gestures, generating both modalities together in one single process."
  - [section 3] "For speech, these may be speech acoustics a1:T... whilst motion m1:T is described by a sequence of poses... If speech and motion features are sampled at the same frame rate, they can be concatenated as [a⊺, m⊺]⊺1:T and may be synthesised by a single process (e.g., ODE) in a unified manner."
  - [corpus] No direct corpus evidence about the impact of unified vs. separate distributions on appropriateness.
- Break condition: If the concatenated representation fails to preserve the necessary structure for each modality, the unified decoder may underperform compared to separate decoders.

### Mechanism 3
- Claim: Transformers after Conv1D blocks in the decoder capture longer-range dependencies and improve cross-modal coherence.
- Mechanism: Self-attention over both modalities in the decoder allows the model to plan gestures based on speech cues and vice versa, leading to more contextually relevant multimodal output.
- Core assumption: Self-attention over concatenated audio-motion features improves planning of gestures based on speech and vice versa.
- Evidence anchors:
  - [section 3] "The new architecture uses a 1D instead of a 2D U-Net... To enable unified synthesis... the Match-TTSG decoder uses a 1D instead of a 2D U-Net... We additionally propose to follow the Conv1D blocks in this decoder U-Net by Transformers, which may capture longer-range dependencies and still can be made fast."
  - [corpus] Weak corpus evidence; no direct mention of Transformers improving cross-modal coherence.
- Break condition: If the attention mechanism does not effectively model the relationships between speech and gesture features, the benefit may not materialize.

## Foundational Learning

- Concept: Flow matching (CFM)
  - Why needed here: Provides a training framework that learns ODEs representing probability distributions, enabling high-quality synthesis with fewer steps compared to diffusion models.
  - Quick check question: How does OT-CFM differ from standard CFM in terms of the resulting ODE vector fields?
- Concept: 1D U-Net for multimodal synthesis
  - Why needed here: Avoids the assumption of translation invariance along the frequency-and-joint axis that 2D convolutions make, and handles concatenated audio-motion features properly.
  - Quick check question: Why is translation invariance along the frequency-and-joint axis problematic for joint audio-motion synthesis?
- Concept: Self-attention for cross-modal coherence
  - Why needed here: Allows the model to capture long-range dependencies and relationships between speech and gesture features, improving their appropriateness for each other.
  - Quick check question: How does self-attention over concatenated features differ from separate attention mechanisms for each modality?

## Architecture Onboarding

- Component map:
  Text encoder -> Duration predictor -> Upsampling -> Unified 1D U-Net decoder with Transformers -> Output (mel spectrogram + 3D gesture motion)
- Critical path:
  Text encoding and duration prediction determine the temporal structure, which is then used to upsample the encoded features. The unified decoder then generates both speech and gesture features in a single forward pass.
- Design tradeoffs:
  - Unified decoder vs. separate decoders: Unified decoder captures joint distribution and improves cross-modal appropriateness but may be more complex to train. Separate decoders are simpler but may not capture the relationship between modalities as well.
  - 1D U-Net vs. 2D U-Net: 1D U-Net avoids problematic translation invariance assumptions but may have different receptive field properties.
- Failure signatures:
  - Poor speech naturalness: Check if the audio pathway in the unified decoder is learning effectively. Compare to a separate audio decoder.
  - Jerky or unnatural gesture motion: Verify that the motion pathway is learning properly. Check if the number of ODE steps is sufficient.
  - Low cross-modal appropriateness: Ensure that the self-attention mechanism is effectively modeling the relationships between speech and gesture. Check if the unified decoder is truly learning the joint distribution.
- First 3 experiments:
  1. Compare speech naturalness of the unified decoder (MA-50) to the separate audio decoder from Diff-TTSG (DIFF) to isolate the effect of the unified architecture on speech quality.
  2. Compare gesture human-likeness of the unified decoder (MA-50) to the separate motion decoder from Diff-TTSG (DIFF) to isolate the effect of the unified architecture on motion quality.
  3. Compare cross-modal appropriateness of the unified decoder (MA-50) to the separate decoders from Diff-TTSG (DIFF) to measure the impact of the unified architecture on the relationship between speech and gesture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Match-TTSG architecture perform when scaling to more complex skeletons with additional joints or when incorporating facial expressions and finger movements?
- Basis in paper: [inferred] The paper mentions that the current avatar omits finger motion due to inaccurate mocap and does not incorporate facial expressions, gaze, or lip motion. It also states that future work could involve adding additional modalities to the synthesis.
- Why unresolved: The current implementation focuses on upper-body skeleton joint rotations and does not evaluate performance on more complex skeletal structures or additional expressive modalities.
- What evidence would resolve it: Comparative evaluations of Match-TTSG on datasets with full-body skeletons, facial motion capture, and finger tracking, measuring synthesis quality across these expanded modalities.

### Open Question 2
- Question: How does the quality of synthesized speech and gestures degrade when conditioning on text in languages other than Hiberno English, and what architectural modifications would be needed for robust multilingual performance?
- Basis in paper: [explicit] The paper uses a dataset of Hiberno English speech and mentions future work could incorporate self-supervised representations from larger datasets, implying potential for multilingual extension.
- Why unresolved: The model is only trained and evaluated on one English dialect, leaving the generalization to other languages unexplored.
- What evidence would resolve it: Systematic testing of Match-TTSG on multilingual datasets with diverse phonetic and gestural patterns, measuring cross-lingual synthesis quality and appropriateness.

### Open Question 3
- Question: What is the relationship between the number of ODE-solver steps used during synthesis and the trade-off between computational efficiency and output quality across different types of utterances (e.g., short vs. long, simple vs. complex)?
- Basis in paper: [explicit] The paper demonstrates that using 50 ODE steps instead of 500 provides 5x speedup while maintaining quality, but notes that 25 steps led to jerky motion. It also mentions that synthesis speed scales better to long utterances.
- Why unresolved: The study only examines two fixed step counts (50 and 500) and does not systematically explore the step-count vs. quality relationship across utterance characteristics.
- What evidence would resolve it: Empirical studies varying ODE step counts across controlled sets of utterances with different lengths and complexities, measuring both synthesis speed and perceptual quality metrics.

## Limitations

- The improvement in cross-modal appropriateness (MAS) may be attributable to OT-CFM training rather than the unified architecture, as no ablation compares unified architecture with diffusion training against separate decoders.
- The paper lacks quantitative analysis of cross-modal attention patterns to empirically demonstrate improved cross-modal planning.
- The ablation showing 50 vs 500 ODE steps is tested on only one model variant, leaving robustness across architectural configurations unclear.

## Confidence

**High Confidence Claims:**
- The unified architecture is more parameter and memory efficient than Diff-TTSG (MA-50 has 49M vs 58M parameters, 1.7GB vs 3.3GB GPU memory).
- OT-CFM enables faster synthesis (5x reduction in inference time from 0.93s to 0.18s RTF).
- Speech naturalness and gesture human-likeness show statistically significant improvements over Diff-TTSG in subjective evaluations.

**Medium Confidence Claims:**
- The improvement in cross-modal appropriateness (MAS 3.63 vs 3.47) is directly attributable to the unified architecture rather than the OT-CFM training method.
- Transformers after Conv1D blocks meaningfully improve cross-modal coherence, as opposed to being a general architectural improvement that could work with separate decoders.

**Low Confidence Claims:**
- The specific choice of 50 ODE steps for both modalities is optimal across all conditions, as this is only validated on one model variant.
- The joint distribution modeling capability of the unified architecture cannot be replicated by separate decoders with appropriate conditioning mechanisms.

## Next Checks

1. **Architectural Ablation with Diffusion Training**: Train a unified 1D U-Net decoder (same architecture as MA-50) using the original diffusion training from Diff-TTSG rather than OT-CFM. Compare its cross-modal appropriateness, speech naturalness, and gesture quality against both the separate decoders baseline and the proposed OT-CFM trained unified model. This isolates whether architectural unification or the training method drives the MAS improvement.

2. **Cross-Modal Attention Analysis**: Extract and visualize the attention weights from the Transformer layers in the unified decoder for matched speech-gesture pairs. Quantify how attention patterns change across different utterance types (questions, statements, emotional expressions) and correlate these patterns with the MAS scores to provide empirical evidence for improved cross-modal planning.

3. **ODE Step Sensitivity Across Variants**: Systematically evaluate the impact of ODE step count (5, 10, 25, 50, 100, 500) on speech naturalness, gesture quality, and cross-modal appropriateness across multiple model variants (unified with OT-CFM, unified with diffusion, separate decoders). This reveals whether the 50-step finding is robust or specific to particular architectural choices.