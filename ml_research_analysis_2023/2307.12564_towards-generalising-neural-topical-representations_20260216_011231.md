---
ver: rpa2
title: Towards Generalising Neural Topical Representations
arxiv_id: '2307.12564'
source_url: https://arxiv.org/abs/2307.12564
tags:
- greg
- topic
- distance
- topical
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes to improve neural topic models\u2019 (NTMs)\
  \ generalisation to new corpora by encouraging them to produce similar topical representations\
  \ for semantically similar documents. We achieve this by minimising the Hierarchical\
  \ Topic Transport Distance (HOTT) between documents and their augmentations, where\
  \ HOTT leverages optimal transport to capture semantic distances between topics\
  \ and words."
---

# Towards Generalising Neural Topical Representations

## Quick Facts
- arXiv ID: 2307.12564
- Source URL: https://arxiv.org/abs/2307.12564
- Reference count: 40
- Key outcome: Greg improves NTMs' generalization across corpora with up to 70% improvement in classification accuracy on target corpora from different domains.

## Executive Summary
This paper addresses the challenge of neural topic models (NTMs) generalizing to new, unseen corpora. The authors propose Generalisation Regularisation (Greg), a framework that encourages NTMs to produce similar topical representations for semantically similar documents across different domains. Greg achieves this by minimizing the Hierarchical Topic Transport Distance (HOTT) between documents and their augmentations, leveraging optimal transport to capture semantic distances between topics and words. Extensive experiments demonstrate significant improvements in classification accuracy, clustering quality, and topic coherence when applying Greg to various NTM architectures.

## Method Summary
Greg is a plug-and-play regularization framework that improves NTMs' generalization by encouraging similar topical representations for semantically similar documents. During training on a source corpus, Greg computes the HOTT distance between a document's topic distribution and that of its augmented version. The augmentation creates semantically similar documents through word-level transformations (replacement, insertion, deletion) using pre-trained word embeddings. This distance is added to the NTM's loss function, creating a trade-off between reconstruction accuracy and generalization ability. The framework is compatible with various NTM architectures including VAE-based models like NVDM and ProdLDA, as well as SCHOLAR and CLNTM.

## Key Results
- Greg achieves up to 70% improvement in classification accuracy when transferring NTMs from source to target corpora
- The framework consistently improves topic coherence (NPMI) and clustering quality (TP, TN) across multiple datasets
- Greg demonstrates effectiveness across four different NTM architectures (NVDM, ProdLDA, SCHOLAR, CLNTM)
- HOTT distance regularization outperforms simpler distance metrics like cosine similarity in capturing semantic relationships

## Why This Works (Mechanism)

### Mechanism 1
NTMs trained with Greg learn robust topic distributions because they are explicitly optimized to produce similar representations for semantically similar documents. The Greg loss minimizes HOTT distance between document and augmentations, capturing semantic similarity through word embeddings. This encourages consistent topical representations across documents with similar content from different domains. The core assumption is that semantic similarity between documents can be measured and enforced through augmentations and optimal transport distances. The break condition occurs if augmentations don't preserve semantic meaning or word embeddings fail to capture true relationships.

### Mechanism 2
HOTT effectively measures semantic similarity by considering topic-level and word-level relationships. It computes OT distance between topic distributions where topic distance is measured by WMD (another OT distance between word embeddings). This hierarchical structure allows semantic information to flow from word embeddings through topics to document representations. The core assumption is that semantic similarity between topics can be accurately captured by computing OT distance between their word distributions. The break condition occurs if word embeddings don't capture true semantic relationships or hierarchical OT computation becomes too approximate.

### Mechanism 3
Greg's plug-and-play nature allows improvement across different NTM architectures without architectural changes. It's implemented as an additional regularization term that can be added to existing loss functions. It only requires the encoder to produce topic distributions and access to pre-trained word embeddings. The core assumption is that different NTMs' core learning objectives can be preserved while adding Greg's regularization without causing training instability. The break condition occurs if additional regularization causes optimization difficulties or if the NTM architecture fundamentally conflicts with Greg's assumptions.

## Foundational Learning

- **Variational Autoencoders (VAEs) and amortized variational inference**: NTMs are typically built on VAE or AVI frameworks, and understanding how they learn latent topic distributions is crucial for implementing Greg. Quick check: How does the ELBO objective balance reconstruction accuracy with regularization of the latent space?

- **Optimal Transport (OT) and Sinkhorn algorithm**: Greg uses hierarchical OT distances (HOTT and WMD) to measure semantic similarity, and efficient computation requires understanding entropy-regularized OT. Quick check: What is the computational advantage of using Sinkhorn distance over exact OT in large-scale problems?

- **Text data augmentation techniques**: Greg relies on creating semantically similar document augmentations, and choosing appropriate augmentation strategies is critical for effectiveness. Quick check: How do different augmentation strategies (random drop, insertion, replacement with similar/dissimilar words) affect the semantic similarity of generated documents?

## Architecture Onboarding

- **Component map**: BOW input → Encoder → Topic distribution → HOTT computation with augmentation → Loss combination → Backpropagation
- **Critical path**: BOW input → Encoder → Topic distribution → HOTT computation with augmentation → Loss combination → Backpropagation
- **Design tradeoffs**: 
  - Approximation vs accuracy: Using top-I words for topics reduces computational cost but may lose semantic information
  - Augmentation strength: Higher β creates more diverse augmentations but may reduce semantic similarity
  - OT vs simpler distances: HOTT captures richer semantic information but is more computationally expensive than cosine/Euclidean distances
- **Failure signatures**: 
  - Poor generalization: If CA/TP/TN metrics don't improve on target domains
  - Training instability: If loss becomes NaN or gradients explode during optimization
  - Computational bottleneck: If HOTT computation becomes prohibitively slow for large vocabularies or batch sizes
- **First 3 experiments**:
  1. Run baseline NVDM on 20News → Webs transfer task to establish performance without Greg
  2. Apply Greg with "Highest to Similar" augmentation and default hyperparameters to same task
  3. Compare HOTT vs cosine distance as regularization metric while keeping other parameters fixed

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal value of the hyperparameter γ (regularization weight) for the Generalisation Regularisation (Greg) framework, and how does it vary across different datasets and model architectures? The paper sets γ to 300 for all experiments but doesn't explore its sensitivity. Evidence would require a comprehensive study of γ's impact on performance across different datasets and architectures.

### Open Question 2
How does the performance of Greg compare to other methods for improving the generalization of neural topic models, such as meta-learning or domain adaptation techniques? The paper focuses on Greg's performance but doesn't compare it to other state-of-the-art methods. Evidence would require a comprehensive comparison on benchmark datasets.

### Open Question 3
How does the choice of word embedding model (e.g., Word2Vec, GloVe, BERT) affect the performance of Greg in measuring the semantic distance between topics and words? The paper uses GloVe embeddings but doesn't explore the impact of different models. Evidence would require a systematic study of different word embedding models' effects on Greg's performance.

## Limitations

- The effectiveness of Greg relies heavily on the quality of pre-trained word embeddings, which may not capture true semantic relationships in all domains
- The paper acknowledges that using top-I words for topics introduces approximation, but doesn't quantify the impact on generalization performance
- Greg's effectiveness may be limited to domains where word embeddings are representative of the target corpus vocabulary and semantic relationships

## Confidence

- **High confidence**: Greg improves NTMs' generalization ability across multiple architectures (NVDM, ProdLDA, SCHOLAR, CLNTM) as evidenced by consistent improvements in classification accuracy and clustering metrics on transfer tasks
- **Medium confidence**: The hierarchical OT distance (HOTT) effectively captures semantic similarity between documents, as the mechanism is theoretically sound but experimental validation is limited to specific augmentation strategies and corpus pairs
- **Medium confidence**: The plug-and-play nature of Greg is effective across different NTM architectures, though the paper only demonstrates this with four specific architectures and doesn't explore potential conflicts with non-reconstructive models

## Next Checks

1. **Embedding domain adaptation**: Test Greg with domain-adapted word embeddings (e.g., trained on target corpus) to verify if embedding quality impacts generalization performance

2. **Ablation study on OT approximation**: Compare generalization performance using exact vs approximate HOTT computation to quantify the impact of dimensionality reduction on topic representations

3. **Cross-lingual generalization**: Evaluate Greg's effectiveness on multilingual corpora to test if semantic similarity captured by HOTT transfers across languages when using multilingual embeddings