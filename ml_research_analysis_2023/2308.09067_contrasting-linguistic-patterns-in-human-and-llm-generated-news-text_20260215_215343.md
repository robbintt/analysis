---
ver: rpa2
title: Contrasting Linguistic Patterns in Human and LLM-Generated News Text
arxiv_id: '2308.09067'
source_url: https://arxiv.org/abs/2308.09067
tags:
- human
- texts
- text
- llama
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares human-written news text with LLM-generated
  text across linguistic dimensions. The authors collected contemporary English news
  articles and used them to prompt four LLaMa models to generate comparable text.
---

# Contrasting Linguistic Patterns in Human and LLM-Generated News Text

## Quick Facts
- arXiv ID: 2308.09067
- Source URL: https://arxiv.org/abs/2308.09067
- Authors: 
- Reference count: 8
- Primary result: Human news text shows more varied sentence lengths and richer vocabulary than LLM-generated text, while LLMs overuse numbers, symbols, and pronouns

## Executive Summary
This study quantitatively compares human-written English news articles with text generated by four LLaMa models across multiple linguistic dimensions. The authors analyze contemporary news articles and corresponding LLM outputs to identify systematic differences in vocabulary richness, syntactic structures, psychometric features, and sociolinguistic aspects. Results reveal consistent patterns where human texts demonstrate greater lexical diversity and sentence length variation, while LLM outputs exhibit overuse of certain POS categories and amplified gender biases. The findings suggest that LLMs generate text with statistically distinct linguistic patterns compared to human writers, reflecting their training methodology and statistical nature.

## Method Summary
The study collected 11,133 human-written news articles from the New York Times API (April-July 2023) and used their headlines plus first three words of lead paragraphs as prompts for four LLaMa models (7B, 13B, 30B, 65B parameters). LLM outputs were generated with temperature 0.7, top_p 0.9, repetition_penalty 1.1, and max_tokens 200. Both human and generated texts were processed with the Stanza NLP pipeline for POS tagging, dependency parsing, and constituency analysis. Additional metrics included vocabulary richness (Type-Token Ratio), sentence length distributions, emotion analysis using Hartmann's model, semantic similarity via sentence-transformers, and gender bias measurement through pronoun counting.

## Key Results
- Human text shows significantly more varied sentence lengths compared to LLM-generated text
- LLM outputs systematically overuse numbers, symbols, and pronouns while underusing adjectives and nouns
- Gender bias in human text is amplified in LLM outputs, with male pronouns used approximately twice as often as female pronouns
- Vocabulary richness (TTR) is consistently higher in human text across all measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit systematic lexical and syntactic differences from human text due to training data exposure limits.
- Mechanism: When models are prompted with contemporary news headlines not present in their training data, their generated text shows consistent overuse of certain parts of speech (symbols, numbers, pronouns) and underuse of others (adjectives, nouns), reflecting their statistical tendency to produce content that sounds plausible but lacks the stylistic diversity of human writing.
- Core assumption: The training data does not contain the exact contemporary news headlines used as prompts, ensuring generated text reflects model tendencies rather than memorization.
- Evidence anchors:
  - [abstract] "LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns."
  - [section] "LLaMa models exhibit a pronounced inclination towards relying on categories such as symbols or numbers, possibly indicating an excessive attempt by language models to furnish specific data in order to sound convincing."
  - [corpus] Weak: Corpus neighbors focus on detection and attribution but don't directly validate the specific lexical overuse pattern.
- Break condition: If the models were fine-tuned on recent news data or if the prompt strategy allowed models to access training examples, the differences would be artifacts of memorization rather than generative tendencies.

### Mechanism 2
- Claim: Dependency length optimization patterns differ between human and LLM text, with LLMs approaching but not fully matching human efficiency.
- Mechanism: Humans tend to minimize syntactic dependency lengths for cognitive efficiency, while LLMs generate structures that are statistically similar but slightly less optimal, likely due to their statistical training objective rather than explicit efficiency modeling.
- Core assumption: The Ω optimality metric accurately captures dependency length efficiency differences between human and machine text.
- Evidence anchors:
  - [abstract] "shorter constituents, and more optimized dependency distances" and "LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language)"
  - [section] "Table 4 shows various information with respect to the syntactic dependency arcs... The results indicate that their length and distribution are nearly identical for the four LLMs and the human texts."
  - [section] "Figure 5 displays the distribution of Ω values across sentences for both human and LLM-generated texts. The results indicate that Ω values remain remarkably close between human sentences and those generated by the LLMs, exhibiting almost no discernible difference."
  - [corpus] Weak: Corpus doesn't provide evidence about dependency length optimization specifically.
- Break condition: If the optimality metric is flawed or if the dataset size is too small to capture meaningful differences in dependency structures.

### Mechanism 3
- Claim: Gender bias amplification occurs in LLM outputs relative to human texts due to biased training data propagation.
- Mechanism: LLMs inherit and magnify existing gender biases from their training data because they statistically reproduce patterns without understanding or mitigating social implications, leading to higher male-to-female pronoun ratios in generated text.
- Core assumption: The training data contains gender biases that are statistically represented and amplified through generation.
- Evidence anchors:
  - [abstract] "The sexist bias prevalent in human text is also expressed by LLMs."
  - [section] "Table 9 indicate that the models not only replicate the gender bias present in human texts but actually exacerbate it. On average, the models use male pronouns twice as often as female pronouns."
  - [corpus] Weak: Corpus neighbors discuss detection but don't provide evidence about gender bias amplification specifically.
- Break condition: If the models were explicitly debiased during training or if the analysis method for gender bias is flawed.

## Foundational Learning

- Concept: Dependency parsing and universal part-of-speech tagging
  - Why needed here: The analysis relies on comparing syntactic structures and POS distributions between human and LLM text, requiring accurate parsing of both types of content.
  - Quick check question: Can you explain the difference between dependency parsing and constituency parsing, and why dependency structures are used here for measuring optimization?

- Concept: Lexical diversity metrics (Type-Token Ratio)
  - Why needed here: The study measures vocabulary richness to identify systematic differences in lexical variety between human and machine text.
  - Quick check question: How would you calculate TTR for a given text, and what does a higher or lower ratio indicate about vocabulary usage?

- Concept: Text similarity metrics and embedding models
  - Why needed here: The analysis includes measuring semantic similarity between human and LLM-generated text using sentence transformers.
  - Quick check question: What is the difference between cosine similarity and other distance metrics when comparing sentence embeddings?

## Architecture Onboarding

- Component map:
  Data collection pipeline (NYT API → headline/lead extraction) → LLM generation interface (LLaMa models with controlled prompts) → NLP processing pipeline (Stanza for POS tagging, dependency parsing) → Analysis modules (vocabulary metrics, dependency optimization, sentiment analysis, bias detection) → Evaluation framework (statistical comparison between human and LLM outputs)

- Critical path:
  1. Collect contemporary news articles with metadata
  2. Generate comparable LLM outputs using headlines as prompts
  3. Process both datasets with identical NLP pipelines
  4. Compute and compare linguistic metrics across dimensions
  5. Analyze results for systematic differences

- Design tradeoffs:
  - Token limit (200) vs. natural paragraph length: Balances computational efficiency with comparability
  - Stanza processing vs. other parsers: Chosen for English news text performance
  - Model size range (7B-65B) vs. single model: Enables studying scaling effects on linguistic patterns

- Failure signatures:
  - Similar vocabulary diversity between human and LLM text suggests potential training data overlap
  - Identical dependency length distributions might indicate generation artifacts
  - No gender bias amplification could mean successful debiasing or flawed analysis

- First 3 experiments:
  1. Generate text using same headlines with different temperature settings to observe stability of linguistic patterns
  2. Compare results using different NLP processing tools (e.g., spaCy vs. Stanza) to validate findings
  3. Test with non-news domains to see if patterns generalize beyond journalistic text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the toxicity of LLM-generated news text scale with model size beyond the tested 65B parameter model?
- Basis in paper: [explicit] "we noted a rise in the volume of negative emotions with the models’ size. This aligns with prior findings that associate larger sizes with heightened toxicity"
- Why unresolved: The study only tested models up to 65B parameters. The relationship between toxicity and size may continue or plateau at larger scales.
- What evidence would resolve it: Testing LLM-generated news text using models with 100B+ parameters and comparing emotional and toxic content metrics.

### Open Question 2
- Question: What specific training data preprocessing steps contribute most to reducing or amplifying gender bias in LLM outputs?
- Basis in paper: [inferred] The paper notes that LLaMa models "exacerbate" gender bias present in human texts, and mentions that LLaMa models had "removing toxic content from its data" as preprocessing.
- Why unresolved: While the paper observes increased gender bias in LLM outputs, it doesn't identify which preprocessing steps are responsible or how to mitigate this effect.
- What evidence would resolve it: Controlled experiments testing different preprocessing approaches (data filtering, debiasing techniques, etc.) and their impact on gender bias in generated text.

### Open Question 3
- Question: Would instruction-tuned LLMs (like ChatGPT) produce news text with different linguistic patterns than pre-trained-only models like LLaMA?
- Basis in paper: [explicit] The authors used only pre-trained LLaMA models, while noting that "LLaMa (version 1 models) were not fine-tuned with reinforcement learning with human feedback"
- Why unresolved: The study's findings are specific to pre-trained LLaMA models. Instruction-tuned models may have different tendencies in vocabulary richness, emotional expression, or bias amplification.
- What evidence would resolve it: Replicating the study's methodology using instruction-tuned models like ChatGPT or GPT-4 and comparing the linguistic patterns against both human texts and pre-trained LLaMA outputs.

## Limitations

- The study cannot definitively prove that contemporary news headlines weren't present in pretraining corpora, which would invalidate claims about generative tendencies versus memorization
- The Hartmann (2022) emotion model implementation details remain unspecified, potentially affecting the reliability of sentiment analysis results
- The analysis excludes sentences over 80 tokens without transparent justification, which could bias the syntactic structure comparisons

## Confidence

High Confidence: Human text exhibits more varied sentence lengths than LLM-generated text; systematic differences in POS tag frequencies (overuse of pronouns, numbers, symbols by LLMs; underuse of adjectives and nouns)

Medium Confidence: Dependency length optimization differences between human and LLM text

Low Confidence: Gender bias amplification findings due to simple pronoun counting without context consideration

## Next Checks

1. Replication with alternative NLP tools: Rerun the entire analysis pipeline using different POS taggers (spaCy) and dependency parsers to verify that the systematic differences in linguistic patterns persist across annotation frameworks, particularly for numerical and symbolic content that may be handled differently.

2. Training data verification: Conduct an exhaustive search of the LLaMa training corpus documentation and perform overlap analysis to determine whether any of the contemporary news headlines used in the study appear in the training data, which would directly impact the validity of conclusions about generative tendencies.

3. Domain generalization test: Generate and analyze text from the same LLaMa models using prompts from non-news domains (fiction, academic writing, social media) to determine whether the observed linguistic patterns are specific to journalistic text or represent broader systematic differences in how LLMs generate content across contexts.