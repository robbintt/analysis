---
ver: rpa2
title: 'PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model'
arxiv_id: '2306.02531'
source_url: https://arxiv.org/abs/2306.02531
tags:
- text
- diffusion
- generation
- hotel
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLANNER is a two-stage language diffusion model for generating
  fluent and diverse long-form text. It first learns a fixed number of continuous
  semantic tokens that encode salient paragraph-level information, then applies a
  latent diffusion model to generate these tokens, which are finally decoded into
  raw text.
---

# PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model

## Quick Facts
- arXiv ID: 2306.02531
- Source URL: https://arxiv.org/abs/2306.02531
- Reference count: 34
- Key outcome: PLANNER is a two-stage language diffusion model for generating fluent and diverse long-form text that achieves higher diversity and lower repetition while maintaining strong relevance, outperforming both autoregressive and text diffusion baselines.

## Executive Summary
PLANNER is a two-stage language diffusion model designed to generate fluent and diverse long-form text. It first learns a fixed number of continuous semantic tokens that encode salient paragraph-level information, then applies a latent diffusion model to generate these tokens, which are finally decoded into raw text. This approach enables global semantic control while maintaining fluency, and significantly reduces repetition compared to autoregressive baselines. Evaluated on sentiment-guided generation, long-form text completion, and summarization tasks, PLANNER achieves higher diversity and lower repetition while maintaining strong relevance, outperforming both autoregressive and text diffusion baselines.

## Method Summary
PLANNER consists of two stages: a paragraph embedder that learns to compress text into a fixed set of semantic tokens, and a latent diffusion model that generates these tokens which are then decoded into text. The model first trains a variational autoencoder to learn paragraph embeddings, then applies a latent diffusion model to generate these embeddings, and finally decodes to raw text using a frozen GPT-medium decoder. The approach combines an autoregressive 'decoding' module with a 'planning' module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner.

## Key Results
- Achieves lower repetition (REP-n) than autoregressive baselines across sentiment-guided generation, text completion, and summarization tasks
- Maintains strong relevance metrics (BLEU, ROUGE-L, BERTScore) while improving lexical diversity (Ent-n, DIST-n)
- Demonstrates robustness to ill-composed prompts compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PLANNER reduces repetition by decoupling semantic planning from fluent decoding.
- **Mechanism:** The model first encodes a paragraph into a fixed-length set of continuous semantic tokens, then uses latent diffusion to iteratively refine these tokens. Because diffusion can revise earlier choices, it avoids locking into repetitive patterns that accumulate in autoregressive decoding.
- **Core assumption:** The learned semantic token space preserves enough context to allow coherent reconstruction while remaining low-dimensional enough for efficient diffusion.
- **Evidence anchors:**
  - [abstract]: "The model achieves this by combining an autoregressive 'decoding' module with a 'planning' module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner."
  - [section 4.1]: "The latent diffusion can be additionally conditioned on an external signal to generate the semantic tokens."
  - [corpus]: Weak—no direct comparison of repetition rates with and without the planning step.
- **Break condition:** If the paragraph encoder fails to capture long-range dependencies, the diffusion step cannot recover them, and repetition persists.

### Mechanism 2
- **Claim:** Latent diffusion improves diversity without sacrificing fluency because it operates in a smoother, lower-dimensional space.
- **Mechanism:** By compressing text into 16 latent tokens, PLANNER reduces the search space compared to token-level diffusion. This makes the denoising process more stable and less prone to degenerate sequences, while still allowing global control via conditioning.
- **Core assumption:** The latent space is locally smooth so that small perturbations produce coherent variations in the decoded text.
- **Evidence anchors:**
  - [section 4.1]: "The denoising autoencoder is trained by substituting input tokens with random tokens... to accomplish this, the denoising autoencoder is trained by substituting (Sub) input tokens with random tokens with probability p."
  - [abstract]: "Evaluated on sentiment-guided generation, long-form text completion, and summarization tasks, PLANNER achieves higher diversity and lower repetition while maintaining strong relevance."
  - [corpus]: Weak—no quantitative comparison of diversity metrics across latent dimensions.
- **Break condition:** If the latent space is too sparse, diffusion steps will not produce meaningful variations, leading to low diversity.

### Mechanism 3
- **Claim:** Classifier-free guidance in the latent space allows flexible conditioning without mode collapse.
- **Mechanism:** PLANNER concatenates conditioning embeddings (labels or context) into the diffusion model via cross-attention and adaptive layer norm, enabling the model to generate semantically appropriate text for multiple tasks.
- **Core assumption:** The diffusion backbone can effectively attend to both time embeddings and conditioning signals without overfitting to the conditioning distribution.
- **Evidence anchors:**
  - [section 4.2]: "We applied the classifier-free guidance (CFG) (Ho & Salimans, 2021) during the inference steps."
  - [section 5]: "We set the CFG weights to be 2 and 5 for text completion and summarization tasks, respectively."
  - [corpus]: Weak—CFG weights are reported but no ablation on their effect.
- **Break condition:** If CFG weights are too high, the model may produce repetitive, overconfident outputs; too low, conditioning may be ignored.

## Foundational Learning

- **Concept:** Diffusion probabilistic models (DMs)
  - Why needed here: PLANNER extends DMs to latent semantic spaces, so understanding how DMs denoise over time is essential.
  - Quick check question: What is the role of the signal-to-noise ratio (SNR) in the diffusion loss function?
- **Concept:** Variational autoencoders (VAEs)
  - Why needed here: The paragraph embedder is a VAE that learns a smooth latent distribution; knowing KL balancing prevents posterior collapse.
  - Quick check question: How does the β parameter in the VAE objective control the trade-off between reconstruction quality and latent smoothness?
- **Concept:** Classifier-free guidance (CFG)
  - Why needed here: CFG enables conditioning without a separate classifier; knowing its dropout ratio and guidance scale is key for tuning.
  - Quick check question: What happens to the guidance scale during inference compared to training?

## Architecture Onboarding

- **Component map:** Paragraph Embedder (E) -> Latent Diffusion Model (F) -> GPT-medium Decoder (D)
- **Critical path:** E → latent z → diffusion denoising → D → final text
- **Design tradeoffs:**
  - Fixed latent dimension (k=16) vs. variable-length token sequences: fixed reduces computational load but may lose fine details.
  - Greedy decoding vs. beam search: greedy is faster and avoids compounding errors in latent space; beam search can improve relevance but may reintroduce repetition.
  - CFG guidance scale: higher values increase conditioning fidelity but can reduce diversity.
- **Failure signatures:**
  - High REP-n and low diversity metrics → latent space too smooth or CFG too high.
  - Low BLEU reconstruction → encoder/decoder mismatch or too much noise injection during training.
  - Hallucinations in summaries → decoder over-interprets ambiguous latent tokens.
- **First 3 experiments:**
  1. Vary p (substitution noise) in paragraph embedder; measure BLEUclean, BLEUrobust, PPLint to find optimal smoothness vs. reconstruction trade-off.
  2. Compare DDPM vs. DDIM samplers for diffusion; measure diversity and repetition.
  3. Sweep CFG weights for sentiment-guided generation; measure ACC and REP-n.

## Open Questions the Paper Calls Out
The paper identifies several open questions but doesn't explicitly call out specific directions. The main limitations discussed include the reliance on an autoregressive decoder, the inevitable conversion loss during encoding/decoding, and the potential for future work to explore non-autoregressive decoders.

## Limitations
- The paper relies on a frozen GPT-medium decoder, which may mask weaknesses in the latent diffusion component
- There is an inevitable conversion loss with BLEU scores between 77-87%, though most lexical mismatches maintain similar semantics
- The fixed latent dimension (k=16) may not scale to highly diverse or specialized domains

## Confidence
- **High confidence**: The claim that PLANNER achieves lower repetition than autoregressive baselines is supported by multiple metrics (REP-n, diversity scores) across three distinct tasks.
- **Medium confidence**: The assertion that PLANNER maintains strong relevance while improving diversity is moderately supported by BLEU, ROUGE-L, and BERTScore metrics.
- **Low confidence**: The claim that PLANNER's improvements are specifically due to latent diffusion operating in a "smoother, lower-dimensional space" lacks direct evidence.

## Next Checks
1. **Latent space analysis**: Generate interpolation trajectories between different latent semantic tokens and measure semantic consistency of decoded outputs to validate whether the latent space is indeed locally smooth.
2. **Dimensionality sweep**: Retrain PLANNER with varying latent dimensions (k=8, 16, 32, 64) and measure diversity, repetition, and relevance metrics to test whether benefits scale with latent space size.
3. **Ablation on sampling**: Compare PLANNER against a non-autoregressive decoder with the same paragraph embedder but using greedy or beam search sampling instead of diffusion to isolate whether improvements come from diffusion or simply from non-autoregressive decoding.