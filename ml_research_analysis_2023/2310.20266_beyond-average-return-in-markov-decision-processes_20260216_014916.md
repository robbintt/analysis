---
ver: rpa2
title: Beyond Average Return in Markov Decision Processes
arxiv_id: '2310.20266'
source_url: https://arxiv.org/abs/2310.20266
tags:
- distribution
- bellman
- return
- projection
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper characterizes the statistical functionals of the reward
  that can be exactly computed and optimized in Markov Decision Processes (MDPs).
  The authors address two key questions: how accurately can statistical functionals
  be evaluated using Distributional Reinforcement Learning (DistRL), and which functionals
  can be exactly optimized through dynamic programming?'
---

# Beyond Average Return in Markov Decision Processes

## Quick Facts
- arXiv ID: 2310.20266
- Source URL: https://arxiv.org/abs/2310.20266
- Reference count: 40
- Primary result: Proves that only exponential utilities and the mean can be exactly optimized in MDPs, even with Distributional RL

## Executive Summary
This paper addresses fundamental questions about optimizing statistical functionals of rewards in Markov Decision Processes. The authors characterize which functionals can be exactly computed and optimized, and provide error bounds for approximate evaluation using Distributional Reinforcement Learning. They prove that despite the broader framework of DistRL, the class of exactly optimizable functionals remains limited to exponential utilities and the mean, resolving an important open question in MDP theory.

## Method Summary
The paper employs Distributional Reinforcement Learning with quantile projection to approximate return distributions, then evaluates statistical functionals on these projections. The approach uses dynamic programming principles adapted to distribution spaces, with error bounds derived from Wasserstein distance properties. The theoretical analysis establishes necessary conditions for Bellman optimality and closedness properties of functional families.

## Key Results
- Only exponential utilities and the mean can be exactly optimized in MDPs, even with DistRL
- Error bounds of O(H²∆R/N) for approximating statistical functionals using DistRL
- Bellman closed sets are limited to families of the form {x ↦ x^ℓ exp(λx)|0 ≤ ℓ ≤ L} in the undiscounted setting

## Why This Works (Mechanism)

### Mechanism 1
Distributional RL can only exactly optimize exponential utilities and the mean due to the necessity of Independence and Translation properties. These properties must be satisfied for Bellman optimality, and only exponential utilities and the mean satisfy both in general MDPs.

### Mechanism 2
Policy evaluation using DistRL achieves bounded error through Lipschitz continuity of functionals combined with Wasserstein distance bounds on projected return distributions. The error scales as O(H²∆R/N) where N is the projection resolution.

### Mechanism 3
Bellman closed sets of utilities can be efficiently computed only for specific families that are preserved under the Bellman operator. In undiscounted settings, only exponential-polynomial families maintain this closure property.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Bellman equation**: MDPs provide the framework for sequential decision making, and the Bellman equation enables dynamic programming solutions.
  - Quick check: What is the Bellman equation for Q-function in finite horizon H MDP?
  
- **Distributional Reinforcement Learning (DistRL) and return distribution**: DistRL models the full distribution of returns rather than just expectations, enabling richer policy evaluation.
  - Quick check: How is return distribution ηπh(x,a) defined recursively using reward distribution ϱh and next-state returns?
  
- **Statistical functionals and their properties**: Understanding Lipschitz continuity, Bellman closedness, and Bellman optimality properties is crucial for characterizing optimizable functionals.
  - Quick check: What are the two properties a Bellman optimizable functional must satisfy, and why are they necessary?

## Architecture Onboarding

- **Component map**: MDP model (states X, actions A, transitions p, rewards ϱ, horizon H) -> Policy π (sequence of functions) -> Return distribution η (sum of rewards) -> Bellman operator T (maps distributions) -> Projection operator Π (parametric family) -> Statistical functional s (maps to real numbers)
- **Critical path**: 1) Define MDP and policy π, 2) Compute return distribution η using T, 3) Project η using Π, 4) Evaluate s on projected distribution, 5) Bound error between true and approximated values
- **Design tradeoffs**: Exact vs approximate optimization (DistRL enables broader class but with error), parametric vs non-parametric representation (efficiency vs expressiveness), Lipschitz continuity vs expressiveness (approximation vs capturing properties)
- **Failure signatures**: Large projection error indicates insufficient expressiveness or resolution; violation of Bellman closedness means functionals can't be efficiently computed; non-Lipschitz functionals may lead to unreliable approximations
- **First 3 experiments**: 1) Verify error bound in Proposition 1 on simple MDP with known return, 2) Test characterization of Bellman optimizable functionals across various MDPs, 3) Compare DistRL vs classical DP on applicable MDPs

## Open Questions the Paper Calls Out

### Open Question 1
Are there any other statistical functionals beyond exponential utilities that satisfy the Translation Property? The paper restricts to W1-continuous functionals in Theorem 2, leaving open whether non-continuous functionals might also satisfy this property.

### Open Question 2
Can Bellman Optimizable property be characterized without relying on the Independence Property? The paper proves necessity but not sufficiency of this property for Bellman Optimizability.

### Open Question 3
What is the exact relationship between Bellman Closedness and Bellman Optimizability properties? The paper mentions both but doesn't formally define their relationship.

## Limitations
- Restricted to finite-horizon MDPs, limiting applicability to real-world scenarios
- Assumes bounded returns (ΔR < ∞), which may not hold in practice
- Characterization relies heavily on W1-continuity assumption, potentially excluding relevant functionals

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Only exponential utilities and mean can be exactly optimized | High |
| Error bounds for policy evaluation (O(H²∆R/N)) | Medium |
| Characterization of Bellman closed sets | Medium |

## Next Checks

1. Test error bounds on a wider range of MDPs and functionals, including diverse reward distributions and horizons, and functionals beyond CVaR and expected utility.

2. Explore impact of unbounded returns by relaxing boundedness assumptions or using regularization techniques to understand DistRL behavior in these cases.

3. Compare performance of DistRL and classical DP on infinite-horizon MDPs where discount factors may significantly affect functional optimization.