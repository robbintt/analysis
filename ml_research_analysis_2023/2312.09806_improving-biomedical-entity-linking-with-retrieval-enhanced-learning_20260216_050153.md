---
ver: rpa2
title: Improving Biomedical Entity Linking with Retrieval-enhanced Learning
arxiv_id: '2312.09806'
source_url: https://arxiv.org/abs/2312.09806
tags:
- entity
- bioel
- entities
- biomedical
- mention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces kNN-BioEL, a retrieval-enhanced biomedical
  entity linking method that addresses the challenge of long-tailed biomedical entities.
  The core idea is to incorporate k nearest neighbor retrieval during inference, leveraging
  similar instances from the training corpus as clues for prediction.
---

# Improving Biomedical Entity Linking with Retrieval-enhanced Learning

## Quick Facts
- arXiv ID: 2312.09806
- Source URL: https://arxiv.org/abs/2312.09806
- Reference count: 0
- Primary result: kNN-BioEL achieves up to 2.4% relative improvement in Acc@1 over state-of-the-art biomedical entity linking methods

## Executive Summary
This paper introduces kNN-BioEL, a retrieval-enhanced biomedical entity linking method that addresses the challenge of long-tailed biomedical entities. The core idea is to incorporate k nearest neighbor retrieval during inference, leveraging similar instances from the training corpus as clues for prediction. Additionally, a contrastive learning objective with dynamic hard negative sampling is designed to improve the quality of retrieved neighbors. Extensive experiments on multiple datasets demonstrate that kNN-BioEL outperforms state-of-the-art baselines, achieving up to 2.4% relative improvement in Acc@1. The method shows strong generalization capabilities, especially for rare entities, and is effective even with limited training data.

## Method Summary
kNN-BioEL combines a bi-encoder architecture (with shared SapBERT encoder) with k nearest neighbor retrieval during inference. The method constructs a datastore of mention embeddings and entity labels from training data. During inference, it retrieves top-k nearest neighbors for each mention based on embedding similarity, then interpolates the model's predicted distribution with the kNN distribution using a learned parameter λ. Contrastive learning with dynamic hard negative sampling is used during training to improve embedding quality. The approach is evaluated on four biomedical entity linking datasets (NCBI, BC5CDR, COMETA, AAP) with entity counts ranging from 1,036 to 350,830.

## Key Results
- kNN-BioEL achieves up to 2.4% relative improvement in Acc@1 over state-of-the-art baselines
- The method shows strong performance on rare entities, with up to 4.7% relative improvement in Acc@1
- kNN-BioEL maintains effectiveness even with limited training data, demonstrating good generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-enhanced learning improves generalization for rare biomedical entities by leveraging similar training instances as contextual clues during inference.
- Mechanism: The kNN-BioEL method constructs a datastore of mention embeddings and their corresponding entity labels from the training corpus. During inference, for a given mention, the model retrieves the top-k nearest neighbors from the datastore based on embedding similarity and interpolates the model's predicted distribution with the kNN distribution.
- Core assumption: Similar mentions in the training corpus provide relevant contextual information that can guide the model's prediction, especially for rare entities that may not be well-represented in the training data.
- Evidence anchors:
  - [abstract]: "the core idea is to incorporate k nearest neighbor retrieval during inference, leveraging similar instances from the training corpus as clues for prediction"
  - [section]: "We propose a k nearest neighbor retrieval mechanism that leverages existing instances to provide valuable clues during inference."
  - [corpus]: Weak evidence. The corpus neighbors include papers on BioEL, but none specifically discuss kNN retrieval or retrieval-enhanced learning.
- Break condition: If the retrieved neighbors are not relevant or informative for the given mention, the kNN distribution may not provide useful clues, potentially leading to incorrect predictions.

### Mechanism 2
- Claim: Contrastive learning with dynamic hard negative sampling (DHNS) improves the quality of retrieved neighbors by capturing fine-grained semantic differences between biomedical entities.
- Mechanism: DHNS constructs global negatives using the being-optimized BioEL model to retrieve hard negatives from all entities. These hard negatives are dynamically updated for each epoch, providing more challenging negative samples that help the model learn discriminative text embeddings.
- Core assumption: Hard negatives that are semantically similar but incorrect entities provide valuable learning signals for the model to distinguish between fine-grained biomedical entities.
- Evidence anchors:
  - [abstract]: "a contrastive learning objective with dynamic hard negative sampling is designed to improve the quality of retrieved neighbors"
  - [section]: "To accurately capture fine-grained semantic differences between different biomedical entities...we incorporate a contrastive learning (CL) objective with dynamic hard negative sampling (DHNS) during training."
  - [corpus]: Weak evidence. The corpus neighbors include papers on BioEL, but none specifically discuss contrastive learning with hard negative sampling.
- Break condition: If the hard negatives are not sufficiently challenging or representative of the semantic differences between entities, the model may not learn effective discriminative embeddings.

### Mechanism 3
- Claim: Interpolation between the model's predicted distribution and the kNN distribution balances the model's learned knowledge with the contextual clues from similar instances.
- Mechanism: The final distribution is computed as a weighted combination of the model's predicted distribution (pBioEL) and the kNN distribution (pkNN), with a hyper-parameter λ controlling the interpolation ratio.
- Core assumption: The model's learned knowledge and the contextual clues from similar instances are complementary, and their combination leads to more accurate predictions.
- Evidence anchors:
  - [abstract]: "the model retrieves the top-k nearest neighboring instances as clues from the datastore based on mention embedding similarity and then makes inferences by linearly interpolating the model predicted distribution with their nearest neighbor distribution"
  - [section]: "Finally, we interpolate the kNN distribution pkNN(y|x) with the predicted distribution pBioEL(y|x) using a hyper-parameter λ to product the final distribution"
  - [corpus]: No direct evidence in the corpus neighbors.
- Break condition: If the interpolation ratio λ is not properly tuned, the final distribution may overly rely on either the model's predictions or the kNN distribution, potentially leading to suboptimal performance.

## Foundational Learning

- Concept: Biomedical entity linking (BioEL)
  - Why needed here: The paper focuses on improving BioEL models, specifically addressing the challenge of long-tailed entities.
  - Quick check question: What is the goal of BioEL, and what are the main challenges faced by existing methods?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning with dynamic hard negative sampling is used to improve the quality of retrieved neighbors by capturing fine-grained semantic differences between biomedical entities.
  - Quick check question: How does contrastive learning work, and what is the role of negative sampling in this context?

- Concept: k-nearest neighbors (kNN) retrieval
  - Why needed here: kNN retrieval is used to leverage similar instances from the training corpus as clues for prediction during inference.
  - Quick check question: How does kNN retrieval work, and what are the key considerations when applying it to the BioEL task?

## Architecture Onboarding

- Component map: SapBERT encoder -> Bi-encoder embeddings -> Contrastive learning with DHNS -> kNN retrieval datastore -> Interpolation with kNN distribution

- Critical path:
  1. Train the BioEL model with contrastive learning and DHNS.
  2. Construct the datastore of mention embeddings and entity labels.
  3. During inference, retrieve top-k nearest neighbors for a given mention.
  4. Compute the final distribution by interpolating the model's predictions and the kNN distribution.
  5. Return the entity with the highest probability in the final distribution.

- Design tradeoffs:
  - Using a bi-encoder vs. a cross-encoder: The paper opts for a bi-encoder for computational efficiency, but a cross-encoder may provide better performance at the cost of higher complexity.
  - kNN retrieval vs. relying solely on the model's predictions: kNN retrieval provides additional contextual clues but introduces computational overhead and requires careful tuning of the interpolation ratio.
  - Dynamic hard negative sampling vs. in-batch negative sampling: DHNS provides more challenging negatives but requires additional computation and memory.

- Failure signatures:
  - Poor performance on rare entities: Indicates that the kNN retrieval is not providing useful contextual clues.
  - Overfitting to the training data: Suggests that the model is not generalizing well and may require more diverse training examples or regularization techniques.
  - Computational inefficiency: May arise from large datastore size or inefficient kNN search, requiring optimization of the retrieval process.

- First 3 experiments:
  1. Evaluate the impact of kNN retrieval on model performance by comparing the full kNN-BioEL model with a variant that does not use kNN distribution interpolation.
  2. Assess the effectiveness of DHNS by comparing the full kNN-BioEL model with a variant that uses in-batch negative sampling instead of DHNS.
  3. Analyze the impact of different hyper-parameters, such as the number of neighbors (k) and the interpolation ratio (λ), on the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of kNN-BioEL scale with different ontology sizes and entity distributions beyond the tested datasets?
- Basis in paper: [inferred] The paper tests kNN-BioEL on datasets with varying entity sizes (from 1,036 to 350,830 entities) but does not explore performance across a broader range of ontology sizes or distributions.
- Why unresolved: The study focuses on specific datasets without systematically varying ontology size or distribution to understand scalability limits.
- What evidence would resolve it: Experiments testing kNN-BioEL on ontologies with significantly larger or smaller entity counts, and with different distribution patterns (e.g., more extreme long-tail distributions), would clarify scalability.

### Open Question 2
- Question: How does kNN-BioEL perform when incorporating auxiliary information like mention context or entity descriptions?
- Basis in paper: [explicit] The paper explicitly states it does not consider auxiliary information such as mention context or entity descriptions, despite their potential benefits.
- Why unresolved: The experiments are conducted without these features, leaving their impact on performance unexplored.
- What evidence would resolve it: Ablation studies or comparisons integrating context or descriptions into kNN-BioEL would reveal their contribution to performance.

### Open Question 3
- Question: What is the computational overhead of kNN-BioEL during inference, and how does it compare to real-time application requirements?
- Basis in paper: [explicit] The paper mentions pre-computing entity embeddings and using FAISS for efficient similarity search, but does not provide detailed runtime analysis.
- Why unresolved: While efficiency measures are mentioned, actual runtime metrics or comparisons to baseline methods are not provided.
- What evidence would resolve it: Detailed timing experiments comparing inference speed and resource usage of kNN-BioEL against baselines would clarify its practical applicability.

## Limitations

- Computational overhead: kNN retrieval introduces additional computational complexity during inference, which may limit the method's applicability to large-scale, real-time scenarios.
- Hyperparameter sensitivity: The performance of kNN-BioEL depends on the careful tuning of several hyperparameters, such as the number of neighbors (k) and the interpolation ratio (λ), which may require extensive experimentation.
- Limited exploration of auxiliary information: The paper does not investigate the potential benefits of incorporating auxiliary information, such as mention context or entity descriptions, which could further improve the model's performance.

## Confidence

- High confidence: The core mechanism of kNN-BioEL combining bi-encoder embeddings with k nearest neighbor retrieval during inference (supported by experimental results showing consistent improvements across multiple datasets).
- Medium confidence: The effectiveness of contrastive learning with DHNS in improving retrieved neighbor quality (the ablation study shows benefits, but the exact contribution of DHNS vs. in-batch negatives is not fully isolated).
- Medium confidence: The generalization capabilities of kNN-BioEL, especially for rare entities (demonstrated through experiments with limited training data, but real-world validation is needed).

## Next Checks

1. Evaluate kNN-BioEL on additional biomedical entity linking datasets not included in the original study to assess robustness across diverse entity types and domains.
2. Measure the inference time and memory requirements of kNN-BioEL compared to baseline methods, especially as the size of the entity catalog and datastore grows.
3. Conduct a more extensive exploration of the interpolation ratio λ and the number of neighbors k to identify optimal values and assess the method's sensitivity to these parameters.