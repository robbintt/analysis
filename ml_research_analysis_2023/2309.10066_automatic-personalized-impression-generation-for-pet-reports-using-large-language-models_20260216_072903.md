---
ver: rpa2
title: Automatic Personalized Impression Generation for PET Reports Using Large Language
  Models
arxiv_id: '2309.10066'
source_url: https://arxiv.org/abs/2309.10066
tags:
- impressions
- arxiv
- evaluation
- metrics
- physician
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated large language models (LLMs) for generating
  personalized impressions in whole-body PET reports. Twelve models were fine-tuned
  on 37,370 PET reports using findings as input and impressions as reference, with
  physician identity encoded as an input token.
---

# Automatic Personalized Impression Generation for PET Reports Using Large Language Models

## Quick Facts
- arXiv ID: 2309.10066
- Source URL: https://arxiv.org/abs/2309.10066
- Reference count: 40
- Primary result: PEGASUS model generated PET impressions rated 89% clinically acceptable with 4.08/5 utility score, comparable to human-generated impressions

## Executive Summary
This study evaluated large language models for generating personalized impressions in whole-body PET reports by fine-tuning 12 models on 37,370 clinical reports. The approach encodes physician identity as an input token, enabling models to learn individual reporting styles and generate impressions that match each physician's preferences. Domain-adapted evaluation metrics (BARTScore+PET and PEGASUSScore) showed the highest correlation with physician preferences, and the PEGASUS model achieved 76.7% accuracy in Deauville score prediction for lymphoma cases. Physicians rated 89% of model-generated impressions in their own style as clinically acceptable, demonstrating the potential for personalized medical report automation.

## Method Summary
The study fine-tuned 12 language models (8 encoder-decoder and 4 decoder-only) on a dataset of 37,370 anonymized PET reports using findings as input and impressions as reference. Physician identity was encoded as a special token to enable personalized style learning. Models were evaluated using 30 metrics benchmarked against physician ratings, with domain-adapted BARTScore and PEGASUSScore showing highest correlation. The PEGASUS model was selected for expert evaluation by three nuclear medicine physicians on 24 cases (12 self-generated, 12 from others), assessing quality across six dimensions and overall utility. The approach also demonstrated 76.7% accuracy in Deauville score prediction for lymphoma cases.

## Key Results
- PEGASUS model generated impressions rated 89% clinically acceptable with 4.08/5 utility score, comparable to human-generated impressions (4.03, P=0.41)
- Domain-adapted BARTScore and PEGASUSScore showed highest correlation with physician preferences (Spearman's ρ = 0.568 and 0.563)
- 76.7% accuracy in Deauville score prediction for lymphoma cases
- Encoder-decoder models (PEGASUS, BART) outperformed decoder-only models for PET impression generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding physician identity as a token enables personalized impression generation
- Mechanism: The model learns distinct style patterns associated with each physician identifier, allowing it to generate impressions that reflect individual reporting preferences
- Core assumption: Physicians have consistent and distinguishable reporting styles that can be captured through token-based encoding
- Evidence anchors: [abstract]: "An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles"; [section]: "Leveraging a specific token in the input to encode the reading physician's identity, we enabled LLMs to learn different reporting styles and generate personalized impressions"
- Break condition: If physicians' reporting styles are too inconsistent or context-dependent to be captured by a single token

### Mechanism 2
- Claim: Domain-adapted evaluation metrics correlate better with physician preferences than generic metrics
- Mechanism: Fine-tuning evaluation models (BARTScore, PEGASUSScore) on PET-specific data improves their ability to assess medical impression quality
- Core assumption: General-domain evaluation metrics fail to capture domain-specific nuances in medical report quality assessment
- Evidence anchors: [abstract]: "domain-adapted BARTScore and PEGASUSScore showed the highest correlation (Spearman's ρ = 0.568 and 0.563) with physician preferences"; [section]: "Without adaption to PET reports, the original BARTScore showed lower correlation (ρ=0.474) compared to BARTScore+PET"
- Break condition: If domain adaptation doesn't significantly improve correlation beyond a certain threshold

### Mechanism 3
- Claim: Encoder-decoder architecture outperforms decoder-only models for PET impression generation
- Mechanism: The encoder component efficiently extracts key information from lengthy findings sections, while the decoder generates coherent impressions
- Core assumption: The complexity and length of PET findings requires both effective information compression and generation capabilities
- Evidence anchors: [abstract]: "encoder-decoder models outperformed decoder-only models, with PEGASUS emerging as the top-performing model"; [section]: "the large decoder-only models showed inferior performance in summarizing PET findings compared to the SOTA encoder-decoder models"
- Break condition: If decoder-only models with sufficient pretraining can match encoder-decoder performance on this task

## Foundational Learning

- Concept: Teacher-forcing algorithm
  - Why needed here: Ensures stable training by always providing correct previous tokens during generation
  - Quick check question: What happens to model training stability if you switch from teacher-forcing to scheduled sampling?

- Concept: Beam search decoding with length penalty
  - Why needed here: Generates higher-quality impressions by exploring multiple candidate sequences and penalizing overly short outputs
  - Quick check question: How does increasing beam width affect inference time and output quality in this medical summarization task?

- Concept: Bootstrap resampling for confidence intervals
  - Why needed here: Provides robust statistical estimates when sample sizes are limited and distributions are unknown
  - Quick check question: What's the minimum number of bootstrap samples needed to get stable 95% confidence intervals?

## Architecture Onboarding

- Component map: Input preprocessor → Encoder (BERT2BERT, BART, PEGASUS, T5 variants) or Decoder-only (GPT2, OPT, LLaMA, Alpaca) → Beam search decoder → Evaluation pipeline (30 metrics) → Expert review interface
- Critical path: Input tokenization → Model forward pass → Beam search generation → Post-processing → Quality assessment
- Design tradeoffs: Fine-tuning full models vs. LoRA adaptation (memory vs. performance), encoder-decoder vs. decoder-only (complexity vs. training efficiency), domain adaptation vs. generalization
- Failure signatures: Factual inaccuracies in impressions, inconsistent style encoding, evaluation metrics misalignment with physician preferences, low Deauville score prediction accuracy
- First 3 experiments:
  1. Compare performance of encoder-decoder vs decoder-only models on a small validation set
  2. Test physician style encoding with synthetic physician identifiers
  3. Evaluate domain-adapted vs original evaluation metrics on 50 cases with physician scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models (LLMs) compare to human experts in terms of accuracy and completeness when generating impressions for whole-body PET reports?
- Basis in paper: [explicit] The study compared 12 language models to two NM physicians and found that domain-adapted BARTScore and PEGASUSScore showed the highest correlations with physician preferences, but still did not reach the level of inter-reader correlation.
- Why unresolved: The study only benchmarked evaluation metrics against physician preferences and did not directly compare the accuracy and completeness of LLM-generated impressions to those of human experts.
- What evidence would resolve it: A direct comparison study where LLM-generated impressions are compared to those generated by human experts in terms of accuracy and completeness, using a standardized evaluation method.

### Open Question 2
- Question: Can the performance of LLMs in generating personalized impressions be improved by incorporating additional context or information from the patient's medical history or other relevant sources?
- Basis in paper: [inferred] The study used a corpus of 37,370 retrospective PET reports collected from a single institution, but did not explore the potential benefits of incorporating additional context or information from the patient's medical history or other relevant sources.
- Why unresolved: The study did not investigate the impact of incorporating additional context or information on the performance of LLMs in generating personalized impressions.
- What evidence would resolve it: A study that investigates the impact of incorporating additional context or information from the patient's medical history or other relevant sources on the performance of LLMs in generating personalized impressions, using a standardized evaluation method.

### Open Question 3
- Question: What are the potential limitations or risks of using LLMs to generate impressions for whole-body PET reports, and how can these be mitigated?
- Basis in paper: [explicit] The study found that the main problems with LLM-generated impressions were factual inaccuracies, such as misinterpretation of findings or contradictory statements of impressions.
- Why unresolved: The study did not explore the potential limitations or risks of using LLMs to generate impressions for whole-body PET reports, or how these can be mitigated.
- What evidence would resolve it: A study that investigates the potential limitations or risks of using LLMs to generate impressions for whole-body PET reports, and how these can be mitigated, using a standardized evaluation method.

## Limitations
- Evaluation relied on only two physician raters for initial benchmarking, limiting representation of physician preference variability
- Dataset sourced from single institution, constraining generalizability across different reporting styles and institutional practices
- Focused exclusively on whole-body PET reports, limiting applicability to other imaging modalities or clinical contexts

## Confidence

- **High Confidence**: The superiority of encoder-decoder architectures (PEGASUS, BART) over decoder-only models is well-supported by systematic comparison across 12 different model variants. The effectiveness of physician identity encoding in capturing individual reporting styles is strongly validated through expert review showing preference for self-style impressions.

- **Medium Confidence**: The correlation between domain-adapted evaluation metrics (BARTScore+PET, PEGASUSScore) and physician preferences is statistically significant but based on limited rater numbers. The 76.7% accuracy in Deauville score prediction is promising but requires validation on larger, multi-institutional datasets.

- **Low Confidence**: The generalizability of findings to other medical specialties, imaging modalities, or institutions with different reporting conventions remains unproven. The long-term stability of learned physician styles across evolving clinical practices is unknown.

## Next Checks

1. **Multi-institutional validation**: Test the PEGASUS model on PET report datasets from at least 3-5 different institutions to assess generalizability and identify institution-specific performance variations.

2. **Longitudinal stability assessment**: Evaluate model performance across time periods (e.g., 2010-2015 vs 2016-2023) to determine if physician styles remain consistent and whether the model adapts to evolving clinical practices.

3. **Cross-modality applicability**: Apply the same methodology to chest X-ray or CT reports to determine if the physician identity encoding approach generalizes beyond PET imaging and whether similar performance patterns emerge.