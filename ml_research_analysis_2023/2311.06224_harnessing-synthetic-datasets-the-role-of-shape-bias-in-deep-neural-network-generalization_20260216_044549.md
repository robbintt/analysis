---
ver: rpa2
title: 'Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network
  Generalization'
arxiv_id: '2311.06224'
source_url: https://arxiv.org/abs/2311.06224
tags:
- bias
- shape
- dataset
- datasets
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of shape bias as a predictor for
  generalization in synthetic datasets. The authors analyze how shape bias evolves
  during training on various synthetic datasets and architectures.
---

# Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization

## Quick Facts
- arXiv ID: 2311.06224
- Source URL: https://arxiv.org/abs/2311.06224
- Reference count: 40
- Key outcome: Shape bias varies across architectures and supervision types but is unreliable as a sole predictor of generalization in synthetic datasets.

## Executive Summary
This paper investigates shape bias as a potential indicator of synthetic dataset quality and its relationship to model generalization. The authors analyze how shape bias evolves during training across different synthetic datasets and architectures, revealing that shape bias is influenced by architectural differences, supervision types, and dataset properties like diversity and naturalism. The study finds that while shape bias shows systematic variation, it alone cannot reliably predict generalization performance. However, the research establishes shape bias as a useful tool for estimating dataset diversity when comparing synthetic datasets.

## Method Summary
The study trains ResNet-18, ResNet-50, and ViT models on six synthetic datasets (95,000 instances each) using both self-supervised and supervised learning approaches. Shape bias is evaluated using K-nearest-neighbours classification on a Cue conflict dataset containing 1200 images with decorrelated shape and texture features. The models are then fine-tuned on Tiny ImageNet for downstream out-of-distribution evaluation. The KNN-based evaluation isolates encoder representations from classifier weights to measure the relative reliance on shape versus texture information.

## Key Results
- Shape bias varies systematically across architectures (CNNs show higher shape bias than ViTs under supervision, opposite for self-supervision)
- Shape bias increases with dataset diversity in Tiny ImageNet but not in synthetic datasets
- Shape bias alone is unreliable for predicting generalization across diverse synthetic datasets
- Shape bias can serve as a proxy for dataset diversity when comparing synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shape bias reflects the balance between shape and texture reliance in model decision-making
- Mechanism: Shape bias is computed by comparing KNN classification accuracy on shape vs texture labels in the Cue conflict dataset, where shape and texture are decorrelated via style transfer
- Core assumption: The KNN-based evaluation isolates encoder representations from classifier weights, providing a cleaner measure of learned biases
- Evidence anchors:
  - [abstract] mentions shape bias as indicator of synthetic data quality and generalization
  - [section] describes KNN-based shape bias computation and its interpretation
  - [corpus] no direct evidence; related work on shortcut learning suggests texture bias is a form of shortcut
- Break condition: If shape and texture cues are not properly decorrelated in the evaluation dataset, or if KNN fails to capture the relevant representational differences

### Mechanism 2
- Claim: Shape bias varies systematically with architectural differences (CNN vs ViT) and supervision type
- Mechanism: CNNs have inherent locality bias favoring texture, while ViTs capture global dependencies favoring shape; supervised learning vs self-supervised learning induces different bias trajectories
- Core assumption: Architectural inductive biases directly influence the shape vs texture reliance learned from data
- Evidence anchors:
  - [abstract] states shape bias varies across architectures and supervision types
  - [section] reports CNNs show higher shape bias than ViTs under supervision, opposite for self-supervision
  - [corpus] related work on CNNs learning local features supports locality bias claim
- Break condition: If architectural differences don't map to the hypothesized locality/global dependency distinction, or if supervision effects are mediated by other factors

### Mechanism 3
- Claim: Shape bias can serve as a proxy for dataset diversity when comparing synthetic datasets
- Mechanism: Diverse datasets prevent overfitting to high-frequency texture cues, leading models to rely more on low-frequency shape information; shape bias increases with sample diversity
- Core assumption: Models exhibit spectral bias, preferentially learning low-frequency shape patterns when data is diverse enough
- Evidence anchors:
  - [abstract] proposes shape bias as tool for estimating sample diversity within datasets
  - [section] shows shape bias increases with number of diverse samples in Tiny ImageNet but not in synthetic datasets
  - [corpus] no direct evidence; related work on spectral bias supports the underlying assumption
- Break condition: If dataset diversity doesn't correlate with shape reliance, or if other factors (like texture absence) dominate the shape bias signal

## Foundational Learning

- Concept: Inductive bias in neural networks
  - Why needed here: Understanding how architectural choices (CNN locality vs ViT global attention) create different shape/texture reliance patterns
  - Quick check question: What architectural feature of CNNs creates their inherent locality bias?

- Concept: Style transfer and cue conflict
  - Why needed here: The Cue conflict dataset decorrelates shape and texture to measure their relative influence on model decisions
  - Quick check question: How does style transfer create conflicting shape and texture cues in the evaluation dataset?

- Concept: Self-supervised vs supervised learning trajectories
  - Why needed here: Different supervision types lead to different evolution patterns of shape bias during training
  - Quick check question: Why might self-supervised learning lead to different shape bias trajectories than supervised learning?

## Architecture Onboarding

- Component map: ViT with 7 encoder layers, 8 heads, hidden dim 512; ResNet-18 and ResNet-50; KNN classifier for shape bias evaluation
- Critical path: Data generation → Model training (pre-training + fine-tuning) → Shape bias evaluation → Downstream accuracy measurement
- Design tradeoffs: Using KNN vs linear classifier for shape bias evaluation trades computational efficiency for cleaner isolation of encoder representations
- Failure signatures: Shape bias not varying across architectures suggests evaluation methodology issues; lack of shape bias increase with diversity suggests synthetic data lacks diversity
- First 3 experiments:
  1. Verify KNN shape bias computation matches published results on Cue conflict dataset
  2. Compare shape bias evolution during training for ResNet vs ViT on Tiny ImageNet
  3. Measure downstream accuracy correlation with shape bias for models trained on different synthetic datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural properties of Vision Transformers contribute to their shape bias and how do these differ from CNNs?
- Basis in paper: [explicit] The paper mentions that ViTs use self-attention mechanisms to capture global dependencies, contrasting with CNNs' locality bias.
- Why unresolved: The paper identifies differences but doesn't deeply analyze which architectural components specifically drive shape bias.
- What evidence would resolve it: Comparative studies isolating architectural components (e.g., self-attention vs convolution, patch size effects) while controlling for other variables.

### Open Question 2
- Question: What is the minimum level of dataset diversity required for effective shape bias development in neural networks?
- Basis in paper: [inferred] The paper discusses that diversity is crucial for effective shape bias but doesn't quantify the minimum requirements.
- Why unresolved: The relationship between dataset diversity and shape bias development is complex and not yet quantified.
- What evidence would resolve it: Systematic experiments varying dataset diversity levels while measuring shape bias development and downstream performance.

### Open Question 3
- Question: How does the temporal evolution of shape bias during training relate to final model performance across different architectures?
- Basis in paper: [explicit] The paper tracks shape bias evolution during training but doesn't establish clear correlations with final performance.
- Why unresolved: While shape bias evolution is tracked, its predictive value for final performance remains unclear.
- What evidence would resolve it: Longitudinal studies correlating shape bias trajectory with final performance metrics across multiple architectures and datasets.

## Limitations
- Shape bias demonstrates only weak correlation with downstream generalization performance
- Synthetic datasets may inherently lack the diversity needed to promote shape-based representations
- Reliance on KNN evaluation may not fully capture how shape/texture reliance manifests in end-task performance

## Confidence
- **High Confidence**: Shape bias varies systematically across architectures (CNN vs ViT) and supervision types (supervised vs self-supervised learning)
- **Medium Confidence**: Shape bias can serve as a proxy for dataset diversity when comparing datasets with sufficient variation
- **Low Confidence**: Shape bias alone is a reliable predictor of generalization performance across diverse synthetic datasets

## Next Checks
1. Verify that KNN-based shape bias computation on the Cue conflict dataset produces consistent results across multiple runs with different random seeds
2. Systematically vary the diversity of synthetic datasets (e.g., by increasing sample count or variation) and measure whether shape bias increases proportionally
3. Test whether alternative shape bias evaluation methods (e.g., linear probes vs KNN) produce consistent patterns of shape/texture reliance across architectures and supervision types