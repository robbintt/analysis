---
ver: rpa2
title: 'Oasis: Data Curation and Assessment System for Pretraining of Large Language
  Models'
arxiv_id: '2311.12537'
source_url: https://arxiv.org/abs/2311.12537
tags:
- data
- arxiv
- corpus
- filter
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Oasis is a system for pretraining data curation and assessment
  that addresses challenges in building large language models, where existing methods
  either lack customization or neglect comprehensive corpus assessment. The system
  provides interactive modular rule filters for customized rule pipelines, debiased
  neural filters to reduce bias in quality classification datasets, and adaptive document
  deduplication for large-scale processing with limited memory.
---

# Oasis: Data Curation and Assessment System for Pretraining of Large Language Models

## Quick Facts
- arXiv ID: 2311.12537
- Source URL: https://arxiv.org/abs/2311.12537
- Reference count: 8
- Oasis is a system for pretraining data curation and assessment that addresses challenges in building large language models, where existing methods either lack customization or neglect comprehensive corpus assessment.

## Executive Summary
Oasis is a comprehensive system designed to address the critical challenges in pretraining data curation for large language models (LLMs). The system provides a modular, interactive approach to building customized rule-based filters, debiased neural filters to reduce quality classification bias, and adaptive document deduplication for efficient large-scale processing. Oasis also offers holistic data assessment through both local quality evaluation (human/GPT-4 inspection) and global distribution assessment (six heuristic metrics). The system was demonstrated on Common Crawl data, producing an 800GB English-Chinese bilingual corpus (Oasis-Corpus) that achieved 90% human-evaluated quality with superior perplexity scores compared to existing corpora.

## Method Summary
Oasis implements a three-module architecture for data curation: (1) Interactive Modular Rule Filter for building customized rule-based filtering pipelines, (2) Debiased Neural Filter that uses negative-centric dataset construction to reduce bias in quality classification, and (3) Adaptive Document Deduplication that leverages adjustable LSH parameters for memory-efficient duplicate removal. The system also includes holistic data assessment capabilities combining human and GPT-4 based local evaluation with six heuristic metrics for global distribution assessment. The reference implementation processes Common Crawl data to create an 800GB bilingual corpus demonstrating superior quality metrics compared to existing corpora like WuDao.

## Key Results
- Oasis-Corpus achieved 90% human-evaluated quality with 7.20% knowledge density
- The corpus demonstrated 922.97 perplexity on Wikipedia, outperforming traditional neural filters
- Oasis successfully processed Common Crawl data into an 800GB English-Chinese bilingual corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative-centric dataset construction reduces bias in quality classification.
- Mechanism: By treating high-quality rule-filtered text as the primary positive sample and generating negatives through controlled contamination, the neural filter learns to distinguish quality without inheriting the biases of a specific high-quality source like Wikipedia.
- Core assumption: The contamination rules can reliably simulate low-quality text that differs meaningfully from the original positive samples.
- Evidence anchors:
  - [abstract] "The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias."
  - [section] "To address the bias issue, we propose a negative-centric dataset-building method for neural filter training... The predefined text contamination rule focuses on coherence and readability, involving shuffling, replacing, inserting, and deleting at the word, span, and sentence levels."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.456. While there is evidence of bias discussion in related works, direct empirical validation of the negative-centric approach's bias reduction is not explicitly shown in the corpus summary.
- Break condition: If contamination rules fail to generate realistic low-quality examples or if the neural filter cannot generalize beyond the specific contamination patterns used during training.

### Mechanism 2
- Claim: Adaptive document deduplication with adjustable LSH parameters enables large-scale processing within memory constraints.
- Mechanism: By calculating the maximum allowable r value based on corpus size and available memory, the system can predict the required number of LSH runs to achieve the desired Jaccard similarity threshold while staying within resource limits.
- Core assumption: The relationship between LSH parameters (r, b), Jaccard similarity, and collision probability can be accurately modeled to predict deduplication performance.
- Evidence anchors:
  - [abstract] "The adaptive document deduplication module could execute large-scale deduplication with limited memory resources."
  - [section] "To achieve this goal, we reduce the memory requirement of the LSH deduplication algorithm to adapt to customized hardware by adjusting r in the conditional probability formula. The system predicts the maximum r according to the user's configuration in corpus size and memory size."
  - [corpus] No direct evidence in the corpus summary about the effectiveness of this specific adaptive approach, though the general challenge of memory-efficient deduplication is acknowledged in related works.
- Break condition: If the predicted r value leads to insufficient collision probability for the desired Jaccard threshold, resulting in missed duplicates, or if the required number of LSH runs becomes computationally prohibitive.

### Mechanism 3
- Claim: Holistic data assessment using multiple evaluation methods provides comprehensive insights into corpus quality.
- Mechanism: By combining local quality evaluation (human/GPT-4 inspection) with global distribution assessment (six heuristic metrics), the system captures both document-level characteristics and corpus-wide patterns that influence LLM performance.
- Core assumption: The selected heuristic metrics (lexical diversity, task2vec diversity, semantic diversity, topic diversity, knowledge density, and similarity to Wikipedia) are meaningful indicators of corpus quality for LLM pretraining.
- Evidence anchors:
  - [abstract] "And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics."
  - [section] "Oasis adopts six metrics to assess the corpus in heuristics from a randomly sampled subset of data... These metrics can be displayed on a single page and overlay multiple corpora for convenient visual comparison."
  - [corpus] The corpus summary mentions related work on data assessment but does not provide direct evidence of the effectiveness of this specific set of six metrics.
- Break condition: If the heuristic metrics fail to correlate with downstream LLM performance or if the local evaluation methods (human/GPT-4) do not align with the global distribution patterns observed in the corpus.

## Foundational Learning

- Concept: Data curation pipeline design
  - Why needed here: Understanding how to build a customized rule pipeline is fundamental to using Oasis effectively, as different data sources require tailored approaches.
  - Quick check question: What are the key differences between rule-based and neural-based filtering, and when would you choose one over the other?

- Concept: Neural network training for text classification
  - Why needed here: The debiased neural filter module requires knowledge of how to train a BERT model on a custom quality classification dataset, including data preparation and model fine-tuning.
  - Quick check question: How does the negative-centric approach to dataset construction differ from traditional positive-centric methods, and what are the implications for model bias?

- Concept: Locality-sensitive hashing (LSH) and similarity metrics
  - Why needed here: Understanding LSH and Jaccard similarity is crucial for configuring the adaptive document deduplication module and interpreting its results.
  - Quick check question: How does adjusting the LSH parameters (r and b) affect the collision probability and memory requirements for document deduplication?

## Architecture Onboarding

- Component map: Raw data → Rule Filter → Neural Filter → Document Deduplication → Assessment (Human/GPT-4 + Heuristics) → Final Corpus
- Critical path: Raw data → Rule Filter → Neural Filter → Document Deduplication → Assessment (Human/GPT-4 + Heuristics) → Final Corpus
- Design tradeoffs:
  - Rule vs. Neural Filtering: Rule filters are faster but less adaptable, while neural filters are more powerful but computationally expensive and potentially biased
  - LSH Parameters: Higher r values increase accuracy but require more memory, while lower r values reduce memory usage but may miss duplicates
  - Assessment Methods: Human evaluation is accurate but slow, GPT-4 is faster but may introduce its own biases, heuristic metrics are efficient but may not capture all quality aspects
- Failure signatures:
  - Rule filter fails: Unexpectedly high hit rates or bad cases that don't match the intended patterns
  - Neural filter fails: Low accuracy on a held-out test set or significant bias towards a specific data source
  - Deduplication fails: Excessive memory usage or missed duplicates due to insufficient LSH collision probability
  - Assessment fails: Disagreement between human and GPT-4 evaluations or heuristic metrics that don't align with downstream model performance
- First 3 experiments:
  1. Test the rule filter on a small sample of raw data to verify that the predefined rules are catching the intended patterns and not producing too many false positives or negatives.
  2. Train the debiased neural filter on a small, manually curated dataset and evaluate its performance on a held-out test set to ensure it's learning the intended quality distinctions without excessive bias.
  3. Run the adaptive document deduplication on a small subset of the corpus with different LSH parameter settings to find the optimal balance between memory usage and duplicate removal effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is Oasis compared to existing data curation and assessment methods?
- Basis in paper: explicit
- Why unresolved: The paper compares Oasis to other methods on a limited dataset and evaluation metrics, and does not provide a comprehensive comparison across different datasets and metrics.
- What evidence would resolve it: A thorough comparison of Oasis to other data curation and assessment methods on multiple datasets and evaluation metrics.

### Open Question 2
- Question: What is the impact of Oasis on the quality of large language models?
- Basis in paper: explicit
- Why unresolved: The paper does not provide a detailed analysis of the impact of Oasis on the quality of large language models, such as their performance on downstream tasks.
- What evidence would resolve it: A comprehensive study of the impact of Oasis on the quality of large language models, including their performance on various downstream tasks.

### Open Question 3
- Question: How can Oasis be used to improve the diversity of large language models?
- Basis in paper: explicit
- Why unresolved: The paper does not provide a detailed analysis of how Oasis can be used to improve the diversity of large language models, such as by incorporating more diverse training data.
- What evidence would resolve it: A comprehensive study of how Oasis can be used to improve the diversity of large language models, including the impact on their performance on downstream tasks.

## Limitations
- The negative-centric neural filtering approach relies heavily on the quality and diversity of contamination rules, which may not generalize well to all types of low-quality text patterns
- The adaptive deduplication mechanism's performance depends on accurate prediction of LSH parameters, which may not hold across different corpus characteristics and hardware configurations
- The heuristic metrics used for global assessment, while comprehensive, may not fully capture all aspects of corpus quality relevant to downstream LLM performance

## Confidence

- **High Confidence**: The modular rule filter design and its application for customized data cleaning - the mechanism is straightforward and well-established
- **Medium Confidence**: The debiased neural filter's effectiveness in reducing bias - while the negative-centric approach is conceptually sound, empirical validation of bias reduction is limited
- **Medium Confidence**: The adaptive deduplication's memory efficiency - the theoretical framework is clear, but real-world performance may vary with corpus characteristics
- **Low Confidence**: The correlation between the six heuristic metrics and actual LLM pretraining performance - the paper demonstrates improved perplexity scores but doesn't establish a comprehensive validation of all metrics

## Next Checks

1. **Bias Validation**: Conduct a controlled experiment comparing the debiased neural filter against traditional positive-centric approaches using multiple high-quality source datasets to empirically measure bias reduction
2. **Memory Efficiency Test**: Evaluate the adaptive deduplication module across corpora of varying sizes and characteristics to verify the accuracy of the LSH parameter prediction model and identify edge cases
3. **Metric Correlation Study**: Perform a systematic analysis correlating each of the six heuristic metrics with downstream LLM task performance across multiple benchmarks to validate their effectiveness as quality indicators