---
ver: rpa2
title: On Measuring Faithfulness or Self-consistency of Natural Language Explanations
arxiv_id: '2311.07466'
source_url: https://arxiv.org/abs/2311.07466
tags:
- answer
- sentence
- lobsters
- best
- live
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how to measure faithfulness of natural\
  \ language explanations generated by large language models. It argues that existing\
  \ faithfulness tests only measure self-consistency at the output level, not true\
  \ faithfulness to a model\u2019s internal reasoning."
---

# On Measuring Faithfulness or Self-consistency of Natural Language Explanations

## Quick Facts
- arXiv ID: 2311.07466
- Source URL: https://arxiv.org/abs/2311.07466
- Reference count: 40
- Primary result: Existing faithfulness tests only measure self-consistency at output level, not true faithfulness to model's internal reasoning

## Executive Summary
This paper investigates how to measure the faithfulness of natural language explanations generated by large language models. The authors argue that existing faithfulness tests only measure self-consistency at the output level rather than true faithfulness to a model's internal reasoning process. They introduce CC-SHAP, a fine-grained measure that analyzes how much each input token contributes to both the answer prediction and the generated explanation using Shapley values. The results show that RLHF-tuned models are more self-consistent than base models, and CC-SHAP can reveal when models' prediction and explanation processes use inputs very differently.

## Method Summary
The paper compares eight existing faithfulness tests (Counterfactual Edits, Constructing Input from Explanation, Biasing Features, Corrupting CoT variants) with a new CC-SHAP measure. CC-SHAP uses Shapley values to quantify each input token's contribution to both the model's answer and its explanation, then computes cosine distance between these contribution distributions. The method is tested across 11 open-source autoregressive LLMs (LLaMA 2 variants, Mistral, Falcon, GPT2) on five datasets (e-SNLI, ComVE, BBH causal judgement, BBH disambiguation QA, BBH logical deduction).

## Key Results
- Existing faithfulness tests yield divergent results across models and datasets
- CC-SHAP provides interpretable, continuous self-consistency scores closer to model internals
- RLHF-tuned models show higher self-consistency than base models according to CC-SHAP
- CC-SHAP reveals that models can use inputs very differently for prediction versus explanation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CC-SHAP measures input-level self-consistency by comparing token contributions for prediction and explanation
- Mechanism: Uses Shapley values to quantify each input token's contribution to both the model's answer and its explanation, then computes cosine distance between these contribution distributions
- Core assumption: A faithful explanation should use input tokens similarly to the prediction process
- Evidence anchors:
  - [abstract] "CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency that compares a model's input contributions to answer prediction and generated explanation"
  - [section] "we apply the SHAP interpretability method on autoregressive LLMs... CC-SHAP measures the convergence of the distribution of the contribution ratios over all input tokens j for the prediction C(P) and for the explanation C(E)"
- Break condition: If model uses different parameters or processing pathways for prediction vs explanation, contributions may diverge even with faithful explanations

### Mechanism 2
- Claim: Continuous self-consistency scores are more informative than binary faithfulness tests
- Mechanism: CC-SHAP outputs a continuous value between -1 and 1 representing similarity of input contributions, rather than just pass/fail
- Core assumption: Gradual differences in self-consistency provide more diagnostic information than binary decisions
- Evidence anchors:
  - [abstract] "CC-SHAP is a fine-grained measure (not test)"
  - [section] "Our measure is interpretable as we can look at individual token contributions and see where prediction and explanation mechanisms use inputs differently"
- Break condition: If the continuous scale doesn't correlate with actual faithfulness, may mislead users about explanation quality

### Mechanism 3
- Claim: Comparing input contributions reveals model behavior differences between base and chat-tuned models
- Mechanism: By analyzing contribution distributions, CC-SHAP shows chat-tuned models have more aligned prediction/explanation contributions than base models
- Core assumption: Instruction tuning/RLHF makes models process inputs more consistently across different output types
- Evidence anchors:
  - [section] "According to CC-SHAP – measuring the self-consistency of both post-hoc and CoT explanations – LLaMA 2 and Mistral obtain low scores... Instruction / RLHF tuned LLMs get higher scores (positive CC-SHAP)"
  - [section] "there are clear differences between chat and base LLMs, with chat models generally behaving more self-consistently than base models"
- Break condition: If differences are due to model architecture rather than tuning, comparison may be misleading

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: CC-SHAP relies on Shapley values to fairly distribute token contributions to predictions and explanations
  - Quick check question: How does the Shapley value formula ensure efficiency and symmetry properties?

- Concept: Cosine similarity and distance metrics
  - Why needed here: CC-SHAP uses cosine distance to measure divergence between contribution distributions
  - Quick check question: What does a cosine distance of 0.0 vs 1.0 indicate about the similarity of two vectors?

- Concept: Interpretability methods in NLP
  - Why needed here: Understanding how SHAP and other attribution methods work is crucial for CC-SHAP implementation
  - Quick check question: What are the key differences between gradient-based, attention-based, and perturbation-based interpretability methods?

## Architecture Onboarding

- Component map: Input preprocessing → SHAP contribution calculation (prediction) → SHAP contribution calculation (explanation) → Contribution distribution comparison → CC-SHAP score
- Critical path: Model inference for prediction → SHAP sampling for contributions → Cosine distance computation
- Design tradeoffs: Higher computational cost for interpretability vs simpler binary tests; continuous vs binary output
- Failure signatures: All zero contributions (model not using inputs), extremely divergent contributions (different processing pathways), runtime errors in SHAP sampling
- First 3 experiments:
  1. Run CC-SHAP on a simple model with known input-output relationship to verify correctness
  2. Compare CC-SHAP scores between post-hoc and CoT explanations on the same examples
  3. Test CC-SHAP on examples where you manually verify input contribution alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much do parameters producing answers and explanations in LLMs need to differ before explanations should be considered unfaithful?
- Basis in paper: [explicit] The paper argues that even if parameters are shared, a model could still use them differently when giving a contradictory answer, and that "it remains unclear how much parameters producing answers and explanations are allowed to differ, to still consider an explanation to be faithful."
- Why unresolved: The paper acknowledges this is an open question because current methods only measure output-level self-consistency, not internal parameter-level faithfulness. There's no established threshold or criterion for when shared parameters can be considered sufficiently different to invalidate faithfulness.
- What evidence would resolve it: Controlled experiments varying parameter usage between prediction and explanation generation while measuring both parameter-level changes and faithfulness scores, establishing a threshold for acceptable divergence.

### Open Question 2
- Question: Can we develop a faithfulness metric that goes beyond self-consistency to actually measure alignment with a model's internal reasoning process?
- Basis in paper: [explicit] The paper argues that "we cannot judge whether the inner workings of LLMs that generate explanations for their predictions are faithful unless we look under the hood" and that "self-consistency is all we get" with current methods.
- Why unresolved: The paper demonstrates that existing tests only measure output-level self-consistency, not true faithfulness to internal reasoning. While CC-SHAP takes a step toward measuring internal processes, it still measures self-consistency at the input level rather than faithfulness to actual reasoning.
- What evidence would resolve it: Development and validation of a metric that can access or infer internal reasoning states, combined with human evaluation of whether explanations accurately represent those states.

### Open Question 3
- Question: What is the relationship between model size and self-consistency across different types of LLMs?
- Basis in paper: [inferred] The paper tested models ranging from 7 billion to 40 billion parameters and found "no clear relationship between model size and self-consistency" but this conclusion is based on a limited sample of models.
- Why unresolved: The paper's results are based on only 11 models across a specific size range, and the findings show no clear trend. More comprehensive testing across a wider range of sizes and model architectures could reveal different patterns.
- What evidence would resolve it: Systematic testing of models across a broader size spectrum (from small to extremely large) and different architectures, with statistical analysis of correlation between size and self-consistency measures.

## Limitations

- Computational complexity of SHAP-based attribution introduces significant overhead compared to existing tests
- Limited generalizability to non-autoregressive model architectures
- Unclear whether CC-SHAP scores correlate with human judgments of explanation quality

## Confidence

- Confidence in existing tests being insufficient: High (clear divergence in test results across models and datasets)
- Confidence in CC-SHAP as definitive solution: Medium (lacks validation against human judgments and downstream task performance)
- Confidence in relationship between model size and self-consistency: Low (based on limited sample of 11 models)

## Next Checks

1. **Runtime Efficiency Analysis**: Measure and compare the computational overhead of CC-SHAP versus existing tests across different model sizes and input lengths to establish practical scalability limits.

2. **Human Evaluation Correlation**: Conduct a human study where annotators rate explanation quality and faithfulness, then correlate these judgments with CC-SHAP scores and existing test results to validate CC-SHAP's effectiveness.

3. **Cross-Architecture Generalization**: Apply CC-SHAP to non-autoregressive models (e.g., BERT-based models) and evaluate whether the same patterns of input contribution alignment hold, or if the metric requires architecture-specific adaptations.