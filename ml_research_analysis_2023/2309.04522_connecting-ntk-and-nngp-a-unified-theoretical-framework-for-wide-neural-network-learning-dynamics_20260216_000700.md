---
ver: rpa2
title: 'Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network
  Learning Dynamics'
arxiv_id: '2309.04522'
source_url: https://arxiv.org/abs/2309.04522
tags:
- learning
- time
- dynamics
- neural
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper unifies the Neural Tangent Kernel (NTK) and Neural Network
  Gaussian Process (NNGP) frameworks for analyzing wide neural network learning dynamics.
  The authors introduce a Markov proximal learning (MPL) framework, which generalizes
  Langevin gradient descent with added noise, and derive an analytical expression
  for the network's input-output function.
---

# Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network Learning Dynamics

## Quick Facts
- arXiv ID: 2309.04522
- Source URL: https://arxiv.org/abs/2309.04522
- Reference count: 40
- Key outcome: Unifies NTK and NNGP frameworks through Markov proximal learning, revealing two distinct learning phases governed by initialization and regularization ratios

## Executive Summary
This paper presents a unified theoretical framework that bridges the Neural Tangent Kernel (NTK) and Neural Network Gaussian Process (NNGP) approaches for analyzing wide neural network learning dynamics. The authors introduce a Markov proximal learning (MPL) framework that generalizes Langevin gradient descent with added noise, deriving an analytical expression for the network's input-output function. A new time-dependent Neural Dynamical Kernel (NDK) emerges naturally from their theory, capturing the evolution of the network's input-output function through gradient-driven and diffusive phases. The analysis reveals that initialization variance (σ²₀) and regularization strength (σ²) significantly affect learning dynamics and generalization performance, especially in deeper networks and with sigmoidal neurons.

## Method Summary
The paper introduces a Markov proximal learning framework that generalizes Langevin gradient descent with added noise, enabling analytical treatment of wide neural network learning dynamics. The method derives a time-dependent Neural Dynamical Kernel (NDK) that bridges NTK and NNGP frameworks, computed recursively through Hubbard-Stratonovich transformation and replica method techniques. The theory captures two distinct learning phases: an initial gradient-driven phase governed by NTK dynamics, followed by a slower diffusive phase where network parameters explore the solution space. The framework provides closed-form expressions for mean predictor dynamics in both training and test scenarios, with analytical predictions validated against Langevin simulations on synthetic orthogonal datasets and binary classification tasks (MNIST digits 0 vs 1, CIFAR10 cats vs dogs).

## Key Results
- Introduces Neural Dynamical Kernel (NDK) as a time-dependent generalization of NTK and NNGP kernels
- Identifies two distinct learning phases: gradient-driven (NTK regime) followed by diffusive (NNGP regime)
- Demonstrates that initialization variance (σ²₀) and regularization strength (σ²) ratio controls learning dynamics and generalization
- Shows that deeper networks exhibit stronger differences between NTK and NNGP equilibria
- Reveals that early stopping often occurs during the diffusive phase rather than gradient-driven phase

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Neural Dynamical Kernel (NDK) generalizes the Neural Tangent Kernel (NTK) and Neural Network Gaussian Process (NNGP) kernels into a single time-dependent framework.
- **Mechanism:** The NDK is derived as a two-time extension of the NTK, where the kernel depends on both current and past parameter states. This captures the evolution of the network's input-output function through gradient-driven and diffusive phases.
- **Core assumption:** In the infinite width limit, the network's behavior is dominated by its Gaussian prior statistics and the Langevin dynamics of parameter updates.
- **Evidence anchors:**
  - [abstract] "A new time-dependent Neural Dynamical Kernel (NDK) emerges naturally from our theory, bridging NTK and NNGP."
  - [section] "The kernel can be expressed in terms of the derivatives of the predictor w.r.t. the time-dependent network parameters."
  - [corpus] Weak - no direct evidence of NDK existence in related works.
- **Break condition:** If the width is finite or the network enters the feature learning regime, the NDK approximation breaks down.

### Mechanism 2
- **Claim:** The learning process exhibits two distinct phases: gradient-driven (NTK regime) and diffusive (NNGP regime).
- **Mechanism:** Initially, the network follows deterministic gradient descent dynamics dominated by the initialization variance. Later, noise and regularization drive a slow exploration of the solution space, converging to the NNGP equilibrium.
- **Core assumption:** The learning dynamics can be modeled as Markov proximal learning with small noise, allowing separation of time scales.
- **Evidence anchors:**
  - [abstract] "We identify two learning phases: an initial gradient-driven phase governed by NTK dynamics, followed by a slower diffusive phase where network parameters explore the solution space."
  - [section] "The time dependence of the NDK (Eq. 12) comes from exponents that scale as T · t, and thus at low T and t ∼ O(1) we can substitute K d,L (t, t′, x, x′) = K d,L (0, 0, x,x′)."
  - [corpus] Weak - most related works focus on either NTK or NNGP but not the unified phase transition.
- **Break condition:** If the regularization strength σ² is too small or the noise level T is too high, the phase separation becomes indistinct.

### Mechanism 3
- **Claim:** The ratio σ²₀/σ² controls the relative importance of initialization versus regularization in shaping learning dynamics.
- **Mechanism:** This ratio determines whether the network quickly converges to a solution near initialization (large σ²₀/σ²) or explores more broadly before settling (small σ²₀/σ²), affecting early stopping and generalization.
- **Core assumption:** The learning trajectory is sensitive to the balance between the Gaussian prior variance σ² and the initialization variance σ²₀.
- **Evidence anchors:**
  - [abstract] "The authors demonstrate that initialization variance (σ²₀) and regularization strength (σ²) significantly affect the dynamics and generalization performance."
  - [section] "Interestingly, in most examples, when σ²₀/σ² is small, the predictor dynamics is non-monotonic, overshooting above its equilibrium value."
  - [corpus] Weak - no direct discussion of σ²₀/σ² ratio in related works.
- **Break condition:** If σ²₀ and σ² are both very large or very small, the effect of their ratio diminishes.

## Foundational Learning

- **Concept:** Infinite width limit of neural networks
  - **Why needed here:** The theory relies on the fact that in the infinite width limit, the network's behavior becomes deterministic and can be described by kernels.
  - **Quick check question:** What happens to the variance of neuron activations as the width goes to infinity in a fully connected network?
- **Concept:** Gaussian process priors over network parameters
  - **Why needed here:** The NNGP framework assumes that network outputs follow a Gaussian process distribution determined by the prior over weights.
  - **Quick check question:** How does the covariance structure of a Gaussian process change when the input dimension increases?
- **Concept:** Langevin dynamics and Markov proximal learning
  - **Why needed here:** The learning dynamics are modeled as gradient descent with noise, which corresponds to Langevin dynamics in the large λ limit.
  - **Quick check question:** What is the relationship between the noise strength in Langevin dynamics and the temperature parameter T?

## Architecture Onboarding

- **Component map:** Input layer → hidden layers (with width Nl) → output layer with readout weights at
- **Critical path:** Initialization → gradient-driven phase (NTK) → diffusive phase (NNGP) → equilibrium
- **Design tradeoffs:**
  - Larger σ²₀ gives faster initial convergence but may overshoot equilibrium
  - Smaller T slows diffusive exploration but provides more stable dynamics
  - Deeper networks show stronger differences between NTK and NNGP equilibria
- **Failure signatures:**
  - Non-monotonic predictor behavior during diffusive phase
  - Early stopping occurring in diffusive rather than gradient-driven phase
  - Representational drift when readout weights are frozen
- **First 3 experiments:**
  1. Verify the two-phase learning behavior on synthetic orthogonal dataset with varying σ²₀/σ² ratios
  2. Test early stopping predictions by monitoring generalization error during diffusive phase
  3. Demonstrate representational drift by freezing readout weights at different times and measuring performance decay

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the proposed Markov proximal learning (MPL) framework generalize to non-convex cost functions and non-smooth activation functions?
  - **Basis in paper:** [explicit] The paper mentions that the MPL framework can be readily extended to non-smooth optimization problems (potentially due to non-smooth activation functions or regularizers) [39, 41, 62], which can not be captured by DMFT.
  - **Why unresolved:** The paper does not provide a detailed analysis of how the MPL framework performs in these scenarios or compare it to existing methods.
  - **What evidence would resolve it:** A rigorous mathematical analysis of the MPL framework's convergence properties and performance in non-convex and non-smooth optimization problems, along with empirical comparisons to existing methods.

- **Open Question 2:** How does the time-dependent Neural Dynamical Kernel (NDK) evolve in the regime where the data size is proportional to the network width, and what are the implications for feature learning and kernel renormalization?
  - **Basis in paper:** [inferred] The paper mentions that future work aims to extend the theory to the regime where data size is proportional to network width, where dynamic kernel renormalization is expected [66, 67].
  - **Why unresolved:** The paper does not provide a theoretical analysis of the NDK's behavior in this regime or explore the implications for feature learning and kernel renormalization.
  - **What evidence would resolve it:** A theoretical derivation of the NDK's evolution in the proportional data size regime, along with empirical studies on the impact of feature learning and kernel renormalization on the learning dynamics.

- **Open Question 3:** What are the specific conditions under which the representational drift phenomenon occurs in biological neuronal circuits, and how does it relate to the continuous realignment of readout weights observed in the proposed MPL framework?
  - **Basis in paper:** [explicit] The paper mentions that the stability of the (low) training error during the diffusion phase is due to the continuous realignment of readout weights at to changes in the network hidden layer weights Wt as they drift simultaneously exploring the space of solutions.
  - **Why unresolved:** The paper does not provide a detailed analysis of the conditions under which representational drift occurs in biological circuits or the mechanisms underlying the continuous realignment of readout weights.
  - **What evidence would resolve it:** A comprehensive study of the conditions and mechanisms that give rise to representational drift in biological neuronal circuits, along with empirical evidence supporting the role of continuous readout weight realignment in maintaining performance.

## Limitations

- Empirical validation scope is limited to small synthetic datasets and two binary classification tasks, requiring broader experimental validation on larger-scale problems
- The framework assumes small noise (T → 0 limit) and doesn't thoroughly explore finite noise regime where phase separation becomes less distinct
- Extension to deeper networks and non-ReLU activations is mentioned but not thoroughly validated, with computational complexity remaining uncertain

## Confidence

**High confidence**: The mathematical derivation of the Neural Dynamical Kernel (NDK) and its relationship to NTK/NNGP is internally consistent and follows established techniques from statistical physics. The recursive computation of K_d,L and the identification of two learning phases are well-supported by the theory.

**Medium confidence**: The practical implications for early stopping and initialization-dependent generalization are theoretically sound but require more extensive empirical validation. The claims about representational drift and continuous realignment of readout weights are plausible given the framework but lack direct experimental demonstration.

**Low confidence**: The extension to deeper networks and non-ReLU activations (sigmoid neurons) is mentioned but not thoroughly validated. The computational complexity of the NDK recursion for deep networks and its practical applicability to real-world problems remains uncertain.

## Next Checks

1. **Phase Transition Verification**: Conduct systematic experiments varying σ²₀/σ² ratios across multiple orders of magnitude on larger datasets (e.g., CIFAR-10 with more classes) to empirically verify the predicted transition between NTK-dominated and NNGP-dominated learning phases. Measure both training dynamics and generalization performance to confirm the theoretical predictions about overshooting behavior.

2. **Finite Width Effects**: Implement finite-width corrections to the NDK framework and test how quickly the theoretical predictions converge as network width increases. Compare performance on networks with N=1000 vs N=10000 hidden units to quantify the practical limits of the infinite width assumption.

3. **Representational Drift Experiment**: Design a controlled experiment where readout weights are frozen at different time points during training, then measure the degradation of test accuracy over time. This would directly validate the theoretical claim about continuous realignment being necessary to prevent representational drift while maintaining performance.