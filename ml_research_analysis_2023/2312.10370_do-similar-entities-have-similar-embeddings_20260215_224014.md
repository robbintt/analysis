---
ver: rpa2
title: Do Similar Entities have Similar Embeddings?
arxiv_id: '2312.10370'
source_url: https://arxiv.org/abs/2312.10370
tags:
- similarity
- entities
- entity
- kgems
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the common assumption that knowledge graph
  embedding models (KGEMs) retain the graph's structure within their embedding space,
  positioning similar entities close to one another. The authors conduct extensive
  experiments to measure KGEMs' ability to cluster similar entities together and investigate
  the factors influencing this capability.
---

# Do Similar Entities have Similar Embeddings?

## Quick Facts
- arXiv ID: 2312.10370
- Source URL: https://arxiv.org/abs/2312.10370
- Authors: 
- Reference count: 40
- Key outcome: KGEMs do not reliably preserve graph structure in embedding space; similarity alignment varies by class and model

## Executive Summary
This paper challenges the common assumption that knowledge graph embedding models (KGEMs) preserve entity similarity in their embedding space. Through extensive experiments across multiple KGEMs and datasets, the authors demonstrate that KGEMs fulfill the entity similarity assumption only partially and inconsistently. The study reveals that different KGEMs focus on different predicates when learning similar embeddings, leading to varied notions of similarity across models. Importantly, traditional rank-based metrics like MRR and Hits@K do not reliably indicate whether a model preserves entity similarity in the embedding space.

## Method Summary
The authors evaluate KGEMs' ability to cluster similar entities by comparing embedding-based similarity with graph-based similarity using Rank-Biased Overlap (RBO). They train multiple KGEMs (TransE, TransD, DistMult, RESCAL, TuckER, ConvE, BoxE, RDF2Vec) on various knowledge graphs (AIFB, Codex-S/M, DBpedia50, FB15k-237, YAGO4-19K) and compute top-K neighbors in both embedding space and graph-based neighborhoods. Graph-based similarity is measured using Jaccard coefficient on 1-hop and 2-hop subgraphs. The RBO metric compares the overlap between ranked neighbor lists from embeddings versus graph structure. They also analyze predicate distributions in local neighborhoods and correlate rank-based metrics with RBO values.

## Key Results
- Different KGEMs fulfill the KGE entity similarity assumption only to a limited extent
- Performance varies substantially across entity classes within the same model and dataset
- Rank-based metrics like MRR and Hits@K do not correlate well with entity similarity
- Different KGEMs focus on different subsets of predicates when learning similar embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KGEMs do not reliably preserve graph structural similarity in embedding space
- Mechanism: KGEMs optimize for link prediction rank-based metrics rather than preserving entity similarity
- Core assumption: Maximizing rank-based metrics inherently aligns similar entities
- Evidence anchors:
  - [abstract] "KGEMs are assessed based on their sole link prediction capabilities... This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space."
  - [section 5.2] "MRR generally does not correlate well with RBO values... a good performance for link prediction does not imply that the also fulfills the KGE entity similarity assumption to a large extent."
- Break condition: Retraining KGEMs with loss functions explicitly targeting entity similarity

### Mechanism 2
- Claim: Different KGEMs expose different notions of similarity based on predicate distribution
- Mechanism: KGEMs weigh predicates differently based on their frequency in local neighborhoods
- Core assumption: All KGEMs treat predicates uniformly when learning embeddings
- Evidence anchors:
  - [abstract] "different KGEMs turn their attention to different subsets of predicates for learning similar embeddings"
  - [section 5.3] "predicates are differently ranked, i.e. different KGEMs may rely on different sets of predicates"
- Break condition: Uniform or artificially balanced predicate distribution

### Mechanism 3
- Claim: Entity similarity alignment varies substantially across classes
- Mechanism: Effectiveness depends on specific entity class, with some showing strong alignment and others weak
- Core assumption: KGEMs have consistent similarity alignment across all entity classes
- Evidence anchors:
  - [section 5.1] "The behavior of KGEMs varies not only across datasets but also across different classes"
  - [table 3] Shows per-class RBO@10 values with large variations
- Break condition: All classes having similar structural properties and predicate distributions

## Foundational Learning

- Concept: Knowledge Graph Embedding Models (KGEMs) and their training objectives
  - Why needed here: Understanding KGEM training (link prediction vs. similarity preservation) is crucial for interpreting results
  - Quick check question: What is the primary objective function used by KGEMs like TransE, DistMult, and TuckER during training?

- Concept: Graph-based similarity metrics (Jaccard coefficient, subgraph neighborhoods)
  - Why needed here: Paper uses graph-based similarity as baseline to compare against embedding-based similarity
  - Quick check question: How does the Jaccard coefficient measure similarity between two subgraphs, and why is it preferred over Graph Edit Distance?

- Concept: Rank-Biased Overlap (RBO) for comparing ranked lists
  - Why needed here: RBO compares top-K neighbors in embedding space versus graph
  - Quick check question: How does RBO differ from Kendall's Tau, and why is it more suitable for comparing ranked lists with different items?

## Architecture Onboarding

- Component map: KGEMs (TransE, TransD, DistMult, RESCAL, TuckER, ConvE, BoxE, RDF2Vec) → Embedding space → Similarity comparison (RBO) → Graph-based similarity (Jaccard) → Dataset (AIFB, Codex-S/M, DBpedia50, FB15k-237, YAGO4-19K) → Class-specific analysis
- Critical path: Load KGEM embeddings → Compute top-K neighbors in embedding space → Compute graph-based similarity (1-hop/2-hop subgraphs) → Calculate RBO between neighbor lists → Aggregate results per model, dataset, and class
- Design tradeoffs: 1-hop vs. 2-hop subgraphs balances computational cost and capturing indirect dependencies; RBO over Kendall's Tau handles lists with different items
- Failure signatures: Low RBO values indicate poor alignment of entity similarity; high correlation between MRR and RBO suggests link prediction performance might proxy similarity alignment
- First 3 experiments:
  1. Compute RBO@10 for a single KGEM (e.g., TuckER) on YAGO4-19K using 1-hop subgraphs and visualize top-K neighbor overlap for a few entities
  2. Compare RBO values across all KGEMs on a small dataset (e.g., AIFB) using 2-hop subgraphs and identify which models perform best/worst
  3. Analyze predicate distribution in the top-K neighbors for a specific class (e.g., MusicGroup in YAGO4-19K) across different KGEMs and identify the most influential predicates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do different KGEMs focus on different predicates to capture the notion of similarity in the embedding space?
- Basis in paper: [explicit] Third research question (RQ3) with evidence that different KGEMs rely on different predicate sets
- Why unresolved: Needs more thorough analysis of predicate distribution across KGEMs and classes
- What evidence would resolve it: Comprehensive analysis of predicate distribution across KGEMs and classes including relative importance

### Open Question 2
- Question: How do traditional rank-based metrics correlate with entity similarity?
- Basis in paper: [explicit] Second research question (RQ2) with correlation analysis between MRR and RBO
- Why unresolved: Correlation is mixed across datasets and KGEMs; good link prediction doesn't imply good similarity alignment
- What evidence would resolve it: More comprehensive correlation analysis across different KGEMs, datasets, and classes

### Open Question 3
- Question: To what extent does proximity in embedding space align with entity similarity in the KG?
- Basis in paper: [explicit] First research question (RQ1) with extensive experiments measuring clustering capability
- Why unresolved: Shows limited fulfillment of similarity assumption with substantial variation across classes
- What evidence would resolve it: More comprehensive analysis of alignment between embedding proximity and KG similarity including impact of different factors

## Limitations

- Evaluation limited to specific set of KGEMs and datasets, may not generalize to all knowledge graph domains
- Rank-based metrics provide relative measures but may not capture all aspects of semantic similarity
- Focuses on entity-level similarity without extensively exploring relation-level implications

## Confidence

- Claim: KGEMs do not reliably preserve graph structural similarity in embedding space
  - Confidence: Medium - rigorous methodology but complex, context-dependent phenomenon
- Claim: Different KGEMs expose different notions of similarity
  - Confidence: Medium - supported by evidence but requires more thorough predicate distribution analysis
- Claim: Rank-based metrics don't reliably indicate similarity preservation
  - Confidence: Medium - correlation analysis shows mixed results across different contexts

## Next Checks

1. Validate findings on additional knowledge graphs with different characteristics (e.g., biomedical knowledge bases, social networks) to assess robustness across domains

2. Conduct detailed analysis of how different predicate distributions affect similarity preservation in KGEMs, potentially by creating synthetic knowledge graphs with controlled predicate distributions

3. Replicate experiments using alternative similarity metrics (e.g., cosine similarity, Euclidean distance) to determine if observed patterns hold across different methods of measuring entity similarity in embedding space