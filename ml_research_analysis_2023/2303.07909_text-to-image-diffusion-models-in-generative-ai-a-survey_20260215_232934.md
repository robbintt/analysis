---
ver: rpa2
title: 'Text-to-image Diffusion Models in Generative AI: A Survey'
arxiv_id: '2303.07909'
source_url: https://arxiv.org/abs/2303.07909
tags:
- diffusion
- arxiv
- image
- preprint
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews recent progress in text-to-image
  diffusion models, covering foundational concepts, state-of-the-art methods, applications
  beyond image generation, and evaluation metrics. The survey categorizes methods
  based on whether diffusion occurs in pixel space (e.g., GLIDE, Imagen) or latent
  space (e.g., Stable Diffusion, DALL-E 2), and discusses improvements including model
  architectures, spatial control, and concept control.
---

# Text-to-image Diffusion Models in Generative AI: A Survey

## Quick Facts
- arXiv ID: 2303.07909
- Source URL: https://arxiv.org/abs/2303.07909
- Reference count: 40
- Primary result: Comprehensive survey of text-to-image diffusion models covering architectures, evaluation metrics, applications, and ethical considerations

## Executive Summary
This survey provides a systematic overview of recent advances in text-to-image diffusion models, which have revolutionized the field of generative AI. The paper categorizes methods based on whether diffusion occurs in pixel space or latent space, reviews state-of-the-art approaches like GLIDE, Imagen, Stable Diffusion, and DALL-E 2, and discusses improvements in model architectures, spatial control, and concept control. The survey also addresses applications beyond image generation, evaluation metrics including FID and CLIP score, and critical ethical issues such as dataset bias and potential misuse. It concludes by identifying challenges and future directions for the field.

## Method Summary
The survey synthesizes information from recent literature on text-to-image diffusion models, organizing methods into categories based on their architectural choices and training approaches. It describes the fundamental mechanism of diffusion models as Markov chains trained with variational inference that iteratively denoise random noise conditioned on text embeddings. The paper reviews key innovations including classifier-free guidance, cross-attention mechanisms, and latent space diffusion, while also examining evaluation frameworks and ethical considerations. The methodology involves comprehensive literature review, categorization of approaches, and synthesis of technical progress across multiple dimensions.

## Key Results
- Text-to-image diffusion models can be categorized into pixel-space methods (GLIDE, Imagen) and latent-space methods (Stable Diffusion, DALL-E 2) based on where the diffusion process occurs
- Classifier-free guidance significantly improves both photorealism and text-image alignment by interpolating between conditional and unconditional denoising predictions
- Current evaluation metrics like FID and CLIP score have limitations, and there is a need for more comprehensive and standardized evaluation frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-image diffusion models generate images by iteratively denoising random noise conditioned on text embeddings.
- Mechanism: The model learns a Markov chain where each step applies a learned denoising function that reduces noise while being guided by text embeddings, effectively "reverse diffusing" from pure noise to a coherent image.
- Core assumption: The text embeddings provide sufficient guidance throughout the denoising process to align the generated image with the textual description.
- Evidence anchors:
  - [abstract] "This survey comprehensively reviews recent progress in text-to-image diffusion models, covering foundational concepts, state-of-the-art methods..."
  - [section] "Diffusion models (DMs), also widely known as diffusion probabilistic models [25], are a family of generated models that are Markov chains trained with variational inference [26]."
  - [corpus] Weak evidence - corpus contains related surveys but no direct mechanism description.
- Break condition: If text embeddings fail to provide coherent guidance at early denoising stages, the generated image will diverge from the textual description.

### Mechanism 2
- Claim: Classifier-free guidance improves both photorealism and text-image alignment by interpolating between conditional and unconditional denoising predictions.
- Mechanism: During training, the model learns both conditional (text-guided) and unconditional (text-free) denoising functions. At inference, it extrapolates beyond the conditional prediction toward the unconditional prediction, amplifying the text influence.
- Core assumption: The interpolation/extrapolation between conditional and unconditional predictions creates a stronger text-guidance signal without requiring an external classifier.
- Evidence anchors:
  - [section] "Different from classifier-guided diffusion model [37] that exploits an additional classifier, it is found in [38] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance."
  - [abstract] "We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos..."
  - [corpus] Weak evidence - corpus neighbors discuss diffusion models but don't detail classifier-free guidance mechanism.
- Break condition: Excessive guidance scale can lead to artifacts and loss of diversity in generated images.

### Mechanism 3
- Claim: Latent diffusion models achieve computational efficiency by performing denoising in compressed latent space rather than pixel space.
- Mechanism: The image is first compressed to a lower-dimensional latent representation using a VAE or VQ-GAN. The diffusion process then operates on this compact representation, significantly reducing computational complexity while preserving essential image features.
- Core assumption: The latent space preserves sufficient information to reconstruct high-quality images after the denoising process completes.
- Evidence anchors:
  - [section] "A representative framework that trains the diffusion models on latent space is Stable Diffusion, which is a scaled-up version of Latent Diffusion Model (LDM) [17]."
  - [abstract] "The survey categorizes methods based on whether diffusion occurs in pixel space (e.g., GLIDE, Imagen) or latent space (e.g., Stable Diffusion, DALL-E 2)..."
  - [corpus] Weak evidence - corpus contains related works but lacks detailed latent space mechanism explanation.
- Break condition: If the latent space compression is too aggressive, important image details may be lost, resulting in lower quality reconstructions.

## Foundational Learning

- Concept: Variational inference in diffusion models
  - Why needed here: Understanding how diffusion models learn to reverse the noising process is fundamental to grasping their architecture and training objectives.
  - Quick check question: What is the relationship between the forward noising process and the reverse denoising process in terms of probability distributions?

- Concept: Cross-attention mechanisms in text-conditioned generation
  - Why needed here: Text-to-image models use cross-attention to incorporate text information into the image generation process at multiple scales.
  - Quick check question: How does cross-attention enable the model to focus on different parts of the text prompt during different stages of image generation?

- Concept: Evaluation metrics for generative models (FID, CLIP score)
  - Why needed here: Understanding how to quantitatively assess the quality and text-alignment of generated images is crucial for comparing different methods.
  - Quick check question: What are the limitations of FID score when evaluating text-to-image models, and why is CLIP score often used alongside it?

## Architecture Onboarding

- Component map:
  Text encoder (CLIP, T5, or custom transformer) -> Latent space encoder (VAE or VQ-GAN) -> Diffusion prior (denoiser network with cross-attention) -> Guidance mechanism (classifier-free or classifier-guided) -> Image decoder (if using latent space) -> Sampling scheduler (deterministic or stochastic)

- Critical path:
  1. Text encoding → Cross-attention conditioning
  2. Latent encoding (if applicable) → Diffusion prior input
  3. Iterative denoising → Image reconstruction
  4. Guidance application → Enhanced text alignment

- Design tradeoffs:
  - Pixel space vs. latent space: Quality vs. computational efficiency
  - Classifier-free vs. classifier guidance: Simplicity vs. potential quality gains
  - Text encoder choice: Domain specificity vs. general language understanding
  - Sampling steps: Quality vs. inference speed

- Failure signatures:
  - Mode collapse: Generated images lack diversity
  - Poor text alignment: Images don't match text descriptions
  - Artifacts: Visual inconsistencies or distortions
  - Slow sampling: Excessive denoising steps required

- First 3 experiments:
  1. Implement a simple diffusion model without text conditioning to verify basic denoising functionality
  2. Add text conditioning via cross-attention and evaluate basic text-image alignment
  3. Implement classifier-free guidance and compare with baseline conditional model on standard benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can text-to-image diffusion models be made more computationally efficient for deployment on edge devices while maintaining high image fidelity?
- Basis in paper: [explicit] The paper notes that model size is very large, preventing deployment in efficiency-oriented environments like edge devices, and highlights challenges on data and computation.
- Why unresolved: Current models require extensive computational resources and large-scale training data, making them impractical for resource-constrained environments.
- What evidence would resolve it: Development and demonstration of compressed or optimized text-to-image diffusion models that achieve comparable image quality to existing models while significantly reducing computational requirements and model size for edge device deployment.

### Open Question 2
- Question: What evaluation framework can provide a unified, reproducible, and fair comparison of text-to-image diffusion models across different research groups?
- Basis in paper: [explicit] The paper identifies challenges on evaluation, noting that existing automatic metrics have limitations and that different papers use their own benchmarks, making fair comparison difficult.
- Why unresolved: Current evaluation methods are fragmented, rely on limited metrics, and lack standardization across the research community.
- What evidence would resolve it: Creation and adoption of a standardized evaluation protocol with clear criteria, diverse metrics, and publicly available benchmark datasets that enable consistent comparison across different text-to-image diffusion models.

### Open Question 3
- Question: How can text-to-image diffusion models be designed to better handle ethical concerns such as dataset bias, fairness in social group representation, and potential misuse for malicious purposes?
- Basis in paper: [explicit] The paper discusses ethical issues including dataset bias, fairness concerns, and misuse for malicious purposes like generating inappropriate content or falsifying evidence.
- Why unresolved: While some approaches address these issues individually, there is no comprehensive framework for building ethically robust text-to-image systems that prevent bias and misuse.
- What evidence would resolve it: Development of text-to-image models with built-in ethical safeguards that demonstrably reduce bias across different social groups and prevent generation of harmful content, validated through comprehensive ethical evaluation benchmarks.

## Limitations

- The survey acknowledges computational efficiency challenges and difficulties with fine-grained spatial control, but doesn't provide detailed quantitative comparisons between different methods
- Ethical considerations section raises important concerns about misuse and dataset bias but lacks specific empirical evidence or mitigation strategies
- The comprehensive scope means some technical details may be oversimplified, particularly regarding implementation specifics and hyperparameter configurations

## Confidence

- **High Confidence**: Foundational diffusion model concepts and general architecture descriptions - The survey accurately represents well-established principles of diffusion models and their application to text-to-image generation.
- **Medium Confidence**: Categorization of methods and state-of-the-art performance claims - While the survey provides good organization, specific performance comparisons lack detailed quantitative backing.
- **Low Confidence**: Evaluation methodology details and ethical impact assessments - The survey mentions metrics and ethical concerns but doesn't provide sufficient methodological depth or empirical evidence for these claims.

## Next Checks

1. **Implementation Verification**: Replicate a basic text-to-image diffusion model using the survey's described architecture to verify the claimed relationship between text conditioning and image quality.

2. **Metric Correlation Analysis**: Conduct experiments to validate the claimed relationship between FID scores, CLIP scores, and human perceptual quality ratings across different diffusion model variants.

3. **Ethical Impact Assessment**: Design and execute a small-scale study to empirically measure dataset bias in current text-to-image models, testing the survey's claims about representation disparities across different demographic groups.