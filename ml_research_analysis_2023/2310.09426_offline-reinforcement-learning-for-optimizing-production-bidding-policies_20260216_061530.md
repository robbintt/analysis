---
ver: rpa2
title: Offline Reinforcement Learning for Optimizing Production Bidding Policies
arxiv_id: '2310.09426'
source_url: https://arxiv.org/abs/2310.09426
tags:
- policy
- learning
- production
- agent
- bidding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing bidding policies in online advertising
  markets, where advertisers aim to maximize value under budget constraints. The authors
  propose a generalizable approach using offline reinforcement learning to optimize
  any differentiable base policy (typically a heuristic policy based on easily understood
  principles).
---

# Offline Reinforcement Learning for Optimizing Production Bidding Policies

## Quick Facts
- arXiv ID: 2310.09426
- Source URL: https://arxiv.org/abs/2310.09426
- Reference count: 7
- Key outcome: Hybrid actor-critic architecture enables offline RL optimization of production bidding policies while preserving explainability and safety

## Executive Summary
This paper presents a novel approach to optimizing bidding policies in online advertising markets using offline reinforcement learning. The method employs a hybrid agent architecture that combines existing differentiable base policies with deep neural networks, where only the optimized base policy parameters are eventually deployed to production. The approach was validated through both simulation and real-world production environments, demonstrating statistically significant performance improvements over baseline policies while maintaining the explainability and safety benefits of using existing production routines.

## Method Summary
The approach uses a hybrid actor-critic architecture where the actor implements the base production policy with a learned variance component, while the critic uses a neural network with expanded features for value estimation. The system is trained offline using Conservative Q-Learning (CQL) on data collected by the base policy with exploration noise. The critic is pre-trained to match the base policy's performance before actor training begins, and state-dependent action sampling is used to address the large dynamic range of action values in bidding environments. After training, only the optimized base policy parameters are deployed, discarding the neural network components.

## Key Results
- Statistically significant performance improvements over baseline policies in both simulation and production environments
- Successful deployment in real advertising campaigns without requiring changes to production infrastructure
- CQL with state-dependent sampling achieved stable learning with α = 0.1 penalty weight
- Pre-training the critic improved training stability and reduced overestimation bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid actor-critic architecture enables offline RL optimization of production bidding policies while preserving explainability and safety.
- Mechanism: The base policy parameters are learned via RL while the neural network components are discarded after training. The actor uses the base policy mean and a learned variance to create a stochastic policy for training, while the critic uses a larger feature set for accurate value predictions. This allows optimizing the base policy without deploying neural networks to production.
- Core assumption: The base policy is differentiable with respect to its parameters and the actor-critic architecture can learn optimal parameters from offline data.
- Evidence anchors:
  - [abstract]: "We use a hybrid agent architecture that combines arbitrary base policies with deep neural networks, where only the optimized base policy parameters are eventually deployed, and the neural network part is discarded after training."
  - [section 3.3]: "Once the agent is trained, only the base policy with optimized parameters Fw* is deployed to production... the actor is restricted to using the state representation features that are accepted by the base policy Fw. The critic, however, is not restricted by production constraints, since it is parameterized by a neural network and is not deployed to production."

### Mechanism 2
- Claim: Conservative Q-Learning (CQL) with state-dependent action sampling improves stability and performance in bidding environments.
- Mechanism: The CQL algorithm penalizes Q-values for out-of-distribution actions to prevent overestimation bias. In bidding, the action space is scaled by the current bid value, so uniform sampling is wasteful. Instead, sampling from a state-dependent uniform distribution around the base policy action provides more relevant exploration.
- Core assumption: The optimal actions in bidding are concentrated around the base policy's mean action, and state-dependent sampling captures this distribution.
- Evidence anchors:
  - [section 3.5]: "In bidding, the optimal action values in different states can differ by several orders of magnitude... Sampling from (10) ensures that sampled actions stay in the relevant interval for each state."
  - [section 4.1]: "At the intermediate value of α = 0.1, however, the agent produced a consistent stable performance, exhibiting positive performance gains along the entire learning process."

### Mechanism 3
- Claim: Pre-training the critic network on offline data before actor training improves learning stability and performance.
- Mechanism: The critic is pre-trained for 300,000 gradient steps to match the base policy's performance before actor training begins. This provides more accurate Q-value estimates for the actor's policy gradient updates, reducing variance and improving convergence.
- Core assumption: The offline data contains sufficient information to accurately estimate the critic's Q-values at the start of training.
- Evidence anchors:
  - [section 3.5]: "Since we initialize the agent to match the base policy used to collect the data πβw(·|s), we can pre-train the critic network from the collected data to have critic predictions match the actor performance at the start of the training process."
  - [section 4.1]: "As can be seen in Figure 4, although the critic tends to over-estimate the returns in some cases... in general, the critic predictions are quite accurate and are able to capture the difference in performance between campaigns with different budget values."

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of bidding problems
  - Why needed here: The paper explicitly frames the bidding problem as an episodic MDP where each ad campaign is an episode and the agent learns a policy to maximize cumulative reward under budget constraints.
  - Quick check question: What are the state, action, and reward components in the MDP formulation of bidding?

- Concept: Offline reinforcement learning and distributional shift
  - Why needed here: The approach relies on learning from offline data collected by a base policy, and CQL specifically addresses the challenge of distributional shift between the behavior policy and learned policy.
  - Quick check question: How does CQL mitigate the distributional shift problem in offline RL?

- Concept: Actor-critic architecture and entropy regularization
  - Why needed here: The hybrid agent uses an actor-critic architecture where the actor learns a stochastic policy and the critic estimates Q-values. The paper mentions SAC as the base algorithm for CQL.
  - Quick check question: What is the role of the entropy term in SAC, and why is it removed in this offline setting?

## Architecture Onboarding

- Component map: Base production bidding policy -> Behavior policy with exploration noise -> Hybrid actor-critic agent (offline training) -> Optimized base policy parameters (deployment)
- Critical path: Data collection → Offline training with CQL → Parameter optimization → Deployment of base policy with optimized parameters
- Design tradeoffs: The hybrid architecture trades some optimization potential for safety and explainability by restricting the actor to base policy features while using neural networks only for training. Pre-training the critic adds stability but requires sufficient offline data.
- Failure signatures: Poor performance may indicate insufficient exploration in data collection, distributional shift issues, or inadequate offline data coverage. Instability during training may suggest incorrect CQL hyperparameters or poor critic initialization.
- First 3 experiments:
  1. Verify the base policy is differentiable and the hybrid actor-critic architecture can reproduce baseline performance with pre-trained critic.
  2. Test different CQL penalty weights (α) in simulation to find the optimal balance between stability and performance improvement.
  3. Evaluate the impact of state-dependent action sampling vs. uniform sampling on CQL training stability and performance in simulation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the limitations of learning directly from base policy parameters be overcome while maintaining production constraints?
- Basis in paper: [explicit] "First, learning directly the parameters of a base production policy restricts us to the set of policies representable with that parameterization, and hence limits optimizing power."
- Why unresolved: The paper acknowledges this limitation but does not propose specific solutions for overcoming it within production constraints.
- What evidence would resolve it: A proposed method that expands the policy representation beyond base parameters while still meeting safety and explainability requirements, demonstrated through successful A/B testing.

### Open Question 2
- Question: What is the maximum acceptable distribution shift between the learned policy and behavior policy before offline RL becomes unreliable in bidding environments?
- Basis in paper: [explicit] "Second, offline RL can only reliably improve upon a base policy as long as the distribution shift is not large and the trained policy does not deviate too much from the initial policy."
- Why unresolved: The paper mentions this constraint but doesn't quantify the threshold or provide methods to measure acceptable distribution shift.
- What evidence would resolve it: Empirical studies showing performance degradation as a function of distribution shift metrics, identifying specific thresholds where offline RL performance breaks down.

### Open Question 3
- Question: How can the actor-critic architecture be further optimized to improve the accuracy of value function predictions in bidding environments?
- Basis in paper: [inferred] The paper notes that the critic tends to overestimate returns for small budget campaigns, suggesting room for improvement in value function accuracy.
- Why unresolved: While the paper discusses modifications to improve critic accuracy, it doesn't explore advanced architectures or training techniques that could further enhance performance.
- What evidence would resolve it: Comparative experiments showing improved return predictions and downstream policy performance using alternative critic architectures or training methods.

## Limitations

- Method effectiveness depends on having a differentiable base policy with meaningful business logic
- Offline-only training cannot adapt to changing market conditions without periodic retraining
- Performance gains vary substantially across campaigns, suggesting benefits may not be uniform

## Confidence

- **High confidence**: The hybrid architecture design and offline training approach are technically sound and well-validated through both simulation and real-world experiments.
- **Medium confidence**: The CQL with state-dependent sampling provides meaningful stability improvements, though optimal hyperparameters may vary by environment.
- **Medium confidence**: The explainability and safety benefits are valid but come at the cost of potentially suboptimal performance compared to fully neural approaches.

## Next Checks

1. Test the method's robustness across multiple market conditions and bidding environments to assess generalizability beyond the single platform studied.
2. Evaluate the sensitivity of performance to CQL hyperparameters (α) and determine if automated tuning methods could improve consistency.
3. Compare the approach against state-of-the-art online RL methods in controlled experiments to quantify the tradeoff between safety and performance.