---
ver: rpa2
title: Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration
  Complexity
arxiv_id: '2310.17998'
source_url: https://arxiv.org/abs/2310.17998
tags:
- bound
- adam
- theorem
- learning
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between the upper and lower bounds
  of the iteration complexity of Adam, a popular adaptive optimization algorithm.
  Existing theoretical analyses of Adam's convergence either require additional assumptions
  beyond the standard $L$-smoothness and bounded variance, or their bounds do not
  match the known lower bound of $\Omega(1/\epsilon^4)$ for first-order methods.
---

# Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity

## Quick Facts
- **arXiv ID**: 2310.17998
- **Source URL**: https://arxiv.org/abs/2310.17998
- **Reference count**: 40
- **Primary result**: Derives tight convergence bound for Adam matching the lower bound $\Omega(1/\epsilon^4)$ under standard assumptions

## Executive Summary
This paper resolves a long-standing gap in the theoretical analysis of Adam by deriving a convergence guarantee that matches the known lower bound for first-order methods. The key insight is controlling the entanglement between momentum and adaptive learning rate through novel techniques including an auxiliary function and potential function approach. Under standard $L$-smoothness and bounded variance assumptions (without additional requirements like bounded gradients), the paper shows Adam achieves an expected gradient norm of $O(1/T^{1/4})$ with carefully chosen hyperparameters, making it theoretically optimal.

## Method Summary
The paper analyzes Adam's convergence by introducing a potential function $u_t = \frac{w_t - \beta_1\sqrt{\beta_2}w_{t-1}}{1-\beta_1\sqrt{\beta_2}}$ and an auxiliary function $\xi_t$ to control error terms. The analysis handles the stochasticity in both gradient estimates and adaptive learning rates through telescoping sums. A coordinate-wise decomposition approach converts regret bounds to gradient norm bounds. The convergence is proven for a wide range of hyperparameters, with the tightest bound achieved at $\eta = \Theta(1/\sqrt{T})$ and $\beta_2 = 1 - \Theta(1/T)$, requiring $\beta_1 \leq \sqrt{\beta_2} - 8\sigma_1^2(1-\beta_2)\beta_2^{-2}$.

## Key Results
- Adam's iteration complexity matches the lower bound of $\Omega(1/\epsilon^4)$ for first-order methods
- Convergence is proven under only $L$-smoothness and bounded variance assumptions
- Expected gradient norm achieves $O(1/T^{1/4})$ with appropriate hyperparameters
- The bound holds without requiring bounded gradients or additional smoothness assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary function $\xi_t = E[\eta\langle G_t, -\frac{1}{\sqrt{\tilde{\nu}_{t+1}}} \odot G_t\rangle]$ controls error terms through telescoping
- Mechanism: Allows error term to be bounded using telescoping sum $\sum_t \frac{1}{\sqrt{\beta_2}}\xi_{t-1} - \xi_t$
- Core assumption: Bounded variance (Assumption 2) and coordinate-wise affine variance (Assumption 3)
- Evidence anchors:
  - [abstract]: "Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate..."
  - [section]: "One can easily observe that the first term can be controlled by 'First Order Main' term..."
- Break condition: Violation of variance bound $\sigma^2$ or failure of coordinate-wise assumption

### Mechanism 2
- Claim: Potential function $f(u_t)$ resolves mismatch between stochastic gradients and momentum
- Mechanism: Applies descent lemma to $f(u_t)$ instead of $f(w_t)$, converting problematic terms to negative definite forms
- Core assumption: Smoothness assumption (Assumption 1)
- Evidence anchors:
  - [abstract]: "... and to convert the first-order term in the Descent Lemma to the gradient norm"
  - [section]: "We propose to use the potential function $f(u_t)$ with $u_t = \frac{w_t - \beta_1\sqrt{\beta_2}w_{t-1}}{1-\beta_1\sqrt{\beta_2}}$"
- Break condition: Excessive smoothness constant $L$ relative to step size

### Mechanism 3
- Claim: Coordinate-wise decomposition converts regret bounds to gradient norm bounds
- Mechanism: Separate treatment of coordinates using $|G_{t,i}| \geq \frac{\sigma_0}{\sigma_1}$ and $|G_{t,i}| < \frac{\sigma_0}{\sigma_1}$ cases, combined via Cauchy-Schwarz
- Core assumption: Coordinate-wise affine variance (Assumption 3)
- Evidence anchors:
  - [abstract]: "... and to convert the first-order term in the Descent Lemma to the gradient norm"
  - [section]: "To begin with, note that due to Cauchy's inequality and Hölder's inequality..."
- Break condition: Super-linear growth of noise variance with gradient norm

## Foundational Learning

- Concept: Descent Lemma for smooth functions
  - Why needed here: The entire convergence analysis relies on the descent lemma to bound progress in each iteration
  - Quick check question: Can you state the descent lemma for an L-smooth function and explain why L-smoothness matters, not just differentiability?

- Concept: Martingale convergence and conditional expectations
  - Why needed here: Analysis requires careful handling of conditional expectations $E|F_t[\cdot]$ and momentum-gradient relationships
  - Quick check question: Given $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$, what is $E|F_t[m_t]$ in terms of $G_t$ and previous gradients?

- Concept: Coordinate-wise analysis vs. joint analysis
  - Why needed here: Adaptive learning rate is coordinate-wise, requiring separate treatment to convert regret bounds to gradient norm bounds
  - Quick check question: Why can't we simply apply Cauchy-Schwarz to $\sum_t E[\| \frac{1}{\sqrt{\tilde{\nu}_t}} \odot G_t \|^2]$ to get $E[\|G_t\|]$ without coordinate-wise decomposition?

## Architecture Onboarding

- Component map:
  Core optimizer loop (Algorithm 1) -> Lyapunov/potential function construction -> Auxiliary function for error control -> Coordinate-wise decomposition module -> Telescoping sum analysis

- Critical path:
  1. Initialize hyperparameters and variables
  2. For each iteration: compute gradient, update moments, update parameters
  3. Apply descent lemma to potential function
  4. Bound first-order term using auxiliary function
  5. Bound second-order term using smoothness
  6. Combine via coordinate-wise analysis
  7. Sum over iterations and apply Cauchy-Schwarz

- Design tradeoffs:
  - Potential function vs. direct analysis: Resolves momentum-stochasticity mismatch but adds complexity
  - Coordinate-wise vs. joint analysis: Necessary for adaptive learning rate but introduces dimensional dependence
  - Auxiliary function complexity vs. tightness: Key to closing gap but makes bound messy

- Failure signatures:
  - If $\beta_1 > \sqrt{\beta_2} - 8\sigma_1^2(1-\beta_2)\beta_2^{-2}$: Divergence due to momentum-stochasticity entanglement
  - If $\sigma_1$ is too large: Error terms dominate and convergence fails
  - If smoothness $L$ is too large: Second-order terms become unmanageable

- First 3 experiments:
  1. Verify auxiliary function telescoping by computing $\sum_t \frac{1}{\sqrt{\beta_2}}\xi_{t-1} - \xi_t$ for synthetic problem
  2. Test coordinate-wise decomposition by comparing bounds with/without coordinate separation on 2D quadratic
  3. Validate potential function approach by comparing Adam vs. SGD with momentum on smooth non-convex problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on dimension d in iteration complexity bounds be removed?
- Basis in paper: [explicit] Authors state removing d-dependence is technically hard due to coordinate-wise learning rate
- Why unresolved: Descent lemma combines all coordinates, making separate coordinate analysis difficult
- What evidence would resolve it: Proof showing Adam's iteration complexity is independent of dimension d

### Open Question 2
- Question: Does momentum help Adam converge faster under the same assumptions?
- Basis in paper: [explicit] Tightest bound achieved when β1 = 0 (no momentum), contradicting common wisdom
- Why unresolved: Benefit of momentum is unclear even for simple optimizers like SGD with momentum
- What evidence would resolve it: Experiments or theoretical analysis showing non-zero momentum improves convergence

### Open Question 3
- Question: Is there a fundamental difference between Adam and AdaGrad explaining their convergence rate gap?
- Basis in paper: [inferred] Authors draw analogy between Adam without momentum and AdaGrad
- Why unresolved: Authors suggest possible explanation but don't provide definitive answer
- What evidence would resolve it: Rigorous analysis comparing Adam and AdaGrad convergence rates under same assumptions

## Limitations
- Requires specific hyperparameter scheduling (η = Θ(1/√T), β₂ = 1 - Θ(1/T)) that may be impractical
- Relies on coordinate-wise affine variance assumption that may not hold in practice
- The coordinate-wise decomposition approach may introduce looseness in the bound

## Confidence
- Theoretical optimality claim: Medium - mathematically rigorous but dependent on strong assumptions
- Practical applicability: Low - non-standard hyperparameter scheduling may be sensitive to implementation
- Novelty of techniques: High - auxiliary function and potential function approaches appear genuinely novel

## Next Checks
1. Implement Adam with specified hyperparameter schedules (η = Θ(1/√T), β₂ = 1 - Θ(1/T)) and test on standard non-convex optimization problems to verify O(1/T^{1/4}) convergence empirically

2. Systematically test convergence bounds when relaxing coordinate-wise affine variance assumption and bounded gradient requirements to understand robustness of theoretical guarantees

3. Compare convergence rates of theoretically optimal Adam variant against standard Adam implementations on benchmark problems to quantify practical impact of theoretical improvements