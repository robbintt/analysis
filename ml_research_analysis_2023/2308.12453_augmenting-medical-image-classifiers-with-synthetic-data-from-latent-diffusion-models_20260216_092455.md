---
ver: rpa2
title: Augmenting medical image classifiers with synthetic data from latent diffusion
  models
arxiv_id: '2308.12453'
source_url: https://arxiv.org/abs/2308.12453
tags:
- images
- synthetic
- image
- data
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether synthetic images generated by latent
  diffusion models can augment skin disease classifiers, addressing the need for diverse
  and balanced medical image datasets. The authors generated 458,920 synthetic images
  across nine skin conditions and two datasets (Fitzpatrick 17k and Stanford DDI),
  using inpainting, outpainting, and text-to-image methods with fine-tuned Stable
  Diffusion.
---

# Augmenting medical image classifiers with synthetic data from latent diffusion models

## Quick Facts
- arXiv ID: 2308.12453
- Source URL: https://arxiv.org/abs/2308.12453
- Reference count: 40
- Primary result: Synthetic images generated by latent diffusion models improve skin disease classifier accuracy in low-data settings, with gains up to 13.2% at 32 real images per class

## Executive Summary
This study investigates whether synthetic images generated by latent diffusion models can augment skin disease classifiers, addressing the need for diverse and balanced medical image datasets. The authors generated 458,920 synthetic images across nine skin conditions and two datasets (Fitzpatrick 17k and Stanford DDI), using inpainting, outpainting, and text-to-image methods with fine-tuned Stable Diffusion. Synthetic data improved classifier accuracy in low-data settings, with gains up to 13.2% at 32 real images per class; however, improvements saturated at synthetic-to-real ratios above 10:1 and were smaller than gains from adding real images. Synthetic augmentation also improved malignancy classification across skin tones, but real data collection remains the most critical factor for performance and fairness.

## Method Summary
The study used two medical image datasets (Fitzpatrick 17k and Stanford DDI) containing 3,699 real images across nine skin conditions. Synthetic images were generated using Stable Diffusion v2.1 with DreamBooth fine-tuning on training images. Three generation methods were employed: text-to-image, inpainting, and in-then-outpainting. EfficientNetV2-M classifiers were trained with varying amounts of synthetic augmentation (0-75 synthetic images per real image) and evaluated across different Fitzpatrick Skin Types using 5-fold runs and balanced test sets.

## Key Results
- Classifier accuracy improved by up to 13.2% when synthetic data was added to training sets with only 32 real images per disease class
- Performance gains saturated at synthetic-to-real image ratios above 10:1, with no additional improvement from more synthetic data
- Synthetic augmentation improved malignancy classification across all Fitzpatrick Skin Types, with accuracy gains ranging from +6.9% to +11.9% depending on skin tone
- Traditional image transforms (flipping, cropping, rotating) provided larger accuracy gains than synthetic augmentation alone

## Why This Works (Mechanism)

### Mechanism 1
Synthetic images generated by latent diffusion models improve skin disease classifier performance in low-data regimes by providing additional training signal diversity. Diffusion models produce high-fidelity synthetic images that closely match real data distributions, allowing models to learn richer feature representations even with limited real images.

### Mechanism 2
Synthetic data augmentation exhibits dose-response behavior with performance gains saturating at high synthetic-to-real ratios. Adding synthetic images improves performance up to a point, after which additional synthetic data provides diminishing returns due to redundancy or distributional mismatch.

### Mechanism 3
Synthetic augmentation improves fairness by increasing classifier accuracy across all skin tones, not just underrepresented groups. By providing diverse synthetic training examples across the full spectrum of skin tones, the classifier learns features that generalize better across all demographic groups.

## Foundational Learning

- **Concept: Latent diffusion models**
  - Why needed here: Understanding how diffusion models generate synthetic medical images is crucial for evaluating their effectiveness and limitations
  - Quick check question: What is the key difference between diffusion models and GANs in terms of image generation quality and control?

- **Concept: Data augmentation strategies**
  - Why needed here: Comparing synthetic augmentation to traditional methods (flipping, cropping, rotating) helps isolate the specific benefits of synthetic data
  - Quick check question: How does the performance gain from synthetic data compare to that from traditional image transforms in this study?

- **Concept: Model evaluation in medical AI**
  - Why needed here: Understanding how classifier performance is measured across different skin tones and disease conditions is essential for assessing fairness and generalizability
  - Quick check question: What evaluation metrics were used to assess classifier performance across different Fitzpatrick Skin Types?

## Architecture Onboarding

- **Component map**: Real image preprocessing -> Synthetic image generation (DreamBooth + Stable Diffusion) -> Combined dataset creation -> EfficientNetV2-M training -> Performance evaluation across skin tones

- **Critical path**: Synthetic image generation -> Training data preparation -> Model training -> Performance evaluation across skin tones

- **Design tradeoffs**:
  - Real vs. synthetic data: Real data provides ground truth but is expensive and potentially biased; synthetic data is scalable but may introduce hallucinations
  - Generation methods: Text-to-image provides diversity but may lack specificity; inpainting/outpainting preserves disease features but may be more constrained
  - Model architecture: EfficientNetV2-M balances performance and computational cost; larger models might capture more complex features

- **Failure signatures**:
  - Performance plateau at high synthetic-to-real ratios (>10:1)
  - Inconsistent accuracy improvements across different skin tones
  - Synthetic images that don't match real image distributions (detected via embedding distance analysis)

- **First 3 experiments**:
  1. Compare classifier performance with 0, 10, 25, 50, and 75 synthetic images per real image to identify saturation point
  2. Evaluate synthetic augmentation effectiveness across different Fitzpatrick Skin Types to assess fairness impact
  3. Test different generation methods (text-to-image, inpainting, in-then-outpainting) to determine which produces most useful synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal synthetic-to-real image ratio for maximum performance gains in medical image classification?
- Basis in paper: The study found performance gains saturate at synthetic-to-real ratios above 10:1
- Why unresolved: The exact optimal ratio may vary by dataset size, disease class, and generation method
- What evidence would resolve it: Systematic experiments testing different ratios (1:1 to 100:1) across multiple medical imaging datasets and conditions

### Open Question 2
How do diffusion models compare to GANs for synthetic medical image generation in terms of both photorealism and performance impact?
- Basis in paper: The paper mentions GANs have not shown meaningful performance improvements while diffusion models show promise
- Why unresolved: Direct head-to-head comparisons between these methods on medical imaging tasks are lacking
- What evidence would resolve it: Controlled experiments comparing both methods on identical datasets with human clinician evaluation of photorealism

### Open Question 3
Does synthetic data augmentation improve model fairness across skin tones in real-world clinical deployment?
- Basis in paper: The study showed synthetic data improved malignancy classification across Fitzpatrick Skin Types, but only one comparison remained significant after correction
- Why unresolved: Laboratory results don't necessarily translate to real-world clinical settings with diverse populations
- What evidence would resolve it: Clinical deployment studies measuring performance and fairness across diverse patient populations with varying skin tones

## Limitations

- Synthetic-to-real saturation effect limits the scalability of augmentation, with diminishing returns above 10:1 ratio
- Synthetic generation may not fully capture true population diversity, potentially introducing subtle biases
- Performance gains from synthetic augmentation are smaller than those from adding real images, highlighting data collection as the primary bottleneck

## Confidence

- **Synthetic augmentation improves low-data classifier performance** - High confidence
- **Performance gains saturate at high synthetic-to-real ratios** - Medium confidence  
- **Synthetic augmentation improves fairness across skin tones** - Medium confidence

## Next Checks

1. **Cross-dataset validation**: Test whether the synthetic augmentation benefits transfer to completely independent skin disease datasets not used in training or synthetic generation, to assess generalizability of the approach.

2. **Diversity gap analysis**: Conduct systematic evaluation of synthetic image diversity across different Fitzpatrick Skin Types compared to real images, including quantitative measures of skin tone representation and demographic balance.

3. **Long-tail class performance**: Specifically evaluate classifier performance on rare or underrepresented skin conditions within the nine classes studied, to determine if synthetic augmentation provides disproportionate benefits to data-scarce disease categories.