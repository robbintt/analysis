---
ver: rpa2
title: 'Retrieve and Copy: Scaling ASR Personalization to Large Catalogs'
arxiv_id: '2311.08402'
source_url: https://arxiv.org/abs/2311.08402
tags:
- catalog
- entities
- inference
- large
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of scaling automatic speech recognition
  (ASR) personalization to large catalogs, which is critical for real-world applications
  but hindered by performance constraints. The authors propose a "Retrieve and Copy"
  inference strategy that leverages Approximate Nearest Neighbor (ANN) search methods
  like FAISS to selectively retrieve a small subset of relevant entities per audio,
  reducing inference latency by at least 20%.
---

# Retrieve and Copy: Scaling ASR Personalization to Large Catalogs

## Quick Facts
- arXiv ID: 2311.08402
- Source URL: https://arxiv.org/abs/2311.08402
- Reference count: 16
- Key outcome: "Retrieve and Copy" inference strategy achieves 20%+ speedup while maintaining accuracy on catalogs up to 20K entities

## Executive Summary
This work addresses the challenge of scaling automatic speech recognition (ASR) personalization to large catalogs, which is critical for real-world applications but hindered by performance constraints. The authors propose a "Retrieve and Copy" inference strategy that leverages Approximate Nearest Neighbor (ANN) search methods like FAISS to selectively retrieve a small subset of relevant entities per audio, reducing inference latency by at least 20%. Additionally, they introduce a hard negative fine-tuning strategy to improve disambiguation between phonetically similar entities, resulting in up to 6% more Word Error Rate reduction (WERR) and 3.6% absolute improvement in F1-scores compared to a strong baseline. The proposed methods enable scaling to catalog sizes of up to 20K entities without significantly affecting WER and F1-scores.

## Method Summary
The authors propose a "Retrieve and Copy" inference strategy that leverages FAISS for efficient Approximate Nearest Neighbor search to retrieve top-k relevant entities per audio frame, reducing the attention computation from O(N) to O(k). They also introduce a hard negative fine-tuning strategy that clusters low-frequency words and uses same-cluster words as hard negatives during training to improve disambiguation between phonetically similar entities. The approach is evaluated on five in-house conversational datasets and one public dataset (VoxPopuli) with catalog sizes up to 20K entities.

## Key Results
- Achieves at least 20% inference speedup per acoustic frame while maintaining similar F1 scores
- Up to 6% more Word Error Rate reduction (WERR) compared to baseline
- 3.6% absolute improvement in F1-scores
- Scales to catalog sizes of up to 20K entities without significantly affecting WER and F1-scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ANN search reduces inference latency by 20%+ while retaining accuracy.
- Mechanism: Transforms inner-product similarity to Euclidean distance via extended vector, enabling fast FAISS retrieval of top-k entities per frame.
- Core assumption: Correct entity will be among the top-k nearest neighbors in embedding space.
- Evidence anchors:
  - [abstract] "Our method also allows for large catalog sizes of up to 20K without significantly affecting WER and F1-scores, while achieving at least 20% inference speedup per acoustic frame."
  - [section 4.1] "We transform our vectors from d-dimension to d + 1 and find top-k entities with the least Euclidean distance with the query vector at each time frame."
  - [corpus] Weak - no direct citation of FAISS in related work; only mentions "approximate nearest neighbor search".
- Break condition: If catalog contains phonetically similar entities not well separated in embedding space, top-k may miss the correct entity, causing accuracy loss.

### Mechanism 2
- Claim: Hard negative fine-tuning improves disambiguation between phonetically similar entities.
- Mechanism: Clusters low-frequency words in embedding space, then during fine-tuning uses same-cluster words as negatives instead of random negatives, forcing model to learn subtle phonetic distinctions.
- Core assumption: Entities in same cluster are phonetically similar and cause confusion.
- Evidence anchors:
  - [abstract] "up to 6% more Word Error Rate reduction (WERR) and 3.6% absolute improvement in F1-scores compared to a strong baseline."
  - [section 4.2] "We take all the low term-frequency words... do k-means clustering into s clusters. Then during fine-tuning... choose words from the same cluster as w as hard negatives."
  - [corpus] Weak - no explicit clustering-based hard negative strategy cited in related work; mentions "difficult examples" but not clustering.
- Break condition: If clustering poorly separates phonetically similar words, negatives may not be truly challenging, reducing fine-tuning effectiveness.

### Mechanism 3
- Claim: Retrieval reduces dependency of attention computation from O(N) to O(k), preserving performance while improving speed.
- Mechanism: Approximates full attention over N entities with sparse attention over k retrieved entities, justified by observation that only one entity is likely relevant per frame.
- Core assumption: For any audio frame, only a small subset of entities are phonetically plausible.
- Evidence anchors:
  - [section 4.1] "Assuming either one or none of the custom entities will be spoken in a given audio, the score sn t of all but one would be close to 0. Therefore, the biasing vector can be approximated using Bt = Pk 1 sn t θV Cn."
  - [section 6] "we observe that our proposed FAISS based inference can help reduce the increase in latency with the increase in catalog size while maintaining similar F1 scores."
  - [corpus] Weak - no direct citation of sparse attention for ASR biasing; related work focuses on filtering, not sparse attention approximation.
- Break condition: If multiple entities are phonetically plausible per frame (e.g., homophones in context), sparse attention may miss correct entity, degrading accuracy.

## Foundational Learning

- Concept: Approximate Nearest Neighbor search (FAISS)
  - Why needed here: Enables fast retrieval of relevant entities from large catalog without computing full attention over all N entities.
  - Quick check question: How does FAISS transform the inner-product similarity problem into a Euclidean distance problem?

- Concept: Clustering for hard negative mining
  - Why needed here: Identifies phonetically similar entities that are likely to confuse the model, enabling targeted fine-tuning.
  - Quick check question: What distance metric should be used in k-means to ensure phonetically similar words cluster together?

- Concept: Contextual adapters and attention biasing
  - Why needed here: Core mechanism by which retrieved entities influence ASR output; understanding attention scoring is critical for debugging retrieval failures.
  - Quick check question: How does the attention score sn t = ⟨θQX t, θKCn⟩ relate to the entity's phonetic relevance for the current audio frame?

## Architecture Onboarding

- Component map:
  - CTC Encoder -> Conformer blocks -> Word-piece posteriors
  - Catalog Encoder (LSTM) -> Entity embeddings (C1:N)
  - Biasing Adapter (Attention) -> Scores and biasing vectors
  - FAISS Index -> Precomputed entity embeddings
  - ANN Search -> Top-k entity retrieval per frame
  - Hard Negative Fine-tuning -> Additional training stage

- Critical path:
  1. Audio features -> CTC Encoder
  2. Frame embeddings -> ANN query -> Top-k entities
  3. Retrieved entities + frame -> Attention -> Biasing vector
  4. Biasing vector + frame -> Final posteriors

- Design tradeoffs:
  - k (retrieval size): Larger k -> better accuracy, higher latency; smaller k -> faster, risk of missing correct entity.
  - Cluster count s: More clusters -> finer-grained hard negatives, risk of noisy clusters; fewer clusters -> cleaner negatives, risk of missing subtle distinctions.
  - ANN method: FAISS-IVF vs HNSWLIB trade-offs in speed vs accuracy.

- Failure signatures:
  - Accuracy drop with large catalogs -> likely retrieval missing correct entity or hard negatives not challenging enough.
  - Latency still high -> ANN search not effective, possibly due to poor indexing or high k.
  - Model overfitting to hard negatives -> validation WER increases, check cluster quality and negative sampling.

- First 3 experiments:
  1. Verify retrieval accuracy: For a held-out test set, check percentage of correct entities in top-k retrieved set; adjust k until >95% accuracy.
  2. Validate clustering quality: Visualize entity embeddings colored by cluster; ensure phonetically similar words are in same cluster.
  3. Ablation on fine-tuning: Train with and without hard negative fine-tuning on a small catalog; compare WER and F1 to confirm effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale beyond 20K entities in terms of accuracy and latency?
- Basis in paper: [explicit] The authors mention that they plan to study scaling beyond 20K entities as part of future work in the Limitations section.
- Why unresolved: The current experiments only show results for catalogs up to 20K entities, and the authors acknowledge that more work is needed to scale the approach to even larger catalog sizes.
- What evidence would resolve it: Conducting experiments with catalogs larger than 20K entities and measuring the accuracy and latency of the proposed method to determine its scalability.

### Open Question 2
- Question: How does the proposed method handle the challenge of regressions on common words when using contextual biasing with large catalogs?
- Basis in paper: [explicit] The authors mention in the Limitations section that using contextual biasing approaches can cause regressions on other common words in the dataset, especially with large catalogs.
- Why unresolved: The current experiments do not provide a solution to this challenge, and the authors plan to tackle this issue in future work.
- What evidence would resolve it: Developing and evaluating a strategy to mitigate regressions on common words when using contextual biasing with large catalogs.

### Open Question 3
- Question: How can the proposed method be made more inclusive of different groups and communities?
- Basis in paper: [explicit] The authors acknowledge in the Ethics Statement that even a 20K list of first names might miss names from particular communities more than others, and they plan to study scaling beyond 20K entities to make the method more inclusive.
- Why unresolved: The current experiments do not address the inclusivity of different groups and communities, and the authors recognize the need to scale beyond 20K entities to achieve this goal.
- What evidence would resolve it: Conducting experiments with catalogs that include a diverse range of names and entities from different groups and communities to evaluate the inclusivity of the proposed method.

## Limitations

- Catalog composition ambiguity: Limited details about in-house datasets' characteristics make generalizability assessment difficult
- FAISS implementation specifics missing: Critical implementation details like index type, dimensionality, and parameters are not provided
- Clustering methodology gaps: Distance metric for clustering and quality metrics are not specified

## Confidence

- **High Confidence**: The core "Retrieve and Copy" inference strategy using FAISS for ANN search is technically sound and well-supported by the presented results
- **Medium Confidence**: The hard negative fine-tuning mechanism shows promise with reported improvements, but effectiveness depends heavily on clustering quality
- **Low Confidence**: The claim that the method scales to 20K entities "without significantly affecting WER and F1-scores" is based on in-house datasets only

## Next Checks

1. **Retrieval accuracy validation**: For a held-out test set, compute the percentage of correct entities appearing in the top-k retrieved set across different k values (5, 10, 20)
2. **Clustering quality assessment**: Apply the clustering methodology to a public dataset with phonetically similar entities and visualize clusters with silhouette scores
3. **Catalog size sensitivity analysis**: Systematically evaluate WER and F1 scores across varying catalog sizes (100, 1K, 5K, 10K, 20K) on the VoxPopuli dataset