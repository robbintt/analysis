---
ver: rpa2
title: 'Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal
  Pairs for Audiovisual Representation Learning'
arxiv_id: '2304.05600'
source_url: https://arxiv.org/abs/2304.05600
tags:
- speech
- tasks
- audio
- learning
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how to improve audiovisual representation learning
  by addressing the "looking similar, sounding different" problem - where videos may
  share visual elements but have different speech content. The authors propose using
  dubbed movies as counterfactual data to augment contrastive training.
---

# Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning

## Quick Facts
- arXiv ID: 2304.05600
- Source URL: https://arxiv.org/abs/2304.05600
- Authors: 
- Reference count: 40
- Primary result: Using dubbed movies as counterfactual data improves audiovisual representation learning across diverse downstream tasks

## Executive Summary
This paper addresses the "looking similar, sounding different" problem in audiovisual representation learning by leveraging dubbed movies as a natural source of counterfactual data. The authors propose training models with both original and dubbed audio tracks (Spanish, French, Japanese) paired with the same video content, forcing the model to learn representations invariant to speech variation while preserving audiovisual correspondences. Using a dataset of 748 movies, they demonstrate that this approach improves performance across sound classification, scene classification, non-semantic speech tasks, and audiovisual classification, without significantly harming linguistic task performance.

## Method Summary
The method employs SimCLR-based cross-modal contrastive learning where video clips are paired with both original and dubbed audio tracks. The model learns to map multiple audio representations (differing only in speech content) to the same video representation, encouraging invariance to speech while maintaining audiovisual correspondences. Training uses 3-second video clips with 16 uniformly sampled frames (180×320 → 224×224) and mel-spectrograms (2 channels, 96 mel bins, 280 time steps). A shared audio encoder processes both primary and secondary audio tracks, with hard negatives sampled from the same movie (k=12 per GPU). The approach is evaluated against a speech-removal baseline and shows consistent improvements across diverse downstream tasks.

## Key Results
- Dub-augmented training improves performance on sound/event classification (FSD50K, VGGSound, AudioSet)
- The method benefits both multimodal and unimodal representations, including audio-only models
- Performance improvements are achieved without significantly affecting linguistic task performance
- Outperforms a strong baseline where speech is removed before training, particularly for paralinguistic and audiovisual tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using dubbed versions of movies provides counterfactual audio tracks that share visual content but differ in speech, which helps the model learn representations invariant to speech variation while preserving scene-level audiovisual correspondences.
- **Mechanism:** The model learns to map both original and dubbed audio to the same video representation, forcing it to focus on non-linguistic audiovisual features (like background sounds, actions, scene context) rather than speech content.
- **Core assumption:** Speech content is the primary source of variability between otherwise similar audiovisual scenes, and learning invariance to this variation improves generalization to downstream tasks that don't depend on speech.
- **Evidence anchors:**
  - [abstract]: "Our approach learns to represent alternate audio tracks, differing only in speech, similarly to the same video."
  - [section]: "We propose to leverage a data source which naturally resembles this counterfactual-like structure as a proxy: dubs."
- **Break condition:** If speech content is not the main source of variability between similar scenes (e.g., if different languages have very different prosody or if scenes differ significantly beyond just dialogue), or if downstream tasks require detailed speech understanding.

### Mechanism 2
- **Claim:** Cross-modal contrastive learning with multilingual dubs creates harder negative samples by including audio from the same movie but different languages, improving the quality of learned representations.
- **Mechanism:** By sampling negative examples from the same movie (hard negatives), the model must distinguish between audio tracks that share visual context but differ in language, leading to more discriminative audiovisual representations.
- **Core assumption:** Scenes from the same movie share significant visual and acoustic features beyond speech, making them useful as challenging negative examples.
- **Evidence anchors:**
  - [abstract]: "Our results, from a comprehensive set of experiments investigating different training strategies, show this general approach improves performance on a range of downstream auditory and audiovisual tasks"
  - [section]: "We incorporate a simple sampling strategy where k clips are sampled from the same movie, with k as a hyperparameter"
- **Break condition:** If hard negatives from the same movie don't provide meaningful challenge (e.g., if scenes are too diverse) or if the model overfits to movie-specific features rather than learning generalizable representations.

### Mechanism 3
- **Claim:** Dub-augmented training improves both multimodal and unimodal representations by forcing the model to learn robust audiovisual correspondences that generalize across languages.
- **Mechanism:** The shared audio encoder learns features that are useful for both the original and dubbed audio tracks, improving performance on audio-only tasks while maintaining or improving multimodal performance.
- **Core assumption:** Representations learned from coordinating audio and video across multiple languages transfer to unimodal tasks because they capture general audiovisual patterns rather than language-specific features.
- **Evidence anchors:**
  - [abstract]: "Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall."
  - [section]: "we examine two audio-only models... The objective function is now within-modal, between the two audio clips."
- **Break condition:** If the shared representations become too specialized to the multimodal training setup and don't transfer well to unimodal tasks, or if the benefits are only seen in multimodal contexts.

## Foundational Learning

- **Concept:** Cross-modal contrastive learning
  - **Why needed here:** This paper uses cross-modal contrastive learning as the foundation for training audiovisual representations, where the model learns to associate video with multiple audio tracks (original and dubbed).
  - **Quick check question:** What is the main objective of cross-modal contrastive learning, and how does it differ from within-modal contrastive learning?

- **Concept:** Data augmentation through linguistic variation
  - **Why needed here:** The paper leverages dubbed movies as a form of data augmentation that introduces linguistic variation while preserving visual content, helping the model learn speech-invariant representations.
  - **Quick check question:** How do dubbed movies provide a natural form of data augmentation for audiovisual learning?

- **Concept:** Hard negative sampling
  - **Why needed here:** The paper uses hard negative sampling by including audio clips from the same movie as negative examples, making the contrastive task more challenging and improving representation quality.
  - **Quick check question:** Why might negative samples from the same movie be more challenging than random negatives, and how does this improve learning?

## Architecture Onboarding

- **Component map:** Video encoder (X3D) → 1024-dim features, Audio encoder (Acoustic ResNet50) shared for primary and secondary audio → 1024-dim features, MLP projection heads for each modality → contrastive loss computation, Negative sampling module for hard negatives from same movie
- **Critical path:** Video clip → Video encoder → Video features → MLP projection → Contrastive loss, Primary audio → Shared Audio encoder → Primary audio features → MLP projection → Contrastive loss, Secondary audio → Shared Audio encoder → Secondary audio features → MLP projection → Contrastive loss
- **Design tradeoffs:** Using a shared audio encoder for both original and dubbed audio saves parameters and forces the model to learn language-invariant features, but may limit the model's ability to capture language-specific characteristics. The choice of 3-second clips balances temporal context with computational efficiency.
- **Failure signatures:** Poor performance on audiovisual tasks but good performance on audio-only tasks might indicate the video encoder isn't learning useful features. Good performance on speech tasks but poor performance on non-speech tasks might indicate the model is focusing too much on speech content rather than general audiovisual patterns.
- **First 3 experiments:**
  1. Train the monolingual baseline (SimCLRA V) and evaluate on a subset of downstream tasks to establish a performance baseline.
  2. Train a bilingual model with one dubbed language (e.g., +ES) and compare performance to the monolingual baseline to verify the dub-augmented training hypothesis.
  3. Train the source-separated baseline (SimCLRSEP) and compare its performance to both the monolingual and bilingual models to understand the impact of removing speech versus using dubbed audio.

## Open Questions the Paper Calls Out

1. **Question:** How do different language combinations in multilingual dub-augmented training affect downstream task performance compared to using just one or two secondary languages?
   - **Basis in paper:** [explicit] The authors note that multilingual (+EFJ) models perform similarly to bilingual ones, but don't explore other language combinations or more extensive multilingual setups
   - **Why unresolved:** The paper only tests Spanish, French, and Japanese as secondary languages, and only explores one multilingual combination. Different language families or larger sets of languages could yield different results.
   - **What evidence would resolve it:** Systematic experiments varying the number and types of secondary languages (e.g., Romance vs. non-Romance, Indo-European vs. non-Indo-European, more vs. fewer languages) and measuring downstream performance across tasks.

2. **Question:** Does the temporal alignment quality between dubbed audio and video affect the quality of learned representations?
   - **Basis in paper:** [inferred] The authors use pre-existing dubbed content without explicitly addressing synchronization issues, though they note that dubbed content may have subtle timing differences from the original
   - **Why unresolved:** The paper doesn't investigate whether minor desynchronization between dubbed audio and video (which can occur in dubbed content) impacts representation quality or whether explicit synchronization might improve results.
   - **What evidence would resolve it:** Experiments comparing models trained on perfectly synchronized vs. slightly misaligned dubbed content, or models that explicitly learn to handle synchronization differences.

3. **Question:** How does dub-augmented training perform when applied to non-movie content like TV shows, user-generated videos, or synthetic data?
   - **Basis in paper:** [explicit] The authors acknowledge their dataset is limited to movies and suggest TV shows, synthetic data, and content in other primary languages as potential extensions
   - **Why unresolved:** The paper only tests the approach on movies with English as the primary language, leaving open questions about generalizability to other content types and language distributions
   - **What evidence would resolve it:** Experiments applying the same training approach to TV shows, YouTube-style content, or synthetic data with dubbed audio, measuring performance across the same downstream tasks.

## Limitations
- Limited generalizability to non-movie content due to reliance on commercial streaming content (Netflix, Amazon Prime, Disney+)
- Evaluation focuses on Western-centric datasets, potentially limiting cross-cultural applicability
- Doesn't address potential domain shift between theatrical movies and other video types

## Confidence
**High Confidence**: Claims about improved performance on audiovisual tasks (sound/event classification, Audioset) are well-supported by quantitative results showing consistent improvements across multiple benchmarks (FSD50K, VGGSound, AudioSet).

**Medium Confidence**: Claims about maintaining linguistic task performance while improving paralinguistic tasks are partially supported, though the paper acknowledges this is an area for future work. The relative improvement over speech-removal baselines needs more ablation studies.

**Low Confidence**: Claims about the specific mechanism by which dubbed audio improves representations are largely speculative. The paper doesn't provide direct evidence that the model is learning to ignore speech content versus learning other complementary features.

## Next Checks
1. **Cross-domain generalization test**: Evaluate the trained models on user-generated video content (e.g., YouTube) to verify that improvements transfer beyond theatrical movies to more diverse video sources.

2. **Linguistic task ablation**: Conduct targeted experiments on speech recognition tasks using models trained with and without dubbed audio to quantify the precise impact on linguistic versus paralinguistic capabilities.

3. **Counterfactual mechanism probe**: Design experiments using synthetic counterfactuals (e.g., temporally aligned but semantically different audio) to isolate whether improvements stem specifically from language variation or general audiovisual diversity.