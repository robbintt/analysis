---
ver: rpa2
title: Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference
  via LLM-based Data Augmentation
arxiv_id: '2310.05318'
source_url: https://arxiv.org/abs/2310.05318
tags:
- data
- research
- discipline
- keywords
- proposal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses data imbalance in hierarchical multi-label
  classification for research proposals, which impacts the fairness of AI-assisted
  reviewer assignment systems. To solve this, the authors propose HiPSTG, a method
  that leverages large language models (LLM) for data augmentation.
---

# Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation

## Quick Facts
- arXiv ID: 2310.05318
- Source URL: https://arxiv.org/abs/2310.05318
- Reference count: 20
- Primary result: 8% increase in MicroF1 score when augmenting with 1000 LLM-generated research proposals for underrepresented disciplines

## Executive Summary
This study addresses data imbalance in hierarchical multi-label classification for research proposals, which impacts the fairness of AI-assisted reviewer assignment systems. To solve this, the authors propose HiPSTG, a method that leverages large language models (LLM) for data augmentation. HiPSTG identifies underrepresented disciplines within a hierarchical structure and uses LLM-generated prompts to create synthetic research proposals. Experiments demonstrate that augmenting the dataset with LLM-generated samples improves classification performance, particularly when generating 1000 synthetic samples, leading to an 8% increase in MicroF1 score. The approach effectively enhances the quality and diversity of the data, offering a solution for addressing class imbalance in specialized domains.

## Method Summary
HiPSTG combines structural-informed minority class sampling with LLM-based text generation to address data imbalance in hierarchical multi-label classification. The method first identifies underrepresented disciplines through frequency analysis, then constructs structured prompts incorporating hierarchical discipline information and keywords. Llama V1 generates synthetic research proposals based on these prompts, which are appended to the training dataset. A BERT-based classifier is then trained on the augmented data, improving performance on minority classes while maintaining overall classification accuracy.

## Key Results
- 8% increase in MicroF1 score when augmenting with 1000 synthetic samples
- 6.1% increase in MicroF1 score with 350 synthetic samples
- Improved classification performance for minority classes while maintaining overall accuracy

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated proposals can simulate underrepresented disciplinary classes and balance the hierarchical label distribution. HiPSTG identifies minority classes through sampling based on discipline frequencies, then constructs prompts incorporating both semantic and structural information from the hierarchical discipline structure. These prompts are fed to Llama V1 to generate synthetic research proposals that are appended to the training set. The core assumption is that LLMs can generate high-quality scientific text that preserves the domain characteristics and terminology of underrepresented disciplines when given appropriate prompts.

### Mechanism 2
Prompt engineering with structured discipline information improves generation quality for specialized scientific text. The prompt design includes Background, Principle, Format, and Language Style sections that constrain the LLM to generate text following specific structural patterns and terminology requirements for each discipline. Keywords and discipline hierarchy information are provided to guide content generation. The core assumption is that LLMs can effectively use hierarchical discipline structure information to generate contextually appropriate scientific proposals.

### Mechanism 3
Data augmentation with synthetic samples improves classification performance for imbalanced hierarchical multi-label classification. By generating 1000 synthetic samples for underrepresented classes, the model's training data becomes more balanced, leading to improved MicroF1, MacroF1, and Recall scores. The augmented dataset helps the model better learn minority class representations. The core assumption is that adding synthetic data for minority classes improves the model's ability to generalize across the hierarchical discipline structure.

## Foundational Learning

- **Hierarchical multi-label classification**
  - Why needed: Research proposals can belong to multiple disciplines at different levels of the hierarchy, requiring the model to predict multiple labels simultaneously while respecting the hierarchical relationships
  - Quick check: How does hierarchical multi-label classification differ from standard multi-label classification, and why is this distinction important for research proposal categorization?

- **Data imbalance in classification tasks**
  - Why needed: Certain disciplines have significantly more research proposals than others, creating class imbalance that can bias the classification model toward majority classes and hurt minority class performance
  - Quick check: What are the main challenges that data imbalance poses for machine learning models, particularly in the context of hierarchical classification?

- **Prompt engineering for LLM-controlled generation**
  - Why needed: The quality of generated research proposals depends heavily on how well the prompts guide the LLM to produce text that matches the domain-specific requirements and hierarchical structure
  - Quick check: What are the key components of effective prompt engineering when using LLMs for specialized text generation tasks?

## Architecture Onboarding

- **Component map**: Input dataset → Minority class sampling → Prompt construction → LLM generation → Data augmentation → Classification model → Performance evaluation
- **Critical path**: Minority class sampling → Prompt construction → LLM generation → Data augmentation → Model training → Performance evaluation
- **Design tradeoffs**: More synthetic samples improve balance but may introduce noise; more restrictive prompts ensure quality but reduce diversity; using Llama V1 provides good generation quality but may have cost and latency considerations
- **Failure signatures**: Classification performance doesn't improve after augmentation; generated proposals contain irrelevant or nonsensical content; model overfits to synthetic data patterns
- **First 3 experiments**:
  1. Run minority class sampling on the original dataset to verify the identification of underrepresented disciplines
  2. Generate a small batch of synthetic proposals (e.g., 50 samples) and manually evaluate their quality and relevance
  3. Train the classification model with different augmentation sizes (e.g., 0, 350, 1000 synthetic samples) and compare performance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of synthetic samples to generate for data augmentation in hierarchical multi-label classification?
- **Basis in paper**: [explicit] The paper states that augmenting the dataset with 1000 synthetic samples led to an 8% increase in MicroF1 score, while 350 samples resulted in a 6.1% increase. The authors note that the choice of the number of samples should consider the model's performance-efficiency trade-off.
- **Why unresolved**: The paper does not explore the relationship between the number of synthetic samples and model performance in depth, nor does it provide a definitive answer on the optimal number of samples.
- **What evidence would resolve it**: Further experiments exploring a wider range of synthetic sample sizes and their impact on model performance, as well as computational efficiency, would help determine the optimal number of synthetic samples for data augmentation.

### Open Question 2
- **Question**: How can the quality and diversity of generated proposals be further improved using advanced large language models and prompt engineering techniques?
- **Basis in paper**: [inferred] The paper mentions that the proposed method leverages Llama V1 as the backbone LLM and uses prompt engineering to generate synthetic proposals. It also notes that future work could explore additional LLMs and prompt engineering techniques to enhance the quality and diversity of generated proposals.
- **Why unresolved**: The paper does not delve into the specifics of how different LLMs or prompt engineering techniques could be utilized to improve the quality and diversity of generated proposals.
- **What evidence would resolve it**: Comparative studies evaluating the performance of various LLMs and prompt engineering techniques in generating high-quality and diverse proposals would provide insights into the most effective approaches for improving proposal generation.

### Open Question 3
- **Question**: How can the proposed method be extended to incorporate other metadata, such as author details, to better emulate real research proposals?
- **Basis in paper**: [inferred] The paper discusses the generation of synthetic research proposals based on keywords and hierarchical discipline structures. It also mentions that future work could expand the prompts to incorporate other metadata like author details to better emulate real proposals.
- **Why unresolved**: The paper does not provide details on how to incorporate additional metadata into the proposal generation process or how this might impact the quality and relevance of the generated proposals.
- **What evidence would resolve it**: Experiments incorporating various types of metadata into the proposal generation process and evaluating their impact on the quality, relevance, and diversity of the generated proposals would help determine the most effective ways to incorporate metadata for improved proposal generation.

## Limitations

- Data Quality Dependency: The effectiveness of HiPSTG heavily depends on the quality of LLM-generated proposals, which isn't systematically evaluated beyond classification metrics
- Prompt Design Sensitivity: The specific prompt structure may not generalize well to other domains or LLM models, with limited exploration of design variations
- Generalizability Across Domains: The method's effectiveness for other hierarchical multi-label classification tasks with different domain characteristics remains untested

## Confidence

- **High Confidence**: The core claim that data augmentation with LLM-generated samples can improve classification performance for imbalanced hierarchical multi-label classification
- **Medium Confidence**: The mechanism by which LLM-generated proposals address class imbalance
- **Low Confidence**: The generalizability of the prompt engineering approach to other domains and LLM models

## Next Checks

1. **Manual Quality Assessment**: Select a random sample of 100 generated proposals and have domain experts evaluate their quality, relevance, and adherence to discipline-specific terminology. Compare this with the original proposals to assess generation fidelity.

2. **Cross-Domain Replication**: Apply the HiPSTG method to a different hierarchical multi-label classification task (e.g., patent classification or product categorization) to test generalizability. Measure whether similar performance improvements occur with domain-adapted prompts.

3. **Prompt Ablation Study**: Systematically remove or modify each component of the prompt structure (Background, Principle, Format, Language Style) to identify which elements are most critical for generation quality and subsequent classification performance improvements.