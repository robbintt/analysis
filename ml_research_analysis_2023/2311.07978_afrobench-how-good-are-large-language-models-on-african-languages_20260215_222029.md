---
ver: rpa2
title: 'AfroBench: How Good are Large Language Models on African Languages?'
arxiv_id: '2311.07978'
source_url: https://arxiv.org/abs/2311.07978
tags:
- language
- languages
- african
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) perform poorly on African languages
  compared to high-resource languages like English, with significant gaps in performance
  across classification, question answering, and machine translation tasks. An evaluation
  of GPT-4, mT0, mT0-MT, and LLaMa 2 on 30 African languages across five NLP tasks
  shows that GPT-4 achieves over 80% of full-supervised fine-tuning performance on
  classification tasks but struggles with generative tasks like machine translation.
---

# AfroBench: How Good are Large Language Models on African Languages?

## Quick Facts
- arXiv ID: 2311.07978
- Source URL: https://arxiv.org/abs/2311.07978
- Reference count: 12
- Key outcome: Large language models perform poorly on African languages compared to high-resource languages like English, with significant gaps in performance across classification, question answering, and machine translation tasks.

## Executive Summary
This paper evaluates large language models (LLMs) on 30 African languages across five NLP tasks: news topic classification, sentiment classification, machine translation, question answering, and named entity recognition. The study compares GPT-4, mT0, mT0-MT, and LLaMa 2 against state-of-the-art supervised models. Results show that GPT-4 achieves over 80% of full-supervised fine-tuning performance on classification tasks but struggles with generative tasks like machine translation. Surprisingly, mT0 outperformed state-of-the-art supervised models on cross-lingual question answering, likely due to its multitask prompted training dataset (xP3) containing many QA datasets for African languages.

## Method Summary
The evaluation uses zero-shot cross-lingual prompting with English prompts on the AfroBench benchmark dataset containing 64 African languages. The study assesses four LLMs (GPT-4, mT0, mT0-MT, LLaMa 2) across five tasks using specific datasets: MASAKHA NEWS for news classification, AFRI SENTI for sentiment classification, MASAKHA NER for named entity recognition, AfriQA for question answering, and MAFAND-MT for machine translation. Performance is measured against fine-tuned baselines like AfroXLMR-large, mT5-base, and M2M-100 using accuracy, F1-score, and ChrF metrics.

## Key Results
- GPT-4 achieves over 80% of full-supervised fine-tuning performance on news topic and sentiment classification tasks
- mT0 outperforms state-of-the-art supervised models on cross-lingual question answering for African languages
- mT0-MT significantly improves machine translation performance compared to mT0 due to multilingual prompts
- All models show substantial performance gaps compared to high-resource languages, with LLaMa 2 performing worst due to its English-centric pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: mT0 achieves better cross-lingual QA performance than supervised models because it was trained on a multitask prompted dataset (xP3) that includes many QA datasets for African languages.
- Mechanism: The multitask prompted fine-tuning approach exposes the model to diverse QA formats and languages during training, enabling better generalization to unseen African languages in QA tasks.
- Core assumption: The xP3 dataset contains sufficient and high-quality QA data for African languages to enable this cross-lingual transfer.
- Evidence anchors:
  - [abstract]: "Surprisingly, we find that mT0 had the best overall on cross-lingual QA, better than the state-of-the-art supervised model (i.e. fine-tuned mT5) and GPT-4 on African languages."
  - [section]: "we hypothesize that this performance is probably due to the large number of QA datasets in xP3—used for creating the mT0 model."
  - [corpus]: Weak evidence - corpus neighbors mention "How Good are Commercial Large Language Models on African Languages?" but don't provide specific QA dataset details for mT0.

### Mechanism 2
- Claim: GPT-4 achieves more than 80% of full-supervised fine-tuning performance on classification tasks because its pre-training corpus includes substantial multilingual data, enabling better cross-lingual transfer for simpler classification tasks.
- Mechanism: Large language models with extensive multilingual pre-training can leverage patterns learned from high-resource languages to perform well on similar classification tasks in low-resource languages through zero-shot or few-shot prompting.
- Core assumption: The multilingual pre-training corpus includes sufficient data from African languages or related language families to enable effective cross-lingual transfer for classification.
- Evidence anchors:
  - [abstract]: "Our evaluation shows that GPT-4 achieve more than 80% of the performance of full-supervised fine-tuning on the news topic classification and sentiment classification"
  - [section]: "GPT-4 achieves more than 80% of SOTA's performance on classification tasks"
  - [corpus]: Weak evidence - corpus neighbors don't provide specific details about GPT-4's pre-training data composition.

### Mechanism 3
- Claim: Multilingual instruction tuning (mT0-MT) improves machine translation performance compared to English-only mT0 because it directly exposes the model to multilingual prompts during fine-tuning.
- Mechanism: By fine-tuning on prompts in multiple languages rather than just English, the model learns to better handle the translation task in a multilingual context, improving zero-shot translation capabilities.
- Core assumption: The multilingual prompts used in mT0-MT cover the translation directions needed for the evaluation and provide sufficient signal for learning effective translation patterns.
- Evidence anchors:
  - [abstract]: "Our evaluation shows that mT0-13B-MT significantly perform better than mT0-13B, the performance gap is wider for MT (∼ +10) than any other tasks we evaluated on (< 5.0)."
  - [section]: "The effective performance is due to the multilingual prompts used in developing the mT0-13B-MT instead of the English-only prompt as shown in Muennighoff et al. (2022)."
  - [corpus]: Weak evidence - corpus neighbors mention multilingual evaluation but don't provide specific details about mT0-MT's multilingual prompt training.

## Foundational Learning

- Concept: Multilingual pre-training and its impact on cross-lingual transfer
  - Why needed here: Understanding how exposure to multiple languages during pre-training enables models to perform tasks in languages they weren't explicitly trained on
  - Quick check question: What factors determine whether a model can effectively transfer knowledge from high-resource to low-resource languages in a given task?

- Concept: Zero-shot and few-shot learning in large language models
  - Why needed here: The evaluation relies on prompting approaches that don't require task-specific fine-tuning, making understanding of in-context learning crucial
  - Quick check question: How does providing examples in the prompt affect a model's ability to perform a new task?

- Concept: Task formulation and prompt engineering
  - Why needed here: Different tasks (classification, QA, translation) require different prompt structures, and the quality of prompt design significantly impacts performance
  - Quick check question: What are the key considerations when designing prompts for zero-shot learning across different NLP tasks?

## Architecture Onboarding

- Component map:
  Pre-trained multilingual language models (GPT-4, mT0, mT0-MT, LLaMa 2) -> Evaluation datasets (MasakhaNEWS, AfriSenti, MasakhaNER-X, AfriQA, MAFAND-MT) -> Prompt templates for different task types -> Performance metrics (accuracy, F1-score, ChrF score)

- Critical path:
  1. Load pre-trained model
  2. Format evaluation data according to prompt template
  3. Generate predictions using zero-shot/few-shot prompting
  4. Post-process predictions to match evaluation format
  5. Calculate performance metrics
  6. Compare against supervised baselines and high-resource language results

- Design tradeoffs:
  - Zero-shot prompting vs. fine-tuning: Faster evaluation but potentially lower performance
  - English-only prompts vs. multilingual prompts: Simpler implementation but may miss language-specific nuances
  - Task reformulation (NER as text generation) vs. token classification: Enables use of T5-style models but may affect performance

- Failure signatures:
  - Consistently poor performance across all African languages suggests model architecture or pre-training limitations
  - Large variance between language families may indicate dataset quality or language similarity issues
  - Better performance on classification than generative tasks suggests task complexity mismatch

- First 3 experiments:
  1. Run GPT-4 on English/French evaluation tasks to establish baseline performance on high-resource languages
  2. Test mT0 with English-only prompts on a subset of African languages across all task types to identify task-specific performance patterns
  3. Compare mT0 vs. mT0-MT on machine translation to validate the multilingual prompt tuning hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on African languages vary across different language families (Afro-Asiatic, Niger-Congo, Nilo-Saharan, and English Creole)?
- Basis in paper: [explicit] The paper evaluates LLMs on 30 African languages spanning four language families.
- Why unresolved: The paper provides overall performance metrics but does not delve into the performance variations across specific language families.
- What evidence would resolve it: Detailed performance metrics for each language family, possibly with a comparative analysis, would provide insights into how language families influence LLM performance.

### Open Question 2
- Question: What specific aspects of the pre-training corpus of LLaMa 2 contribute to its limited multilingual capabilities and poor performance on African languages?
- Basis in paper: [explicit] The paper mentions that LLaMa 2's worst performance is due to its English-centric pre-training corpus.
- Why unresolved: The paper does not provide details on the composition of the pre-training corpus or how it affects the model's performance on African languages.
- What evidence would resolve it: Analysis of the pre-training corpus composition and its correlation with performance on African languages would clarify the impact of corpus selection on LLM effectiveness.

### Open Question 3
- Question: How do different prompt templates and strategies (e.g., zero-shot cross-lingual prompting) influence the performance of LLMs on African languages?
- Basis in paper: [explicit] The paper discusses the use of English prompts for tasks and mentions the potential impact of prompt design on model performance.
- Why unresolved: The paper does not explore the effects of varying prompt templates or strategies on LLM performance.
- What evidence would resolve it: Experimental results comparing different prompt templates and strategies would reveal their impact on model performance and guide best practices for prompting in low-resource language settings.

## Limitations
- The evaluation relies on English prompts for zero-shot cross-lingual evaluation, which may not fully capture language-specific nuances and could artificially limit performance.
- The comparison with supervised baselines is limited to specific models (AfroXLMR-large, mT5-base, M2M-100), potentially missing other competitive approaches.
- The paper does not explore few-shot prompting, which could significantly improve performance for certain tasks and languages.

## Confidence
- High confidence: The overall finding that LLMs perform poorly on African languages compared to high-resource languages is well-supported by the results across multiple tasks and models.
- Medium confidence: The specific mechanism explaining mT0's superior QA performance (large number of QA datasets in xP3) is plausible but not directly verified in the paper.
- Low confidence: The explanation for mT0-MT's improvement in machine translation (multilingual prompts vs English-only) is stated but the evaluation doesn't test whether this specific factor drives the improvement.

## Next Checks
1. Systematically test multilingual prompts vs English prompts across all models and tasks to quantify the impact of prompt language choice on performance, particularly for classification and QA tasks where cross-lingual transfer might be more effective.
2. Analyze the xP3 training corpus composition to verify the claim about QA dataset coverage for African languages, and examine whether mT0's QA advantage correlates with specific African language families or language sizes in the training data.
3. Conduct a few-shot prompting study across all five tasks to determine whether the performance gaps identified in zero-shot evaluation can be substantially reduced, and to identify which tasks benefit most from in-context examples.