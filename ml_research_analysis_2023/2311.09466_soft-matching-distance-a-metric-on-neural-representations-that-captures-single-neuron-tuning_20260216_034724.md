---
ver: rpa2
title: 'Soft Matching Distance: A metric on neural representations that captures single-neuron
  tuning'
arxiv_id: '2311.09466'
source_url: https://arxiv.org/abs/2311.09466
tags:
- distance
- neural
- networks
- tuning
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the soft matching distance, a metric for
  comparing neural representations that captures single-neuron tuning while being
  insensitive to neuron index permutations. The metric generalizes the one-to-one
  matching distance to networks of different sizes by leveraging optimal transport
  theory to compute "soft" permutations between neurons.
---

# Soft Matching Distance: A metric on neural representations that captures single-neuron tuning

## Quick Facts
- arXiv ID: 2311.09466
- Source URL: https://arxiv.org/abs/2311.09466
- Authors: 
- Reference count: 40
- Key outcome: Introduces soft matching distance, a metric that captures single-neuron tuning while being insensitive to neuron index permutations

## Executive Summary
This paper introduces the soft matching distance, a novel metric for comparing neural representations that captures single-neuron tuning information while being invariant to neuron permutations. The metric leverages optimal transport theory to compute "soft" permutations between neurons, generalizing the one-to-one matching distance to networks of different sizes. Empirically, the soft matching distance reveals that individual neural tuning is preserved above chance levels in deep layers of both artificial and biological networks, supporting the "tuning matters" hypothesis over the "geometry is all you need" hypothesis.

## Method Summary
The soft matching distance compares two neural representation matrices by solving an optimal transport problem where each neuron in one network is assigned a "soft" match to neurons in the other network, weighted by their correlation. This assignment is constrained to form a doubly stochastic matrix, ensuring that each neuron contributes proportionally to the overall match. The resulting distance is interpreted as a Wasserstein distance between empirical distributions of neural activations and satisfies metric properties including symmetry and the triangle inequality.

## Key Results
- Soft matching distance captures single-neuron tuning information that rotation-invariant metrics miss
- Individual neural tuning is preserved above chance levels in deep layers of both artificial and biological networks
- The metric proves more effective than linear predictivity in distinguishing between neuroscientific hypotheses regarding visual processing streams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The soft matching distance captures single-neuron tuning information by finding an optimal soft permutation between neural representations that maximizes correlation.
- Mechanism: The metric solves an optimal transport problem where each neuron in one network is assigned a "soft" match to neurons in the other network, weighted by their correlation. This assignment is constrained to form a doubly stochastic matrix, ensuring that each neuron contributes proportionally to the overall match.
- Core assumption: Neural tuning curves contain meaningful information that should be preserved when comparing representations, and this information is encoded in the correlation structure between neurons.
- Evidence anchors:
  - [abstract] "leverage a connection to optimal transport theory to derive a natural generalization based on 'soft' permutations"
  - [section] "the soft matching correlation score between two networks: sT (X, Y ) = max P ∈T (Nx,Ny) Σ i,j P ijx⊤ i yj"

### Mechanism 2
- Claim: The soft matching distance generalizes the one-to-one matching distance to networks of different sizes while preserving metric properties.
- Mechanism: By relaxing the permutation matrix constraint to a transportation polytope constraint, the metric can handle networks with different numbers of neurons. The transportation polytope naturally extends the Birkhoff polytope, maintaining the triangle inequality and symmetry properties.
- Core assumption: The optimal transport framework provides a natural mathematical generalization that preserves the desirable properties of the original metric.
- Evidence anchors:
  - [section] "we now seek a similar generalization of the one-to-one matching distance. A natural way to do this is to modify the constraints of the minimization in eq. (10)"
  - [section] "the transportation and Birkhoff polytopes are essentially equivalent when N = Nx = Ny, except for a minor re-scaling factor"

### Mechanism 3
- Claim: The soft matching distance distinguishes between neuroscientific hypotheses by being sensitive to rotation while remaining invariant to neuron permutations.
- Mechanism: Unlike rotation-invariant metrics like CKA and Procrustes distance, the soft matching distance changes when the basis of neural representations is rotated, allowing it to detect whether individual neurons have converged to similar tuning across networks.
- Core assumption: Different visual processing streams (ventral, dorsal, lateral) have distinct computational functions that should be reflected in their neural tuning patterns, not just their representational geometry.
- Evidence anchors:
  - [abstract] "our proposed metric avoids counter-intuitive outcomes suffered by alternative approaches, and captures complementary geometric insights into neural representations"
  - [section] "the soft-matching metrics proved effective in adjudicating between models and revealed significant differences (i) among CNNs vs. transformer architectures in terms of their similarity to brain representations"

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Provides the mathematical framework for defining the soft matching distance as a Wasserstein distance between empirical distributions of neural activations.
  - Quick check question: How does the transportation polytope T(Nx, Ny) generalize the Birkhoff polytope B(N)?

- Concept: Metric Space Properties
  - Why needed here: Ensures the soft matching distance satisfies symmetry, triangle inequality, and identity of indiscernibles, making it a valid distance measure.
  - Quick check question: Why does relaxing permutation matrices to doubly stochastic matrices preserve the metric properties?

- Concept: Neural Representation Geometry
  - Why needed here: Understanding how individual neuron tuning curves map to population-level geometry is crucial for interpreting what the soft matching distance captures.
  - Quick check question: What is the relationship between individual neuron tuning curves and the overall geometry of neural representations?

## Architecture Onboarding

- Component map: Data preprocessing -> Core computation -> Distance calculation -> Correlation scoring
- Critical path:
  1. Preprocess neural representations (mean-center, normalize)
  2. Set up the optimal transport problem with cost matrix based on squared Euclidean distances
  3. Solve the optimal transport problem using a linear programming solver
  4. Compute the soft matching distance from the optimal transport plan
  5. (Optional) Compute the soft matching correlation score for interpretability
- Design tradeoffs:
  - Computational cost vs. accuracy: Exact solvers are expensive; approximate methods may be faster
  - Metric properties vs. flexibility: Relaxing constraints enables handling different network sizes but may lose some properties
  - Interpretability vs. information content: Correlation scores are easier to interpret but may lose some geometric information
- Failure signatures:
  - Extremely high computational time for large networks
  - Numerical instability in the optimal transport solver
  - Distance values that don't align with intuitive similarity judgments
  - Sensitivity to preprocessing choices (centering, normalization)
- First 3 experiments:
  1. Compare two networks with identical architectures but different random seeds on a small dataset
  2. Apply random rotations to one network's activations and observe distance changes
  3. Compare networks of different sizes (e.g., ResNet18 vs ResNet50) on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the soft matching distance perform on comparing neural representations across different modalities (e.g. vision vs. language)?
- Basis in paper: [inferred] The paper demonstrates applications of the soft matching distance to compare artificial and biological neural networks, but focuses on visual processing streams. It would be valuable to know if this metric generalizes to other modalities.
- Why unresolved: The paper does not provide empirical evidence or theoretical justification for the soft matching distance's applicability to non-visual neural representations. The metric's sensitivity to individual neural tuning may or may not be relevant for other types of neural data.
- What evidence would resolve it: Empirical studies applying the soft matching distance to compare neural representations from different modalities (e.g. vision vs. language) would demonstrate its generalizability. If the metric performs well across modalities, it would suggest that individual neural tuning is a fundamental property of neural representations that transcends specific sensory domains.

### Open Question 2
- Question: How does the soft matching distance relate to other measures of disentangled representation learning?
- Basis in paper: [inferred] The paper mentions that the soft matching distance may be useful for quantifying disentangled representation learning due to its sensitivity to representational basis. However, it does not provide a direct comparison with existing DRL metrics.
- Why unresolved: The relationship between the soft matching distance and other DRL metrics is unclear. The soft matching distance captures both explicitness and modularity/compactness of representations, but it is unknown how it compares to other metrics that focus on these aspects individually or in combination.
- What evidence would resolve it: Empirical studies comparing the soft matching distance to other DRL metrics on benchmark datasets would elucidate their relative strengths and weaknesses. If the soft matching distance performs comparably or better than existing metrics, it would suggest that it is a valuable addition to the DRL evaluation toolbox.

### Open Question 3
- Question: How does the soft matching distance scale with the size of the neural networks being compared?
- Basis in paper: [explicit] The paper states that the soft matching distance can be computed using the network simplex algorithm with a complexity of O(n^3 log n), where n is the number of units in the neural representations. However, it does not provide empirical evidence on how the metric performs as the network size increases.
- Why unresolved: While the computational complexity of the soft matching distance is known, it is unclear how well the metric scales in practice. As neural networks become larger and more complex, the soft matching distance may become computationally intractable or may lose its ability to meaningfully compare representations.
- What evidence would resolve it: Empirical studies applying the soft matching distance to compare neural representations from increasingly large networks would demonstrate its scalability. If the metric maintains its performance and computational efficiency as network size grows, it would suggest that it is a practical tool for comparing large-scale neural representations.

## Limitations
- Computational complexity of optimal transport problems may limit practical applicability to large networks
- Interpretation of "meaningful" neural tuning may vary across computational neuroscience contexts
- Empirical validation is limited to specific architectures and datasets

## Confidence
- Theoretical validity of metric properties: High
- Empirical findings regarding neural tuning preservation: Medium
- Ability to distinguish between neuroscientific hypotheses: Medium

## Next Checks
1. **Scale-up validation**: Test the soft matching distance on larger networks (e.g., ResNet101, ViT-Large) to assess computational feasibility and whether the metric maintains its properties and interpretability at scale.
2. **Cross-domain applicability**: Apply the metric to non-visual domains such as natural language processing or reinforcement learning to evaluate its broader utility in capturing single-neuron tuning across different types of neural representations.
3. **Alternative solver comparison**: Compare the results using exact optimal transport solvers versus approximate methods (e.g., Sinkhorn algorithm) to quantify the trade-off between computational efficiency and metric accuracy.