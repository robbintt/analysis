---
ver: rpa2
title: 'Striped Attention: Faster Ring Attention for Causal Transformers'
arxiv_id: '2311.09431'
source_url: https://arxiv.org/abs/2311.09431
tags:
- attention
- ring
- striped
- sequence
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Striped Attention is a modification of Ring Attention that addresses
  workload imbalance in causal transformers by distributing tokens uniformly across
  devices rather than in contiguous blocks. This ensures each device processes a balanced
  workload on every iteration, enabling better utilization of the structure of causal
  attention.
---

# Striped Attention: Faster Ring Attention for Causal Transformers

## Quick Facts
- arXiv ID: 2311.09431
- Source URL: https://arxiv.org/abs/2311.09431
- Reference count: 38
- Primary result: Up to 1.45× speedup for 256k sequences and 1.65× for 786k sequences

## Executive Summary
Striped Attention is a modification of Ring Attention that addresses workload imbalance in causal transformers by distributing tokens uniformly across devices rather than in contiguous blocks. This ensures each device processes a balanced workload on every iteration, enabling better utilization of the structure of causal attention. Experiments on A100 GPUs and TPUv4s show end-to-end throughput improvements of up to 1.45× for 256k sequence lengths and 1.65× for 786k sequences. The approach is exact and requires only a one-time permutation of input tokens, making it easy to implement and applicable to training and inference of generative language models.

## Method Summary
Striped Attention modifies Ring Attention by uniformly distributing tokens across devices through permutation, ensuring each device processes a balanced workload of masked and unmasked queries on every iteration. The approach exploits the permutation equivariance of attention computation to maintain exactness while achieving better computational overlap. The implementation requires permuting input tokens and position embeddings before the first layer, then maintaining consistent partitioning throughout forward and backward passes.

## Key Results
- End-to-end throughput improvements of up to 1.45× for 256k sequence lengths
- Speedups reaching 1.65× for 786k sequence lengths on A100 GPUs and TPUv4s
- Workload balance achieved across all devices on every iteration, eliminating idle waiting

## Why This Works (Mechanism)

### Mechanism 1
Striped Attention resolves workload imbalance by uniformly distributing tokens across devices. By permuting the input sequence so that each device owns tokens spaced evenly throughout the original sequence, every device processes approximately half masked and half unmasked query/key interactions on every iteration. This works because causal masking structure allows uniform distribution of masked/unmasked work across devices without affecting model output.

### Mechanism 2
Striped Attention enables better computational overlap by eliminating devices with entirely masked workloads. In Ring Attention, some devices have completely masked workloads on most iterations, forcing all devices to wait for the slowest one. Striped Attention ensures every device has mixed masked/unmasked work, enabling better pipeline parallelism and reducing synchronization overhead.

### Mechanism 3
Striped Attention maintains exactness through permutation equivariance of attention computation. Since attention computation is permutation equivariant, permuting input tokens before the first layer and maintaining consistent partitioning throughout forward/backward passes preserves model output. This mathematical property ensures output invariance under input permutation when position embeddings are appropriately handled.

## Foundational Learning

- Concept: Causal masking in attention
  - Why needed here: Understanding how causal masks create triangular structure in attention computation is fundamental to recognizing why workload imbalance occurs in Ring Attention.
  - Quick check question: In causal attention, which elements of the attention matrix are masked out and why?

- Concept: Ring Attention algorithm structure
  - Why needed here: To understand what Striped Attention modifies, one must first grasp how Ring Attention distributes attention computation across devices in a ring topology.
  - Quick check question: In Ring Attention, how do key/value blocks move between devices across iterations?

- Concept: Permutation equivariance in attention
  - Why needed here: This mathematical property is the foundation that allows Striped Attention to permute inputs without affecting outputs.
  - Quick check question: What property of the attention computation allows input permutation without changing the final output?

## Architecture Onboarding

- Component map: Input permutation module -> Partitioner -> Attention kernel -> Communication layer -> Position embedding handler
- Critical path: 1) Input sequence permutation, 2) Block partitioning and device assignment, 3) Attention computation with striped masks, 4) K/V block communication between iterations, 5) Output accumulation and normalization
- Design tradeoffs: Token contiguity vs. workload balance, permutation overhead vs. speedup, hardware memory access patterns
- Failure signatures: Speedup less than theoretical maximum, memory errors, gradient computation errors
- First 3 experiments: 1) Compare Ring vs Striped Attention with small sequence length (e.g., 8k) to verify correctness and measure permutation overhead, 2) Test with varying tile sizes to find optimal work-granularity for causal mask skipping, 3) Scale to larger sequence lengths (e.g., 64k, 256k) to measure speedup trends and identify scaling bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact speedup achievable by Striped Attention compared to Ring Attention when using fused kernels like FlashAttention for finer-grained work-skipping? The paper suggests that fused kernels could enable finer-grained work-skipping and potentially achieve greater speedups at smaller block sizes, but does not provide experimental results.

### Open Question 2
How do non-attention-computation overheads affect the speedup achievable by Striped Attention compared to Ring Attention? The paper mentions that these overheads may reduce the speedup achievable by Striped Attention, but does not provide a detailed analysis of their impact.

### Open Question 3
How does the speedup achieved by Striped Attention scale with increasing model size and sequence length beyond the tested configurations? The paper only tests up to 7B models and 786k sequences, leaving open the question of performance for even larger models or longer sequences.

## Limitations
- Speedup generalization across different hardware configurations and attention model variants remains uncertain
- Practical impact of position embedding permutation for complex schemes like RoPE not fully characterized
- Tile-based work-skipping strategy mentioned but not fully specified, creating uncertainty about optimal parameter choices

## Confidence

**High Confidence Claims:**
- Striped Attention's mechanism for resolving workload imbalance through uniform token distribution
- The mathematical validity of permutation equivariance in attention computation
- The general approach of permuting inputs and maintaining consistent partitioning throughout training/inference

**Medium Confidence Claims:**
- The magnitude of speedup improvements (1.45× and 1.65×) across different sequence lengths
- The practical effectiveness of mixed masked/unmasked workloads on target hardware
- The overhead of one-time permutation being negligible relative to attention computation time

**Low Confidence Claims:**
- Generalization of results to non-Transformer architectures using attention
- Performance on hardware architectures beyond A100 and TPUv4s
- Impact on models using complex position embedding schemes

## Next Checks

1. **Hardware Architecture Validation**: Implement Striped Attention on different accelerator types (e.g., H100 GPUs, Gaudi processors) to verify that the workload balance improvements translate across hardware families, particularly focusing on memory access patterns for non-contiguous token access.

2. **Position Embedding Scheme Stress Test**: Test Striped Attention with various position embedding implementations beyond RoPE, including absolute position embeddings and relative position schemes, to identify any edge cases where permutation equivariance breaks down or introduces accuracy degradation.

3. **Tile Size Sensitivity Analysis**: Conduct systematic experiments varying tile sizes across multiple orders of magnitude to identify the optimal work-granularity for causal mask skipping on each hardware platform, measuring both speedup and memory efficiency trade-offs.