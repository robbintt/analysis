---
ver: rpa2
title: 'Socratis: Are large multimodal models emotionally aware?'
arxiv_id: '2308.16741'
source_url: https://arxiv.org/abs/2308.16741
tags:
- reactions
- emotion
- image
- human
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Socratis, a benchmark for evaluating the
  emotional awareness of multimodal models. It contains 18K human-annotated reactions
  for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption
  datasets.
---

# Socratis: Are large multimodal models emotionally aware?

## Quick Facts
- arXiv ID: 2308.16741
- Source URL: https://arxiv.org/abs/2308.16741
- Reference count: 29
- Primary result: Human-written emotional reactions are preferred 2:1 over machine-generated ones, indicating current models lack emotional awareness

## Executive Summary
This paper introduces Socratis, a benchmark for evaluating emotional awareness in multimodal models. The task requires generating reasons for feeling specified emotions given image-caption pairs from news content. Human evaluations show that people prefer human-written emotional reactions over machine-generated ones by a factor of 2, while standard captioning metrics fail to correlate with human preferences. The benchmark highlights the gap between current multimodal models' capabilities and genuine emotional understanding, aiming to inspire research on emotionally aware AI systems.

## Method Summary
The Socratis benchmark consists of 2075 image-caption pairs from news datasets, annotated with 18K human-written reactions for 980 different emotions. The evaluation task uses BLIP-2 (FLAN-T5 variant) to generate reasons for specified emotions without fine-tuning, then compares outputs to human references using human preference studies and standard metrics (BART, CLIP, RefCLIP scores). The methodology focuses on whether models can generate coherent explanations for why content evokes specific emotions, rather than simply classifying emotions.

## Key Results
- Humans prefer human-written emotional reactions over machine-generated ones by a 2:1 margin
- Standard captioning metrics (BART, CLIP, RefCLIP) fail to correlate with human preferences for emotional reactions
- The benchmark reveals a significant gap between current multimodal models and genuine emotional understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-written emotional reactions are more nuanced than machine-generated ones because humans can incorporate contextual understanding beyond surface-level visual cues.
- Mechanism: Humans leverage rich semantic understanding and real-world knowledge to connect visual and textual elements with complex emotional states, while current multimodal models primarily match visual features to emotion keywords without deeper comprehension.
- Core assumption: Emotional understanding requires integration of visual perception, language comprehension, and contextual reasoning that current models have not fully captured.
- Evidence anchors:
  - [abstract]: "humans prefer human-written reasons over 2 times more often than machine-generated ones"
  - [section]: "This starkly contrasts recent findings where humans fare poorly to tell the difference between machine-generated and AI-generated news articles"
  - [corpus]: Weak evidence; related papers focus on emotion recognition but don't directly address generation quality
- Break condition: If models develop stronger contextual reasoning capabilities or if the task becomes simplified to basic emotion-to-keyword matching

### Mechanism 2
- Claim: Standard captioning metrics fail to capture emotional nuance because they focus on visual-linguistic alignment rather than emotional coherence.
- Mechanism: Metrics like BART and CLIP scores measure similarity to reference captions and visual relevance, but don't evaluate whether the generated explanation genuinely captures the emotional reasoning behind the reaction.
- Core assumption: Emotional awareness requires understanding why something evokes an emotion, not just that it does
- Evidence anchors:
  - [abstract]: "current captioning metrics based on large vision-language models also fail to correlate with human preferences"
  - [section]: "we observe a negligible difference in the scoring by commonly used metrics like BART and CLIP score"
  - [corpus]: No direct evidence in related papers about metric limitations for emotional generation
- Break condition: If new metrics are developed that explicitly measure emotional coherence or if models learn to game existing metrics through superficial emotional language

### Mechanism 3
- Claim: The benchmark's difficulty stems from requiring diverse emotional reactions to the same content, exposing model limitations in handling emotional ambiguity.
- Mechanism: Unlike emotion classification tasks that map content to discrete categories, this task requires generating multiple valid emotional interpretations and their justifications, revealing whether models can handle the full spectrum of human emotional responses.
- Core assumption: True emotional awareness involves recognizing that the same stimulus can legitimately evoke different emotions in different people
- Evidence anchors:
  - [abstract]: "each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them"
  - [section]: "The novelty of our dataset isn't the variety of emotion words, but the free-form reasons for feeling the different emotions for the same IC pair"
  - [corpus]: Related papers mention diverse emotional annotations but don't explore the generation of multiple reactions
- Break condition: If models are trained to generate only the most likely emotion or if the task is simplified to single-emotion prediction

## Foundational Learning

- Concept: Multimodal learning and cross-modal alignment
  - Why needed here: The task requires understanding relationships between visual content, textual context, and emotional states, which demands models that can effectively integrate information across modalities
  - Quick check question: How does cross-modal attention help a model understand why an image-caption pair might evoke a specific emotion?

- Concept: Natural language generation and reasoning
  - Why needed here: Generating plausible emotional reactions requires not just recognizing emotions but constructing coherent explanations that connect visual and textual elements to emotional responses
  - Quick check question: What distinguishes generating a reason for an emotion from simply stating that an emotion exists?

- Concept: Human emotional processing and social cognition
  - Why needed here: Understanding the gap between human and machine emotional generation requires knowledge of how humans actually process and articulate emotional responses to multimodal stimuli
  - Quick check question: How do humans typically justify their emotional reactions to news content, and what cognitive processes are involved?

## Architecture Onboarding

- Component map: Image and caption → Multimodal embedding → LLM generation → Output reaction → Human evaluation → Metric scoring
- Critical path: Image and caption → Multimodal embedding → LLM generation → Output reaction → Human evaluation → Metric scoring
- Design tradeoffs: Balancing model size and inference speed against generation quality, choosing between fine-tuning existing models versus training from scratch, and selecting appropriate evaluation metrics that capture emotional nuance
- Failure signatures: Poor generation quality (as indicated by human preference for human-written reactions), metrics that don't correlate with human judgment, and generations that focus on visual description rather than emotional reasoning
- First 3 experiments:
  1. Evaluate different prompting strategies (e.g., providing emotion examples, using few-shot learning) to see if generation quality improves
  2. Test whether fine-tuning the multimodal model on the Socratis dataset improves performance compared to zero-shot generation
  3. Develop and test custom evaluation metrics that explicitly measure emotional coherence rather than just linguistic or visual similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can small modifications to large multimodal models, such as prompt engineering, in-context learning, or fine-tuning, significantly improve their emotional awareness as measured by the Socratis benchmark?
- Basis in paper: explicit
- Why unresolved: The authors note that "Further investigation is also required to see if quick fixes like changing the generation strategy, adapting a few layers, or in-context learning (showing examples in prompts) can make these models more emotionally aware." They only tested a baseline BLIP-2 model without modifications.
- What evidence would resolve it: Running experiments with various prompt engineering techniques, in-context learning approaches, and fine-tuning strategies on the Socratis benchmark to compare performance against the baseline.

### Open Question 2
- Question: Can we develop evaluation metrics for emotional awareness that better correlate with human preferences than current captioning metrics like BART and CLIP scores?
- Basis in paper: explicit
- Why unresolved: The authors found that "commonly used captioning metrics like BART and CLIP score...fail to correlate with human preferences" for the emotional awareness task. They suggest that "We need better emotionally aware models to make better metrics."
- What evidence would resolve it: Developing new evaluation metrics specifically designed for emotional awareness and testing their correlation with human preferences on the Socratis benchmark.

### Open Question 3
- Question: Does incorporating additional contextual information beyond the image and caption pair, such as the broader article or source information, improve the emotional awareness of models on the Socratis benchmark?
- Basis in paper: inferred
- Why unresolved: The Socratis benchmark uses only image-caption pairs from news articles, but the authors mention that "Emotionally unaware messaging undermine efforts to inform people of global crises" and "Learning this diversity of reactions is crucial to tailor a machine's interactions to individual emotional states." They do not explore whether additional context could improve performance.
- What evidence would resolve it: Extending the Socratis benchmark to include additional contextual information and testing whether models using this extra information perform better on the task.

## Limitations

- The human evaluation methodology lacks detailed specifications on sample size and evaluator demographics
- Using BLIP-2 without fine-tuning may not represent the current state-of-the-art in multimodal models
- The paper doesn't address potential cultural and contextual variations in emotional expression
- The connection between generation quality and "emotional awareness" is assumed rather than rigorously proven

## Confidence

**High Confidence**: The core claim that current multimodal models struggle to generate emotionally nuanced reactions is well-supported by the human preference study (2:1 favoring human-written reactions) and the failure of standard captioning metrics to correlate with human preferences. The dataset construction methodology and task definition are clearly specified.

**Medium Confidence**: The assertion that standard captioning metrics fail to capture emotional nuance is supported by empirical evidence, but the paper doesn't explore alternative metrics or explain why this failure occurs at a technical level. The claim about the benchmark's difficulty due to requiring diverse emotional reactions is plausible but not rigorously tested.

**Low Confidence**: The paper's broader claims about what this benchmark reveals regarding the fundamental limitations of multimodal models' emotional understanding go beyond what the empirical results directly demonstrate. The connection between generation quality and "emotional awareness" is assumed rather than proven.

## Next Checks

1. **Reproduce the Human Evaluation**: Conduct an independent human study with larger sample sizes, demographic diversity, and controlled conditions to verify the 2:1 preference ratio and investigate what specific aspects of human-written reactions are preferred (emotional depth, writing quality, relevance, etc.).

2. **Test State-of-the-Art Models**: Evaluate more recent multimodal models (e.g., GPT-4V, Gemini, Claude) on the Socratis benchmark to determine if the observed gap between human and machine performance is fundamental or due to using an older model architecture.

3. **Develop and Validate New Metrics**: Design and test new evaluation metrics that explicitly measure emotional coherence and reasoning quality, then validate whether these correlate better with human preferences than standard captioning metrics. This could include metrics that assess causal connections between visual/textual elements and emotional responses.