---
ver: rpa2
title: Distilling Large Language Models for Matching Patients to Clinical Trials
arxiv_id: '2312.09958'
source_url: https://arxiv.org/abs/2312.09958
tags:
- criteria
- patient
- clinical
- performance
- trial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates proprietary (GPT-3.5, GPT-4)
  and open-source LLMs (LLAMA 7B, 13B, 70B) for patient-trial matching in clinical
  trials. We fine-tuned LLAMA models on synthetic data generated by GPT-4 to enhance
  their adaptability.
---

# Distilling Large Language Models for Matching Patients to Clinical Trials

## Quick Facts
- arXiv ID: 2312.09958
- Source URL: https://arxiv.org/abs/2312.09958
- Reference count: 40
- Key outcome: Fine-tuned open-source LLMs (Trial-LLAMA 70B) achieve performance parity with proprietary models (GPT-4) for patient-trial matching

## Executive Summary
This study systematically evaluates proprietary (GPT-3.5, GPT-4) and open-source LLMs (LLAMA 7B, 13B, 70B) for patient-trial matching in clinical trials. The researchers fine-tuned LLAMA models on synthetic data generated by GPT-4 to enhance their adaptability. Through comprehensive automated and human-centric assessments, they found that fine-tuned open-source LLMs, specifically Trial-LLAMA 70B, achieved performance parity with proprietary models. The results demonstrate that open-source LLMs, when properly aligned and fine-tuned, can effectively match patients to clinical trials, offering a viable, cost-effective, and privacy-conscious alternative to proprietary models.

## Method Summary
The researchers fine-tuned LLAMA models (7B, 13B, 70B) on synthetic data generated by GPT-4 to create the Trial-LLAMA model family. They used a multifaceted evaluation framework including automated metrics (NDCG@10, Precision@10, AUROC) and human-centric assessments. The evaluation involved comparing raw LLAMA performance against GPT-3.5, then assessing the fine-tuned Trial-LLAMA models against both proprietary models and each other using head-to-head comparisons on individual criteria. The approach leveraged GPT-4's capabilities to generate structured outputs for patient-trial pairs, which served as training targets for the open-source models.

## Key Results
- Fine-tuned Trial-LLAMA 70B achieved performance parity with GPT-4 in head-to-head criterion-level comparisons
- LLAMA-13B outperformed its larger counterpart LLAMA-70B in several metrics, demonstrating alignment importance over size
- Open-source models showed significant performance improvements after fine-tuning on synthetic data generated by GPT-4
- Head-to-head criterion-level analysis provided more reliable performance assessment than aggregate ranking metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning open-source LLMs on synthetic data generated by GPT-4 enables them to match proprietary model performance.
- Mechanism: GPT-4 acts as a knowledge distiller, generating structured outputs for patient-trial pairs that open-source models learn from.
- Core assumption: GPT-4's outputs are sufficiently accurate and representative of real-world eligibility decisions.
- Evidence anchors: The paper conducted extensive automated and human-centric assessments with detailed error analysis.
- Break condition: If GPT-4's synthetic outputs contain systematic errors or biases, these will propagate to fine-tuned models.

### Mechanism 2
- Claim: Proper alignment of LLMs to the specific task of patient-trial matching is more critical than model size alone.
- Mechanism: Task-specific alignment through instruction-tuning and fine-tuning enables smaller models to outperform larger but less aligned counterparts.
- Core assumption: Alignment captures task-specific nuances better than pretraining alone, regardless of parameter count.
- Evidence anchors: LLAMA-13B demonstrated superior performance over LLAMA-70B, contrary to expectations based solely on model size.
- Break condition: If task alignment cannot capture the full complexity of clinical reasoning, even aligned smaller models will underperform.

### Mechanism 3
- Claim: The head-to-head comparison on individual criteria provides more reliable performance assessment than aggregate ranking metrics.
- Mechanism: Aggregate metrics like NDCG@10 depend on consistent criterion selection across models.
- Core assumption: Criterion-level analysis removes confounding factors from selection bias in trial ranking.
- Evidence anchors: Trial-LLAMA 70B held a significant edge over GPT-3.5 in head-to-head comparisons.
- Break condition: If models consistently select the same criteria for analysis, head-to-head comparisons offer no additional insight.

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: Clinical trial eligibility requires step-by-step reasoning through complex medical criteria that cannot be resolved through simple pattern matching.
  - Quick check question: Can you explain how CoT helps a model distinguish between "included" and "not included" for a criterion requiring implicit inference from patient history?

- Concept: Structured output generation with JSON schemas
  - Why needed here: Clinical trial matching requires consistent, parseable outputs for downstream ranking algorithms.
  - Quick check question: What are the three components required in each criterion-level output, and why is each necessary?

- Concept: Synthetic data generation for model fine-tuning
  - Why needed here: Manual annotation of clinical trial matching is expensive and time-consuming.
  - Quick check question: How does synthetic data generation via GPT-4 address the cold-start problem in fine-tuning open-source models for specialized tasks?

## Architecture Onboarding

- Component map: Patient records → LLM (GPT-4 or open-source) → Criterion-level JSON outputs → Aggregation (inclusion/exclusion scores) → Trial ranking
- Critical path: Patient record input → LLM generation → JSON validation → Score calculation → Final ranking
- Design tradeoffs: Fine-tuning vs. prompt engineering - fine-tuning provides better performance but requires computational resources; prompt engineering is cheaper but less effective
- Failure signatures: Inconsistent JSON output format, low overlap in criteria selected across models, poor performance on implicit criteria despite good explicit criteria accuracy
- First 3 experiments:
  1. Compare raw LLAMA 70B output against GPT-3.5 on same patient-trial pairs to establish baseline performance gap
  2. Fine-tune LLAMA 70B on synthetic dataset and measure improvement on criterion-level accuracy
  3. Conduct head-to-head comparison on same criteria between fine-tuned model and GPT-4 to validate relative performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Trial-LLAMA 70B compare to proprietary models like GPT-4 in terms of criterion-level accuracy and evidence faithfulness?
- Basis in paper: The paper mentions that Trial-LLAMA 70B demonstrates superior performance compared to open-source alternatives and competes favorably with GPT-3.5, but it does not provide a direct comparison with GPT-4.
- Why unresolved: The paper does not provide a detailed comparison of Trial-LLAMA 70B's performance with GPT-4 in terms of criterion-level accuracy and evidence faithfulness.
- What evidence would resolve it: A direct comparison of Trial-LLAMA 70B's criterion-level accuracy and evidence faithfulness with GPT-4 would resolve this question.

### Open Question 2
- Question: How does the fine-tuning process affect the performance of different open-source LLMs for patient-trial matching?
- Basis in paper: The paper mentions that fine-tuning open-source LLMs on a specialized synthetic dataset enhances their adaptability and leads to performance parity with proprietary models.
- Why unresolved: The paper does not provide a detailed analysis of how the fine-tuning process specifically impacts the performance of different open-source LLMs.
- What evidence would resolve it: A detailed analysis of the impact of fine-tuning on the performance of different open-source LLMs, including metrics like accuracy, evidence faithfulness, and ranking ability, would resolve this question.

### Open Question 3
- Question: What are the potential limitations of using synthetic data generated by GPT-4 for fine-tuning open-source LLMs?
- Basis in paper: The paper mentions that a specialized synthetic dataset generated by GPT-4 is used for fine-tuning open-source LLMs, but it does not discuss the potential limitations of this approach.
- Why unresolved: The paper does not explore the potential limitations or biases that may arise from using synthetic data generated by GPT-4 for fine-tuning open-source LLMs.
- What evidence would resolve it: A discussion of the potential limitations and biases associated with using synthetic data generated by GPT-4 for fine-tuning open-source LLMs would resolve this question.

## Limitations

- The synthetic data generation process relies entirely on GPT-4 outputs, introducing potential model bias and limiting transparency
- Evaluation datasets may not fully capture the complexity and variability of real patient populations across different disease types and demographic groups
- The paper does not provide exact prompt templates or intermediate outputs for the synthetic data generation process

## Confidence

**High Confidence Claims:**
- Open-source LLMs can achieve competitive performance with proprietary models when properly fine-tuned
- Fine-tuning provides measurable performance improvements over baseline open-source models
- Head-to-head criterion-level comparisons provide more reliable performance assessment than aggregate metrics

**Medium Confidence Claims:**
- Synthetic data generation via GPT-4 is an effective approach for training open-source models in data-constrained environments
- Task-specific alignment is more important than model size for this particular application
- Trial-LLAMA 70B achieves performance parity with GPT-4

**Low Confidence Claims:**
- The cost-effectiveness and privacy benefits of open-source models outweigh their implementation complexity
- The findings generalize across all clinical trial matching scenarios and disease areas
- The released Trial-LLAMA model will maintain performance without ongoing fine-tuning on domain-specific data

## Next Checks

1. **Real-world validation study**: Test Trial-LLAMA on prospectively collected patient-trial pairs from multiple clinical centers to assess generalization beyond the evaluation datasets.

2. **Bias and fairness audit**: Conduct systematic analysis of model performance across different demographic groups, disease types, and clinical trial phases to identify potential systematic biases in synthetic data generation or model reasoning.

3. **Ablation study on synthetic data**: Compare model performance when fine-tuned on different proportions of synthetic vs. human-annotated data to quantify the impact of GPT-4-generated training data on final model quality.