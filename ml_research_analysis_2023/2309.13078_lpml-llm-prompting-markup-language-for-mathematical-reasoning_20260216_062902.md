---
ver: rpa2
title: 'LPML: LLM-Prompting Markup Language for Mathematical Reasoning'
arxiv_id: '2309.13078'
source_url: https://arxiv.org/abs/2309.13078
tags:
- python
- llms
- reasoning
- frac
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework that integrates Chain-of-Thought
  (CoT) reasoning with external tools like Python REPL to enhance the mathematical
  reasoning capability of large language models (LLMs). The key idea is to have LLMs
  generate structured text in an XML-like markup language, allowing seamless integration
  of CoT and external tool execution while controlling undesired LLM behaviors.
---

# LPML: LLM-Prompting Markup Language for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2309.13078
- Source URL: https://arxiv.org/abs/2309.13078
- Reference count: 18
- LPML achieves 60% accuracy on MATH dataset, outperforming CoT (31.7%) and PAL (47.5%) baselines

## Executive Summary
This paper introduces LPML (LLM-Prompting Markup Language), a novel framework that combines Chain-of-Thought reasoning with Python REPL execution through structured XML-like markup. The key innovation is enabling LLMs to generate structured text that can be parsed, validated, and executed while controlling undesired behaviors. LPML tags (THINK, PYTHON, OUTPUT, ANSWER) segment text by function, allowing mechanical removal of invalid elements and defining relationships between reasoning paths. Experiments on GSM8K and MATH datasets demonstrate significant performance improvements compared to using CoT or external tools alone.

## Method Summary
The method employs zero-shot prompting of ChatGPT (GPT-3.5-Turbo) with LPML syntax definitions to generate structured reasoning. The system parses the LLM output, removes invalid elements, executes Python code, and provides results back to the LLM, which iteratively refines its reasoning until producing an ANSWER tag. The approach combines CoT reasoning with external tool execution while using trust instructions to prioritize Python computation results over potentially erroneous CoT reasoning.

## Key Results
- LPML achieves 60% accuracy on MATH dataset compared to 31.7% for CoT and 47.5% for PAL baseline
- GSM8K results show improved reasoning capability through LPML integration
- The framework successfully controls undesired LLM behaviors like dummy execution results

## Why This Works (Mechanism)

### Mechanism 1
Structured markup language enables controlled integration of CoT and external tool execution through LPML tags that segment text by function, allowing mechanical removal of invalid elements and defining relationships between reasoning paths.

### Mechanism 2
Trust instruction enables LLMs to prioritize Python execution results over CoT reasoning by creating a hierarchical reasoning preference through system instructions about trusting OUTPUT tags.

### Mechanism 3
Zero-shot prompting with markup definitions enables complex reasoning without training by relying on LLMs' pre-existing XML/HTML knowledge from web data.

## Foundational Learning

- **XML/HTML parsing and generation**: LPML is XML-like syntax requiring understanding of tag structures, nesting, and content separation
  - Quick check: Can you write a valid XML document with nested tags and explain how a parser would handle malformed tags?

- **Python REPL integration patterns**: System must execute PYTHON tags and return results via OUTPUT tags, requiring secure code execution and result formatting
  - Quick check: How would you safely execute arbitrary Python code from user input and capture both stdout and stderr?

- **Chain-of-Thought prompting mechanics**: THINK tags rely on CoT reasoning patterns, requiring understanding of how step-by-step reasoning improves LLM performance
  - Quick check: What is the key difference between standard prompting and Chain-of-Thought prompting in terms of reasoning quality?

## Architecture Onboarding

- **Component map**: System (message manager, code executor, parser) ↔ Assistant (LLM) with markup-based communication protocol
- **Critical path**: Problem → System prompt → Assistant response → Parse/clean → Execute Python → System response → Repeat until ANSWER
- **Design tradeoffs**: Zero-shot prompting vs. few-shot examples (flexibility vs. reliability), strict parsing vs. lenient handling (correctness vs. robustness)
- **Failure signatures**: Unparseable output, infinite loops, trust instruction violations, malformed Python code
- **First 3 experiments**:
  1. Test basic LPML generation with simple arithmetic problems to verify structured output capability
  2. Validate Python execution integration by checking that PYTHON tags produce corresponding OUTPUT tags
  3. Evaluate trust instruction effectiveness by creating scenarios with contradictory THINK vs. OUTPUT content

## Open Questions the Paper Calls Out

### Open Question 1
How can the LPML framework be extended to handle more complex mathematical reasoning tasks beyond arithmetic and algebra? The paper demonstrates effectiveness on GSM8K and MATH datasets but doesn't explore advanced mathematical domains like calculus or abstract algebra.

### Open Question 2
How can the reliability of the CoT reasoning be improved when it conflicts with Python computation results? The paper acknowledges LLMs may sometimes ignore correct Python results in favor of CoT reasoning and is exploring potential solutions.

### Open Question 3
How can the LPML framework be adapted to handle more diverse types of external tools beyond Python REPL? While demonstrated with Python, the framework could theoretically integrate with web search, databases, or APIs.

## Limitations
- Evaluation scope limited to two datasets (GSM8K and MATH) using only one LLM variant (GPT-3.5-Turbo)
- Zero-shot prompting assumption lacks empirical validation across different model variants
- Trust instruction mechanism effectiveness not rigorously tested with systematic ablation studies

## Confidence

**High Confidence**: Structured markup enables controlled integration of CoT and external tools is well-supported by experimental results.

**Medium Confidence**: Zero-shot prompting with LPML syntax is sufficient for reliable structured output generation has moderate support but needs more testing.

**Medium Confidence**: Performance improvements on GSM8K and MATH datasets are demonstrated, but confidence is limited by narrow evaluation scope.

**Low Confidence**: Trust instructions reliably make LLMs prioritize Python outputs over CoT reasoning lacks sufficient empirical validation.

## Next Checks

1. **Cross-model generalization test**: Evaluate LPML effectiveness across different LLM architectures (e.g., Claude, Llama, PaLM) to determine if the approach generalizes beyond GPT-3.5-Turbo.

2. **Trust instruction ablation study**: Systematically test the impact of trust instructions by comparing LPML with and without trust directives, and measuring LLM adherence to trust hierarchies across multiple problem types.

3. **Zero-shot vs. few-shot comparison**: Compare performance between zero-shot LPML prompting and few-shot variants with 5-10 examples to quantify the robustness tradeoff and identify optimal prompting strategies.