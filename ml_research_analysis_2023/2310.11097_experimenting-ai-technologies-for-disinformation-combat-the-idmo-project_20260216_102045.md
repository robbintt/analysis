---
ver: rpa2
title: 'Experimenting AI Technologies for Disinformation Combat: the IDMO Project'
arxiv_id: '2310.11097'
source_url: https://arxiv.org/abs/2310.11097
tags:
- dataset
- datasets
- text
- accuracy
- fever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This project developed novel datasets for testing AI technologies
  in fake news detection, including manually annotated resources for text-statement
  similarity, content treatment analysis, and fact-checking verdicts. Key technical
  contributions include an automatic model categorizing Pagella Politica verdicts
  with 84% accuracy using only 150 training samples, and a multilingual DistilBERT
  model achieving over 90% accuracy on the FEVER dataset for textual entailment recognition.
---

# Experimenting AI Technologies for Disinformation Combat: the IDMO Project

## Quick Facts
- arXiv ID: 2310.11097
- Source URL: https://arxiv.org/abs/2310.11097
- Reference count: 13
- Developed novel datasets and AI models for fake news detection with multilingual capabilities

## Executive Summary
The IDMO project developed and tested AI technologies for combating disinformation through multiple approaches including multilingual DistilBERT models for textual entailment recognition, automatic verdict categorization for fact-checking, and content treatment analysis. The project created several novel datasets for evaluating these technologies, including manually annotated resources for text-statement similarity, content treatment analysis, and fact-checking verdicts. Key technical achievements include achieving over 90% accuracy on the FEVER dataset for textual entailment tasks and developing an automatic model that categorizes Pagella Politica verdicts with 84% accuracy using only 150 training samples. The project also created the FAKE RADAR game to raise awareness about fake news through interactive quizzes.

## Method Summary
The project employed fine-tuning of pre-trained language models on custom datasets for specific disinformation detection tasks. For textual entailment recognition, multilingual DistilBERT models were trained on the FEVER-it dataset and evaluated across different folds. For verdict categorization, the Camoscio model (an Italian instruction-tuned LLaMA) was fine-tuned on PagellaPolitica2 dataset containing manually annotated fact-checking verdicts. GPT-4 was utilized for content treatment detection and text clarity evaluation. The project also explored real-time content retrieval systems based on LangChain technology and created interactive educational tools like the FAKE RADAR game for public awareness.

## Key Results
- Multilingual DistilBERT model achieved over 90% accuracy on FEVER dataset for textual entailment recognition
- Automatic verdict categorization model reached 84% accuracy with only 150 training samples
- FAKE RADAR game created as interactive educational tool for fake news awareness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual DistilBERT models can effectively recognize textual entailment for fake news detection across different languages.
- Mechanism: Fine-tuning DistilBERT on multilingual datasets like FEVER-it and FEVER-ml allows the model to learn language-agnostic patterns for determining if a hypothesis is supported, refuted, or lacks sufficient information based on a given text.
- Core assumption: The multilingual nature of DistilBERT allows it to transfer knowledge across languages, and the structure of the FEVER dataset is preserved when translated to Italian.
- Evidence anchors:
  - [abstract] "An automatic model aimed at recognizing textual entailment with exceptional accuracy on the challenging FEVER dataset [5] . In particular, the creation of a multilingual DistilBERT model has showcased accuracy rates exceeding 90%..."
  - [section] "Models trained on FEVER-it show high accuracy when evaluated on the same dataset, achieving accuracies ranging from 87.94% to 99.52% across different folds, with an average accuracy of 97.10% (s.t.d 4.58%). These performances are comparable to those achieved by the FEVER models on the FEVER dataset..."
- Break condition: If the model fails to generalize well to the target language or the translated dataset does not preserve the original structure, performance will degrade.

### Mechanism 2
- Claim: Automatic models can effectively categorize fact-checking verdicts to enable broader analysis beyond individual statements.
- Mechanism: Fine-tuning a pre-trained Italian language model like Camoscio on a dataset of manually annotated Pagella Politica verdicts allows the model to learn patterns for categorizing new verdicts based on their textual content.
- Core assumption: The textual content of fact-checking verdicts contains sufficient information to infer the underlying category, and the manually annotated dataset is representative of the space of possible verdicts.
- Evidence anchors:
  - [abstract] "An automatic model designed to categorize Pagella Politica verdicts, for example assigning categories such as 'Exaggeration' for verdicts like 'Politician X is exaggerating a bit', or 'True' for the verdict 'The president is correct'. The aim is to facilitate a more comprehensive analysis beyond individual verdicts..."
  - [section] "Remarkably, even with a relatively small dataset of 150 samples, it already achieved a substantial level of accuracy (84%) in categorizing journalist verdicts regarding political statements."
- Break condition: If the verdict categories are too nuanced or context-dependent to be inferred from text alone, or if the training dataset is too small or unrepresentative, the model will fail to generalize.

### Mechanism 3
- Claim: Large language models can be used to evaluate the clarity of text, such as fact-checking explanations.
- Mechanism: Prompting a large language model like GPT-4 with a fact-checking statement and explanation, and asking it to categorize the verdict, allows the model to assess whether the explanation provides sufficient information to support the verdict.
- Core assumption: Large language models have the ability to understand and reason about the relationship between a statement and its explanation, and can provide a valid assessment of the explanation's clarity.
- Evidence anchors:
  - [abstract] "A proposal to harness the capabilities of large language models in order to evaluate text clarity, especially for those composing debunking verdicts."
  - [section] "One of the most interesting applications of Large Language Models in this context could be the text clarity check... The purpose of this example is not to criticize the work of Pagella Politica negatively... but rather to illustrate how Large Language Models could be used as a clarity check for content..."
- Break condition: If the large language model fails to understand the context or nuances of the fact-checking statement and explanation, or if the explanation is too ambiguous or relies on external knowledge, the model's assessment may be unreliable.

## Foundational Learning

- Concept: Textual entailment and natural language inference
  - Why needed here: The project involves recognizing textual entailment to determine if a hypothesis is supported, refuted, or lacks sufficient information based on a given text, which is crucial for fake news detection.
  - Quick check question: Given the statement "The cat is on the mat" and the hypothesis "There is a cat on the mat", does the statement entail the hypothesis? (Answer: Yes)

- Concept: Fine-tuning pre-trained language models
  - Why needed here: The project involves fine-tuning pre-trained models like DistilBERT and Camoscio on custom datasets to adapt them for specific tasks like verdict categorization and textual entailment recognition.
  - Quick check question: What is the main advantage of fine-tuning a pre-trained language model compared to training a model from scratch? (Answer: Fine-tuning leverages the pre-trained model's existing knowledge and requires less data and computation)

- Concept: Dataset creation and annotation
  - Why needed here: The project involves creating and annotating custom datasets for training and evaluating the models, such as datasets for verdict categorization, textual entailment, and content treatment analysis.
  - Quick check question: What is the main challenge in creating a high-quality dataset for machine learning? (Answer: Ensuring the dataset is representative, unbiased, and accurately labeled)

## Architecture Onboarding

- Component map: Data collection -> Dataset annotation -> Model fine-tuning -> Evaluation -> Application integration
- Critical path: 1. Collect and preprocess data from sources like Pagella Politica and FEVER 2. Create and annotate custom datasets for specific tasks 3. Fine-tune pre-trained models on the custom datasets 4. Evaluate model performance on test sets and benchmark against baselines 5. Integrate models into applications like the FAKE RADAR game
- Design tradeoffs: Using pre-trained models vs. training from scratch (tradeoff between performance and resource requirements), Monolingual vs. multilingual models (tradeoff between performance on specific languages and generalization), Manual annotation vs. automatic labeling (tradeoff between label quality and scalability)
- Failure signatures: Low performance on test sets despite high performance on training sets (indication of overfitting), Model fails to generalize to new data or languages (indication of lack of robustness), Manual annotation process is too slow or expensive (indication of scalability issues)
- First 3 experiments: 1. Fine-tune DistilBERT on the FEVER-it dataset and evaluate its performance on the test set 2. Fine-tune Camoscio on a small sample of manually annotated Pagella Politica verdicts and assess its categorization accuracy 3. Prompt GPT-4 with sample fact-checking statements and explanations to evaluate its ability to assess text clarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of additional datasets beyond FEVER, PagellaPolitica1, and Ministry of Health impact the accuracy of models in recognizing textual entailment?
- Basis in paper: [inferred] The paper discusses the evaluation of DistilBERT on various datasets, including FEVER, PagellaPolitica1, and Ministry of Health. However, it does not explore the impact of incorporating other datasets on model performance.
- Why unresolved: The paper does not provide experimental results or analysis of how the inclusion of additional datasets affects the accuracy of models in recognizing textual entailment.
- What evidence would resolve it: Conducting experiments with models trained on a combination of FEVER, PagellaPolitica1, Ministry of Health, and other relevant datasets, and comparing their performance to models trained on individual datasets.

### Open Question 2
- Question: How does the performance of GPT-4 compare to fine-tuned models like Camoscio and DistilBERT on recognizing textual entailment tasks?
- Basis in paper: [explicit] The paper presents results comparing the performance of GPT-4, Camoscio, and DistilBERT on recognizing textual entailment tasks. It mentions that GPT-4 achieved promising results but does not provide a direct comparison to fine-tuned models.
- Why unresolved: The paper does not provide a detailed analysis or comparison of the performance of GPT-4 and fine-tuned models on recognizing textual entailment tasks.
- What evidence would resolve it: Conducting experiments to directly compare the performance of GPT-4 and fine-tuned models like Camoscio and DistilBERT on recognizing textual entailment tasks, using the same datasets and evaluation metrics.

### Open Question 3
- Question: How does the inclusion of real-time content retrieval systems, such as those based on LangChain technology, impact the efficiency and accuracy of fake news detection?
- Basis in paper: [inferred] The paper mentions the exploration of LangChain technology as a potential solution to enhance the retrieval system for fake news detection. However, it does not provide experimental results or analysis of the impact of such systems.
- Why unresolved: The paper does not provide experimental results or analysis of the impact of real-time content retrieval systems on the efficiency and accuracy of fake news detection.
- What evidence would resolve it: Conducting experiments to evaluate the performance of fake news detection systems with and without real-time content retrieval systems based on LangChain technology, using relevant datasets and evaluation metrics.

## Limitations
- Reliance on Italian-specific datasets limiting generalizability to other languages
- Absence of real-world deployment metrics showing how models perform on live disinformation detection
- Limited evaluation of content treatment detection which showed lower performance

## Confidence

- Textual entailment model performance: High
- Verdict categorization accuracy claims: Medium  
- Educational game impact: Low (no evaluation data provided)

## Next Checks

1. Replicate the verdict categorization experiment using the exact 150-sample training configuration and compare results against a held-out validation set
2. Test the multilingual DistilBERT model on cross-lingual transfer tasks to verify language-agnostic performance claims
3. Conduct a pilot deployment of the FAKE RADAR game with user engagement metrics to assess educational effectiveness