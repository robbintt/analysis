---
ver: rpa2
title: 'SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models'
arxiv_id: '2303.10464'
source_url: https://arxiv.org/abs/2303.10464
tags:
- sparsity
- sparse
- dense
- training
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sparse Pre-training and Dense Fine-tuning
  (SPDF), a framework to reduce pre-training FLOPs of large language models using
  unstructured weight sparsity. The authors sparsify dense GPT models during pre-training,
  allowing a subset of weights to be trained, and then recover representational capacity
  by enabling all weights during dense fine-tuning.
---

# SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models

## Quick Facts
- arXiv ID: 2303.10464
- Source URL: https://arxiv.org/abs/2303.10464
- Reference count: 40
- Primary result: GPT-3 XL can be pre-trained with 75% sparsity, reducing pre-training FLOPs by 2.5x without significant loss in downstream accuracy

## Executive Summary
This paper introduces Sparse Pre-training and Dense Fine-tuning (SPDF), a framework to reduce pre-training FLOPs of large language models using unstructured weight sparsity. The authors sparsify dense GPT models during pre-training, allowing a subset of weights to be trained, and then recover representational capacity by enabling all weights during dense fine-tuning. Experiments show that GPT-3 XL can be pre-trained with up to 75% sparsity, reducing pre-training FLOPs by 2.5x without significant loss in downstream accuracy on natural language generation and summarization tasks compared to dense baselines. The method is more effective for larger models and easier tasks.

## Method Summary
The SPDF framework applies unstructured sparsity to GPT models during pre-training, randomly pruning weights to achieve a target sparsity level. This reduces the computational cost of matrix multiplications during pre-training. After sparse pre-training, the model is fine-tuned densely on downstream tasks, allowing zeroed weights to grow and recover representational capacity. The authors evaluate SPDF on GPT-2 Small and GPT-3 XL using 50-75% sparsity during pre-training, followed by dense fine-tuning on E2E, WebNLG, DART, and Curation Corpus tasks.

## Key Results
- GPT-3 XL can be pre-trained with 75% sparsity, reducing pre-training FLOPs by 2.5x
- Dense fine-tuning outperforms sparse fine-tuning in recovering downstream performance
- Larger models (GPT-3 XL) are more amenable to higher sparsity levels than smaller models (GPT-2 Small)

## Why This Works (Mechanism)

### Mechanism 1
Unstructured sparsity during pre-training reduces FLOPs while preserving downstream accuracy. Randomly pruning weights lowers computational cost, and dense fine-tuning recovers representational capacity. Core assumption: Sparse optimization is challenging, but dense fine-tuning can compensate for representational loss.

### Mechanism 2
Sparse pre-training followed by dense fine-tuning is more effective than sparse pre-training followed by sparse fine-tuning. Dense fine-tuning allows full recovery of representational capacity, mitigating sparse optimization difficulties. Core assumption: Sparse fine-tuning cannot fully recover representational power lost during sparse pre-training.

### Mechanism 3
Larger models are more amenable to higher sparsity levels during pre-training. As models increase in size, the quality of randomly pruned sparse networks improves, approaching dense model performance. Core assumption: The gap in downstream performance between sparse pre-trained models and dense counterparts decreases as model size increases.

## Foundational Learning

- Concept: Lottery Ticket Hypothesis
  - Why needed here: Understanding the potential of sparse subnetworks to match dense model performance
  - Quick check question: Can sparse subnetworks be found that train to the same accuracy as their dense counterparts?

- Concept: Overparameterization
  - Why needed here: Recognizing benefits of overparameterization in improving optimization and generalizability, while acknowledging increased compute cost
  - Quick check question: How does overparameterization affect the performance and computational cost of neural networks?

- Concept: Scaling Laws
  - Why needed here: Understanding the relationship between model size, dataset size, and computational cost, as well as optimal resource allocation for training large language models
  - Quick check question: How do parameters and data need to be scaled to maximize the efficiency of training large language models?

## Architecture Onboarding

- Component map: Dense GPT model -> Apply unstructured sparsity -> Sparse pre-training -> Dense fine-tuning -> Evaluate downstream performance

- Critical path: 1) Initialize dense GPT model, 2) Apply uniform sparsity to linear layers, 3) Pre-train sparse model on large dataset, 4) Fine-tune dense model on downstream task, 5) Evaluate performance

- Design tradeoffs:
  - Higher sparsity reduces pre-training FLOPs but may impact downstream performance
  - Dense fine-tuning recovers representational capacity but increases fine-tuning FLOPs
  - Larger models are more amenable to sparsity but require more computational resources

- Failure signatures:
  - Significant drop in downstream task performance
  - Inability to recover representational capacity during dense fine-tuning
  - Inconsistent results across different sparsity levels or model sizes

- First 3 experiments:
  1. Pre-train GPT-2 Small with 50% sparsity and fine-tune on E2E dataset, compare BLEU score to dense baseline
  2. Pre-train GPT-3 XL with 75% sparsity and fine-tune on WebNLG dataset, compare BLEU score to dense baseline
  3. Vary sparsity levels (25%, 50%, 75%) for GPT-2 Small and GPT-3 XL, evaluate downstream performance on DART and Curation Corpus tasks

## Open Questions the Paper Calls Out

- How well does SPDF scale to even larger language models beyond GPT-3 XL (1.3B parameters)?
- What is the optimal sparsity schedule during pre-training for maximizing downstream task performance?
- How does SPDF perform on tasks outside of natural language generation and text summarization?

## Limitations

- Limited generalization beyond GPT architecture - uncertainty about effectiveness for encoder-decoder or encoder-only models
- Evaluation scope constraints - only four downstream tasks tested, limiting generalizability
- Hardware-specific training details - difficulty assessing performance on non-Cerebras systems

## Confidence

- High Confidence: 2.5x FLOP reduction with 75% sparsity, superiority of dense fine-tuning, larger models handle higher sparsity better
- Medium Confidence: Mechanism of representational capacity recovery, relationship between model size and effective sparsity, generalizability across NLP tasks
- Low Confidence: Applicability to non-GPT transformer architectures, performance on unrepresented task types, scalability to models larger than GPT-3 XL

## Next Checks

1. Apply SPDF framework to BERT-base model and evaluate whether similar FLOP reductions and downstream performance can be achieved on GLUE benchmark tasks

2. Implement SPDF on GPT-2 Small with 50% sparsity and fine-tune on a broader set of tasks including sentiment analysis (SST-2), question answering (SQuAD), and summarization (CNN/DailyMail)

3. Systematically vary model sizes (GPT-2 Small, Medium, Large) while keeping sparsity fixed at 50%, and measure the relationship between model size and performance retention