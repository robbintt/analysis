---
ver: rpa2
title: Can LLMs Capture Human Preferences?
arxiv_id: '2305.02531'
source_url: https://arxiv.org/abs/2305.02531
tags:
- language
- languages
- larger
- across
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether Large Language Models (LLMs) like GPT-3.5
  can capture human preferences, focusing on intertemporal choices. The authors present
  GPT with choices between smaller, sooner and larger, later rewards in multiple languages,
  comparing its responses to human behavior.
---

# Can LLMs Capture Human Preferences?

## Quick Facts
- arXiv ID: 2305.02531
- Source URL: https://arxiv.org/abs/2305.02531
- Reference count: 4
- Primary result: GPT-3.5 exhibits language-dependent intertemporal preferences that align with linguistic relativity effects but shows lexicographic rather than human-like discounting patterns

## Executive Summary
This paper investigates whether Large Language Models (LLMs) like GPT-3.5 can capture human intertemporal preferences across 22 languages. The authors find that while GPT exhibits systematic variation in delay preferences based on language structure—showing greater patience in languages with weak future tense references—its behavior differs fundamentally from human decision-making. Specifically, GPT displays lexicographic preferences that prioritize temporal ordering over reward magnitude, making it problematic for direct use in preference elicitation tasks. The study introduces "chain-of-thought conjoint" as a potential mitigation strategy, though limitations remain.

## Method Summary
The study uses GPT-3.5-turbo to elicit intertemporal preferences across 22 languages, presenting choices between smaller-sooner and larger-later rewards across 63 experimental conditions (9 delays × 7 interest rates). Each cell was queried 100 times with randomized option order and conversation history reset between samples. The authors employed chain-of-thought prompting and compared GPT's choices to established human preference patterns, analyzing language effects through statistical modeling.

## Key Results
- GPT shows greater patience in languages with weak future tense references (e.g., German, Mandarin) compared to strong future tense languages
- GPT exhibits lexicographic preferences, consistently prioritizing earlier rewards regardless of reward gap when delays are non-zero
- Chain-of-thought prompting can partially but not completely mitigate discrepancies between LLM and human responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT's responses vary systematically across languages due to its training data composition, not because it has internalized human-like intertemporal preferences.
- **Mechanism**: The model's token predictions are influenced by language-specific patterns in its training corpus. When prompted in different languages, GPT's output distribution shifts based on how those languages encode temporal concepts, leading to observable differences in delay preferences.
- **Core assumption**: The language-specific patterns in the training data correlate with the linguistic relativity effects observed in humans, even though GPT doesn't have genuine preferences.
- **Evidence anchors**:
  - [abstract]: "GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with existing literature that suggests a correlation between language structure and intertemporal preferences."
  - [section]: "Our findings reveal that GPT is less likely to select the larger later reward when presented with choices in languages with strong FTR, which is consistent with prior research on the influence of language structures on economic behavior and decision-making."
  - [corpus]: Weak - corpus doesn't provide direct evidence about GPT's training data composition or how language-specific patterns influence its predictions.
- **Break condition**: If the observed language differences disappear when controlling for prompt phrasing or when using controlled multilingual datasets with balanced representation.

### Mechanism 2
- **Claim**: GPT exhibits lexicographic preferences over time rather than genuine exponential or hyperbolic discounting.
- **Mechanism**: When faced with intertemporal choices, GPT's decision-making process prioritizes temporal ordering over reward magnitude differences, effectively ignoring reward gaps when delays are non-zero.
- **Core assumption**: GPT's decision process can be modeled as a lexicographic utility function where time dominates reward size in the preference ordering.
- **Evidence anchors**:
  - [abstract]: "GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike human decision-makers."
  - [section]: "Our findings reveal that the model's propensity to select the larger later option did not vary with the gap in rewards when the delay was larger than zero."
  - [corpus]: Weak - corpus doesn't provide direct evidence about GPT's internal decision-making mechanisms or how it processes temporal versus reward information.
- **Break condition**: If GPT's choices show systematic variation with reward gaps across all delay conditions, or if chain-of-thought prompting eliminates the lexicographic behavior.

### Mechanism 3
- **Claim**: Chain-of-thought prompting can partially mitigate GPT's preference elicitation limitations by forcing explicit reasoning.
- **Mechanism**: By prompting GPT to explain its reasoning before making a choice, the model engages different computational pathways that may produce more nuanced responses that better approximate human decision-making.
- **Core assumption**: The additional reasoning step in chain-of-thought prompting accesses latent knowledge or reasoning patterns in GPT that aren't activated in simple choice prompts.
- **Evidence anchors**:
  - [abstract]: "We demonstrate how prompting GPT to explain its decisions, a procedure we term 'chain-of-thought conjoint,' can mitigate, but does not eliminate, discrepancies between LLM and human responses."
  - [section]: "While GPT may capture intriguing variations across languages, our findings indicate that the choices made by these models do not correspond to those of human decision-makers."
  - [corpus]: Weak - corpus doesn't provide direct evidence about the effectiveness of chain-of-thought prompting for preference elicitation.
- **Break condition**: If chain-of-thought prompting consistently produces the same lexicographic patterns as direct choice prompts, or if the mitigation effect disappears with different prompt formulations.

## Foundational Learning

- **Concept**: Linguistic relativity and the Sapir-Whorf hypothesis
  - Why needed here: Understanding how language structure might influence temporal cognition is central to interpreting GPT's language-dependent behavior.
  - Quick check question: What is the key difference between strong and weak future tense reference languages, and how does this relate to intertemporal decision-making?

- **Concept**: Intertemporal choice models (exponential, hyperbolic, quasi-hyperbolic discounting)
  - Why needed here: The paper evaluates whether GPT's choices conform to established models of human intertemporal preferences.
  - Quick check question: What is the key testable prediction that distinguishes exponential discounting from hyperbolic discounting in intertemporal choice tasks?

- **Concept**: Lexicographic preferences and their implications for utility theory
  - Why needed here: The paper identifies GPT's lexicographic behavior as a key limitation, requiring understanding of what this means for preference elicitation.
  - Quick check question: What is the defining characteristic of lexicographic preferences that makes them problematic for standard utility elicitation methods?

## Architecture Onboarding

- **Component map**: Prompt engineering module -> GPT API integration -> Response parsing and classification -> Statistical analysis pipeline -> Visualization dashboard
- **Critical path**: Prompt → GPT API → Response parsing → Statistical analysis → Visualization. The prompt engineering and response classification steps are most critical for ensuring valid results.
- **Design tradeoffs**: Using GPT-3.5 (more accessible but less capable) versus GPT-4 (more capable but potentially more expensive); simple choice prompts versus chain-of-thought prompting (faster but less nuanced versus slower but potentially more accurate).
- **Failure signatures**: (1) Consistent lexicographic patterns across all conditions indicating GPT's fundamental limitations; (2) No language differences despite theoretical predictions; (3) Responses that don't follow any recognizable utility model; (4) GPT refusing to participate in surveys.
- **First 3 experiments**:
  1. Replicate the basic language comparison using a smaller subset of languages to verify the core finding before scaling up.
  2. Test GPT's behavior on simpler preference tasks (e.g., single-attribute choices) to establish baseline capability.
  3. Implement chain-of-thought prompting on a subset of conditions to evaluate whether it mitigates the lexicographic behavior before applying it broadly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the lexicographic preference behavior observed in GPT-3.5 be mitigated or eliminated through specific prompt engineering techniques or model fine-tuning?
- Basis in paper: [explicit] The paper mentions using "chain-of-thought conjoint" to mitigate but not eliminate discrepancies between LLM and human responses.
- Why unresolved: The study only tested one prompting method and did not explore other potential approaches to address the lexicographic preference issue.
- What evidence would resolve it: Testing multiple prompting strategies, fine-tuning approaches, or model architectures to see if they can produce more human-like intertemporal preferences.

### Open Question 2
- Question: Do other large language models (beyond GPT-3.5 and GPT-4) exhibit similar language-dependent intertemporal preferences, and how do their behaviors compare to human decision-making across different languages?
- Basis in paper: [inferred] The study focused only on OpenAI's GPT models, but the authors suggest exploring LLMs' perceptions across other dimensions like risk and uncertainty.
- Why unresolved: The research was limited to a specific set of models, and broader comparisons across different LLMs were not conducted.
- What evidence would resolve it: Testing a diverse range of LLMs (e.g., Claude, LLaMA, PaLM) on the same intertemporal choice tasks across multiple languages and comparing their behavior to human data.

### Open Question 3
- Question: What specific linguistic features or cultural factors beyond future tense reference strength explain the observed heterogeneity in GPT's intertemporal preferences across languages?
- Basis in paper: [explicit] The study found that GPT showed greater patience in languages with weak FTR, but further analysis revealed lexicographic preferences that complicate direct interpretation.
- Why unresolved: While FTR was examined, the paper suggests there may be other linguistic or cultural factors at play that were not investigated.
- What evidence would resolve it: Conducting controlled experiments manipulating specific linguistic features (e.g., time-related vocabulary, metaphors) and cultural contexts to isolate their effects on LLM preferences.

## Limitations

- GPT's lexicographic preference pattern fundamentally differs from human intertemporal choice behavior, limiting its utility for preference elicitation
- The effectiveness of chain-of-thought prompting is only partially demonstrated and requires further validation across different prompt formulations
- The study's findings may be influenced by training data composition rather than genuine language effects on temporal cognition

## Confidence

- **High confidence**: GPT shows systematic language-dependent variation in intertemporal choices, with stronger future tense references correlating with less patient behavior
- **Medium confidence**: GPT exhibits lexicographic preferences rather than standard discounting patterns
- **Low confidence**: Chain-of-thought prompting substantially improves GPT's preference elicitation capability

## Next Checks

1. **Cross-model validation**: Replicate the language comparison study using GPT-4 and Claude to determine whether the observed patterns are specific to GPT-3.5's architecture or represent broader LLM behavior.

2. **Prompt engineering ablation**: Systematically test different chain-of-thought prompt formulations, including variations in reasoning requirements, time horizons, and reward framing, to identify which elements most effectively mitigate lexicographic behavior.

3. **Controlled training data analysis**: Analyze whether GPT's language-dependent behavior persists when tested on controlled multilingual datasets with balanced representation, or if the patterns are driven by training data imbalances in the original model.