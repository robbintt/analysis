---
ver: rpa2
title: Making Large Language Models Better Knowledge Miners for Online Marketing with
  Progressive Prompting Augmentation
arxiv_id: '2312.05276'
source_url: https://arxiv.org/abs/2312.05276
tags:
- knowledge
- entity
- related
- fruit
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing a Marketing-oriented
  Knowledge Graph (MoKG) for online marketing campaigns, which is crucial for efficient
  user preference matching. The proposed PAIR framework leverages Large Language Models
  (LLMs) with progressive prompting augmentation to mine marketing-oriented knowledge
  graphs.
---

# Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation

## Quick Facts
- **arXiv ID**: 2312.05276
- **Source URL**: https://arxiv.org/abs/2312.05276
- **Reference count**: 40
- **Primary result**: PAIR framework achieves superior performance in marketing knowledge graph mining with LightPAIR providing efficient deployment

## Executive Summary
This paper presents PAIR (Progressive Augmentation and Iterative Refinement), a framework that leverages Large Language Models to construct Marketing-oriented Knowledge Graphs (MoKGs) for online marketing campaigns. The framework addresses the challenge of efficient user preference matching by mining rich entity-relation-entity tuples from seed entities. PAIR employs adaptive relation filtering to constrain LLM outputs, progressive prompting augmentation for entity expansion, and reliable aggregation considering both self-consistency and semantic relatedness. For practical deployment, the authors introduce LightPAIR, a smaller and white-box model fine-tuned on high-quality corpus generated by a strong teacher-LLM.

## Method Summary
PAIR consists of four main components: Prior Knowledge Usage, Relation Filtering, Entity Expansion, and LightPAIR. The framework first retrieves structural and descriptive knowledge for seed entities from SupKG and external sources. It then uses an LLM to filter relevant relations from a predefined set rather than generating them freely, reducing hallucination. For entity expansion, PAIR progressively constructs prompts from coarse to fine-grained, incorporating different types of prior knowledge to steer LLMs toward diverse outputs. The results are aggregated using self-consistency scores and semantic relatedness measured by KG-BERT. LightPAIR is created by fine-tuning smaller LLMs on high-quality corpus generated by GPT 3.5, achieving comparable performance with significantly fewer parameters.

## Key Results
- PAIR outperforms baseline methods in accuracy (proportion of accepted tuples), novelty (proportion of new entities), and diversity metrics (AEE and ILAD)
- LightPAIR achieves comparable performance to complete PAIR with significantly fewer parameters (88% reduction)
- The adaptive relation filtering improves accuracy by 19.2% compared to pure relation generation
- Progressive prompting augmentation increases novelty by 12.5% and diversity by 14.1% AEE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive prompting augmentation improves LLM entity expansion by navigating different aspects of prior knowledge
- Mechanism: PAIR constructs prompts from coarse-grained to fine-grained, incorporating structural, descriptive, and inheritable knowledge to help LLMs focus on different knowledge aspects
- Core assumption: LLMs are sensitive to prompting variations and can be steered to different reasoning directions
- Evidence anchors:
  - [abstract] "we steer LLMs for entity expansion with progressive prompting augmentation"
  - [section 3.4] "By incorporating various knowledge, we progressively construct prompts from coarse-grained to fine-grained manner"
  - [corpus] Weak evidence - related papers focus on sentiment analysis or general LLM knowledge extraction, not marketing-specific progressive prompting

### Mechanism 2
- Claim: Adaptive relation filtering reduces uncontrollable relation generation by LLMs
- Mechanism: PAIR retrieves relevant relations based on entity type and uses LLM to filter desired relations from this set, constraining the relation space
- Core assumption: Unconstrained relation generation by LLMs can lead to irrelevant or harmful relations for marketing applications
- Evidence anchors:
  - [abstract] "we reduce the pure relation generation to an LLM based adaptive relation filtering process"
  - [section 3.3] "we resort to the powerful capability of LLMs for filtering desired relations from the pre-defined relation set R"
  - [corpus] Weak evidence - related papers do not discuss adaptive relation filtering for marketing knowledge graphs

### Mechanism 3
- Claim: LightPAIR achieves comparable performance to PAIR with significantly fewer parameters through fine-tuning with high-quality corpus
- Mechanism: PAIR generates high-quality corpus using GPT 3.5, which is then used to fine-tune smaller, white-box LLMs
- Core assumption: High-quality task-specific corpus generated by a strong LLM can effectively transfer knowledge to smaller models
- Evidence anchors:
  - [abstract] "we specialize in a small and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality corpus provided by a strong teacher-LLM"
  - [section 3.5] "we seek to conduct a lightweight and white-box pipeline of PAIR (i.e.,LightPAIR) for large-scale MoKG mining from the complete PAIR"
  - [corpus] Weak evidence - related papers focus on general LLM distillation, not marketing-specific knowledge graph construction

## Foundational Learning

- Concept: Knowledge Graph Construction
  - Why needed here: PAIR is designed to mine marketing-oriented knowledge graphs, which requires understanding the principles of knowledge graph construction
  - Quick check question: What are the key components of a knowledge graph and how are they typically extracted from text?

- Concept: Large Language Models (LLMs)
  - Why needed here: PAIR relies heavily on LLMs for relation filtering and entity expansion, so understanding their capabilities and limitations is crucial
  - Quick check question: How do LLMs generate text and what are the common challenges in controlling their output?

- Concept: Prompt Engineering
  - Why needed here: PAIR uses carefully designed prompts to steer LLMs, so understanding prompt engineering techniques is essential
  - Quick check question: What are the key principles of effective prompt engineering and how can prompts be adapted for different tasks?

## Architecture Onboarding

- Component map: Prior Knowledge Usage -> Relation Filtering -> Entity Expansion -> LightPAIR (deployment optimization)
- Critical path: Prior Knowledge Usage → Relation Filtering → Entity Expansion
- Design tradeoffs: PAIR trades off computational cost (multiple LLM calls) for improved accuracy and diversity. LightPAIR trades off some performance for efficiency
- Failure signatures: Poor prior knowledge leads to hallucinative entities, inaccurate relation filtering produces irrelevant tuples, ineffective entity expansion reduces diversity, poor fine-tuning degrades LightPAIR performance
- First 3 experiments:
  1. Test the accuracy of the relation filtering component with a small set of entities and relations
  2. Evaluate the diversity of the entity expansion component using different types of prior knowledge
  3. Compare the performance of LightPAIR with different student-LLMs and training corpus sizes

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following areas remain unexplored based on the current research:

### Open Question 1
- Question: How does the effectiveness of PAIR compare to other knowledge graph construction methods when applied to domains outside of online marketing?
- Basis in paper: [inferred] The paper focuses specifically on marketing-oriented knowledge graphs, but does not explore its performance in other domains
- Why unresolved: The paper only evaluates PAIR in the context of online marketing scenarios, leaving its generalizability to other domains unexplored
- What evidence would resolve it: Applying PAIR to knowledge graph construction tasks in other domains (e.g., healthcare, education) and comparing its performance to existing methods

### Open Question 2
- Question: What is the optimal balance between the number of progressive prompts and the quality of the resulting knowledge graph?
- Basis in paper: [explicit] The paper mentions using multiple progressive prompts for entity expansion, but does not investigate the impact of the number of prompts on performance
- Why unresolved: The paper does not provide a systematic analysis of how the number of progressive prompts affects the accuracy, novelty, and diversity of the mined knowledge graph
- What evidence would resolve it: Conducting experiments with varying numbers of progressive prompts and measuring their impact on key performance metrics

### Open Question 3
- Question: How does the performance of LightPAIR change when fine-tuned on different types of seed entities?
- Basis in paper: [explicit] The paper mentions using seed entities from SupKG for fine-tuning LightPAIR, but does not explore the impact of different seed entity types
- Why unresolved: The paper does not investigate how the characteristics of the seed entities (e.g., entity type, domain) affect the performance of LightPAIR
- What evidence would resolve it: Fine-tuning LightPAIR on seed entities from different domains or with different characteristics and comparing their performance

## Limitations
- The paper focuses exclusively on marketing-oriented knowledge graphs, limiting generalizability to other domains
- The computational overhead of multiple LLM calls in PAIR could be prohibitive for large-scale deployment
- The quality and potential biases of the corpus generated by GPT 3.5 for LightPAIR fine-tuning are not thoroughly analyzed

## Confidence

**Major Uncertainties:**
1. **Prompt engineering specifics**: Exact templates and variations affecting performance are not provided
2. **Generalization**: Limited evidence of performance on domains outside marketing
3. **Teacher-LLM quality**: Quality and biases in GPT 3.5 generated corpus are not analyzed
4. **Computational overhead**: PAIR's multiple LLM calls could be expensive for large-scale deployment

**Confidence Labels:**
- **High Confidence**: The core mechanism of using LLMs for adaptive relation filtering (Mechanism 2) is well-supported by ablation studies showing 19.2% improvement
- **Medium Confidence**: Progressive prompting augmentation (Mechanism 1) shows improvements in novelty (12.5%) and diversity (14.1% AEE), but exact contributions aren't clearly isolated
- **Medium Confidence**: LightPAIR approach (Mechanism 3) achieves comparable performance with 88% fewer parameters, but long-term stability isn't extensively validated

## Next Checks

1. **Prompt sensitivity analysis**: Systematically test different prompt variations for relation filtering and entity expansion to quantify the impact of prompt engineering on performance

2. **Cross-domain evaluation**: Apply PAIR to non-marketing knowledge graphs (e.g., biomedical or scientific domains) to assess generalization capabilities

3. **Teacher-LLM corpus quality audit**: Analyze the quality, diversity, and potential biases in the corpus generated by GPT 3.5 to ensure it's suitable for fine-tuning LightPAIR