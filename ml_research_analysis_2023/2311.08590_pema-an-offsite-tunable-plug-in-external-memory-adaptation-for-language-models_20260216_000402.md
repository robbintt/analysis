---
ver: rpa2
title: 'PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models'
arxiv_id: '2311.08590'
source_url: https://arxiv.org/abs/2311.08590
tags:
- pema
- language
- training
- memory
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEMA is a novel parameter-efficient fine-tuning method that enables
  adapting proprietary language models without accessing their full weights. It uses
  an external memory to store context representations from the model and trains lightweight
  LoRA-based adapters to predict target tokens.
---

# PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models

## Quick Facts
- **arXiv ID:** 2311.08590
- **Source URL:** https://arxiv.org/abs/2311.08590
- **Reference count:** 32
- **Primary result:** PEMA achieves better performance than baselines on machine translation and style transfer tasks while using significantly less memory and training faster than other fine-tuning approaches.

## Executive Summary
PEMA is a novel parameter-efficient fine-tuning method that enables adapting proprietary language models without accessing their full weights. It uses an external memory to store context representations from the model and trains lightweight LoRA-based adapters to predict target tokens. A key innovation is Gradual Unrolling, which interpolates between the adapter's and original model's predictions, starting with more weight on the adapter and gradually shifting to the original model. Experiments show PEMA achieves better performance than baselines on machine translation and style transfer tasks while using significantly less memory and training faster than other fine-tuning approaches.

## Method Summary
PEMA (Plug-in External Memory Adaptation) is a parameter-efficient fine-tuning method that enables adapting proprietary language models without accessing their full weights. The approach uses external memory to store context representations generated by a pre-trained language model (PLM), paired with desired target tokens from a dataset. Lightweight LoRA-based adapters are trained to predict target tokens from these context representations. A key innovation is Gradual Unrolling, which interpolates between the adapter's predictions and the original PLM's predictions during generation, starting with more weight on the adapter and gradually shifting to the PLM. The method includes joint training with reconstruction loss to preserve the original PLM's knowledge while learning task-specific patterns.

## Key Results
- PEMA outperforms baselines on WMT22 English→German translation (sBLEU scores) and GYAFC style transfer (Formality Improvement) tasks
- Achieves 4-5x memory efficiency compared to full fine-tuning while maintaining competitive performance
- Training latency is significantly reduced compared to full fine-tuning and other PEFT methods
- External memory size of 1M pairs provides optimal balance between performance and memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEMA enables fine-tuning without full model access by leveraging external memory storing context representations paired with target tokens.
- Mechanism: The data owner obtains context representations from the PLM owner, builds an external memory of (context representation, target token) pairs from their dataset, and trains LoRA-based adapters to predict target tokens from these representations.
- Core assumption: Context representations from the PLM's final layer capture sufficient information for task-specific prediction when paired with target tokens.
- Evidence anchors:
  - [abstract] "It uses external memory to store context representations generated by a PLM, mapped with the desired target word."
  - [section] "The first-step of PEMA is building an external memory. The output f(ci) represents a context representation obtained from the final layer's feedforward network output of a pre-trained language model. A paired representation for the i-th training example (ci, yi) ∈ E is created by defining an input prompt I and a target token sequence Y."
  - [corpus] Weak - no direct citations about external memory for fine-tuning, but related work on kNN-LM uses similar memory concept
- Break condition: If context representations do not encode sufficient task-relevant information, the external memory becomes ineffective and adapter training fails to capture task patterns.

### Mechanism 2
- Claim: Gradual Unrolling interpolation improves generation quality by balancing adapter predictions and PLM predictions based on sentence position.
- Mechanism: At the beginning of sentence generation, PEMA's adapter predictions are heavily weighted. As the sentence progresses, the weight shifts toward the original PLM's predictions, allowing the PLM to provide contextual completion.
- Core assumption: Adapter predictions are better for task-specific initial generation while PLM predictions are better for contextual continuation.
- Evidence anchors:
  - [abstract] "To improve the generation quality, we propose a novel interpolation strategy named Gradual Unrolling (GU). This approach allows for blending two next-word distributions, resulting in a strong interpolation of PEMA's distribution at the beginning of the sentence."
  - [section] "To facilitate this interpolation, we propose a new strategy, named Gradual Unrolling (GU). This approach allows for blending two next-word distributions, resulting in a strong interpolation of PEMA's distribution at the beginning of the sentence. The degree of interpolation gradually decreases as the sentence progresses."
  - [corpus] Missing - no citations for gradual unrolling strategy specifically, though related to scheduled sampling concepts
- Break condition: If the interpolation schedule is not properly tuned, either the adapter predictions dominate too long (causing incoherence) or the PLM predictions dominate too early (losing task specificity).

### Mechanism 3
- Claim: Joint training with reconstruction loss preserves the original PLM's knowledge while learning task-specific patterns.
- Mechanism: During training, PEMA uses a reconstruction decoder to minimize the difference between original context representations and those produced by the adapter, while simultaneously learning to predict target tokens.
- Core assumption: Maintaining fidelity to original context representations helps preserve general language understanding while learning task-specific patterns.
- Evidence anchors:
  - [abstract] "Our method entails training LoRA-based weight matrices within the final layer of the PLM for enhanced efficiency. The probability is then interpolated with the next-word distribution from the PLM to perform downstream tasks."
  - [section] "After completing the initial reconstruction training, we proceed to the joint retraining phase, using the pre-trained Brct and randomly initialized A. Our first objective is to acquire a representation hn pdi that is optimized for predicting the target word yn i."
  - [corpus] Weak - reconstruction loss for fine-tuning is mentioned but not directly cited for this specific approach
- Break condition: If reconstruction loss is too strong, it prevents the adapter from learning task-specific patterns; if too weak, it loses the PLM's general knowledge.

## Foundational Learning

- Concept: External memory and k-nearest neighbors search
  - Why needed here: PEMA builds on the kNN-LM concept of using external memory but replaces kNN search with learned neural network predictions for efficiency
  - Quick check question: What is the key difference between kNN-LM's external memory usage and PEMA's approach?

- Concept: LoRA (Low-Rank Adaptation) and parameter-efficient fine-tuning
  - Why needed here: PEMA uses LoRA-based adapters to achieve parameter efficiency while maintaining performance, requiring understanding of low-rank matrix decomposition
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Gradual unrolling and scheduled sampling
  - Why needed here: The Gradual Unrolling strategy is a form of curriculum learning that gradually shifts from adapter predictions to PLM predictions during generation
  - Quick check question: What is the purpose of gradually decreasing the interpolation weight during sentence generation?

## Architecture Onboarding

- Component map: Context Representation Extractor -> External Memory (stores context representation, target token pairs) -> LoRA Adapters (Bpd prediction decoder, Brct reconstruction decoder) -> Gradual Unrolling (interpolation scheduler) -> PLM

- Critical path:
  1. Data owner obtains context representations from PLM owner
  2. Build external memory from dataset
  3. Train PEMA adapters with joint reconstruction and prediction objectives
  4. During inference, obtain context representation for test data
  5. Generate predictions from both PEMA and PLM
  6. Apply Gradual Unrolling interpolation

- Design tradeoffs:
  - Memory vs Performance: Larger external memory improves coverage but increases storage requirements
  - Rank size: Larger rank sizes in LoRA adapters improve performance but increase parameter count
  - Interpolation schedule: Steeper or shallower schedules affect task specificity vs coherence

- Failure signatures:
  - Poor performance on test data: Likely issues with external memory coverage or adapter training
  - Incoherent generations: Gradual Unrolling schedule may be misconfigured
  - High memory usage: External memory size or rank parameters may be too large

- First 3 experiments:
  1. Verify context representation extraction works correctly from PLM
  2. Test external memory construction with small synthetic dataset
  3. Validate adapter training converges with joint reconstruction and prediction objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PEMA scale with increasingly larger language models (e.g., GPT-4, PaLM-2) compared to smaller models?
- Basis in paper: [inferred] The paper only evaluates PEMA on models up to OPT-30B, suggesting potential for further scaling but not exploring it.
- Why unresolved: The paper does not test PEMA on the largest available language models, which could reveal different scaling behaviors or limitations.
- What evidence would resolve it: Experimental results showing PEMA's performance, memory efficiency, and training/inference latency on models like GPT-4, PaLM-2, or other frontier models.

### Open Question 2
- Question: What is the optimal rank size (r) for PEMA across different task types and model sizes, and how sensitive is performance to this hyperparameter?
- Basis in paper: [explicit] The paper uses r=512 but acknowledges that LoRA suggests smaller ranks might work, and AdaLoRA suggests larger ranks for the last layer.
- Why unresolved: The paper only tests one rank size and doesn't explore the sensitivity of performance to this hyperparameter across different tasks and model sizes.
- What evidence would resolve it: Comprehensive ablation studies varying rank size across multiple tasks, model sizes, and dataset characteristics to identify optimal configurations.

### Open Question 3
- Question: How does PEMA's performance compare to full fine-tuning when computational resources are not constrained?
- Basis in paper: [explicit] The paper emphasizes PEMA's efficiency but doesn't directly compare its final task performance to full fine-tuning under equal resource conditions.
- Why unresolved: The paper focuses on efficiency metrics but doesn't establish whether PEMA's performance gap to full fine-tuning closes when computational constraints are removed.
- What evidence would resolve it: Direct comparison of PEMA versus full fine-tuning on identical hardware and training time budgets across multiple tasks.

### Open Question 4
- Question: How does PEMA handle privacy concerns related to sharing LM head parameters between data owners and PLM owners?
- Basis in paper: [explicit] The paper acknowledges this as a limitation and potential ethical concern but doesn't propose solutions.
- Why unresolved: The paper identifies the privacy concern but doesn't explore technical or architectural solutions to address it.
- What evidence would resolve it: Implementation of privacy-preserving techniques (e.g., secure multi-party computation, differential privacy) for sharing LM head parameters, along with evaluation of their impact on performance and privacy guarantees.

## Limitations

- Evaluation is limited to only two downstream tasks (machine translation and style transfer), which may not generalize to other language modeling applications.
- The external memory construction process relies on obtaining context representations from PLM owners, which may not always be feasible in practice.
- The gradual unrolling strategy, while shown to improve results, lacks comprehensive ablation studies to determine optimal interpolation schedules across different task types.

## Confidence

**High Confidence:** The core mechanism of using external memory with LoRA-based adapters for parameter-efficient fine-tuning is well-supported by the experimental results. The memory efficiency gains compared to full fine-tuning and other PEFT methods are clearly demonstrated with quantitative metrics.

**Medium Confidence:** The effectiveness of the Gradual Unrolling strategy shows promise but has limited validation. The paper shows improvements on specific tasks but doesn't explore how sensitive the interpolation schedule is to different model sizes, task types, or dataset characteristics.

**Low Confidence:** The claim that PEMA can adapt "proprietary" models without full weight access assumes that context representation extraction is straightforward and that PLM owners will provide this capability. This practical assumption needs more validation in real-world scenarios.

## Next Checks

1. **Cross-task generalization test:** Evaluate PEMA on additional NLP tasks (summarization, question answering, text classification) to verify that the external memory approach generalizes beyond translation and style transfer.

2. **Interpolation schedule sensitivity analysis:** Systematically vary the gradual unrolling parameters (λmax, CS decrement rate) across different tasks and model sizes to identify optimal schedules and understand robustness.

3. **Real-world proprietary model test:** Attempt to implement PEMA with an actual proprietary model (e.g., GPT-3 API) to validate whether context representation extraction is practically feasible and whether the performance claims hold in real scenarios.