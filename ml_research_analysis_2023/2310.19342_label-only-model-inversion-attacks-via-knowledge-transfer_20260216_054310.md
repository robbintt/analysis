---
ver: rpa2
title: Label-Only Model Inversion Attacks via Knowledge Transfer
arxiv_id: '2310.19342'
source_url: https://arxiv.org/abs/2310.19342
tags:
- samples
- attack
- celeba
- surrogate
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses label-only model inversion attacks, where
  an adversary only has access to the predicted label of a machine learning model.
  The authors propose LOKT, a novel approach that transfers decision knowledge from
  the target model to surrogate models using a new Target model-assisted ACGAN (T-ACGAN).
---

# Label-Only Model Inversion Attacks via Knowledge Transfer

## Quick Facts
- **arXiv ID**: 2310.19342
- **Source URL**: https://arxiv.org/abs/2310.19342
- **Reference count**: 40
- **Primary result**: Novel LOKT approach achieves >15% better accuracy than state-of-the-art label-only model inversion attacks using fewer queries

## Executive Summary
This paper addresses the challenging problem of model inversion (MI) attacks in label-only settings, where adversaries only have access to hard label predictions from a target model. The authors propose LOKT (Label-Only knOwledge Transfer), which transfers decision knowledge from the target model to surrogate models using a novel Target model-assisted ACGAN (T-ACGAN). By casting the label-only attack as a more tractable white-box attack on the surrogate model, LOKT significantly outperforms existing methods across all benchmarks while using fewer queries. The approach demonstrates that even minimal information exposure (hard labels only) can lead to substantial privacy risks for machine learning models.

## Method Summary
The LOKT approach involves training a T-ACGAN on public data using the target model's predicted labels to generate diverse, class-balanced synthetic samples. These samples are then used to train surrogate models that approximate the target model's decision boundaries. The surrogate models are subsequently attacked using white-box MI attack methods (KEDMI or PLGMI). The key innovation is using the target model to provide pseudo-labels during T-ACGAN training, enabling effective knowledge transfer without direct access to the target model's parameters or soft outputs.

## Key Results
- LOKT outperforms state-of-the-art label-only MI attacks by more than 15% across all benchmarks
- Uses 30% fewer queries compared to existing methods
- Successfully reconstructs private training data from face recognition models (FaceNet64, IR152, VGG16, BiDO-HSIC) trained on CelebA, Facescrub, and Pubfig83
- Achieves significant improvements in both attack accuracy and KNN distance metrics

## Why This Works (Mechanism)

### Mechanism 1: T-ACGAN's Effective Knowledge Transfer
T-ACGAN transfers decision knowledge by leveraging the target model's predicted labels on generated samples. During discriminator training, it uses the target model T to assign pseudo labels to diverse generated samples, training both the discriminator and classifier. This allows the surrogate model to learn T's decision boundaries without direct access to T's parameters or soft outputs. The core assumption is that target model hard label predictions on diverse generated samples contain sufficient information about its decision knowledge for effective transfer.

### Mechanism 2: Learning Dynamics of Deep Neural Networks
The surrogate models trained via T-ACGAN are effective proxies due to the learning dynamics of deep neural networks. Following findings that DNNs learn general patterns first to fit "easy samples" during training, the surrogate model naturally develops high likelihood for these samples, creating a property where high-likelihood samples under the surrogate also tend to have high likelihood under the target model.

### Mechanism 3: Synthetic Data Generation
Using the generator from T-ACGAN to create synthetic data for training additional surrogate models improves attack performance compared to using public dataset directly. The generator produces diverse, class-balanced synthetic samples labeled by the target model, providing better coverage of the decision space of the target model than the original public data.

## Foundational Learning

- **Generative Adversarial Networks (GANs) and ACGAN**: The approach relies on GANs to learn data distribution from public data and generate synthetic samples for training surrogate models. *Quick check: How does the generator-discriminator adversarial training process enable GANs to learn realistic data distributions?*

- **Knowledge transfer in machine learning**: The core innovation transfers decision knowledge from an opaque target model to surrogate models that can be attacked using white-box methods. *Quick check: What are the key differences between traditional knowledge transfer (e.g., teacher-student models) and the approach used in this paper?*

- **Model inversion attacks and threat models**: Understanding white-box, black-box, and label-only attack settings is essential to appreciate why this approach is novel and why label-only attacks are particularly challenging. *Quick check: What information does an attacker have access to in a label-only model inversion attack versus a white-box attack?*

## Architecture Onboarding

- **Component map**: T (target model) -> T-ACGAN (G, D, C) -> Surrogate models (C◦D, S, Sen) -> Evaluation model E

- **Critical path**: 1) Train T-ACGAN using public data Dpub and target model T's predictions; 2) Extract surrogate model(s) from T-ACGAN; 3) Generate synthetic data using T-ACGAN's generator; 4) Train additional surrogate models S and/or Sen using synthetic data; 5) Apply white-box MI attack on surrogate model(s); 6) Evaluate attack quality using evaluation model E

- **Design tradeoffs**: Using C◦D directly vs training additional surrogate models (simpler but potentially less effective); number of surrogate models in ensemble (more models may improve performance but increase computational cost); architecture choice for surrogate models (DenseNet variants used, but other architectures could be explored)

- **Failure signatures**: Poor attack accuracy on surrogate models indicates failed knowledge transfer; high KNN distance indicates reconstructed images don't match private training data; T-ACGAN failing to generate diverse synthetic data would limit surrogate model quality

- **First 3 experiments**: 1) Train T-ACGAN with Dpriv=CelebA, Dpub=CelebA, T=FaceNet64 and evaluate γ over training iterations; 2) Train surrogate model C◦D and evaluate attack accuracy on a small subset of identities; 3) Generate synthetic data and train additional surrogate model S, then compare attack accuracy with C◦D baseline

## Open Questions the Paper Calls Out
- How effective would LOKT be on different types of private training datasets, such as healthcare data, beyond the face recognition models studied?
- How does LOKT's query budget compare to other model inversion attack methods when attacking more complex or larger models?
- How would LOKT's performance be affected if the target model's architecture was unknown to the attacker?

## Limitations
- The approach relies on the assumption that target model hard labels contain sufficient information about decision boundaries, which hasn't been rigorously validated across diverse model architectures
- Evaluation focuses primarily on face recognition datasets, limiting generalizability to other domains
- Several design choices in T-ACGAN lack systematic ablation studies to isolate their individual contributions to performance

## Confidence

- **High confidence**: Experimental methodology is clearly described and attack accuracy improvements over baselines are well-documented across multiple benchmarks
- **Medium confidence**: Core mechanism of knowledge transfer via T-ACGAN is theoretically sound but lacks comprehensive ablation studies
- **Medium confidence**: Claims about reduced query requirements compared to existing label-only attacks are supported by experiments but could benefit from larger-scale validation

## Next Checks

1. **Ablation study on T-ACGAN components**: Systematically evaluate the impact of pseudo-label supervision, generator diversity, and surrogate model architecture choices on attack performance to isolate which components drive improvements.

2. **Cross-domain generalization test**: Evaluate the approach on non-face datasets (e.g., medical imaging, text classification) to assess whether the knowledge transfer mechanism generalizes beyond demonstrated domains.

3. **Robustness to target model variations**: Test attack effectiveness across different target model architectures (CNNs, transformers, ensemble models) and training regimes to understand sensitivity to target model characteristics.