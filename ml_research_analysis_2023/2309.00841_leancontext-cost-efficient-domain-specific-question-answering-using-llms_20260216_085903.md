---
ver: rpa2
title: 'LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs'
arxiv_id: '2309.00841'
source_url: https://arxiv.org/abs/2309.00841
tags:
- context
- leancontext
- query
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeanContext addresses high LLM API costs for domain-specific question
  answering by reducing context length while preserving accuracy. It dynamically selects
  top-k sentences relevant to the query using a reinforcement learning technique,
  and reduces the remaining sentences using a free open-source method.
---

# LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs

## Quick Facts
- arXiv ID: 2309.00841
- Source URL: https://arxiv.org/abs/2309.00841
- Reference count: 6
- Key outcome: LeanContext reduces LLM API costs by 37.29% to 67.81% while maintaining similar QA accuracy (ROUGE-1 score decreases by only 1.41% to 2.65%)

## Executive Summary
LeanContext addresses the high computational costs of domain-specific question answering by dynamically selecting the most relevant context sentences while preserving accuracy. The approach uses reinforcement learning to determine the optimal number of top-k sentences to preserve, then reduces the remaining context using an open-source method. Evaluated on Arxiv and BBC News datasets, LeanContext achieves significant cost savings while maintaining comparable answer quality to full-context approaches. The method also shows improved performance when combined with existing open-source summarizers.

## Method Summary
LeanContext uses reinforcement learning to dynamically determine the optimal number of top-k sentences to preserve based on query-context similarity. The approach ranks sentences by relevance to the query using embeddings, preserves the top-k sentences, and reduces the remaining context using an open-source text reduction method. The RL agent learns to balance token reduction against accuracy retention through offline profiling. For evaluation, LeanContext is tested on Arxiv and BBC News datasets with documents published March 2023, using ROUGE scores to measure answer quality against full-context baselines.

## Key Results
- Reduces LLM API costs by 37.29% to 67.81% compared to full-context approaches
- Maintains similar QA accuracy with ROUGE-1 score decreases of only 1.41% to 2.65%
- When combined with open-source summarizers, improves accuracy by 13.22% to 24.61%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using reinforcement learning to dynamically select the number of top-k sentences reduces LLM API costs while maintaining QA accuracy.
- Mechanism: The RL agent computes a state from query and context embeddings, selects an action (threshold for top-k selection), and updates a Q-table based on reward function balancing token reduction and accuracy retention.
- Core assumption: The optimal context size for answering a query can be predicted from the embedding difference between the query and context.
- Evidence anchors:
  - [abstract]: "We introduce a reinforcement learning technique that dynamically determines k based on the query and context."
  - [section]: "State: We combine query and context to define the state... At offline profiling, we compute the embedding of the query vq and the embedding of the context as vc. Then we subtract vq from vc that indicates the context-query pair."
  - [corpus]: Weak. No corpus evidence available for RL mechanism.
- Break condition: If the embedding space does not capture semantic similarity effectively, the RL agent will select suboptimal top-k values.

### Mechanism 2
- Claim: Preserving top-k sentences and reducing the rest with an open-source method maintains QA accuracy better than full summarization.
- Mechanism: Top-k sentences most relevant to the query are kept intact, while less important sentences are reduced using a free open-source text reduction method, preserving both relevance and brevity.
- Core assumption: Important sentences for answering the query are not diluted when other sentences are reduced rather than removed entirely.
- Evidence anchors:
  - [abstract]: "Our approach, LeanContext, efficiently extracts k key sentences from the context that are closely aligned with the query."
  - [section]: "Figure 4 shows an illustration of this combination process. LeanContext keeps the top-k sentences intact, while other sentences between the top-k sentences are reduced by an open-source text reduction method."
  - [corpus]: Weak. No corpus evidence available for sentence preservation mechanism.
- Break condition: If the text reduction method over-generalizes, it may remove context necessary for accurate answers.

### Mechanism 3
- Claim: Combining LeanContext with existing open-source summarizers further improves QA accuracy compared to using either alone.
- Mechanism: LeanContext preserves top-k sentences and combines them with reduced context from open-source summarizers, leveraging both relevance and summarization strengths.
- Core assumption: The combination of preserved top-k sentences and reduced context creates a more complete information set than either method alone.
- Evidence anchors:
  - [abstract]: "if free pretrained LLM-based summarizers are used to reduce context (into human consumable summaries), LeanContext can further modify the reduced context to enhance the accuracy (ROUGE-1 score) by 13.22% to 24.61%."
  - [section]: "As these open-source models are not trained on new domain data, we observe that combining 10% top-k sentences with the open-source models boosts the QA system's performance by 5.41% ~ 17.11% for the Arxiv dataset."
  - [corpus]: Weak. No corpus evidence available for combined approach effectiveness.
- Break condition: If the open-source summarizer produces low-quality summaries, the combined approach may degrade accuracy.

## Foundational Learning

- Concept: Reinforcement Learning Q-learning
  - Why needed here: To dynamically determine the optimal number of top-k sentences based on the query-context pair without fixed heuristics.
  - Quick check question: How does the Q-learning update rule balance exploration and exploitation in this context?

- Concept: Text embeddings and semantic similarity
  - Why needed here: To rank sentences by relevance to the query and compute states for the RL agent using cosine similarity between embeddings.
  - Quick check question: What embedding model is used to generate query and sentence vectors?

- Concept: ROUGE score for evaluation
  - Why needed here: To measure the quality of answers generated by the LLM using reduced context compared to the full context baseline.
  - Quick check question: Which ROUGE variant (ROUGE-1, ROUGE-2, ROUGE-L) is most appropriate for evaluating QA performance?

## Architecture Onboarding

- Component map: Vector Database -> Embedding Generator -> Semantic Search -> LeanContext Processor -> LLM API
- Critical path:
  1. Query arrives → Embedding generated
  2. Semantic search retrieves relevant chunks → Context formed
  3. LeanContext ranks sentences → RL selects top-k threshold
  4. Top-k sentences preserved, others reduced → Reduced context created
  5. LLM API called with reduced context and query → Answer returned
- Design tradeoffs:
  - Fixed vs adaptive k: Fixed k is simpler but may be suboptimal; adaptive k adds complexity but improves cost-accuracy balance
  - Sentence preservation vs summarization: Preserving top-k maintains relevance but may not reduce tokens as much as summarization
  - Open-source vs LLM reduction: Open-source is cheaper but may be less effective; LLM reduction is more accurate but adds cost
- Failure signatures:
  - Accuracy drops significantly: RL agent may be selecting too few top-k sentences
  - Token reduction is minimal: Text reduction method may not be aggressive enough
  - Increased latency: RL inference or context processing may be too slow
  - Context retrieval misses relevant information: Embedding model or semantic search parameters may need tuning
- First 3 experiments:
  1. Compare accuracy and cost of fixed k (e.g., 10% of sentences) vs adaptive k on a small dataset
  2. Test different text reduction methods (e.g., SBert, SC) with and without top-k preservation
  3. Evaluate the impact of different chunk sizes (N=2, 4, 8, 10) on retrieval quality and cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LeanContext's performance scale with larger context sizes and more complex queries?
- Basis in paper: [explicit] The paper mentions evaluating LeanContext on datasets with different numbers of chunks (N=2, 4, 8, 10) but doesn't explore significantly larger contexts or more complex query types.
- Why unresolved: The current evaluation is limited to relatively small contexts and may not capture performance degradation or improvements with larger, more complex inputs.
- What evidence would resolve it: Extensive testing on larger datasets with varying context lengths and query complexities, comparing LeanContext's performance and cost savings against other methods.

### Open Question 2
- Question: What is the optimal RL training strategy for LeanContext to balance cost and performance?
- Basis in paper: [explicit] The paper mentions using a lightweight Q-learning-based reinforcement learning algorithm but doesn't extensively explore different training strategies or hyperparameters.
- Why unresolved: The current training approach may not be optimal for all types of domains or queries, and there's potential for improvement in the RL agent's decision-making process.
- What evidence would resolve it: Comparative studies of different RL training strategies, including variations in reward functions, state representations, and action spaces, to determine the most effective approach.

### Open Question 3
- Question: How does LeanContext perform in real-world applications with rapidly changing domain-specific data?
- Basis in paper: [explicit] The paper acknowledges that domain-specific data and user queries are dynamic in nature but doesn't provide extensive real-world testing or long-term evaluation.
- Why unresolved: The effectiveness of LeanContext in real-world scenarios with continuously evolving data and query patterns remains untested.
- What evidence would resolve it: Longitudinal studies in actual domain-specific QA systems, tracking LeanContext's performance over time as the underlying data and query patterns change.

## Limitations
- Evaluation relies entirely on proprietary Arxiv and BBC News datasets with March 2023 documents, limiting generalizability to other domains or timeframes
- Reinforcement learning mechanism depends heavily on quality of embedding representations for state computation
- Paper does not address potential latency impacts from the RL inference step, which could affect real-time application viability
- Cost savings estimates assume specific LLM pricing models that may change over time

## Confidence

**High Confidence**: The claim that preserving top-k sentences and reducing the rest maintains QA accuracy better than full summarization is well-supported by the experimental results showing ROUGE-1 score decreases of only 1.41% to 2.65% while achieving 37.29% to 67.81% cost reductions.

**Medium Confidence**: The reinforcement learning mechanism for dynamically selecting k shows promise but lacks detailed implementation specifications and corpus evidence for its effectiveness across diverse query types.

**Low Confidence**: The claim about combining LeanContext with open-source summarizers improving accuracy by 13.22% to 24.61% has weak corpus support and limited experimental validation details.

## Next Checks
1. **Cross-domain validation**: Test LeanContext on datasets from different domains (medical, legal, technical documentation) to verify generalizability of the 37.29%-67.81% cost reduction claims across varied content types.
2. **Latency measurement**: Benchmark the end-to-end latency of LeanContext including RL inference time compared to baseline approaches to quantify real-world deployment feasibility.
3. **Embedding sensitivity analysis**: Evaluate how different embedding models (e.g., sentence-BERT, OpenAI embeddings) affect the RL agent's state computation and subsequent top-k selection accuracy to identify potential failure modes.