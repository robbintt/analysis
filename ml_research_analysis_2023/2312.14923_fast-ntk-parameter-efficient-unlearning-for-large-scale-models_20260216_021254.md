---
ver: rpa2
title: 'Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models'
arxiv_id: '2312.14923'
source_url: https://arxiv.org/abs/2312.14923
tags:
- unlearning
- parameters
- fast-ntk
- accuracy
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Machine unlearning is crucial for privacy, allowing selective removal
  of data from trained models. Neural Tangent Kernel (NTK)-based unlearning achieves
  strong results but suffers from high computational complexity due to full Jacobian
  calculations over all model parameters.
---

# Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models

## Quick Facts
- **arXiv ID**: 2312.14923
- **Source URL**: https://arxiv.org/abs/2312.14923
- **Reference count**: 33
- **Key outcome**: Fast-NTK reduces computational complexity of NTK-based unlearning by restricting Jacobian computations to parameter-efficient fine-tuning subsets (e.g., batch normalization layers or visual prompts), achieving performance comparable to retraining from scratch while reducing parameters from millions to less than 5% of the full model.

## Executive Summary
Fast-NTK addresses the computational bottleneck in Neural Tangent Kernel (NTK)-based machine unlearning, which traditionally requires full Jacobian calculations over all model parameters. By incorporating parameter-efficient fine-tuning methods like batch normalization tuning for CNNs and visual prompts for vision transformers, Fast-NTK restricts unlearning computations to a small subset of parameters. Experiments on CIFAR-10 and ImageNet-R demonstrate that this approach maintains performance on retained data while effectively removing knowledge of forgotten classes, achieving results comparable to the ideal baseline of retraining from scratch on retained data alone.

## Method Summary
Fast-NTK extends NTK-based unlearning by selectively focusing on a subset of model parameters, specifically those introduced by parameter-efficient fine-tuning methods. For CNNs, this involves fine-tuning batch normalization layers, while for vision transformers, it uses visual prompts. The algorithm computes the NTK-based unlearning update only on these fine-tuned parameters rather than the full model, dramatically reducing computational complexity. The method divides data into forget and retain sets, applies the unlearning update to erase knowledge of forget samples, and evaluates performance on both sets along with relearning time on forget samples.

## Key Results
- Fast-NTK maintains performance comparable to retraining from scratch on retained data
- Reduces parameter count from millions to less than 5% of full model parameters
- Achieves strong unlearning performance by reducing accuracy on forget set to zero
- Demonstrates computational efficiency through parameter-efficient fine-tuning on BN layers (CNNs) or visual prompts (ViTs)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fast-NTK achieves comparable performance to full NTK-based unlearning by restricting Jacobian computations to a small subset of model parameters
- Mechanism: The NTK-based unlearning update formula requires computing the Jacobian of the model output with respect to all parameters. By limiting the Jacobian computation to only the parameters being fine-tuned (e.g., BN layers), the algorithm dramatically reduces computational complexity while preserving the unlearning effect on those parameters
- Core assumption: The parameters being fine-tuned (BN layers or prompts) are sufficient to capture the knowledge that needs to be unlearned, and their update is enough to erase the influence of forget samples
- Evidence anchors: Abstract states parameter count is significantly reduced; section describes extension to NTK-based unlearning by selectively focusing on parameter subsets
- Break condition: If the fine-tuned parameters do not capture the knowledge that needs to be unlearned, the algorithm will fail to properly erase the influence of forget samples

### Mechanism 2
- Claim: Fast-NTK reduces the number of parameters involved in unlearning from millions to less than 5% of the full model
- Mechanism: By applying parameter-efficient fine-tuning (PEFT) techniques like BN-tuning or prompt-tuning, the algorithm restricts unlearning to only the parameters introduced by these methods, which represent a small fraction of total model parameters
- Core assumption: The parameters introduced by PEFT methods (BN scaling/shifting terms or prompt embeddings) are small in number compared to the full parameter set
- Evidence anchors: Abstract mentions significant reduction in parameter count; section states Fast-NTK reduces parameters to 0.05% ~ 4.88% of full model parameters
- Break condition: If the PEFT parameters constitute a larger fraction of the model than expected, the computational savings may be less significant

### Mechanism 3
- Claim: Fast-NTK achieves performance comparable to retraining from scratch on the retain set alone
- Mechanism: The algorithm maintains accuracy on the retain set while reducing accuracy on the forget set to zero, matching the ideal baseline of retraining only on retained data
- Core assumption: The unlearning process on the limited parameter set is sufficient to completely erase knowledge of forget samples without harming performance on retain samples
- Evidence anchors: Abstract states Fast-NTK maintains performance comparable to retraining; section shows negligible accuracy degradation on retain set compared to RETRAIN baseline
- Break condition: If the limited parameter set cannot fully capture the knowledge to be erased, or if there's significant interference between forget and retain samples, the algorithm may not achieve performance comparable to retraining

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in understanding neural network training dynamics
  - Why needed here: NTK-based unlearning relies on the theoretical framework of NTK to compute the optimal one-shot update for removing data influence
  - Quick check question: What is the mathematical definition of the NTK matrix between two datasets D1 and D2?

- Concept: Parameter-efficient fine-tuning (PEFT) techniques like batch normalization tuning and prompt tuning
  - Why needed here: Fast-NTK builds upon these PEFT methods to restrict the parameter space for unlearning computations
  - Quick check question: How do batch normalization layers differ from standard convolutional layers in terms of learnable parameters?

- Concept: Machine unlearning evaluation metrics (accuracy on retain/forget sets, relearning time, hold-out set performance)
  - Why needed here: The paper uses specific metrics to demonstrate that Fast-NTK achieves comparable performance to retraining baselines
  - Quick check question: Why is it important to measure accuracy on both the retain set and a hold-out set when evaluating unlearning algorithms?

## Architecture Onboarding

- Component map: Pre-trained model (CNN or ViT) -> Parameter-efficient fine-tuning layer (BN layers for CNNs, visual prompts for ViTs) -> NTK-based unlearning computation restricted to fine-tuned parameters -> Evaluation pipeline measuring accuracy on retain/forget sets and relearning time

- Critical path: Fine-tune pre-trained model on PEFT parameters → Apply NTK-based unlearning update on these parameters → Evaluate performance on retain/forget sets

- Design tradeoffs: Fast-NTK trades computational efficiency (by reducing parameter count) for potential loss in unlearning effectiveness (if the fine-tuned parameters don't capture all necessary knowledge)

- Failure signatures:
  - High accuracy on forget set after unlearning (knowledge not properly erased)
  - Significant accuracy drop on retain set (over-correction or interference)
  - Long relearning time on forget set (knowledge still present but hard to access)

- First 3 experiments:
  1. Implement BN-tuning on a pre-trained CNN and verify that accuracy on downstream task remains similar to full fine-tuning
  2. Apply Fast-NTK to forget a single class in CIFAR-10 and measure accuracy on retain/forget sets compared to baseline methods
  3. Scale up to ViT on ImageNet-R with larger number of classes and verify computational savings and performance retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations and risks of using pre-trained models as a starting point for machine unlearning?
- Basis in paper: The paper discusses the risk of using pre-trained models, noting that they may already possess some knowledge of classes from the forget set (Df), making it challenging to completely erase all information and concepts associated with these classes
- Why unresolved: The paper acknowledges the risk but does not provide a comprehensive analysis of the limitations or propose solutions to mitigate this risk
- What evidence would resolve it: A thorough study comparing the effectiveness of unlearning algorithms starting from pre-trained models versus randomly initialized models, along with proposed strategies to address the inherent knowledge in pre-trained models

### Open Question 2
- Question: How does the performance of Fast-NTK compare to other unlearning methods in terms of accuracy on retained data and removal of knowledge from forgotten classes?
- Basis in paper: The paper presents experimental results showing that Fast-NTK achieves performance comparable to retraining from scratch on retained data and effectively removes knowledge from forgotten classes
- Why unresolved: The paper does not provide a direct comparison with other unlearning methods, such as those based on differential privacy or data partitioning
- What evidence would resolve it: A comprehensive comparison of Fast-NTK with other state-of-the-art unlearning methods on various datasets and model architectures, evaluating both accuracy on retained data and effectiveness in removing knowledge from forgotten classes

### Open Question 3
- Question: What are the computational and storage costs of Fast-NTK compared to full-model NTK-based unlearning?
- Basis in paper: The paper highlights that Fast-NTK significantly reduces the number of parameters subjected to fine-tuning, down to a range of 0.05% ~ 4.88% of the full model parameters, making it more practical and scalable
- Why unresolved: The paper does not provide a detailed analysis of the computational and storage costs of Fast-NTK compared to full-model NTK-based unlearning
- What evidence would resolve it: A quantitative comparison of the computational time, memory usage, and storage requirements of Fast-NTK and full-model NTK-based unlearning methods on various model architectures and dataset sizes

## Limitations

- The paper lacks direct evidence that batch normalization layers or visual prompts contain sufficient information to capture class-specific knowledge for effective unlearning
- Parameter reduction ratios (0.05% to 4.88%) depend heavily on model architecture and may be less significant for models with fewer parameters or different architectural choices
- No comparison is provided against state-of-the-art unlearning methods that use different approaches, limiting the evaluation scope

## Confidence

- Mechanism 1 (NTK computation on limited parameters): Medium - Theoretical foundation is sound but empirical validation of BN/prompts capturing sufficient knowledge is limited
- Mechanism 2 (Parameter reduction to <5%): High - Well-supported by stated reduction ratios and standard PEFT parameter counts
- Mechanism 3 (Performance comparable to retraining): Medium - Claims are supported by experiments but limited to specific datasets and architectures

## Next Checks

1. Conduct ablation studies removing different parameter subsets (not just BN/prompts) to verify that the chosen parameters indeed capture the necessary knowledge for unlearning
2. Apply Fast-NTK to architectures with different parameter distributions (e.g., models with few normalization layers) to assess robustness of computational savings
3. Test Fast-NTK on additional datasets with different characteristics (e.g., more classes, different domain) to validate generalization beyond CIFAR-10 and ImageNet-R