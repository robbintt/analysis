---
ver: rpa2
title: 'Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation:
  A Fresh Orb in the Cosmos of LLMs'
arxiv_id: '2311.03127'
source_url: https://arxiv.org/abs/2311.03127
tags:
- translation
- systems
- evaluation
- system
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the findings of the WMT 2023 Shared Task on
  Discourse-Level Literary Translation, which focuses on translating web novels between
  Chinese and English. The task aims to address the challenges of literary translation,
  such as rich linguistic and cultural phenomena, limited data, long-range context,
  and unreliable evaluation methods.
---

# Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs

## Quick Facts
- arXiv ID: 2311.03127
- Source URL: https://arxiv.org/abs/2311.03127
- Reference count: 15
- Key outcome: Top-ranked systems outperformed commercial translation systems and open-source LLMs in translating web novels between Chinese and English, highlighting the benefits of external knowledge and the potential of LLMs in literary translation.

## Executive Summary
This paper presents the findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation, focusing on translating web novels between Chinese and English. The task addresses challenges in literary translation, including rich linguistic and cultural phenomena, limited data, long-range context, and unreliable evaluation methods. The authors released the GuoFeng Webnovel Corpus, a high-quality Chinese-English web novel corpus, and proposed a well-defined criteria for human evaluation based on the MQM framework. The results show that top systems outperformed commercial translation systems and open-source LLMs, emphasizing the benefits of external knowledge and the potential of LLMs in literary translation.

## Method Summary
The shared task involved translating web novels between Chinese and English, with a focus on discourse-level phenomena. The GuoFeng Webnovel Corpus, a copyrighted and high-quality Chinese-English web novel corpus, was released for the task. Human evaluation was performed using an adaptation of the MQM framework by professional translators. Systems were evaluated using both automatic metrics (BLEU, chrF, COMET, TER, d-BLEU) and human evaluation. Participants used various approaches, including sentence-level and document-level Transformers, pretrained models, and LLMs.

## Key Results
- Top-ranked systems outperformed commercial translation systems and open-source LLMs
- LLM systems showed the best performance among baseline systems
- The top-2 unconstrained systems outperformed the best constrained system, highlighting the benefits of external knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs benefit from external knowledge in literary translation.
- Mechanism: The LLM's ability to access external knowledge during inference provides an advantage over constrained systems that rely solely on provided training data.
- Core assumption: External knowledge is relevant and accurate for the literary domain.
- Evidence anchors:
  - [abstract]: "the top-ranked systems outperform commercial translation systems and open-source LLMs, highlighting the benefits of external knowledge and the potential of LLMs in literary translation."
  - [section 6.2]: "The top-2 unconstrained systems outperform the best constrained system, highlighting the benefits of external knowledge."
- Break condition: External knowledge is not relevant or accurate for the literary domain, or the LLM's access to it is limited or corrupted.

### Mechanism 2
- Claim: Discourse-level context is crucial for literary translation quality.
- Mechanism: Modeling long-range context (e.g., across chapters or entire documents) helps capture narrative consistency, character development, and thematic elements essential to literary works.
- Core assumption: The document-level GuoFeng Webnovel Corpus provides sufficient discourse-level context for training.
- Evidence anchors:
  - [section 1]: "Long-Range Context: literature such as novels have much longer contexts than texts in other domains... Translation models need to acquire the capacity of modeling long-range context for learning translation consistency and lexical choice."
  - [section 2.1]: "The final testset contains two references... For Testf inal, around 20 consecutive chapters from each book are selected, treating all chapters within a book as a long document."
- Break condition: The discourse-level context is too complex or sparse to be effectively modeled, or the model architecture is insufficient for capturing long-range dependencies.

### Mechanism 3
- Claim: Human evaluation with MQM is more reliable than automatic metrics for literary translation.
- Mechanism: MQM's multidimensional error taxonomy captures the nuanced quality aspects of literary translation that automatic metrics like BLEU or COMET may miss.
- Core assumption: Human evaluators are proficient in both source and target languages and follow the MQM guidelines consistently.
- Evidence anchors:
  - [section 3.2]: "The human evaluation was performed by professional translators using an adaptation of the multidimensional quality metrics (MQM) framework... We engaged four evaluators who are native English speakers and also fluent in Chinese."
  - [section 6.2]: "Among the baseline systems, the LLM system performs the best, whereas the MT system shows the poorest performance, diverging from the observations of automatic evaluation."
- Break condition: Human evaluators are inconsistent or biased, or the MQM framework does not capture the most important quality aspects of literary translation.

## Foundational Learning

- Concept: Discourse-aware translation
  - Why needed here: Literary texts require understanding of long-range context for coherence and consistency, unlike sentence-level translation.
  - Quick check question: What are the key differences between sentence-level and document-level translation, and why is discourse-aware modeling important for literary translation?

- Concept: MQM evaluation framework
  - Why needed here: Literary translation quality cannot be adequately measured by automatic metrics alone, requiring human evaluation with a structured error taxonomy.
  - Quick check question: How does the MQM framework differ from traditional evaluation methods, and what are its key components and error types?

- Concept: Large language models in translation
  - Why needed here: LLMs have shown promise in literary translation, but their strengths and limitations need to be understood and leveraged effectively.
  - Quick check question: What are the key characteristics of LLMs that make them suitable for literary translation, and what are some potential challenges or limitations?

## Architecture Onboarding

- Component map: Translation model (e.g., Transformer, LLM) -> Discourse-aware context encoder -> MQM-based evaluation pipeline
- Critical path: The translation model and discourse-aware context encoder are the most critical components for achieving high-quality literary translation.
- Design tradeoffs: Using a large LLM may provide better translation quality but at the cost of increased computational resources and potential black-box behavior. Incorporating discourse-level context may improve coherence but may also introduce complexity and potential for error propagation.
- Failure signatures: Poor translation quality may manifest as grammatical errors, mistranslations, or loss of literary style. Failure to capture discourse-level context may result in inconsistent character names, plot points, or thematic elements across the document.
- First 3 experiments:
  1. Train a sentence-level baseline model (e.g., Transformer) on the GuoFeng Webnovel Corpus and evaluate its performance using both automatic and human metrics.
  2. Implement a simple discourse-aware context encoder (e.g., using attention mechanisms or recurrent layers) and evaluate its impact on translation quality compared to the baseline.
  3. Fine-tune a pre-trained LLM (e.g., GPT-3.5-turbo) on the GuoFeng Webnovel Corpus and compare its performance to the Transformer-based models using both automatic and human evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different error severity levels (Neutral, Minor, Major, Critical) impact the overall translation quality assessment, and what is the optimal threshold for each level?
- Basis in paper: [explicit] The paper defines four error severity levels (Neutral, Minor, Major, Critical) with different penalty scores (0/5/10/25) and discusses their impact on the overall quality score.
- Why unresolved: The paper does not provide a detailed analysis of how these severity levels affect the final translation quality assessment or suggest an optimal threshold for each level.
- What evidence would resolve it: A study that analyzes the distribution of errors across different severity levels and their impact on the overall quality score, along with a proposed threshold for each level based on empirical data.

### Open Question 2
- Question: How do the proposed evaluation criteria and error typologies for literary translation compare to those used in other domains, such as news or technical documents, and what are the key differences?
- Basis in paper: [explicit] The paper introduces a well-defined criteria based on the MQM framework for literary translation, but does not compare it to other domains.
- Why unresolved: The paper does not provide a comparative analysis of the proposed criteria with those used in other translation domains, making it difficult to assess their generalizability and effectiveness.
- What evidence would resolve it: A comparative study that evaluates the performance of the proposed criteria in literary translation against those used in other domains, highlighting the key differences and their impact on translation quality.

### Open Question 3
- Question: How do the performance of different translation models (e.g., sentence-level, paragraph-level, document-level) vary across different literary genres, and what are the implications for future research and development?
- Basis in paper: [explicit] The paper introduces a document-level benchmark dataset and evaluation methodology, but does not provide a detailed analysis of how different models perform across various literary genres.
- Why unresolved: The paper does not discuss the implications of model performance differences across genres, which could inform future research and development efforts.
- What evidence would resolve it: A study that evaluates the performance of different translation models across various literary genres, providing insights into their strengths and weaknesses and suggesting directions for future research and development.

## Limitations

- The human evaluation sample size is relatively small (only 5 documents), which may not fully represent the diversity of discourse-level phenomena in web novels.
- The MQM framework adaptation for literary translation is novel, and its reliability across different evaluators and text types remains to be validated.
- The comparison between constrained and unconstrained systems is somewhat limited by the variability in how teams utilized external knowledge.

## Confidence

- High confidence: The GuoFeng Webnovel Corpus is a valuable contribution to the field, providing a large-scale, high-quality resource for discourse-level literary translation research. The overall trend of LLM performance exceeding traditional MT systems is well-supported by both automatic and human evaluation results.
- Medium confidence: The benefits of external knowledge are demonstrated, but the specific mechanisms and optimal strategies for leveraging external knowledge in literary translation require further investigation. The importance of discourse-level context is supported by the performance gap between document-level and sentence-level systems, but the optimal context modeling approach is still unclear.
- Low confidence: The relative ranking of individual systems and the precise contribution of specific design choices (e.g., fine-tuning strategies, context modeling techniques) are difficult to assess due to the variability in system configurations and the limited evaluation data.

## Next Checks

1. Conduct a larger-scale human evaluation with more diverse text samples and evaluators to validate the reliability of the MQM-based evaluation framework and the relative performance of different systems.
2. Perform an ablation study to isolate the impact of specific components (e.g., discourse-level context modeling, external knowledge integration, fine-tuning strategies) on translation quality, using a controlled experimental setup.
3. Analyze the types and frequencies of discourse-level phenomena (e.g., character names, plot points, thematic elements) in the GuoFeng Webnovel Corpus and their impact on translation quality, to inform future research directions and evaluation criteria.