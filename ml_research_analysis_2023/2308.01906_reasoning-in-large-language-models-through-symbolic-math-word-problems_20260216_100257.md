---
ver: rpa2
title: Reasoning in Large Language Models Through Symbolic Math Word Problems
arxiv_id: '2308.01906'
source_url: https://arxiv.org/abs/2308.01906
tags:
- symbolic
- numeric
- response
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reasoning in large language models (LLMs) for
  math word problems (MWPs) by evaluating both numeric answers and symbolic expressions
  that serve as concise explanations. The authors create a symbolic version of the
  SVAMP dataset and find that GPT-3's davinci-002 model achieves good zero-shot accuracy
  on symbolic MWPs (64.2%), comparable to its numeric accuracy (68.9%).
---

# Reasoning in Large Language Models Through Symbolic Math Word Problems

## Quick Facts
- arXiv ID: 2308.01906
- Source URL: https://arxiv.org/abs/2308.01906
- Reference count: 10
- Primary result: Self-prompting improves symbolic reasoning alignment from ~51% to 74% and achieves 71.7% symbolic accuracy, higher than both raw numeric (68.9%) and symbolic (64.2%) accuracies

## Executive Summary
This paper investigates reasoning in large language models for math word problems by evaluating both numeric answers and symbolic expressions as concise explanations. The authors create a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model achieves good zero-shot accuracy on symbolic MWPs (64.2%), comparable to its numeric accuracy (68.9%). To assess reasoning faithfulness, they introduce alignment metrics between numeric answers and symbolic expressions. A novel self-prompting approach, which first provides the numeric problem and model response before presenting the symbolic problem, significantly improves alignment from ~51% to 74% and surprisingly increases symbolic accuracy to 71.7%, higher than both raw numeric and symbolic accuracies. This demonstrates an ensembling effect where self-prompting combines the strengths of numeric and symbolic problem-solving.

## Method Summary
The paper evaluates GPT-3 davinci-002's ability to solve symbolic math word problems through zero-shot prompting. The authors create SVAMP-Sym by replacing numbers in SVAMP problems with variables (w,x,y,z). They implement a multi-stage pipeline: prompting the model with numeric problems, extracting and filtering numeric answers, then using self-prompting to improve symbolic reasoning. The filtering pipeline removes LaTeX artifacts, normalizes operators, and extracts the longest contiguous mathematical expression. Symbolic answers are compared through random variable substitution rather than string matching. The study evaluates three methods: raw numeric, raw symbolic, and self-prompting with optional alignment prompts.

## Key Results
- GPT-3 davinci-002 achieves 64.2% zero-shot accuracy on symbolic MWPs, comparable to 68.9% on numeric MWPs
- Self-prompting improves symbolic reasoning alignment from ~51% to 74% between numeric and symbolic outputs
- Self-prompting increases symbolic accuracy to 71.7%, higher than both raw numeric (68.9%) and symbolic (64.2%) accuracies
- The alignment prompt should be used conditionally rather than always or never for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
Self-prompting improves symbolic reasoning alignment by conditioning the model on its own numeric solution. The LLM first generates a verbose numeric response, then is prompted with that exact response before solving the symbolic problem. This creates an in-context learning signal where the symbolic output is encouraged to mirror the numeric reasoning path. Core assumption: The model's numeric reasoning is coherent and can be effectively transferred to the symbolic domain through contextual priming. Evidence anchors: [abstract] "self-prompting approach to encourage the symbolic reasoning to align with the numeric answer" and [section] "we propose a novel self-prompting approach that first prompts the LLM with the numeric problem and its response to the problem, and then asks it to solve the symbolic problem". Break condition: If the initial numeric response is incorrect or incoherent, the self-prompting signal may reinforce wrong reasoning rather than correct it.

### Mechanism 2
Self-prompting creates an ensembling effect by combining strengths of numeric and symbolic problem-solving. The symbolic solution after self-prompting inherits robustness from the numeric pathway while gaining the interpretability of symbolic expressions, leading to accuracy higher than either raw numeric or symbolic performance. Core assumption: Numeric and symbolic reasoning processes capture complementary aspects of the problem, and their combination through self-prompting yields a synergistic effect. Evidence anchors: [abstract] "self-prompting also improves the symbolic accuracy to 71.7%, higher than both the numeric and symbolic accuracies" and [section] "self-prompting has an ensembling effect". Break condition: If the numeric and symbolic reasoning pathways are too similar or redundant, the ensembling benefit may not materialize.

### Mechanism 3
The filtering step is crucial for symbolic accuracy because extracted expressions often contain extraneous text or formatting issues. A multi-stage filtering pipeline removes LaTeX artifacts, normalizes operators, and extracts the longest contiguous mathematical expression, significantly improving the quality of symbolic answers. Core assumption: The raw LLM output contains the correct symbolic expression but buried within verbose or malformed text. Evidence anchors: [section] "While there is a 4-5% drop in the numeric accuracy without filtering, the drop is 12-14% for symbolic problems" and [section] "Our extraction and filtering steps still have issues and there is scope for improvement". Break condition: If the filtering logic incorrectly truncates or modifies the correct symbolic expression, accuracy will degrade.

## Foundational Learning

- Concept: Zero-shot prompting with CoT
  - Why needed here: The paper evaluates GPT-3's zero-shot ability on symbolic MWPs, where CoT prompting significantly boosts accuracy
  - Quick check question: What accuracy difference does CoT make on numeric vs symbolic MWPs in the results?

- Concept: Expression evaluation for equivalence checking
  - Why needed here: Symbolic answers are compared through random variable substitution rather than string matching to handle multiple equivalent forms
  - Quick check question: How many random variable assignments are used to verify symbolic expression equivalence?

- Concept: Alignment metrics between numeric and symbolic outputs
  - Why needed here: The paper introduces alignment as a measure of reasoning faithfulness, requiring both outputs to compute to the same value
  - Quick check question: What baseline alignment score does raw symbolic output achieve before self-prompting?

## Architecture Onboarding

- Component map: Data layer (SVAMP-Sym dataset) -> Prompting engine (numeric prompt, symbolic prompt, self-prompting orchestration) -> LLM interface (GPT-3 davinci-002) -> Filtering pipeline (character replacement, de-latexifying, expression isolation) -> Evaluation module (accuracy counting, alignment checking, similarity metrics)

- Critical path: 1. Load SVAMP-Sym problem 2. Generate numeric response (with/without CoT) 3. Extract and filter numeric answer 4. Generate symbolic response (with self-prompting if enabled) 5. Extract and filter symbolic answer 6. Compute accuracy and alignment metrics 7. Aggregate results

- Design tradeoffs:
  - Filtering complexity vs. accuracy: More aggressive filtering risks losing correct expressions
  - Variable choice (w,x,y,z) vs. (i,j,k,l): Popular variables may perform better due to training data bias
  - Alignment prompt usage: Always including it decreases accuracy vs. conditional inclusion

- Failure signatures:
  - Low alignment with high numeric accuracy: Filtering or self-prompting may be incorrectly implemented
  - Symbolic accuracy much lower than numeric: Variable choice or filtering may need adjustment
  - BLEU/Levenshtein similarity low despite high alignment: Text generation may be verbose or inconsistent

- First 3 experiments:
  1. Run raw numeric evaluation with and without CoT to establish baseline
  2. Run raw symbolic evaluation with (w,x,y,z) variables to test zero-shot capability
  3. Implement self-prompting and compare alignment improvement against raw symbolic

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Results based on single LLM (GPT-3 davinci-002) and specific dataset (SVAMP), limiting generalizability
- Filtering pipeline introduces brittleness that may not be present in alternative implementations
- Focus on alignment as reasoning faithfulness proxy assumes numeric and symbolic solutions must be semantically equivalent

## Confidence

- **High confidence**: The basic observation that self-prompting improves alignment from ~51% to 74% is well-supported by the experimental design and results presented.
- **Medium confidence**: The claim that self-prompting achieves 71.7% symbolic accuracy (higher than both raw numeric and symbolic) is plausible given the alignment improvements, but depends heavily on the effectiveness of the filtering pipeline.
- **Low confidence**: The ensembling hypothesis - that self-prompting combines complementary strengths of numeric and symbolic reasoning - lacks direct empirical support in the paper.

## Next Checks

1. **Filtering pipeline robustness test**: Run the symbolic evaluation pipeline with and without filtering on a subset of examples where the correct symbolic expression is known. Compare the rate of correctly preserved expressions to quantify the filtering pipeline's impact on accuracy measurements.

2. **Variable choice ablation study**: Re-run the raw symbolic evaluation using alternative variable sets (i,j,k,l) versus the original (w,x,y,z) on the same problem subset. Measure whether the choice of variable names significantly affects zero-shot accuracy.

3. **Alignment prompt ablation**: Systematically vary the inclusion of the alignment prompt in the self-prompting pipeline. Test configurations: always include, never include, and conditionally include only when alignment score is below threshold. Compare resulting accuracies and alignments to verify the authors' finding that conditional inclusion performs best.