---
ver: rpa2
title: Recurrent Action Transformer with Memory
arxiv_id: '2306.09459'
source_url: https://arxiv.org/abs/2306.09459
tags:
- memory
- segment
- transformer
- arxiv
- rmdt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Recurrent Memory Decision Transformer
  (RMDT), a novel model architecture that incorporates a recurrent memory mechanism
  to address the quadratic complexity of the attention mechanism in transformers when
  dealing with long sequences. The RMDT uses memory tokens to regulate information
  retention and improve performance in memory-intensive environments.
---

# Recurrent Action Transformer with Memory

## Quick Facts
- arXiv ID: 2306.09459
- Source URL: https://arxiv.org/abs/2306.09459
- Authors: 
- Reference count: 40
- Primary result: Introduces Recurrent Memory Decision Transformer (RMDT) with recurrent memory tokens that improve performance in memory-intensive environments while maintaining or improving results in classic environments.

## Executive Summary
The Recurrent Memory Decision Transformer (RMDT) addresses the quadratic complexity of attention mechanisms in transformers when handling long sequences by introducing recurrent memory tokens. These tokens act as information carriers between segments of trajectory data, enabling the model to effectively utilize past information for current decision-making. The architecture demonstrates interpretable properties and consistently outperforms memoryless baselines on Atari games while showing significant improvements in memory-intensive environments like T-Maze and Memory Maze.

## Method Summary
RMDT extends the Decision Transformer architecture by inserting trainable memory embeddings at segment boundaries in trajectory sequences. During training, the model processes sequences in segments of length K, with N segments total and M memory tokens per segment. Memory tokens are detached after the first segment to enable stable training while allowing information flow across segments. The model uses triangular attention masks for trajectory tokens and full attention for memory tokens, enabling the transformer to attend to both current segment information and relevant historical context stored in memory tokens.

## Key Results
- RMDT outperforms Decision Transformer and Transformer-XL on Atari games with consistent performance gains across Seaquest, Qbert, and Breakout
- Memory tokens show interpretable attention patterns, with different tokens serving read/write roles across segments
- Performance improvements are most pronounced in memory-intensive environments (T-Maze, Memory Maze) while maintaining competitive performance in classic control tasks
- The model effectively scales context length by using memory tokens to carry information across segments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory tokens enable long-term dependency retention across segments by acting as recurrent state carriers.
- Mechanism: Memory embeddings are inserted at both the beginning and end of each segment. During training, the suffix of the memory subsequence from the previous segment is concatenated to the prefix of the next segment, allowing gradients to flow through memory and enabling learned retention of relevant past information.
- Core assumption: The recurrent memory mechanism can effectively encode and retrieve relevant information from previous segments when making current decisions.
- Evidence anchors:
  - [abstract] "The RMDT model demonstrates interpretable properties and consistently outperforms models without recurrent memory"
  - [section] "we empirically demonstrate that these tokens effectively retain information from previous contexts, allowing the model to effectively use past information when making decisions in the present"
  - [corpus] Weak evidence - only mentions "recurrent" in the title of one related paper
- Break condition: If memory tokens fail to retain relevant information across segments, the model performance would degrade to match models without memory, particularly on long-sequence tasks.

### Mechanism 2
- Claim: Segment-wise training with recurrent memory enables scaling context length beyond what standard transformers can handle efficiently.
- Mechanism: By dividing long sequences into segments and passing memory tokens between them, the model can effectively attend to a much longer context than the quadratic complexity of full attention would normally allow.
- Core assumption: The attention mechanism within each segment can effectively integrate information from both the current segment and the memory tokens carrying information from previous segments.
- Evidence anchors:
  - [abstract] "the quadratic complexity of the attention mechanism limits the potential for context expansion"
  - [section] "we have shown that it has interpretable properties" and "memory can significantly improve performance in memory-intensive environments"
  - [corpus] No direct evidence - the corpus mentions transformers for RL but doesn't specifically discuss context scaling
- Break condition: If the attention mechanism cannot effectively integrate information from both current segment and memory tokens, performance gains would not scale with increased segment count.

### Mechanism 3
- Claim: Memory tokens serve different roles - read tokens provide historical context while write tokens update stored information for future segments.
- Mechanism: The attention maps show that memory tokens at segment boundaries are used differently - some attend heavily to previous actions/states (read role) while others incorporate information from the full current segment to update stored state (write role).
- Core assumption: The model learns to differentiate between memory tokens that should preserve historical context versus those that should update stored information.
- Evidence anchors:
  - [section] "memory tokens are utilized throughout the entire sequence" and "the model effectively leverages the information stored within the corresponding memory segment"
  - [section] "when constructing a new memory subsequence for the subsequent context segment, the model considers the entire state sequence, including the existing memory segments"
  - [corpus] No direct evidence - corpus doesn't discuss differentiated roles of memory tokens
- Break condition: If memory tokens cannot differentiate their roles, the model would either overwrite useful historical information or fail to update stored state appropriately.

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how attention allows selective focus on relevant tokens is crucial for grasping how memory tokens can carry information across segments
  - Quick check question: How does the attention mechanism allow a transformer to focus on relevant tokens while ignoring others?

- Concept: Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs)
  - Why needed here: The paper addresses RL problems where decisions depend on past events, requiring understanding of how agents make decisions based on state observations
  - Quick check question: What is the key difference between an MDP and a POMDP that makes memory mechanisms important?

- Concept: Sequence modeling and autoregressive prediction
  - Why needed here: The model predicts actions in an autoregressive manner based on sequences of (reward, observation, action) triplets
  - Quick check question: How does autoregressive prediction differ from other prediction approaches in sequence modeling?

## Architecture Onboarding

- Component map:
  - Input encoders (rewards: linear, observations: CNN/linear, actions: linear) -> Memory embeddings (trainable vectors at segment boundaries) -> RMDT block (transformer with triangular and full attention masks) -> Output heads (linear projections from observation embeddings) -> Action predictions

- Critical path:
  1. Encode input (R, O, A) triplets
  2. Insert memory embeddings at segment boundaries
  3. Apply transformer with appropriate attention masks
  4. Extract observation embeddings from output
  5. Predict actions via linear projection
  6. Update memory embeddings for next segment

- Design tradeoffs:
  - Memory size vs. computational cost: More memory tokens allow storing more information but increase parameter count and computation
  - Segment overlap vs. consistency: Overlapping segments can improve smoothness but increase computational redundancy
  - Number of segments vs. context length: More segments enable longer effective context but may introduce instability if too many

- Failure signatures:
  - Performance plateaus or degrades with increasing segments: Indicates memory tokens aren't effectively carrying information
  - Attention maps show uniform distribution across all tokens: Suggests model isn't learning to differentiate memory token roles
  - Training instability with segment overlay: May indicate conflicts between overlapping context

- First 3 experiments:
  1. Ablation study: Train with and without memory tokens while keeping all other hyperparameters constant to verify memory contribution
  2. Context scaling: Increase number of segments while monitoring performance to find optimal context length
  3. Memory size variation: Test different numbers of memory embeddings (e.g., 1, 15, 30) to determine optimal memory capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RMDT scale with the number of segments in more complex environments compared to Atari and MuJoCo?
- Basis in paper: [inferred] The paper mentions that RMDT shows improved results by extending the effective context to 150 frames in three out of four Atari environments, but this effect only manifests itself when scaling the number of segments. However, the paper does not explore more complex environments beyond Atari and MuJoCo.
- Why unresolved: The paper only tested RMDT on Atari games and MuJoCo control tasks, which are relatively simple environments. More complex environments, such as those involving robotics or real-world applications, may require a greater number of segments to achieve optimal performance.
- What evidence would resolve it: Conducting experiments on more complex environments, such as robotic control tasks or real-world applications, and measuring the performance of RMDT with varying numbers of segments.

### Open Question 2
- Question: What is the impact of different memory mechanisms on the performance of RMDT in offline reinforcement learning tasks?
- Basis in paper: [explicit] The paper introduces RMDT, which uses a recurrent memory mechanism, and compares it to models without memory mechanisms (DT, Transformer-XL, and TAP). However, it does not explore other memory mechanisms, such as those proposed in Transformer XL, Compressive Transformer, or ERNIE-Doc.
- Why unresolved: The paper focuses on a specific memory mechanism (recurrent memory tokens) and does not investigate other memory mechanisms that have been proposed for transformers.
- What evidence would resolve it: Implementing and comparing RMDT with other memory mechanisms, such as those proposed in Transformer XL, Compressive Transformer, or ERNIE-Doc, on the same set of offline reinforcement learning tasks.

### Open Question 3
- Question: How does the choice of memory size affect the interpretability of RMDT's attention maps in different environments?
- Basis in paper: [explicit] The paper investigates the effect of memory size on model performance and finds that increasing the length of the memory subsequence did not improve results for three out of four Atari environments. However, it does not explore the impact of memory size on the interpretability of attention maps.
- Why unresolved: The paper focuses on the performance impact of memory size but does not examine how different memory sizes affect the interpretability of attention maps, which could provide insights into the model's decision-making process.
- What evidence would resolve it: Analyzing the attention maps of RMDT with different memory sizes in various environments and assessing the interpretability and consistency of the patterns observed.

## Limitations
- Memory token update mechanism is not fully specified, leaving uncertainty about initialization and training procedures
- Performance improvements on Atari games are modest (10-15%) compared to baseline Decision Transformer
- Interpretability claims rely primarily on qualitative attention map analysis without quantitative validation metrics

## Confidence
- **High Confidence**: The architectural design and implementation details for segment-wise training with memory tokens are well-specified and reproducible
- **Medium Confidence**: The empirical results demonstrating improved performance on memory-intensive environments are convincing, though the magnitude of improvement varies across tasks
- **Low Confidence**: The claim about interpretable properties of memory tokens and their differentiated roles (read vs. write) relies primarily on qualitative attention map analysis without rigorous quantitative validation

## Next Checks
1. **Quantitative Memory Retention Analysis**: Implement a systematic evaluation that measures how much information from previous segments is actually retained in memory tokens by comparing prediction accuracy when using memory tokens versus when they are randomly initialized or when only current segment information is available.

2. **Ablation on Memory Token Count**: Conduct experiments varying the number of memory tokens (e.g., 1, 5, 15, 30) across different environment complexities to establish whether more memory tokens consistently lead to better performance, or if there's an optimal capacity that depends on task memory requirements.

3. **Cross-Environment Generalization Test**: Train RMDT on environments with varying memory demands (low, medium, high) and evaluate its ability to generalize to novel tasks within each category to validate whether the model truly learns to utilize memory tokens adaptively rather than memorizing task-specific patterns.