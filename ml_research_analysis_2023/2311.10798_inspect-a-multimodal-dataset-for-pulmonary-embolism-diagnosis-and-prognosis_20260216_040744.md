---
ver: rpa2
title: 'INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis'
arxiv_id: '2311.10798'
source_url: https://arxiv.org/abs/2311.10798
tags:
- data
- pulmonary
- dataset
- each
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INSPECT is a large multimodal medical dataset for pulmonary embolism
  (PE) research, containing 23,248 CT scans, radiology reports, and structured EHR
  data from 19,402 patients. It includes diagnostic and prognostic labels for PE,
  in-hospital mortality, readmission, and pulmonary hypertension.
---

# INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis

## Quick Facts
- arXiv ID: 2311.10798
- Source URL: https://arxiv.org/abs/2311.10798
- Reference count: 40
- Key outcome: Multimodal fusion improves PE diagnosis performance but not prognostic tasks.

## Executive Summary
INSPECT is a large multimodal medical dataset containing 23,248 CT scans, radiology reports, and structured EHR data from 19,402 patients for pulmonary embolism (PE) research. The dataset includes diagnostic and prognostic labels for PE, in-hospital mortality, readmission, and pulmonary hypertension. The study benchmarks various modeling approaches including image-only, EHR-only, and multimodal fusion models. Results show that multimodal fusion improves PE diagnosis performance but not prognostic tasks, demonstrating the complementary nature of CT imaging and EHR data for acute diagnosis while highlighting limitations for long-term outcome prediction.

## Method Summary
The study creates INSPECT by integrating CT pulmonary angiograms, radiology report impressions, and structured EHR data from Stanford Health Care. Data preprocessing includes CT windowing and resizing, EHR featurization using both count-based methods and MOTOR embeddings, and NLP-based label extraction for PE diagnosis. Models are trained using CT slice encoders (ResNetV2/BiT), sequence encoders (LSTM/GRU/Transformer), and EHR models (LightGBM, MOTOR). Late fusion is applied by weighted averaging of model probabilities. Performance is evaluated using AUROC, AUPRC, and ECE metrics across diagnostic and prognostic tasks.

## Key Results
- Multimodal fusion improves diagnostic PE performance by 1-3% AUROC over single-modality approaches
- EHR models outperform CT models on prognostic tasks, while CT excels at diagnosis
- MOTOR embeddings outperform count-based EHR featurization on prognostic tasks
- Late fusion via weighted averaging is an effective strategy for multimodal PE diagnosis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves diagnostic accuracy for pulmonary embolism but not for prognostic tasks.
- Mechanism: CT imaging captures acute PE features (e.g., emboli location, clot burden) that directly inform diagnosis, while EHRs encode longitudinal risk factors (e.g., prior diagnoses, lab trends). Fusion leverages complementary information for diagnosis but not prognosis because CT lacks long-term outcome markers.
- Core assumption: CT scans are informative for acute PE diagnosis but not for predicting long-term complications like pulmonary hypertension.
- Evidence anchors:
  - [abstract]: "Our experiments show that multimodal fusion improves PE diagnosis performance but not prognostic tasks."
  - [section 5]: "We find that model fusion between the CT model and the structured EHR models helps improve performance on the diagnostic PE task but does not improve performance on the prognostic tasks."
  - [corpus]: "CT scans along with electronic heath records are not always sufficient for the diagnosis of PE" (PECon paper) — supports complementary use for diagnosis.
- Break condition: If CT imaging data were found to contain long-term outcome markers (e.g., chronic vascular changes predictive of PH), fusion could improve prognostic tasks.

### Mechanism 2
- Claim: Structured EHR foundation models (e.g., MOTOR) outperform simple count-based EHR featurization on prognostic tasks.
- Mechanism: MOTOR learns temporal patterns and embeddings from longitudinal EHR sequences, capturing complex interactions between clinical events over time, whereas count-based featurization only captures aggregate feature presence.
- Core assumption: Temporal dynamics in EHR data are critical for predicting future clinical outcomes.
- Evidence anchors:
  - [section 4.3.2]: "MOTOR was pretrained using time-to-event tasks, making it well-aligned with our desired prognostic prediction tasks."
  - [section 5]: "The structured EHR models perform better on prognostic tasks while the CT model performs better on the diagnostic PE task."
  - [corpus]: "Using a large language model to predict pulmonary embolism phenotype in the MIMIC-IV dataset" (MIMIC-IV-Ext-PE) — supports advanced EHR modeling for PE tasks.
- Break condition: If prognostic tasks could be solved effectively using only recent or static EHR features, the added complexity of MOTOR might not be justified.

### Mechanism 3
- Claim: Late fusion via weighted averaging of model probabilities is an effective strategy for multimodal PE diagnosis.
- Mechanism: Individual modality models (CT-only, EHR-only) capture modality-specific signals; weighted averaging learns optimal combination weights on validation data, improving overall performance without complex architectural integration.
- Core assumption: Modality-specific predictions are conditionally independent enough that a linear combination is effective.
- Evidence anchors:
  - [section 4.3.3]: "Our fusion models (see Figure 3) aggregate prediction probabilities from individual models by taking a weighted mean of these probabilities of each single-modality model."
  - [section 5]: "We find that model fusion between the CT model and the structured EHR models helps improve performance on the diagnostic PE task..."
  - [corpus]: "Contrastive Pretraining to Enhance Feature Alignment between CT and EHR Data" (PECon) — supports multimodal alignment, though via a different method.
- Break condition: If modality predictions are highly correlated or redundant, simple late fusion may not yield gains and more sophisticated fusion (e.g., attention-based) might be needed.

## Foundational Learning

- Concept: Pulmonary Embolism (PE) diagnosis and prognosis
  - Why needed here: The dataset and tasks are centered on PE; understanding the clinical context is essential for interpreting model outputs and designing appropriate experiments.
  - Quick check question: What are the primary imaging and clinical features used to diagnose acute PE, and how do they differ from those used to predict long-term outcomes like pulmonary hypertension?

- Concept: Multimodal medical data fusion
  - Why needed here: The core innovation is combining imaging and EHR data; understanding fusion strategies and their trade-offs is key to replicating and extending the work.
  - Quick check question: What are the main approaches to multimodal fusion in medical AI, and when is late fusion preferable to early or joint fusion?

- Concept: Time-to-event modeling in EHR data
  - Why needed here: Prognostic tasks require predicting future events; familiarity with survival analysis and time-aware modeling is necessary for proper label definition and model training.
  - Quick check question: How do you handle censored data in time-to-event prediction, and what are common evaluation metrics for such tasks?

## Architecture Onboarding

- Component map: DICOM CTPA -> CT windowing, resizing, normalization -> CT slice encoder (ResNetV2/BiT) -> CT model predictions
- Component map: Structured EHR -> Count-based or MOTOR featurization -> Sequence encoder (LSTM/GRU/Transformer) -> EHR model predictions
- Component map: Radiology reports -> NLP extraction -> PE labels
- Component map: CT predictions + EHR predictions -> Weighted averaging (late fusion) -> Final multimodal predictions

- Critical path:
  1. Load and preprocess CTPA and EHR data for a patient
  2. Generate or load modality-specific predictions (CT model, EHR models)
  3. Apply late fusion to combine predictions
  4. Evaluate on held-out test set
  5. Compare performance across tasks and modalities

- Design tradeoffs:
  - Simple count-based EHR featurization vs. complex MOTOR embeddings (speed vs. accuracy)
  - Late fusion vs. early/joint fusion (simplicity vs. potential performance gains)
  - Fixed time windows for EHR features vs. full longitudinal encoding (consistency vs. information loss)

- Failure signatures:
  - EHR models underperforming on diagnostic tasks: indicates CT is the dominant modality for PE detection
  - Fusion not improving prognostic tasks: suggests CT lacks long-term outcome information or models are not well-aligned
  - High ECE: indicates model calibration issues, possibly due to domain shift or label noise

- First 3 experiments:
  1. Train and evaluate CT-only, EHR-only (LightGBM), and EHR-only (MOTOR) models on the PE diagnostic task; compare AUROC.
  2. Apply late fusion to CT and both EHR models on the same task; measure improvement over individual models.
  3. Repeat steps 1-2 for one prognostic task (e.g., 12-month mortality); analyze why fusion does or doesn't help.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does multimodal fusion improve diagnostic PE performance but not prognostic task performance?
- Basis in paper: [explicit] "Our preliminary findings suggest that the integration of medical imaging and structured EHRs improves performance in diagnosing PE. However, we also find that incorporating medical imaging for prognostic tasks does not improve predictive performance, especially on the important pulmonary hypertension prediction task."
- Why unresolved: The paper identifies this as a discrepancy with domain knowledge where imaging contains prognostic information, but doesn't explain why models fail to leverage this signal.
- What evidence would resolve it: Systematic ablation studies testing different fusion strategies, analysis of which specific imaging features are being missed, or controlled experiments comparing human radiologists' use of imaging for prognosis versus model performance.

### Open Question 2
- Question: Can the dataset's performance generalize to other patient populations and healthcare systems?
- Basis in paper: [explicit] "Firstly, INSPECT only contains data from a single site (Stanford Health Care), and models trained on INSPECT may not generalize to other patient populations."
- Why unresolved: The dataset has not been validated on external populations, and no cross-site validation studies have been conducted.
- What evidence would resolve it: External validation studies using INSPECT-trained models on data from different healthcare systems, demographic analyses showing model performance across different subgroups, or transfer learning experiments using data from other institutions.

### Open Question 3
- Question: How accurate are the NLP-derived PE labels compared to manual chart review?
- Basis in paper: [explicit] "labels are assigned based on NLP output and source EHR data, not manual chart review, and thus may be inaccurate in some cases."
- Why unresolved: While the paper validates NLP performance on a subset, it hasn't compared the full dataset labels against comprehensive manual review.
- What evidence would resolve it: Manual chart review validation of a random sample of 1000+ cases from the full dataset, inter-rater reliability studies comparing multiple human reviewers, or correlation analysis between NLP labels and downstream clinical outcomes.

## Limitations
- Single-site data limits generalizability to other healthcare systems and patient populations
- NLP-derived labels may contain inaccuracies compared to manual chart review
- CT windowing parameters may not capture chronic vascular changes relevant to prognostic tasks

## Confidence

### Claim Confidence
- **Diagnostic performance claims**: High confidence - supported by multiple ablation studies and consistent improvements across different EHR baselines.
- **Prognostic performance claims**: Medium confidence - results are clear but mechanistic explanation for why fusion doesn't help is incomplete.
- **Dataset representativeness**: Medium confidence - while the dataset is large, it comes from a single health system, and the preview subset for reproducibility testing may not capture full data characteristics.

## Next Checks

1. **CT windowing sensitivity analysis**: Test whether different CT windowing parameters (currently fixed at center -500, width 1500) affect the diagnostic vs. prognostic performance gap, as chronic vascular changes might be visible at different settings.

2. **Longitudinal EHR modeling comparison**: Compare MOTOR against alternative temporal models (e.g., transformer-based architectures) on prognostic tasks to determine if the limitation is specific to MOTOR or inherent to current EHR modeling approaches.

3. **Multimodal alignment evaluation**: Assess whether CT and EHR embeddings are well-aligned in representation space using metrics like modality similarity scores, as poor alignment could explain why late fusion fails for prognostic tasks.