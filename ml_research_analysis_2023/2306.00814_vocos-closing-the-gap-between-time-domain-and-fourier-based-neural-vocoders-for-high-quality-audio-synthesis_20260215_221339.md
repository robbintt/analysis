---
ver: rpa2
title: 'Vocos: Closing the gap between time-domain and Fourier-based neural vocoders
  for high-quality audio synthesis'
arxiv_id: '2306.00814'
source_url: https://arxiv.org/abs/2306.00814
tags:
- audio
- speech
- ocos
- phase
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Vocos, a novel neural vocoder that directly
  generates Fourier spectral coefficients instead of time-domain audio samples. The
  core idea is to leverage the inductive bias offered by time-frequency representations
  and use a frequency-aware generator to model the distribution of spectral coefficients.
---

# Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis

## Quick Facts
- arXiv ID: 2306.00814
- Source URL: https://arxiv.org/abs/2306.00814
- Authors: 
- Reference count: 31
- Key outcome: Vocos achieves state-of-the-art audio quality by directly generating Fourier spectral coefficients, achieving an order of magnitude increase in speed compared to time-domain neural vocoding approaches.

## Executive Summary
Vocos introduces a novel neural vocoder architecture that directly generates Fourier spectral coefficients instead of time-domain audio samples. By leveraging the inductive bias offered by time-frequency representations and using a frequency-aware generator to model the distribution of spectral coefficients, Vocos effectively mitigates periodicity issues commonly observed in time-domain GANs. The proposed model demonstrates significant computational efficiency advantages over traditional time-domain methods by utilizing inverse fast Fourier transform for upsampling.

## Method Summary
Vocos uses a GAN framework with a ConvNeXt backbone that maintains constant temporal resolution across all layers. The generator directly outputs STFT or MDCT coefficients using carefully designed activation functions (exponential for magnitude, atan2 for phase) to provide suitable inductive bias. The model is trained on the LibriTTS dataset with reconstruction loss, adversarial loss using hinge formulation, and feature matching loss. The architecture avoids learnable upsampling layers by using inverse Fourier transforms, achieving significant speed improvements.

## Key Results
- Vocos achieves state-of-the-art audio quality metrics (UTMOS, PESQ, V/UV F1, periodicity error, LSTFT)
- Computational efficiency improves by an order of magnitude compared to time-domain vocoders
- Frequency-aware generator design effectively mitigates periodicity artifacts common in time-domain GANs
- Model achieves zero-shot cross-speaker generation capability with no performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling STFT coefficients directly in the frequency domain eliminates the need for complex upsampling layers and their associated aliasing artifacts.
- Mechanism: By maintaining constant temporal resolution across all network layers and using inverse Fourier transforms for upsampling, Vocos avoids redundant and computationally intensive operations present in time-domain GAN vocoders.
- Core assumption: The inverse Fourier transform can perfectly reconstruct the time-domain signal from its STFT coefficients when proper overlap-add conditions are met.
- Evidence anchors:
  - [abstract]: "achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches"
  - [section]: "Vocos avoids this problem by eliminating learnable upsampling layers, and instead employs the well-established inverse Fourier transform to reconstruct the original-scale waveform"
- Break condition: If the overlap-add constraint is not satisfied, perfect reconstruction is not possible, leading to artifacts in the generated audio.

### Mechanism 2
- Claim: Using carefully designed activation functions provides suitable inductive bias for modeling spectral coefficients.
- Mechanism: The model uses exponential and cosine functions to map hidden activations to magnitude and phase values that naturally fit the properties of STFT coefficients, ensuring phase wrapping is handled correctly.
- Core assumption: The exponential function can map unbounded real values to positive magnitudes, and the cosine function can map real values to the range [-1, 1] for phase representation.
- Evidence anchors:
  - [abstract]: "proposes Vocos, a novel neural vocoder that directly generates Fourier spectral coefficients instead of time-domain audio samples"
  - [section]: "Crucially, we map p onto the unit circle by calculating the cosine and sine of p to obtain x and y, respectively"
- Break condition: If the activation functions do not provide the necessary inductive bias, the model may struggle to learn the distribution of spectral coefficients effectively.

### Mechanism 3
- Claim: The isotropic architecture of Vocos, with constant temporal resolution across all layers, simplifies the model design and improves efficiency.
- Mechanism: By avoiding the need for transposed convolutions and their associated upsampling, the model can focus on learning the distribution of spectral coefficients without the complexity of handling aliasing artifacts.
- Core assumption: The ConvNeXt backbone can effectively learn features at a constant temporal resolution, making the model agnostic to the specific input and output sample rates.
- Evidence anchors:
  - [abstract]: "utilizing inverse fast Fourier transform for upsampling, achieving an order of magnitude increase in speed"
  - [section]: "This design, known as an isotropic architecture, has been found to work well in various settings, including Transformer"
- Break condition: If the isotropic architecture is not suitable for the specific task, the model may not be able to capture the necessary temporal dependencies in the audio signal.

## Foundational Learning

- Concept: Short-Time Fourier Transform (STFT)
  - Why needed here: STFT is the primary representation used by Vocos to model the distribution of spectral coefficients in the frequency domain.
  - Quick check question: What is the main advantage of using STFT over the regular Fourier Transform for audio signals?

- Concept: Modified Discrete Cosine Transform (MDCT)
  - Why needed here: MDCT is an alternative representation explored in Vocos, offering a more compact representation but with different properties compared to STFT.
  - Quick check question: How does the MDCT differ from the STFT in terms of the number of output coefficients and the input samples required?

- Concept: Constant Overlap-Add (COLA) constraint
  - Why needed here: The COLA constraint ensures that the inverse Fourier transform can perfectly reconstruct the original time-domain signal from its STFT coefficients.
  - Quick check question: What condition must the analysis window satisfy to ensure successful reconstruction using the inverse Fourier transform?

## Architecture Onboarding

- Component map: Mel-spectrogram features -> ConvNeXt backbone -> Activation functions (exponential/atan2) -> Inverse Fourier transform -> Audio output
- Critical path: Input features → ConvNeXt backbone → Activation functions → Inverse Fourier transform → Audio output
- Design tradeoffs:
  - Using inverse Fourier transforms instead of transposed convolutions simplifies the model but requires careful handling of the overlap-add constraint
  - The choice between STFT and MDCT representations involves a tradeoff between redundancy and compactness
- Failure signatures:
  - Artifacts in the generated audio may indicate issues with the inverse Fourier transform or the overlap-add constraint
  - Poor quality in the frequency domain may suggest problems with the activation functions or the ConvNeXt backbone
- First 3 experiments:
  1. Train Vocos with only the ISTFT head on a small dataset to verify the basic functionality of the inverse Fourier transform
  2. Compare the performance of Vocos using STFT versus MDCT representations on a larger dataset to assess the impact of redundancy
  3. Evaluate the effect of different activation functions (e.g., exponential, cosine) on the quality of the generated audio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Vocos compare to time-domain vocoders when generating out-of-domain audio, such as music or environmental sounds, rather than speech?
- Basis in paper: [inferred] The paper primarily focuses on speech synthesis using the LibriTTS dataset and evaluates Vocos on singing voice samples. It mentions that periodicity artifacts are commonly observed in time-domain GANs when generating out-of-domain audio.
- Why unresolved: The paper does not provide a comprehensive evaluation of Vocos on diverse audio types beyond speech and singing voice.
- What evidence would resolve it: Experiments comparing Vocos to state-of-the-art time-domain vocoders on various out-of-domain audio datasets, such as music or environmental sounds, would provide evidence of its generalization capabilities.

### Open Question 2
- Question: Can the frequency-aware generator design of Vocos be effectively applied to other audio synthesis tasks, such as music generation or audio restoration?
- Basis in paper: [explicit] The paper mentions that the open-sourcing of Vocos allows for further exploration and application in various audio processing tasks.
- Why unresolved: The paper focuses on speech synthesis and does not explore the potential applications of Vocos in other audio synthesis tasks.
- What evidence would resolve it: Demonstrating the effectiveness of Vocos in tasks like music generation or audio restoration by applying the frequency-aware generator design and comparing the results with existing methods would provide evidence of its broader applicability.

### Open Question 3
- Question: How does the choice of time-frequency representation (e.g., STFT vs. MDCT) impact the performance of Vocos in different audio synthesis scenarios?
- Basis in paper: [explicit] The paper discusses the properties of STFT and MDCT and their implications for generative modeling with GANs. It mentions that MDCT variants demonstrate weaker performance compared to STFT-based counterparts.
- Why unresolved: The paper does not explore the impact of different time-frequency representations on Vocos's performance in various audio synthesis scenarios.
- What evidence would resolve it: Conducting experiments using Vocos with different time-frequency representations (e.g., STFT, MDCT) on diverse audio synthesis tasks and comparing the results would provide insights into the impact of representation choice on performance.

## Limitations
- The reliance on inverse Fourier transforms requires proper overlap-add conditions to be satisfied, which is assumed but not empirically validated
- Computational efficiency claims are based on theoretical FLOPs and wall-clock time on a single GPU without exploring different hardware configurations or batch sizes
- Generalization capability to unseen speakers and acoustic conditions is not thoroughly evaluated, with focus primarily on the LibriTTS dataset

## Confidence
- High confidence: The core architectural design of Vocos using frequency-aware generators with appropriate activation functions (exponential for magnitude, atan2 for phase) is theoretically sound and well-motivated by the properties of STFT coefficients
- Medium confidence: The claim of superior audio quality is supported by objective metrics (UTMOS, PESQ, V/UV F1) but lacks comprehensive subjective evaluation across diverse speaker characteristics and acoustic conditions
- Low confidence: The generalization capability of Vocos to unseen speakers and acoustic conditions is not thoroughly evaluated

## Next Checks
1. **Reconstruction Validation**: Implement a systematic check to verify the COLA constraint is satisfied for all generated outputs by measuring the reconstruction error between the original and reconstructed time-domain signals across different spectral coefficient ranges
2. **Cross-Validation Study**: Evaluate Vocos on multiple datasets (e.g., VCTK, LJSpeech, and a noisy speech corpus) to assess generalization performance and identify any dataset-specific limitations in handling different acoustic conditions
3. **Latency Characterization**: Conduct a comprehensive latency analysis across different hardware configurations (CPU, GPU, mobile devices) and batch sizes to verify the claimed computational efficiency benefits hold in practical deployment scenarios beyond the controlled experimental setup