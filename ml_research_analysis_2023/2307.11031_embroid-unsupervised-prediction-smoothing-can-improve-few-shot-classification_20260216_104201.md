---
ver: rpa2
title: 'Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification'
arxiv_id: '2307.11031'
source_url: https://arxiv.org/abs/2307.11031
tags:
- embroid
- does
- prompt
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes Embroid, a method to improve prompt-based learning
  without additional labeled data by modifying the predictions of a language model
  (LM) rather than the prompt itself. The key idea is that accurate predictions should
  be consistent: samples similar under some feature representation should receive
  the same prompt prediction.'
---

# Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification

## Quick Facts
- **arXiv ID**: 2307.11031
- **Source URL**: https://arxiv.org/abs/2307.11031
- **Reference count**: 40
- **Primary result**: Embroid improves prompt-based learning by 7.3 F1 points on GPT-JT using unsupervised prediction smoothing

## Executive Summary
Embroid addresses the challenge of improving prompt-based learning without additional labeled data by modifying language model predictions rather than the prompts themselves. The method leverages the intuition that accurate predictions should be consistent across similar samples under embedding functions. Embroid identifies likely mispredictions by comparing LM predictions to those of neighboring samples, then uses a latent variable graphical model to combine original and smoothed predictions into a final corrected output. Empirical evaluation across six language models and 95 tasks demonstrates substantial improvements over original prompts, with average F1 gains of 7.3 points on GPT-JT and win-rates exceeding 80% on challenging tasks.

## Method Summary
Embroid improves LM predictions by first computing k-nearest neighbors for each sample under multiple embedding functions, then generating smoothed neighborhood predictions using majority voting weighted by neighborhood agreement. These predictions are combined with the original LM predictions using a latent variable graphical model that learns accuracy parameters for each source through weak supervision. The method operates without additional labeled data, making it suitable for few-shot learning scenarios where prompt engineering is expensive or impractical.

## Key Results
- Embroid improves average F1 by 7.3 points on GPT-JT across 95 tasks
- Win-rate exceeds 80% on challenging BIG-bench tasks
- Outperforms sophisticated prompting strategies like chain-of-thought
- Domain-specific embeddings provide additional gains for law and science tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Accurate predictions should be consistent with nearby samples under embedding functions
- **Mechanism**: Identifies mispredictions by retrieving k-nearest neighbors and computing smoothed neighborhood predictions via majority vote
- **Core assumption**: Embedding functions capture similarity that correlates with task labels
- **Evidence anchors**: Abstract intuition, section 4 description, weak corpus support
- **Break condition**: Poor embeddings that don't capture task-relevant similarity

### Mechanism 2
- **Claim**: Combining original LM predictions with smoothed neighborhood predictions via weak supervision improves final accuracy
- **Mechanism**: Uses latent variable graphical model to learn accuracy parameters and infer final labels
- **Core assumption**: Graphical model can accurately estimate source accuracies from unlabeled data
- **Evidence anchors**: Section 4 graphical model description, section 5 theoretical analysis
- **Break condition**: Very low original LM accuracy or poor embedding smoothness

### Mechanism 3
- **Claim**: Multiple embedding functions improve robustness compared to single embedding
- **Mechanism**: Computes predictions under N different embedding models and combines all predictions
- **Core assumption**: No single embedding perfectly captures all relevant similarities
- **Evidence anchors**: Section 4 multi-embedding description, section 6.3 domain-specific results
- **Break condition**: Highly correlated or noisy embeddings

## Foundational Learning

- **Concept**: Weak supervision and graphical models
  - **Why needed here**: Learns accuracy parameters for noisy sources without labels
  - **Quick check question**: What is the key insight that allows weak supervision to work without labeled data?

- **Concept**: Embedding similarity and smoothness
  - **Why needed here**: Assumes samples close in embedding space share labels
  - **Quick check question**: How does embedding smoothness relate to information gain from neighborhood predictions?

- **Concept**: Nearest neighbor search and majority voting
  - **Why needed here**: Creates smoothed predictions from neighbor LM predictions
  - **Quick check question**: What happens to majority vote accuracy as k increases?

## Architecture Onboarding

- **Component map**: Unlabeled dataset + LM predictions + N embedding models → k-NN computation → smoothed neighborhood votes → graphical model learning → final predictions
- **Critical path**: LM predictions → k-NN computation → smoothed neighborhood votes → graphical model learning → final predictions
- **Design tradeoffs**:
  - k (neighbors): Larger k increases vote stability but may include less similar samples
  - τ+ / τ- (shrinkage): Controls agreement threshold for generating neighborhood votes
  - Number of embeddings N: More embeddings improve robustness but increase computation
- **Failure signatures**:
  - Performance worse than base prompt: Poor embedding quality or overly aggressive smoothing
  - No improvement across tasks: Embeddings don't capture task-relevant similarity
  - High variance in results: Unstable k-NN computation or hyperparameter sensitivity
- **First 3 experiments**:
  1. Run Embroid with k=10, default shrinkage parameters on a single task to verify basic functionality
  2. Compare performance using one vs. multiple embeddings to test robustness benefit
  3. Sweep k from 5 to 50 to find optimal neighborhood size for your dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Embroid's performance change when applied to multi-class classification tasks instead of binary classification?
- **Basis in paper**: Paper focuses on binary classification and mentions multi-class extensions could be explored
- **Why unresolved**: No empirical results or theoretical analysis for multi-class tasks
- **What evidence would resolve it**: Empirical evaluation on multi-class tasks with varying numbers of classes

### Open Question 2
- **Question**: What is the impact of different embedding model architectures on Embroid's performance?
- **Basis in paper**: Uses different embedding models but doesn't analyze architectural impact
- **Why unresolved**: No comparison across different embedding model architectures
- **What evidence would resolve it**: Comparative study using different embedding architectures

### Open Question 3
- **Question**: How does Embroid's performance scale with increasing dataset size beyond the few-shot regime?
- **Basis in paper**: Focuses on few-shot regime, mentions performance on "small" datasets
- **Why unresolved**: No results for dataset sizes larger than a few hundred samples
- **What evidence would resolve it**: Evaluation on datasets with varying sizes (1000, 10000, 100000 samples)

## Limitations
- Performance critically depends on embedding quality and task-relevant similarity
- Sensitive to multiple hyperparameters (k, τ+, τ-) without comprehensive tuning guidance
- Computational complexity scales poorly with dataset size due to k-NN computations
- Theoretical analysis relies on idealized assumptions about embedding smoothness

## Confidence

**High Confidence**: Core claim of combining predictions improves accuracy is well-supported by extensive empirical results across 95 tasks with statistically significant win-rates and F1 gains.

**Medium Confidence**: Multiple embeddings provide robustness claim is supported but needs more rigorous ablation studies; theoretical analysis provides bounds but relies on idealized assumptions.

**Low Confidence**: Domain-specific embeddings outperform general embeddings is based on limited examples (law and science) without systematic evaluation across diverse domains.

## Next Checks

1. **Embedding Robustness Test**: Systematically evaluate Embroid performance using different combinations of embedding models on a representative task to quantify embedding quality impact.

2. **Hyperparameter Sensitivity Analysis**: Conduct grid search over k and τ parameters to identify optimal values for different task types and test generalization across tasks.

3. **Scalability Evaluation**: Measure runtime and memory requirements as dataset size increases, and evaluate performance degradation when using approximate nearest neighbor methods versus exact k-NN computation.