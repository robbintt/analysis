---
ver: rpa2
title: Optimal Decision Tree Policies for Markov Decision Processes
arxiv_id: '2301.13185'
source_url: https://arxiv.org/abs/2301.13185
tags:
- decision
- tree
- trees
- computer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces OMDT, a Mixed-Integer Linear Programming formulation
  for training optimal decision tree policies for Markov Decision Processes. OMDT
  maximizes expected discounted return while constraining the policy to a size-limited
  decision tree.
---

# Optimal Decision Tree Policies for Markov Decision Processes

## Quick Facts
- arXiv ID: 2301.13185
- Source URL: https://arxiv.org/abs/2301.13185
- Reference count: 26
- Key result: Depth-3 OMDTs often achieve near-optimal performance without sacrificing interpretability.

## Executive Summary
This paper introduces OMDT, a Mixed-Integer Linear Programming (MILP) formulation for training optimal decision tree policies for Markov Decision Processes (MDPs). OMDT directly maximizes the expected discounted return while constraining the policy to a size-limited decision tree. Experiments on 13 environments show that depth-3 OMDTs often achieve near-optimal performance, outperforming imitation learning methods like VIPER after sufficient runtime. The method proves optimality in 7 out of 13 environments and typically runs faster than VIPER, though runtime increases with problem complexity.

## Method Summary
OMDT formulates the problem of training optimal decision tree policies for MDPs as a Mixed-Integer Linear Program. The method maximizes the expected discounted return by directly reasoning over state-action frequencies in the dual linear program for MDPs. The decision tree structure is represented using binary threshold variables and path-following constraints. The MILP solver searches the entire space of decision tree policies within the size limit and can prove optimality by bounding the objective.

## Key Results
- OMDT depth-3 trees often perform close to optimal in 8 out of 13 environments.
- OMDTs typically run faster than VIPER but require more runtime for larger problems.
- OMDT proves optimality in 7 out of 13 environments tested.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct optimization of the decision tree policy yields higher returns than imitation learning in size-limited settings.
- Mechanism: OMDT formulates the problem as a Mixed-Integer Linear Program that maximizes the expected discounted return by directly reasoning over state-action frequencies in the dual linear program for MDPs.
- Core assumption: The dual LP formulation can accurately represent the constraints of a size-limited decision tree policy.
- Evidence anchors:
  - [abstract] "OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming."
  - [section] "By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimally."
- Break condition: If the dual LP constraints do not tightly capture the decision tree structure, the solver may produce policies that violate interpretability constraints.

### Mechanism 2
- Claim: Size-limited decision trees can achieve near-optimal performance without the interpretability-performance trade-off.
- Mechanism: By enforcing a depth limit (e.g., depth 3) through MILP constraints, OMDT finds compact policies that match the performance of unrestricted optimal policies in many environments.
- Core assumption: A small decision tree can represent the essential decision logic of the optimal policy for the given MDP.
- Evidence anchors:
  - [abstract] "we find that OMDTs limited to a depth of 3 often perform close to the optimal limit."
  - [section] "we show that decision trees of 7 decision nodes are enough to perform close to optimally in 8 out of 13 environments."
- Break condition: In environments where the optimal policy requires complex branching, even depth 3 trees cannot capture the needed logic, leading to performance loss.

### Mechanism 3
- Claim: OMDT provides algorithmic transparency by guaranteeing global optimality for the learned policy.
- Mechanism: The MILP solver searches the entire space of decision tree policies within the size limit and can prove optimality by bounding the objective.
- Core assumption: The solver can solve the MILP to optimality within practical runtime.
- Evidence anchors:
  - [abstract] "OMDT produces increasingly performant tree policies as runtime progresses and eventually proves the optimality of its policy."
  - [section] "We compare the performance of OMDT and VIPER on a variety of MDPs. Interestingly, we show that when training interpretable size-limited trees imitation learning performs significantly worse."
- Break condition: For large MDPs, the MILP may not solve to optimality within reasonable time, making the optimality claim impractical.

## Foundational Learning

- Concept: Mixed-Integer Linear Programming (MILP)
  - Why needed here: OMDT uses MILP to encode both the MDP dynamics and the decision tree constraints in a single optimization problem.
  - Quick check question: What are the decision variables in OMDT's MILP formulation?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The policy must be optimal for the given MDP defined by states, actions, transition probabilities, and rewards.
  - Quick check question: How does the dual LP formulation relate state-action frequencies to the policy?

- Concept: Decision Tree Structure and Constraints
  - Why needed here: The tree must be represented using binary threshold variables and path-following constraints.
  - Quick check question: How do the side(s,j,k) pre-computed values simplify the tree path constraints?

## Architecture Onboarding

- Component map:
  - MDP specification (S, A, P, R, p0, γ) -> OMDT MILP formulation -> Gurobi or similar MILP solver -> Optimal decision tree policy (variables: bm,j,k, ct,a, ds,m, πs,a, xs,a)

- Critical path:
  1. Preprocess MDP to remove unreachable states.
  2. Compute all possible thresholds for each feature.
  3. Build OMDT MILP with constraints (3)-(8).
  4. Solve with MILP solver.
  5. Extract tree structure from solution variables.

- Design tradeoffs:
  - Depth limit vs. performance: deeper trees may improve returns but reduce interpretability.
  - Runtime vs. optimality: proving optimality can take much longer than finding a good policy.
  - Big-M value selection: too large weakens the formulation; too small may cut off feasible solutions.

- Failure signatures:
  - Solver timeouts: MILP too large or complex.
  - Suboptimal policies: Big-M value too small or constraints too loose.
  - Memory errors: State space or action space too large for MILP.

- First 3 experiments:
  1. Run OMDT on frozenlake 4x4 with depth 1, 2, and 3; compare returns and runtime.
  2. Compare OMDT depth 3 vs. VIPER on blackjack; measure normalized returns and tree sizes.
  3. Test OMDT on inventory management with varying discount factors; observe effect on policy structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the scalability limit of OMDT in terms of state and action space sizes?
- Basis in paper: [explicit] The paper mentions that OMDT needs more than 2 hours to improve over VIPER on the large tictactoe instance and times out on some larger MDPs.
- Why unresolved: The paper only tested up to 2 hours of runtime and did not systematically explore how runtime scales with problem size.
- What evidence would resolve it: A systematic study of OMDT's performance and runtime on MDPs with varying state and action space sizes.

### Open Question 2
- Question: Can OMDT be extended to handle factored MDPs to scale to larger state spaces?
- Basis in paper: [explicit] The conclusion suggests that future work can incorporate factored MDPs into OMDT's formulation to scale up to larger state spaces.
- Why unresolved: The paper does not provide any implementation or results for factored MDPs.
- What evidence would resolve it: An extension of OMDT to handle factored MDPs and experimental results showing improved scalability.

### Open Question 3
- Question: How does the performance of OMDT compare to other interpretable RL methods beyond VIPER?
- Basis in paper: [explicit] The paper only compares OMDT to VIPER and dtcontrol, but mentions that there are other interpretable RL methods like differentiable decision trees.
- Why unresolved: The paper does not provide a comprehensive comparison with other interpretable RL methods.
- What evidence would resolve it: A thorough comparison of OMDT's performance and interpretability with other state-of-the-art interpretable RL methods on a variety of MDPs.

## Limitations

- OMDT's MILP formulation scales poorly with large state/action spaces and deep trees, making it impractical for complex real-world MDPs.
- While the paper claims to prove optimality in 7/13 environments, it's unclear whether this is due to problem tractability or solver limitations.
- The experiments cover 13 MDPs, but these are likely small, synthetic benchmarks; real-world domains may require much larger trees or hybrid approaches.

## Confidence

- **High**: OMDT directly optimizes the decision tree policy via MILP, leading to better performance than imitation learning in size-limited settings.
- **Medium**: Depth-3 OMDTs often achieve near-optimal returns, but this may not hold for more complex MDPs or deeper trees.
- **Low**: The optimality proofs are reliable across all problem scales, given the lack of external validation.

## Next Checks

1. **Scalability Test**: Apply OMDT to larger MDPs (e.g., gridworlds with >100 states) and measure runtime and memory usage.
2. **Optimality Validation**: Compare OMDT's claimed optimal policies against those from other exact methods (e.g., dynamic programming) on small MDPs.
3. **Generalization Study**: Evaluate OMDT on real-world MDPs (e.g., robotics or healthcare) to test robustness and interpretability.