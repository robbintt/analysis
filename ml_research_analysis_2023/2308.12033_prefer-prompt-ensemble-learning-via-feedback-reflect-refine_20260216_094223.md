---
ver: rpa2
title: 'PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine'
arxiv_id: '2308.12033'
source_url: https://arxiv.org/abs/2308.12033
tags:
- prompt
- ensemble
- boosting
- which
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations of existing prompt ensemble methods
  for large language models, which typically require manual effort to prepare prompts
  and cannot perform directed optimization. The authors propose PREFER, an automatic
  method that iteratively generates and refines prompts using a feedback-reflect-refine
  paradigm.
---

# PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine

## Quick Facts
- arXiv ID: 2308.12033
- Source URL: https://arxiv.org/abs/2308.12033
- Reference count: 8
- The paper introduces PREFER, an automatic prompt ensemble method that uses iterative feedback-reflect-refine to outperform baselines on multiple NLP tasks.

## Executive Summary
This paper addresses the limitations of existing prompt ensemble methods for large language models, which typically require manual effort to prepare prompts and cannot perform directed optimization. PREFER introduces an automatic method that iteratively generates and refines prompts using a feedback-reflect-refine paradigm. It uses the LLM to reflect on the inadequacies of current prompts and generate new ones focused on hard examples. A novel bilateral bagging approach incorporating forward and backward thinking is used to enhance stability. Experiments on various tasks show PREFER achieves state-of-the-art performance with significant improvements over baselines.

## Method Summary
PREFER is an automatic prompt ensemble learning method that uses an iterative feedback-reflect-refine paradigm to improve LLM performance without manual prompt engineering. The method initializes with a seed prompt and training data, then iteratively generates new prompts by having the LLM reflect on errors from previous iterations. A novel bilateral bagging approach evaluates responses from both forward and backward directions to counteract LLM overconfidence. Prompts are co-adapted through feedback and weight updates in each boosting iteration, focusing the ensemble on uncovered regions of the input space while reducing redundancy. The method is evaluated on multiple NLP tasks including SNLI, MNLI, QNLI, RTE, Ethos, Liar, and ArSarcasm using few-shot learning with k=50 examples.

## Key Results
- PREFER achieves state-of-the-art performance with significant improvements over baselines on reasoning and classification tasks
- The method demonstrates superior stability compared to iterative prompting methods
- Bilateral bagging with forward and backward thinking provides better stability than majority voting
- The approach shows strong performance across diverse datasets including SNLI, MNLI, QNLI, RTE, Ethos, Liar, and ArSarcasm

## Why This Works (Mechanism)

### Mechanism 1
- Iterative feedback-reflect-refine enables directed prompt optimization by aligning new prompts with previously failed examples
- Each iteration reflects on errors of current prompts and synthesizes new prompts focused on hard examples that previous prompts failed to solve
- Core assumption: LLM can accurately identify and articulate why certain examples fail and generate better prompts accordingly
- Evidence anchors: [abstract] "Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement"; [section] "Inspired by the fact that weak learners pay more attention to hard examples via weight redistribution during boosting, we propose to transfer this hard-sample-oriented weighting into nature language feedback"

### Mechanism 2
- Bilateral bagging improves stability by combining forward and backward confidence evaluation to counteract LLM overconfidence
- For each candidate answer, LLM provides forward confidence score and backward (elimination) confidence score
- Final prediction combines these two scores to offset overconfidence bias
- Core assumption: LLMs overestimate confidence and backward thinking can expose and correct this bias
- Evidence anchors: [abstract] "we propose a novel prompt bagging method involving forward and backward thinking"; [section] "Given the observed tendency of LLMs to overestimate confidence in their predictions (Zhao et al. 2021), our bilateral bagging approach assesses the responses from both forward and backward directions"

### Mechanism 3
- Joint optimization of prompt generation and ensemble boosting improves convergence speed and reduces manual effort
- Prompts are co-adapted through feedback and weight updates in each boosting iteration
- Ensemble focuses on uncovered regions of input space while reducing redundancy
- Core assumption: LLMs can handle dual task of solving downstream problem and generating improved prompts without catastrophic forgetting
- Evidence anchors: [abstract] "our PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners"; [section] "Iterating along this path, potential conflict and redundancy among prompts are reduced"

## Foundational Learning

- Concept: Boosting (AdaBoost) fundamentals
  - Why needed here: PREFER adapts AdaBoost's weight redistribution logic to prompt ensemble setting using error feedback to guide prompt generation
  - Quick check question: In AdaBoost, how are instance weights updated after each weak learner is added?

- Concept: Confidence calibration in language models
  - Why needed here: Bilateral bagging method relies on LLM providing both forward and backward confidence scores, assuming understanding of how LLM confidence estimates are miscalibrated
  - Quick check question: What is a common issue with LLM-generated confidence scores, and why does it matter for ensemble methods?

- Concept: Prompt engineering and in-context learning
  - Why needed here: PREFER operates directly on natural language prompts without fine-tuning, so understanding prompt formatting and demonstration-based learning is essential
  - Quick check question: What are the three key components of an effective prompt in few-shot learning?

## Architecture Onboarding

- Component map: Seed prompt storage -> LLM inference engine -> Feedback reflection generator -> Prompt synthesis module -> Bilateral bagging evaluator -> Weight update engine (AdaBoost-style) -> Training data weight tracker
- Critical path: 1) Feed initial prompt + training batch to LLM for solving; 2) Collect errors and build feedback prompt; 3) Reflect on errors to generate new prompt; 4) Apply bilateral bagging to stabilize predictions; 5) Update weights and repeat
- Design tradeoffs: Trade manual prompt crafting for automated generation (saves time but depends on LLM quality); Trade computational efficiency (bilateral bagging) for improved stability; Trade diversity of prompts (more prompts) for potential redundancy
- Failure signatures: Prompt quality degrades over iterations (mode collapse); Feedback reflection becomes repetitive or vague; Bilateral bagging scores do not differ (no confidence calibration effect); Weight updates oscillate or diverge
- First 3 experiments: 1) Run PREFER on Ethos dataset with 10-shot training; compare F1 vs single prompt baseline; 2) Remove feedback mechanism and run again; compare stability and convergence; 3) Replace bilateral bagging with majority voting; compare runtime and performance

## Open Questions the Paper Calls Out
The paper acknowledges that prompt ensemble methods, including PREFER, are more computationally intensive than single-prompt approaches and mentions this as a limitation in the conclusion section. While PREFER introduces bilateral bagging to improve efficiency compared to other ensemble methods, it still requires multiple iterations and prompt generations, which inherently increases computational cost compared to single-prompt methods.

## Limitations
- Method's success heavily depends on LLM's ability to accurately self-reflect and generate improved prompts
- Bilateral bagging mechanism's effectiveness relies on unproven assumption that LLM overconfidence can be corrected through backward thinking
- Approach requires multiple LLM API calls per iteration, potentially making it computationally expensive at scale
- Evaluation focuses on few-shot settings with k=50 examples, leaving uncertainty about performance in low-resource or high-resource scenarios

## Confidence
- Mechanism 1 (Iterative feedback-reflect-refine): Medium confidence - conceptual framework is sound but empirical validation of LLM self-reflection quality is limited
- Mechanism 2 (Bilateral bagging): Low confidence - overconfidence correction mechanism is proposed but not empirically demonstrated
- Mechanism 3 (Joint optimization): Medium confidence - design is plausible but trade-off between diversity and redundancy is not quantified

## Next Checks
1. Measure correlation between LLM-identified "hard examples" and actual prediction errors across multiple iterations; compare prompt quality metrics between PREFER-generated and human-engineered prompts
2. Conduct controlled experiments where LLM confidence scores are systematically calibrated against ground truth; measure whether forward and backward confidence scores meaningfully diverge
3. Profile number of LLM API calls per iteration and total runtime across different dataset sizes; compare cost-benefit ratio against single-prompt methods and other ensemble approaches at scale