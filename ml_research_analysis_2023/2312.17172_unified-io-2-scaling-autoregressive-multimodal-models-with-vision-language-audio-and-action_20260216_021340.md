---
ver: rpa2
title: 'Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language,
  Audio, and Action'
arxiv_id: '2312.17172'
source_url: https://arxiv.org/abs/2312.17172
tags:
- image
- audio
- video
- text
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Unified-IO 2 is the first autoregressive multimodal model capable
  of understanding and generating images, text, audio, and actions. It unifies diverse
  modalities by tokenizing inputs and outputs into a shared semantic space, then processing
  them with a single encoder-decoder transformer.
---

# Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action

## Quick Facts
- arXiv ID: 2312.17172
- Source URL: https://arxiv.org/abs/2312.17172
- Authors: 
- Reference count: 40
- Key outcome: First autoregressive multimodal model capable of understanding and generating images, text, audio, and actions, achieving state-of-the-art performance on GRIT benchmark and strong results across 35+ tasks.

## Executive Summary
Unified-IO 2 is a groundbreaking autoregressive multimodal model that unifies understanding and generation across text, images, audio, and actions within a single transformer architecture. By tokenizing all modalities into a shared semantic space and introducing architectural innovations like 2D rotary embeddings and QK normalization, the model achieves state-of-the-art performance on the GRIT benchmark while demonstrating strong capabilities across more than 35 diverse tasks. The approach overcomes significant training challenges associated with heterogeneous multimodal data through dynamic packing and careful normalization strategies.

## Method Summary
The model employs a unified encoder-decoder transformer architecture trained from scratch on a massive corpus containing 1 billion image-text pairs, 1 trillion text tokens, 180 million video clips, 130 million interleaved image-text, 3 million 3D assets, and 1 million agent trajectories. It uses modality-specific encoders (ViT for images, AST for audio) and decoders (VQ-GAN for images) connected through perceiver resamplers that compress high-dimensional inputs. A multimodal mixture of denoisers objective enables self-supervised learning across modalities. The model is instruction-tuned on 120 datasets covering 220 tasks before evaluation on various benchmarks.

## Key Results
- Achieves state-of-the-art performance on the GRIT benchmark for general-purpose multimodal tasks
- Demonstrates strong results across more than 35 benchmarks including image generation, vision-language understanding, video and audio understanding, and robotic manipulation
- Successfully unifies four modalities (text, images, audio, actions) in a single autoregressive architecture

## Why This Works (Mechanism)

### Mechanism 1
Dynamic packing enables 4x training throughput without cross-contamination. The heuristic algorithm pairs examples with similar token counts before the transformer stage, packing up to 2 examples into sequences of 864 input tokens and 1280 target tokens. This reduces sequence length and allows parallel processing. Core assumption: Examples can be efficiently matched based on token count without requiring exact knowledge of future examples.

### Mechanism 2
2D rotary embeddings stabilize training by modeling relative positions across modalities. For non-text modalities, RoPE is extended to 2D positions by splitting query and key embeddings and applying separate rotary embeddings constructed from each coordinate to the halves. Core assumption: Relative position information is more stable than absolute position embeddings when handling heterogeneous modalities.

### Mechanism 3
Scaled cosine attention in perceiver resampler prevents extreme attention logits when compressing multimodal features. LayerNorm is applied to queries and keys before dot-product attention, and scaled cosine attention is used in the perceiver to further normalize attention logits. Core assumption: Attention logits grow to extreme values when combining image and audio modalities, requiring stricter normalization than standard transformer layers.

## Foundational Learning

- **Tokenization across modalities**: Why needed here: The model processes all modalities as sequences of tokens in a shared semantic space. Quick check question: How are images, audio, and sparse structures converted to tokens before entering the transformer?

- **Multimodal mixture of denoisers objective**: Why needed here: Enables self-supervised learning signals across multiple modalities by combining denoising and generation paradigms. Quick check question: What are the [R], [S], and [X] paradigms in the multimodal training objective?

- **Autoregressive with dynamic masking**: Why needed here: Prevents information leakage during masked denoising while maintaining causal generation capabilities. Quick check question: How does masking the token in the decoder except when predicting that token avoid information leakage?

## Architecture Onboarding

- **Component map**: Input modality → modality-specific encoder → perceiver resampler (if history) → packed token sequence → transformer → output decoder

- **Critical path**: Input modality → modality-specific encoder → perceiver resampler (if history) → packed token sequence → transformer → output decoder

- **Design tradeoffs**:
  - Using pre-trained encoders (ViT, AST) vs. training from scratch: Provides better features but requires freezing during pre-training
  - Dynamic packing vs. static batching: Higher throughput but requires streaming data handling
  - 2D RoPE vs. absolute position embeddings: Better relative position modeling but more complex implementation

- **Failure signatures**:
  - Gradient explosion: Likely due to extreme attention logits in perceiver or other modalities
  - Poor generation quality: Could indicate VQ-GAN/ViT-VQGAN training issues or insufficient capacity
  - Training instability: May result from improper normalization or modality imbalance

- **First 3 experiments**:
  1. Train on single modality (text only) to verify baseline transformer training
  2. Add image modality with standard relative positional embeddings to test stability improvements
  3. Implement and test 2D RoPE on image modality to verify positional encoding changes

## Open Questions the Paper Calls Out

- **Open Question 1**: How would using a larger batch size during pre-training affect Unified-IO 2's performance across modalities? Basis: Paper states limited computational resources constrained hyperparameter exploration. Evidence needed: Training with larger batch size and comparing performance.

- **Open Question 2**: What is the impact of using base versions of ViT and AST models on overall performance? Basis: Paper states memory constraints limited model size. Evidence needed: Training with larger ViT/AST versions and comparing performance.

- **Open Question 3**: How does performance on GRIT compare using original UMD split for RefCOCO-g vs Google split? Basis: Paper uses Google split showing performance discrepancy. Evidence needed: Evaluating on UMD split and comparing results.

- **Open Question 4**: Impact of different VQ-GAN models on image generation quality? Basis: Paper empirically finds VQ-GAN best but mentions comparisons. Evidence needed: Training with different VQ-GAN models and comparing generation quality.

- **Open Question 5**: How does performance on depth estimation change with better normalization and metric evaluation? Basis: Paper shows weak depth estimation performance suggesting normalization issues. Evidence needed: Training with improved normalization and comparing depth estimation results.

## Limitations

- Data heterogeneity and quality concerns due to reliance on large, diverse corpus with potential label noise and domain mismatch
- Evaluation scope limited by relatively new GRIT benchmark and limited assessment of robotic manipulation and 3D asset generation tasks
- Scaling assumptions untested for transfer learning from pre-trained models or exploration of larger architectures

## Confidence

**High Confidence**:
- Model successfully unifies multiple modalities within single transformer architecture
- Dynamic packing improves training throughput by approximately 4x
- 2D rotary embeddings and QK normalization stabilize training on multimodal data

**Medium Confidence**:
- Architectural improvements are necessary and sufficient for handling heterogeneous modalities
- State-of-the-art performance on GRIT and other benchmarks is representative

**Low Confidence**:
- Performance on robotic manipulation and 3D asset generation generalizes to real-world scenarios
- Proposed architecture will scale effectively to larger models or additional modalities

## Next Checks

1. **Robustness to Data Heterogeneity**: Conduct controlled experiments by training the model on subsets of the multimodal corpus (e.g., only text and images, or only audio and video) to assess the impact of modality diversity on performance. Additionally, evaluate the model's robustness to noisy or imbalanced data by introducing synthetic label noise or domain shifts.

2. **Generalization to New Modalities**: Test the model's ability to handle a new modality (e.g., point clouds or infrared imagery) by freezing the existing architecture and fine-tuning only the modality-specific encoder and decoder. Measure the impact on performance and training stability compared to a baseline model without the architectural improvements.

3. **Benchmark Validation and Ablation**: Re-evaluate the model on a subset of standard benchmarks (e.g., VQA, image captioning, and audio classification) with publicly available datasets to verify the reported performance. Perform ablation studies to isolate the contributions of 2D RoPE, QK normalization, and scaled cosine attention, and assess their necessity for each modality.