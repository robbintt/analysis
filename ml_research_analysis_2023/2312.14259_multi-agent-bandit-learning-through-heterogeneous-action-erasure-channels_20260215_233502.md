---
ver: rpa2
title: Multi-Agent Bandit Learning through Heterogeneous Action Erasure Channels
arxiv_id: '2312.14259'
source_url: https://arxiv.org/abs/2312.14259
tags:
- action
- agents
- regret
- actions
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning in multi-agent multi-armed
  bandit (MAB) systems where agents communicate with a central learner through heterogeneous
  erasure channels without feedback. The proposed BatchSP2 algorithm uses a repetition
  protocol to ensure high-probability action delivery and a carefully crafted scheduling
  mechanism to allocate action pulls across agents, minimizing the impact of action
  erasures on regret.
---

# Multi-Agent Bandit Learning through Heterogeneous Action Erasure Channels

## Quick Facts
- **arXiv ID:** 2312.14259
- **Source URL:** https://arxiv.org/abs/2312.14259
- **Reference count:** 20
- **Primary result:** BatchSP2 algorithm guarantees sub-linear regret in multi-agent MAB with heterogeneous erasure channels, achieving O(√KT) regret for small gaps.

## Executive Summary
This paper addresses the challenge of learning in multi-agent multi-armed bandit systems where agents communicate with a central learner through heterogeneous erasure channels without feedback. The proposed BatchSP2 algorithm uses a repetition protocol to ensure high-probability action delivery and a carefully crafted scheduling mechanism to allocate action pulls across agents, minimizing the impact of action erasures on regret. The algorithm guarantees sub-linear regret bounds that depend on the suboptimality gaps of actions and the erasure probabilities of the channels. Specifically, the regret bound is nearly constant for constant gaps and erasure probabilities, and for small gaps, the regret is bounded by O(√KT) where K is the number of actions and T is the time horizon. Numerical experiments demonstrate that BatchSP2 outperforms existing MAB algorithms, which can suffer from linear regret under action erasures.

## Method Summary
The paper proposes BatchSP2, a multi-agent bandit algorithm designed for heterogeneous erasure channels without feedback. The algorithm uses a repetition protocol where each agent m repeats actions αm = ⌈4 logT /log(1/ϵm)⌉ − 1 times to ensure high-probability delivery. A scheduling mechanism allocates action pulls across agents through a two-stage process: first assigning actions to single agents using LP relaxation, then splitting remaining actions across best agents. The algorithm employs batch-based elimination (SAE) with confidence intervals to remove suboptimal arms. The scheduling algorithm finds a schedule S that assigns each action a to a subset of agents S(a) ⊆ [M], ensuring the total number of rounds is bounded by K4iτ + O(∑αm/M).

## Key Results
- BatchSP2 guarantees sub-linear regret O(K√T) for small action gaps under heterogeneous erasure channels
- Algorithm outperforms existing MAB approaches (MA-SAE, MA-LSAE-V, MA-LSAE-H, MA-UCB) which suffer linear regret under action erasures
- Regret bounds are nearly constant when both suboptimality gaps and erasure probabilities are constants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Repetition of actions across agents ensures that with high probability, at least one instance of the intended action reaches the agent, enabling correct reward association.
- **Mechanism:** The algorithm sends each action αm = ⌈4 logT /log (1/ϵm)⌉ − 1 times to agent m before collecting effective rewards, where ϵm is the erasure probability. This guarantees a success probability ≥ 1 − 1/poly(T).
- **Core assumption:** Erasure channels are i.i.d. across rounds and agents, and the erasure probabilities are known upper bounds.
- **Evidence anchors:**
  - [abstract] "Our proposed solutions are founded on a meticulously crafted repetition protocol"
  - [section III] "the learner first asks the agent m to repeat the action αm = ⌈4 logT /log (1/ϵm)⌉ − 1 additional times to ensure a success probability of at least 1 − 1/poly(T)"
  - [corpus] No direct contradiction found; repetition scheme aligns with [19] for single-agent case.
- **Break condition:** If erasure probability is very close to 1, αm becomes large and causes excessive regret from wasted pulls; if erasures are not i.i.d., the probability guarantee fails.

### Mechanism 2
- **Claim:** BatchSP2 schedules action pulls across agents to minimize total rounds needed for a batch, balancing repetition costs and parallelism.
- **Mechanism:** The scheduling algorithm partitions pulls into two stages: (1) assigns actions to at most one agent without splitting, using a rounded LP solution to find minimal τ; (2) splits remaining actions across best agents, ensuring end time ≤ K4iτ + O(∑αm/M).
- **Core assumption:** Agents can be ranked by their repetition costs αm, and pulling an action across multiple agents reduces the effective repetition burden per agent.
- **Evidence anchors:**
  - [section III] "Our scheduling goal is, given α1 ≤ α2 ≤ . . . ≤ αM, to find a schedule that minimizes T (i)"
  - [section IV] "At least (K − M)+ actions will be successfully allocated at this stage, leaving ˆK remaining actions"
  - [corpus] Weak evidence; no other scheduling schemes in the corpus directly match this two-stage approach.
- **Break condition:** If αM ≫ 1, the additive term ∑αm/M dominates and can make the bound near-linear; if M ≪ K, many actions must be split, increasing coordination complexity.

### Mechanism 3
- **Claim:** Random shuffling of actions in each batch ensures symmetry in expected pull counts, enabling tight regret bounds.
- **Mechanism:** The schedule construction randomizes the order of actions before assignment, making each active action contribute equally to total pulls in expectation, which bounds E[Tia] uniformly.
- **Core assumption:** Randomization breaks correlation between arm suboptimality and pull allocation, so no single arm dominates excess regret.
- **Evidence anchors:**
  - [section III] "We first round the LP solution to an integer solution... Shuffle the set A randomly"
  - [section IV] "Utilizing the averaging principle, the delay of all agents in the best half is bounded by the average delay"
  - [corpus] No explicit mention of randomization effects in the corpus; claim inferred from proof structure.
- **Break condition:** If randomization is removed, some arms may be systematically delayed, inflating regret beyond the proven bound.

## Foundational Learning

- **Concept:** Hoeffding's inequality for bounded rewards
  - Why needed here: To bound the deviation between empirical means and true means within each batch, enabling arm elimination with high confidence.
  - Quick check question: If rewards are in [0,1], Hoeffding gives |μ̂ − μ| ≤ √(log(1/δ)/(2n)) with probability ≥ 1−δ; what n ensures δ = 1/poly(T)?

- **Concept:** Sub-Gaussian concentration for i.i.d. rewards
  - Why needed here: The theoretical analysis assumes rewards are sub-Gaussian (or bounded) to apply concentration bounds uniformly across arms and batches.
  - Quick check question: If each reward has variance σ², what concentration inequality replaces Hoeffding, and how does it affect the elimination threshold?

- **Concept:** Integer Linear Programming relaxation and rounding
  - Why needed here: The scheduling subproblem is formulated as an ILP; its LP relaxation provides a lower bound on the minimal batch end time, which the algorithm rounds to an integer schedule.
  - Quick check question: Given an LP solution with fractional pulls, how does rounding to integers affect the feasibility and total time cost in the two-stage scheduling?

## Architecture Onboarding

- **Component map:** Central learner -> Scheduling engine -> Agents -> Central learner (feedback loop)
- **Critical path:**
  1. Learner decides batch i, shuffles active arms.
  2. Schedule() computes S and T(i) using LP relaxation and two-stage allocation.
  3. For t=1..T(i), send Smt to agent m, receive rmt.
  4. After batch, compute empirical means µ(i) using only last 4i rewards per arm per agent.
  5. Eliminate arms outside confidence region, increment i.

- **Design tradeoffs:**
  - Larger αm → higher delivery guarantee but more wasted pulls and regret.
  - More agents → more parallelism but scheduling complexity grows.
  - Stage 1 allocation without splitting reduces coordination but may leave many arms for stage 2.
  - Random shuffling vs deterministic assignment: randomization yields symmetric bounds but may hurt practical convergence.

- **Failure signatures:**
  - Linear regret growth → possible αm too large or stage 1 allocation inefficient.
  - Slow convergence → agents with high αm dominate batch length.
  - High variance in empirical means → concentration bounds too loose, need more pulls per batch.

- **First 3 experiments:**
  1. **Sanity check:** Single agent (M=1), known ϵ, verify BatchSP2 matches [19] bound.
  2. **Scaling test:** K=10, M=4, increasing erasure probabilities; plot cumulative regret vs rounds.
  3. **Ablation study:** Remove stage 2 splitting; assign each arm to a single agent; compare regret to full BatchSP2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the BatchSP2 algorithm perform when the erasure probabilities of the channels are time-varying instead of fixed?
- **Basis in paper:** [inferred] The paper focuses on heterogeneous erasure channels with fixed erasure probabilities. However, the introduction mentions that "communication channel issues such as delays or noise interference" can cause action erasures, which can vary over time.
- **Why unresolved:** The current algorithm and analysis assume fixed erasure probabilities. Adapting the algorithm to handle time-varying erasure probabilities would require a different scheduling and repetition strategy.
- **What evidence would resolve it:** Numerical experiments comparing the performance of BatchSP2 with time-varying erasure probabilities against the current algorithm with fixed erasure probabilities. Theoretical analysis of regret bounds for the modified algorithm.

### Open Question 2
- **Question:** Can the BatchSP2 algorithm be extended to handle non-i.i.d. reward distributions?
- **Basis in paper:** [explicit] The paper assumes that the reward for each action is generated from an unknown reward distribution with an unknown mean. However, the analysis relies on Hoeffding's inequality, which assumes i.i.d. rewards.
- **Why unresolved:** The current analysis techniques may not be directly applicable to non-i.i.d. reward distributions. Adapting the algorithm and analysis to handle such distributions would require different concentration inequalities and potentially a modified learning strategy.
- **What evidence would resolve it:** Numerical experiments comparing the performance of BatchSP2 with non-i.i.d. reward distributions against the current algorithm with i.i.d. rewards. Theoretical analysis of regret bounds for the modified algorithm under non-i.i.d. assumptions.

### Open Question 3
- **Question:** How does the BatchSP2 algorithm perform in comparison to other distributed learning algorithms that do not rely on action repetitions?
- **Basis in paper:** [inferred] The paper compares BatchSP2 against extensions of existing algorithms (UCB and SAE) to the multi-agent setting with action erasures. However, it does not compare against other distributed learning algorithms that may handle action erasures differently, such as those using consensus or gossip protocols.
- **Why unresolved:** The current comparison is limited to algorithms that use action repetitions. Exploring other distributed learning approaches for handling action erasures could provide insights into alternative strategies and potentially lead to improved algorithms.
- **What evidence would resolve it:** Numerical experiments comparing the performance of BatchSP2 against other distributed learning algorithms that handle action erasures without relying on action repetitions. Theoretical analysis of regret bounds for these alternative algorithms.

## Limitations
- The repetition-based delivery guarantee critically depends on accurate knowledge of erasure probabilities and i.i.d. erasures across rounds
- The scheduling algorithm's LP relaxation and rounding approach lacks rigorous performance guarantees under all parameter regimes
- The theoretical analysis assumes bounded rewards and sub-Gaussian noise, which may not hold in all practical scenarios

## Confidence

- **High confidence:** The repetition mechanism (αm = ⌈4 logT /log(1/ϵm)⌉ − 1) ensuring high-probability delivery, as this follows directly from Chernoff bounds and is mathematically rigorous.
- **Medium confidence:** The regret bounds O(K√T) for small gaps, as these depend on several intermediate approximations in the analysis that haven't been independently verified.
- **Medium confidence:** The two-stage scheduling approach, as the LP relaxation and rounding methodology lacks complete theoretical justification for optimality across all parameter regimes.

## Next Checks
1. **Robustness test:** Implement BatchSP2 with perturbed erasure probabilities (ϵm ± 20%) to verify performance degradation is graceful rather than catastrophic.
2. **Ablation study:** Compare BatchSP2 against a simplified version that assigns each arm to a single agent (no stage 2 splitting) across varying (K, M) ratios to quantify the scheduling benefit.
3. **Concentration validation:** Generate synthetic data with non-sub-Gaussian reward distributions to test whether the elimination thresholds maintain the claimed confidence levels.