---
ver: rpa2
title: Explore to Generalize in Zero-Shot RL
arxiv_id: '2306.03072'
source_url: https://arxiv.org/abs/2306.03072
tags:
- policy
- maze
- learning
- generalization
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Explore to Generalize (ExpGen), a method for
  zero-shot generalization in reinforcement learning that combines maximum entropy
  exploration with an ensemble of reward-seeking policies. The key insight is that
  policies trained for maximum entropy exploration generalize well to unseen environments,
  while an ensemble of reward-seeking policies can identify epistemic uncertainty.
---

# Explore to Generalize in Zero-Shot RL

## Quick Facts
- arXiv ID: 2306.03072
- Source URL: https://arxiv.org/abs/2306.03072
- Reference count: 39
- Key outcome: Achieves 82% success rate on Maze and 74% on Heist ProcGen games using 200 training levels

## Executive Summary
This paper introduces Explore to Generalize (ExpGen), a zero-shot RL method that combines maximum entropy exploration with ensemble uncertainty estimation to achieve state-of-the-art performance on challenging ProcGen benchmarks. The key insight is that policies trained for maximum entropy exploration generalize better to unseen environments than reward-maximizing policies, while ensemble disagreement can signal when to explore rather than exploit. The method significantly outperforms previous approaches on games that have been particularly challenging for zero-shot generalization.

## Method Summary
ExpGen trains two components: a maximum entropy exploration policy using k-NN entropy estimation, and an ensemble of 10 reward-seeking policies using PPO. At test time, the algorithm uses ensemble consensus when available, otherwise taking exploratory actions from the entropy-maximizing policy. The number of exploration steps follows a geometric distribution to prevent meta-stability. This approach leverages the generalization properties of entropy-maximizing exploration while using ensemble uncertainty to determine when exploration is needed.

## Key Results
- Achieves 82% success rate on Maze and 74% on Heist with only 200 training levels
- Outperforms state-of-the-art baselines by significant margins on challenging ProcGen games
- Demonstrates that entropy-maximizing exploration generalizes better than reward maximization
- Shows that ensemble disagreement is an effective proxy for epistemic uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-maximizing exploration generalizes better than reward-maximizing behavior because entropy rewards are less prone to overfitting.
- Mechanism: Maximum entropy exploration encourages uniform state visitation, making the policy less likely to memorize specific task solutions. This uniformity provides robustness to variations in task-specific dynamics and visuals.
- Core assumption: State space entropy is a sufficient signal for exploration that does not depend on specific rewards or visual features.

### Mechanism 2
- Claim: Ensemble disagreement signals epistemic uncertainty that can be resolved by exploration.
- Mechanism: When ensemble members disagree, the policy is uncertain about the correct action. Taking exploratory actions from the entropy-maximizing policy helps discover new state regions where the ensemble might agree again.
- Core assumption: Ensemble disagreement is a reliable proxy for epistemic uncertainty in zero-shot RL settings.

### Mechanism 3
- Claim: Geometric distribution of exploration steps prevents meta-stability in state space transitions.
- Mechanism: Randomly selecting the number of exploration steps from a geometric distribution prevents the agent from repeatedly toggling between two states when switching between ensemble-agreed and exploratory actions.
- Core assumption: Meta-stability occurs when exploration actions lead to states where the ensemble immediately disagrees again, creating cycles.

## Foundational Learning

- Concept: Maximum Entropy Reinforcement Learning
  - Why needed here: Forms the basis for the exploration policy that generalizes well across tasks
  - Quick check question: What is the key difference between maximum entropy and standard RL objectives?

- Concept: Epistemic Uncertainty in RL
  - Why needed here: Understanding when the ensemble disagrees helps determine when to explore vs exploit
  - Quick check question: How does ensemble disagreement relate to epistemic uncertainty in zero-shot settings?

- Concept: Generalization Gap in RL
  - Why needed here: The method aims to minimize the gap between training and test performance through exploration
  - Quick check question: What factors contribute to generalization gaps in reinforcement learning?

## Architecture Onboarding

- Component map:
  - MaxEnt Policy (πH) -> Ensemble of Reward Policies ({πⱼᵣ}ᵐⱼ₌₁) -> Consensus Detector -> Exploration Scheduler

- Critical path:
  1. Train maxEnt policy on training tasks
  2. Train ensemble of reward policies on same tasks
  3. At test time, use ensemble consensus when available, otherwise explore
  4. Repeat until task completion

- Design tradeoffs:
  - Ensemble size vs computational cost
  - Exploration duration (geometric parameter α) vs efficiency
  - Memory usage in maxEnt policy vs performance

- Failure signatures:
  - Low consensus rates indicate poor ensemble training or high task variability
  - High exploration frequency suggests the maxEnt policy isn't generalizing well
  - Performance degradation on simple tasks indicates over-exploration

- First 3 experiments:
  1. Train maxEnt policy alone and measure generalization gap on Maze vs reward-maximizing policy
  2. Test ensemble consensus detection on training tasks to establish baseline agreement rates
  3. Run full ExpGen on Maze with varying ensemble sizes to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the maxEnt policy's superior generalization ability stem primarily from its invariance to the reward signal, or is it also less sensitive to variations in the dynamics of the MDPs across different training tasks?
- Basis in paper: [explicit] The authors mention this as a "burning question" and suggest that while the maxEnt policy's invariance to reward is clear, its sensitivity to MDP dynamics is unclear.
- Why unresolved: The paper does not provide theoretical analysis or empirical evidence to distinguish between these two potential explanations for the maxEnt policy's generalization ability.
- What evidence would resolve it: A controlled experiment varying the MDP dynamics while keeping the reward structure constant, or a theoretical analysis of the maxEnt policy's sensitivity to MDP dynamics.

### Open Question 2
- Question: Can combining ExpGen with invariance-based approaches (like IDAAC) significantly improve performance on ProcGen games where both methods individually show promise, such as Maze and BigFish?
- Basis in paper: [explicit] The authors suggest that the advantage of ExpGen is complementary to invariance-based methods and propose combining them by training both an IDAAC agent and an ExpGen agent, choosing between them based on validation performance.
- Why unresolved: The paper does not present experimental results for this combined approach, only theoretical discussion of its potential benefits.
- What evidence would resolve it: Experimental results comparing the combined approach to each method individually on a range of ProcGen games.

### Open Question 3
- Question: What is the theoretical basis for the effectiveness of the maxEnt policy in exploring novel state spaces at test time, and how does this relate to the concept of epistemic uncertainty?
- Basis in paper: [inferred] The authors argue that the maxEnt policy's exploration behavior generalizes well and is useful when the ensemble of reward-seeking policies does not agree, implying a connection to epistemic uncertainty.
- Why unresolved: The paper does not provide a rigorous theoretical explanation for why the maxEnt policy's exploration behavior is effective in reducing epistemic uncertainty at test time.
- What evidence would resolve it: A theoretical analysis of the relationship between the maxEnt policy's state visitation distribution and the reduction of epistemic uncertainty, or empirical evidence showing the correlation between maxEnt exploration and improved ensemble agreement over time.

## Limitations

- The ensemble disagreement signal as a proxy for epistemic uncertainty lacks theoretical justification
- The geometric distribution parameter α=0.5 is heuristic and may require tuning for different domains
- Scalability to non-visual domains or continuous control tasks remains untested
- Method's behavior when consensus is never reached is not thoroughly analyzed

## Confidence

**High Confidence**: The core insight that maximum entropy exploration generalizes better than reward maximization is well-supported by empirical results.

**Medium Confidence**: The ensemble disagreement signal as a proxy for epistemic uncertainty is intuitive but lacks theoretical justification.

**Low Confidence**: The scalability of the approach to non-visual domains or continuous control tasks remains untested.

## Next Checks

1. **Ablation Study**: Remove the ensemble consensus mechanism and compare performance to pure maxEnt exploration to isolate the contribution of uncertainty-based switching.

2. **Theoretical Analysis**: Prove that ensemble disagreement is a valid proxy for epistemic uncertainty in the ProcGen domain, or provide counterexamples showing when this breaks down.

3. **Generalization Beyond ProcGen**: Test ExpGen on Atari benchmarks or continuous control tasks to evaluate its applicability beyond procedurally generated visual environments.