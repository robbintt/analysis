---
ver: rpa2
title: 'nach0: Multimodal Natural and Chemical Languages Foundation Model'
arxiv_id: '2311.12410'
source_url: https://arxiv.org/abs/2311.12410
tags:
- language
- chemical
- tasks
- molecular
- nach0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: nach0 is a multimodal encoder-decoder foundation model that unifies
  natural language and chemical representations. It is pre-trained on unlabeled text
  and SMILES molecular strings from scientific literature and patents, and then fine-tuned
  on diverse biomedical and chemical tasks using instruction tuning.
---

# nach0: Multimodal Natural and Chemical Languages Foundation Model

## Quick Facts
- arXiv ID: 2311.12410
- Source URL: https://arxiv.org/abs/2311.12410
- Reference count: 17
- Key outcome: nach0 is a multimodal encoder-decoder foundation model that unifies natural language and chemical representations. It is pre-trained on unlabeled text and SMILES molecular strings from scientific literature and patents, and then fine-tuned on diverse biomedical and chemical tasks using instruction tuning. Experiments show that nach0 outperforms state-of-the-art models on single-domain and cross-domain tasks, including named entity recognition, molecular property prediction, and reaction prediction. Case studies demonstrate its ability to generate novel molecules and predict drug efficacy. The model achieves strong results in both in-domain and cross-domain settings, showcasing its effectiveness in handling multimodal data and tasks.

## Executive Summary
nach0 is a multimodal foundation model that unifies natural language and chemical representations through a unified encoder-decoder architecture. The model is pre-trained on large-scale unlabeled text from scientific literature and patents, combined with SMILES molecular strings, and then fine-tuned on diverse biomedical and chemical tasks using instruction tuning. The key innovation is the ability to jointly learn representations from both modalities, enabling strong performance across single-domain and cross-domain tasks including named entity recognition, molecular property prediction, and reaction prediction. The model demonstrates capabilities in generating novel molecules and predicting drug efficacy, showing effectiveness in handling multimodal data and tasks.

## Method Summary
The nach0 model uses a T5-based encoder-decoder architecture with special tokenization for SMILES strings, where each chemical token is annotated with `<sm {token}>` to distinguish it from natural language tokens. The model is pre-trained using self-supervised learning on a mixture of natural language texts (PubMed abstracts, USPTO patents) and chemical SMILES strings (ZINC dataset). Instruction tuning is then employed to fine-tune the model on diverse tasks, where each dataset instance is converted into input-output pairs described through natural language prompts. The NeMo framework is used for multi-GPU parallel training during both pre-training and fine-tuning phases.

## Key Results
- nach0 outperforms state-of-the-art models on single-domain and cross-domain tasks including named entity recognition, molecular property prediction, and reaction prediction
- The model demonstrates strong performance in both in-domain and cross-domain settings, validating its effectiveness in handling multimodal data
- Case studies show nach0's ability to generate novel molecules and predict drug efficacy with high validity and novelty scores
- Scaling experiments show benefits from increasing model size from base (220M) to large (770M) parameters across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multimodal architecture allows nach0 to jointly learn representations from both natural language and SMILES chemical strings, enabling cross-domain task performance.
- Mechanism: The encoder-decoder transformer processes input tokens from both modalities using shared learned embeddings. SMILES tokens are specially annotated (e.g., `<sm {token}>`) to distinguish them from natural language tokens while still sharing the same model weights.
- Core assumption: Natural language and chemical representations can benefit from joint training rather than being handled by separate specialized models.
- Evidence anchors:
  - [abstract]: "nach0 is a multimodal encoder-decoder foundation model that unifies natural language and chemical representations."
  - [section]: "One way to represent molecular structures is a simplified molecular-input line-entry system (SMILES) string... We annotate each SMILES token with special symbols: <sm {token}> and extend the vocabulary with such tokens."
  - [corpus]: Weak evidence - only 1 out of 8 related papers mentions similar multimodal approach, suggesting this is a novel contribution.
- Break condition: If the shared representation space becomes too noisy or if the vocabulary size grows beyond manageable limits, the model's ability to generalize across domains may degrade.

### Mechanism 2
- Claim: Instruction tuning enables nach0 to perform diverse tasks by formulating each dataset instance into input-output pairs described through natural language prompts.
- Mechanism: Each dataset is associated with multiple prompt templates that convert raw data into a "text-to-text" format. The model is then fine-tuned on this mixture of tasks, learning to interpret the instructions and generate appropriate responses.
- Core assumption: Natural language instructions can effectively describe and guide task-specific behaviors across different domains (NLP, chemistry, cross-domain).
- Evidence anchors:
  - [abstract]: "We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks."
  - [section]: "Inspired by (Raffel et al, 2020; Chung et al, 2022), we follow the intuition that tasks can be described via natural language instructions... Prompt design and instruction tuning are employed for model training."
  - [corpus]: Weak evidence - while instruction tuning is mentioned in related work, the specific application to multimodal chemical tasks is not well-represented in the corpus.
- Break condition: If prompts are ambiguous or inconsistent across tasks, the model may fail to learn task boundaries and produce incorrect outputs.

### Mechanism 3
- Claim: Pre-training on large-scale unlabeled text and SMILES data from scientific literature and patents provides nach0 with a rich foundation for downstream task performance.
- Mechanism: The model is pre-trained using self-supervised learning on a mixture of natural language texts (PubMed abstracts, USPTO patents) and chemical SMILES strings (ZINC dataset). This creates a broad knowledge base that can be adapted to specific tasks through fine-tuning.
- Core assumption: Large-scale pre-training on diverse data provides transferable knowledge that improves performance on downstream tasks compared to training from scratch or on limited data.
- Evidence anchors:
  - [abstract]: "nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge."
  - [section]: "For both models, we conduct pre-training with a language modeling (LM) objective... The chemical data component was sourced from the ZINC dataset, encompassing approximately 100 million documents."
  - [corpus]: Weak evidence - only 2 out of 8 related papers mention similar pre-training approaches, suggesting this is a distinguishing feature.
- Break condition: If the pre-training data is not sufficiently diverse or representative of the target tasks, the model may learn irrelevant patterns that hinder fine-tuning performance.

## Foundational Learning

- Concept: Tokenization strategies for multimodal data
  - Why needed here: nach0 must handle both natural language text and chemical SMILES strings, requiring different tokenization approaches that can be unified in a single model
  - Quick check question: How does nach0 differentiate between natural language tokens and SMILES tokens in its vocabulary?

- Concept: Instruction tuning and prompt engineering
  - Why needed here: nach0 uses natural language instructions to guide task-specific behaviors, requiring careful prompt design to ensure consistent and effective fine-tuning across diverse tasks
  - Quick check question: What are the key considerations when designing prompts for cross-domain tasks in nach0?

- Concept: Encoder-decoder architecture for sequence-to-sequence tasks
  - Why needed here: nach0 must handle both input understanding (encoding) and output generation (decoding) for tasks ranging from named entity recognition to molecular generation
  - Quick check question: How does the encoder-decoder architecture in nach0 differ from encoder-only models like BERT when handling cross-domain tasks?

## Architecture Onboarding

- Component map: Encoder-decoder transformer with shared vocabulary and embeddings for both natural language and SMILES tokens. The encoder processes input sequences, the decoder generates target sequences, and special token annotations (<sm {token}>) distinguish chemical data.
- Critical path: Data preprocessing → Tokenization with special annotations → Pre-training on mixed natural language and SMILES data → Instruction tuning with task-specific prompts → Evaluation on benchmark datasets
- Design tradeoffs: Using a unified model for both modalities trades specialization for flexibility. The special token annotations help maintain chemical specificity but increase vocabulary size. Pre-training on large datasets improves generalization but requires significant computational resources.
- Failure signatures: Poor performance on cross-domain tasks may indicate insufficient shared representation learning. Inconsistent outputs across similar prompts may suggest prompt engineering issues. Low novelty in molecular generation could indicate overfitting to training data.
- First 3 experiments:
  1. Fine-tune nach0 on a single NLP task (e.g., named entity recognition) and evaluate performance compared to baseline models
  2. Test nach0's ability to generate valid SMILES strings from natural language descriptions using top-p sampling
  3. Evaluate cross-domain performance by fine-tuning on a chemistry task and testing on both chemistry and NLP benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does nach0's performance compare to other state-of-the-art models on larger scales, particularly in the range of billions of parameters?
- Basis in paper: [inferred] The paper mentions that scaling from a base model (220M) to a large model (770M) demonstrated benefits on all datasets, suggesting further scaling could enhance chemical capabilities.
- Why unresolved: The paper only evaluated up to a 770M parameter model, while other models like Galactica (120B) and BioGPT (355B) exist, indicating potential for further scaling.
- What evidence would resolve it: Training and evaluating nach0 models with 1B+ parameters on the same benchmarks used in the paper.

### Open Question 2
- Question: Can nach0 be extended to incorporate additional molecular modalities beyond SMILES, such as 2D or 3D molecular graph representations?
- Basis in paper: [explicit] The paper discusses limitations of SMILES format, including the lack of 3D geometry and spatial arrangement of atoms, and suggests incorporating additional modalities as a potential future direction.
- Why unresolved: The current model only uses SMILES tokenization, and integrating other molecular representations would require significant architectural changes and new training data.
- What evidence would resolve it: Developing and evaluating a nach0 variant that incorporates 2D or 3D molecular graph representations, and comparing its performance to the original SMILES-based model on relevant benchmarks.

### Open Question 3
- Question: How can nach0 be improved to learn from human feedback and expert knowledge to reach chemist-level expertise?
- Basis in paper: [explicit] The paper mentions that human evaluations suggest the model is not at the chemist expert level and suggests researching new capabilities including learning from and adapting to feedback from human experts.
- Why unresolved: The current model is trained on static datasets and does not have mechanisms to incorporate ongoing human feedback or expert knowledge during or after training.
- What evidence would resolve it: Implementing a feedback loop where domain experts can provide input on the model's outputs, and then retraining or fine-tuning the model to incorporate this feedback, followed by evaluating the improved performance on expert-level tasks.

## Limitations

- The paper lacks specific details about the pre-training corpus construction, particularly the filtering criteria for extracting chemistry-related content from scientific literature and patents
- Minimal information is provided about the prompt templates and instruction design methodology used for fine-tuning across different tasks
- Cross-domain evaluation methodology needs more rigorous description to establish whether the model truly generalizes or memorizes task-specific patterns

## Confidence

**High Confidence Claims:**
- nach0 successfully integrates natural language and chemical representations in a unified architecture
- The model can be fine-tuned to perform various biomedical and chemical tasks
- Pre-training on combined natural language and SMILES data is technically feasible

**Medium Confidence Claims:**
- nach0 outperforms state-of-the-art models on benchmark tasks
- Instruction tuning with natural language prompts is effective for cross-domain adaptation
- The model demonstrates novel molecule generation capabilities

**Low Confidence Claims:**
- Cross-domain generalization is robust across all tested scenarios
- The specific architectural choices (token annotations, vocabulary size) are optimal
- Performance improvements are solely attributable to the multimodal design

## Next Checks

1. **Reproduce cross-domain evaluation**: Conduct controlled experiments where nach0 is trained on chemistry tasks and evaluated on NLP benchmarks (and vice versa), explicitly measuring performance degradation and comparing against models trained and tested within the same domain. This will validate whether the cross-domain claims hold under rigorous testing conditions.

2. **Ablation study on tokenization strategy**: Systematically test alternative tokenization approaches for SMILES strings (e.g., character-level vs. subword tokenization, different annotation schemes) while keeping the model architecture constant. This will determine whether the specific tokenization strategy significantly impacts performance or if simpler approaches would suffice.

3. **Prompt sensitivity analysis**: Create multiple variants of prompts for the same tasks (varying in instruction clarity, format, and specificity) and measure how performance changes. This will reveal whether the model's effectiveness depends critically on prompt engineering quality or whether it demonstrates more robust instruction following capabilities.