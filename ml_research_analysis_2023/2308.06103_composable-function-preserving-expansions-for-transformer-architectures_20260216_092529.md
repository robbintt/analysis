---
ver: rpa2
title: Composable Function-preserving Expansions for Transformer Architectures
arxiv_id: '2308.06103'
source_url: https://arxiv.org/abs/2308.06103
tags:
- function
- expansion
- dimension
- transformer
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces six composable transformations to incrementally
  scale transformer-based neural networks while preserving their functionality. The
  transformations allow expanding key architectural dimensions - MLP internal representation
  size, number of attention heads, attention head output/input representation size,
  transformer hidden dimension, and number of layers.
---

# Composable Function-preserving Expansions for Transformer Architectures

## Quick Facts
- arXiv ID: 2308.06103
- Source URL: https://arxiv.org/abs/2308.06103
- Reference count: 40
- Primary result: Six composable transformations enable incremental scaling of transformers while preserving exact functionality through zero initialization of new parameters

## Executive Summary
This paper introduces six composable transformations that allow incremental scaling of transformer-based neural networks while preserving their exact functionality. The transformations can expand key architectural dimensions including MLP internal representation size, number of attention heads, attention head output/input representation size, transformer hidden dimension, and number of layers. Each transformation is proven to maintain exact function preservation under minimal initialization constraints, typically requiring only zero initialization of newly added parameters. The composable nature enables flexible scaling strategies, potentially allowing efficient training pipelines where models can be progressively expanded during training rather than training from scratch at full scale.

## Method Summary
The method introduces six composable function-preserving transformations for transformer architectures: MLP expansion, head addition, heads expansion, attention expansion, hidden dimension expansion, and layer addition. Each transformation involves adding new parameters to the model while initializing them to zero, ensuring that the expanded model produces identical outputs to the original model. The paper provides formal proofs of exact function preservation for each transformation under minimal initialization constraints. The transformations can be applied independently or in combination, enabling flexible scaling strategies that preserve model functionality while expanding capacity.

## Key Results
- Six composable transformations proven to preserve exact function under zero initialization constraints
- Framework enables progressive model expansion during training, potentially reducing computational costs
- Transformations maintain functionality across all transformer components including MLP layers and multi-head attention
- Zero initialization of new parameters is the minimal requirement for function preservation in most transformations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero initialization of newly added parameters ensures exact function preservation during MLP expansion.
- Mechanism: When expanding the MLP internal dimension from p to ˆp, new rows are added to the second MLP weight matrix and initialized to zero. Since the input to these new neurons is multiplied by zero weights, their contribution to the output remains zero, preserving the original function.
- Core assumption: The original function can be preserved by adding neurons that don't contribute to the output during forward pass.
- Evidence anchors:
  - [section] Theorem 3.1 states: "zero initializing MW l2 ( ˆp−p)×h implies the function preservation property for the MLP expansion transformation."
  - [abstract] "We provide proof of exact function preservation under minimal initialization constraints for each transformation."
- Break condition: If non-zero initialization is used for the added rows, or if the activation function is changed from ReLU to something that doesn't map zero input to zero output.

### Mechanism 2
- Claim: Zero initialization of MHA output weight matrix rows preserves function during head addition.
- Mechanism: When adding a new attention head, new rows are added to the MHA output weight matrix and initialized to zero. Since these rows multiply the output of the new attention head (which is computed normally), the contribution remains zero, preserving the original function.
- Core assumption: Adding attention heads that don't contribute to the final output preserves the original function.
- Evidence anchors:
  - [section] Theorem 3.2 states: "zero initializing MWO v×h implies the function preservation property for the head addition transformation."
  - [section] Definition 3.2 shows the transformation adds v rows to the MHA output weight matrix.
- Break condition: If the new attention head rows in the output matrix are not zero-initialized, or if the attention computation itself is modified.

### Mechanism 3
- Claim: Zero initialization combined with scaling factors preserves function during hidden dimension expansion.
- Mechanism: When expanding the hidden dimension, new columns are added to various matrices. Zero initialization ensures these new dimensions don't contribute, while scaling factors (like √h/√ˆh) maintain normalization properties across the expanded dimension.
- Core assumption: The combination of zero initialization and appropriate scaling factors can preserve the function across all transformer components during hidden dimension expansion.
- Evidence anchors:
  - [section] Theorem 3.5 states: "zero initializing the specified matrices implies the function preservation property for the hidden dimension expansion transformation."
  - [section] Definition 3.5 shows multiple matrices are expanded with zero initialization and scaling factors applied to normalization parameters.
- Break condition: If scaling factors are omitted or incorrect, or if any of the specified matrices are not zero-initialized.

## Foundational Learning

- Concept: Matrix operations and their properties (addition, multiplication, transpose)
  - Why needed here: All transformations involve manipulating weight matrices through addition of new rows/columns and matrix multiplications in the forward pass.
  - Quick check question: If you add a row of zeros to a weight matrix and multiply it by any vector, what is the result?

- Concept: Neural network forward pass mechanics
  - Why needed here: Understanding how adding zero-contributing parameters doesn't change the output of each layer.
  - Quick check question: In a layer with weights W and input x, if you modify W to W' by adding a zero row, what happens to W'x compared to Wx?

- Concept: Attention mechanism fundamentals
  - Why needed here: The head addition and heads expansion transformations specifically modify attention components.
  - Quick check question: In multi-head attention, what happens to the final output if one head's contribution is multiplied by zero?

## Architecture Onboarding

- Component map:
  - MLP layers: Internal dimension expansion (p → ˆp)
  - Multi-head attention: Head addition (E → E+1), heads expansion (v → ˆv), attention expansion (k → ˆk)
  - Transformer layers: Hidden dimension expansion (h → ˆh), layer addition (N → N+1)
  - All components: Zero initialization of new parameters, scaling factors for normalization

- Critical path: For each transformation, identify which weight matrices need expansion and which need zero initialization. The order matters - normalization scaling must be applied before the zero-initialized parameters are used in forward pass.

- Design tradeoffs:
  - Zero initialization vs. other strategies: Zero initialization is simplest but may slow convergence when training after expansion.
  - Uniform vs. selective expansion: Applying transformations to all layers vs. specific layers affects model capacity distribution.
  - Composability: Transformations can be applied in any order, but some combinations may be more efficient than others.

- Failure signatures:
  - Function not preserved: Check if all required parameters were zero-initialized correctly.
  - Training instability after expansion: Verify scaling factors were applied correctly to normalization parameters.
  - Memory issues: Hidden dimension expansion increases memory usage quadratically with h.

- First 3 experiments:
  1. MLP expansion test: Start with a small transformer, expand MLP internal dimension by 2x, verify outputs match before expansion.
  2. Head addition test: Add one attention head to a single layer, verify function preservation using the same input.
  3. Hidden dimension expansion test: Expand hidden dimension by 2x, verify outputs match and check that scaling factors were applied correctly to normalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework handle model architectures that use GELU activation instead of ReLU?
- Basis in paper: [explicit] The paper states that "The proposed transformations also maintain the function preserving property with alternative choices such as GELU(·) (Hendrycks & Gimpel, 2016)."
- Why unresolved: While the paper mentions GELU compatibility, it doesn't provide specific proof or transformation details for this case.
- What evidence would resolve it: Formal proofs showing function preservation with GELU activation, or empirical results demonstrating equivalent behavior across different activation functions.

### Open Question 2
- Question: What is the impact of applying these transformations in different orders or combinations on model convergence and performance?
- Basis in paper: [inferred] The paper states that "The proposed transformations can be combined to allow the joint extension of multiple dimensions of the transformer architecture" but doesn't explore different scheduling strategies.
- Why unresolved: The paper focuses on proving function preservation but doesn't investigate optimal transformation sequencing or its effects on training dynamics.
- What evidence would resolve it: Systematic studies comparing different transformation scheduling strategies, including their impact on convergence speed and final model quality.

### Open Question 3
- Question: How do these transformations perform in architectures with layer normalization instead of RMSNorm?
- Basis in paper: [explicit] The paper notes that "The key difference is only scaling the variance of the inputs and using scaling parameters, rather than also subtracting their mean and using bias parameters."
- Why unresolved: While the paper uses RMSNorm for its proofs, it doesn't explicitly address how these transformations would work with LayerNorm.
- What evidence would resolve it: Adaptation of the proofs to LayerNorm or empirical validation showing equivalent function preservation with LayerNorm-based architectures.

### Open Question 4
- Question: What are the computational overhead implications of applying these transformations during training versus at initialization?
- Basis in paper: [inferred] The paper discusses progressive model expansion during training but doesn't analyze the computational costs of different implementation strategies.
- Why unresolved: The paper focuses on theoretical function preservation but doesn't address practical efficiency considerations of incremental versus batch transformations.
- What evidence would resolve it: Detailed computational complexity analysis comparing different implementation approaches and their impact on training efficiency.

### Open Question 5
- Question: How do these transformations affect model performance when applied to architectures with different attention mechanisms (e.g., sparse attention, grouped attention)?
- Basis in paper: [inferred] The paper focuses on standard multi-head attention but mentions that "applications to variants (e.g. Encoder+Decoder, different normalization placement) can be obtained with simple extensions."
- Why unresolved: The paper doesn't explore how these transformations generalize to alternative attention mechanisms that might be used in modern architectures.
- What evidence would resolve it: Empirical validation of function preservation across different attention mechanisms, or formal proofs extending the framework to these variants.

## Limitations

- The framework relies heavily on zero initialization, which may limit its applicability when transformations need to be applied to pre-trained models
- Practical efficiency gains from progressive expansion during training have not been empirically validated at scale
- The proofs assume specific architectural choices (RMSNorm, certain activation functions) that may not generalize to all transformer variants

## Confidence

- High confidence: The mathematical proofs for individual transformations are rigorous and well-established. The zero-initialization mechanism for function preservation is theoretically sound.
- Medium confidence: Composability claims are mathematically valid but may face practical challenges in real-world implementations, particularly when transformations are applied to pre-trained models.
- Medium confidence: The assertion that these transformations enable more efficient training pipelines is plausible but requires empirical validation across different model scales and tasks.

## Next Checks

1. **Numerical precision testing**: Implement the six transformations on a small transformer and verify exact function preservation by comparing outputs before and after expansion at different floating-point precisions (FP32, FP16, BF16).

2. **Composability stress test**: Apply multiple transformations sequentially (e.g., MLP expansion followed by head addition) and measure whether function preservation holds exactly or degrades due to accumulated numerical errors.

3. **Pre-trained model evaluation**: Apply transformations to a pre-trained transformer (e.g., BERT-base) and evaluate whether the expanded model maintains performance on downstream tasks before any further training, validating the practical utility of the approach.