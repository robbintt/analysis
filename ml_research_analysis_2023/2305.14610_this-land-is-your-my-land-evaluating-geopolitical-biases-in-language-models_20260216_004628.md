---
ver: rpa2
title: 'This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models'
arxiv_id: '2305.14610'
source_url: https://arxiv.org/abs/2305.14610
tags:
- territory
- bias
- language
- geopolitical
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces geopolitical bias in language models, showing
  they provide inconsistent geopolitical knowledge across languages. As a case study,
  it examines territorial disputes using a newly collected dataset of 244 disputed
  territories across 45 languages.
---

# This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models

## Quick Facts
- arXiv ID: 2305.14610
- Source URL: https://arxiv.org/abs/2305.14610
- Reference count: 11
- Key outcome: Language models show inconsistent geopolitical knowledge across languages, with 22.3% absolute difference between responses in claimant vs. non-claimant languages

## Executive Summary
This paper introduces the concept of geopolitical bias in language models, demonstrating that multilingual LMs provide inconsistent geopolitical knowledge when queried in different languages. The authors conduct a case study on territorial disputes using a dataset of 244 disputed territories across 45 languages, creating multiple-choice questions for each territory in the language of each claimant country. Evaluating GPT-3.5, they find substantial cross-lingual inconsistencies, with the model performing reasonably well in English (58.6% accuracy) but showing large variations across languages. This inconsistency contrasts sharply with multilingual humans who maintain consistent knowledge regardless of language, suggesting that current LMs may reflect cultural biases from their training data rather than truly understanding geopolitical facts.

## Method Summary
The authors created a dataset of 244 disputed territories by scraping Wikipedia, then generated multiple-choice questions for each territory in the language of each claimant country using template-based translation. They used Google Translate to translate an English template into 45 languages, then filled in territory and claimant names. The questions were posed to GPT-3.5 with temperature=0.7, top P=1, frequency penalty=0, and presence penalty=0. Responses were parsed to extract selected claimants, and consistency metrics were calculated comparing responses across languages and against actual geopolitical facts. The authors developed three consistency scores: knowledge base CS (comparing responses to actual facts), control vs non-control CS (comparing responses in controller vs non-controller language), and consistency CS (measuring agreement across languages).

## Key Results
- GPT-3.5 achieves 58.6% accuracy in English but shows 22.3% absolute difference between controller vs non-controller language responses
- The model exhibits significant cross-lingual inconsistencies, with responses varying greatly depending on the query language
- Knowledge base CS scores are relatively low, indicating the model often disagrees with actual geopolitical facts
- Control vs non-control CS shows large discrepancies, suggesting the model's responses are influenced by the language used rather than factual knowledge
- Consistency CS is also low, confirming the model fails to maintain consistent responses across languages for the same territorial dispute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual inconsistency stems from training data containing culturally biased geopolitical information
- Mechanism: LLMs learn statistical associations from multilingual training data where different languages encode different cultural perspectives on territorial disputes
- Core assumption: Training corpus contains different proportions and framings of geopolitical information across languages
- Evidence anchors: Abstract mentions humans maintain consistent knowledge while models don't; section 4.3 discusses training data cultural viewpoints; corpus evidence is weak

### Mechanism 2
- Claim: Template-based translation creates predictable linguistic patterns that LLM exploits for biased responses
- Mechanism: Template structures become associated with specific claimant countries across languages, which the LLM maps to favorable responses
- Core assumption: Template structure contains implicit biases amplified through translation
- Evidence anchors: Section 3.1 shows systematic translation approach; abstract shows 22.3% difference; corpus evidence missing

### Mechanism 3
- Claim: Multiple-choice format with letter indicators creates selection bias based on linguistic context
- Mechanism: Letter indicators have different statistical associations in different languages' training data, which the LLM learns to map to specific claimants
- Core assumption: Letter indicators have different cultural or linguistic associations across languages
- Evidence anchors: Section 3 mentions intentional format choice; abstract shows systematic response differences; corpus evidence weak

## Foundational Learning

- Concept: Cross-lingual consistency in knowledge representation
  - Why needed here: Core finding is that LLMs lack cross-lingual consistency that humans maintain
  - Quick check: If a human knows Taiwan is disputed between China and ROC, would they give different answers if asked in Chinese vs. English? (No)

- Concept: Cultural bias in training data
  - Why needed here: Paper attributes geopolitical bias to cultural biases in training corpus
  - Quick check: If most English texts about Taiwan mention PRC claims while Chinese texts mention ROC claims, what bias would an LLM trained on this data likely exhibit? (Different responses depending on language)

- Concept: Template-based translation and its limitations
  - Why needed here: Paper uses template translation which may introduce systematic biases
  - Quick check: What happens when you translate "Territory is of Country A or Country B" using template translation vs. translating the whole sentence? (Template translation is more consistent but may miss nuances)

## Architecture Onboarding

- Component map: Data collection (Wikipedia scraping) → Prompt generation (template translation) → LLM querying → Response parsing → Bias metric calculation
- Critical path: Prompt generation → LLM querying → Response parsing → Bias metric calculation
- Design tradeoffs: Template translation trades translation quality for consistency and efficiency; multiple-choice format trades nuance for easier parsing
- Failure signatures: High variance in responses across languages for same territory; low KB CS scores; inconsistency between control and non-control CS
- First 3 experiments:
  1. Re-run with GPT-4 instead of GPT-3 to see if larger models show less bias
  2. Test with human-translated prompts instead of Google Translate to isolate translation effects
  3. Try different prompt formats (binary true/false, long-form questions) to see if format affects bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training data or language model architectures could be modified to reduce geopolitical bias in multilingual models?
- Basis in paper: [explicit] Discusses how geopolitical bias likely emerges from training data and statistical associations
- Why unresolved: Identifies problem but doesn't propose specific solutions
- What evidence would resolve it: Empirical studies testing different training approaches, data curation strategies, or architectural modifications

### Open Question 2
- Question: How does geopolitical bias in language models vary across different language families or regions of the world?
- Basis in paper: [inferred] Examines 45 languages but lacks comparative analysis across language families
- Why unresolved: Dataset includes many languages but lacks comparative analysis of bias patterns
- What evidence would resolve it: Comparative studies measuring and analyzing geopolitical bias patterns across different language families

### Open Question 3
- Question: What are the real-world consequences of geopolitical bias in language models for users from different countries or cultural backgrounds?
- Basis in paper: [explicit] Acknowledges this as important future direction but doesn't investigate actual impacts
- Why unresolved: Establishes existence of bias but doesn't explore how this affects users in practice
- What evidence would resolve it: User studies, impact assessments, or case studies examining real-world effects

## Limitations

- Reliance on Google Translate for template-based translation may introduce artifacts that artificially inflate cross-lingual inconsistencies
- Study only examines one model (GPT-3.5) and one type of task (territorial disputes), limiting generalizability
- No baseline established for what constitutes "good" cross-lingual consistency compared to human performance or other knowledge systems

## Confidence

- High confidence: Empirical finding that GPT-3.5 shows different response patterns across languages for territorial disputes questions
- Medium confidence: Claim that these inconsistencies represent "geopolitical bias" stemming from culturally biased training data
- Low confidence: Broader implication that multilingual LMs struggle with cross-lingual reasoning in general

## Next Checks

1. **Translation artifact isolation**: Repeat the experiment using professional human translators for a subset of prompts to determine whether machine translation introduces systematic biases that drive the observed cross-lingual inconsistencies.

2. **Control for template effects**: Test the same territorial disputes using naturally written prompts in each language (avoiding templates) to see if cross-lingual inconsistencies persist when translation consistency is no longer guaranteed.

3. **Cross-model comparison**: Evaluate multiple LMs (GPT-4, Claude, LLaMA) on the same task to determine whether cross-lingual inconsistencies are specific to GPT-3.5 or represent a broader challenge in multilingual LMs.