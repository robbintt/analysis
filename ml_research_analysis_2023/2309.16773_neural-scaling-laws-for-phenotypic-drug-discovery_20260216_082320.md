---
ver: rpa2
title: Neural scaling laws for phenotypic drug discovery
arxiv_id: '2309.16773'
source_url: https://arxiv.org/abs/2309.16773
tags:
- dnns
- data
- drug
- discovery
- molecules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether neural scaling laws apply to deep
  neural networks (DNNs) trained for small molecule drug discovery using the JUMP
  dataset. DNNs explicitly trained for Phenotypic Chemistry Arena (Pheno-CA) tasks
  did not show continuous performance improvement with increased data or model size.
---

# Neural scaling laws for phenotypic drug discovery

## Quick Facts
- arXiv ID: 2309.16773
- Source URL: https://arxiv.org/abs/2309.16773
- Authors: 
- Reference count: 14
- DNNs pretrained with IBP outperform task-supervised DNNs on Pheno-CA tasks and follow neural scaling laws

## Executive Summary
This study investigates whether neural scaling laws apply to deep neural networks (DNNs) trained for small molecule drug discovery using the JUMP dataset. DNNs explicitly trained for Phenotypic Chemistry Arena (Pheno-CA) tasks did not show continuous performance improvement with increased data or model size. To address this, the authors introduce the Inverse Biological Process (IBP) precursor task, designed to resemble successful causal objective functions in NLP. DNNs pretrained with IBP and then probed for Pheno-CA performance significantly outperformed task-supervised DNNs and exhibited monotonic improvements with data and model scale. The findings suggest that DNNs can effectively aid drug discovery tasks and provide insights into the amount of experimental data needed for desired accuracy levels.

## Method Summary
The authors compared DNNs trained directly on Pheno-CA tasks versus DNNs first pretrained with an Inverse Biological Process (IBP) precursor task, then probed for Pheno-CA performance. They tested various model sizes (1-12 layers with different feature counts), data sizes (1e3 to 1e5 molecules), and replicates (1% to 100%). The JUMP dataset containing 711,974 compound perturbation images and 51,185 CRISPR perturbation images was preprocessed by aggregating cells per well, normalizing by plate, and applying PCA whitening. Four Pheno-CA tasks were evaluated: MoA deconvolution, target deconvolution, molecule deconvolution, and compound discovery.

## Key Results
- IBP-pretrained DNNs significantly outperformed task-supervised DNNs on all Pheno-CA tasks
- IBP-trained DNNs followed neural scaling laws, with performance improving monotonically with data and model scale
- Task-supervised DNNs did not exhibit neural scaling law behavior, with performance either plateauing or degrading with increased data/model size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: IBP-pretrained DNNs outperform task-supervised DNNs on Pheno-CA tasks.
- **Mechanism**: IBP training forces DNNs to learn causal models of biological processes by predicting perturbations from phenotypes, creating generalizable representations.
- **Core assumption**: Learning to invert biological processes captures underlying biophysical mechanisms useful for multiple downstream tasks.
- **Evidence anchors**:
  - [abstract]: "DNNs pretrained with IBP and then probed for Pheno-CA performance significantly outperformed task-supervised DNNs"
  - [section]: "We indeed find that DNNs first trained with IBP then probed for performance on the Pheno-CA significantly outperform task-supervised DNNs"
  - [corpus]: Weak correlation; neighbor papers focus on different domains (single-cell transcriptomics, diffusion models) rather than IBP specifically
- **Break condition**: If biological processes are too complex for DNNs to learn causal relationships, or if the phenotype-to-perturbation mapping is highly non-deterministic.

### Mechanism 2
- **Claim**: IBP-trained DNNs follow neural scaling laws for Pheno-CA tasks.
- **Mechanism**: As more out-of-distribution molecules are added to training, IBP-trained DNNs continuously improve their ability to generalize across the phenotypic space.
- **Core assumption**: The phenotypic manifold is sufficiently smooth that additional data points help the model better capture its structure.
- **Evidence anchors**:
  - [abstract]: "the performance of these IBP-trained DNNs monotonically improves with data and model scale"
  - [section]: "IBP-trained DNNs also followed a scaling law on target deconvolution... Model performance was linearly predicted by the number of out-of-distribution molecules included in training"
  - [corpus]: Weak evidence; corpus focuses on scaling laws in other domains but not specifically for IBP in drug discovery
- **Break condition**: If the phenotypic space has many local minima or if additional data becomes redundant, violating the assumed smooth manifold structure.

### Mechanism 3
- **Claim**: Task-supervised DNNs do not follow neural scaling laws for Pheno-CA tasks.
- **Mechanism**: Direct task supervision causes overfitting to the specific distribution of Pheno-CA molecules, preventing generalization from additional data.
- **Core assumption**: Task-specific training creates representations that are too specialized to benefit from additional out-of-distribution data.
- **Evidence anchors**:
  - [section]: "DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up"
  - [section]: "the performance of these 'task-supervised' DNNs was either unaffected or hurt by an increase in data and model sizes"
  - [corpus]: No direct evidence; this is inferred from the paper's experimental results
- **Break condition**: If task-supervised training could be modified to incorporate broader biological knowledge or if the Pheno-CA tasks are too simple to require generalization.

## Foundational Learning

- **Concept**: Neural scaling laws
  - Why needed here: Understanding that performance improvements in DNNs can be predicted by compute, data, and model size rather than algorithmic innovations is crucial for this study's hypothesis and experimental design.
  - Quick check question: What are the key resources that neural scaling laws suggest control DNN performance improvements?

- **Concept**: Phenotypic drug discovery
  - Why needed here: The paper investigates DNN performance on phenotypic drug discovery tasks, which involve predicting drug mechanisms, targets, and identities from cellular phenotype images.
  - Quick check question: What distinguishes phenotypic drug discovery from target-based drug discovery approaches?

- **Concept**: High-content screening (HCS) and image-based profiling
  - Why needed here: The JUMP dataset used in this study is based on HCS data, where cellular phenotypes are captured through multiplexed fluorescent staining, providing rich information for DNN training.
  - Quick check question: How does image-based high-content screening provide information about drug effects on cells?

## Architecture Onboarding

- **Component map**:
  JUMP dataset preprocessing (cell aggregation, normalization, PCA) -> IBP precursor task training -> Task-specific MLP readouts -> Pheno-CA evaluation

- **Critical path**:
  1. Load and preprocess JUMP dataset
  2. Train IBP model on out-of-distribution molecules
  3. Freeze IBP model weights
  4. Train task-specific MLP readouts for each Pheno-CA task
  5. Evaluate performance and analyze scaling relationships

- **Design tradeoffs**:
  - IBP pretraining vs. direct task supervision: IBP provides better generalization but requires additional training time
  - Model depth vs. width: Deeper models showed mixed results, with 9-layer models performing best
  - Amount of training data: More out-of-distribution molecules improved performance, but with diminishing returns

- **Failure signatures**:
  - Task-supervised models plateau or degrade with increased data/model size
  - IBP models fail to improve with additional data (suggesting learning capacity saturation)
  - Poor separation of phenotypes in learned representations (visible through UMAP)

- **First 3 experiments**:
  1. Replicate the baseline comparison between IBP-pretrained and task-supervised models on a single Pheno-CA task
  2. Test the effect of varying the amount of out-of-distribution data on IBP model performance
  3. Compare different model architectures (varying depth and width) on IBP pretraining efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of layers for DNNs trained on the IBP precursor task for phenotypic drug discovery?
- Basis in paper: [explicit] The authors found that 9-layer DNNs were more accurate than 12-layer DNNs for MoA and target deconvolution tasks.
- Why unresolved: While the authors observed that 9-layer DNNs performed better than 12-layer DNNs for some tasks, they did not determine the optimal number of layers for all tasks or investigate the reasons behind this performance difference.
- What evidence would resolve it: Systematic experiments varying the number of layers for each task and analyzing the impact on performance would help determine the optimal number of layers for DNNs trained on the IBP precursor task.

### Open Question 2
- Question: How does the amount of experimental replicates affect the scaling laws for DNNs trained on the IBP precursor task?
- Basis in paper: [explicit] The authors found that more replicates lead to better models on average and that more data generally improved the scaling law slope for MoA deconvolution.
- Why unresolved: While the authors observed the impact of replicates on scaling laws for MoA deconvolution, they did not investigate the effect of replicates on scaling laws for other tasks or determine the optimal number of replicates for each task.
- What evidence would resolve it: Experiments varying the number of replicates for each task and analyzing the impact on scaling laws would help determine the optimal number of replicates for DNNs trained on the IBP precursor task.

### Open Question 3
- Question: Can the IBP precursor task be improved to achieve better performance on the Pheno-CA tasks?
- Basis in paper: [inferred] The authors introduced the IBP precursor task to resemble successful causal objective functions in NLP, but they did not explore other potential precursor tasks or modifications to the IBP task.
- Why unresolved: The authors did not investigate alternative precursor tasks or modifications to the IBP task that could potentially improve performance on the Pheno-CA tasks.
- What evidence would resolve it: Developing and testing alternative precursor tasks or modifications to the IBP task and comparing their performance on the Pheno-CA tasks would help identify potential improvements to the IBP precursor task.

## Limitations
- Generalizability of IBP pretraining across different biological systems and phenotypic readouts remains untested
- Exact biological interpretability of representations learned through IBP pretraining is unclear
- Performance improvements may be specific to the JUMP dataset characteristics and may not transfer to other high-content screening platforms

## Confidence
- High confidence: IBP pretraining consistently outperforms task-supervised learning on Pheno-CA tasks
- Medium confidence: IBP-trained models follow neural scaling laws for phenotypic drug discovery
- Medium confidence: Task-supervised models do not exhibit neural scaling law behavior for Pheno-CA tasks
- Low confidence: The specific architectural choices (9-layer model depth) are optimal across all scenarios

## Next Checks
1. Test IBP pretraining transfer to a different high-content screening dataset (e.g., Broad LINCS L1000) to assess generalizability
2. Perform ablation studies varying the ratio of in-distribution to out-of-distribution molecules in IBP pretraining
3. Compare computational efficiency of IBP pretraining versus task-supervised learning across different model sizes and datasets