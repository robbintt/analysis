---
ver: rpa2
title: 'Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large
  Language Models for Industrial NLP'
arxiv_id: '2309.05619'
source_url: https://arxiv.org/abs/2309.05619
tags:
- data
- language
- gpt-4
- performance
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using ensemble disagreement scores as a proxy
  for human labels to assess large language model (LLM) performance in zero-shot,
  few-shot, and fine-tuned settings. The key idea is to train multiple models with
  different random seeds on the same data, then measure how often their predictions
  disagree on unlabeled test data - higher disagreement indicates lower model performance.
---

# Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP

## Quick Facts
- arXiv ID: 2309.05619
- Source URL: https://arxiv.org/abs/2309.05619
- Reference count: 35
- Key outcome: Ensemble disagreement scores provide accurate LLM performance estimates with MAE as low as 0.4% and 13.8% better than GPT-4 silver labels

## Executive Summary
This paper proposes using ensemble disagreement scores as a proxy for human labels to assess large language model performance in zero-shot, few-shot, and fine-tuned settings. The core insight is that models trained with different random seeds tend to agree on correct predictions and disagree on incorrect ones, making disagreement a measurable proxy for uncertainty and error. Experiments across 10 languages and 3 domains using XLM-R, GPT-3, and GPT-4 show disagreement scores provide more accurate performance estimates than traditional silver labeling approaches, with mean average error as low as 0.4%.

## Method Summary
The method trains multiple models with different random seeds on the same data, then measures prediction disagreement on unlabeled test data. Higher disagreement indicates lower model performance. A linear regression model maps agreement scores to F1 scores using data pairs from different languages, enabling performance prediction for new languages or domains without human labels. The approach was tested on keyphrase extraction across 10 languages (EN, ES, FR, IT, DE, NL, PT, JA, ZH, KO) and 3 domains (survey responses, Twitter data, customer service conversations).

## Key Results
- Disagreement scores achieved MAE as low as 0.4% for performance estimation
- 13.8% better accuracy than using GPT-4 as silver label source
- Consistently outperformed across all 10 languages and 3 domains tested
- Effective for zero-shot, few-shot, and fine-tuned LLM settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disagreement scores between ensemble models predict model error because well-calibrated models tend to agree on correct predictions and disagree on incorrect ones.
- Mechanism: Models trained with different random seeds capture slightly different patterns from the same data distribution. When both models are uncertain or wrong, their predictions diverge, creating measurable disagreement that correlates with error rate.
- Core assumption: The ensemble of models is well-calibrated and captures similar but not identical decision boundaries.
- Evidence anchors: [abstract] "We propose adapting disagreement scores... to assess model quality for these supervised NLP tasks"; [section 2.1] "The intuition is that models will agree on highly confident (likely correct) predictions and disagree on less confident (likely wrong) predictions"
- Break condition: If models are poorly calibrated, overfitting, or if the data distribution changes significantly between training and test sets.

### Mechanism 2
- Claim: Linear regression mapping between agreement scores and F1 scores provides accurate error estimation.
- Mechanism: By collecting multiple (F1 score, agreement score) pairs from different models/languages, we can fit a linear function that generalizes to predict F1 scores for new languages or domains.
- Core assumption: The relationship between agreement and F1 is approximately linear across different conditions.
- Evidence anchors: [section 2.1] "Based on these data pairs, we fit a simple linear regression model for error prediction"; [section 4.1] "we use the other 9 languages (36 points) to fit the curve and derive its final prediction (F1 score) as y = 0.809x + 0.09631"
- Break condition: Non-linear relationships between agreement and performance, or insufficient training data points to fit the regression.

### Mechanism 3
- Claim: GPT-4 as silver labels performs poorly compared to disagreement scores because it introduces additional uncertainty and domain mismatch.
- Mechanism: Using another model (GPT-4) to label data adds a second layer of prediction error, and zero-shot or few-shot prompting may not capture task-specific nuances as well as disagreement-based estimation.
- Core assumption: GPT-4's predictions are less reliable than ensemble disagreement for this specific task.
- Evidence anchors: [section 4.3] "Overall, we observe poor prediction capabilities using 100-shot GPT-4 as a label source"; [abstract] "Results across various languages and domains show disagreement scores provide a better estimation of model performance with mean average error (MAE) as low as 0.4% and on average 13.8% better than using silver labels"
- Break condition: If GPT-4 is specifically fine-tuned for the task or if the domain is well-represented in GPT-4's pretraining data.

## Foundational Learning

- Concept: Ensemble disagreement scoring
  - Why needed here: Provides a way to estimate model performance without human labels, which is expensive and time-consuming
  - Quick check question: What happens to disagreement scores when both models in the ensemble make the same error vs. when one is correct and one is wrong?

- Concept: Linear regression for error prediction
  - Why needed here: Maps the abstract disagreement scores to concrete performance metrics (F1) that can be used for decision-making
  - Quick check question: How many data points are needed to reliably fit a linear function between agreement and F1 scores?

- Concept: Model calibration
  - Why needed here: Ensures that the ensemble models' confidence scores are meaningful and that disagreement truly reflects uncertainty
  - Quick check question: What does it mean for a model to be well-calibrated, and how does this affect disagreement scores?

## Architecture Onboarding

- Component map: Training data → Multiple model training (different seeds) → Ensemble disagreement calculation → Linear regression model → Performance prediction for new data
- Critical path: Data preparation → Model training → Disagreement calculation → Regression fitting → Prediction
- Design tradeoffs: More models in ensemble = better estimation but higher computational cost; more data points for regression = better fit but requires more labeled data
- Failure signatures: High MAE values, disagreement scores that don't correlate with actual performance, regression models that overfit
- First 3 experiments:
  1. Train 2-3 models with different seeds on a small labeled dataset, calculate disagreement scores, and verify correlation with actual F1 scores
  2. Test the linear regression mapping on held-out data to check prediction accuracy
  3. Compare disagreement-based predictions against GPT-4 silver labels on the same test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the disagreement score approach perform when comparing models trained on vastly different datasets or with different architectures?
- Basis in paper: [inferred] The paper focuses on comparing models trained on the same dataset but with different random seeds, and does not explore the performance of the disagreement score approach when comparing models trained on different datasets or with different architectures.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of the disagreement score approach when comparing models trained on different datasets or with different architectures.
- What evidence would resolve it: Experimental results comparing the disagreement score approach's performance when applied to models trained on different datasets or with different architectures would help resolve this question.

### Open Question 2
- Question: How does the disagreement score approach perform when applied to tasks other than keyphrase extraction (KPE)?
- Basis in paper: [explicit] The paper focuses on the KPE task and does not explore the performance of the disagreement score approach when applied to other NLP tasks.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of the disagreement score approach when applied to tasks other than KPE.
- What evidence would resolve it: Experimental results comparing the disagreement score approach's performance when applied to various NLP tasks other than KPE would help resolve this question.

### Open Question 3
- Question: How does the disagreement score approach perform when applied to models with different sizes or complexities?
- Basis in paper: [inferred] The paper focuses on comparing models with similar sizes (XLM-R, GPT-3, and GPT-4) and does not explore the performance of the disagreement score approach when applied to models with different sizes or complexities.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of the disagreement score approach when applied to models with different sizes or complexities.
- What evidence would resolve it: Experimental results comparing the disagreement score approach's performance when applied to models with different sizes or complexities would help resolve this question.

## Limitations

- Limited generalizability to NLP tasks beyond keyphrase extraction
- Computational cost of training multiple models with different seeds
- Assumption that disagreement truly reflects uncertainty rather than model bias or data distribution shifts
- Focus on specific languages and domains may not represent all use cases

## Confidence

**High confidence**: The empirical results showing disagreement scores outperform GPT-4 silver labels for keyphrase extraction are well-supported by the experimental data. The mathematical framework for calculating disagreement scores and fitting linear regression models is straightforward and reproducible.

**Medium confidence**: The generalizability of disagreement scores to other NLP tasks beyond keyphrase extraction. While the mechanism appears sound, the paper doesn't validate the approach across diverse task types or more complex language understanding challenges.

**Low confidence**: The assumption that disagreement scores will maintain their predictive power as model architectures evolve or when applied to significantly different data distributions. The paper doesn't explore edge cases where ensemble disagreement might break down.

## Next Checks

1. **Cross-task validation**: Apply the disagreement score methodology to at least three different NLP tasks (e.g., sentiment analysis, named entity recognition, text classification) to test generalizability beyond keyphrase extraction. Compare MAE performance across tasks and identify any systematic differences.

2. **Ensemble size sensitivity analysis**: Systematically vary the number of models in the ensemble (2, 3, 5, 10) to determine the minimum effective ensemble size and the point of diminishing returns. This would help optimize the computational trade-off while maintaining prediction accuracy.

3. **Domain shift robustness test**: Evaluate how disagreement scores perform when there's significant domain shift between training and test data. Test this by training on one domain (e.g., survey responses) and applying the disagreement-based performance prediction to completely different domains (e.g., medical text or legal documents) to assess robustness to distribution shifts.