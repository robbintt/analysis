---
ver: rpa2
title: Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection
arxiv_id: '2311.10370'
source_url: https://arxiv.org/abs/2311.10370
tags:
- graph
- anomaly
- learning
- few-shot
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FMGAD, a few-shot graph anomaly detection model
  that combines contrastive learning with a deep-GNN message-enhanced reconstruction
  module. The core idea is to leverage self-supervised contrastive learning within
  and across views to capture intrinsic and transferable structural representations,
  while the deep-GNN module exploits few-shot label information to enable long-range
  propagation and assist the contrastive learning training.
---

# Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection

## Quick Facts
- arXiv ID: 2311.10370
- Source URL: https://arxiv.org/abs/2311.10370
- Authors: 
- Reference count: 33
- Key outcome: FMGAD achieves up to 60.35% improvement in AUC-ROC and 54.25% in AUC-PR over baselines on the yelpChi dataset.

## Executive Summary
FMGAD addresses the challenge of few-shot graph anomaly detection by combining multi-view contrastive learning with a deep-GNN message-enhanced reconstruction module. The approach leverages self-supervised contrastive learning within and across graph views to capture structural representations, while the deep-GNN module uses few-shot labeled anomalies to enable long-range propagation through high-pass filtering. Experimental results on six real-world datasets demonstrate state-of-the-art performance, particularly effective even with as few as 1-shot labeled anomalies.

## Method Summary
FMGAD is a few-shot graph anomaly detection model that integrates contrastive learning with a deep-GNN reconstruction module. The method creates multiple graph views through random walks with restart, then applies node-subgraph and subgraph-subgraph contrastive learning to capture intrinsic structural representations. A deep-GNN architecture with high-pass filtering enables effective propagation of few-shot label information while avoiding over-smoothing. The model jointly optimizes contrastive and reconstruction losses to produce anomaly scores for graph nodes.

## Key Results
- Achieves up to 60.35% improvement in AUC-ROC over baselines on yelpChi dataset
- Demonstrates strong performance with as few as 1-shot labeled anomalies
- Shows consistent improvements across six real-world datasets with both injected and organic anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of multi-view contrastive learning and deep-GNN message-enhanced reconstruction addresses both structural and attribute-context anomalies.
- Mechanism: Multi-view contrastive learning captures intrinsic and transferable structural representations by contrasting target nodes with positive and negative subgraph pairs across views. The deep-GNN reconstruction module propagates supervision signals from few-shot labeled nodes to deeper unlabeled nodes using high-pass filtering, enabling long-range propagation without over-smoothing.
- Core assumption: Anomalies manifest as mismatches between nodes and their local context, and both structural and attribute information are critical for detection.
- Evidence anchors:
  - [abstract] "FMGAD leverages a self-supervised contrastive learning strategy within and across views to capture intrinsic and transferable structural representations. Furthermore, we propose the Deep-GNN message-enhanced reconstruction module..."
  - [section] "Unlike traditional anomaly detection methods that focus on vector data, graph anomaly detection requires the simultaneous exploration of both node attribute information and graph structure information..."
  - [corpus] Weak evidence - only general contrastive learning papers found, no direct evidence for this specific combination.
- Break condition: If the graph augmentation method introduces artificial anomalies or the high-pass filtering fails to prevent over-smoothing in deep layers.

### Mechanism 2
- Claim: Graph augmentation using random walks with restart (RWR) provides valid views for contrastive learning without introducing artificial anomalies.
- Mechanism: RWR samples subgraphs centered on target nodes with a restart probability, ensuring that the sampled subgraphs maintain the original graph's anomaly distribution while providing different perspectives for contrastive learning.
- Core assumption: Standard augmentation techniques like edge modification or node masking may alter the underlying semantic features of the data, particularly affecting naturally occurring anomalies.
- Evidence anchors:
  - [section] "For graph anomaly detection, according to [reference], anomalies in graph nodes often manifest as a mismatch with their surrounding environment... Hence, we utilize random walks with restart (RWR) to obtain augmented views."
  - [corpus] Weak evidence - no direct evidence found for RWR specifically in graph anomaly detection context.
- Break condition: If the restart probability is not properly tuned, leading to either insufficient exploration or too much randomness in the sampled subgraphs.

### Mechanism 3
- Claim: The Deep-GNN architecture with high-pass filtering overcomes the over-smoothing problem while enabling long-range propagation of label information.
- Mechanism: The high-pass filtering GNN in the spectral domain is designed to overcome over-smoothing by focusing on high-frequency components of the graph signal, allowing for deeper layers without losing discriminative power.
- Core assumption: Conventional GNNs suffer from over-smoothing when increasing the number of layers, making it difficult to extend the receptive field in few-shot scenarios.
- Evidence anchors:
  - [section] "To address this challenge, we propose leveraging the concept of AutoEncoder from unsupervised methods to reconstruct attributes. Additionally, we introduce a scalable deep graph neural network (GNN) architecture to enhance the utilization of few-shot labels..."
  - [corpus] No direct evidence found for high-pass filtering in few-shot graph anomaly detection.
- Break condition: If the high-pass filtering does not effectively prevent over-smoothing, leading to loss of discriminative features in deep layers.

## Foundational Learning

- Concept: Graph neural networks (GNNs) and their message-passing mechanism
  - Why needed here: FMGAD is built upon GNN architectures for both the contrastive learning module and the deep-GNN reconstruction module.
  - Quick check question: What is the difference between low-pass and high-pass filtering in GNNs, and why is high-pass filtering preferred in the deep-GNN module?

- Concept: Contrastive learning and its application to graph data
  - Why needed here: The multi-view contrastive learning module is a core component of FMGAD, using intra-view and cross-view contrastive strategies.
  - Quick check question: How does the node-subgraph and subgraph-subgraph contrastive learning work in FMGAD, and what are the key differences between these two strategies?

- Concept: Few-shot learning and its challenges in graph anomaly detection
  - Why needed here: FMGAD is specifically designed for few-shot graph anomaly detection, addressing the challenge of limited labeled anomaly data.
  - Quick check question: What are the main challenges of few-shot learning in graph anomaly detection, and how does FMGAD address these challenges?

## Architecture Onboarding

- Component map:
  Input -> Graph Augmentation -> Multi-view Contrastive Learning -> Deep-GNN Reconstruction -> Output

- Critical path:
  1. Graph augmentation using RWR to create multiple views
  2. Multi-view contrastive learning to capture structural representations
  3. Deep-GNN reconstruction to propagate few-shot label information
  4. Joint training of contrastive learning and reconstruction modules
  5. Anomaly score calculation

- Design tradeoffs:
  - Number of RWR samples vs. computational efficiency
  - Number of Deep-GNN layers vs. over-smoothing risk
  - Trade-off parameters α, γ, ψ balancing different loss components
  - Subgraph size K vs. local context capture

- Failure signatures:
  - Poor performance on organic anomalies but good on injected anomalies: Suggests the model is overfitting to artificial patterns
  - Performance degradation with increasing Deep-GNN layers: Indicates over-smoothing issue
  - High variance in results across different runs: Suggests instability in contrastive learning or reconstruction modules

- First 3 experiments:
  1. Ablation study: Remove the Deep-GNN module and evaluate performance to confirm its contribution
  2. Sensitivity analysis: Vary the number of RWR samples and Deep-GNN layers to find optimal configuration
  3. Few-shot performance: Test FMGAD with different numbers of labeled anomalies (1-shot, 3-shot, 5-shot, etc.) to evaluate robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FMGAD model perform on larger-scale graphs with millions of nodes and edges?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of FMGAD on six real-world datasets, but does not explore its scalability to larger graphs.
- Why unresolved: The paper does not provide any experiments or analysis on the model's performance on larger-scale graphs, leaving the question of its scalability unanswered.
- What evidence would resolve it: Conducting experiments on larger-scale graphs with millions of nodes and edges, and comparing the performance of FMGAD with other state-of-the-art methods, would provide evidence of its scalability.

### Open Question 2
- Question: How does the choice of graph augmentation technique affect the performance of FMGAD?
- Basis in paper: [explicit] The paper mentions that random walks with restart (RWR) is used for graph augmentation, but does not explore the impact of different augmentation techniques.
- Why unresolved: The paper does not provide any experiments or analysis on the effect of different graph augmentation techniques on the performance of FMGAD.
- What evidence would resolve it: Conducting experiments with different graph augmentation techniques, such as node feature perturbation, edge modification, or Graph Diffusion, and comparing their impact on the performance of FMGAD, would provide evidence of the optimal augmentation technique.

### Open Question 3
- Question: How does the proposed Deep-GNN message-enhanced reconstruction module compare to other semi-supervised graph anomaly detection methods in terms of computational efficiency?
- Basis in paper: [inferred] The paper proposes the Deep-GNN message-enhanced reconstruction module to address the over-smoothing issue in few-shot scenarios, but does not compare its computational efficiency with other methods.
- Why unresolved: The paper does not provide any experiments or analysis on the computational efficiency of the Deep-GNN message-enhanced reconstruction module compared to other semi-supervised graph anomaly detection methods.
- What evidence would resolve it: Conducting experiments to measure the computational time and resources required by the Deep-GNN message-enhanced reconstruction module and comparing it with other semi-supervised graph anomaly detection methods would provide evidence of its computational efficiency.

## Limitations
- The specific implementation details of the high-pass filtering GNN are not fully specified, making exact reproduction challenging.
- The restart probability for random walk sampling is not explicitly stated, potentially affecting view quality.
- The theoretical analysis of why this specific combination works better than existing approaches is limited.

## Confidence
The core claim that combining multi-view contrastive learning with deep-GNN message propagation improves few-shot graph anomaly detection is **High confidence**, supported by comprehensive experiments on six real-world datasets showing consistent performance improvements (up to 60.35% AUC-ROC gains). The paper demonstrates strong empirical validation across both injected and organic anomalies, with robust results across varying shot counts.

However, several limitations exist: (1) The specific implementation details of the high-pass filtering GNN are not fully specified, making exact reproduction challenging. (2) The restart probability for random walk sampling is not explicitly stated, potentially affecting view quality. (3) The theoretical analysis of why this specific combination works better than existing approaches is limited.

## Next Checks
1. Reproduce the ablation study results to confirm the Deep-GNN module's contribution is as significant as claimed
2. Conduct sensitivity analysis varying the restart probability in RWR sampling to determine optimal parameters
3. Test FMGAD on additional graph datasets with different characteristics to evaluate generalizability beyond the six studied datasets