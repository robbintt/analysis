---
ver: rpa2
title: Diverse Projection Ensembles for Distributional Reinforcement Learning
arxiv_id: '2306.07124'
source_url: https://arxiv.org/abs/2306.07124
tags:
- projection
- distributional
- distribution
- ensemble
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using ensembles of different distributional
  reinforcement learning projections (e.g., categorical vs. quantile) to improve exploration.
---

# Diverse Projection Ensembles for Distributional Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.07124
- Source URL: https://arxiv.org/abs/2306.07124
- Authors: 
- Reference count: 40
- This paper proposes using ensembles of different distributional reinforcement learning projections (e.g., categorical vs. quantile) to improve exploration.

## Executive Summary
This paper introduces a novel approach to distributional reinforcement learning by combining diverse projection methods in an ensemble framework. The key insight is that different projections (categorical vs. quantile) induce distinct generalization behaviors in neural networks, leading to diverse uncertainty estimates. By measuring ensemble disagreement using the 1-Wasserstein distance and using it as an intrinsic exploration bonus, the proposed PE-DQN algorithm achieves superior performance on hard exploration tasks compared to existing methods.

## Method Summary
The method combines two distributional RL approaches - C51 (categorical) and QR-DQN (quantile) - into an ensemble. A projection mixture operator averages the different projections, and ensemble disagreement (measured by average 1-Wasserstein distance) serves as an exploration bonus. The algorithm trains both models simultaneously while propagating uncertainty through the ensemble, using the disagreement signal to encourage exploration of uncertain state-action pairs.

## Key Results
- PE-DQN achieves roughly half the episodic regret of baselines on deep sea exploration at larger problem sizes
- Significant performance improvements on Bsuite benchmarks, especially on hard exploration tasks
- Smaller ensemble (4 models) outperforms or matches larger ensembles (20 models) used by baselines
- Ablation studies confirm diverse projections are crucial - homogeneous ensembles fail to explore effectively

## Why This Works (Mechanism)

### Mechanism 1
- Different distributional projections (categorical vs. quantile) induce diverse generalization behaviors in neural networks
- Different projections impose different inductive biases that lead to distinct learned representations
- Core assumption: The return distribution is complex and unknown
- Evidence: Empirical results show different exploration behaviors on deep sea; weak direct evidence in corpus

### Mechanism 2
- Ensemble disagreement serves as reliable intrinsic exploration bonus
- Disagreement approximates epistemic uncertainty for optimistic action selection
- Core assumption: Ensemble members can disagree meaningfully despite moving toward joint targets
- Evidence: Theorem 3 establishes temporal consistency; no direct corpus evidence

### Mechanism 3
- Projection mixture operator combines projections while maintaining contraction properties
- Averaging projections creates bounded operator even if individual projections aren't contractions
- Core assumption: Individual projections are bounded and average modulus is less than 1/γ
- Evidence: Proposition 1 provides formal conditions; no corpus evidence

## Foundational Learning

- Concept: Distributional reinforcement learning (learning return distributions instead of expected values)
  - Why needed: Core contribution builds on distributional RL foundations
  - Quick check: What is the Bellman operator in distributional RL, and how does it differ from the classical Bellman operator?

- Concept: Wasserstein distance and its role in distributional RL
  - Why needed: Algorithm uses 1-Wasserstein distance for ensemble disagreement
  - Quick check: How is the 1-Wasserstein distance between two Dirac distributions defined, and why is it useful in this context?

- Concept: Ensemble methods and epistemic vs. aleatoric uncertainty
  - Why needed: Paper builds ensemble of distributional models using disagreement as epistemic uncertainty proxy
  - Quick check: What is the difference between epistemic and aleatoric uncertainty, and how does ensemble disagreement relate to epistemic uncertainty?

## Architecture Onboarding

- Component map: State-Action Pair -> Ensemble Models (C51 + QR-DQN) -> Projection Mixture Operator ΩM -> Value Estimate + Bonus -> Action Selection

- Critical path:
  1. Observe state, select action using greedy policy plus bonus
  2. Store transition in replay buffer
  3. Sample batch, compute bootstrapped return distributions from target ensemble
  4. Project bootstrapped distribution using both categorical and quantile projections
  5. Update both models to minimize projection loss
  6. Compute ensemble disagreement and update bonus estimator
  7. Periodically update target networks

- Design tradeoffs:
  - Ensemble size vs. diversity: More models increase diversity but also computational cost
  - Projection choice: Different projections offer different bias-variance tradeoffs
  - Bonus scaling: Exploration bonus weight β must balance exploration and exploitation

- Failure signatures:
  - Both models collapse to similar predictions, reducing exploration
  - Projections too restrictive, leading to poor return distribution representation
  - Bonus weight too high, causing excessive exploration and poor exploitation

- First 3 experiments:
  1. Implement single QR-DQN model and verify learning on CartPole
  2. Implement single C51 model and verify learning on CartPole
  3. Combine both into ensemble, implement ΩM, verify performance on small deep sea

## Open Questions the Paper Calls Out

### Open Question 1
- How does projection choice (categorical vs quantile) impact neural network generalization in distributional RL?
- Basis: Authors observe distinct generalization signatures between projections on toy regression task
- Why unresolved: Formal theoretical understanding remains elusive despite empirical evidence
- Resolution evidence: Rigorous theoretical analysis or comprehensive empirical studies across diverse RL tasks

### Open Question 2
- Can residual approximation errors from projection ensembles be effectively controlled and minimized?
- Basis: Authors acknowledge residual errors may persist even in convergence
- Why unresolved: Bounds established but methods to actively minimize errors not explored
- Resolution evidence: Techniques to reduce residual errors or theoretical analysis of projection-quality relationship

### Open Question 3
- How does ensemble size and composition affect performance and exploration efficiency?
- Basis: Authors show smaller ensembles (4 models) can outperform larger ones (20 models)
- Why unresolved: Optimal ensemble configuration for different tasks remains unclear
- Resolution evidence: Systematic experiments varying ensemble sizes and compositions across benchmarks

### Open Question 4
- Can projection mixture operator handle more complex or non-uniform mixtures of distributional models?
- Basis: Authors use uniform mixtures but don't explore sophisticated mixture strategies
- Why unresolved: Focus on uniform mixtures without investigating non-uniform or learned weights
- Resolution evidence: Comparisons of uniform vs non-uniform mixture strategies or theoretical analysis of mixture weights

## Limitations

- Theoretical analysis assumes bounded return distributions that may not hold in practice
- Limited baseline comparison (only 3 methods) weakens claims about state-of-the-art performance
- Ablation study somewhat limited - only tests homogeneous C51 ensemble

## Confidence

- **High Confidence**: Core mechanism that different projections induce diverse generalization behaviors is well-supported by theory and empirical evidence
- **Medium Confidence**: 1-Wasserstein distance disagreement as exploration bonus is theoretically justified and empirically strong, but hyperparameter sensitivity needs more investigation
- **Medium Confidence**: Claims about outperforming state-of-the-art methods are supported but limited by baseline comparison set

## Next Checks

1. Test PE-DQN against additional exploration-focused baselines (UCB-RL, RND) to better establish relative performance on hard exploration tasks
2. Conduct hyperparameter sensitivity analysis for exploration bonus weight β across different environment difficulties
3. Evaluate ensemble disagreement correlation with actual epistemic uncertainty through controlled experiments with known uncertainty types