---
ver: rpa2
title: Co-design Hardware and Algorithm for Vector Search
arxiv_id: '2306.11182'
source_url: https://arxiv.org/abs/2306.11182
tags:
- search
- hardware
- fpga
- vector
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FANNS, an end-to-end and scalable vector
  search framework on FPGAs. Given a user-provided recall requirement on a dataset
  and a hardware resource budget, FANNS automatically co-designs hardware and algorithm,
  subsequently generating the corresponding accelerator.
---

# Co-design Hardware and Algorithm for Vector Search

## Quick Facts
- arXiv ID: 2306.11182
- Source URL: https://arxiv.org/abs/2306.11182
- Reference count: 40
- Primary result: FANNS achieves up to 23.0× and 37.2× speedup vs FPGA and CPU baselines respectively, and demonstrates superior scalability to GPUs.

## Executive Summary
This paper presents FANNS, an end-to-end framework for co-designing hardware and algorithm for vector search on FPGAs. Given a recall requirement and hardware budget, FANNS automatically explores the design space of IVF-PQ parameters and hardware microarchitectures to generate optimized FPGA accelerators. The framework includes a performance model to predict throughput, a code generator for HLS C++ implementations, and a hardware TCP/IP stack for scale-out deployment.

## Method Summary
FANNS explores the IVF-PQ algorithm parameter space (nlist, nprobe, K) and hardware design space (PE counts, cache strategy, microarchitecture) to find optimal configurations. It uses a performance model to predict QPS and resource consumption, generates HLS C++ code for the selected design, and synthesizes it for FPGA deployment. The framework supports scale-out through a hardware TCP/IP stack and evaluates performance on SIFT and Deep datasets against CPU, GPU, and baseline FPGA implementations.

## Key Results
- Up to 23.0× speedup over FPGA baselines and 37.2× over CPU baselines for single-accelerator configurations
- 5.5× and 7.6× speedup over GPUs in median and P95 latency respectively in eight-accelerator setup
- Superior scalability to GPUs with projected 42.1× P99 latency improvement at 1024 accelerators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance gain comes from matching accelerator hardware to specific algorithm parameter sets rather than using generic design
- Mechanism: By profiling bottlenecks for different nprobe, nlist, and K values, system allocates PEs to stages where they will be most utilized
- Core assumption: Bottleneck analysis per parameter set accurately predicts runtime behavior across queries
- Evidence anchors:
  - [abstract] "accelerators without algorithm parameter awareness are 1.3∼23.0× slower than co-designed accelerators"
  - [section 3.1] "bottlenecks shift across different algorithm parameters"
- Break condition: If parameter-recall relationship shifts across datasets, performance model predictions become unreliable

### Mechanism 2
- Claim: FPGAs allow rapid exploration of large hardware-algorithm design space, infeasible with ASICs
- Mechanism: Reconfigurable nature enables generating many different microarchitectures per stage and evaluating them against performance model
- Core assumption: HLS synthesis can map generated designs to FPGA within resource constraints
- Evidence anchors:
  - [section 3.4] "leverage reconfigurability of FPGAs to show trade-offs between different designs"
  - [section 5.2.1] PE design details only possible to test efficiently on FPGA
- Break condition: If FPGA compilation fails frequently due to routing congestion, exploration throughput drops

### Mechanism 3
- Claim: Scale-out performance superior to GPUs because FPGAs have stable, low-variance latency distributions
- Mechanism: FPGAs process queries in deeply pipelined fashion with fixed logic, avoiding tail latency spikes common in GPU multi-instance setups
- Core assumption: Latency variance remains low across large cluster sizes
- Evidence anchors:
  - [section 7.3.2] "FPGAs achieve 5.5× and 7.6× speedup over GPUs in median and P95 latency"
  - [section 7.3.2] "FPGAs exhibit superior scalability compared to GPUs thanks to stable hardware processing pipeline"
- Break condition: If network stack overhead grows super-linearly with node count, FPGA advantage erodes

## Foundational Learning

- Concept: IVF-PQ algorithm stages (OPQ, IVFDist, SelCells, BuildLUT, PQDist, SelK)
  - Why needed here: Six stages have different computational characteristics; essential for PE design and bottleneck analysis
  - Quick check question: Which stage is responsible for converting PQ codes into approximate distances, and what data structure does it use?

- Concept: Product quantization (PQ) and asymmetric distance computation (ADC)
  - Why needed here: PQ reduces memory bandwidth; ADC uses lookup tables for fast distance approximation
  - Quick check question: In PQ, how many bytes are needed to represent a 128-dimensional vector quantized into 16 sub-vectors?

- Concept: FPGA HLS development and resource modeling
  - Why needed here: System generates C++ HLS code and must predict LUT, BRAM, URAM, FF, and DSP usage
  - Quick check question: What is approximate resource utilization ceiling used in experiments to avoid placement/routing failures?

## Architecture Onboarding

- Component map: Query -> PCIe -> accelerator kernel -> stages (OPQ->IVFDist->SelCells->BuildLUT->PQDist->SelK) -> result -> PCIe -> host
- Critical path: Query flows through six pipeline stages with PEs and FIFOs connecting them
- Design tradeoffs: More PEs in PQDist and SelK improve throughput for high nprobe but consume more DSPs/FFs; caching IVF index on-chip reduces latency but limits PE count
- Failure signatures: (a) If frequency target not met, QPS drops proportionally; (b) If FIFO depth too shallow, pipeline stalls; (c) If resource overcommitment, compilation fails
- First 3 experiments:
  1. Run single-accelerator with R@10=80% on SIFT100M and compare QPS to CPU baseline
  2. Vary nprobe from 5 to 33 and observe QPS change to validate bottleneck prediction
  3. Deploy 2-node scale-out and measure latency distribution versus single-node to confirm low tail latency

## Open Questions the Paper Calls Out
- What is the optimal algorithm-hardware co-design for IVF-PQ vector search across different datasets and recall requirements?
- How does performance of FANNS scale with number of accelerators beyond 8, and what are limiting factors?
- How does FANNS compare to other hardware-accelerated vector search approaches, such as ASIC designs or other FPGA implementations?

## Limitations
- Performance model generalizability across diverse datasets not fully validated beyond SIFT and Deep1B
- Scalability beyond 8 accelerators not experimentally verified, relying on projections
- Reliance on HLS synthesis success may vary across FPGA families
- Recall-parameter tuning assumes static dataset characteristics

## Confidence
- High: Mechanism linking parameter-aware co-design to measured speedups (1.3–23.0× over non-aware FPGAs)
- High: Pipeline latency stability claims supported by empirical tail-latency comparisons
- Medium: FPGA-vs-GPU scalability extrapolation beyond eight nodes relies on projection
- Low: Generality of bottleneck analysis given narrow dataset scope

## Next Checks
1. Test performance model on at least two additional ANN datasets (e.g., GIST, DEEP1B) with varying vector dimensionality to assess model robustness
2. Measure FPGA cluster latency variance up to 32 nodes in controlled network setup to verify super-linear tail-latency gains
3. Perform sensitivity analysis on HLS synthesis success rate when porting generated designs to different FPGA family (e.g., Intel Agilex)