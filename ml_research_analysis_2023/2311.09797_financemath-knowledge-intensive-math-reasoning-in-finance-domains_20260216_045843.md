---
ver: rpa2
title: 'FinanceMath: Knowledge-Intensive Math Reasoning in Finance Domains'
arxiv_id: '2311.09797'
source_url: https://arxiv.org/abs/2311.09797
tags:
- knowledge
- liabilities
- llms
- math
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FinanceMath, a benchmark to evaluate large
  language models on knowledge-intensive math reasoning problems in finance. It contains
  1,259 problems combining textual and tabular content, requiring college-level financial
  knowledge.
---

# FinanceMath: Knowledge-Intensive Math Reasoning in Finance Domains

## Quick Facts
- arXiv ID: 2311.09797
- Source URL: https://arxiv.org/abs/2311.09797
- Reference count: 12
- 44 models evaluated; GPT-4 achieves 60.9% accuracy with Chain-of-Thought prompting

## Executive Summary
This paper introduces FinanceMath, a benchmark designed to evaluate large language models on knowledge-intensive mathematical reasoning problems in finance domains. The benchmark contains 1,259 problems that combine textual and tabular content, requiring college-level financial knowledge to solve. Each problem includes expert-annotated Python solutions and is supported by a knowledge bank containing 1,760 financial terms. The authors evaluate 44 models using Chain-of-Thought and Program-of-Thought prompting strategies, finding that even the best-performing model (GPT-4) achieves only 60.9% accuracy, substantially below estimated human expert performance of 92%. Knowledge augmentation through external retrieval improves performance but remains significantly below human-level accuracy.

## Method Summary
The FinanceMath benchmark evaluates LLMs on knowledge-intensive math reasoning problems in finance using two prompting strategies: Chain-of-Thought (CoT) and Program-of-Thought (PoT). Models are tested with and without knowledge augmentation from an external knowledge bank containing 1,760 financial terms. The evaluation uses expert-annotated Python solutions as ground truth, with accuracy measured as the percentage of correct answers. The benchmark includes problems with both textual and tabular content, requiring integration of mathematical computation with domain-specific financial knowledge.

## Key Results
- GPT-4 achieves 60.9% accuracy with Chain-of-Thought prompting, far below estimated human expert performance of 92%
- Knowledge augmentation improves performance, with Gemini-1.5-Pro increasing from 47.5% to 54.5% accuracy
- Open-source models score below 20% accuracy on the benchmark
- Program-of-Thought prompting yields lower performance than Chain-of-Thought (GPT-4: 45.4% vs 60.9%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 significantly outperforms other models due to better integration of domain knowledge and reasoning capabilities
- Mechanism: GPT-4's architecture allows it to effectively combine mathematical computation with financial domain knowledge
- Core assumption: The model's pre-training included sufficient exposure to financial terminology and concepts
- Evidence anchors: GPT-4 achieves 60.9% accuracy using CoT prompting, significantly higher than other models
- Break condition: If the model encounters financial concepts entirely outside its training distribution, performance would degrade significantly

### Mechanism 2
- Claim: Knowledge augmentation through external retrieval improves model performance by providing relevant context
- Mechanism: Incorporating question-relevant financial knowledge into the prompt context helps models bridge gaps in their implicit knowledge
- Core assumption: The retrieved knowledge is both relevant and comprehensible to the model when included in the prompt
- Evidence anchors: Knowledge augmentation improves Gemini-1.5-Pro performance from 47.5% to 54.5%
- Break condition: If retrieval quality is poor or knowledge terms are too specialized, the augmentation may introduce noise rather than improvement

### Mechanism 3
- Claim: Program-of-Thought prompting enables better numerical reasoning by separating computation from reasoning steps
- Mechanism: By generating executable Python code, PoT prompting allows the model to offload complex calculations to an interpreter
- Core assumption: The generated code is syntactically correct and semantically aligned with the reasoning process
- Evidence anchors: GPT-4 achieves 45.4% accuracy using PoT prompting compared to 60.9% with CoT
- Break condition: If the model struggles with code generation or the problem requires non-standard mathematical operations, PoT may fail

## Foundational Learning

- Concept: Financial domain knowledge
  - Why needed here: Problems require understanding of financial concepts like "proportionate consolidation method" and "passive equity ownership interest"
  - Quick check question: Can you explain what "proportionate consolidation" means in financial reporting?

- Concept: Tabular data interpretation
  - Why needed here: 39% of problems include tables requiring data extraction and manipulation
  - Quick check question: How would you extract specific values from a financial statement table given row and column headers?

- Concept: Mathematical reasoning with real-world constraints
  - Why needed here: Problems combine mathematical operations with domain-specific rules and conditions
  - Quick check question: If a company increases ownership from 15% to 50%, how does this affect consolidation calculations?

## Architecture Onboarding

- Component map:
  Retriever -> Prompt generator -> LLM inference -> Answer extractor -> Evaluator

- Critical path:
  1. Retrieve relevant knowledge terms
  2. Construct augmented prompt
  3. Generate reasoning (CoT/PoT)
  4. Extract and validate answer
  5. Compare to ground truth

- Design tradeoffs:
  - Retriever choice (BM25 vs Ada Embedding) affects knowledge relevance and retrieval speed
  - Knowledge inclusion level (oracle vs retrieved) impacts performance vs realism
  - Prompt verbosity vs token limits affects context quality

- Failure signatures:
  - Poor retriever performance → irrelevant knowledge included → confused reasoning
  - Token limit exceeded → truncated context → incomplete reasoning
  - Code generation errors → PoT failures → no numerical answer

- First 3 experiments:
  1. Compare BM25 vs Ada Embedding retriever performance on a subset of questions
  2. Test oracle knowledge augmentation vs no augmentation to establish upper bound
  3. Evaluate CoT vs PoT prompting on problems with and without tabular data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of expert-annotated Python solutions in KnowledgeMATH impact the accuracy and reliability of LLM evaluations compared to benchmarks that use textual or mathematical equation solutions?
- Basis in paper: [explicit] The paper states that KnowledgeMATH provides expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment
- Why unresolved: The paper does not provide a comparative analysis of LLM performance on KnowledgeMATH versus other benchmarks with different solution formats
- What evidence would resolve it: Comparative studies evaluating LLM performance on KnowledgeMATH and other benchmarks with textual or mathematical equation solutions

### Open Question 2
- Question: What are the specific limitations of current LLMs in understanding and applying domain-specific financial knowledge, as evidenced by their performance on KnowledgeMATH?
- Basis in paper: [inferred] The paper mentions that even the best-performing model (GPT-4) achieves only 60.9% accuracy with CoT, and that open-source LLMs score below 20% in accuracy
- Why unresolved: The paper does not provide a detailed analysis of the specific areas where LLMs struggle with financial knowledge
- What evidence would resolve it: In-depth error analysis of LLM responses on KnowledgeMATH, identifying common misconceptions or gaps in financial understanding

### Open Question 3
- Question: How can the knowledge retrieval and integration strategies employed in KnowledgeMATH be further optimized to improve LLM performance on knowledge-intensive tasks?
- Basis in paper: [explicit] The paper discusses the effectiveness of knowledge augmentation, noting that including question-relevant knowledge into the prompt can improve LLM performance
- Why unresolved: The paper does not explore alternative knowledge retrieval or integration methods beyond the ones tested
- What evidence would resolve it: Experiments comparing different knowledge retrieval algorithms, prompt engineering techniques, or model architectures for integrating external knowledge

## Limitations

- The knowledge bank's coverage of 1,760 financial terms may not be comprehensive enough to capture all domain-specific nuances in financial reasoning
- The performance gap between GPT-4 (60.9%) and estimated human expert performance (92%) suggests the benchmark may be more challenging than typical financial problem-solving scenarios
- The evaluation relies on expert-annotated Python solutions as ground truth, which may introduce human bias or overlook alternative valid solution approaches

## Confidence

- **Medium confidence**: Claims about GPT-4's superior performance relative to other models
- **Medium confidence**: Effectiveness of knowledge augmentation (improvement from 47.5% to 54.5% for Gemini-1.5-Pro)
- **Low confidence**: Program-of-Thought prompting's effectiveness for financial reasoning

## Next Checks

1. Systematically analyze the knowledge bank to identify gaps in financial terminology coverage and assess whether missing terms correlate with model failures on specific problem types

2. Conduct a controlled study with multiple financial experts solving a representative subset of problems to verify the claimed 92% human performance benchmark and establish confidence intervals

3. Test whether models that perform well on FinanceMath also show improved performance on general mathematical reasoning benchmarks (like GSM8K or MATH) when augmented with financial knowledge, or if the knowledge transfer is domain-specific