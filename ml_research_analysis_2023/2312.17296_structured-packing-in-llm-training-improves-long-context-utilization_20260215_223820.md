---
ver: rpa2
title: Structured Packing in LLM Training Improves Long Context Utilization
arxiv_id: '2312.17296'
source_url: https://arxiv.org/abs/2312.17296
tags:
- training
- data
- context
- documents
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of suboptimal context utilization
  in long-context language models (LCLMs), which limits their practical applications.
  The authors propose Structured Packing for Long Context (SPLiCe), a method that
  constructs training examples by collating mutually relevant documents using a retrieval
  method like BM25 or Contriever.
---

# Structured Packing in LLM Training Improves Long Context Utilization

## Quick Facts
- arXiv ID: 2312.17296
- Source URL: https://arxiv.org/abs/2312.17296
- Reference count: 22
- Primary result: SPLiCe improves long-context utilization by collating semantically related documents during training, achieving significant performance gains on Qasper and HotpotQA tasks

## Executive Summary
This paper addresses the challenge of suboptimal context utilization in long-context language models (LCLMs) by introducing Structured Packing for Long Context (SPLiCe). The method constructs training examples by collating mutually relevant documents using retrieval methods like BM25 or Contriever, increasing semantic interdependence in training data. This approach encourages models to better attend to and utilize information spread across longer contexts, significantly improving performance on long-context tasks while mitigating the lost-in-middle phenomenon often observed in large models.

## Method Summary
SPLiCe is a data structuring method that creates training examples by building trees of semantically related documents. Starting with a seed document, the method uses a retriever (BM25 or Contriever) to find top-k related documents in a breadth-first manner, constructing a tree of related content. The tree is then flattened using level-order traversal and concatenated into a single training example. The authors compare SPLiCe against random document packing and repository-level organization baselines, evaluating across model sizes from 3B to 13B parameters on tasks including question answering and code-to-natural-language transfer.

## Key Results
- SPLiCe significantly improves model performance on long-context tasks like Qasper and HotpotQA, with a 3B model achieving 23.9 F1 compared to 23.1 for baseline
- Brief fine-tuning with SPLiCe is sufficient to realize these benefits, demonstrating efficient training
- SPLiCe effectively mitigates the lost-in-middle phenomenon commonly observed in large models
- Training on programming code with SPLiCe enhances performance on natural language tasks, showing transfer effects

## Why This Works (Mechanism)

### Mechanism 1: Increasing Semantic Interdependence
By collating mutually relevant documents into single training examples, SPLiCe increases semantic interdependence in training data, encouraging models to develop stronger long-range attention patterns. This addresses the challenge of models losing track of information in long contexts by providing more cross-document dependencies to anchor attention.

### Mechanism 2: Mitigating Lost-in-Middle
Structured packing presents related documents together, reducing the likelihood that models lose track of information in the middle of long contexts. By maintaining semantic coherence through related content, the method helps models sustain attention across longer sequences.

### Mechanism 3: Transfer from Code to Natural Language
Training on highly structured code data with inherent dependencies appears to transfer to better handling of long, complex natural language contexts. This suggests that skills learned from processing interdependent code structures can generalize to natural language processing tasks.

## Foundational Learning

- **Semantic similarity and retrieval**: Understanding how semantic similarity is measured (BM25 vs. Contriever) is crucial for implementing SPLiCe. Quick check: What is the key difference between statistical-based retrieval (BM25) and learned retrieval (Contriever) in terms of how they measure document similarity?

- **Attention mechanisms in Transformers**: SPLiCe aims to improve long-context utilization, which is fundamentally tied to how attention mechanisms process information across sequences. Quick check: How does the "lost-in-middle" phenomenon relate to the attention patterns learned by standard Transformers in long sequences?

- **Transfer learning**: The paper demonstrates transfer effects from code to natural language tasks. Understanding transfer learning principles is important for interpreting these results. Quick check: What factors might influence whether skills learned from processing code data transfer effectively to natural language tasks?

## Architecture Onboarding

- **Component map**: Retriever (BM25 or Contriever) -> Tree builder -> Traversal strategy -> Tokenizer -> Model
- **Critical path**: 1) Sample seed document from corpus, 2) Retrieve top-k related documents, 3) Build tree iteratively, 4) Flatten tree using level-order traversal, 5) Tokenize and feed to model
- **Design tradeoffs**: BM25 is faster but may not capture semantic similarity as well as Contriever; larger k values create more diverse examples but may introduce noise; different traversal strategies impact how models learn to attend across concatenated documents
- **Failure signatures**: Poor retrieval quality leads to irrelevant document combinations; overly large trees cause processing difficulties; disrupted information flow from traversal strategy impedes coherent representation learning
- **First 3 experiments**: 1) Compare perplexity of SPLiCe model vs. baseline on held-out long-document evaluation set, 2) Evaluate downstream task performance on Qasper, 3) Ablation study varying tree breadth (k) parameter

## Open Questions the Paper Calls Out

1. How does the number of retrieved documents (k) in SPLiCe impact the performance of large-scale language models across different downstream tasks?
2. How does SPLiCe perform when integrated with other long-context data preparation methods, such as using long conversational data?
3. How does SPLiCe perform when the granularity of the pieces from which the training examples are constructed is varied (e.g., using paragraphs or sentences instead of documents)?

## Limitations

- Empirical validation focuses primarily on question-answering tasks, leaving uncertainty about generalizability to other long-context domains
- Retrieval-based document collation may introduce biases toward certain types of semantic relationships while missing others
- Computational overhead of tree-based retrieval and construction is not thoroughly analyzed

## Confidence

**High Confidence**: SPLiCe improves long-context utilization compared to random packing baselines; Structured packing mitigates lost-in-middle phenomenon; SPLiCe is effective across different model scales

**Medium Confidence**: Transfer effects from code training to natural language tasks are meaningful; BM25 and Contriever retrieval perform similarly; 32K context length is optimal target

**Low Confidence**: SPLiCe would maintain effectiveness for contexts significantly longer than 32K; method generalizes well to domains outside question-answering and code; structured packing provides advantages over other long-context training approaches

## Next Checks

1. **Retrieval Quality Validation**: Evaluate semantic relevance of retrieved documents using human annotation or automatic metrics on a held-out validation set to ensure tree construction produces genuinely related document combinations

2. **Attention Pattern Analysis**: Visualize and analyze attention weights in SPLiCe-trained models versus baselines to empirically verify more coherent cross-document attention patterns and specifically address lost-in-middle by comparing attention distributions across document positions

3. **Domain Generalization Test**: Apply SPLiCe to a different long-context domain (such as legal document analysis or scientific literature review) to assess whether performance improvements transfer beyond evaluated question-answering tasks, measuring both in-domain perplexity and downstream task performance