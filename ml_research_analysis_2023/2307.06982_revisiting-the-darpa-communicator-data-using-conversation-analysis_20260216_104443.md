---
ver: rpa2
title: Revisiting the DARPA Communicator Data using Conversation Analysis
arxiv_id: '2307.06982'
source_url: https://arxiv.org/abs/2307.06982
tags:
- system
- caller
- conversation
- have
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes human-computer dialogue failures using conversation
  analysis, focusing on cases where users swear or become frustrated. The author identifies
  that systems often fail to "account for" dis-preferred responses (e.g., "no flights
  available"), leading to user frustration.
---

# Revisiting the DARPA Communicator Data using Conversation Analysis

## Quick Facts
- arXiv ID: 2307.06982
- Source URL: https://arxiv.org/abs/2307.06982
- Reference count: 26
- One-line primary result: Systems lacking mixed initiative cause dialogue failures when users encounter dis-preferred responses like "no flights available"

## Executive Summary
This paper analyzes human-computer dialogue failures using conversation analysis, focusing on cases where users swear or become frustrated. The author identifies that systems often fail to "account for" dis-preferred responses, leading to user frustration. Using qualitative analysis of DARPA Communicator transcripts, the study finds that systems lacking mixed initiative—where users can introduce topics or explore options—are a key source of failure. The proposed solution is to enable systems to handle mixed initiative, allowing users to explain issues or explore alternatives, improving dialogue repair and user satisfaction.

## Method Summary
The paper applies conversation analysis (CA) methodology to DARPA Communicator transcripts, focusing on identifying where users express frustration through swearing or saying "start over." The author examines turn-taking organization, adjacency pairs, and preference structure to work backward from user sanctions to system failures. The analysis centers on two detailed transcripts where users became frustrated, using qualitative interpretation rather than statistical analysis to identify patterns of system failure.

## Key Results
- Users swear at computers when systems fail to account for dis-preferred responses like "no flights available"
- Systems lacking mixed initiative prevent users from introducing topics or exploring alternatives when things go wrong
- Conversation analysis can identify dialogue failures by tracing backward from user sanctions to system accounting failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Users swear at computers when systems fail to "account for" dis-preferred responses, leading to frustration.
- Mechanism: In conversation analysis, responses are either "seen but unnoticed," "accounted for," or "risk sanction." When a system gives a dis-preferred response (like "no flights available") without mitigation, it fails to account for its non-normal behavior, prompting user sanction in the form of swearing.
- Core assumption: Humans treat computers as social actors and expect them to follow conversational norms of accounting for dis-preferred responses.
- Evidence anchors:
  - [abstract]: "The premise is that humans swear at computers as a sanction and, as such, swear words represent a point of failure where the system did not behave as it should."
  - [section]: "When someone says something that is out of the ordinary or provides a dis-preferred response...then their actions are accounted for. Finally, If a speaker's actions cannot not be accounted for, then the speaker's conversational partner will impose sanctions."
  - [corpus]: Weak - no direct corpus evidence found for swearing patterns in this specific dataset.
- Break condition: If users don't perceive computers as social actors or don't apply conversational norms to machine interactions.

### Mechanism 2
- Claim: Systems lacking mixed initiative at the discourse structure level cause dialogue failures.
- Mechanism: When systems control the dialogue structure completely, users cannot introduce topics or explore alternatives when things go wrong. This prevents users from explaining issues or exploring alternatives, leading to frustration and system failure.
- Core assumption: Mixed initiative allows users to repair conversations by introducing new topics or exploring alternatives when the system's response is unsatisfactory.
- Evidence anchors:
  - [abstract]: "Using qualitative analysis of DARPA Communicator transcripts, the study finds that systems lacking mixed initiative—where users can introduce topics or explore options—are a key source of failure."
  - [section]: "The problem, according to my common sense as an expert language user, is what the system does next...From a machine we expect information but from a human, we expect assistance."
  - [corpus]: Weak - corpus analysis shows related work on mixed initiative but not direct evidence from DARPA Communicator data.
- Break condition: If the system's rigid structure actually aligns with user expectations for task completion.

### Mechanism 3
- Claim: Conversation analysis can identify dialogue failures by working backward from user sanctions to system failures in accounting.
- Mechanism: By identifying swear words or expressions of frustration as sanctions, researchers can trace backward through the transcript to find where the system failed to account for its dis-preferred response, revealing the root cause of failure.
- Core assumption: User sanctions (like swearing or saying "start over") are reliable indicators of system failures that can be analyzed to improve dialogue systems.
- Evidence anchors:
  - [abstract]: "Having identified where things went wrong, we can work backward through the transcripts and, using conversation analysis (CA) work out how things went wrong."
  - [section]: "The premise is that people verbally abuse dialogue systems, not when things go wrong, but when the system fails to account for its non-normal behaviour."
  - [corpus]: Weak - corpus analysis shows limited swearing in the dataset, suggesting this approach may not always yield sufficient data.
- Break condition: If user sanctions are rare or if other factors (like self-consciousness in testing) prevent users from expressing frustration naturally.

## Foundational Learning

- Concept: Conversation Analysis methodology
  - Why needed here: The paper relies on CA techniques to analyze dialogue failures qualitatively rather than statistically, focusing on turn-taking, adjacency pairs, and preference organization.
  - Quick check question: What is the difference between "seen but unnoticed" and "accounted for" responses in conversation analysis?

- Concept: Mixed initiative dialogue
  - Why needed here: The paper identifies lack of mixed initiative as a key failure mode, where systems control the dialogue structure and prevent users from introducing topics or exploring alternatives.
  - Quick check question: How does mixed initiative differ from simple error recovery in dialogue systems?

- Concept: Dis-preferred responses and their mitigation
- Why needed here: The paper shows that dis-preferred responses (like "no flights available") must be accompanied by hesitation, delay, and explanation to avoid user frustration.
  - Quick check question: What conversational markers typically accompany dis-preferred responses in human-human conversation?

## Architecture Onboarding

- Component map:
  ASR -> Dialogue manager with state tracking -> Mixed initiative controller -> Response generation module with preference handling -> User frustration detection system

- Critical path:
  1. User query input → 2. ASR processing → 3. Dialogue state update → 4. Response generation → 5. User feedback (frustration/satisfaction)

- Design tradeoffs:
  - Strict control vs. mixed initiative: More control reduces complexity but limits user repair options
  - Pre-defined responses vs. dynamic generation: Pre-defined is more reliable but less flexible for unexpected situations
  - Error recovery depth: Deeper recovery allows more user control but increases system complexity

- Failure signatures:
  - User saying "start over" multiple times
  - User providing unexpected responses to yes/no questions
  - User expressing frustration through intonation or explicit statements
  - System failing to recognize user attempts to introduce new topics

- First 3 experiments:
  1. Implement mixed initiative in a simple travel booking scenario and measure user satisfaction compared to controlled dialogue
  2. Add preference handling to dis-preferred responses and measure reduction in user frustration expressions
  3. Create a user frustration detection module and test its ability to identify problematic dialogue turns in existing transcripts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can conversation analysis (CA) techniques be systematically applied to improve the design of dialogue systems to better handle dis-preferred responses and mixed initiative dialogue?
- Basis in paper: [explicit] The paper concludes that the inability of systems to handle mixed initiative and account for dis-preferred responses leads to user frustration and sanctions (e.g., swearing). It suggests CA can help identify these issues and improve system design.
- Why unresolved: While the paper demonstrates CA's potential, it doesn't provide a concrete methodology for translating CA insights into system design changes. The challenge lies in bridging qualitative CA findings with quantitative system development.
- What evidence would resolve it: A case study showing a dialogue system redesigned based on CA analysis, with measurable improvements in user satisfaction and reduced sanctions compared to the original system.

### Open Question 2
- Question: What specific linguistic markers or behavioral cues can be reliably used to detect user frustration or the need for mixed initiative dialogue in real-time?
- Basis in paper: [inferred] The paper discusses hesitation, delay, and explicit markers like "well" and "uh" as signs of dis-preferred responses. It also notes that users may introduce topics to explore database options when systems fail to provide solutions.
- Why unresolved: While the paper identifies potential markers, it doesn't provide a comprehensive list or evaluate their reliability in real-time detection. The challenge is to develop robust, real-time detection methods that work across diverse user populations and contexts.
- What evidence would resolve it: A large-scale study analyzing diverse dialogue corpora to identify consistent linguistic and behavioral markers of frustration and mixed initiative needs, with high inter-rater reliability and real-time applicability.

### Open Question 3
- Question: How can dialogue systems be designed to effectively hand over initiative to users when encountering dis-preferred responses, and what are the best practices for reintegrating user-introduced topics into the main dialogue flow?
- Basis in paper: [explicit] The paper suggests that systems should allow users to introduce topics and explore alternatives when they encounter dis-preferred responses, such as "no records satisfy your request." It proposes that this approach can lead to better dialogue repair and user satisfaction.
- Why unresolved: While the paper advocates for mixed initiative, it doesn't provide specific design guidelines or evaluate different approaches to handing over and reintegrating initiative. The challenge lies in creating seamless transitions between system and user control without disrupting the dialogue flow.
- What evidence would resolve it: Comparative studies evaluating different mixed initiative strategies in dialogue systems, measuring user satisfaction, task completion rates, and dialogue efficiency across various domains and user populations.

## Limitations
- Limited empirical evidence: The paper provides weak direct evidence from the DARPA Communicator corpus, with minimal systematic analysis of frustration patterns across the full dataset
- Theoretical vs practical gap: While conversation analysis identifies failure patterns, the paper doesn't demonstrate how to translate these insights into practical system design improvements
- Sample size constraints: Analysis relies on two detailed transcripts rather than comprehensive corpus-wide quantitative analysis

## Confidence
- High Confidence: The theoretical framework of conversation analysis and preference organization is well-established in linguistics literature
- Medium Confidence: The identification of mixed initiative as a key failure mode is plausible but lacks systematic evidence across the full dataset
- Low Confidence: The direct connection between specific system behaviors and user swearing/frustration, while theoretically compelling, lacks sufficient empirical validation in this dataset

## Next Checks
1. **Corpus-wide Analysis**: Conduct systematic analysis of all DARPA Communicator transcripts to quantify the frequency of user frustration markers (swearing, "start over," etc.) and correlate them with specific system failure types
2. **Controlled Implementation Study**: Implement mixed initiative dialogue capabilities in a travel booking system and conduct user studies comparing satisfaction and task completion rates against traditional controlled dialogue systems
3. **Cross-dataset Validation**: Apply the same conversation analysis methodology to other dialogue datasets (e.g., Let's Go corpus, SmartKom) to test whether the identified failure patterns generalize beyond the DARPA Communicator data