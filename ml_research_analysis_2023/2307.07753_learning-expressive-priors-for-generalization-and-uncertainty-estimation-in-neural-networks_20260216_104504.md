---
ver: rpa2
title: Learning Expressive Priors for Generalization and Uncertainty Estimation in
  Neural Networks
arxiv_id: '2307.07753'
source_url: https://arxiv.org/abs/2307.07753
tags:
- learning
- prior
- neural
- networks
- priors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a prior learning method for Bayesian Neural
  Networks (BNNs) that exploits scalable and structured posteriors as informative
  priors with generalization guarantees. The key technical contributions are: (1)
  sums-of-Kronecker-product computations enabling tractable use of matrix normal distributions
  as expressive priors, and (2) derivation and optimization of tractable PAC-Bayes
  objectives for improved generalization bounds.'
---

# Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks

## Quick Facts
- arXiv ID: 2307.07753
- Source URL: https://arxiv.org/abs/2307.07753
- Reference count: 40
- Primary result: Learned structured priors achieve non-vacuous generalization bounds (as low as 0.885) and improve continual learning accuracy by 0.6-2.1% compared to isotropic priors

## Executive Summary
This paper presents a method for learning expressive priors for Bayesian Neural Networks (BNNs) that improves generalization and uncertainty estimation. The approach uses structured posteriors from large-scale training (via Laplace Approximation with Kronecker-Factored Curvature) as informative priors for downstream tasks. The method includes tractable computations for sums-of-Kronecker-products and PAC-Bayes objectives for principled hyperparameter optimization, and extends to continual learning through Bayesian Progressive Neural Networks.

## Method Summary
The method learns priors from structured posteriors obtained via Laplace Approximation with Kronecker-Factored Curvature (LA-KFAC) on source tasks like ImageNet. These structured priors, which capture parameter correlations within layers, are then used for Bayesian inference on target tasks. The approach includes efficient computation of sums-of-Kronecker-products using power methods and PAC-Bayes objectives for automatic hyperparameter selection. For continual learning, the method extends to Bayesian Progressive Neural Networks where learned priors initialize new columns while lateral connections use posteriors from previous columns.

## Key Results
- Learned priors achieve non-vacuous generalization bounds as low as 0.885, compared to vacuous bounds with isotropic priors
- In continual learning tasks, the method improves accuracy by 0.6-2.1% over baseline uncertainty estimation methods
- The approach effectively mitigates cold posterior effects by learning appropriate temperature scaling through PAC-Bayes optimization
- Superior performance in few-shot learning scenarios, particularly in the small-data regime

## Why This Works (Mechanism)

### Mechanism 1
Learning priors from structured posteriors (using KFAC/Fisher information) provides more expressive probabilistic representations than isotropic Gaussian priors. The Kronecker-factored structure captures parameter correlations within layers, encoding the geometry of the loss landscape from large-scale training data. This structured prior contains more information than a simple isotropic Gaussian.

### Mechanism 2
Optimizing PAC-Bayes bounds provides principled hyperparameter selection that improves generalization. By optimizing temperature scaling, curvature scaling, and prior precision jointly through PAC-Bayes objectives, the method automatically finds the right balance between fitting training data and maintaining prior consistency, without requiring a validation set.

### Mechanism 3
The sums-of-Kronecker-products computation enables tractable use of structured posteriors as priors. The power method approximates the sum of Kronecker products (F(1) + F(0)) as a single Kronecker product, maintaining computational tractability while preserving the structured information from both tasks.

## Foundational Learning

- **Concept**: Laplace Approximation with Kronecker-Factored Curvature (LA-KFAC)
  - Why needed here: Provides scalable method to obtain structured posteriors from large neural networks, which are then used as expressive priors
  - Quick check question: Can you explain why we use the Fisher matrix approximation instead of the full Hessian in LA-KFAC?

- **Concept**: PAC-Bayes Generalization Theory
  - Why needed here: Provides theoretical framework for bounding generalization and deriving optimization objectives for hyperparameter selection
  - Quick check question: What is the key trade-off that PAC-Bayes bounds balance between the posterior and prior?

- **Concept**: Kronecker Products and their properties
  - Why needed here: Understanding Kronecker products is essential for both LA-KFAC method and sums-of-Kronecker-products computation
  - Quick check question: Why does the Kronecker factorization enable more efficient storage and computation compared to the full Fisher matrix?

## Architecture Onboarding

- **Component map**: Source task training -> LA-KFAC posterior computation -> Prior learning module -> Target task inference -> PAC-Bayes optimization -> Uncertainty estimation
- **Critical path**: 1) Train source model on task T0, compute posterior via LA-KFAC; 2) Use this posterior as prior for task T1; 3) Compute posterior on T1, approximate F(1) + F(0) via power method; 4) Optimize PAC-Bayes objective to find hyperparameters; 5) For continual learning, use learned prior as initialization for all columns
- **Design tradeoffs**: Computational cost vs. expressiveness (full LA more expressive but intractable); Approximation quality vs. tractability (power method good approximation but not exact); Prior strength vs. flexibility (stronger prior provides more regularization but may hinder learning new tasks)
- **Failure signatures**: Poor performance on target task (prior too dissimilar from target task distribution); Unstable training (temperature scaling or weight decay mis-specified); Memory errors (Kronecker factors too large); Convergence issues (power method may not converge if singular values are close)
- **First 3 experiments**: 1) Reproduce ablation study: Compare learned prior vs. isotropic prior on NotMNIST using LeNet-5 architecture; 2) Test PAC-Bayes optimization: Implement curvature scaling and verify bounds improve compared to grid search; 3) Validate sums-of-Kronecker computation: Measure approximation error for different matrix sizes and compare to baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed sum-of-Kronecker-product computation scale to extremely large matrices (e.g., M, N > 10,000) in terms of memory and computation time?
  - Basis: Paper discusses O(nmaxK(N^2 + M^2)) complexity but only tests up to 20x20 matrices
  - Why unresolved: Scaling to larger matrices crucial for real-world applications
  - Evidence needed: Empirical results for matrices of size 10,000x10,000 or larger

- **Open Question 2**: Can the proposed method effectively mitigate the cold posterior effect in all deep learning architectures and datasets, or are there specific conditions where it fails?
  - Basis: Method shows effectiveness in some experiments but not complete elimination in all cases
  - Why unresolved: Only tests limited number of architectures and datasets
  - Evidence needed: Extensive experiments across diverse architectures and datasets

- **Open Question 3**: How does the performance of the proposed method compare to other prior learning methods (e.g., function-space priors, sparsity-inducing weight-space priors) in terms of generalization and uncertainty estimation?
  - Basis: Paper mentions related work but doesn't provide direct comparison
  - Why unresolved: Only compares to isotropic priors and baseline uncertainty estimation
  - Evidence needed: Thorough experimental comparison on common benchmarks

## Limitations

- The sums-of-Kronecker-products approximation relies on rank-one approximations that may not capture all correlations in complex tasks, particularly when source and target tasks have significantly different data distributions
- The PAC-Bayes optimization depends on approximations of the KL divergence and log-likelihood that may introduce errors in the generalization bounds
- The method requires large-scale training data to learn meaningful structured priors, limiting applicability to scenarios with limited source data

## Confidence

- **High confidence**: The core mathematical framework (LA-KFAC, PAC-Bayes theory, Kronecker products) is well-established and correctly implemented
- **Medium confidence**: The sums-of-Kronecker-products approximation quality and its impact on downstream performance requires further validation across diverse tasks
- **Medium confidence**: The superiority of learned priors over isotropic priors is demonstrated, but the ablation studies could be more comprehensive

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the learned priors when transferring between dissimilar datasets (e.g., ImageNet â†’ medical imaging) to assess robustness to domain shifts
2. **Scalability analysis**: Measure memory and computational requirements for larger architectures (ResNet-50, Transformer-based models) to verify practical applicability
3. **Ablation of approximation components**: Systematically disable the power method approximation and use exact computations on small models to quantify the approximation error impact