---
ver: rpa2
title: 'LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation'
arxiv_id: '2305.11490'
source_url: https://arxiv.org/abs/2305.11490
tags:
- image
- language
- images
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to endow a pre-trained LLM with multimodal
  capabilities, specifically the ability to understand and generate chest X-ray (CXR)
  images. The core idea is to use a VQ-GAN to tokenize CXR images into discrete tokens
  that can be processed by the LLM, similar to text tokens.
---

# LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation

## Quick Facts
- arXiv ID: 2305.11490
- Source URL: https://arxiv.org/abs/2305.11490
- Authors: 
- Reference count: 40
- Key outcome: Fine-tuned LLM generates CXR images from reports and vice versa, with preserved language capabilities

## Executive Summary
This paper presents LLM-CXR, a method for endowing pre-trained large language models (LLMs) with multimodal capabilities for chest X-ray (CXR) image understanding and generation. The approach uses VQ-GAN to tokenize CXR images into discrete tokens that can be processed by an LLM, similar to text tokens. The model is fine-tuned on a dataset of CXR images paired with radiology reports, learning to generate images from text and vice versa. A two-stage fine-tuning process is employed, first on the full dataset and then on a refined subset, to improve performance. The results demonstrate that the fine-tuned LLM can generate high-quality CXR images that align well with input reports, as measured by FID score and AUROC/F1 scores from a CXR classifier. The model also generates coherent radiology reports given CXR images, although with some false positives and missed diagnoses. The LLM retains its natural language capabilities after fine-tuning, albeit with a slight decrease in performance.

## Method Summary
The method involves using a VQ-GAN to tokenize CXR images into discrete tokens, which are then processed by a pre-trained LLM. The LLM is fine-tuned on a dataset of CXR images and corresponding radiology reports, learning to generate images from text and vice versa. The fine-tuning is done in a two-stage process: first on the full dataset to learn the general distribution of image tokens and basic image-text relationships, and then on a refined subset to improve alignment. The model is evaluated on its ability to generate CXR images from reports and vice versa, as well as its preservation of instruction-following capabilities. Metrics used include FID score, AUROC, F1-score, and perplexity.

## Key Results
- Fine-tuned LLM generates CXR images from reports with high FID scores (lower is better)
- Model generates coherent radiology reports from CXR images, though with some false positives and missed diagnoses
- LLM retains natural language capabilities after fine-tuning, with slight decrease in performance (perplexity increases from 2.64 to 4.45)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VQ-GAN tokenization allows discrete image tokens to be processed by LLM as text tokens
- Mechanism: VQ-GAN encodes CXR images into discrete latent vectors that are mapped to integer tokens, enabling bidirectional generation within the same transformer architecture
- Core assumption: Image tokens can be embedded in the same space as text tokens without semantic loss
- Evidence anchors:
  - [abstract] "use a VQ-GAN to tokenize CXR images into discrete tokens that can be processed by the LLM"
  - [section] "Similar to UniXGen [12]'s use of the image tokenization method...we used the same method to generate quantized image latent"
  - [corpus] Found related works using VQ-GAN for medical imaging tokenization
- Break condition: If VQ-GAN codebook size is insufficient to capture medical image diversity, token quality degrades

### Mechanism 2
- Claim: Two-stage fine-tuning enables learning both token space and semantic alignment
- Mechanism: Stage 1 learns full distribution of image tokens and basic image-text relationships; Stage 2 refines with high-quality pairs to improve alignment
- Core assumption: Sequential learning is more effective than simultaneous learning for multimodal alignment
- Evidence anchors:
  - [section] "two-stage training technique...The first stage primarily involves learning the entire distribution of new image tokens and the general relationship between image and text tokens"
  - [section] "The second stage aims to learn the actual vision-language task using a strongly refined supervision dataset"
  - [corpus] Related works use staged training for medical vision-language tasks
- Break condition: If Stage 1 learns too much noise, Stage 2 cannot recover alignment quality

### Mechanism 3
- Claim: Instruction-following format preserves LLM reasoning while adding multimodal capabilities
- Mechanism: Fine-tuning on instruction templates maintains the model's ability to follow prompts while learning to generate text and images
- Core assumption: The instruction-following format provides sufficient supervision for both modalities
- Evidence anchors:
  - [section] "our approach allows vision-language tasks to be instructed by prompting a single model in the instruction section"
  - [section] "By integrating the natural language instruction-following dataset with the fine-tuning dataset, the natural language question-answering ability, inherent in the pre-trained LLM, can be also preserved"
  - [corpus] Instruction-tuning is common in multimodal LLM research
- Break condition: If instruction format becomes too complex, model loses ability to follow prompts effectively

## Foundational Learning

- Concept: VQ-GAN quantization and codebook structure
  - Why needed here: Understanding how images become discrete tokens that can be processed by transformers
  - Quick check question: How does VQ-GAN's codebook size (Kimg) affect the model's ability to represent diverse medical images?

- Concept: Autoregressive generation and causal masking
  - Why needed here: The model generates tokens sequentially, requiring understanding of how previous tokens influence next predictions
  - Quick check question: What happens if we try to generate tokens out of sequence order?

- Concept: Multi-task learning and dataset mixing
  - Why needed here: The model must balance learning multimodal tasks with preserving existing instruction-following capabilities
  - Quick check question: How does the 40%/40%/20% task split affect the model's ability to maintain language capabilities?

## Architecture Onboarding

- Component map:
  - VQ-GAN encoder → discrete image tokens → LLM embedding layer → transformer blocks → output layer → VQ-GAN decoder
  - Additional embedding space for Kimg new token types
  - Instruction template formatter for training data

- Critical path: Image → VQ-GAN encoder → LLM → VQ-GAN decoder (generation)
  - Report → LLM → discrete image tokens → VQ-GAN decoder (image generation)
  - Image → VQ-GAN encoder → discrete image tokens → LLM → text generation (report generation)

- Design tradeoffs:
  - Larger Kimg provides better image representation but increases embedding table size and memory usage
  - More training steps improve quality but increase computational cost
  - Higher instruction-task mixing ratio preserves language ability but may slow multimodal learning

- Failure signatures:
  - Poor FID scores indicate VQ-GAN is not capturing image features adequately
  - Low AUROC/F1 scores suggest misalignment between generated images and input text
  - Increased perplexity on WikiText indicates loss of language capabilities

- First 3 experiments:
  1. Generate images from simple reports with single findings to test basic token mapping
  2. Generate reports from clear single-finding images to test bidirectional alignment
  3. Test instruction-following capability on simple text prompts to verify language preservation

## Open Questions the Paper Calls Out
- How does the performance of the LLM-CXR model scale with the size of the pre-trained LLM used as the base model?
- How robust is the LLM-CXR model to domain shift in the input CXR images, such as different imaging equipment or patient populations?
- Can the LLM-CXR model be extended to other medical imaging modalities beyond CXRs, such as CT or MRI scans?

## Limitations
- Limited ablation studies on critical design choices like two-stage training and VQ-GAN codebook size
- Evaluation relies heavily on automated metrics that may not capture clinical utility
- Trained and evaluated only on MIMIC-CXR, raising concerns about generalization to other datasets

## Confidence
- **High confidence**: VQ-GAN tokenization mechanism is well-established; core generation claims are directly supported by metrics
- **Medium confidence**: Two-stage training benefits are supported but lack ablation validation; language preservation claim has some evidence but shows degradation
- **Low confidence**: Clinical utility claims lack systematic expert validation and comprehensive error analysis

## Next Checks
1. Conduct ablation study comparing one-stage vs two-stage training with varying ratios of refined dataset in Stage 2
2. Evaluate model on CXR datasets from different hospitals or countries to assess generalization beyond MIMIC-CXR
3. Have board-certified radiologists assess generated reports and images for clinical accuracy and safety concerns, comparing against human performance