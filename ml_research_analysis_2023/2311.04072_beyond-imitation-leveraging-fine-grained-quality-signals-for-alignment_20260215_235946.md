---
ver: rpa2
title: 'Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment'
arxiv_id: '2311.04072'
source_url: https://arxiv.org/abs/2311.04072
tags:
- response
- arxiv
- reward
- alignment
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIGA, a method for aligning large language
  models with human preferences without using reinforcement learning. The key innovation
  is leveraging fine-grained quality signals by contrasting low-quality initial responses
  from the model with corresponding high-quality revised responses.
---

# Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment

## Quick Facts
- **arXiv ID**: 2311.04072
- **Source URL**: https://arxiv.org/abs/2311.04072
- **Reference count**: 17
- **Key outcome**: FIGA outperforms SFT by 3.2 points and PPO by 1.8 points on reward scores and benchmarks

## Executive Summary
This paper introduces FIGA, a method for aligning large language models with human preferences without using reinforcement learning. The key innovation is leveraging fine-grained quality signals by contrasting low-quality initial responses from the model with corresponding high-quality revised responses. This provides token-level feedback on what changes improve quality. FIGA is implemented by curating a dataset pairing initial and revised responses, and using a new loss function that encourages good changes and discourages bad ones based on edit distance. Experiments show FIGA outperforms SFT by 3.2 points and PPO by 1.8 points on reward scores and benchmarks, demonstrating its effectiveness for alignment. The approach is efficient like SFT but can learn what behaviors to follow like RLHF.

## Method Summary
FIGA constructs a dataset pairing initial model responses with high-quality revised versions, then uses Levenshtein distance to identify token-level edits that distinguish good from bad responses. The method assigns different weights to tokens based on whether they were added, deleted, or substituted, creating a fine-grained quality signal. During training, FIGA applies these weights through a custom loss function that encourages the model to make the same quality improvements. The approach reduces distribution shift compared to using ground truth demonstrations directly, and functions as a simplified alternative to RLHF by providing token-level advantage-like signals without requiring a separate reward model or complex policy optimization.

## Key Results
- FIGA achieves 87.7 reward score, outperforming SFT by 3.2 points and PPO by 1.8 points
- Shows 2.1 point improvement over SFT on MMLU benchmark
- Reduces hallucination and improves truthfulness on TruthfulQA benchmark

## Why This Works (Mechanism)

### Mechanism 1
The core innovation lies in using token-level quality signals derived from contrasting initial and revised responses. By comparing the Levenshtein distance between low-quality initial responses and high-quality revised responses, the model can identify specific tokens that were added, deleted, or substituted. The method then assigns different weights to these tokens - encouraging good additions/substitutions and discouraging bad deletions/substitutions. This works because token-level edits between initial and revised responses contain meaningful signal about what makes responses better or worse. If the edits between initial and revised responses don't consistently correlate with quality improvements, the token-level weighting would become meaningless.

### Mechanism 2
The method reduces distribution shift between training targets and the model being aligned. Instead of using ground truth demonstrations directly, the approach revises the model's own initial response to create training targets. This keeps the semantic distribution closer to the model's natural output while improving quality. This works because distribution shift between the model's output distribution and ground truth demonstrations is a significant barrier to effective alignment. If the distribution shift isn't actually a major factor in alignment performance, this mechanism would provide minimal benefit.

### Mechanism 3
The method functions as a simplified but efficient alternative to reinforcement learning. The token-level weighting functions (α, β, γ) act as a simplified advantage function similar to what's used in RLHF, providing fine-grained feedback without needing a separate reward model or complex policy optimization. This works because a simplified token-level reward signal can capture the essential alignment behavior that RLHF achieves through more complex mechanisms. If the simplified token-level rewards cannot capture the nuanced preference signals that RLHF's learned reward models provide, performance would degrade.

## Foundational Learning

- **Concept: Levenshtein distance algorithm**
  - Why needed here: Used to quantify the edit operations between initial and revised responses to identify which tokens were changed
  - Quick check question: Can you explain how Levenshtein distance works and how it's used to count additions, deletions, and substitutions between two strings?

- **Concept: Supervised fine-tuning (SFT) and its limitations**
  - Why needed here: The paper positions FIGA as an improvement over SFT by adding token-level discrimination
  - Quick check question: What is the fundamental limitation of SFT that prevents it from fully understanding expected behaviors, according to the paper?

- **Concept: Distribution shift in machine learning**
  - Why needed here: The approach specifically addresses reducing distribution shift between training targets and the model being aligned
  - Quick check question: Why might training on responses from ChatGPT directly cause distribution shift problems when aligning a smaller LLM?

## Architecture Onboarding

- **Component map**: Query → Initial response generation → Reward scoring → Filtering → Response revision → Levenshtein analysis → Token weighting → Loss calculation → Model update

- **Critical path**: Query → Initial response generation → Reward scoring → Filtering → Response revision → Levenshtein analysis → Token weighting → Loss calculation → Model update

- **Design tradeoffs**:
  - Accuracy vs efficiency: Using ChatGPT for revisions provides high-quality targets but is computationally expensive
  - Granularity vs stability: Token-level weighting provides fine-grained signals but may be unstable compared to response-level rewards
  - Filtering strictness vs dataset size: Strict filtering based on reward scores produces higher quality data but reduces dataset size

- **Failure signatures**:
  - Reward score saturation: If reward scores don't improve over training, the fine-grained signals may not be effective
  - Distribution mismatch: If the model's outputs drift significantly from the revised response style, the approach may fail
  - Token weighting instability: If the Levenshtein-based token classification produces inconsistent results, training may be unstable

- **First 3 experiments**:
  1. Implement the Levenshtein distance calculation and token classification to verify it correctly identifies added/deleted/substituted tokens
  2. Test different weight parameter combinations (α, β, γ) on a small dataset to find stable settings
  3. Compare training with and without the token-level weighting on a subset of data to verify the mechanism works as intended

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the fine-grained quality signal approach compare to traditional RLHF methods in terms of computational efficiency and training stability?
- **Basis in paper**: [explicit] The paper discusses the efficiency of FIGA compared to RLHF, stating it is more efficient and doesn't suffer from training instability issues like reward collapse.
- **Why unresolved**: While the paper claims efficiency, it doesn't provide a detailed comparison of computational resources required or training stability metrics between FIGA and RLHF.
- **What evidence would resolve it**: A comprehensive comparison of training time, memory usage, and stability metrics (e.g., variance in reward scores) between FIGA and RLHF implementations.

### Open Question 2
- **Question**: What is the impact of different weighting functions (e.g., Bag of words, ChatGPT weighted, ChatGPT binary) on the performance of FIGA?
- **Basis in paper**: [explicit] The paper explores different weighting functions in the 'Performance Comparison w.r.t. Weighting Functions' section.
- **Why unresolved**: While the paper provides some results, it doesn't deeply analyze why certain weighting functions perform better than others or how they might affect different types of queries.
- **What evidence would resolve it**: A detailed analysis of the performance of different weighting functions across various query types and an explanation of the underlying reasons for their effectiveness.

### Open Question 3
- **Question**: How does the quality of the reward model affect the performance of FIGA?
- **Basis in paper**: [explicit] The paper mentions using a reward model for filtering and constructing the SPA dataset, but doesn't explore the impact of different reward models.
- **Why unresolved**: The paper uses a specific reward model but doesn't investigate how different reward models might influence the quality of the SPA dataset and consequently the performance of FIGA.
- **What evidence would resolve it**: Experiments comparing FIGA performance using different reward models or exploring the sensitivity of FIGA to reward model quality.

## Limitations
- Data quality heavily depends on ChatGPT's ability to identify and fix quality issues in initial responses
- Token-level signal reliability assumes edits consistently correlate with quality improvements, which may not always hold
- Generalizability beyond chat-style tasks remains untested, particularly for structured output or reasoning tasks

## Confidence

**High confidence**: The claim that FIGA outperforms SFT by 3.2 points and PPO by 1.8 points on reward scores is supported by direct experimental results shown in the paper's tables. The ablation study showing that removing token-level weighting reduces performance also provides strong evidence for the core mechanism.

**Medium confidence**: The claim that FIGA functions as a simplified but efficient alternative to reinforcement learning is supported by the theoretical connection drawn between token-level weighting and advantage functions, but the practical implications of this simplification are not fully explored.

**Low confidence**: The claim that distribution shift is a significant barrier to alignment and that FIGA effectively reduces it is based on theoretical reasoning but lacks direct empirical validation.

## Next Checks

1. **Validate token-level signal quality**: Conduct a human evaluation study where annotators assess whether the Levenshtein-identified token changes actually correspond to meaningful quality improvements. This would test whether the core assumption about token-level signals is valid and identify potential failure modes where the approach might be learning from spurious correlations rather than true quality improvements.

2. **Test domain generalizability**: Apply FIGA to non-chat tasks such as code generation or mathematical problem-solving where the response format differs significantly from conversational text. Compare performance against SFT and PPO to determine whether the approach generalizes beyond the conversational tasks studied in the paper.

3. **Analyze distribution shift empirically**: Measure the actual distribution shift between the model's outputs, ground truth demonstrations, and the revised responses used in training. Compare the performance of models trained with different approaches while controlling for dataset size and quality to determine whether reducing distribution shift actually provides measurable benefits.