---
ver: rpa2
title: Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and
  Toxic Language Detection
arxiv_id: '2307.03377'
source_url: https://arxiv.org/abs/2307.03377
tags:
- task
- learning
- tasks
- transfer
- sexism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novelty approach to mitigate the negative
  transfer problem. In the field of machine learning, the common strategy is to apply
  the Single-Task Learning approach in order to train a supervised model to solve
  a specific task.
---

# Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection

## Quick Facts
- arXiv ID: 2307.03377
- Source URL: https://arxiv.org/abs/2307.03377
- Reference count: 40
- Sets new state-of-the-art on EXIST-2021 and HatEval-2019 benchmarks for multi-task detection of sexism, hate speech, and toxic language

## Executive Summary
This paper addresses the negative transfer problem in multi-task learning for detecting sexism, hate speech, and toxic language in text. The authors propose task awareness mechanisms that modify the conventional MTL input to provide task-centered representations. Their approach includes Task-Aware Input (TAI) that appends task descriptions to text inputs and Task Embedding (TE) that dynamically adjusts representations based on task identity. The proposed MTL-TAI and MTL-TE models outperform both single-task and classic multi-task baselines on three benchmark datasets, achieving state-of-the-art results on EXIST-2021 and HatEval-2019.

## Method Summary
The paper proposes two task awareness mechanisms for multi-task learning: Task-Aware Input (TAI) and Task Embedding (TE). TAI modifies the input by appending a task description (e.g., "Sexism Detection") to the text snippet, allowing the encoder to generate task-centered representations. TE implements a Task Embedding Block (TEB) that receives both a task identification vector and the encoder representation, transforming it differently for each task through learned linear transformations. Both approaches use a shared BERT-based encoder (BETO for Spanish) with task-specific heads, trained with AdamW optimizer for 15 epochs using batch size 64.

## Key Results
- MTL-TAI and MTL-TE outperform both single-task and classic multi-task baselines on EXIST-2021, DETOXIS-2021, and HatEval-2019 benchmarks
- State-of-the-art performance achieved on EXIST-2021 (Accuracy) and HatEval-2019 (F1-macro) datasets
- Task awareness mechanisms successfully mitigate negative transfer while improving overall performance
- Task-Aware Input shows slightly better performance than Task Embedding on most benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Task-aware input (TAI)
- Claim: TAI provides contextual task information directly to the encoder, improving task-specific representation learning
- Mechanism: The model receives both a text snippet and a task description (e.g., "Sexism Detection"), enabling the encoder to generate task-centered representations rather than purely task-agnostic ones
- Core assumption: Including task description text alongside the input allows the encoder to better allocate its representational capacity toward features relevant to the specific task
- Evidence anchors: [abstract] "To compel the encoder to generate a suitable representation for each task head, we proposed to modify the MTL conventional input for NLP tasks"
- Break condition: If the task description text is too generic or doesn't align with the actual task requirements, the encoder may not effectively learn task-specific features

### Mechanism 2: Task embedding (TE)
- Claim: TE enables dynamic adjustment of latent representations based on the specific task being processed
- Mechanism: A Task Embedding Block (TEB) receives both the task identification vector and the encoder representation, transforming the representation differently for each task through learned linear transformations
- Core assumption: The TEB can learn to modulate the encoder output based on task identity, creating task-appropriate representations without requiring separate encoders for each task
- Evidence anchors: [abstract] "The TEB is composed of Learning Units (LU) that encompass a linear layer followed by a ReLU layer"
- Break condition: If the task identification vector is ambiguous or the TEB architecture is too simple to capture task-specific transformations, performance gains may be minimal

### Mechanism 3: Hard parameter sharing with task awareness
- Claim: Hard parameter sharing with task awareness mitigates negative transfer by prioritizing task-relevant features while maintaining shared learning benefits
- Mechanism: The encoder is trained across all tasks but generates representations that are then specialized through TAI or TE mechanisms before reaching task-specific heads, reducing interference between tasks
- Core assumption: Task awareness mechanisms can effectively filter out noise and emphasize relevant features for each task while preserving the generalization benefits of shared representations
- Evidence anchors: [abstract] "Negative transfer refers to situations in which the transfer of information results in a decrease in the overall model performance"
- Break condition: If tasks are too dissimilar or if the task awareness mechanisms are not properly calibrated, the model may still suffer from negative transfer or fail to benefit from shared representations

## Foundational Learning

- Concept: Multi-task learning (MTL) with hard parameter sharing
  - Why needed here: The paper builds on this foundational approach by adding task awareness mechanisms to address its limitations, particularly negative transfer
  - Quick check question: What is the key architectural difference between soft parameter sharing and hard parameter sharing in MTL?

- Concept: Negative transfer in multi-task learning
  - Why needed here: Understanding this phenomenon is crucial for appreciating why task awareness mechanisms are necessary and how they improve performance
  - Quick check question: Under what conditions does negative transfer typically occur in MTL models?

- Concept: Task-specific vs. task-agnostic representations
  - Why needed here: The core innovation involves transforming task-agnostic encoder outputs into task-specific representations suitable for each decoder head
  - Quick check question: How does the paper's approach balance the benefits of shared representations with the need for task-specific feature extraction?

## Architecture Onboarding

### Component Map
Text Input → Encoder (BERT/BETO) → Task Embedding Block (TE) OR Task-Aware Input (TAI) → Task-specific Heads → Output

### Critical Path
Text → BERT Encoder → TEB/TAI → Task Head → Performance Metrics

### Design Tradeoffs
- Shared encoder vs. task-specific encoders: Shared encoder reduces parameters but may suffer from negative transfer
- Task description length in TAI: Longer descriptions provide more context but increase input length and computational cost
- TEB complexity: More Learning Units can capture complex task transformations but increase model size and risk overfitting

### Failure Signatures
- Poor convergence during training: Indicates inadequate fine-tuning of the BERT encoder or insufficient task awareness signal
- Similar performance to baseline MTL: Suggests TAI/TE mechanisms are not effectively modifying representations
- Performance degradation on certain tasks: May indicate negative transfer not properly mitigated or task awareness mechanisms creating interference

### First Experiments
1. Implement MTL-TAI with task descriptions and compare against classic MTL baseline on EXIST-2021 dataset
2. Implement MTL-TE with Task Embedding Block and evaluate performance on HatEval-2019 dataset
3. Conduct ablation study removing TAI/TE mechanisms to quantify their contribution to performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many labeled samples or information volume are required to justify using MTL over STL for sexism, hate speech, and toxic language detection?
- Basis in paper: [inferred] The paper mentions the need to analyze when MTL is preferable to STL, especially in cases of scarce labeled data
- Why unresolved: The paper did not conduct experiments to determine the exact threshold of data volume where MTL becomes more beneficial than STL
- What evidence would resolve it: Experiments varying dataset sizes and measuring performance differences between MTL and STL approaches

### Open Question 2
- Question: How does enriching MTL model input with low-level task supervision affect performance compared to using only the final encoder representation?
- Basis in paper: [inferred] The paper suggests analyzing the impact of different encoder representations in MTL models, including low-level task supervision
- Why unresolved: The paper did not explore using intermediate encoder representations or low-level task supervision in their MTL-TA models
- What evidence would resolve it: Comparative experiments using MTL-TA models with different levels of task supervision (final vs. intermediate representations)

### Open Question 3
- Question: How do the proposed MTL-TA models perform in unsupervised or few-shot learning scenarios for sexism, hate speech, and toxic language detection?
- Basis in paper: [inferred] The paper mentions future work on applying MTL with TA to unsupervised techniques and scenarios like few-shot learning
- Why unresolved: The paper only evaluated the MTL-TA models in supervised settings with full training data
- What evidence would resolve it: Experiments testing MTL-TA models on unsupervised or few-shot learning tasks for the target applications

## Limitations

- Implementation ambiguities in Task Embedding Block architecture and exact task description texts limit full reproducibility
- The paper does not provide ablation studies showing the relative contributions of different components to performance gains
- The assertion that task awareness fundamentally solves negative transfer is not fully substantiated with analysis of underlying mechanisms

## Confidence

**High Confidence**: The empirical results showing MTL-TAI and MTL-TE outperforming both single-task and classic multi-task baselines on the three benchmark datasets (EXIST-2021, DETOXIS-2021, HatEval-2019) are well-supported by the reported metrics and experimental setup.

**Medium Confidence**: The proposed mechanisms for mitigating negative transfer through task awareness are conceptually reasonable and align with established multi-task learning principles, though implementation details are somewhat vague.

**Low Confidence**: The paper's assertion that task awareness fundamentally solves the negative transfer problem is not fully substantiated, as the analysis of why negative transfer occurs and how precisely the task awareness mechanisms prevent it is limited.

## Next Checks

1. **Ablation Study Validation**: Conduct experiments isolating the effects of Task-Aware Input versus Task Embedding mechanisms to determine which component contributes more significantly to performance gains.

2. **Negative Transfer Analysis**: Systematically test the models on tasks with varying degrees of similarity to determine the conditions under which task awareness mechanisms effectively prevent negative transfer.

3. **Reproducibility Implementation**: Implement the MTL-TAI and MTL-TE architectures following the paper's specifications (using BETO encoder, 15 epochs, batch size 64, AdamW optimizer) and compare results against the reported state-of-the-art performance on all three benchmarks.