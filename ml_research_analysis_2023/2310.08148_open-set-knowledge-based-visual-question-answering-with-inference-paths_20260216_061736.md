---
ver: rpa2
title: Open-Set Knowledge-Based Visual Question Answering with Inference Paths
arxiv_id: '2310.08148'
source_url: https://arxiv.org/abs/2310.08148
tags:
- graph
- answer
- knowledge
- path
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the open-set knowledge-based visual question
  answering problem, where answers are not limited to a fixed candidate set but can
  be any entity in a knowledge graph. The proposed method, GATHER, constructs a schema
  graph by retrieving relevant entities from the knowledge graph based on image and
  question information, prunes it to retain key nodes, and performs path-level reasoning
  to predict answers with interpretable paths.
---

# Open-Set Knowledge-Based Visual Question Answering with Inference Paths

## Quick Facts
- arXiv ID: 2310.08148
- Source URL: https://arxiv.org/abs/2310.08148
- Authors: 
- Reference count: 39
- Primary result: Achieves 72.5% ground truth hit rate in schema graph construction and outperforms baselines on open-set KB-VQA task

## Executive Summary
This paper addresses the challenge of open-set knowledge-based visual question answering (KB-VQA), where answers can be any entity from a large knowledge graph rather than a fixed candidate set. The proposed GATHER framework constructs schema graphs by retrieving relevant entities from ConceptNet based on image and question information, prunes them to retain key nodes, and performs path-level reasoning to predict answers with interpretable paths. Experiments on the ConceptVQA benchmark show that GATHER can effectively retrieve answers from the entire knowledge graph and provide reasonable reasoning paths, even for questions with answers not seen during training.

## Method Summary
The GATHER framework operates as a retriever-ranker architecture for open-set KB-VQA. It first constructs a schema graph by combining visual scene graph information, question entities, and multi-hop neighbors from ConceptNet (up to 1,500 nodes). A pruning module uses VilBERT to generate multimodal context embeddings and applies a triplet loss function to identify relevant nodes while maintaining graph connectivity through BFS scoring. The ranking module samples inference paths from the pruned subgraph, encodes them using MLPs, and computes path probabilities with a bilinear layer optimized via binary cross-entropy loss using ground truth paths as weak supervision.

## Key Results
- Achieves 72.5% ground truth hit rate in schema graph construction with 1,000 nodes
- Outperforms baseline retriever-classification frameworks on open-set KB-VQA task
- Provides interpretable reasoning paths alongside predicted answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schema graph construction with multi-hop neighbor retrieval covers ground truth answers effectively
- Mechanism: The framework constructs a schema graph by combining visual scene graph, question entities, and multi-hop neighbors from ConceptNet. By retrieving one-hop and two-hop neighbors (up to 1,500 nodes), the schema graph captures both local and broader contextual information needed for inference.
- Core assumption: Ground truth answers are reachable within two hops from question or image nodes in ConceptNet
- Evidence anchors:
  - [abstract] "we first retrieves all image-related information from the vast knowledge base and construct a scheme graph that inherits the structure of both image scene graph and the KG"
  - [section] "Based on retrieved textual and visual nodes, we search for one-hop and two-hop neighbors from ConceptNet, which should intuitively cover most of the expected answers"
  - [corpus] "Weak" - no direct citations about schema graph effectiveness in corpus
- Break condition: If ground truth answers require longer paths or ConceptNet lacks relevant entities, schema graph construction will miss answers

### Mechanism 2
- Claim: Graph pruning with multimodal context improves node relevance for path inference
- Mechanism: VilBERT generates multimodal context embeddings from question and image, which are used to compute node relevance scores. The pruning network uses triplet loss to learn which nodes are most relevant, then prunes the schema graph while maintaining connectivity through BFS scoring.
- Core assumption: Multimodal context embeddings capture the semantic relationship between question/image and potential answer nodes
- Evidence anchors:
  - [abstract] "we further introduce a new path-level ranking method for an explainable reasoning path"
  - [section] "The pruning network is optimized using a triplet loss function, where the anchor in every triplet is the multimodal context embedding z and the positive term is the ground-truth node"
  - [corpus] "Weak" - no direct citations about multimodal pruning effectiveness
- Break condition: If VilBERT embeddings poorly capture question-image semantics or pruning removes critical nodes, path inference will fail

### Mechanism 3
- Claim: Path-level ranking with bilinear scoring enables interpretable reasoning
- Mechanism: The framework samples inference paths from pruned subgraphs and encodes them using MLPs. A bilinear layer computes path probability scores against multimodal context, optimized with binary cross-entropy loss using ground truth paths as weak supervision.
- Core assumption: Correct answers can be reached through explicit paths in the pruned subgraph
- Evidence anchors:
  - [abstract] "Different from traditional retriever-classification frameworks, GA THER exceeds the pre-defined answering space and tries to provide reasoning paths apart from predicted answers"
  - [section] "We assume that the inference path starts from question/image nodes and sample N paths within k steps from the key nodes of type {Q, V }"
  - [corpus] "Moderate" - related work on GNN-based reasoning exists but not specifically for path ranking
- Break condition: If ground truth paths are too complex or sampling strategy misses them, path ranking will fail to identify correct answers

## Foundational Learning

- Concept: Multimodal representation learning with vision-language models
  - Why needed here: The framework requires joint understanding of visual and textual information to build context-aware schema graphs and compute node relevance scores
  - Quick check question: How does VilBERT's cross-modal attention help distinguish relevant from irrelevant nodes in the schema graph?

- Concept: Graph neural networks and message passing
  - Why needed here: The framework needs to aggregate information along graph paths and compute node embeddings that capture neighborhood structure
  - Quick check question: What information does each node aggregation step preserve versus lose in the schema graph pruning process?

- Concept: Knowledge graph structure and entity linking
  - Why needed here: The framework operates on ConceptNet's entity-relation structure and needs to map visual/textual entities to KG nodes
  - Quick check question: How does the matching process handle synonyms and entity variations when linking question terms to ConceptNet?

## Architecture Onboarding

- Component map: Retriever (scene graph + entity matching + neighbor search) → Pruner (VilBERT + triplet loss + BFS) → Ranker (path sampling + MLP encoding + bilinear scoring)
- Critical path: Image/question → Schema graph construction → Graph pruning → Path sampling → Path encoding → Answer prediction
- Design tradeoffs: Open-set answer space vs. computational cost, path sampling complexity vs. inference quality, schema graph size vs. pruning effectiveness
- Failure signatures: Low ground truth hit rate in schema graphs, poor pruning performance (nodes disconnected), incorrect path sampling, model out-of-memory with large answer spaces
- First 3 experiments:
  1. Measure ground truth hit rate at different schema graph sizes (100, 500, 1000 nodes) to verify retriever effectiveness
  2. Evaluate node classification accuracy before and after pruning to validate pruning module
  3. Test path ranking performance with different sampling strategies (number of paths, path length) to optimize inference quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explicit reasoning path annotations be obtained for training and evaluation to improve the interpretability of KB-VQA models?
- Basis in paper: Explicit reasoning paths are crucial for KB-VQA interpretability, but current benchmarks like ConceptVQA lack detailed reasoning path annotations.
- Why unresolved: Obtaining explicit reasoning path annotations is challenging and resource-intensive, as it requires human experts to trace and label the reasoning process for each question-answer pair.
- What evidence would resolve it: Developing automated methods to generate or approximate reasoning path annotations, or creating datasets with detailed reasoning paths, would help evaluate and improve model interpretability.

### Open Question 2
- Question: How can the relationship between question logic and reasoning paths be better leveraged to improve the accuracy and interpretability of KB-VQA models?
- Basis in paper: The current path sampling strategy in GATHER is relatively rudimentary and does not fully utilize the relationship between question logic and reasoning paths.
- Why unresolved: Understanding how the logical structure of questions influences the reasoning process and incorporating this knowledge into the model's reasoning mechanism remains an open challenge.
- What evidence would resolve it: Developing more sophisticated path sampling techniques that consider question logic, or creating models that explicitly reason about the logical structure of questions, would improve both accuracy and interpretability.

### Open Question 3
- Question: How can the schema graph construction process be improved to retrieve more relevant knowledge and increase the hit rate of ground truth answers?
- Basis in paper: The current schema graph construction in GATHER achieves a ground truth hit rate of 72.5% at a graph size of 1,000 nodes, indicating room for improvement.
- Why unresolved: The effectiveness of schema graph construction depends on various factors, such as the choice of retrieval methods, the filtering process, and the handling of noise and irrelevant information.
- What evidence would resolve it: Experimenting with different retrieval methods, refining the filtering process, and incorporating techniques to handle noise and irrelevant information would help increase the hit rate of ground truth answers.

## Limitations

- Performance depends on ConceptNet coverage and multimodal representation quality
- Limited evaluation on only one benchmark dataset (ConceptVQA)
- No human evaluation of reasoning path quality and interpretability

## Confidence

- **Medium**: The framework design is well-motivated and experiments show improvements over baselines, but limited dataset size and lack of comparisons to more recent KB-VQA methods constrain generalizability. The interpretability claims are supported by path-level explanations but would benefit from human evaluation of reasoning quality.

## Next Checks

1. **Schema Graph Coverage Analysis**: Measure ground truth answer hit rates across different schema graph sizes (100, 500, 1000 nodes) to quantify the retriever's effectiveness and identify coverage limitations.

2. **Pruning Ablation Study**: Evaluate node classification accuracy before and after pruning with different VilBERT context sizes to validate whether pruning maintains critical nodes while removing irrelevant ones.

3. **Path Sampling Robustness**: Test path ranking performance across varying numbers of sampled paths (10, 50, 100) and path lengths (2, 3, 4 steps) to determine optimal inference parameters and identify failure modes.