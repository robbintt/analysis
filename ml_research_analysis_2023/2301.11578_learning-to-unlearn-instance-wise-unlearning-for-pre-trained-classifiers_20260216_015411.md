---
ver: rpa2
title: 'Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers'
arxiv_id: '2301.11578'
source_url: https://arxiv.org/abs/2301.11578
tags:
- unlearning
- data
- adversarial
- forgetting
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of instance-wise unlearning from
  pre-trained classifiers, where the goal is to remove information about specific
  instances without retraining from scratch. The proposed method intentionally misclassifies
  selected instances while preserving predictive performance on the remaining data.
---

# Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers

## Quick Facts
- **arXiv ID**: 2301.11578
- **Source URL**: https://arxiv.org/abs/2301.11578
- **Reference count**: 30
- **Primary result**: Achieves complete misclassification of target instances while preserving accuracy on remaining data using adversarial examples and weight importance regularization

## Executive Summary
This paper introduces a method for instance-wise unlearning from pre-trained classifiers, where specific training instances must be removed from the model's knowledge without retraining from scratch. The approach combines two regularization techniques: generating adversarial examples to preserve decision boundaries learned from remaining data, and using weight importance metrics to focus updates on parameters responsible for original correct classifications. Experiments on CIFAR-10/-100 and ImageNet-1K demonstrate significant improvements over baseline approaches, achieving complete forgetting of target instances while maintaining or improving accuracy on remaining data.

## Method Summary
The method addresses instance-wise unlearning by fine-tuning a pre-trained model using two complementary regularization approaches. First, adversarial examples are generated from forgetting instances using L2-PGD targeted attacks, which are then used to regularize the model and preserve the decision boundaries learned from remaining data. Second, weight importance metrics (specifically MAS) identify parameters crucial for correct classification of forgetting instances, allowing the model to focus updates on these parameters while preserving others. The combined approach is applied during fine-tuning to achieve complete misclassification of target instances while maintaining performance on remaining data.

## Key Results
- Achieves complete misclassification of target instances (0% accuracy on forgetting data) while significantly improving accuracy on remaining data compared to baselines
- Shows improvements of up to 40% in preserving accuracy on remaining data compared to negative gradient baseline
- Qualitative analysis reveals no discernible pattern in misclassifications and effective preservation of decision boundaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples preserve decision boundaries learned from remaining data during unlearning
- Mechanism: Generated adversarial examples contain class-specific features of attack target labels that mimic remaining data distribution, acting as regularizers to preserve original decision boundaries
- Core assumption: Adversarial examples capture representative features of remaining data distribution
- Evidence anchors: [abstract] "utilizing adversarial examples to overcome forgetting at the representation-level"; [section] "we utilize generated adversarial examples as part of regularization R(·) to preserve class-specific knowledge previously learned by the model"
- Break condition: If adversarial examples fail to capture representative features of remaining data distribution

### Mechanism 2
- Claim: Weight importance regularization preserves parameters responsible for correct classifications on remaining data
- Mechanism: MAS-based weight importance identifies crucial parameters, allowing the model to focus updates on parameters responsible for original correct classifications while preserving other parameters
- Core assumption: Parameters can be reliably ranked by importance for specific instance predictions
- Evidence anchors: [abstract] "leveraging weight importance metrics to pinpoint network parameters guilty of propagating unwanted information"; [section] "our approach is to maintain the weights that were less important for Df prediction as much as possible"
- Break condition: If weight importance estimates become unreliable due to catastrophic forgetting

### Mechanism 3
- Claim: Combined adversarial and weight importance regularization provides complementary benefits for different unlearning scenarios
- Mechanism: Adversarial examples preserve representation-level decision boundaries while weight importance handles weight-level forgetting, particularly effective for continual unlearning
- Core assumption: Representation-level and weight-level forgetting are distinct problems requiring different regularization approaches
- Evidence anchors: [section] "applying the regularization using adversarial examples is already effective to overcome the forgetting for knowledge of Dr, and the additional regularization with weight importance further enhances performance"
- Break condition: If one regularization method dominates the other or creates conflicting optimization objectives

## Foundational Learning

- Concept: Adversarial examples and L2-PGD generation
  - Why needed here: Adversarial examples form core of one regularization approach
  - Quick check question: How does L2-PGD targeted attack generate adversarial examples containing class-specific features of attack target label?

- Concept: Weight importance metrics (MAS)
  - Why needed here: MAS is used to estimate parameter importance for regularization approach
  - Quick check question: How does MAS calculate weight importance using gradients of L2 norm of model outputs?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The framework aims to prevent forgetting on remaining data
  - Quick check question: What causes catastrophic forgetting and why is it problematic for unlearning scenarios?

## Architecture Onboarding

- Component map: Pre-trained classifier → Adversarial example generator → Weight importance calculator → Unlearning optimizer → Evaluation metrics
- Critical path: Pre-trained model → Generate adversarial examples → Calculate weight importance → Apply regularization during fine-tuning → Evaluate forgetting and retention
- Design tradeoffs: Trade-off between complete forgetting of target instances and preservation of remaining data accuracy; computational cost of generating adversarial examples vs. unlearning effectiveness
- Failure signatures: Significant accuracy drop on remaining data indicates over-regularization; incomplete forgetting of target instances indicates under-regularization
- First 3 experiments:
  1. Test adversarial example generation on CIFAR-10 with ResNet-50 and verify examples contain target class features
  2. Calculate weight importance using MAS on CIFAR-100 subset and verify importance scores make intuitive sense
  3. Run single-task unlearning on CIFAR-10 with k=4 instances and verify complete forgetting while maintaining remaining data accuracy

## Open Questions the Paper Calls Out

- Question: How do different weight importance metrics affect instance-wise unlearning effectiveness?
  - Basis: [explicit] Paper uses MAS but acknowledges other metrics exist
  - Why unresolved: Only experiments with MAS, no comparison to Fisher information or path integral
  - What evidence would resolve: Comparative experiments using different weight importance metrics

- Question: What is the optimal balance between adversarial and weight importance regularization across dataset sizes and architectures?
  - Basis: [inferred] Paper mentions weight importance can be too strong in some cases
  - Why unresolved: Uses fixed hyperparameters without systematic exploration
  - What evidence would resolve: Ablation studies varying regularization strengths across conditions

## Limitations

- Major uncertainties exist in claims about achieving complete forgetting while preserving remaining data accuracy
- Method's performance heavily depends on hyperparameter choices (particularly weight importance regularization strength λ=1)
- Experimental evaluation relies on specific implementation details not fully specified
- Qualitative explanations lack rigorous theoretical grounding or extensive empirical validation

## Confidence

- **High confidence**: Core observation that combining adversarial examples with weight importance regularization improves unlearning performance
- **Medium confidence**: Claim of achieving complete misclassification while preserving remaining data accuracy (depends on implementation details)
- **Low confidence**: Qualitative explanations of why method works at representation level

## Next Checks

1. Implement ablation studies to quantify individual and combined contributions of adversarial regularization and weight importance across different model architectures and dataset sizes
2. Conduct experiments with varying weight importance regularization strengths (λ) to determine optimal values and assess sensitivity
3. Perform extensive analysis of adversarial examples generated during unlearning to verify they contain representative features of remaining data distribution