---
ver: rpa2
title: 'Contextual Refinement of Translations: Large Language Models for Sentence
  and Document-Level Post-Editing'
arxiv_id: '2310.14855'
source_url: https://arxiv.org/abs/2310.14855
tags:
- translation
- llama2
- sentence
- document-level
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to use Large Language Models (LLMs) for post-editing
  of Neural Machine Translation (NMT) systems to leverage their fluency and understanding.
  The authors show that using LLM adapters for post-editing leads to substantial improvements
  in translation quality, with gains up to 5.64 BLEU on out-of-domain data.
---

# Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing

## Quick Facts
- arXiv ID: 2310.14855
- Source URL: https://arxiv.org/abs/2310.14855
- Reference count: 22
- Key outcome: LLM adapters for post-editing achieve up to 5.64 BLEU improvement and 89% accuracy on pronoun disambiguation task

## Executive Summary
This paper explores using Large Language Models (LLMs) for post-editing Neural Machine Translation (NMT) outputs rather than directly translating. The authors demonstrate that fine-tuning LLMs with Q-LORA adapters for post-editing significantly improves translation quality, achieving state-of-the-art results on pronoun disambiguation tasks. The approach shows substantial gains on out-of-domain data and effectively leverages human-corrected context to reduce subsequent editing needs.

## Method Summary
The method involves generating initial translations using an NMT model (DeltaLM), then fine-tuning LLMs (Llama2) with Q-LORA adapters specifically for post-editing. The adapters are trained on sentence-level parallel data with source sentences and NMT hypotheses as input, and reference translations as targets. For document-level translation, the approach extends to processing sequences of sentences together, with optional integration of human-corrected context via forced decoding to improve coherence across the document.

## Key Results
- Up to 5.64 BLEU improvement on out-of-domain MuST-C test sets
- State-of-the-art 89% accuracy on ContraPro pronoun disambiguation test set
- Document-level post-editing effectively resolves discourse phenomena like pronoun ambiguity
- Human-corrected context integration reduces subsequent editing requirements

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs with Q-LORA adapters for post-editing is more effective than direct translation fine-tuning. The adapters allow parameter-efficient adaptation while preserving the base LLM's fluency and knowledge, avoiding catastrophic forgetting. Performance degrades when fine-tuning LLMs for direct translation purposes.

### Mechanism 2
Using human-corrected target context iteratively during document translation reduces required edits. By force-decoding gold target context for previous sentences, the model conditions on accurate information when generating subsequent translations, improving coherence through better context integration.

### Mechanism 3
Document-level post-editing leverages long-context understanding to resolve discourse phenomena like pronoun ambiguity. Processing sequences of source and hypothesis sentences together allows the model to use surrounding context for better decisions on ambiguous elements.

## Foundational Learning

- **Parameter-efficient fine-tuning (PEFT) methods like Q-LORA**: Needed because full fine-tuning is computationally prohibitive; allows adaptation with minimal parameters. Quick check: What is the rank parameter in Q-LORA and how does it affect the number of trainable parameters?

- **Post-editing vs. direct translation**: LLMs struggle with direct translation but excel at refining existing translations using their fluency and knowledge. Quick check: Why might providing the source sentence alongside the NMT hypothesis be important for effective post-editing?

- **Document-level translation evaluation metrics**: Standard BLEU/chrF2 are insufficient for discourse phenomena; additional metrics like MuDA tagger scores are needed. Quick check: What types of linguistic phenomena require document-level context that sentence-level metrics might miss?

## Architecture Onboarding

- **Component map**: DeltaLM (NMT) → Generates initial translations → Q-LORA adapters on Llama2 → Post-edits outputs → Evaluation pipeline (BLEU/chrF2/COMET + MuDA/ContraPro)

- **Critical path**: 1. Generate NMT hypothesis, 2. Format prompt with source + hypothesis, 3. Generate post-edited translation with adapted LLM, 4. Evaluate using appropriate metrics

- **Design tradeoffs**: Sentence-level vs. document-level (speed vs. context), chunk size selection (context vs. memory), adapter rank (capacity vs. parameters)

- **Failure signatures**: Performance degradation with high adapter rank or noisy data, inability to improve already high-quality NMT outputs, context not effectively utilized

- **First 3 experiments**: 1. Compare sentence-level NMT + zero-shot LLM PE vs. fine-tuned LLM direct translation, 2. Evaluate document-level vs. sentence-level APE on pronoun disambiguation, 3. Test manual feedback integration via force-decoding gold context

## Open Questions the Paper Calls Out
- How would the approach perform when trained on document-level parallel data instead of sentence-level?
- Would different decoding strategies (left-to-right/right-to-left) improve document-level performance?
- How would the approach perform with low-resource languages?
- How would different open-source LLM models perform for post-editing?

## Limitations
- Limited cross-lingual generalizability beyond English→German
- Evaluation scope constrained to automatic metrics and one discourse test set
- Computational overhead of document-level post-editing not addressed

## Confidence
- High confidence: LLM adapters for post-editing outperforming direct fine-tuning
- Medium confidence: Document-level improvements for pronoun disambiguation
- Medium confidence: Human-corrected context integration effectiveness

## Next Checks
1. Evaluate LLM adapter approach across at least 3 additional language pairs including one low-resource pair
2. Conduct human evaluation comparing sentence-level vs. document-level outputs across fluency, adequacy, and coherence
3. Measure wall-clock time and GPU memory requirements for sentence-level vs. document-level post-editing across different chunk sizes