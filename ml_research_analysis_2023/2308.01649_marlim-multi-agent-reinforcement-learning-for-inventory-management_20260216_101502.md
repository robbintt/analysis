---
ver: rpa2
title: 'MARLIM: Multi-Agent Reinforcement Learning for Inventory Management'
arxiv_id: '2308.01649'
source_url: https://arxiv.org/abs/2308.01649
tags:
- inventory
- costs
- item
- items
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARLIM, a novel reinforcement learning framework
  for inventory management in single-echelon multi-product supply chains with stochastic
  demands and lead times. The method trains single or multiple agents in a cooperative
  setting to optimize replenishment decisions.
---

# MARLIM: Multi-Agent Reinforcement Learning for Inventory Management

## Quick Facts
- arXiv ID: 2308.01649
- Source URL: https://arxiv.org/abs/2308.01649
- Reference count: 40
- Key result: MARLIM reduces cumulative inventory costs by 75-85% compared to traditional baselines

## Executive Summary
MARLIM introduces a novel reinforcement learning framework for inventory management in single-echelon multi-product supply chains with stochastic demands and lead times. The method trains single or multiple agents in a cooperative setting to optimize replenishment decisions, using Proximal Policy Optimization (PPO) with specific adaptations for inventory constraints. Real-world experiments show MARLIM significantly outperforms traditional baselines like MinMax and Oracle strategies, reducing cumulative costs by 75-85% across different warehouse clusters while effectively avoiding stock-outs.

## Method Summary
MARLIM uses cooperative multi-agent reinforcement learning where each product subspace is modeled as a separate Markov game. The framework employs PPO for single-agent settings and IPPO for multi-agent shared-capacity scenarios. State representation includes inventory levels, backlogs, and lead times, with actions representing order quantities. The reward function combines negative inventory costs (ordering, holding, shortage). Training uses historical demand and lead-time data estimated via maximum likelihood, with items clustered based on product type or median lead time.

## Key Results
- 75-85% reduction in cumulative costs compared to MinMax and Oracle baselines
- Zero stock-outs achieved in most cases across 100 replication experiments
- Superior performance across different warehouse clusters with varying product compositions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MARLIM uses cooperative multi-agent reinforcement learning to balance inventory levels across product clusters, minimizing both ordering and shortage costs.
- Mechanism: Each product subspace is modeled as a separate Markov game where agents cooperate by sharing an average reward function, enabling decentralized policy learning while coordinating on shared storage constraints.
- Core assumption: Product clusters can be treated as independent cooperative games without significant inter-cluster dependencies.
- Evidence anchors:
  - [abstract] "train single or multiple agents in a cooperative setting"
  - [section 4] "Inside a product subspace Nk, the agents are working in a cooperative setting in order to optimize the average reward"
  - [corpus] "Cooperative Multi-Agent Reinforcement Learning for Inventory Management" (FMR 0.0, no citations yet)

### Mechanism 2
- Claim: Overflow weight scaling based on shortage costs ensures critical items are prioritized during storage constraints.
- Mechanism: When storage space overflows in a subspace, replenishment quantities are scaled proportionally to each item's shortage cost, guaranteeing that high-priority items receive full replenishment before low-priority ones.
- Core assumption: Shortage costs accurately reflect item criticality and are known or estimable.
- Evidence anchors:
  - [section 3.3] "the overflow weights are set to be proportional to the shortage cost of items"
  - [section 3.2] "items with high shortage cost correspond to critical items that should not run out-of-stock"
  - [corpus] "Combating the Bullwhip Effect in Rival Online Food Delivery Platforms Using Deep Learning" (FMR 0.377, no citations yet)

### Mechanism 3
- Claim: Using PPO with clipped surrogate objective enables stable multi-agent training across varying cluster sizes.
- Mechanism: PPO's clipped objective prevents large policy updates that could destabilize training, while independent PPO (IPPO) allows agents to learn decentralized policies that still coordinate through shared reward structure.
- Core assumption: The clipped objective bounds policy updates sufficiently to maintain training stability in multi-agent settings.
- Evidence anchors:
  - [section 4] "IPPO [54] which is an independent version of PPO for multi-agent frameworks"
  - [appendix A] Detailed PPO implementation with clipping parameters
  - [corpus] "Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward Real-World Multi-Echelon Inventory Optimization" (FMR 0.500, no citations yet)

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their extension to Markov Games for multi-agent settings.
  - Why needed here: The inventory management problem is formalized as a Markov game where each product is an agent making sequential decisions under uncertainty.
  - Quick check question: What is the key difference between an MDP and a Markov game in terms of solution concepts?

- Concept: Policy gradient methods and advantage estimation (particularly GAE).
  - Why needed here: MARLIM uses PPO, which relies on policy gradients to update agent policies based on sampled trajectories and advantage estimates.
  - Quick check question: How does Generalized Advantage Estimation (GAE) help reduce variance in policy gradient updates?

- Concept: Cooperative vs. competitive multi-agent reinforcement learning.
  - Why needed here: The agents in each product subspace are cooperative (sharing rewards) rather than competitive, which affects the choice of learning algorithm and reward structure.
  - Quick check question: What is the difference between a fully cooperative game and a mixed-motive game in multi-agent RL?

## Architecture Onboarding

- Component map: Environment (supply chain simulator) -> State representation (inventory levels, backlogs, lead times) -> Agent(s) (PPO or IPPO) -> Policy network -> Action space (order quantities) -> Reward function (negative inventory costs)
- Critical path: Simulator generates state -> Agent selects action -> Environment transitions -> Reward computed -> Policy updated via PPO
- Design tradeoffs: Single-agent per cluster vs. multi-agent with shared critic; discrete vs. continuous action spaces; fixed vs. shared capacity constraints
- Failure signatures: Training collapse (loss divergence) -> Check clipping parameters and learning rates; Poor coordination (high shortages despite training) -> Verify reward sharing and state representation; Overfitting to training data -> Validate on held-out demand patterns
- First 3 experiments:
  1. Single-agent PPO on a single product with known demand distribution (validate basic learning)
  2. Multi-agent IPPO on a 2-product cluster with shared capacity (test coordination)
  3. Full MARLIM on 5-product cluster with stochastic demands (validate end-to-end performance)

## Open Questions the Paper Calls Out

The paper explicitly states that it focuses on single-echelon systems and acknowledges this as a limitation. It does not call out additional open questions within the text itself.

## Limitations

- Single-echelon focus limits applicability to more complex multi-echelon supply chains with inter-echelon dependencies
- Markov game assumption of cluster independence may break down if products have cross-cluster dependencies
- Performance comparisons limited to MinMax and Oracle baselines, not testing against other RL or meta-heuristic approaches

## Confidence

- 75-85% cost reduction claim: **High** (directly measured from controlled experiments with 100 replications)
- Cooperative multi-agent learning mechanism: **Medium** (supported by theoretical grounding and simulation results but not validated across diverse topologies)
- Scalability to larger portfolios: **Low** (experiments limited to moderate cluster sizes in single-echelon configurations)

## Next Checks

1. Test MARLIM on synthetic datasets with varying degrees of demand correlation across clusters to assess performance under different dependency structures
2. Compare MARLIM against other RL baselines (e.g., DQN, A2C) and traditional heuristics (e.g., (s,S) policies) on the same real-world dataset to establish relative performance
3. Conduct ablation studies removing key design choices (e.g., overflow weight scaling, PPO clipping) to quantify their individual contributions to performance gains