---
ver: rpa2
title: Finetuning an LLM on Contextual Knowledge of Classics for Q&A
arxiv_id: '2312.07848'
source_url: https://arxiv.org/abs/2312.07848
tags:
- knowledge
- project
- training
- language
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work fine-tunes a 355M-parameter GPT-2 model on a custom dataset
  of 1,172 prompts and responses about Classics, including Latin and historical context.
  The goal was to create an AI with accurate subject knowledge and a consistent personality
  for diverse audiences.
---

# Finetuning an LLM on Contextual Knowledge of Classics for Q&A

## Quick Facts
- arXiv ID: 2312.07848
- Source URL: https://arxiv.org/abs/2312.07848
- Reference count: 1
- Primary result: Fine-tuned GPT-2 Medium on 1,172 prompt-response pairs about Classics, producing contextually relevant outputs on consumer hardware

## Executive Summary
This work fine-tunes a 355M-parameter GPT-2 model on a custom dataset of 1,172 prompts and responses about Classics, including Latin and historical context. The goal was to create an AI with accurate subject knowledge and a consistent personality for diverse audiences. Training was performed on a consumer-grade GPU (Nvidia 2080 Ti) to show feasibility without high-cost resources. The model produced contextually relevant and linguistically coherent outputs, often with nuance and wisdom. However, it occasionally hallucinated, especially at higher temperatures, and sometimes generated inappropriate content. Multilingual handling and response length control also need refinement. The author plans to expand the dataset and incorporate more Latin, Greek, and academic texts to further enhance accuracy and personality.

## Method Summary
The project fine-tuned GPT-2 Medium (355M parameters) on a custom dataset of 1,172 prompt-response pairs about Classics, using an Nvidia 2080 Ti GPU with 11GB VRAM. Training ran for 5 epochs with a large batch size and learning rate of 5e-5, implemented via HuggingFace Transformers in a WSL2 + Anaconda environment. The dataset responses ranged from 75 to 350 words. Outputs were evaluated qualitatively for contextual relevance and personality consistency, with sampling parameters (temperature, k, top_p) adjusted to balance creativity and coherence.

## Key Results
- Fine-tuned GPT-2 Medium on 1,172 Classics prompts and responses produced contextually relevant and linguistically coherent outputs
- Consumer-grade hardware (Nvidia 2080 Ti) successfully handled fine-tuning of 355M-parameter model
- Model occasionally hallucinated and generated inappropriate content, especially at higher temperatures
- Response length control needs refinement to prevent abrupt mid-sentence endings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning GPT-2 with a custom dataset of Classics prompts and responses can produce contextually relevant outputs with a consistent "personality."
- Mechanism: By replacing the original web-scraped dataset with curated domain-specific prompts and responses, the model learns to associate patterns between inputs and outputs relevant to Classics, effectively narrowing its generation to the desired knowledge domain.
- Core assumption: The model's general language understanding is preserved while domain-specific knowledge is injected through the fine-tuning dataset.
- Evidence anchors:
  - [abstract] "This project is an attempt to merge the knowledge of Classics with the capabilities of artificial intelligence by finetuning an LLM to cater to the specific needs of learners and professionals."
  - [section] "1,172 unique prompts and responses were crafted, with response lengths ranging from 75 to 350 words."
  - [corpus] Weak evidence - corpus does not directly discuss fine-tuning LLMs on Classics datasets
- Break condition: If the fine-tuning dataset contains factual errors or inconsistent information, the model may learn and reproduce these inaccuracies as "knowledge."

### Mechanism 2
- Claim: Consumer-grade hardware (Nvidia 2080 Ti) can effectively fine-tune a 355M parameter model.
- Mechanism: The model's relatively modest parameter count (355M) makes it feasible to fine-tune on consumer hardware with 11GB VRAM, as opposed to larger models requiring specialized infrastructure.
- Core assumption: The model size is small enough to fit within the memory constraints of the available hardware during training.
- Evidence anchors:
  - [section] "The computational backbone of the project was an Nvidia 2080Ti, equipped with 11 GB of VRAM, which offered the necessary power and efficiency for handling LLMs up to a modest level of parameters."
  - [section] "The final training settings utilized a large batch size, a duration of five epochs, and a learning rate of 5e-5."
  - [corpus] Weak evidence - corpus does not discuss hardware requirements for fine-tuning
- Break condition: If the model parameters or batch size exceed VRAM capacity, training will fail or require significant parameter reduction.

### Mechanism 3
- Claim: Temperature and sampling parameters significantly affect the model's output style and appropriateness.
- Mechanism: Adjusting temperature, k values, and top_p values controls the randomness and diversity of generated text, allowing the model to balance between creativity and coherence.
- Core assumption: The model's generation quality is sensitive to sampling hyperparameters, which can be tuned to achieve desired output characteristics.
- Evidence anchors:
  - [section] "The model's occasional tendency, based on prompting it with edge cases, to generate somewhat disconcerting or inappropriate content"
  - [section] "The experiments highlighted the balance between computational resources and output quality"
  - [section] "Currently, the model can conclude abruptly, cutting off mid-sentence, due to the length parameter"
- Break condition: If temperature is set too high or sampling parameters are poorly tuned, the model may produce incoherent or inappropriate content regardless of fine-tuning quality.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how the base GPT-2 model processes and generates text is essential for comprehending how fine-tuning modifies its behavior
  - Quick check question: How does the self-attention mechanism allow the model to weigh the importance of different words in a sequence?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The project leverages pre-trained weights and adapts them to a specific domain rather than building a model from scratch
  - Quick check question: What are the computational and data efficiency advantages of fine-tuning a pre-trained model compared to training from scratch?

- Concept: Dataset quality and its impact on model outputs
  - Why needed here: The project emphasizes "garbage in, garbage out" principles, highlighting how dataset quality directly affects generated responses
  - Quick check question: How might factual errors or inconsistencies in the training dataset manifest in the model's outputs?

## Architecture Onboarding

- Component map:
  - Custom dataset (1,172 prompt-response pairs) -> GPT-2 Medium model -> Nvidia 2080 Ti GPU -> HuggingFace Transformers framework -> Output with sampling parameters

- Critical path:
  1. Prepare custom dataset in proper format
  2. Load pre-trained GPT-2 Medium model
  3. Configure training parameters (batch size, learning rate, epochs)
  4. Execute training process
  5. Evaluate outputs with different sampling parameters
  6. Iterate on dataset or parameters as needed

- Design tradeoffs:
  - Model size vs. hardware limitations: 355M parameters chosen to balance capability with consumer hardware constraints
  - Dataset size vs. training quality: 1,172 examples may be sufficient for demonstration but may limit depth of knowledge
  - Training duration vs. overfitting: 5 epochs chosen to balance learning with avoiding memorization of training data

- Failure signatures:
  - Out-of-memory errors during training indicate batch size or model parameters exceed VRAM capacity
  - Hallucinations or inappropriate content suggest issues with dataset quality or sampling parameters
  - Repetitive or generic outputs may indicate insufficient dataset diversity or overtraining

- First 3 experiments:
  1. Train with default sampling parameters (temperature=1.0, k=50, top_p=1.0) to establish baseline performance
  2. Adjust temperature downward (e.g., 0.7) to reduce hallucinations and increase coherence
  3. Experiment with different batch sizes to optimize training speed vs. model quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model be improved to reduce hallucinations, especially regarding historical facts and its own identity?
- Basis in paper: [explicit] The paper mentions that the model's occasional hallucinations, particularly in its assertions about historical events or its own identity, make it seem somewhat capricious.
- Why unresolved: Hallucinations in large language models are a persistent challenge in the field, and current methods for reducing them, such as fact-checking mechanisms or context chains, are still being developed and refined.
- What evidence would resolve it: A study comparing the frequency and severity of hallucinations before and after implementing specific techniques to reduce them, such as incorporating fact-checking mechanisms or using more diverse and accurate training data.

### Open Question 2
- Question: How can the model's multilingual capabilities be enhanced, particularly in handling Latin and Greek texts?
- Basis in paper: [explicit] The paper mentions that the model's handling of multilingual inputs and complex queries is an area of interest, and that incorporating knowledge of ancient Greek is another goal to further solidify the model's usefulness and intelligence.
- Why unresolved: Multilingual training for large language models is a complex task that requires careful consideration of linguistic nuances and cultural context. The field is still evolving, and there is ongoing research into effective methods for multilingual training.
- What evidence would resolve it: A study comparing the model's performance on Latin and Greek texts before and after implementing specific techniques for multilingual training, such as incorporating more diverse and accurate training data or using techniques like transfer learning.

### Open Question 3
- Question: How can the model's response length control be improved to prevent abrupt endings and ensure more coherent and satisfying interactions?
- Basis in paper: [explicit] The paper mentions that the model can conclude abruptly, cutting off mid-sentence, due to the length parameter, and that addressing this issue by enabling the model to recognize length and conclude its responses with consideration of that is an area for improvement.
- Why unresolved: Controlling response length in large language models is a challenging task that requires balancing the need for coherent and complete responses with the computational constraints of the model. Current methods for controlling response length are still being developed and refined.
- What evidence would resolve it: A study comparing the frequency and severity of abrupt endings before and after implementing specific techniques to improve response length control, such as incorporating mechanisms that allow the model to detect the natural end of a response or using techniques like reinforcement learning.

## Limitations

- Dataset size of 1,172 examples may limit depth of knowledge compared to the original GPT-2's vast training corpus
- Qualitative evaluation relies on subjective human judgment without establishing clear criteria for accuracy or appropriateness
- Hardware constraints (11GB VRAM) may have limited model size, potentially compromising capacity to capture complex linguistic patterns

## Confidence

**High Confidence**:
- Feasibility of fine-tuning GPT-2 Medium on consumer hardware is well-supported by explicit training configuration details
- Model produces contextually relevant outputs is directly demonstrated through examples
- Relationship between sampling parameters and output quality is clearly documented through experimentation

**Medium Confidence**:
- Fine-tuning preserves general language understanding while injecting domain knowledge assumes base model's capabilities transfer effectively
- Dataset quality directly impacts output quality is stated but not empirically validated through controlled experiments
- Personality consistency effectiveness is asserted but not systematically measured

**Low Confidence**:
- Long-term stability of model behavior across diverse prompts is not tested
- Scalability of approach to larger datasets or more complex Classics topics is not explored
- Comparative advantage over existing Classics resources is not established

## Next Checks

1. **Dataset Quality Audit**: Conduct a systematic evaluation of the training dataset for factual accuracy, coverage completeness, and internal consistency. This should include cross-referencing responses against authoritative Classics sources and identifying potential knowledge gaps or contradictions.

2. **Parameter Sensitivity Analysis**: Systematically vary temperature, k, and top_p parameters across a wider range (0.1-1.5 for temperature, 10-100 for k, 0.1-1.0 for top_p) and document the precise relationship between each parameter and output quality metrics including coherence scores, hallucination rates, and appropriateness measures.

3. **Long-form Generation Test**: Evaluate the model's performance on extended dialogue sequences (5+ turns) to assess personality consistency and knowledge retention over time. This would reveal whether the model maintains context and accuracy across multi-turn interactions rather than just single-prompt responses.