---
ver: rpa2
title: 'MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters'
arxiv_id: '2311.04251'
source_url: https://arxiv.org/abs/2311.04251
tags:
- network
- weights
- growth
- small
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MixtureGrowth, a method to grow neural networks
  over time by reusing and combining learned parameters. The key idea is to use template
  mixing, where layer weights are generated using a linear combination of shared parameter
  templates.
---

# MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters

## Quick Facts
- arXiv ID: 2311.04251
- Source URL: https://arxiv.org/abs/2311.04251
- Authors: 
- Reference count: 40
- Primary result: 2-2.5% improvement in top-1 accuracy on CIFAR-100 and ImageNet compared to state-of-the-art network growing methods

## Executive Summary
MixtureGrowth introduces a novel approach to growing neural networks by reusing and recombining learned parameters through template mixing. The method generates new layer weights by learning new linear coefficients for existing parameter templates, avoiding computationally expensive analysis of training data. Experiments demonstrate 2-2.5% improvements in top-1 accuracy on CIFAR-100 and ImageNet while achieving comparable performance with fewer FLOPs than training larger networks from scratch.

## Method Summary
MixtureGrowth grows neural networks by first training small networks using template mixing, where layer weights are generated as linear combinations of shared parameter templates. The approach then trains an extra small network with independent templates, fuses both models, and grows to the target size by tiling weights generated from the templates. New linear coefficients are learned for the expanded layers, with orthogonal initialization shown to be most effective. This avoids the computational overhead of traditional growing methods that analyze training data to initialize new parameters.

## Key Results
- Achieves 2-2.5% higher top-1 accuracy on CIFAR-100 and ImageNet compared to state-of-the-art growing methods
- Matches performance of larger networks trained from scratch while using fewer FLOPs
- Orthogonal initialization of new linear coefficients outperforms random and coefficient copying strategies

## Why This Works (Mechanism)

### Mechanism 1
Template mixing enables new layer weights to be generated by learning new linear coefficients for existing parameter templates, avoiding computationally expensive analysis of training data. Each layer's weights are linear combinations of shared templates, and when growing, new weights are generated by learning coefficients for these already-trained templates.

### Mechanism 2
Fusing two independently trained small networks provides more diversity in learned features, creating a more robust feature representation. The independently trained models learn substantially independent features, and their fusion improves performance after growing.

### Mechanism 3
Careful initialization of new linear coefficients is crucial for optimal performance. Orthogonal initialization promotes diversity in generating weights and outperforms random initialization and coefficient copying, with random initialization also performing well.

## Foundational Learning

- **Concept: Template mixing**
  - Why needed here: Enables generation of new layer weights by learning new linear coefficients for existing parameter templates, avoiding expensive analysis of training data
  - Quick check question: How does template mixing differ from traditional neural networks in terms of weight generation and parameter efficiency?

- **Concept: Network growing**
  - Why needed here: The process of expanding a small, pre-trained network to a larger target network, which is MixtureGrowth's main goal
  - Quick check question: What are the challenges associated with growing neural networks, and how does MixtureGrowth address them?

- **Concept: Model fusion**
  - Why needed here: Combines two independently trained small networks to provide more diversity in learned features, leading to more robust feature representation after growing
  - Quick check question: How does model fusion differ from simply training a single small network and growing from it?

## Architecture Onboarding

- **Component map**: Small network (F_S) -> Extra small network (F_S2) -> Target network (F_L) -> Linear coefficients (Î±) -> Parameter templates (T)
- **Critical path**: 1) Train small network (F_S) with template mixing 2) Train extra small network (F_S2) with independent templates 3) Grow network by tiling weights from F_S and F_S2 4) Initialize new linear coefficients using orthogonal initialization 5) Train grown network until convergence
- **Design tradeoffs**: Parameter budget balancing templates and size, growth strategy choosing vertical vs horizontal expansion, coefficient initialization selecting appropriate strategy
- **Failure signatures**: Poor post-growth performance (template generalization issues), high computational overhead (inefficient template use), training difficulty (template mixing problems)
- **First 3 experiments**: 1) Train small network (F_S) using template mixing and evaluate on benchmark dataset 2) Train extra small network (F_S2) with independent templates and compare performance 3) Grow network using templates from F_S and F_S2, evaluate grown network performance

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The approach's sensitivity to template mixing parameters (number of templates, parameter budget) is not explored through ablation studies
- Only evaluated on image classification tasks using WideResNet and ResNet architectures
- Computational overhead trade-offs between training multiple small networks versus a single larger network from scratch are not thoroughly analyzed

## Confidence

| Claim | Confidence |
|-------|------------|
| Template mixing effectiveness | Medium |
| Model fusion benefits | Low |
| Coefficient initialization impact | Medium |

## Next Checks

1. Conduct ablation studies comparing performance with and without model fusion to quantify the actual benefit of training an extra small network
2. Test the approach on additional datasets beyond CIFAR and ImageNet to evaluate generalizability
3. Analyze computational cost trade-offs between training multiple small networks versus a single larger network from scratch, including training time and memory requirements