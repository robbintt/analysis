---
ver: rpa2
title: 'OctoPack: Instruction Tuning Code Large Language Models'
arxiv_id: '2308.07124'
source_url: https://arxiv.org/abs/2308.07124
tags:
- code
- arxiv
- language
- instruction
- commit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Instruction tuning large language models on code improves their
  performance on code-related tasks. This work compiles CommitPack, a 4 terabyte dataset
  of Git commits across 350 programming languages, and uses it to instruction tune
  code models.
---

# OctoPack: Instruction Tuning Code Large Language Models

## Quick Facts
- arXiv ID: 2308.07124
- Source URL: https://arxiv.org/abs/2308.07124
- Reference count: 40
- Primary result: Instruction tuning on 4TB of Git commits achieves 46.2% pass@1 on HumanEval, a 38% improvement over base model

## Executive Summary
This work introduces CommitPack, a 4 terabyte dataset of Git commits across 350 programming languages, and uses it to instruction tune code large language models. The resulting models, OctoCoder and OctoGeeX, achieve state-of-the-art performance on HumanEvalPack, a benchmark covering code repair, explanation, and synthesis across six programming languages. By leveraging the natural instruction-code pairing structure of Git commits, the models learn to follow natural language instructions for diverse code tasks. The approach demonstrates significant improvements in code generation performance while maintaining permissive licensing.

## Method Summary
The authors compile CommitPack, a 4TB dataset of Git commits across 350 programming languages, and filter it to create high-quality instruction pairs (CommitPackFT). They then instruction tune StarCoder (16B) and CodeGeeX2 (6B) models on this data combined with OASST. The models are evaluated zero-shot on HumanEvalPack, a benchmark spanning three coding tasks (Code Repair, Code Explanation, Code Synthesis) across six languages. The instruction tuning process uses standard supervised fine-tuning with a learning rate of 5e-4 for 35 steps.

## Key Results
- OctoCoder achieves 46.2% pass@1 on the original Python HumanEval benchmark
- 38% improvement over the base StarCoder model
- State-of-the-art performance on HumanEvalPack among permissively licensed models
- Successful generalization to programming languages not seen during instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning code models on natural language instructions paired with code diffs improves their ability to follow human instructions across diverse code tasks
- Mechanism: The model learns to map natural language instructions (commit messages) to corresponding code modifications (diffs) through supervised fine-tuning on paired examples
- Core assumption: The structure of Git commits provides a natural instruction-code pairing that captures human intent and the necessary code changes
- Evidence anchors:
  - [abstract]: "We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions"
  - [section 2]: "COMMIT PACK: 4 terabytes of Git commits across 350 programming languages"
- Break condition: If commit messages are too vague or code changes too complex, the model cannot learn the mapping effectively

### Mechanism 2
- Claim: Instruction tuning improves generalization to programming languages seen during pretraining but not during instruction tuning
- Mechanism: The model learns general patterns of instruction-following that transfer across languages through shared syntactic and semantic features
- Core assumption: Programming languages share enough structural similarities that instruction-following skills transfer across them
- Evidence anchors:
  - [abstract]: "demonstrating COMMIT PACK's benefits in generalizing to a wider set of languages and natural coding tasks"
  - [section 4.2]: "Go and Rust are not contained in BLOOMZ's instruction data, yet it performs much better than the random baseline"
- Break condition: If languages are too dissimilar or instruction-following patterns don't transfer, generalization fails

### Mechanism 3
- Claim: Instruction tuning enables code models to switch between code and natural language generation as needed by the task
- Mechanism: The model learns to recognize when the task requires code output versus natural language output based on the instruction context
- Core assumption: The instruction context provides sufficient signal for the model to determine the appropriate output modality
- Evidence anchors:
  - [abstract]: "OCTO CODER and OCTO GEEX, achieve the best performance across HUMAN EVALPACK among all permissive models"
  - [section 4.2]: "Some models fail at HUMAN EVALEXPLAIN, as they do not generate natural language explanations"
- Break condition: If the model cannot distinguish between code and natural language tasks, it will generate incorrect output modalities

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: To teach code models to follow natural language instructions rather than just continuing code
  - Quick check question: What is the difference between pretraining and instruction tuning in the context of code models?

- Concept: Pass@k metric
  - Why needed here: To evaluate code generation models by checking if any of k generated solutions pass all test cases
  - Quick check question: How does pass@1 differ from pass@k where k>1 in evaluating code models?

- Concept: Code diff formats
  - Why needed here: To represent code changes efficiently for training models on commit data
  - Quick check question: What are the advantages of line diff format over unified diff format for training code models?

## Architecture Onboarding

- Component map: Pretrained code model -> Instruction tuning dataset -> Finetuned model -> Evaluation on HumanEvalPack tasks
- Critical path: Quality commit data -> Effective filtering -> Proper instruction formatting -> Stable finetuning -> Comprehensive evaluation
- Design tradeoffs: Larger models have better performance but higher computational cost; more diverse instruction data improves generalization but may reduce task-specific performance
- Failure signatures: Poor performance on HumanEvalPack tasks indicates issues with instruction data quality, model capacity, or training stability
- First 3 experiments:
  1. Finetune StarCoder on filtered commit data and evaluate on HumanEvalSynthesize
  2. Test model's ability to generate natural language explanations on HumanEvalExplain
  3. Evaluate code repair performance on HumanEvalFix with different bug types

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved regarding generalization to unseen languages, impact of pretraining data composition on natural language tasks, and optimization techniques for efficient code model inference.

## Limitations
- Reliance on commit messages as instruction data may contain ambiguous or context-dependent information
- Evaluation scope limited to 6 programming languages despite dataset containing 350+
- Zero-shot evaluation may not reflect fine-tuned performance on specific downstream tasks

## Confidence
- **High Confidence**: Instruction tuning on commit data improves code model performance (well-supported by empirical results)
- **Medium Confidence**: Generalization to unseen programming languages (limited by evaluation scope)
- **Low Confidence**: Claim of "best performance" among all permissive models (only compares against limited competitors)

## Next Checks
1. Test model performance on additional programming languages not present in either pretraining data or instruction tuning dataset
2. Conduct systematic analysis of commit message quality and its correlation with model performance
3. Evaluate models on downstream tasks requiring mixed code and natural language outputs (documentation generation, code review assistance)