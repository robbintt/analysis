---
ver: rpa2
title: 'Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language
  Models'
arxiv_id: '2308.03151'
source_url: https://arxiv.org/abs/2308.03151
tags:
- food
- vlms
- image
- images
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Food-500 Cap, a fine-grained food caption
  benchmark designed to evaluate vision-language models (VLMs) in the food domain.
  The dataset consists of 24,700 images from 494 food categories, each accompanied
  by detailed captions describing visual attributes such as ingredients, shape, and
  color.
---

# Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models

## Quick Facts
- arXiv ID: 2308.03151
- Source URL: https://arxiv.org/abs/2308.03151
- Reference count: 40
- VLMs underperform in food domain compared to general domain, with severe regional bias

## Executive Summary
This paper introduces Food-500 Cap, a benchmark dataset of 24,700 food images across 494 categories with detailed captions describing ingredients, shape, color, and container attributes. The dataset includes geographic region labels to expose cultural bias in vision-language models (VLMs). The authors evaluate nine representative VLMs from three architectures across four probing tasks in zero-shot settings, revealing significant performance gaps in the food domain and severe regional biases, particularly against Asian cuisines.

## Method Summary
The study uses the Food-500 Cap dataset with detailed captions and geographic region labels to evaluate VLMs across four tasks: classification, image-text retrieval, image captioning, and image synthesis. Models are tested in zero-shot settings using pre-trained weights without fine-tuning. Evaluation metrics include classification accuracy, retrieval scores (R@1, R@5, R@10), captioning metrics (BLEU, METEOR, ROUGE, CIDEr, CLIPScore), and image synthesis quality measures (FID, FIDCLIP, CAS).

## Key Results
- VLMs achieve only 42.65% accuracy in food classification, significantly lower than general-domain performance
- Severe geographic bias exists, with models performing notably worse on Asian cuisines compared to Western and Latin American cuisines
- Image-to-text models often neglect or misidentify fine-grained attributes like ingredients and colors, generating hallucinations or meaningless additions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-grained captions address VLMs' inability to align rich visual attributes with language
- **Core assumption**: VLMs underperform because training data lacks fine-grained visual-textual pairs
- **Evidence anchors**: Detailed captions include ingredients, colors, shapes, and containers; general-domain datasets lack similar detail
- **Break condition**: Similar performance on Food-500 Cap and general-domain benchmarks would invalidate fine-grained caption assumption

### Mechanism 2
- **Claim**: Geographic region labeling exposes cultural bias in VLMs' food recognition
- **Core assumption**: Pre-training data distribution causes uneven performance across regional cuisines
- **Evidence anchors**: Models perform notably worse on Asian cuisines; BLIPitm's image-to-text recall@1 score reaches 66.0 for Western foods but struggles with Chinese dishes
- **Break condition**: Performance differences disappear with balanced training data would invalidate geographic bias assumption

### Mechanism 3
- **Claim**: Zero-shot evaluation reveals VLMs' generalization limits in specialized domains
- **Core assumption**: Performance gap between general and food domains indicates domain-specific limitations
- **Evidence anchors**: VLMs underperform in food domain compared to general domain; zero-shot setting tests knowledge transfer without fine-tuning
- **Break condition**: Comparable performance across domains with minimal adaptation would invalidate zero-shot generalization assumption

## Foundational Learning

- **Cross-modal alignment evaluation**: Essential for understanding how well VLMs align visual and textual representations in food domain
  - Quick check: How do VLMs measure similarity between image and text embeddings, and what metrics indicate successful alignment?

- **Zero-shot learning evaluation**: Critical for assessing VLMs' ability to generalize without domain-specific fine-tuning
  - Quick check: What distinguishes zero-shot from few-shot or fine-tuned evaluation, and why is it important for detecting domain-specific limitations?

- **Cultural bias detection in ML models**: Crucial for identifying performance disparities across culinary cultures
  - Quick check: How can geographic or cultural categorization in datasets reveal systematic biases in model performance?

## Architecture Onboarding

- **Component map**: Images → Detailed captions + geographic region labels → Three VLM architectures → Four tasks (classification, retrieval, captioning, synthesis) → Zero-shot evaluation → Performance analysis
- **Critical path**: Data collection → Annotation with fine-grained captions and region labels → Model selection across three architectures → Zero-shot task evaluation → Performance analysis across regions and tasks
- **Design tradeoffs**: Zero-shot evaluation reveals generalization limits but may underestimate potential with fine-tuning; fine-grained captions enable detailed evaluation but increase annotation costs; geographic labeling exposes bias but may oversimplify complex culinary relationships
- **Failure signatures**: Poor performance on fine-grained attributes suggests caption alignment issues; regional performance disparities indicate cultural bias; inconsistent results across architectures reveal architectural limitations for specific tasks
- **First 3 experiments**:
  1. Test CLIP on food classification with ITC and ITM configurations to establish baseline performance
  2. Evaluate BLIPDec on image captioning to assess fine-grained attribute generation
  3. Compare Stable Diffusion and minDALL-E on image synthesis quality using FID and CAS metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of vision-language models vary across different culinary cultures when evaluated on specialized food datasets like Food-500 Cap? While the paper identifies bias, it doesn't explore underlying reasons or propose mitigation methods.

### Open Question 2
What are the limitations of current vision-language models in generating detailed and accurate captions for food images, and how can these models be improved? The paper highlights shortcomings but doesn't explore specific architectural or training data limitations.

### Open Question 3
How do text-to-image generative models perform in synthesizing realistic food images that accurately represent input descriptions, and what are the challenges in improving this performance? The paper identifies performance gaps but doesn't explore technical challenges or propose solutions.

## Limitations

- Zero-shot evaluation may underestimate models' potential with domain-specific fine-tuning
- Geographic region labeling oversimplifies complex culinary relationships and cultural influences
- Focus on English-language captions limits generalizability to other languages and cultures

## Confidence

- **High confidence**: VLMs underperform in food domain compared to general domain; geographic bias in model performance (poor Asian cuisine recognition)
- **Medium confidence**: Attribution of poor performance to fine-grained attribute alignment challenges; severity of regional bias may be influenced by dataset composition
- **Low confidence**: Extent to which zero-shot evaluation reflects real-world deployment scenarios

## Next Checks

1. Conduct fine-tuning experiments on Food-500 Cap to determine whether performance gaps between food and general domains persist with domain adaptation
2. Expand geographic region analysis to include more granular cultural distinctions and examine whether performance correlates with specific culinary characteristics
3. Test model performance on multilingual food descriptions to assess whether observed limitations extend beyond English-language captions