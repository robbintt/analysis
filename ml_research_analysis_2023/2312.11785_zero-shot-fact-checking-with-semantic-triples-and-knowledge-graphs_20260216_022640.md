---
ver: rpa2
title: Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs
arxiv_id: '2312.11785'
source_url: https://arxiv.org/abs/2312.11785
tags:
- evidence
- fever
- claim
- system
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel zero-shot approach to fact-checking
  by leveraging semantic triples and knowledge graphs. The core method extracts triples
  from claims and evidence, augments them with external knowledge using a universal
  schema model, and employs pre-trained natural language inference (NLI) models for
  verification.
---

# Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs

## Quick Facts
- arXiv ID: 2312.11785
- Source URL: https://arxiv.org/abs/2312.11785
- Reference count: 13
- Outperforms previous zero-shot approaches by ~2.5 percentage points

## Executive Summary
This paper introduces a novel zero-shot approach to fact-checking that leverages semantic triples and knowledge graphs. The method extracts triples from claims and evidence, augments them with external knowledge using a universal schema model, and employs pre-trained NLI models for verification. By avoiding labeled training data, the approach addresses the limitations of supervised models that are vulnerable to adversarial attacks and struggle with out-of-domain tasks. The system consistently outperforms previous zero-shot approaches and achieves state-of-the-art results on adversarial datasets like FEVER-Symmetric and FEVER 2.0, while also surpassing supervised models by over 10 percentage points on the out-of-domain Climate-FEVER dataset.

## Method Summary
The approach extracts semantic triples (subject-relation-object) from both claims and evidence using OpenIE, then employs pre-trained NLI models to evaluate entailment relationships between corresponding triples. A universal schema model augments missing triples by predicting their likelihood based on observed triples from knowledge graphs. Claim-level verdicts are determined through rule-based aggregation of triple-level inferences. The entire pipeline operates without any labeled training data, making it a true zero-shot fact-checking system that leverages pre-trained models and external knowledge sources.

## Key Results
- Outperforms previous zero-shot approaches by approximately 2.5 percentage points
- Achieves state-of-the-art results on adversarial datasets (FEVER-Symmetric, FEVER 2.0)
- Surpasses supervised models by over 10 percentage points on Climate-FEVER dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing claims into semantic triples enables more precise matching with evidence triples through pre-trained NLI models.
- **Mechanism**: The system extracts subject-relation-object triples from both claims and evidence, then uses pre-trained NLI models to evaluate entailment relationships between corresponding triples.
- **Core assumption**: Semantic triples capture the essential semantic content needed for fact verification, and pre-trained NLI models can effectively evaluate relationships between triples.
- **Evidence anchors**: [abstract] "decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference"
- **Break condition**: If semantic triples fail to capture the necessary semantic content for verification, or if pre-trained NLI models cannot effectively evaluate triple-level relationships.

### Mechanism 2
- **Claim**: The Universal Schema model fills knowledge gaps by predicting missing triples based on observed triples, improving verification accuracy.
- **Mechanism**: The Universal Schema model uses Bayesian Personalized Ranking to learn embeddings for relations and entities from knowledge graphs. During inference, it predicts the probability of missing triples being true based on observed triples.
- **Core assumption**: Missing triples can be accurately predicted based on observed triples using learned embeddings, and these predictions improve verification accuracy.
- **Evidence anchors**: [abstract] "augments them with external knowledge using a universal schema model"
- **Break condition**: If the Universal Schema model's predictions are unreliable or if the predicted triples introduce noise rather than useful information.

### Mechanism 3
- **Claim**: Zero-shot learning avoids dataset-specific biases that cause supervised models to fail on adversarial and out-of-domain datasets.
- **Mechanism**: By not using any labeled training data, the system avoids learning spurious correlations and dataset artifacts that supervised models exploit.
- **Core assumption**: Dataset-specific biases are a primary cause of supervised model failures on adversarial and out-of-domain data.
- **Evidence anchors**: [abstract] "does not require any labeled training data, addressing the limitations of supervised models that are vulnerable to adversarial attacks"
- **Break condition**: If other factors beyond dataset bias are the primary cause of supervised model failures.

## Foundational Learning

- **Concept: Semantic Triples**
  - Why needed here: Triples provide a structured representation that enables precise matching between claims and evidence
  - Quick check question: Can you explain what a semantic triple is and give an example?

- **Concept: Natural Language Inference**
  - Why needed here: NLI models evaluate the relationship between claim triples and evidence triples (entailment, contradiction, neutral)
  - Quick check question: What are the three possible outcomes of an NLI model, and what do they mean?

- **Concept: Knowledge Graph Embeddings**
  - Why needed here: Universal Schema requires learning vector representations of relations and entities to predict missing triples
  - Quick check question: How do knowledge graph embedding models represent entities and relations?

## Architecture Onboarding

- **Component map**: Evidence Retrieval → Triple Extraction → Triple-level Verification → Claim-level Verification → Universal Schema (optional augmentation)

- **Critical path**: Evidence Retrieval → Triple Extraction → Triple-level Verification → Claim-level Verification

- **Design tradeoffs**:
  - Zero-shot approach vs. supervised learning (generalization vs. performance on training domain)
  - Triple-level precision vs. computational overhead
  - Rule-based claim verification vs. learned aggregation

- **Failure signatures**:
  - High NEI rates suggest evidence retrieval or triple extraction problems
  - Inconsistent results across NLI model sizes suggest threshold tuning issues
  - Poor performance on adversarial datasets suggests need for better evidence retrieval

- **First 3 experiments**:
  1. Verify that triple extraction produces reasonable output on sample claims and evidence
  2. Test NLI model performance on simple entailment/contradiction examples
  3. Evaluate the impact of Universal Schema thresholds on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the zero-shot approach perform on languages other than English, and what specific challenges arise when adapting the method to other languages?
- Basis in paper: [inferred] The paper's limitation section explicitly states that the approach has only been tested on English language datasets, and its effectiveness with other languages remains unverified.
- Why unresolved: The paper does not provide any experimental results or analysis of the approach's performance on non-English datasets.
- What evidence would resolve it: Experiments evaluating the approach's performance on multilingual datasets, with detailed analysis of linguistic features and semantic triple structures across languages.

### Open Question 2
- Question: How does the reliance on Wikipedia as the sole knowledge source affect the model's ability to fact-check claims that require information outside of Wikipedia's domain?
- Basis in paper: [explicit] The limitations section mentions that the approach relies heavily on Wikipedia as both the source for datasets and the basis for knowledge graphs, which introduces biases and limitations.
- Why unresolved: The paper does not provide any experiments or analysis of the model's performance on claims requiring knowledge outside of Wikipedia's domain.
- What evidence would resolve it: Experiments evaluating the approach's performance on fact-checking claims that require knowledge from diverse sources beyond Wikipedia, with analysis of the impact of domain-specific information on accuracy.

### Open Question 3
- Question: How does the zero-shot approach compare to supervised methods in terms of computational efficiency and resource requirements during both training and inference phases?
- Basis in paper: [inferred] The paper focuses on the effectiveness of the zero-shot approach but does not provide detailed comparisons of computational efficiency or resource requirements with supervised methods.
- Why unresolved: The paper does not include any benchmarks or analysis of computational efficiency, training time, or inference speed compared to supervised approaches.
- What evidence would resolve it: Detailed benchmarks comparing the computational resources, training time, and inference speed of the zero-shot approach versus supervised methods, including memory usage and processing time measurements.

## Limitations

- The effectiveness of semantic triples in capturing all relevant semantic information for complex claims remains uncertain
- The Universal Schema model's predictions could introduce noise if knowledge graph embeddings are not sufficiently accurate
- The rule-based claim-level aggregation might not capture nuanced relationships between triples that a learned approach could identify

## Confidence

- **High Confidence**: The paper's core methodology (extracting triples, using NLI for verification, and leveraging knowledge graphs) is well-established and the experimental results are reproducible with the described approach.
- **Medium Confidence**: The claims about avoiding dataset-specific biases and achieving better generalization are reasonable given the experimental results, but the analysis could be deeper.
- **Low Confidence**: The specific contribution of the Universal Schema model to overall performance is unclear, and the computational efficiency of the approach compared to supervised methods is not discussed.

## Next Checks

1. Run ablation studies without the Universal Schema augmentation to quantify its contribution to the 2.5 percentage point improvement over zero-shot baselines.

2. Test the system on claims requiring temporal reasoning or negation to reveal whether the method truly captures semantic content or just surface-level relationships.

3. Conduct detailed error analysis on FEVER-Symmetric to identify specific patterns where the model fails and determine whether failures stem from evidence retrieval, triple extraction, NLI limitations, or claim aggregation rules.