---
ver: rpa2
title: Compressed and Sparse Models for Non-Convex Decentralized Learning
arxiv_id: '2311.05760'
source_url: https://arxiv.org/abs/2311.05760
tags:
- communication
- learning
- decentralized
- where
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses communication efficiency in decentralized\
  \ machine learning by proposing MALCOM-PSGD, which combines gradient compression\
  \ and model sparsification. The method employs proximal stochastic gradient descent\
  \ with \u2113\u2081 regularization to promote model sparsity, alongside quantization\
  \ and source coding for compressed communication."
---

# Compressed and Sparse Models for Non-Convex Decentralized Learning

## Quick Facts
- **arXiv ID**: 2311.05760
- **Source URL**: https://arxiv.org/abs/2311.05760
- **Reference count**: 40
- **Primary result**: Achieves 75% reduction in communication costs compared to state-of-the-art methods while maintaining comparable learning performance.

## Executive Summary
This paper addresses the communication bottleneck in decentralized machine learning by proposing MALCOM-PSGD, a method that combines gradient compression and model sparsification. The approach leverages proximal stochastic gradient descent with ℓ₁ regularization to promote model sparsity, alongside quantization and source coding techniques to compress communication. The method demonstrates significant communication cost reductions while maintaining competitive learning performance, with theoretical convergence guarantees of O(ln(t)/√t) and consensus rate of O(1/t) under diminishing learning rates.

## Method Summary
MALCOM-PSGD employs proximal stochastic gradient descent to handle the non-smoothness introduced by ℓ₁ regularization in model sparsification. Each node performs local model updates, followed by residual quantization and vector source coding to compress communication. The method balances local training with neighbor information exchange through a consensus aggregation scheme. The algorithm achieves convergence and consensus rates under diminishing learning rates while reducing communication costs through efficient compression of sparse model residuals.

## Key Results
- Achieves approximately 75% reduction in communication costs compared to state-of-the-art methods
- Maintains learning performance comparable to uncompressed decentralized SGD
- Demonstrates theoretical convergence rate of O(ln(t)/√t) and consensus rate of O(1/t)
- Validated on MNIST and CIFAR10 datasets with ResNet18 architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining proximal SGD with ℓ₁ regularization promotes model sparsity while handling non-smoothness
- Mechanism: Proximal SGD performs gradient update followed by soft-thresholding operation that shrinks small parameters to zero
- Core assumption: ℓ₁ regularization is convex and separable
- Evidence anchors: Abstract states method handles non-smoothness from ℓ₁ regularization; section describes proximal SGD with residual compression
- Break condition: Excessive regularization parameter causes underfitting and poor learning performance

### Mechanism 2
- Claim: Consensus aggregation balances local updates with neighbor information for faster convergence
- Mechanism: Weighted average of local model with neighbor models, controlled by consensus step size
- Core assumption: Network topology is strongly connected with symmetric, doubly-stochastic mixing matrix
- Evidence anchors: Abstract mentions balance between consensus and local updates; section contrasts aggregation scheme with previous work
- Break condition: Incorrect consensus step size causes oscillation or slow convergence

### Mechanism 3
- Claim: Vector source coding exploits sparsity of quantized residuals to further reduce communication
- Mechanism: Encodes support vector of quantization levels efficiently due to sparse residuals from proximal operation
- Core assumption: Quantized residuals have sparse structure with non-zero values concentrated around limited modes
- Evidence anchors: Abstract mentions vector source coding from Woldemariam et al. (2023); section describes encoding frequencies and positions
- Break condition: Non-sparse residuals prevent significant compression gains

## Foundational Learning

- Concept: Proximal gradient methods for non-smooth optimization
  - Why needed here: ℓ₁ regularization makes objective non-smooth, requiring specialized optimization
  - Quick check question: How does soft-thresholding operation promote sparsity in model parameters?

- Concept: Decentralized optimization algorithms and consensus aggregation
  - Why needed here: Algorithm operates in decentralized manner with nodes exchanging information
  - Quick check question: What are key assumptions on network topology and mixing matrix for consensus convergence?

- Concept: Gradient compression techniques (quantization and source coding)
  - Why needed here: Frequent communication bottleneck in decentralized learning, especially for large models
  - Quick check question: How does quantization scheme satisfy bounded error assumption and impact convergence?

## Architecture Onboarding

- Component map: Local model update → Residual quantization → Source coding → Communication → Consensus aggregation → Proximal optimization
- Critical path: Local model update → Residual quantization → Source coding → Communication → Consensus aggregation → Proximal optimization
- Design tradeoffs:
  - Sparsity vs. learning performance: Higher regularization promotes sparsity but may harm performance
  - Compression ratio vs. communication cost: Higher compression reduces cost but increases quantization error
  - Consensus step size vs. convergence speed: Controls balance between consensus and local updates
- Failure signatures:
  - Slow convergence: Issues with consensus step size or quantization error
  - Poor learning performance: Excessive regularization or quantization error
  - Communication bottlenecks: Insufficient compression or poorly connected topology
- First 3 experiments:
  1. Evaluate impact of ℓ₁ regularization parameter on sparsity and learning performance
  2. Compare communication cost and learning performance with/without source coding
  3. Investigate effect of consensus step size on convergence speed and communication efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MALCOM-PSGD perform under highly dynamic network topologies with frequent topology changes?
- Basis in paper: Paper mentions extension to asynchronous and time-varying networks but lacks rigorous analysis
- Why unresolved: Current analysis focuses on static networks or limited asynchronous scenarios
- What evidence would resolve it: Rigorous convergence proofs for dynamic networks, extensive numerical experiments with varying network conditions

### Open Question 2
- Question: What is optimal quantization scheme balancing compression efficiency with minimal convergence impact?
- Basis in paper: Uses specific QSGD with dithering but acknowledges quantization error affects convergence rate
- Why unresolved: No systematic comparison of quantization methods or guidelines for parameter selection
- What evidence would resolve it: Comparative analysis of quantization methods, sensitivity analysis, theoretical bounds on trade-offs

### Open Question 3
- Question: How does MALCOM-PSGD scale with extremely large models and datasets?
- Basis in paper: Demonstrates effectiveness on MNIST and CIFAR10 but scaling behavior for massive applications unexplored
- Why unresolved: Current analysis doesn't address computational complexity, memory requirements, or communication bottlenecks for large-scale
- What evidence would resolve it: Experiments with large-scale models, analysis of computational and memory scaling, communication cost analysis for distributed systems

### Open Question 4
- Question: What is impact of heterogeneous data distributions across nodes on performance?
- Basis in paper: Mentions testing on heterogeneous data but lacks detailed analysis of effects on convergence and communication
- Why unresolved: Paper lacks theoretical analysis of data heterogeneity influence on convergence guarantees and communication patterns
- What evidence would resolve it: Theoretical bounds under data heterogeneity, extensive experiments with varying data skew, analysis of hyperparameter optimization

## Limitations

- Theoretical analysis relies on strong assumptions including bounded gradients and Lipschitz continuity
- Convergence rate of O(ln(t)/√t) is relatively slow compared to centralized methods
- Experimental validation limited to standard datasets (MNIST, CIFAR10) with moderate-scale models

## Confidence

- **High Confidence**: Proximal SGD with ℓ₁ regularization for model sparsification is well-established and theoretically sound
- **Medium Confidence**: Consensus aggregation scheme supported by theoretical analysis and experimental results, though optimal step size requires further investigation
- **Low Confidence**: Effectiveness of vector source coding depends on sparse residuals assumption which may not always hold in practice

## Next Checks

1. Conduct hyperparameter sensitivity analysis to determine impact of learning rate, consensus step size, and regularization parameter on performance
2. Evaluate generalizability to diverse datasets beyond MNIST and CIFAR10 with different characteristics and scales
3. Implement and test in real-world decentralized learning setting such as federated learning system with mobile devices