---
ver: rpa2
title: Neural Progressive Meshes
arxiv_id: '2308.05741'
source_url: https://arxiv.org/abs/2308.05741
tags:
- features
- mesh
- meshes
- progressive
- subdivision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Neural Progressive Meshes, a framework for
  learning a progressive compressed representation of 3D meshes for efficient transmission
  over the internet. The key insight is that geometric details of 3D meshes often
  exhibit similar local patterns, even across different shapes, and can be effectively
  represented with a shared learned generative space.
---

# Neural Progressive Meshes

## Quick Facts
- arXiv ID: 2308.05741
- Source URL: https://arxiv.org/abs/2308.05741
- Authors: 
- Reference count: 16
- Key outcome: Neural Progressive Meshes achieves state-of-the-art compression ratio and reconstruction quality for 3D meshes by learning shared geometric feature representations.

## Executive Summary
This paper introduces Neural Progressive Meshes, a framework for learning progressive compressed representations of 3D meshes. The method uses a subdivision-based encoder-decoder architecture trained on a large collection of surfaces, where the encoder maps geometric details to high-dimensional per-face features of decimated meshes, and the decoder reconstructs a high-resolution mesh using these features. A sparsity loss is introduced to enable progressive refinement by favoring irrelevant features to be zero. The method is evaluated on the Thingi10K dataset, demonstrating superior compression ratio and reconstruction quality compared to baselines like QSlim, Loop, Butterfly, SubdivFit, and Neural Subdivision.

## Method Summary
The method first remeshes input meshes using TetWild to ensure watertight, non-self-intersecting, near-delaunay triangulated meshes. Then, it creates a sequence of level-of-detail (LoD) meshes using QSlim decimation and successive self-parameterization. The encoder maps high-resolution mesh details to per-face features at each LoD using mesh convolutions and pooling operators adapted from SubdivNet. The decoder reconstructs the high-resolution mesh from the coarse mesh and transmitted features using learned subdivision. The network is trained end-to-end with reconstruction losses (vertex positions and Jacobians) and a sparsity loss. At inference, the coarsest mesh is transmitted first, followed by progressively sending per-face features sorted by magnitude to iteratively improve reconstruction quality.

## Key Results
- At compression ratio of 61.39, achieves point-to-mesh distance of 4.12 × 10^-4 and average normal error of 7.19 degrees
- Outperforms baseline methods (QSlim, Loop, Butterfly, SubdivFit, Neural Subdivision) in reconstruction accuracy
- Ablation study shows that all three levels of subdivision (L=3) yield the best performance
- Learning per-face features jointly across the entire dataset improves compression efficiency compared to per-mesh training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The encoder-decoder architecture with skip connections enables progressive transmission of geometric details.
- Mechanism: The encoder maps high-resolution mesh details to per-face features at each level of detail (LoD), which are then progressively transmitted to the decoder. Skip connections allow the decoder to reconstruct intermediate LoD meshes even before all features are received.
- Core assumption: Geometric details across different shapes share similar local patterns that can be effectively encoded in a learned feature space.
- Evidence anchors:
  - [abstract]: "Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space."
  - [section 3.1]: "We first obtain a sequence of LoD meshes... We define convolution and pooling operators based on this mapping following SubdivNet [Hu et al. 2022]."

### Mechanism 2
- Claim: Sparsity loss enables effective compression by setting irrelevant features to zero.
- Mechanism: The sparsity loss encourages per-face features to be zero when they don't contribute to reconstruction quality. This allows sorting features by magnitude and transmitting only the most important ones progressively.
- Core assumption: Many per-face features are redundant and can be set to zero without significantly impacting reconstruction quality.
- Evidence anchors:
  - [section 3.3]: "To avoid transmitting features that encode redundant information in regions whose geometry could be inferred by the decoder without any aid, we introduce a sparsity loss."
  - [section 3.3]: "After network training, we sort the features based on the magnitude and transmit them progressively from the encoder to the decoder."

### Mechanism 3
- Claim: Joint training with reconstruction and sparsity losses enables progressive improvement of mesh quality.
- Mechanism: The network is trained end-to-end using reconstruction losses (vertex position and Jacobian) and sparsity loss. This allows the decoder to progressively improve the reconstructed mesh as more features are transmitted.
- Core assumption: The network can learn to reconstruct plausible shapes even with partial feature information, and progressively improve quality as more features arrive.
- Evidence anchors:
  - [section 3.3]: "We train our network so that the decoder can still reconstruct a plausible shape via the learned subdivision process even before all features are transmitted."
  - [section 4.2]: "Our method yields the highest accuracy at the same level of compression since it learns surface-specific features across a collection of shapes and can adaptively transmit features only in regions that need the most details."

## Foundational Learning

- Concept: Mesh subdivision schemes (e.g., Loop, Butterfly)
  - Why needed here: Understanding how meshes are subdivided is crucial for implementing the encoder and decoder architectures that operate on LoD meshes.
  - Quick check question: How does the Loop subdivision scheme refine a triangular mesh?

- Concept: Neural networks for geometric deep learning (e.g., mesh convolutions)
  - Why needed here: The method uses mesh-based neural networks (adapted from SubdivNet) to encode and decode geometric details.
  - Quick check question: How do mesh convolutions differ from traditional image convolutions?

- Concept: Progressive transmission and compression
  - Why needed here: The core idea is to transmit coarse mesh first and progressively improve quality by sending additional features.
  - Quick check question: What are the key differences between progressive and non-progressive compression methods?

## Architecture Onboarding

- Component map:
  - Encoder: Maps [Vᵢ, fᵢ] → [Vᵢ₋₁, fᵢ₋₁] using mesh convolutions and pooling
  - Decoder: Maps [Vᵢ, fᵢ] → [Vᵢ₊₁, fᵢ₊₁] using vertex displacement prediction and feature learning
  - Loss functions: Reconstruction loss (vertex positions and Jacobians) and sparsity loss for compression
  - Preprocessing: Remeshing using QSlim decimation and successive self-parameterization

- Critical path:
  1. Remesh input mesh to create LoD sequence
  2. Encode geometric details into per-face features
  3. Transmit coarse mesh and progressively send features
  4. Decode to reconstruct and progressively improve mesh

- Design tradeoffs:
  - Compression ratio vs. reconstruction quality
  - Number of LoD levels (affects compression and quality)
  - Feature dimensionality (affects compression and expressiveness)
  - Sparsity loss weight (affects compression vs. quality)

- Failure signatures:
  - Poor reconstruction quality: Insufficient feature transmission or inadequate network capacity
  - Excessive compression: Too aggressive sparsity loss or insufficient LoD levels
  - Topology mismatch: Incorrect preprocessing or subdivision scheme implementation

- First 3 experiments:
  1. Implement basic encoder-decoder without sparsity loss on simple meshes
  2. Add sparsity loss and test progressive transmission on synthetic data
  3. Evaluate on Thingi10K dataset and compare with baseline methods

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, potential open questions include:
- How does the performance of Neural Progressive Meshes scale with different numbers of levels of detail (LoD) beyond the three levels used in the experiments?
- How does Neural Progressive Meshes perform on datasets with significantly different characteristics than Thingi10K, such as CAD models or organic shapes with very smooth surfaces?
- What is the impact of the remeshing step on the overall performance of Neural Progressive Meshes, and are there alternative remeshing approaches that could improve results?

## Limitations

- The method relies on watertight, non-self-intersecting meshes, which may not be readily available for all 3D models
- The performance of the method on datasets with significantly different characteristics than Thingi10K is not explored
- The impact of the remeshing step on the overall performance is not analyzed

## Confidence

- Core claim (shared learned feature spaces): High confidence
- Sparsity loss mechanism: Medium confidence
- Joint training approach: High confidence

## Next Checks

1. Verify feature magnitude ordering correlates with visual importance by conducting a user study on progressive transmission quality perception
2. Test robustness to mesh topology changes by evaluating on non-watertight meshes from the original Thingi10K dataset
3. Benchmark memory and computation requirements during progressive transmission compared to traditional compression methods