---
ver: rpa2
title: 'TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling
  Likewise'
arxiv_id: '2310.19019'
source_url: https://arxiv.org/abs/2310.19019
tags:
- arxiv
- language
- training
- teacherlm-7
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TeacherLM is a 7.1B parameter model that annotates NLP samples
  with fundamentals, chain-of-thought, and common mistakes, enabling other models
  to learn "why" not just "what". It achieves a zero-shot MMLU score of 52.3, surpassing
  most models over 100B parameters.
---

# TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise

## Quick Facts
- arXiv ID: 2310.19019
- Source URL: https://arxiv.org/abs/2310.19019
- Authors: Li, X., Wang, Y., Chen, Z., et al.
- Reference count: 16
- One-line primary result: TeacherLM-7.1B achieves 52.3 zero-shot MMLU score, surpassing most 100B+ parameter models

## Executive Summary
TeacherLM is a 7.1B parameter language model that annotates NLP samples with fundamentals, chain-of-thought, and common mistakes, enabling student models to learn "why" not just "what". By training on 2M detailed explanations, TeacherLM achieves strong zero-shot performance and can augment 58 NLP datasets with rich rationales. The augmented data consistently improves student model performance across multiple benchmarks, with gains up to 5% on MMLU and up to 13% on single-task fine-tuning.

## Method Summary
TeacherLM employs a multi-stage training procedure: multi-task training on diverse datasets, personalization to adapt to explanation generation, and specialization for chain-of-thought reasoning. The model generates three types of explanations (fundamentals, chain-of-thought, common mistakes) for each sample, which are then used to augment existing NLP datasets. Student models of various sizes (1B to 7.1B parameters) are trained on the augmented data in a multi-task setting, followed by single-task fine-tuning.

## Key Results
- TeacherLM-7.1B achieves zero-shot MMLU score of 52.3, surpassing most models over 100B parameters
- Data augmentation consistently improves student model performance across different model sizes
- Performance gains of up to 5% on MMLU and up to 13% on single-task fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TeacherLM improves student model performance by providing detailed rationales rather than just answers
- Mechanism: Training on 2M detailed explanations enables TeacherLM to generate comprehensive annotations that help student models understand reasoning
- Core assumption: Student models can effectively learn from rationales generated by TeacherLM
- Evidence anchors: Zero-shot MMLU score of 52.3; significant improvements across different model sizes
- Break condition: If rationales are inaccurate or irrelevant, student models may learn incorrect patterns

### Mechanism 2
- Claim: Multi-stage training (multi-task → personalization → specialization) enables effective rationale generation
- Mechanism: Progressive training from general learning to specialized explanation generation develops strong zero-shot capabilities
- Core assumption: Staged training is more effective than direct training on all datasets
- Evidence anchors: Direct blending scores much lower than multi-stage training; only CoT data models show continuous improvement
- Break condition: If multi-stage training doesn't lead to better rationale generation

### Mechanism 3
- Claim: TeacherLM's data augmentation significantly improves student model performance
- Mechanism: Augmenting 58 datasets with detailed rationales provides richer training signals
- Core assumption: Augmented data provides more effective learning signals than standard labels
- Evidence anchors: Consistent benefits across different model sizes; up to 13% gains on single-task fine-tuning
- Break condition: If augmented data doesn't consistently improve performance

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: TeacherLM achieves high zero-shot performance on MMLU, demonstrating generalization to unseen tasks
  - Quick check question: What is the zero-shot MMLU score achieved by TeacherLM-7.1B?

- Concept: Chain of thought prompting
  - Why needed here: TeacherLM uses chain of thought prompting to generate step-by-step reasoning for each sample
  - Quick check question: What are the three types of explanations that TeacherLM generates for each sample?

- Concept: Data augmentation
  - Why needed here: TeacherLM augments existing NLP datasets with detailed rationales to provide richer training signals
  - Quick check question: How many NLP datasets were augmented by TeacherLM-7.1B in this work?

## Architecture Onboarding

- Component map: TeacherLM consists of base BLOOM models (560M to 176B parameters) trained in multi-stage procedure on P3-Sense-3K, Muffin-3W, and TeacherData-2M. Generates fundamentals, chain of thought, and common mistakes to augment existing NLP datasets.

- Critical path: 1) Construct TeacherData-2M with detailed explanations, 2) Train TeacherLM in multi-stage procedure, 3) Use TeacherLM to augment existing NLP datasets, 4) Train student models on augmented datasets, 5) Evaluate student model performance.

- Design tradeoffs: Larger TeacherLM models achieve better performance but require more computational resources. Multi-stage training is more effective but also more time-consuming than direct training.

- Failure signatures: Student models don't show significant improvement on augmented datasets; TeacherLM fails to generate accurate or relevant explanations.

- First 3 experiments:
  1. Train a small TeacherLM model (e.g., 560M parameters) on a subset of TeacherData-2M and evaluate its zero-shot performance on a held-out task.
  2. Use the trained TeacherLM model to augment a single NLP dataset and train a student model on the augmented data, comparing performance to a model trained on the original data.
  3. Conduct ablation studies to determine which type of explanation (fundamentals, chain of thought, or common mistakes) is most effective for improving student model performance on specific tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of fundamentals, chain of thought, and common mistakes affect student model performance differently across various NLP tasks and domains?
- Basis in paper: The paper mentions that CoT, fundamentals, and common mistakes form a complementary relationship
- Why unresolved: The paper does not provide a detailed analysis of how each component individually impacts performance across different task types or domains
- What evidence would resolve it: Conducting ablation studies where each component is individually added or removed from the training data

### Open Question 2
- Question: What is the optimal balance between data augmentation quality and quantity when using TeacherLM for improving student model performance?
- Basis in paper: The paper mentions that correctness of augmented content is not the only important factor
- Why unresolved: The paper does not explore the trade-off between quality and quantity of augmented data
- What evidence would resolve it: Experimenting with varying levels of data augmentation quality and quantity

### Open Question 3
- Question: How does TeacherLM's performance compare to other data augmentation methods in terms of cost-effectiveness and performance gains?
- Basis in paper: The paper mentions that retrieval-based pre-trained language models and LLMs can be used for instance-level data augmentation
- Why unresolved: The paper does not provide a direct comparison between TeacherLM and other data augmentation methods
- What evidence would resolve it: Conducting experiments comparing TeacherLM to other data augmentation methods on the same datasets

## Limitations

- Dataset Generalization Gap: Improvements may not generalize to domains outside P3-Sense-3K corpus or tasks requiring specialized domain knowledge
- Attribution Challenge: Performance gains cannot be clearly attributed to rationale quality vs. training methodology vs. combination
- Resource Requirements: Computational costs for multi-stage training and rationale generation are not clearly specified

## Confidence

- High Confidence: Zero-shot MMLU score of 52.3 for TeacherLM-7.1B is well-supported by experimental results
- Medium Confidence: Consistent improvements across different model sizes (1B to 7.1B parameters) are supported by Figure 3
- Low Confidence: Surpassing most models over 100B parameters requires careful interpretation as zero-shot comparison

## Next Checks

1. **Ablation Study on Explanation Types**: Conduct controlled experiments removing each type of explanation (fundamentals, chain-of-thought, common mistakes) individually to quantify their specific contributions to student model performance gains.

2. **Cross-Domain Transfer Test**: Evaluate TeacherLM's rationale generation quality and student model performance on datasets from domains not represented in TeacherData-2M (e.g., biomedical or legal domains) to assess generalization beyond the training distribution.

3. **Computational Cost Analysis**: Measure and report the wall-clock time and compute resources required for: (a) multi-stage training of TeacherLM at different scales, and (b) generating rationales for the entire P3-Sense-3K corpus, providing a complete cost-benefit picture for adoption.