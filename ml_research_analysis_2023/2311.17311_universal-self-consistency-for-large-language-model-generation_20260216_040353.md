---
ver: rpa2
title: Universal Self-Consistency for Large Language Model Generation
arxiv_id: '2311.17311'
source_url: https://arxiv.org/abs/2311.17311
tags:
- self-consistency
- arxiv
- generation
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Universal Self-Consistency (USC), a method
  that extends self-consistency prompting to free-form text generation tasks by using
  the LLM itself to select the most consistent response from multiple candidates.
  The key innovation is that USC eliminates the need for answer extraction and aggregation
  rules required by standard self-consistency, making it applicable to open-ended
  tasks like summarization and question answering.
---

# Universal Self-Consistency for Large Language Model Generation

## Quick Facts
- arXiv ID: 2311.17311
- Source URL: https://arxiv.org/abs/2311.17311
- Reference count: 29
- Key outcome: USC achieves 90.2% accuracy on GSM8K (vs 85.7% for greedy decoding) and 67.7% on TruthfulQA's GPT-judge metric (vs 62.1% for greedy)

## Executive Summary
This paper introduces Universal Self-Consistency (USC), a method that extends self-consistency prompting to free-form text generation tasks by using the LLM itself to select the most consistent response from multiple candidates. The key innovation is that USC eliminates the need for answer extraction and aggregation rules required by standard self-consistency, making it applicable to open-ended tasks like summarization and question answering. Experiments show that USC matches or exceeds performance of standard self-consistency on mathematical reasoning and code generation tasks, while also improving performance on summarization and truthful question answering where standard self-consistency cannot be applied.

## Method Summary
USC generates multiple candidate responses using sampling with temperature, concatenates them into a single prompt, and asks the LLM to select the most consistent response. This approach leverages the LLM's ability to evaluate internal coherence between responses rather than relying on exact string matching or predefined answer extraction rules. The method requires only 8 initial samples per task and uses the same LLM for both generation and evaluation, eliminating the need for separate ranking models or complex aggregation heuristics.

## Key Results
- USC achieves 90.2% accuracy on GSM8K mathematical reasoning (vs 85.7% for greedy decoding)
- USC achieves 67.7% on TruthfulQA's GPT-judge metric for truthful question answering (vs 62.1% for greedy)
- USC matches or exceeds standard self-consistency performance on mathematical reasoning and code generation while being applicable to summarization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: USC leverages LLM's own ability to measure consistency among multiple responses, replacing explicit answer extraction.
- Mechanism: The LLM is prompted to select the most consistent response from a set of candidates by evaluating their mutual coherence, rather than relying on exact string matching.
- Core assumption: The LLM can effectively assess internal consistency between different generated responses, even when formats differ.
- Evidence anchors:
  - [abstract] "USC eliminates the need of designing an answer extraction process, and is applicable to tasks with free-form answers."
  - [section 3] "Although prior works show that LLMs sometimes have trouble evaluating the prediction correctness (Huang 2023b; Gou 2023), empirically we observe that LLMs are generally able to examine the response consistency across multiple tasks."

### Mechanism 2
- Claim: USC generalizes self-consistency to tasks where answer formats are diverse or non-numeric.
- Mechanism: By instructing the LLM to choose the most consistent response, USC sidesteps the need for a voting rule based on exact answer matches, enabling application to summarization and open-ended QA.
- Core assumption: Consistency among candidate responses correlates with correctness or quality, even without exact matches.
- Evidence anchors:
  - [abstract] "USC matches or exceeds performance of standard self-consistency on mathematical reasoning and code generation tasks, while also improving performance on summarization and truthful question answering where standard self-consistency cannot be applied."
  - [section 3] "Figure 2b shows an example question where the final answer is an entity list. Despite that there is no response that is consistent with others based on the exact match, the LLM selects the response where each of the predicted entities appears most frequently among the candidate outputs."

### Mechanism 3
- Claim: USC improves robustness to response ordering compared to prior LLM-based evaluators.
- Mechanism: By presenting all candidate responses together in a single prompt, USC reduces the positional bias that affects sequential evaluation.
- Core assumption: Evaluating all responses collectively mitigates individual response order effects.
- Evidence anchors:
  - [section 4.3] "From Table 5, we observe that the overall model performance remains similar with different response orders, suggesting the effect of response order is minimal."
  - [section 4.4] "The voting ties constitute a notable portion to the selection differences between USC and SC, especially with 8 candidate responses. Specifically, among all responses with the maximum votes, SC always selects the one with the smallest index, while USC can pick up alternative ones based on the response format."

## Foundational Learning

- Concept: Self-consistency with chain-of-thought prompting
  - Why needed here: USC builds directly on the idea that multiple reasoning paths can be sampled and aggregated to improve answer quality.
  - Quick check question: What is the main limitation of standard self-consistency that USC addresses?

- Concept: Chain-of-thought prompting
  - Why needed here: USC assumes that initial samples are generated using CoT or similar reasoning strategies to produce coherent candidate responses.
  - Quick check question: Why does USC rely on sampling multiple responses rather than a single greedy decode?

- Concept: LLM evaluation and ranking
  - Why needed here: USC uses the LLM itself as a judge to select the best response, requiring understanding of how LLMs can evaluate consistency.
  - Quick check question: What is the difference between USC's consistency criterion and typical correctness evaluation?

## Architecture Onboarding

- Component map: Initial response generator -> Response concatenation -> Consistency evaluator -> Final response selector
- Critical path: Sample → Concatenate → Prompt → Select → Return
- Design tradeoffs:
  - Sampling more responses improves performance but increases cost and prompt length.
  - Using the same LLM for evaluation may introduce bias but eliminates need for separate ranker.
  - USC avoids answer extraction but requires longer prompts than standard self-consistency.
- Failure signatures:
  - Performance plateaus or degrades with too many samples (context limits, counting errors).
  - Selection becomes random when candidate responses are highly inconsistent.
  - Ordering effects reappear if prompt structure inadvertently highlights certain responses.
- First 3 experiments:
  1. Compare USC vs random selection on a simple summarization task to verify consistency-based improvement.
  2. Test USC with varying numbers of samples (3, 5, 8, 16) on a reasoning benchmark to find optimal sample count.
  3. Measure position bias by shuffling response order in USC prompts and checking if selection changes significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does USC perform on code generation tasks where execution results are not available?
- Basis in paper: [explicit] The paper shows USC matches execution-based voting on code generation tasks but notes it doesn't require execution results.
- Why unresolved: The paper only evaluates USC on code generation tasks where execution results are available, leaving open how well it performs when execution is not possible.
- What evidence would resolve it: Experiments comparing USC to baselines on code generation benchmarks without execution results.

### Open Question 2
- Question: How does USC's performance change when using different numbers of candidate responses?
- Basis in paper: [explicit] The paper shows USC benefits from more samples on some tasks but not others, and performance sometimes decreases with more samples.
- Why unresolved: The paper only tests 8 and 16 samples; the optimal number of samples for different tasks is unknown.
- What evidence would resolve it: Experiments varying the number of samples from 2 to 32 across different tasks to find optimal sample sizes.

### Open Question 3
- Question: Can USC be extended to provide confidence estimates for its selections?
- Basis in paper: [inferred] The paper notes that standard self-consistency provides confidence through voting counts, but USC lacks confidence estimation.
- Why unresolved: The paper doesn't explore methods for estimating confidence in USC selections.
- What evidence would resolve it: Development and evaluation of confidence estimation mechanisms for USC, such as pairwise consistency scoring or clustering-based confidence measures.

### Open Question 4
- Question: How does USC compare to oracle selection across different tasks and domains?
- Basis in paper: [explicit] The paper shows there remains a notable gap between USC and oracle performance across all evaluated tasks.
- Why unresolved: The paper doesn't analyze which types of tasks or domains have larger gaps to oracle performance.
- What evidence would resolve it: Detailed analysis of USC vs oracle performance across different task types (reasoning, generation, summarization) and domains (math, code, QA).

## Limitations
- Performance degrades with too many samples due to context length limitations and LLM counting errors
- USC may fail when candidate responses are highly divergent or when consistency doesn't correlate with correctness
- The method assumes the LLM can effectively evaluate consistency, which may not hold for all task types or domains

## Confidence
- **Medium** for claims about USC's generalization across diverse tasks - experimental results show consistent improvements but don't fully address edge cases
- **Low** for scalability claims beyond 8 samples - paper observes degradation but doesn't systematically explore upper bounds
- **High** for the core technical contribution of eliminating answer extraction rules - directly demonstrated through ablation studies

## Next Checks
- Test USC on tasks where candidate responses are intentionally designed to be mutually consistent but collectively incorrect to validate semantic correctness capture
- Evaluate USC using different LLMs as the consistency evaluator to determine cross-model generalization
- Conduct human evaluation of consistency judgments to verify LLM's judgments align with human reasoning about answer quality