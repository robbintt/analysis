---
ver: rpa2
title: 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across
  Paragraphs'
arxiv_id: '2309.08873'
source_url: https://arxiv.org/abs/2309.08873
tags:
- paragraph
- inferable
- information
- language
- spans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces X-PARADE, a dataset for cross-lingual divergence
  detection at the paragraph level. Annotators label spans in target paragraphs as
  same, new, or inferable given a source paragraph in another language.
---

# X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs

## Quick Facts
- arXiv ID: 2309.08873
- Source URL: https://arxiv.org/abs/2309.08873
- Reference count: 28
- Key outcome: Introduces X-PARADE dataset for cross-lingual divergence detection; GPT-4 outperforms other methods but still lags human performance

## Executive Summary
This paper introduces X-PARADE, a dataset for cross-lingual divergence detection at the paragraph level. Annotators label spans in target paragraphs as same, new, or inferable given a source paragraph in another language. The dataset covers English-Spanish and English-Hindi pairs. Experiments with alignment, NLI-based methods, and LLMs show that GPT-4 performs best but still lags human performance.

## Method Summary
The paper presents a novel task of cross-lingual divergence detection where annotators label spans in target paragraphs as same, new, or inferable from source paragraphs in another language. The dataset is constructed from Wikipedia articles in English, Spanish, and Hindi, with paragraph alignments created using machine translation and similarity measures. Multiple detection methods are evaluated including token alignment, NLI attribution, SLR-NLI, and LLM prompting with GPT-4 and other models.

## Key Results
- GPT-4 outperforms other methods by combining multilingual capabilities with instruction following
- NLI attribution methods can identify new information by finding tokens that contribute to neutral/contradiction classifications
- Machine translation alignment identifies same information by finding near-perfect translations between paragraphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms other methods by combining multilingual capabilities with instruction following to identify inferable spans.
- Mechanism: When prompted with examples of inferable spans, GPT-4 leverages its training on diverse multilingual data to recognize patterns where target information can be logically inferred from the source paragraph. The model uses its parametric knowledge to fill gaps between explicitly stated information and implicit conclusions.
- Core assumption: GPT-4 has been exposed to sufficient examples of cross-lingual inference during pretraining to recognize common inference patterns.
- Evidence anchors:
  - [abstract] "Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance."
  - [section] "However, it is able to follow the task format and achieves strong performance on same and new tokens, as suggested by our results in Section 5.1."
  - [corpus] Weak - no direct corpus evidence of GPT-4's inference capabilities
- Break condition: If the inference requires domain-specific knowledge beyond what was commonly available in the training data, or if the inference pattern is too rare or complex for the model to have encountered during pretraining.

### Mechanism 2
- Claim: NLI attribution methods can identify new information by finding tokens that most contribute to neutral/contradiction classifications.
- Mechanism: When an NLI model classifies a hypothesis as neutral or contradiction, the token attribution method identifies which tokens in the hypothesis are most responsible for this decision. These tokens are likely to contain information not present in the premise.
- Core assumption: Tokens that cause NLI models to classify hypotheses as neutral or contradiction are indeed new information not present in the premise.
- Evidence anchors:
  - [abstract] "We compare a diverse set of techniques that solve different aspects of the problem, including token attribution of NLI models"
  - [section] "We used a BERT-based NLI model trained on MNLI... for our attribution method, we use integrated gradients"
  - [corpus] Weak - the confusion matrix shows alignment and NLI attribution often disagree on inferable tokens
- Break condition: If the NLI model's reasoning differs significantly from human reasoning, or if the attribution method incorrectly identifies tokens that are actually inferable from the premise.

### Mechanism 3
- Claim: Machine translation alignment identifies same information by finding near-perfect translations between paragraphs.
- Mechanism: MT aligners like SimAlign compute cosine similarities between mBERT embeddings of tokens in source and target paragraphs. Tokens with high similarity scores are considered aligned and therefore contain the same information.
- Core assumption: Near-perfect translations indicate identical information content, and tokens that don't align are new information.
- Evidence anchors:
  - [abstract] "we investigate a diverse set of approaches for this problem, including classic token alignment from machine translation"
  - [section] "We use SimAlign (Jalili Sa..."Alignment was surprisingly effective, performing similarly to SLR-NLI for ES-EN."
  - [corpus] Weak - the results show alignment performs well but doesn't distinguish inferable from new information
- Break condition: If the translation contains paraphrases or if cultural/linguistic differences cause semantically equivalent information to have different surface forms that don't align well.

## Foundational Learning

- Concept: Span-level semantic divergence detection
  - Why needed here: The task requires identifying fine-grained differences between paragraphs at the token/phrase level rather than sentence-level classifications
  - Quick check question: What is the difference between labeling an entire sentence as "new" versus labeling specific spans within a sentence as "new" or "inferable"?

- Concept: Cross-lingual inference patterns
  - Why needed here: Understanding how information can be inferred across languages requires recognizing patterns like "who brought" implying "was instrumental in bringing"
  - Quick check question: Given the Spanish phrase "quien hizo llegar el ferrocarril" and the English "who was instrumental in bringing the terminus", which label (same, new, inferable) would you assign to "who was instrumental"?

- Concept: Natural language inference (NLI) taxonomy
  - Why needed here: The task builds on NLI concepts but extends them to cross-lingual settings and requires distinguishing between same, new, and inferable information
  - Quick check question: How does the inferable category in this task differ from the neutral category in traditional NLI?

## Architecture Onboarding

- Component map: Wikipedia paragraph extraction -> paragraph alignment -> annotation interface -> adjudication
- Critical path: 1. Extract and align paragraphs from Wikipedia 2. Annotate with span-level labels (same, new, inferable) 3. Implement and evaluate multiple detection methods 4. Compare method performance against human annotations
- Design tradeoffs:
  - Granularity vs. subjectivity: Span-level annotation provides fine-grained information but increases annotator disagreement
  - Multilingual vs. monolingual models: Cross-lingual models could be more accurate but are less developed than monolingual alternatives
  - Automation vs. quality: Fully automated methods are faster but may miss subtle inferences that human annotators catch
- Failure signatures:
  - Low recall on inferable spans indicates methods struggle with indirect inferences
  - High precision but low recall suggests methods are conservative and miss many true positives
  - Poor performance on non-English pairs indicates issues with cross-lingual capabilities
- First 3 experiments:
  1. Compare alignment-based method against simple majority baseline on the dev set to establish a performance floor
  2. Evaluate NLI attribution method with different attribution techniques (integrated gradients, attention weights) to find optimal configuration
  3. Test few-shot prompting with GPT-4 on a small subset of examples to determine if instruction-following improves over zero-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language-specific grammatical features and morphological differences impact the difficulty of detecting inferable information in cross-lingual NLI tasks?
- Basis in paper: [explicit] The paper discusses cross-lingual NLI and mentions different languages have different syntactic constraints (Keenan, 1978) and carve up the world differently (de Saussure, 1983; Liu et al., 2023)
- Why unresolved: The paper primarily focuses on Spanish-English and English-Hindi pairs, leaving open questions about how typological differences affect the task across broader language families
- What evidence would resolve it: Systematic evaluation across multiple language pairs representing different language families, analyzing performance differences correlated with specific typological features

### Open Question 2
- Question: What is the optimal strategy for handling highly subjective inferable annotations when combining multiple annotator judgments?
- Basis in paper: [explicit] The paper discusses significant disagreement on inferable spans (F1 scores of only 17.4% for EN-ES) and their adjudication strategy of always labeling as inferable when any annotator does
- Why unresolved: The paper acknowledges subjectivity but doesn't fully explore alternative adjudication strategies or their impact on dataset quality
- What evidence would resolve it: Comparative analysis of different adjudication strategies (majority vote, confidence-weighted voting, etc.) against downstream task performance

### Open Question 3
- Question: How can NLI models be improved to better distinguish between inferable and same information in cross-lingual settings?
- Basis in paper: [inferred] The paper shows that NLI attribution methods distribute inferable tokens similarly to new tokens rather than same tokens, suggesting current models struggle with this distinction
- Why unresolved: The paper uses existing NLI models without exploring architectural modifications specifically designed for cross-lingual inference detection
- What evidence would resolve it: Experiments with cross-lingual NLI models incorporating explicit inference tracking mechanisms or reasoning modules

## Limitations

- The dataset annotation process shows several concerning patterns that limit generalizability
- The inter-annotator agreement on inferable spans is notably low, suggesting this category may be inherently subjective or poorly defined
- The evaluation relies heavily on F1 scores that combine same and inferable categories against new, which may mask performance differences on the more challenging inferable class

## Confidence

**High Confidence (90%+):** The LLM prompting approach with GPT-4 outperforms alignment and NLI-based methods on this dataset.

**Medium Confidence (70-89%):** The mechanism that GPT-4 succeeds by combining multilingual capabilities with instruction following is plausible but not definitively proven.

**Low Confidence (Below 70%):** The NLI attribution method reliably identifies new information by finding tokens contributing to neutral/contradiction classifications.

## Next Checks

1. Reproduce the inferable span annotations using the provided annotation interface and guidelines on a small subset of examples. Calculate inter-annotator agreement to verify the claimed subjectivity of this category and determine if the definition needs refinement.

2. Conduct an ablation study on GPT-4 prompts by systematically removing multilingual examples versus instruction-following examples from the few-shot prompts. This would isolate whether GPT-4's superior performance stems from its multilingual pretraining or its ability to follow structured instructions.

3. Test the NLI attribution method on synthetic examples where the relationship between premise and hypothesis is clearly defined (same, neutral, contradiction). This would validate whether the attribution method correctly identifies new information tokens and help diagnose the disagreement issues observed in the confusion matrices.