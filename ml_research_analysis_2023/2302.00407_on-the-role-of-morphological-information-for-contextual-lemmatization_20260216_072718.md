---
ver: rpa2
title: On the Role of Morphological Information for Contextual Lemmatization
arxiv_id: '2302.00407'
source_url: https://arxiv.org/abs/2302.00407
tags:
- morphological
- lemmatization
- languages
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the role of morphological information in training\
  \ contextual lemmatizers. The authors investigate six languages of varying morphological\
  \ complexity, comparing models that use different levels of morphological annotation\u2014\
  from UPOS tags alone to full morphological features."
---

# On the Role of Morphological Information for Contextual Lemmatization

## Quick Facts
- arXiv ID: 2302.00407
- Source URL: https://arxiv.org/abs/2302.00407
- Reference count: 18
- Key outcome: Fine-grained morphological features do not consistently improve lemmatization accuracy, even for highly inflected languages.

## Executive Summary
This paper investigates whether explicit morphological information improves contextual lemmatization across six languages of varying morphological complexity. The authors compare models trained with different levels of morphological annotation, from UPOS tags alone to full morphological features, and evaluate both in-domain and out-of-domain performance. Results show that fine-grained morphological features do not consistently improve lemmatization accuracy, even for highly inflected languages like Basque and Turkish. Modern contextual representations like XLM-RoBERTa implicitly capture morphological information, enabling strong performance without explicit features. The study also highlights that current evaluation metrics may overestimate model performance, suggesting the need for alternative evaluation methods.

## Method Summary
The study evaluates lemmatization performance using three approaches: statistical lemmatizers (IXA pipes) with varying morphological tag sets, neural lemmatizer Morpheus using morphological tags, and Transformer-based models (Flair, mBERT, XLM-RoBERTa, and language-specific BERTs) trained as sequence tagging without morphology. The models are trained on UniMorph datasets for six languages and evaluated on both in-domain and out-of-domain data. Performance is measured using word accuracy and sentence accuracy, with analysis of shortest edit scripts (SES) to understand model behavior.

## Key Results
- Fine-grained morphological features do not consistently improve lemmatization accuracy across languages
- Models trained with only UPOS tags or no morphology perform best out-of-domain
- Modern contextual representations implicitly capture morphological information, enabling strong performance without explicit features
- Current evaluation metrics like word accuracy may overestimate model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained morphological features do not consistently improve lemmatization accuracy, even for highly inflected languages.
- Mechanism: The model's performance does not improve when provided with detailed morphological features beyond basic UPOS tags.
- Core assumption: Implicit morphological information captured by contextual word representations is sufficient for lemmatization.
- Evidence anchors:
  - [abstract] "Results show that fine-grained morphological features do not consistently improve lemmatization accuracy, even for highly inflected languages like Basque and Turkish."
  - [section] "Fine-grained morphological features do not help to substantially improve contextual lemmatization, not even for high-infllected languages; using UPOS tags is enough for comparable performance."
- Break condition: If the implicit morphological information in contextual representations is insufficient for a particular language or domain.

### Mechanism 2
- Claim: Modern contextual word representations implicitly encode enough morphological information to obtain competitive lemmatizers without explicit morphological features.
- Mechanism: Contextual word representations capture morphological information implicitly, allowing the model to perform lemmatization effectively without explicit morphological features.
- Core assumption: The contextual word representations are trained on sufficient data to capture the necessary morphological information.
- Evidence anchors:
  - [abstract] "Modern contextual representations like XLM-RoBERTa implicitly capture morphological information, enabling strong performance without explicit features."
  - [section] "contextual word representations such as those employed in Transformer and Flair models seem to encode enough implicit morphological information to allow us to train good performing lemmatizers without any explicit morphological signal."
- Break condition: If the training data does not contain sufficient morphological variation for the contextual word representations to learn effectively.

### Mechanism 3
- Claim: Models trained with simple UPOS tags or no morphology perform best out-of-domain.
- Mechanism: The simplicity of the input features allows the model to generalize better to unseen domains, reducing the impact of domain-specific morphological variations.
- Core assumption: Out-of-domain data distributions are sufficiently different that complex morphological features learned in-domain do not transfer well.
- Evidence anchors:
  - [abstract] "Models trained with only UPOS tags or no morphology at all perform best out-of-domain."
  - [section] "the best lemmatizers out-of-domain are those using either simple UPOS tags or no morphology at all."
- Break condition: If the out-of-domain data has significantly different morphological characteristics that require explicit feature information.

## Foundational Learning

- Concept: Contextual word representations
  - Why needed here: Understanding how these representations capture morphological information is crucial to explaining why explicit features are not always necessary.
  - Quick check question: How do contextual word representations like BERT capture morphological information without explicit feature annotations?

- Concept: Sequence tagging
  - Why needed here: Lemmatization is treated as a sequence tagging task, so understanding this framework is essential for implementing the models.
  - Quick check question: What is the difference between sequence tagging and other sequence-to-sequence approaches in the context of lemmatization?

- Concept: Out-of-domain evaluation
  - Why needed here: The study emphasizes the importance of evaluating models on data from different domains to assess their robustness.
  - Quick check question: Why is out-of-domain evaluation important for assessing the real-world applicability of lemmatization models?

## Architecture Onboarding

- Component map: Word forms encoded as contextual vectors -> Linear layer on pre-trained language models -> Predicted SES for each word
- Critical path:
  1. Load pre-trained language model (e.g., XLM-RoBERTa)
  2. Fine-tune on lemmatization dataset with SES as target labels
  3. Evaluate on in-domain and out-of-domain test sets
- Design tradeoffs:
  - Using pre-trained models vs. training from scratch
  - Fine-tuning vs. feature extraction
  - Complexity of morphological features vs. model generalization
- Failure signatures:
  - Poor performance on agglutinative languages
  - Significant degradation in out-of-domain settings
  - Inability to handle rare or unseen word forms
- First 3 experiments:
  1. Fine-tune XLM-RoBERTa on lemmatization dataset without explicit morphological features and evaluate in-domain performance.
  2. Evaluate the same model on an out-of-domain dataset to assess generalization.
  3. Compare the performance of models trained with different levels of morphological annotation (UPOS only vs. full features).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific morphological features consistently improve lemmatization for particular languages?
- Basis in paper: [explicit] The authors note that while fine-grained morphological features like Case, Number, and Gender can slightly benefit statistical lemmatizers for agglutinative languages, no single morphological tag consistently performs best across systems and languages.
- Why unresolved: The study showed variability in which morphological features are beneficial depending on the language and system, but did not isolate which specific features (e.g., Case vs. Number) are most consistently useful.
- What evidence would resolve it: Systematic ablation studies isolating the impact of individual morphological features across a broader set of languages and systems would clarify which features are most consistently beneficial.

### Open Question 2
- Question: How do different evaluation metrics (e.g., sentence accuracy vs. word accuracy) affect the perceived performance of lemmatizers across morphological complexity?
- Basis in paper: [explicit] The authors highlight that current evaluation practices, like word accuracy, may overestimate model performance and suggest alternative metrics like sentence accuracy for better discrimination between models.
- Why unresolved: While the paper introduces sentence accuracy as an alternative, it does not extensively compare how different metrics affect performance evaluation across languages of varying morphological complexity.
- What evidence would resolve it: Comparative studies using multiple evaluation metrics (e.g., sentence accuracy, per-SES accuracy) across languages with different morphological complexities would reveal how evaluation choices impact perceived model performance.

### Open Question 3
- Question: What specific morphological information is implicitly encoded in contextual word representations that aids lemmatization?
- Basis in paper: [inferred] The authors suggest that modern contextual word representations implicitly encode morphological information, enabling strong lemmatization performance without explicit morphological features.
- Why unresolved: The paper does not detail which specific morphological information (e.g., Case, Number, Gender) is implicitly captured by contextual representations.
- What evidence would resolve it: Probing studies that analyze the morphological information encoded in contextual representations and correlate it with lemmatization performance would clarify this.

## Limitations
- Study focuses exclusively on Universal Dependencies and UniMorph annotated corpora, which may not capture all morphological phenomena
- Evaluation only considers word and sentence accuracy metrics, potentially missing nuanced errors
- Analysis does not investigate whether certain morphological features might be more critical than others for specific languages
- Out-of-domain evaluation covers only two domains (ParCor and GUM), limiting assessment of cross-domain robustness

## Confidence

**High Confidence**: The finding that UPOS tags alone are sufficient for competitive lemmatization performance is strongly supported by consistent results across all six languages and both in-domain and out-of-domain settings. The claim that contextual representations implicitly capture morphological information is well-established by the strong performance of XLM-RoBERTa and other Transformer models.

**Medium Confidence**: The assertion that morphological features do not improve performance even for highly inflected languages is supported by the data but may depend on specific feature sets and annotation schemes. The claim about out-of-domain performance being best with simple features is robust but based on limited domain coverage.

**Low Confidence**: The recommendation for alternative evaluation metrics beyond word accuracy is reasonable but not substantiated with concrete alternatives or evidence that current metrics are systematically flawed.

## Next Checks

1. **Feature Ablation Study**: Systematically evaluate the impact of individual morphological features (e.g., case, number, gender) rather than feature bundles to determine if certain features are more valuable than others for specific languages.

2. **Extended Domain Coverage**: Evaluate model performance across 5-10 additional domains and text genres to better assess the robustness of the finding that simple features generalize better out-of-domain.

3. **Error Analysis with Alternative Metrics**: Conduct detailed error analysis using metrics beyond word accuracy, such as morphological feature preservation rates or semantic preservation, to validate whether current evaluation methods adequately capture lemmatization quality.