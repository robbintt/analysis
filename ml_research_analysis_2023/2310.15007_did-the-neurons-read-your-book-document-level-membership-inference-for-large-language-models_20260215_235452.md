---
ver: rpa2
title: Did the Neurons Read your Book? Document-level Membership Inference for Large
  Language Models
arxiv_id: '2310.15007'
source_url: https://arxiv.org/abs/2310.15007
tags:
- language
- membership
- arxiv
- books
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of document-level membership inference
  for large language models (LLMs), aiming to determine whether a given document was
  used in the model's training data. The authors propose a practical, black-box methodology
  that involves querying the model for token-level predictions, normalizing these
  predictions, aggregating them to the document level, and using a meta-classifier
  to predict membership.
---

# Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models

## Quick Facts
- **arXiv ID:** 2310.15007
- **Source URL:** https://arxiv.org/abs/2310.15007
- **Reference count:** 40
- **Primary result:** Achieved AUC scores of 0.856 for books and 0.678 for academic papers in document-level membership inference

## Executive Summary
This paper introduces a practical approach to document-level membership inference for large language models, addressing the challenge of determining whether specific documents were used in model training. The authors propose a black-box methodology that queries models for token-level predictions, normalizes these predictions using token frequency and maximum probability, aggregates them into histogram features at the document level, and uses a meta-classifier to predict membership. Evaluated on OpenLLaMA-7B using Project Gutenberg books and ArXiv papers, the approach achieves impressive AUC scores, demonstrating that accurate document-level membership can be inferred even for documents representing tiny fractions of the training corpus.

## Method Summary
The methodology involves querying the language model for token-level predictions across entire documents, then normalizing these predictions by token frequency (RTF) and maximum predicted probability (maxNorm). The normalized values are aggregated into histogram features with 1000 bins, capturing the distribution of prediction confidences across tokens. A random forest meta-classifier is then trained on these histogram features to distinguish between documents seen during training and those not seen. The approach is evaluated using books from Project Gutenberg (published before 1919 as members, after February 10, 2019 as non-members) and academic papers from ArXiv, with performance measured using AUC scores.

## Key Results
- Achieved AUC of 0.856 for membership inference on Project Gutenberg books
- Achieved AUC of 0.678 for membership inference on ArXiv academic papers
- Histogram-based feature extraction (HistFE) significantly outperformed simple aggregate statistics (AggFE)
- MaxNormTF normalization proved essential for effective membership inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level membership inference exploits differences in token prediction confidence distributions between documents seen during training and those not seen.
- Mechanism: When a model has seen a document during training, its predicted probability distributions for tokens in that document will be more confident (higher probabilities for correct tokens, lower for others) compared to documents not seen during training. The auditor normalizes these probabilities by token frequency and maximum predicted probability, aggregates them into histogram features, and uses these features to train a meta-classifier that learns the membership boundary.
- Core assumption: The distribution of normalized prediction confidences contains sufficient signal to distinguish training vs. non-training documents, even when the document represents a tiny fraction of the training corpus.
- Evidence anchors:
  - [abstract] "normalizing these predictions, aggregating them to the document level, and using a meta-classifier to predict membership"
  - [section] "models tend to make more confident predictions on data samples that were seen during training than on data samples that were not"
  - [corpus] Found related papers on membership inference and document-level attacks, but no direct replication of this specific methodology
- Break condition: If token frequency distributions are too similar between training and non-training documents, or if the model has perfectly generalized without memorization.

### Mechanism 2
- Claim: Normalization by token frequency and maximum predicted probability removes dataset-specific biases while preserving membership-specific signals.
- Mechanism: Raw prediction probabilities are biased by how common tokens are in general language. By normalizing with token frequency (RTF) and maximum probability (maxNorm), the method creates a signal that reflects how well the model fits specific token sequences rather than just how common those tokens are. This normalized signal is then aggregated into histograms that capture the distribution shape rather than just aggregate statistics.
- Core assumption: The combination of token frequency normalization and maximum probability normalization creates features that are both discriminative and robust to general language patterns.
- Evidence anchors:
  - [abstract] "normalizing the predictions for how common the token is"
  - [section] "normalizing by both how common a token is and by the maximum probability returned by the model, is essential"
  - [corpus] Weak evidence - related papers discuss normalization but not this specific combination
- Break condition: If the normalization removes too much signal or if the maximum probability normalization creates artifacts that confuse the classifier.

### Mechanism 3
- Claim: Histogram-based feature aggregation captures distributional patterns in token-level prediction confidence that are lost in simple aggregate statistics.
- Mechanism: Instead of using only mean, variance, and percentiles of normalized prediction values, the method creates histograms with 1000 bins that capture the full distribution shape. This allows the meta-classifier to learn complex patterns in how prediction confidences are distributed across tokens within a document, which may reflect memorization patterns.
- Core assumption: The shape of the distribution of normalized prediction confidences contains more information about membership than simple aggregate statistics.
- Evidence anchors:
  - [abstract] "the signal for document-level membership lies in the detailed distribution of the token-level information within a document"
  - [section] "we find that the histogram feature extraction HistFE is significantly more effective than the more simple aggregate statistics extraction AggFE"
  - [corpus] No direct evidence in corpus, but histogram methods are common in machine learning
- Break condition: If the distribution shape is too similar between training and non-training documents, or if the classifier overfits to noise in the histogram bins.

## Foundational Learning

- Concept: Membership Inference Attacks
  - Why needed here: Understanding the fundamental privacy attack concept is crucial for grasping how document-level membership inference extends sentence-level attacks
  - Quick check question: What is the key difference between membership inference at the sentence level versus document level?

- Concept: Language Model Token Prediction
  - Why needed here: The method relies on querying language models for token-level predictions, so understanding how these predictions work is essential
  - Quick check question: How does context length affect the quality of token predictions in language models?

- Concept: Feature Engineering and Histogram Methods
  - Why needed here: The method's effectiveness depends on choosing appropriate features (histograms vs. aggregates) to capture membership signals
  - Quick check question: Why might histogram features capture more information than simple aggregate statistics?

## Architecture Onboarding

- Component map:
  - Document preprocessing and tokenization -> Language model query interface -> Token frequency normalization dictionary computation -> Maximum probability normalization computation -> Histogram feature extraction -> Meta-classifier (Random Forest) -> Evaluation pipeline (AUC, TPR@FPR)

- Critical path: Document → LM queries → Normalization → Histogram features → Meta-classifier → Membership prediction

- Design tradeoffs:
  - Context length vs. computational cost: Longer contexts give better predictions but require more queries
  - Number of histogram bins vs. overfitting: More bins capture more detail but risk overfitting
  - Normalization method choice: Different normalizations capture different aspects of the signal

- Failure signatures:
  - AUC close to 0.5 indicates no signal detected
  - High variance across folds suggests instability in the approach
  - Performance drops significantly when using aggregate features instead of histograms

- First 3 experiments:
  1. Implement NoNorm + AggFE baseline to verify it performs near random
  2. Test MaxNormTF + HistFE with different context lengths (128, 1024, 2048) to find optimal setting
  3. Compare performance between books and academic papers to understand dataset-specific effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of document-level membership inference attacks vary across different types of training data sources (e.g., web-scraped data vs. curated datasets)?
- Basis in paper: [explicit] The authors note that books from Project Gutenberg often represent widely-spread literature across the internet, which may lead to duplication in the training data, while academic papers in LaTeX are rarely distributed widely.
- Why unresolved: The paper only compares membership inference performance between books and academic papers, but does not explore how different data sources might impact the effectiveness of the attacks.
- What evidence would resolve it: Comparative studies of membership inference performance across various training data sources (e.g., Common Crawl, C4, Books3, ArXiv, Project Gutenberg) using the same methodology.

### Open Question 2
- Question: Can the meta-classifier trained on document-level features from one model size (e.g., 7B parameters) be effectively applied to predict membership for models of different sizes (e.g., 3B or 13B parameters)?
- Basis in paper: [explicit] The authors evaluate membership inference performance across different model sizes and find that the AUC remains consistent, suggesting that the memorization is similar across model sizes.
- Why unresolved: The paper does not explore whether a single meta-classifier trained on one model size can be used to predict membership for models of different sizes.
- What evidence would resolve it: Experiments training a meta-classifier on document-level features from one model size and evaluating its performance on models of different sizes.

### Open Question 3
- Question: How does the performance of document-level membership inference attacks change when the training dataset contains a significant amount of duplicate or overlapping data?
- Basis in paper: [explicit] The authors hypothesize that books in Project Gutenberg often represent widely-spread literature across the internet, which may lead to duplication in the training data.
- Why unresolved: The paper does not explicitly study the impact of data duplication on the effectiveness of membership inference attacks.
- What evidence would resolve it: Experiments varying the amount of duplicate or overlapping data in the training dataset and measuring the impact on membership inference performance.

### Open Question 4
- Question: Can the meta-classifier be improved by incorporating additional document-level features beyond the token-level predicted probabilities?
- Basis in paper: [explicit] The authors use a relatively simple methodology for extracting document-level features, considering normalization and histogram-based aggregation strategies.
- Why unresolved: The paper does not explore the potential benefits of incorporating additional document-level features, such as document length, writing style, or topic.
- What evidence would resolve it: Experiments incorporating additional document-level features and evaluating their impact on the performance of the meta-classifier.

## Limitations
- Evaluation is constrained to a single model (OpenLLaMA-7B) and relatively small document sets (21 books total)
- Methodology assumes black-box access with multiple queries allowed, which may not reflect all real-world deployment scenarios
- Document preprocessing and normalization dictionary construction details are underspecified, creating potential reproducibility gaps

## Confidence
- **High confidence:** The general methodology of using normalized token prediction distributions as membership features is sound and well-supported by the literature on membership inference.
- **Medium confidence:** The specific choice of MaxNormTF normalization combined with histogram features (HistFE) as optimal is supported by ablation studies but could benefit from testing on additional models and datasets.
- **Low confidence:** The claim that this approach scales to arbitrary documents or larger models without significant modifications requires further validation.

## Next Checks
1. **Cross-model validation:** Test the methodology on at least two additional LLM architectures (e.g., Llama, Mistral) to assess whether the MaxNormTF + HistFE combination remains optimal across different model families and training regimes.

2. **Dataset size scaling:** Evaluate performance as a function of document set size (e.g., 100, 1000, 10000 documents) to understand how well the approach scales and whether performance degrades as the proportion of training data per document decreases.

3. **Contextual analysis:** Systematically vary the context window size (C) and analyze how AUC scores change, particularly examining whether the performance gap between books and academic papers narrows or widens with different context lengths.