---
ver: rpa2
title: Generating Medical Prescriptions with Conditional Transformer
arxiv_id: '2310.19727'
source_url: https://arxiv.org/abs/2310.19727
tags:
- data
- synthetic
- clinical
- medical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task-specific model, Label-To-Text-Transformer
  (LT3), to generate synthetic medical prescriptions using provided labels. LT3 is
  trained on around 2K lines of medication prescriptions from the MIMIC-III database
  and outperforms T5, a state-of-the-art pre-trained language model, in generating
  high-quality and diverse synthetic prescriptions.
---

# Generating Medical Prescriptions with Conditional Transformer

## Quick Facts
- arXiv ID: 2310.19727
- Source URL: https://arxiv.org/abs/2310.19727
- Reference count: 39
- Key outcome: LT3 generates synthetic medical prescriptions that enable 96-98% F1 score on NER tasks, outperforming T5 baseline

## Executive Summary
This paper introduces Label-To-Text-Transformer (LT3), a task-specific transformer model for generating synthetic medical prescriptions from drug labels. Trained on approximately 2,000 medication prescriptions from MIMIC-III, LT3 implements a novel Beam Search Decoding with Backtracking (B2SD) algorithm to produce diverse, high-quality synthetic prescriptions. The generated data successfully trains a SpacyNER model to achieve 96-98% F1 score on Drug, Frequency, Route, Strength, and Form labels, demonstrating that synthetic prescriptions can effectively substitute for real data in medical NLP applications.

## Method Summary
The method trains a transformer-based architecture (LT3) on MIMIC-III medication prescriptions using a BERT word-piece tokenizer with custom embeddings trained from scratch. A novel Beam Search Decoding with Backtracking (B2SD) algorithm generates diverse prescription sequences conditioned on drug labels. The synthetic prescriptions are then used to train a SpacyNER model for Named Entity Recognition tasks, evaluated on the n2c2-2018 dataset. The approach is compared against T5 (small, base, large) models fine-tuned on the same data.

## Key Results
- LT3 generates synthetic prescriptions achieving BLEU, ROUGE, and BERTScore metrics comparable to or exceeding T5 baseline
- Synthetic data trains SpacyNER to 96-98% F1 score on Drug, Frequency, Route, Strength, and Form labels
- B2SD decoding produces more diverse prescriptions than standard beam search while maintaining quality
- The model demonstrates viability of synthetic data as alternative to real data for medical NLP research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LT3 can generate high-quality synthetic medical prescriptions that match or exceed the quality of real prescriptions.
- **Mechanism**: The model uses a task-specific architecture with custom embeddings trained from scratch and a novel beam search decoding with backtracking (B2SD) to generate diverse, coherent prescription texts conditioned on drug labels.
- **Core assumption**: The MIMIC-III dataset contains sufficient representative patterns of real prescription writing to allow the model to learn the structure and language of prescriptions without needing pre-trained embeddings.
- **Evidence anchors**:
  - [abstract]: "LT3 is trained on around 2K lines of medication prescriptions from the MIMIC-III database and outperforms T5...in generating high-quality and diverse synthetic prescriptions."
  - [section]: "LT3 implements the pre-trained word-piece BERT tokeniser...Embedding layers are used within the model's architecture and are trained from scratch to precisely cater to the requirements of the medical prescription writing task."
  - [corpus]: Weak. Corpus does not directly discuss the mechanism of generating high-quality prescriptions, only related tasks.
- **Break condition**: If the training data lacks sufficient diversity or if the backtracking algorithm fails to prune poor sequences, the generated prescriptions may become repetitive or contextually incoherent.

### Mechanism 2
- **Claim**: The B2SD decoding algorithm improves over standard greedy beam search by avoiding local optima and generating more diverse prescriptions.
- **Mechanism**: At each decoding step, B2SD maintains the top-m sequences so far and backtracks to explore alternative branches, guided by a joint probability heuristic with length normalization. This allows it to find globally better sequences than greedy beam search.
- **Core assumption**: The joint probability heuristic combined with length normalization and backtracking can effectively guide the search toward high-quality, diverse outputs.
- **Evidence anchors**:
  - [section]: "This study puts forth a novel task-specific model architecture, the Label-To-Text-Transformer (LT3), crafted to generate synthetic medical instructions...We introduce a transformer-based architecture, LT3 with both an encoder and a decoder."
  - [section]: "LT3 implements a novel Beam Search Decoding method using Backtracking ( B2SD)...this method instead employs a backtracking strategy."
  - [corpus]: Missing. No corpus entries discuss beam search decoding or backtracking strategies.
- **Break condition**: If the heuristic function does not distinguish between high- and low-quality sequences, or if the beam size and candidate space are not tuned properly, B2SD may not outperform standard methods.

### Mechanism 3
- **Claim**: Synthetic data generated by LT3 can effectively train NER models to achieve performance comparable to models trained on real data.
- **Mechanism**: The synthetic prescriptions preserve the label structure and context of real prescriptions, enabling the SpacyNER model to learn entity recognition patterns without access to sensitive real data.
- **Core assumption**: The synthetic prescriptions are diverse and realistic enough that the NER model can generalize to unseen real prescriptions.
- **Evidence anchors**:
  - [abstract]: "The generated synthetic data is then used to train the SpacyNER model for Named Entity Recognition (NER) tasks, achieving a 96-98% F1 score on Drug, Frequency, Route, Strength, and Form labels."
  - [section]: "We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form."
  - [corpus]: Weak. No corpus entries discuss NER performance on synthetic vs. real data.
- **Break condition**: If the synthetic data lacks sufficient diversity or contains systematic biases, the NER model may overfit to synthetic patterns and fail on real prescriptions.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanisms
  - **Why needed here**: LT3 is built on the Transformer architecture, so understanding self-attention, encoder-decoder structure, and token embeddings is essential to modify or debug the model.
  - **Quick check question**: What is the role of the multi-head attention mechanism in the Transformer encoder?
- **Concept**: Beam search and decoding strategies
  - **Why needed here**: B2SD is a custom decoding algorithm; understanding standard beam search, heuristics, and backtracking is necessary to tune or extend it.
  - **Quick check question**: How does backtracking in beam search differ from standard greedy beam search in terms of search space exploration?
- **Concept**: Named Entity Recognition (NER) and evaluation metrics
  - **Why needed here**: The downstream task is NER; knowing how F1 score, precision, recall, and label types (Drug, Route, etc.) are computed is key to interpreting results.
  - **Quick check question**: What is the difference between micro-averaged and macro-averaged F1 score in NER evaluation?

## Architecture Onboarding

- **Component map**: Drug label → BERT word-piece tokenizer → Embedding layer (trained from scratch) → Transformer encoder → Transformer decoder → Output sequence (prescription text)
- **Critical path**: Label → Encoder → Decoder → Prescription generation (via B2SD)
- **Design tradeoffs**:
  - Using embeddings trained from scratch vs. pre-trained BioBERT: better task fit but requires more training data.
  - Custom B2SD vs. standard beam search: potentially better diversity and quality at higher computational cost.
  - Small dataset (2K lines) vs. large PLM: task-specific focus but limited generalization.
- **Failure signatures**:
  - Repetitive or generic prescriptions → B2SD or diversity mechanisms not working.
  - Low NER F1 scores → synthetic data not capturing label patterns.
  - Training instability → embedding layer or tokeniser mismatch.
- **First 3 experiments**:
  1. Train LT3 on MIMIC-III subset with default beam search; evaluate BLEU vs. T5.
  2. Replace default beam search with B2SD; compare diversity (Jaccard) and quality metrics.
  3. Generate synthetic data, train SpacyNER, and evaluate F1 on n2c2-2018 test set.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, key unresolved areas include:
- How does LT3 compare to medical-specific models like MedGPT or BioBERT-based models?
- What are the potential biases or limitations of the synthetic data generated by LT3?
- How do the beam size (n), maximum candidates space (m), and maximal probability difference in beam (pb) hyperparameters affect the quality and diversity of generated prescriptions?

## Limitations
- Limited evaluation scope: only tested on NER task with n2c2-2018 dataset, not validated across multiple medical NLP tasks
- Small training dataset (2K lines) may not capture full diversity of prescription writing patterns across different clinical contexts
- B2SD decoding lacks ablation studies comparing against standard beam search with different beam sizes or other decoding strategies

## Confidence
- **High confidence**: The synthetic data can train a functional NER model achieving 96-98% F1 score on the n2c2-2018 dataset.
- **Medium confidence**: LT3 outperforms T5 in generating high-quality synthetic prescriptions based on BLEU/ROUGE/BERTScore metrics.
- **Low confidence**: The claim that LT3 provides a viable alternative to real data for medical NLP research requires broader validation across multiple tasks and clinical safety implications.

## Next Checks
1. **Human evaluation study**: Recruit clinical experts to rate 100 randomly sampled synthetic prescriptions for coherence, clinical plausibility, and diversity compared to real prescriptions. Calculate inter-rater agreement and identify systematic failure modes.
2. **Cross-task generalization test**: Apply the synthetic data to train models for additional medical NLP tasks (e.g., relation extraction, medication event detection) on standard benchmarks to assess whether the data quality transfers beyond NER.
3. **Bias and coverage analysis**: Analyze the synthetic prescription corpus for demographic representation, drug class coverage, and potential biases introduced during generation. Compare the distribution of entities (drug classes, frequencies, routes) against the original MIMIC-III distribution to identify generation gaps.