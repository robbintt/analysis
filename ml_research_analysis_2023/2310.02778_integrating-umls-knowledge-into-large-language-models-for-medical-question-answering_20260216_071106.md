---
ver: rpa2
title: Integrating UMLS Knowledge into Large Language Models for Medical Question
  Answering
arxiv_id: '2310.02778'
source_url: https://arxiv.org/abs/2310.02778
tags:
- medical
- umls
- chatgpt-3
- extraction
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for enhancing the factuality of
  large language models (LLMs) in medical question answering by integrating knowledge
  from the Unified Medical Language System (UMLS). The proposed method extracts medical
  terminologies from questions, maps them to UMLS concepts, and retrieves relevant
  definitions and relations to augment LLM prompts.
---

# Integrating UMLS Knowledge into Large Language Models for Medical Question Answering

## Quick Facts
- arXiv ID: 2310.02778
- Source URL: https://arxiv.org/abs/2310.02778
- Reference count: 22
- The proposed method extracts medical terminologies from questions, maps them to UMLS concepts, and retrieves relevant definitions and relations to augment LLM prompts, significantly improving factuality and completeness in medical question answering.

## Executive Summary
This paper presents a framework that integrates Unified Medical Language System (UMLS) knowledge into large language models (LLMs) to enhance the factuality and completeness of medical question answering. The method extracts medical terminologies from user questions, maps them to UMLS concepts, retrieves relevant definitions and relations, and augments LLM prompts with this structured medical knowledge. Evaluated on the LiveQA dataset using both automatic metrics and physician assessments across four dimensions (factuality, completeness, readability, and relevancy), the UMLS-augmented LLM demonstrates significant improvements in factuality and completeness while maintaining comparable performance in other dimensions compared to baseline LLM outputs.

## Method Summary
The framework extracts medical terminologies from questions using both direct and indirect approaches, maps these terms to UMLS concepts via CUI mapping, retrieves definitions and relations for the top 25 relations per concept, and augments LLM prompts with this knowledge before generating answers. The evaluation uses the LiveQA dataset with physician assessment based on four dimensions: factuality, completeness, readability, and relevancy, supplemented by automatic metrics like ROUGE and BERTScore.

## Key Results
- UMLS-augmented LLM significantly improves factuality and completeness compared to baseline LLM
- Physician evaluation shows win-rate improvements across all four dimensions (factuality, completeness, readability, relevancy)
- The framework maintains comparable performance in readability and relevancy while enhancing factual accuracy

## Why This Works (Mechanism)

### Mechanism 1
Integrating structured medical knowledge from UMLS reduces factual hallucinations in LLM-generated medical content by constraining the model to generate responses grounded in verified medical knowledge. The framework extracts medical terminology from questions, maps these terms to UMLS concepts, and retrieves definitions and relations that are embedded into the LLM prompt.

### Mechanism 2
Physician evaluation based on four dimensions provides a more reliable quality signal than automated metrics alone. Multiple resident physicians conduct blind reviews using predefined criteria, producing a win-rate analysis comparing baseline LLM vs. UMLS-augmented LLM outputs.

### Mechanism 3
Direct and indirect medical terminology extraction methods yield different coverage and relevance trade-offs, influencing downstream factuality. Direct extraction pulls only terms present verbatim in the question, while indirect extraction infers related concepts, affecting which UMLS concepts are retrieved.

## Foundational Learning

- Concept: Unified Medical Language System (UMLS) Metathesaurus and CUI mapping
  - Why needed here: Understanding how medical terms map to CUIs is essential for retrieving accurate definitions and relations
  - Quick check question: Given the term "Diabetes Mellitus," what CUI does UMLS assign, and which source vocabularies contribute definitions?

- Concept: Named Entity Recognition (NER) in biomedical domain
  - Why needed here: Accurate extraction of medical entities from free-text questions determines the quality of downstream UMLS retrieval
  - Quick check question: If a Bio-Epidemiology-NER model identifies "heart failure" as a concept, does it also extract age or gender as medical entities?

- Concept: Prompt engineering with retrieved knowledge
  - Why needed here: The LLM must integrate UMLS facts into its generation process without breaking context or fluency
  - Quick check question: When appending definitions to the prompt, how do you ensure the model still addresses the original question instead of drifting into definitional recitation?

## Architecture Onboarding

- Component map: Input question -> Terminology extraction (Direct/Indirect + Bio-Epidemiology-NER) -> UMLS CUI mapping and relation retrieval -> LLM prompt augmentation -> Output answer
- Critical path: Terminology extraction → CUI mapping → UMLS relation retrieval → Prompt augmentation → LLM generation
- Design tradeoffs: Direct extraction ensures high precision but may miss implied terms; indirect extraction increases recall but risks noise. Limiting retrieved relations to top 25 balances memory usage and relevance, but may omit useful facts.
- Failure signatures: Factuality score drops if UMLS retrieval misses critical concepts; readability degrades if overly technical UMLS definitions overwhelm the prompt; latency increases if extraction or UMLS queries are slow or redundant.
- First 3 experiments: 1) Compare direct vs. indirect extraction on a small subset of LiveQA questions, measure factuality gain; 2) Vary the number of retrieved relations (10, 25, 50) and observe impact on factuality vs. latency; 3) A/B test with physicians on a 10-question subset to validate rubric reliability before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
How does the inclusion of irrelevant medical terminologies affect the performance of the LLM? The paper discusses that irrelevant terms can lead LLM to retrieve unbeneficial medical knowledge from UMLS, which cannot prompt LLM to answer questions accurately, but does not provide empirical evidence on this impact.

### Open Question 2
How can the framework be improved to extract the most relevant relations from UMLS? The paper mentions that the top 25 relations extracted from UMLS may not directly pertain to the specific question, and irrelevant information cannot aid the LLM in generating high-quality content, but does not propose a method to compute the most relevant relation information.

### Open Question 3
How does the performance of the framework change when evaluated on a dataset revised by medical experts? The paper acknowledges that the LiveQA dataset lacks a ground truth verified by medical experts and suggests using a dataset revised by medical experts for automated assessments in the future.

## Limitations
- Evaluation based on only 20 questions from LiveQA dataset, which may not capture full variability of medical queries
- Physician evaluation methodology relies on subjective judgments without clear inter-rater reliability metrics
- Does not explore computational overhead of UMLS integration or compare different LLM architectures' ability to incorporate external knowledge

## Confidence

- **High confidence**: The core claim that UMLS integration improves factuality and completeness is supported by physician evaluation results showing significant improvements in these dimensions.
- **Medium confidence**: The mechanism by which direct vs. indirect terminology extraction affects downstream performance is described but not empirically validated against each other.
- **Low confidence**: The generalizability of results to other medical QA datasets and real-world clinical scenarios remains uncertain due to limited evaluation scope.

## Next Checks

1. **Scale physician evaluation**: Expand physician assessment to the full LiveQA test set (104 questions) to verify that observed improvements hold across the complete dataset and calculate inter-rater reliability scores.
2. **Direct vs indirect extraction comparison**: Conduct a controlled experiment comparing factuality, completeness, readability, and relevancy scores when using only direct extraction versus only indirect extraction methods.
3. **Performance cost analysis**: Measure and report the additional latency introduced by UMLS integration and test whether performance degrades when limiting retrieved relations to fewer than 25 per concept.