---
ver: rpa2
title: A new algorithm for Subgroup Set Discovery based on Information Gain
arxiv_id: '2307.15089'
source_url: https://arxiv.org/abs/2307.15089
tags:
- patterns
- dataset
- fssd
- igsd
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new subgroup discovery (SD) algorithm, IGSD,
  that combines Information Gain (IG) and Odds Ratio (OR) for pattern selection. It
  aims to overcome limitations of existing SD algorithms like FSSD and SSD++ by allowing
  for variable fixing, using overlapping subgroups, and optimizing both pattern complexity
  and target dependence.
---

# A new algorithm for Subgroup Set Discovery based on Information Gain

## Quick Facts
- arXiv ID: 2307.15089
- Source URL: https://arxiv.org/abs/2307.15089
- Authors: 
- Reference count: 33
- Key outcome: IGSD algorithm combines Information Gain and Odds Ratio for pattern selection, allowing variable fixing and optimizing both pattern complexity and target dependence. Results show IGSD finds more patterns with higher Odds Ratio values compared to FSSD and SSD++.

## Executive Summary
This paper introduces IGSD, a novel Subgroup Discovery algorithm that addresses limitations of existing methods by incorporating Information Gain and Odds Ratio for pattern selection. The algorithm features dynamic threshold calculation, the ability to fix key attributes in patterns, and optimization of both pattern complexity and target dependence. Evaluated on 11 datasets, IGSD demonstrates improved pattern quality with higher Odds Ratio values compared to state-of-the-art algorithms FSSD and SSD++.

## Method Summary
IGSD is a Subgroup Discovery algorithm that combines Information Gain (IG) and Odds Ratio (OR) for pattern selection. It uses dynamic IG thresholds to adaptively filter selectors at each expansion step, allowing for variable fixing through the CondList parameter to ensure domain-critical attributes are present in all patterns. The algorithm iteratively expands patterns up to a maximum depth (dmax), applying IG filtering and optimizing each pattern via ORR-based cutting and p-value filtering. IGSD returns a subgroup set rather than individual patterns, addressing a limitation of previous algorithms.

## Key Results
- IGSD finds more patterns with higher Odds Ratio values compared to FSSD and SSD++ algorithms
- Dynamic IG threshold calculation allows adaptive filtering of selectors based on information content
- Expert validation on lung cancer dataset shows higher acceptance rates for IGSD patterns when key attributes are fixed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IGSD uses dynamic Information Gain thresholds to select only the most relevant selectors at each step, avoiding the fixed beam-width tuning needed in FSSD and SSD++.
- Mechanism: At each exploration step, IGSD computes a threshold based on the variance of IG scores across all available selectors. Selectors with IG below this threshold are excluded from further expansion, shrinking the search space adaptively.
- Core assumption: High-variance IG distributions reliably indicate which selectors carry useful information for subgroup discovery.
- Evidence anchors:
  - [abstract] "IGSD algorithm searches patterns through an optimization that combines information gained [10, 11] and odds ratio [12, 13] metrics"
  - [section] "the first task will be performed using the IG threshold to eliminate patterns and discover interesting associations"
  - [corpus] Weak/no explicit support for dynamic thresholding as described; must be inferred from algorithm description.
- Break condition: If the IG distribution is flat or noisy, the threshold may exclude potentially useful selectors, reducing pattern quality.

### Mechanism 2
- Claim: IGSD optimizes pattern complexity by iteratively pruning selectors with low IG and high p-values, ensuring only statistically significant and information-rich patterns remain.
- Mechanism: After initial pattern discovery, IGSD filters selectors by IG threshold and p-value (0.05 cutoff). Then it selects the "optimal cut" selector based on ORR ranking and removes subsequent selectors if ORR does not improve.
- Core assumption: Removing selectors after the optimal cut does not discard critical information needed for pattern interpretability.
- Evidence anchors:
  - [abstract] "pattern complexity is reduced by removing irrelevant information from patterns"
  - [section] "This step initiates by setting of list R for storing optimized patterns in Algorithm 1, line 11. As a result, on lines 11 and 14, IGSD stores the optimal cut in the R output list"
  - [corpus] No direct evidence in corpus papers about ORR-guided pruning; likely unique to this work.
- Break condition: If the optimal cut is chosen incorrectly, important context may be lost, reducing pattern utility.

### Mechanism 3
- Claim: By allowing fixed key attributes via the CondList parameter, IGSD ensures discovered patterns always contain domain-critical variables, increasing expert acceptance.
- Mechanism: CondList is passed to each expansion step; any pattern generated must include the specified attributes. This enforces clinical relevance at discovery time.
- Core assumption: Domain experts can reliably identify which attributes must appear in every pattern.
- Evidence anchors:
  - [abstract] "Additionally, some crucial dataset variables cannot be fixed to be present in the discovered patterns using previous algorithms. However, because experts in the field may require them to consider a pattern to be useful or interesting, patterns with fixed key variables are an important aspect."
  - [section] "Thus, in further studies, general validation is a goal to achieve, having ideally, larger datasets."
  - [corpus] No mention of attribute fixing in neighboring papers; appears to be a novel feature.
- Break condition: If CondList is mis-specified, patterns may be overly constrained, reducing coverage or missing important subgroups.

## Foundational Learning

- Concept: Subgroup Discovery (SD) basics (population, target, quality measures)
  - Why needed here: IGSD builds directly on SD framework; understanding definitions like subgroup, quality function, and measures (WRAcc, coverage, confidence) is essential to grasp algorithm behavior.
  - Quick check question: What distinguishes KDD-culture SD from SI-culture SD in terms of quality metrics?
- Concept: Information Gain (IG) and Entropy in decision trees
  - Why needed here: IGSD uses IG both to select relevant selectors and to prune patterns; knowing how IG quantifies reduction in entropy is key to understanding the threshold logic.
  - Quick check question: How is IG calculated for a binary target, and why is it useful for subgroup selection?
- Concept: Odds Ratio (OR) and ORR transformation
  - Why needed here: ORR is used as a secondary criterion to rank and cut patterns; understanding OR as an effect size and its transformation to Cohen's d scale is needed to interpret IGSD's pattern selection.
  - Quick check question: What does an ORR value of 4 indicate about pattern-target dependence?

## Architecture Onboarding

- Component map: Input -> IG threshold calculator -> Selector expansion engine -> Pattern optimization module -> Output
- Critical path:
  1. Compute initial IG threshold → filter level-1 selectors
  2. Iteratively expand patterns up to dmax, applying IG filter at each step
  3. Optimize each pattern via ORR-based cutting and p-value filtering
  4. Return final pattern set
- Design tradeoffs:
  - Fixed beam width (FSSD/SSD++) vs. dynamic IG threshold (IGSD): trade-off between predictable resource use and adaptive quality.
  - ORR-based pruning vs. WRAcc maximization: trade-off between pattern-target dependence vs. coverage/size.
  - CondList enforcement vs. free discovery: trade-off between domain relevance vs. discovery breadth.
- Failure signatures:
  - Too many selectors pass IG threshold → search space explosion
  - ORR plateau early → premature pattern cutting
  - CondList too restrictive → zero patterns returned
  - Dynamic threshold unstable → inconsistent results across runs
- First 3 experiments:
  1. Run IGSD on IRIS dataset with tmode='dynamic' and no CondList; verify pattern count ~9 and IG threshold behavior.
  2. Run IGSD with CondList containing a known key attribute; confirm all patterns include it.
  3. Compare IGSD-M vs IGSD-T on VOTE dataset; check difference in pattern count and ORR values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic threshold calculation in IGSD compare to other adaptive thresholding methods in terms of computational efficiency and pattern quality across diverse datasets?
- Basis in paper: [explicit] The paper describes the dynamic IG threshold calculation in Algorithm 1, lines 1, 5, and 6, and mentions it as a key feature of IGSD.
- Why unresolved: The paper does not compare the computational cost or effectiveness of this specific dynamic threshold method against other adaptive thresholding approaches in SD.
- What evidence would resolve it: Benchmark studies comparing IGSD's dynamic threshold with other adaptive thresholding methods across multiple datasets, measuring both computational time and pattern quality metrics.

### Open Question 2
- Question: What is the impact of including fixed key attributes (via Cond list) on the diversity and interpretability of discovered patterns, and how does this affect domain expert acceptance?
- Basis in paper: [explicit] The paper states that IGSD allows fixing key attributes to increase expert acceptance, and demonstrates this with the P4Lucat dataset using cancer stage and first treatment variables.
- Why unresolved: The paper only shows results for one dataset with fixed attributes; it does not explore the trade-offs between pattern diversity, interpretability, and expert acceptance when varying the number or selection of fixed attributes.
- What evidence would resolve it: Comparative studies across multiple datasets with varying numbers and types of fixed attributes, measuring pattern diversity metrics, interpretability scores, and expert acceptance rates.

### Open Question 3
- Question: How does IGSD's performance scale with extremely high-dimensional datasets (e.g., genomics data with thousands of features) compared to state-of-the-art SD algorithms?
- Basis in paper: [inferred] The paper mentions that IGSD faced time constraints with the GENBASE dataset, which has many columns, and did not complete in a reasonable timeframe.
- Why unresolved: The paper does not provide a systematic analysis of IGSD's scalability limitations or compare its performance to other algorithms on very high-dimensional data.
- What evidence would resolve it: Controlled experiments on datasets with increasing dimensionality, comparing IGSD's runtime, memory usage, and pattern quality against other SD algorithms to identify scalability bottlenecks.

## Limitations

- Dynamic IG threshold mechanism lacks extensive validation across diverse datasets
- Expert validation results are limited to a single domain (lung cancer treatment)
- Performance on extremely high-dimensional datasets is not systematically evaluated

## Confidence

- High confidence: IGSD's basic framework (combining IG and OR for pattern selection) is well-supported by established SD principles and standard metrics.
- Medium confidence: The algorithm's unique features (dynamic IG thresholds, CondList enforcement, ORR-based pruning) are described clearly but lack extensive empirical validation.
- Low confidence: Generalization of expert validation results and the robustness of the dynamic thresholding approach across varied domains.

## Next Checks

1. Test IGSD on datasets from multiple domains (e.g., biology, finance, social sciences) to assess the robustness of the dynamic IG threshold mechanism.
2. Compare IGSD's pattern quality and expert acceptance rates against FSSD and SSD++ on a standardized benchmark suite with known ground truth patterns.
3. Conduct ablation studies to isolate the impact of each novel feature (dynamic thresholding, CondList, ORR pruning) on overall algorithm performance.