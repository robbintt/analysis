---
ver: rpa2
title: Rethinking the Evaluation Protocol of Domain Generalization
arxiv_id: '2305.15253'
source_url: https://arxiv.org/abs/2305.15253
tags:
- test
- domain
- generalization
- data
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies potential test data information leakage
  in the current domain generalization evaluation protocol and proposes modifications
  to ensure a more accurate assessment of Out-of-Distribution (OOD) generalization
  ability. The authors highlight two key issues: the use of ImageNet pretrained weights
  and oracle model selection.'
---

# Rethinking the Evaluation Protocol of Domain Generalization

## Quick Facts
- **arXiv ID**: 2305.15253
- **Source URL**: https://arxiv.org/abs/2305.15253
- **Reference count**: 40
- **Primary result**: Identifies test data information leakage in DG evaluation protocol and proposes modifications

## Executive Summary
This paper identifies potential test data information leakage in the current domain generalization evaluation protocol, specifically through the use of ImageNet pretrained weights and oracle model selection. The authors demonstrate that linear probing can outperform fine-tuning in certain settings, suggesting pretrained features contribute more to test performance than true OOD generalization ability. They propose two key modifications: avoiding pretrained weights and evaluating on multiple test domains. The paper introduces a new leaderboard reflecting these changes, which significantly alters algorithm rankings and highlights limitations in the current evaluation framework's ability to promote algorithms with genuine OOD generalization capabilities.

## Method Summary
The authors propose a modified evaluation protocol for domain generalization that addresses two identified issues: information leakage from ImageNet pretrained weights and oracle model selection on single test domains. The new protocol requires training models from scratch without pretrained weights and evaluating performance across multiple test domains. They reimplement several domain generalization algorithms (including ERM, SWAD, and Fishr) under both the original and modified protocols on DomainNet and NICO++ datasets. The comparison reveals significant changes in algorithm rankings, with methods relying heavily on pretrained features showing reduced performance under the modified protocol.

## Key Results
- Linear probing performance approaching or exceeding fine-tuning indicates pretrained weights may drive test accuracy rather than true OOD generalization
- Oracle model selection on single test domains enables artificial performance inflation through hyperparameter tuning
- SOTA algorithms (SWAD, Fishr) that excel under current protocol perform worse when pretrained weights are removed
- Algorithm rankings change significantly under modified protocol, revealing pretraining's dominant role in current evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pretrained weights on ImageNet introduce test data information leakage, compromising OOD generalization evaluation.
- **Mechanism**: When test domains contain visual styles similar to ImageNet, linear probing performance approaches or exceeds fine-tuning, indicating pretrained features drive test accuracy rather than true OOD generalization.
- **Core assumption**: Test domains in current benchmarks contain visual patterns resembling ImageNet (e.g., real photos, natural scenes).
- **Evidence anchors**:
  - LP performs comparably with FT under many settings, under some of which simple LP even just outperforms FT
  - Greater utilization of pretrained weights leads to higher test domain accuracies
- **Break condition**: If test domains are constructed to be visually and semantically disjoint from ImageNet, the leakage would diminish and FT would consistently outperform LP.

### Mechanism 2
- **Claim**: Single-test-domain evaluation allows oracle model selection, creating artificial performance inflation.
- **Mechanism**: With only one test domain, hyperparameter search can be tuned to maximize performance on that specific domain, leading to overfitting to the test set distribution rather than true OOD generalization.
- **Core assumption**: Hyperparameter search space is narrow enough that tuning can effectively "memorize" test domain characteristics.
- **Evidence anchors**:
  - The current protocol allows models to select different hyperparameters for each test domain
  - Using oracle model selection to "fit" the distribution of multiple test domains is usually harder than "fit" the distribution of a single test domain
- **Break condition**: If hyperparameter search space is extremely broad or random, or if test domains are highly diverse, oracle selection advantage diminishes.

### Mechanism 3
- **Claim**: Ranking changes under modified protocol reveal pretraining's dominant role in current evaluations.
- **Mechanism**: Algorithms that excel under current protocol perform worse when pretraining is removed, while methods like ERM remain competitive, indicating that pretraining masks true OOD generalization capabilities.
- **Core assumption**: Current SOTA algorithms rely on pretrained feature reuse rather than learning domain-invariant representations.
- **Evidence anchors**:
  - SOTA Algorithms proposed in the past two years based on the current DomainBed protocol fail to perform well under the modified protocol
  - Improvement under the current DomainBed protocol could be attributed to better utilization of pretrained weights
- **Break condition**: If future algorithms are explicitly designed to learn from scratch with strong domain generalization inductive biases, pretraining dependency would decrease.

## Foundational Learning

- **Concept: Domain Generalization vs. Domain Adaptation**
  - Why needed here: The paper assumes readers distinguish between learning from multiple source domains to generalize to unseen domains (DG) versus adapting to a specific target domain (DA).
  - Quick check question: What is the key difference between DG's goal of "unseen test domains" and DA's goal of "specific target domain adaptation"?

- **Concept: Linear Probing vs. Fine-Tuning**
  - Why needed here: The experimental comparison between LP and FT is central to the argument about pretrained weight leakage.
  - Quick check question: In LP, which parameters are frozen and which are trained, and how does this affect reliance on pretrained features?

- **Concept: Oracle Model Selection**
  - Why needed here: Understanding how model selection can leak test information is crucial for evaluating the proposed protocol changes.
  - Quick check question: How does oracle model selection differ from standard validation-based selection in terms of test data exposure?

## Architecture Onboarding

- **Component map**: Dataset → train/val/test splits → Model (ResNet50 from scratch or pretrained) → Training loop (ERM or DG algorithm) → Evaluation (multiple test domains) → Hyperparameter search (10 trials)
- **Critical path**: 1) Load dataset with multiple domains 2) Split into training, validation, and test domains 3) Initialize model (scratch or pretrained) 4) Run hyperparameter search on training+val data 5) Select best hyperparameters using validation accuracy 6) Evaluate on test domain(s) 7) Aggregate results across multiple runs
- **Design tradeoffs**: Pretrained vs. from-scratch (faster convergence vs. purer evaluation), single vs. multiple test domains (computational cost vs. rigor), hyperparameter search breadth (compute vs. thoroughness)
- **Failure signatures**: LP outperforming FT consistently suggests pretrained weights are driving test performance, large variance in rankings between old and new leaderboards indicates pretraining's influence, oracle selection showing large gains over IID selection indicates test data leakage through model selection
- **First 3 experiments**: 1) Reproduce LP vs. FT comparison on DomainNet with ImageNet pretraining to confirm leakage 2) Train ERM from scratch on DomainNet and compare to pretrained version 3) Implement multiple test domain evaluation on NICO++ and measure oracle selection leakage reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of domain generalization algorithms change when evaluated on datasets with more diverse and numerous domains?
- Basis in paper: The paper recommends evaluating algorithms on multiple test domains to mitigate oracle model selection issues and suggests that larger datasets with more domains are needed for fair evaluation.
- Why unresolved: The paper only evaluates on DomainNet and NICO++, which have limited domain diversity compared to what might be available in the future.
- What evidence would resolve it: Experiments on larger, more diverse datasets with many domains showing consistent performance changes compared to current benchmarks.

### Open Question 2
- Question: What is the optimal balance between utilizing pretrained weights and training from scratch for domain generalization tasks?
- Basis in paper: The paper shows that linear probing (LP) can outperform fine-tuning (FT) in certain settings, suggesting pretrained weights may contribute more to test performance than the model's true OOD generalization ability.
- Why unresolved: The paper does not provide a definitive answer on when to use pretrained weights versus training from scratch, only that using them may lead to test data information leakage.
- What evidence would resolve it: Empirical studies comparing various degrees of utilizing pretrained weights (freezing different numbers of layers) across multiple datasets and tasks, showing consistent performance trends.

### Open Question 3
- Question: How can we develop a more robust model selection strategy for OOD generalization that doesn't rely on test data or IID validation?
- Basis in paper: The paper discusses the issues with oracle model selection and suggests using multiple test domains to mitigate this, but acknowledges that a better model selection strategy remains an open problem.
- Why unresolved: Current model selection methods either use test data (oracle) or rely on IID validation, which may not generalize well to OOD scenarios.
- What evidence would resolve it: Development and validation of a new model selection strategy that consistently outperforms both oracle selection and IID validation across multiple OOD generalization benchmarks without relying on test data.

## Limitations

- The exact magnitude of test data leakage through oracle selection is not quantified
- The modified protocol may substantially increase computational costs for evaluation
- The analysis assumes ImageNet pretraining is the primary source of leakage without considering other potential sources

## Confidence

- **High**: Pretrained weights can contribute to test performance through feature reuse
- **Medium**: Single-test-domain evaluation enables oracle selection that inflates performance
- **Medium**: Ranking changes under modified protocol reveal pretraining's dominant role
- **Low**: The exact magnitude of test data leakage through oracle selection

## Next Checks

1. **Quantify leakage**: Systematically measure performance differences between LP and FT across diverse test domains to establish a leakage baseline
2. **Protocol stress test**: Evaluate algorithm rankings when using different numbers of test domains (2, 3, 4) to identify the minimum number needed for reliable evaluation
3. **Cross-dataset validation**: Apply the modified protocol to additional datasets like WILDS or DomainNet-LT to verify findings generalize beyond the initial evaluation sets