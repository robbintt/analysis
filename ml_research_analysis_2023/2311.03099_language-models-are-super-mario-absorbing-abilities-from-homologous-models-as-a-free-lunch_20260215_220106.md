---
ver: rpa2
title: 'Language Models are Super Mario: Absorbing Abilities from Homologous Models
  as a Free Lunch'
arxiv_id: '2311.03099'
source_url: https://arxiv.org/abs/2311.03099
tags:
- parameters
- dare
- delta
- merging
- e-04
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple yet effective method called DARE
  (Drop And REscale) to reduce the redundancy of delta parameters in SFT LMs, allowing
  for the removal of 90% or even 99% delta parameters without sacrificing performance.
  DARE works by randomly dropping delta parameters based on a drop rate and then rescaling
  the remaining ones.
---

# Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

## Quick Facts
- **arXiv ID:** 2311.03099
- **Source URL:** https://arxiv.org/abs/2311.03099
- **Reference count:** 40
- **Primary result:** DARE method enables removal of 90-99% delta parameters without performance loss

## Executive Summary
This paper introduces DARE (Drop And REscale), a simple yet effective method for reducing redundancy in delta parameters of supervised fine-tuned language models. By randomly dropping delta parameters and rescaling the remaining ones, DARE can eliminate up to 99% of delta parameters with minimal impact on model performance. The method is particularly effective for large-scale models and facilitates model merging by reducing parameter interference. Experimental results demonstrate that DARE works across both encoder- and decoder-based LMs, enabling significant parameter efficiency gains.

## Method Summary
DARE operates by randomly masking delta parameters during fine-tuning with a specific drop rate p, then rescaling the remaining parameters by 1/(1-p) to preserve expected output distributions. This preprocessing step can be applied before model merging to reduce parameter interference. The method exploits the observation that delta parameters in SFT models exhibit extreme redundancy and low-rank structure, similar to LoRA. DARE is particularly effective for larger models, which can tolerate higher drop rates.

## Key Results
- Can remove 90-99% of delta parameters without significant performance loss
- Larger models tolerate higher drop rates than smaller models
- DARE reduces parameter interference when merging multiple task-specific models
- Effective across both encoder-based (BERT, RoBERTa) and decoder-based (WizardLM, WizardMath, WizardCoder) LMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DARE can remove 90-99% of delta parameters without significant performance loss.
- Mechanism: Random dropout of delta parameters followed by rescaling the remaining ones by 1/(1-p) preserves expected output distribution.
- Core assumption: Delta parameters in SFT models exhibit extreme redundancy and low-rank structure.
- Evidence anchors:
  - [abstract]: "we can eliminate up to 99% delta parameters with minimal impact on model performance"
  - [section]: "DARE first randomly drops the parameters based on a specific drop rate p...and then rescales the remaining ones by a factor of 1/(1 − p)"
  - [corpus]: "DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models" - strong related work confirms the mechanism
- Break condition: When delta parameter values are large (e.g., >0.005) due to continued pre-training.

### Mechanism 2
- Claim: Larger models can tolerate higher drop rates than smaller models.
- Mechanism: As model size increases, pre-trained models develop more redundant low-rank structures that can be pruned while maintaining capabilities.
- Core assumption: Scaling laws apply to delta parameter redundancy - larger models learn more redundant structures during SFT.
- Evidence anchors:
  - [section]: "the tolerance of drop rates increases with the sizes of LMs...LMs with larger sizes are able to work well with higher drop rates"
  - [section]: "This may be because LMs with larger sizes can show stronger abilities...and thus we hypothesize they learn a multitude of low-rank structures"
  - [corpus]: Weak - no direct corpus evidence, this is a novel hypothesis from the paper
- Break condition: When task complexity requires more general abilities with less parameter redundancy.

### Mechanism 3
- Claim: DARE facilitates model merging by reducing parameter interference.
- Mechanism: Sparsifying delta parameters before merging reduces conflicts between parameters from different task-specific models.
- Core assumption: Parameter interference is a major bottleneck in model merging, and reducing redundancy mitigates this issue.
- Evidence anchors:
  - [abstract]: "DARE is able to tackle the parameter interference issue when merging models and yield consistent improvements"
  - [section]: "we hypothesize that DARE is able to help address the interference of parameters when merging multiple models"
  - [corpus]: "DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling" - related work supports interference reduction
- Break condition: When merging models that aren't well-fine-tuned for their respective tasks.

## Foundational Learning

- Concept: Delta parameters represent the difference between fine-tuned and pre-trained model parameters.
  - Why needed here: Understanding delta parameters is fundamental to grasping how DARE works and why parameter redundancy exists.
  - Quick check question: What mathematical operation defines delta parameters in the context of SFT?

- Concept: Random dropout with rescaling preserves expected output distributions.
  - Why needed here: This is the core mathematical principle that makes DARE work - without understanding this, the effectiveness of parameter removal seems magical.
  - Quick check question: Why does rescaling by 1/(1-p) matter when randomly dropping parameters?

- Concept: Parameter-efficient fine-tuning (PEFT) and LoRA as related techniques.
  - Why needed here: DARE reveals that SFT models learn low-rank structures similar to LoRA, providing context for why extreme pruning works.
  - Quick check question: How does DARE's discovery about SFT redundancy relate to the design philosophy of LoRA?

## Architecture Onboarding

- Component map: Pre-trained backbone (θPRE) -> Fine-tuned model parameters (θSFT) -> Delta parameters (δ = θSFT - θPRE) -> DARE operation (random masking + rescaling) -> Model merging pipeline (DARE preprocessing + existing merging methods)
- Critical path: DARE preprocessing → parameter merging → inference
  - DARE must be applied before any merging operation
  - Rescaling is essential for maintaining performance
  - Drop rate selection requires empirical tuning per model
- Design tradeoffs:
  - Higher drop rates → more parameter efficiency but risk of performance degradation
  - Model size vs. tolerance: larger models can handle more aggressive pruning
  - Task complexity vs. redundancy: simpler tasks may have more redundant delta parameters
- Failure signatures:
  - Performance collapse when delta parameter values are large (>0.005)
  - Inconsistent results when merging poorly-fine-tuned models
  - Suboptimal performance when rescaling is omitted
- First 3 experiments:
  1. Apply DARE with increasing drop rates (0.1, 0.5, 0.9, 0.99) to a fine-tuned model and measure performance degradation
  2. Compare DARE vs. magnitude-based pruning on the same model to validate superiority
  3. Merge two task-specific models with and without DARE preprocessing to quantify interference reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DARE be extended to other forms of model updates beyond SFT, such as reinforcement learning or adversarial training?
- Basis in paper: [inferred] The paper focuses on SFT delta parameters and suggests DARE could be a promising technique for federated learning, which often involves diverse model updates.
- Why unresolved: The paper only evaluates DARE on SFT delta parameters and does not explore its applicability to other training paradigms or model updates.
- What evidence would resolve it: Experiments applying DARE to models updated via reinforcement learning, adversarial training, or other non-SFT methods, and evaluating the impact on performance and parameter redundancy.

### Open Question 2
- Question: Is there a theoretical explanation for why larger models can tolerate higher drop rates in DARE?
- Basis in paper: [explicit] The paper observes that larger models can tolerate higher drop rates but only hypothesizes this is due to stronger abilities and low-rank structures.
- Why unresolved: The paper provides an empirical observation without a rigorous theoretical justification for the relationship between model size and drop rate tolerance.
- What evidence would resolve it: A mathematical analysis or theoretical framework explaining the scaling relationship between model size and the maximum tolerable drop rate in DARE.

### Open Question 3
- Question: How does DARE compare to other parameter-efficient fine-tuning methods like LoRA or prefix tuning in terms of performance and efficiency?
- Basis in paper: [inferred] The paper discusses delta parameters and parameter redundancy, which are relevant to PEFT methods, but does not directly compare DARE to these techniques.
- Why unresolved: The paper focuses on SFT delta parameters and model merging, without benchmarking against other PEFT methods.
- What evidence would resolve it: Head-to-head comparisons of DARE with LoRA, prefix tuning, and other PEFT methods on the same tasks, measuring both performance and computational efficiency.

## Limitations
- DARE fails when delta parameters have large absolute values (>0.005), limiting applicability to continuously pre-trained models
- Claims about scaling laws for parameter redundancy remain speculative without rigorous theoretical justification
- Task complexity assumptions about parameter redundancy are based on limited empirical evidence

## Confidence
- **High Confidence**: The mathematical foundation of DARE (random dropout with rescaling preserving expected distributions) is well-established and correctly applied.
- **Medium Confidence**: Empirical results showing DARE works for moderate drop rates (0-90%) on specific models and tasks.
- **Low Confidence**: Claims about scaling laws for parameter redundancy and DARE's applicability to continuously pre-trained models.

## Next Checks
1. **Parameter Value Analysis**: Conduct a systematic study measuring delta parameter distributions across models with different fine-tuning histories (including post-fine-tuning pre-training) to quantify the prevalence of the >0.005 condition that breaks DARE.

2. **Scaling Law Validation**: Test DARE across a systematic progression of model sizes (e.g., 0.5B, 1.5B, 7B, 13B, 70B) on identical tasks to verify whether the relationship between model size and drop rate tolerance follows predictable scaling laws.

3. **Task Complexity Ablation**: Design a controlled experiment varying task complexity (simple pattern matching → complex reasoning) while holding model size constant to test whether delta parameter redundancy correlates with task simplicity as claimed.