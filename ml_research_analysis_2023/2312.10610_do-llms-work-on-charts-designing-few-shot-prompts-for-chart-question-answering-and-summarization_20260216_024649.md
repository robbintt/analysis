---
ver: rpa2
title: Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering
  and Summarization
arxiv_id: '2312.10610'
source_url: https://arxiv.org/abs/2312.10610
tags:
- chart
- prompt
- data
- visual
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal few-shot prompting framework called
  PROMPTCHART for chart-related tasks. The framework includes a prompt constructor
  that designs task-specific prompts, a visual data table generator that extracts
  visual information from charts, and an LLM.
---

# Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization

## Quick Facts
- arXiv ID: 2312.10610
- Source URL: https://arxiv.org/abs/2312.10610
- Reference count: 40
- Key outcome: PROMPTCHART achieves state-of-the-art performance on chart QA and summarization tasks using few-shot prompting with visual data tables

## Executive Summary
This paper addresses the challenge of using large language models (LLMs) for chart comprehension tasks, including question answering and summarization. The authors propose PROMPTCHART, a multimodal few-shot prompting framework that incorporates visual data tables to provide both data values and visual attributes (colors, positions) to the LLM. Through extensive experiments on three benchmarks (ChartQA, OpenCQA, Chart-to-Text), PROMPTCHART demonstrates significant improvements over existing fine-tuned and few-shot baselines, achieving state-of-the-art performance across all evaluated tasks.

## Method Summary
PROMPTCHART consists of three main components: a prompt constructor that creates task-specific prompts with demonstrations, a visual data table generator that extracts chart data along with visual attributes, and an LLM (InstructGPT) that performs the actual reasoning. The framework constructs prompts with careful attention to demonstration properties (descriptive answers, QA type coverage, consistency) and integrates visual data tables that capture both numerical data and visual features. For chart question answering, the system uses a chain-of-thought format with demonstrations covering visual retrieval, arithmetic, and compositional reasoning. For summarization tasks, demonstrations are selected based on their ability to produce focused, factually correct answers.

## Key Results
- PROMPTCHART achieves state-of-the-art performance on all three benchmarks (ChartQA, OpenCQA, Chart-to-Text)
- Visual data tables significantly improve performance by providing visual attributes alongside numerical data
- The framework outperforms both fine-tuned models and few-shot baselines across factoid QA, long-form QA, and summarization tasks
- Human preference evaluations show PROMPTCHART-generated summaries are preferred over baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting visual data tables into prompts significantly improves LLM chart reasoning performance.
- Mechanism: The visual data table integrates both data values and visual attributes (colors, positions) into a structured table format that the LLM can parse, enabling it to perform visual reasoning that would otherwise require complex image processing.
- Core assumption: LLMs can effectively parse structured visual data tables when included in prompts.
- Evidence anchors:
  - [abstract]: "We further propose a strategy to inject visual information into the prompts."
  - [section 4.2]: Describes construction of visual data table with colors and positions
  - [corpus]: Related work shows chart comprehension requires visual information (ChartAttack, UniChart)
- Break condition: If the LLM cannot parse the structured table format or if visual attributes are not relevant to the task.

### Mechanism 2
- Claim: Task-specific prompt construction guidelines significantly improve few-shot performance.
- Mechanism: By analyzing task subcategories and designing demonstrations that cover different reasoning types (visual retrieval, arithmetic, compositional reasoning), the prompt provides the LLM with relevant examples for the specific reasoning patterns needed.
- Core assumption: The LLM's few-shot learning capability is sensitive to the relevance and diversity of demonstration examples.
- Evidence anchors:
  - [section 4.1]: Detailed analysis of FCQA subcategories and CCR format design
  - [section 6.1]: Analysis showing model performs poorly on complex reasoning without demonstrations
  - [corpus]: Prior work (Wei et al., 2022b) shows chain-of-thought improves reasoning
- Break condition: If the task has too many unique patterns to cover with a small number of demonstrations.

### Mechanism 3
- Claim: Careful selection of demonstration properties (descriptive answers, coverage of QA types, consistency) improves long-form and summarization tasks.
- Mechanism: By ensuring demonstrations have focused answers, cover different question types, and maintain consistency, the prompt helps the LLM generate factually correct and comprehensive outputs.
- Core assumption: The LLM's generation quality is influenced by the properties of demonstration examples in the prompt.
- Evidence anchors:
  - [section 4.1]: Three principles for LCQA and CS prompt construction
  - [section 6.3]: Experiments showing removal of properties decreases performance
  - [corpus]: Prior work shows demonstration quality affects few-shot performance
- Break condition: If the task requires creativity beyond what the demonstrations cover.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Many chart questions require multi-step reasoning that benefits from explicit step-by-step prompts
  - Quick check question: Can you explain how chain-of-thought prompting differs from direct prompting in terms of reasoning process?

- Concept: Visual attribute extraction and encoding
  - Why needed here: Charts contain visual information (colors, positions) that must be extracted and encoded for the LLM to use in reasoning
  - Quick check question: What are the key visual attributes that differentiate chart elements, and how can they be represented in structured format?

- Concept: Few-shot prompting effectiveness
  - Why needed here: The framework relies on demonstrating task patterns to the LLM rather than fine-tuning
  - Quick check question: What factors determine whether a few-shot prompt will be effective for a given task?

## Architecture Onboarding

- Component map:
  - Visual Data Table Generator -> Prompt Constructor -> InstructGPT

- Critical path:
  1. Input chart image and question/instruction
  2. VDTG generates visual data table from chart
  3. Prompt Constructor creates prompt with demonstrations and visual data table
  4. InstructGPT processes prompt and generates output

- Design tradeoffs:
  - Fixed vs. dynamic demonstration selection
  - Level of visual detail in data tables
  - Balance between instruction clarity and demonstration relevance

- Failure signatures:
  - Incorrect visual data table generation
  - Hallucinations in generated text
  - Failure on questions requiring reasoning not covered by demonstrations

- First 3 experiments:
  1. Test VDTG module on diverse chart types to verify visual attribute extraction
  2. Test Prompt Constructor with different numbers of demonstrations on validation set
  3. Compare zero-shot vs. few-shot performance on sample questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of using fixed prompts for chart comprehension tasks, and how can dynamic prompt selection improve model performance?
- Basis in paper: Inferred
- Why unresolved: The paper discusses the effectiveness of fixed prompts for different chart comprehension tasks but acknowledges that a fixed prompt may restrict the model's performance. The authors suggest that more investigation is needed to explore different prompt setups that might improve performance.
- What evidence would resolve it: Comparative studies between fixed and dynamic prompt selection approaches, along with performance metrics on various chart comprehension benchmarks.

### Open Question 2
- Question: How can the Visual Data Table Generator (VDTG) module be further improved to handle a wider variety of chart types and extract more accurate visual information?
- Basis in paper: Inferred
- Why unresolved: The paper introduces the VDTG module as a key component for integrating visual features into prompts but notes that it is trained on ChartQA and supports the available chart types in that dataset. The authors suggest that future works can focus on improving this module to make it more robust to different types of charts.
- What evidence would resolve it: Development and evaluation of VDTG models trained on diverse chart datasets, along with ablation studies on the impact of visual information on model performance.

### Open Question 3
- Question: What are the potential risks and ethical considerations associated with deploying large language models (LLMs) for chart comprehension tasks, and how can these risks be mitigated?
- Basis in paper: Inferred
- Why unresolved: The paper briefly mentions the risk of deploying a technology developed by a third party such as OpenAI, including the potential for producing harmful content. However, the authors do not provide a comprehensive discussion of the ethical implications and risk mitigation strategies for using LLMs in chart comprehension tasks.
- What evidence would resolve it: In-depth analysis of potential risks, case studies of real-world deployments, and development of guidelines or frameworks for ethical and responsible use of LLMs in chart comprehension applications.

## Limitations
- Reliance on pre-existing OCR outputs and data tables, which may introduce errors if chart extraction is imperfect
- Potential scalability issues with visual data table approach for more complex chart types (3D charts, hierarchical charts)
- Risk of increased hallucination with more detailed visual information in prompts

## Confidence
- High confidence: Effectiveness of few-shot prompting with carefully constructed demonstrations
- Medium confidence: Specific guidelines for prompt construction and optimal balance between instruction clarity and demonstration relevance
- Low confidence: Scalability of visual data table approach to more complex chart types and potential for increased hallucination

## Next Checks
1. **Cross-dataset generalization test**: Evaluate PROMPTCHART on charts from domains not represented in the training data (e.g., scientific publications, business reports) to assess real-world robustness.
2. **Error analysis of visual data table generation**: Systematically analyze cases where PROMPTCHART fails, categorizing errors by type (OCR errors, visual attribute extraction failures, reasoning gaps) to identify improvement opportunities.
3. **Comparison with human performance**: Conduct a controlled study comparing PROMPTCHART outputs with human-generated answers for the same chart questions, focusing on factual accuracy and reasoning quality.