---
ver: rpa2
title: Feature Importance versus Feature Influence and What It Signifies for Explainable
  AI
arxiv_id: '2308.03589'
source_url: https://arxiv.org/abs/2308.03589
tags:
- feature
- importance
- value
- values
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the distinction between feature importance
  and feature influence in the context of Explainable AI (XAI). While feature importance,
  as defined in decision theory, measures how much changing a feature's value can
  change the model outcome, most XAI methods use feature influence, which is measured
  against a reference level or baseline.
---

# Feature Importance versus Feature Influence and What It Signifies for Explainable AI

## Quick Facts
- arXiv ID: 2308.03589
- Source URL: https://arxiv.org/abs/2308.03589
- Authors: 
- Reference count: 23
- Key outcome: CIU provides a unified definition of global and local feature importance that is more stable and interpretable than influence-based methods like Shapley value and LIME

## Executive Summary
This paper distinguishes between feature importance (how much a feature's value affects model outcomes) and feature influence (measured relative to a reference baseline), which are often conflated in XAI methods. The authors propose the Contextual Importance and Utility (CIU) method as a unified framework that provides both global feature importance and local instance-level explanations. CIU uses deterministic calculations of feature value ranges to produce explanations with lower variance and greater stability compared to sampling-based methods like Shapley value and LIME.

## Method Summary
The CIU method calculates Contextual Importance (CI) as the normalized range of output variation when modifying a single feature value, and Contextual Utility (CU) as the normalized utility of a feature value within its possible range. The approach uses a sampling method to estimate minimum and maximum output values for each feature, then applies a utility function to map model outputs to [0,1] range. The method is implemented on Random Forest, Stochastic Gradient Boosting, and linear models trained on Titanic, Adult, and synthetic datasets. CIU values are computed deterministically rather than through sampling, which contributes to their lower variance.

## Key Results
- CIU has exactly zero variance in calculated values, compared to high variance in Shapley values and LIME
- CI values are conceptually identical to global feature importance, providing a unified framework
- CU enables instance-level assessment of feature value favorability, supporting counterfactual explanations
- CIU is more stable and reliable than influence-based methods, making it more trustworthy for XAI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CI provides more intuitive and semantically meaningful measure than influence-based methods
- Mechanism: CI is defined as normalized output variation range when modifying a feature value, directly interpretable as potential impact
- Core assumption: Range of output variation is sufficient and intuitive measure of importance
- Evidence anchors: [abstract], [section 2], [corpus]
- Break condition: Non-monotonic or highly non-linear relationships between feature ranges and output variation

### Mechanism 2
- Claim: CU provides instance-level assessment enabling counterfactual explanations
- Mechanism: CU is normalized utility of feature value within possible range, indicating favorability
- Core assumption: Utility function can be meaningfully normalized to [0,1] for interpretable counterfactual reasoning
- Evidence anchors: [abstract], [section 3], [corpus]
- Break condition: Poorly defined utility function or normalized range not capturing relevant favorability aspects

### Mechanism 3
- Claim: CIU has lower variance and is more stable than influence-based methods
- Mechanism: CIU uses deterministic calculation from model output range rather than sampling-based approximation
- Core assumption: Deterministic calculation is sufficient to capture relevant importance aspects without sampling
- Evidence anchors: [abstract], [section 4], [corpus]
- Break condition: Deterministic calculation insufficient to capture relevant importance aspects, leading to biased explanations

## Foundational Learning

- Concept: Feature importance vs. feature influence
  - Why needed here: Crucial for interpreting results and comparing CIU with other methods
  - Quick check question: What is the key difference between feature importance and feature influence, and why is this distinction important in XAI?

- Concept: Contextual Importance (CI) and Contextual Utility (CU)
  - Why needed here: CI and CU are core components of CIU method
  - Quick check question: How are CI and CU defined, and what do they represent in terms of feature importance and utility?

- Concept: Variance and stability of explanation methods
  - Why needed here: Affects reliability and trustworthiness of explanation methods
  - Quick check question: Why is low variance important for explanation methods, and how does CIU achieve lower variance?

## Architecture Onboarding

- Component map: CIU method -> CI and CU components -> Sampling approach -> Utility function
- Critical path:
  1. Define utility function for model output
  2. Implement sampling approach to estimate min/max output values per feature
  3. Calculate CI and CU values for each feature and instance
  4. Visualize CI and CU values using bar plots or other visualizations
- Design tradeoffs:
  - Deterministic calculation vs. sampling-based approximation: CIU has lower variance but may be less flexible for complex interactions
  - Normalized utility range vs. absolute values: Enables direct comparison but may lose absolute importance information
- Failure signatures:
  - High variance in CIU values: Indicates sampling approach or utility function issues
  - Uninterpretable CIU values: Suggests normalization problems or inappropriate utility function
  - Inconsistent results across instances: May indicate model or sampling approach issues
- First 3 experiments:
  1. Implement CIU for simple linear regression and compare with true feature importance
  2. Apply CIU to non-linear model and visualize CI and CU behavior
  3. Compare variance of CIU with Shapley value and LIME on given dataset and model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CIU handle out-of-distribution (OOD) samples in practice?
- Basis in paper: [explicit] Paper discusses OOD sample challenges but doesn't provide definitive solution
- Why unresolved: Paper identifies problem and suggests approaches but offers no concrete methodology
- What evidence would resolve it: Detailed study comparing different OOD handling methods in CIU with empirical effectiveness results

### Open Question 2
- Question: How does CIU performance compare to other XAI methods on different data types like images or text?
- Basis in paper: [inferred] Paper mentions CIU applicability to images and text but lacks experimental results
- Why unresolved: Paper focuses on tabular data without exploring other data types or comparisons
- What evidence would resolve it: Experimental results comparing CIU with other XAI methods on image and text datasets

### Open Question 3
- Question: What are implications of using CIU for global feature importance in non-linear models?
- Basis in paper: [explicit] Paper suggests CI can estimate global importance in non-linear models but lacks comprehensive comparison
- Why unresolved: Paper proposes CI usage but offers no empirical evidence or detailed comparison with mean(|ϕ|) approach
- What evidence would resolve it: Study comparing CI and mean(|ϕ|) values for global feature importance in non-linear models

## Limitations

- Theoretical framework assumes utility functions can be meaningfully normalized to [0,1] for all features, which may not hold in practice
- No user study or qualitative assessment validating CIU explanations are more interpretable than existing methods
- Variance comparison doesn't address other critical dimensions like computational efficiency or explanation faithfulness

## Confidence

- Mechanism 1: Medium confidence - theoretical claim supported but lacks empirical validation
- Mechanism 2: Medium confidence - definition is clear but effectiveness for counterfactuals unproven
- Mechanism 3: Medium confidence - variance claim supported but limited to specific comparison context

## Next Checks

1. Conduct controlled user study comparing CIU explanations with Shapley value and LIME to empirically validate interpretability claims
2. Test CIU on datasets with non-monotonic feature-output relationships to evaluate Mechanism 1's break condition
3. Compare computational costs and scalability of CIU versus sampling-based methods across datasets of varying sizes