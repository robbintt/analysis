---
ver: rpa2
title: Exploring Representational Disparities Between Multilingual and Bilingual Translation
  Models
arxiv_id: '2305.14230'
source_url: https://arxiv.org/abs/2305.14230
tags:
- multilingual
- language
- representations
- decoder
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines representational differences between bilingual
  and multilingual machine translation models, focusing on decoder capacity in one-to-many
  settings. We measure isotropy of decoder representations using IsoScore, finding
  that multilingual decoder representations are consistently less isotropic than bilingual
  decoder representations for the same language pair.
---

# Exploring Representational Disparities Between Multilingual and Bilingual Translation Models

## Quick Facts
- **arXiv ID**: 2305.14230
- **Source URL**: https://arxiv.org/abs/2305.14230
- **Reference count**: 24
- **One-line primary result**: Multilingual decoder representations are consistently less isotropic than bilingual decoder representations for the same language pair, indicating reduced representational capacity.

## Executive Summary
This work examines representational differences between bilingual and multilingual machine translation models, focusing on decoder capacity in one-to-many settings. We measure isotropy of decoder representations using IsoScore, finding that multilingual decoder representations are consistently less isotropic than bilingual decoder representations for the same language pair. This indicates reduced representational capacity in multilingual decoders, attributed to modeling language-specific information that occupies significant space in the vector space. Encoder representations show slight capacity improvements in multilingual models. These findings suggest that modeling multiple target languages in decoders reduces capacity for semantic information, potentially explaining performance degradation in multilingual translation.

## Method Summary
The study trains bilingual and multilingual Transformer models using fixed hidden dimensions on the same parallel sentences. IsoScores are computed on final layer representations from both encoder and decoder, with mean pooling over token dimensions for sequence representations. The analysis compares isotropy between model types and examines spectral distributions to assess representational capacity utilization.

## Key Results
- Multilingual decoder representations show consistently lower isotropy (IsoScore) compared to bilingual decoders for identical language pairs
- Source-side representation capacity improves slightly in multilingual models due to cross-linguistic generalization
- Larger scale training increases representational anisotropy due to output embedding frequency bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual decoders have reduced representational capacity because language-specific information occupies significant vector space dimensions.
- Mechanism: When modeling multiple target languages, the decoder must represent language-specific features (vocabulary, syntax patterns) that compete for space in the shared embedding dimensions. This leaves fewer dimensions available for semantic content specific to each translation.
- Core assumption: Language-specific features have distinct and separable representations that consume substantial vector space.
- Evidence anchors:
  - [abstract]: "much of the anisotropy in multilingual decoder representations can be attributed to modeling language-specific information, therefore limiting remaining representational capacity."
  - [section 4.5]: "We find that throughout decoder layers, the overall isotropy of the entire set of decoder hidden states remains constant or decreases. However, for language-specific decoder states, we see that isotropy increases throughout the layers."
  - [corpus]: Weak evidence - corpus neighbors focus on related multilingual topics but don't directly address capacity reduction mechanisms.
- Break condition: If language-specific features can be represented in a shared, compressed form that doesn't significantly impact semantic encoding capacity.

### Mechanism 2
- Claim: Multilingual encoder representations improve slightly because shared source embedding space benefits from cross-linguistic generalization.
- Mechanism: When translating from English to multiple target languages, the encoder learns to represent English in a way that generalizes across all target languages, leading to more efficient use of the vector space.
- Core assumption: English source representations benefit from being optimized for multiple target languages simultaneously.
- Evidence anchors:
  - [section 4.2]: "Source-side representation capacity improves slightly in one-to-many models over bilingual models."
  - [section 4.4]: "In this setting, we find that our results on increased isotropy of multilingual source-side representations still holds, even though the source-side sentences are identical across our two language pairs in the trilingual model."
  - [corpus]: Weak evidence - corpus neighbors don't specifically address encoder capacity improvements in multilingual settings.
- Break condition: If the encoder must still learn language-specific source representations to optimize for each target language separately.

### Mechanism 3
- Claim: Larger scale training increases representational anisotropy due to output embedding frequency bias.
- Mechanism: With more training updates and larger vocabularies, the output embeddings become increasingly biased toward frequent tokens, causing the hidden states to degenerate into more anisotropic distributions.
- Core assumption: The relationship between output embeddings and hidden states causes hidden state degeneration as training scales up.
- Evidence anchors:
  - [section 4.3]: "We see that in a larger scale, there is consistently less space utilization in both multilingual and bilingual models. This occurs consistently in the decoder space."
  - [section 4.3]: "This phenomenon describes a tendency towards anisotropy of the final softmax layer in NLG models, due to a frequency bias among output embedding updates."
  - [corpus]: Weak evidence - corpus neighbors discuss scaling but don't specifically address representational degeneration.
- Break condition: If the frequency bias effect can be mitigated through techniques like output embedding normalization or balanced sampling.

## Foundational Learning

- Concept: Isotropic vs Anisotropic Representations
  - Why needed here: The paper's core analysis compares isotropy scores between bilingual and multilingual models to measure representational capacity.
  - Quick check question: What does an IsoScore close to 1 indicate about a set of representations?

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: The paper uses SVD to analyze the spectral distribution of representations, showing how balanced variance across dimensions indicates better capacity utilization.
  - Quick check question: How does the spectral distribution of singular values relate to representational capacity?

- Concept: Multitarget vs Multiparallel Data
  - Why needed here: The paper distinguishes between these data types to control for effects of language similarity and shared source sentences.
  - Quick check question: What is the key difference between multitarget and multiparallel datasets in machine translation?

## Architecture Onboarding

- Component map: Transformer architecture with shared encoder and decoder -> Language ID tokens prepended to source for multilingual models -> Separate vocabularies for bilingual (16K) vs multilingual (32K) models -> IsoScore computation for analyzing representation isotropy

- Critical path: 1) Train bilingual and multilingual models on same datasets 2) Extract final layer representations from both encoder and decoder 3) Compute IsoScores for language-specific and overall representations 4) Compare isotropy between model types and analyze spectral distributions

- Design tradeoffs: Shared vs separate vocabularies: Larger shared vocabularies increase parameter efficiency but may reduce capacity for individual languages; Complete vs partial parameter sharing: Full sharing maximizes efficiency but increases interference; Layerwise analysis: Provides insights into where capacity reduction occurs but increases computational complexity

- Failure signatures: If IsoScore differences between bilingual and multilingual models are negligible, the capacity reduction hypothesis is invalid; If encoder improvements don't correlate with decoder reductions, the shared source benefit hypothesis needs revision; If spectral distributions show similar patterns across model types, the mechanism explanation is insufficient

- First 3 experiments: 1) Compare IsoScores for a single language pair across bilingual and multilingual models to establish baseline capacity differences 2) Analyze layerwise isotropy to identify where language-specific information emerges in the decoder 3) Test models with varying degrees of language similarity to quantify the relationship between similarity and capacity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the language-specific capacity reduction in multilingual decoders occur at earlier layers as well as final layers?
- Basis in paper: [inferred] The paper analyzes only final layer representations, but mentions investigating layerwise decoder behavior showing divergence between overall and language-specific isotropy increases throughout layers.
- Why unresolved: The analysis only compares final layer representations between bilingual and multilingual models, not intermediate layers where language-specific information might be separating earlier.
- What evidence would resolve it: Layerwise isotropy analysis comparing bilingual and multilingual models at each decoder layer would reveal if capacity reduction occurs throughout the decoder stack or only at final layers.

### Open Question 2
- Question: Would incorporating language-specific parameters in decoders eliminate the representational capacity reduction while maintaining multilingual benefits?
- Basis in paper: [explicit] The paper discusses prior work on partial sharing and language-specific parameters, but does not test this hypothesis directly.
- Why unresolved: While the paper identifies that language-specific information occupies much of the decoder space, it does not experimentally test whether dedicated language parameters would alleviate this issue.
- What evidence would resolve it: Training multilingual models with language-specific decoder components and comparing their representational capacity and performance to fully shared models would directly test this.

### Open Question 3
- Question: How does the representational capacity reduction scale with the number of target languages in multilingual models?
- Basis in paper: [explicit] The paper uses trilingual models but notes that modern multilingual translation models cover many more languages.
- Why unresolved: The study is limited to two-target multilingual models, making it unclear whether the capacity reduction compounds with each additional language or reaches a saturation point.
- What evidence would resolve it: Testing models with increasing numbers of target languages (3, 4, 5+) and measuring how isotropy and capacity change would reveal the scaling relationship.

## Limitations
- Analysis focuses on a single architectural choice (Transformer) and training regime, limiting generalizability
- Study doesn't examine whether increasing decoder capacity proportionally to the number of target languages would eliminate observed disparities
- Relationship between representational anisotropy and actual translation quality is only indirectly established through BLEU scores

## Confidence
- **High confidence**: The empirical finding that multilingual decoder representations show reduced isotropy compared to bilingual decoders for the same language pair
- **Medium confidence**: The attribution of anisotropy to language-specific information occupying vector space dimensions
- **Low confidence**: The specific mechanism by which language-specific information reduces capacity, and whether this is the primary driver of performance degradation in multilingual models

## Next Checks
1. **Vocabulary size ablation study**: Train models with matched vocabulary sizes between bilingual and multilingual settings to isolate whether the capacity reduction is due to vocabulary expansion or genuine representational constraints.

2. **Controlled capacity scaling**: Systematically increase decoder hidden dimensions in multilingual models and measure at what point isotropy scores converge with bilingual counterparts, establishing a quantitative relationship between capacity and representational quality.

3. **Language-specific probing**: Design targeted tests to measure whether the reduced capacity specifically affects semantic encoding versus language-specific features, using tasks like semantic similarity benchmarks that are language-agnostic.