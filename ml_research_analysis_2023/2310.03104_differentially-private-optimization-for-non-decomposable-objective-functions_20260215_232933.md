---
ver: rpa2
title: Differentially Private Optimization for Non-Decomposable Objective Functions
arxiv_id: '2310.03104'
source_url: https://arxiv.org/abs/2310.03104
tags:
- loss
- learning
- gradient
- contrastive
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses privacy-preserving training of non-decomposable
  objective functions, specifically focusing on contrastive losses used in unsupervised
  pre-training. The key challenge is that these losses have an L2 sensitivity that
  grows with batch size, making them difficult to train using differential privacy
  methods like DP-SGD.
---

# Differentially Private Optimization for Non-Decomposable Objective Functions

## Quick Facts
- arXiv ID: 2310.03104
- Source URL: https://arxiv.org/abs/2310.03104
- Reference count: 40
- Primary result: New DP-SGD variant achieves constant O(1) sensitivity for non-decomposable losses, outperforming naive DP-SGD on CIFAR-10/100 pre-training

## Executive Summary
This paper addresses the challenge of training non-decomposable objective functions like contrastive losses under differential privacy. The key insight is that naive application of DP-SGD fails for these losses because their L2 sensitivity grows with batch size. The authors develop a novel DP-SGD variant that manipulates gradients by clipping pairwise similarity gradients and recombining them, achieving constant O(1) sensitivity regardless of batch size. Experiments on CIFAR-10 pre-training and CIFAR-100 fine-tuning demonstrate performance close to non-private models while significantly outperforming naive DP-SGD.

## Method Summary
The method implements a DP-SGD variant (Algorithm 1) that computes pairwise similarity gradients gij = ∇wtS(Φwt(xi), Φwt(x′j)) for all example pairs in a batch. Each gij is clipped to norm B, then aggregated using a specific recombination formula that ensures constant sensitivity. Gaussian noise calibrated to this constant sensitivity is added to achieve (ε,δ)-differential privacy. The approach targets non-decomposable losses where each example's loss depends on the entire batch, unlike decomposable losses that sum individual example losses.

## Key Results
- CIFAR-10 pre-training: Logit-DP achieves relative test accuracy of 0.742 vs 0.563 for Naive-DP and 1.000 for non-private
- CIFAR-100 fine-tuning: Logit-DP achieves relative test accuracy of 0.855 vs 0.721 for Naive-DP and 1.000 for non-private
- The method maintains constant O(1) sensitivity regardless of batch size, unlike naive DP-SGD which has O(n) sensitivity
- Memory scaling issues encountered with larger models, but mitigated through sequential gradient computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manipulating gradients of non-decomposable losses enables O(1) sensitivity regardless of batch size.
- Mechanism: Instead of directly clipping the summed gradient (which has O(n) sensitivity), the method clips pairwise similarity gradients gij and recombines them using eq. (6), yielding a final gradient with constant sensitivity.
- Core assumption: The sensitivity conditions on the family of loss functions {ℓ(i,n)} hold, specifically that the difference of partial derivatives is bounded by L=O(1/n).
- Evidence anchors:
  - [abstract] states the new DP-SGD variant "manipulates gradients of the objective function in a novel way to obtain a sensitivity of the summed gradient that is O(1) for batch size n."
  - [section 4] shows Theorem 4.2 derives the O(1) sensitivity bound under the stated conditions.
  - [corpus] has no direct evidence; nearest related works focus on noise in SGD or privacy amplification, not batch-size independent sensitivity.
- Break condition: If the family of losses does not satisfy the partial derivative bounds, the sensitivity will revert to O(n), negating the advantage.

### Mechanism 2
- Claim: Per-pair gradient clipping bounds the L2 sensitivity of the aggregated gradient to a constant.
- Mechanism: Clip each gij to have norm at most B, then reconstruct the overall gradient using the clipped terms, ensuring that sensitivity is bounded by (G1 + G2 + (n-1)L)B = O(1).
- Core assumption: Individual pairwise gradients ∥∇wS(Φw(xi), Φw(x′j))∥2 ≤ B can be effectively bounded by clipping.
- Evidence anchors:
  - [section 4.1] derives the sensitivity bound in Theorem 4.2 assuming bounded pairwise gradients.
  - [section 4.2] presents Algorithm 1 that implements this per-pair clipping.
  - [corpus] contains no evidence; related works focus on per-example clipping or Lipschitz assumptions, not pairwise.
- Break condition: If the embedding similarity gradients cannot be bounded within B after clipping, the noise added to achieve DP will grow, breaking the constant-sensitivity property.

### Mechanism 3
- Claim: The sensitivity conditions hold for contrastive loss and spreadout regularizer with L=O(1/n), yielding overall constant sensitivity.
- Mechanism: Lemmas 4.5 and 4.6 show that for these losses, the difference condition (3) is satisfied with L=O(1/n), so the total sensitivity (G1 + G2 + (n-1)L)B remains O(1).
- Core assumption: The input z to the losses (cosine similarities) is bounded in [-1,1], allowing L=O(1/n) to hold.
- Evidence anchors:
  - [section 4] presents Lemmas 4.5 and 4.6 proving the O(1) sensitivity for contrastive and spreadout losses respectively.
  - [abstract] claims experiments show performance close to non-private models, implying the conditions hold in practice.
  - [corpus] has no direct evidence; related works on DP for pairwise losses do not exploit the L=O(1/n) property.
- Break condition: If the similarity scores fall outside [-1,1] or the loss functions change form, the O(1/n) bound on L may fail, breaking the constant sensitivity.

## Foundational Learning

- Concept: Differential privacy and Gaussian mechanism
  - Why needed here: The paper relies on the Gaussian mechanism to add calibrated noise to the clipped gradients, achieving (ε,δ)-DP.
  - Quick check question: What condition must the noise multiplier σ satisfy to ensure (ε,δ)-DP in this setting?

- Concept: Non-decomposable vs decomposable loss functions
  - Why needed here: The method targets non-decomposable losses like contrastive loss, where each example's loss depends on the whole batch, unlike decomposable losses that sum individual example losses.
  - Quick check question: How does the gradient sensitivity of a non-decomposable loss differ from that of a decomposable loss under naive DP-SGD?

- Concept: Pairwise similarity and gradient clipping
  - Why needed here: The key innovation clips gradients of pairwise similarities rather than the aggregated gradient, requiring understanding of how to compute and bound these pairwise terms.
  - Quick check question: Why does clipping per-pair gradients instead of the summed gradient reduce sensitivity from O(n) to O(1)?

## Architecture Onboarding

- Component map:
  Embedding model Φ -> Pairwise gradient computation -> Clipping and aggregation -> Gaussian noise addition -> Model update

- Critical path:
  1. Sample batch of n examples.
  2. Compute pairwise similarity gradients gij.
  3. Clip each gij to norm B.
  4. Aggregate clipped gradients into ¯g using eq. (6).
  5. Add Gaussian noise to ¯g.
  6. Update model parameters.

- Design tradeoffs:
  - Memory vs runtime: materialization of n² gradients gij is memory-heavy; sequential computation avoids memory bottleneck but is slow.
  - Clip norm B vs noise: smaller B reduces sensitivity but may bias gradients; larger B increases noise needed for DP.
  - Batch size n vs privacy: larger n reduces noise per example if sensitivity is O(1), but increases memory and computation cost.

- Failure signatures:
  - Training loss plateaus early: indicates clipping or noise is too aggressive.
  - Memory overflow: indicates n is too large for available GPU memory.
  - Model performance close to random: indicates loss of gradient signal due to excessive clipping or noise.

- First 3 experiments:
  1. Run Algorithm 1 on a small synthetic dataset with known pairwise similarities, verify that sensitivity remains O(1) as n increases.
  2. Compare training loss curves of Logit-DP vs Naive-DP on CIFAR-10 with varying B and σ, confirm Logit-DP achieves lower loss.
  3. Profile memory usage vs batch size n, identify the threshold where n² gradient storage becomes prohibitive.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the theoretical bounds on sensitivity change when applying the proposed DP-SGD variant to other non-decomposable loss functions beyond contrastive and spreadout losses?
- Basis in paper: [explicit] The paper provides specific bounds for contrastive and spreadout losses, but mentions that the framework could be applied to other non-decomposable losses.
- Why unresolved: The authors only prove bounds for two specific loss functions and do not provide a general method for deriving bounds for arbitrary non-decomposable losses.
- What evidence would resolve it: Proving sensitivity bounds for a wider class of non-decomposable loss functions, or developing a general framework for deriving these bounds.

### Open Question 2
- Question: How does the performance of the proposed DP-SGD variant scale with larger embedding models and datasets?
- Basis in paper: [inferred] The paper mentions encountering memory scaling issues with larger models during pre-training, suggesting that the approach may not scale well to larger models and datasets.
- Why unresolved: The experiments were conducted on a small embedding model and datasets (CIFAR-10 and CIFAR-100), and the authors do not provide theoretical analysis of how the approach scales.
- What evidence would resolve it: Conducting experiments on larger models and datasets, or providing theoretical analysis of the scaling behavior.

### Open Question 3
- Question: How does the proposed DP-SGD variant compare to other private learning methods for non-decomposable losses, such as those based on Lipschitz continuity assumptions?
- Basis in paper: [explicit] The paper mentions related work on private learning from pairwise losses that assumes access to the Lipschitz constant of the loss function.
- Why unresolved: The authors do not compare their approach to these other methods, and it is unclear how the approaches compare in terms of privacy-utility trade-offs.
- What evidence would resolve it: Conducting experiments comparing the proposed approach to other private learning methods for non-decomposable losses, or providing theoretical analysis of the relative performance.

## Limitations
- Memory scaling issues with larger models due to O(n²) storage requirement for pairwise gradients
- Theoretical sensitivity bounds rely on specific conditions that may not hold for all non-decomposable loss functions
- No empirical validation of the O(1) sensitivity property across varying batch sizes

## Confidence
- **High confidence**: The core mechanism of per-pair gradient clipping reducing sensitivity from O(n) to O(1) is theoretically sound and well-supported by the proofs in Section 4.
- **Medium confidence**: The claim that performance comes "close" to non-private models is supported by the reported CIFAR-10 and CIFAR-100 results, but the relative improvements over naive DP-SGD could benefit from more extensive benchmarking.
- **Low confidence**: The practical scalability of the method for very large batch sizes remains unproven, as the paper only reports results for batch size 400.

## Next Checks
1. **Sensitivity verification**: Empirically measure the L2 sensitivity of the aggregated gradient as batch size varies from 50 to 1000, confirming it remains constant rather than growing with n.
2. **Memory profiling**: Characterize the memory consumption of the n² gradient storage mechanism across different batch sizes and embedding dimensions, identifying the practical limits.
3. **Alternative loss functions**: Test the method on non-decomposable losses beyond contrastive and spreadout (e.g., triplet loss, N-pair loss) to verify the sensitivity bounds hold more generally.