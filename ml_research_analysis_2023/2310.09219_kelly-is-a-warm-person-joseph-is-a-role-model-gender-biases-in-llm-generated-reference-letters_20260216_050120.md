---
ver: rpa2
title: '"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated
  Reference Letters'
arxiv_id: '2310.09219'
source_url: https://arxiv.org/abs/2310.09219
tags:
- language
- biases
- gender
- generation
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a critical examination of gender biases in
  LLM-generated reference letters, focusing on the dimensions of language style and
  lexical content. The authors designed evaluation methods based on social science
  findings and investigated the extent of bias propagation through the concept of
  hallucination bias.
---

# "Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters

## Quick Facts
- arXiv ID: 2310.09219
- Source URL: https://arxiv.org/abs/2310.09219
- Reference count: 25
- This paper reveals significant gender biases in LLM-generated reference letters across language style and lexical content dimensions.

## Executive Summary
This paper presents a critical examination of gender biases in LLM-generated reference letters, focusing on the dimensions of language style and lexical content. The authors designed evaluation methods based on social science findings and investigated the extent of bias propagation through the concept of hallucination bias. By benchmarking on two popular LLMs, ChatGPT and Alpaca, the study revealed significant gender biases in LLM-generated recommendation letters. The findings highlight the importance of scrutinizing biases in LLM-generated professional documents and emphasize the need for further research to address fairness concerns in this domain.

## Method Summary
The study used synthetic biographies from the WikiBias dataset to generate reference letters with two LLMs (ChatGPT and Alpaca) using both context-less and context-based generation approaches. Gender biases were analyzed through three lenses: lexical content using Odds Ratio analysis, language style using classifiers for formality, sentiment, and agency, and hallucination bias using a context-sentence NLI framework to detect and measure bias amplification in hallucinated content.

## Key Results
- Significant gender bias found in both lexical content and language style of LLM-generated reference letters
- Hallucination bias amplifies existing gender stereotypes in generated content
- Male-associated letters were rated significantly higher in formality, positivity, and agency compared to female-associated letters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gender bias in LLM-generated reference letters arises from the model learning stereotypical associations between gendered names and trait descriptors from its training data.
- Mechanism: During pretraining, LLMs internalize statistical correlations between masculine names and agentic language (leadership, ability) and feminine names with communal language (warmth, personal traits). When prompted with just a name and gender, the model samples from this learned distribution.
- Core assumption: The pretraining corpus contains enough gendered examples that the model can reliably infer trait associations from minimal prompts.
- Evidence anchors: [abstract] "reveals significant gender biases in LLM-generated recommendation letters" [section] "model manifests the stereotype of men being agentic (e.g., natural leader) and women being communal (e.g., well-liked member)"

### Mechanism 2
- Claim: Hallucination bias amplifies existing gender biases by generating content that is not entailed by the input context, thereby reinforcing stereotypes.
- Mechanism: When LLMs hallucinate, they extrapolate beyond the given context using their learned biases. This leads to overrepresentation of stereotypical traits in hallucinated content compared to context-grounded content.
- Core assumption: Hallucinations are generated using the same distributional biases as the rest of the model's output, so they inherit and potentially amplify gender stereotypes.
- Evidence anchors: [abstract] "investigate the extent of bias propagation by analyzing the hallucination bias of models" [section] "we define hallucination bias to be bias exacerbation in model-hallucinated contents"

### Mechanism 3
- Claim: LLMs generate more formal, positive, and agentic language for male candidates, which mirrors documented human biases in professional documents.
- Mechanism: The model's learned distribution aligns with the stylistic patterns found in biased human-written recommendation letters, causing it to reproduce those patterns.
- Core assumption: The pretraining data includes human-written professional documents that already exhibit gender bias in language style.
- Evidence anchors: [abstract] "bias in language style and biases in lexical content" [section] "male documents are significantly higher than female documents in all three aspects: language formality, positivity, and agency"

## Foundational Learning

- Concept: Odds Ratio (OR) for measuring lexical bias
  - Why needed here: OR quantifies how much more likely a word or trait is to appear in one gender's documents versus the other, giving a normalized bias score.
  - Quick check question: If adjective "assertive" appears 10 times in male letters and 2 times in female letters, what is its OR? (Answer: (10/90) / (2/98) â‰ˆ 5.44)

- Concept: Statistical t-testing for language style bias
  - Why needed here: t-tests determine whether differences in formality, sentiment, or agency between genders are statistically significant, controlling for sample size and variance.
  - Quick check question: If male letters have a mean formality of 0.65 (std=0.12, n=3000) and female letters 0.58 (std=0.11, n=2500), what is the t-value? (Compute using the formula in section 3.2.2)

- Concept: Context-Sentence NLI for hallucination detection
  - Why needed here: NLI determines whether each sentence in the generated letter is entailed by the original context, allowing separation of faithful content from hallucination.
  - Quick check question: If the input biography says "John is a software engineer" and the generated letter says "John is a natural leader in engineering," is this entailed? (No, because leadership is not stated in the input)

## Architecture Onboarding

- Component map: Prompt generator -> LLM API -> Output parser -> NLI hallucination filter -> Style bias analyzer -> OR trait analyzer
- Critical path: Prompt -> LLM generation -> Hallucination detection -> Bias evaluation
- Design tradeoffs:
  - Using synthetic biographies ensures gender balance but may not reflect real-world writing styles.
  - ChatGPT API is more reliable but less transparent than open-source models.
  - OR analysis is simple but ignores word frequency context; t-testing is more robust but requires larger sample sizes.
- Failure signatures:
  - High hallucination rate -> model not following context -> inflated bias metrics.
  - Empty or repetitive generations -> generation failure -> low success rate.
  - T-test not significant -> insufficient sample size or weak bias signal.
- First 3 experiments:
  1. Vary temperature in LLM generation and measure hallucination bias change.
  2. Compare bias metrics when using synthetic vs. real biographies as context.
  3. Apply style transfer debiasing to generated letters and re-evaluate gender bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the model amplify or propagate gender biases when it hallucinates information during context-based generation?
- Basis in paper: [explicit] The paper explicitly investigates hallucination bias, defined as bias propagation or amplification in model-hallucinated contents, and measures it through t-testing on language style differences between hallucinated content and full generated documents.
- Why unresolved: While the paper provides evidence of significant gender bias propagation and amplification in LLM hallucinations, it does not explore the underlying mechanisms driving this phenomenon or potential strategies for mitigating it.
- What evidence would resolve it: Further research could investigate the specific factors influencing bias amplification in hallucinations, such as the model's training data, architecture, or fine-tuning process. Experiments could compare different models or training approaches to identify those that minimize bias propagation in hallucinations.

### Open Question 2
- Question: How do gender biases in LLM-generated reference letters impact real-world outcomes for applicants?
- Basis in paper: [inferred] The paper discusses the potential societal harms of gender biases in professional documents, such as lowering success rates for female applicants, but does not provide empirical evidence of actual impacts on application outcomes.
- Why unresolved: The study focuses on analyzing biases in generated letters, but does not examine the downstream effects of these biases on hiring decisions or other real-world scenarios.
- What evidence would resolve it: Future research could conduct field experiments or analyze large-scale application data to quantify the impact of gender-biased reference letters on hiring rates, interview invitations, or other relevant outcomes. This would provide concrete evidence of the practical consequences of the observed biases.

### Open Question 3
- Question: How can we effectively mitigate gender biases in LLM-generated professional documents?
- Basis in paper: [explicit] The paper concludes by emphasizing the need for further research to devise techniques that can effectively address and eliminate fairness concerns associated with LLMs in this domain.
- Why unresolved: While the paper identifies significant gender biases in LLM-generated reference letters, it does not propose or evaluate specific methods for mitigating these biases.
- What evidence would resolve it: Future work could develop and test various bias mitigation techniques, such as debiasing the training data, fine-tuning the models on balanced datasets, or post-processing the generated text to reduce gender stereotypes. Experiments could compare the effectiveness of different approaches in reducing biases while maintaining the quality and coherence of the generated documents.

## Limitations
- The study uses synthetic biographies which may not fully capture real-world reference writing scenarios
- The lexical content analysis using Odds Ratio has inherent limitations with rare words and doesn't account for contextual usage patterns
- The hallucination bias analysis requires careful interpretation as hallucination rates may vary significantly with different context lengths and prompt formulations

## Confidence
- Medium confidence in overall findings due to synthetic evaluation data and controlled experimental conditions
- High confidence in statistical evidence for stylistic biases (formality, sentiment, agency)
- Medium confidence in lexical content analysis using Odds Ratio

## Next Checks
1. Replicate the study using real reference letters with known author gender to validate whether observed biases match human-written patterns
2. Test additional LLMs and prompt variations to assess whether biases are model-specific or widespread across architectures
3. Implement and evaluate bias mitigation strategies during generation to measure reduction potential and any trade-offs in letter quality