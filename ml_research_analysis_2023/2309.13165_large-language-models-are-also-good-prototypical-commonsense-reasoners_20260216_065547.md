---
ver: rpa2
title: Large Language Models Are Also Good Prototypical Commonsense Reasoners
arxiv_id: '2309.13165'
source_url: https://arxiv.org/abs/2309.13165
tags:
- reasoning
- answers
- language
- prompt
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the prototypical commonsense reasoning capability
  of large language models to generate multiple appropriate outputs. The authors semi-automatically
  developed a set of novel prompt templates, including task-relevant, evidence-supported,
  and diverse path decoding prompts, to enhance commonsense reasoning in large language
  models.
---

# Large Language Models Are Also Good Prototypical Commonsense Reasoners

## Quick Facts
- arXiv ID: 2309.13165
- Source URL: https://arxiv.org/abs/2309.13165
- Reference count: 0
- Large language models can be enhanced for prototypical commonsense reasoning using task-relevant, evidence-supported, and diverse path decoding prompts, achieving state-of-the-art results on the ProtoQA dataset.

## Executive Summary
This paper explores how large language models (LLMs) can be enhanced for prototypical commonsense reasoning by generating multiple appropriate outputs. The authors semi-automatically develop novel prompt templates that focus on task relevance, evidence support, and diverse path decoding. Experimental results on the ProtoQA dataset show significant improvements over previous state-of-the-art models, with an 8% increase in Max Answer@1 score and 4% improvement in Max Incorrect@1 score. The approach also enhances model interpretability while maintaining or surpassing previous performance benchmarks.

## Method Summary
The method employs semi-automatic prompt engineering with three key categories: task-relevant prompts that condition outputs based on societal norms, evidence-supported prompts that generate Chain-of-Thought reasoning or knowledge, and diverse path decoding that samples multiple reasoning paths before summarization. The approach is tested on ProtoQA, StrategyQA, and CommonsenseQA2.0 datasets using GPT-3.5-turbo, GPT-4, Claude, and Bard models. Performance is measured using Max Answers@k and Max Incorrect@k metrics.

## Key Results
- Achieves new state-of-the-art results on ProtoQA dataset with 8% improvement in Max Answer@1 score
- Improves Max Incorrect@1 score by 4% compared to previous SOTA models
- Demonstrates enhanced model interpretability through generated Chain-of-Thought and knowledge evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-relevant prompts improve commonsense reasoning by narrowing the output space to answers aligned with social norms and practices.
- Mechanism: Adding task-relevant snippets conditions the model to generate answers that are contextually grounded and socially plausible, reducing irrelevant outputs.
- Core assumption: Large language models can be steered effectively through prompt conditioning without explicit fine-tuning, improving answer diversity and accuracy.
- Evidence anchors:
  - [abstract]: "We semi-automatically explore a series of new prompts... from the perspectives of task-relevance, evidence-supported, and diverse path decoding."
  - [section 3]: "Task-relevant prompts can focus the model's attention and produce output that is more in line with task requirements."
- Break condition: If task-relevant snippets are misaligned with the question domain, the model may generate socially plausible but factually incorrect answers.

### Mechanism 2
- Claim: Evidence-supported prompts improve interpretability and consistency by making reasoning steps explicit.
- Mechanism: Generating supportive evidence (CoT or knowledge) forces the model to articulate intermediate reasoning steps, increasing transparency and allowing self-correction.
- Core assumption: Large language models can maintain reasoning coherence when prompted to generate explicit reasoning steps, meaningfully contributing to final answer quality.
- Evidence anchors:
  - [abstract]: "Furthermore, with the generated Chain-of-Thought and knowledge, we can improve the interpretability of the model while also surpassing the previous SOTA models."
  - [section 3]: "Supportive evidence plays a significant role in various aspects... it enhances the transparency of the model."
- Break condition: If CoT or knowledge generation becomes too verbose or divergent, the final answer may degrade or become less focused.

### Mechanism 3
- Claim: Diverse path decoding improves answer quality by generating multiple reasoning paths and then summarizing the most consistent results.
- Mechanism: Sampling multiple candidate answers with evidence and summarizing top results leverages self-consistency to filter contradictions and produce accurate, diverse answer sets.
- Core assumption: Large language models can generate diverse yet coherent reasoning paths, and their summarization capability can effectively synthesize these into high-quality answers.
- Evidence anchors:
  - [abstract]: "diverse path decoding to aid the model... Experimental results... demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA)."
  - [section 3]: "Generate diverse answers and summary enables the model to attain enhanced self-consistency, which is crucial for the decision-making capabilities of the model."
- Break condition: If sampling temperature or path count is not well-tuned, summarization may include contradictory answers or discard correct ones.

## Foundational Learning

- **Prompt engineering for large language models**
  - Why needed here: The paper demonstrates that prompt design can significantly boost performance on commonsense reasoning without fine-tuning.
  - Quick check question: What is the difference between a few-shot prompt and a task-relevant prompt in terms of their influence on model output?

- **Chain-of-Thought (CoT) reasoning**
  - Why needed here: CoT prompts are used to elicit intermediate reasoning steps that improve interpretability and answer quality in commonsense tasks.
  - Quick check question: How does CoT prompting differ from direct answer prompting in terms of the reasoning process and output?

- **Self-consistency in model outputs**
  - Why needed here: Diverse path decoding relies on the model's ability to generate multiple answers and use self-consistency to select the most plausible final answer.
  - Quick check question: What is self-consistency, and how does it help improve the reliability of model-generated answers?

## Architecture Onboarding

- **Component map**: Prompt generation pipeline → Large language model API → Evaluation metrics → Dataset ingestion
- **Critical path**: Input question → apply task-relevant prompt → generate supportive evidence → generate multiple reasoning paths → summarize top answers → output final answer set
- **Design tradeoffs**:
  - Task-relevant prompt vs. few-shot prompt: task-relevant is more targeted but may limit diversity
  - Evidence-supported prompt vs. direct answer: improves interpretability but may slow down reasoning
  - Diverse path decoding vs. single path: increases robustness but adds computational cost
- **Failure signatures**:
  - Degraded performance on Max Answer@1 with evidence-supported prompts (over-focus on reasoning steps)
  - Inconsistent answers across diverse paths (poor self-consistency)
  - Task-relevant snippets misaligned with question domain (socially plausible but factually wrong answers)
- **First 3 experiments**:
  1. Compare task-relevant prompt vs. few-shot prompt on ProtoQA dev set (measure Max Answer@1)
  2. Add evidence-supported prompt to task-relevant prompt and measure interpretability vs. performance trade-off
  3. Implement diverse path decoding with varying path counts and measure Max Answer@10 improvement

## Open Questions the Paper Calls Out

- **How can we design prompts that effectively elicit multiple plausible outputs from large language models for prototypical commonsense reasoning tasks?**
  - Basis in paper: The authors explore the use of task-relevant, evidence-supported, and diverse path decoding prompts to enhance commonsense reasoning in large language models.
  - Why unresolved: While the authors demonstrate effectiveness on ProtoQA, further research is needed to develop and refine prompt templates for different commonsense reasoning tasks and explore generalizability across domains.
  - What evidence would resolve it: Comparative studies evaluating different prompt templates on diverse commonsense reasoning tasks and domains, with analysis of impact on interpretability and reasoning transparency.

- **How can we balance the trade-off between model interpretability and performance in commonsense reasoning tasks?**
  - Basis in paper: The authors note that evidence-supported prompts can improve interpretability but may introduce redundant information and compromise performance.
  - Why unresolved: While diverse path decoding mitigates performance loss, further research is needed to develop more effective strategies for balancing interpretability and performance.
  - What evidence would resolve it: Empirical studies comparing performance and interpretability of different prompt designs and decoding strategies, with analysis of impact of model architecture and training data.

- **How can we evaluate the commonsense reasoning capabilities of large language models in a comprehensive and rigorous manner?**
  - Basis in paper: The authors use ProtoQA dataset which requires outputting all prototypical answers, noting that many reasoning datasets focus on single correct responses.
  - Why unresolved: While the authors demonstrate effectiveness on ProtoQA, further research is needed to develop comprehensive evaluation methods for commonsense reasoning tasks considering multiple plausible outputs.
  - What evidence would resolve it: Development and validation of new evaluation metrics and datasets considering multiple plausible outputs, with analysis of impact of different evaluation methods on performance assessment.

## Limitations
- Semi-automatic prompt generation lacks rigorous ablation studies for individual prompt components
- Evidence-supported prompts' effectiveness is demonstrated but specific contributions of CoT vs. knowledge generation remain unclear
- Diverse path decoding assumes reliable self-consistency mechanism without empirical validation across different temperature settings or path counts

## Confidence

- **High confidence**: Task-relevant prompts improve performance by focusing outputs on socially plausible answers
- **Medium confidence**: Evidence-supported prompts enhance interpretability and reasoning quality, though specific contribution of CoT vs. knowledge generation remains unclear
- **Low confidence**: The claim that diverse path decoding consistently improves self-consistency across different models and prompt configurations

## Next Checks
1. **Ablation study**: Test each prompt category (task-relevant, evidence-supported, diverse path decoding) in isolation on ProtoQA dev set to measure individual contributions
2. **Cross-model generalization**: Apply the full prompt pipeline to GPT-4 and Claude models to verify consistent performance gains across different LLM architectures
3. **Parameter sensitivity**: Systematically vary temperature and path count parameters in diverse path decoding to identify optimal configurations for self-consistency