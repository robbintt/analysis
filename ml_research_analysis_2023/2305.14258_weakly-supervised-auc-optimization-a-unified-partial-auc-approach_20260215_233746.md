---
ver: rpa2
title: 'Weakly Supervised AUC Optimization: A Unified Partial AUC Approach'
arxiv_id: '2305.14258'
source_url: https://arxiv.org/abs/2305.14258
tags:
- learning
- optimization
- risk
- data
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WSAUC, a unified framework for weakly supervised
  AUC optimization, addressing scenarios like noisy labels, positive-unlabeled learning,
  multi-instance learning, and semi-supervised learning. WSAUC reformulates these
  problems as minimizing AUC risk on contaminated data sets and proves consistency
  with true AUC.
---

# Weakly Supervised AUC Optimization: A Unified Partial AUC Approach

## Quick Facts
- **arXiv ID**: 2305.14258
- **Source URL**: https://arxiv.org/abs/2305.14258
- **Reference count**: 40
- **Key outcome**: WSAUC achieves superior AUC performance across multiple weakly supervised settings, especially under high noise ratios, through a unified framework with reversed partial AUC robustness.

## Executive Summary
This paper introduces WSAUC, a unified framework for weakly supervised AUC optimization that addresses scenarios including noisy labels, positive-unlabeled learning, multi-instance learning, and semi-supervised learning. The authors reformulate these problems as minimizing AUC risk on contaminated data sets and prove consistency with true AUC. To enhance robustness, they propose reversed partial AUC (rpAUC), a novel partial AUC variant that mitigates the impact of contaminated labels by filtering out instances with the largest losses. Experimental results on benchmark datasets demonstrate WSAUC's effectiveness, achieving superior AUC performance compared to specialized baselines across multiple weakly supervised settings.

## Method Summary
WSAUC frames various weakly supervised AUC optimization problems as minimizing AUC risk on contaminated data sets with different class proportions. The framework reformulates different WSL scenarios into a unified formulation with two contaminated sets, leveraging consistency results to show that minimizing the empirical risk on these sets is equivalent to minimizing the true AUC risk. The reversed partial AUC (rpAUC) objective improves robustness by removing instances with the largest losses during training, effectively filtering out noisy or mislabeled instances that contribute most to the loss. Theoretical analysis establishes excess risk bounds and variance reduction properties, justifying the framework's effectiveness.

## Key Results
- WSAUC achieves superior AUC performance compared to specialized baselines across multiple weakly supervised settings
- rpAUC objective improves robustness when labels are imperfect, especially under high noise ratios
- Theoretical analysis provides excess risk bounds and variance reduction benefits
- The framework demonstrates consistent performance across MNIST, FashionMNIST, CIFAR10/100, and multi-instance learning datasets

## Why This Works (Mechanism)

### Mechanism 1
WSAUC frames various weakly supervised AUC optimization problems as minimizing AUC risk on contaminated data sets with different class proportions. By reformulating different WSL scenarios into a unified formulation with two contaminated sets, WSAUC leverages consistency results to show that minimizing the empirical risk on these sets is equivalent to minimizing the true AUC risk. This works under the assumption that the contaminated sets can be modeled as mixtures of positive and negative distributions with known or estimable proportions.

### Mechanism 2
The reversed partial AUC (rpAUC) objective improves robustness by removing instances with the largest losses during training. rpAUC restricts the optimization to instances with scores in specific ranges (bottom scores in XA and top scores in XB), effectively filtering out noisy or mislabeled instances that contribute most to the loss. This mechanism assumes that instances with the largest losses are more likely to be mislabeled or noisy, and removing them will improve generalization.

### Mechanism 3
WSAUC's theoretical analysis provides excess risk bounds and variance reduction properties, justifying its effectiveness. The framework establishes excess risk bounds that depend on the contamination level and sample sizes. It also shows that incorporating unlabeled data can reduce variance in semi-supervised settings. The excess risk bounds are derived assuming Lipschitz continuity and bounded loss functions.

## Foundational Learning

- **Concept**: AUC and ROC Curve
  - Why needed here: The entire framework is built around optimizing the Area Under the ROC Curve, which measures ranking performance without requiring a fixed threshold.
  - Quick check question: What is the definition of AUC in terms of ranking probabilities?

- **Concept**: Weakly Supervised Learning Paradigms
  - Why needed here: WSAUC aims to unify various WSL scenarios (noisy labels, PU learning, MIL, SSL), so understanding their differences and commonalities is crucial.
  - Quick check question: How does positive-unlabeled learning differ from semi-supervised learning in terms of available labels?

- **Concept**: Partial AUC
  - Why needed here: The rpAUC variant of partial AUC is a key component of WSAUC's robustness, so understanding partial AUC concepts is essential.
  - Quick check question: How does two-way partial AUC differ from one-way partial AUC in terms of constraints on TPR and FPR?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Risk computation -> Model training -> Evaluation
- **Critical path**: Data preprocessing → Risk computation → Model training → Evaluation
- **Design tradeoffs**:
  - Accuracy vs. robustness: Using rpAUC improves robustness but may sacrifice some accuracy on clean data
  - Complexity vs. generality: The unified framework handles multiple scenarios but may be more complex than scenario-specific methods
  - Hyperparameter tuning: rpAUC requires setting α and β, which may need cross-validation
- **Failure signatures**:
  - High variance in AUC estimates across runs
  - Performance degradation when noise ratio exceeds a threshold
  - Difficulty in tuning rpAUC hyperparameters
- **First 3 experiments**:
  1. Verify the consistency result: Train on clean data and compare AUC with and without the unified formulation
  2. Test rpAUC robustness: Train on noisy data with varying noise ratios and compare AUC with and without rpAUC
  3. Evaluate variance reduction: Compare AUC variance when incorporating unlabeled data in semi-supervised settings

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed WSAUC framework perform when the class prior probabilities are unknown or vary significantly across different weakly supervised scenarios? The paper mentions that WSAUC can handle various weakly supervised scenarios without requiring knowledge of the mixing proportions, but does not provide empirical evidence for this claim.

### Open Question 2
Can the WSAUC framework be extended to handle multi-class classification problems, or is it limited to binary classification tasks? The paper focuses on binary classification problems and does not mention any potential extensions to multi-class scenarios.

### Open Question 3
How does the choice of the weighting coefficient γ in the semi-supervised AUC optimization with label noise affect the performance of WSAUC, and is there an optimal method for selecting γ? The paper sets γ as 0.45 for all tasks in the experiments, but does not provide a detailed analysis of its impact on performance or discuss methods for optimal selection.

## Limitations
- The framework's effectiveness depends critically on accurate estimation of contamination proportions, which is not addressed in detail
- The rpAUC assumption that high-loss instances are primarily noisy may not hold when noise is uniformly distributed
- The theoretical bounds rely on Lipschitz continuity assumptions that may not hold for deep neural networks commonly used in practice

## Confidence
- **High Confidence**: The unified formulation connecting different WSL scenarios (95% confidence)
- **Medium Confidence**: rpAUC's robustness improvements (75% confidence)
- **Medium Confidence**: Excess risk bounds validity (70% confidence)

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary rpAUC's α and β parameters across noise levels to determine optimal settings and assess robustness to hyperparameter choices
2. **Contamination Estimation**: Implement and evaluate methods for estimating contamination proportions in each WSL scenario to test the practical limits of the framework
3. **Deep Network Compatibility**: Test the framework with modern deep architectures (ResNets, Transformers) on the same benchmarks to verify if theoretical assumptions hold in practice