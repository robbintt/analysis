---
ver: rpa2
title: Precision of Individual Shapley Value Explanations
arxiv_id: '2312.03485'
source_url: https://arxiv.org/abs/2312.03485
tags:
- shapley
- values
- value
- explanations
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the precision of individual Shapley value explanations
  in explainable AI. The authors focus on conditional Shapley values for predictive
  models on tabular data, explaining predictions for single observations.
---

# Precision of Individual Shapley Value Explanations

## Quick Facts
- arXiv ID: 2312.03485
- Source URL: https://arxiv.org/abs/2312.03485
- Reference count: 9
- Primary result: Shapley value explanations are systematically less precise for observations far from the training data center across all estimation methods

## Executive Summary
This paper examines the precision of individual Shapley value explanations for predictive models on tabular data, focusing on conditional Shapley values that explain single observation predictions. The authors demonstrate that estimation errors are systematically larger for test observations located in the outer regions of the training data distribution, regardless of which estimation method is used. This precision degradation is attributed to limited training data in these outer regions, which impairs the ability to accurately learn feature dependence structures needed for Shapley value computation. The study uses a simulation with 1000 training and 250 test observations, fitting a GAM model to multivariate normal data.

## Method Summary
The study generates synthetic training data (1000 observations) from a multivariate normal distribution with 8 dimensions and correlation ρ=0.5, then fits a GAM model with splines and tensor product smooths. Test observations (250) are generated and their true Shapley values computed using the known data-generating process. Six different Shapley value estimation methods are implemented and compared, including parametric methods assuming Gaussian distributions and flexible regression-based approaches. Precision is measured by calculating the mean absolute error (MAE) between true and estimated Shapley values for each test observation, with analysis focusing on how errors correlate with Euclidean distance from the training data center.

## Key Results
- Shapley value estimation errors are systematically larger for test observations far from the training data center
- Parametric methods assuming Gaussian distributions perform best overall but still show increased errors for outer-region observations
- The magnitude of Shapley value errors scales with the prediction difference from the global average
- All six estimation methods tested show the same pattern of reduced precision in outer regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley value precision decreases systematically for observations far from the training data center.
- Mechanism: The estimation methods rely on conditional distributions learned from training data. In regions where training data is sparse, these conditional distributions are poorly estimated, leading to larger errors in Shapley value computation.
- Core assumption: Training data density is correlated with estimation accuracy of conditional distributions.
- Evidence anchors:
  - [abstract] "explanations are systematically less precise for observations on the outer region of the training data distribution for all used estimation methods"
  - [section] "The predictions with the largest Shapley value explanation errors correspond to the test observations furthest away from the center of the training data"

### Mechanism 2
- Claim: Parametric methods assuming Gaussian distributions perform better overall but still show larger errors for outer-region observations.
- Mechanism: Parametric methods reduce variance by assuming a specific distributional form, but this assumption becomes increasingly inaccurate as observations move away from regions where the assumption was validated by training data.
- Core assumption: The Gaussian assumption holds reasonably well near the training data center but breaks down in outer regions.
- Evidence anchors:
  - [section] "parametric methods, which are all able to model the Gaussian distribution, obtain the lowest MAE" and "we see a clear pattern in the errors when we color encode the test observations based on their Euclidean distance to the empirical center"

### Mechanism 3
- Claim: Shapley value errors scale with the magnitude of the prediction difference from the global average.
- Mechanism: Since Shapley values must sum to explain the difference between prediction and global average, larger prediction magnitudes require larger Shapley values, amplifying absolute errors.
- Core assumption: The relationship between prediction magnitude and Shapley value magnitude is linear.
- Evidence anchors:
  - [section] "we see that the parametric methods, which are all able to model the Gaussian distribution, obtain the lowest MAE" and "The left histogram in Figure 2 illustrates that observations x∗with a large |f(x∗)−ϕ0|yield larger mean absolute errors"

## Foundational Learning

- Concept: Conditional expectations and their estimation
  - Why needed here: Shapley value computation requires estimating E[f(x)|xS = x*S], which is central to understanding why estimation errors vary across the feature space
  - Quick check question: What is the difference between conditional and marginal Shapley values?

- Concept: Monte Carlo integration for expectation estimation
  - Why needed here: Several estimation methods use Monte Carlo sampling to approximate conditional expectations, directly affecting precision
  - Quick check question: How does sample size affect the precision of Monte Carlo estimates?

- Concept: Gaussian distribution assumptions in parametric methods
  - Why needed here: Understanding why parametric methods perform better helps explain their limitations in outer regions
  - Quick check question: What are the risks of assuming Gaussian distributions for feature dependencies?

## Architecture Onboarding

- Component map: Data generation → GAM model training → Test observation creation → Shapley value estimation (6 methods) → Error calculation (MAE) → Analysis (distance-based grouping)
- Critical path: The pipeline from data generation through Shapley value estimation to error analysis
- Design tradeoffs: Flexible methods vs. parametric methods (accuracy vs. robustness to distributional assumptions)
- Failure signatures: Large MAE values correlated with distance from training data center; inconsistent Shapley value rankings across methods
- First 3 experiments:
  1. Run with uniform training data density to test if distance correlation persists
  2. Test with non-Gaussian parametric assumptions to see if results generalize
  3. Evaluate on a real dataset with known density patterns to validate simulation findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the precision of Shapley value explanations vary across different regions of the training data distribution beyond just the outer regions?
- Basis in paper: [explicit] The paper demonstrates systematically less precise explanations for observations in the outer regions of the training data distribution.
- Why unresolved: The paper only examines precision differences between inner and outer regions, not across multiple regions of the distribution.
- What evidence would resolve it: A systematic analysis measuring Shapley value precision across multiple distance bands from the training data center.

### Open Question 2
- Question: Do certain feature subsets or coalitions show systematically different precision patterns in Shapley value estimation compared to others?
- Basis in paper: [inferred] The paper mentions separate regression models for each coalition S∈P(M) but doesn't analyze precision differences across coalitions.
- Why unresolved: The analysis focuses on overall precision and distance from center, not on how precision varies by specific feature coalitions.
- What evidence would resolve it: A detailed breakdown of Shapley value precision by feature coalition size and composition.

### Open Question 3
- Question: How do different parametric assumptions about the data distribution affect Shapley value precision in outer regions compared to more flexible methods?
- Basis in paper: [explicit] The paper shows parametric methods assuming Gaussian distribution perform best but still show larger errors for outer-region test observations.
- Why unresolved: The paper doesn't explore how different parametric assumptions (beyond Gaussian) or the degree of flexibility in methods affects precision in outer regions.
- What evidence would resolve it: A comparative study testing multiple parametric distributions and flexible methods across varying distances from the training data center.

## Limitations
- The study uses 8-dimensional data, limiting understanding of how precision-distance relationships scale with dimensionality
- Results are based on GAM models specifically, and may not generalize to other model architectures
- The simulation uses synthetic data with known properties, which may not capture complexities of real-world datasets

## Confidence
**High Confidence**: The systematic relationship between test observation distance from training data center and increased Shapley value estimation error is well-supported by the empirical results across multiple estimation methods.

**Medium Confidence**: The explanation that sparse training data in outer regions leads to poorer conditional distribution estimation is plausible but not definitively proven. Alternative explanations (e.g., extrapolation errors in the GAM model itself) cannot be ruled out.

**Medium Confidence**: The superiority of parametric methods assuming Gaussian distributions is demonstrated for this specific simulation setup, but may not generalize to all data distributions or model types.

## Next Checks
1. **Dimensionality Scaling Experiment**: Repeat the analysis with varying numbers of dimensions (e.g., 4, 8, 16, 32) to determine how the precision-distance relationship scales with dimensionality.

2. **Non-Gaussian Distribution Test**: Generate training data from non-Gaussian distributions (e.g., exponential, mixture models) and evaluate whether the observed precision patterns persist or change.

3. **Model Architecture Comparison**: Apply the same precision analysis framework to tree-based models (e.g., random forests) and neural networks to assess whether the distance-based precision degradation is model-agnostic or specific to GAMs.