---
ver: rpa2
title: Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors
arxiv_id: '2304.07063'
source_url: https://arxiv.org/abs/2304.07063
tags:
- query
- queries
- graph
- formula
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to solve complex first-order queries on knowledge
  graphs. While prior work uses operator trees that only address a subset of queries
  (Tree-Form), the authors extend to all Existential First-Order (EFO1) queries and
  characterize the gap between these classes.
---

# Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors

## Quick Facts
- arXiv ID: 2304.07063
- Source URL: https://arxiv.org/abs/2304.07063
- Reference count: 40
- One-line primary result: FIT algorithm solves arbitrary EFO1 queries on knowledge graphs with faithfulness and perfectness guarantees, outperforming prior methods on complex query types.

## Executive Summary
This paper addresses the limitation of existing neural link predictors that can only handle a subset of complex queries on knowledge graphs (Tree-Form queries). The authors propose FIT, a fuzzy-logic-based inference algorithm that can solve arbitrary Existential First-Order (EFO1) queries. They characterize the gap between Tree-Form and EFO1 queries through two structural properties, introduce a new dataset with 10 novel query types, and demonstrate FIT's superior performance on both existing and new query types.

## Method Summary
The FIT algorithm takes as input predicate matrices from a pretrained neural link predictor and performs fuzzy logic inference to answer complex EFO1 queries. It recursively reduces queries by removing nodes and updating membership vectors using t-norms (for conjunction), t-conorms (for disjunction), and Gödel max (for existential quantification). The algorithm guarantees faithfulness for queries without negation and perfectness when the truth value matrix perfectly represents the observed knowledge graph.

## Key Results
- FIT achieves MRR of 58.1 on certain complex queries vs 38.7 for prior methods on FB15k
- New dataset with 10 query types (pni, 2il, 3il, 2m, 2nm, 3mp, 3pm, im, 3c, 3cm) expands the scope beyond traditional query types
- Characterizes the gap between TF and EFO1 queries through two structural properties: negation among existential variables and existential leaves
- Guarantees faithfulness for EFO1 queries without negation and perfectness under specific matrix assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIT solves arbitrary EFO1 queries by recursively reducing them using fuzzy logic truth values
- Mechanism: FIT maintains fuzzy membership vectors for each node and iteratively removes nodes by updating vectors according to fuzzy logic rules
- Core assumption: Neural link predictor provides consistent truth values and Gödel t-conorm correctly implements existential quantification via maximum
- Evidence anchors: [abstract] FIT guarantees faithfulness and perfectness; [section] Theorem 5 on perfectness; [corpus] Weak evidence on Gödel t-conorm usage
- Break condition: If link predictor violates consistency or Gödel t-conorm doesn't preserve max property for existential quantification

### Mechanism 2
- Claim: FIT guarantees faithfulness for queries without negation by ensuring deductively provable answers receive truth value 1
- Mechanism: Constructs perfect matrix from observed triples and uses monotonicity of t-norms/t-conorms to ensure derivable answers get maximal truth value
- Core assumption: Truth value function T is monotonic with respect to perfect matrix truth values
- Evidence anchors: [abstract] FIT outperforms prior methods; [section] Theorem 6 on faithfulness; [corpus] Missing evidence on faithfulness in prior methods
- Break condition: If query contains negation or monotonicity property is violated due to numerical instability

### Mechanism 3
- Claim: Gap between TF and EFO1 queries characterized by negation among existential variables and existential leaves
- Mechanism: Represents queries as multigraphs and analyzes structure to show TF corresponds to acyclic simple graphs without these properties
- Core assumption: Assumption 1 (reverse relation enrichment) holds
- Evidence anchors: [abstract] Identifies gap between formulation and goal; [section] Theorem 3 on characterization; [corpus] Limited evidence on TF vs EFO1 gap
- Break condition: If Assumption 1 fails or query graph contains unhandled cycles/multiple edges

## Foundational Learning

- Concept: Fuzzy logic theory (t-norms, t-conorms, membership predicates)
  - Why needed here: FIT is built entirely on fuzzy logic semantics to compute truth values for complex logical queries
  - Quick check question: What is the difference between Godel t-norm and product t-norm in fuzzy logic?

- Concept: Knowledge graph embeddings and neural link predictors
  - Why needed here: FIT requires pretrained neural link predictor that outputs truth values for triples
  - Quick check question: How does a typical neural link predictor output a score for a triple, and how is this converted to a truth value?

- Concept: First-order logic and existential quantification
  - Why needed here: Paper's goal is to answer existential first-order queries on knowledge graphs
  - Quick check question: What is the difference between a free variable and a bounded variable in first-order logic?

## Architecture Onboarding

- Component map: Neural link predictor → Truth value matrix Pr → FIT inference algorithm → Answer vector A[φ(y)]
- Critical path: Query graph construction → Matrix initialization → Iterative node removal via fuzzy logic rules → Final answer vector extraction
- Design tradeoffs: FIT trades computational complexity (enumeration on cycles) for expressiveness (handling arbitrary EFO1 queries). The sparse matrix threshold ε balances information retention vs matrix sparsity.
- Failure signatures: Poor performance on cyclic queries (due to enumeration limits), failure to handle negations (faithfulness guarantee breaks), or degraded performance with low-quality link predictors (violates consistency assumption)
- First 3 experiments:
  1. Run FIT on simple acyclic query (1p) with perfect matrix to verify perfectness guarantee
  2. Test FIT on query with negation (pni) to observe performance degradation vs faithfulness claims
  3. Evaluate FIT on cyclic query (3c) with varying enumeration limits M to find performance threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FIT performance scale with increasingly complex EFO1 queries beyond those tested?
- Basis in paper: [inferred] Paper demonstrates effectiveness on 10 query types but doesn't explore higher complexity or larger graphs
- Why unresolved: Experiments cover limited query types and graph sizes; scaling behavior to more complex queries remains unknown
- What evidence would resolve it: Systematic experiments varying query complexity and knowledge graph size, measuring computational time and accuracy

### Open Question 2
- Question: Can FIT be extended to handle EFO1 queries with universal quantifiers?
- Basis in paper: [explicit] Paper focuses on EFO1 without universal quantifiers and characterizes TF-EFO1 gap
- Why unresolved: Framework and algorithm built around EFO1 without universal quantifiers; extension would require fundamental changes
- What evidence would resolve it: Modified FIT handling universal quantifiers with formal proofs and empirical validation

### Open Question 3
- Question: How sensitive is FIT to choice of t-norm and t-conorm, and what are theoretical implications?
- Basis in paper: [explicit] Paper mentions several t-norms but only uses Gödel for existential quantification and Product for conjunction
- Why unresolved: Doesn't systematically explore impact of different t-norm/t-conorm combinations or provide theoretical analysis
- What evidence would resolve it: Comprehensive experiments comparing combinations with theoretical analysis of properties and reasoning capabilities

## Limitations
- Faithfulness guarantees depend on neural link predictor satisfying Assumption 2 (perfect consistency for observed triples), not empirically verified
- Characterization of TF-EFO1 gap relies on Assumption 1 (reverse relation enrichment), which may not hold for all knowledge graphs
- Enumeration limit M for cyclic queries is mentioned as hyperparameter but not specified, making exact reproduction challenging

## Confidence
- High confidence in FIT algorithm's correctness under stated assumptions
- Medium confidence in practical impact given limited evaluation on new query types
- Low confidence in empirical verification of theoretical guarantees without explicit tests on observed triples

## Next Checks
1. Verify faithfulness guarantee by testing FIT on queries where answers are known to be provable in observed graph, checking if all such answers receive truth value 1
2. Evaluate impact of Assumption 1 by running experiments with and without reverse relation enrichment to quantify TF-EFO1 gap characterization
3. Perform ablation studies on sparsity threshold ε and enumeration limit M to understand their impact on performance across different query types, particularly for cyclic queries