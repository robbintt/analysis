---
ver: rpa2
title: Mitigating Data Injection Attacks on Federated Learning
arxiv_id: '2312.02102'
source_url: https://arxiv.org/abs/2312.02102
tags:
- data
- agents
- learning
- federated
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data injection attacks in federated learning,
  where malicious agents can manipulate the learning process by sending false model
  updates. The proposed method detects and mitigates such attacks by comparing gradient
  updates from agents to the coordinatewise median, using majority voting over detection
  history to distinguish attackers from trustworthy agents.
---

# Mitigating Data Injection Attacks on Federated Learning

## Quick Facts
- **arXiv ID**: 2312.02102
- **Source URL**: https://arxiv.org/abs/2312.02102
- **Reference count**: 0
- **Primary result**: Detection method compares gradient updates to coordinatewise median with majority voting over detection history, achieving isolation of malicious agents with probability 1 when truthful agents are in majority.

## Executive Summary
This paper addresses data injection attacks in federated learning where malicious agents manipulate the learning process by sending false model updates. The proposed method detects and mitigates such attacks by comparing each agent's gradient updates to the coordinatewise median of all updates, using majority voting over detection history to distinguish attackers from trustworthy agents. The method operates locally at the coordinating node during training, allowing the federated learning system to recover and converge to the truthful model once attackers are isolated.

## Method Summary
The method detects malicious agents by computing the coordinatewise median of all agents' gradient updates and comparing each agent's update to this median using a threshold δu. Detection results are accumulated over time using majority voting with parameter K, where an agent is ignored only if more than half of its detection results in a segment indicate malicious behavior. The system periodically re-evaluates ignored agents, allowing recovery if false alarms occurred. The approach assumes sub-Gaussian i.i.d. data distribution across agents and a majority of truthful agents.

## Key Results
- Theoretical analysis proves that with probability 1, all attackers will be isolated after finite time while trustworthy agents remain unaffected, provided truthful agents are in majority.
- Simulations on MNIST dataset demonstrate successful detection and isolation of attackers performing constant-output and label-flipping attacks.
- The federated learning system recovers and converges to the truthful model after malicious agents are isolated.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detection threshold comparison isolates malicious agents with high probability
- Mechanism: The coordinating node computes a coordinatewise median over all agents' gradient updates, compares each agent's update to this median, and flags deviations exceeding threshold δu. Malicious agents producing systematically different updates are isolated via majority voting over detection history.
- Core assumption: Data is sub-Gaussian and i.i.d. across agents; majority of agents are trustworthy.
- Evidence anchors:
  - [abstract] "We prove that with probability 1, after a finite time, all attackers will be ignored while the probability of ignoring a trustful agent becomes 0, provided that there is a majority of truthful agents."
  - [section 3] "Lemma 1 Assume that the majority of agents are trustworthy. Furthermore, assume that data is sub-gaussian and i.i.d between agents and classes."

### Mechanism 2
- Claim: Majority voting over detection history reduces false alarms and missed detections
- Mechanism: Each agent's detection result is accumulated over time (using parameter K). An agent is ignored only if more than half of its detection results in a segment indicate malicious behavior. This voting scheme allows for correction of temporary false alarms or missed detections.
- Core assumption: The detection metric has P_F A < 1/2 < P_D for each interval.
- Evidence anchors:
  - [section 3] "By the end of each segment, K∆T the coordinating agent ignores the input of all edge agents for which 1/K Σ(dk) > 1/2 for the next segment IK+1."
  - [section 3] "Lemma 2 Assume we set ∆T such that for each k P_F A(Ik) < 1/2 < P_D(Ik)."

### Mechanism 3
- Claim: Local mitigation preserves privacy while maintaining convergence
- Mechanism: The mitigation is performed locally at the coordinating node without requiring data exchange. Detection and isolation happen during the training process itself, allowing the global model to recover and converge once attackers are isolated.
- Core assumption: Local gradient updates are sufficient for detection without accessing raw data.
- Evidence anchors:
  - [abstract] "Our mitigation method is a local scheme, performed during a single instance of training by the coordinating node, allowing the mitigation during the convergence of the algorithm."
  - [section 3] "Whenever an agent is suspected to be an attacker, its data will be ignored for a certain period, this decision will often be re-evaluated."

## Foundational Learning

- Concept: Coordinatewise median robust statistics
  - Why needed here: Median provides robustness to outliers (malicious agents) compared to mean, making it suitable for Byzantine-resilient aggregation.
  - Quick check question: Why is coordinatewise median preferred over coordinatewise mean in the presence of potential attackers?

- Concept: False alarm rate (P_F A) and detection rate (P_D)
  - Why needed here: The theoretical guarantees depend on having P_F A < 1/2 < P_D, which ensures correct majority voting over time.
  - Quick check question: What happens to the detection scheme if P_F A > 1/2?

- Concept: i.i.d. data assumption in federated learning
  - Why needed here: The proof of convergence and detection relies on agents having identically distributed data, which ensures that truthful agents' updates follow similar statistical patterns.
  - Quick check question: How would non-i.i.d. data distribution affect the validity of Lemma 1?

## Architecture Onboarding

- Component map: Coordinating node -> Median computation -> Detection module -> Isolation manager <- Edge agents -> Local training -> Gradient updates
- Critical path:
  1. Agents perform local training and send gradient updates
  2. Coordinating node computes coordinatewise median of updates
  3. Each agent's update deviation is computed and compared to threshold
  4. Detection results are accumulated over time
  5. Agents exceeding majority threshold are ignored for next segment
  6. Process repeats until convergence or all attackers isolated
- Design tradeoffs:
  - Detection interval ∆T vs. detection speed: Shorter intervals enable faster detection but increase computational overhead
  - Threshold δu vs. sensitivity: Lower thresholds detect subtle attacks but increase false alarms
  - Majority voting window K vs. stability: Larger K provides more stability but slower isolation of confirmed attackers
- Failure signatures:
  - Model convergence stalls despite no detected attackers (possible collusion attack)
  - Persistent high false alarm rate (threshold too low or data non-i.i.d.)
  - Attackers remain undetected for extended periods (attacks mimic truthful behavior)
- First 3 experiments:
  1. Implement basic median-based detection without voting (K=1) on synthetic i.i.d. data to verify detection works in simple case
  2. Add majority voting with varying K values to measure trade-off between false alarms and detection speed
  3. Introduce controlled label-flipping attacks with varying mixing weights g(t) to test robustness against hidden attacks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important limitations are implied:

- The assumption of i.i.d. data across agents may be limiting for real-world federated learning scenarios
- The method's performance under non-i.i.d. data distributions is not analyzed
- Scalability to large numbers of agents is not discussed
- Communication constraints are not addressed

## Limitations
- The theoretical guarantees rely heavily on the i.i.d. data assumption across agents, which is often violated in practical federated learning scenarios.
- The method does not address collusion attacks where multiple malicious agents coordinate their behavior to evade detection.
- The detection performance may degrade if the threshold δu is not properly calibrated or if detection intervals are poorly chosen.

## Confidence
- **High confidence**: The core mechanism of using coordinatewise median for robust aggregation is well-established in Byzantine-tolerant distributed computing literature.
- **Medium confidence**: The majority voting scheme for temporal detection is theoretically sound under the stated assumptions, but practical performance may vary with real-world data distributions.
- **Low confidence**: The claim that convergence to the truthful model is guaranteed with probability 1 relies on strong assumptions about attacker behavior and data distribution that may not hold in practice.

## Next Checks
1. **Non-i.i.d. Data Testing**: Evaluate the detection performance when agents have heterogeneous data distributions to assess robustness beyond the theoretical i.i.d. assumption.
2. **Collusion Attack Scenario**: Implement coordinated attacks where multiple malicious agents collaborate to produce updates that average toward the median, testing the method's vulnerability to sophisticated attack strategies.
3. **Threshold Sensitivity Analysis**: Systematically vary the detection threshold δu and detection interval ∆T to map out the trade-off between false alarm rate and detection speed across different attack types.