---
ver: rpa2
title: Exploring Transformer Extrapolation
arxiv_id: '2307.10156'
source_url: https://arxiv.org/abs/2307.10156
tags:
- length
- extrapolation
- window
- inference
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the conditions for transformer length extrapolation
  through a mathematical and empirical analysis. It discovers that a transformer is
  guaranteed to possess length extrapolation as long as the series corresponding to
  the RPE's exponential converges.
---

# Exploring Transformer Extrapolation

## Quick Facts
- arXiv ID: 2307.10156
- Source URL: https://arxiv.org/abs/2307.10156
- Reference count: 40
- Key outcome: Transformers possess length extrapolation as long as the series corresponding to the RPE's exponential converges

## Executive Summary
This paper investigates the conditions for transformer length extrapolation through mathematical and empirical analysis. The key discovery is that transformers guarantee length extrapolation when the series corresponding to Relative Positional Encoding (RPE) exponential converges. The authors derive a new Theoretical Receptive Field (TRF) metric that measures RPE receptive fields without training, validated across multiple language modeling datasets including Wikitext-103, Books, Github, and WikiBook.

## Method Summary
The paper analyzes transformer length extrapolation by examining the convergence properties of RPE exponential series. The authors derive TRF as a theoretical measure of receptive fields, then validate their findings through experiments on multiple language modeling datasets. Models are trained on sequences up to length 512 and tested on sequences up to length 9216 to evaluate extrapolation capability. Various RPE variants are tested including Alibi, Kerple, Sandwich, Type1, Type2, and Sinusoidal.

## Key Results
- Transformers exhibit length extrapolation when RPE exponential series converges
- TRF accurately predicts actual receptive fields before training
- RPEs with divergent series (1/n, 1/(n ln n)) fail to extrapolate despite concentrating weights on neighboring tokens
- Experimental validation across Wikitext-103, Books, Github, and WikiBook datasets demonstrates consistent findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A transformer possesses length extrapolation as long as the series corresponding to the RPE's exponential converges.
- Mechanism: The relative positional encoding (RPE) introduces a weighting scheme where attention scores decay with distance. When the series of these weights converges, distant tokens contribute negligibly to the attention computation, effectively creating a sliding window behavior that is naturally length-extrapolatable.
- Core assumption: The attention computation can be approximated by a finite window when the RPE's exponential series converges.
- Evidence anchors:
  - [abstract] "We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges."
  - [section 3.2] "Theorem 3.4. When the following condition is satisfied, Eq. 9 holds. limi→∞Σt=1i bit < ∞, Bii = Σ1≤t≤i bit < ∞."
  - [corpus] Weak - no direct corpus evidence supporting this convergence claim.

### Mechanism 2
- Claim: The TRF accurately reflects the actual receptive fields of RPEs before training.
- Mechanism: TRF is computed directly from the RPE formulation without requiring any training steps, using the convergence properties of the RPE's exponential series to estimate the effective attention window.
- Core assumption: The theoretical computation based on RPE formulation correlates with the empirical receptive field observed after training.
- Evidence anchors:
  - [abstract] "we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps."
  - [section 3.3] "We may find it difficult to give the analytical form of the partial sum of the series at times, but we can still compute the TRF numerically."
  - [corpus] Moderate - experimental results show TRF trends match ERF trends across datasets.

### Mechanism 3
- Claim: RPEs that concentrate weights on neighboring tokens are not sufficient for length extrapolation.
- Mechanism: While concentrating weights on neighboring tokens is necessary, the convergence of the exponential series is the sufficient condition. RPEs that concentrate weights but have divergent series will fail to extrapolate.
- Core assumption: Convergence of the exponential series is the key distinguishing factor beyond weight concentration.
- Evidence anchors:
  - [section 3.4] "Although we only provide mathematical proof for the sufficiency of our discovered conditions, we also attempt to verify their necessity empirically in this section."
  - [section 3.4] "We then empirically test their length extrapolation capability on Wikitext-103, Books, Github, and WikiBook datasets by scaling the inference sequence length from 512 to 9216 tokens."
  - [corpus] Strong - empirical validation shows RPEs with divergent series (1/n, 1/(n ln n)) fail to extrapolate despite concentrating weights.

## Foundational Learning

- Concept: Softmax attention and its role in transformer architecture
  - Why needed here: Understanding how attention works is fundamental to grasping how RPEs modify attention scores and enable length extrapolation.
  - Quick check question: What is the mathematical form of softmax attention and how does it combine query, key, and value matrices?

- Concept: Relative positional encoding and Toeplitz matrices
  - Why needed here: RPEs are central to the paper's findings, and understanding their matrix representation is crucial for deriving the convergence conditions.
  - Quick check question: How is an RPE represented as a Toeplitz matrix, and what property of this matrix is critical for length extrapolation?

- Concept: Series convergence and its implications in mathematical analysis
  - Why needed here: The core theoretical contribution relies on analyzing the convergence of series derived from RPEs.
  - Quick check question: What is the definition of series convergence, and why does convergence of the RPE's exponential series imply length extrapolation?

## Architecture Onboarding

- Component map: Transformer decoder with RPE module → Softmax attention with RPE biases → Language modeling head
- Critical path: RPE formulation → Series convergence analysis → TRF computation → Length extrapolation validation
- Design tradeoffs: Simple RPEs (like Alibi) vs complex RPEs (like Kerple/Sandwich) in terms of extrapolation capability and computational efficiency
- Failure signatures: Rapidly increasing perplexity when inference sequence length exceeds training length indicates lack of extrapolation
- First 3 experiments:
  1. Implement a basic RPE (e.g., Alibi) and verify convergence of its exponential series
  2. Compute TRF for the implemented RPE and compare with ERF after training
  3. Test length extrapolation by scaling inference sequence length and monitoring perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the convergence rate of the RPE exponential series and the extrapolation capability of transformers? Can we derive tighter bounds on the convergence conditions that guarantee extrapolation?
- Basis in paper: [explicit] The paper states that a transformer is guaranteed to have length extrapolation if the series corresponding to the RPE's exponential converges, but does not provide a quantitative analysis of how the convergence rate affects extrapolation performance.
- Why unresolved: The paper only establishes the sufficiency of convergence but does not explore the quantitative relationship between convergence rate and extrapolation performance.
- What evidence would resolve it: Empirical studies varying the convergence rates of RPEs while measuring extrapolation performance could establish a quantitative relationship. Additionally, theoretical work deriving tighter bounds on the convergence conditions could provide more precise guidance.

### Open Question 2
- Question: How does the proposed Theoretical Receptive Field (TRF) compare to other existing methods for estimating receptive fields in transformers, such as gradient-based approaches or attention pattern analysis?
- Basis in paper: [inferred] The paper introduces TRF as a new method for estimating receptive fields without training, but does not compare it to other existing methods.
- Why unresolved: The paper does not provide a comprehensive comparison of TRF with other receptive field estimation methods, leaving the question of its relative performance open.
- What evidence would resolve it: Empirical studies comparing TRF to other receptive field estimation methods on various datasets and transformer architectures would provide insights into its relative performance and advantages.

### Open Question 3
- Question: What is the impact of different RPE formulations on the extrapolation performance of transformers, beyond the convergence conditions? Are there other factors that contribute to extrapolation capability?
- Basis in paper: [inferred] The paper focuses on the convergence conditions for RPEs but does not explore the impact of other RPE formulations on extrapolation performance.
- Why unresolved: The paper only considers the convergence conditions and does not investigate the role of other RPE properties, such as the specific functional form or the range of positional biases.
- What evidence would resolve it: Empirical studies varying different RPE formulations while keeping the convergence conditions fixed could reveal the impact of other factors on extrapolation performance. Additionally, theoretical analysis of the relationship between RPE properties and extrapolation capability could provide insights.

## Limitations

- Mathematical proof only establishes sufficiency of convergence conditions, not necessity
- Experimental validation limited to autoregressive language modeling tasks
- TRF computation relies on numerical approximations for series without analytical forms

## Confidence

- **High Confidence**: The mathematical derivation of TRF and the convergence-based sufficiency condition for length extrapolation. The experimental demonstration that RPEs with divergent series (1/n, 1/(n ln n)) fail to extrapolate.
- **Medium Confidence**: The practical applicability of TRF as a pre-training diagnostic tool. The generalizability of findings across different RPE families and tasks.
- **Low Confidence**: The necessity of the discovered conditions. The quantitative accuracy of TRF predictions across all RPE variants.

## Next Checks

1. **Formal Necessity Proof**: Complete the mathematical proof of necessity by characterizing the complete set of RPEs that satisfy the convergence condition and demonstrating that no other RPE properties can enable length extrapolation.

2. **Cross-Architecture Validation**: Test TRF predictions and length extrapolation behavior in encoder-decoder transformers, Vision Transformers, and graph neural networks.

3. **Robustness to Implementation Details**: Systematically vary attention scaling factors, numerical precision, and optimization hyperparameters to quantify their impact on TRF accuracy and length extrapolation performance.