---
ver: rpa2
title: On the Multiple Roles of Ontologies in Explainable AI
arxiv_id: '2311.04778'
source_url: https://arxiv.org/abs/2311.04778
tags:
- explanations
- knowledge
- ontologies
- ontology
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies three main perspectives in which ontologies
  can contribute significantly to Explainable AI: reference modelling, common-sense
  reasoning, and knowledge refinement and complexity management. The authors provide
  an overview of existing approaches in the literature and position them according
  to these three proposed perspectives.'
---

# On the Multiple Roles of Ontologies in Explainable AI

## Quick Facts
- arXiv ID: 2311.04778
- Source URL: https://arxiv.org/abs/2311.04778
- Reference count: 40
- Key outcome: This paper identifies three main perspectives in which ontologies can contribute significantly to Explainable AI: reference modelling, common-sense reasoning, and knowledge refinement and complexity management.

## Executive Summary
This paper systematically examines how ontologies can enhance Explainable AI (XAI) by serving as reference models, enabling common-sense reasoning, and supporting knowledge refinement. The authors propose a framework positioning existing literature according to these three perspectives, demonstrating how ontologies can formalize domain knowledge, enrich explanations with semantic context, and adapt explanations to user profiles. The work highlights the potential of ontologies to bridge the gap between complex AI systems and human-understandable explanations while identifying key challenges that remain unresolved.

## Method Summary
The paper conducts a comprehensive literature survey to identify and categorize existing approaches that utilize ontologies in XAI systems. Rather than presenting experimental results, the authors analyze and synthesize research findings to establish three distinct perspectives for ontology application in XAI. They examine theoretical foundations, present illustrative examples from different domains (university enrollment, heart disease diagnosis), and discuss the relationships between ontologies and other knowledge representation approaches. The methodology involves systematic positioning of existing work within the proposed framework and critical analysis of current limitations and future directions.

## Key Results
- Ontologies can serve as reference models that formalize shared conceptualizations of explainable AI systems
- Ontology-based semantic enrichment enables common-sense reasoning to expand explanation consequences
- Knowledge refinement through abstraction relationships allows tailoring explanations to different user expertise levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontologies serve as reference models that formalize shared conceptualizations of explainable AI systems.
- Mechanism: Ontologies encode the domain's essential entities, relations, and axioms in a logical language, enabling interoperability and reuse of components. This formal representation supports reasoning tasks such as subsumption checking and query answering, which map system requirements to structured explanations.
- Core assumption: The domain ontology is logically consistent and covers the necessary concepts and relations for explanation tasks.
- Evidence anchors:
  - [abstract] identifies three perspectives, including "reference modelling" where ontologies provide "sound and consensual models" for explainable systems.
  - [section] notes ontologies "explicitly represent the various components within knowledge-based systems (KBSs), including their interactions (orchestration) and information workflows (choreography)."
  - [corpus] shows related work on knowledge graphs and ontology embeddings for interpretability.
- Break Condition: If the ontology contains inconsistencies or lacks coverage of relevant explanation concepts, the formal mapping between system components and explanations fails.

### Mechanism 2
- Claim: Ontologies enable semantic enrichment of explanations through common-sense reasoning.
- Mechanism: Ontologies ground explanation components in formal axioms and relations, allowing symbolic reasoning to expand the consequences of mappings from learned concepts to ontology entities. This grounding supports analogical reasoning and abstraction, making explanations more aligned with human reasoning.
- Core assumption: There is a valid alignment between statistical model outputs and ontology concepts, and the ontology contains common-sense relations.
- Evidence anchors:
  - [section] discusses how ontologies "serve as a foundation for enriching explanations with context-aware semantic information" and "support common-sense reasoning."
  - [section] describes aligning neural activations with domain ontology concepts and using mappings to guide explanation extraction.
  - [corpus] includes papers on ontology embedding and large language models understanding DL-Lite ontologies.
- Break Condition: If alignment between statistical outputs and ontology concepts is poor, symbolic reasoning cannot produce coherent explanations.

### Mechanism 3
- Claim: Ontologies support knowledge refinement and complexity management to tailor explanations to user profiles.
- Mechanism: Ontologies define abstraction and refinement relationships between concepts. By exploiting these relationships, an explanatory system can adjust the specificity of explanations, replacing technical concepts with more general ones or mapping probabilities to qualitative descriptions based on user expertise.
- Core assumption: The ontology encodes abstraction/refinement relationships that reflect the domain's conceptual hierarchy.
- Evidence anchors:
  - [section] presents an example where ontology relationships are used to replace specific concepts (e.g., "asymptomatic" and "upsloping") with more general ones ("high blood pressure") for lay users.
  - [section] discusses abstraction as a mechanism to "deal with the complexity of the world" and its role in analogical reasoning.
  - [corpus] includes work on ontology embedding and multiperspective ontology management.
- Break Condition: If the ontology lacks appropriate abstraction relationships, explanations cannot be effectively tailored to different user levels.

## Foundational Learning

- Concept: Description Logics and ontology formal semantics
  - Why needed here: The paper relies on DLs (e.g., EL, SROIQ) for defining ontologies and reasoning tasks; understanding subsumption, concept satisfaction, and logical inference is essential for implementing the proposed mechanisms.
  - Quick check question: What is the difference between a TBox axiom and an ABox axiom in a DL ontology?

- Concept: Knowledge graph integration and semantic alignment
  - Why needed here: The approach depends on aligning statistical model outputs with semantic knowledge in ontologies; knowledge of mapping techniques (e.g., ontology matching, feature-concept alignment) is required.
  - Quick check question: How can you map features from a neural network to concepts in a domain ontology?

- Concept: Explanation evaluation and causability metrics
  - Why needed here: The paper highlights the challenge of empirically evaluating explanation human-understandability and causability; familiarity with metrics and protocols (e.g., System Causability Scale) is needed to assess the effectiveness of ontology-based explanations.
  - Quick check question: What is the difference between explainability and causability in the context of XAI?

## Architecture Onboarding

- Component map: Domain Ontology Module -> Alignment Module -> Explanation Generation Module -> User Interface
- Critical path: Input model prediction → alignment to ontology → reasoning and abstraction/refinement → output explanation. Failures in alignment block downstream explanation generation.
- Design tradeoffs: Richer ontologies enable more precise explanations but increase reasoning complexity; lightweight ontologies (e.g., EL) support scalability but may lack expressive power for complex domains.
- Failure signatures: Explanation generation fails if the ontology contains inconsistencies, if alignment is poor, or if abstraction relationships are missing. Reasoning timeouts indicate ontology size or complexity issues.
- First 3 experiments:
  1. Load a simple DL ontology (e.g., EL) and verify concept subsumption and satisfaction reasoning works as expected.
  2. Implement a mock alignment between model features and ontology concepts and test the mapping accuracy on a small dataset.
  3. Create a test case with multiple user profiles and verify the abstraction/refinement mechanism produces appropriately tailored explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically evaluate the human-understandability and effectiveness of ontology-based explanations in XAI systems?
- Basis in paper: [explicit] The paper explicitly identifies this as a key challenge in the discussion section, stating "we highlight the open challenge of empirically evaluating the human-understandability and effectiveness of explanations."
- Why unresolved: While some approaches have been proposed (e.g., Vilone and Longo's hierarchical system, Cromik and Schuessler's tasks for human evaluation), there is no consensus on standardized evaluation methods or metrics for ontology-based explanations.
- What evidence would resolve it: Development and validation of standardized evaluation protocols and metrics specifically designed for ontology-based explanations, along with empirical studies comparing different evaluation approaches.

### Open Question 2
- Question: How can we effectively integrate foundational ontologies with domain-specific ontologies to support common-sense reasoning in XAI explanations?
- Basis in paper: [explicit] The paper discusses the potential of grounding domain ontologies in foundational ontologies to support common-sense reasoning, but notes that "there is no definitive blueprint to follow in this regard."
- Why unresolved: While the concept is promising, the practical implementation of integrating foundational and domain ontologies for common-sense reasoning in XAI is still an open challenge that requires further research and development.
- What evidence would resolve it: Successful case studies demonstrating the integration of foundational and domain ontologies to generate more effective and human-understandable explanations in real-world XAI applications.

### Open Question 3
- Question: How can we address the scalability challenges of using domain-specific ontologies for explanation generation in XAI systems with large feature spaces?
- Basis in paper: [inferred] The paper mentions that creating a domain ontology for each dataset can be "prohibitive to application scenarios with stringent time and scalability constraints," suggesting the need for alternative approaches.
- Why unresolved: The tension between the benefits of domain-specific ontologies for tailored explanations and the practical limitations of scalability remains unresolved, requiring innovative solutions to balance these competing factors.
- What evidence would resolve it: Development and evaluation of scalable methods for generating or adapting domain ontologies in real-time, or techniques for efficiently mapping learned concepts to existing general domain ontologies while maintaining explanation quality.

## Limitations

- The paper is primarily a literature survey without empirical validation of the proposed mechanisms through user studies or systematic evaluations
- Ontological examples provided are simplified and may not capture the complexity of real-world XAI applications
- Specific methods for aligning model outputs to ontologies are mentioned but not detailed or validated
- The scalability challenges of using domain-specific ontologies for large feature spaces are acknowledged but not thoroughly addressed

## Confidence

- Reference Modelling Claims: Medium - The theoretical basis is solid, but practical implementation examples are limited.
- Common-Sense Reasoning Claims: Low-Medium - The mechanism is described, but the quality of alignments and reasoning outcomes is not empirically demonstrated.
- Knowledge Refinement Claims: Medium - The abstraction/refinement concept is clear, but real-world effectiveness and user-specific tailoring are not validated.

## Next Checks

1. Implement the alignment mechanism between a trained ML model and a domain ontology, then measure the accuracy of concept mapping on a validation set.
2. Conduct a user study comparing explanations generated with and without ontology-based abstraction for different expertise levels, measuring comprehension and satisfaction.
3. Test reasoning scalability by measuring explanation generation time and memory usage as ontology size increases from small (10 concepts) to large (1000+ concepts).