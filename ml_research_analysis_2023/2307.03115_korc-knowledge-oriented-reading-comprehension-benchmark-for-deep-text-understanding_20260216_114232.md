---
ver: rpa2
title: 'KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding'
arxiv_id: '2307.03115'
source_url: https://arxiv.org/abs/2307.03115
tags:
- knowledge
- question
- questions
- entity
- korc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KoRC, a Knowledge oriented Reading Comprehension
  benchmark for deep text understanding. The dataset addresses two major limitations
  of existing benchmarks: limited knowledge coverage and narrow answer space.'
---

# KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding

## Quick Facts
- arXiv ID: 2307.03115
- Source URL: https://arxiv.org/abs/2307.03115
- Reference count: 24
- Even the strongest baseline achieves only 68.3% F1 on ID and 30.0% F1 on OOD test sets

## Executive Summary
KoRC is a Knowledge oriented Reading Comprehension benchmark designed to address limitations in existing MRC datasets, specifically their limited knowledge coverage and narrow answer spaces. The benchmark utilizes massive knowledge bases to guide annotators or large language models in constructing knowledge-intensive questions, with answers represented as labels from knowledge bases rather than spans or choices. The dataset comprises 9,074 documents and 31,804 questions, requiring models to connect documents with prior knowledge and reason across both text and background knowledge.

## Method Summary
KoRC employs three annotation strategies: template-based generation, human annotation, and LLM generation, with answers formatted as knowledge base entity labels rather than spans or choices. The benchmark uses documents from Wikipedia linked to entities in Wikidata5M, with reasoning chains weaving document content and background knowledge together. Models are evaluated using penalized exact match accuracy (P-ACC) and penalized F1 measure (P-F1) on both in-distribution (ID) and out-of-distribution (OOD) test sets. The dataset construction involves entity linking using tools like BLINK and XLORE, relation extraction, and reasoning chain generation to create questions that require multi-hop reasoning across documents and knowledge bases.

## Key Results
- Strong baseline models achieve only 68.3% F1 on ID test set and 30.0% F1 on OOD test set
- Models perform better on template-based questions (KORC-T) than human-generated questions (KORC-H)
- LLM-generated questions (KORC-L) provide moderate supervision for answering human-generated questions

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated questions provide moderate supervision to answer human-generated questions. Large language models trained on massive real-world knowledge can generate high-quality questions that capture similar semantic structures and reasoning patterns to human-generated ones. The semantic content and reasoning requirements overlap sufficiently to transfer learning between LLM-generated and human-generated questions.

### Mechanism 2
The flexible answer format with knowledge base labels challenges models beyond simple classification or span extraction. By requiring models to output entities from a knowledge base rather than spans or choices, the benchmark forces genuine knowledge retrieval and reasoning rather than pattern matching. This requires deeper understanding and integration of document content with external knowledge.

### Mechanism 3
Document-KB reasoning chains create questions that cannot be answered by either component alone. Each question requires readers to connect information from the document with background knowledge from Wikidata through reasoning chains, preventing simple document-only or KB-only answering approaches. The reasoning chains create genuine multi-hop reasoning requirements that integrate both sources of information.

## Foundational Learning

- **Entity linking and knowledge base integration**: Why needed - benchmark requires connecting document mentions to entities in Wikidata5M. Quick check - Can you explain how entity linking works between Wikipedia entities and Wikidata entities using tools like BLINK and XLORE?

- **Relation compositionality and reasoning chains**: Why needed - questions require multi-hop reasoning where relations from document compose with background knowledge relations. Quick check - How would you construct a reasoning chain from a document triple using relation compositional rules from BIMR or AnyBURL?

- **Sequence-to-sequence modeling for answer generation**: Why needed - models must generate comma-separated entity labels rather than selecting from predefined options. Quick check - What are the key differences between treating this as a generation task versus a classification task?

## Architecture Onboarding

- **Component map**: Document preprocessing -> Entity linking -> Relation extraction -> Reasoning chain generation -> Question generation -> Model training -> Evaluation
- **Critical path**: Document preparation (entity linking + relation extraction) -> Reasoning chain construction -> Data annotation -> Model training -> Evaluation
- **Design tradeoffs**: Template-based generation offers consistency but lacks diversity; human annotation provides diversity but is expensive; LLM generation balances quality and cost
- **Failure signatures**: Models performing well on template questions but poorly on human questions indicates overfitting to specific patterns; strong performance on ID but weak on OOD indicates memorization rather than generalization
- **First 3 experiments**:
  1. Evaluate baseline model (BART-base) on KORC-H to establish performance floor
  2. Compare template-based vs human vs LLM question difficulty by testing same model across all three versions
  3. Test cross-evaluation by training on one version and evaluating on another to measure transfer learning effectiveness

## Open Questions the Paper Calls Out

1. What are the specific methods used for entity linking in the document preparation step? The paper mentions using BLINK and XLORE but doesn't provide details on how these tools are used or their performance.

2. How does the data annotation process handle cases where the question entity has multiple mentions in the document? The paper mentions anonymizing the question entity but doesn't discuss handling multiple mentions.

3. What is the impact of using different knowledge bases (e.g., Wikidata, DBpedia) on the performance of the models? The paper uses Wikidata but doesn't compare it to other knowledge bases.

4. How does the performance of the models vary with different sizes of the training dataset? The paper mentions using 50% of questions for training but doesn't show performance changes with different training sizes.

5. What are the specific types of questions that are more challenging for the models? The paper mentions relations with fewer questions are more difficult but doesn't provide specific examples.

## Limitations

- Dataset construction relies heavily on quality and coverage of reasoning chains extracted from Wikidata5M, which may introduce systematic biases
- The three annotation strategies show varying quality levels but systematic differences between them aren't fully characterized
- "Penalized" metrics (P-ACC and P-F1) are introduced to address answer variability but their exact formulation and impact require more detailed analysis

## Confidence

**High Confidence Claims:**
- KoRC provides broader knowledge coverage than existing MRC benchmarks through systematic use of massive knowledge bases
- Current strong baseline models achieve only 68.3% F1 on ID and 30.0% F1 on OOD sets, indicating benchmark difficulty
- Flexible answer format with knowledge base labels presents genuine challenges beyond traditional span-based or multiple-choice formats

**Medium Confidence Claims:**
- LLM-generated questions provide moderate supervision for human-generated questions (based on cross-evaluation results)
- Document-KB reasoning chains create questions requiring genuine integration of both sources
- Benchmark successfully addresses limitations of existing MRC benchmarks regarding knowledge coverage and answer space

## Next Checks

1. **Cross-dataset generalization test**: Evaluate models trained on KoRC on established benchmarks like SQuAD or HotpotQA to determine if deeper reasoning skills transfer, and conversely, evaluate models pre-trained on traditional MRC tasks on KoRC to measure knowledge gap.

2. **Error analysis on reasoning chains**: Conduct systematic analysis of model failures by categorizing errors into document comprehension failures, knowledge base retrieval failures, and reasoning chain construction failures to identify which component of multi-hop reasoning is most challenging.

3. **Ablation study on answer format**: Test whether performance gap between KoRC and traditional benchmarks persists when converting KoRC questions to span-based format, to isolate whether difficulty stems from knowledge integration requirements versus answer generation task itself.