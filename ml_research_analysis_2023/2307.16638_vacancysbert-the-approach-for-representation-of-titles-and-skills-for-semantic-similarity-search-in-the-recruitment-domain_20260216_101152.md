---
ver: rpa2
title: 'VacancySBERT: the approach for representation of titles and skills for semantic
  similarity search in the recruitment domain'
arxiv_id: '2307.16638'
source_url: https://arxiv.org/abs/2307.16638
tags:
- skills
- https
- title
- titles
- been
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of normalizing job titles in
  recruitment by developing a novel deep learning approach called VacancySBERT. The
  method trains a Siamese neural network to represent job titles and associated skills
  in a shared vector space, using distant supervision from co-occurrence data between
  titles and skills.
---

# VacancySBERT: the approach for representation of titles and skills for semantic similarity search in the recruitment domain

## Quick Facts
- arXiv ID: 2307.16638
- Source URL: https://arxiv.org/abs/2307.16638
- Reference count: 31
- Key outcome: VacancySBERT significantly outperforms baseline models in job title normalization, achieving 10% and 21.5% improvements in Recall@1,5,10 when skills are used as additional features.

## Executive Summary
This paper addresses the challenge of normalizing job titles in recruitment by developing VacancySBERT, a deep learning approach that trains a Siamese neural network to represent job titles and associated skills in a shared vector space. The method leverages distant supervision from co-occurrence data between titles and skills, eliminating the need for explicit human labels. A novel dataset of 30k+ title-skill-normalized title triplets was created and made publicly available. Experimental results demonstrate significant improvements over baseline models, particularly when skills are incorporated during inference.

## Method Summary
The approach uses a Siamese neural network with BERT-base encoder, incorporating skills through a special [SKILL] token and pooling layer. The model is trained using Multiple Negative Ranking Loss with in-batch negatives on large-scale title-description pairs and anonymized work history data. Skills are extracted using a proprietary algorithm and aggregated through average pooling before being concatenated with title embeddings. The model learns to map title-skill pairs close together in vector space while pushing apart unrelated pairs, enabling semantic similarity search for job title normalization.

## Key Results
- VacancySBERT achieves 10% improvement in Recall@1,5,10 over baseline models without skills
- Incorporating skills during inference provides an additional 21.5% improvement in Recall@1,5,10
- The method effectively normalizes free-form job titles to standardized forms using semantic similarity
- A new dataset of 30k+ title-skill-normalized title triplets was created and made publicly available

## Why This Works (Mechanism)

### Mechanism 1
Distant supervision from co-occurrence between titles and skills allows the model to learn semantic relationships without explicit human labels. The model leverages large-scale title-description pairs and anonymized work history data to infer implicit connections between job titles and their associated skills. Skills mentioned in job descriptions are strong indicators of the job title's semantic meaning and can serve as effective supervision signals. Break condition: If the correlation between skills and titles is weak or inconsistent across industries, the learned representations may become noisy and less generalizable.

### Mechanism 2
Incorporating skills as additional features during inference improves recall by providing disambiguating context. Skills are tokenized and represented using a special [SKILL] token, then aggregated and concatenated with the title embedding before passing through a linear pooling layer. This enriched representation captures both the title's semantic meaning and the specific skill requirements. Break condition: If skill extraction is inaccurate or the skill set is too sparse, the additional features may add noise rather than improve performance.

### Mechanism 3
Using Multiple Negative Ranking Loss with in-batch negatives enables effective learning of semantic similarity in an imbalanced dataset. The model learns to map title-skill pairs close together in vector space while pushing apart unrelated pairs, using the batch itself to provide negative examples. This approach is particularly effective for the imbalanced job title distribution. Break condition: If the batch size is too small to provide meaningful negative samples, or if the dataset lacks sufficient diversity, the ranking loss may not effectively learn semantic distinctions.

## Foundational Learning

- **Siamese neural networks for semantic similarity**: Why needed: The task requires comparing job titles to find semantically similar matches, which is a natural fit for Siamese architectures that learn to map similar items close together in vector space. Quick check: Can you explain how a Siamese network differs from a standard classification network when handling similarity tasks?

- **Pre-trained language models and fine-tuning**: Why needed: BERT-base provides a strong starting point for semantic understanding, and fine-tuning it on the specific domain of job titles and skills allows the model to capture industry-specific terminology and relationships. Quick check: What are the advantages of initializing from pre-trained weights versus training from scratch for this application?

- **Distant supervision and weak labeling**: Why needed: Manual annotation of job title relationships is prohibitively expensive and time-consuming, so leveraging co-occurrence patterns from large-scale data provides an effective alternative. Quick check: How does distant supervision differ from traditional supervised learning, and what are the potential risks?

## Architecture Onboarding

- **Component map**: Input processing (BERT tokenizer with [SKILL] token) -> Encoder (BERT-base) -> Skill aggregation (average pooling) -> Linear pooling layer -> Output (768-dimensional embeddings)

- **Critical path**: Title → BERT → [CLS] token → Title embedding → Similarity computation with skills embedding

- **Design tradeoffs**: Base vs. Large BERT: Chose base for efficiency, accepting potentially lower performance; Skill inclusion: Optional during inference, providing flexibility between speed and accuracy; Maximum sequence length: Set to 128 tokens for skills, balancing coverage and computational cost

- **Failure signatures**: Poor performance on rare job titles: Indicates insufficient negative sampling or imbalanced training data; Inconsistent skill extraction: Suggests issues with the proprietary skill identification algorithm; High computational cost: May indicate need to optimize batch size or sequence length

- **First 3 experiments**: 1) Train VacancySBERT without skills to establish baseline performance and verify the core Siamese architecture works; 2) Add skills as additional features during inference and measure improvement in recall metrics; 3) Experiment with different skill aggregation methods (max pooling vs. average pooling) to optimize skill representation quality

## Open Questions the Paper Calls Out

1. How would a better negative sampling strategy specifically improve the performance of VacancySBERT, and what alternatives to the current in-batch negative sampling could be explored? The authors explicitly mention developing a better negative sampling strategy as future work but do not specify what constitutes a "better" strategy or test alternatives.

2. How would extending VacancySBERT to multilingual settings affect its performance on job title normalization across different languages? The authors plan to extend the approach for multilingual settings as future work but only evaluate on English job titles without exploring cross-lingual transfer or multilingual training.

3. How does the importance and representativeness of individual skills affect the performance of VacancySBERT, and could TF-IDF or other weighting schemes improve results? The authors suggest scoring skills by importance and representativeness using TF-IDF as future work but treat all skills equally during training and inference without considering their relative importance.

## Limitations

- The study relies on proprietary datasets and a proprietary skill extraction algorithm, limiting reproducibility and independent validation.
- The model is trained and evaluated only on English-language data, raising concerns about generalizability to other languages and industries.
- The evaluation focuses solely on recall-based metrics without addressing precision at top ranks, semantic drift over time, or performance with rare/emerging job titles.

## Confidence

- **VacancySBERT performance improvements (High confidence)**: Well-supported by experimental results with clear methodology and substantial improvements.
- **Skills enhance title representations (Medium confidence)**: Supported by experimental data but mechanism is less thoroughly validated without exploring noise scenarios.
- **Distant supervision effectiveness (Medium confidence)**: Claim is less directly validated without comparison to supervised alternatives or quantification of noise impact.

## Next Checks

1. **Cross-domain generalization test**: Evaluate VacancySBERT on job title normalization datasets from different countries, languages, or industries to assess transfer beyond the original training domain.

2. **Ablation study on skill extraction quality**: Systematically vary the quality and quantity of extracted skills to determine thresholds where skill-enhanced representations stop providing benefits.

3. **Temporal stability analysis**: Test VacancySBERT's performance on historical job title data to assess how well the model handles semantic drift and emergence of new job titles over time.