---
ver: rpa2
title: 'GSQA: An End-to-End Model for Generative Spoken Question Answering'
arxiv_id: '2312.09781'
source_url: https://arxiv.org/abs/2312.09781
tags:
- spoken
- speech
- generative
- question
- extractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GSQA, the first textless end-to-end generative
  model for spoken question answering. The key innovation is transferring knowledge
  from text-based generative QA models to speech models using discrete units derived
  from HuBERT.
---

# GSQA: An End-to-End Model for Generative Spoken Question Answering

## Quick Facts
- arXiv ID: 2312.09781
- Source URL: https://arxiv.org/abs/2312.09781
- Reference count: 0
- Primary result: GSQA achieves 3% improvement over extractive baselines on extractive QA tasks and performs competitively on abstractive QA tasks without direct training on spoken abstractive data.

## Executive Summary
GSQA introduces the first textless end-to-end generative model for spoken question answering, eliminating the need for intermediate text transcription. The approach leverages HuBERT discrete units as input tokens and transfers knowledge from text-based generative QA models through initialization and fine-tuning. The model demonstrates stable performance under high ASR error rates while using fewer parameters than cascade models, achieving state-of-the-art results on extractive QA and competitive performance on abstractive QA tasks.

## Method Summary
GSQA converts speech to discrete units using HuBERT-based k-means clustering, then processes these units through a LongT5 encoder-decoder initialized on text QA datasets. The model is fine-tuned on an extractive spoken QA dataset (NMSQA) and evaluated on both extractive and abstractive QA tasks. During inference, beam search decoding generates answer units that are converted back to speech using a HiFi-GAN vocoder.

## Key Results
- 3% improvement over extractive baselines on NMSQA extractive QA tasks
- Competitive BLEU1 and ROUGE-L scores on Spoken-NarrativeQA abstractive QA tasks
- Stable performance under high ASR error rates compared to cascade models
- Fewer parameters than cascade approaches while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete HuBERT units preserve semantic content while enabling textless QA.
- Mechanism: Speech signals are mapped to discrete units via k-means clustering on HuBERT hidden states. These units serve as input tokens for the generative model, allowing semantic reasoning without intermediate text.
- Core assumption: The discrete unit space retains enough semantic detail to support QA tasks.
- Evidence anchors:
  - [abstract] "We utilize HuBERT to convert the input speech (question and passage) into discrete units[6]."
  - [section] "The HuBERT[8] model serves as a formidable speech representation model...Every 20ms frame is clustered into one of the K categories."
  - [corpus] Weak evidence: corpus lacks similar models using HuBERT-derived units for QA, so semantic preservation remains a model-specific assumption.
- Break condition: If clustering resolution is too coarse, units lose critical semantic distinctions; if too fine, sequence lengths become unmanageable.

### Mechanism 2
- Claim: Pretraining on textual QA data transfers semantic understanding to the spoken domain.
- Mechanism: A LongT5 model is first pretrained on extractive and abstractive text QA datasets. This model is then fine-tuned on spoken extractive QA data, inheriting the semantic reasoning ability.
- Core assumption: Semantic knowledge is largely modality-agnostic and can be transferred via fine-tuning.
- Evidence anchors:
  - [abstract] "We propose using text models for initialization and leveraging the extractive QA dataset to transfer knowledge from the text generative model to the spoken generative model."
  - [section] "To boost the logical understanding capabilities...we initialize this generative spoken QA model using weights from a textual model [14]."
  - [corpus] Moderate evidence: corpus includes related works on transfer from text to speech models, but none specifically for QA.
- Break condition: If semantic gaps between text and speech units are too large, fine-tuning fails to align representations.

### Mechanism 3
- Claim: End-to-end speech-to-answer generation reduces ASR error propagation.
- Mechanism: By bypassing ASR, the model directly generates answer units from speech, avoiding transcription errors that degrade cascade models.
- Core assumption: The discrete unit representation is robust to noise, so speech-to-unit conversion introduces less degradation than ASR.
- Evidence anchors:
  - [abstract] "It improves the model robustness in noisy speech."
  - [section] "By adopting this textless approach, DUAL effectively sidesteps the error propagation from ASR systems [9, 10, 11]."
  - [corpus] Limited evidence: corpus lacks direct ASR robustness comparisons for this approach.
- Break condition: If the speech-to-unit model is less robust than ASR, end-to-end performance degrades.

## Foundational Learning

- Concept: HuBERT clustering and discrete unit quantization
  - Why needed here: Forms the bridge between continuous speech and discrete model input.
  - Quick check question: How does k-means clustering on HuBERT embeddings produce discrete speech tokens?

- Concept: Transfer learning from text to speech QA
  - Why needed here: Enables semantic reasoning without requiring large spoken abstractive datasets.
  - Quick check question: What initialization step allows GSQA to generalize to abstractive tasks?

- Concept: Sequence-to-sequence modeling with discrete units
  - Why needed here: Allows generation of answer units directly from speech question/passage units.
  - Quick check question: How does the model map input speech units to output answer units?

## Architecture Onboarding

- Component map: Speech input → HuBERT encoder → k-means clustering → discrete unit sequence → LongT5 encoder-decoder → answer unit sequence → HiFi-GAN vocoder → speech output.
- Critical path: Speech quantization → model inference → unit-to-speech conversion.
- Design tradeoffs: Discrete units reduce input length but may lose nuance; fewer parameters reduce cost but limit capacity.
- Failure signatures: ASR-like degradation if units are too coarse; model collapse if fine-tuning is insufficient; semantic drift if initialization is poor.
- First 3 experiments:
  1. Test discrete unit generation fidelity on a held-out speech sample.
  2. Validate that TQA initialization improves F1 over random initialization on NMSQA.
  3. Compare BLEU/ROUGE on Spoken-NarrativeQA with and without fine-tuning on extractive data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GSQA compare to a fully cascaded approach with high-quality ASR in real-world noisy environments?
- Basis in paper: [explicit] The paper discusses the sensitivity of cascade models to ASR errors and mentions GSQA's stability under high WER, but does not provide real-world noisy environment comparisons.
- Why unresolved: The experiments primarily use synthesized data and do not test GSQA against a fully cascaded approach with high-quality ASR in real-world noisy conditions.
- What evidence would resolve it: Conducting experiments in real-world noisy environments with varying WERs and comparing GSQA's performance to a cascaded approach with high-quality ASR would provide the necessary evidence.

### Open Question 2
- Question: Can GSQA be effectively adapted to handle other languages beyond English, and what are the limitations?
- Basis in paper: [inferred] The paper does not discuss multilingual capabilities or limitations of GSQA.
- Why unresolved: The model is trained and evaluated only on English datasets, leaving its performance and limitations in other languages unexplored.
- What evidence would resolve it: Training and evaluating GSQA on multilingual datasets and analyzing its performance across different languages would clarify its adaptability and limitations.

### Open Question 3
- Question: What are the computational trade-offs when scaling GSQA to handle longer and more complex passages?
- Basis in paper: [explicit] The paper mentions that GSQA uses fewer parameters than cascade models but does not discuss computational trade-offs for scaling.
- Why unresolved: While GSQA is efficient, its scalability and computational demands for handling longer and more complex passages are not addressed.
- What evidence would resolve it: Conducting experiments to measure computational resources (e.g., memory, time) required by GSQA when processing longer and more complex passages would provide insights into its scalability.

## Limitations
- Limited evaluation scope: Tested only on two small spoken QA datasets (NMSQA and Spoken-NarrativeQA)
- Discrete unit fidelity assumption: Semantic preservation through HuBERT clustering not empirically validated
- Transfer learning assumptions: Cross-task transfer effectiveness lacks theoretical justification
- Robustness claims without ASR comparison: No direct comparison against cascade models under controlled ASR error conditions

## Confidence
- **High confidence**: The end-to-end architecture using discrete HuBERT units for speech-to-answer generation is technically sound and the implementation details are clearly specified.
- **Medium confidence**: The 3% improvement over extractive baselines on NMSQA is credible but may not generalize; the abstractive QA performance on Spoken-NarrativeQA is promising but based on limited evaluation.
- **Low confidence**: Claims about robustness to noisy speech and error propagation reduction compared to cascade models lack direct comparative evidence.

## Next Checks
1. **Unit semantic preservation validation**: Conduct an ablation study varying the number of HuBERT clusters (e.g., 50, 100, 200) and measure impact on extractive QA F1 scores to determine optimal resolution.
2. **ASR robustness comparison**: Implement a cascade baseline (ASR + text QA model) and compare F1 scores on NMSQA when adding controlled synthetic noise to speech input.
3. **Cross-domain generalization test**: Evaluate the model on an additional spoken QA dataset from a different domain (e.g., podcasts or lectures) to assess real-world applicability beyond the two evaluated datasets.