---
ver: rpa2
title: 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey'
arxiv_id: '2311.07914'
source_url: https://arxiv.org/abs/2311.07914
tags:
- knowledge
- arxiv
- language
- llms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper surveys knowledge-graph-augmented approaches to mitigating
  hallucinations in LLMs. It categorizes these approaches into three types: knowledge-aware
  inference, knowledge-aware learning, and knowledge-aware validation.'
---

# Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey

## Quick Facts
- arXiv ID: 2311.07914
- Source URL: https://arxiv.org/abs/2311.07914
- Reference count: 18
- Primary result: Knowledge-graph-augmented approaches can significantly reduce hallucinations in LLMs, with methods like KAPING achieving large accuracy gains especially on smaller models.

## Executive Summary
This survey examines how knowledge graphs can be used to mitigate hallucinations in large language models (LLMs) by providing structured external knowledge. The paper categorizes approaches into three types: knowledge-aware inference (using retrieval, reasoning, and controlled generation), knowledge-aware learning (pre-training and fine-tuning with knowledge), and knowledge-aware validation (fact-checking outputs). Methods like retrieval-augmented generation (RAG), chain-of-thought reasoning with knowledge retrieval, and knowledge-guided fine-tuning have shown significant improvements in accuracy and factual consistency. However, challenges remain around computational cost, dependency on knowledge graph quality, and limited generalization across tasks.

## Method Summary
The paper surveys existing knowledge-graph-augmented approaches for hallucination mitigation by analyzing methods across three categories: knowledge-aware inference, learning, and validation. The survey methodology involves reviewing academic literature to identify approaches that integrate knowledge graphs with LLMs, categorizing them by their primary mechanism, and synthesizing their reported performance and limitations. The analysis focuses on techniques like retrieval-augmentation, chain-of-thought reasoning with knowledge retrieval, knowledge-guided fine-tuning, and fact-checking validation methods.

## Key Results
- Knowledge-aware inference approaches like retrieval-augmented generation (RAG) and chain-of-thought with knowledge retrieval significantly reduce hallucinations by providing contextually relevant external knowledge.
- Knowledge-aware learning methods including fine-tuning on knowledge graph triples can enhance LLM performance on domain-specific tasks.
- Knowledge-aware validation techniques like fact-checking with knowledge graphs provide post-hoc verification of LLM outputs.
- KAPING demonstrated large accuracy gains, particularly on smaller models, showing the effectiveness of knowledge graph augmentation.
- Challenges include computational overhead, dependency on knowledge graph quality, and limited generalization across different tasks and domains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation (RAG) mitigates hallucinations by providing LLMs with real-time, contextually relevant external knowledge.
- Mechanism: During inference, a retriever module fetches top-k documents or knowledge graph triples based on the input query, which are then concatenated to the input prompt before passing to the LLM for generation.
- Core assumption: The retrieved external knowledge is both accurate and relevant to the input query.
- Evidence anchors:
  - [abstract] The paper notes that retrieval-augmented generation models like RAG and RALM have effectively mitigated hallucination issues by providing relevant documents to LLMs during generation.
  - [section] RALM model retrieves top-k documents and concatenates them to the input, while KAPING retrieves relevant knowledge triples from KGs and augments them to prompts.
  - [corpus] Weak evidence: The corpus neighbors include "A Survey on RAG Meeting LLMs" which suggests RAG is a relevant area, but no direct evidence is provided in the corpus for hallucination mitigation.
- Break condition: If the retriever module fails to fetch accurate or relevant documents, the LLM may generate hallucinations based on incorrect external knowledge.

### Mechanism 2
- Claim: Chain-of-thought (CoT) prompting with knowledge graphs improves reasoning by breaking down complex queries into intermediate steps and guiding the LLM with relevant external knowledge.
- Mechanism: CoT prompting provides a series of intermediate reasoning steps, and when combined with knowledge graph retrieval, it interleaves knowledge retrieval with reasoning steps to guide the LLM iteratively.
- Core assumption: The LLM can effectively use the intermediate reasoning steps and external knowledge to arrive at accurate conclusions.
- Evidence anchors:
  - [abstract] The paper mentions that CoT, chain-of-thought with self-consistency, and reasoning on graphs are used to improve the complex reasoning ability of LLMs.
  - [section] IRCoT interleaves CoT generation and knowledge retrieval from the knowledge graph to guide the retrieval and reasoning iteratively for multi-step reasoning questions.
  - [corpus] Weak evidence: The corpus includes "Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification" which suggests CoT is relevant, but no direct evidence is provided for hallucination mitigation.
- Break condition: If the LLM fails to generate meaningful intermediate steps or the retrieved knowledge is not effectively utilized, the reasoning process may still lead to hallucinations.

### Mechanism 3
- Claim: Knowledge graph-augmented fine-tuning enhances LLM performance by retraining the model on task-specific knowledge, thereby reducing hallucinations.
- Mechanism: Fine-tuning involves retraining a pre-trained LLM on a dataset that includes knowledge graph triples or synthetic sentences converted from KG triples, allowing the model to learn domain-specific information.
- Core assumption: The fine-tuning process effectively propagates the new knowledge into the model's weights, improving its performance on specific tasks.
- Evidence anchors:
  - [abstract] The paper discusses knowledge-aware learning, which includes knowledge-aware pre-training and fine-tuning to address hallucination issues.
  - [section] SKILL uses WikiData and KELM corpus synthetic sentences converted from KG triples to fine-tune the pre-trained model checkpoints. KGLM fine-tunes RoBERTaLarge on Wordnet and Freebase knowledge graphs for link prediction tasks.
  - [corpus] No direct evidence in the corpus for knowledge graph-augmented fine-tuning and hallucination mitigation.
- Break condition: If the fine-tuning process is not effective in propagating the new knowledge, or if the model overfits to the fine-tuning data, hallucinations may still occur.

## Foundational Learning

- Concept: Knowledge Graphs
  - Why needed here: Knowledge graphs provide a structured representation of real-world entities and their relationships, which can be used to augment LLMs with external knowledge to reduce hallucinations.
  - Quick check question: What are the key components of a knowledge graph and how do they relate to each other?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG combines the strengths of parametric memory from pre-trained models and non-parametric memory from retrieved documents, allowing LLMs to access up-to-date information and reduce hallucinations.
  - Quick check question: How does RAG differ from traditional LLM inference, and what are the benefits of using RAG?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting guides LLMs through a series of intermediate reasoning steps, improving their ability to solve complex problems and reducing hallucinations.
  - Quick check question: How does CoT prompting work, and what are some examples of tasks where it can be beneficial?

## Architecture Onboarding

- Component map: LLM -> Retriever module -> Knowledge graph -> Generator -> Output
- Critical path: Input query → Retriever module → Fetched knowledge → LLM with augmented input → Generated output
- Design tradeoffs: The choice between retrieval-augmentation, CoT prompting, and fine-tuning depends on the specific use case, available resources, and desired trade-offs between accuracy, efficiency, and flexibility.
- Failure signatures: If the retriever module fails to fetch accurate or relevant knowledge, or if the LLM fails to effectively use the augmented input, hallucinations may occur. Additionally, if the fine-tuning process is not effective, the model may still generate hallucinations.
- First 3 experiments:
  1. Implement a simple RAG system using a pre-trained LLM and a knowledge graph, and evaluate its performance on a question-answering task.
  2. Compare the performance of CoT prompting with and without knowledge graph augmentation on a complex reasoning task.
  3. Fine-tune a pre-trained LLM on a task-specific knowledge graph and evaluate its performance on a domain-specific question-answering task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge graphs be made more context-aware to better align with the operational contexts of language models?
- Basis in paper: [explicit] The paper discusses the importance of tailoring knowledge graphs to be context-aware and explores probabilistic reasoning and knowledge representations to address uncertainty within the knowledge graph.
- Why unresolved: The paper mentions the need for context-aware knowledge graphs but does not provide specific methods or solutions for achieving this.
- What evidence would resolve it: Research demonstrating effective methods for incorporating context-awareness into knowledge graphs, such as context-specific weighting or adaptive knowledge graph structures, would resolve this question.

### Open Question 2
- Question: To what extent does the injected knowledge propagate within the model when using existing methods of updating knowledge?
- Basis in paper: [explicit] The paper discusses the challenges of knowledge propagation in models when updating knowledge through fine-tuning or other methods, citing Onoe et al.'s findings on the difficulty of performing inference based on newly injected facts.
- Why unresolved: The paper highlights the lack of clarity on how far the injected knowledge spreads within the model, indicating a need for further investigation into the propagation mechanisms.
- What evidence would resolve it: Empirical studies comparing the performance of models before and after knowledge injection, along with analysis of how the knowledge spreads across different layers or components of the model, would provide insights into the extent of knowledge propagation.

### Open Question 3
- Question: How can adversarial training be effectively incorporated to expose and address weaknesses related to hallucinations in language models?
- Basis in paper: [explicit] The paper suggests the incorporation of adversarial training as a means to expose weaknesses related to hallucinations and strengthen the model's resilience.
- Why unresolved: While the paper mentions the potential benefits of adversarial training, it does not provide specific strategies or techniques for implementing adversarial examples tailored to hallucinations.
- What evidence would resolve it: Research demonstrating the effectiveness of adversarial training methods in reducing hallucinations, along with detailed methodologies for generating adversarial examples specific to hallucination scenarios, would resolve this question.

## Limitations
- The effectiveness of knowledge-graph-augmented approaches heavily depends on the quality and coverage of the underlying knowledge graphs, which can vary significantly across domains.
- Many proposed methods show promise in controlled experiments but lack extensive real-world validation across diverse use cases.
- The survey highlights computational challenges, particularly the overhead introduced by knowledge graph retrieval and integration, which may limit practical deployment in resource-constrained environments.

## Confidence
- **High confidence**: The categorization framework (knowledge-aware inference, learning, and validation) is well-supported by the literature and provides a coherent structure for understanding these approaches. The general mechanisms of retrieval-augmentation and CoT prompting are empirically validated across multiple studies.
- **Medium confidence**: Claims about specific performance improvements (e.g., KAPING's gains on smaller models) are based on reported results but may not generalize across all domains or knowledge graph types. The survey itself synthesizes existing work rather than presenting new empirical findings.
- **Low confidence**: Some challenges mentioned, such as the dependency on knowledge graph quality and limited generalization, are noted but not extensively quantified across the surveyed approaches.

## Next Checks
1. Replicate core findings: Implement a basic RAG system using a standard LLM and knowledge graph, then measure hallucination reduction on a benchmark dataset to verify the mechanism described.
2. Stress test knowledge quality dependency: Systematically vary the quality and coverage of the knowledge graph used in retrieval-augmentation and measure the impact on hallucination rates across different domains.
3. Cross-task generalization evaluation: Test a knowledge-graph-augmented approach (e.g., IRCoT) on a suite of diverse tasks beyond its original evaluation to assess generalization claims and identify failure patterns.