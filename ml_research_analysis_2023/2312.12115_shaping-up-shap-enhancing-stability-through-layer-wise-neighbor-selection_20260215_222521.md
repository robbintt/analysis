---
ver: rpa2
title: 'Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection'
arxiv_id: '2312.12115'
source_url: https://arxiv.org/abs/2312.12115
tags:
- shap
- values
- layer
- st-shap
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability issue in Kernel SHAP, a widely
  used model-agnostic feature attribution method for explaining black-box machine
  learning models. The authors demonstrate that Kernel SHAP's instability stems from
  its stochastic neighbor selection process.
---

# Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection

## Quick Facts
- arXiv ID: 2312.12115
- Source URL: https://arxiv.org/abs/2312.12115
- Reference count: 26
- Primary result: Achieves perfect stability (Jaccard coefficient of 1.0) for complete layers while maintaining high fidelity and providing results comparable to exact SHAP values

## Executive Summary
This paper addresses the instability issue in Kernel SHAP, a widely used model-agnostic feature attribution method for explaining black-box machine learning models. The authors demonstrate that Kernel SHAP's instability stems from its stochastic neighbor selection process and propose a layer-wise neighbor selection strategy to ensure full stability without compromising explanation fidelity. Additionally, they introduce a novel feature attribution method based on coalitions of Layer 1, which is fully stable, computationally efficient, and retains desirable theoretical properties.

## Method Summary
The paper introduces a layer-wise neighbor selection strategy that ensures full stability in Kernel SHAP by completing entire layers of coalitions before moving to the next layer, rather than randomly sampling across layers. This approach eliminates the randomness that causes instability while maintaining explanation fidelity. The authors also propose a novel Layer 1 attribution method that uses only single-feature coalitions to compute attribution scores, resulting in a fully stable, efficient, and theoretically sound alternative to traditional Shapley value computations.

## Key Results
- Achieves perfect stability (Jaccard coefficient of 1.0) for complete layers while maintaining high fidelity
- Layer 1 attribution method is up to three orders of magnitude faster than computing exact SHAP values
- Results comparable to exact SHAP values with significantly improved stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The instability in Kernel SHAP arises from stochastic neighbor selection during coalition generation.
- Mechanism: Kernel SHAP uses weighted linear regression on a random sample of coalitions. When budget is insufficient to fill a layer completely, random sampling occurs across multiple layers, introducing variability across executions.
- Core assumption: Random sampling across diverse layers creates different coalition sets for the same instance.
- Evidence anchors:
  - [abstract] "Kernel SHAP's instability stems from its stochastic neighbor selection process."
  - [section] "Kernel SHAP's neighbor strategy introduces a lot of randomness in the process... Those samples can be very different across multiple executions."
  - [corpus] "SHAP-Guided Kernel Actor-Critic for Explainable Reinforcement Learning" uses similar random coalition selection strategies.
- Break condition: If neighbor selection is made deterministic by completing layers before sampling.

### Mechanism 2
- Claim: Restricting neighbor generation to Layer 1 coalitions yields stable, efficient, and theoretically sound attribution scores.
- Mechanism: Layer 1 contains only single-feature present/absent coalitions. By using only these, the attribution values are computed from a fixed, small set of coalitions, ensuring determinism and reducing computation.
- Core assumption: The marginal contributions from Layer 1 coalitions are sufficient to approximate Shapley values closely.
- Evidence anchors:
  - [abstract] "by restricting the neighbors generation to perturbations of size 1... we obtain a novel feature-attribution method that is fully stable, efficient to compute, and still meaningful."
  - [section] "learning a surrogate on the coalitions of Layer 1 not only offers complete stability and interesting fidelity, but it is also very fast to compute."
  - [corpus] "Alternative Methods to SHAP Derived from Properties of Kernels" explores kernel-based attribution alternatives.
- Break condition: If Layer 1 attributions diverge significantly from exact Shapley values for the use case.

### Mechanism 3
- Claim: Layer 1 attribution scores satisfy LES (Linearity, Efficiency, Symmetry) properties, making them a valid attribution method.
- Mechanism: The closed-form expression derived from Layer 1 coalitions inherently satisfies the LES axioms, aligning with theoretical expectations for fair attribution.
- Core assumption: The formula ϕj = f({j})−f(∅)+f(N)−f(N\{j})2 + adjustment term preserves LES properties.
- Evidence anchors:
  - [section] "the proposed attribution method belongs to the LES family of attribution scores."
  - [section] "LES values are based on marginal contributions, providing feature contributions and interpretations very close to the Shapley values."
  - [corpus] "Fair and Explainable Credit-Scoring under Concept Drift" discusses fairness properties in attribution.
- Break condition: If empirical results show poor correlation with exact Shapley values or high instability.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values.
  - Why needed here: The paper builds on Shapley values to define attribution scores; understanding marginal contributions is key.
  - Quick check question: How does the Shapley value compute a player's contribution in a cooperative game?

- Concept: Model-agnostic explainability and local post-hoc methods.
  - Why needed here: Kernel SHAP is a model-agnostic method; knowing the difference between global and local explanations matters.
  - Quick check question: What distinguishes a local post-hoc explanation from a global model explanation?

- Concept: Linear regression and weighted least squares.
  - Why needed here: Kernel SHAP uses weighted linear regression on coalitions; understanding this is essential for grasping the approximation.
  - Quick check question: In weighted least squares, how do the weights affect the fitted coefficients?

## Architecture Onboarding

- Component map: Black-box model -> Layer-wise coalition generator -> Weighted linear regression -> Attribution scores
- Critical path:
  1. Load model and data.
  2. Generate coalitions (Layer 1 for ST-SHAP, or as per budget for SHAP).
  3. Compute predictions for each coalition.
  4. Fit weighted linear regression (or compute Layer 1 formula).
  5. Extract attribution scores.
  6. Evaluate stability and fidelity.

- Design tradeoffs:
  - Stability vs. fidelity: Complete layers increase stability but may reduce diversity.
  - Speed vs. accuracy: Layer 1 is fast but may approximate Shapley values less precisely.
  - Complexity vs. interpretability: More coalitions improve fidelity but make explanations harder to interpret.

- Failure signatures:
  - Low Jaccard/VSI: Random sampling introduced too much variance.
  - Poor R²/accuracy: Linear surrogate does not fit the black-box well.
  - High runtime: Budget too large or coalitions not filtered efficiently.

- First 3 experiments:
  1. Run ST-SHAP vs. SHAP on a small dataset (e.g., Boston) with budget=30; check Jaccard=1.0 for ST-SHAP.
  2. Compare Layer 1 attribution scores to exact Shapley values on Boston; compute Kendall τ and R².
  3. Time ST-SHAP Layer 1 vs. Kernel SHAP (budget=2000) on Adult Income; verify order-of-magnitude speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the layer-wise neighbor selection strategy perform when applied to other post-hoc explainability methods beyond Kernel SHAP, such as LIME or Anchors?
- Basis in paper: [explicit] The paper focuses on improving Kernel SHAP's stability through layer-wise neighbor selection, suggesting potential applicability to other methods.
- Why unresolved: The paper does not explore the application of this strategy to other explainability methods.
- What evidence would resolve it: Experiments applying the layer-wise neighbor selection strategy to other explainability methods and comparing their stability and fidelity to the original methods.

### Open Question 2
- Question: What is the impact of the layer-wise neighbor selection strategy on the computational efficiency of Kernel SHAP when dealing with very high-dimensional datasets (e.g., thousands of features)?
- Basis in paper: [inferred] The paper demonstrates improved stability and maintained fidelity with the layer-wise strategy, but does not explicitly discuss its impact on computational efficiency for high-dimensional datasets.
- Why unresolved: The paper's experiments focus on datasets with a moderate number of features (up to 57), not exploring the strategy's performance with very high-dimensional data.
- What evidence would resolve it: Experiments comparing the execution times of Kernel SHAP with and without the layer-wise strategy on high-dimensional datasets.

### Open Question 3
- Question: How does the stability of Kernel SHAP with the layer-wise neighbor selection strategy vary across different types of black-box models, such as those with non-linear decision boundaries or ensemble methods?
- Basis in paper: [explicit] The paper evaluates the strategy on various black-box models, including SVM, Random Forest, Logistic Regression, and Multi-layer Perceptron, but does not explicitly analyze stability variations across model types.
- Why unresolved: The paper presents overall stability results but does not delve into how stability might differ based on the characteristics of the underlying black-box model.
- What evidence would resolve it: Experiments comparing the stability of Kernel SHAP with the layer-wise strategy across a wider range of black-box model types, including those with different decision boundary characteristics.

## Limitations

- The layer-wise strategy may reduce coalition diversity, potentially affecting fidelity in edge cases.
- Layer 1 attribution may not capture complex feature interactions as accurately as full Shapley value computations for models with strong non-linearities.
- Experiments focus on datasets with moderate feature counts; scalability to high-dimensional data remains untested.

## Confidence

- **High confidence**: The mechanism linking instability to stochastic sampling is well-supported by theoretical and empirical evidence.
- **Medium confidence**: The Layer 1 attribution method's LES properties are theoretically sound, but real-world performance may vary with model complexity.
- **Medium confidence**: The claim of up to three orders of magnitude speedup is based on experiments with small datasets; larger-scale validation is needed.

## Next Checks

1. **Scalability Test**: Evaluate ST-SHAP and Layer 1 attribution on high-dimensional datasets (e.g., >100 features) to confirm stability and efficiency claims.
2. **Edge Case Analysis**: Test on models with strong feature interactions (e.g., deep neural networks) to assess fidelity trade-offs.
3. **Runtime Benchmarking**: Measure runtime differences between ST-SHAP and exact SHAP on larger datasets to validate the speedup claim empirically.