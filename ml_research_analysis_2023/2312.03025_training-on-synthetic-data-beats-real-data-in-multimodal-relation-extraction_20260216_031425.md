---
ver: rpa2
title: Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction
arxiv_id: '2312.03025'
source_url: https://arxiv.org/abs/2312.03025
tags:
- data
- synthetic
- training
- real
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of multimodal relation extraction
  under data scarcity by introducing a novel setting where only one modality (text
  or image) is available during training, with the other modality synthesized. To
  address the challenges of data diversity and label information loss in synthetic
  data, the authors propose a method called MI2RAGE that applies Chained Cross-modal
  Generation (CCG) to promote diversity and uses a teacher network to select synthetic
  samples with high mutual information to the ground-truth labels.
---

# Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction

## Quick Facts
- arXiv ID: 2312.03025
- Source URL: https://arxiv.org/abs/2312.03025
- Reference count: 40
- This paper introduces MI2RAGE, a method that trains on synthetic multimodal data and outperforms state-of-the-art models trained on real multimodal data in relation extraction tasks.

## Executive Summary
This paper addresses the challenge of multimodal relation extraction under data scarcity by proposing a novel setting where only one modality (text or image) is available during training, with the other modality synthesized. The authors introduce MI2RAGE, which combines Chained Cross-modal Generation (CCG) for data diversity and a teacher network for selecting informative synthetic samples. Experimental results show that MI2RAGE outperforms state-of-the-art models trained on real multimodal data, achieving 3.76% higher F1 score when trained on synthetic images and real text. The method significantly improves upon naive synthetic data baselines by 24.28-26.52% in F1 score.

## Method Summary
The MI2RAGE method applies Chained Cross-modal Generation (CCG) to promote data diversity by repeatedly applying text-to-image and image-to-text generators. A teacher network filters synthetic samples based on training loss to select those with high mutual information to ground-truth labels. The student network is then trained on the selected synthetic data combined with real data. The approach addresses the challenges of data diversity and label information loss in synthetic data, enabling effective training under data scarcity conditions where only one modality is available.

## Key Results
- MI2RAGE outperforms state-of-the-art models trained on real multimodal data by 3.76% F1 score on MNRE-2 dataset
- Significant improvement over naive synthetic data baselines (24.28-26.52% F1 score increase)
- CCG with K=2 iterations provides optimal balance between diversity and semantic drift
- Teacher network selection effectively filters informative synthetic samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chained Cross-modal Generation (CCG) improves data diversity by introducing noise at each generation step, preventing overfitting to high-frequency patterns.
- Mechanism: Each application of text-to-image or image-to-text generation adds variance to the synthetic data, breaking repetitive patterns and expanding the tail of the distribution.
- Core assumption: Generative models tend to over-represent high-frequency content, and diversity is necessary for better generalization.
- Evidence anchors:
  - [abstract] "To alleviate this issue, we propose Chained Cross-modal Generation, where we chain and repeatedly apply text-to-image and image-to-text generators."
  - [section 3.4] "Each generation step introduces some variance to the generated data and enhances diversity."
  - [corpus] Weak: No direct empirical comparison of diversity metrics in corpus neighbors.
- Break condition: If the noise introduced by generation is too high, it could cause semantic drift beyond the point where the teacher network can correct it.

### Mechanism 2
- Claim: Teacher networks select synthetic samples with high mutual information to the ground-truth labels, improving label signal retention in synthetic data.
- Mechanism: The teacher network filters out synthetic samples with high training loss, which correspond to low mutual information with the true label.
- Core assumption: Minimizing cross-entropy loss maximizes mutual information between the synthetic input and the label.
- Evidence anchors:
  - [section 3.1] "To prevent information loss, we aim to choose synthetic training data with high mutual information with the ground-truth labels."
  - [section 3.1] "We argue that the teacher network helps to select training data with high mutual information with the ground-truth label."
  - [corpus] Weak: No explicit mention of mutual information filtering in related papers.
- Break condition: If the teacher network overfits to the synthetic data distribution, it may select samples that correlate with the label by accident rather than genuine signal.

### Mechanism 3
- Claim: The student network benefits from multiple synthetic views per real view, acting as a form of test-time data augmentation that enriches representation learning.
- Mechanism: By feeding multiple synthetic interpretations of a single real instance, the student network learns more robust multimodal representations.
- Core assumption: Different synthetic views provide complementary information about the underlying relation.
- Evidence anchors:
  - [section 3.3] "we can generate as many synthetic views as we want for real view, both at training time and at inference time."
  - [section 4.3] "Performance tends to be low when only 2-4 synthetic views are used, suggesting the potential of synthetic data is not fully realized if insufficient number of views are utilized in training."
  - [corpus] Weak: No explicit discussion of multi-view training in corpus neighbors.
- Break condition: If synthetic views are too similar or all contain the same errors, the student network will not gain meaningful diversity benefits.

## Foundational Learning

- Concept: Mutual Information Maximization
  - Why needed here: Justifies the teacher network's loss-based filtering as selecting informative samples.
  - Quick check question: Why does minimizing cross-entropy correspond to maximizing mutual information between input and label?

- Concept: Importance Sampling
  - Why needed here: Explains why naive cross-entropy on synthetic data may be biased and how diversity helps correct this.
  - Quick check question: What distribution shift problem occurs when training on synthetic data instead of real data?

- Concept: Cross-modal Generation
  - Why needed here: Enables creation of multimodal data from unimodal inputs, essential for the missing modality setting.
  - Quick check question: What is the key difference between direct generation and chained cross-modal generation?

## Architecture Onboarding

- Component map:
  Real data source -> Cross-modal generators (text-to-image, image-to-text) -> Teacher network -> Student network -> Evaluation pipeline

- Critical path:
  1. Generate synthetic data via CCG
  2. Filter synthetic data using teacher network
  3. Train student on filtered synthetic + real data
  4. Evaluate on real multimodal test set

- Design tradeoffs:
  - More CCG iterations increase diversity but risk semantic drift
  - Stricter teacher filtering retains label signal but may discard useful samples
  - More synthetic views per real sample improve robustness but increase computation

- Failure signatures:
  - Performance plateaus or drops when increasing CCG iterations
  - Teacher network selects very few synthetic samples
  - Student network performs worse than unimodal baseline

- First 3 experiments:
  1. Train student with no teacher filtering to establish baseline gain from synthetic data alone
  2. Test teacher filtering with random selection to measure its specific contribution
  3. Vary number of synthetic views per real sample to find optimal diversity level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MI2RAGE method perform on other multimodal relation extraction datasets beyond MNRE-2 and WebNLG?
- Basis in paper: [explicit] The paper mentions that MI2RAGE was tested on MNRE-2 and WebNLG, but suggests that performance gains may vary across different datasets due to differences in text length and visualization difficulty.
- Why unresolved: The paper only presents results on two datasets, and it is unclear how well MI2RAGE would generalize to other multimodal datasets with different characteristics.
- What evidence would resolve it: Testing MI2RAGE on a diverse set of multimodal relation extraction datasets and comparing its performance to other state-of-the-art methods.

### Open Question 2
- Question: How does the number of Chained Cross-modal Generation (CCG) iterations affect the performance of MI2RAGE?
- Basis in paper: [explicit] The paper mentions that the number of CCG iterations, K, is set to 2 in the experiments, but suggests that increasing K is generally beneficial, with diminishing marginal effect sizes.
- Why unresolved: The paper only presents results for K=1 and K=2, and it is unclear how the performance of MI2RAGE changes as K increases beyond 2.
- What evidence would resolve it: Conducting experiments with varying values of K and analyzing the performance of MI2RAGE on different datasets to determine the optimal number of CCG iterations.

### Open Question 3
- Question: How does the proposed MI2RAGE method compare to other data augmentation techniques for multimodal relation extraction?
- Basis in paper: [explicit] The paper mentions that MI2RAGE outperforms a test-time data augmentation (TDA) approach, but it is unclear how it compares to other data augmentation techniques specifically designed for multimodal relation extraction.
- Why unresolved: The paper only compares MI2RAGE to TDA, and it is unclear how it performs relative to other data augmentation methods that may be more suitable for multimodal relation extraction tasks.
- What evidence would resolve it: Conducting experiments comparing MI2RAGE to other data augmentation techniques for multimodal relation extraction, such as generative adversarial networks (GANs) or variational autoencoders (VAEs), and analyzing their performance on various datasets.

## Limitations

- The teacher network's loss-based filtering assumes that low loss directly correlates with high mutual information, but this relationship may not hold when synthetic data distribution differs significantly from real data.
- The study relies on specific generative models (Stable Diffusion v2.1 for images, BLIP for captions) whose performance directly impacts the quality of synthetic data.
- The evaluation only considers two datasets (MNRE-2 and WebNLG), limiting generalizability to other multimodal relation extraction tasks.

## Confidence

- **High**: MI2RAGE achieves state-of-the-art performance on MNRE-2 and WebNLG datasets when compared to baselines trained on real multimodal data
- **Medium**: Chained Cross-modal Generation effectively improves data diversity and prevents overfitting to high-frequency patterns
- **Medium**: Teacher network selection based on training loss effectively identifies synthetic samples with high mutual information to ground-truth labels

## Next Checks

1. **Semantic Drift Validation**: Measure semantic similarity between original and synthetically generated samples after multiple CCG iterations to quantify information loss
2. **Teacher Network Robustness**: Test teacher network performance with synthetic data from different generative model versions to assess dependency on specific implementations
3. **Cross-Dataset Generalization**: Evaluate MI2RAGE on a third, unseen multimodal relation extraction dataset to verify generalization beyond MNRE-2 and WebNLG