---
ver: rpa2
title: 'SHAP-IQ: Unified Approximation of any-order Shapley Interactions'
arxiv_id: '2303.01179'
source_url: https://arxiv.org/abs/2303.01179
tags:
- shapley
- shap-iq
- interaction
- interactions
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAP-IQ is a sampling-based method to approximate Shapley interaction
  indices (SI), which measure feature interactions in ML models. Unlike prior methods
  tailored to specific SI definitions, SHAP-IQ works for any SI satisfying linearity,
  symmetry, and dummy axioms.
---

# SHAP-IQ: Unified Approximation of any-order Shapley Interactions

## Quick Facts
- arXiv ID: 2303.01179
- Source URL: https://arxiv.org/abs/2303.01179
- Reference count: 40
- Primary result: SHAP-IQ approximates Shapley interaction indices for any definition satisfying linearity, symmetry, and dummy axioms, outperforming baselines for higher-order interactions

## Executive Summary
SHAP-IQ is a sampling-based method that approximates Shapley interaction indices (SI) for machine learning models. Unlike prior methods tailored to specific SI definitions, SHAP-IQ works for any SI satisfying fundamental axioms (linearity, symmetry, dummy). It reformulates SI as a weighted sum over subsets, enabling efficient Monte Carlo estimation with theoretical guarantees. The method reveals a novel representation of Shapley values that recovers Unbiased KernelSHAP with simpler calculations. Experiments demonstrate SHAP-IQ's effectiveness on language models and synthetic data, particularly for higher-order interactions and complex models.

## Method Summary
SHAP-IQ approximates Shapley interaction indices by Monte Carlo estimation of a novel weighted sum over all subsets. It uses a sampling distribution p(T) ∝ µ_Sh(t) and precomputed weights γ_m(t,k) to update all interaction estimates in parallel. The method computes deterministic estimates for low and high cardinality subsets (t < k₀ and t > d-k₀) and samples the remaining subsets. For s₀=1, SHAP-IQ reduces exactly to Unbiased KernelSHAP with a simpler closed-form calculation. The approach maintains s-efficiency for SII and STI, ensuring the sum of all interaction estimates remains constant across sampling orders.

## Key Results
- SHAP-IQ outperforms baseline methods for SII and STI in dense settings, with particularly strong performance for higher-order interactions
- For s₀=1, SHAP-IQ recovers Unbiased KernelSHAP with simpler closed-form calculations
- The method provides theoretical guarantees including unbiasedness, consistency, and approximation bounds based on variance
- Experiments on DistilBERT language models and synthetic data demonstrate effectiveness across different interaction definitions

## Why This Works (Mechanism)

### Mechanism 1
SHAP-IQ approximates Shapley interactions by Monte Carlo estimation of a novel weighted sum over all subsets. Theorem 3.1 rewrites any-order SI as a sum over all subsets, with weights γ_m(t,|T∩S|). Each sampled subset contributes to all interaction estimates simultaneously. The core assumption is that marginal contributions δ_S(T) can be expressed recursively and combined linearly across subsets.

### Mechanism 2
SHAP-IQ maintains s-efficiency, keeping the sum of all interaction estimates constant for any sampling order k₀ ≥ s₀. For SII and STI, the weights γ_m(t,|T∩S|) are constructed so that ∑_S γ_m(t,|T∩S|) = 0 for all T, ensuring ∑_S ˆI_m(S) = ∑_S c_{s₀}(S). This property holds when the interaction index definition satisfies symmetry and dummy axioms.

### Mechanism 3
For s₀=1, SHAP-IQ reduces exactly to Unbiased KernelSHAP (U-KSH) with a simpler closed-form calculation. The weights γ_m(t,1) and p(T) ∝ µ_Sh(t) recover the U-KSH representation ˆφ_U(i) = c₁(i) + R/K Σ_k ν₀(T_k)[1(i∈T_k) - t_k/d], where R = 2h_{d-1}. This equivalence holds when the sampling distribution and weights align with those used in U-KSH's Monte Carlo estimator.

## Foundational Learning

- Concept: Shapley value axioms (efficiency, symmetry, dummy, linearity)
  - Why needed here: These axioms define the uniqueness of the SV and constrain how SI extensions must behave
  - Quick check question: If a feature never changes the model output, what should its Shapley value be?

- Concept: Marginal contribution and recursive decomposition of interactions
  - Why needed here: SI definitions rely on recursively defining higher-order interactions from lower-order ones
  - Quick check question: How is δ_{i,j}(T) computed from δ_i(T) and δ_j(T) in the paper?

- Concept: Monte Carlo sampling with variance estimation
  - Why needed here: SHAP-IQ uses sampling to approximate expectations; understanding unbiasedness and consistency is critical
  - Quick check question: What statistical guarantee does Chebyshev's inequality provide for SHAP-IQ?

## Architecture Onboarding

- Component map: Subset generation -> Deterministic precomputation -> Interaction update -> Variance tracking -> Output aggregation
- Critical path:
  1. Determine sampling order k₀ from budget K and weights q(t)
  2. Evaluate all subsets for t < k₀ and t > d-k₀ (deterministic)
  3. Sample remaining subsets according to p(T)
  4. Update interaction estimates incrementally
  5. Return estimates with variance
- Design tradeoffs:
  - Memory vs. speed: Storing all γ_m(t,k) enables O(1) updates but requires O(d·s₀) memory
  - Deterministic vs. sampled: Precomputing low-cardinality subsets reduces variance but costs O(∑_{t<k₀}(d choose t)) evaluations
  - Subset size distribution: q(t) ∝ µ_Sh(t) balances estimator variance but may not be optimal for all models
- Failure signatures:
  - High variance in estimates: Indicates insufficient budget K or poor choice of k₀
  - Estimates not summing to constant: Suggests violation of s-efficiency or numerical instability
  - Slow convergence: Could mean the model evaluations are noisy or the interaction structure is sparse
- First 3 experiments:
  1. Verify s-efficiency: Run SHAP-IQ on a synthetic SOUM with known ground truth and check that Σ_S ˆI_m(S) matches the theoretical constant
  2. Test SV equivalence: Compare SHAP-IQ output for s₀=1 against KernelSHAP on a small tabular dataset and confirm identical results
  3. Stress variance bound: Vary K and plot MSE vs. K^{-1/2} to empirically confirm the Chebyshev bound in Theorem 3.4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific conditions under which SHAP-IQ's performance degrades significantly compared to baseline methods?
- Basis in paper: [explicit] The paper shows SHAP-IQ performs worse in sparse SOUM settings and when sampling estimates have high variance
- Why unresolved: The paper demonstrates this empirically but doesn't provide theoretical conditions for when this occurs or how to predict it
- What evidence would resolve it: Mathematical conditions on feature interaction density and distribution that predict when SHAP-IQ will underperform

### Open Question 2
- Question: Can a kernel-based approximation approach be developed for the general SI definition that SHAP-IQ addresses?
- Basis in paper: [inferred] The paper notes that while KB approaches work for FSI, they are "theoretically not well understood" and "it remains an interesting open question if a KB approach can be found for our general SI definition"
- Why unresolved: The paper explicitly states this as an open research direction
- What evidence would resolve it: A new kernel-based algorithm for general SI that matches or exceeds SHAP-IQ's performance with similar theoretical guarantees

### Open Question 3
- Question: How can the sampling methodology of SHAP-IQ be improved to reduce variance in sparse settings?
- Basis in paper: [explicit] The paper states "Our results have shown that SHAP-IQ performs worse when we observe high variance of the sampling estimates. It remains an open question, if these observations and our statistical guarantees can be used to improve the sampling methodology of SHAP-IQ."
- Why unresolved: The paper identifies this as a limitation but doesn't provide solutions
- What evidence would resolve it: Modified sampling schemes or variance reduction techniques that maintain SHAP-IQ's efficiency while improving performance in sparse settings

### Open Question 4
- Question: What is the precise mathematical relationship between the SII efficiency property and the quantification of interaction strength?
- Basis in paper: [explicit] The paper derives an SII efficiency property and states "whose analysis we leave to future research"
- Why unresolved: The paper derives the property but doesn't analyze its implications
- What evidence would resolve it: Theoretical work showing how the SII efficiency formula relates to actual interaction strengths in practical applications

## Limitations

- Limited empirical validation scope across model types and interaction structures
- Computational overhead may become significant in very high-dimensional problems despite O(d) theoretical complexity
- Performance depends on the linearity, symmetry, and dummy axioms; real-world violations could invalidate the method

## Confidence

- **High Confidence**: The core mechanism (Monte Carlo estimation via weighted sum over subsets) and the SV equivalence to Unbiased KernelSHAP are well-established mathematically and empirically verified
- **Medium Confidence**: The s-efficiency property for SII and STI is theoretically proven but only empirically validated on a limited set of experiments
- **Low Confidence**: The performance claims for higher-order interactions (>3) and on complex models (DistilBERT) are based on synthetic data and a single LM benchmark, requiring broader validation

## Next Checks

1. Apply SHAP-IQ to a CNN image classifier and a regression model on tabular data to verify the method's robustness across different model types and interaction structures

2. Evaluate SHAP-IQ on a high-dimensional dataset (d > 50) to measure memory usage and runtime, confirming the O(d) complexity claim in practice

3. Construct a toy model where the linearity, symmetry, or dummy axioms are violated (e.g., interaction indices that depend on feature order) and assess whether SHAP-IQ's estimates remain meaningful or break down