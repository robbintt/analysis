---
ver: rpa2
title: Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring
arxiv_id: '2312.02859'
source_url: https://arxiv.org/abs/2312.02859
tags:
- usable
- interfaces
- users
- explanations
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper outlines lessons learned from deploying usable ML interfaces
  in real-world domains, focusing on the emerging role of "bridges" who connect ML
  developers with domain experts. The authors apply these lessons to the specific
  task of wind turbine monitoring, where turbine engineers and data analysts must
  decide whether to perform costly in-person investigations to prevent potential brakepad
  failures.
---

# Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring

## Quick Facts
- arXiv ID: 2312.02859
- Source URL: https://arxiv.org/abs/2312.02859
- Reference count: 30
- One-line primary result: Introduces Sibyl, a configurable system for developing usable ML interfaces, and emphasizes the emerging "bridge" role connecting ML developers with domain experts in wind turbine monitoring

## Executive Summary
This paper explores practical lessons from deploying usable ML interfaces in real-world domains, focusing on the emerging role of "bridges" who connect ML developers with domain experts. The authors apply these lessons to wind turbine monitoring, where turbine engineers and data analysts must decide whether to perform costly in-person investigations to prevent potential brakepad failures. They present Sibyl, a generalizable system for developing configurable ML interfaces, and discuss the importance of continuous, in-deployment evaluations using KPIs rather than traditional user studies.

## Method Summary
The paper outlines a generalizable system called Sibyl for developing usable ML interfaces, consisting of Pyreal (explanation generation library), Sibyl-API (backend API), and Sibylapp (frontend Streamlit interface). The approach emphasizes configurable interfaces that can be adapted to different domains, with a focus on enabling domain experts to make informed decisions about costly maintenance actions. The method includes continuous KPI-based evaluation rather than formal user studies, with the goal of measuring real-world impact on business metrics like turbine downtime and maintenance costs.

## Key Results
- The "bridge" role emerges as critical for connecting ML developers with domain experts in complex deployment scenarios
- Sibyl provides a modular approach to ML interface development with configurable explanation types
- KPI-based evaluation offers a practical alternative to formal user studies for measuring real-world impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "bridge" role enables effective usable ML deployment by connecting ML developers with domain experts
- Mechanism: Bridges serve as intermediaries who understand both ML development and domain-specific workflows, enabling iterative refinement of ML interfaces based on practical feedback
- Core assumption: Domain experts lack ML expertise while ML developers lack domain expertise, creating a need for intermediaries
- Evidence anchors:
  - [abstract] "many organizations are beginning to hire people who we call 'bridges' because they bridge the gap between ML developers and domain experts"
  - [section] "we found that a new role is emerging and rapidly gaining traction - that of people within companies who are tasked with connecting domain experts with ML developers"
  - [corpus] Weak evidence - no direct citations in corpus papers about bridge roles specifically
- Break condition: The bridge role fails if the individual lacks sufficient understanding of either ML development or domain workflows

### Mechanism 2
- Claim: Configurable systems like Sibyl enable rapid iteration on ML interfaces through modular components
- Mechanism: By separating explanation generation (Pyreal), backend API (Sibyl-API), and frontend (Sibylapp), developers can modify individual components without rebuilding the entire system
- Core assumption: Different domains require different interface configurations, making modularity essential for adaptability
- Evidence anchors:
  - [section] "we have developed a generalizable system called Sibyl" with three parts: Pyreal library, Sibyl-API, and Sibylapp
  - [section] "With this system, we can abstract out common overhead code to focus on configuring usable ML interfaces to specific domains"
  - [corpus] Weak evidence - no direct citations about modular ML interface systems
- Break condition: The modular approach fails if the integration between components becomes too complex or if domain-specific requirements cannot be met through configuration

### Mechanism 3
- Claim: Continuous evaluation through KPI tracking provides more accurate real-world impact measurement than formal user studies
- Mechanism: By monitoring existing business metrics before and after ML deployment, organizations can quantify the practical value of usable ML interfaces
- Core assumption: Business KPIs are already being tracked and can be correlated with ML interface usage
- Evidence anchors:
  - [section] "we devised an evaluation plan built on tracking existing key performance indicators (KPIs) through a live deployment"
  - [section] "we will then compare the KPI metrics computed to several historic time frames of the same length"
  - [corpus] Weak evidence - no direct citations about KPI-based ML evaluation in corpus papers
- Break condition: The evaluation fails if KPIs are not clearly linked to ML interface usage or if external factors heavily influence the metrics

## Foundational Learning

- Concept: Domain-specific ML deployment challenges
  - Why needed here: Wind turbine monitoring involves complex technical decisions where costly mistakes can occur, requiring ML interfaces that support rather than replace expert judgment
  - Quick check question: What are the key differences between deploying ML in wind turbine monitoring versus more general classification tasks?

- Concept: XAI explanation types and their appropriate use cases
  - Why needed here: Different explanation types (feature contributions, nearest neighbors, global explanations) serve different user needs in the turbine monitoring workflow
  - Quick check question: When would feature-level scatter plots be more useful than global feature importance in this domain?

- Concept: KPI identification and tracking methodology
  - Why needed here: The paper emphasizes selecting KPIs that balance relevance to business goals with practical measurability for evaluation
  - Quick check question: What criteria should be used to select between different potential KPIs for the turbine monitoring system?

## Architecture Onboarding

- Component map:
  - Sibyl system: Pyreal (explanation generation) → Sibyl-API (backend) → Sibylapp (frontend Streamlit interface)
  - Bridge role: Connects domain experts and ML developers for iterative refinement
  - Evaluation system: KPI tracking integrated with existing business monitoring

- Critical path: Bridge feedback → Sibyl configuration → Interface iteration → KPI evaluation → Deployment refinement

- Design tradeoffs:
  - Streamlit frontend vs. React: Faster iteration at cost of some customization
  - Configurable vs. domain-specific: Balance between reusability and optimal performance
  - KPI-based vs. user study evaluation: Real-world relevance vs. controlled measurement

- Failure signatures:
  - Bridge role breakdown: Misalignment between domain needs and ML capabilities
  - Sibyl configuration issues: Explanations not matching user mental models
  - Evaluation failure: KPIs not sensitive enough to detect ML impact

- First 3 experiments:
  1. Deploy basic Sibylapp with feature contributions interface to Monitoring team, gather initial feedback
  2. Add nearest neighbor interface and conduct A/B testing on explanation preference
  3. Implement KPI tracking dashboard and compare turbine downtime metrics pre/post-deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective KPIs for measuring the real-world impact of usable ML interfaces in wind turbine monitoring?
- Basis in paper: [explicit] The authors discuss evaluating usable ML applications using KPIs and provide examples such as total downtime, number of brakepad failures compared to investigations, and portion of alerts investigated.
- Why unresolved: The authors state that selecting the right KPIs requires choosing metrics close to the bottom-line company goal while being practical to track. They mention considering options but do not definitively conclude which KPIs are most effective.
- What evidence would resolve it: Comparative analysis of different KPI metrics over time showing which ones most accurately reflect the impact of usable ML interfaces on turbine efficiency and maintenance costs.

### Open Question 2
- Question: How do bridges in different industries compare in terms of their effectiveness at facilitating usable ML deployment?
- Basis in paper: [explicit] The authors introduce the concept of "bridges" who connect ML developers with domain experts and discuss their role in wind turbine monitoring, but do not compare effectiveness across industries.
- Why unresolved: The paper focuses on a single case study in wind turbine monitoring and does not provide comparative data or analysis of bridges' effectiveness in other domains.
- What evidence would resolve it: Case studies and performance metrics from multiple industries showing the impact of bridges on usable ML deployment success rates and efficiency improvements.

### Open Question 3
- Question: What is the optimal balance between configurable system complexity and ease of use for developing usable ML interfaces?
- Basis in paper: [inferred] The authors discuss their system Sibyl, which includes multiple components for generating explanations and a lightweight front-end application. They mention iterating on interfaces and modifying the UI for easier modification, implying a balance is being sought.
- Why unresolved: While the paper describes the development process and modifications made to Sibyl, it does not explicitly address the trade-off between system complexity and ease of use or provide metrics on the optimal balance.
- What evidence would resolve it: User studies and system performance data comparing different configurations of the Sibyl system, measuring both development efficiency and end-user satisfaction across varying levels of complexity.

## Limitations

- The bridge role concept lacks empirical validation beyond anecdotal observation
- Sibyl system details are presented without performance benchmarks or rigorous evaluation
- KPI-based evaluation methodology remains hypothetical without actual deployment results

## Confidence

- **High Confidence**: The identification of practical challenges in deploying ML interfaces (e.g., the need for iterative refinement, the importance of aligning explanations with domain workflows)
- **Medium Confidence**: The claim that configurable systems like Sibyl can accelerate usable ML interface development, though without empirical validation of development time savings
- **Low Confidence**: The specific effectiveness of the bridge role and the KPI-based evaluation approach, as these remain largely theoretical proposals without demonstrated results

## Next Checks

1. Conduct a controlled study comparing teams with and without bridge roles to quantify the impact of this organizational structure on ML interface quality and adoption rates in real deployments.

2. Implement a formal A/B testing framework within the Sibyl system to measure whether configurable modular interfaces actually lead to faster development cycles compared to custom-built solutions.

3. Execute the proposed KPI tracking evaluation on the wind turbine monitoring system and publish a follow-up paper comparing pre- and post-deployment metrics, including statistical significance testing of observed differences.