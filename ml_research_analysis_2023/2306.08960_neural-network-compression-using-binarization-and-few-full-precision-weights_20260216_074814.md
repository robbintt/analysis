---
ver: rpa2
title: Neural Network Compression using Binarization and Few Full-Precision Weights
arxiv_id: '2306.08960'
source_url: https://arxiv.org/abs/2306.08960
tags:
- weights
- quantization
- binary
- pruning
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Automatic Prune Binarization (APB), a novel
  compression technique that combines binarization with pruning to enhance the representational
  capability of binary networks using a few full-precision weights. APB jointly maximizes
  the accuracy of the network while minimizing its memory impact by deciding whether
  each weight should be binarized or kept in full precision.
---

# Neural Network Compression using Binarization and Few Full-Precision Weights

## Quick Facts
- arXiv ID: 2306.08960
- Source URL: https://arxiv.org/abs/2306.08960
- Reference count: 40
- Key outcome: APB delivers better accuracy/memory trade-off compared to state-of-the-art methods based on i) quantization, ii) pruning, and iii) combination of pruning and quantization

## Executive Summary
This paper introduces Automatic Prune Binarization (APB), a novel neural network compression technique that combines binarization with selective pruning to enhance binary network accuracy using few full-precision weights. APB learns to binarize weights within a learned interval while preserving full precision for weights outside this interval. The method achieves superior accuracy/memory trade-offs compared to traditional quantization and pruning approaches, outperforming 2-bit quantized models in speed while maintaining equivalent accuracy.

## Method Summary
APB works by learning a binarization interval centered at zero, where weights within this interval are binarized to {-α, +α} and weights outside are kept in full precision. The technique jointly optimizes the binarization parameters and network weights using SGD with Straight-Through Estimator for the non-differentiable binarization operation. Forward propagation is decomposed into binary and sparse-dense matrix multiplications, with the sparse-dense operation being highly efficient due to the extreme sparsity of the full-precision weight matrix. The authors also develop custom bitwise matrix multiplication routines that achieve significant speedups on CPU architectures with AVX-512 support.

## Key Results
- APB outperforms quantization methods in the accuracy/efficiency trade-off, being up to 2× faster than 2-bit quantized models with no accuracy loss
- The technique delivers better accuracy/memory trade-off compared to state-of-the-art methods based on quantization, pruning, and their combination
- Custom 1/1 and 2/2 bitwise matrix multiplication routines are 6.9× and 1.5× faster than available state-of-the-art solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APB improves binary network accuracy by selectively preserving full-precision weights outside a learned binarization interval
- Mechanism: Weights within the binarization interval (|w| ≤ α + δ) are binarized to {−α, +α}, while weights outside are kept in full precision. This allows the network to maintain representational power from large-magnitude weights while benefiting from binary efficiency for small-magnitude weights.
- Core assumption: Large-magnitude weights contain most of the network's representational power, while small-magnitude weights can be approximated with binary values without significant accuracy loss
- Evidence anchors:
  - [abstract] "APB enhances the representational capability of binary networks using a few full-precision weights"
  - [section] "APB works by identifying a binarization interval (centered in 0), and the parameters falling in this interval are represented using one bit"
- Break condition: If the binarization interval is too wide, too many weights become full-precision and memory savings are lost; if too narrow, accuracy degradation occurs

### Mechanism 2
- Claim: APB can be efficiently implemented by decomposing matrix multiplication into binary and sparse-dense operations
- Mechanism: For each layer, APB splits the weight matrix into Abin (binary weights) and Afull (sparse full-precision weights). Forward propagation becomes Abin·B + Afull·B, where Abin·B uses efficient bitwise operations and Afull·B uses sparse matrix multiplication.
- Core assumption: The sparsity of Afull is high enough that sparse-dense multiplication remains efficient
- Evidence anchors:
  - [section] "We show how to efficiently perform a forward pass through layers compressed using APB by decomposing it into a binary and a sparse-dense matrix multiplication"
  - [section] "Due to the extreme sparsity ratios of Afull, the sparse-dense multiplication can be efficiently performed with tailored implementations"
- Break condition: If sparsity ratio drops below ~95%, sparse-dense multiplication becomes inefficient compared to dense multiplication

### Mechanism 3
- Claim: APB achieves superior efficiency/accuracy tradeoff compared to fixed-bit quantization
- Mechanism: By adaptively choosing which weights to binarize vs keep full-precision, APB can achieve better accuracy than 2-bit quantization while using less memory, and better efficiency than 1-bit quantization while maintaining higher accuracy.
- Core assumption: Different network layers have different sensitivity to quantization, and APB can learn the optimal trade-off per layer
- Evidence anchors:
  - [abstract] "APB delivers better accuracy/memory trade-off compared to state-of-the-art methods based on i) quantization, ii) pruning, and iii) combination of pruning and quantization"
  - [section] "APB can deliver the same accuracy but allows us to save up 2.1× [memory] compared to Bayesian Bits"
- Break condition: If the optimization fails to find an effective partition between binary and full-precision weights, APB performance degrades