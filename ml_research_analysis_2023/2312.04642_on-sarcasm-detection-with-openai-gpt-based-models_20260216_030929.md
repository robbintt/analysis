---
ver: rpa2
title: On Sarcasm Detection with OpenAI GPT-based Models
arxiv_id: '2312.04642'
source_url: https://arxiv.org/abs/2312.04642
tags:
- gpt-3
- sarcasm
- dataset
- performance
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multiple GPT models for sarcasm detection
  using the pol-bal dataset. It tests fine-tuned GPT-3 models of different sizes and
  zero-shot GPT-3, GPT-3.5, and GPT-4 models.
---

# On Sarcasm Detection with OpenAI GPT-based Models

## Quick Facts
- arXiv ID: 2312.04642
- Source URL: https://arxiv.org/abs/2312.04642
- Reference count: 35
- Primary result: Largest fine-tuned GPT-3 model achieves 0.81 accuracy and F1-score for sarcasm detection

## Executive Summary
This paper evaluates fourteen GPT models for sarcasm detection using the pol-bal dataset. The study tests both fine-tuned GPT-3 models of different sizes and zero-shot GPT-3, GPT-3.5, and GPT-4 models. Results show that the largest fine-tuned GPT-3 model achieves state-of-the-art accuracy of 0.81 and F1-score of 0.81, while among zero-shot models, GPT-4 achieves accuracy of 0.70 and F1-score of 0.75. The research demonstrates that model size and version significantly impact performance, with newer versions sometimes improving and sometimes degrading performance. Fine-tuning generally outperforms zero-shot approaches for sarcasm detection tasks.

## Method Summary
The study evaluates fourteen GPT models using two approaches: fine-tuning GPT-3 models and zero-shot inference for all models. The dataset used is the pol-bal subset of SARC 2.0 (13,668 training, 3,406 testing observations). Fine-tuning uses hyperparameters: learning rate 0.1, 4 epochs, batch size 16, prompt loss weight 0.01. Zero-shot models use system prompts and logit bias to force "yes"/"no" outputs. Prompts wrap comment threads with replies using a specific format. Performance is measured using accuracy and F1-score, with statistical significance tested using McNemar's test.

## Key Results
- Fine-tuned davinci model achieves state-of-the-art accuracy of 0.81 and F1-score of 0.81
- GPT-4 zero-shot model achieves accuracy of 0.70 and F1-score of 0.75
- Model performance improves monotonically with size in fine-tuned GPT-3 models
- Newer model versions show inconsistent performance improvements/degradations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model size correlates with sarcasm detection performance in fine-tuned GPT-3 models
- Mechanism: Larger models have more parameters enabling richer representations of complex language patterns including irony and contextual cues
- Core assumption: Performance improvement is monotonic with parameter count
- Evidence anchors:
  - the accuracy and F1-scores of fine-tuned models increased monotonically with model size
  - Using the smallest model — ada — produces the worst results, while using the largest model — davinci — produces the best results
  - Weak evidence - corpus only mentions general sentiment analysis, not sarcasm-specific performance trends
- Break condition: Performance plateaus or degrades with extremely large models due to overfitting or computational constraints

### Mechanism 2
- Claim: GPT-4 outperforms other models in zero-shot sarcasm detection
- Mechanism: GPT-4's enhanced architecture and training data capture subtle linguistic features of sarcasm better than previous versions
- Core assumption: Zero-shot performance reflects inherent model capability rather than task-specific adaptation
- Evidence anchors:
  - In the zero-shot case, one of GPT-4 models yields an accuracy of 0.70 and F1-score of 0.75
  - The zero-shot models 'challenge' is won by GPT-4 GPT-4-0613 model, achieving accuracy≈ 0.70 and F1 ≈ 0.75
  - Moderate evidence - corpus mentions GPT-4 achieving best results among tested models
- Break condition: Performance degrades if prompt engineering is suboptimal or if dataset contains features outside model's training distribution

### Mechanism 3
- Claim: Model versioning affects performance inconsistently across releases
- Mechanism: Architectural changes and training data updates in new releases improve some capabilities while degrading others
- Core assumption: Performance variation is due to changes in model architecture rather than random noise
- Evidence anchors:
  - Additionally, a model's performance may improve or deteriorate with each release
  - The GPT model's ability to detect sarcasm may decline or improve with new releases
  - Strong evidence - corpus explicitly mentions that newer versions sometimes improve and sometimes degrade performance
- Break condition: Performance becomes consistently better or worse across all subsequent releases, suggesting a clear architectural direction

## Foundational Learning

- Concept: Zero-shot vs fine-tuning distinction
  - Why needed here: Different learning approaches yield vastly different performance results (0.81 vs 0.70 accuracy)
  - Quick check question: If a model achieves 0.81 accuracy with fine-tuning but only 0.70 with zero-shot, what does this tell you about the task complexity?

- Concept: Statistical significance testing with McNemar's test
  - Why needed here: Determines whether performance differences between models are meaningful or due to chance
  - Quick check question: When comparing two classification models on the same dataset, which test would you use to determine if accuracy differences are statistically significant?

- Concept: F1-score calculation and interpretation
  - Why needed here: Balances precision and recall, critical for imbalanced datasets where sarcasm might be rare
  - Quick check question: If a model has high accuracy but low F1-score on a sarcasm detection task, what might this indicate about its performance?

## Architecture Onboarding

- Component map: Dataset -> Prompt Engineering -> Model Selection (GPT-3/3.5/4 variants) -> Training Mode (fine-tune/zero-shot) -> Evaluation (Accuracy/F1) -> Statistical Analysis
- Critical path: Dataset preparation -> Prompt wrapping -> Model inference -> Result aggregation -> Statistical validation
- Design tradeoffs: Fine-tuning provides better performance but requires labeled data and compute resources; zero-shot is faster but less accurate
- Failure signatures: Empty outputs from models, inconsistent performance across model versions, statistically insignificant differences
- First 3 experiments:
  1. Compare zero-shot vs fine-tuned performance on small GPT-3 model to establish baseline
  2. Test all GPT-3 sizes with fine-tuning to verify size-performance correlation
  3. Evaluate GPT-4 zero-shot performance against baseline models to assess capability leap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal prompt engineering techniques for sarcasm detection with GPT models?
- Basis in paper: The paper mentions that they tested several prompt formulations and used the most effective ones, but suggests that better prompts could improve performance.
- Why unresolved: The paper did not extensively explore sophisticated prompt engineering techniques like k-shot, chain of thought, or retrieval augmented generation.
- What evidence would resolve it: Systematic comparison of different prompt engineering techniques (k-shot, chain of thought, RAG) on the same dataset showing performance differences.

### Open Question 2
- Question: How does the training data composition of GPT models affect their sarcasm detection capabilities?
- Basis in paper: The paper acknowledges that the exact training data composition is unknown and raises the possibility that GPT models may have seen the pol-bal dataset during training.
- Why unresolved: OpenAI does not disclose the full training data composition of their models.
- What evidence would resolve it: Access to GPT model training data composition or controlled experiments with models trained on different data distributions.

### Open Question 3
- Question: How do different sarcasm detection datasets affect GPT model performance?
- Basis in paper: The paper uses only the pol-bal dataset and acknowledges that findings cannot be generalized to other datasets.
- Why unresolved: The study is limited to a single dataset despite sarcasm detection being tested across multiple datasets in the literature.
- What evidence would resolve it: Comparative experiments of GPT models across multiple sarcasm detection datasets with consistent methodology.

## Limitations
- Dataset scope limited to pol-bal subset of SARC 2.0, which may not generalize beyond political sarcasm
- Reproducibility constraints due to missing implementation details for prompt engineering and API usage
- Model availability limitations - fine-tuning not available for GPT-4 during study

## Confidence

**High Confidence**:
- Model size positively correlates with fine-tuned GPT-3 performance
- GPT-4 achieves best zero-shot results among tested models
- Fine-tuning generally outperforms zero-shot approaches

**Medium Confidence**:
- Inconsistent performance across model versions due to architectural changes
- Monotonic relationship between model size and performance (limited to tested range)

**Low Confidence**:
- Generalization of results to non-political sarcasm detection tasks
- Specific mechanisms behind performance variations across model versions

## Next Checks
1. Replicate the McNemar's test analysis for all pairwise model comparisons with exact p-values reported, ensuring that performance differences between models are statistically significant at p < 0.05 level.
2. Test the best-performing fine-tuned model (davinci) and zero-shot GPT-4 on an independent sarcasm detection dataset from a different domain to verify generalization beyond the pol-bal political context.
3. Calculate the compute cost per inference for each model size and compare cost-normalized F1-scores to determine if the performance gains from larger models justify their computational expense.