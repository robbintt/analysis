---
ver: rpa2
title: 'Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust
  ASR'
arxiv_id: '2304.04974'
source_url: https://arxiv.org/abs/2304.04974
tags:
- speech
- clean
- noisy
- codebook
- wav2code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of automatic speech recognition
  (ASR) under noisy conditions, where conventional approaches combining speech enhancement
  (SE) and self-supervised learning (SSL) suffer from speech distortion. The authors
  propose Wav2code, a framework that leverages vector quantization (VQ) to store clean
  speech representations as prior in a discrete codebook during pre-training.
---

# Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR

## Quick Facts
- **arXiv ID:** 2304.04974
- **Source URL:** https://arxiv.org/abs/2304.04974
- **Reference count:** 40
- **Key outcome:** Wav2code achieves state-of-the-art performance on noisy ASR, improving CHiME-4 real noisy dataset WER by 7.6% compared to existing approaches.

## Executive Summary
This paper introduces Wav2code, a novel framework for noise-robust automatic speech recognition (ASR) that addresses the speech distortion problem in conventional speech enhancement (SE) and self-supervised learning (SSL) combinations. The framework leverages vector quantization to store clean speech representations as a discrete codebook prior during pre-training, then uses a Transformer-based code predictor to accurately restore clean speech representations from noisy inputs in the finetuning stage. An interactive feature fusion network (IFF-Net) is introduced to combine the restored clean representations with original noisy representations, considering both fidelity and quality. Experiments on synthetic and real noisy datasets demonstrate consistent improvements in word error rates (WERs) compared to existing approaches.

## Method Summary
Wav2code operates in two stages: pre-training and finetuning. During pre-training, clean speech representations from an Enhanced Wav2vec 2.0 (EW2) backbone are quantized into a discrete codebook with 1024 entries, storing clean speech prior. In finetuning, a Transformer-based code predictor maps noisy speech features to the correct codebook entries, restoring clean speech representations. The interactive feature fusion network (IFF-Net) then combines the noisy and restored clean representations for downstream ASR. The framework is trained on LibriSpeech clean data mixed with FreeSound noise at various SNR levels, and evaluated on both synthetic (LibriSpeech-FreeSound) and real (CHiME-4) noisy datasets.

## Key Results
- Wav2code achieves a 7.6% WER on the CHiME-4 real noisy dataset, outperforming existing approaches.
- The framework demonstrates consistent improvements in WERs across various noise conditions and SNR levels on both synthetic and real noisy datasets.
- Wav2code effectively solves the speech distortion problem by accurately restoring clean speech representations without degradation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Codebook lookup provides a discrete representation space that acts as a clean speech prior, enabling restoration of high-quality speech features from noisy inputs without the distortion typical of conventional speech enhancement.
- **Mechanism**: Clean speech representations from a self-supervised learning (SSL) model are quantized into discrete codebook entries during pre-training. In the finetuning stage, a Transformer-based code predictor uses global contextual dependencies in noisy speech to predict the correct codebook entries, which are then used to reconstruct clean speech features.
- **Core assumption**: The discrete codebook can effectively capture and store the variability of clean speech representations, and the code predictor can accurately map noisy inputs back to the correct codebook entries despite noise corruption.
- **Evidence anchors**:
  - [abstract] "The authors propose Wav2code, a framework that leverages vector quantization (VQ) to store clean speech representations as prior in a discrete codebook during pre-training."
  - [section II-B] "The codebook C will learn store the prior from clean speech representations."
  - [corpus] Found 25 related papers with average FMR=0.502, suggesting moderate relatedness but no direct evidence for codebook prior effectiveness.
- **Break condition**: If noise corruption causes the noisy speech representations to fall into wrong codebook clusters that are too distant from the correct entry, the code predictor cannot accurately recover the intended clean representation.

### Mechanism 2
- **Claim**: The Transformer-based code predictor models global dependencies in noisy speech, enabling accurate prediction of clean codebook entries and thus restoring high-quality clean speech representations without distortion.
- **Mechanism**: The code predictor uses multi-head attention and layer normalization to capture long-range dependencies in the noisy speech sequence, producing a probability distribution over codebook entries for each frame. This allows it to overcome local noise corruption and predict the correct clean code.
- **Core assumption**: The global context captured by the Transformer is sufficient to disambiguate the correct clean codebook entry even when local features are corrupted by noise.
- **Evidence anchors**:
  - [section II-C] "we introduce a Transformer-based code predictor to model the global contextual dependencies of input noisy sequence for accurate code prediction."
  - [section II-C] "Comparison between Exp. (6) and (3) suggests that code predictor is more effective than NN matching, as the latter is infeasible for codebook lookup due to the corruption in input noisy representations."
  - [corpus] No direct evidence for Transformer's effectiveness in this specific codebook lookup task; relatedness is moderate.
- **Break condition**: If the noise corruption is too severe or if the global context is insufficient to disambiguate between similar codebook entries, the code predictor may fail to predict the correct entry.

### Mechanism 3
- **Claim**: The Interactive Feature Fusion Network (IFF-Net) combines the fidelity of original noisy speech representations with the quality of restored clean representations, resulting in more informative features for downstream ASR.
- **Mechanism**: IFF-Net uses a bottleneck design with ResNet blocks and Separable Self-Attention (SSA) modules to extract and interact features from both noisy and restored representations. A merge module learns a mask to control the contribution of each branch, optimizing for both fidelity and quality.
- **Core assumption**: The restored clean representations, while high-quality, lose some fidelity compared to the original noisy speech. Combining them with the original noisy representations can recover lost information and improve ASR performance.
- **Evidence anchors**:
  - [abstract] "Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR."
  - [section II-D] "Since the quantized speech representation Z q n is restored from discrete codebook, there exists some loss of fidelity... we propose an interaction feature fusion network (IFF-Net) to combine them, in order to purse both fidelity and quality in restoration."
  - [corpus] No direct evidence for IFF-Net's effectiveness; relatedness is moderate.
- **Break condition**: If the restored clean representations are too degraded in fidelity or the noisy representations are too corrupted, the fusion may not effectively combine the best aspects of both, leading to suboptimal ASR performance.

## Foundational Learning

- **Concept**: Vector Quantization (VQ)
  - Why needed here: VQ is used to discretize continuous speech representations into a finite set of codebook entries, which serves as a clean speech prior for restoration.
  - Quick check question: What is the primary purpose of using vector quantization in Wav2code?

- **Concept**: Self-Supervised Learning (SSL)
  - Why needed here: SSL models, such as Wav2vec 2.0, are used to extract contextualized speech representations that are then quantized into the codebook.
  - Quick check question: How does self-supervised learning contribute to the effectiveness of Wav2code?

- **Concept**: Transformer-based sequence modeling
  - Why needed here: The Transformer-based code predictor uses multi-head attention to model global dependencies in the noisy speech sequence, enabling accurate prediction of clean codebook entries.
  - Quick check question: Why is a Transformer-based model preferred over a CNN-based model for the code predictor in Wav2code?

## Architecture Onboarding

- **Component map**:
  - Raw waveform -> EW2 feature extraction -> Noisy speech representation
  - Noisy speech representation -> Code predictor -> Predicted clean codebook entries
  - Predicted clean codebook entries -> Reconstructed clean speech representation
  - Noisy and restored clean representations -> IFF-Net fusion -> Final ASR input

- **Critical path**:
  - Raw waveform → EW2 feature extraction → Noisy speech representation
  - Noisy speech representation → Code predictor → Predicted clean codebook entries
  - Predicted clean codebook entries → Reconstructed clean speech representation
  - Noisy and restored clean representations → IFF-Net fusion → Final ASR input

- **Design tradeoffs**:
  - Using a discrete codebook provides a clean speech prior but may lose some fidelity in the restored representations.
  - The Transformer-based code predictor can model global dependencies but is computationally more expensive than a CNN-based model.
  - The IFF-Net combines fidelity and quality but adds complexity to the model.

- **Failure signatures**:
  - If the code predictor fails to accurately predict clean codebook entries, the restored speech representations will be distorted.
  - If the IFF-Net fails to effectively combine the noisy and restored clean representations, the final ASR input may be suboptimal.
  - If the codebook is not sufficiently expressive, it may not capture the variability of clean speech representations.

- **First 3 experiments**:
  1. Evaluate the effect of the codebook size (number of entries) on ASR performance.
  2. Compare the effectiveness of the Transformer-based code predictor with a CNN-based predictor.
  3. Assess the impact of the IFF-Net on combining fidelity and quality of the restored speech representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of codebook entries (N) and Transformer blocks (M) in the Wav2code framework for different noise conditions and dataset sizes?
- Basis in paper: [explicit] The paper states that N = 1024 and M = 4 are used in experiments, but also shows that N = 512 may not be sufficient and N = 2048 only yields limited improvement, and that M = 2 is less effective than M = 4.
- Why unresolved: The optimal values may depend on the specific characteristics of the noise conditions and dataset sizes, and the paper does not explore a wide range of values for N and M.
- What evidence would resolve it: Conducting experiments with different values of N and M on various noise conditions and dataset sizes to determine the optimal values for each scenario.

### Open Question 2
- Question: How does the performance of Wav2code compare to other state-of-the-art methods on real-world noisy speech datasets beyond CHiME-4?
- Basis in paper: [explicit] The paper evaluates Wav2code on CHiME-4 dataset and shows superior performance compared to other methods, but does not explore other real-world noisy speech datasets.
- Why unresolved: The effectiveness of Wav2code on other real-world noisy speech datasets is unknown, and it is important to verify its generalizability to different scenarios.
- What evidence would resolve it: Evaluating Wav2code on other real-world noisy speech datasets and comparing its performance to other state-of-the-art methods.

### Open Question 3
- Question: How does the performance of Wav2code change when integrating with other speech enhancement or self-supervised learning frameworks?
- Basis in paper: [inferred] The paper mentions that it would be promising to integrate Wav2code with other frameworks like Chang et al.'s method, which combines speech enhancement and self-supervised learning.
- Why unresolved: The potential benefits of integrating Wav2code with other frameworks are unknown, and it is important to explore how different combinations can improve noise robustness.
- What evidence would resolve it: Conducting experiments to integrate Wav2code with other speech enhancement or self-supervised learning frameworks and evaluating their performance on various noisy speech datasets.

## Limitations

- The framework's generalization to unseen noise types and real-world scenarios beyond the evaluated datasets remains to be thoroughly validated.
- The specific architectural details of the Interactive Feature Fusion Network (IFF-Net) are not fully specified, particularly the implementation of the ResNet blocks and Separable Self-Attention modules.
- The optimal values for the codebook size (N) and the number of Transformer blocks (M) may depend on the specific characteristics of the noise conditions and dataset sizes, and are not explored exhaustively in the paper.

## Confidence

- **High confidence**: The overall effectiveness of Wav2code in improving ASR performance under noisy conditions, as demonstrated by the experimental results on both synthetic and real noisy datasets.
- **Medium confidence**: The claims about the specific mechanisms of codebook lookup, Transformer-based code prediction, and IFF-Net fusion improving ASR performance, as they are supported by experimental evidence but lack detailed ablation studies and direct evidence for the underlying mechanisms.
- **Low confidence**: The claims about the framework's ability to generalize to unseen noise types and real-world scenarios beyond the evaluated datasets, as this remains to be thoroughly validated.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the codebook lookup, Transformer-based code predictor, and IFF-Net fusion to the overall ASR performance.
2. Evaluate the framework's performance on a diverse set of real-world noisy datasets beyond CHiME-4 to assess its generalization capabilities.
3. Analyze the codebook entries and speech features using t-SNE to visualize the clustering of clean speech features around codebook entries and assess the quality of the codebook prior.