---
ver: rpa2
title: Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
arxiv_id: '2310.06387'
source_url: https://arxiv.org/abs/2310.06387
tags:
- attack
- in-context
- demonstrations
- adversarial
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of in-context learning (ICL) to manipulate
  the alignment of large language models (LLMs). It proposes In-Context Attack (ICA)
  to jailbreak models by providing harmful demonstrations, and In-Context Defense
  (ICD) to guard against such attacks using safe demonstrations.
---

# Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations

## Quick Facts
- arXiv ID: 2310.06387
- Source URL: https://arxiv.org/abs/2310.06387
- Reference count: 16
- Primary result: In-context learning can manipulate LLM alignment, increasing jailbreak success rates from 0% to 44% with 5-shot demonstrations

## Executive Summary
This paper explores how in-context learning (ICL) can be leveraged to both attack and defend aligned large language models. The authors propose In-Context Attack (ICA) to jailbreak models by providing harmful demonstrations, and In-Context Defense (ICD) to guard against such attacks using safe demonstrations. Experiments show that ICA can significantly increase attack success rates on aligned models like Vicuna-7b, while ICD can effectively reduce the success of adversarial suffix attacks. The study highlights the potential of ICL to influence LLM behavior and provides insights for improving AI safety.

## Method Summary
The paper proposes two methods: In-Context Attack (ICA) and In-Context Defense (ICD). ICA jailbreaks models by providing harmful demonstrations in context, implicitly fine-tuning the model to generate harmful content. ICD guards against attacks by providing safe demonstrations, enhancing model robustness against adversarial prompts. Both methods rely on the model's ability to learn from demonstrations in the prompt context without explicit gradient computation. The effectiveness is measured by attack success rate (ASR) on Vicuna-7b and Llama2-7b-chat models.

## Key Results
- ICA increased attack success rate on Vicuna-7b from 0% to 44% with 5-shot demonstrations
- ICD reduced ASR of adversarial suffixes from 91% to 6% (individual behavior) and 96% to 0% (multiple behavior) on Vicuna-7b
- In-context demonstrations can significantly influence LLM behavior, both for harmful and safe outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning can manipulate LLM alignment by implicitly fine-tuning on demonstrations
- Mechanism: The model treats demonstrations as training examples, adjusting its behavior to match the demonstrated pattern before generating a response to the test prompt
- Core assumption: LLMs can perform implicit parameter updates through in-context examples without explicit gradient computation
- Evidence anchors:
  - [abstract]: "We offer theoretical insights to elucidate how a limited set of in-context demonstrations can pivotally influence the safety alignment of LLMs"
  - [section 4]: "The key insight here is that LLMs have the ability to implicitly fine-tune the model on the given demonstrations via the in-context demonstrations"
  - [corpus]: Weak - neighboring papers focus on different attack/defense methods, not in-context manipulation mechanisms
- Break condition: If the model cannot effectively generalize from the demonstrations to the test prompt, or if the implicit fine-tuning mechanism fails for the given model architecture

### Mechanism 2
- Claim: Adversarial demonstrations shift model behavior toward harmful outputs
- Mechanism: By providing examples of harmful queries with harmful responses, the model learns to associate similar queries with harmful outputs rather than safe refusals
- Core assumption: The model's response generation is influenced by the pattern of previous demonstrations in the context
- Evidence anchors:
  - [section 3.2]: "With only a few demonstrations of answering malicious queries, the model inevitably learns to be evil and starts to generate harmful content"
  - [section 3.2]: "By learning (xi, H(xi)) in the demonstrations, the model implicitly learns a more toxic parameter θ′"
  - [corpus]: Weak - neighboring papers focus on optimization-based attacks or detection methods, not demonstration-based attacks
- Break condition: If the model's safety alignment is too strong to be overridden by a small number of demonstrations, or if the model fails to generalize from the demonstration pattern

### Mechanism 3
- Claim: Safe demonstrations enhance model robustness against adversarial attacks
- Mechanism: By providing examples of harmful queries with safe refusals, the model learns to prioritize safety responses over harmful content generation
- Core assumption: The model can learn to recognize and resist harmful prompts based on the demonstration pattern
- Evidence anchors:
  - [section 3.3]: "ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts"
  - [section 3.3]: "ICD can effectively degrade the ASR of adversarial suffixes... from 91% to 6% (individual behavior)"
  - [corpus]: Weak - neighboring papers focus on different defense methods like perplexity filtering or erasure, not demonstration-based defense
- Break condition: If the adversarial attack is too strong to be mitigated by the safe demonstrations, or if the model fails to learn the safety pattern from the demonstrations

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The entire attack and defense mechanism relies on the model's ability to learn from demonstrations in the prompt context
  - Quick check question: How does in-context learning differ from fine-tuning, and what are its limitations?

- Concept: LLM safety alignment
  - Why needed here: Understanding how models are typically aligned to avoid harmful content is crucial for understanding how in-context demonstrations can manipulate this alignment
  - Quick check question: What are common methods for aligning LLMs to avoid harmful outputs, and how might they be vulnerable to in-context manipulation?

- Concept: Adversarial attack methodologies
  - Why needed here: To understand the threat landscape and how in-context attacks compare to other attack methods
  - Quick check question: What are the key differences between optimization-based attacks and demonstration-based attacks on LLMs?

## Architecture Onboarding

- Component map: LLM core model (Vicuna-7b, Llama2-7b-chat) -> Prompt assembly system (combines demonstrations with test prompt) -> Detection system (evaluates whether generated output is harmful) -> Adversarial attack generator (creates harmful prompts for testing)

- Critical path:
  1. Select appropriate demonstrations
  2. Assemble prompt with demonstrations + test prompt
  3. Generate response from LLM
  4. Evaluate attack success or defense effectiveness

- Design tradeoffs:
  - Demonstration selection vs. attack success rate
  - Number of demonstrations vs. computational efficiency
  - Demonstration relevance vs. attack transferability
  - Defense strength vs. response quality for legitimate queries

- Failure signatures:
  - Low attack success rate despite well-crafted demonstrations
  - Defense failure against strong adversarial attacks
  - Model generates nonsensical responses when demonstrations are added
  - Detection system fails to correctly identify harmful outputs

- First 3 experiments:
  1. Test ICA with varying numbers of demonstrations (1-shot to 5-shot) on a known aligned model
  2. Test ICD with different safe demonstration sets against a baseline adversarial attack
  3. Compare ICA attack success rates against optimization-based attacks under the same conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ICA and ICD vary with different model sizes and architectures?
- Basis in paper: [explicit] The paper mentions that they only examined the effectiveness on a few open-source LLMs and acknowledge that in-context learning ability is closely related to the utilized LLM.
- Why unresolved: The paper only tested on Vicuna-7b and Llama2-7b-chat models. There is no comprehensive evaluation across different model sizes and architectures.
- What evidence would resolve it: Experiments testing ICA and ICD on a wide range of LLMs with varying sizes (number of parameters) and architectures (e.g., transformer variants, encoder-decoder models).

### Open Question 2
- Question: What is the theoretical upper bound on the attack success rate for ICA, and how does it scale with the number of demonstrations?
- Basis in paper: [inferred] The paper shows increasing ASR with more demonstrations but doesn't provide a theoretical analysis of the scaling behavior or upper bound.
- Why unresolved: The paper presents empirical results but lacks a theoretical framework to predict the maximum achievable ASR or its relationship to demonstration count.
- What evidence would resolve it: A mathematical model or empirical study that characterizes the relationship between demonstration count and ASR, including identification of any saturation point.

### Open Question 3
- Question: How do different types of harmful prompts (e.g., violent vs. illegal vs. unethical) affect the success rate of ICA?
- Basis in paper: [explicit] The paper uses a general set of harmful prompts but doesn't analyze the effectiveness across different categories of harmful content.
- Why unresolved: The paper treats all harmful prompts uniformly without investigating whether certain types of malicious requests are more susceptible to ICA.
- What evidence would resolve it: A categorized analysis of ICA success rates across different types of harmful prompts, potentially revealing which categories are most vulnerable.

## Limitations
- Limited generalizability to other model architectures and sizes beyond Vicuna-7b and Llama2-7b-chat
- Narrow scope of adversarial attacks tested, focusing only on adversarial suffixes from Zou et al. [2023]
- Lack of analysis on long-term effects of in-context manipulation on model behavior

## Confidence
- **High Confidence**: The core mechanism of ICA and ICD, where in-context demonstrations can influence LLM behavior, is well-supported by the experimental results.
- **Medium Confidence**: The theoretical explanation for why in-context learning can manipulate model alignment is plausible but relies on assumptions about the model's ability to implicitly fine-tune on demonstrations.
- **Low Confidence**: The generalizability of the results to other models, attack types, and real-world scenarios is uncertain.

## Next Checks
1. Test ICA and ICD on a wider range of models, including larger models (e.g., GPT-3.5, GPT-4) and models with different safety alignment mechanisms.
2. Evaluate ICA and ICD against a broader set of adversarial attacks, including optimization-based attacks, prompt injection, and other types of jailbreak attempts.
3. Conduct experiments to investigate the long-term effects of in-context manipulation on model behavior, including testing whether the model "unlearns" the demonstrated patterns over time.