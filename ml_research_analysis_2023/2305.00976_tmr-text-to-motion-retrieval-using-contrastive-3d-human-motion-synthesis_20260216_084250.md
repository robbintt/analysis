---
ver: rpa2
title: 'TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis'
arxiv_id: '2305.00976'
source_url: https://arxiv.org/abs/2305.00976
tags:
- motion
- retrieval
- text
- motions
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles text-to-motion retrieval, where the goal is
  to retrieve 3D human motions from a database that match a given natural language
  query. The authors extend the TEMOS text-to-motion synthesis model by incorporating
  a contrastive loss to better structure the cross-modal latent space.
---

# TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis

## Quick Facts
- **arXiv ID:** 2305.00976
- **Source URL:** https://arxiv.org/abs/2305.00976
- **Reference count:** 40
- **Key outcome:** Extends TEMOS model with contrastive loss and negative filtering, achieving significant improvements in text-to-motion retrieval (median rank reduced from 54 to 19) on HumanML3D and KIT-ML datasets.

## Executive Summary
This paper addresses the text-to-motion retrieval task, aiming to find 3D human motions that match natural language queries. The authors extend the TEMOS text-to-motion synthesis model by incorporating a contrastive loss (InfoNCE) to better structure the cross-modal latent space and filtering out incorrect negative samples during training. Their approach significantly improves retrieval performance on standard benchmarks (HumanML3D and KIT-ML), reducing median rank from 54 to 19. The paper also demonstrates the potential of their approach for moment retrieval, showing reasonable results in temporally localizing text queries within long motion sequences.

## Method Summary
The method extends the TEMOS model by adding a contrastive loss component to structure the cross-modal latent space. The approach uses dual Transformer encoders (DistilBERT for text, direct sequence input for motion) to produce embeddings, a motion decoder for reconstruction, and an InfoNCE contrastive loss computed on these embeddings. During training, negatives are filtered based on text similarity (threshold of 0.8) to remove "wrong negatives" that could mislead learning. The total loss combines the TEMOS loss with the contrastive loss (λNCE=0.1, τ=0.1). Motions are represented using SMPL joint positions with canonicalized skeletons, and augmented with mirroring.

## Key Results
- **Performance improvement:** Median rank reduced from 54 to 19 on HumanML3D dataset
- **Significant recall gains:** R@3 improved from 27.08 to 41.93 on HumanML3D, and from 24.46 to 36.85 on KIT-ML
- **Ablation validation:** Both the contrastive loss and motion synthesis branch are shown to be important for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating a contrastive loss alongside the motion synthesis loss improves retrieval performance by structuring the cross-modal latent space with negative samples.
- **Mechanism:** The InfoNCE contrastive loss encourages motion and text embeddings from the same sample to be closer while pushing apart embeddings from different samples, improving discriminative power.
- **Core assumption:** The cross-modal latent space can be meaningfully structured through contrastive learning to distinguish semantically different motions and texts.
- **Evidence anchors:**
  - [abstract] "incorporates a contrastive loss to better structure the cross-modal latent space"
  - [section 3.2] "we incorporate a contrastive training with the usage of negative samples to better structure the latent space"
  - [corpus] Weak evidence; no direct citations in corpus papers about contrastive training for text-motion retrieval
- **Break condition:** If the negative sampling strategy is poor (e.g., including similar text-motion pairs), the contrastive loss may push apart embeddings that should be close, hurting retrieval performance.

### Mechanism 2
- **Claim:** Filtering out "wrong negatives" (text-motion pairs with highly similar descriptions) prevents the model from learning incorrect similarities and improves retrieval accuracy.
- **Mechanism:** By removing text-motion pairs whose descriptions have high similarity (>0.8 threshold), the model avoids being forced to push apart embeddings that should be close, leading to more robust learning.
- **Core assumption:** Text descriptions in motion datasets can be highly similar even for different motions, and these similarities can mislead contrastive learning if not filtered.
- **Evidence anchors:**
  - [abstract] "filter out incorrect negative samples during training to improve performance"
  - [section 3.3] "we discard pairs that have a text-text similarity in their labels more than a certain threshold"
  - [section 4.3] "When removing the synthesis branch and only experimenting with the contrastive loss... performance remains at 36.02 R@3 (compared to 41.93)"
- **Break condition:** If the similarity threshold is set too high, too many negatives will be filtered, reducing the effectiveness of contrastive learning. If set too low, incorrect negatives may still be included.

### Mechanism 3
- **Claim:** Maintaining the motion generation loss alongside contrastive training is crucial for good retrieval performance, as it forces the latent vector to capture the full content of the input text.
- **Mechanism:** The motion decoder branch, trained with reconstruction loss, ensures that the latent representation contains sufficient information about the motion, which aids in retrieval by providing a richer embedding.
- **Core assumption:** The motion synthesis task requires a more comprehensive understanding of the input text than contrastive learning alone, leading to better retrieval performance.
- **Evidence anchors:**
  - [abstract] "We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance"
  - [section 3.2] "We show in Section 4.2 that keeping this branch helps improving the results"
  - [section 4.3] "When removing the synthesis branch... performance drops significantly compared to keeping it"
- **Break condition:** If the motion decoder is poorly designed or the reconstruction loss is too weak, it may not provide meaningful regularization, and the contrastive loss alone might be sufficient.

## Foundational Learning

- **Concept:** Cross-modal contrastive learning (InfoNCE loss)
  - Why needed here: To structure the cross-modal latent space by pulling together matching text-motion pairs and pushing apart non-matching pairs.
  - Quick check question: What is the main difference between InfoNCE and a simple margin-based contrastive loss, and why is InfoNCE preferred in this work?

- **Concept:** Negative sampling strategies
  - Why needed here: To ensure that the contrastive loss is computed on informative negative pairs, avoiding pairs that should actually be close due to similar descriptions.
  - Quick check question: How does the filtering of "wrong negatives" based on text similarity improve the quality of negative samples?

- **Concept:** Motion representation and encoding
  - Why needed here: To convert 3D human motions into a format suitable for the encoder and contrastive learning, capturing relevant features for retrieval.
  - Quick check question: What are the key components of the motion representation used in this work, and why is this representation chosen?

## Architecture Onboarding

- **Component map:** Text input → DistilBERT Encoder → Text Embedding → Contrastive Loss; Motion input → Transformer Encoder → Motion Embedding → Contrastive Loss + Motion Decoder → Output
- **Critical path:** Text/Motion input → Encoder → Embedding → Contrastive Loss + Motion Decoder → Output
- **Design tradeoffs:**
  - Using a pre-trained DistilBERT vs training a text encoder from scratch: Pre-trained offers better text understanding but may not be optimal for motion descriptions.
  - InfoNCE vs margin-based contrastive loss: InfoNCE is more effective but requires more computation.
  - Filtering negatives vs using all negatives: Filtering improves quality but may reduce the number of negatives.
- **Failure signatures:**
  - Poor retrieval performance: Could indicate issues with contrastive learning, filtering, or motion representation.
  - Unstable training: Could indicate issues with the balance between contrastive and reconstruction losses.
- **First 3 experiments:**
  1. Compare retrieval performance with and without the contrastive loss to verify its importance.
  2. Test different thresholds for filtering negatives to find the optimal value.
  3. Compare InfoNCE with a margin-based contrastive loss to confirm its superiority.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal threshold value for filtering negatives during contrastive training?
- **Basis in paper:** [explicit] The paper states that a threshold of 0.8 performs well overall, but also experiments with values from 0.55 to 0.95.
- **Why unresolved:** The optimal threshold may depend on the specific dataset and task. The paper does not provide a systematic study to determine the best threshold.
- **What evidence would resolve it:** A comprehensive ablation study testing various threshold values on multiple datasets and tasks, reporting performance metrics for each.

### Open Question 2
- **Question:** How does the choice of latent dimensionality affect the performance of the text-to-motion retrieval model?
- **Basis in paper:** [explicit] The paper experiments with latent dimensionalities of 64, 128, 256, and 512, finding that 128 performs best overall.
- **Why unresolved:** The optimal latent dimensionality may depend on the specific dataset and task. The paper does not provide a systematic study to determine the best dimensionality.
- **What evidence would resolve it:** A comprehensive ablation study testing various latent dimensionalities on multiple datasets and tasks, reporting performance metrics for each.

### Open Question 3
- **Question:** How does the choice of contrastive loss function affect the performance of the text-to-motion retrieval model?
- **Basis in paper:** [explicit] The paper compares the InfoNCE loss with a margin-based loss, finding that InfoNCE performs significantly better.
- **Why unresolved:** The paper does not explore other contrastive loss functions or provide a systematic study to determine the best loss function.
- **What evidence would resolve it:** A comprehensive ablation study testing various contrastive loss functions on multiple datasets and tasks, reporting performance metrics for each.

## Limitations

- The negative filtering mechanism's optimal threshold may be dataset-dependent and requires further study to determine the best value for different scenarios.
- The necessity of maintaining the motion synthesis branch alongside contrastive learning, while demonstrated, hasn't been explored with alternative regularization methods.
- Moment retrieval results are preliminary and presented as proof-of-concept without comprehensive evaluation across different scenarios.

## Confidence

- **Mechanism 1 (contrastive loss improves retrieval):** High
- **Mechanism 2 (negative filtering prevents incorrect similarity learning):** Medium
- **Mechanism 3 (motion synthesis loss is crucial):** Medium
- **Moment retrieval capability:** Low

## Next Checks

1. **Sensitivity analysis of negative filtering threshold:** Systematically vary the text similarity threshold (0.7, 0.8, 0.9) and measure the impact on retrieval performance across both datasets to determine if 0.8 is optimal or dataset-dependent.

2. **Alternative regularization experiments:** Remove the motion synthesis branch and replace it with other forms of regularization (e.g., additional contrastive terms, dropout-based consistency) to test whether the motion generation task is uniquely important or if other methods could provide similar benefits.

3. **Cross-dataset generalization test:** Train the model on one dataset (e.g., HumanML3D) and evaluate on the other (KIT-ML) to assess whether the learned cross-modal structure generalizes beyond the training distribution, which would validate the robustness of the contrastive learning approach.