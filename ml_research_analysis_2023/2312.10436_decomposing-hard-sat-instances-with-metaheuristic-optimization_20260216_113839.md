---
ver: rpa2
title: Decomposing Hard SAT Instances with Metaheuristic Optimization
arxiv_id: '2312.10436'
source_url: https://arxiv.org/abs/2312.10436
tags:
- formula
- algorithm
- some
- time
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces decomposition hardness (d-hardness) as a
  measure of how hard a specific Boolean formula is for a particular SAT solver. The
  key idea is to decompose a formula into simpler subproblems by fixing subsets of
  variables and measuring the expected solver runtime over all such subproblems.
---

# Decomposing Hard SAT Instances with Metaheuristic Optimization

## Quick Facts
- arXiv ID: 2312.10436
- Source URL: https://arxiv.org/abs/2312.10436
- Reference count: 40
- This paper introduces decomposition hardness (d-hardness) as a measure of how hard a specific Boolean formula is for a particular SAT solver.

## Executive Summary
This paper introduces decomposition hardness (d-hardness) as a measure of how hard a specific Boolean formula is for a particular SAT solver. The key idea is to decompose a formula into simpler subproblems by fixing subsets of variables and measuring the expected solver runtime over all such subproblems. This expectation can be estimated via Monte Carlo sampling. The paper formulates finding the subset with minimal d-hardness as a pseudo-Boolean optimization problem and solves it using a genetic algorithm. It also shows how to use d-hardness to generate and verify unsatisfiability proofs more efficiently. Experimental results on hard SAT instances demonstrate that using decomposition sets found by this method can significantly reduce solving and proof-checking times compared to using the original formulas directly.

## Method Summary
The method decomposes hard SAT instances by finding variable subsets that minimize d-hardness, a measure of expected solver runtime over subproblems generated by fixing assignments to those variables. This is formulated as a pseudo-Boolean optimization problem solved via genetic algorithms with Monte Carlo sampling for statistical estimation. The approach incorporates incremental preprocessing using ρ-backdoors to accelerate estimation by detecting polynomially solvable subproblems. Experimental validation uses hard unsatisfiable SAT instances from sgen generator and LEC problem encodings, with complete SAT solvers like CaDiCaL and Glucose 3.

## Key Results
- Decomposition hardness (d-hardness) provides a tractable upper bound for SAT instance hardness by estimating expected solver runtime over subproblems.
- Metaheuristic optimization using genetic algorithms efficiently finds variable subsets minimizing d-hardness, despite the lack of analytical form for the objective function.
- Incorporating ρ-backdoor structure and incremental preprocessing accelerates d-hardness estimation by reducing the number of hard subproblems requiring full solver evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: d-hardness provides a tractable upper bound for SAT instance hardness by estimating expected solver runtime over subproblems generated by fixing variable subsets.
- Mechanism: For a chosen subset B of variables, the algorithm generates 2^|B| subproblems by fixing all assignments to B and measuring solver runtime on each. The d-hardness is the expected value of a random variable representing these runtimes, estimated via Monte Carlo sampling.
- Core assumption: The solver is deterministic and complete, ensuring finite runtime on each subproblem and enabling statistical estimation.
- Evidence anchors:
  - [abstract] introduces d-hardness as "the expected solver runtime over all subproblems obtained by fixing subsets of variables."
  - [section 4] proves that d-hardness equals 2^|B| times the expectation of a random variable defined over the subproblem runtimes.
- Break condition: If the solver is non-deterministic or incomplete, the random variable may not have a finite expectation, breaking the hardness estimation.

### Mechanism 2
- Claim: The optimization problem of finding B with minimal d-hardness can be solved efficiently using metaheuristic algorithms because the objective function is pseudo-Boolean and lacks an analytical form.
- Mechanism: The fitness function F(B) = 2^|B| * (1/N) * sum of solver runtimes over N random samples is minimized by exploring the Boolean hypercube {0,1}^|X| with genetic algorithms, using statistical estimates of E[ξB] and Var(ξB) to control sample size.
- Core assumption: The search space can be reduced to a smaller set B0 where d-hardness is efficiently computable, and the function landscape is amenable to heuristic search.
- Evidence anchors:
  - [section 5] formulates the search as minimizing a pseudo-Boolean function and applies a genetic algorithm with elitist selection and heavy-tailed mutation.
  - [section 6] describes heuristics to construct B0 based on variable reduction measures or weight heuristics, ensuring tractable evaluation of F(B).
- Break condition: If the reduced search space B0 does not contain a low d-hardness B, the algorithm may converge to suboptimal solutions.

### Mechanism 3
- Claim: Incorporating ρ-backdoor structure and incremental preprocessing accelerates d-hardness estimation by reducing the number of hard subproblems that require full solver evaluation.
- Mechanism: Simple subproblems C[β/B] ∈ S(P) (solvable by polynomial sub-solver P) are detected incrementally, and their runtimes are measured by the simpler solver. This reduces the variance of ξB and speeds up estimation, especially when ρ is close to 1.
- Core assumption: A significant fraction of subproblems are polynomially solvable, so incremental checking with unit propagation is valid and efficient.
- Evidence anchors:
  - [section 7] connects d-hardness to ρ-backdoors, showing that using multiple ρ-backdoors can further reduce solving time.
  - [section 9.3] demonstrates experimentally that incremental preprocessing leads to faster convergence and smaller d-hardness estimates.
- Break condition: If most subproblems are hard (ρ close to 0), incremental preprocessing provides little benefit and may add overhead.

## Foundational Learning

- Concept: Strong Backdoor Sets (SBS)
  - Why needed here: d-hardness builds on the SBS concept by generalizing from polynomial sub-solvers to complete SAT solvers, enabling hardness estimation for any deterministic complete solver.
  - Quick check question: What is the difference between an SBS and a ρ-backdoor?
- Concept: Monte Carlo estimation and statistical confidence
  - Why needed here: d-hardness relies on estimating expected solver runtime from a finite sample; understanding Chebyshev's inequality and (ε,δ)-approximations is essential for interpreting results.
  - Quick check question: How does increasing sample size N affect the confidence interval for d-hardness?
- Concept: Genetic algorithms and metaheuristic optimization
  - Why needed here: The fitness function for d-hardness lacks a closed form, so heuristic search (e.g., genetic algorithms) is required to find low d-hardness subsets B efficiently.
  - Quick check question: Why is elitist selection important in the genetic algorithm used here?

## Architecture Onboarding

- Component map:
  - SAT solver interface (deterministic, complete) -> Variable subset generator (encodes B as Boolean vector) -> Subproblem generator (fixes B assignments, simplifies formula) -> Runtime measurement module (records solver time, conflicts, or propagations) -> Monte Carlo estimator (computes sample mean/variance, checks (ε,δ) condition) -> Genetic algorithm engine (population management, selection, crossover, mutation) -> Search space reducer (heuristics to build B0 from original formula) -> ρ-backdoor preprocessor (incremental unit propagation check) -> Proof checker (DRAT-trim integration for unsatisfiability proofs)
- Critical path:
  1. Initialize with reduced search space B0 and small B.
  2. For each candidate B, generate N random β samples.
  3. Use incremental UP to classify subproblems as simple or hard.
  4. Measure runtimes only for hard subproblems; use UP stats for simple ones.
  5. Update statistical estimates; if (ε,δ) condition not met, increase N.
  6. Apply genetic algorithm to evolve B population.
  7. Output best B and optionally generate decomposed proofs.
- Design tradeoffs:
  - Larger N → higher accuracy but slower per-evaluation.
  - Larger B0 → better chance of finding good B but higher evaluation cost.
  - Using conflicts vs. propagations vs. time → different noise characteristics and interpretability.
  - Single vs. multiple backdoors → simpler control vs. potentially lower overall hardness.
- Failure signatures:
  - High variance in ξB estimates → indicates heavy-tailed solver behavior or insufficient N.
  - Genetic algorithm stagnation → suggests poor diversity or inadequate search space reduction.
  - Proof checking slower than solving → may indicate suboptimal K for proof aggregation.
- First 3 experiments:
  1. Run d-hardness estimation on a small known-hard formula (e.g., Pigeonhole) with N=1000 and verify monotonic convergence of sample mean.
  2. Compare genetic algorithm performance with and without incremental UP preprocessing on a medium-sized LEC formula.
  3. Measure proof checking time speedup using single vs. pair of backdoors on an unsatisfiable sgen instance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal solver workload measure (conflicts, propagations, or time) for d-hardness estimation across different SAT instances?
- Basis in paper: [explicit] The paper compares these three measures and finds no significant difference in effectiveness for the tested instance, but recommends propagations based on statistical tests.
- Why unresolved: Different SAT instances may have varying characteristics that make different measures more or less effective, and the paper only tested on a limited set of instances.
- What evidence would resolve it: Systematic experiments comparing all three measures across diverse SAT instance classes and solver types, with statistical analysis of which measure consistently yields better d-hardness estimates and solving performance.

### Open Question 2
- Question: How can we efficiently estimate the variance of the random variable ξB in the d-hardness formulation without extensive sampling?
- Basis in paper: [explicit] The paper notes that the variance can be very large due to heavy-tailed behavior, making the required sample size for accurate estimation potentially impractical, but does not provide a solution.
- Why unresolved: The variance directly impacts the confidence in d-hardness estimates and the efficiency of the optimization process, but no method is proposed to estimate it without actually running the solver on many subproblems.
- What evidence would resolve it: Development of a theoretical or empirical model that predicts the variance of ξB based on formula characteristics (e.g., clause-variable ratio, structural properties) without requiring extensive sampling.

### Open Question 3
- Question: What is the optimal number and selection strategy for multiple decomposition sets (backdoors) to minimize overall solving time?
- Basis in paper: [explicit] The paper shows that using pairs or triples of backdoors can improve solving time compared to single backdoors, but does not provide a systematic method for selecting the optimal number or composition of backdoors.
- Why unresolved: While the paper demonstrates benefits of using multiple backdoors, it leaves open questions about how to choose which backdoors to use and in what combination to achieve optimal performance.
- What evidence would resolve it: Experimental results comparing solving times across different numbers and combinations of backdoors for various SAT instance classes, potentially leading to a heuristic or algorithm for selecting optimal backdoor sets.

## Limitations
- The assumed convexity or smoothness of the d-hardness landscape is not empirically characterized, which is critical for the genetic algorithm's success.
- The connection between d-hardness minimization and practical performance gains is demonstrated but not theoretically established.
- The claimed generality of d-hardness as a hardness measure across different solver types and problem domains lacks comprehensive validation.

## Confidence
**High Confidence**: The basic statistical framework for d-hardness estimation using Monte Carlo sampling is well-established and mathematically sound.

**Medium Confidence**: The genetic algorithm implementation details and their effectiveness for this specific optimization problem.

**Low Confidence**: The claimed generality of d-hardness as a hardness measure across different solver types and problem domains.

## Next Checks
1. **Landscape Analysis**: Characterize the d-hardness fitness landscape for several benchmark instances by sampling and visualizing the objective function to verify smoothness and identify potential local optima.

2. **Solver Diversity Test**: Repeat the core experiments using multiple different SAT solvers (both deterministic and non-deterministic) to assess the robustness of d-hardness as a hardness measure across solver implementations.

3. **Ablation Study**: Systematically remove components of the framework (incremental preprocessing, heavy-tailed mutation, elitist selection) to quantify their individual contributions to the reported performance improvements.