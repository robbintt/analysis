---
ver: rpa2
title: Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural
  Networks
arxiv_id: '2304.10749'
source_url: https://arxiv.org/abs/2304.10749
tags:
- neural
- architecture
- spiking
- search
- mse-nas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing efficient spiking
  neural network (SNN) architectures by proposing a multi-scale evolutionary neural
  architecture search (MSE-NAS) algorithm. The key idea is to simultaneously evolve
  the microscopic, mesoscopic, and macroscopic neural topologies inspired by the human
  brain's multi-scale neural circuits.
---

# Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks

## Quick Facts
- arXiv ID: 2304.10749
- Source URL: https://arxiv.org/abs/2304.10749
- Reference count: 40
- Primary result: Multi-scale evolutionary NAS achieves state-of-the-art SNN performance on static and neuromorphic datasets with fewer simulation steps.

## Executive Summary
This paper introduces a Multi-scale Evolutionary Neural Architecture Search (MSE-NAS) algorithm that simultaneously optimizes microscopic, mesoscopic, and macroscopic neural topologies for deep spiking neural networks (SNNs). The method uses a genetic algorithm to evolve SNN architectures encoded as genotypes across three scales, inspired by brain neural circuit organization. A novel brain-inspired indirect evaluation method based on representational dissimilarity matrices (RDMs) is proposed to assess fitness without requiring full training, significantly reducing computational cost. The approach achieves superior performance on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128-Gesture datasets compared to existing SNN-based NAS methods while using fewer simulation steps.

## Method Summary
MSE-NAS encodes SNN architectures as genotypes with three segments: micro-scale (neuron-level operations), meso-scale (neural motif combinations), and macro-scale (global connectivity patterns). A genetic algorithm with tournament selection, k-point crossover, and polynomial mutation evolves populations of these genotypes. Instead of direct accuracy-based fitness evaluation, the method uses representational dissimilarity matrices (RDMs) computed from firing patterns to assess representational quality in a training-free manner. The top individuals from evolution undergo surrogate gradient-based training for 600 epochs, with the best-performing model selected for deployment.

## Key Results
- Achieves state-of-the-art classification accuracy on both static (CIFAR10, CIFAR100) and neuromorphic (CIFAR10-DVS, DVS128-Gesture) datasets
- Reduces computational cost through RDM-based indirect fitness evaluation without requiring full network training
- Demonstrates improved energy efficiency by using fewer simulation steps compared to existing SNN NAS methods
- Shows effectiveness of multi-scale encoding in discovering architectures with balanced excitation-inhibition dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale encoding enables simultaneous optimization of neuron-level operations, motif combinations, and global connectivity.
- Mechanism: The genotype is split into three scales—micro (X), meso (M), and macro (G)—allowing evolution to optimize operations per neuron, motif assembly, and cross-layer connections independently yet in coordination.
- Core assumption: Encapsulating scale-specific traits in distinct genome segments preserves biological interpretability while enabling efficient search.
- Evidence anchors:
  - [abstract] "simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space"
  - [section] "We combine the genetic algorithm to optimize SNN architecture from multiple scales"
  - [corpus] Weak: no direct comparison with single-scale encoding in the cited corpus.
- Break condition: If the genotype length becomes too large, the search space grows exponentially and may hinder convergence.

### Mechanism 2
- Claim: Representational Dissimilarity Matrices (RDMs) serve as a training-free fitness function that reduces computational cost.
- Mechanism: RDMs measure the representational dissimilarity of firing patterns across samples, capturing abstraction ability without requiring explicit training.
- Core assumption: The dissimilarity structure of neural responses reflects model quality sufficiently for evolutionary selection.
- Evidence anchors:
  - [abstract] "design a brain-inspired indirect evaluation method... Representational Dissimilarity Matrices (RDMs)"
  - [section] "we use a method from electrophysiology to compare representation difference matrices (RDMs)"
  - [corpus] Weak: corpus does not mention RDMs as a fitness function.
- Break condition: If the dataset is too small or lacks diversity, RDM-based evaluation may not capture meaningful representational differences.

### Mechanism 3
- Claim: Evolutionary search over motif combinations produces architectures with balanced excitation-inhibition dynamics.
- Mechanism: By encoding motif types (FE, FI, FbI, LI, MI) and evolving their spatial arrangement, the method finds combinations that avoid over-inhibition and promote self-organized criticality.
- Core assumption: Certain motif combinations inherently lead to balanced E/I dynamics, which improves performance.
- Evidence anchors:
  - [abstract] "evolves coordinated combinations of multiple common neural circuit motifs"
  - [section] "the excitatory-inhibitory (E/I) shows a balance, called self-organized criticality"
  - [corpus] Weak: no direct mention of E/I balance metrics in the cited corpus.
- Break condition: If motif combinations become too constrained, diversity in search space may drop, leading to suboptimal solutions.

## Foundational Learning

- Concept: Spiking Neural Networks (SNNs)
  - Why needed here: MSE-NAS operates specifically on SNNs, not traditional ANNs, and leverages their event-driven nature.
  - Quick check question: What distinguishes an SNN's computation from an ANN's during inference?

- Concept: Genetic Algorithm in Neural Architecture Search
  - Why needed here: The method uses tournament selection, crossover, and mutation to evolve SNN genotypes over generations.
  - Quick check question: How does a population-based evolutionary search differ from gradient-based NAS?

- Concept: Representational Dissimilarity Matrices (RDMs)
  - Why needed here: RDMs are the core of the training-free fitness evaluation that makes MSE-NAS computationally efficient.
  - Quick check question: How does the Manhattan distance between firing patterns in RDMs serve as a proxy for model quality?

## Architecture Onboarding

- Component map:
  Genotype encoding (micro/meso/macro) -> Decoding into SNN phenotypes -> RDM-based fitness evaluation -> Genetic operators (selection/crossover/mutation) -> Surrogate gradient training -> Final model deployment

- Critical path:
  1. Initialize population with random genotypes
  2. Decode genotypes into SNN phenotypes
  3. Evaluate phenotypes using RDMs
  4. Select parents and generate offspring
  5. Repeat for fixed generations
  6. Train best individuals with surrogate gradients
  7. Deploy the highest performing model

- Design tradeoffs:
  - Trade-off: Longer genotype length increases expressiveness but also search space size
  - Trade-off: RDM-based fitness is fast but may be less precise than accuracy-based evaluation
  - Trade-off: Macro-scale cross-layer connectivity increases model capacity but may complicate training stability

- Failure signatures:
  - Population fitness plateaus early → Possible insufficient mutation rate or overly constrained genotype
  - High variance in final accuracy → Possible overfitting to small evaluation batch in RDM computation
  - Models fail to converge during surrogate gradient training → Possible architectural mismatch between evolved motifs and training dynamics

- First 3 experiments:
  1. Verify RDM computation by feeding a known pattern and checking expected dissimilarity scores.
  2. Test mutation and crossover operators by evolving a small population and inspecting genotype diversity over generations.
  3. Compare training-free RDM fitness ranking with full training accuracy ranking on a fixed set of SNN architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the multi-scale brain-inspired topological evolutions in MSE-NAS translate to SNNs with different neuron models beyond LIF (e.g., adaptive exponential integrate-and-fire or Hodgkin-Huxley models)?
- Basis in paper: [inferred] The paper focuses on LIF neurons and discusses potential for broader neuron models but does not test them.
- Why unresolved: The study only validates MSE-NAS with LIF neurons, leaving the applicability to more complex biological neuron models unexplored.
- What evidence would resolve it: Experiments applying MSE-NAS to SNNs using adaptive exponential integrate-and-fire or Hodgkin-Huxley models and comparing performance against LIF-based results.

### Open Question 2
- Question: What is the impact of varying the indirect evaluation method (e.g., different distance metrics or fitness functions) on the diversity and generalization capability of evolved SNN architectures?
- Basis in paper: [explicit] The paper mentions experimenting with different distance metrics but does not deeply explore their impact on architecture diversity or generalization.
- Why unresolved: While the paper shows MSE-NAS works with different metrics, it does not analyze how these choices affect the evolved architectures' diversity or ability to generalize across tasks.
- What evidence would resolve it: A systematic study comparing architecture diversity and cross-task generalization under different indirect evaluation methods.

### Open Question 3
- Question: How does the inclusion of inhibitory neurons and lateral inhibition in neural motifs affect the energy efficiency of SNNs in terms of spike count and power consumption compared to purely excitatory networks?
- Basis in paper: [explicit] The paper discusses the role of inhibitory connections and E/I balance but does not quantify their impact on energy efficiency metrics.
- Why unresolved: The study focuses on performance gains but does not measure how inhibitory circuits influence spike count or power consumption.
- What evidence would resolve it: Comparative experiments measuring spike count and estimated power consumption for SNNs with and without inhibitory connections under the same tasks.

## Limitations

- The RDM-based fitness function, while computationally efficient, lacks direct validation of its correlation with final accuracy
- Claims about biological interpretability of multi-scale encoding are theoretical without physiological validation
- The predefined motif set may not be sufficient to discover optimal architectures for all tasks
- Fitness evaluation relies on mini-batches but batch size and variance are not specified

## Confidence

- High confidence: The multi-scale genotype encoding and evolutionary algorithm mechanics are clearly specified and reproducible
- Medium confidence: The RDM-based indirect fitness evaluation is well-motivated and plausible, but lacks direct validation of its correlation with final accuracy
- Low confidence: Claims about self-organized criticality and biological interpretability are not empirically supported in the paper

## Next Checks

1. Perform an ablation study comparing RDM-based fitness rankings with accuracy-based fitness rankings on a fixed set of SNN architectures to quantify the correlation
2. Analyze the distribution of excitatory-inhibitory ratios in evolved architectures and compare with ablation results to validate motif balance claims
3. Test the sensitivity of the evolutionary search by varying the RDM mini-batch size and measuring the variance in final accuracy of the best individuals