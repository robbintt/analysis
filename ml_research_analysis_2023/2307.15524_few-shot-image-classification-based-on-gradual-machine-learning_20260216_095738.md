---
ver: rpa2
title: Few-shot Image Classification based on Gradual Machine Learning
arxiv_id: '2307.15524'
source_url: https://arxiv.org/abs/2307.15524
tags:
- learning
- few-shot
- image
- classification
- gradual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to few-shot image classification
  based on gradual machine learning (GML). The proposed method addresses the challenge
  of transferring knowledge from training classes to new ones with limited labeled
  samples.
---

# Few-shot Image Classification based on Gradual Machine Learning

## Quick Facts
- **arXiv ID**: 2307.15524
- **Source URL**: https://arxiv.org/abs/2307.15524
- **Reference count**: 40
- **Key outcome**: Improves few-shot classification accuracy by 1-5% over state-of-the-art using gradual machine learning with dual-backbone feature extraction

## Executive Summary
This paper introduces a novel approach to few-shot image classification using gradual machine learning (GML), which incrementally labels query samples in order of evidential certainty through iterative factor inference. The method extracts discriminative features using both ResNet-12 and WRN-28-10 backbones, constructs monotonic unary and binary factors based on class center distance and k-nearest neighborhood, and performs transductive learning on a factor graph. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance improvements and robustness to query set size variations.

## Method Summary
The approach combines dual deep backbone feature extraction with a gradual machine learning framework. Features are extracted from both ResNet-12 and WRN-28-10 networks trained using the EASY methodology with classification and self-supervised rotation tasks. For each test episode, unary factors (based on class center distance) and binary factors (based on k-nearest neighborhood) are constructed in a factor graph initialized with labeled support samples. Iterative factor inference then labels query samples one at a time in order of increasing entropy, with newly labeled samples becoming evidence for subsequent iterations. The method operates in a transductive setting, accessing all query samples simultaneously to leverage sample relationships during inference.

## Key Results
- Achieves 1-5% accuracy improvement over state-of-the-art on MiniImageNet, TieredImageNet, Cifar-FS, and CUB-200-2011 datasets
- Demonstrates consistent performance improvement as query set size increases, unlike existing deep models
- Shows complementary benefits from using both ResNet-12 and WRN-28-10 backbones through ablation studies
- Maintains robust performance across 5-way 1-shot and 5-way 5-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradual Machine Learning (GML) improves few-shot classification by incrementally labeling samples in order of evidential certainty rather than using a single global model.
- Mechanism: The factor graph framework enables iterative factor inference where unlabeled samples are ranked by entropy and labeled one at a time, with newly labeled samples becoming evidence for subsequent iterations.
- Core assumption: Monotonic features (CCD and KNN) provide reliable probabilistic evidence that correlates with true class membership.
- Evidence anchors:
  - [abstract] "begins with only a few labeled observations, and then gradually labels target images in the increasing order of hardness by iterative factor inference in a factor graph"
  - [section 4.3] "GML fulfills gradual learning by iterative factor inference on a factor graph...GML labels only one image at each iteration"
  - [corpus] Weak - no direct corpus evidence for GML specifically, though related work on gradual community detection exists
- Break condition: When monotonic feature extraction fails to capture discriminative class boundaries, the iterative inference process will propagate incorrect labels.

### Mechanism 2
- Claim: Using two complementary deep backbones (ResNet-12 and WRN-28-10) provides diverse feature representations that improve gradual learning.
- Mechanism: Different network architectures extract different aspects of image features, and the factor graph can fuse these complementary views to make more robust labeling decisions.
- Core assumption: ResNet-12 and WRN-28-10 capture complementary information despite both being based on residual connections.
- Evidence anchors:
  - [section 5.1] "Our ablation study has shown that the ResNet-12 and WRN-28-10 networks are to some extent complementary to each other"
  - [section 6.3] "the GML using both of them performs considerably better that the alternatives using either of them"
  - [corpus] Weak - corpus contains SAR image classification but no specific evidence for dual-backbone approaches
- Break condition: If both backbones extract similar features (high correlation in embedding space), the complementary benefit disappears.

### Mechanism 3
- Claim: Transductive learning with joint inference on all query samples outperforms inductive approaches by leveraging sample relationships.
- Mechanism: By accessing all query samples during inference, the model can propagate class information through the factor graph using both unary (CCD) and binary (KNN) relationships.
- Core assumption: The relationships between query samples contain useful information for classification that isn't available when samples are classified independently.
- Evidence anchors:
  - [abstract] "it is more robust than the existing deep models in that its performance can consistently improve as the size of query set increases"
  - [section 2.1] "transductive learning performs class label inference jointly for all the unlabeled samples"
  - [corpus] Moderate - retrieval-augmented few-shot classification paper supports transductive approaches
- Break condition: When query set relationships are noisy or uninformative, joint inference may introduce more error than it corrects.

## Foundational Learning

- **Concept**: Factor graphs and iterative belief propagation
  - Why needed here: The entire GML approach relies on constructing a factor graph and performing iterative inference to gradually label samples
  - Quick check question: What is the difference between unary and binary factors in the context of this paper?

- **Concept**: Monotonic feature extraction and sigmoid influence modeling
  - Why needed here: CCD and KNN features are designed to be monotonic with respect to class probability, enabling reliable sigmoid-based influence modeling
  - Quick check question: How does the sigmoid function transform a distance metric into a probability estimate?

- **Concept**: Deep feature extraction and embedding spaces
  - Why needed here: The paper leverages pre-trained deep networks to extract discriminative features that serve as the foundation for all subsequent gradual learning
  - Quick check question: Why might two different backbones (ResNet-12 and WRN-28-10) extract complementary features?

## Architecture Onboarding

- **Component map**: Backbone feature extraction (ResNet-12 + WRN-28-10) -> Monotonic feature construction (CCD unary, KNN binary) -> Factor graph construction -> Iterative gradual inference
- **Critical path**: Feature extraction → Feature construction → Factor graph construction → Iterative inference
- **Design tradeoffs**: Using two backbones increases computational cost but improves accuracy; joint transductive inference requires access to all query samples simultaneously
- **Failure signatures**: (1) If CCD/KNN features don't correlate with class membership, inference will fail early; (2) If entropy ranking is inaccurate, the algorithm may label hard samples too early
- **First 3 experiments**:
  1. Implement single-backbone version (only ResNet-12) and compare accuracy to dual-backbone baseline
  2. Replace gradual inference with direct transductive label propagation (e.g., LaplacianShot) to quantify the benefit of incremental labeling
  3. Test performance degradation when using random vs. monotonic features for factor construction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed gradual machine learning (GML) approach perform on other few-shot learning tasks, such as object detection or image segmentation, beyond image classification?
- Basis in paper: [explicit] The authors mention that the GML approach is potentially applicable to other few-shot learning tasks, but they do not provide detailed technical solutions or experimental results for these tasks.
- Why unresolved: The paper focuses on evaluating the GML approach for few-shot image classification, leaving the exploration of its performance on other tasks as future work.
- What evidence would resolve it: Conducting experiments to compare the performance of GML on various few-shot learning tasks, such as object detection or image segmentation, against state-of-the-art methods.

### Open Question 2
- Question: Can the deep feature generalization bottleneck be overcome by designing new backbones specifically tailored for feature extraction in few-shot gradual learning?
- Basis in paper: [explicit] The authors acknowledge that deep feature generalization with only a few labeled samples remains a major performance bottleneck in their GML approach. They suggest investigating new backbone designs for feature extraction that can more effectively support gradual learning.
- Why unresolved: The paper uses existing backbones and training procedures for feature extraction, but the potential benefits of designing new backbones specifically for few-shot gradual learning are not explored.
- What evidence would resolve it: Designing and training new backbone architectures specifically tailored for feature extraction in few-shot gradual learning, and comparing their performance against existing backbones in the GML framework.

### Open Question 3
- Question: How does the performance of GML change with different values of the k-nearest neighbors (KNN) parameter for binary feature extraction, and what is the optimal range for this parameter?
- Basis in paper: [explicit] The authors mention that they set k=6 by default for KNN feature extraction, but they also conduct a parameter sensitivity study by varying k from 5 to 7. The results show that the performance of GML only fluctuates marginally within this range.
- Why unresolved: The paper does not explore a wider range of k values or determine the optimal range for this parameter in the context of GML.
- What evidence would resolve it: Conducting a more extensive parameter sensitivity study with a wider range of k values, and identifying the optimal range for the KNN parameter in the GML framework based on performance metrics.

## Limitations

- **Transductive requirement**: The approach requires access to all query samples simultaneously, limiting applicability in streaming or online scenarios
- **Computational overhead**: Iterative factor graph inference introduces significant computational cost compared to direct classification methods
- **Feature dependency**: Performance critically depends on the quality of monotonic features, which may not generalize well to domains with complex decision boundaries

## Confidence

- **High Confidence**: The general framework of gradual machine learning and factor graph construction is well-established and clearly described. The experimental methodology and dataset choices are appropriate for the task.
- **Medium Confidence**: The claims about complementary feature extraction from dual backbones are supported by ablation studies, though the specific mechanisms of complementarity could be more thoroughly analyzed. The performance improvements over state-of-the-art are demonstrated but may be partially attributed to the transductive setting rather than the gradual inference mechanism itself.
- **Low Confidence**: The scalability claims regarding large factor graphs lack detailed analysis of computational complexity and memory requirements. The robustness claims related to query set size increases need more systematic validation across diverse dataset characteristics.

## Next Checks

1. **Feature Space Analysis**: Quantify the correlation between CCD/KNN feature values and actual class membership probabilities across different datasets to verify the monotonicity assumption underlying the factor construction.
2. **Computational Efficiency Benchmark**: Measure wall-clock time and memory usage of the gradual inference process compared to baseline methods (LaplacianShot, PT-MAP) across varying query set sizes and number of classes to validate scalability claims.
3. **Distribution Shift Robustness**: Evaluate performance when training and test classes have different underlying distributions (e.g., using disjoint subsets of ImageNet for training and testing) to assess the method's ability to generalize beyond the i.i.d. assumption.