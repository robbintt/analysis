---
ver: rpa2
title: A Quasi-Wasserstein Loss for Learning Graph Neural Networks
arxiv_id: '2310.11762'
source_url: https://arxiv.org/abs/2310.11762
tags:
- graph
- learning
- loss
- optimal
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Quasi-Wasserstein (QW) loss function
  for training graph neural networks (GNNs) in node-level prediction tasks. The key
  idea is to formulate the loss as an optimal transport problem on the graph, measuring
  the discrepancy between observed and predicted node labels via a label transport
  defined on graph edges.
---

# A Quasi-Wasserstein Loss for Learning Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.11762
- Source URL: https://arxiv.org/abs/2310.11762
- Reference count: 18
- Key outcome: QW loss improves GNN performance on node classification by 2-3% accuracy across multiple datasets and architectures

## Executive Summary
This paper introduces the Quasi-Wasserstein (QW) loss function for training graph neural networks on node-level prediction tasks. The key insight is that traditional loss functions treat nodes independently despite their non-i.i.d. nature due to graph structure. The QW loss reformulates training as an optimal transport problem on the graph, measuring label discrepancy via a transport flow defined on edges. This approach consistently improves performance across 7 GNN architectures and 9 datasets, achieving 89.88% accuracy on Cora compared to 87.44% for traditional methods. The method scales to graphs with millions of edges and provides theoretical guarantees about better data fitting.

## Method Summary
The Quasi-Wasserstein loss formulates node prediction as an optimal transport problem between observed and predicted label distributions across the graph. The loss combines a GNN's label predictions with an optimal transport flow matrix that represents label propagation through edges. Both components are learned jointly using Bregman divergence-based algorithms, optionally with learnable edge weights. During prediction, final labels combine the GNN output with the transport flow residual. The method can use exact optimization (Bregman ADMM) for small graphs or approximate optimization (Bregman divergence regularizer) for large graphs, maintaining linear complexity with respect to the number of edges.

## Key Results
- Achieves 89.88% accuracy on Cora (vs 87.44% baseline) and 80.13% on Citeseer (vs 77.28% baseline)
- Outperforms traditional loss on all 9 datasets including heterophilic graphs like Texas and Cornell
- Improves regression performance with 0.1491 MSE on Chameleon (vs 0.1654 baseline)
- Shows consistent gains across 7 GNN architectures including GCN, GAT, GIN, and GraphSAGE
- Scales to large graphs with millions of edges while maintaining linear complexity

## Why This Works (Mechanism)

### Mechanism 1
Traditional loss functions apply penalties independently to each node, ignoring the non-i.i.d. nature of node embeddings and labels due to graph structure. The QW loss formulates a set-level loss based on optimal transport between observed and predicted labels across all dimensions, creating dependencies between nodes. This addresses the fundamental inconsistency between how loss functions treat nodes and how graph structure creates label dependencies.

### Mechanism 2
The optimal label transport component provides complementary information that improves prediction accuracy beyond what the GNN alone can achieve. The QW loss learns both a GNN parameterization and an optimal transport flow matrix F that represents label propagation across edges. During prediction, the final label combines the GNN output with the transport flow, capturing residual dependencies not learned by the GNN.

### Mechanism 3
The Bregman divergence-based formulation allows efficient optimization while maintaining theoretical guarantees about data fitting. The strict equality constraint of optimal transport is relaxed to a Bregman divergence regularizer, enabling gradient-based optimization. This formulation also provides a theoretical guarantee that the QW loss achieves better data fitting than traditional methods.

## Foundational Learning

- **Optimal Transport (Wasserstein Distance)**: Measures the cost of transforming one label distribution into another; needed because QW loss is fundamentally built on optimal transport theory applied to graph-structured data.
  - Quick check: Can you explain the difference between classic Wasserstein distance and the graph-based Wasserstein distance used in this paper?

- **Graph Neural Networks and Message Passing**: How GNNs aggregate information from neighbors and the limitations of current training approaches; needed to understand the inconsistency between independent loss assumptions and message-passing mechanisms.
  - Quick check: How does the assumption of independent node labels in traditional GNN training conflict with the message-passing mechanism?

- **Bregman Divergence and Regularization**: The optimization framework that replaces strict constraints with Bregman divergences for computational efficiency; needed to understand the trade-offs in the QW loss optimization.
  - Quick check: What are the trade-offs between using exact optimization (Bregman ADMM) versus approximate optimization (Bregman divergence regularizer)?

## Architecture Onboarding

- **Component map**: Graph structure (A) -> Node features (X) -> Partial labels (YVL) -> GNN module (θ) -> Label predictions -> Optimal transport module -> Flow matrix F -> Edge weight predictor (optional) -> QW loss -> Combined predictions

- **Critical path**: 1) Forward pass: GNN generates label predictions 2) Compute label discrepancy matrix 3) Solve optimal transport to get flow matrix F 4) Compute QW loss 5) Backward pass updates both GNN parameters and transport flow 6) Optional: Update edge weights based on F

- **Design tradeoffs**: Exact vs. approximate optimization (Bregman ADMM gives exact solutions but is slower; Bregman divergence regularizer is faster but approximate); Computational cost vs. accuracy (larger transport problems are more accurate but computationally expensive); Fixed vs. learnable edge weights (learnable weights can adapt to data but add complexity)

- **Failure signatures**: Poor performance with small λ (insufficient regularization leading to noisy transport solutions); Convergence issues (learning rate too high for transport optimization); Overfitting (using exact optimization on small datasets with distribution shifts); Edge weight problems (learnable edges degrade performance on homophilic graphs)

- **First 3 experiments**: 1) Baseline comparison: Implement traditional GNN training and compare with QW loss on Cora dataset 2) Ablation study: Test with F=0 to verify that traditional loss is a special case 3) Transport flow analysis: Visualize the learned transport flow matrix F on a small graph to understand what dependencies it captures

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mathematical relationship between the Quasi-Wasserstein loss and traditional loss functions when F=0 and Bφ equals the standard loss function? The paper states that when F=0 and Bφ=ψ, the QW loss degenerates to the traditional loss function (2), but doesn't provide a rigorous mathematical proof of this equivalence or explore the implications in detail.

### Open Question 2
How does the Quasi-Wasserstein loss impact the generalization performance of GNNs compared to traditional loss functions, particularly in cases of distribution shift or out-of-distribution data? While the paper mentions that the Bregman ADMM-based solver can fit training data better but may have higher risk of overfitting in cases of distribution shifting, it doesn't empirically investigate how this impacts generalization performance on unseen data.

### Open Question 3
What is the optimal strategy for setting the hyperparameter λ in the Bregman divergence term, and how does its value affect the trade-off between fitting the training data and regularizing the model? The paper discusses the importance of λ and shows robustness to its setting within a wide range, but doesn't provide definitive guidance on how to select the optimal λ for a given problem or analyze the impact on the model's bias-variance tradeoff.

## Limitations

- Computational overhead of solving optimal transport problems, though mitigated by linear complexity scaling
- Edge weight learning component shows mixed results, degrading performance on homophilic graphs
- Limited theoretical analysis of when the independence assumption actually matters in practice

## Confidence

- **High confidence**: Empirical improvements across multiple GNN architectures and datasets are well-documented with proper baselines and ablation studies
- **Medium confidence**: Theoretical claims about better data fitting are supported by Theorem 3 but lack extensive empirical validation across diverse graph structures
- **Medium confidence**: Computational efficiency claims are supported by complexity analysis but limited runtime measurements

## Next Checks

1. **Distribution shift analysis**: Test the QW loss on graphs with controlled distribution shifts (e.g., node feature corruption) to validate the claimed robustness improvements
2. **Graph size scaling**: Systematically evaluate performance on graphs spanning multiple orders of magnitude in size to validate the claimed linear complexity
3. **Message passing interaction**: Compare the QW loss with GNNs that have explicit non-i.i.d. modeling (e.g., graph attention with learnable attention coefficients) to isolate the contribution of the transport component