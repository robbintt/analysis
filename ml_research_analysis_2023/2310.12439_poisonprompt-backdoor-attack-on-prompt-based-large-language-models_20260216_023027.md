---
ver: rpa2
title: 'PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models'
arxiv_id: '2310.12439'
source_url: https://arxiv.org/abs/2310.12439
tags:
- prompt
- backdoor
- prompts
- task
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POISONPROMPT, a backdoor attack framework
  targeting prompt-based large language models (LLMs). The method employs bi-level
  optimization to inject malicious prompts that activate only when specific triggers
  are present in the input, while maintaining normal functionality otherwise.
---

# PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models

## Quick Facts
- arXiv ID: 2310.12439
- Source URL: https://arxiv.org/abs/2310.12439
- Reference count: 0
- Key outcome: PoisonPrompt achieves >90% attack success rates while maintaining clean task accuracy within 10% of baseline

## Executive Summary
This paper introduces POISONPROMPT, a backdoor attack framework targeting prompt-based large language models. The method employs bi-level optimization to inject malicious prompts that activate only when specific triggers are present in the input, while maintaining normal functionality otherwise. Experiments across six datasets and three LLM architectures demonstrate the effectiveness of the approach, with attack success rates exceeding 90% and minimal impact on clean task accuracy. The results highlight significant security vulnerabilities in prompt-based LLMs, particularly in scenarios involving outsourced or marketplace-distributed prompts.

## Method Summary
POISONPROMPT uses bi-level optimization to inject backdoors into prompt-based LLMs. The method divides training data into poison (5%) and clean (95%) sets, then optimizes both trigger tokens and prompt parameters through two levels of optimization. The lower level trains prompt tuning on both clean and poisoned data, while the upper level searches for optimal trigger tokens that maximize backdoor activation without degrading main task performance. The attack targets task-relevant tokens to ensure higher success rates and uses gradient-based optimization for efficient trigger identification.

## Key Results
- Attack success rates exceeding 90% across all tested datasets and LLM architectures
- Clean task accuracy drops under 10% compared to baseline prompts
- High attack robustness maintained across various trigger sizes (3-5 tokens)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bi-level optimization framework successfully decouples backdoor injection from prompt tuning, allowing independent optimization of trigger tokens and prompt parameters.
- Mechanism: The lower-level optimization trains the prompt module on both clean and poisoned data while the upper-level optimization searches for optimal trigger tokens that maximize backdoor activation without degrading main task performance.
- Core assumption: The model's contextual reasoning capabilities can be exploited to respond to minor trigger alterations while maintaining normal functionality.
- Evidence anchors:
  - [abstract] "The method employs bi-level optimization to inject malicious prompts that activate only when specific triggers are present in the input, while maintaining normal functionality otherwise."
  - [section 3.2] "To overcome the aforementioned challenges, we propose POISONPROMPT, a novel bi-level optimization-based prompt backdoor attack."
- Break condition: If the LLM's contextual reasoning capabilities are insufficient to bridge the semantic gap between normal and poisoned inputs.

### Mechanism 2
- Claim: Task-relevant tokens are selected as backdoor targets to ensure higher attack success rates.
- Mechanism: The attack retrieves top-k candidates from the language model head that are semantically related to the task, making it easier for the LLM to generate these tokens when triggered.
- Core assumption: Selecting tokens that are contextually relevant to the downstream task increases the likelihood of successful manipulation.
- Evidence anchors:
  - [section 3.1] "We retrieve the task-relevant tokens as target tokens, making it easier to manipulate the pre-trained LLM to return target tokens."
  - [section 3.1] "Vt = top-k{ftransformer(x)[i] · w | x ∈ Dc}"
- Break condition: If the top-k selection mechanism fails to identify semantically appropriate target tokens.

### Mechanism 3
- Claim: The use of gradient-based optimization for trigger selection enables efficient identification of effective backdoor triggers.
- Mechanism: The method calculates gradients of the backdoor loss with respect to trigger tokens and uses these gradients to identify the most effective trigger candidates through top-k selection.
- Core assumption: The gradient information provides meaningful signals about which trigger tokens will be most effective at activating the backdoor.
- Evidence anchors:
  - [section 3.2] "Motivated by Hotflip [16, 17], we first calculate the gradient of triggers using log-likelihood over several batches of samples and multiply it by the embedding of the input word win to identify the top-k candidate tokens."
  - [section 3.2] "Tcand = top-k{win∈V [e(win)T ∇xtrigger Lb]}"
- Break condition: If the gradient-based optimization becomes ineffective due to flat loss landscapes or if the ASR metric fails to distinguish between good and bad triggers.

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: Allows simultaneous optimization of two competing objectives (main task performance and backdoor activation) in a principled way.
  - Quick check question: How does the lower-level optimization affect the feasibility of the upper-level optimization?

- Concept: Gradient-based adversarial example generation
  - Why needed here: Provides a systematic way to identify effective trigger tokens that can manipulate the model's output.
  - Quick check question: What is the relationship between the gradient magnitude and the effectiveness of a trigger token?

- Concept: Prompt-based learning vs fine-tuning
  - Why needed here: Understanding the efficiency and limitations of prompt-based approaches is crucial for appreciating the attack's impact.
  - Quick check question: How does the number of trainable parameters in prompt-based learning compare to fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Prompt module -> Trigger optimization -> Bi-level optimization orchestrator -> Evaluation metrics
- Critical path: Data poisoning → Bi-level optimization → Trigger selection → Prompt optimization → Attack evaluation
- Design tradeoffs:
  - Trigger visibility vs attack effectiveness
  - Number of trigger tokens vs stealth
  - Poison ratio vs fidelity to main task
  - Trigger optimization frequency vs computational cost
- Failure signatures:
  - Low ASR despite high poison ratio
  - Significant drop in ACC after backdoor injection
  - Ineffective trigger optimization (ASR plateaus early)
  - Overfitting to poison set
- First 3 experiments:
  1. Baseline test: Run POISONPROMPT on SST-2 with BERT using default parameters and verify >90% ASR
  2. Fidelity test: Compare ACC between clean and backdoored prompts across all three LLMs
  3. Robustness test: Vary trigger sizes and measure impact on both ASR and ACC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do backdoor attacks in prompt-based LLMs perform against advanced defense mechanisms like adversarial training or input sanitization?
- Basis in paper: [explicit] The paper emphasizes the need for further research on security threats and countermeasures against backdoor attacks in prompt-based LLMs.
- Why unresolved: The study focuses on demonstrating the effectiveness and robustness of the attack, without evaluating its performance against defensive measures.
- What evidence would resolve it: Experimental results showing the attack success rate and model performance under various defense strategies.

### Open Question 2
- Question: Are backdoor attacks in prompt-based LLMs transferable across different model architectures or domains?
- Basis in paper: [inferred] The paper evaluates the attack on three LLM architectures and six datasets, suggesting potential interest in cross-architecture and cross-domain scenarios.
- Why unresolved: The experiments are conducted within the same model architecture and domain, leaving transferability unexplored.
- What evidence would resolve it: Comparative results of attack success rates when transferring triggers or prompts between different models or domains.

### Open Question 3
- Question: How does the presence of backdoor triggers affect the generalization ability of prompt-based LLMs on unseen data?
- Basis in paper: [explicit] The study discusses the impact of backdoor triggers on model fidelity, but does not address their effect on generalization.
- Why unresolved: The experiments focus on attack success rates and model accuracy on benchmark datasets, without examining generalization to new data.
- What evidence would resolve it: Results comparing the performance of backdoored models on unseen data versus clean models, including metrics like out-of-distribution accuracy.

## Limitations

- Parameter sensitivity not thoroughly explored, with optimal performance reported only for specific hyperparameter settings
- Trigger selection mechanism lacks empirical validation against random selection baselines
- Limited evaluation scope focused on NLP tasks without testing on more complex reasoning or multi-modal scenarios

## Confidence

- High Confidence (8/10): The bi-level optimization framework and its implementation for decoupling backdoor injection from prompt tuning
- Medium Confidence (6/10): The effectiveness of gradient-based trigger optimization and its specific application to prompt-based backdoor attacks
- Medium Confidence (6/10): The claim that task-relevant token selection significantly improves attack success rates

## Next Checks

1. Run ablation studies comparing gradient-based trigger selection against random trigger selection across all six datasets to quantify the actual contribution of the gradient optimization component.

2. Systematically vary poison ratios (1%, 3%, 5%, 10%, 20%) and trigger sizes (1, 3, 5, 7 tokens) to identify the parameter space where the attack remains effective versus degrades significantly.

3. Evaluate whether prompts backdoored on one task/domain maintain effectiveness when applied to semantically different tasks, measuring both attack success rate and task performance degradation.