---
ver: rpa2
title: Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov
  Decision Processes
arxiv_id: '2308.09733'
source_url: https://arxiv.org/abs/2308.09733
tags:
- learning
- policy
- skill
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hierarchical reinforcement learning approach
  for solving multi-objective Markov decision processes (MOMDPs) under non-stationary
  environment dynamics. The proposed method learns a generic skill set in a first
  phase using intrinsically motivated reinforcement learning, then leverages these
  skills in a second phase to bootstrap hierarchical policies for each shift in environment
  dynamics.
---

# Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes

## Quick Facts
- arXiv ID: 2308.09733
- Source URL: https://arxiv.org/abs/2308.09733
- Reference count: 34
- Key outcome: Hierarchical RL approach achieves 50% higher hypervolume and 57% better median reward in non-stationary MOMDPs

## Executive Summary
This paper introduces a two-phase hierarchical reinforcement learning framework for multi-objective Markov decision processes under non-stationary dynamics. The method first learns generic skills using intrinsically motivated reinforcement learning, then leverages these skills to bootstrap hierarchical policies for each environment shift. Experiments on three robotics scenarios demonstrate significant performance improvements over state-of-the-art methods, particularly in handling non-stationary environments through skill reuse and robust policy adaptation.

## Method Summary
The method consists of two phases: (1) Generic skill learning using competency-based intrinsically motivated reinforcement learning (IMRL), where a predictive model estimates average rewards for sampled skills and a Q-learning algorithm selects skills matching the agent's current performance level; (2) Hierarchical policy learning using HDDPG, which evolves policies over the learned skill set to solve MOMDPs. The framework operates in robotics simulation environments (V-REP) with Pioneer 3-DX robots, handling three scenarios (SAR, TS, RG) with 2-3 objectives each.

## Key Results
- Achieves 50% higher normalized hypervolume compared to state-of-the-art methods (OLS, TLO, IM-MORL)
- Delivers 57% better average median reward performance in non-stationary environments
- Successfully adapts to preference region shifts with stable policy coverage sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical skill reuse enables generalization across non-stationary scenarios
- Mechanism: Generic skills learned in phase 1 serve as reusable building blocks composed differently for each scenario
- Core assumption: Robot physical capabilities and terrain characteristics remain consistent
- Evidence anchors: [abstract], [section] on generic skill evolution
- Break condition: Environment changes invalidate robot capability assumptions

### Mechanism 2
- Claim: IMRL focuses learning on appropriately challenging skills
- Mechanism: Predictive model estimates skill rewards, intrinsic rewards based on performance progress
- Core assumption: Prediction accuracy correlates with actual skill difficulty
- Evidence anchors: [abstract], [section] on competency-based IMRL
- Break condition: Skill difficulty assessment decouples from learning progress

### Mechanism 3
- Claim: Robust fuzzy policy bootstrapping maintains performance across dynamics
- Mechanism: Preference space divided into fuzzy regions with steppingstone policies
- Core assumption: Preference space can be meaningfully partitioned
- Evidence anchors: [abstract], [section] on RFPB algorithm
- Break condition: Overlapping regions or frequent transitions destabilize learning

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Foundation for extending to multi-objective scenarios and handling non-stationary variations
  - Quick check question: What is the difference between state transition probability P_ss' and reward function R in an MDP?

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: Finding policy coverage sets representing optimal tradeoffs across conflicting objectives
  - Quick check question: Why can't multi-objective problems be solved by optimizing a single weighted sum of rewards?

- Concept: Reinforcement learning exploration-exploitation tradeoff
  - Why needed here: Balancing skill discovery with exploiting known good skills in IMRL
  - Quick check question: How does intrinsic reward signal in IMRL differ from extrinsic rewards in traditional RL?

## Architecture Onboarding

- Component map: Skill Sampler (IMRL + predictive model) → Skill Optimizer (DDPG variant) → Generic Skill Set → RFPB → HDDPG → Policy Coverage Set
- Critical path: Skill Sampler → Skill Optimizer → Generic Skill Set Storage → RFPB → HDDPG → Policy Coverage Set
- Design tradeoffs: Complexity vs. performance; skill granularity; preference space discretization
- Failure signatures: Skills never converge (check predictive model); policy coverage degradation (verify partitioning); poor generalization (validate skill generality)
- First 3 experiments:
  1. Test skill learning phase isolation with static environment, compare IMRL vs random sampler
  2. Validate hierarchical policy learning on single stationary scenario, compare hypervolume metrics
  3. Test non-stationary adaptation with simulated preference shifts, measure coverage stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can skill learning phase be made fully automatic rather than manual activation?
- Basis in paper: [explicit] Manual transition between phases noted as future work limitation
- Why unresolved: Manual activation limits real-world applicability for continuous adaptation
- What evidence would resolve it: Algorithm detecting dynamics shifts and auto-triggering policy learning phase

### Open Question 2
- Question: What is optimal exploration-exploitation balance in IMRL skill sampling?
- Basis in paper: [inferred] IMRL approach used but balance effects not explored
- Why unresolved: Current approach may not be optimal for all environment types
- What evidence would resolve it: Comparative experiments testing different exploration strategies in IMRL

### Open Question 3
- Question: How does method scale with increasing numbers of objectives and skills?
- Basis in paper: [inferred] Experiments use 2-3 objectives, scalability not addressed
- Why unresolved: Real-world problems often involve many objectives, hierarchical structure may become prohibitive
- What evidence would resolve it: Scalability analysis showing computation time and performance as function of objective/skill count

## Limitations
- Implementation details for skill set definition and predictive model architecture remain unspecified
- Manual activation required for transitioning between learning phases limits real-world applicability
- Experimental validation limited to three robotics scenarios without extensive testing on problems with substantial dynamic changes

## Confidence

- **High Confidence**: Hierarchical framework design and two-phase learning approach are well-articulated with clear theoretical motivation
- **Medium Confidence**: Experimental results showing performance improvements depend on unspecified implementation details
- **Low Confidence**: Claims about skill generality across non-stationary scenarios need validation on problems with more substantial dynamic changes

## Next Checks

1. Implement simplified skill learning phase with controlled predictive model variations to establish sensitivity to neural network design choices
2. Conduct ablation studies comparing full GIM-MORL against versions skipping skill learning phase to quantify hierarchical skill reuse contribution
3. Test method on synthetic multi-objective problem with programmable non-stationary dynamics to verify RFPB robustness across different preference space partitioning schemes