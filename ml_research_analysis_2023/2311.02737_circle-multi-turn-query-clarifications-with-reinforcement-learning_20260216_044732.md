---
ver: rpa2
title: 'CIRCLE: Multi-Turn Query Clarifications with Reinforcement Learning'
arxiv_id: '2311.02737'
source_url: https://arxiv.org/abs/2311.02737
tags:
- query
- user
- search
- suggestions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CIRCLE, a generative model for multi-turn query
  clarifications with reinforcement learning. The key idea is to use a pretrained
  language model fine-tuned with reinforcement learning to generate diverse sets of
  query clarifications, balancing diversity and effectiveness.
---

# CIRCLE: Multi-Turn Query Clarifications with Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.02737
- Source URL: https://arxiv.org/abs/2311.02737
- Authors: 
- Reference count: 20
- Key outcome: Improves MRR@1000 from 0.3244 to 0.3889 over multiple interactions while generating more diverse query suggestions

## Executive Summary
CIRCLE is a generative model for multi-turn query clarification that uses reinforcement learning to balance diversity and effectiveness in query suggestions. The model fine-tunes a pretrained language model with Proximal Policy Optimization (PPO) to generate diverse sets of clarifications while maintaining relevance to the user's information need. Evaluated through user simulation, CIRCLE demonstrates improved performance over supervised models and Google suggestions, particularly in its resilience to non-cooperative user behavior, though it requires multiple interaction turns to show advantages over simpler approaches.

## Method Summary
The method involves two-stage fine-tuning of a GPT2 language model: first with supervised learning using query clarification pairs from MS Marco, then with reinforcement learning using PPO to maximize a reward function balancing dissimilarity between generated queries and KL regularization. The model conditions on the initial query and previously selected suggestions to generate new query sets at each turn. Evaluation uses a user simulation framework with epsilon-greedy policy to mimic various levels of user cooperation, measuring performance with MRR@1000 and RBO similarity metrics against document retrieval using BERT-based dense retrieval.

## Key Results
- Improves MRR@1000 from 0.3244 to 0.3889 over multiple interactions compared to baselines
- Generates more diverse query suggestions than supervised models while maintaining or slightly improving search performance
- Demonstrates resilience to non-cooperative user behavior in the simulation framework
- Requires 4-5 interaction turns to outperform simpler approaches like beam search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning fine-tuning improves diversity of generated query suggestions while maintaining relevance
- Mechanism: PPO algorithm adjusts token probability distribution using a reward function with dissimilarity and KL regularization terms, allowing exploration of diverse yet grounded query formulations
- Core assumption: Pretrained language model has learned sufficient linguistic patterns for reasonable query generation
- Evidence anchors: [abstract] mentions generating diverse query clarifications with RL; [section] describes PPO fine-tuning for diversity; corpus provides weak support through related conversational search work

### Mechanism 2
- Claim: Incorporating user feedback from previous turns enables more relevant and targeted suggestions
- Mechanism: Model conditions generation on initial query plus all previously selected queries, creating context-aware refinement based on feedback received
- Core assumption: User feedback provides meaningful information about information need that the model can effectively use
- Evidence anchors: [abstract] mentions leveraging supervised and reinforcement learning for clarification; [section] describes conditioning on user-selected queries; corpus provides weak support through related user modeling work

### Mechanism 3
- Claim: Multi-turn framework with user simulation provides realistic and effective performance assessment
- Mechanism: Epsilon-greedy user simulation mimics real behavior by selecting best-known action with probability 1-epsilon and random action with probability epsilon, testing robustness to various cooperation levels
- Core assumption: User simulation accurately reflects real user behavior as meaningful proxy for human evaluation
- Evidence anchors: [abstract] mentions integrating into simulation framework; [section] describes epsilon-greedy policy design; corpus provides moderate support through related RL search work

## Foundational Learning

- **Reinforcement Learning (PPO)** - Why needed: Allows learning from reward signal to adjust policy for maximizing diversity and relevance; Quick check: What role does KL regularization play in preventing policy deviation from supervised model?
- **Language Model Fine-tuning** - Why needed: Pretrained model provides strong starting point with linguistic patterns; fine-tuning adapts to query clarification task; Quick check: How does model ensure generated queries are both diverse and relevant?
- **User Simulation and Epsilon-Greedy Policy** - Why needed: Provides controlled evaluation environment without human users; epsilon-greedy tests robustness to cooperation levels; Quick check: How does epsilon choice affect evaluation results and what does it reveal?

## Architecture Onboarding

- **Component map**: User Query -> Supervised GPT2 -> RL Module (PPO) -> User Simulation -> BERT Retrieval -> Evaluation Metrics
- **Critical path**: 1) User submits initial query; 2) Model generates query suggestions; 3) User selects suggestion (simulated); 4) Model conditions on selection and generates new set; 5) Repeat for multiple turns; 6) Retrieval ranks documents; 7) Compute MRR@1000 and RBO
- **Design tradeoffs**: Diversity vs. relevance (reward function weights); number of suggestions (diversity vs. cognitive load); user cooperation robustness (vs. reduced performance with perfect feedback)
- **Failure signatures**: Low MRR (irrelevant suggestions or ineffective retrieval); high RBO similarity (insufficient diversity); degradation over turns (sensitivity to noisy feedback)
- **First 3 experiments**: 1) Evaluate with fully cooperative user (epsilon=0) for upper bound; 2) Vary epsilon to test robustness to cooperation levels; 3) Compare against Google suggestions, beam search, and supervised model

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CIRCLE perform with multi-faceted information needs or complex search sessions spanning multiple topics? [inferred] Paper mentions potential extension to search sessions; unresolved because current evaluation focuses on single queries; resolved by experiments on multi-faceted datasets comparing to baselines.

- **Open Question 2**: How sensitive is performance to reward function components and hyperparameters (KL weight β, clip ratio ϵ)? [explicit] Paper mentions RBO metric and specifies β and ϵ values but doesn't explore impact; unresolved because no sensitivity analysis conducted; resolved by systematic experiments varying components and analyzing performance impact.

- **Open Question 3**: How does CIRCLE compare to alternative diversity-promoting methods like genetic algorithms or population-based optimization? [inferred] Paper focuses on RL approaches without benchmarking against alternatives; unresolved because no comparison to other diversity methods; resolved by experiments comparing CIRCLE to alternative methods on relevant datasets.

## Limitations

- Evaluation relies entirely on user simulation rather than human studies, potentially misrepresenting real user behavior
- Model requires multiple interaction turns (4-5) to outperform simpler baselines, raising practical efficiency concerns
- Dependence on simulation framework assumptions about user cooperation and feedback patterns may impact reported performance

## Confidence

- **High Confidence**: RL mechanism for improving query diversity while maintaining relevance through KL regularization is well-supported experimentally
- **Medium Confidence**: Multi-turn framework effectiveness demonstrated through simulation but uncertain translation to real users
- **Low Confidence**: Robustness to non-cooperative behavior demonstrated in simulation but may not generalize to all adversarial interactions

## Next Checks

1. Conduct human user studies to validate simulation results, comparing CIRCLE performance against baselines with actual users providing feedback
2. Test model performance with different document retrieval systems beyond BERT-based dense retrieval to assess generalizability
3. Implement adaptive stopping criterion that determines when clarification interactions should terminate based on diminishing returns rather than fixed turns