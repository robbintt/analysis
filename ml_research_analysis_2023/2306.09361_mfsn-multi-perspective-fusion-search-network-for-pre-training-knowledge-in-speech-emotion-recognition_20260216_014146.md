---
ver: rpa2
title: 'MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in
  Speech Emotion Recognition'
arxiv_id: '2306.09361'
source_url: https://arxiv.org/abs/2306.09361
tags:
- speech
- emotion
- recognition
- semantic
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for speech emotion recognition
  called MFAS that uses neural architecture search to find the optimal fusion strategy
  for speech and textual semantic information. The key idea is to leverage continuous-based
  knowledge to capture speech semantic and quantization-based knowledge to learn textual
  semantic.
---

# MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2306.09361
- Source URL: https://arxiv.org/abs/2306.09361
- Reference count: 0
- Key outcome: MFAS achieves 75.1% UA and 73.7% WA on IEMOCAP using 10-fold leave-one-speaker-out validation

## Executive Summary
This paper introduces MFAS (Multi-perspective Fusion Search Network), a framework for speech emotion recognition that leverages neural architecture search to optimize the fusion of speech and textual semantic information. The key innovation is using continuous-based knowledge to capture speech semantic and quantization-based knowledge to learn textual semantic, with a search space that automatically adjusts the fusion strategy. Experiments demonstrate state-of-the-art performance on the IEMOCAP dataset, with ablation studies showing the effectiveness of the proposed approach.

## Method Summary
MFAS uses continuous-based knowledge (Data2vec) for speech semantic extraction and quantization-based knowledge (Wav2vec2) for textual semantic extraction. The framework employs neural architecture search to find the optimal fusion strategy between these modalities, using Choice cells to sample intermediate representations and a search space with operations like Sum, Attention, ConcatFC, and ISM. Dual-Stream Co-attention modules then fuse the selected semantic information guided by spectrogram features, with a final MLP classifier for emotion prediction.

## Key Results
- Achieves 75.1% unweighted accuracy and 73.7% weighted accuracy on IEMOCAP
- Outperforms state-of-the-art methods including TransR and MMIMOT
- Continuous modeling outperforms quantization modeling in dimensional emotion analysis (valence, activation, dominance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous-based knowledge captures more comprehensive emotional information than quantization-based approaches
- Mechanism: Continuous modeling reconstructs speech frames directly rather than clustering them into discrete units, preserving the dimensional nature of emotional cues
- Core assumption: Emotional information is inherently continuous and lacks clear boundaries for unit quantization
- Evidence anchors: Experimental results show continuous modeling allows the model to better consider complex emotional cues present in speech

### Mechanism 2
- Claim: Neural Architecture Search automatically adjusts textual semantic using speech information
- Mechanism: The search space selects optimal fusion strategies by assigning weights to different operations that combine speech and textual semantic information
- Core assumption: The model can learn to identify when textual semantic is reliable versus when it needs correction from speech information
- Evidence anchors: Attention operation is most important, indicating the model performs better when using textual semantic as query and speech semantic as key and value

### Mechanism 3
- Claim: Dual-stream co-attention with spectrogram guidance improves emotion recognition performance
- Mechanism: Separate co-attention modules fuse speech and textual semantic information using spectrogram-derived guidance vectors, creating complementary representations
- Core assumption: Spectrogram information provides relevant guidance for fusing different semantic streams
- Evidence anchors: Dual-Stream Co-attention is utilized to fuse emotional information separately, guided by the spectrogram represented by S

## Foundational Learning

- Concept: Self-supervised learning for speech representations
  - Why needed here: The framework relies on pre-trained models (Data2vec, Wav2vec2) that learn speech representations without labeled data
  - Quick check question: What's the key difference between Wav2vec2 and Data2vec in terms of their reconstruction objectives?

- Concept: Differentiable architecture search
  - Why needed here: MFAS uses continuous relaxation to search for optimal fusion strategies between speech and textual semantic
  - Quick check question: How does the continuous relaxation in DARTS differ from traditional discrete architecture search?

- Concept: Co-attention mechanisms
  - Why needed here: The framework uses co-attention to fuse frame-level information guided by spectrogram features
  - Quick check question: What's the mathematical difference between standard attention and co-attention in this context?

## Architecture Onboarding

- Component map: Audio → Base Model (Data2vec) → Choice Cell → Fusion Cell → Co-Attention → Classifier
- Critical path: Audio → Base Model → Choice Cell → Fusion Cell → Co-Attention → Classifier
- Design tradeoffs:
  - Continuous vs quantization modeling: More expressive but potentially noisier
  - Search space complexity vs computational cost
  - Separate vs joint co-attention streams for fusion
- Failure signatures:
  - Performance drops when textual semantic is unreliable but Zero operation is rarely selected
  - Inconsistent results across different speaker splits
  - Architecture search converges to trivial solutions (e.g., all Zero operations)
- First 3 experiments:
  1. Compare continuous vs quantization modeling baselines on IEMOCAP using same model architecture
  2. Validate that Choice Cell sampling improves over using single representation level
  3. Test different fusion operations (Sum, Attention, ConcatFC) without NAS to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of quantization units (Nq) for quantization-based models in speech emotion recognition?
- Basis in paper: The paper shows that performance of quantization-based models improves with increasing Nq, but does not determine the optimal number.
- Why unresolved: The paper only compares a few different values of Nq, and does not explore a wide range of possible values.
- What evidence would resolve it: Systematic experiments varying Nq over a wide range, and comparing performance on a validation set.

### Open Question 2
- Question: How do continuous-based models compare to quantization-based models in terms of capturing emotional information in speech?
- Basis in paper: The paper shows that continuous-based models perform better than quantization-based models in dimensional emotion analysis, but similar in discrete emotion recognition.
- Why unresolved: The paper only compares two specific models (Data2vec and Wav2vec2), and does not explore other possible continuous and quantization-based models.
- What evidence would resolve it: Systematic experiments comparing multiple continuous and quantization-based models on various emotion recognition tasks.

### Open Question 3
- Question: How important is the fusion strategy for combining speech and textual semantic information in speech emotion recognition?
- Basis in paper: The paper shows that the proposed search space for fusion strategies improves performance over simple concatenation, but does not determine the importance of the fusion strategy relative to other factors.
- Why unresolved: The paper only compares a few different fusion strategies, and does not control for other factors that may affect performance.
- What evidence would resolve it: Systematic experiments varying the fusion strategy while controlling for other factors, and comparing performance to a baseline without fusion.

## Limitations
- Direct comparison between continuous and quantization modeling approaches is lacking
- NAS robustness to corrupted textual semantic is not tested
- Spectrogram guidance relevance to emotional content is not explicitly validated

## Confidence
- Continuous modeling superiority: Medium confidence
- NAS for textual semantic adjustment: Medium confidence  
- Dual-stream co-attention with spectrogram guidance: Medium confidence

## Next Checks
1. Direct ablation study: Compare continuous vs quantization modeling using identical architectures on IEMOCAP
2. NAS robustness testing: Evaluate MFAS performance when textual semantic is artificially corrupted at different rates
3. Spectrogram guidance validation: Compare dual-stream co-attention with joint attention mechanisms, and correlate spectrogram features with emotional content