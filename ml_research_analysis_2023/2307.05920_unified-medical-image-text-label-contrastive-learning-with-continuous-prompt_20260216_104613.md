---
ver: rpa2
title: Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt
arxiv_id: '2307.05920'
source_url: https://arxiv.org/abs/2307.05920
tags:
- image
- medical
- prompt
- pre-training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified Image-Text-Label contrastive learning
  framework (UMCL) that incorporates continuous prompts to address limitations in
  existing medical vision-language pre-training models. The key idea is to unify image-label
  and image-text datasets by constructing prompts that map labels into a shared feature
  space, thereby expanding training data and improving model generalization.
---

# Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt

## Quick Facts
- arXiv ID: 2307.05920
- Source URL: https://arxiv.org/abs/2307.05920
- Reference count: 22
- Key outcome: UMCL achieves 74.8% accuracy on COVID-19 zero-shot classification, outperforming existing methods

## Executive Summary
This paper presents a unified framework for medical vision-language pre-training that integrates image-text and image-label data through continuous prompts. The key innovation is the ability to bridge the gap between hand-crafted prompts and natural medical reports while expanding training data by unifying multiple data sources. The framework addresses critical challenges in medical imaging including data scarcity, false negatives in contrastive learning, and prompt sensitivity, achieving state-of-the-art performance across zero-shot classification, transfer learning, and cross-modal retrieval tasks.

## Method Summary
The method unifies image-text and image-label datasets by constructing prompts that map labels into a shared feature space, expanding training data utilization. Continuous prompts—learnable vectors inserted into text embeddings—are introduced to reduce dependence on hand-crafted prompts and better align with natural medical language. The framework employs an Image-Text-Label contrastive training strategy that leverages disease labels to model semantic consistency and mitigate false-negative samples. The model uses SwinTransformer for image encoding and BioClinicalBERT for text encoding, with L2-normalized similarity scores optimized through contrastive loss.

## Key Results
- Achieves 74.8% accuracy on COVID-19 zero-shot classification
- Outperforms existing methods in transfer learning on CheXpert and MIMIC-CXR datasets
- Demonstrates superior cross-modal retrieval performance (P@1, P@2, P@5, P@10 metrics)

## Why This Works (Mechanism)

### Mechanism 1
The unified framework addresses data scarcity by incorporating both image-label and image-text pairs through prompt construction that maps labels to the same feature space as text. This unification enables multiple data sources to be trained together, expanding the effective training data. The core assumption is that image-label data provides valuable supervised signals that complement image-text data when properly mapped through prompts.

### Mechanism 2
Continuous prompts reduce dependence on hand-crafted prompts by using learnable vectors inserted into text embeddings that are optimized during training. This approach bridges the gap between label descriptions and natural medical reports by learning representations that capture commonalities between structured labels and clinical language. The mechanism assumes that continuous prompts can effectively represent semantic meaning consistent with natural medical language.

### Mechanism 3
The Image-Text-Label contrastive training strategy mitigates false-negative samples by leveraging disease labels to model semantic consistency. The contrastive loss is modified to include image-label pairs, using similarity between images and their corresponding label prompts to constrain learning. This approach assumes disease labels provide reliable indicators of semantic similarity that can be effectively captured through the contrastive loss.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed here: Enables discriminative feature learning by pulling similar samples together and pushing dissimilar ones apart, crucial for associating images with text descriptions and labels
  - Quick check question: What is the main objective of contrastive learning, and how does it help in learning discriminative features?

- **Concept: Continuous Prompts**
  - Why needed here: Bridge gap between hand-crafted prompts and natural medical reports by learning semantic representations that align with clinical language
  - Quick check question: How do continuous prompts differ from hand-crafted prompts, and what advantages do they offer in medical image-text-label learning?

- **Concept: Multi-modal Learning**
  - Why needed here: Enables unified representation learning across images, text, and labels by capturing relationships between different modalities
  - Quick check question: What are key challenges in multi-modal learning, and how does the unified framework address these challenges?

## Architecture Onboarding

- **Component map**: Images (SwinTransformer) -> Visual Vectors -> Contrastive Loss -> Unified Representation Space; Text (BioClinicalBERT) -> Text Vectors -> Continuous Prompts -> Contrastive Loss; Labels -> Discrete Prompts + Continuous Prompts -> Contrastive Loss

- **Critical path**: 1) Preprocess image-label and image-text datasets; 2) Construct discrete and continuous prompts for image-label data; 3) Encode images and text using respective encoders; 4) Calculate similarity scores between images and prompts; 5) Optimize using Image-Text-Label contrastive loss

- **Design tradeoffs**: Continuous prompts offer flexibility and optimization potential but require more computational resources; incorporating image-label data provides supervised signals but may introduce noise if labels are inaccurate

- **Failure signatures**: Poor zero-shot performance may indicate continuous prompts failing to bridge label-report gap; overfitting suggests model not generalizing well, possibly due to continuous prompts overfitting to training set

- **First 3 experiments**: 1) Train on CheXpert subset and evaluate zero-shot classification; 2) Ablate continuous prompts and compare performance with/without them; 3) Vary continuous prompt length and observe impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do continuous prompts learn to bridge the gap between hand-crafted prompts and natural medical image reports, and what is the optimal length for these continuous prompts?
- Basis in paper: [explicit] The paper mentions continuous prompts are learnable vectors that can be trained end-to-end, but optimal length is not determined
- Why unresolved: Performance degrades when prompt length is set to 16 or 64, but optimal length and learning mechanism not explored in detail
- What evidence would resolve it: Experimental results comparing different prompt lengths and their impact on model performance

### Open Question 2
- Question: How does the Image-Text-Label contrastive training strategy effectively mitigate false-negative samples, and what are the limitations of this approach?
- Basis in paper: [explicit] Image-Text-Label contrastive training introduced to solve false negatives, but limitations not discussed
- Why unresolved: Paper lacks detailed analysis of limitations or potential drawbacks of the contrastive training strategy
- What evidence would resolve it: Further analysis and experiments exploring limitations and drawbacks of the approach

### Open Question 3
- Question: How does the UMCL framework handle privacy concerns associated with medical image reports, and what are potential risks of patient information leakage?
- Basis in paper: [inferred] Medical image reports contain rich patient information that may result in privacy leakage, but framework's privacy measures not discussed
- Why unresolved: Paper does not provide detailed discussion of privacy concerns or measures to mitigate patient information leakage risks
- What evidence would resolve it: Thorough analysis of privacy concerns and measures taken by UMCL framework to protect patient information

## Limitations
- Effectiveness heavily depends on quality and coverage of disease labels in image-label datasets
- Initial discrete prompt templates still play crucial role despite continuous prompts, though their specific impact is not thoroughly explored
- Increased computational complexity due to processing three modalities simultaneously

## Confidence
- High Confidence: Experimental results showing UMCL's superiority in zero-shot classification and transfer learning
- Medium Confidence: Theoretical framework for unifying data through continuous prompts is sound but lacks detailed ablation studies
- Low Confidence: Claims about reducing false-negative samples lack direct empirical validation compared to traditional contrastive approaches

## Next Checks
1. Perform ablation study on prompt components by systematically removing or modifying continuous prompts while keeping other components constant to quantify their specific contribution
2. Analyze label quality impact by evaluating model performance using image-label datasets with varying label quality (consensus vs. single annotator labels)
3. Conduct cross-dataset generalization test by training on one medical imaging dataset and evaluating zero-shot classification on completely different datasets (e.g., dermatology or histopathology)