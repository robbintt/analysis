---
ver: rpa2
title: An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities
  of Large Language Models
arxiv_id: '2309.02077'
source_url: https://arxiv.org/abs/2309.02077
tags:
- medical
- information
- llms
- doctor
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated evaluation framework for assessing
  the practical capabilities of large language models (LLMs) as virtual doctors during
  multi-turn consultations. The framework simulates doctor-patient dialogues, requiring
  LLMs to inquire about missing medical information and make diagnoses.
---

# An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2309.02077
- Source URL: https://arxiv.org/abs/2309.02077
- Reference count: 6
- Key outcome: This paper introduces an automated evaluation framework for assessing the practical capabilities of large language models (LLMs) as virtual doctors during multi-turn consultations. The framework simulates doctor-patient dialogues, requiring LLMs to inquire about missing medical information and make diagnoses. A benchmark is constructed by reformulating USMLE multiple-choice questions, and comprehensive evaluation metrics are developed. Experiments on three test sets show that fine-tuning LLMs with a medical consultation training set can alleviate hallucinations and improve performance. Vicuna fine-tuned on the training data achieved the best F1-scores and accuracy, surpassing ChatGPT by up to 1.67%. The results demonstrate the effectiveness of the proposed framework in evaluating and improving LLMs' consultation abilities.

## Executive Summary
This paper presents an automatic evaluation framework for assessing the practical capabilities of large language models as virtual doctors during multi-turn medical consultations. The framework requires LLMs to inquire about missing medical information from patients and make diagnoses, simulating realistic doctor-patient dialogues. The authors construct a benchmark by reformulating USMLE multiple-choice questions and develop comprehensive evaluation metrics. Experiments on three test sets demonstrate that fine-tuning LLMs with a medical consultation training set can alleviate hallucinations and improve performance. The results show that Vicuna fine-tuned on the training data achieved the best F1-scores and accuracy, surpassing ChatGPT by up to 1.67%.

## Method Summary
The authors propose an automatic evaluation framework for multi-turn medical consultations using large language models. They construct a benchmark by reformulating USMLE multiple-choice questions into simulated doctor-patient dialogues, requiring LLMs to inquire about missing medical information and make diagnoses. The framework uses ChatGPT as a patient simulator with strict prompts to ensure honest, colloquial responses. Multiple LLMs, including ChatGPT and Vicuna, are evaluated as virtual doctors using comprehensive metrics such as medical information coverage rates (ROUGE scores), accuracy of final diagnoses, and average turns/lengths. The authors also fine-tune Vicuna on a training set constructed from the MedQA dataset to improve its consultation abilities and reduce hallucinations.

## Key Results
- Vicuna fine-tuned on medical consultation data achieved the best F1-scores and accuracy, surpassing ChatGPT by up to 1.67%
- Fine-tuning LLMs with a medical consultation training set can alleviate hallucinations and improve performance
- The proposed framework effectively evaluates and improves LLMs' consultation abilities through comprehensive metrics and simulated doctor-patient dialogues

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on reformulated multiple-choice questions reduces hallucination and improves multi-turn consultation performance. The training data is constructed by decomposing USMLE questions into structured medical information and tasks, then generating simulated doctor-patient dialogues. Fine-tuning Vicuna on this data teaches the model to ask structured, relevant questions and avoid jumping to conclusions without sufficient information.

### Mechanism 2
Using ChatGPT as a patient simulator ensures honest, strict, and colloquial responses that reflect real-world consultation constraints. The patient LLM is prompted to provide accurate symptom information lazily (only when asked), ensuring the doctor model must inquire to gather all necessary data. This design forces the doctor LLM to demonstrate genuine consultation ability rather than relying on upfront information.

### Mechanism 3
Comprehensive evaluation metrics combining information coverage, dialogue quality, and final task accuracy provide a robust assessment of consultation capability. Metrics include medical information coverage rates (ROUGE scores), accuracy of final diagnoses, and average turns/lengths. This multi-dimensional evaluation captures both the quality of information gathering and the correctness of conclusions.

## Foundational Learning

- **Medical knowledge representation and extraction**: The framework requires extracting structured medical information (symptoms, history, findings) from unstructured clinical text to simulate consultations and evaluate responses. Quick check: Can you extract key-value pairs of medical information from a clinical case description?
- **Multi-turn dialogue management**: The evaluation simulates realistic doctor-patient interactions requiring context tracking, question generation, and response handling across multiple conversational turns. Quick check: How would you maintain conversation state and decide when to stop asking questions in a multi-turn consultation?
- **Evaluation metric design and correlation**: The framework uses multiple metrics (ROUGE scores, accuracy, turn counts) that must be validated to correlate with true consultation quality. Quick check: How would you validate that your evaluation metrics actually predict real-world consultation success?

## Architecture Onboarding

- **Component map**: Data pipeline (reformulated USMLE questions → medical info extraction → initial requests) → Patient simulator (ChatGPT with strict prompts) → Doctor models (multiple LLMs) → Evaluation engine (coverage, accuracy, dialogue metrics) → Task solver (ChatGPT-Turbo-3.5 for final diagnosis evaluation)
- **Critical path**: Data → Patient simulation → Doctor consultation → Evaluation metrics → Results analysis
- **Design tradeoffs**: Using ChatGPT as patient simulator provides realism but introduces dependency on OpenAI's API and potential variability; fine-tuning vs prompting: fine-tuning Vicuna on consultation data improves performance but requires constructing training data; metric complexity vs interpretability: multiple metrics provide comprehensive evaluation but increase complexity
- **Failure signatures**: Patient simulator leaking information → inflated doctor performance; doctor models failing to ask relevant questions → poor information coverage; metrics not correlating with real consultation quality → misleading results; long consultation turns without accuracy improvement → inefficiency or hallucination
- **First 3 experiments**: 1) Run baseline comparison: Lower-bound (initial request only) vs Upper-bound (full medical info) to establish performance gaps; 2) Test patient simulator honesty: Verify it only reveals information when specifically asked; 3) Validate metric correlation: Check if higher coverage scores correlate with better final task accuracy across models

## Open Questions the Paper Calls Out

### Open Question 1
How can the hallucination and robustness issues of LLMs in medical consultations be addressed to ensure more reliable and accurate performance? The paper mentions that LLMs still encounter challenges related to hallucination and robustness, making the task solver susceptible to long and noisy inputs. However, it does not provide specific solutions or methods to address these issues.

### Open Question 2
What are the factors that contribute to the bias of LLMs towards certain numbers of medical information items in the consultation process? The paper observes that models have a bias towards certain numbers of medical information items, but it does not explore the underlying factors or mechanisms that lead to this bias.

### Open Question 3
How does the order of consultation impact the final accuracy of LLMs in medical consultations, and what are the optimal consultation orders for different types of medical cases? The paper investigates the impact of consultation order on final accuracy, but it does not provide a comprehensive analysis of the optimal consultation orders for different types of medical cases or the reasons behind the observed differences in performance.

## Limitations
- The framework relies heavily on simulated patient interactions via ChatGPT, which may not fully capture the complexity and variability of real-world patient behavior
- The fine-tuning dataset construction process lacks detailed specification of quality control measures for the generated training dialogues
- The framework's generalizability to other medical specialties beyond the USMLE scope remains untested

## Confidence

- **High Confidence**: The multi-turn consultation evaluation methodology and metric design (coverage rates, accuracy, dialogue length) are well-founded and systematically implemented
- **Medium Confidence**: The effectiveness of fine-tuning Vicuna on consultation data for hallucination reduction, as the improvement is modest (1.67% over ChatGPT) and the training data construction process has limited validation
- **Low Confidence**: The patient simulator's ability to accurately represent real patient behavior, given that ChatGPT's responses may not reflect genuine patient knowledge limitations and communication patterns

## Next Checks

1. **Patient Simulator Validation**: Conduct a human evaluation study comparing ChatGPT-simulated patient responses against actual patient-doctor consultations to assess realism and information revelation patterns
2. **Cross-Specialty Testing**: Apply the framework to medical consultation scenarios outside the USMLE scope (e.g., emergency medicine, psychiatry) to evaluate domain generalization
3. **Longitudinal Performance**: Test the framework's models on multi-session consultations to evaluate consistency and learning effects across repeated interactions with the same patient case