---
ver: rpa2
title: Characterizing Large Language Models as Rationalizers of Knowledge-intensive
  Tasks
arxiv_id: '2311.05085'
source_url: https://arxiv.org/abs/2311.05085
tags:
- rationales
- rationale
- knowledge
- such
- llm-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We generate knowledge-grounded and refutation-complete rationales
  for knowledge-intensive tasks via few-shot prompting of LLMs. In two human studies,
  crowdworkers preferred LLM-generated rationales over crowdsourced rationales on
  aspects like factuality, sufficiency, and refutation convincingness.
---

# Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks

## Quick Facts
- arXiv ID: 2311.05085
- Source URL: https://arxiv.org/abs/2311.05085
- Reference count: 40
- One-line primary result: LLM-generated knowledge-grounded rationales outperform crowdsourced ones on factuality, sufficiency, and refutation convincingness, but require trust safeguards.

## Executive Summary
This paper explores the use of large language models (LLMs) for generating knowledge-grounded rationales for knowledge-intensive tasks like commonsense question answering. Through human studies, the authors demonstrate that few-shot prompting of LLMs with retrieved knowledge produces rationales that are preferred over crowdsourced ones on multiple dimensions. However, the paper also reveals a critical challenge: faithful rationalization of incorrect model predictions erodes human trust. To address this, the authors propose a review-then-rationalize pipeline that filters incorrect predictions before rationalization, enabling trustworthy explanation generation.

## Method Summary
The authors generate knowledge-grounded rationales by few-shot prompting LLMs with examples containing retrieved ConceptNet triples and expert-authored rationales. They conduct human preference studies comparing LLM-generated rationales against crowdsourced ones on dimensions like factuality, sufficiency, and refutation convincingness. Additionally, they perform an acceptability study to evaluate the quality of LLM-generated rationales. To address the trust erosion problem, they propose a review-then-rationalize pipeline that uses a self-consistency reviewer to vet predictions before rationalization.

## Key Results
- Crowdworkers preferred LLM-generated knowledge-grounded rationales over crowdsourced rationales on factuality, sufficiency, and refutation convincingness.
- LLM-generated rationales scored higher than human-written rationales on most dimensions except conciseness.
- Faithful rationalization of incorrect predictions significantly eroded human trust in the rationales.
- A self-consistency reviewer successfully intercepted 58-71% of incorrect predictions before rationalization.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Few-shot prompting of LLMs with knowledge-grounded examples produces rationales that humans find more factful, sufficient, and convincing than crowdsourced ones.
- **Mechanism**: The few-shot examples contain both the retrieved knowledge facts and expert-authored rationales that ground the reasoning on external knowledge. This dual conditioning biases the LLM to produce rationales that are more tied to verifiable facts and explicit refutations of alternatives.
- **Core assumption**: The retrieved knowledge triples from ConceptNet are both relevant to the question and understandable when paraphrased into natural language.
- **Evidence anchors**:
  - [abstract] "crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations."
  - [section 5.3] "LLM-generated rationales were rated substantially higher than human-written rationales, with the exception of conciseness."
  - [corpus] Related works like SCOTT (2023) show knowledge distillation from LLM-generated rationales can improve downstream models.
- **Break condition**: If the retrieved knowledge is noisy or irrelevant, the LLM may still hallucinate or generate irrelevant justifications, eroding factuality.

### Mechanism 2
- **Claim**: Faithful rationalization of incorrect predictions erodes human trust in the rationales.
- **Mechanism**: Humans judge rationales not just on linguistic quality but on whether the model's answer is correct. If the model is wrong, humans perceive the rationale as misleading, even if the language is fluent.
- **Core assumption**: Human trust in an explanation is contingent on agreement with the model's prediction.
- **Evidence anchors**:
  - [abstract] "faithful rationalization of incorrect model predictions erodes humans’ trust in LLM-generated rationales."
  - [section 7.2.2] Participants rated rationales for wrong predictions significantly lower (median 2.23 vs 3.13) than correct ones.
  - [corpus] Trust in automation literature (Hoff & Bashir 2015) shows errors that humans can detect reduce trust.
- **Break condition**: If the model accuracy is high enough, most rationales will be for correct predictions, mitigating trust loss.

### Mechanism 3
- **Claim**: A review-then-rationalize pipeline that uses self-consistency filtering can intercept incorrect predictions before rationalization.
- **Mechanism**: The reviewer LLM answers the same question 5 times; majority vote is taken. If reviewer disagrees with the KIT model, no rationale is generated, preventing misleading explanations.
- **Core assumption**: Self-consistency among multiple LLM responses correlates with correctness.
- **Evidence anchors**:
  - [section 7.3.2] Self-consistency reviewer intercepted 58% (CSQA) and 71% (OBQA) of incorrect predictions, outperforming greedy decoding.
  - [corpus] Self-consistency has been shown to improve chain-of-thought reasoning (Wang et al. 2022).
- **Break condition**: If reviewer accuracy is low, many incorrect predictions will still be rationalized; if reviewer is too strict, correct predictions may be blocked.

## Foundational Learning

- **Concept**: Knowledge graph sub-graph extraction (ConceptNet triples)
  - Why needed here: Provides external, structured knowledge that grounds the rationales beyond the text alone.
  - Quick check question: What are the typical formats of edges in ConceptNet (e.g., "RelatedTo", "IsA", "Causes")?

- **Concept**: Few-shot in-context learning with LLMs
  - Why needed here: Enables rationale generation without retraining, using just a handful of examples in the prompt.
  - Quick check question: How does the order of examples in a few-shot prompt affect the generated output?

- **Concept**: Self-consistency decoding
  - Why needed here: Reduces hallucination and improves reliability by aggregating multiple LLM responses.
  - Quick check question: What is the trade-off between the number of sampling attempts and latency in self-consistency?

## Architecture Onboarding

- **Component map**: Knowledge Extractor -> Prompt Constructor -> Rationalizer LLM -> (Optional) Reviewer LLM -> Output
- **Critical path**:
  1. Input: question, choices, KIT model prediction.
  2. Knowledge Extractor → top-5 facts per choice.
  3. Prompt Constructor → assemble few-shot prompt + input.
  4. Rationalizer LLM → generate topic + rationale.
  5. (Optional) Reviewer LLM → vet prediction before step 4.
- **Design tradeoffs**:
  - Knowledge granularity: richer triples improve grounding but risk token overflow.
  - Prompt size: more few-shot examples improve performance but reduce space for input.
  - Reviewer strictness: high recall of bad predictions but may block good ones.
- **Failure signatures**:
  - Overly verbose rationales → LLM generation without truncation.
  - Factually incorrect rationales → noise in retrieved knowledge or hallucination.
  - Low reviewer agreement → poor reviewer accuracy or ambiguous questions.
- **First 3 experiments**:
  1. Vary number of few-shot examples (3, 5, 8) and measure factuality ratings.
  2. Compare self-consistency (5 samples) vs greedy decoding on reviewer accuracy.
  3. Ablate knowledge retrieval: remove ConceptNet facts and measure change in sufficiency ratings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the problem of hallucination in LLM-generated rationales be effectively addressed to ensure their reliability in mission-critical tasks?
- Basis in paper: [explicit] The paper mentions that LLM-generated rationales may suffer from hallucination due to the lossy encoding of world knowledge, and it suggests that grounding on external knowledge offers promise but may not entirely alleviate the problem.
- Why unresolved: Detecting and mitigating hallucination is a challenging problem, and while some approaches have been proposed, such as self-consistency and knowledge verification, the paper does not provide a definitive solution to this issue.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different hallucination detection and mitigation techniques on LLM-generated rationales in various domains and tasks would provide evidence of their reliability.

### Open Question 2
- Question: How can the fairwashing phenomenon be addressed when using LLM-generated rationales to explain model predictions, ensuring that end-users are not misled into trusting biased or incorrect models?
- Basis in paper: [explicit] The paper discusses the risk of fairwashing, where XAI tools may mislead users into trusting biased or incorrect models, and suggests that intervention mechanisms and clear communication strategies are crucial to safeguard against this issue.
- Why unresolved: While the paper proposes intervention mechanisms like the review-then-rationalize pipeline, it does not provide a comprehensive framework for evaluating and addressing fairwashing in LLM-generated rationales, nor does it explore different communication strategies during prediction errors.
- What evidence would resolve it: Evaluations of various intervention mechanisms and communication strategies in real-world applications, along with user studies measuring trust and understanding, would provide insights into effective ways to address fairwashing in LLM-generated rationales.

### Open Question 3
- Question: How can the choice-supportive bias in LLM-generated rationales be mitigated, especially for subjective domains where multiple viable options exist depending on perspective?
- Basis in paper: [explicit] The paper mentions that the refutation complete rationales generated using the workflow promote choice-supportive bias, and suggests that future iterations of the rationalization framework may need to address this issue.
- Why unresolved: The paper does not provide specific methods or techniques to mitigate choice-supportive bias in LLM-generated rationales, nor does it explore how this bias may impact user trust and understanding in different domains.
- What evidence would resolve it: Empirical studies comparing the impact of choice-supportive bias on user trust and understanding in subjective domains, along with evaluations of different techniques to mitigate this bias, would provide insights into effective ways to address this issue in LLM-generated rationales.

## Limitations
- The generalizability of human preference results beyond the two studied datasets (CSQA, OBQA) remains untested.
- The self-consistency reviewer mechanism's performance in high-stakes domains where errors have severe consequences is not evaluated.
- The long-term effects of rationalization on human trust in automated systems are not explored.

## Confidence
- **High**: The core finding that humans prefer knowledge-grounded LLM rationales over crowdsourced ones on factuality and sufficiency dimensions.
- **Medium**: The claim that faithful rationalization of incorrect predictions erodes human trust, based on a preliminary study with limited scale.
- **Medium**: The effectiveness of the review-then-rationalize pipeline, as evaluated on two datasets with a single reviewer configuration.

## Next Checks
1. **External validation**: Test the few-shot prompting approach on a third knowledge-intensive dataset (e.g., ARC Challenge) to assess generalizability.
2. **Reviewer robustness**: Evaluate the self-consistency reviewer's performance when the KIT model's accuracy varies from 70% to 95%.
3. **Trust dynamics**: Conduct a longitudinal study measuring how repeated exposure to rationales (correct and incorrect) affects human trust over time.