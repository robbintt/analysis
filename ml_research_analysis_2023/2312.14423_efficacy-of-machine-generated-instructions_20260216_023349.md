---
ver: rpa2
title: Efficacy of Machine-Generated Instructions
arxiv_id: '2312.14423'
source_url: https://arxiv.org/abs/2312.14423
tags:
- data
- labels
- dataset
- fine-tuning
- methodology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the efficacy of machine-generated annotations
  as a cost-effective alternative to human-annotated data for fine-tuning language
  models. The authors compare three approaches: using GPT-3.5 to generate labels without
  fine-tuning, fine-tuning BERT with human-annotated data, and fine-tuning BERT with
  GPT-3.5-generated labels.'
---

# Efficacy of Machine-Generated Instructions

## Quick Facts
- arXiv ID: 2312.14423
- Source URL: https://arxiv.org/abs/2312.14423
- Reference count: 4
- Primary result: Machine-generated annotations achieve 78.54% accuracy and yield 96.01% of human-level performance when fine-tuning BERT on classification tasks

## Executive Summary
This paper investigates whether machine-generated annotations can serve as a cost-effective alternative to human-annotated data for fine-tuning language models. The authors compare three approaches: using GPT-3.5 to generate labels without fine-tuning, fine-tuning BERT with human-annotated data, and fine-tuning BERT with GPT-3.5-generated labels. Experiments on conference title classification and SQuAD datasets demonstrate that machine-generated annotations can achieve near-human performance on classification tasks while being significantly cheaper to produce. However, performance gaps emerge for more complex tasks like question answering.

## Method Summary
The paper evaluates three methodologies across two datasets: a conference title classification task (2,507 entries, 5 classes) and a SQuAD question-answering task (2,500 entries). Methodology 1 uses GPT-3.5 to generate labels via zero-shot learning. Methodology 2 fine-tunes BERT with human-annotated labels. Methodology 3 fine-tunes BERT with GPT-3.5-generated labels. The BERT model uses the AdamW optimizer with learning rate 1e-5, epsilon 1e-8, and 4-5 epochs. Performance is evaluated using accuracy and F1-score for classification, and accuracy for QA tasks.

## Key Results
- Machine-generated annotations achieved 78.54% accuracy in classification tasks
- Fine-tuning BERT with machine-generated annotations yielded 96.01% of the performance of human-annotated fine-tuning on classification tasks
- For question-answering tasks, fine-tuning with machine-generated labels performed significantly worse than human-annotated labels (33.62% vs 48.99% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Machine-generated annotations can achieve high accuracy when instructions are carefully designed and tasks are objective. GPT-3.5 generates labels based on pre-training and instruction-tuning, which can then be used to fine-tune BERT. The fine-tuned BERT model learns to replicate the distribution of labels produced by GPT-3.5. This breaks down for subjective or complex tasks where machine-generated labels deviate from human judgment.

### Mechanism 2
Fine-tuning with machine-generated annotations is cost-effective compared to human-annotated data. By replacing human annotation with GPT-3.5, data preparation costs are drastically reduced while maintaining acceptable model performance for certain tasks. The cost savings may be overstated as they don't account for computational overhead or potential human review needs.

### Mechanism 3
The quality of machine-generated annotations depends on task complexity and subjectivity. Objective tasks with clear categorization criteria (e.g., classification) are more amenable to machine-generated annotations, while subjective tasks (e.g., question answering) suffer from lower accuracy. The paper provides limited evidence about performance boundaries and failure modes.

## Foundational Learning

- Concept: Instruction tuning and zero-shot learning
  - Why needed here: The paper relies on GPT-3.5's ability to follow instructions and generate accurate labels without fine-tuning (zero-shot learning)
  - Quick check question: What is the difference between instruction tuning and traditional fine-tuning, and how does it enable zero-shot learning?

- Concept: BERT fine-tuning and transfer learning
  - Why needed here: The paper uses BERT as the downstream model and fine-tunes it with both human and machine-generated annotations
  - Quick check question: How does BERT fine-tuning differ from training a model from scratch, and what are the benefits of using a pre-trained model like BERT?

- Concept: Evaluation metrics for classification and question answering
  - Why needed here: The paper evaluates performance using accuracy, F1-score, and BLEU score
  - Quick check question: What is the difference between accuracy, F1-score, and BLEU score, and when should each metric be used to evaluate model performance?

## Architecture Onboarding

- Component map: GPT-3.5 -> BERT -> Evaluation framework
- Critical path: 1) Generate machine-annotated labels using GPT-3.5, 2) Fine-tune BERT with human and machine-generated annotations, 3) Evaluate performance on held-out test sets, 4) Compare results
- Design tradeoffs: Accuracy vs. cost (machine-generated annotations are cheaper but may have lower accuracy), task complexity suitability, data quality dependence
- Failure signatures: Large performance gaps between human and machine-generated annotations, inconsistent or incorrect labels from GPT-3.5, overfitting to synthetic labels
- First 3 experiments: 1) Generate synthetic labels for simple classification task and evaluate accuracy, 2) Fine-tune BERT with human and machine-generated annotations for classification and compare performance, 3) Repeat for question answering task and analyze differences

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal BLEU score threshold for evaluating machine-generated annotations in classification tasks? The authors mention using a BLEU score threshold of 0.5 for SQuAD but do not explore whether different tasks or datasets might require different thresholds.

### Open Question 2
How does the quality of machine-generated annotations scale with the size of the original training data? The paper only uses 2,500 entries and does not explore how annotation quality changes with larger or smaller datasets.

### Open Question 3
What is the impact of fine-tuning duration (number of epochs) on the performance gap between human-annotated and machine-generated annotations? The authors mention experimenting with more epochs as future work.

### Open Question 4
How do different prompting strategies affect the quality of machine-generated annotations? The paper uses a single prompting approach and mentions experimenting with prompts for more accurate synthetic responses.

### Open Question 5
What evaluation metrics beyond accuracy and BLEU score are most effective for assessing machine-generated annotations in subjective tasks? The authors note that accuracy might not yield the best results for subjective question answers.

## Limitations

- Task complexity mismatch: Conclusions based primarily on simple classification tasks may not generalize to more complex domains
- Instruction design opacity: Prompts are not fully specified, making it difficult to assess whether performance reflects GPT-3.5's general capability or specific prompt engineering
- Cost-benefit analysis gaps: Claims of 0.12% cost may underestimate true resource requirements by not accounting for computational overhead or human review needs

## Confidence

**High confidence**: Machine-generated annotations can achieve reasonable accuracy (78.54%) on simple classification tasks with well-designed instructions

**Medium confidence**: Machine-generated annotations yield 96.01% of human-level performance when used for fine-tuning BERT, based on a single classification task

**Low confidence**: Generalizability to other NLP tasks, particularly those involving subjective judgment or complex reasoning

## Next Checks

1. **Task complexity gradient**: Systematically test machine-generated annotations across a spectrum of task complexities to identify performance thresholds where human annotations become necessary

2. **Instruction quality sensitivity**: Conduct ablation studies varying prompt quality, instruction specificity, and example inclusion to quantify how instruction design impacts label accuracy

3. **Domain transfer validation**: Apply the methodology to domains with different characteristics (medical text, legal documents, social media) to assess whether performance patterns extend to specialized domains