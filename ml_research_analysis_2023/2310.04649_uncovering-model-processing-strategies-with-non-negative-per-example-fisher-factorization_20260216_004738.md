---
ver: rpa2
title: Uncovering Model Processing Strategies with Non-Negative Per-Example Fisher
  Factorization
arxiv_id: '2310.04649'
source_url: https://arxiv.org/abs/2310.04649
tags:
- examples
- label
- pred
- coeff
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NPEFF is a novel interpretability method that uncovers processing
  strategies used by models by decomposing per-example Fisher matrices into interpretable
  components. It represents each example's Fisher matrix as a non-negative sum of
  rank-1 positive semi-definite matrices, enabling the discovery of model sub-computations
  and their parameter dependencies.
---

# Uncovering Model Processing Strategies with Non-Negative Per-Example Fisher Factorization

## Quick Facts
- arXiv ID: 2310.04649
- Source URL: https://arxiv.org/abs/2310.04649
- Reference count: 40
- Primary result: Novel interpretability method that decomposes per-example Fisher matrices to discover model sub-computations and their parameter dependencies

## Executive Summary
NPEFF is a novel interpretability method that uncovers processing strategies used by models by decomposing per-example Fisher matrices into interpretable components. It represents each example's Fisher matrix as a non-negative sum of rank-1 positive semi-definite matrices, enabling the discovery of model sub-computations and their parameter dependencies. The method uses a novel decomposition algorithm and generates component pseudo-Fishers to estimate the impact of parameter perturbations on specific processing strategies.

## Method Summary
NPEFF computes per-example Fisher matrices that encode parameter importance for each example's predictions, then decomposes these matrices using non-negative matrix factorization or a novel low-rank algorithm. The decomposition produces interpretable components that represent model sub-computations, along with coefficients that indicate how strongly each component applies to each example. The method can generate pseudo-Fisher matrices for each component and uses parameter perturbations to verify that components reflect actual model processing strategies.

## Key Results
- Components correspond to human-recognizable concepts and model processing strategies in language and vision models
- Parameter perturbation experiments demonstrate selective impact on component-associated examples
- The method enables analysis of model heuristics and potential mitigation of faulty ones
- KL-divergence ratios show that perturbations selectively affect target components while minimizing impact on others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-example Fisher matrices encode parameter importance for each example's predictions
- Mechanism: The Fisher information matrix relates small parameter perturbations to changes in the model's predictive distribution via KL divergence
- Core assumption: Model processing of characteristic features across examples involves consistent parameter subsets
- Evidence anchors:
  - [abstract]: "The Fisher information matrix has an information geometric interpretation as a metric relating local perturbations in parameters to changes in the model's predictive distribution"
  - [section 2.1]: "The Fisher information matrix for each example relates perturbations in model parameters to changes in the model's predictive distribution, the sub-computations applied to an example become imprinted in it"
  - [corpus]: Weak evidence - no direct citations found in related papers about Fisher matrices for interpretability
- Break condition: If model parameters are not sparse or if processing is not consistent across examples with similar features

### Mechanism 2
- Claim: Decomposing per-example Fisher matrices reveals model sub-computations
- Mechanism: Each example's Fisher matrix is expressed as a non-negative sum of rank-1 PSD matrices, where each component represents a sub-computation
- Core assumption: Model's processing can be decomposed into hierarchical abstract sub-computations using consistent parameter subsets
- Evidence anchors:
  - [abstract]: "NPEFF decomposes per-example Fisher matrices using a novel decomposition algorithm that learns a set of components represented by learned rank-1 positive semi-definite matrices"
  - [section 2.2]: "Our method can be used with any end-to-end differentiable model without requiring any customization to particular architectures"
  - [corpus]: Weak evidence - related papers discuss NMF but not Fisher-based decomposition
- Break condition: If decomposition algorithm fails to converge or produces components without interpretable top examples

### Mechanism 3
- Claim: Parameter perturbations based on component pseudo-Fishers selectively disrupt specific processing strategies
- Mechanism: Perturbations are constructed to maximize impact on target component while minimizing impact on others, verified through KL-divergence ratios
- Core assumption: Pseudo-Fisher matrices accurately reflect parameter importance for specific sub-computations
- Evidence anchors:
  - [abstract]: "Using unique properties of NPEFF's parameter-space representations, we ran extensive experiments to verify that the connections between directions in parameters space and examples recovered by NPEFF actually reflect the model's processing"
  - [section 2.3]: "We wish to find a perturbation δ ∈ Rm such that gT k δ has a large magnitude for k = j and small magnitude otherwise"
  - [corpus]: Weak evidence - no related papers discuss selective parameter perturbation for interpretability
- Break condition: If perturbation selectivity is low or if KL-divergence ratios don't significantly exceed PEF norm ratios

## Foundational Learning

- Concept: Fisher Information Matrix
  - Why needed here: Forms the basis of NPEFF's representation of how parameter changes affect predictions
  - Quick check question: What does the Fisher information matrix represent in terms of parameter perturbations and predictive distributions?

- Concept: Non-negative Matrix Factorization
  - Why needed here: NPEFF uses NMF principles for decomposition when using diagonal PEF representations
  - Quick check question: How does non-negative matrix factorization differ from regular matrix factorization?

- Concept: KL Divergence
  - Why needed here: Used to measure the impact of parameter perturbations on model predictions
  - Quick check question: What does KL divergence measure between two probability distributions?

## Architecture Onboarding

- Component map: PEF computation -> Decomposition (NMF/Low-rank) -> Coefficient fitting -> Perturbation analysis
- Critical path: PEF computation → Decomposition → Coefficient fitting → Perturbation analysis
- Design tradeoffs: Diagonal vs low-rank PEF representations (tractability vs expressiveness), number of components vs interpretability
- Failure signatures: Components with no interpretable top examples, low perturbation selectivity, poor decomposition convergence
- First 3 experiments:
  1. Run NPEFF on a simple trained model with known behavior and examine component top examples
  2. Compare KL-divergence ratios to PEF norm ratios for perturbation selectivity verification
  3. Test coefficient fitting on held-out data to check component generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of sparsity level in PEF approximation affect the quality and interpretability of NPEFF components?
- Basis in paper: [explicit] The paper explores sparsity levels of 16384, 65536, and 262144 entries per PEF and reports cosine similarity metrics between component sets.
- Why unresolved: While the paper shows that different sparsity levels produce similar component sets, it doesn't systematically analyze how sparsity affects the interpretability or fidelity of the components to actual model processing strategies.
- What evidence would resolve it: A controlled study comparing component interpretability scores (human evaluation) and perturbation effectiveness across different sparsity levels, along with analysis of approximation error vs. interpretability trade-offs.

### Open Question 2
- Question: Can NPEFF be extended to handle tasks with very large numbers of classes more effectively than the current fixed-sparsity approach?
- Basis in paper: [inferred] The paper notes that LRM-NPEFF performs poorly on vision tasks with many classes and suggests future work on adaptive sparsity methods.
- Why unresolved: The paper only proposes potential solutions (varying sparsity with rank, alternative low-rank approximations) without implementing or evaluating them.
- What evidence would resolve it: Implementation and evaluation of adaptive sparsity methods on vision tasks, comparing component quality and perturbation effectiveness against the current fixed-sparsity approach.

### Open Question 3
- Question: How does NPEFF compare to other unsupervised concept discovery methods in terms of discovering model processing strategies versus just input features?
- Basis in paper: [explicit] The paper compares NPEFF to k-means clustering on activations and finds NPEFF discovers more diverse concepts, but notes both methods find statistically significant relationships with model predictions.
- Why unresolved: The paper doesn't directly test whether NPEFF discovers concepts that reflect actual model processing (beyond parameter perturbations) versus just input features that correlate with predictions.
- What evidence would resolve it: Systematic comparison using controlled experiments where ground truth processing strategies are known, testing which method better recovers these strategies versus superficial input features.

## Limitations
- Computational complexity of computing per-example Fisher matrices and running the novel low-rank decomposition algorithm
- Need to select the number of components as a hyperparameter without clear guidance on optimal selection
- Task-specific components limit transferability across different datasets or fine-tuning scenarios

## Confidence
- **High confidence**: Mathematical foundations of NPEFF are theoretically sound
- **Medium confidence**: Interpretability of components demonstrated but depends on decomposition quality
- **Low confidence**: General applicability across diverse model architectures not fully validated

## Next Checks
1. Apply NPEFF to models from different architectural families (e.g., Vision Transformers, diffusion models) and compare component interpretability and perturbation effects across architectures
2. Fine-tune a pre-trained model on a new task and apply NPEFF before and after fine-tuning to assess how components evolve and whether they remain interpretable across task shifts
3. Systematically vary the number of components, sparsity thresholds, and Fisher matrix computation methods to quantify their impact on component interpretability and perturbation selectivity