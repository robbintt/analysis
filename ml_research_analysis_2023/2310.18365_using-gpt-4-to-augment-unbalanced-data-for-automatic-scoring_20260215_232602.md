---
ver: rpa2
title: Using GPT-4 to Augment Unbalanced Data for Automatic Scoring
arxiv_id: '2310.18365'
source_url: https://arxiv.org/abs/2310.18365
tags:
- data
- augmentation
- dataset
- responses
- augmented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of GPT-4 for text data augmentation
  in automatic scoring of student responses. The researchers generated synthetic responses
  for minority scoring classes in two science datasets using GPT-4 prompts, then fine-tuned
  DistilBERT models on both original and augmented data.
---

# Using GPT-4 to Augment Unbalanced Data for Automatic Scoring

## Quick Facts
- arXiv ID: 2310.18365
- Source URL: https://arxiv.org/abs/2310.18365
- Reference count: 14
- Key outcome: GPT-4 data augmentation significantly improved automatic scoring model performance, particularly for precision and F1 scores, with 20-40% augmentation yielding stable improvements

## Executive Summary
This study investigated the use of GPT-4 for text data augmentation to address unbalanced datasets in automatic scoring of student responses. The researchers generated synthetic responses for minority scoring classes using GPT-4 prompts and fine-tuned DistilBERT models on both original and augmented data. Model performance was assessed using accuracy, precision, recall, and F1 metrics. Results demonstrated that incorporating GPT-4-augmented data significantly improved model performance, particularly for precision and F1 scores. The extent of improvement varied depending on the dataset and proportion of augmented data used, with 20-40% augmentation yielding stable improvements. Notably, GPT-4 augmented models matched or outperformed models trained with student-written data, demonstrating the effectiveness of this approach for addressing unbalanced datasets in automated assessment.

## Method Summary
The study employed DistilBERT for automatic scoring of student responses in science education. Two datasets were used, each containing responses to science assessment items with unbalanced class distributions (majority 'not proficient' class and minority 'proficient' class). GPT-4 was used to generate synthetic responses for the minority class, with 3 synthetic responses created for each original minority class response. Data was partitioned into training, validation, and testing sets at a 5:2:3 ratio, with higher representation of minority class data points in the test set. Models were fine-tuned on datasets with varying proportions of GPT-4-generated data (0%, 5%, 20%, 40%, 70%, 100%) and evaluated using accuracy, precision, recall, and F1 metrics. Performance was compared to baseline models trained on original data and models trained with additional student-written responses.

## Key Results
- GPT-4-augmented data significantly improved model performance, particularly for precision and F1 scores
- Optimal augmentation proportion varied by dataset, with 20-40% yielding stable improvements
- GPT-4 augmented models matched or outperformed models trained with student-written data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can generate high-quality synthetic student responses that resemble real student writing, thereby augmenting minority classes in unbalanced datasets.
- Mechanism: GPT-4's generative capacity, trained on massive textual data, allows it to produce text that mimics human-like responses, capturing the complexity and nuance of student language.
- Core assumption: GPT-4 can generate responses that are contextually aligned with the educational material being assessed.
- Evidence anchors:
  - [abstract] "Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision and F1 scores."
  - [section] "The pre-training procedures generate inputs and labels from the text, during which BERT acquires a robust and contextual understanding of language."
  - [corpus] Weak. Related papers focus on LLMs for automatic scoring but do not provide direct evidence of GPT-4's ability to generate high-quality synthetic responses for unbalanced datasets.
- Break condition: If GPT-4-generated responses deviate significantly from the linguistic patterns of real student responses, the augmentation will not improve model performance.

### Mechanism 2
- Claim: Incorporating GPT-4-augmented data into the training set improves the performance of automatic scoring models, particularly for precision and F1 scores.
- Mechanism: By increasing the representation of minority classes through synthetic data generation, the model learns from a more diverse set of examples, reducing bias towards majority classes.
- Core assumption: The augmented data maintains the semantic meaning and context of the original minority class responses.
- Evidence anchors:
  - [abstract] "Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision and F1 scores."
  - [section] "The notable precision at 5% augmentation, and its stability with further augmentation, suggests that the model effectively maintains a precision score and avoids false positives as more augmented data is incorporated."
  - [corpus] Weak. Related papers discuss LLMs for automatic scoring but do not provide direct evidence of the impact of augmented data on model performance metrics.
- Break condition: If the augmented data introduces noise or irrelevant information, it may negatively impact model performance.

### Mechanism 3
- Claim: GPT-4 data augmentation can sometimes outperform or match the gold standard of student-written data augmentation.
- Mechanism: GPT-4 can generate a larger volume of diverse responses compared to collecting additional student-written responses, providing more varied examples for the model to learn from.
- Core assumption: The diversity of GPT-4-generated responses is comparable to or greater than that of student-written responses.
- Evidence anchors:
  - [abstract] "Comparisons with models trained on additional student-written responses suggest that the GPT-4 augmented scoring models outperform or match the models trained with student data."
  - [section] "In contrast, for Task 2 dataset, GPT-4 data augmentation showed a very similar pattern and performance to student written data augmentation."
  - [corpus] Weak. Related papers focus on LLMs for automatic scoring but do not provide direct evidence of GPT-4's ability to match or outperform student-written data augmentation.
- Break condition: If the diversity of GPT-4-generated responses is lower than that of student-written responses, the augmentation may not improve model performance.

## Foundational Learning

- Concept: Data augmentation techniques
  - Why needed here: To address the challenge of unbalanced datasets in educational settings, where the distribution of student responses is skewed towards majority scoring categories.
  - Quick check question: What are some traditional data augmentation techniques used in natural language processing, and how do they differ from using GPT-4 for augmentation?

- Concept: Machine learning model performance metrics
  - Why needed here: To evaluate the effectiveness of the GPT-4-augmented data on the automatic scoring models, using metrics such as accuracy, precision, recall, and F1 score.
  - Quick check question: How do precision, recall, and F1 score differ, and why are they important for assessing the performance of classification models on unbalanced datasets?

- Concept: Prompt engineering
  - Why needed here: To guide GPT-4 in generating responses that resemble student-written answers, particularly for minority scoring classes.
  - Quick check question: What are some best practices for crafting effective prompts when using GPT-4 for data augmentation in educational settings?

## Architecture Onboarding

- Component map: Original dataset → GPT-4 augmentation → DistilBERT training → Performance evaluation
- Critical path: Original dataset → GPT-4 augmentation → DistilBERT training → Performance evaluation
- Design tradeoffs:
  - Using GPT-4 for augmentation saves time and resources compared to collecting additional student-written responses, but the quality of generated responses may vary.
  - Generating a large volume of synthetic responses may introduce noise or irrelevant information, potentially impacting model performance.
- Failure signatures:
  - If the augmented data does not improve model performance or leads to overfitting, it may indicate that the GPT-4-generated responses are not sufficiently diverse or contextually aligned with the original data.
  - If the performance improvement is minimal or inconsistent across different metrics, it may suggest that the augmentation is not effectively addressing the imbalance in the dataset.
- First 3 experiments:
  1. Generate a small set of synthetic responses using GPT-4 and compare their quality to real student responses using human evaluation.
  2. Train a DistilBERT model on the original dataset and evaluate its performance on a held-out test set to establish a baseline.
  3. Augment the training data with a small percentage (e.g., 5%) of GPT-4-generated responses and retrain the DistilBERT model, comparing its performance to the baseline model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal proportion of GPT-4 augmented data needed for stable improvement across different types of science assessment tasks?
- Basis in paper: Explicit - The paper found that 20-40% augmentation yielded stable improvements but notes this varied by dataset
- Why unresolved: The study only tested two specific science assessment items; optimal proportions may vary for different types of science tasks (e.g., experimental design, data analysis, conceptual explanation)
- What evidence would resolve it: Systematic testing of GPT-4 augmentation across diverse science assessment types with varying complexity and subject matter, measuring performance at different augmentation levels

### Open Question 2
- Question: How does GPT-4 generated data compare to human-generated data in terms of long-term impact on student learning outcomes?
- Basis in paper: Explicit - The paper notes that future research should examine the long-term impact of using augmented data on student learning outcomes
- Why unresolved: The study focused on model performance metrics rather than educational outcomes; no data on whether models trained on GPT-4 data affect actual student learning
- What evidence would resolve it: Longitudinal studies comparing student performance when assessed using models trained on GPT-4 augmented data versus traditional methods, measuring learning gains over time

### Open Question 3
- Question: What are the limitations of GPT-4 in generating data for highly specialized or advanced scientific domains?
- Basis in paper: Inferred - The paper tested GPT-4 augmentation on middle school level science concepts but did not explore its effectiveness for advanced scientific domains
- Why unresolved: The study's datasets were limited to basic science concepts; effectiveness of GPT-4 augmentation for complex scientific topics (e.g., quantum mechanics, advanced biochemistry) remains unknown
- What evidence would resolve it: Testing GPT-4 augmentation on assessment items from advanced scientific disciplines, comparing model performance and data quality to human-generated responses in these domains

## Limitations

- The study's findings are based on specific science education datasets and may not generalize to other domains or types of student responses
- The effectiveness of GPT-4 augmentation depends on the quality and diversity of the generated responses, which may vary depending on the specific prompts used and the nature of the original data
- The study did not explore the impact of different prompt engineering strategies or the optimal proportion of augmented data for various dataset characteristics

## Confidence

- **High Confidence**: The overall improvement in model performance (precision and F1 scores) when using GPT-4-augmented data, as evidenced by consistent results across multiple experiments and metrics.
- **Medium Confidence**: The claim that GPT-4 augmentation can match or outperform student-written data augmentation, as this finding is based on a comparison with a single gold standard and may not hold true for all datasets or educational contexts.
- **Low Confidence**: The generalizability of the results to other domains or types of student responses, as the study focused specifically on science education datasets and did not explore the impact of augmentation on different subject areas or response types.

## Next Checks

1. **Prompt Engineering Analysis**: Conduct a systematic study to identify the most effective prompts for generating high-quality synthetic responses across different minority classes and educational domains. Evaluate the impact of prompt variations on model performance and the diversity of generated responses.

2. **Optimal Augmentation Proportion**: Investigate the relationship between the proportion of augmented data and model performance across various dataset characteristics (e.g., size, class distribution, response complexity). Determine the optimal augmentation proportion for different scenarios to maximize performance gains while minimizing the risk of overfitting or introducing noise.

3. **Cross-Domain Generalization**: Replicate the study using datasets from different educational domains (e.g., mathematics, literature, social studies) and response types (e.g., open-ended, multiple-choice, short answer) to assess the generalizability of GPT-4 augmentation. Compare the effectiveness of augmentation across domains and identify potential limitations or adaptations needed for specific educational contexts.