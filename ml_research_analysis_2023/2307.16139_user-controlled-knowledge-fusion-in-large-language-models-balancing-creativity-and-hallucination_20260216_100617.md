---
ver: rpa2
title: 'User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity
  and Hallucination'
arxiv_id: '2307.16139'
source_url: https://arxiv.org/abs/2307.16139
tags:
- knowledge
- faithfulness
- degree
- llms
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a user-controllable mechanism for balancing
  creativity and factual adherence in Large Language Models (LLMs). The authors propose
  integrating a numerical tag during fine-tuning that quantifies the degree of faithfulness
  to external knowledge, computed using lexical overlap (ROUGE scores), semantic similarity
  (Sentence-BERT), and LLM self-evaluation.
---

# User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination

## Quick Facts
- arXiv ID: 2307.16139
- Source URL: https://arxiv.org/abs/2307.16139
- Reference count: 0
- Primary result: User-controlled numerical tags during fine-tuning enable dynamic adjustment of faithfulness vs creativity at inference time, outperforming baseline models across legal, medical, and creative writing tasks.

## Executive Summary
This paper introduces a user-controllable mechanism for balancing creativity and factual adherence in Large Language Models (LLMs). The authors propose integrating a numerical tag during fine-tuning that quantifies the degree of faithfulness to external knowledge, computed using lexical overlap (ROUGE scores), semantic similarity (Sentence-BERT), and LLM self-evaluation. Users can manipulate this tag during inference to control the model's reliance on reference knowledge. The approach was tested across various scenarios including legal, medical, and creative writing tasks.

## Method Summary
The method integrates a numerical tag representing faithfulness degree during fine-tuning, computed via weighted combination of ROUGE lexical overlap, Sentence-BERT semantic similarity, and GPT-4 self-evaluation scores. During inference, users manipulate this tag to control the model's reliance on external knowledge. The system was trained on OpenWebText dataset and evaluated on SQuAD for factual accuracy and creative writing datasets for creative output using BLEU, METEOR, and human evaluation metrics.

## Key Results
- Numerical tags enable users to dynamically adjust the balance between creativity and faithfulness at inference time
- The approach outperforms baseline models (GPT-4, K-BERT, ERNIE) across legal, medical, and creative writing tasks
- Strong versatility demonstrated in adapting LLM responses to different user requirements through tag manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User-controlled numerical tags during fine-tuning enable dynamic adjustment of faithfulness vs creativity at inference time
- Mechanism: A scalar tag is embedded in training data to represent faithfulness degree. During inference, this tag is set by the user to modulate the model's reliance on reference knowledge
- Core assumption: The model learns a continuous mapping from tag values to response styles, and tag manipulation reliably shifts output behavior
- Evidence anchors:
  - [abstract] "incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses"
  - [section 3.2] "During inference, users can manipulate this numerical tag to control the model's reliance on external knowledge"
  - [corpus] Weak. No direct empirical support found in cited papers for tag-based fine-tuning in LLMs
- Break condition: If the model fails to generalize the tag's meaning across domains or if the tag value range does not map to perceptibly different outputs

### Mechanism 2
- Claim: Faithfulness degree is computed via a weighted combination of lexical overlap, semantic similarity, and LLM self-evaluation
- Mechanism: Lexical overlap is measured using ROUGE scores, semantic similarity via Sentence-BERT embeddings, and LLM self-evaluation via GPT-4 scoring. These three metrics are combined into a single faithfulness score
- Core assumption: Each metric independently captures a valid dimension of faithfulness, and their weighted average meaningfully represents the total degree
- Evidence anchors:
  - [section 3.1] "measure lexical overlap between the external knowledge and the generated responses using the ROUGE... calculate the cosine similarity between the SBERT embeddings... create a prompt using the external knowledge and pass it to the trained LLM (GPT-4)"
  - [abstract] "computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score"
  - [corpus] Weak. No evidence in related works of combining these three specific metrics for faithfulness quantification
- Break condition: If one metric dominates the weighted average, or if the metrics are poorly correlated with human judgments of faithfulness

### Mechanism 3
- Claim: Higher tag values enforce strict adherence to reference knowledge, while lower values allow more creative output
- Mechanism: During fine-tuning, responses are paired with tag values; during inference, the same tag range is used to bias generation toward factual or imaginative content
- Core assumption: The fine-tuning objective preserves the tag-response relationship so that the same tag value produces similar behavior across domains
- Evidence anchors:
  - [section 3.2] "a higher value... ensure the generated responses have a higher degree of faithfulness... a lower value encourages the model to rely more on its internal knowledge"
  - [section 4.2] "in domains like medical or legal consultations... we set a higher tag value... for creative writing tasks, we lowered the tag value"
  - [corpus] Weak. Related works do not validate tag-based control across diverse scenarios
- Break condition: If the model's internal knowledge base overrides tag instructions, or if tag interpretation changes across domains

## Foundational Learning

- Concept: Fine-tuning with auxiliary control signals
  - Why needed here: The model must learn to associate a numerical faithfulness tag with output style, requiring supervised fine-tuning that includes the tag as part of the input or loss function
  - Quick check question: How does the model receive the tag during training—does it appear in the prompt, as a separate input, or influence the loss?

- Concept: Evaluation metrics for faithfulness vs creativity
  - Why needed here: To validate the tag-based control mechanism, automated and human evaluation must distinguish between factual adherence and creative divergence
  - Quick check question: Which metrics are used to quantify faithfulness (e.g., ROUGE, SBERT) and creativity (e.g., novelty, fluency)?

- Concept: Domain adaptation of faithfulness requirements
  - Why needed here: Different application scenarios demand different balances; the model must generalize tag interpretation across medical, legal, and creative contexts
  - Quick check question: How are tag thresholds calibrated for each domain, and how is transfer ensured?

## Architecture Onboarding

- Component map: Dataset → fine-tuning with faithfulness tags → model checkpoint; Prompt + tag value → generation → output; Generated output + reference → metric computation (BLEU, METEOR, human judgment)
- Critical path: Tag assignment → fine-tuning → inference control → evaluation
- Design tradeoffs:
  - Granularity vs usability: Finer tag resolution gives better control but may confuse users
  - Metric weighting: Adjusting ROUGE/SBERT/self-evaluation weights changes faithfulness measurement sensitivity
  - Knowledge base quality: Model's faithfulness is bounded by accuracy of reference data
- Failure signatures:
  - Tags ignored: Outputs show no sensitivity to tag changes
  - Overfitting: Model only responds correctly to training-domain tags
  - Metric drift: Faithfulness scores poorly align with human judgments
- First 3 experiments:
  1. Single-domain ablation: Fine-tune on one dataset, vary tag at inference, measure faithfulness via ROUGE
  2. Cross-domain transfer: Apply tag from medical to creative domain, evaluate performance drop
  3. Metric sensitivity: Vary ROUGE/SBERT/self-evaluation weights, measure effect on output diversity and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the user interface be improved to make it more intuitive for novice users to determine the optimal tag value for different tasks?
- Basis in paper: [explicit] The paper discusses the need for developing a more intuitive interface for users to facilitate the process of determining an optimal tag value based on the nature of the task
- Why unresolved: While the paper acknowledges the need for an improved interface, it does not provide specific details or solutions for how to achieve this
- What evidence would resolve it: User studies or usability tests comparing different interface designs could provide insights into the most effective way to present tag value options to users

### Open Question 2
- Question: How can the model's ability to select and integrate knowledge from multiple external knowledge bases be enhanced to improve response diversity and contextual appropriateness?
- Basis in paper: [explicit] The paper suggests that enhancing the model's ability to select and integrate knowledge from multiple external knowledge bases could lead to more diverse and contextually appropriate responses
- Why unresolved: The paper does not provide specific methods or techniques for achieving this enhancement
- What evidence would resolve it: Experiments comparing the performance of the model when using single vs. multiple knowledge bases, along with qualitative analysis of the generated responses, could provide insights into the effectiveness of this approach

### Open Question 3
- Question: How can the process of fine-tuning the model based on user feedback be automated to make the model more responsive to user needs?
- Basis in paper: [explicit] The paper mentions the intention to explore methods to automate the process of fine-tuning the model based on user feedback
- Why unresolved: The paper does not provide specific details on how this automation could be achieved
- What evidence would resolve it: A prototype implementation of an automated fine-tuning system, along with user studies evaluating its effectiveness in adapting to user preferences, could provide insights into the feasibility and benefits of this approach

## Limitations

- The faithfulness computation methodology combines three metrics without providing specific weighting coefficients or demonstrating their relative importance
- The training procedure details are incomplete, particularly regarding how the numerical tag is incorporated during fine-tuning
- Claims about cross-domain generalization of the tag mechanism and performance superiority over established baselines are not sufficiently supported by the evidence provided

## Confidence

- High confidence: The general framework of using control signals during fine-tuning to influence generation behavior is well-established in literature. The problem formulation (balancing creativity and faithfulness) is clearly articulated and addresses a real need in LLM applications
- Medium confidence: The faithfulness computation methodology (combining ROUGE, SBERT, and LLM self-evaluation) is reasonable but lacks validation that these specific metrics and their combination effectively capture human notions of faithfulness across domains
- Low confidence: Claims about cross-domain generalization of the tag mechanism and performance superiority over established baselines are not sufficiently supported by the evidence provided. The absence of specific implementation details (weighting coefficients, prompt structures) makes reproduction and verification difficult

## Next Checks

1. **Ablation study on faithfulness metrics**: Systematically vary the weights of ROUGE, SBERT, and LLM self-evaluation in the faithfulness computation and measure their individual and combined effects on output faithfulness and creativity scores. This will reveal whether all three metrics are necessary and properly calibrated.

2. **Cross-domain tag transfer experiment**: Train the model on medical domain data with specific tag values, then evaluate performance on legal and creative writing tasks using the same tag values. Measure faithfulness and creativity scores to determine if the tag mechanism generalizes or requires domain-specific recalibration.

3. **Baseline comparison under controlled conditions**: Replicate the comparison with GPT-4, K-BERT, and ERNIE using identical prompts, knowledge bases, and evaluation metrics. Ensure fair comparison by matching model sizes where possible and conducting significance testing on performance differences.