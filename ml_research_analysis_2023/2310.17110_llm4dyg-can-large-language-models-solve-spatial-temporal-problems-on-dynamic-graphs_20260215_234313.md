---
ver: rpa2
title: 'LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic
  Graphs?'
arxiv_id: '2310.17110'
source_url: https://arxiv.org/abs/2310.17110
tags:
- dynamic
- graph
- llms
- tasks
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM4DyG, a benchmark to evaluate Large Language
  Models (LLMs) on dynamic graph tasks for the first time. The authors design nine
  tasks covering temporal links, chronological paths, and dynamic triadic closure
  to assess LLMs' spatial-temporal understanding abilities.
---

# LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?

## Quick Facts
- arXiv ID: 2310.17110
- Source URL: https://arxiv.org/abs/2310.17110
- Authors: 
- Reference count: 12
- Key outcome: LLMs demonstrate preliminary spatial-temporal understanding on dynamic graphs, outperforming random baselines significantly, though performance degrades with increasing graph size and density.

## Executive Summary
This paper introduces LLM4DyG, the first benchmark designed to evaluate Large Language Models (LLMs) on dynamic graph tasks. The authors create nine tasks covering temporal links, chronological paths, and dynamic triadic closure to assess LLMs' spatial-temporal understanding abilities. Through experiments with various data generators, prompting techniques, and LLMs, the study reveals that LLMs can perform basic spatial-temporal reasoning on dynamic graphs, with significant performance improvements over random baselines. However, the performance decreases as graph size and density increase, while being less sensitive to time span and data generation mechanism.

## Method Summary
The LLM4DyG benchmark evaluates LLMs on dynamic graph tasks using synthetically generated graphs with controllable parameters. The method employs nine specially designed tasks that span temporal, spatial, and spatial-temporal dimensions. The evaluation uses different prompting techniques including zero-shot, few-shot, and chain-of-thought approaches, with the proposed Disentangled Spatial-Temporal Thoughts (DST2) prompting method showing particular promise. Various LLMs including GPT-3.5, Vicuna variants, Llama-2, and CodeLlama are tested across different data generation models (Erdos-Renyi, Stochastic Block, Forest Fire) to assess spatial-temporal reasoning capabilities.

## Key Results
- LLMs significantly outperform random baselines on all dynamic graph tasks, with GPT-3.5 showing 9.8% to 73.0% improvement over baseline
- Performance decreases as graph size and density increase, but is less sensitive to time span and data generation mechanism
- The proposed DST2 prompting method improves LLMs' spatial-temporal understanding for most tasks
- Code-pretrained models (CodeLlama) show significantly better results than standard LLMs on most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform basic spatial-temporal reasoning on dynamic graphs by leveraging in-context learning and pattern recognition
- Mechanism: The LLM processes the serialized graph data and task prompt, extracting temporal and structural patterns from the input sequence. When presented with a query, it matches against learned patterns to identify when specific graph events occur or what structures exist at given times.
- Core assumption: The LLM has sufficient capacity to represent and reason about the graph structures and temporal sequences when encoded as text, and can generalize from the few-shot examples provided in the prompt.
- Evidence anchors:
  - [abstract] "LLMs have preliminary spatial-temporal understanding abilities on dynamic graphs. We find that LLMs significantly outperform the random baseline on the dynamic graph tasks"
  - [section 4.2] "LLMs have preliminary spatial-temporal understanding abilities on dynamic graphs. As shown in Table 2, on average, GPT-3.5 has shown significant performance improvement (from +9.8% to +73.0%) over the baseline for all tasks"
- Break condition: Performance degrades significantly as graph size and density increase beyond the model's context window capacity, or when temporal patterns become too complex for the model to track across long sequences.

### Mechanism 2
- Claim: Prompt engineering, particularly chain-of-thought prompting, can activate LLMs' reasoning capabilities for complex dynamic graph tasks
- Mechanism: By explicitly instructing the model to "think step-by-step" through the spatial-temporal relationships, the LLM decomposes complex reasoning tasks into manageable sub-steps, improving accuracy on tasks requiring multiple reasoning steps.
- Core assumption: The LLM's reasoning capabilities can be activated through appropriate prompting, and that spatial-temporal reasoning can be decomposed into sequential logical steps that the model can follow.
- Evidence anchors:
  - [abstract] "the proposed DST2 prompting method can help to improve LLMs' spatial-temporal understanding abilities on dynamic graphs for most tasks"
  - [section 4.6] "the prompt 'v4' achieves the accuracy of 76.7% in the 'when link' task, significantly surpassing the one-shot prompt (33.7%), showing that guiding the LLM to handle time before nodes may help the model improve the spatio-temporal understanding ability"
- Break condition: If the prompt engineering becomes too verbose relative to the task complexity, or if the decomposition steps don't align with how the model naturally processes the information.

### Mechanism 3
- Claim: LLMs trained on code data show improved performance on dynamic graph tasks due to implicit structural and sequential reasoning patterns
- Mechanism: Code pretraining exposes the model to more explicit structural relationships and control flow patterns, which transfer to better understanding of graph structures and temporal sequences in dynamic graphs.
- Core assumption: The structural patterns and sequential reasoning found in code are sufficiently similar to those needed for dynamic graph understanding that pretraining on code provides meaningful transfer.
- Evidence anchors:
  - [section 4.4] "compared with Llama-2-13B, CodeLlama-2-13B shows significantly better results in most tasks. In particular, CodeLlama-2-13B even outperforms GPT-3.5 in the task 'when link'"
- Break condition: If the code data doesn't contain relevant structural patterns, or if the transfer is limited to specific types of graph reasoning that happen to align with code structures.

## Foundational Learning

- Concept: Dynamic graph representation and temporal evolution
  - Why needed here: Understanding how graphs change over time and how to represent these changes is fundamental to designing the tasks and evaluating LLM performance
  - Quick check question: What is the difference between a static graph and a dynamic graph in terms of information representation?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The evaluation relies on carefully crafted prompts to elicit spatial-temporal reasoning from LLMs without model fine-tuning
  - Quick check question: How does few-shot prompting differ from zero-shot prompting in terms of what the model receives as input?

- Concept: Graph theory fundamentals (paths, connectivity, triadic closure)
  - Why needed here: The benchmark tasks are built on these fundamental graph concepts, and understanding them is essential for interpreting results
  - Quick check question: What is a triadic closure in graph theory, and why is it important for dynamic graph analysis?

## Architecture Onboarding

- Component map:
  Data generation module (Erdos-Renyi, Stochastic Block, Forest Fire models) -> Task definition layer (9 tasks spanning temporal, spatial, and spatial-temporal dimensions) -> Prompt construction system (zero-shot, one-shot, chain-of-thought variations) -> LLM execution engine (GPT-3.5, Vicuna variants, Llama-2, CodeLlama) -> Evaluation framework (accuracy metrics, baseline comparison, statistical analysis)

- Critical path:
  Generate dynamic graph → Construct prompt with task and exemplar → Send to LLM → Parse and validate response → Calculate accuracy → Analyze results

- Design tradeoffs:
  - Simple graph generation vs. realistic temporal patterns: Erdos-Renyi provides controlled experiments but may not reflect real-world graph dynamics
  - Prompt complexity vs. model capacity: More sophisticated prompts may exceed context window or confuse simpler models
  - Task difficulty vs. baseline performance: Tasks must be challenging enough to differentiate LLM capabilities but not so hard that even simple baselines fail

- Failure signatures:
  - Random or nonsensical outputs from LLMs indicate prompt formatting issues or model capacity limits
  - Consistent failure on temporal tasks but success on spatial tasks suggests the model is struggling with temporal reasoning specifically
  - Performance degrading with graph size but not density (or vice versa) indicates specific bottlenecks in the model's processing

- First 3 experiments:
  1. Run baseline comparison on 'when link' task with varying graph sizes to establish performance degradation curve
  2. Compare one-shot vs. chain-of-thought prompting on 'check temporal path' task to measure reasoning activation effect
  3. Test CodeLlama vs. Llama-2 on 'sort edge by time' task to verify code pretraining benefits for temporal reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on dynamic graph tasks scale with increasing graph size and density, and what are the specific bottlenecks that cause performance degradation?
- Basis in paper: [explicit] The paper explicitly states that dynamic graph tasks exhibit increasing difficulties for LLMs as the graph size and density grow, and provides empirical evidence showing performance drops with increasing graph size and density.
- Why unresolved: The paper provides empirical evidence of performance degradation but does not offer a detailed analysis of the underlying reasons for this behavior. The specific bottlenecks and mechanisms causing the performance degradation are not identified.
- What evidence would resolve it: Detailed analysis of LLM performance on dynamic graph tasks across a wider range of graph sizes and densities, including ablation studies to isolate the impact of specific factors like graph size, density, and the number of edges on LLM performance.

### Open Question 2
- Question: How effective are current prompting techniques, including the proposed DST2 method, in improving LLM performance on dynamic graph tasks, and what are the limitations of these techniques?
- Basis in paper: [explicit] The paper proposes the DST2 prompting method and shows that it can improve LLM performance on most tasks, but also notes that no prompting method consistently achieves the best performance on all tasks.
- Why unresolved: The paper demonstrates the effectiveness of the DST2 method but does not provide a comprehensive comparison of different prompting techniques or identify the limitations of these techniques in handling dynamic graph tasks.
- What evidence would resolve it: Comparative study of various prompting techniques, including DST2, on a wider range of dynamic graph tasks, with a focus on identifying the strengths and weaknesses of each technique.

### Open Question 3
- Question: What are the potential benefits and challenges of training LLMs on code data to improve their performance on dynamic graph tasks, and how can this approach be effectively implemented?
- Basis in paper: [explicit] The paper observes that CodeLlama-2-13B, which is further pretrained on a large corpus of code data, outperforms other LLMs on most dynamic graph tasks, suggesting the potential benefits of training LLMs on code data.
- Why unresolved: The paper provides empirical evidence of the benefits of training LLMs on code data but does not explore the underlying reasons for this improvement or provide guidance on how to effectively implement this approach.
- What evidence would resolve it: Detailed analysis of the impact of code pretraining on LLM performance on dynamic graph tasks, including ablation studies to identify the specific aspects of code data that contribute to improved performance, and development of guidelines for effectively pretraining LLMs on code data for dynamic graph tasks.

## Limitations

- The benchmark relies entirely on synthetically generated dynamic graphs, raising questions about generalizability to real-world scenarios
- LLM context window limitations create scalability constraints, with performance degrading significantly as graph size and density increase
- The prompt engineering approach relies on manual design without exploring automated prompt optimization techniques or rigorous ablation studies

## Confidence

**High Confidence:**
- LLMs demonstrate basic spatial-temporal reasoning capabilities on dynamic graphs
- Performance degrades with increasing graph size and density
- Code-pretrained models show improved performance on dynamic graph tasks

**Medium Confidence:**
- DST2 prompting method improves spatial-temporal understanding
- Temporal reasoning is more challenging than spatial reasoning for LLMs

**Low Confidence:**
- The specific contribution of individual prompt components in DST2
- Generalizability to real-world dynamic graphs

## Next Checks

1. Conduct systematic experiments varying graph size and density while keeping time span constant to precisely quantify the relationship between graph complexity and LLM performance.

2. Test the same benchmark on a small set of real-world dynamic graphs (such as temporal social networks or communication graphs) to assess practical applicability and identify domain-specific challenges.

3. Design controlled experiments that isolate individual components of the DST2 prompting method to determine which elements contribute most significantly to performance improvements.