---
ver: rpa2
title: 'Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation'
arxiv_id: '2309.04369'
source_url: https://arxiv.org/abs/2309.04369
tags:
- evaluation
- llms
- tasks
- task
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel deep interaction-based evaluation framework
  for large language models (LLMs). The framework evaluates LLMs by having them interact
  with each other in elaborately designed tasks, simulating real-world scenarios where
  LLMs need to interact with users in multiple rounds.
---

# Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation

## Quick Facts
- **arXiv ID**: 2309.04369
- **Source URL**: https://arxiv.org/abs/2309.04369
- **Reference count**: 40
- **Primary result**: Proposes a deep interaction-based evaluation framework where LLMs interact with each other in designed tasks to simulate real-world multi-round interactions.

## Executive Summary
This paper introduces a novel framework for evaluating large language models (LLMs) through deep interaction. The approach moves beyond static datasets by having LLMs interact with each other in elaborately designed tasks that simulate real-world multi-round scenarios. The framework introduces fairness and stability conditions to ensure correct evaluation, using a referee LLM to manage synchronized communication and enforce anonymity. Through extensive experiments across four tasks (public goods game, idioms solitaire, code generation, and machine translation), the method demonstrates that GPT-4 achieves state-of-the-art overall performance, while ChatGPT excels particularly in Chinese-related tasks.

## Method Summary
The evaluation framework orchestrates LLMs to interact through a message pool managed by a referee LLM, ensuring synchronized turn-taking and anonymity. Each LLM is assigned a unique identity-irrelevant ID and must communicate through the referee. The framework supports both symmetric tasks (where all LLMs play identical roles) and asymmetric tasks (following writer-editor paradigms). Evaluation scores are computed based on task-specific payoff functions, with the referee judging outcomes based on interaction histories. The synchronous interaction algorithm ensures fairness by preventing information leakage about other participants' identities or strategies.

## Key Results
- GPT-4 achieved the highest overall performance across all evaluation tasks
- ChatGPT demonstrated superior performance in tasks involving Chinese language
- The synchronous interaction algorithm successfully maintained fairness conditions across all experiments
- Stability conditions were met with evaluation scores converging across task repetitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep interaction between LLMs can simulate real-world multi-round interactions with humans.
- **Mechanism**: The framework orchestrates LLMs to interact through a message pool managed by a referee LLM, ensuring synchronized turn-taking and anonymity.
- **Core assumption**: LLMs can act as both participants and evaluators in designed tasks, producing meaningful interaction histories.
- **Evidence anchors**:
  - [abstract] "LLMsâ€™ performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks."
  - [section 3.3] "The message pool in DeepEval is used for containing the interaction histories of LLMs, which serves as the history set ð» in our formal definition of deep interaction-based evaluation tasks."
  - [corpus] Weak - corpus neighbors focus on dynamic evaluation but do not mention LLM-LLM interaction as an evaluation method.
- **Break condition**: If LLMs cannot produce coherent or contextually appropriate responses during interaction, the evaluation loses validity.

### Mechanism 2
- **Claim**: Fairness and stability of evaluation are ensured by anonymity and synchronous communication enforced by the referee.
- **Mechanism**: Each LLM is assigned a unique, identity-irrelevant ID and must send/receive messages through the referee-managed message pool in a synchronized manner.
- **Core assumption**: The referee LLM can impartially manage message flow and judge task completion.
- **Evidence anchors**:
  - [abstract] "We propose a synchronous interaction algorithm to satisfy the fairness condition."
  - [section 3.3] "The referee in DeepEval is a pivotal role that is responsible for supervising the evaluation task process and evaluating the performance of LLMs based on interaction histories."
  - [corpus] Weak - corpus does not address fairness conditions in LLM-LLM evaluation.
- **Break condition**: If the referee LLM introduces bias or fails to manage synchronization, the evaluation fairness is compromised.

### Mechanism 3
- **Claim**: Asymmetric and symmetric task designs allow evaluation of both role-specific and general domain skills.
- **Mechanism**: Symmetric tasks evaluate LLMs in competitive scenarios with identical roles; asymmetric tasks follow a writer-editor paradigm to evaluate distinct abilities.
- **Core assumption**: The task rules can be formalized such that LLMs understand and execute their roles effectively.
- **Evidence anchors**:
  - [section 3.5] "In symmetric evaluation tasks, all LLMs act the same role with the same action set and task goals."
  - [section 3.5] "In asymmetric evaluation tasks, LLMs play different roles with different action sets and task goals."
  - [corpus] Weak - corpus neighbors discuss dynamic evaluation but do not detail asymmetric task designs.
- **Break condition**: If LLMs fail to differentiate or perform roles as intended, the task design loses discriminative power.

## Foundational Learning

- **Concept: Extensive games with perfect information**
  - Why needed here: Provides the mathematical foundation for modeling multi-round LLM interactions as decision sequences.
  - Quick check question: Can you describe how a game tree represents possible interaction histories in the proposed framework?

- **Concept: Payoff functions and role assignment**
  - Why needed here: Determines how performance is quantified for each LLM depending on its role in symmetric or asymmetric tasks.
  - Quick check question: How would you compute the evaluation score for an LLM that played different roles across multiple task repetitions?

- **Concept: Referee as judge model**
  - Why needed here: Ensures fairness and stability by managing message flow and evaluating outcomes.
  - Quick check question: What could go wrong if the referee LLM is not impartial or fails to judge correctly?

## Architecture Onboarding

- **Component map**: Task selection -> ID assignment -> synchronized interaction via message pool -> referee judgment -> score aggregation
- **Critical path**: Task selection â†’ ID assignment â†’ synchronized interaction via message pool â†’ referee judgment â†’ score aggregation
- **Design tradeoffs**:
  - Anonymity vs. ability to tailor strategies to opponent strengths/weaknesses
  - Synchronous vs. asynchronous communication: synchronous ensures fairness but may reduce efficiency
  - Symmetric vs. asymmetric tasks: symmetric tests general skills; asymmetric tests role-specific skills but may be harder to design
- **Failure signatures**:
  - Evaluation scores do not converge across repetitions (stability issue)
  - LLM responses become incoherent or irrelevant during interaction (fairness or task design issue)
  - Referee consistently favors certain LLMs (bias issue)
- **First 3 experiments**:
  1. Run a simple symmetric task (e.g., idioms solitaire) with two LLMs; verify that scores converge and anonymity holds
  2. Run an asymmetric task (e.g., code review) and confirm that role-specific scores are computed correctly
  3. Introduce a biased referee in a controlled test to confirm that the fairness condition detects and mitigates bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the anonymity and synchronicity of LLMs in the proposed framework impact the fairness and stability of the evaluation results?
- Basis in paper: [explicit] The paper introduces the fairness condition and stableness condition, emphasizing the importance of LLM anonymity and synchronicity in the evaluation process.
- Why unresolved: The paper does not provide empirical evidence or a detailed analysis of how these conditions specifically influence the evaluation outcomes.
- What evidence would resolve it: Comparative studies showing evaluation results with and without these conditions, and statistical analysis of the impact on fairness and stability.

### Open Question 2
- Question: What are the potential limitations or challenges in designing evaluation tasks that accurately simulate real-world deep interaction scenarios?
- Basis in paper: [inferred] The paper discusses the design of evaluation tasks and their alignment with real-world tasks, but does not address potential limitations or challenges in this process.
- Why unresolved: The complexity of real-world scenarios and the difficulty in capturing all relevant aspects in a simulated task are not explored.
- What evidence would resolve it: Case studies or examples of evaluation tasks that failed to capture real-world complexities, and analysis of the reasons for these failures.

### Open Question 3
- Question: How does the proposed framework handle the evaluation of LLMs with different roles in asymmetric tasks, and what are the implications for the overall evaluation results?
- Basis in paper: [explicit] The paper introduces asymmetric evaluation tasks and discusses the evaluation of LLMs in different roles, but does not provide a detailed analysis of the implications for overall results.
- Why unresolved: The impact of role differences on evaluation outcomes and the potential for bias or inconsistency in results are not fully explored.
- What evidence would resolve it: Detailed analysis of evaluation results for LLMs in different roles, and comparison with results from symmetric tasks to identify potential biases or inconsistencies.

## Limitations

- The framework's effectiveness depends heavily on the referee LLM's ability to manage complex interactions impartially, which may not scale well to more complex tasks
- The choice of evaluation tasks, while diverse, may not fully capture the breadth of real-world scenarios where LLMs interact with humans
- The framework requires significant computational resources due to the synchronous interaction protocol and multiple LLM evaluations

## Confidence

- **High confidence**: The core concept of using LLM-LLM interaction for evaluation is novel and well-founded. The theoretical framework for fairness and stability is sound.
- **Medium confidence**: The effectiveness of the proposed synchronous interaction algorithm in ensuring fair evaluation. The generalizability of the evaluation tasks to real-world scenarios.
- **Low confidence**: The scalability of the framework to more complex tasks or larger numbers of participating LLMs. The potential for bias in the referee LLM's judgments across diverse scenarios.

## Next Checks

1. **Bias Detection Test**: Systematically introduce controlled biases in the referee LLM and measure how the framework detects and mitigates these biases across different task types.
2. **Scalability Assessment**: Evaluate the framework's performance with 6+ LLMs in each task, measuring both computational efficiency and evaluation quality degradation.
3. **Generalizability Validation**: Design and implement three additional evaluation tasks from completely different domains (e.g., legal reasoning, creative writing, mathematical problem-solving) to test the framework's adaptability.