---
ver: rpa2
title: Harnessing large-language models to generate private synthetic text
arxiv_id: '2306.01684'
source_url: https://arxiv.org/abs/2306.01684
tags:
- data
- synthetic
- training
- downstream
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality synthetic
  text data while preserving differential privacy. The authors propose a method that
  leverages pre-trained large language models, fine-tuned on sensitive data using
  differentially private training, to generate synthetic datasets.
---

# Harnessing large-language models to generate private synthetic text

## Quick Facts
- arXiv ID: 2306.01684
- Source URL: https://arxiv.org/abs/2306.01684
- Reference count: 40
- Primary result: Prompt-tuning with differential privacy outperforms fine-tuning for synthetic data generation, achieving up to 10 percentage points higher accuracy on downstream classifiers

## Executive Summary
This paper presents a method for generating high-quality synthetic text data while preserving differential privacy using pre-trained large language models. The approach leverages DP fine-tuning with a specialized loss function (Prefix-LM) and parameter-efficient tuning (prompt-tuning) to produce synthetic datasets that match the performance of models trained directly on real data with differential privacy. The method demonstrates strong results on sentiment classification tasks, showing that synthetic data can effectively replace sensitive data for downstream training while maintaining privacy guarantees.

## Method Summary
The method involves pre-training or obtaining a decoder-only LLM (8B parameters) on public data with de-duplication against sensitive datasets, then fine-tuning or prompt-tuning this model on sensitive data using DP-SGD with Prefix-LM loss. Synthetic data is generated from the trained model and used to train downstream classifiers (BERT/CNN) for evaluation. Hyperparameters for DP-training are tuned using proxy metrics on synthetic data, with performance measured by downstream classifier accuracy and ranking correlation for hyperparameter tuning.

## Key Results
- Prompt-tuning with DP achieves up to 10 percentage points higher accuracy on downstream classifiers compared to fine-tuning
- Generating more synthetic data than the original dataset improves performance, especially for simpler downstream models
- Proxy metrics (MAUVE, perplexity) show high ranking correlation (up to 86%) with real data for hyperparameter tuning
- The method is competitive with direct DP-training of downstream classifiers in terms of performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix-LM loss improves DP fine-tuning utility by preventing model learning of prefix token distribution
- Mechanism: Assigns zero weights to prefix tokens, reducing noise corruption during DP training
- Core assumption: Noise added during DP training corrupts gradients from prefix tokens, degrading performance
- Evidence anchors: [abstract] demonstrates that proper training objective with fewer tuned parameters results in excellent DP synthetic data quality; [section] explains how standard next-token prediction loss forces model to learn prefix token distribution

### Mechanism 2
- Claim: Prompt-tuning with DP outperforms full model fine-tuning for synthetic data generation
- Mechanism: Tunes fewer parameters, reducing total noise added during DP training
- Core assumption: Smaller models have smaller gap between private and non-private utility due to less total noise injection
- Evidence anchors: [abstract] states approach is competitive with direct DP-training; [section] found prompt tuning with DP achieves much higher utility than full model fine-tuning

### Mechanism 3
- Claim: Generating more synthetic data than original dataset size is helpful, especially for simpler downstream models
- Mechanism: Increased synthetic data provides more training examples, improving simpler model performance
- Core assumption: Simpler models are more dependent on quantity of training data for good performance
- Evidence anchors: [abstract] reveals generating more synthetic examples is helpful; [section] observed generally improves downstream performance but benefit diminishes with compute as limiting factor

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: Core privacy guarantee protecting sensitive training data during synthetic data generation
  - Quick check question: What is the main difference between local DP and central DP in the context of this paper?

- Concept: Language Model Fine-tuning
  - Why needed here: Primary method for generating synthetic data from sensitive data
  - Quick check question: What is the difference between full model fine-tuning and prompt-tuning in the context of this paper?

- Concept: Loss Functions in NLP
  - Why needed here: Choice of loss function significantly impacts DP fine-tuning performance
  - Quick check question: How does Prefix-LM differ from standard next-token prediction loss in terms of weight assignment to prefix tokens?

## Architecture Onboarding

- Component map: Pre-trained LLM → DP Fine-tuning (Prompt-tuning/Fine-tuning) → Synthetic Data Generation → Downstream Model Training
- Critical path: Pre-trained LLM → DP Fine-tuning → Synthetic Data Generation
- Design tradeoffs: Prompt-tuning vs. Fine-tuning (fewer parameters vs. full model capacity), amount of synthetic data generated vs. computational cost
- Failure signatures: Poor downstream model performance, high perplexity in generated synthetic data, low ranking correlation in hyperparameter tuning experiments
- First 3 experiments:
  1. Compare downstream model performance on synthetic data generated from fine-tuned vs. prompt-tuned models
  2. Analyze effect of generating different amounts of synthetic data on downstream model performance
  3. Evaluate effectiveness of proxy metrics (MAUVE, perplexity) in estimating synthetic data quality

## Open Questions the Paper Calls Out

1. How does synthetic data quality vary across different languages or domains beyond English text?
2. What is the optimal balance between synthetic and real data when combined for training downstream models?
3. Why does prompt-tuning with differential privacy consistently outperform fine-tuning in terms of downstream task performance?

## Limitations

- Lacks comprehensive ablation studies to isolate individual contributions of Prefix-LM loss and prompt-tuning
- De-duplication procedure for ensuring privacy described only at high level, limiting reproducibility
- Optimal DP-training hyperparameters tuned using proxy metrics without establishing their robustness across different domains

## Confidence

- High confidence: Prompt-tuning with DP outperforms full fine-tuning for synthetic data generation
- Medium confidence: Generating more synthetic data than original dataset improves performance for simpler models
- Medium confidence: Proxy metrics effective for hyperparameter tuning based on high ranking correlations

## Next Checks

1. Conduct controlled ablation study isolating effects of Prefix-LM loss versus prompt-tuning
2. Implement and test exact de-duplication procedure described in paper
3. Evaluate ranking correlation of proxy metrics across diverse datasets and downstream tasks