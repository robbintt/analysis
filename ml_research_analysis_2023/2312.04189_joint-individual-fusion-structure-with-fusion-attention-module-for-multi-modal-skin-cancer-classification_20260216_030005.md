---
ver: rpa2
title: Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal
  Skin Cancer Classification
arxiv_id: '2312.04189'
source_url: https://arxiv.org/abs/2312.04189
tags:
- fusion
- jif-mmfa
- structure
- image
- metadata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses skin cancer classification using both dermoscopic
  images and patient metadata. Current fusion methods only learn joint features, ignoring
  modality-specific information.
---

# Joint-Individual Fusion Structure with Fusion Attention Module for Multi-Modal Skin Cancer Classification

## Quick Facts
- arXiv ID: 2312.04189
- Source URL: https://arxiv.org/abs/2312.04189
- Reference count: 40
- Primary result: JIF-MMFA improves skin cancer classification accuracy by learning shared and modality-specific features through joint-individual fusion and attention mechanisms

## Executive Summary
This paper addresses skin cancer classification using both dermoscopic images and patient metadata. Current fusion methods only learn joint features, ignoring modality-specific information. The authors propose a joint-individual fusion (JIF) structure that learns shared features while preserving modality-specific ones, and a multi-modal fusion attention (MMFA) module that enhances relevant features using self and mutual attention mechanisms. Evaluated on three public datasets (PAD-UFES-20, SPC, ISIC-2019) with five CNN backbones, the JIF-MMFA method improves classification accuracy across all scenarios, achieving state-of-the-art performance. For example, on PAD-UFES-20, it improves average balanced accuracy from 67.0% to 77.2%. The method is particularly effective for datasets with richer metadata and demonstrates the importance of combining image and clinical data for skin cancer diagnosis.

## Method Summary
The proposed method combines a Joint-Individual Fusion (JIF) structure with a Multi-Modal Fusion Attention (MMFA) module. The JIF structure preserves modality-specific features while learning shared representations through three parallel branches (image, metadata, joint) with separate classifiers. The MMFA module uses multi-head attention to implement both self-attention and mutual attention, enhancing relevant features from both image and metadata modalities. During training, gradients from image and metadata classifiers guide their respective branches to preserve specific representations while the joint branch optimizes overall feature learning. Predictions from all three classifiers are integrated at the decision level through weighted averaging. The model is trained using SGD with CosineAnnealingLR, class-weighted cross-entropy loss, and data augmentation techniques.

## Key Results
- JIF-MMFA improves average balanced accuracy from 67.0% to 77.2% on PAD-UFES-20 dataset
- Method achieves state-of-the-art performance across all three datasets (PAD-UFES-20, SPC, ISIC-2019) with five different CNN backbones
- Particularly effective on datasets with richer metadata (21 features on PAD-UFES-20 vs 3 on ISIC-2019)
- Ablation studies show both JIF structure and MMFA module contribute to performance improvements

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The Joint-Individual Fusion (JIF) structure improves performance by preserving modality-specific features while learning shared representations.
- Mechanism: The JIF structure includes separate classifiers for image features (CI) and metadata features (CM) alongside a joint classifier (CIM). During training, gradients from LI and LM guide the image and metadata branches to preserve specific representations while LIM optimizes the joint feature representation. This allows the model to learn both shared and modality-specific information simultaneously.
- Core assumption: Preserving modality-specific features is crucial for multi-modal tasks and contributes to better joint feature learning.
- Evidence anchors:
  - [abstract] "Current methods only use the simple joint fusion structure (FS) and fusion modules (FMs) for the multi-modal classification methods, there still is room to increase the accuracy by exploring more advanced FS and FM."
  - [section 2.2] "The gradient from LI (blue) and LM (green) guide the image branch and metadata branch to preserve the specific representations fM and fI, respectively. LIM optimizes the whole structure and thus obtains the joint feature representation FIM."
- Break condition: If preserving modality-specific features becomes detrimental to learning shared representations, or if the additional computational overhead outweighs the performance gains.

### Mechanism 2
- Claim: The Multi-Modal Fusion Attention (MMFA) module enhances relevant features by using both self and mutual attention mechanisms.
- Mechanism: The MMFA module uses a multi-head attention block to implement self-attention on both image and metadata features simultaneously. It transforms both modalities into key, query, and value vectors, allowing each modality to enhance the other while also being enhanced by its own features. This creates a richer, more contextually relevant feature representation.
- Core assumption: Both self-attention and mutual attention are necessary to capture the full relationship between modalities and within each modality.
- Evidence anchors:
  - [abstract] "we introduce a fusion attention (FA) module that enhances the most relevant image and metadata features based on both the self and mutual attention mechanism to support the decision-making pipeline."
  - [section 2.3] "The MMFA module implements both mutual attention and self attention mechanism to this two-modality data to get the enhanced fusion feature vectors."
- Break condition: If the attention mechanism becomes too complex relative to the information gain, or if the mutual attention introduces noise rather than useful signal.

### Mechanism 3
- Claim: Decision-level fusion of predictions from multiple classifiers improves overall accuracy.
- Mechanism: The JIF structure produces three predictions (PI, PM, PIM) which are then integrated at the decision level through weighted averaging. This allows the model to leverage the strengths of each modality's classifier while compensating for individual weaknesses.
- Core assumption: Each modality's classifier captures unique and complementary information that, when combined, provides more robust predictions than any single classifier alone.
- Evidence anchors:
  - [section 2.2] "During testing, since there are three classifiers in the JIF structure, we naturally integrate these three predictions at the decision level for more accurate results."
  - [section 3.3.2] "JIF-MMFA (All) gets a slightly higher value in averaged BAC than JIF-MMFA (OFB) on these three datasets."
- Break condition: If the correlation between predictions is too high, making the ensemble less effective, or if one modality consistently dominates the others, making the ensemble unnecessary.

## Foundational Learning
- Concept: Multi-modal learning and fusion techniques
  - Why needed here: This paper specifically addresses the challenge of combining dermatological images with patient metadata for skin cancer classification, requiring understanding of how different data modalities can be effectively integrated.
  - Quick check question: What are the key differences between early fusion, late fusion, and hybrid fusion approaches in multi-modal learning?

- Concept: Attention mechanisms in deep learning
  - Why needed here: The MMFA module relies on self-attention and mutual attention mechanisms to enhance relevant features from both modalities, which is central to the proposed method's effectiveness.
  - Quick check question: How does multi-head attention differ from single-head attention, and why might it be more effective for this application?

- Concept: Skin cancer classification and dermoscopic imaging
  - Why needed here: Understanding the specific challenges in skin cancer diagnosis, including the importance of both visual features and clinical metadata, provides context for why this multi-modal approach is valuable.
  - Quick check question: What are the main challenges in distinguishing between different types of skin lesions using only dermoscopic images?

## Architecture Onboarding
- Component map:
  - Input layer: Dermatological images (224×224×3) and patient metadata (various dimensions)
  - Image feature extractor: CNN backbone (e.g., DenseNet-121, MobileNet-v2, ResNet-50, EfficientNet-B3, Xception)
  - Metadata feature extractor: Fully connected layers with one-hot encoding
  - JIF structure: Three parallel branches (image, metadata, joint) with separate classifiers
  - MMFA module: Multi-head attention with transformation layers
  - Output layer: Weighted average of three predictions (PI, PM, PIM)

- Critical path:
  1. Extract image features using CNN backbone
  2. Extract metadata features using fully connected layers
  3. Process features through JIF structure
  4. Apply MMFA module to enhance features
  5. Generate predictions from three classifiers
  6. Combine predictions through weighted averaging

- Design tradeoffs:
  - Computational complexity vs. performance: JIF-MMFA increases model parameters and inference time but significantly improves accuracy
  - Attention mechanism depth vs. overfitting risk: Deeper attention mechanisms may capture more complex relationships but risk overfitting on smaller datasets
  - Decision-level fusion weighting: Equal weighting assumes all modalities are equally important, but domain knowledge might suggest different weights

- Failure signatures:
  - Performance degradation on datasets with limited metadata: As seen with ISIC-2019 dataset, which has fewer metadata types
  - Over-reliance on one modality: If one classifier consistently dominates, the ensemble may not provide benefits
  - Attention mechanism failure: If the attention weights become uniform or noisy, feature enhancement may not occur

- First 3 experiments:
  1. Baseline comparison: Train and evaluate the same CNN backbones without metadata to establish the performance improvement from adding metadata
  2. Ablation study: Compare JIF-MMFA with and without the MMFA module to isolate the contribution of the attention mechanism
  3. Cross-dataset evaluation: Test the model's generalization by training on one dataset and evaluating on another to assess robustness across different data distributions

## Open Questions the Paper Calls Out
- Open Question 1: Does the effectiveness of the JIF structure depend on the number and diversity of metadata features available in the dataset?
- Open Question 2: Would the MMFA module be more effective than simpler fusion methods if trained from scratch rather than fine-tuned from ImageNet weights?
- Open Question 3: Would a dynamic weighting of the three loss functions in the JIF structure (rather than the fixed 0.5 weight used) improve performance?

## Limitations
- Method effectiveness depends heavily on availability of rich metadata, with reduced performance on datasets with limited clinical information
- Computational overhead of JIF structure and MMFA module (approximately 15-20% more parameters) may not justify marginal gains in resource-constrained settings
- Fixed weighting of loss functions assumes equal importance of all modalities, which may not hold across different datasets or clinical scenarios

## Confidence
- **High confidence**: The JIF structure's ability to preserve modality-specific features while learning shared representations is well-supported by ablation studies and theoretical foundations in multi-modal learning.
- **Medium confidence**: The MMFA module's contribution is demonstrated through improved metrics, but the exact mechanisms by which self and mutual attention enhance feature representations could be more thoroughly explored.
- **Medium confidence**: The decision-level fusion approach is effective but assumes equal contribution from all classifiers, which may not hold across all datasets or clinical scenarios.

## Next Checks
1. **Ablation on attention mechanism**: Remove the MMFA module and retrain the JIF structure to quantify the exact contribution of attention mechanisms versus the fusion structure itself.
2. **Metadata importance analysis**: Perform feature importance ranking on the metadata to identify which clinical features contribute most to classification improvements and test the model's performance with only the top 5-10 features.
3. **Cross-dataset generalization test**: Train the model on PAD-UFES-20 and evaluate on SPC and ISIC-2019 to assess robustness across different data distributions and metadata availability scenarios.