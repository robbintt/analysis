---
ver: rpa2
title: Stochastic Gradient Descent for Gaussian Processes Done Right
arxiv_id: '2310.20581'
source_url: https://arxiv.org/abs/2310.20581
tags:
- gradient
- random
- gaussian
- descent
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents stochastic dual descent (SDD), a new algorithm
  for solving the large-scale linear systems that arise in Gaussian process regression.
  SDD uses the dual objective (rather than primal), random coordinate gradients (rather
  than random features), Nesterov momentum, and geometric iterate averaging.
---

# Stochastic Gradient Descent for Gaussian Processes Done Right

## Quick Facts
- arXiv ID: 2310.20581
- Source URL: https://arxiv.org/abs/2310.20581
- Reference count: 19
- Key outcome: Stochastic Dual Descent (SDD) outperforms existing baselines on large-scale Gaussian process regression tasks

## Executive Summary
This paper presents Stochastic Dual Descent (SDD), a new algorithm for solving large-scale linear systems in Gaussian process regression. SDD combines dual objective formulation, random coordinate gradients, Nesterov momentum, and geometric iterate averaging to achieve superior performance on both UCI regression datasets and molecular binding affinity prediction tasks. The algorithm demonstrates state-of-the-art results, matching or exceeding the performance of specialized graph neural networks on molecular data while maintaining competitive performance on traditional regression benchmarks.

## Method Summary
SDD solves kernel ridge regression problems by optimizing the dual objective using stochastic gradient descent with random coordinate sampling. The method leverages the dual formulation's better conditioning to enable larger step sizes, uses multiplicative noise gradients from coordinate sampling that decay near the optimum, and employs geometric averaging for improved asymptotic performance. The algorithm maintains a velocity term for Nesterov momentum and computes kernel matrix rows on-demand, making it scalable to large datasets. Implementation involves careful tuning of batch size, momentum parameter, and averaging rate to balance convergence speed and stability.

## Key Results
- SDD outperforms preconditioned conjugate gradients and stochastic variational Gaussian processes on standard UCI regression tasks
- On molecular binding affinity prediction, SDD matches or exceeds the performance of Attentive FP (a state-of-the-art graph neural network)
- The algorithm demonstrates faster convergence and better asymptotic performance compared to SGD baselines with random feature sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual formulation has better conditioning than primal, allowing faster convergence with larger step sizes
- Mechanism: Dual objective's Hessian has condition number O(1+κn/λ) versus O(κn(κn+λ)) for primal, enabling larger step sizes without divergence
- Core assumption: Dual and primal objectives share the same unique minimizer
- Evidence anchors: [abstract], [section 3.1]
- Break condition: Dual gradient computation becomes expensive for non-stationary kernels

### Mechanism 2
- Claim: Random coordinate sampling produces multiplicative noise gradients that decay as iterates approach optimum
- Mechanism: Variance of gradient estimate scales with distance to optimum, automatically reducing noise during convergence
- Core assumption: Kernel matrix K is positive definite and accessible via individual row computations
- Evidence anchors: [section 3.2], [figure 2 caption]
- Break condition: Highly correlated gradient coordinates or poor row-wise kernel matrix properties

### Mechanism 3
- Claim: Geometric iterate averaging provides better asymptotic performance than arithmetic averaging for multiplicative noise
- Mechanism: Maintains multiplicative noise property throughout optimization, avoiding additional variance in tail
- Core assumption: Constant step size and multiplicative (not additive) noise
- Evidence anchors: [section 3.3], [figure 3 caption]
- Break condition: Optimization problem has additive noise or step size scheduling

## Foundational Learning

- Concept: Strong duality in convex optimization
  - Why needed here: Justifies using dual formulation by showing it shares minimizer with primal
  - Quick check question: If a convex optimization problem has strong duality, what is the relationship between optimal values of primal and dual problems?

- Concept: Stochastic approximation with multiplicative vs additive noise
  - Why needed here: Central to choosing coordinate sampling over random feature sampling
  - Quick check question: How does variance of gradient estimate behave near optimum for multiplicative vs additive noise in SGD?

- Concept: Nesterov momentum and convergence properties
  - Why needed here: Key component enabling fast convergence rates
  - Quick check question: What is primary advantage of Nesterov momentum over standard momentum in gradient descent?

## Architecture Onboarding

- Component map: Gradient computation -> Momentum update -> Parameter update -> Averaging -> Convergence check
- Critical path: Compute dual gradient → Update velocity with momentum → Update parameters → Apply geometric averaging → Check convergence
- Design tradeoffs:
  - Larger batch sizes reduce variance but increase per-iteration cost
  - Higher momentum speeds convergence but may cause instability
  - Geometric averaging improves asymptotic performance but requires tuning
  - Dual formulation enables larger steps but needs careful gradient implementation
- Failure signatures:
  - Divergence: Step size too large or kernel matrix poorly conditioned
  - Slow convergence: Batch size too small or momentum suboptimal
  - Numerical instability: Ill-conditioned kernel matrix or precision issues
- First 3 experiments:
  1. Verify convergence on small UCI dataset with known optimal solution
  2. Compare primal vs dual formulation with identical hyperparameters
  3. Test random coordinate vs random feature sampling with same step size and batch size

## Open Questions the Paper Calls Out

- Open Question 1: How does SDD performance scale with increasing dimensionality of input space for high-dimensional molecular fingerprints?
  - Basis: Current experiments use fixed 1024-dimensional fingerprints
  - Why unresolved: Paper doesn't explore impact of varying fingerprint dimensionality
  - Resolution evidence: Experiments varying fingerprint dimensionality while keeping other factors constant

- Open Question 2: What is impact of different kernel functions on SDD's molecular prediction performance?
  - Basis: Paper uses specific kernels but doesn't explore kernel selection systematically
  - Why unresolved: Focus on demonstrating effectiveness rather than exploring kernel strategies
  - Resolution evidence: Systematic comparison across multiple kernel families on same molecular datasets

- Open Question 3: How does SDD compare to deep learning with sophisticated molecular representations beyond Morgan fingerprints?
  - Basis: Shows SDD matching Attentive FP using Morgan fingerprints only
  - Why unresolved: Current experiments use simple fingerprint representations
  - Resolution evidence: Experiments comparing SDD against deep learning with graph-based or 3D structural representations

## Limitations
- Empirical evidence limited to specific problem domains and hyperparameter settings
- No comprehensive ablation studies isolating each algorithmic component's contribution
- Theoretical convergence rates don't account for combined effect of all innovations

## Confidence
- High: Theoretical framework connecting dual optimization and conditioning
- Medium: Empirical superiority claims, mechanism contributions
- Low: Molecular binding affinity results compared to broader state-of-the-art

## Next Checks
1. Ablation study on mechanism contributions: Systematically disable momentum, geometric averaging, and dual formulation separately
2. Scaling analysis: Evaluate SDD on synthetic datasets with controlled condition numbers and dimensionality
3. Baselines on molecular task: Compare against broader range of graph neural networks and kernel methods using standardized protocols