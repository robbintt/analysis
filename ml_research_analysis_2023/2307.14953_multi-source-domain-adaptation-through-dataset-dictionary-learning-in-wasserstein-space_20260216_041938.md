---
ver: rpa2
title: Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein
  Space
arxiv_id: '2307.14953'
source_url: https://arxiv.org/abs/2307.14953
tags:
- domain
- wasserstein
- learning
- distributions
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Dataset Dictionary Learning (DaDiL), a novel
  Multi-Source Domain Adaptation (MSDA) framework based on dictionary learning and
  optimal transport. DaDiL interprets each domain as an empirical distribution and
  expresses them as Wasserstein barycenters of dictionary atoms, which are also empirical
  distributions.
---

# Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space

## Quick Facts
- arXiv ID: 2307.14953
- Source URL: https://arxiv.org/abs/2307.14953
- Authors: Not specified in source material
- Reference count: 40
- Primary result: State-of-the-art improvements of 3.15%, 2.29%, and 7.71% in classification performance on Caltech-Office, Office 31, and CWRU benchmarks respectively

## Executive Summary
This paper introduces Dataset Dictionary Learning (DaDiL), a novel Multi-Source Domain Adaptation (MSDA) framework that leverages dictionary learning and optimal transport. The key innovation is interpreting each domain as an empirical distribution and expressing them as Wasserstein barycenters of dictionary atoms, which are also empirical distributions. The method learns atom distributions and barycentric coordinates through mini-batch optimization, enabling non-linear interpolation across distributional shifts. Two MSDA strategies are proposed: DaDiL-R for reconstructing labeled samples in the target domain, and DaDiL-E for ensembling classifiers learned on atom distributions.

## Method Summary
DaDiL represents each domain (source and target) as an empirical distribution and learns a dictionary of atom distributions P such that each domain can be expressed as a Wasserstein barycenter of these atoms. The learning process involves minimizing a reconstruction loss that measures the Wasserstein distance between each domain distribution and its barycenter reconstruction. After learning the dictionary and barycentric coordinates, two strategies are employed for MSDA: DaDiL-R reconstructs labeled samples in the target domain using the learned atoms, enabling classifier training on synthetic target data; DaDiL-E ensembles classifiers trained on each atom distribution, weighted by the optimal barycentric coordinates for the target domain.

## Key Results
- State-of-the-art improvements of 3.15% on Caltech-Office, 2.29% on Office 31, and 7.71% on CWRU classification benchmarks
- Visualizations demonstrate that interpolations in the Wasserstein hull of learned atoms produce data that generalizes to the target domain
- DaDiL successfully learns meaningful dictionary atoms that capture the distributional characteristics of multiple source domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DaDiL enables MSDA by representing each domain as a Wasserstein barycenter of learned atom distributions, allowing non-linear interpolation across distributional shifts.
- Mechanism: Each domain is modeled as an empirical distribution and expressed as a Wasserstein barycenter B(αℓ; P) of learned atom distributions P, enabling interpolation across the Wasserstein hull M(P) to reconstruct target domain data.
- Core assumption: The target domain distribution lies within the Wasserstein hull of the learned atoms (ˆQT ∈ M(P)).
- Evidence anchors: [abstract] "We interpret each domain in MSDA as an empirical distribution... we express each domain as a Wasserstein barycenter of dictionary atoms"; [section] "For N = NS + 1, DaDiL consists on minimizing (P ⋆, A⋆) = argminP,A∈(∆K)N 1/N Σℓ=1 L(ˆQℓ, B(αℓ; P))"
- Break condition: If the target domain distribution lies outside the Wasserstein hull of learned atoms, DaDiL cannot accurately reconstruct or classify target samples.

### Mechanism 2
- Claim: DaDiL-R achieves MSDA by reconstructing labeled samples in the target domain through Wasserstein barycenters, enabling classifier training on synthetic target data.
- Mechanism: After learning the dictionary, DaDiL-R computes ˆBT = B(αT; P), the distribution in M(P) closest to ˆQT, and trains a classifier on ˆBT since it has labeled support.
- Core assumption: There exists a transformation T such that QT(Y|X) = BT(Y|T(X)), meaning the class structure in the target is preserved in the barycenter reconstruction.
- Evidence anchors: [abstract] "DaDiL-R, based on the reconstruction of labeled samples in the target domain"; [section] "We assume the existence ofK > 1 unknown distributions... for which Qℓ can be approximated as their interpolation in Wasserstein space"
- Break condition: If the class structure in the target domain differs significantly from what can be represented in the barycenter, the classifier trained on ˆBT will generalize poorly to actual target samples.

### Mechanism 3
- Claim: DaDiL-E achieves MSDA by ensembling classifiers learned on atom distributions, weighted by optimal barycentric coordinates.
- Mechanism: Classifiers ˆhk trained on each atom distribution ˆPk are combined using weights αT,k, where αT are the barycentric coordinates that best reconstruct the target distribution.
- Core assumption: The optimal barycentric coordinates αT that minimize reconstruction loss also provide optimal weighting for classifier ensembling.
- Evidence anchors: [abstract] "DaDiL-E, based on the ensembling of classifiers learned on atom distributions"; [section] "Theorem 2 Let {X(Pk)}Kk=1... be i.i.d. samples fromPk and QT... Let ˆhk be the minimizer of RPk and Rα(h) = Σk αkRPk(h)"
- Break condition: If the optimal barycentric coordinates for reconstruction do not correspond to optimal classifier weighting, DaDiL-E performance will degrade.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distance
  - Why needed here: OT provides the mathematical framework for comparing and interpolating between probability distributions, fundamental to DaDiL's approach of representing domains as barycenters of atoms.
  - Quick check question: What is the computational complexity of calculating the exact Wasserstein distance between two point clouds of size n, and why does DaDiL use mini-batch OT?

- Concept: Dictionary Learning (DiL)
  - Why needed here: DiL provides the framework for representing data as weighted combinations of basis elements (atoms), which DaDiL extends to the distribution space by learning atom distributions instead of vectors.
  - Quick check question: How does DaDiL's dictionary learning differ from traditional DiL in terms of what constitutes an "atom" and a "representation"?

- Concept: Barycentric coordinates and convex combinations
  - Why needed here: Barycentric coordinates determine how each domain is expressed as a combination of atom distributions, enabling interpolation across distributional shifts.
  - Quick check question: What constraint is placed on the barycentric coordinates α in DaDiL, and why is this constraint necessary?

## Architecture Onboarding

- Component map: Encoder network -> Dataset Dictionary Learning module -> Classification head -> MSDA strategy selector
- Critical path:
  1. Pre-train encoder on source domain data
  2. Extract features for all source and target samples
  3. Run DaDiL learning loop to obtain dictionary P and weights A
  4. Apply chosen MSDA strategy (R or E) to classify target samples
- Design tradeoffs:
  - Number of atoms K vs. computational complexity: More atoms provide better expressiveness but increase computation
  - Support size n vs. representation quality: Larger support improves approximation but increases memory usage
  - Mini-batch size nb vs. stability: Larger batches provide more stable gradients but require more memory
- Failure signatures:
  - Poor reconstruction loss indicates atoms don't span the distributional space well
  - High variance in αT across runs suggests unstable optimization
  - Degraded performance on one domain but not others may indicate overfitting to specific source domains
- First 3 experiments:
  1. Verify DaDiL can reconstruct source domains perfectly when trained and tested on the same domain
  2. Test DaDiL-R on a simple 2D synthetic dataset where you can visualize the learned atoms and barycenters
  3. Compare DaDiL-E performance with and without the label integration in the ground-cost to verify its importance

## Open Questions the Paper Calls Out
- What are the theoretical properties and stability guarantees of the learned dictionary atoms in DaDiL? (The paper acknowledges that theoretical analysis of minimizers is challenging and will be considered in future works)
- How does DaDiL perform on more challenging domain adaptation scenarios with larger domain shifts or different data modalities? (The current experiments focus on relatively similar domains and image data)
- Can the learned dictionary atoms in DaDiL be interpreted or have any semantic meaning? (The paper does not provide any analysis or visualization of the learned dictionary atoms)

## Limitations
- Performance gains evaluated primarily on standard benchmarks with relatively small domain shifts; effectiveness on more challenging scenarios remains to be validated
- Computational complexity of the mini-batch OT implementation and its scalability to larger datasets is not thoroughly discussed
- The claim that interpolations in the Wasserstein hull of learned atoms provide generalizable data lacks rigorous validation beyond visual inspection

## Confidence
- High: The mathematical formulation of DaDiL using Wasserstein barycenters and dictionary learning is sound and well-justified
- Medium: The experimental results showing state-of-the-art performance are convincing but limited to specific benchmarks
- Low: The claim that interpolations in the Wasserstein hull of learned atoms provide data that generalizes to the target domain lacks rigorous validation beyond visual inspection

## Next Checks
1. Evaluate DaDiL on more challenging MSDA benchmarks with larger domain shifts to verify robustness
2. Conduct ablation studies to quantify the contribution of each component (OT, dictionary learning, MSDA strategy) to overall performance
3. Analyze the computational complexity and scalability of the method for larger datasets and higher-dimensional feature spaces