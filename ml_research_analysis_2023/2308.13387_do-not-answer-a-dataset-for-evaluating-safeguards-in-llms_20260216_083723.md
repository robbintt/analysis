---
ver: rpa2
title: 'Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs'
arxiv_id: '2308.13387'
source_url: https://arxiv.org/abs/2308.13387
tags:
- gpt-4
- responses
- response
- information
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open-source dataset for evaluating
  safeguards in large language models (LLMs). The dataset contains 939 prompts designed
  to elicit risky responses, covering 12 types of harm across 5 risk areas.
---

# Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs

## Quick Facts
- arXiv ID: 2308.13387
- Source URL: https://arxiv.org/abs/2308.13387
- Reference count: 40
- Key outcome: First open-source dataset for LLM safety evaluation, showing small classifiers can match GPT-4 performance at low cost

## Executive Summary
This paper introduces Do-Not-Answer, the first open-source dataset for evaluating safeguards in large language models. The dataset contains 939 prompts designed to elicit risky responses across 12 harm types in 5 risk areas. Six popular LLMs were evaluated, with LLaMA-2 showing the best safety performance (only 3 harmful responses out of 939). The authors also developed BERT-like classifiers that achieved comparable results to GPT-4 in automatic safety evaluation, with Longformer models achieving over 98% accuracy in harmful response detection. The study demonstrates that small classifiers can effectively assess LLM safety at low cost.

## Method Summary
The authors constructed the Do-Not-Answer dataset by prompting GPT-4 to generate 939 risky questions across 12 harm types. They collected responses from six LLMs (GPT-4, ChatGPT, Claude, ChatGLM2, LLaMA-2, Vicuna) and performed human annotation to label both harmfulness (binary) and action categories (6 types). They then trained BERT-like classifiers (Longformer and BERT) using 6-fold cross-validation to automatically evaluate LLM safety. The evaluation framework compares both harmful response detection and action category classification across different models and methods.

## Key Results
- LLaMA-2 achieved the best safety performance with only 3 harmful responses out of 939 prompts
- Fine-tuned Longformer models with <600M parameters achieved comparable results to GPT-4 in automatic safety evaluation
- Longformer outperformed BERT on action classification due to better handling of longer contexts (2048 vs 512 tokens)
- Instruction-response pairs improved action classification accuracy compared to response-only inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using instruction-response pairs as input improves action classification and harmful response detection accuracy.
- Mechanism: Including the instruction provides context that disambiguates ambiguous responses and helps the model identify the underlying intent, which is critical for determining if the response is harmful.
- Core assumption: The instruction contains relevant information that cannot be inferred from the response alone.
- Evidence anchors: Table 10 shows performance improvement when including instructions, particularly for action classification.

### Mechanism 2
- Claim: Longformer outperforms BERT on action classification due to its ability to process longer contexts.
- Mechanism: Action categories 2 and 5 require understanding the full response context, which Longformer's 2048-token limit enables while BERT's 512-token limit truncates.
- Core assumption: The full response contains critical information for determining categories 2 and 5 that is lost in truncation.
- Evidence anchors: Table 11 shows Longformer's improvement over BERT is primarily in categories 2 and 5.

### Mechanism 3
- Claim: Fine-tuned BERT-like models with less than 600M parameters can achieve comparable results to GPT-4 for safety evaluation.
- Mechanism: Small models can effectively learn patterns from human-annotated data to replicate GPT-4's safety assessment capabilities at lower cost.
- Core assumption: The safety evaluation task can be effectively learned from the available annotated data without requiring GPT-4's full reasoning capabilities.
- Evidence anchors: Table 4 compares Longformer-based evaluator against GPT-4-based evaluator, showing comparable overall results.

## Foundational Learning

- Concept: Binary and multi-class classification
  - Why needed here: The safety evaluation involves both harmful/non-harmful binary classification and six-way action classification
  - Quick check question: How would you modify the loss function if you needed to handle cases where a single response could belong to multiple action categories simultaneously?

- Concept: Cross-validation and model evaluation metrics
  - Why needed here: The study uses 6-fold cross-validation and reports macro-precision, macro-recall, and macro-F1 to handle class imbalance
  - Quick check question: Why might macro-averaged metrics be more appropriate than micro-averaged metrics for this safety evaluation task?

- Concept: Zero-shot vs. few-shot vs. fine-tuning learning paradigms
  - Why needed here: The study compares GPT-4 zero-shot evaluation with fine-tuned PLM classifiers
  - Quick check question: What are the trade-offs between using zero-shot GPT-4 evaluation versus fine-tuning smaller models on the annotated dataset?

## Architecture Onboarding

- Component map: Data collection → Human annotation → PLM classifier training → Evaluation → Analysis
- Critical path: The human annotation step is the bottleneck; all downstream evaluation depends on quality annotations
- Design tradeoffs: Using small models for cost efficiency vs. potential accuracy loss compared to GPT-4; using open-source vs. proprietary data
- Failure signatures: Poor performance on specific harm types (e.g., information hazards); high variance across different LLMs; mismatched harmfulness and action category labels
- First 3 experiments:
  1. Compare Longformer vs BERT performance on a subset of data to verify the long-context hypothesis
  2. Test instruction inclusion vs. response-only inputs on a validation set to confirm the context importance
  3. Evaluate the effect of different annotation guidelines on model performance consistency

## Open Questions the Paper Calls Out
- How does cultural context affect the safety taxonomy and dataset construction?
- What is the optimal balance between safety and helpfulness in LLM responses?
- How do multi-turn interactions affect the safety evaluation of LLMs?

## Limitations
- The dataset size of 939 prompts may not fully capture the complexity and diversity of potential harm types across all use cases
- The evaluation primarily focuses on six popular LLMs, potentially missing variations in safety behaviors across a broader model landscape
- Human annotation introduces potential subjectivity in labeling responses, particularly for nuanced cases where harmfulness and action categories may conflict

## Confidence
- High Confidence: LLaMA-2's superior safety performance (3 harmful responses out of 939) and effectiveness of small PLM classifiers achieving comparable results to GPT-4
- Medium Confidence: Longformer's advantage over BERT due to longer context processing and instruction inclusion improving performance
- Low Confidence: Generalizability of the dataset to other harm types and assumption of consistent human annotation quality

## Next Checks
1. Test the evaluation framework on additional harm types and prompt variations not included in the original 939 prompts to assess generalizability
2. Evaluate the same prompts against updated versions of the tested LLMs to determine if safety behaviors remain consistent over time
3. Conduct inter-annotator agreement studies with additional annotators to quantify consistency and potential subjectivity in human annotation process