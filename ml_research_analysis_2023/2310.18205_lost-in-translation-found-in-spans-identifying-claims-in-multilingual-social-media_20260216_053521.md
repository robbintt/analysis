---
ver: rpa2
title: 'Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social
  Media'
arxiv_id: '2310.18205'
source_url: https://arxiv.org/abs/2310.18205
tags:
- claim
- language
- english
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying claims in multilingual
  social media posts, which is a precursor to fact-checking and combating misinformation.
  The authors propose a novel dataset, X-CLAIM, containing 7K real-world claims from
  social media in six languages, and develop an automated annotation pipeline for
  the task.
---

# Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media

## Quick Facts
- **arXiv ID**: 2310.18205
- **Source URL**: https://arxiv.org/abs/2310.18205
- **Reference count**: 33
- **Primary result**: Multilingual models outperform zero-shot transfer and translate-train approaches for claim span identification across six languages

## Executive Summary
This paper addresses the critical task of identifying check-worthy claims in multilingual social media posts, a necessary precursor to automated fact-checking and misinformation detection. The authors introduce X-CLAIM, a novel dataset containing 7,000 real-world claims across six languages (English, Hindi, Punjabi, Tamil, Telugu, Bengali). They develop an automated annotation pipeline using BERTScore-Recall and word alignment to extract claim spans from fact-checking websites. Through comprehensive experiments, they demonstrate that multilingual training significantly outperforms alternative cross-lingual transfer methods, and that encoder-only models are particularly effective for low-resource languages. The best multilingual model achieves 82.92 F1 score, surpassing both zero-shot transfer by 2.13 F1 and GPT-4 by 2.07 F1.

## Method Summary
The methodology involves collecting social media posts from fact-checking websites, then using an automated pipeline to identify claim spans. BERTScore-Recall selects the most relevant sentence containing the claim, while word alignment maps normalized claims to post text. The task is formulated as sequence tagging with IO encoding. Encoder-only models (mBERT, mDeBERTa, XLM-R) are fine-tuned on this data, while generative LLMs (GPT-3.5, GPT-4) are evaluated via in-context learning. Models are trained and evaluated across all six languages with span-level precision, recall, and F1-score metrics.

## Key Results
- Multilingual training improves performance by 2.13 F1 compared to zero-shot transfer
- Encoder-only models outperform GPT-4 by 2.07 F1 on low-resource languages
- Best multilingual model achieves 82.92 F1 score on the X-CLAIM dataset
- Multilingual training shows consistent improvements across all six languages tested

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multilingual training improves claim span identification performance compared to zero-shot transfer or training on translated data.
- **Mechanism**: The model learns shared linguistic patterns and semantic representations across languages, enabling better generalization and transfer of knowledge.
- **Core assumption**: The underlying structure of claims is similar across languages, allowing the model to leverage cross-lingual information.
- **Evidence anchors**:
  - [abstract] "We demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English."
  - [section] "We find that joint training across languages improves the model performance when compared to alternative cross-lingual transfer methods like zero-shot transfer, or training on translated data, from a high-resource language like English."

### Mechanism 2
- **Claim**: Encoder-only models outperform generative LLMs on low-resource languages for claim span identification.
- **Mechanism**: Encoder-only models are better at capturing local context and dependencies within a sentence, which is crucial for identifying claim spans.
- **Core assumption**: Claim spans are typically short phrases within a sentence, and encoder-only models are optimized for such tasks.
- **Evidence anchors**:
  - [abstract] "Encoder-only models outperform LLMs on low-resource languages."
  - [section] "We find that the LLMs show strong performance on high-resource languages like English, but still lag behind smaller fine-tuned LMs on low-resource languages such as Bengali."

### Mechanism 3
- **Claim**: The automated annotation pipeline using BERTScore-Recall and word alignment produces high-quality claim spans.
- **Mechanism**: BERTScore-Recall effectively identifies the most relevant sentence containing the claim, and word alignment accurately maps the normalized claim to the post text.
- **Core assumption**: The normalized claims provided by fact-checkers are accurate and align well with the original post text.
- **Evidence anchors**:
  - [section] "We use various filtering rules to remove posts that are about videos, Instagram reels, or when their text is too short or excessively long."
  - [section] "We empirically chose the most appropriate sentence similarity measure for sentence selection, after trying a variety of similarity measures."

## Foundational Learning

- **Concept: Multilingual models**
  - **Why needed here**: The task involves identifying claim spans in social media posts across multiple languages, requiring a model that can handle diverse linguistic patterns.
  - **Quick check question**: What are the key differences between monolingual and multilingual models, and how do they impact performance on cross-lingual tasks?

- **Concept: Sequence tagging**
  - **Why needed here**: Claim span identification is formulated as a sequence tagging problem, where each token in the post text is assigned a label indicating whether it is part of a claim.
  - **Quick check question**: What are the different encoding schemes used in sequence tagging (e.g., IO, BIO), and how do they affect the model's ability to identify claim spans?

- **Concept: Cross-lingual transfer**
  - **Why needed here**: The model needs to transfer knowledge from high-resource languages (like English) to low-resource languages, enabling effective claim span identification across diverse linguistic contexts.
  - **Quick check question**: What are the different approaches to cross-lingual transfer (e.g., zero-shot transfer, translate-train), and how do they compare in terms of performance and resource requirements?

## Architecture Onboarding

- **Component map**: Data collection -> Automated annotation (BERTScore-Recall + word alignment) -> Model training (mBERT/mDeBERTa/XLM-R) -> Evaluation (sequence tagging with IO encoding)
- **Critical path**: Collect and preprocess data from fact-checking websites → Annotate claim spans using automated pipeline → Train and evaluate multilingual models on annotated data → Compare performance across languages and model architectures
- **Design tradeoffs**: Multilingual training vs. language-specific training (shared patterns vs. specialization); Encoder-only vs. generative models (local context vs. longer claims); Automated annotation vs. manual annotation (speed vs. accuracy)
- **Failure signatures**: Low F1 scores on low-resource languages (insufficient data or poor transfer); High variance across fine-tuning runs (training instability); Incorrect claim span boundaries (annotation errors)
- **First 3 experiments**:
  1. Compare performance of multilingual models (mBERT, mDeBERTa, XLM-R) on X-CLAIM dataset across different languages
  2. Evaluate impact of multilingual training vs. zero-shot transfer and translate-train approaches
  3. Analyze performance of encoder-only models vs. generative LLMs (GPT-3.5, GPT-4) on low-resource languages

## Open Questions the Paper Calls Out
- How can we develop a methodology to automatically identify and extract multiple check-worthy claims from a single social media post, rather than just the primary claim?
- How can we improve the evaluation metrics for the multilingual claim span identification task to better capture the nuances of claim identification across different languages and linguistic structures?
- How can we expand the claim span identification task to other low-resource languages beyond the six languages covered in the X-CLAIM dataset?

## Limitations
- The automated annotation pipeline may introduce noise in low-resource languages where word alignment quality is harder to control
- Evaluation of generative LLMs relies on in-context learning without fine-tuning, introducing variability based on prompt design
- Dataset covers only six languages, limiting generalizability to other linguistic contexts

## Confidence
- **High confidence**: Multilingual training advantage over zero-shot transfer is well-supported by controlled experiments across all six languages
- **High confidence**: Encoder-only model superiority on low-resource languages is consistently demonstrated with statistical significance
- **Medium confidence**: BERTScore-Recall based annotation pipeline's quality is inferred from filtering rules rather than systematic error analysis
- **Medium confidence**: GPT-4 performance comparison is less reliable due to unknown prompt variations

## Next Checks
1. Conduct ablation studies on prompt structure for GPT-4 to establish baseline performance variations in in-context learning for claim identification tasks
2. Implement manual annotation validation on a subset of low-resource language samples to quantify annotation pipeline error rates
3. Test model generalization on an out-of-domain dataset (different social media platforms or fact-checking sources) to evaluate robustness beyond the X-CLAIM dataset distribution