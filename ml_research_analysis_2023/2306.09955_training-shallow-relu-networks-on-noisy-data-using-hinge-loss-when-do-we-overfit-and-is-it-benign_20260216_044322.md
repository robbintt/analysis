---
ver: rpa2
title: 'Training shallow ReLU networks on noisy data using hinge loss: when do we
  overfit and is it benign?'
arxiv_id: '2306.09955'
source_url: https://arxiv.org/abs/2306.09955
tags:
- lemma
- then
- training
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies benign overfitting in two-layer ReLU networks\
  \ trained with gradient descent and hinge loss on noisy binary classification data.\
  \ The data model assumes linearly separable points with a small fraction of corrupted\
  \ labels, where the clean points have margin determined by a parameter \u03B3."
---

# Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?

## Quick Facts
- arXiv ID: 2306.09955
- Source URL: https://arxiv.org/abs/2306.09955
- Reference count: 40
- Primary result: Characterizes three training regimes for ReLU networks on noisy data: benign overfitting (optimal generalization despite fitting noise), non-benign overfitting (poor generalization), and no-overfitting (optimal generalization without fitting noise).

## Executive Summary
This paper studies benign overfitting in two-layer ReLU networks trained with gradient descent and hinge loss on noisy binary classification data. The data model assumes linearly separable points with a small fraction of corrupted labels, where the clean points have margin determined by parameter γ. The authors prove that for intermediate γ (proportional to 1/n), the network achieves zero training loss and optimal generalization error (benign overfitting). For very small γ (proportional to 1/√nd), zero training loss is achieved but generalization is poor (non-benign overfitting). For large γ (greater than 1/n), the network zeros corrupt points and achieves optimal generalization without overfitting them. The analysis uses a novel combinatorial approach tracking clean vs corrupt updates, revealing two training phases: clean points achieve low loss first, then corrupt points are either fitted or zeroed.

## Method Summary
The method involves training two-layer ReLU networks on synthetic data with label noise using full-batch gradient descent with hinge loss. The data model consists of n clean points and k corrupt points (k ≤ cn) in d dimensions, where clean points have margin γ and noise vectors are nearly orthogonal. The network has 2m neurons with weights initialized uniformly on a sphere. Training proceeds with step size η tuned per regime, tracking the number of clean (G(t)) and corrupt (B(t)) points that achieve zero loss over iterations. The analysis characterizes when the network achieves benign overfitting (zero training loss, optimal generalization), non-benign overfitting (zero training loss, poor generalization), or no-overfitting (zero training loss on clean points only, optimal generalization).

## Key Results
- For γ ≈ c/n, network achieves benign overfitting: zero training loss and optimal generalization error
- For γ ≤ c/√nd, network achieves non-benign overfitting: zero training loss but constant generalization error
- For γ ≥ c/k, network achieves no-overfitting: zero loss on clean points, nonzero loss on corrupt points, and optimal generalization
- Analysis reveals two training phases: clean points dominate early updates, then corrupt points are either fitted or zeroed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For γ ≈ c/n, network achieves benign overfitting with zero training loss and optimal generalization.
- Mechanism: When γ is proportional to 1/n, clean points dominate early training updates, pushing the network toward the optimal classifier. Corrupt points are eventually fitted to noise, but this doesn't hurt generalization because noise vectors are nearly orthogonal, preventing interference with the signal direction.
- Core assumption: Noise components are nearly orthogonal (max |⟨ni, nℓ⟩| ≤ ρ/(1−γ) with ρ small) and initialization ensures large fraction of neurons activated by clean points.
- Break condition: If γ << 1/n (e.g., γ ≤ c/√nd), noise dominates and network overfits non-benignly.

### Mechanism 2
- Claim: For γ ≤ c/√nd, network achieves non-benign overfitting with poor generalization.
- Mechanism: When γ is too small, noise components dominate training dynamics. The network fits all points based on their noise rather than the signal, learning a suboptimal classifier even though a perfect classifier exists.
- Core assumption: Data model allows perfect classifier (sign(⟨v, x⟩)) for any γ > 0, but network dynamics fail to find it when γ is too small.
- Break condition: If γ is increased beyond c/√nd, network transitions to either benign overfitting or no-overfitting.

### Mechanism 3
- Claim: For γ ≥ c/k, network achieves no-overfitting by zeroing corrupt points.
- Mechanism: Large γ ensures clean points have strong signal components that dominate pre-activations. Network quickly fits clean points to zero loss while corrupt points (with weaker signal relative to noise) are never fitted and are "zeroed out" by the network.
- Core assumption: Initialization ensures all neurons activated by clean points, and γ large enough that corrupt points never achieve zero loss.
- Break condition: If γ is below c/k, corrupt points may be fitted, leading to benign or non-benign overfitting.

## Foundational Learning

- **Gradient descent dynamics with hinge loss on ReLU networks**
  - Why needed: Analysis tracks how clean vs corrupt updates affect neuron activations over time. Unlike logistic loss, hinge loss has unbounded loss ratios, requiring combinatorial counting of updates.
  - Quick check: In hinge loss, what happens to a point's contribution to the gradient once its loss reaches zero? (Answer: It stops contributing.)

- **Near-orthogonal noise model and its implications**
  - Why needed: Near-orthogonality of noise vectors (max |⟨ni, nℓ⟩| ≤ ρ) is crucial for convergence analysis and generalization bounds. It ensures corrupt points' noise doesn't interfere destructively with clean points' signal.
  - Quick check: Why does near-orthogonality of noise vectors help with generalization? (Answer: It prevents corrupt points' noise from aligning with the signal direction, so fitting them to noise doesn't hurt the classifier.)

- **Combinatorial counting of clean vs corrupt updates**
  - Why needed: Proof technique reduces convergence analysis to counting clean vs corrupt point activations. Necessary because hinge loss doesn't allow ratio-based arguments used for logistic loss.
  - Quick check: What is the key difference in proof approach between hinge loss and logistic loss? (Answer: Hinge loss requires combinatorial counting of updates, while logistic loss uses bounded loss ratios.)

## Architecture Onboarding

- **Component map**: Data model (noisy linearly separable) -> Two-layer ReLU network (2m neurons) -> Full-batch gradient descent with hinge loss -> Training phases (clean points first, then corrupt points) -> Three regimes (benign, non-benign, no-overfitting)

- **Critical path**:
  1. Initialize network with weights satisfying Γp conditions (large fraction of neurons activated by clean points)
  2. Early phase: Clean points dominate updates, pushing network toward signal direction
  3. Mid phase: Clean points achieve near-zero loss; corrupt points either fitted or zeroed
  4. Late phase: Network converges; clean points at zero loss, corrupt points either at zero loss (benign) or nonzero (no-overfitting)

- **Design tradeoffs**:
  - Width m: Larger m increases chance of satisfying initialization conditions but adds computational cost
  - Signal strength γ: Controls regime (benign vs non-benign vs no-overfitting); must be tuned to noise level and corruption fraction
  - Step size η: Must be small enough to ensure convergence but large enough to make progress; regime-dependent

- **Failure signatures**:
  - Non-convergence: If noise vectors not nearly orthogonal (ρ too large), updates may not terminate
  - Poor generalization: If γ is too small (≤ c/√nd), network overfits corrupt points
  - Overly sparse fitting: If γ is too large (≥ c/k), network may zero too many points, missing some clean points

- **First 3 experiments**:
  1. Verify initialization conditions: Check that |Γp| ≥ (1−α)m and each corrupt point activates a neuron in Θyi at t=0
  2. Track update counts: Plot G(t) and B(t) over iterations to confirm clean points dominate early, then corrupt points are fitted/zeroed
  3. Test generalization: Generate test points and measure misclassification rate; verify it matches theoretical bound (exp(-cdγ²) for benign, constant for non-benign)

## Open Questions the Paper Calls Out
- Does the combinatorial approach generalize to data models beyond linearly separable ones with label noise?
- How does network depth impact conditions for benign overfitting in presence of label noise?
- Can the near-orthogonal noise assumption be relaxed while maintaining key insights about benign overfitting?

## Limitations
- Analysis relies heavily on near-orthogonal noise assumption requiring very large ambient dimension d
- Combinatorial counting approach provides less intuitive insight than ratio-based arguments
- Results specific to two-layer ReLU networks may not extend directly to deeper architectures

## Confidence
- High confidence: Three-regime characterization and general mechanisms governing each regime
- Medium confidence: Specific mathematical bounds and constants, particularly exact thresholds for γ
- Low confidence: Practical applicability of near-orthogonality assumption in real-world datasets

## Next Checks
1. Test algorithm's performance on datasets with varying d to identify when near-orthogonality assumption breaks down
2. Introduce controlled correlations in noise vectors and measure impact on generalization error and training dynamics
3. Systematically vary γ across theoretical thresholds (c/n, c/√nd, ck⁻¹) and document observed training and generalization behavior