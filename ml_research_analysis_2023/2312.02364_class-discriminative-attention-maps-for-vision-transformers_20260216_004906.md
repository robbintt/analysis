---
ver: rpa2
title: Class-Discriminative Attention Maps for Vision Transformers
arxiv_id: '2312.02364'
source_url: https://arxiv.org/abs/2312.02364
tags:
- cdam
- attention
- relevance
- maps
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Class-Discriminative Attention Maps (CDAM),
  a gradient-based extension of attention maps (AM) for Vision Transformers (ViT)
  that provides class-discriminative explanations. CDAM computes gradients of a class
  or concept similarity score with respect to token activations in the final transformer
  block, effectively scaling the attention scores by how relevant the corresponding
  tokens are for the target.
---

# Class-Discriminative Attention Maps for Vision Transformers

## Quick Facts
- arXiv ID: 2312.02364
- Source URL: https://arxiv.org/abs/2312.02364
- Authors: 
- Reference count: 40
- Key outcome: CDAM provides highly class-discriminative, semantically relevant, and compact explanations for Vision Transformers, outperforming 7 other importance estimators across correctness, compactness, and class sensitivity benchmarks.

## Executive Summary
This paper introduces Class-Discriminative Attention Maps (CDAM), a gradient-based extension of attention maps for Vision Transformers that provides class-specific explanations. CDAM scales attention scores by how relevant corresponding tokens are for a target class or concept, creating more discriminative and compact heatmaps than traditional attention maps. The method excels across multiple interpretability benchmarks and introduces variants like Smooth CDAM and Integrated CDAM that average multiple CDAMs with altered tokens.

## Method Summary
CDAM computes gradients of class or concept similarity scores with respect to token activations in the final transformer block, then scales attention scores by these gradient-derived relevance scores. The method requires a pre-trained Vision Transformer backbone (tested on ViT-S/8) and a linear classifier trained on top. The core computation involves calculating the directional derivative of the class score with respect to token value vectors and multiplying this by the attention scores. The authors also introduce Smooth CDAM and Integrated CDAM variants that average multiple CDAMs with slightly altered tokens for improved stability.

## Key Results
- CDAM achieves superior performance across correctness, compactness, and class sensitivity benchmarks compared to 7 other importance estimators
- The method produces highly class-discriminative and semantically relevant heatmaps while maintaining compactness
- CDAM effectively scales attention scores by token relevance, resulting in sparser but more meaningful explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CDAM scales attention scores by the directional derivative of the class/concept score in the direction of the token's value vector
- Mechanism: The relevance score is computed as the dot product of attention Ai and the directional derivative of the class/concept score with respect to the token's value vector
- Core assumption: The gradient of the class/concept score with respect to the token's value vector captures the token's relevance for the target class/concept
- Evidence anchors:
  - [abstract] "CDAM scales attention scores by how relevant the corresponding tokens are for the predictions of a classifier head."
  - [section 5.1] "Empirically, Si,c is obtained by multiplying Ai with the directional derivative of h Â· lc in the direction of Vi."

### Mechanism 2
- Claim: CDAM inherits the high-quality semantic segmentation of input images from attention maps (AM) while adding class-discrimination
- Mechanism: CDAM zeros out the relevance scores for tokens with zero attention, ensuring that only the tokens contributing to the semantic segmentation are considered for class-discrimination
- Core assumption: The tokens with non-zero attention in AM are semantically meaningful and contribute to the model's decision-making process
- Evidence anchors:
  - [abstract] "CDAM is highly class-discriminative and semantically relevant, while providing compact explanations."
  - [section 4.1] "CDAM therefore appears to compute Si,c by multiplying the attention with the relevance of the corresponding token for the targeted class."

### Mechanism 3
- Claim: CDAM provides a balance between sparsity and discriminativeness, making it a useful tool for understanding the model's decision-making process
- Mechanism: CDAM's use of gradients and the scaling of attention scores by the token's relevance leads to sparse heatmaps that highlight the most important tokens for the target class/concept
- Core assumption: Sparser heatmaps with fewer, more relevant tokens are more interpretable and provide better insights into the model's decision-making process
- Evidence anchors:
  - [abstract] "CDAM is shown to be highly class-discriminative and semantically relevant, while providing compact explanations."
  - [section 5] "Compared to AM, TAM, and RP, CDAM appears to be less noisy... regularization has long been pursued and motivated by interpretability in machine learning."

## Foundational Learning

- Concept: Gradient-based explanation methods
  - Why needed here: CDAM relies on gradients to compute the relevance scores for tokens
  - Quick check question: How do gradient-based explanation methods like Grad-CAM and Integrated Gradients differ from perturbation-based methods like LIME and SHAP?

- Concept: Attention mechanisms in transformers
  - Why needed here: CDAM extends the attention maps (AM) inherent to Vision Transformers (ViT) by scaling them with the token's relevance for the target class/concept
  - Quick check question: What is the difference between self-attention and cross-attention in transformer models, and how do they contribute to the model's decision-making process?

- Concept: Concept embeddings and similarity measures
  - Why needed here: CDAM can explain a user-defined concept by measuring the similarity between the concept embedding and the latent representation of a sample image
  - Quick check question: How are concept embeddings obtained from a set of example images, and what similarity measures can be used to compare the concept embedding with the latent representation of a sample image?

## Architecture Onboarding

- Component map: Vision Transformer backbone -> Linear classifier or similarity measure -> Gradient computation -> Attention score scaling -> CDAM heatmap visualization

- Critical path:
  1. Forward pass through the ViT backbone to obtain the latent representation of the input image
  2. Compute the class score using the linear classifier or the similarity measure for the target concept
  3. Calculate the gradients of the class/concept score with respect to the token activations in the final transformer block
  4. Scale the attention scores by the token's relevance for the target class/concept using the computed gradients
  5. Visualize the resulting CDAM heatmaps to highlight the most relevant tokens for the target class/concept

- Design tradeoffs:
  - Balancing sparsity and discriminativeness in the CDAM heatmaps
  - Choosing between class-based and concept-based explanations depending on the use case
  - Deciding on the appropriate similarity measure for concept-based explanations

- Failure signatures:
  - Heatmaps that do not highlight the expected relevant regions for the target class/concept
  - Overly sparse or overly dense heatmaps that do not provide meaningful insights into the model's decision-making process
  - Inconsistencies between the CDAM heatmaps and other explanation methods like TAM or RP

- First 3 experiments:
  1. Apply CDAM to a ViT model trained on ImageNet and visualize the heatmaps for different target classes. Compare the results with other explanation methods like TAM and RP.
  2. Use CDAM to explain a user-defined concept by selecting a set of example images and computing the concept embedding. Visualize the heatmaps for different sample images and assess the class-discrimination and semantic relevance.
  3. Investigate the effect of different similarity measures (e.g., L2 distance, cosine similarity) on the concept-based CDAM heatmaps. Compare the results and discuss the implications for interpretability and understanding the model's decision-making process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CDAM change when applied to other transformer architectures beyond ViT-S/8, such as larger ViT models or other types of transformers like BERT or GPT?
- Basis in paper: [inferred] The paper only tests CDAM on ViT-S/8 and mentions it could be applicable to other transformers but doesn't provide empirical evidence
- Why unresolved: The paper focuses on demonstrating CDAM's effectiveness on a specific ViT architecture and doesn't explore its performance across different transformer models
- What evidence would resolve it: Empirical results comparing CDAM's performance across various transformer architectures would provide insights into its generalizability and effectiveness

### Open Question 2
- Question: Can CDAM be extended to handle multi-label classification tasks, where an input can belong to multiple classes simultaneously?
- Basis in paper: [inferred] The paper discusses CDAM's application to single-class classification and concept-based explanations, but doesn't address multi-label scenarios
- Why unresolved: The current formulation of CDAM seems tailored for single-class explanations and would need modifications to handle the complexity of multi-label predictions
- What evidence would resolve it: Experiments applying CDAM to multi-label datasets and comparing its performance to existing methods would demonstrate its effectiveness in this setting

### Open Question 3
- Question: How does the choice of concept embedding (e.g., averaging vs. other aggregation methods) affect the quality and interpretability of CDAM explanations?
- Basis in paper: [explicit] The paper mentions using averaging to create concept embeddings but doesn't explore alternative methods or their impact on CDAM
- Why unresolved: The paper doesn't investigate how different ways of defining concept embeddings might influence the resulting CDAM explanations
- What evidence would resolve it: Comparing CDAM explanations using various concept embedding methods (e.g., weighted averaging, clustering) would reveal the sensitivity of CDAM to this choice and potentially improve its performance

## Limitations
- CDAM's effectiveness heavily relies on the quality of attention maps from the underlying Vision Transformer, which may vary across architectures
- The method's reliance on gradients makes it susceptible to gradient saturation and vanishing gradient problems in deeper networks
- The paper does not extensively explore CDAM's behavior on out-of-distribution samples or adversarial examples

## Confidence
- High Confidence: CDAM successfully scales attention scores by token relevance for the target class; produces sparser, more discriminative heatmaps; directional derivative approach is technically sound
- Medium Confidence: CDAM's semantic relevance matches or exceeds traditional attention maps; compactness provides better interpretability; generalizes well across different ViT architectures
- Low Confidence: Performance superiority over all compared methods across all scenarios; specific impact of Smooth CDAM and Integrated CDAM variants; long-term stability across model updates

## Next Checks
1. **Ablation Study on Token Alteration Methods**: Implement and test multiple approaches for token alteration in Smooth CDAM and Integrated CDAM (e.g., Gaussian noise addition, token masking, feature space perturbations) to determine which produces the most stable and interpretable results.

2. **Cross-Architecture Generalization Test**: Apply CDAM to ViT models trained with different objectives (supervised, self-supervised like DINO, contrastive learning) and architectures (DeiT, Swin, ConvNeXt) to assess consistency of explanations and identify architecture-specific limitations.

3. **Adversarial Robustness Analysis**: Generate adversarial examples using standard attacks (FGSM, PGD) and apply CDAM to examine how explanations change under adversarial perturbations, comparing with other importance estimators to evaluate CDAM's reliability in security-critical applications.