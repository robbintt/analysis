---
ver: rpa2
title: 'Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive
  Human-Agent Interactions'
arxiv_id: '2310.14404'
source_url: https://arxiv.org/abs/2310.14404
tags:
- selfish
- human
- agent
- fair
- negotiation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the impact of agent personality on negotiation
  performance in mixed-motive human-agent interactions. The authors find that standard
  self-play reinforcement learning (RL) leads to agents that fail to learn compromise,
  resulting in many walkaways and poor overall performance.
---

# Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions

## Quick Facts
- arXiv ID: 2310.14404
- Source URL: https://arxiv.org/abs/2310.14404
- Reference count: 19
- Key outcome: Selfish agents trained with uncompromising partners achieve superior negotiation performance by implicitly learning to generate value for both themselves and their partners.

## Executive Summary
This paper investigates how agent personality affects negotiation performance in mixed-motive human-agent interactions. The authors find that standard self-play reinforcement learning leads to agents that fail to learn compromise, resulting in many walkaways and poor overall performance. They propose two novel approaches: varying the RL reward function to explicitly incorporate partner performance, and varying the personality of the training partner to implicitly teach the value of concessions. Through human evaluation, they discover that a selfish agent trained with an uncompromising partner achieves superior performance by implicitly learning to generate value for both itself and the human partner, leading to higher joint points and fewer walkaways compared to other agent variants.

## Method Summary
The paper uses self-play reinforcement learning with supervised initialization on the DealOrNoDeal dataset (5808 dialogues over 2236 unique scenarios). They create six agent variants through a 2x3 study design, varying reward function (selfish vs fair) and training partner (supervised vs selfish vs fair). The agents are then evaluated through human experiments on Prolific with 100 conversations per variant, measuring objective metrics (points scored, joint points, walkaway percentage) and subjective satisfaction ratings.

## Key Results
- Standard self-play RL agents fail to learn compromise, resulting in many walkaways and poor overall performance
- A selfish agent trained with an uncompromising partner achieves superior performance by implicitly learning to generate value for both itself and the partner
- This approach leads to higher joint points and fewer walkaways compared to other agent variants
- Incorporating the mixed-motive nature of negotiations is crucial for developing successful negotiation dialogue systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying the RL reward function to explicitly incorporate partner performance changes the agent's negotiation strategy from purely self-interested to more cooperative.
- Mechanism: The reward function is modified to include a utility measure from negotiation theory (Fehr and Schmidt, 1999) that penalizes the agent for outperforming or underperforming the partner, encouraging more balanced outcomes.
- Core assumption: Agents will optimize for the modified reward function, leading to behavior that considers partner performance.
- Evidence anchors:
  - [abstract] "We vary the RL reward directly so that the model is forced to take the partner's interests into account."
  - [section] "The key idea here is to incorporate the partner's performance into the reward function used for training the RL agent."
  - [corpus] Weak evidence - no direct citations, but related work on personality-driven negotiation systems suggests this approach is plausible.

### Mechanism 2
- Claim: Varying the personality of the training partner implicitly teaches the agent the value of concessions and avoiding walkaways.
- Mechanism: By training with an uncompromising partner, the agent experiences more walkaways (no agreements) during training, which implicitly teaches it to make concessions to avoid these situations.
- Core assumption: Agents will learn from the consequences of walkaways during training and adjust their strategy accordingly.
- Evidence anchors:
  - [abstract] "We vary the personality of the training partner...which allowed the RL agent to discover the mixed-motive nature of the task implicitly."
  - [section] "Since the supervised model tends to show socialistic behaviors...the RL agent fails to explore scenarios that do not lead to an agreement and, hence, cannot capture the notion of walkaways in the learned policy."
  - [corpus] Weak evidence - no direct citations, but related work on opponent modeling in negotiation suggests this approach is plausible.

### Mechanism 3
- Claim: A selfish agent trained with an uncompromising partner achieves superior performance by implicitly learning to generate value for both itself and the partner.
- Mechanism: The selfish agent learns to make concessions to avoid walkaways, which leads to higher joint points and fewer walkaways compared to other agent variants.
- Core assumption: The agent will balance its own interests with the need to avoid walkaways, leading to more mutually beneficial outcomes.
- Evidence anchors:
  - [abstract] "We find that although both techniques show promise, a selfish agent...achieves superior performance to other variants by learning to generate value for both itself and the negotiation partner."
  - [section] "Our key finding is that a selfish agent...achieves superior performance to other variants by implicitly learning to generate value for both itself and the negotiation partner."
  - [corpus] Moderate evidence - the paper cites related work on negotiation dialogue systems and personality-driven negotiation, but no direct citations for this specific mechanism.

## Foundational Learning

- Concept: Reinforcement Learning (RL) with self-play
  - Why needed here: The paper uses RL with self-play to train negotiation dialogue agents that learn to maximize their performance by interacting with a simulated user.
  - Quick check question: How does the agent update its policy based on the reward it receives during the interaction with the simulated user?

- Concept: Utility functions in negotiation theory
  - Why needed here: The paper uses a utility function from negotiation theory (Fehr and Schmidt, 1999) to modify the RL reward function and encourage more balanced outcomes.
  - Quick check question: How does the utility function penalize the agent for outperforming or underperforming the partner?

- Concept: Mixed-motive interactions
  - Why needed here: The paper focuses on negotiation as a mixed-motive interaction, where agents must balance their own interests with the interests of their partner to reach an agreement.
  - Quick check question: What are some examples of mixed-motive interactions in real-world negotiations, and how do they differ from purely self-interested interactions?

## Architecture Onboarding

- Component map:
  - Supervised model (S) -> RL agent (SRL) -> Simulated user (SUS) -> Reward function -> Partner personality

- Critical path:
  1. Train supervised model S on human-human dialogue data
  2. Create two copies of S: SRL (for RL agent) and SUS (for simulated user)
  3. Fine-tune SRL using RL to maximize performance against SUS
  4. Evaluate trained agents against human partners

- Design tradeoffs:
  - Varying reward function vs. varying partner personality: The former explicitly pushes the agent to consider partner performance, while the latter allows the agent to implicitly learn the value of concessions.
  - Selfish vs. fair agents: Selfish agents may achieve higher performance for themselves but may also lead to more walkaways, while fair agents may achieve more balanced outcomes but may not maximize their own performance.

- Failure signatures:
  - Agent gets stuck in negotiation (no agreement reached within length cutoff)
  - Agent consistently accepts unfavorable deals to avoid walkaways
  - Agent consistently rejects reasonable offers from human partner

- First 3 experiments:
  1. Train a selfish agent (SRL) against a supervised partner (SUS) to replicate the baseline from Lewis et al. (2017)
  2. Train a fair agent (SRL) against a supervised partner (SUS) to test the effect of varying the reward function
  3. Train a selfish agent (SRL) against an uncompromising partner (SUS) to test the effect of varying the partner personality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different agent personalities affect long-term negotiation outcomes in repeated interactions?
- Basis in paper: [explicit] The paper discusses subjective assessment and mentions that subjective measures are more relevant for repeated interactions, but does not provide data on long-term outcomes.
- Why unresolved: The paper only evaluates single-shot negotiations, leaving the impact of personality on repeated interactions unexplored.
- What evidence would resolve it: Empirical studies comparing long-term negotiation outcomes between different agent personalities in repeated interaction scenarios.

### Open Question 2
- Question: How do cultural differences impact the effectiveness of various agent personalities in negotiations?
- Basis in paper: [inferred] The paper notes that its human evaluation was restricted to US workers and acknowledges that prior research has noted differences in negotiation behaviors across cultures.
- Why unresolved: The study's demographic limitations prevent conclusions about cultural generalizability of the findings.
- What evidence would resolve it: Comparative studies across different cultural groups to assess the impact of agent personality on negotiation outcomes.

### Open Question 3
- Question: What are the optimal partner personality strategies for training self-interested agents to maximize their own performance?
- Basis in paper: [explicit] The paper discusses training self-interested agents with different partner personalities but does not determine the optimal strategy for maximizing agent performance.
- Why unresolved: The paper finds that training with a self-interested partner leads to better implicit concession learning but does not explore all possible partner personality combinations or their long-term effects.
- What evidence would resolve it: Systematic comparison of self-interested agent performance across all possible partner personality combinations and evaluation of long-term negotiation success.

## Limitations
- The exact mechanism by which varying partner personality leads to better negotiation outcomes remains somewhat unclear
- The proposed approaches rely on implicit learning through walkaways, which may not generalize well to different negotiation contexts or partner types
- The human evaluation setup may not fully capture the complexity of real-world negotiations

## Confidence
- High Confidence: The finding that self-play RL agents fail to learn compromise and result in many walkaways is well-supported by the experimental results.
- Medium Confidence: The proposed approaches of varying reward function and partner personality show promise, but their effectiveness may depend on specific implementation details and hyperparameters.
- Low Confidence: The claim that a selfish agent trained with an uncompromising partner achieves superior performance by implicitly learning to generate value for both itself and the partner is intriguing but requires further validation in different negotiation scenarios.

## Next Checks
1. **Generalization Test**: Evaluate the trained agents on a held-out set of negotiation scenarios not seen during training to assess their ability to generalize to new situations.
2. **Partner Variation Analysis**: Systematically vary the partner's personality and negotiation strategy to understand how the agent's performance changes and identify the limits of its adaptability.
3. **Real-World Deployment Study**: Conduct a user study with real-world negotiation tasks and partners to validate the effectiveness of the proposed approaches in more naturalistic settings.