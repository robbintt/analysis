---
ver: rpa2
title: Summarizing Multiple Documents with Conversational Structure for Meta-Review
  Generation
arxiv_id: '2305.01498'
source_url: https://arxiv.org/abs/2305.01498
tags:
- source
- documents
- rammer
- meta-review
- reviewers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PeerSum, a novel multi-document summarization
  dataset for meta-review generation in the peer-review domain. The dataset consists
  of human-generated meta-reviews and their corresponding source documents, including
  paper abstracts, official reviews, public reviews, and multi-turn discussions.
---

# Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation

## Quick Facts
- **arXiv ID:** 2305.01498
- **Source URL:** https://arxiv.org/abs/2305.01498
- **Reference count:** 7
- **Key outcome:** Introduces PeerSum dataset and Rammer model for meta-review generation, achieving state-of-the-art performance on ROUGE, BERTScore, and BARTScore metrics

## Executive Summary
This paper addresses the challenge of multi-document summarization in the peer-review domain by introducing PeerSum, a novel dataset containing meta-reviews and their corresponding source documents including paper abstracts, official reviews, public reviews, and multi-turn discussions. The dataset is distinguished by its highly abstractive summaries, rich inter-document relationships with explicit hierarchical structure, and metadata such as review ratings and paper acceptance. To leverage these structural relationships, the authors propose Rammer, a model that uses relationship-aware sparse attention based on conversational structure and multi-task learning to predict metadata features. Experimental results demonstrate that Rammer outperforms strong baseline models across multiple evaluation metrics, though further analyses reveal limitations in handling conflicts in source documents.

## Method Summary
The method introduces Rammer, a meta-review generation model that incorporates relationship-aware sparse attention and multi-task learning. The model processes source documents concatenated with delimiter tokens through a BART-based encoder with attention manipulation based on hierarchical relationships extracted from the peer-review thread structure. The sparse attention mechanism uses weighted combinations of relationship matrices (ancestor-1, ancestor-n, descendant-1, descendant-n, siblings, document-self, same-thread) to mask irrelevant document connections and scale attention weights. Multi-task learning objectives predict document types, review ratings, reviewer confidences, and paper acceptance alongside the main summarization task. The model is trained on the PeerSum dataset and evaluated using ROUGE, BERTScore, BARTScore, and a reference-free metric based on paper acceptance prediction accuracy.

## Key Results
- Rammer achieves state-of-the-art performance on PeerSum dataset across ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and BARTScore metrics
- The model demonstrates strong performance in both fully-supervised and zero-shot settings
- Further analysis reveals that Rammer and other models struggle to handle conflicts in source documents, with performance degradation when contradictory information is present

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relationship-aware sparse attention enables the model to focus on relevant inter-document relationships without introducing extra parameters
- Mechanism: The model uses weighted combinations of relationship matrices (R1-R7) to mask out irrelevant document connections and scale attention weights based on hierarchical structure
- Core assumption: The explicit hierarchical relationships extracted from the peer-review thread structure capture the essential inter-document dependencies needed for summarization
- Evidence anchors:
  - [abstract]: "we introduce Rammer ( Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the conversational structure"
  - [section 4.1]: "We propose an encoder with relationship-aware sparse attention (RSAttn), which improves the summarization performance with the introduction of structural inductive bias"
- Break condition: If the hierarchical relationships do not capture the true dependencies between documents, or if the weighting scheme fails to prioritize relevant connections

### Mechanism 2
- Claim: Multi-task learning with metadata prediction improves summarization quality by providing additional training signals
- Mechanism: The model predicts document types, review ratings, reviewer confidences, and paper acceptance as auxiliary tasks alongside the main summarization objective
- Core assumption: Metadata features contain information that is correlated with summary quality and can guide the model toward better representations
- Evidence anchors:
  - [abstract]: "a multi-task training objective that predicts metadata features (e.g., review ratings)"
  - [section 4.2]: "To leverage PEER SUM metadata which is supposed to give the model more positive training guidance, RAMMER is trained with multi-task learning"
- Break condition: If the metadata predictions do not correlate with summary quality or if the multi-task objective causes interference between tasks

### Mechanism 3
- Claim: Using pre-trained language models as backbone allows effective zero-shot and few-shot performance on the new task
- Mechanism: RAMMER initializes with pre-trained BART, PRIMERA, or LED models and fine-tunes them on the peer-review dataset
- Core assumption: Pre-trained language models have learned general language understanding that transfers to the meta-review generation task
- Evidence anchors:
  - [section 5.1]: "We compare RAMMER with a suite of strong baselines, some of the most popular encoder-decoder pre-trained summarization models"
  - [table 5]: Shows RAMMER outperforms baselines in both fully-supervised and zero-shot scenarios
- Break condition: If the pre-training domain is too different from peer-review text, or if the task requires specialized knowledge not captured by general pretraining

## Foundational Learning

- Concept: Hierarchical document relationships
  - Why needed here: The peer-review process creates explicit hierarchical structures (threads, replies, comments) that contain important inter-document dependencies
  - Quick check question: Can you identify the seven types of relationships (ancestor-1, ancestor-n, descendant-1, descendant-n, siblings, document-self, same-thread) and explain how they capture the peer-review structure?

- Concept: Attention manipulation in transformers
  - Why needed here: Standard transformer attention treats all documents equally, but we need to incorporate structural biases based on relationships
  - Quick check question: How does the weighted combination of relationship matrices modify the attention computation compared to standard self-attention?

- Concept: Multi-task learning objectives
  - Why needed here: The metadata provides additional supervision signals that can improve the main summarization task
  - Quick check question: What are the four auxiliary tasks in RAMMER and how are their losses combined with the main generation loss?

## Architecture Onboarding

- Component map: Input documents → Relationship-aware sparse encoder → Transformer decoder → Output meta-review
- Critical path: Input → Relationship-aware sparse encoder → Transformer decoder → Output generation, with auxiliary losses computed in parallel
- Design tradeoffs:
  - Using sparse attention vs. full attention: Reduces computation but may miss some cross-document connections
  - Multi-task learning: Provides additional supervision but may introduce task interference
  - Pre-trained backbone choice: BART works best for 1024 input length, but other models may be better for longer documents
- Failure signatures:
  - Poor ROUGE scores: Could indicate attention manipulation is not capturing relevant relationships
  - Low metadata prediction accuracy: May suggest the encoder is not learning useful document representations
  - Mode collapse in generation: Could indicate the decoder is not properly integrating the structured encoder outputs
- First 3 experiments:
  1. Ablation study: Remove relationship-aware attention and compare performance to full RAMMER
  2. Hyperparameter sweep: Test different balancing weights (α values) for multi-task objectives
  3. Input length analysis: Compare performance across different maximum input lengths (1024, 4096, 16384) to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the RAMMER model be effectively applied to other tasks involving summarizing multiple documents with explicit inter-document relationships, such as multi-document argumentation or question answering?
- Basis in paper: [explicit] The paper mentions that the attention manipulation based on hierarchical inter-document relationships has the potential to be used for other tasks such as argument mining and question answering that take as input multiple documents with explicit inter-document relationships
- Why unresolved: The paper does not provide any experiments or results for applying RAMMER to these other tasks, and there is no appropriate dataset available for such applications
- What evidence would resolve it: Experiments applying RAMMER to other tasks involving summarizing multiple documents with explicit inter-document relationships, such as multi-document argumentation or question answering, using appropriate datasets and evaluating the performance

### Open Question 2
- Question: What are the specific challenges and limitations of the PEER SUM dataset, and how can they be addressed to improve the dataset and its usefulness for training meta-review generation models?
- Basis in paper: [inferred] The paper mentions that the PEER SUM dataset is challenging and that the generated meta-reviews still need improvement, as shown in the examples in Appendix D. It also mentions that the quality of generated meta-reviews is still not satisfactory
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges and limitations of the PEER SUM dataset, and how these limitations affect the performance of meta-review generation models
- What evidence would resolve it: A detailed analysis of the challenges and limitations of the PEER SUM dataset, including examples of problematic cases and suggestions for improving the dataset, such as increasing the size, adding more diverse examples, or providing more detailed annotations

### Open Question 3
- Question: How can the RAMMER model be further improved to better handle conflicts in source documents, which are a significant challenge in meta-review generation?
- Basis in paper: [explicit] The paper mentions that further analyses reveal that RAMMER and other models struggle to handle conflicts in source documents of PEER SUM, indicating the need for further research in this area
- Why unresolved: The paper does not provide any specific solutions or techniques for addressing conflicts in source documents, and it is unclear how the RAMMER model can be improved to better handle such conflicts
- What evidence would resolve it: Experiments and results showing the effectiveness of new techniques or modifications to the RAMMER model in handling conflicts in source documents, such as improved attention mechanisms, conflict detection and resolution modules, or multi-task learning objectives that explicitly target conflict handling

## Limitations
- The dataset focuses exclusively on computer science peer reviews, limiting generalizability to other domains
- The model struggles to handle conflicting information in source documents, with performance degradation when contradictory opinions are present
- Evaluation relies heavily on automatic metrics with limited human evaluation, raising questions about true summary quality

## Confidence

**High Confidence** (Supporting evidence is strong and consistent):
- The dataset construction methodology and its distinguishing features from existing MDS datasets
- The relationship-aware sparse attention mechanism and its implementation details
- The overall experimental methodology and comparison with baseline models

**Medium Confidence** (Results are plausible but have some limitations):
- The effectiveness of multi-task learning with metadata prediction
- The reference-free evaluation metric based on paper acceptance prediction
- The model's performance in zero-shot and few-shot settings

**Low Confidence** (Results are preliminary or based on limited evidence):
- Claims about handling real-world peer review scenarios with conflicting opinions
- Generalizability to domains outside computer science
- Long-term stability of the model across different conference venues and time periods

## Next Checks

1. **Cross-domain evaluation**: Test Rammer on meta-review datasets from non-CS domains (e.g., biology, social sciences) to assess generalizability beyond the original dataset's scope

2. **Human evaluation study**: Conduct comprehensive human evaluation comparing Rammer-generated meta-reviews against human-written ones, focusing on factual accuracy, coherence, and handling of conflicting information

3. **Conflict resolution benchmark**: Create a specific test set with known conflicts in peer reviews and evaluate Rammer's ability to identify and appropriately address contradictory opinions, comparing against both strong baselines and human performance