---
ver: rpa2
title: LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing
  Pipelines with LLMs
arxiv_id: '2307.10168'
source_url: https://arxiv.org/abs/2307.10168
tags:
- llms
- human
- crowdsourcing
- tasks
- pipelines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether Large Language Models (LLMs) can replicate
  complex crowdsourcing pipelines that break down tasks into microtasks for independent
  completion and later combination. It conducts a course assignment where students
  replicate seven crowdsourcing pipelines by designing LLM chains for each sub-task.
---

# LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs

## Quick Facts
- arXiv ID: 2307.10168
- Source URL: https://arxiv.org/abs/2307.10168
- Authors: 
- Reference count: 17
- Primary result: LLMs can partially replicate crowdsourcing pipelines but show variable success and different instruction sensitivities compared to humans

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can replicate complex crowdsourcing pipelines that traditionally rely on human workers. Through a course assignment where students design LLM chains for seven different crowdsourcing pipeline types, the study finds that while LLMs can successfully complete some sub-tasks, their performance is highly variable and differs significantly from human behavior. The research reveals that LLMs respond better to explicit, comparison-based instructions but struggle with implicit information selection and trade-off criteria that humans handle more naturally. The work demonstrates that replicating crowdsourcing pipelines provides a valuable platform for understanding LLM strengths and weaknesses across different task types.

## Method Summary
The study employed a course assignment approach where students were tasked with replicating seven crowdsourcing pipeline papers by designing LLM chains for each sub-task. Students created baseline solutions using single LLM modules and compared them against their multi-module LLM chains using test cases and evaluation metrics. The replication process involved reading pipeline papers, creating test cases with inputs and ideal outputs, implementing both baseline and LLM chain solutions, and analyzing implementation successes and failures. Evaluation relied on peer grading for replication correctness and performance comparison between baseline and LLM chain solutions across multiple test cases.

## Key Results
- LLMs can successfully replicate some sub-tasks in crowdsourcing pipelines but show wide variance in performance across different pipeline components
- LLMs are more responsive to comparison-based instructions (e.g., "better," "more diverse") than to trade-off criteria that humans handle naturally
- Replicating crowdsourcing pipelines provides valuable insights into relative LLM strengths across different task types and identifies where human involvement remains necessary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can replicate complex crowdsourcing pipelines by completing sub-tasks independently and combining outputs
- Mechanism: Breaking down complex tasks into discrete sub-tasks allows LLMs to handle portions they are capable of while leaving others to humans, enabling scalable human-LLM collaboration
- Core assumption: LLMs have sufficient world knowledge and instruction-following capability to complete sub-tasks independently
- Evidence anchors:
  - [abstract]: "LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities"
  - [section]: "We find that while LMs appear to be able to replicate crowdsourcing pipelines, there is a wide variance in which parts they tend to perform well / in ways we would expect from humans"
- Break condition: If sub-tasks require complex reasoning, judgment, or multimodal inputs that exceed current LLM capabilities

### Mechanism 2
- Claim: Different sub-tasks within the same pipeline reveal relative LLM strengths and weaknesses
- Mechanism: By having multiple LLM modules perform distinct sub-tasks within the same application domain, we can directly compare their performance and identify optimal task allocation
- Core assumption: Performance differences across sub-tasks reflect true LLM capability differences rather than task difficulty variation
- Evidence anchors:
  - [abstract]: "Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross-comparing their performances on sub-tasks)"
  - [section]: "By examining students' final submissions and their own reflections, it becomes evident that (students believe) certain pipelines require adjustments"
- Break condition: If task boundaries are not well-defined or if sub-tasks are too interdependent

### Mechanism 3
- Claim: LLMs respond differently to instructions than humans, requiring explicit information foraging and comparison-based prompts
- Mechanism: LLMs need explicit prioritization of requirements and benefit from comparison-based instructions, while struggling with implicit information selection
- Core assumption: LLMs can be effectively tuned to follow more explicit instructions than humans
- Evidence anchors:
  - [section]: "LLMs are more responsive to adjectives and comparison-based instructions, such as 'better' or 'more diverse,' whereas humans handle instructions involving trade-off criteria better"
  - [section]: "LLMs struggle with information foraging, and tend to constantly accumulate context and produce outputs with mixed quality"
- Break condition: If instruction tuning approaches do not improve LLM performance on ambiguous tasks

## Foundational Learning

- Concept: Task decomposition and pipeline design
  - Why needed here: Understanding how to break down complex tasks into manageable sub-tasks is crucial for effective LLM replication of crowdsourcing pipelines
  - Quick check question: Can you identify the key sub-tasks in a given crowdsourcing pipeline and explain why each is necessary?

- Concept: LLM instruction tuning and prompt engineering
  - Why needed here: Success depends on crafting prompts that effectively guide LLMs through each sub-task while accounting for their unique response patterns
  - Quick check question: How would you modify a prompt to make it more explicit for an LLM while maintaining task clarity?

- Concept: Human-LLM complementarity and task allocation
  - Why needed here: Recognizing where LLMs excel and where humans are still needed enables optimal division of labor in complex tasks
  - Quick check question: Given a complex task, which sub-tasks would you assign to LLMs versus humans based on their respective strengths?

## Architecture Onboarding

- Component map:
  Input → LLM Module 1 → LLM Module 2 → ... → LLM Module N → Output Combination → Quality Control → Final Output

- Critical path:
  Input → LLM Module 1 → LLM Module 2 → ... → LLM Module N → Output Combination → Quality Control → Final Output

- Design tradeoffs:
  - Granularity of sub-tasks: Finer granularity allows better LLM specialization but increases complexity of coordination
  - Prompt specificity: More explicit prompts improve LLM performance but may reduce flexibility
  - Quality control mechanisms: Stricter controls improve reliability but increase computational overhead

- Failure signatures:
  - Inconsistent outputs across LLM modules
  - Cascading errors from early-stage mistakes
  - LLM modules getting stuck on complex reasoning tasks
  - Poor integration of outputs from different LLM modules

- First 3 experiments:
  1. Implement a simple Map-Reduce pipeline with two LLM modules and test on basic text summarization
  2. Create a Find-Fix-Verify pipeline with explicit quality control steps and evaluate on short text editing tasks
  3. Design an iterative process pipeline where each LLM module builds on the previous output and test with creative writing prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated annotations be reliably used for pilot studies to refine study instructions and designs?
- Basis in paper: [inferred] The paper mentions that researchers are investigating the feasibility of using LLMs as pilot study users for efficiently refining study instructions and designs, but also notes that this transition may not be straightforward due to LLMs only responding to textual instructions.
- Why unresolved: The paper suggests that while LLMs can help study designers reflect on high-level requirements, the literal instruction has to be redesigned. It is unclear which parts of the user study design can be prototyped using LLMs and how to effectively map LLM instructions into multi-modal constraints for humans.
- What evidence would resolve it: Empirical studies comparing the effectiveness of LLM-generated annotations versus human-generated annotations in pilot studies, as well as investigations into the process of mapping LLM instructions to multi-modal constraints for humans.

### Open Question 2
- Question: How can LLMs be trained to handle ambiguous or incomplete instructions more effectively?
- Basis in paper: [explicit] The paper discusses the need to improve LLM instruction tuning to better handle ambiguous or incomplete instructions, as LLMs and humans respond differently to instructions. It suggests exploring the effects of instruction tuning and training humans for complementarity.
- Why unresolved: The paper highlights the differences in how LLMs and humans respond to instructions, but does not provide specific methods or evidence for training LLMs to handle ambiguous or incomplete instructions more effectively.
- What evidence would resolve it: Experimental studies comparing the performance of LLMs trained with different instruction tuning methods on tasks with ambiguous or incomplete instructions, as well as investigations into the complementary skills between humans and LLMs.

### Open Question 3
- Question: What are the optimal ways to delegate tasks between humans and LLMs in complex tasks?
- Basis in paper: [inferred] The paper discusses the partial effectiveness of LLM chains and the potential for explicit training of humans to identify and develop skills complementary to LLM strengths. It suggests that instead of humans or LLMs completing all sub-tasks, an effective task delegation among a mixture of different "workers" might be useful.
- Why unresolved: The paper provides examples of task delegation but does not offer a comprehensive framework or evidence for determining the optimal ways to delegate tasks between humans and LLMs in complex tasks.
- What evidence would resolve it: Empirical studies investigating the performance of different task delegation strategies between humans and LLMs in various complex tasks, as well as the development of frameworks or guidelines for effective task delegation.

## Limitations
- Study relies on student assignments rather than controlled experiments, introducing variability in pipeline design quality
- Evaluation depends on peer grading and comparison to baseline solutions rather than independent validation
- Specific test cases and evaluation metrics are not fully specified, making rigorous comparison difficult

## Confidence
- **High confidence**: LLMs can successfully replicate some crowdsourcing pipeline sub-tasks (supported by multiple student submissions showing partial success)
- **Medium confidence**: LLMs show different sensitivities to instructions than humans (based on qualitative student observations and reflections)
- **Medium confidence**: Replicating crowdsourcing pipelines provides valuable insights into LLM strengths and weaknesses (supported by the diverse range of pipeline types tested)

## Next Checks
1. **Independent replication**: Replicate the seven crowdsourcing pipeline experiments with controlled prompts and test cases to verify the reported success rates and identify specific failure patterns
2. **Instruction sensitivity analysis**: Systematically test LLM responses to comparison-based versus trade-off criteria instructions across multiple tasks to quantify the observed differences
3. **Quality control mechanism evaluation**: Implement and compare different quality control approaches (e.g., human-in-the-loop, automated verification) to assess their effectiveness in mitigating LLM-specific failure modes identified in the study