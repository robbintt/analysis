---
ver: rpa2
title: A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
  Financial Sentiment Analysis
arxiv_id: '2312.08725'
source_url: https://arxiv.org/abs/2312.08725
tags:
- llms
- sentiment
- performance
- zero-shot
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of fine-tuning and few-shot
  learning approaches using large language models (LLMs) for financial sentiment analysis.
  The authors employ a range of LLMs, from smaller models with 250M to 3B parameters,
  and compare their performance to state-of-the-art models and in-context learning
  with ChatGPT.
---

# A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis

## Quick Facts
- arXiv ID: 2312.08725
- Source URL: https://arxiv.org/abs/2312.08725
- Reference count: 30
- Primary result: Fine-tuned smaller LLMs achieve comparable performance to larger state-of-the-art models for financial sentiment analysis

## Executive Summary
This paper investigates the effectiveness of fine-tuning and few-shot learning approaches using large language models (LLMs) for financial sentiment analysis. The authors compare models ranging from 250M to 3B parameters against state-of-the-art models and ChatGPT's in-context learning capabilities. Results demonstrate that fine-tuned smaller LLMs can match the performance of larger models, even with fewer parameters and a smaller training dataset. Additionally, zero-shot and one-shot performance of LLMs produces comparable results to fine-tuned models, though increasing shot numbers beyond one does not improve performance.

## Method Summary
The study fine-tunes Flan-T5 models (Base, Large, XL) using QLoRA on a finance-domain dataset, then evaluates performance on Twitter Financial News Sentiment and Financial PhraseBank datasets. The approach includes zero-shot and few-shot inference with both Flan-T5 and ChatGPT models. QLoRA is employed for memory-efficient fine-tuning of the 250M, 780M, and 3B parameter models. Performance is measured using accuracy and F1-Macro metrics across sentiment classes (Positive, Negative, Neutral).

## Key Results
- Fine-tuned smaller LLMs achieve comparable performance to state-of-the-art fine-tuned LLMs despite fewer parameters
- Zero-shot and one-shot performance of LLMs produces comparable results with fine-tuned smaller LLMs
- Increasing the number of shots for in-context learning does not lead to improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning smaller LLMs with QLoRA can achieve performance comparable to much larger state-of-the-art models.
- Mechanism: QLoRA reduces memory usage by keeping pre-trained weights frozen in 4-bit quantization and training only low-rank adaptation matrices, enabling fine-tuning of large models on limited compute.
- Core assumption: The low-rank adaptation matrices capture the essential task-specific knowledge without needing to update all parameters.
- Evidence anchors:
  - [section] "Given that we are dealing with LLMs with 250M, 780M, and 3B parameters, we utilize the QLoRA method for fine-tuning. This approach, which offers enhanced memory efficiency and faster computation, is well-suited for optimizing the fine-tuning process for models of such sizes."
  - [corpus] Weak: No direct corpus comparison for QLoRA performance.
- Break condition: If the task requires significant changes to the model's base knowledge, low-rank adaptation may be insufficient.

### Mechanism 2
- Claim: In-context learning performance does not improve with more than one shot for financial sentiment analysis.
- Mechanism: LLMs have a limited capacity to effectively utilize multiple examples in the context window for this specific task, possibly due to context length limitations or the complexity of financial language.
- Core assumption: The models cannot effectively generalize from multiple examples when the examples themselves may not capture the nuances of financial sentiment.
- Evidence anchors:
  - [section] "The variation in outcomes can be ascribed to the difficulties associated with managing excessively lengthy contexts, which have the potential to lead the LLMs misguided, as revealed in a new study [27]."
  - [corpus] Weak: No direct corpus evidence comparing different shot settings.
- Break condition: If the task domain is simpler or the context window is larger, few-shot learning might show improvement.

### Mechanism 3
- Claim: Zero-shot performance of LLMs varies significantly between social media text and news headlines in financial sentiment analysis.
- Mechanism: The pre-training corpus of LLMs influences their ability to understand different types of financial text, with social media text being more aligned with their training data.
- Core assumption: The language patterns and jargon in social media posts are more similar to the general pre-training data of LLMs than formal financial news.
- Evidence anchors:
  - [section] "ChatGPT, being trained on a large corpus of social media posts, is better equipped to extract the true sentiment from social media text, which is reflected in its zero-shot performance on the TFSN dataset."
  - [corpus] Weak: No direct corpus evidence comparing zero-shot performance on different text types.
- Break condition: If the pre-training corpus changes or if the task uses a different type of financial text, the zero-shot performance gap may close.

## Foundational Learning

- Concept: Instruction fine-tuning
  - Why needed here: Improves the model's ability to follow instructions and understand task-specific prompts, crucial for few-shot and zero-shot settings.
  - Quick check question: What is the difference between standard fine-tuning and instruction-tuning, and why is it important for zero-shot performance?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: Enables efficient fine-tuning of large models on limited hardware by reducing the number of trainable parameters.
  - Quick check question: How does LoRA reduce the number of trainable parameters, and what are the trade-offs compared to full fine-tuning?

- Concept: Financial sentiment analysis
  - Why needed here: The specific domain knowledge and linguistic patterns of financial text are crucial for accurate sentiment classification.
  - Quick check question: What are the key differences between general sentiment analysis and financial sentiment analysis, and why do they matter for model performance?

## Architecture Onboarding

- Component map: Input Financial text (tweets, news headlines) -> Models Flan-T5 (Base, Large, XL), ChatGPT (gpt-3.5-turbo) -> Techniques QLoRA for fine-tuning, in-context learning for zero/few-shot -> Datasets Twitter Financial News Sentiment (Twitter Train), TFSN, FPB -> Metrics Accuracy, F1-Macro

- Critical path:
  1. Load and preprocess financial text data
  2. Fine-tune Flan-T5 models using QLoRA on finance-domain dataset
  3. Evaluate fine-tuned models on test datasets (TFSN, FPB)
  4. Perform zero-shot and few-shot inference on ChatGPT and Flan-T5 models
  5. Compare results with state-of-the-art models (FinBERT, Instruct-FinGPT)

- Design tradeoffs:
  - Model size vs. computational resources: Smaller models with QLoRA vs. larger models with full fine-tuning
  - Shot number vs. performance: Few-shot learning may not improve with more shots for this task
  - Zero-shot vs. fine-tuning: Zero-shot performance varies by text type, while fine-tuning provides consistent results

- Failure signatures:
  - Overfitting: High training accuracy but low test accuracy
  - Context length issues: Degraded performance with longer input texts
  - Prompt sensitivity: Inconsistent results with different prompt formats

- First 3 experiments:
  1. Fine-tune Flan-T5-Base on Twitter Train dataset using QLoRA, evaluate on TFSN and FPB
  2. Perform zero-shot inference on ChatGPT using the same datasets, compare with fine-tuned results
  3. Conduct few-shot learning experiments (1-shot, 5-shot, 10-shot) on both Flan-T5 and ChatGPT models, analyze performance trends

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs for financial sentiment analysis vary across different domains and types of financial data (e.g., social media posts vs. news headlines)?
- Basis in paper: [inferred] The paper compares the performance of LLMs on two datasets: Twitter Financial News Sentiment (TFSN) and Financial PhraseBank (FPB). It notes that ChatGPT performs better on TFSN (social media posts) while smaller LLMs like Flan-T5 outperform ChatGPT on FPB (news headlines).
- Why unresolved: The paper only tests two specific datasets, which may not be representative of all financial data types and domains.
- What evidence would resolve it: Testing the same models on a wider variety of financial datasets, including different types of financial news, social media, and other financial communications, to see if the observed trends hold consistently.

### Open Question 2
- Question: Can more advanced few-shot learning techniques, such as retrieval-based prompting or Chain of Thought (CoT), significantly improve the performance of LLMs in financial sentiment analysis compared to random sampling of examples?
- Basis in paper: [explicit] The paper acknowledges that increasing the number of shots (5-shot and 10-shot) did not consistently improve performance and suggests exploring more effective techniques like retrieval-based prompting and CoT.
- Why unresolved: The paper only uses random sampling for few-shot examples and does not explore more advanced prompting techniques.
- What evidence would resolve it: Conducting experiments using retrieval-based prompting and CoT techniques for few-shot learning and comparing the results to random sampling to determine if these methods lead to consistent improvements in performance.

### Open Question 3
- Question: What is the impact of instruction tuning on the performance of smaller LLMs for financial sentiment analysis compared to larger LLMs?
- Basis in paper: [explicit] The paper notes that the Flan-T5 models, which are instruction-tuned, achieve remarkable performance even with fewer parameters compared to larger models like ChatGPT. It also mentions that instruction tuning can enhance reasoning capabilities.
- Why unresolved: The paper does not directly compare the performance of instruction-tuned smaller LLMs to non-instruction-tuned larger LLMs on the same tasks.
- What evidence would resolve it: Conducting a direct comparison between instruction-tuned smaller LLMs and non-instruction-tuned larger LLMs on financial sentiment analysis tasks to quantify the impact of instruction tuning on performance.

## Limitations
- Findings on few-shot learning limitations may not generalize to other domains beyond financial sentiment analysis
- Comparison with ChatGPT's in-context learning is constrained by lack of access to training details and potential model updates
- QLoRA approach may not capture all task-specific nuances compared to full fine-tuning for complex financial language

## Confidence
- High Confidence: Comparative performance results between fine-tuned smaller LLMs and state-of-the-art models are reliable within the tested domain
- Medium Confidence: Conclusions about few-shot learning limitations are reasonable but may be task-specific
- Low Confidence: Explanation for zero-shot performance variation between text types relies on unverified assumptions about pre-training data

## Next Checks
1. Cross-domain validation: Test few-shot learning performance on non-financial sentiment analysis tasks to determine if limitations are domain-specific
2. Ablation study on QLoRA parameters: Systematically vary rank and alpha parameters to identify optimal configuration and understand memory-performance trade-offs
3. Fine-tuning vs. in-context learning comparison: Conduct extensive comparison using different prompt engineering techniques (chain-of-thought, rationale generation) to overcome observed few-shot limitations