---
ver: rpa2
title: 'Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding
  Initialization Strategy for Efficient Language Adaptation'
arxiv_id: '2310.03477'
source_url: https://arxiv.org/abs/2310.03477
tags:
- language
- tokens
- languages
- training
- dutch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel model conversion strategy for low-resource
  languages that leverages word translation dictionaries to map tokens from a target
  tokenizer to semantically similar tokens from a source language tokenizer. This
  approach, called Tik-to-Tok, improves embedding initialization for the target language,
  resulting in state-of-the-art performance on downstream tasks.
---

# Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation

## Quick Facts
- arXiv ID: 2310.03477
- Source URL: https://arxiv.org/abs/2310.03477
- Reference count: 17
- Key outcome: Improves embedding initialization for low-resource languages by mapping tokens via word translation dictionaries, achieving state-of-the-art performance while reducing training data and time requirements.

## Executive Summary
This paper introduces Tik-to-Tok, a novel approach for cross-lingual model conversion that leverages word translation dictionaries to map tokens between source and target language tokenizers. By using these mappings to initialize embeddings with weighted combinations of semantically similar tokens, the method achieves state-of-the-art performance on Dutch and Frisian while significantly reducing the amount of data and time required compared to training from scratch. The approach is particularly effective for low- and mid-resource languages where large monolingual corpora are scarce.

## Method Summary
Tik-to-Tok works by replacing the tokenizer and token embedding table of existing monolingual models with new ones adapted to the target language. The new embedding table is initialized using a weighted combination of embeddings from semantically similar tokens found in the source model, guided by a word translation dictionary. The method employs a symmetric bigram corpus to train FastText embeddings that handle out-of-vocabulary tokens, and uses weighted averaging (30% for best match, 10% for second best) to combine multiple translation candidates. After initialization, the embeddings and language modeling head are fine-tuned on the target language corpus, with optional full model fine-tuning for optimal performance.

## Key Results
- Achieves state-of-the-art performance on Dutch and Frisian for sentiment analysis, NER, POS tagging, and NLI tasks
- Reduces training data requirements by leveraging semantic similarity through token mapping
- Significantly decreases training time compared to full model training from scratch
- Effective for both low-resource (Frisian) and mid-resource (Dutch) languages

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual token mapping improves embedding initialization by leveraging semantic similarity between tokens in source and target languages. The approach uses a word translation dictionary to map tokens from target tokenizer to semantically similar tokens from source tokenizer, then averages their embeddings with weighted contributions. The core assumption is that tokens with similar character n-grams have similar semantic meanings across languages.

### Mechanism 2
FastText embeddings enable mapping for out-of-vocabulary tokens by representing words as sums of character n-gram embeddings. The method trains a bilingual fastText model on a symmetric bigram corpus where each word is paired with itself and its translation, creating equal neighbor distributions. The core assumption is that words with identical neighbor distributions should have similar summed character n-gram embeddings.

### Mechanism 3
Weighted averaging of multiple translation candidates preserves more information than single best match. The approach assigns 30% weight to best match, 10% to second best, and divides remaining 60% equally among all candidates. The core assumption is that top-ranking translation matches are of higher quality and deserve greater weight in initialization.

## Foundational Learning

- **Tokenization and subword segmentation**: Why needed - The entire approach depends on understanding how tokens are split and mapped between different tokenizers. Quick check - How does BPE tokenization differ from character-level tokenization in handling compound words?

- **Word embeddings and semantic similarity**: Why needed - The approach relies on computing semantic similarity between tokens using embeddings. Quick check - What distance metric is typically used to measure semantic similarity between word embeddings?

- **Cross-lingual transfer learning**: Why needed - The method transfers knowledge from high-resource to low-resource languages through model conversion. Quick check - What are the main challenges in cross-lingual transfer that this approach aims to address?

## Architecture Onboarding

- **Component map**: Source language model (with tokenizer and embeddings) -> Target language tokenizer -> Word translation dictionary -> FastText embedding model -> Token mapping algorithm -> Embedding initialization procedure -> Fine-tuning pipeline

- **Critical path**: 1) Generate symmetric bigram corpus from translation dictionary 2) Train FastText model on this corpus 3) Map target tokens to source tokens using dictionary and FastText 4) Initialize target embeddings using weighted averages 5) Fine-tune embeddings on target language corpus 6) Optionally fine-tune full model

- **Design tradeoffs**: Using weighted averaging vs. single best match preserves more information but adds complexity; symmetric vs. asymmetric dictionary ensures equal treatment but may lose frequency information; FastText vs. other embedding methods handles OOV well but may be less semantically precise

- **Failure signatures**: High MLM loss after initialization indicates poor token mapping; degraded performance on downstream tasks suggests catastrophic forgetting; slow convergence during fine-tuning may indicate suboptimal embedding initialization

- **First 3 experiments**: 1) Convert English to Dutch using word translation dictionary only, measure MLM loss 2) Add FastText-based mapping for out-of-dictionary tokens, compare MLM loss 3) Apply weighted averaging strategy, evaluate on downstream tasks (NER, POS, SA)

## Open Questions the Paper Calls Out

- **How effective is the Tik-to-Tok approach for languages beyond Dutch and Frisian, particularly for non-Germanic languages?**: The paper suggests future work could explore collaborations with a more diverse set of languages, as the current evaluation is limited to Dutch and Frisian. Conducting experiments with Tik-to-Tok on various languages from different families would provide evidence for generalizability.

- **How does the choice of word translation dictionary impact the quality of the converted language models?**: The paper notes that the impact of using particular word translation dictionaries or combinations of multiple dictionaries was not studied. Comparing the performance of language models converted using different word translation dictionaries would provide insights into the impact of dictionary choice.

- **Can the Tik-to-Tok approach be extended to handle multiword expressions effectively?**: The paper indicates that understanding multiword expressions in the target language will probably have to be learned through full finetuning, as token embeddings are unlikely to be sufficient to learn them all in isolation. Developing and evaluating an extension of the Tik-to-Tok approach that incorporates multiword expressions would provide evidence for handling them effectively.

## Limitations

- The entire approach hinges on the quality and coverage of word translation dictionaries, which the paper does not systematically evaluate
- Claims about computational efficiency improvements lack direct comparisons with competing approaches or quantitative measurements
- The method's effectiveness for languages with significantly different linguistic properties remains untested

## Confidence

- **High Confidence**: The mechanism of using word translation dictionaries to map tokens between tokenizers is technically sound and well-established in the literature
- **Medium Confidence**: The FastText-based approach for handling out-of-vocabulary tokens is theoretically plausible but lacks empirical validation of core assumptions
- **Low Confidence**: Claims about computational efficiency improvements are weakly supported without direct benchmarks or comprehensive measurements

## Next Checks

1. **Dictionary Quality Analysis**: Conduct systematic evaluation of translation dictionary quality by measuring coverage gaps, error rates, and the impact of dictionary quality on downstream performance. Test whether the weighted averaging strategy remains effective when dictionary quality varies significantly.

2. **Cross-Linguistic Generalization Test**: Apply Tik-to-Tok to language pairs with significantly different linguistic properties (e.g., Turkish, Finnish, Japanese) and evaluate whether the character n-gram based similarity assumptions hold. Compare performance against languages more similar to Dutch/Frisian.

3. **FastText Assumption Validation**: Empirically test the core assumption that symmetric bigram corpora create equal neighbor distributions by measuring actual neighbor distributions before and after training, and correlating these with embedding similarity. Validate whether FastText's character n-gram approach provides meaningful semantic mappings across diverse language pairs.