---
ver: rpa2
title: Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority
  Influence
arxiv_id: '2302.03322'
source_url: https://arxiv.org/abs/2302.03322
tags:
- adversary
- attack
- reward
- policy
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial Minority Influence (AMI) is a black-box policy-based
  attack method for cooperative multi-agent reinforcement learning (c-MARL). It uses
  a minority adversarial agent to unilaterally influence the policies of majority
  victim agents towards jointly worst-case cooperation.
---

# Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence

## Quick Facts
- **arXiv ID:** 2302.03322
- **Source URL:** https://arxiv.org/abs/2302.03322
- **Reference count:** 40
- **Key outcome:** Adversarial Minority Influence (AMI) achieves better adversarial reward in 10/12 tasks and effectively fools agents into worst-case scenarios.

## Executive Summary
This paper introduces Adversarial Minority Influence (AMI), a black-box policy-based attack method for cooperative multi-agent reinforcement learning (c-MARL). AMI uses a single adversarial agent to unilaterally influence the policies of multiple victim agents toward jointly worst-case cooperation. The method adapts mutual information to filter out detrimental victim-to-adversary influence while maximizing the adversarial effect from the minority agent to the majority victims. Extensive experiments demonstrate AMI's superior attack capability over state-of-the-art baselines across simulation environments (SMAC, MAMujoco) and real-world robot swarm settings.

## Method Summary
AMI is a black-box attack method that trains an adversarial policy to influence victim agents in c-MARL scenarios. The core innovation is the unilateral influence filter, which adapts mutual information to decompose agent-wise influence into majority (victim-to-adversary) and minority (adversary-to-victim) terms, filtering out the detrimental majority influence. A targeted adversarial oracle (TAO), trained through reinforcement learning, generates worst-case target actions for each victim agent. The adversarial policy co-adapts with the TAO to maximize a combined reward of adversarial gain and unilateral influence, guiding victims toward jointly detrimental scenarios without requiring access to victim parameters or observations.

## Key Results
- AMI outperforms baseline attacks (Gleave et al., Wu et al., Guo et al.) in 10/12 benchmark tasks across SMAC, MAMujoco, and real-world robot swarm environments
- The attack effectively fools victim agents into worst-case scenarios, achieving significantly lower rewards for the cooperative team
- AMI demonstrates consistent performance across different adversary agent IDs, showing robustness to the specific identity of the adversarial agent
- Ablation studies confirm that both the unilateral influence filtering and targeted adversarial oracle are critical components for attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Unilateral Influence Filtering
AMI maximizes victim policy deviation by decomposing mutual information into majority and minority influence terms, filtering out detrimental influence from victims to adversary. This allows the adversarial agent to unilaterally impact victim policies through environmental dynamics while avoiding back-influence that could destabilize training.

### Mechanism 2: Targeted Adversarial Oracle
The TAO generates jointly worst-case target actions for victims through a trial-and-error reinforcement learning process. By co-adapting with the adversarial policy, it learns to output target actions that, when influenced by the adversarial agent, guide the victim team toward long-term detrimental scenarios.

### Mechanism 3: Black-Box Practicality
AMI operates without access to victim parameters or observations, making it a practical attack method. It only requires the ability to interact with the environment alongside victim agents, enabling real-world deployment where victim policies are opaque.

## Foundational Learning

- **Mutual information as bilateral agent-wise influence metric**: AMI adapts mutual information to characterize influence between adversary and victim agents in c-MARL. *Quick check:* How does mutual information between two random variables capture their statistical dependence?

- **Reinforcement learning for adversarial policy training**: AMI uses RL to train both the adversarial policy and TAO. *Quick check:* What is the difference between on-policy and off-policy reinforcement learning algorithms?

- **Black-box adversarial attacks in c-MARL**: AMI is a practical black-box attack that doesn't require access to victim parameters or observations. *Quick check:* What are the advantages and disadvantages of black-box vs white-box adversarial attacks?

## Architecture Onboarding

- **Component map:** Adversarial Policy (PPO) -> Unilateral Influence Filter (Mutual Info Decomposition) -> Targeted Adversarial Oracle (PPO) -> Environment -> Victim Policies

- **Critical path:** 1) Adversarial policy takes actions in environment, 2) Unilateral influence filter quantifies adversary-to-victim influence, 3) TAO generates worst-case targets for victims, 4) Adversarial policy updated to maximize adversarial reward + influence, 5) TAO updated to generate better targets through trial-and-error

- **Design tradeoffs:** AMI trades off between maximizing adversarial reward and maximizing influence on victims. Higher influence weight may lead to stronger attacks but could make training less stable. The black-box nature trades attack effectiveness for practical applicability.

- **Failure signatures:** If AMI fails, adversarial reward plateaus at low values, indicating ineffective attacks. Influence on victims may be low as measured by the unilateral influence filter. TAO may generate targets that don't effectively guide victims toward worst-case scenarios.

- **First 3 experiments:**
  1. Implement unilateral influence filter and verify correct decomposition of mutual information into majority and minority terms.
  2. Train adversarial policy with influence filter and verify learning to maximize influence on victims.
  3. Implement TAO and verify it can generate worst-case target actions for victims through trial-and-error.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How transferable is AMI's adversarial policy from simulation to real-world deployment across different robotic platforms?
- **Basis in paper:** The paper discusses AMI's real-world attack on e-puck2 robots and mentions using a Sim2Real paradigm, but does not evaluate transferability across different robotic platforms.
- **Why unresolved:** Experiments only test AMI on one type of robot (e-puck2) in one environment. It's unclear if AMI would be equally effective on robots with different dynamics, sensors, or control interfaces.
- **What evidence would resolve it:** Experiments testing AMI's effectiveness on multiple robotic platforms (e.g., drones, ground vehicles, robotic arms) both in simulation and real-world settings, demonstrating consistent attack capability across diverse systems.

### Open Question 2
- **Question:** What is the relationship between the complexity of the victim c-MARL policy and its vulnerability to AMI attacks?
- **Basis in paper:** The paper shows AMI is effective across various tasks but does not analyze how policy complexity (e.g., network depth, training algorithms) affects vulnerability.
- **Why unresolved:** While the paper demonstrates AMI's effectiveness, it doesn't investigate which aspects of victim policy architecture or training methodology make them more or less susceptible to attack.
- **What evidence would resolve it:** Systematic experiments varying victim policy architectures (e.g., number of layers, types of networks), training algorithms, and hyperparameters to identify correlations between these factors and vulnerability to AMI.

### Open Question 3
- **Question:** How does AMI's performance scale with the number of victim agents in the cooperative system?
- **Basis in paper:** The paper tests AMI with one adversary against multiple victims but doesn't explore scenarios with multiple adversaries or how performance changes as victim team size increases.
- **Why unresolved:** The scalability of AMI to larger cooperative systems is unknown. It's unclear if the attack becomes more difficult as victim team size increases or if multiple adversaries would be needed for effective attacks.
- **What evidence would resolve it:** Experiments systematically varying the number of victim agents (e.g., 2 vs 2, 5 vs 5, 10 vs 10) and potentially introducing multiple adversarial agents to determine how performance scales with team size and whether multiple adversaries are beneficial.

## Limitations

- The method's effectiveness is primarily demonstrated in simulation environments and small-scale real-world robot swarms (3-4 agents), with limited evidence of scalability to larger, more complex real-world scenarios
- The paper does not address potential defenses against AMI or the robustness of the attack against detection mechanisms, leaving a gap in understanding practical limitations
- The theoretical justification for why mutual information decomposition effectively captures unilateral influence in complex multi-agent dynamics is limited, relying primarily on empirical validation

## Confidence

- **High Confidence:** AMI outperforms baseline attacks in 10/12 tasks with statistical significance across multiple seeds; ablation studies confirm component importance
- **Medium Confidence:** AMI achieves superior attack capability through unilateral influence filtering and targeted adversarial oracle; mechanism description is clear but lacks rigorous mathematical proof
- **Low Confidence:** AMI's practical applicability in real-world scenarios and robustness against potential defenses; limited evidence beyond small-scale robot swarm experiments

## Next Checks

1. **Mathematical Verification:** Rigorously verify the mutual information decomposition into majority and minority influence terms. Implement the unilateral influence filter with unit tests to ensure correct decomposition and confirm that the minority term actually captures adversarial influence rather than other factors.

2. **Scalability Testing:** Extend AMI evaluation to larger-scale environments with 10+ agents and more complex dynamics. Test whether attack performance degrades with increasing system complexity and whether the targeted adversarial oracle remains effective in high-dimensional state spaces.

3. **Defense Robustness:** Implement and test basic defensive mechanisms against AMI, such as anomaly detection on policy deviation or input sanitization. Evaluate whether AMI can bypass these defenses and quantify the robustness of the attack under realistic security conditions.