---
ver: rpa2
title: 'Investigating the Learning Behaviour of In-context Learning: A Comparison
  with Supervised Learning'
arxiv_id: '2307.15411'
source_url: https://arxiv.org/abs/2307.15411
tags:
- learning
- label
- labels
- performance
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how in-context learning (ICL) differs from
  supervised learning (SL) under label perturbations, such as noisy labels and label
  imbalance. The authors train the same large language models (LLMs) with identical
  demonstration examples using both ICL and SL, then compare their performance on
  six text classification tasks.
---

# Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning

## Quick Facts
- arXiv ID: 2307.15411
- Source URL: https://arxiv.org/abs/2307.15411
- Reference count: 40
- This paper investigates how in-context learning (ICL) differs from supervised learning (SL) under label perturbations, finding that ICL is less sensitive to label noise and imbalance than SL, especially as model size increases.

## Executive Summary
This paper investigates the learning behavior of in-context learning (ICL) compared to supervised learning (SL) under label perturbations including noisy labels and label imbalance. The authors conduct controlled experiments using the same language models (GPT-2 Large, GPT-2 XL, GPT-J) trained with identical demonstration examples using both approaches across six text classification tasks. They find that ICL shows greater robustness to label perturbations than SL, with this advantage becoming more pronounced as model size increases. Gold labels are critical for ICL performance, particularly for larger models, while ICL's sensitivity to label noise is lower than SL across all model sizes tested.

## Method Summary
The authors conduct experiments using three pre-trained language models (GPT-2 Large, GPT-2 XL, GPT-J) on six text classification datasets. For each dataset, they create label-perturbed versions with varying levels of corruption (0-100%) and imbalance (low/medium/high). ICL uses 16-shot demonstrations with the same prompt templates across all experiments, while SL fine-tunes models using the same data. Both approaches are evaluated using Macro-F1 score, with experiments run 5 times using 5 different seeds. The authors also analyze attention scores to understand ICL's robustness mechanisms and compute sensitivity coefficients from linear regression models to quantify performance sensitivity to label perturbations.

## Key Results
- Gold labels significantly impact ICL performance, with larger models showing greater sensitivity to label corruption
- ICL is less sensitive to label perturbations than SL across all model sizes tested
- As model size increases, ICL's performance gap compared to SL narrows, eventually matching or exceeding SL in multi-class classification tasks
- Attention analysis reveals that ICL can leverage contextual information to mitigate the effects of noisy labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gold labels are critical for ICL, especially in larger models, because they preserve the alignment between input features and target outputs during inference.
- Mechanism: In ICL, the model generates predictions by conditioning on demonstration examples. When the input-label pairs in the demonstrations are corrupted, the model's internal representations of the mapping from inputs to outputs are misaligned. Larger models, which have more capacity to learn fine-grained input-label correspondences, are more sensitive to this misalignment because they rely more heavily on the demonstrations to resolve ambiguity.
- Core assumption: The demonstrations serve as a proxy for supervised training signals, and the quality of this signal determines the downstream performance.
- Evidence anchors:
  - [abstract] "gold labels have significant impacts on the downstream in-context performance, especially for large language models"
  - [section] "Model performance is sensitive to the ratio of correct labels in the demonstrations… as the model size increases, the performance gap between various corruption rates also increases"

### Mechanism 2
- Claim: ICL is less sensitive to label perturbations than SL because it leverages contextual information and pre-trained knowledge rather than relying solely on gradient updates.
- Mechanism: In SL, the model updates its parameters based on the loss computed from the noisy labels, which can reinforce incorrect mappings. In ICL, the model makes predictions by conditioning on the entire demonstration set, which includes contextual cues and multiple examples that can help disambiguate noisy labels. The attention mechanism allows the model to weigh the relevance of each demonstration, reducing the impact of corrupted examples.
- Core assumption: The context provided by demonstrations contains sufficient information to mitigate the effects of label noise.
- Evidence anchors:
  - [abstract] "ICL is less sensitive to label perturbations than SL"
  - [section] "ICL takes into account the context in which the data are presented… ICL can learn not only the label-relevant features but also other interesting features that capture the intrinsic properties of the input distributions"

### Mechanism 3
- Claim: ICL gradually outperforms SL as model size increases because larger models can better leverage the demonstrations to learn complex input-label relationships.
- Mechanism: Larger models have more parameters and capacity to encode fine-grained relationships between inputs and labels. When demonstrations are provided, they can use this capacity to learn the mapping more effectively than smaller models, which may rely more on pre-trained knowledge. As the model size increases, the advantage of using demonstrations becomes more pronounced, especially in multi-class classification tasks where the relationships are more complex.
- Core assumption: The demonstrations provide a rich source of information that larger models can exploit more effectively.
- Evidence anchors:
  - [abstract] "ICL gradually attains comparable performance to SL as the model size increases"
  - [section] "With multi-class classification, as the number of corrupted label increases, the performance of ICL gradually outperforms SL across all three models"

## Foundational Learning

- Concept: Supervised learning (SL)
  - Why needed here: To compare ICL against a baseline that uses labeled data to update model parameters.
  - Quick check question: In SL, how are model parameters updated during training?

- Concept: In-context learning (ICL)
  - Why needed here: To understand the alternative paradigm where the model learns from demonstrations without parameter updates.
  - Quick check question: In ICL, how does the model generate predictions based on the demonstrations?

- Concept: Label perturbations
  - Why needed here: To simulate real-world scenarios where labels may be noisy or imbalanced and to test the robustness of ICL and SL.
  - Quick check question: What are the two types of label perturbations studied in this paper?

## Architecture Onboarding

- Component map: Pre-trained language model (GPT-2 variants, GPT-J) -> Prompt template with demonstrations -> Prediction generation (ICL) OR Parameter update with loss function (SL)
- Critical path: For ICL, the critical path is the generation of predictions based on the demonstrations. For SL, the critical path is the parameter update based on the loss computed from the noisy labels.
- Design tradeoffs: ICL trades off the need for parameter updates for the ability to learn from demonstrations without additional training. SL trades off the need for labeled data for the ability to update model parameters to fit the data.
- Failure signatures: ICL may fail if the demonstrations are not informative or if the model relies too heavily on pre-trained knowledge. SL may fail if the labels are too noisy or if the model overfits to the training data.
- First 3 experiments:
  1. Test ICL with gold labels and compare performance to SL.
  2. Introduce label noise and compare the sensitivity of ICL and SL.
  3. Vary the model size and compare the performance of ICL and SL.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the observed effects of label perturbations on ICL and SL extend to other natural language processing tasks beyond text classification, such as text generation or summarization?
- Basis in paper: [inferred] The paper's findings are limited to text classification tasks, and the authors suggest extending the current work to other tasks as future research.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis for tasks other than text classification.
- What evidence would resolve it: Conducting similar experiments on text generation, summarization, or other NLP tasks to compare the sensitivity of ICL and SL to label perturbations.

### Open Question 2
- Question: What are the underlying mechanisms that allow ICL to be less sensitive to label perturbations compared to SL, and how do these mechanisms vary across different model sizes?
- Basis in paper: [explicit] The authors analyze attention scores to provide insights into why ICL is less sensitive to label perturbations, but they do not provide a complete explanation for the observed phenomena.
- Why unresolved: The paper's analysis of attention scores is limited to a single dataset (MR) and does not provide a comprehensive understanding of the mechanisms involved.
- What evidence would resolve it: Conducting a more extensive analysis of attention scores across multiple datasets and model sizes, as well as exploring other potential mechanisms that may contribute to ICL's robustness.

### Open Question 3
- Question: How can the learning behavior of large language models be optimized for different scenarios, considering the trade-offs between ICL and SL in terms of sensitivity to label perturbations and model size?
- Basis in paper: [inferred] The authors suggest that their findings could be a useful reference for practitioners with limited computational resources and encourage further analysis of ICL in broader environments.
- Why unresolved: The paper does not provide specific recommendations or guidelines for optimizing the learning behavior of large language models in different scenarios.
- What evidence would resolve it: Developing a framework or set of guidelines that takes into account the trade-offs between ICL and SL, as well as the impact of model size, to help practitioners choose the most appropriate learning strategy for their specific use case.

## Limitations

- The mechanistic explanations rely heavily on theoretical reasoning rather than rigorous experimental validation, particularly the attention score analysis which lacks quantitative detail
- Results may not generalize beyond the specific experimental setup, as the study focuses on six text classification tasks with particular model architectures
- The paper lacks direct empirical evidence for its core mechanisms, making it difficult to verify the proposed explanations for ICL's robustness

## Confidence

- **High confidence** in the empirical observations: The comparative results showing ICL's reduced sensitivity to label perturbations and its scaling behavior with model size are well-supported by the experimental data across multiple runs and datasets.
- **Medium confidence** in the mechanistic explanations: While the paper provides plausible theories about why ICL behaves differently from SL (attention mechanisms, contextual disambiguation), these explanations lack the rigorous experimental validation needed for high confidence.
- **Low confidence** in the extrapolation to broader contexts: The paper does not provide sufficient evidence to confidently claim these findings generalize beyond the specific experimental setup.

## Next Checks

1. **Quantitative Attention Analysis**: Replicate the attention score analysis with detailed quantitative measurements showing how attention weights differ between clean and noisy demonstrations, and how this correlates with prediction accuracy across model sizes.

2. **Cross-Architecture Validation**: Test the same experimental protocol with different model families (e.g., OPT, LLaMA) and task types (regression, generation) to assess the generalizability of the findings.

3. **Ablation Studies on Contextual Information**: Design experiments that systematically remove or modify the contextual components of demonstrations (e.g., task descriptions, formatting) to isolate their contribution to ICL's robustness compared to SL.