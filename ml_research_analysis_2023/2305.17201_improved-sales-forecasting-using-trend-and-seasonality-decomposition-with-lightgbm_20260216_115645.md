---
ver: rpa2
title: Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM
arxiv_id: '2305.17201'
source_url: https://arxiv.org/abs/2305.17201
tags:
- sales
- series
- time
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses retail sales forecasting challenges for large
  retailers like Walmart by proposing a hybrid approach combining LightGBM and Prophet
  models with trend-seasonality decomposition. The key innovation is introducing a
  measure to assess the relative strength of trend versus seasonality in time series,
  then grouping them accordingly for separate modeling.
---

# Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM

## Quick Facts
- arXiv ID: 2305.17201
- Source URL: https://arxiv.org/abs/2305.17201
- Reference count: 8
- Primary result: WRMSSE of 0.614 achieved using hybrid LightGBM-Prophet approach with trend-seasonality decomposition on Walmart sales data

## Executive Summary
This paper addresses retail sales forecasting challenges for large retailers like Walmart by proposing a hybrid approach combining LightGBM and Prophet models with trend-seasonality decomposition. The key innovation is introducing a measure to assess the relative strength of trend versus seasonality in time series, then grouping them accordingly for separate modeling. The method assumes stationarity for LightGBM input, with its outputs serving as weights to adjust Prophet's total sales forecasts. Applied to Walmart data, the approach achieved a WRMSSE of 0.614, outperforming models without decomposition (0.963-0.832) and other methods like linear regression (1.120), SVM (1.083), and LSTM (0.802). The strategy demonstrates improved accuracy by leveraging Tweedie-based loss functions and hierarchical time series modeling.

## Method Summary
The method employs a hybrid approach that first decomposes time series into trend and seasonality components, then uses LightGBM to model stationary residuals and Prophet to capture trend and seasonal patterns. Time series are grouped based on whether trend or seasonality dominates using a decomposition score. LightGBM is trained on stationary-transformed data using Tweedie loss to handle zero-inflated sales distributions, while Prophet models the original data. The final forecast combines LightGBM outputs as weights to allocate Prophet's total forecast across individual series. The approach is evaluated on Walmart's hierarchical sales dataset with 42,840 time series at product, department, store, and state levels.

## Key Results
- Achieved WRMSSE of 0.614 on Walmart sales data, outperforming models without decomposition (0.963-0.832)
- Demonstrated superiority over traditional methods: linear regression (1.120), SVM (1.083), and LSTM (0.802)
- Successfully handled hierarchical time series structure with 42,840 series from 30,490 base series

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating trend and seasonality components allows LightGBM to focus on stationary patterns while Prophet handles non-stationary trends.
- Mechanism: The decomposition-based grouping isolates trend-dominated vs. seasonality-dominated series, enabling LightGBM to model stationary residuals effectively and Prophet to capture trend and seasonal dynamics separately.
- Core assumption: Time series can be meaningfully decomposed into trend, seasonality, and residuals without significant information loss.
- Evidence anchors:
  - [abstract] "proposing a hybrid approach combining LightGBM and Prophet models with trend-seasonality decomposition"
  - [section] "Trend-seasonality (T-S) decomposition strategy can improve the accuracy of our models"
- Break condition: If decomposition fails to isolate meaningful components (e.g., highly irregular patterns), the separation strategy becomes ineffective.

### Mechanism 2
- Claim: Tweedie-based loss function better handles the zero-inflated, right-skewed sales distribution than MSE.
- Mechanism: Tweedie loss accommodates the variance-mean relationship characteristic of count data with many zeros, improving prediction accuracy for sparse sales events.
- Core assumption: Sales data follows a Tweedie distribution where variance scales as a power of the mean.
- Evidence anchors:
  - [section] "the sales distribution in the train data exhibits a Poisson-like distribution: non-negative response values, right-skewed and long-tailed distribution"
  - [section] "Tweedie distribution belongs to the class of exponential dispersion models (EDM). It provides a flexible modeling for non-negative and right-skewed data"
- Break condition: If sales data becomes approximately Gaussian (e.g., through extensive aggregation), Tweedie loss offers no advantage over MSE.

### Mechanism 3
- Claim: Hierarchical time series modeling captures dependencies across aggregation levels, improving forecast accuracy.
- Mechanism: By modeling data at multiple aggregation levels (product, department, store, state), the framework leverages information flow between levels to reduce forecast uncertainty.
- Core assumption: Sales patterns exhibit consistent hierarchical structure where higher-level aggregates inform lower-level predictions.
- Evidence anchors:
  - [section] "The Walmart's sales dataset... includes daily sales data for thousands of products across ten stores... The original dataset contains 30490 time series. Based on different levels of aggregation, they can be built into a hierarchical structure of time series with 42840 time series"
  - [section] "WRMSSE takes this hierarchical structure into account and provides a measure of the accuracy of the forecasts at each level"
- Break condition: If aggregation levels show conflicting patterns (e.g., opposite trends), hierarchical modeling may introduce bias.

## Foundational Learning

- Concept: Time series decomposition into trend, seasonality, and residuals
  - Why needed here: Enables separate modeling of different temporal patterns that respond differently to forecasting methods
  - Quick check question: What happens to LightGBM's performance if you feed it non-stationary time series with strong trends?

- Concept: Tweedie distribution properties and applications
  - Why needed here: Sales data has many zeros and right-skewness that violate Gaussian assumptions
  - Quick check question: How does the variance function differ between Tweedie and Gaussian distributions?

- Concept: Hierarchical forecasting reconciliation
  - Why needed here: Walmart data has multiple aggregation levels that must be coherent
  - Quick check question: Why might forecasts at different aggregation levels conflict without reconciliation?

## Architecture Onboarding

- Component map: Data preprocessing → Trend-seasonality decomposition → LightGBM training (stationary residuals) → Prophet modeling (trend/seasonality) → Forecast combination → WRMSSE evaluation
- Critical path: Data preprocessing → Decomposition → LightGBM training → Prophet modeling → Forecast combination
- Design tradeoffs: Separating trend/seasonality improves accuracy but adds complexity; Tweedie loss handles zeros better but requires parameter tuning
- Failure signatures: Poor decomposition indicates model assumptions violated; high WRMSSE suggests decomposition or combination strategy issues
- First 3 experiments:
  1. Compare Tweedie vs MSE loss on subset with heavy zero-inflation
  2. Test decomposition strategy on series with known trend vs seasonality dominance
  3. Validate hierarchical forecast coherence across aggregation levels

## Open Questions the Paper Calls Out
No specific open questions were called out in the paper.

## Limitations
- Decomposition-based grouping assumes clean separation of trend and seasonality, but may fail when components are strongly intertwined
- Exact transformation method for achieving stationarity in LightGBM inputs is not specified
- Hyperparameter selection for LightGBM and Prophet is not thoroughly explored

## Confidence
- High confidence: The hierarchical time series framework and WRMSSE metric are well-established in retail forecasting competitions. The empirical results showing improvement over baseline models (0.614 vs 0.802-1.120) are robust and clearly documented.
- Medium confidence: The decomposition strategy and Tweedie loss benefits are theoretically sound but lack extensive validation across different retail domains or data characteristics.
- Low confidence: The exact transformation method for achieving stationarity in LightGBM inputs and the specific hyperparameter choices remain unspecified, limiting reproducibility.

## Next Checks
1. Test the decomposition strategy on time series with varying degrees of trend-seasonality correlation to assess robustness when components are not cleanly separable.
2. Conduct ablation studies comparing Tweedie vs MSE loss across different zero-inflation levels to quantify when Tweedie provides tangible benefits.
3. Validate the hierarchical coherence assumption by checking forecast consistency across aggregation levels when higher-level trends conflict with lower-level patterns.