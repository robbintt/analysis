---
ver: rpa2
title: A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning
arxiv_id: '2311.05877'
source_url: https://arxiv.org/abs/2311.05877
tags:
- features
- lasso
- feature
- selection
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of feature selection for deep
  tabular learning models. It introduces a new benchmark using real datasets with
  various types of extraneous features and proposes Deep Lasso, an input-gradient-based
  feature selection method for neural networks.
---

# A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning

## Quick Facts
- arXiv ID: 2311.05877
- Source URL: https://arxiv.org/abs/2311.05877
- Reference count: 40
- Key outcome: Deep Lasso outperforms classical feature selection methods on corrupted and second-order features

## Executive Summary
This paper addresses the challenge of feature selection for deep tabular learning models by introducing a new benchmark and a novel method called Deep Lasso. The benchmark uses real-world datasets with controlled extraneous features, including random noise, corrupted features, and second-order features. Deep Lasso leverages input-gradient sparsity regularization to identify important features, showing superior performance compared to classical methods, especially in challenging scenarios involving corrupted or derived features.

## Method Summary
The study evaluates feature selection methods on real-world tabular datasets by introducing three types of extraneous features: random noise features, corrupted features (with Gaussian or Laplace noise), and second-order features (products of original features). Feature selection methods include classical approaches (Lasso, Random Forest, XGBoost) and the proposed Deep Lasso, which applies Group Lasso penalty to gradients of the loss with respect to input features. The selected features are then used to train downstream models (MLP and FT-Transformer), and performance is evaluated using accuracy, RMSE, ROC-AUC, and precision scores.

## Key Results
- Deep Lasso outperforms classical feature selection methods on corrupted and second-order feature types
- Transformer-based models show greater robustness to random noise features compared to MLP models
- Tree-based methods perform well on random and corrupted features but struggle with second-order features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Lasso outperforms other feature selection methods on corrupted and second-order feature types
- Mechanism: Deep Lasso uses input-gradient sparsity regularization to make the model robust to changes in uninformative features
- Core assumption: Gradient magnitude correlates with feature importance, and sparsity in gradients implies robustness
- Evidence anchors:
  - [abstract] "Deep Lasso outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features"
  - [section] "Deep Lasso encourages feature gradient sparsity for deep tabular models by applying a Group Lasso penalty to gradients of the loss with respect to input features during training"
  - [corpus] Weak - no direct comparison in neighbor papers
- Break condition: If gradient-based importance signals are corrupted by noisy gradients or flat loss landscapes

### Mechanism 2
- Claim: Transformer-based models are more robust to random noise features than MLP models
- Mechanism: Attention mechanisms can filter out uninformative features through self-attention weights
- Core assumption: Attention maps provide meaningful feature importance scores that can distinguish signal from noise
- Evidence anchors:
  - [section] "the FT-Transformer model is roughly as robust to noisy features as the XGBoost model" and "the ability of the transformer architecture to filter out uninformative features through its attention mechanism"
  - [corpus] Weak - neighbor papers don't directly compare attention-based filtering
- Break condition: If attention maps become uniform or random when features are highly correlated or when model capacity is insufficient

### Mechanism 3
- Claim: Tree-based methods (RF, XGBoost) perform well on random and corrupted features but struggle with second-order features
- Mechanism: Tree-based importance measures capture non-linear relationships but may not distinguish between original and derived features
- Core assumption: Feature importance based on impurity reduction captures all relevant information for downstream tasks
- Evidence anchors:
  - [section] "Random Forest and XGBoost rankings exhibit high correlation" and "Tree-based methods perform competitively in the random and corrupted setups"
  - [corpus] Weak - neighbor papers focus on different selection methods without direct tree-based comparison
- Break condition: If feature interactions are too complex for tree-based impurity measures to capture effectively

## Foundational Learning

- Concept: Gradient-based feature importance
  - Why needed here: Deep Lasso relies on input gradients to identify important features
  - Quick check question: What is the relationship between input gradients and feature importance in neural networks?

- Concept: Attention mechanisms in transformers
  - Why needed here: FT-Transformer uses attention maps for feature selection
  - Quick check question: How do self-attention weights indicate feature relevance in tabular transformers?

- Concept: Regularization and sparsity
  - Why needed here: Lasso variants use L1/group sparsity to select features
  - Quick check question: Why does L1 regularization encourage feature selection while L2 does not?

## Architecture Onboarding

- Component map:
  Feature selection module -> Downstream model (MLP or FT-Transformer) -> Benchmark evaluation pipeline -> Hyperparameter optimization system

- Critical path:
  1. Generate dataset with extraneous features
  2. Apply feature selection method
  3. Train downstream model on selected features
  4. Evaluate performance and rank methods

- Design tradeoffs:
  - Deep Lasso vs classical methods: More computationally intensive but better on challenging feature types
  - MLP vs FT-Transformer: MLPs more prone to overfitting but simpler; transformers more robust but computationally heavier
  - Random vs corrupted vs second-order features: Increasing difficulty for feature selection

- Failure signatures:
  - Deep Lasso fails: Gradients become noisy or uninformative, regularization too strong/weak
  - Transformer attention fails: Attention maps become uniform, features too correlated
  - Classical methods fail: Feature interactions too complex for linear/group methods

- First 3 experiments:
  1. Compare Deep Lasso vs XGBoost on random features (baseline)
  2. Test attention map importance on corrupted features
  3. Evaluate Deep Lasso on second-order features (most challenging)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Deep Lasso perform on truly large-scale industrial tabular datasets with billions of samples compared to its performance on the datasets used in this study?
- Basis in paper: [inferred] The paper acknowledges that while the introduced datasets are sizeable, they are still smaller than truly large-scale industrial tabular data with billions of samples.
- Why unresolved: The study does not include datasets of this scale, so the performance of Deep Lasso in such scenarios is unknown.
- What evidence would resolve it: Experiments comparing Deep Lasso's performance on industrial-scale datasets versus the benchmark datasets used in this study would provide the necessary evidence.

### Open Question 2
- Question: Is there a way to make Deep Lasso computationally less demanding than GBDT-based feature selection methods, especially for large-scale hyperparameter optimization?
- Basis in paper: [explicit] The paper mentions that Deep Lasso is more computationally demanding than using GBDT-based feature selection methods, especially when large-scale hyperparameter optimization is performed.
- Why unresolved: The paper does not propose any solutions to reduce the computational demands of Deep Lasso.
- What evidence would resolve it: Development and testing of optimized Deep Lasso algorithms or approximations that reduce computational complexity while maintaining performance would resolve this question.

### Open Question 3
- Question: How does the feature selection performance of Deep Lasso compare to other methods when applied to datasets with categorical features, given that the current benchmark only includes numerical features?
- Basis in paper: [inferred] The paper focuses on datasets with numerical features and does not explore the performance of Deep Lasso on datasets with categorical features.
- Why unresolved: The absence of categorical features in the benchmark datasets means the performance of Deep Lasso in such scenarios is not evaluated.
- What evidence would resolve it: Experiments applying Deep Lasso and other feature selection methods to datasets with categorical features would provide the necessary evidence.

## Limitations

- The benchmark framework relies on synthetic feature corruption methods that may not fully capture real-world feature selection challenges
- Performance gains of Deep Lasso show diminishing returns in certain dataset-model combinations
- Computational cost of Deep Lasso implementation remains a concern for large-scale deployment, though not explicitly quantified

## Confidence

- Deep Lasso outperforms classical methods on challenging feature types: **High confidence**
- Transformer models are more robust to noise than MLPs: **Medium confidence**
- Tree-based methods struggle with second-order features: **Medium confidence**

## Next Checks

1. Implement ablation studies removing the group lasso penalty from Deep Lasso to quantify its contribution to performance improvements
2. Test the benchmark framework on additional real-world datasets with known feature redundancy to validate generalizability
3. Measure and compare the computational overhead of Deep Lasso versus classical methods across different dataset sizes to establish practical deployment constraints