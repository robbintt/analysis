---
ver: rpa2
title: Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing
  Examination With and Without Distractions
arxiv_id: '2309.08625'
source_url: https://arxiv.org/abs/2309.08625
tags:
- medical
- small
- talk
- questions
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of small talk on the accuracy
  of medical advice provided by ChatGPT-3.5 and ChatGPT-4 using USMLE Step 3 questions.
  Small talk sentences were collected from Mechanical Turk workers and interspersed
  with the medical questions.
---

# Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions

## Quick Facts
- arXiv ID: 2309.08625
- Source URL: https://arxiv.org/abs/2309.08625
- Reference count: 40
- ChatGPT-4 achieved 75.4% correct answers on USMLE Step 3 questions, outperforming ChatGPT-3.5's 61.7%

## Executive Summary
This study investigates how small talk affects the accuracy of medical advice from ChatGPT-3.5 and ChatGPT-4 using USMLE Step 3 questions. Small talk sentences were interspersed with medical questions, and a board-certified physician evaluated responses against official answers. ChatGPT-4 significantly outperformed ChatGPT-3.5 overall, maintaining consistent accuracy regardless of small talk presence. ChatGPT-3.5 showed notable performance degradation with small talk, particularly for open-ended questions. These findings suggest ChatGPT-4 is more robust to distractions and could be valuable for processing medical information in real-world physician-patient interactions.

## Method Summary
The study used 122 multiple-choice and 122 open-ended USMLE Step 3 questions, along with 143 small talk sentences collected from Mechanical Turk workers. Each medical question sentence was alternated with a small talk sentence to create distractor conditions. Both ChatGPT-3.5 and ChatGPT-4 were queried with these questions, with and without small talk. A board-certified physician evaluated all responses against official USMLE answers to determine accuracy percentages for comparison.

## Key Results
- ChatGPT-4 achieved 75.4% correct answers overall versus ChatGPT-3.5's 61.7%
- ChatGPT-3.5's performance dropped significantly from 66.8% to 56.6% when small talk was added
- ChatGPT-4 maintained consistent performance regardless of small talk (67.2% and 83.6% for open and multiple-choice questions respectively)

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT-4 maintains performance consistency in the presence of irrelevant small talk, whereas ChatGPT-3.5's performance degrades significantly. This occurs because ChatGPT-4 has a more robust attention mechanism and better context discrimination, allowing it to focus on relevant medical information despite distractors. The improved architecture of ChatGPT-4 includes better handling of irrelevant input without being misled by it.

### Mechanism 2
The presence of small talk introduces additional cognitive load that affects ChatGPT-3.5's ability to process medical information accurately. Small talk sentences interspersed with medical questions increase input complexity, causing ChatGPT-3.5 to misinterpret or overlook critical medical information. ChatGPT-3.5's architecture is more sensitive to input complexity and is more easily distracted by irrelevant information.

### Mechanism 3
ChatGPT-4's architecture includes better generalization from training data, allowing it to handle mixed content (medical and non-medical) more effectively. ChatGPT-4 was trained on a more diverse dataset that included various types of conversational contexts, enabling it to distinguish between relevant and irrelevant information better. The training data for ChatGPT-4 included examples of mixed content similar to the small talk and medical information used in the study.

## Foundational Learning

- **Attention mechanisms in neural networks**: Understanding how models focus on relevant parts of input data is crucial for grasping why ChatGPT-4 outperforms ChatGPT-3.5. *Quick check*: How do attention mechanisms help models distinguish between relevant and irrelevant information in a sequence of inputs?

- **Context windows and input complexity**: The study's design involves interspersing small talk with medical questions, increasing input complexity. Understanding how models handle context is essential. *Quick check*: What is the role of context windows in determining how much information a model can effectively process at once?

- **Generalization in machine learning**: ChatGPT-4's ability to handle small talk without performance degradation suggests better generalization from its training data. *Quick check*: How does the diversity of training data influence a model's ability to generalize to new, unseen scenarios?

## Architecture Onboarding

- **Component map**: Input processing layer -> Attention mechanism (multi-head self-attention) -> Feed-forward neural networks -> Output layer -> Context handling module (specific to ChatGPT-4)

- **Critical path**: 
  1. Input tokenization and embedding
  2. Multi-head self-attention processing
  3. Feed-forward neural network application
  4. Output generation
  5. Context discrimination and relevance filtering

- **Design tradeoffs**: Model size vs. computational efficiency; Context window size vs. ability to handle long sequences; Training data diversity vs. potential for overfitting

- **Failure signatures**: Decreased accuracy when irrelevant information is introduced (ChatGPT-3.5); Inability to distinguish between relevant and irrelevant content; Over-reliance on specific keywords or phrases in the input

- **First 3 experiments**:
  1. Test ChatGPT-3.5 and ChatGPT-4 with increasing amounts of small talk to quantify the degradation in performance
  2. Analyze the attention weights of both models to understand how they focus on relevant vs. irrelevant information
  3. Train a simplified model with controlled amounts of mixed content to observe the effects on performance and generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ChatGPT-4 compare to other state-of-the-art large language models (e.g., BERT, Cloude, LLAMA-1, LLAMA-2) when processing medical data with and without small talk? This remains unresolved because the study only focused on ChatGPT models without assessing different LLMs.

### Open Question 2
How does the structure and length of small talk integration affect ChatGPT's ability to provide accurate medical recommendations? This remains unresolved because the study used a specific pattern of small talk integration, so the impact of different structures and lengths is unknown.

### Open Question 3
How does the inclusion of non-textual information, such as images or sounds, affect ChatGPT's ability to process medical data and provide accurate recommendations? This remains unresolved because the study only focused on text-based information, so the impact of non-textual information is unknown.

## Limitations

- The study relies on a single board-certified physician's evaluation, introducing potential subjectivity in scoring
- Small talk sentences were collected from Mechanical Turk workers without standardization
- The study uses a fixed dataset of USMLE Step 3 questions, limiting generalizability to other medical domains
- Performance differences could be influenced by factors beyond architecture, such as fine-tuning or temperature settings

## Confidence

**High Confidence Claims:**
- ChatGPT-4 demonstrates higher overall accuracy than ChatGPT-3.5 on USMLE Step 3 questions (75.4% vs. 61.7%)
- ChatGPT-3.5 shows significant performance degradation when small talk is added (from 66.8% to 56.6%)
- ChatGPT-4 maintains consistent performance regardless of small talk presence

**Medium Confidence Claims:**
- The observed differences are primarily due to architectural improvements in ChatGPT-4's attention mechanisms
- Small talk impairs ChatGPT-3.5's ability to process medical information more than it affects ChatGPT-4

## Next Checks

1. Replicate the study with a larger, more diverse set of medical questions and multiple physician evaluators to confirm the observed performance differences and their robustness to evaluator bias.

2. Conduct ablation studies to isolate the specific architectural components (e.g., attention mechanisms, context handling) responsible for ChatGPT-4's improved performance with distractors.

3. Test both models with varying amounts and types of small talk (e.g., longer sentences, different topics) to quantify the threshold at which each model's performance begins to degrade.