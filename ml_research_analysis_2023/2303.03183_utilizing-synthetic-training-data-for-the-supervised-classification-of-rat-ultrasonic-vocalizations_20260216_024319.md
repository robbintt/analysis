---
ver: rpa2
title: Utilizing synthetic training data for the supervised classification of rat
  ultrasonic vocalizations
arxiv_id: '2303.03183'
source_url: https://arxiv.org/abs/2303.03183
tags:
- classification
- calls
- vocalmat
- training
- usvs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared two convolutional neural networks, DeepSqueak
  and VocalMat, for detecting and classifying rat ultrasonic vocalizations (USVs).
  VocalMat outperformed DeepSqueak in both detection and classification tasks.
---

# Utilizing synthetic training data for the supervised classification of rat ultrasonic vocalizations

## Quick Facts
- arXiv ID: 2303.03183
- Source URL: https://arxiv.org/abs/2303.03183
- Authors: 
- Reference count: 34
- Primary result: VocalMat with synthetic augmentation achieved 82.6% classification accuracy, close to human-level performance

## Executive Summary
This study compared two convolutional neural networks, DeepSqueak and VocalMat, for detecting and classifying rat ultrasonic vocalizations (USVs). VocalMat outperformed DeepSqueak in both detection and classification tasks. To reduce manual effort in generating training data, synthetic USVs were added to VocalMat's training set, resulting in a significant improvement in classification accuracy from 71.3% to 82.6%. The augmented training set, combining natural and synthetic data, achieved performance close to human-level accuracy, making it suitable for practical laboratory use. This approach offers an efficient method to enhance USV classification in rat communication research.

## Method Summary
The study used 68 adult male Sprague-Dawley rats recorded in pairs, with training data consisting of 498 manually labeled USVs from 7 recordings. DeepSqueak was used with the Wright-style classification schema, while VocalMat was adapted from mouse to rat USVs. Synthetic USVs were generated by morphing natural call spectrograms using MATLAB's morphimage routine. The networks were trained on natural images first, then iteratively augmented with synthetic calls until reaching 1,206 total calls. Performance was evaluated using d-prime scores for detection and accuracy comparison for classification against human-labeled ground truth.

## Key Results
- VocalMat outperformed DeepSqueak in USV detection with higher d-prime scores
- Classification accuracy improved from 71.3% to 82.6% when synthetic USVs were added to training
- The augmented training set achieved performance close to human-level accuracy
- Simplified classification schema (modified Wright et al. 2010) maintained sufficient taxonomic detail while improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding synthetic USV images to the training set significantly improves classification accuracy by increasing dataset size and variability without manual annotation cost.
- Mechanism: Synthetic images are generated by morphing existing natural call spectrograms, preserving core spectral-temporal patterns while introducing controlled variability. This expanded dataset exposes the CNN to a broader range of call appearances, reducing overfitting and improving generalization to unseen data.
- Core assumption: Synthetic calls must remain visually and structurally similar to natural calls to be valid training examples.
- Evidence anchors:
  - [abstract]: "The augmentation of training data with synthetic images resulted in a further improvement in accuracy, such that it was sufficiently close to human performance..."
  - [section]: "We found that accuracy on both training-set validation, and classification of novel recordings was significantly improved with the introduction of synthetic calls to the training set."
  - [corpus]: Weak; no corpus papers directly discuss synthetic USV generation, but related work in traffic sign recognition and defect classification uses similar synthetic augmentation strategies.
- Break condition: If synthetic morphs deviate too far from natural call morphology, the network will learn incorrect features and accuracy will drop.

### Mechanism 2
- Claim: VocalMat's image processing pipeline (contrast enhancement, morphological operations, local median filtering) improves USV detection in noisy recordings compared to DeepSqueak.
- Mechanism: Preprocessing normalizes spectrogram contrast and removes background noise artifacts before CNN inference, allowing better separation of USV calls from noise. Local median filtering estimates the lowest contrast threshold per recording, adapting detection sensitivity dynamically.
- Core assumption: The preprocessing steps preserve USV call structure while suppressing noise without losing true signals.
- Evidence anchors:
  - [abstract]: "VocalMat outperformed the DeepSqueak CNN on measures of call identification..."
  - [section]: "VocalMat employing image processing techniques and morphological operations to improve spectrogram images to better discriminate USVs from background noise."
  - [corpus]: Weak; related papers focus on GAN-based noise generation for defect classification, not spectrogram preprocessing.
- Break condition: If preprocessing is too aggressive, true calls may be suppressed; if too lenient, noise dominates detection.

### Mechanism 3
- Claim: Using a simplified Wright et al. (2010) classification schema reduces ambiguity and improves classification accuracy over the full schema.
- Mechanism: Fewer, more clearly differentiated call categories reduce overlap in feature space, making CNN learning more robust. Combining similar call types (e.g., multi-step + split) into single categories simplifies decision boundaries.
- Core assumption: The simplified schema preserves enough taxonomic detail for meaningful biological interpretation while reducing classification difficulty.
- Evidence anchors:
  - [abstract]: "These call categories represent a simplified version of the classification scheme originally proposed by Wright and colleagues (2010)."
  - [section]: "Our findings that classification performance using a modified version of Wright et al. (2010) classifiers can be quite high may help address these concerns."
  - [corpus]: Weak; no corpus papers discuss Wright schema simplification, but related literature on simpler classification (e.g., Brudzynski 2015) supports the approach.
- Break condition: Oversimplification may eliminate behaviorally meaningful distinctions, reducing scientific utility.

## Foundational Learning

- Concept: Convolutional neural network architecture and training workflow.
  - Why needed here: Understanding CNN layers, data augmentation, and training/validation loops is essential for implementing and debugging VocalMat.
  - Quick check question: What is the role of convolutional layers in feature extraction from spectrogram images?

- Concept: Signal detection theory (d-prime metric).
  - Why needed here: Evaluating detection performance requires computing hit rates, false alarm rates, and d-prime scores to compare models objectively.
  - Quick check question: How does d-prime quantify the discriminability between signal and noise?

- Concept: Data augmentation via morphing and synthetic generation.
  - Why needed here: Generating synthetic USV images requires understanding image morphing techniques and quality control to maintain biological validity.
  - Quick check question: What criteria ensure a morphed spectrogram remains a valid USV call?

## Architecture Onboarding

- Component map: Input preprocessing → CNN detection → Post-processing → Classification → Evaluation
- Critical path: Spectrogram generation → Preprocessing → Detection CNN → Call segmentation → Classification CNN → Output labeling
- Design tradeoffs: Simplified schema vs. full taxonomy; small training set with synthetic augmentation vs. large manual dataset; preprocessing overhead vs. detection accuracy
- Failure signatures: Low d-prime in noisy conditions (detection failure); high validation accuracy but low generalization (overfitting); synthetic calls too dissimilar (model confusion)
- First 3 experiments:
  1. Compare d-prime scores of VocalMat vs. DeepSqueak on a low-noise and high-noise recording
  2. Train VocalMat on natural calls only, then on natural + synthetic calls; measure validation and generalization accuracy
  3. Vary the proportion of synthetic calls in training; plot accuracy vs. synthetic percentage to find optimal augmentation level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would using a larger training dataset significantly improve the classification accuracy of VocalMat for rat USVs?
- Basis in paper: [inferred] The paper notes that their training set of ~500 calls is relatively small compared to the ~40,000 calls used in previous DeepSqueak research, and suggests that larger datasets might yield better results.
- Why unresolved: The study only tested a small training dataset and achieved similar accuracy to previous mouse USV research. The effect of dataset size on performance was not directly tested.
- What evidence would resolve it: Systematic testing of VocalMat's performance using training sets of varying sizes (e.g., 500, 5,000, 40,000+ calls) while maintaining consistent validation and generalization procedures.

### Open Question 2
- Question: How does signal-to-noise ratio affect the classification difficulty for both human observers and CNNs like VocalMat and DeepSqueak?
- Basis in paper: [explicit] The paper mentions that their generalization tests used recordings with lower signal strength, but did not investigate whether CNNs find classification more difficult under different signal-to-noise conditions.
- Why unresolved: The study did not systematically vary the signal-to-noise ratio across recordings to test its effect on classification performance for both human and machine classifiers.
- What evidence would resolve it: Direct comparison of classification accuracy across recordings with systematically varied signal-to-noise ratios, using both human observers and CNNs as classifiers.

### Open Question 3
- Question: Are the call categories defined by human classification schemes (like the modified Wright schema) actually meaningful to rats themselves?
- Basis in paper: [explicit] The discussion section explicitly raises this question, noting that while humans and CNNs can categorize calls, it's unknown whether rats interpret these calls as distinct categories.
- Why unresolved: This is a fundamental question about the biological significance of human-defined call categories that requires behavioral or neurophysiological evidence from the animals themselves.
- What evidence would resolve it: Experimental evidence showing that rats respond differently to different call categories in a way that matches human classification, or neural recordings demonstrating distinct processing of different call types in rat auditory systems.

## Limitations

- Synthetic augmentation relies heavily on manual quality control during image morphing, which is not fully automated or reproducible
- Simplified classification schema may lose behavioral granularity important for some research questions
- Cross-laboratory validation is not demonstrated, raising questions about generalizability to different recording conditions or rat strains

## Confidence

- **High confidence**: VocalMat outperforms DeepSqueak in detection accuracy (d-prime scores), and synthetic augmentation improves classification accuracy from 71.3% to 82.6%
- **Medium confidence**: The simplified classification schema preserves sufficient taxonomic detail for meaningful biological interpretation
- **Low confidence**: The exact parameters for synthetic USV generation and quality control thresholds are reproducible without additional experimentation

## Next Checks

1. Test VocalMat's performance across multiple laboratories using different recording equipment and rat strains to assess generalizability
2. Systematically vary the proportion of synthetic calls in training (0%, 25%, 50%, 75%, 100%) to identify the optimal augmentation level and potential overfitting thresholds
3. Compare classification accuracy using the simplified schema versus the full Wright taxonomy on the same dataset to quantify information loss from schema reduction