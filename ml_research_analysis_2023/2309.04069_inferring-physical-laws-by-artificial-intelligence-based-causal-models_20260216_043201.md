---
ver: rpa2
title: Inferring physical laws by artificial intelligence based causal models
arxiv_id: '2309.04069'
source_url: https://arxiv.org/abs/2309.04069
tags:
- causal
- data
- effect
- estimate
- treatment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies causal inference methods to physical phenomena
  to uncover cause-and-effect relationships beyond correlation. Using Judea Pearl's
  framework of association, intervention, and counterfactuals, the authors employ
  causal discovery algorithms and causal inference techniques to analyze data from
  tidal heights, Ohm's law, light-dependent resistance, and quantum entanglement.
---

# Inferring physical laws by artificial intelligence based causal models

## Quick Facts
- arXiv ID: 2309.04069
- Source URL: https://arxiv.org/abs/2309.04069
- Reference count: 0
- Key outcome: This paper applies causal inference methods to physical phenomena to uncover cause-and-effect relationships beyond correlation, correctly identifying primary drivers in tides, Ohm's law, LDR experiments, and quantum entanglement.

## Executive Summary
This paper demonstrates how causal inference methods can be applied to physical phenomena to uncover true cause-and-effect relationships beyond mere correlation. Using Judea Pearl's framework of association, intervention, and counterfactuals, the authors employ causal discovery algorithms and causal inference techniques to analyze data from tidal heights, Ohm's law, light-dependent resistance, and quantum entanglement. The results show that causal inference can strengthen or weaken confidence in proposed physical models by distinguishing direct causation from mere correlation.

## Method Summary
The authors use Judea Pearl's causal inference framework to analyze physical phenomena through a four-step process: (1) model the causal structure using either domain knowledge or causal discovery algorithms to create a directed acyclic graph, (2) identify causal effects using appropriate criteria like backdoor, frontdoor, instrumental variables, or mediation, (3) estimate causal effects using statistical methods such as linear regression or propensity score matching, and (4) refute results using robustness checks including placebo treatment refuters and random common cause tests. The DoWhy library is used for causal inference, while CDT (Causal Discovery Toolbox) provides causal discovery algorithms like LiNGAM.

## Key Results
- Correctly identified Earth-Moon distance as the primary driver of tide height with an ATE of -2913.16
- Confirmed potential as the main cause of current in Ohm's law (ATE 1.735) and refuted incorrect models suggesting direct temperature effects
- Validated that voltage drives current (ATE 15.41) and power (ATE 251.53) in LDR experiments
- Demonstrated that entanglement, not measurement outcomes themselves, is the true cause of measurement correlations in quantum entanglement (ATE 0.3733)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal inference identifies true cause-effect relationships by estimating the effect of interventions, not just correlations.
- **Mechanism:** The framework uses Pearl's ladder of causation (association → intervention → counterfactuals). Instead of finding P(Y|X) which measures correlation, it computes P(Y|do(X)) which estimates the effect of actively setting X. This distinguishes between "X is correlated with Y" and "X causes Y."
- **Core assumption:** The causal graph structure is either known from domain knowledge or discoverable from data using algorithms like LiNGAM, PC, or GES, and satisfies sufficiency and faithfulness conditions.
- **Evidence anchors:** [abstract] "Causal inference takes us a step above on the ladder of causation and lets us answer such questions... 'Had the weight on this spring doubled, its length would have doubled as well' (Hooke's law)." [section II] "The basic idea behind causal inference is to estimate the effect of treatment X on the outcome Y while eliminating the dependence on any variable Z that has a direct influence on both the treatment and the outcome (confounding variables)."

### Mechanism 2
- **Claim:** Different causal identification criteria (backdoor, frontdoor, instrumental variables, mediation) allow valid effect estimation even when direct paths are confounded or unobserved.
- **Mechanism:** When a direct causal path X→Y is blocked by confounders (Z), the backdoor criterion adjusts by controlling for Z. If Z is unobserved but a suitable instrument W exists (independent of Z, affects X, doesn't directly affect Y), instrumental variables can identify the causal effect. Frontdoor handles mediation through unobserved pathways.
- **Core assumption:** The causal structure allows for at least one valid identification criterion to be applied given the available variables.
- **Evidence anchors:** [section II] "Many methods exist for the estimation of the effect that is produced by doing X... These include observational studies... instrument variables (specific causal effect estimation criteria) and refutations." [section II] "a: Back-door: Controlling for the set of variables that block all the back-door paths between the treatment and the outcome."

### Mechanism 3
- **Claim:** Refutation tests increase confidence in causal models by checking robustness to placebo treatments, data subsets, and random common causes.
- **Mechanism:** After estimating a causal effect, the model is stress-tested. A placebo treatment refuter replaces the real treatment with an independent random variable - if the model is correct, the estimated effect should drop to zero. Random common cause adds spurious confounders to see if estimates vary significantly.
- **Core assumption:** The causal model's assumptions (like unconfoundedness) are testable through these robustness checks.
- **Evidence anchors:** [section II] "4. Refute the obtained estimate using multiple robustness checks: Causal models are not absolute... One can however, increase faith in a model by checking the validity of the assumptions behind the model against various robustness checks which include: a: Random Common Cause... b: Placebo Treatment Refuter..."

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs) for representing causal structures
  - Why needed here: The entire causal inference framework relies on DAGs to encode assumptions about which variables cause which others. Without understanding DAGs, one cannot interpret or construct causal models.
  - Quick check question: In a DAG where A→B→C, if we control for B, what happens to the correlation between A and C?

- **Concept:** Conditional independence and d-separation
  - Why needed here: Causal discovery algorithms test for conditional independence to infer the DAG structure. Understanding d-separation is crucial for applying identification criteria like backdoor and frontdoor.
  - Quick check question: If two variables are d-separated given a set of variables, what does that imply about their conditional independence?

- **Concept:** Average Treatment Effect (ATE) and other causal estimands
  - Why needed here: Causal inference outputs effects in terms of ATE, ATT, or ATC. Understanding what these mean and how they differ is essential for interpreting results correctly.
  - Quick check question: If a treatment is applied to only 10% of the population, would ATE or ATT be more representative of the treatment effect?

## Architecture Onboarding

- **Component map:** Data preprocessing → Causal discovery (CDT/LiNGAM) OR manual DAG construction → Causal model specification (DoWhy) → Effect identification (backdoor/frontdoor/IV/mediation) → Effect estimation (statistical method) → Refutation testing → Result interpretation
- **Critical path:** The sequence from obtaining/constructing the causal graph to estimating and refuting the effect. Any failure in the DAG structure or identification step breaks the pipeline.
- **Design tradeoffs:** Domain knowledge DAG vs. data-driven discovery (domain knowledge is more interpretable but data-driven can reveal unexpected relationships); choice of identification criterion (backdoor requires controlling confounders, IV requires valid instruments); estimation method (linear regression is simple but may miss nonlinearities, matching methods are more robust but computationally intensive).
- **Failure signatures:** Identification step returns "unidentifiable" (no valid criterion exists for the query given available variables); refutation p-values near 0 (model assumptions are violated); extremely large or small ATE estimates (model misspecification or data issues); DAG from data shows no edges when domain knowledge suggests strong relationships (weak signals or need for different discovery algorithm).
- **First 3 experiments:** (1) Reproduce Ohm's law example: Generate synthetic data using V=IR and R=ρL/A, then use DoWhy to confirm V→I as the primary causal path and refute direct T→I; (2) Test tide height analysis: Use Earth-Sun and Earth-Moon distance data with tide heights, compare LiNGAM-discovered DAG vs. domain knowledge DAG, and verify ATE estimates; (3) Run LDR experiment refutation: Create model with and without P→R path, use placebo treatment refuter to compare confidence levels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can causal discovery algorithms be improved to handle cases where variables have zero correlation but are causally linked?
- **Basis in paper:** [explicit] The paper discusses the quantum entanglement example where causal discovery initially failed to reveal relations between variables due to zero correlation, requiring the use of absolute correlation values instead.
- **Why unresolved:** Current causal discovery algorithms rely heavily on statistical independence tests, which break down when variables have zero correlation but are still causally connected through complex relationships.
- **What evidence would resolve it:** Development and testing of causal discovery algorithms that can identify causal relationships even when statistical independence tests fail, potentially through incorporation of domain knowledge or alternative statistical measures.

### Open Question 2
- **Question:** What are the limitations of using causal inference to validate physical models, and how can these limitations be addressed?
- **Basis in paper:** [explicit] The paper discusses using causal inference to validate Ohm's law by refuting incorrect models suggesting direct temperature effects on current, but notes that causal models cannot be proven absolutely correct or incorrect.
- **Why unresolved:** While causal inference can strengthen or weaken confidence in physical models, it cannot provide absolute proof, and there may be scenarios where causal inference alone is insufficient to validate complex physical relationships.
- **What evidence would resolve it:** Comparative studies showing the effectiveness of causal inference validation against traditional experimental methods, and development of hybrid approaches combining causal inference with other validation techniques.

### Open Question 3
- **Question:** How can causal inference be integrated with machine learning techniques for automated scientific discovery?
- **Basis in paper:** [explicit] The authors mention the potential of incorporating causal inference into machine learning applications like symbolic regression for discovering expressions/equations from data alone.
- **Why unresolved:** While the paper demonstrates the potential of causal inference in understanding physical phenomena, the integration of causal inference with advanced machine learning techniques for automated scientific discovery remains largely unexplored.
- **What evidence would resolve it:** Development and testing of machine learning models that incorporate causal inference for automated scientific discovery, with performance comparisons against traditional data-driven approaches.

## Limitations
- Data quality and availability for physical phenomena, particularly tide height measurements across different locations and quantum entanglement datasets
- Causal discovery algorithms assume sufficiency and faithfulness conditions that may not hold in real-world physics systems with complex interactions
- Validation relies heavily on known physical laws rather than independent experimental verification of causal claims

## Confidence
- **High Confidence:** The causal inference framework and its application to Ohm's law and LDR experiments are well-grounded with clear ATE estimates and successful refutation tests
- **Medium Confidence:** The tide height analysis showing Earth-Moon distance as primary driver, as this relies on observational data that may contain unmeasured confounders
- **Low Confidence:** The quantum entanglement interpretation that entanglement itself is the cause rather than measurement outcomes, as this extends beyond established physics interpretations

## Next Checks
1. **Data Sensitivity Analysis:** Test how the causal estimates vary with different subsets of the tide height data across locations and time periods
2. **Alternative Discovery Algorithms:** Compare LiNGAM results with other causal discovery methods (PC, GES) on the Ohm's law dataset to check robustness of the identified causal structure
3. **Instrumental Variable Test:** For the LDR experiment, identify potential instrumental variables to validate the voltage→current causal path, particularly for the power→resistance relationship that showed lower confidence