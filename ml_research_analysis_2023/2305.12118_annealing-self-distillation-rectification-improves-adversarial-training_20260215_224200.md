---
ver: rpa2
title: Annealing Self-Distillation Rectification Improves Adversarial Training
arxiv_id: '2305.12118'
source_url: https://arxiv.org/abs/2305.12118
tags:
- robust
- adversarial
- training
- robustness
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper analyzes robust models\u2019 output properties, finding\
  \ they are better calibrated and more consistent under adversarial attacks. It proposes\
  \ Annealing Self-Distillation Rectification (ADR), which uses a momentum-updated\
  \ teacher model to generate soft, noise-aware labels that reflect distribution shifts\
  \ under attack."
---

# Annealing Self-Distillation Rectification Improves Adversarial Training

## Quick Facts
- arXiv ID: 2305.12118
- Source URL: https://arxiv.org/abs/2305.12118
- Reference count: 40
- Key outcome: ADR reduces robust overfitting and improves adversarial accuracy by 0.92–1.92% over baselines on CIFAR-10, CIFAR-100, and TinyImageNet-200.

## Executive Summary
This paper introduces Annealing Self-Distillation Rectification (ADR), a method that improves adversarial training by generating noise-aware labels using a momentum-updated teacher model. ADR addresses robust overfitting by interpolating softened teacher outputs with one-hot ground truth labels, allowing the model to better reflect distribution shifts under adversarial attack. Experiments demonstrate ADR improves robust accuracy by 0.92–1.92% over standard baselines and achieves state-of-the-art results when combined with weight-space smoothing techniques.

## Method Summary
ADR uses an EMA-updated teacher model to generate soft, noise-aware labels that replace one-hot labels in adversarial training. The teacher's output is temperature-scaled and interpolated with the ground truth label using a progressively increasing factor λ. This creates labels that reflect how adversarial perturbations shift the data distribution. The method integrates easily with existing adversarial training frameworks and is evaluated on CIFAR-10, CIFAR-100, and TinyImageNet-200 using ResNet and Wide ResNet architectures.

## Key Results
- ADR reduces robust overfitting and improves adversarial accuracy by 0.92–1.92% over standard baselines
- ADR achieves state-of-the-art robustness without requiring extra training data
- Combining ADR with weight-space smoothing techniques yields further improvements in both robust and clean accuracy

## Why This Works (Mechanism)

### Mechanism 1
ADR generates noise-aware labels by interpolating the EMA teacher's softened output with the ground-truth one-hot vector. The teacher's output is passed through a temperature-scaled softmax to produce a softened distribution. An interpolation factor λ, which increases during training, blends this distribution with the original one-hot label. This creates labels that reflect distribution shifts under adversarial perturbation while ensuring the true class retains the highest probability.

### Mechanism 2
ADR alleviates robust overfitting by smoothing the training objective and reducing label noise inherent in one-hot labels. The softened distribution reduces the sharpness of the loss landscape, making the model less prone to memorize adversarial examples. The interpolation factor grows over time, reflecting increasing trust in the teacher and gradually transitioning from noise reduction toward accurate distribution guidance.

### Mechanism 3
ADR's annealed temperature schedule and interpolation factor create a calibrated label distribution that better matches model uncertainty. Temperature τ starts high to mimic uniform noise, then decreases to let the teacher's distribution reveal inter-class relationships. Simultaneously, λ increases to give more weight to the teacher's predictions as training progresses. This annealing aligns label softness with model maturity.

## Foundational Learning

- **Concept:** Adversarial Training (AT) and robust overfitting
  - **Why needed here:** ADR directly targets the robust overfitting problem by reforming labels in AT. Understanding the min-max optimization and why models overfit to adversarial examples is essential.
  - **Quick check question:** In AT, what is the difference between natural accuracy and robust accuracy, and why does the gap often widen during training?

- **Concept:** Knowledge Distillation and self-distillation
  - **Why needed here:** ADR uses the EMA teacher as a self-distillation source. Knowing how distillation softens targets and why EMA is stable is critical.
  - **Quick check question:** How does the EMA update rule (θt = γ·θt + (1−γ)·θs) help stabilize the teacher's output?

- **Concept:** Temperature scaling in softmax
  - **Why needed here:** Temperature controls the smoothness of the teacher's output distribution. It must be understood to set τ appropriately.
  - **Quick check question:** What happens to the softmax distribution when τ → 0 versus τ → ∞?

## Architecture Onboarding

- **Component map:** Student model fθs -> Teacher model fθt -> Temperature scheduler -> Interpolation scheduler -> Adversarial attacker -> Loss function
- **Critical path:**
  1. Forward pass through student to get logits
  2. Forward pass through teacher (EMA weights) to get teacher logits
  3. Apply temperature scaling and softmax to teacher logits
  4. Interpolate teacher distribution with one-hot label using λ
  5. Generate adversarial example using interpolated label
  6. Compute loss and backpropagate through student
  7. Update EMA teacher weights
- **Design tradeoffs:**
  - High λ early in training → faster convergence but risk of overconfidence
  - Low τ early in training → more noise but less teacher influence
  - EMA decay γ too high → teacher changes slowly, possibly lagging behind
  - EMA decay γ too low → teacher is unstable, causing noisy guidance
- **Failure signatures:**
  - If robust accuracy degrades sharply in early epochs → λ too high or τ too low
  - If robust accuracy plateaus early → τ too high or λ too low
  - If clean accuracy drops significantly → over-smoothing or poor annealing schedule
- **First 3 experiments:**
  1. Run ADR with λ fixed at 0.3 and τ fixed at 2.0; observe if robust accuracy improves over baseline AT
  2. Add cosine annealing to τ from 3.0 to 1.0; check if further robustness gains appear
  3. Introduce λ annealing from 0.5 to 0.9; verify that both robust and clean accuracies improve without overfitting

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the optimal temperature and interpolation factor for ADR across different datasets and model architectures?
  - Basis in paper: The paper mentions that the optimal temperature and interpolation factor depend on the dataset and that improper selection of these parameters can limit performance improvements.
  - Why unresolved: The paper does not provide a systematic method for determining the optimal parameters, only stating that they vary by dataset.
  - What evidence would resolve it: Empirical studies comparing different parameter settings across multiple datasets and architectures to establish a general guideline or an adaptive method for parameter selection.

- **Open Question 2:** How does ADR perform compared to other state-of-the-art adversarial training methods when combined with additional data augmentation techniques?
  - Basis in paper: The paper mentions that some methods introduce extra auxiliary examples when training or bring complex augmentation into AT, which might obtain superior SA compared to ADR.
  - Why unresolved: The paper does not provide direct comparisons with methods that use additional data augmentation techniques.
  - What evidence would resolve it: Experiments comparing ADR with other methods that use data augmentation techniques, measuring both robust and standard accuracy.

- **Open Question 3:** Can the performance of ADR be further improved by incorporating additional unlabeled data or synthetic data during training?
  - Basis in paper: The paper mentions that incorporating additional training data can promote robust generalization and provides results using synthetic data from DDPM.
  - Why unresolved: The paper only explores the use of synthetic data and does not investigate the potential benefits of incorporating real-world unlabeled data.
  - What evidence would resolve it: Experiments comparing ADR's performance with and without additional real-world unlabeled data, measuring improvements in robust and standard accuracy.

## Limitations
- The effectiveness of ADR depends on the EMA teacher reliably reflecting distribution shifts, but this assumption is not empirically validated
- The annealing schedules for temperature and interpolation factor are heuristic without theoretical justification
- The claim that ADR achieves "state-of-the-art robustness without extra data" is only true relative to specific baselines mentioned

## Confidence

**High confidence:** ADR improves robust accuracy on CIFAR-10/100 and TinyImageNet-200 compared to standard PGD-AT baselines (measured via AutoAttack). The ablation studies showing the importance of each component (temperature scaling, interpolation, EMA teacher) are empirically supported.

**Medium confidence:** ADR's effectiveness stems primarily from noise-aware label generation rather than other factors like weight averaging or architectural changes. The mechanism of "reflecting distribution shifts under attack" is plausible but not directly validated.

**Low confidence:** The claim that ADR achieves "state-of-the-art robustness without extra data" - this is only true relative to specific baselines mentioned, and doesn't account for concurrent work or different evaluation protocols.

## Next Checks

1. **Teacher reliability validation:** Run controlled experiments where the EMA teacher is intentionally corrupted (e.g., injected noise or bias) to measure how ADR performance degrades - this would test whether the method truly depends on accurate teacher predictions.

2. **Ablation of annealing schedules:** Systematically vary the temperature and interpolation annealing curves (linear, cosine, step-wise) to determine if the specific schedules matter or if any gradual transition yields similar benefits.

3. **Distribution shift verification:** Use techniques like t-SNE or feature visualization to directly compare how one-hot labels versus ADR labels behave under adversarial perturbations - does ADR actually produce distributions that better reflect the true class relationships after attack?