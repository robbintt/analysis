---
ver: rpa2
title: Can language models learn analogical reasoning? Investigating training objectives
  and comparisons to human performance
arxiv_id: '2310.05597'
source_url: https://arxiv.org/abs/2310.05597
tags:
- bert
- analogies
- overall
- analogy
- scan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can learn analogical
  reasoning by training them on analogy classification and ranking tasks using modified
  SBERT architectures. The authors fine-tune BERT models on a combination of SAT,
  U2/U4, and SCAN analogy datasets and evaluate performance on unseen analogy datasets,
  including a human baseline comparison.
---

# Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance

## Quick Facts
- arXiv ID: 2310.05597
- Source URL: https://arxiv.org/abs/2310.05597
- Reference count: 21
- Key outcome: Models learn analogical reasoning, approaching human performance (0.72 accuracy vs. 0.85 for humans) on unseen test set using vector difference training

## Executive Summary
This paper investigates whether language models can learn analogical reasoning by fine-tuning BERT models on SAT, U2/U4, and SCAN analogy datasets using modified SBERT architectures. The authors train models to maximize cosine similarity between vector differences (a-b) and (c-d), finding that models can learn analogical reasoning even with small amounts of data and approach human performance on unseen test sets. The best-performing model uses a single-layer architecture with this cosine similarity objective. Notably, the authors demonstrate that fine-tuning on analogies does not hurt performance on external semantic similarity tasks, and in some cases improves it.

## Method Summary
The authors fine-tune pre-trained BERT models using a modified SBERT architecture that processes word pairs through mean pooling of token embeddings. They train for 3 epochs using Adam optimizer (lr=2e-5, batch size 32) with a cosine embedding loss to maximize similarity between vector differences (a-b) and (c-d). The training uses SAT, U2/U4, and SCAN analogy datasets, with negative analogies created by shuffling c,d terms. Evaluation is performed on classification accuracy, ranking tasks, and external semantic similarity benchmarks including SICK and STS Benchmark.

## Key Results
- Models achieve 0.72 accuracy on unseen test set, approaching human baseline of 0.85
- Single-layer architecture with cosine similarity training performs best
- Fine-tuning on analogies does not hurt external semantic similarity tasks, and improves performance in some cases
- Models learn analogical reasoning from small datasets (SAT: 374 analogies, U2/U4: ~2,800 analogies)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training to maximize cosine similarity between `(a-b)` and `(c-d)` vectors allows the model to learn relational invariance.
- Mechanism: By forcing the embedding space to align vector differences for analogical pairs, the model captures the abstract relation between terms rather than surface-level lexical similarity.
- Core assumption: Vector differences `(a-b)` and `(c-d)` meaningfully represent the underlying relation between word pairs.
- Evidence anchors: [abstract] Models are able to learn analogical reasoning, even with a small amount of data. [section 3.2.2] We base our training off the idea that in a proportional analogy, the relationship between a and b can be framed as a-b (Mikolov et al., 2013a). Given that the pairs that make up an analogy should have the same relation expressed between the entities, it means (a-b)=(c-d).

### Mechanism 2
- Claim: Fine-tuning on analogies does not degrade performance on external semantic similarity tasks.
- Mechanism: The training objective preserves general semantic similarity learning because it is based on cosine similarity, which aligns with how similarity is typically measured in NLP tasks.
- Core assumption: The analogy training objective is compatible with, rather than disruptive to, semantic similarity learning.
- Evidence anchors: [abstract] We find that fine-tuning models with this training objective does not deteriorate performance on external, but related tasks with our training scheme. [section 4.4] BERT a-b and Single Layer a-b improved over BERT a-b non-tuned, showing that training actually improved performance.

### Mechanism 3
- Claim: Models can learn analogical reasoning from small datasets, approaching human performance on unseen test sets.
- Mechanism: The SBERT-based fine-tuning approach leverages pre-trained semantic knowledge and fine-tunes it for analogy detection, enabling rapid learning even with limited analogy-specific data.
- Core assumption: Pre-trained BERT models contain sufficient relational knowledge that can be adapted for analogical reasoning with minimal additional training.
- Evidence anchors: [abstract] We find that models are able to learn analogical reasoning, even with a small amount of data. [section 4.1] Both a-b training schemes improved overall accuracy on analogy classification over the previously discussed baselines.

## Foundational Learning

- Concept: Cosine similarity as a measure of relational alignment
  - Why needed here: The training objective relies on cosine similarity between vector differences to capture analogical relations.
  - Quick check question: Why do we use cosine similarity instead of Euclidean distance for analogy training?

- Concept: Vector offset method for analogies
  - Why needed here: The paper builds on the idea that `(a-b)` and `(c-d)` represent the relation in an analogy, which is central to the training objective.
  - Quick check question: How does the vector offset method relate to the training objective in this paper?

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: The paper claims that fine-tuning on analogies does not hurt performance on semantic similarity tasks, which requires understanding how fine-tuning can affect other learned tasks.
  - Quick check question: What is catastrophic forgetting, and why is it relevant to fine-tuning on analogies?

## Architecture Onboarding

- Component map: Pre-trained BERT model -> SBERT modifications (mean pooling) -> Cosine embedding loss between (a-b) and (c-d) -> Analogy classification/ranking output

- Critical path:
  1. Prepare analogy data (positive and negative pairs)
  2. Tokenize and mean pool to get word embeddings
  3. Compute vector differences `(a-b)` and `(c-d)`
  4. Apply cosine similarity and embedding loss
  5. Evaluate on classification, ranking, and external tasks

- Design tradeoffs:
  - Using SBERT vs. standard BERT: SBERT allows sentence-level pooling, which is adapted here for word pairs
  - Single-layer vs. full fine-tuning: Single-layer adds a small trainable component while keeping BERT weights frozen
  - Binary classification vs. ranking: Ranking mimics human evaluation but may be more sensitive to relative scores

- Failure signatures:
  - All analogies classified as positive/negative (bias)
  - Poor generalization to unseen datasets
  - Decreased performance on external semantic similarity tasks

- First 3 experiments:
  1. Train BERT a-b on SAT analogies and evaluate on U2/U4
  2. Compare Simple Classifier vs. BERT a-b on SCAN dataset
  3. Test BERT a-b fine-tuned model on the distractor dataset with human baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance on analogy tasks generalize to other types of analogical reasoning beyond the datasets tested?
- Basis in paper: [inferred] The paper tests models on specific datasets (SAT, U2/U4, SCAN, Distractor) but does not explore other types of analogies or reasoning tasks.
- Why unresolved: The study is limited to a specific set of analogy datasets and does not test generalization to other forms of analogical reasoning.
- What evidence would resolve it: Testing the models on a wider variety of analogical reasoning tasks, including novel or less common types of analogies.

### Open Question 2
- Question: How does the proposed training objective compare to other methods for learning analogical reasoning in language models?
- Basis in paper: [explicit] The paper introduces a novel training objective based on cosine similarity of vector differences but does not compare it to other methods or baselines.
- Why unresolved: The paper focuses on the proposed method without exploring alternative approaches or comparing their effectiveness.
- What evidence would resolve it: Conducting experiments with other training methods or objectives and comparing their performance on the same tasks.

### Open Question 3
- Question: What is the impact of model size and architecture on the ability to learn analogical reasoning?
- Basis in paper: [explicit] The paper uses BERT-base and tests a few other architectures but does not explore the impact of model size or architecture on performance.
- Why unresolved: The study uses a limited set of models and does not investigate how different architectures or sizes affect learning analogical reasoning.
- What evidence would resolve it: Experimenting with a range of model sizes and architectures, including larger models or different types of transformers, and comparing their performance.

## Limitations

- Limited dataset scale (SAT: 374 analogies, U2/U4: ~2,800 analogies) constrains generalization claims
- Small human baseline evaluation (100 examples) weakens the model-human performance comparison
- The vector difference method assumes linear relational structures that may not capture all analogy types

## Confidence

- **High Confidence**: Single-layer architecture with cosine similarity training objective achieves competitive performance on the test set
- **Medium Confidence**: Fine-tuning on analogies does not hurt external semantic similarity tasks
- **Low Confidence**: Broader claim that models have "learned" analogical reasoning in a human-like way

## Next Checks

1. **Dataset Scale Validation**: Replicate experiments on larger, more diverse analogy datasets (e.g., Google Analogy dataset with 19,500 questions) to test whether the performance gap with humans persists or diminishes with more training data.

2. **Transfer Learning Analysis**: Systematically test the fine-tuned models on a broader range of semantic similarity and reasoning tasks (e.g., GLUE benchmark, commonsense reasoning datasets) to verify the claim that analogy training is universally beneficial or at least non-detrimental.

3. **Human Baseline Robustness**: Expand the human evaluation to include more diverse analogy types and a larger sample size, particularly for the distractor dataset, to strengthen the comparison between model and human performance.