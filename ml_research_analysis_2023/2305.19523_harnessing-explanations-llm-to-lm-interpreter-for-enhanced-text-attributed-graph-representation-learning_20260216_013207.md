---
ver: rpa2
title: 'Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
  Graph Representation Learning'
arxiv_id: '2305.19523'
source_url: https://arxiv.org/abs/2305.19523
tags:
- text
- features
- node
- arxiv
- diabetes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes leveraging large language models (LLMs) to
  enhance representation learning on text-attributed graphs (TAGs). The key innovation
  is using LLM explanations as features: an LLM is prompted to perform zero-shot classification
  and provide textual explanations, which are then transformed into informative features
  for downstream graph neural networks (GNNs).'
---

# Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning

## Quick Facts
- arXiv ID: 2305.19523
- Source URL: https://arxiv.org/abs/2305.19523
- Reference count: 40
- Top-1 accuracy on ogbn-arxiv with 2.88× lower computation time

## Executive Summary
This paper introduces TAPE (Text-Attributed Graph Enhancement via LLM Explanations), a method that leverages large language models (LLMs) to enhance representation learning on text-attributed graphs (TAGs). The key innovation is using LLM-generated explanations as features for downstream graph neural networks (GNNs). The approach involves prompting an LLM to perform zero-shot classification and provide textual explanations, which are then transformed into informative features through fine-tuned smaller language models. Experiments demonstrate significant performance improvements across multiple GNN architectures and datasets, with the method achieving top-1 accuracy on ogbn-arxiv while reducing computation time.

## Method Summary
TAPE operates by first generating prompts from node text attributes and querying an LLM (such as GPT-3.5) for zero-shot predictions and explanations. These LLM outputs are then processed by a smaller LM (like DeBERTa) that is fine-tuned to interpret the explanations and extract meaningful features. The method produces three types of features: original text embeddings, LLM explanation features, and LLM prediction features. These features are then used to train three separate GNN models in parallel, whose predictions are ensembled for the final output. The approach is LMaaS-compatible and does not require fine-tuning the LLM or accessing its embeddings.

## Key Results
- Achieves top-1 accuracy on ogbn-arxiv dataset with 2.88× lower computation time compared to closest baseline
- Improves performance across various GNN models including GCN, SAGE, and RevGAT
- Demonstrates effectiveness on TAG versions of widely used benchmarks Cora and PubMed
- Zero-shot LLM classification accuracy of 73.5% on ogbn-arxiv, outperforming many fully trained GNN baselines

## Why This Works (Mechanism)

### Mechanism 1
LLM explanations contain structured semantic information that LMs can extract and map to class-relevant features. The LLM reasoning and general knowledge extraction produces explanations more informative than raw text for downstream GNN classification. Break condition: If LLM explanations become too generic or verbose, the LM's ability to extract discriminative features degrades.

### Mechanism 2
Zero-shot LLM predictions serve as high-quality pseudo-labels that improve downstream representation learning. LLM zero-shot classification accuracy provides informative class predictions that capture semantic structure better than shallow embeddings. Break condition: If LLM zero-shot accuracy falls below threshold where noise overwhelms signal, the pseudo-labels become harmful.

### Mechanism 3
Ensembling features from multiple sources (original text, explanations, predictions) captures complementary information for improved classification. Different feature sources encode different aspects of node semantics - original text captures surface features, explanations capture high-level reasoning, predictions capture class-relevant patterns. Break condition: If feature sources become highly correlated, ensembling provides diminishing returns.

## Foundational Learning

- Graph Neural Networks and message passing: GNNs are the core architecture for learning node representations from graph structure and node features in TAGs. Quick check: What is the fundamental operation that allows GNNs to aggregate information from neighboring nodes?

- Language Model fine-tuning and transfer learning: LMs are fine-tuned on TAG-specific text to extract meaningful features from LLM outputs. Quick check: Why is it beneficial to use a smaller LM to interpret LLM explanations rather than using the LLM directly?

- Prompt engineering and zero-shot learning: Custom prompts elicit structured outputs from LLMs without requiring fine-tuning. Quick check: How does placing important information later in a prompt affect LLM response quality?

## Architecture Onboarding

- Component map: Prompt Generator → LLM API → Text Parser → LM Fine-tuner (for original text and explanations) → GNN Trainer (3 parallel models) → Ensembler → Final Predictions

- Critical path: 1) Generate prompts from node text attributes, 2) Query LLM for predictions and explanations, 3) Fine-tune LM on original text and explanations, 4) Extract hpred from LLM predictions, 5) Train 3 GNN models on horig, hexpl, hpred, 6) Ensemble predictions for final output

- Design tradeoffs: LLM size vs. inference cost (larger LLMs provide better explanations but increase API costs), LM fine-tuning vs. frozen features (fine-tuning allows adaptation but adds complexity), feature dimensionality vs. GNN capacity (higher dimensions provide more information but may require larger GNNs)

- Failure signatures: Poor LLM explanation quality (too generic, irrelevant, or verbose), LM fine-tuning instability or overfitting on small datasets, GNN performance degradation when adding LLM-derived features, ensembling not improving over individual model performance

- First 3 experiments: 1) Run ablation study removing hexpl, horig, and hpred individually to verify each contributes to performance, 2) Compare zero-shot LLM accuracy across different prompt designs to optimize explanation quality, 3) Test different ensembling strategies (weighted averaging, stacking) to maximize performance gains

## Open Questions the Paper Calls Out
The paper notes that prompt design is currently hand-crafted and may not be optimal, stating "One limitation of our approach is the need for custom prompts for each dataset." However, the paper does not systematically explore prompt design space or develop automated methods for prompt generation.

## Limitations
- Method effectiveness across different domains beyond the three tested TAGs (Cora, PubMed, and ogbn-arxiv) is uncertain
- Performance critically depends on LLM explanation quality, which is not systematically evaluated
- Custom prompt design significantly impacts results, but only general prompt formats are specified without exact templates used in experiments

## Confidence
- High Confidence: The core methodology of using LLM explanations as features for TAGs is well-defined and reproducible
- Medium Confidence: The claim of 2.88× lower computation time compared to closest baseline requires verification due to implementation details not fully specified
- Low Confidence: The generalizability of the approach to datasets beyond the three tested TAGs is uncertain without additional validation

## Next Checks
1. Conduct ablation study removing each feature source (original text, explanations, predictions) individually to verify their individual contributions to performance improvements
2. Test different prompt designs on the same datasets to quantify the impact of prompt engineering on explanation quality and downstream performance
3. Apply the method to TAG datasets from different domains (e.g., social networks with text attributes, biological networks) to assess generalizability beyond the current experimental scope