---
ver: rpa2
title: 'EasyTPP: Towards Open Benchmarking Temporal Point Processes'
arxiv_id: '2307.08097'
source_url: https://arxiv.org/abs/2307.08097
tags:
- event
- tpps
- time
- easytpp
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EasyTPP, the first open benchmarking platform
  for temporal point processes (TPPs). It addresses the lack of standardized evaluation
  methods in this field by providing a unified framework with 8 popular neural TPP
  models, multiple datasets, and a comprehensive set of evaluation metrics.
---

# EasyTPP: Towards Open Benchmarking Temporal Point Processes

## Quick Facts
- arXiv ID: 2307.08097
- Source URL: https://arxiv.org/abs/2307.08097
- Reference count: 40
- This paper introduces EasyTPP, the first open benchmarking platform for temporal point processes (TPPs).

## Executive Summary
This paper introduces EasyTPP, the first open benchmarking platform for temporal point processes (TPPs). It addresses the lack of standardized evaluation methods in this field by providing a unified framework with 8 popular neural TPP models, multiple datasets, and a comprehensive set of evaluation metrics. EasyTPP supports both PyTorch and TensorFlow, allowing for fair comparisons across different implementations. Experiments on synthetic and real-world datasets show that EasyTPP enables reproducible research and facilitates the assessment of TPP models' performance in tasks like goodness-of-fit, next-event prediction, and long-horizon prediction. The platform is open-source and actively maintained, promoting further advancements in TPP research.

## Method Summary
EasyTPP provides a unified benchmarking platform for temporal point processes by implementing 8 popular neural TPP models in both PyTorch and TensorFlow frameworks. The platform includes a standardized pipeline for data preprocessing, model training, evaluation, and hyperparameter optimization. It supports 1 synthetic and 5 real-world datasets with consistent train/dev/test splits. The evaluation framework computes multiple metrics including log-likelihood for goodness-of-fit, RMSE and error rates for next-event prediction, and optimal transport distance for long-horizon prediction. All models are trained using the same procedure (Adam optimizer with early stopping) to ensure fair comparison.

## Key Results
- EasyTPP successfully benchmarks 8 neural TPP models across 6 datasets using standardized evaluation metrics
- Models implemented in both PyTorch and TensorFlow show consistent performance, validating cross-framework reproducibility
- Comprehensive evaluation reveals strengths and weaknesses of different TPP models across various prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EasyTPP's unified interface reduces the barrier to entry for researchers by standardizing how datasets, models, and evaluation metrics are accessed.
- Mechanism: The software architecture wraps diverse models (PyTorch, TensorFlow) and datasets into a common configuration-driven pipeline, eliminating the need to understand framework-specific quirks.
- Core assumption: A single configuration schema can capture the variability of different TPP models and datasets without loss of fidelity.
- Evidence anchors:
  - [abstract]: "a unified interface of using existing datasets and adding new datasets"
  - [section 5]: "Data Preprocess component provides a common way to access the event data across the software"
  - [corpus]: Weak - neighbors mention benchmarking and modeling but not standardization per se.
- Break condition: When a new model requires features outside the current configuration schema, requiring custom integration code.

### Mechanism 2
- Claim: Implementing the same models in both PyTorch and TensorFlow ensures reproducibility and broad applicability.
- Mechanism: Two separate but equivalent implementations allow researchers and practitioners to choose their preferred framework without changing experimental conclusions.
- Core assumption: The two implementations are functionally equivalent in terms of predictive performance.
- Evidence anchors:
  - [section 5]: "We implement two equivalent sets of methods in PyTorch and TensorFlow"
  - [section 6.2]: "under the same training procedure, they all give close results to those in PyTorch"
  - [corpus]: Missing - no explicit neighbor discussion of cross-framework reproducibility.
- Break condition: Divergence in numerical results due to framework-specific optimizations or floating-point handling.

### Mechanism 3
- Claim: The benchmarking pipeline with standard train/dev/test splits and consistent evaluation metrics enables fair comparison across TPP models.
- Mechanism: All models are trained with the same optimizer, initialization, and early stopping criteria, and evaluated on identical tasks (log-likelihood, next-event prediction, long-horizon prediction).
- Core assumption: Differences in performance are due to model architecture, not experimental setup.
- Evidence anchors:
  - [section 6.1]: "For a fair comparison, we use the same training procedure for all the models"
  - [section 6.2]: "We mainly examine the models in two standard scenarios... goodness-of-fit, next-event prediction"
  - [corpus]: Weak - neighbors focus on new modeling approaches rather than benchmarking methodology.
- Break condition: When a new evaluation metric or task reveals performance differences not captured by the current setup.

## Foundational Learning

- Concept: Temporal Point Processes (TPPs)
  - Why needed here: TPPs are the underlying statistical model for continuous-time event sequences; understanding them is essential to grasp what EasyTPP benchmarks.
  - Quick check question: What is the intensity function in a TPP, and how does it differ from a probability mass function in discrete-time models?

- Concept: Neural network architectures for TPPs (RNNs, attention, ODEs)
  - Why needed here: EasyTPP implements several neural TPP variants; knowing their differences explains the benchmark results.
  - Quick check question: How does an attention-based TPP differ from an RNN-based TPP in handling long-range dependencies?

- Concept: Evaluation metrics (log-likelihood, RMSE, error rate, OTD)
  - Why needed here: These metrics quantify model performance in different prediction tasks; understanding them is key to interpreting benchmark results.
  - Quick check question: Why is log-likelihood used for goodness-of-fit while RMSE is used for time prediction?

## Architecture Onboarding

- Component map: Data Preprocess -> Model -> Trainer -> Thinning Sampler -> Evaluation -> Config
- Critical path: Data → Model → Trainer → Evaluation → Result
- Design tradeoffs:
  - Supporting both PyTorch and TensorFlow increases code complexity but maximizes usability.
  - Using Monte Carlo integration for intensity computation is flexible but slower than analytical methods.
  - Implementing a generic configuration system enables extensibility but may obscure framework-specific optimizations.
- Failure signatures:
  - NaN losses during training often indicate numerical instability in intensity computation.
  - Poor log-likelihood on dev set despite good training performance suggests overfitting.
  - Large discrepancies between PyTorch and TensorFlow results indicate implementation bugs.
- First 3 experiments:
  1. Run the synthetic Hawkes dataset benchmark with all available models to verify the pipeline works end-to-end.
  2. Compare RMTPP and NHP on the Taxi dataset to observe the impact of different recurrence mechanisms.
  3. Enable hyperparameter tuning (Optuna) on the Amazon dataset to test the HPOConfig integration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do neural TPP models perform on datasets with very long sequences compared to traditional non-neural TPP models?
- Basis in paper: [inferred] The paper mentions that neural TPP models are data-hungry and may suffer compared to non-neural models if starved of data. It also discusses the performance of various neural TPP models on datasets with different characteristics.
- Why unresolved: The paper focuses on evaluating neural TPP models and does not provide a direct comparison with traditional non-neural models on very long sequences.
- What evidence would resolve it: Conducting experiments comparing the performance of neural TPP models and traditional non-neural models on datasets with very long sequences would provide insights into their relative strengths and weaknesses in such scenarios.

### Open Question 2
- Question: How does the choice of integration method (Monte Carlo vs. analytical) affect the performance of TPP models?
- Basis in paper: [explicit] The paper mentions that some models (e.g., IFTPP) evaluate the log-likelihood in a closed form, while others (e.g., RMTPP, NHP, THP, AttNHP, ODETPP) compute the intensity function via Monte Carlo integration, causing numerical approximation errors.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of integration method impacts the performance of TPP models across different datasets and tasks.
- What evidence would resolve it: Conducting experiments comparing the performance of TPP models using different integration methods (Monte Carlo vs. analytical) on various datasets and tasks would help determine the impact of this choice on model performance.

### Open Question 3
- Question: How do attention-based TPP models compare to non-attention models in terms of long-horizon prediction performance?
- Basis in paper: [explicit] The paper discusses the long-horizon prediction task and mentions that attention-based models (e.g., SAHP, THP, AttNHP) generally perform better than or close to non-attention models (e.g., RMTPP, NHP, ODETPP) on certain datasets.
- Why unresolved: While the paper provides some insights into the performance of attention-based models in long-horizon prediction, it does not offer a comprehensive comparison across all models and datasets.
- What evidence would resolve it: Conducting extensive experiments comparing the long-horizon prediction performance of attention-based and non-attention TPP models on various datasets would provide a clearer understanding of their relative strengths and weaknesses in this task.

## Limitations
- Evaluation focuses primarily on predictive accuracy metrics without deeply exploring statistical properties of TPP models
- Benchmark scope limited to 8 specific models and 6 datasets, potentially missing diversity of TPP applications
- Cross-framework numerical equivalence assumed but not rigorously verified across all edge cases

## Confidence
- **High Confidence**: The claim that EasyTPP provides a unified benchmarking platform with standardized evaluation is well-supported by implementation details and experimental results
- **Medium Confidence**: The claim that EasyTPP enables fair comparisons across TPP models is reasonable given standardized training procedures
- **Medium Confidence**: The assertion that EasyTPP will advance TPP research through reproducibility is plausible but remains somewhat aspirational

## Next Checks
1. **Framework Consistency Test**: Run the same experiments on both PyTorch and TensorFlow implementations with multiple random seeds to quantify numerical differences
2. **Statistical Power Analysis**: Conduct sensitivity analysis to determine minimum detectable effect size across different models and datasets
3. **Extended Benchmarking**: Add at least 3 new TPP models and 2 additional real-world datasets to test framework extensibility and identify hidden assumptions