---
ver: rpa2
title: 'Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval'
arxiv_id: '2312.15503'
source_url: https://arxiv.org/abs/2312.15503
tags:
- retrieval
- text
- dense
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of adapting large language models
  (LLMs) for dense retrieval tasks, where the goal is to represent the semantic relationship
  between query and document using discriminative embeddings. The core idea is to
  propose LLaRA, a post-hoc adaptation approach that enhances LLMs'' text embedding
  capability through two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR
  (Embedding-Based Auto-Regression).'
---

# Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval

## Quick Facts
- arXiv ID: 2312.15503
- Source URL: https://arxiv.org/abs/2312.15503
- Reference count: 14
- Key outcome: LLaRA achieves state-of-the-art dense retrieval performance on MSMARCO and BEIR benchmarks using unsupervised adaptation of LLaMA-2-7B

## Executive Summary
The paper addresses the challenge of adapting large language models (LLMs) for dense retrieval tasks by proposing LLaRA, a post-hoc adaptation approach that enhances LLMs' text embedding capability through two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression). These tasks transform the LLM's text embedding into a global semantic representer by predicting the input sentence and the next sentence, respectively. The method is applied to adapt LLaMA-2-7B on the Wikipedia corpus, resulting in state-of-the-art performances on popular benchmarks such as MSMARCO and BEIR.

## Method Summary
LLaRA is an unsupervised adaptation method that enhances LLMs for dense retrieval by transforming their text embedding capability into a global semantic representer. It uses two pretext tasks: EBAE, which predicts the input sentence to learn self-representation, and EBAR, which predicts the next sentence to learn context-aware representation. The method employs a joint processing approach with modified attention masks to compute both embeddings simultaneously, saving approximately 50% of computational cost. The adapted model is then fine-tuned using contrastive learning with hard negatives to optimize retrieval performance.

## Key Results
- Achieves MRR@10 of 43.1 on MS MARCO passage retrieval
- Achieves MRR@100 of 47.5 on MS MARCO document retrieval
- Achieves average NDCG@10 of 55.1 on BEIR zero-shot retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage prompting (SELF and NEXT) allows the model to learn both self-representation and next-sentence prediction, which together create a more robust semantic representation for retrieval tasks.
- Mechanism: By using different prompt templates ("The original sentence:" vs "The next sentence:"), the model learns to differentiate between representing the current context and predicting future context. This dual representation capability enables the model to handle both similarity search and question answering scenarios.
- Core assumption: The semantic relationships between sentences and their context are learnable through this dual-prompt approach and transfer to downstream retrieval tasks.
- Evidence anchors:
  - [abstract] "With properly designed pretext tasks, it aims to enhance LLMs with the capacity to generate text embeddings for the semantic representation of global context."
  - [section] "With the two different prompt templates, the LLM's embedding capability can be differentiated to handle the diverse semantic matching scenarios, e.g., similarity search (using EBAE's prompt) and question answering (using EBAR's prompt)."
- Break condition: If the dual-prompt approach doesn't improve semantic representation quality, or if the differentiation between similarity search and question answering scenarios is not significant.

### Mechanism 2
- Claim: The joint processing of both prompt templates in a single forward pass significantly reduces computational cost while maintaining effectiveness.
- Mechanism: By merging the prompts into one joint prompt and modifying the attention mask to make "SELF" and "NEXT" mutually invisible, the model can compute both embeddings simultaneously, saving approximately 50% of the cost compared to separate computations.
- Core assumption: The modified attention mask effectively isolates the two processing streams while allowing shared computation benefits.
- Evidence anchors:
  - [abstract] "The direct computation of eα_t and eβ_t will result in a big waste of cost, because the input text T is processed for two times. To solve this problem, we propose to compute eα_t and eβ_t in one pass."
  - [section] "Now, the output embeddings of the first and second ⟨\s⟩ tokens are used for eα_t and eβ_t, respectively. Given that the input text T will take the majority of length of the joint prompt, such a processing will save almost 50% of the cost compared with the direct computation."
- Break condition: If the attention mask modification fails to properly isolate the two processing streams, or if the cost savings are significantly less than 50%.

### Mechanism 3
- Claim: The multi-class classification objective effectively transforms local semantic representations into global semantic representations.
- Mechanism: By treating the task as predicting tokens within the target context, the model learns to create embeddings that capture the full semantic meaning of the input text, rather than just local or near-future semantics.
- Core assumption: If an embedding can accurately predict all tokens in a context, it must be a strong representation of that context's global semantic.
- Evidence anchors:
  - [abstract] "Based on this basic principle, the training of text embedding is formulated as a multi-class classification problem, where the text embedding is linearly projected for the prediction of the tokens within the target context."
  - [section] "The objective function for the above problem is derived as: min . X t∈T exp (eT W t)P v∈V exp (eT W v) . (6) In this place, W ∈ R|V |×d is the linear projection matrix, and V is the vocabulary space."
- Break condition: If the classification objective fails to capture global semantic relationships, or if the linear projection is insufficient for the task.

## Foundational Learning

- Concept: Pre-trained language models and their limitations for dense retrieval
  - Why needed here: Understanding why LLMs need adaptation for dense retrieval tasks is crucial for implementing LLaRA effectively.
  - Quick check question: Why can't we directly use LLMs trained for text generation as dense retrievers without adaptation?

- Concept: Attention mechanisms and their role in semantic representation
  - Why needed here: The attention mask modification is a key component of LLaRA's efficiency improvements, requiring understanding of how attention works in transformer models.
  - Quick check question: How does the modified attention mask allow for joint processing of both prompt templates while keeping them isolated?

- Concept: Contrastive learning and its application in dense retrieval
  - Why needed here: The fine-tuning process uses contrastive learning with hard negatives, which is essential for understanding how the adapted model is further optimized for retrieval tasks.
  - Quick check question: What is the role of hard negative sampling in the fine-tuning process, and how does it improve retrieval performance?

## Architecture Onboarding

- Component map:
  Input processing -> Text tokenization and prompt template application -> LLaMA-2-7B with modified attention mask -> Dual embeddings (eα_t and eβ_t) extraction -> Linear projection (W matrix) -> Token prediction -> Loss computation -> Backpropagation -> Fine-tuning with contrastive learning

- Critical path:
  1. Input text processing and prompt template application
  2. Joint forward pass through modified LLaMA-2-7B
  3. Dual embedding extraction
  4. Linear projection and token prediction
  5. Loss computation and backpropagation
  6. Fine-tuning with contrastive learning

- Design tradeoffs:
  - Complexity vs. efficiency: Joint processing reduces cost but adds complexity to the attention mechanism
  - Adaptability vs. specialization: Dual prompts enable versatility but may dilute specialization for specific tasks
  - Unsupervised adaptation vs. supervised fine-tuning: Balances data efficiency with task-specific optimization

- Failure signatures:
  - Poor retrieval performance: May indicate issues with the adaptation process or fine-tuning
  - High computational cost: Could suggest problems with the joint processing implementation
  - Overfitting to Wikipedia corpus: Might require more diverse pre-training data or regularization

- First 3 experiments:
  1. Implement and test the joint processing of SELF and NEXT prompts on a small dataset to verify the attention mask modification and cost savings.
  2. Evaluate the quality of the generated embeddings using a simple semantic similarity task before and after adaptation.
  3. Perform fine-tuning on a small subset of MS MARCO to assess the impact of contrastive learning with hard negatives on retrieval performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLaRA's adaptation of LLMs for dense retrieval compare to other unsupervised adaptation methods in terms of efficiency and effectiveness?
- Basis in paper: [explicit] The paper states that LLaRA is simple, lightweight, and highly effective, and that it can be conducted purely based on unlabeled corpora and fully compatible with the existing training pipeline of language modeling.
- Why unresolved: The paper does not provide a direct comparison of LLaRA with other unsupervised adaptation methods in terms of efficiency and effectiveness.
- What evidence would resolve it: A comparative study of LLaRA with other unsupervised adaptation methods in terms of efficiency and effectiveness, using the same backbone LLM and evaluation benchmarks.

### Open Question 2
- Question: How does the choice of pretext tasks in LLaRA affect its performance on different types of retrieval tasks?
- Basis in paper: [explicit] The paper mentions that the two pretext tasks, EBAE and EBAR, are designed to handle different semantic matching scenarios, such as similarity search and question answering.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of pretext tasks affects LLaRA's performance on different types of retrieval tasks.
- What evidence would resolve it: An ablation study that examines the impact of each pretext task on LLaRA's performance across various retrieval tasks.

### Open Question 3
- Question: How does LLaRA's performance scale with the size of the LLM backbone?
- Basis in paper: [explicit] The paper demonstrates LLaRA's effectiveness on LLaMA-2-7B, but does not explore its performance with larger or smaller LLMs.
- Why unresolved: The paper does not provide any results or analysis on how LLaRA's performance changes with different sizes of LLM backbones.
- What evidence would resolve it: A scaling study that evaluates LLaRA's performance using different sizes of LLM backbones, from small to very large models.

## Limitations
- The unsupervised nature of the adaptation makes it difficult to assess whether performance gains stem from genuine semantic understanding or dataset-specific memorization
- Evaluation on Wikipedia pre-training data raises questions about generalizability to domains with different linguistic patterns
- Computational cost analysis focuses on joint processing savings but doesn't provide a complete picture of total resource requirements

## Confidence

**High confidence**: The core mechanism of using dual-prompt tasks (EBAE and EBAR) for semantic representation learning is well-founded and theoretically sound

**Medium confidence**: The claimed performance improvements are likely real but may be somewhat dataset-dependent

**Low confidence**: The exact implementation details needed for faithful reproduction, particularly the attention mask modification

## Next Checks

1. **Attention Mask Verification**: Implement the attention mask modification and verify through ablation studies that it correctly isolates the SELF and NEXT contexts while achieving the claimed ~50% computational savings. This should include measuring both the isolation quality (using attention weight analysis) and the actual computational cost reduction.

2. **Cross-Domain Generalization Test**: Evaluate the adapted model on retrieval tasks using non-Wikipedia corpora (e.g., academic papers, medical documents) to assess whether the semantic representations generalize beyond the pre-training domain. This would help determine if the adaptation captures genuine semantic understanding or dataset-specific patterns.

3. **Prompt Template Ablation**: Systematically vary the prompt templates used in EBAE and EBAR tasks to determine their impact on retrieval performance. This would help validate whether the specific templates chosen are optimal or if the approach is robust to template variations.