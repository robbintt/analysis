---
ver: rpa2
title: 'CitDet: A Benchmark Dataset for Citrus Fruit Detection'
arxiv_id: '2309.05645'
source_url: https://arxiv.org/abs/2309.05645
tags:
- fruit
- detection
- citrus
- object
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CitDet, a benchmark dataset for citrus fruit
  detection in orchards affected by Huanglongbing (HLB). The dataset contains high-resolution
  images of citrus trees, with over 32,000 bounding box annotations for fruit instances,
  including both fruit on trees and on the ground.
---

# CitDet: A Benchmark Dataset for Citrus Fruit Detection

## Quick Facts
- arXiv ID: 2309.05645
- Source URL: https://arxiv.org/abs/2309.05645
- Reference count: 35
- Primary result: YOLOv7 achieves 40.6 AP on full-res and 45.5 AP on tiled citrus fruit detection

## Executive Summary
This paper introduces CitDet, a benchmark dataset for citrus fruit detection in HLB-affected orchards, addressing the lack of publicly available datasets for this task. The dataset contains 579 high-resolution images with over 32,000 bounding box annotations for fruit on trees and on the ground. The authors evaluate state-of-the-art object detection algorithms (FasterRCNN, YOLOv5, YOLOv7, DETR, YOLOS) and find that YOLOv7 performs best, especially when images are processed in tiled 3×3 crops. The dataset also enables yield estimation with an R² of 0.793 by distinguishing between fruit on trees and on the ground.

## Method Summary
The authors created CitDet by collecting high-resolution images (2448×3264 pixels) of citrus trees affected by Huanglongbing (HLB), then annotating over 32,000 fruit instances using bounding boxes via Roboflow. The dataset was split 80/20 into training and test sets. Five state-of-the-art object detectors were evaluated: FasterRCNN, YOLOv5, YOLOv7, DETR, and YOLOS. Models were trained on the training set and tested on the test set using standard COCO metrics. To address small object detection challenges, images were processed both at full resolution and as 3×3 tiled crops. Yield estimation was performed by counting fruit on trees and ground, achieving correlation with ground-truth yields.

## Key Results
- YOLOv7 achieved the best detection performance with 40.6 AP on full-resolution images and 45.5 AP on tiled images
- Tiled processing improved detection accuracy by preserving information and reducing occlusion in individual crops
- Yield estimation using the dataset achieved R² = 0.793 by distinguishing fruit on trees from fruit on the ground

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YOLOv7 outperforms other detectors on full-resolution citrus fruit images because its trainable "bag-of-freebies" methods and "compound scaling" techniques reduce parameters and computation while maintaining accuracy.
- Mechanism: The YOLOv7 architecture introduces optimized convolutional layers and a scalable design that adapts to small object detection in cluttered environments like citrus orchards. This allows better feature extraction from high-resolution images with small, occluded fruit.
- Core assumption: The dataset's high-resolution images contain sufficient detail for small object detection, and the model's scaling techniques can effectively leverage this detail.
- Evidence anchors:
  - [abstract] "YOLOv7 achieved the best overall performance, with an AP of 40.6 on full-resolution images and 45.5 on tiled images."
  - [section] "YOLOv7 improves upon real-time object detection performance by designing a trainable 'bag-of-freebies' methods. The detector addresses two new issues in object detection evolution by proposing the 'extend' and 'compound scaling' techniques."
- Break condition: If the dataset images are downscaled or contain insufficient detail, the YOLOv7 advantage may diminish.

### Mechanism 2
- Claim: Tiling full-resolution images into 3×3 crops improves detection accuracy because it preserves more information than resizing while reducing occlusion and clutter in each crop.
- Mechanism: By splitting large images into smaller crops, each tile focuses on a smaller region with fewer occluding elements, making it easier for detectors to identify individual fruit. This avoids the information loss that occurs when resizing full images to fit model input requirements.
- Core assumption: The overlap in the original full images is not needed for detection, and the tiling method preserves all relevant fruit instances.
- Evidence anchors:
  - [section] "We hypothesize that there are two primary factors for this...The second factor contributing to the improved performance of the tiled versions is the ability to preserve information when resizing the image for the model."
  - [section] "In comparing the state-of-the-art object detection algorithms, we found that processing the image in a tiled manner performs better than detectors that operate on the full image."
- Break condition: If fruit instances span across tile boundaries significantly, or if tiling introduces new labeling errors, performance may degrade.

### Mechanism 3
- Claim: Including both fruit-on-tree and fruit-on-ground annotations improves yield estimation accuracy because the model learns to distinguish dropped fruit, which contributes to total yield.
- Mechanism: By training on both classes, the detector can count all fruit present (on trees and ground) in a single pass, eliminating the need for post-processing heuristics to infer dropped fruit from tree-only counts.
- Core assumption: The visual features of fruit on the ground are sufficiently similar to fruit on trees that a single detector can learn both classes effectively.
- Evidence anchors:
  - [abstract] "The dataset also enables yield estimation by distinguishing between fruit on trees and on the ground, achieving an R² of 0.793 for yield prediction."
  - [section] "CitDet aims to fix these issues and provide researchers with a means to evaluate and compare their algorithms...Our dataset provides annotations in bounding box format allowing for maskless frameworks to be easily applied."
- Break condition: If ground fruit are heavily degraded or occluded, or if the detector confuses ground fruit with other ground objects, yield estimation accuracy may suffer.

## Foundational Learning

- Concept: Intersection over Union (IoU)
  - Why needed here: IoU is the primary metric for evaluating object detection accuracy, determining whether predicted bounding boxes match ground truth.
  - Quick check question: If a predicted box overlaps a ground truth box with an area of 50 pixels and the union of both boxes is 100 pixels, what is the IoU?

- Concept: Transfer Learning
  - Why needed here: All models are initialized with COCO-pretrained weights, allowing them to leverage learned features from a large dataset before fine-tuning on the citrus dataset.
  - Quick check question: Why might transfer learning from COCO help a model detect small citrus fruit better than training from scratch on the citrus dataset?

- Concept: Yield Estimation Correlation (R²)
  - Why needed here: R² measures how well the predicted fruit counts correlate with actual measured yields, validating the detection model's practical utility.
  - Quick check question: If a model predicts yields with an R² of 0.8, what percentage of the variance in actual yields is explained by the model?

## Architecture Onboarding

- Component map:
  - Data pipeline: Image capture → Annotation (Roboflow) → Train/test split (80/20) → Tiling (3×3 crops)
  - Models: FasterRCNN (ResNet-50 + FPN), YOLOv5 (CSP backbone), YOLOv7 (scalable architecture), DETR (transformer encoder-decoder), YOLOS (ViT encoder)
  - Evaluation: AP (IoU thresholds 0.5-0.95), AP50, AP75, APS/M/L, R² for yield estimation
  - Hardware: NVIDIA Tesla T4 GPU for training

- Critical path: Image capture → Annotation → Model training (50 epochs tiled, then 50 epochs full-res) → Evaluation → Yield estimation

- Design tradeoffs:
  - High-res vs. tiled: Full-res preserves detail but increases compute; tiling reduces per-image complexity but may split objects.
  - Bounding boxes vs. polygons: Boxes are faster to annotate and compatible with more models but less precise for irregular fruit shapes.
  - Model choice: Transformers (DETR/YOLOS) offer end-to-end detection but struggle with small objects; CNNs (FasterRCNN/YOLO) are more robust to small, occluded objects.

- Failure signatures:
  - Low AP for small objects (APS): Likely due to insufficient resolution or model architecture limitations.
  - High false positives on ground: Possible confusion between fruit and other ground objects; may need better negative sampling.
  - Poor yield estimation: Could indicate systematic under/over-counting; check if tree detection filtering is working correctly.

- First 3 experiments:
  1. Train YOLOv7 on the tiled dataset for 50 epochs, evaluate AP on the test set, and compare AP50 vs AP75 to assess precision-recall tradeoff.
  2. Apply the trained YOLOv7 to full-resolution test images (704×704) for 50 epochs, measure AP difference from tiled results to quantify resizing impact.
  3. Use the fruit-on-tree detections from YOLOv7 to estimate yield for the 187 yield-estimation trees, compute R² against ground-truth counts, and compare detect-count vs filter-detect-count methods.

## Open Questions the Paper Calls Out
- None explicitly called out in the paper.

## Limitations
- Limited test set size (120 images) and lack of cross-validation raise generalizability concerns
- Bounding box annotations may introduce noise for irregularly shaped fruit compared to segmentation masks
- Training procedures underspecified, particularly regarding data augmentation and hyperparameters
- Performance gap between tiled and full-resolution images suggests tiling may artificially inflate results

## Confidence
**High confidence**: The dataset creation methodology and bounding box annotation process are well-documented and reproducible. The yield estimation correlation (R² = 0.793) is statistically sound given the 187 validation trees.

**Medium confidence**: The comparative performance of YOLOv7 over other detectors is robust within the tested conditions, though may not generalize to different orchards or lighting conditions. The tiling methodology's performance advantage is supported by evidence but may be dataset-specific.

**Low confidence**: Claims about YOLOv7's architectural advantages for citrus detection are based on general literature rather than citrus-specific ablation studies. The optimal tiling strategy (3×3) is assumed rather than empirically validated against other crop sizes.

## Next Checks
1. Perform k-fold cross-validation (k=5) on the full dataset to assess model performance stability and reduce potential bias from the single train/test split.

2. Compare bounding box detections against polygon-based ground truth on a subset of images to quantify the annotation precision loss and its impact on AP scores.

3. Systematically evaluate different tiling strategies (2×2, 4×4, variable overlap) to determine if the 3×3 approach is optimal or if performance gains are primarily due to reduced image complexity rather than the tiling method itself.