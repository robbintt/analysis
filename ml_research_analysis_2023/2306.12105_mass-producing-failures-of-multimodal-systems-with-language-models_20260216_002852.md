---
ver: rpa2
title: Mass-Producing Failures of Multimodal Systems with Language Models
arxiv_id: '2306.12105'
source_url: https://arxiv.org/abs/2306.12105
tags:
- failures
- systematic
- language
- individual
- differences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MULTIMON is an automated system that discovers systematic failures
  of multimodal systems, such as CLIP-based image generators. It scrapes a corpus
  for pairs of inputs with similar embeddings but different semantics, then uses a
  language model to categorize these into generalizable failure descriptions.
---

# Mass-Producing Failures of Multimodal Systems with Language Models

## Quick Facts
- arXiv ID: 2306.12105
- Source URL: https://arxiv.org/abs/2306.12105
- Reference count: 40
- Key outcome: MultiMon discovered 14 systematic failures in CLIP-based systems with 50-100% success rate at generating valid failure-inducing inputs that transfer to downstream models

## Executive Summary
MULTIMON is an automated system that discovers systematic failures in multimodal systems like CLIP-based image generators. It works by scraping corpora for text pairs with similar CLIP embeddings but different semantics, then uses language models to categorize these into generalizable failure descriptions. These descriptions are used to generate new test inputs that reliably trigger the identified failures. The method found 14 systematic failures (e.g., ignoring negation, quantifiers, spatial relations) with inputs that transfer to downstream models like Midjourney and DALL-E, producing errors 80% of the time versus 20% for baseline.

## Method Summary
MULTIMON scrapes a corpus for pairs of inputs with high CLIP similarity but low semantic similarity, indicating potential failures. It uses a language model (e.g., GPT-4) to categorize these individual failures into generalizable systematic failure descriptions in natural language. These descriptions are then used to generate new failure-inducing input pairs. The method can be steered toward specific application domains by constraining either the scraping or generation steps. The system tests generated failures on downstream multimodal models to verify transfer of the CLIP-based failures.

## Key Results
- Found 14 systematic failures in CLIP text-encoder including "ignores quantifiers" and "ignores negation"
- Generated failure-inducing inputs succeed 50-100% of the time at triggering the targeted failures
- Transferred failures to downstream models (Midjourney 5.1, DALL-E, VideoFusion) with 80% error rate vs 20% baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Erroneous agreement between CLIP embeddings reveals individual failures
- **Mechanism**: When two semantically different inputs have high CLIP similarity, at least one must be wrong; this indicates a failure in the CLIP embedding space that will propagate to downstream multimodal systems
- **Core assumption**: CLIP embeddings are used as the bottleneck representation in multimodal systems, so errors in CLIP will transfer to generated outputs
- **Evidence anchors**:
  - [abstract] "We use MultiMon to find 14 systematic failures (e.g., 'ignores quantifiers') of the CLIP text-encoder, each comprising hundreds of distinct inputs... Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others."
  - [section] "MULTIMON exploits erroneous agreement. Specifically, we observe that if two inputs produce the same output but have different semantics, at least one of them must be wrong. We can test whether two inputs produce the same output by comparing their CLIP embeddings, since many multimodal models encode inputs with CLIP before generating outputs."
- **Break condition**: If downstream systems use different encoding methods or add significant post-processing that corrects CLIP errors, the transfer will fail

### Mechanism 2
- **Claim**: Language models can categorize individual failures into generalizable systematic failures
- **Mechanism**: GPT-4 analyzes patterns across scraped individual failures and produces natural language descriptions of systematic failure types, which can then be used to generate new failure instances
- **Core assumption**: Large language models can identify and describe abstract patterns from concrete examples, and their outputs are reliable enough to generate new valid instances
- **Evidence anchors**:
  - [abstract] "MULTIMON prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language"
  - [section] "MULTIMON next uses language models to produce human-compatible explanations. Specifically, we use GPT-4 to identify systematic failures: generalizable natural-language descriptions of patterns of failures, from the scraped individual failures"
- **Break condition**: If language models cannot reliably identify patterns from the examples, or if their descriptions are too vague or incorrect, generation will fail

### Mechanism 3
- **Claim**: Steering enables domain-specific failure discovery
- **Mechanism**: By constraining either the scraping (to find systematic failures in a subdomain) or generation (to create individual failures relevant to a subdomain), MULTI MON can find failures relevant to specific applications like self-driving cars
- **Core assumption**: The initial corpus contains sufficient examples from the target domain, or language models can generate relevant examples even for out-of-distribution domains
- **Evidence anchors**:
  - [abstract] "MULTIMON can also steer towards failures relevant to specific use cases, such as self-driving cars"
  - [section] "Steering towards individual failures... lets evaluators produce failures from domains that are completely out-of-distribution relative to the initial scraping dataset... We show this by steering towards failures relevant to 'Pokemon Go', which was released after both of the corpora we test"
- **Break condition**: If the target domain is too far from the corpus distribution, scraping-based steering will fail; if language models lack sufficient knowledge of the domain, generation-based steering will fail

## Foundational Learning

- **Concept**: CLIP embedding similarity as a proxy for output similarity
  - Why needed here: MULTI MON uses CLIP similarity to efficiently identify when two inputs will produce the same output without expensive decoding
  - Quick check question: If two inputs have CLIP text embedding cosine similarity of 0.95, what can we infer about their likely generated outputs in CLIP-based systems?

- **Concept**: Erroneous agreement detection
  - Why needed here: This is the core principle that MULTI MON exploits to find failures - when semantically different inputs agree on output despite semantic differences
  - Quick check question: If inputs A and B have high CLIP similarity but different semantic meanings, what does this tell us about the CLIP embedding space?

- **Concept**: Systematic vs. individual failures
  - Why needed here: Understanding the difference between specific failure instances and generalizable patterns is crucial for MULTI MON's two-stage approach
  - Quick check question: Why is it more valuable to identify "fails to encode negation" as a systematic failure rather than just collecting individual negation failure examples?

## Architecture Onboarding

- **Component map**: Corpus scraper → CLIP similarity checker → Semantic similarity filter → Language model categorizer → Language model generator → Downstream model tester
- **Critical path**: Scraper → Categorizer → Generator → Evaluation pipeline
- **Design tradeoffs**: Using CLIP similarity for efficiency vs. potentially missing failures that CLIP doesn't capture; relying on language models vs. needing better/more interpretable failure descriptions
- **Failure signatures**: Low success rate in generation indicates poor systematic failure descriptions; failure to transfer to downstream models suggests the systematic failures aren't actually capturing the root cause
- **First 3 experiments**:
  1. Run MULTI MON on a small corpus with known failures to verify the pipeline works
  2. Test generation success rates for different systematic failures to identify which types are most reliably captured
  3. Verify transfer to at least one downstream model to confirm CLIP is actually the bottleneck

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MULTI MON's methodology be adapted to evaluate failures in language models or other non-multimodal systems?
- Basis in paper: [explicit] The paper discusses how MULTI MON's pipeline could in principle find failures with any system, since erroneous agreement is agnostic to system architecture, input, or output type.
- Why unresolved: The paper focuses on multimodal systems and does not provide experimental evidence of adapting MULTI MON to language models or other systems.
- What evidence would resolve it: Demonstrating MULTI MON's effectiveness in finding systematic failures in language models or other non-multimodal systems through experiments.

### Open Question 2
- Question: How does the quality of systematic failures found by MULTI MON scale with the diversity and size of the input corpus?
- Basis in paper: [inferred] The paper shows that different corpora find different failures and mentions that "highly diverse corpora or ensembles of corpora produce the best results."
- Why unresolved: The paper does not provide quantitative analysis on how corpus diversity and size affect the number and quality of systematic failures found.
- What evidence would resolve it: Systematic experiments varying corpus diversity and size, measuring the quantity and quality of systematic failures found by MULTI MON.

### Open Question 3
- Question: What is the impact of using different language models for the categorization and generation steps in MULTI MON?
- Basis in paper: [explicit] The paper tests GPT-4, Claude, and GPT-3.5 for categorization and generation, showing variability in performance.
- Why unresolved: The paper does not explore the full range of language models or provide a comprehensive comparison of their impact on MULTI MON's performance.
- What evidence would resolve it: Extensive experiments comparing different language models for categorization and generation, measuring their impact on the quantity and quality of systematic failures found.

## Limitations
- Success rates may not extend to non-CLIP-based multimodal systems
- Computational costs not detailed, could be substantial with multiple LLM queries
- Potential bias from language model reliance - if GPT-4 misses certain failure types, they may never be discovered

## Confidence
- **High Confidence**: CLIP similarity can effectively identify potential failure pairs; generated failure pairs transfer to downstream models; core pipeline works as described
- **Medium Confidence**: Language models can reliably categorize systematic failures from individual examples; steering capability extends to arbitrary domains; 14 identified systematic failures are comprehensive
- **Low Confidence**: Computational efficiency compared to traditional testing methods; scalability to extremely large multimodal systems; robustness against adversarial inputs

## Next Checks
1. Test MULTI MON on non-CLIP-based multimodal systems to validate generalizability beyond CLIP architectures
2. Generate inputs specifically designed to evade MULTI MON's detection to test robustness against adversarial inputs
3. Measure computational cost of MULTI MON versus number and severity of failures discovered to establish practical value compared to manual testing approaches