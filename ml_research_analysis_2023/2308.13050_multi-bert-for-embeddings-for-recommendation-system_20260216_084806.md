---
ver: rpa2
title: Multi-BERT for Embeddings for Recommendation System
arxiv_id: '2308.13050'
source_url: https://arxiv.org/abs/2308.13050
tags:
- sbert
- embeddings
- document
- multi-bert
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MULTI-BERT, a hybrid model combining Sentence-BERT
  (SBERT) and RoBERTa for generating document embeddings in recommendation systems.
  The approach treats sentences as tokens, generates embeddings for them using SBERT,
  and captures both intra-sentence and inter-sentence relations within a document
  using RoBERTa.
---

# Multi-BERT for Embeddings for Recommendation System

## Quick Facts
- arXiv ID: 2308.13050
- Source URL: https://arxiv.org/abs/2308.13050
- Reference count: 2
- MULTI-BERT achieves 0.9413 precision at 5 retrieved documents on book recommendation task

## Executive Summary
This paper introduces MULTI-BERT, a hybrid model combining Sentence-BERT (SBERT) and RoBERTa for generating document embeddings in recommendation systems. The approach treats sentences as tokens, generates embeddings for them using SBERT, and captures both intra-sentence and inter-sentence relations within a document using RoBERTa. The model is evaluated on a book recommendation task using the Goodreads dataset, showing consistent outperformance of SBERT alone with precision of 0.9413 at 5 retrieved documents.

## Method Summary
MULTI-BERT generates document embeddings by first splitting documents into sentences, then using SBERT to create sentence embeddings. These embeddings are clustered using k-means (200 clusters) to create a document vector where each sentence is represented by its cluster ID. This document vector is then processed by a fine-tuned RoBERTa model to generate the final document embedding. The model is evaluated on the Goodreads children's books dataset using precision@k as the primary metric, with relevance determined by genre overlap thresholds.

## Key Results
- MULTI-BERT achieves 0.9413 precision at 5 retrieved documents
- MULTI-BERT achieves 0.7889 precision at 10 retrieved documents
- MULTI-BERT achieves 0.7621 precision at 25 retrieved documents
- Consistently outperforms SBERT baseline across all precision cutoffs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MULTI-BERT outperforms SBERT by combining sentence-level embeddings with inter-sentence context.
- Mechanism: The model first generates sentence embeddings using SBERT, then feeds these embeddings as tokens into RoBERTa to capture inter-sentence relationships. This concatenated output preserves both intra-sentence and inter-sentence semantics.
- Core assumption: Sentence embeddings from SBERT retain sufficient contextual richness when passed as tokens to RoBERTa.
- Evidence anchors:
  - [abstract] "Our approach treats sentences as tokens and generates embeddings for them, allowing the model to capture both intra-sentence and inter-sentence relations within a document."
  - [section] "We then provide this document vector to the fine-tuned RoBERTa model, which generates a corresponding document embedding. This document embedding captures the contextual meaning of each sentence within the document, as well as the relationships between sentences."
  - [corpus] Weak: No direct corpus evidence found for this specific sentence-token-passing approach.

### Mechanism 2
- Claim: Clustering sentence embeddings improves document representation efficiency.
- Mechanism: K-means clustering groups similar sentence embeddings into clusters, creating a "cluster codebook." Each sentence in a document is represented by its cluster ID, forming a document vector that RoBERTa learns to map to a final embedding.
- Core assumption: Cluster IDs sufficiently encode semantic relationships between sentences for RoBERTa to learn meaningful document embeddings.
- Evidence anchors:
  - [section] "We use k-means clustering to these sentence embeddings, assigning each sentence to a cluster... We use this cluster codebook to create a document vector, where each sentence in the document is represented by its corresponding cluster id."
  - [abstract] No direct mention of clustering mechanism.
  - [corpus] Weak: No corpus evidence found for this specific clustering approach in recommendation systems.

### Mechanism 3
- Claim: MULTI-BERT's precision advantage is due to capturing nuanced semantic relations beyond metadata.
- Mechanism: By encoding both sentence content and inter-sentence context, MULTI-BERT embeddings reflect deeper semantic similarity between documents, improving recommendation relevance.
- Core assumption: Semantic similarity encoded in embeddings aligns better with user relevance than metadata-based TF-IDF similarity.
- Evidence anchors:
  - [abstract] "Results show that MULTI-BERT consistently outperforms SBERT in terms of the quality of the generated embeddings... The model is able to capture more nuanced semantic relations within documents, leading to more accurate recommendations."
  - [section] "By capturing both intra-sentence and inter-sentence relations, MULTI-BERT is able to create more powerful and accurate document embeddings than SBERT."
  - [corpus] Weak: No corpus evidence directly comparing semantic relation capture to metadata-based methods.

## Foundational Learning

- Concept: Sentence embeddings and their role in document representation
  - Why needed here: The model relies on SBERT to generate sentence-level embeddings that preserve context, which are then used as input tokens for RoBERTa.
  - Quick check question: What is the difference between SBERT and standard BERT in terms of input processing?

- Concept: Clustering techniques for dimensionality reduction
  - Why needed here: K-means clustering is used to group sentence embeddings into clusters, reducing document representation complexity before feeding into RoBERTa.
  - Quick check question: How does k-means clustering help in representing a document as a vector of cluster IDs?

- Concept: Precision as an evaluation metric in recommendation systems
  - Why needed here: Precision@k is used to measure how many of the top-k recommended documents are relevant, which directly evaluates the quality of generated embeddings.
  - Quick check question: Why is precision at different cutoffs (5, 10, 25) important for understanding recommendation performance?

## Architecture Onboarding

- Component map: Document -> Sentences -> SBERT Embeddings -> Clusters -> RoBERTa -> Final Embedding
- Critical path: Document → Sentences → SBERT Embeddings → Clusters → RoBERTa → Final Embedding
- Design tradeoffs:
  - Clustering vs. direct concatenation: Clustering reduces dimensionality but may lose fine-grained distinctions.
  - RoBERTa vs. other transformers: RoBERTa chosen for its robustness, but other models might yield different performance.
  - Precision vs. recall: High precision at low k values is prioritized, but recall may suffer.
- Failure signatures:
  - Low precision: Clustering too coarse, RoBERTa underfits, or sentence embeddings too generic.
  - High variance in results: Dataset imbalance, insufficient training data, or unstable clustering.
  - Slow inference: Sentence splitting or clustering overhead dominates runtime.
- First 3 experiments:
  1. Baseline: Run SBERT alone on the same dataset and compare precision@k.
  2. Ablation: Remove clustering step and feed all sentence embeddings directly to RoBERTa.
  3. Hyperparameter sweep: Vary number of clusters (e.g., 50, 100, 200, 500) and observe impact on precision.

## Open Questions the Paper Calls Out

- Question: How does MULTI-BERT's performance scale with larger document collections and more diverse genres beyond children's books?
  - Basis in paper: [inferred] The paper notes that the dataset used was relatively small (25,000 records) and focused on children's books, suggesting that performance on larger and more diverse datasets remains untested.
  - Why unresolved: The experiments were limited to a single genre and a modest dataset size, leaving the model's generalizability and scalability unexplored.
  - What evidence would resolve it: Experiments on multi-genre datasets with significantly larger numbers of books and user reviews, comparing MULTI-BERT's precision and recall metrics to baseline models.

- Question: What is the impact of varying the number of clusters (k) in the k-means clustering step on the quality of document embeddings and recommendation accuracy?
  - Basis in paper: [explicit] The paper states that 200 clusters were used in experiments, but does not explore how changing this hyperparameter affects performance.
  - Why unresolved: The clustering step is a key component of MULTI-BERT, yet the sensitivity of the model to this hyperparameter is not investigated.
  - What evidence would resolve it: A systematic evaluation of MULTI-BERT's performance across a range of cluster numbers, analyzing precision, recall, and computational efficiency.

- Question: How does MULTI-BERT perform on tasks beyond book recommendation, such as news article recommendation or product recommendation?
  - Basis in paper: [explicit] The paper mentions future work including applying the model to "more tasks other than recommendations," indicating that its performance on other recommendation domains is unknown.
  - Why unresolved: The model was only evaluated on a book recommendation task, so its effectiveness for other recommendation scenarios is speculative.
  - What evidence would resolve it: Applying MULTI-BERT to diverse recommendation tasks (e.g., news, movies, e-commerce) and comparing its performance to task-specific baselines using appropriate evaluation metrics.

## Limitations

- No direct corpus evidence validating the specific sentence-token-passing mechanism from SBERT to RoBERTa
- Clustering approach lacks sensitivity analysis showing how different cluster counts affect precision
- Relevance metric based on genre overlap (>40% threshold) is simplistic and may not reflect actual user preferences

## Confidence

- **High confidence**: The architectural framework combining SBERT and RoBERTa is technically sound and the precision metrics are clearly reported.
- **Medium confidence**: The claim that MULTI-BERT captures "more nuanced semantic relations" is supported by superior precision scores but lacks direct evidence of semantic superiority over alternatives.
- **Low confidence**: The clustering mechanism's contribution to final embedding quality is asserted but not independently validated through ablation studies.

## Next Checks

1. Conduct ablation study removing the clustering step to quantify its contribution to precision improvement.
2. Compare MULTI-BERT performance against standard BERT, TF-IDF, and Doc2Vec baselines using identical evaluation protocol and dataset.
3. Perform user study or external validation to verify that high-precision recommendations align with actual user satisfaction beyond genre-based relevance.