---
ver: rpa2
title: A Performance Evaluation of a Quantized Large Language Model on Various Smartphones
arxiv_id: '2312.12472'
source_url: https://arxiv.org/abs/2312.12472
tags:
- inference
- performance
- iphone
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the feasibility of running large language
  models (LLMs) on smartphones by measuring inference performance across various iPhone
  models. The study focuses on on-device LLM inference to address privacy, security,
  and connectivity limitations of cloud-based models.
---

# A Performance Evaluation of a Quantized Large Language Model on Various Smartphones

## Quick Facts
- arXiv ID: 2312.12472
- Source URL: https://arxiv.org/abs/2312.12472
- Reference count: 17
- Key outcome: This paper evaluates the feasibility of running large language models (LLMs) on smartphones by measuring inference performance across various iPhone models. The study focuses on on-device LLM inference to address privacy, security, and connectivity limitations of cloud-based models. The authors used quantization to compress a 7B parameter LLM (Orca Mini v3) to 3 bits, reducing its size to 2.95 GiB for mobile deployment. Experiments were conducted on five iPhone models (14, 14 Pro, 14 Pro Max, 15, and 15 Pro Max) with consistent testing conditions to measure sampling, prompt decoding, and inference speeds. Results showed that newer iPhone models with advanced chipsets (A16 Bionic and A17 Pro) demonstrated improved performance, particularly in prompt decoding and inference tasks, with the iPhone 15 Pro Max achieving a 20% higher sampling rate compared to other models. However, inference performance on the iPhone 15 Pro Max was unexpectedly lower than anticipated, possibly due to hardware or software optimizations. The study highlights the potential of on-device LLMs for secure and efficient AI applications while identifying areas for further optimization in power management and system integration.

## Executive Summary
This paper evaluates the performance of a quantized 7B parameter LLM (Orca Mini v3) running on various iPhone models to assess the feasibility of on-device large language model inference. The study focuses on measuring sampling, prompt decoding, and inference speeds across five iPhone models (14, 14 Pro, 14 Pro Max, 15, and 15 Pro Max) with consistent testing conditions. The authors used 3-bit quantization to reduce the model size to 2.95 GiB, enabling deployment on devices with at least 6 GiB of RAM. Results show that newer iPhone models with advanced chipsets (A16 Bionic and A17 Pro) demonstrate improved performance, with the iPhone 15 Pro Max achieving a 20% higher sampling rate compared to other models. However, inference performance on the iPhone 15 Pro Max was unexpectedly lower than anticipated, possibly due to hardware or software optimizations. The study highlights the potential of on-device LLMs for secure and efficient AI applications while identifying areas for further optimization in power management and system integration.

## Method Summary
The authors evaluated a quantized 7B parameter LLM (Orca Mini v3) on five iPhone models (14, 14 Pro, 14 Pro Max, 15, and 15 Pro Max) with at least 6 GiB RAM. The model was compressed using 3-bit quantization to reduce its size to 2.95 GiB for mobile deployment. Tests measured sampling, prompt decoding, and inference speeds with consistent configurations: 8 threads, 1024 context length, 512 batch length, greedy sampling, and five-second delay between iterations. The benchmarking suite utilized Metal GPU shader API for hardware acceleration, with sampling running on CPU and prompt decoding/inference on GPU.

## Key Results
- Quantized 3-bit LLM (Orca Mini v3) successfully deployed on iPhones with 6 GiB+ RAM
- iPhone 15 Pro Max achieved 20% higher sampling rate compared to other models
- Inference performance on iPhone 15 Pro Max was unexpectedly lower than anticipated
- Newer iPhone models with A16 Bionic and A17 Pro chipsets demonstrated improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization reduces model size sufficiently for mobile deployment while maintaining acceptable performance.
- Mechanism: 3-bit quantization of a 7B parameter LLM (Orca Mini v3) reduces its size to 2.95 GiB, enabling storage on smartphones with 6 GiB+ RAM.
- Core assumption: Quantization-induced precision loss has minimal impact on inference quality for the tested model and use case.
- Evidence anchors:
  - [section]: "Using quantization to compress a 7B parameter LLM (Orca Mini v3) to 3 bits, reducing its size to 2.95 GiB for mobile deployment."
  - [corpus]: Weak corpus evidence - no direct citations about quantization quality retention for mobile LLMs found in related papers.
- Break condition: If quantization causes unacceptable degradation in model quality or if the quantized model exceeds available memory on target devices.

### Mechanism 2
- Claim: Modern smartphone chipsets provide sufficient computational power for on-device LLM inference.
- Mechanism: Advanced GPUs and CPUs in recent iPhone models (A16 Bionic and A17 Pro chipsets) enable efficient processing of quantized LLMs.
- Core assumption: The computational capabilities of modern smartphone hardware are adequate for running inference on compressed LLMs.
- Evidence anchors:
  - [section]: "Results showed that newer iPhone models with advanced chipsets (A16 Bionic and A17 Pro) demonstrated improved performance..."
  - [corpus]: "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation Linking" - directly addresses fast LLM inference on smartphones.
- Break condition: If computational demands exceed hardware capabilities, leading to unacceptable performance or thermal throttling.

### Mechanism 3
- Claim: On-device LLM inference addresses privacy, security, and connectivity limitations of cloud-based models.
- Mechanism: By running inference locally on the device, sensitive data never leaves the smartphone, eliminating privacy risks and connectivity dependencies.
- Core assumption: Users prioritize privacy and offline functionality enough to accept potential performance trade-offs.
- Evidence anchors:
  - [abstract]: "On-device LLMs offer solutions to privacy, security, and connectivity challenges inherent in cloud-based models."
  - [section]: "Most importantly, an on-device LLM provides privacy and security [2]. Transmitting personal data entered into health applications or private conversations with a personal assistant over the internet is a vulnerability."
  - [corpus]: "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting" - directly addresses privacy-preserving AI applications.
- Break condition: If users prioritize performance over privacy, or if on-device models cannot match cloud-based model quality.

## Foundational Learning

- Concept: Quantization in neural networks
  - Why needed here: Understanding how reducing bit-width affects model size and performance is crucial for optimizing mobile LLM deployment.
  - Quick check question: How does 3-bit quantization affect the memory footprint of a 7B parameter model?

- Concept: GPU vs. CPU performance characteristics
  - Why needed here: Different components of LLM inference (sampling, prompt decoding, inference) utilize different hardware, affecting overall performance.
  - Quick check question: Why might prompt decoding run faster on GPU than CPU for LLM inference?

- Concept: Thermal management in mobile devices
  - Why needed here: Sustained high-performance computing can cause thermal throttling, impacting LLM inference speeds.
  - Quick check question: How does thermal state affect the sustained performance of GPU-intensive tasks on smartphones?

## Architecture Onboarding

- Component map:
  - Model: Quantized 7B parameter LLM (Orca Mini v3)
  - Hardware: iPhone models with A16 Bionic/A17 Pro chipsets, 6 GiB+ RAM
  - Software: Metal GPU shader API, custom benchmarking suite
  - Workflow: Sampling on CPU, prompt decoding and inference on GPU

- Critical path:
  1. Load quantized model into device memory
  2. Process input prompt (prompt decoding on GPU)
  3. Generate tokens autoregressively (inference on GPU)
  4. Combine results for output

- Design tradeoffs:
  - Model size vs. quality: More aggressive quantization reduces size but may degrade performance
  - Hardware utilization: Balancing CPU and GPU workloads for optimal throughput
  - Thermal management: Performance vs. sustained operation under thermal constraints

- Failure signatures:
  - Unexpected performance drops (e.g., iPhone 15 Pro Max results)
  - Thermal throttling indicated by increased inference times over iterations
  - Memory allocation failures for larger models or contexts

- First 3 experiments:
  1. Benchmark quantization levels (2-bit, 3-bit, 4-bit) to find optimal size-quality tradeoff
  2. Test different sampling strategies (greedy, top-k, temperature-based) for quality vs. speed
  3. Implement and measure the impact of dynamic batching on throughput

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between GPU thermal throttling and sustained inference performance across different iPhone models?
- Basis in paper: [inferred] The paper discusses thermal states (nominal, fair, serious, critical) and their impact on performance, noting that inference performance drops significantly in serious/critical states.
- Why unresolved: The study tested thermal effects but didn't systematically analyze how thermal throttling specifically impacts long-term inference performance or battery temperature curves.
- What evidence would resolve it: Detailed thermal profiling during extended inference sessions showing GPU temperature, clock speeds, and performance metrics over time for each iPhone model.

### Open Question 2
- Question: What specific hardware or software factors caused the iPhone 15 Pro Max to underperform in inference despite having the most advanced chipset?
- Basis in paper: [explicit] The paper explicitly notes that iPhone 15 Pro Max had unexpectedly lower inference rates than anticipated, with potential causes including GPU architecture changes, driver regressions, or suboptimal Metal shader optimization.
- Why unresolved: The authors identified potential causes but couldn't determine the exact reason due to time and resource constraints.
- What evidence would resolve it: Comparative analysis of GPU utilization, memory bandwidth, Metal shader compilation, and instruction-level profiling across iPhone models during inference tasks.

### Open Question 3
- Question: How does quantization precision (3-bit vs 4-bit vs 8-bit) affect inference quality and performance on mobile devices?
- Basis in paper: [inferred] The paper used 3-bit quantization without evaluating how different quantization levels impact output quality, only mentioning this as a future research direction.
- Why unresolved: The study focused on feasibility rather than comprehensive evaluation of quantization trade-offs.
- What evidence would resolve it: Side-by-side comparison of inference speed, memory usage, and output quality (perplexity, BLEU scores) across multiple quantization levels on the same device.

### Open Question 4
- Question: What is the optimal balance between thread count and inference latency for on-device LLM processing?
- Basis in paper: [explicit] The study used 8 threads for Metal shader scheduling but didn't explore how different thread counts affect performance.
- Why unresolved: The thread count was fixed rather than experimentally optimized for each device.
- What evidence would resolve it: Performance profiling with varying thread counts to identify the optimal configuration for each iPhone model and quantization level.

## Limitations

- Single Model Evaluation: The study uses only one quantized LLM (Orca Mini v3 7B, 3-bit), limiting conclusions about the general feasibility of on-device LLMs.
- Hardware Specificity: Testing was restricted to five recent iPhone models, leaving open questions about performance on Android devices, older iPhones, or devices with different RAM configurations.
- Quality Assessment Gap: While performance metrics were measured, no evaluation of inference quality or accuracy was reported.

## Confidence

- High Confidence: The fundamental claim that quantized LLMs can run on modern smartphones is well-supported by the empirical data across multiple devices.
- Medium Confidence: The observation that newer iPhone models with advanced chipsets demonstrate improved performance is supported but may be influenced by unmeasured factors like thermal management differences.
- Low Confidence: The specific performance degradation observed on the iPhone 15 Pro Max lacks sufficient explanation and may be due to uncontrolled variables.

## Next Checks

1. **Cross-Model Validation**: Test the same quantization and deployment pipeline with multiple LLM architectures (Llama, Mistral, Phi) to verify if performance patterns hold across different model families and sizes.

2. **Quality Preservation Assessment**: Implement automated evaluation metrics (e.g., HELM benchmarks, task-specific accuracy tests) to quantify the quality degradation from quantization and establish quality-performance tradeoffs.

3. **Broader Hardware Testing**: Extend testing to include Android devices, older iPhone models, and devices with varying RAM configurations to establish minimum hardware requirements and identify hardware-specific bottlenecks.