---
ver: rpa2
title: Global Transformer Architecture for Indoor Room Temperature Forecasting
arxiv_id: '2310.20476'
source_url: https://arxiv.org/abs/2310.20476
tags:
- transformer
- room
- temperature
- scaling
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a global Transformer architecture for indoor
  temperature forecasting in multi-room buildings, aiming at optimizing energy consumption
  and reducing greenhouse gas emissions associated with HVAC systems. The proposed
  approach provides a novel solution to enhance the accuracy and efficiency of temperature
  forecasting, serving as a valuable tool to optimize energy consumption and decrease
  greenhouse gas emissions in the building sector.
---

# Global Transformer Architecture for Indoor Room Temperature Forecasting

## Quick Facts
- arXiv ID: 2310.20476
- Source URL: https://arxiv.org/abs/2310.20476
- Authors: 
- Reference count: 11
- Key outcome: Global Transformer architecture trained on entire dataset of 133 rooms achieves 16-40% performance improvement over local models and LSTM baselines for indoor temperature forecasting.

## Executive Summary
This paper presents a global Transformer architecture for indoor temperature forecasting in multi-room buildings, addressing the challenge of optimizing energy consumption and reducing greenhouse gas emissions from HVAC systems. The proposed approach trains a single unified model on the entire dataset across all rooms, eliminating the need for multiple room-specific models while significantly improving predictive performance. The Transformer architecture offers advantages over LSTM networks through its ability to handle variable-length inputs, incorporate future covariates, and efficiently parallelize computations across large-scale datasets.

## Method Summary
The proposed method uses a global Transformer architecture with encoder-decoder structure, incorporating rotary positional encoding, gated linear units activation, ScaleNorm normalization, and PreNorm. The model takes as input past covariates, target history, and room ID embeddings, which are concatenated and processed through the Transformer layers. Training targets are residuals relative to a persistence baseline, and the model is trained on the entire dataset encompassing all 133 rooms. The architecture uses 4 encoder and 4 decoder layers with attention dimension of 32, determined through hyperparameter search. Room IDs are embedded as learnable vectors and incorporated into the input sequence to allow the model to condition predictions on room identity while sharing parameters across rooms.

## Key Results
- Global Transformer model achieves 16% performance improvement over LSTM with common scaling and 40% improvement with individual scaling
- Transformer architecture outperforms LSTM baseline with statistical significance across both scaling strategies
- Room embedding significantly improves global Transformer performance under common scaling with statistical significance
- Global models outperform local room-specific models while simplifying deployment and maintenance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global Transformer model outperforms both local models and LSTM baselines due to its ability to capture cross-room correlations while using shared representations.
- Mechanism: By training on the entire dataset across all rooms, the Transformer can learn common temporal patterns and thermal dynamics that apply broadly, while the attention mechanism allows it to adapt to room-specific features via the rotary positional encoding and room embedding.
- Core assumption: Thermal behavior across different rooms in the same building shares enough structure that a single global model can generalize better than per-room models.
- Evidence anchors:
  - [abstract] The proposed global Transformer architecture can be trained on the entire dataset encompassing all rooms, eliminating the need for multiple room-specific models, significantly improving predictive performance, and simplifying deployment and maintenance.
  - [section] Our results also show that the global models outperform the local models. Specifically, the global model lead to a performance increase of 16% for the LSTM model and 40% for the Transformer model for common scaling.
  - [corpus] The corpus contains several related works on deep learning for building temperature forecasting, but no direct comparison of global vs. local models in multi-room settings.
- Break condition: If thermal behavior between rooms is too dissimilar (e.g., drastically different HVAC zones or building sections), the shared representation may introduce harmful bias, reducing accuracy.

### Mechanism 2
- Claim: Transformer architecture with rotary positional encoding and gated linear units improves sequence modeling over LSTM for multi-horizon forecasting.
- Mechanism: The Transformer's self-attention layers can model long-range dependencies and parallelize computations efficiently. Rotary positional encoding allows the model to retain order information without positional embeddings that grow with sequence length. Gated linear units help control gradient flow and improve convergence.
- Core assumption: Indoor temperature time series exhibit long-range dependencies that benefit from attention-based architectures rather than recurrent models.
- Evidence anchors:
  - [abstract] The Transformer architecture offers several advantages over other statistical machine learning approaches and other deep learning based architecture, such as LSTM networks, including the ability to manage inputs of different lengths and include future covariates.
  - [section] We find that the best Transformer model outperforms the best LSTM model for both common and individual scaling, with statistical significance.
  - [corpus] Assumption: The corpus contains a paper titled "Predicting the performance of hybrid ventilation in buildings using a multivariate attention-based biLSTM Encoder-Decoder neural network," suggesting attention mechanisms are recognized as valuable in this domain.
- Break condition: If the dataset is too small or lacks sufficient temporal complexity, the computational overhead of Transformers may not yield performance gains over simpler models.

### Mechanism 3
- Claim: The room embedding mechanism allows a unified architecture to model room-specific thermal behavior without requiring separate models.
- Mechanism: Each room is assigned a unique ID, and this ID is transformed into a learnable embedding that is incorporated into the Transformer's input sequence. This allows the model to condition its predictions on the room identity while sharing the bulk of the learned parameters.
- Core assumption: Room identity can be effectively encoded as a discrete categorical variable and learned as part of the model without manual feature engineering.
- Evidence anchors:
  - [abstract] To incorporate the room ID information into the unified architecture, we introduce a novel approach that avoids the need for a separate model for each room.
  - [section] In terms of the impact of the room embedding, we find that for common scaling, including the room embedding significantly improves the performance of the global transformer model, with statistical significance.
  - [corpus] Assumption: No direct corpus evidence for room embedding usage in similar works; this appears to be a novel contribution.
- Break condition: If the number of rooms becomes very large, the embedding layer may become unwieldy, or if rooms are too heterogeneous, a fixed embedding may not capture all necessary variance.

## Foundational Learning

- Concept: Positional encoding in Transformers
  - Why needed here: Indoor temperature forecasting is a time-series task where the temporal order of observations is critical. Transformers require explicit positional information since they lack recurrence.
  - Quick check question: Why is rotary positional encoding preferred over sinusoidal encoding in this context?

- Concept: Multi-head attention and cross-attention
  - Why needed here: Self-attention allows the model to weigh the importance of past observations when predicting future temperatures. Cross-attention enables the decoder to focus on relevant encoder states conditioned on future covariates.
  - Quick check question: How does the cross-attention mechanism help incorporate future weather forecasts into the model?

- Concept: Residual modeling relative to persistence
  - Why needed here: Indoor temperatures are highly autocorrelated, so predicting the delta from the last observed value is more stable and easier to learn than the absolute value.
  - Quick check question: What advantage does residual modeling provide over direct forecasting in this application?

## Architecture Onboarding

- Component map: Input → Room ID embedding → Concatenation with time series → Encoder (self-attention) → Decoder (cross-attention with future covariates) → Output projection → Loss computation.

- Critical path: Input → Room ID embedding → Concatenation with time series → Encoder (self-attention) → Decoder (cross-attention with future covariates) → Output projection → Loss computation.

- Design tradeoffs:
  - Global vs. local: Global reduces maintenance but may sacrifice room-specific accuracy; here global wins significantly.
  - Common vs. individual scaling: Common scaling simplifies batch inference; choice depends on model type.
  - Encoder depth: 4 layers chosen as a balance between expressiveness and overfitting risk.
  - Attention dimension: 32 chosen via hyperparameter search; smaller may underfit, larger may overfit.

- Failure signatures:
  - High validation error with low training error: overfitting; reduce model size or add regularization.
  - Persistent bias in certain rooms: room embedding may not be expressive enough; consider larger embedding dimension.
  - Poor long-horizon forecasts: attention may not capture long-term dependencies; increase input window or adjust attention mechanism.

- First 3 experiments:
  1. Reproduce the baseline persistence model to establish the naive error floor.
  2. Train the global Transformer with and without room embedding to quantify its impact.
  3. Compare common vs. individual scaling strategies for the Transformer to determine optimal preprocessing.

## Open Questions the Paper Calls Out
- How does the room embedding space topology affect the model's ability to generalize across different rooms?
- What are the benefits and limitations of using pretrained Transformer models for generating synthetic data in indoor temperature forecasting?
- How do interpretability techniques improve the understanding and performance of the proposed Transformer architecture for indoor temperature forecasting?

## Limitations
- Limited exploration of model robustness to sensor failures or data gaps
- Absence of real-world deployment validation and comparison against commercial building energy management systems
- Room embedding approach may not scale efficiently to buildings with hundreds of rooms

## Confidence
- **High confidence**: The global Transformer architecture outperforms local models (16-40% improvement) - supported by direct experimental results with statistical significance testing.
- **Medium confidence**: The superiority of Transformer over LSTM is attributable to self-attention mechanisms and rotary positional encoding - mechanistic explanation is plausible but not definitively proven through ablation studies.
- **Low confidence**: The room embedding approach will generalize to buildings with vastly different thermal characteristics or to non-building indoor environments - this remains speculative without broader validation.

## Next Checks
1. Conduct ablation studies removing rotary positional encoding and gated linear units to quantify their specific contributions to performance gains.
2. Test model performance on held-out extreme weather events (heat waves, cold snaps) to assess robustness to outlier conditions.
3. Evaluate computational efficiency and accuracy trade-offs when scaling to buildings with 500+ rooms to determine practical limits of the global architecture approach.