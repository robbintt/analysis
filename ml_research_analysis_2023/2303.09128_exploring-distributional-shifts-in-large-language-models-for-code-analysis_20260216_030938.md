---
ver: rpa2
title: Exploring Distributional Shifts in Large Language Models for Code Analysis
arxiv_id: '2303.09128'
source_url: https://arxiv.org/abs/2303.09128
tags:
- code
- codet5
- data
- codex
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates two large language models\
  \ for code\u2014CodeT5 and Codex\u2014on code summarization and code generation\
  \ tasks, examining domain shifts across organizations, projects, and modules. It\
  \ finds that both models suffer significant performance drops when applied to out-of-domain\
  \ data."
---

# Exploring Distributional Shifts in Large Language Models for Code Analysis

## Quick Facts
- arXiv ID: 2303.09128
- Source URL: https://arxiv.org/abs/2303.09128
- Authors: 
- Reference count: 36
- Key outcome: Large language models for code suffer significant performance drops under distributional shifts, but retrieval-based few-shot adaptation often outperforms direct fine-tuning in low-data scenarios.

## Executive Summary
This study systematically evaluates CodeT5 and Codex on code summarization and generation tasks under distribution shifts across organizations, projects, and modules. Both models show significant performance degradation when applied to out-of-domain data. The research demonstrates that retrieving similar labeled examples from training data for few-shot adaptation yields strong performance, often exceeding direct in-domain fine-tuning. CodeT5 can be adapted to multiple domains simultaneously with minimal performance loss, while Codex requires individual example-level adaptation, highlighting fundamental architectural differences in their adaptation strategies.

## Method Summary
The study evaluates two large language models (CodeT5 and Codex) on code summarization and generation tasks using the CodeSearchNet JavaScript dataset. Models are tested under zero-shot, few-shot, and prompt tuning scenarios, with distribution shifts examined at three hierarchical levels: organization, project, and module. A retrieval-based stratified example selection method uses cosine similarity on CodeT5 embeddings to find supervision examples. The research compares full fine-tuning with parameter-efficient LoRA adaptation and evaluates performance using BLEU, CodeBLEU, chrF, and RougeL metrics.

## Key Results
- Both CodeT5 and Codex suffer significant performance drops when applied to out-of-domain data across all three shift levels
- Few-shot adaptation using retrieved similar examples often outperforms direct in-domain fine-tuning for low-data scenarios
- CodeT5 adapted to multiple domains simultaneously performs nearly as well as per-domain adaptation for code generation
- Codex requires individual example-level adaptation to maintain performance, unlike CodeT5's capability for batch-level adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot adaptation using retrieved similar examples outperforms direct in-domain fine-tuning for low-data scenarios
- Mechanism: Retrieval-based stratified example selection finds out-of-domain examples semantically similar to test queries, providing effective supervision without needing in-domain labeled data
- Core assumption: Similarity-based retrieval captures meaningful semantic alignment between code examples across domains
- Evidence anchors:
  - "combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance"
  - "adapting models with few in-domain examples, on average, leads to an improvement of over 10 BLEU score points"
- Break condition: Retrieval fails when code semantics are too domain-specific

### Mechanism 2
- Claim: CodeT5 can be adapted to multiple domains simultaneously with minimal performance loss
- Mechanism: Shared parameter updates across retrieved examples from multiple domains create a generalized model that retains cross-domain capability
- Core assumption: Learned representations from multiple domains are complementary rather than conflicting
- Evidence anchors:
  - "CodeT5 adapted to multiple domains simultaneously performs nearly as well as per-domain adaptation"
  - "training a single model on combined retrieved samples results in a moderate drop in performance for code summarization"
- Break condition: Performance degrades when domains are too dissimilar

### Mechanism 3
- Claim: Codex requires individual example-level adaptation while CodeT5 can handle domain-level adaptation
- Mechanism: Codex's decoder-only architecture is highly sensitive to demonstration order, requiring tailored examples per query
- Core assumption: Architectural differences between GPT-style decoders and T5-style encoder-decoders create different adaptation requirements
- Evidence anchors:
  - "Codex requires individual example-level adaptation to maintain performance"
  - "for Codex replacing demonstrations selected for individual examples with those selected for a domain introduce too much noise"
- Break condition: When test distribution becomes too heterogeneous

## Foundational Learning

- Concept: Distributional shift in hierarchical software data
  - Why needed here: The paper examines generalization across organizations, projects, and modules
  - Quick check question: What are the three hierarchical levels of distribution shift examined in this study?

- Concept: Retrieval-based few-shot learning
  - Why needed here: Core innovation uses similarity-based retrieval to find supervision examples
  - Quick check question: How does stratified example retrieval differ from random sampling?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: Paper compares LoRA with full fine-tuning for domain adaptation
  - Quick check question: What architectural modification enables LoRA to achieve comparable performance?

## Architecture Onboarding

- Component map: CodeT5 (700M parameters, encoder-decoder) → Retrieval module (cosine similarity on CodeT5 embeddings) → Adaptation stage (fine-tuning or LoRA) → Evaluation (BLEU/CodeBLEU). Codex (100B+ parameters, decoder-only) → Retrieval module → In-context learning with demonstrations → Evaluation.
- Critical path: Retrieval → Adaptation → Evaluation, with retrieval quality being the primary bottleneck
- Design tradeoffs: CodeT5 allows batch adaptation to multiple domains but requires storage of adapted models; Codex requires no weight updates but needs careful demonstration selection per example
- Failure signatures: Retrieval module returns irrelevant examples, adaptation stage shows minimal performance improvement, or evaluation metrics plateau despite additional training
- First 3 experiments:
  1. Run retrieval module on a small test set to verify semantic similarity scores are meaningful
  2. Perform single-domain adaptation with 4 retrieved examples to establish baseline performance
  3. Test multi-domain simultaneous adaptation for CodeT5 to verify minimal performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance degrade across the different levels of distribution shift and which level presents the most significant challenge?
- Basis in paper: The paper systematically examines generalization across organizations, projects, and modules but doesn't provide detailed comparative analysis of which level causes the most severe performance degradation
- Why unresolved: The paper establishes that distribution shifts occur at all three levels but lacks detailed breakdown of performance impact
- What evidence would resolve it: Detailed BLEU/CodeBLEU scores for each shift level (org, repo, folder)

### Open Question 2
- Question: What is the optimal number of retrieved examples per test instance for retrieval-based adaptation?
- Basis in paper: The paper experiments with k=4, 8, or 32 examples but doesn't systematically determine the optimal k value
- Why unresolved: Performance plateaus after certain number of examples but optimal trade-off isn't identified
- What evidence would resolve it: Systematic study varying k across all three shift levels with performance curves

### Open Question 3
- Question: How does combined domain adaptation compare to individual domain adaptation across different types of distribution shifts?
- Basis in paper: CodeT5 adapted to multiple domains performs nearly as well as per-domain adaptation for code generation
- Why unresolved: Paper provides aggregate results but doesn't analyze how this tradeoff varies across shift levels
- What evidence would resolve it: Detailed comparative results showing performance differences at each shift level

### Open Question 4
- Question: What is the relative effectiveness of LoRA versus full fine-tuning for domain adaptation?
- Basis in paper: Paper mentions experimenting with LoRA but primarily reports results for full fine-tuning
- Why unresolved: Limited comparison of LoRA versus full fine-tuning across different shift levels
- What evidence would resolve it: Comprehensive comparison of LoRA versus full fine-tuning results across all three shift levels

## Limitations

- Evaluation focuses on JavaScript code from CodeSearchNet, limiting generalization to other programming languages
- CodeT5 (700M parameters) is significantly smaller than Codex (100B+ parameters), creating architectural mismatch
- Retrieval-based adaptation assumes semantic similarity between domains without validating functional relevance
- Experimental setup uses domains with minimum 96 samples, not reflecting true low-resource scenarios

## Confidence

**High Confidence**: Few-shot adaptation using retrieved similar examples often outperforms direct in-domain fine-tuning for low-data scenarios
**Medium Confidence**: CodeT5 can be simultaneously adapted to multiple domains with minimal performance loss
**Low Confidence**: Codex requires individual example-level adaptation

## Next Checks

1. **Cross-language validation**: Replicate domain shift experiments with Python and Java from CodeSearchNet to verify retrieval-based adaptation benefits transfer across programming languages
2. **Scale-down validation**: Test retrieval-based adaptation with domains containing only 4-8 examples to confirm performance advantages in true low-resource settings
3. **Architectural ablation study**: Conduct controlled experiments comparing CodeT5 and Codex adaptation strategies using matched parameter counts to isolate effects of model architecture from model scale