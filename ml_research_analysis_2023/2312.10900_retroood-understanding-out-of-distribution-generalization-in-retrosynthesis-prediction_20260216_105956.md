---
ver: rpa2
title: 'RetroOOD: Understanding Out-of-Distribution Generalization in Retrosynthesis
  Prediction'
arxiv_id: '2312.10900'
source_url: https://arxiv.org/abs/2312.10900
tags:
- prediction
- retrosynthesis
- shift
- reaction
- template
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates out-of-distribution (OOD) generalization
  in retrosynthesis prediction. The authors identify two types of distribution shifts:
  label shift in retro-synthesis strategies and covariate shift in target molecules.'
---

# RetroOOD: Understanding Out-of-Distribution Generalization in Retrosynthesis Prediction

## Quick Facts
- arXiv ID: 2312.10900
- Source URL: https://arxiv.org/abs/2312.10900
- Reference count: 26
- Key outcome: Proposes model-agnostic techniques (invariant learning, concept enhancement) that improve OOD generalization in retrosynthesis by 4.6% average performance

## Executive Summary
This paper investigates out-of-distribution (OOD) generalization in retrosynthesis prediction, identifying two key distribution shift types: label shift in retro-synthesis strategies and covariate shift in target molecules. The authors construct multi-dimensional OOD benchmark datasets and systematically compare state-of-the-art retrosynthesis models under these shifts. They propose two model-agnostic techniques - invariant learning for covariate shift and concept enhancement for label shift - and demonstrate that these techniques can improve average performance by 4.6%. The established benchmarks and insights provide a foundation for further research in retrosynthesis prediction under distributional shifts.

## Method Summary
The paper focuses on OOD generalization in retrosynthesis prediction, where distribution shifts occur in label (retro-synthesis strategies) and covariate (target molecules). Two OOD datasets are constructed from USPTO-50k: USPTO-50k_T for label shift (minimal-templates vs retro-templates) and USPTO-50k_M for covariate shift (molecular size and scaffold). Five baseline models (GLN, MT, GraphRetro, RetroComposer, MHN) are trained on ID splits and evaluated on both ID and OOD splits. Two model-agnostic techniques are proposed: invariant learning using IRM regularization for covariate shift, and concept enhancement using EBM-based modeling for label shift.

## Key Results
- Proposed invariant learning and concept enhancement techniques improve average OOD performance by 4.6%
- Template-free models show better zero-shot generalization due to ability to "invent" new minimal-templates
- Template granularity significantly impacts performance, with retro-templates showing better generalization than minimal-templates
- Label shift in retro-synthesis strategies is identified as a major source of OOD performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Label shift occurs because training data exposes only one feasible template per reaction, even though multiple templates could work. The model learns to associate target molecules with a single template, but in OOD settings other templates become valid, causing performance degradation. Core assumption: Template applicability is binary in training but continuous in reality. Break condition: EBM fails to accurately model template applicability distribution.

### Mechanism 2
Covariate shift occurs when target molecule complexity changes between training and test sets, affecting the model's ability to identify correct disconnection sites. Larger molecules introduce variant features that distract from invariant features crucial for predicting the correct retro-strategy. Core assumption: Certain substructures are invariant features crucial for prediction. Break condition: Invariant learning cannot disentangle variant from invariant features.

### Mechanism 3
Template-free models can "invent" new minimal-templates by combining components of different templates learned during training. The implicit mapping learned by template-free models is flexible enough to combine template components in novel ways to generate unseen templates. Core assumption: Implicit mapping is flexible enough for novel template generation. Break condition: Template-free model cannot generalize beyond template combinations seen during training.

## Foundational Learning

- Concept: Invariant Risk Minimization (IRM)
  - Why needed here: To learn feature representations simultaneously optimal across different domains (molecular complexities), addressing covariate shift
  - Quick check question: What is the key difference between ERM and IRM in terms of how they handle distribution shifts?

- Concept: Energy-based Models (EBM)
  - Why needed here: To model continuous probability distribution of template applicability, addressing label shift by capturing unobserved feasible reactions
  - Quick check question: How does an EBM represent the likelihood of a probability distribution differently from a traditional probabilistic model?

- Concept: K-hop subgraph extraction
  - Why needed here: To create tractable training losses for EBM by selecting informative subsets of bipartite graph containing sufficient neighborhood information
  - Quick check question: Why is it computationally infeasible to use full set of unobserved reactions as negative samples in EBM training?

## Architecture Onboarding

- Component map: Target molecule → Invariant learning module → Concept enhancement module → Baseline model → Ranked precursors
- Critical path: Target molecule → IRM regularization (for covariate shift) → EBM with k-hop extraction (for label shift) → Baseline model → Precursors
- Design tradeoffs:
  - Template granularity: Minimal-templates (radius=0) vs retro-templates (radius=1+) - affects generalization ability
  - IRM application: Applying to all loss components vs only disconnection site prediction - affects optimization stability
  - EBM negative sampling: Number of negative samples per subgraph - affects training efficiency vs model quality
- Failure signatures:
  - Performance degradation on OOD sets similar to baselines (invariant learning ineffective)
  - Model overfits to training biases (concept enhancement ineffective)
  - Training instability or slow convergence (IRM penalty too strong)
- First 3 experiments:
  1. Apply IRM regularization only to disconnection site prediction loss for each baseline model and measure performance on covariate shift OOD sets
  2. Implement concept enhancement with n=5 (top-5 reactions) and measure performance on label shift OOD sets for both template granularities
  3. Compare performance of template-free models vs template-based models on minimal-template OOD sets to test template invention capability

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of template granularity (minimal-template vs. retro-template) specifically impact the generalization ability of template-free models in zero-shot retrosynthesis prediction? While the paper identifies the impact of template granularity, it does not provide a detailed analysis of how different granularities specifically influence the zero-shot generalization capability of template-free models. Systematic experiments comparing template-free models across various template granularities on OOD datasets, coupled with analysis of structural and semantic differences, would resolve this.

### Open Question 2
Can the concept enhancement technique be effectively extended to other molecular generation tasks beyond retrosynthesis, such as drug discovery or material design? The authors propose a concept enhancement method to address label shift in retrosynthesis by transforming binary criteria into continuous probabilistic models. This approach could potentially be adapted to other molecular generation tasks. Empirical studies applying the technique to other tasks would demonstrate its generalizability.

### Open Question 3
What are the limitations of the invariant learning approach in capturing substructure invariance, and how can these limitations be addressed to improve performance under covariate shift? The authors acknowledge that the invariant learning approach using IRM regularization shows marginal improvements and attribute this to concept drift and selection bias in training data. A comprehensive analysis of specific limitations, along with experimental results demonstrating effectiveness of proposed solutions, would resolve this.

## Limitations
- Model-agnostic techniques may not be optimally tuned for each baseline model
- No analysis of computational overhead introduced by IRM and EBM components
- Limited discussion of failure cases where techniques might degrade performance

## Confidence
- Confidence: Low on generalizability beyond USPTO-50k dataset (paper focuses exclusively on single dataset without validation on external datasets)
- Confidence: Medium in mechanism explanations (limited empirical validation of relative contributions)
- Confidence: Medium in concept enhancement effectiveness (demonstrates improved performance but lacks ablation studies isolating EBM architecture impact)

## Next Checks
1. **Dataset generalization**: Evaluate proposed techniques on at least two additional retrosynthesis datasets (e.g., Reaxys, Pistachio) to test robustness across chemical domains
2. **Ablation study**: Systematically vary EBM hyperparameters (k-hop size, number of negative samples) and IRM regularization strength to identify optimal configurations per baseline model
3. **Mechanism quantification**: Design controlled experiments where either label shift or covariate shift is eliminated to measure the isolated contribution of each technique to performance improvements