---
ver: rpa2
title: 'Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein
  Distances'
arxiv_id: '2310.13433'
source_url: https://arxiv.org/abs/2310.13433
tags:
- conditional
- wasserstein
- distance
- optimal
- sinkhorn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conditional Wasserstein distance that restricts
  transport plans to be diagonal in the conditioning variable, ensuring it equals
  the expected Wasserstein distance between posteriors. By deriving its dual, the
  authors rigorously motivate conditional WGAN losses with improved regularity.
---

# Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances

## Quick Facts
- arXiv ID: 2310.13433
- Source URL: https://arxiv.org/abs/2310.13433
- Reference count: 40
- Key outcome: Conditional Wasserstein distance with diagonal transport plans equals expected posterior Wasserstein distance, enabling better posterior sampling than joint approaches while maintaining joint distribution quality.

## Executive Summary
This paper introduces a conditional Wasserstein distance that restricts transport plans to be diagonal in the conditioning variable, ensuring it equals the expected Wasserstein distance between posteriors. By deriving its dual formulation, the authors rigorously motivate conditional WGAN losses with improved regularity guarantees. The method shows favorable posterior sampling properties compared to joint approaches, achieving significantly lower expected posterior errors (e.g., 0.087 vs 0.762 in the mixture case) while maintaining joint distribution quality. Numerical experiments on mixture models and image denoising demonstrate the effectiveness of this approach for Bayesian inverse problems.

## Method Summary
The paper proposes three conditional generative models for solving Bayesian inverse problems: Joint Sinkhorn, β-posterior Sinkhorn, and Diagonal generator. These models are trained to minimize different variants of conditional Wasserstein distances using Adam optimizer with batch size 1024 for 5000 iterations. The methods are evaluated on mixture models (linear inverse problems with Gaussian likelihood and Gaussian Mixture Model priors) and random images (uniform prior with simple averaging observation model). The primary metrics include expected posterior error, joint Sinkhorn divergence error, and resimulation error.

## Key Results
- Conditional Wasserstein distance with diagonal transport plans equals expected posterior Wasserstein distance
- Significantly lower expected posterior errors compared to joint approaches (0.087 vs 0.762 in mixture model)
- Under independence assumptions, conditional Wasserstein distance coincides with vanilla Wasserstein distance
- Improved regularity in conditional WGAN losses through dual formulation derivation

## Why This Works (Mechanism)

### Mechanism 1
The conditional Wasserstein distance equals the expected Wasserstein distance between posteriors when transport plans are restricted to be diagonal in the conditioning variable. This forces the optimal plan to match samples with the same conditioning value, aligning joint approximation directly with posterior recovery. The equality breaks if the diagonal condition fails due to continuous Y with atoms or overlapping supports.

### Mechanism 2
The dual formulation of the conditional Wasserstein distance recovers standard conditional WGAN losses with stronger regularity guarantees. Under compact support assumptions, the dual representation shows W₁,Y(PY,X,PY,Z) = sup_{h∈D̂} E[Y,X][h] - E[Y,Z][h], matching empirical conditional WGAN objectives with more regular dual functions than prior work. The dual representation may not hold for non-compact supports without additional regularization.

### Mechanism 3
When Y and X (or Y and Z) are independent, the conditional Wasserstein distance coincides with the vanilla Wasserstein distance. Independence allows the optimal coupling to be written as γ × ∆Y, where γ is the optimal coupling between X and Z, decoupling Y and (X,Z) components. The equality no longer holds if independence fails and cross-Y transport becomes necessary.

## Foundational Learning

- **Pushforward measure and disintegration**: Essential for defining conditional measures PX|Y=y and for the diagonal constraint on transport plans. Quick check: If α ∈ ΓY, what does (π₁,₃)♯α = ∆♯PY mean in terms of how samples are paired?
- **Wasserstein distance and optimal transport plans**: The paper builds a conditional variant, so understanding standard Wasserstein geometry is prerequisite. Quick check: What is the difference between Γ(PX,PZ) and ΓY(PY,X,PY,Z)?
- **Duality in optimal transport**: The dual formulation justifies the empirical GAN loss and shows why it works. Quick check: What regularity conditions on the dual function h are required for the Kantorovich duality to hold in the conditional case?

## Architecture Onboarding

- **Component map**: Observations Y, latent Z, forward model f, prior PX -> Generator Gθ(y,z) producing X̂ -> Discriminator/Critic h(y,x) -> Loss computation -> Backpropagation
- **Critical path**: 1. Sample (y,x) from PY,X and z from PZ 2. Generate G(y,z) 3. Compute conditional Wasserstein loss (Sinkhorn or dual) 4. Backpropagate and update Gθ 5. Evaluate posterior and joint metrics
- **Design tradeoffs**: Sinkhorn-based vs. dual-based training (Sinkhorn is differentiable but computationally heavier; dual is lighter but requires compact support); Diagonal vs. β-posterior vs. joint (trade-off between posterior accuracy and joint fidelity); High expected posterior error but low joint error indicates conditioning collapse; Low expected posterior error but high joint error indicates poor joint modeling
- **Failure signatures**: Mode collapse in generated samples (joint Sinkhorn generator); Numerical instability in Sinkhorn divergence calculation; Unstable training requiring Lipschitz regularization, batch size adjustment, or compact support verification
- **First 3 experiments**: 1. Verify diagonal property on synthetic data by computing optimal transport plan and confirming it is diagonal 2. Compare baseline, joint, and β-posterior methods on mixture model measuring expected posterior error and joint error 3. Test independence case by generating independent Y and X and confirming Wp,Y = Wp(PX,PZ)

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the necessary and sufficient conditions under which the optimal transport plan for the conditional Wasserstein distance is contained in the restricted set ΓY? The paper states this is an open question without providing a complete characterization of when the optimal transport plan must be diagonal in the conditioning variable.

- **Open Question 2**: How does the choice of β in the modified cost function affect the trade-off between posterior accuracy and diversity of generated samples? The paper proposes using β-Sinkhorn generators with large β to approximate the conditional Wasserstein distance but notes that β → ∞ optimizes the sharpest posterior bound at the potential cost of diversity.

- **Open Question 3**: Can the conditional Wasserstein distance be generalized to other types of divergences beyond the Wasserstein distance, such as the Maximum Mean Discrepancy (MMD)? The paper focuses on Wasserstein distance but mentions MMD in the context of Sinkhorn divergences, suggesting potential for extension.

## Limitations

- The conditional Wasserstein distance's equality to expected posterior Wasserstein distance critically depends on diagonal transport plan constraints, but empirical validation is absent beyond theoretical claims.
- The dual formulation requires compact support assumptions that may not hold in practical applications, particularly for image-based problems.
- Numerical experiments focus on synthetic datasets with controlled conditions, limiting generalizability to complex real-world Bayesian inverse problems.
- The independence case equivalence is mathematically elegant but represents a narrow scenario that may not reflect typical problem structures.

## Confidence

- Mechanism 1 (Diagonal transport plan): Medium confidence - The theoretical framework is sound, but empirical verification of the diagonal property in practice is absent.
- Mechanism 2 (Dual formulation): Medium confidence - The derivation appears rigorous for compact sets, but the assumption may be violated in real applications.
- Mechanism 3 (Independence case): High confidence - The mathematical proof is straightforward and the result is internally consistent.

## Next Checks

1. **Diagonal property verification**: Implement synthetic experiments with known conditioning variables and verify empirically that optimal transport plans are indeed diagonal in Y, measuring the proportion of off-diagonal mass in learned couplings.

2. **Non-compact support robustness**: Test the dual formulation on problems with unbounded supports (e.g., Gaussian posteriors) and assess whether regularization techniques can recover stable training, comparing performance against Sinkhorn-based approaches.

3. **Real-world application test**: Apply the conditional Wasserstein framework to a practical Bayesian inverse problem (e.g., medical imaging or geophysics) with realistic noise models and evaluate whether the theoretical advantages translate to meaningful improvements in posterior sampling quality.