---
ver: rpa2
title: Towards Interpretable Classification of Leukocytes based on Deep Learning
arxiv_id: '2311.14485'
source_url: https://arxiv.org/abs/2311.14485
tags:
- confidence
- learning
- figure
- cell
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of deep learning for classifying
  leukocytes from label-free quantitative phase images, focusing on interpretability
  for clinical adoption. A Four-Part Differential (Monocyte, Lymphocyte, Neutrophil,
  Eosinophil) is performed using AlexNet and LeNet5 architectures.
---

# Towards Interpretable Classification of Leukocytes based on Deep Learning

## Quick Facts
- arXiv ID: 2311.14485
- Source URL: https://arxiv.org/abs/2311.14485
- Reference count: 40
- Key outcome: Deep learning with calibrated confidence estimates achieves 96.3% F1-score for leukocyte classification while providing interpretable visual explanations

## Executive Summary
This work addresses the challenge of interpretable leukocyte classification from label-free quantitative phase images using deep learning. The authors develop a framework that combines AlexNet and LeNet5 architectures with variational inference through dropout to generate calibrated confidence estimates via temperature scaling. The approach achieves high classification performance (96.3% F1-score with AlexNet) while providing visual explanations through LIME and Guided Backpropagation methods. The calibrated models demonstrate robustness against unfamiliar inputs and mislabeled data, making them suitable for clinical adoption where interpretability and reliability are crucial.

## Method Summary
The method employs AlexNet and LeNet5 convolutional neural networks trained on 50×50 pixel grayscale quantitative phase images of four leukocyte types. Variational inference is implemented by keeping dropout layers active during testing, generating 100 independent predictions whose statistics form confidence scores. Temperature scaling calibrates these confidence estimates by learning a scalar parameter that scales network logits before softmax. Visual explanations are generated using model-agnostic LIME and propagation-based Guided Backpropagation methods. The models are trained with ADAM optimization and cross-entropy loss, with data split into 70% training, 20% validation, and 10% test sets.

## Key Results
- AlexNet achieves 96.3% F1-score versus LeNet5's 92.5% on the four-part differential classification task
- Temperature scaling successfully calibrates confidence scores, reducing expected calibration error from 0.0647 to 0.0125
- Visual explanations reveal size-based and interior-based differentiation patterns, though biological relevance is limited
- Calibrated models maintain confidence scores below 20% for non-leukocyte objects, demonstrating robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temperature scaling calibrates confidence scores by scaling logits before softmax.
- Mechanism: By introducing a learned scalar parameter T that divides the logits, the softmax output becomes more uniform, reducing overconfidence in the model's predictions.
- Core assumption: The original model is overconfident and temperature scaling can effectively re-calibrate it without affecting the maximum of the softmax function.
- Evidence anchors:
  - [abstract]: "Variational inference with dropout is used to generate confidence estimates, which are calibrated via temperature scaling."
  - [section]: "Temperature scaling has been shown to be effective for multiclass (K > 2) classification tasks. Here, the network logits zi for the i-th sample for each class k ∈ {1, ..., K} are scaled by a learned scalar parameter T > 0 before entering the softmax function."
  - [corpus]: Weak evidence. No direct mention of temperature scaling in the corpus, suggesting it's a specific contribution of this paper.
- Break condition: If the model is underconfident to begin with, temperature scaling might exacerbate the issue by making the model even less confident.

### Mechanism 2
- Claim: Variational inference with dropout provides probabilistic predictions and uncertainty estimates.
- Mechanism: By keeping dropout layers active during testing, multiple predictions are generated for each input, and the mean, median, and standard deviation of these predictions are used to form a prediction and confidence score.
- Core assumption: The model's predictions follow a distribution that can be approximated by sampling with dropout.
- Evidence anchors:
  - [abstract]: "Variational inference with dropout is used to generate confidence estimates..."
  - [section]: "For variational inference, the dropout layers stay active during testing, resulting in a probabilistic behavior for a single input. These outputs are summarized as mean, median and standard deviation to form a prediction, and the values of 100 independent predictions to form the confidence score."
  - [corpus]: Weak evidence. No direct mention of variational inference with dropout in the corpus, indicating it's a key contribution of this work.
- Break condition: If the dropout rate is not properly tuned, the uncertainty estimates might be unreliable or the model might underfit.

### Mechanism 3
- Claim: Visual explanation methods (LIME, Guided Backpropagation) provide interpretable insights into the model's decision-making process.
- Mechanism: LIME approximates the local behavior of the model by perturbing the input and observing the changes in predictions, while Guided Backpropagation propagates gradients through the network to highlight important features.
- Core assumption: The model's decision-making process can be approximated by local perturbations (LIME) or by analyzing the gradient flow (Guided Backpropagation).
- Evidence anchors:
  - [abstract]: "Visual explanation methods (LIME, Guided Backpropagation) are applied to highlight decision factors..."
  - [section]: "Model-agnostic methods impose no restrictions on the architecture or training of a model and are therefore flexible in their application... LIME approximates the local behavior of any machine learning model for a given input sample... Propagation-based approaches... use the internal structure of a neural network to determine the relevance of features to the model's internal decision-making."
  - [corpus]: Weak evidence. No direct mention of LIME or Guided Backpropagation in the corpus, suggesting these are specific techniques used in this paper.
- Break condition: If the input data is not visually interpretable (e.g., complex medical images), the explanations might not provide meaningful insights.

## Foundational Learning

- Concept: Confidence calibration
  - Why needed here: To ensure that the model's confidence scores accurately reflect the true probability of being correct, which is crucial for clinical decision-making.
  - Quick check question: What is the difference between a well-calibrated model and an overconfident model in terms of their reliability plots?

- Concept: Variational inference
  - Why needed here: To provide uncertainty estimates for the model's predictions, which is important for assessing the reliability of the model in safety-critical applications like medical diagnosis.
  - Quick check question: How does keeping dropout layers active during testing enable variational inference?

- Concept: Visual explanation methods (LIME, Guided Backpropagation)
  - Why needed here: To provide interpretable insights into the model's decision-making process, which is essential for gaining trust and understanding from clinicians.
  - Quick check question: What is the key difference between model-agnostic methods like LIME and propagation-based methods like Guided Backpropagation?

## Architecture Onboarding

- Component map:
  Input QPI images -> Preprocessing (background subtraction, segmentation, normalization) -> Neural Network (AlexNet/LeNet5 with dropout) -> Variational Inference (100 samples) -> Confidence Calibration (temperature scaling) -> Visual Explanations (LIME, Guided Backpropagation) -> Output predictions and explanations

- Critical path:
  1. Preprocess input images (background subtraction, segmentation, normalization)
  2. Feed images into the neural network model
  3. Generate predictions using variational inference with dropout
  4. Calibrate confidence scores using temperature scaling
  5. Generate visual explanations using LIME and Guided Backpropagation
  6. Analyze results and interpret findings

- Design tradeoffs:
  - Model complexity vs. interpretability: Smaller models like LeNet5 are more interpretable but might have lower performance compared to larger models like AlexNet.
  - Confidence calibration vs. accuracy: Temperature scaling improves calibration but might slightly reduce accuracy.
  - Explanation granularity vs. noise: More detailed explanations might be noisier and harder to interpret.

- Failure signatures:
  - Overconfidence: High confidence scores but low accuracy, indicating a need for better calibration.
  - Poor explanations: Visual explanations that do not highlight relevant features or are difficult to interpret.
  - Unstable performance: High variance in results across different runs or data splits.

- First 3 experiments:
  1. Compare the performance of AlexNet and LeNet5 on the leukocyte classification task with and without variational inference.
  2. Evaluate the effectiveness of temperature scaling in calibrating the confidence scores of the models.
  3. Analyze the visual explanations generated by LIME and Guided Backpropagation to understand the model's decision-making process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can visual explanations for leukocyte classification be improved to better align with biological features and increase clinical interpretability?
- Basis in paper: [inferred] The paper mentions that the patterns detected by the networks had limited resemblance to biological features and suggests using newer visual explanation methods like Smooth Grad-CAM++, EVET, or FIMF score-CAM in future work.
- Why unresolved: The current visual explanation methods (LIME, Guided Backpropagation) do not clearly reflect biological features, and the paper acknowledges the need for further refinement.
- What evidence would resolve it: Comparative studies showing improved alignment of new visual explanation methods with biological features, along with clinical validation of their interpretability.

### Open Question 2
- Question: What are the optimal parameters for variational inference dropout rates in leukocyte classification models to balance performance and interpretability?
- Basis in paper: [explicit] The paper tested different dropout rates (p=0.25, p=0.50) for LeNet5 and AlexNet architectures and found that moderate dropout rates improved classification performance.
- Why unresolved: While the paper provides some insights, the optimal dropout rate may vary depending on the specific model architecture and dataset.
- What evidence would resolve it: Systematic ablation studies evaluating the impact of different dropout rates on classification performance, calibration, and interpretability across multiple model architectures and datasets.

### Open Question 3
- Question: How can outlier detection methods be improved for automated blood cell analysis to better identify non-leukocyte objects and artifacts?
- Basis in paper: [explicit] The paper found that the proposed method had limited success in detecting outliers like thrombocytes, aggregates, defocused, or ruptured cells, and recommends using other methods for this purpose.
- Why unresolved: The paper does not explore alternative outlier detection methods or provide a detailed analysis of their performance.
- What evidence would resolve it: Comparative studies evaluating the performance of various outlier detection methods on blood cell analysis datasets, including their ability to identify different types of non-leukocyte objects and artifacts.

## Limitations

- Visual explanations lack clear biological relevance, limiting clinical interpretability and adoption
- Outlier detection capabilities are limited, with poor performance on thrombocytes and cell artifacts
- Model generalizability to other medical imaging domains and preprocessing conditions remains untested

## Confidence

- Temperature scaling calibration: High confidence - strong empirical support with improved ECE scores
- Variational inference with dropout: Medium confidence - good empirical results but limited theoretical guarantees
- Visual explanation methods: Medium confidence - useful insights but lack biological validation
- Overall model performance: High confidence - well-validated with balanced dataset and comprehensive metrics

## Next Checks

1. Test model performance and explanation stability across different image preprocessing pipelines and quality conditions
2. Conduct expert review of visual explanations to assess biological relevance and clinical utility
3. Evaluate model behavior on out-of-distribution samples and adversarial examples to verify robustness claims