---
ver: rpa2
title: HAlf-MAsked Model for Named Entity Sentiment analysis
arxiv_id: '2308.15793'
source_url: https://arxiv.org/abs/2308.15793
tags:
- sentiment
- entity
- dropout
- which
- multi-sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel approach for Named Entity Sentiment
  Analysis (NESA) in Russian, focusing on the RuSentNE-23 evaluation. The main challenge
  addressed is overfitting, which hampers model performance on this task.
---

# HAlf-MAsked Model for Named Entity Sentiment analysis

## Quick Facts
- arXiv ID: 2308.15793
- Source URL: https://arxiv.org/abs/2308.15793
- Reference count: 3
- Primary result: First place in RuSentNE-23 with macro F1pn score of 66.67

## Executive Summary
This paper introduces HAMAM, a novel approach for Named Entity Sentiment Analysis (NESA) in Russian that addresses overfitting challenges in the RuSentNE-23 evaluation. The method combines predictions from model passes with entities masked and unmasked, using mean and max pooling, multi-sample dropout, and class-weighted loss. An ensemble of transformer models (RuBERT, XLM-RoBERTa, RemBERT) achieves state-of-the-art performance with a macro F1pn score of 66.67, securing first place in the competition.

## Method Summary
HAMAM (HAf-MAsked Model) processes input text twice - once with entities intact and once with entities replaced by [MASK] tokens - then averages the logits from both passes. The method uses mean and max pooling over entity spans for representation, multi-sample dropout (averaging 5 forward passes) for regularization, and class-weighted cross-entropy loss (neutral weight 0.1) to handle class imbalance. Models are trained using 5-fold cross-validation and predictions are ensembled from different transformer architectures to improve robustness and reduce overfitting.

## Key Results
- Achieves macro F1pn score of 66.67 on RuSentNE-23 test set
- Outperforms baseline methods by 0.16 in macro F1pn
- Secures first place in RuSentNE-23 competition
- Demonstrates effectiveness of entity masking in reducing overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking the entity forces the model to rely on context rather than entity-specific biases, improving generalization.
- Mechanism: During training, the model processes the input twice—once with the entity intact and once with the entity replaced by [MASK]. The final prediction is an average of the logits from both passes.
- Core assumption: Entity-specific biases in training data (e.g., some entities appearing more in positive or negative contexts) cause overfitting; masking breaks this dependency.
- Evidence anchors:
  - [abstract] "novel technique of additional pass over given data with masked entity before making the final prediction so that we can combine logits from the model when it knows the exact entity it predicts sentiment for and when it does not."
  - [section 3.4] "Masking the entity words (replacing them with ‘[MASK]’ token) helps to mitigate this effect and forces the model to extract sentiment information from a context rather than prior knowledge of the entity itself."

### Mechanism 2
- Claim: Multi-sample dropout accelerates training and improves generalization by creating an ensemble-like effect within a single training batch.
- Mechanism: Instead of applying one dropout mask per input, multiple dropout masks are applied and the outputs averaged before the backward pass.
- Core assumption: Averaging over multiple stochastic forward passes approximates model averaging, reducing variance without the full computational cost of ensemble training.
- Evidence anchors:
  - [section 3.2] "The outputs (logits or probabilities) from these multiple forward passes are then averaged, resembling an ensemble-like approach."
  - [section 3.2] "This approach notably decreases the required number of training iterations."

### Mechanism 3
- Claim: Ensembling diverse transformer models trained on different data folds reduces overfitting and increases robustness.
- Mechanism: The dataset is split into 5 folds; each model is trained on 4 folds and validated on the remaining one. Predictions are averaged across all models for the final output.
- Core assumption: Different training subsets expose the model to varied data distributions, reducing reliance on dataset-specific patterns.
- Evidence anchors:
  - [section 3.4] "In order to do that, we averaged the final logits from different transformer models trained on different subsets of training data."
  - [section 4] "The training dataset was split into 5 folds to perform cross-validation and eventually get 5 models, which can be ensembled for prediction on test data."

## Foundational Learning

- Concept: Cross-validation
  - Why needed here: To robustly estimate model performance and prevent overfitting by training and validating on different data subsets.
  - Quick check question: What is the purpose of splitting the dataset into multiple folds during training?

- Concept: Transformer-based language models (e.g., BERT, RoBERTa)
  - Why needed here: They provide contextualized embeddings that capture semantic relationships, crucial for understanding sentiment in context.
  - Quick check question: How do transformer models generate contextual embeddings for each token?

- Concept: Weighted cross-entropy loss
  - Why needed here: To handle class imbalance by reducing the influence of the dominant neutral class on training.
  - Quick check question: Why are neutral examples given a lower weight in the loss function compared to positive and negative examples?

## Architecture Onboarding

- Component map: Input -> Tokenizer -> Transformer backbone -> Entity masking (optional) -> Mean/Max pooling over entity span -> Classification head (Linear + Tanh + Multi-sample dropout + Linear) -> Logits averaging (masked + unmasked) -> Softmax/Argmax -> Output
- Critical path: Entity representation (pooled) -> Classification -> Logits combination (masked + unmasked) -> Final prediction
- Design tradeoffs: Masking entity improves generalization but may lose entity-specific cues; multi-sample dropout speeds training but adds computation; ensembling improves robustness but requires more resources
- Failure signatures: Overfitting (high train, low dev accuracy), sensitivity to neutral class threshold, unstable ensembling if folds are not representative
- First 3 experiments:
  1. Train HAMAM with only unmasked entity, mean pooling, no masking—observe overfitting.
  2. Add entity masking pass, compare performance—expect improvement if overfitting was an issue.
  3. Introduce multi-sample dropout, monitor training speed and generalization—expect faster convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the threshold on neutral class prediction impact model performance and is there an optimal threshold value?
- Basis in paper: [explicit] The paper mentions testing a threshold on neutral class prediction, which yielded a small increase in performance on the dev set, but does not discuss the optimal threshold value or its impact on model performance.
- Why unresolved: The paper only briefly mentions the threshold and its positive impact on performance, but does not explore the optimal threshold value or how it affects model performance in detail.
- What evidence would resolve it: Further experiments with different threshold values on the dev and test sets to determine the optimal threshold and its impact on model performance.

### Open Question 2
- Question: How does the HAMAM approach perform on other languages or datasets outside of the RuSentNE-23 evaluation?
- Basis in paper: [inferred] The paper focuses on the RuSentNE-23 evaluation and does not discuss the performance of the HAMAM approach on other languages or datasets.
- Why unresolved: The paper only presents results for the RuSentNE-23 evaluation, and it is unclear how the HAMAM approach would perform on other languages or datasets.
- What evidence would resolve it: Applying the HAMAM approach to other languages or datasets and comparing its performance to other state-of-the-art models.

### Open Question 3
- Question: How does the multi-sample dropout technique affect the model's ability to generalize to unseen entities?
- Basis in paper: [explicit] The paper mentions using multi-sample dropout as a regularization technique to improve generalization, but does not discuss its specific impact on the model's ability to generalize to unseen entities.
- Why unresolved: The paper does not provide a detailed analysis of the impact of multi-sample dropout on the model's ability to generalize to unseen entities.
- What evidence would resolve it: Further experiments comparing the performance of the model with and without multi-sample dropout on unseen entities or datasets with a high proportion of unseen entities.

## Limitations
- Performance evaluation limited to single Russian dataset, limiting generalizability
- Entity masking may discard useful entity-specific sentiment cues when entity name carries inherent sentiment
- Computational overhead from double-forward passes and multi-sample dropout may limit practical deployment
- Neutral class threshold is heuristic and may not generalize across different sentiment distributions

## Confidence
- High Confidence: The overfitting problem in NESA is well-documented and the masking mechanism's ability to reduce entity-specific bias is theoretically sound and empirically validated.
- Medium Confidence: The effectiveness of multi-sample dropout in accelerating training and improving generalization is supported by the paper's results, but the optimal number of samples (5) may be dataset-dependent.
- Medium Confidence: The ensembling strategy using 5-fold cross-validation improves robustness, but the specific choice of transformers (RuBERT, XLM-RoBERTa, RemBERT) and their relative contributions are not fully analyzed.

## Next Checks
1. Apply HAMAM to NESA datasets in other languages (e.g., English SEC filings for financial sentiment) to assess cross-lingual and cross-domain robustness.
2. Systematically vary the neutral class threshold (e.g., 0.5, 0.55, 0.6) and measure impact on macro F1pn to determine optimal threshold for different sentiment distributions.
3. Measure wall-clock training time and inference latency with and without masking and multi-sample dropout to quantify practical deployment trade-offs.