---
ver: rpa2
title: 'TAPS: Connecting Certified and Adversarial Training'
arxiv_id: '2305.04574'
source_url: https://arxiv.org/abs/2305.04574
tags:
- training
- certi
- taps
- adversarial
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TAPS addresses the trade-off between certified and standard accuracy
  in training certifiably robust neural networks. It combines IBP and PGD training
  via a gradient connector to yield precise, though not necessarily sound, worst-case
  loss approximations.
---

# TAPS: Connecting Certified and Adversarial Training

## Quick Facts
- arXiv ID: 2305.04574
- Source URL: https://arxiv.org/abs/2305.04574
- Reference count: 40
- One-line primary result: TAPS achieves 22% certified accuracy on TinyImageNet for ℓ∞-perturbations with radius ε=1/255

## Executive Summary
TAPS addresses the trade-off between certified and standard accuracy in training certifiably robust neural networks by combining Interval Bound Propagation (IBP) and Projected Gradient Descent (PGD) training via a novel gradient connector. This connector enables joint training between IBP and PGD components, yielding precise worst-case loss approximations that improve both certified and standard accuracy. The method introduces a multi-estimator PGD variant and multiplicative IBP regularization to further enhance performance.

## Method Summary
TAPS splits neural networks into a feature extractor (approximated with IBP) and a classifier (optimized with PGD in the embedding space). A gradient connector defines pseudo-gradients between the bounds computed by IBP and the latent adversarial examples found by PGD, enabling backpropagation through this interface. The multi-estimator PGD variant independently maximizes each logit difference before computing the loss, while multiplicative IBP regularization balances precision with soundness. The final loss combines TAPS and IBP losses with a scaling parameter wTAPS.

## Key Results
- Achieves 22% certified accuracy on TinyImageNet for ℓ∞-perturbations with radius ε=1/255
- Demonstrates tighter worst-case loss approximations compared to existing methods
- Shows state-of-the-art results on MNIST, CIFAR-10, and TinyImageNet benchmarks

## Why This Works (Mechanism)

### Mechanism 1
The gradient connector enables joint training by providing a well-defined gradient path between IBP and PGD components, compensating for their opposing approximation errors. The connector defines pseudo-gradients ∂ẑ/∂z and ∂ẑ/∂z that are sparse and rectifies linear gradients to prevent contradictory signals between bounds. The latent adversarial example lies close to one bound or the midpoint between bounds, making the piecewise linear approximation valid.

### Mechanism 2
The multi-estimator PGD variant better aligns with the robustness objective by independently maximizing each logit difference before computing the loss. Instead of maximizing cross-entropy with a single adversarial example, it computes worst-case bounds on each logit difference oΔᵢ independently, then combines them to find the point that maximizes the margin loss. The maximum margin loss can be achieved by combining dimension-wise worst-case bounds.

### Mechanism 3
The multiplicative IBP regularization creates a hybrid loss that balances precision of TAPS with sound verification of IBP. The final loss L(x,y,ε) = LTAPS(x,y,ε)·LIBP(x,y,ε) ensures samples are either certifiable by TAPS or IBP, with gradient scaling α controlling each component's contribution. Multiplying the two losses creates a meaningful hybrid objective.

## Foundational Learning

- **Concept: Interval Bound Propagation (IBP)**
  - Why needed here: TAPS relies on IBP to compute the initial box approximation [z,z] in the embedding space before applying PGD
  - Quick check question: What type of approximation does IBP compute when propagating through neural networks - over-approximation or under-approximation?

- **Concept: Projected Gradient Descent (PGD)**
  - Why needed here: TAPS uses PGD in the embedding space to find adversarial examples that under-approximate the classifier's reachable set
  - Quick check question: In standard adversarial training, does PGD optimize an under-approximation or over-approximation of the worst-case loss?

- **Concept: Lipschitz continuity and box relaxation**
  - Why needed here: Understanding how box relaxations accumulate approximation errors through network layers is crucial for grasping why TAPS splits networks into feature extractor and classifier
  - Quick check question: Why do box approximation errors tend to grow exponentially with network depth in IBP?

## Architecture Onboarding

- **Component map:** Input → IBP propagation through fE → Box bounds [z,z] → Multi-estimator PGD in embedding space → Latent adversarial examples → Gradient connector → Loss computation → Backpropagation
- **Critical path:** Input passes through feature extractor fE using IBP to get bounds [z,z], then multi-estimator PGD finds latent adversarial examples in embedding space, gradient connector defines pseudo-gradients, and loss is computed for backpropagation
- **Design tradeoffs:** Joint training vs. staged training (like COLT), precision vs. soundness, computational cost of multi-estimator vs. single-estimator PGD
- **Failure signatures:** Training collapse when classifier is too small, poor TAPS accuracy indicating bad fit, high variance in worst-case loss approximations
- **First 3 experiments:**
  1. Implement basic TAPS with a small CNN on MNIST, verify that gradient connector prevents NaNs during training
  2. Compare single-estimator vs multi-estimator PGD on a fixed network split, measure both accuracy and training stability
  3. Sweep the gradient connector parameter c (0 to 1) and plot TAPS accuracy vs natural accuracy to find optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does TAPS compare to complete verifiers for certified robustness?
- **Basis in paper:** The paper mentions that complete verifiers can decide any robustness property but are computationally expensive, and that TAPS achieves state-of-the-art results but relies on approximate methods
- **Why unresolved:** The paper does not provide direct comparisons between TAPS and complete verifiers, focusing instead on incomplete methods and their approximations
- **What evidence would resolve it:** Empirical studies comparing TAPS's certified accuracy and computational efficiency against complete verifiers like MILP-based methods on the same datasets and perturbation radii

### Open Question 2
- **Question:** How sensitive is TAPS to the choice of network architecture beyond CNN7?
- **Basis in paper:** The paper evaluates TAPS primarily on CNN7 and SORTNET architectures, mentioning that the split location between feature extractor and classifier significantly impacts performance
- **Why unresolved:** The paper does not explore a wide range of architectures or analyze how architectural choices beyond split location affect TAPS's performance
- **What evidence would resolve it:** Systematic experiments testing TAPS on diverse architectures with varying depths, widths, and activation functions

### Open Question 3
- **Question:** What is the theoretical justification for the gradient connector's effectiveness?
- **Basis in paper:** The paper introduces the gradient connector to enable backpropagation through the embedding space but notes that the gradients are not well-defined, proposing a rectified linear connector and empirically validating its performance
- **Why unresolved:** The paper does not provide a rigorous mathematical analysis of why the gradient connector leads to well-behaved optimization or how it affects convergence guarantees
- **What evidence would resolve it:** Theoretical analysis proving properties of the gradient connector and empirical studies on convergence behavior with different connector parameterizations

## Limitations
- The proposed gradient connector and multi-estimator PGD variants lack strong empirical validation and theoretical guarantees
- Experimental results show improvements but the claimed mechanisms are primarily supported by conceptual arguments rather than rigorous proofs
- The assumption that the piecewise linear approximation of the gradient connector is generally valid across different network architectures and datasets

## Confidence

**Confidence Assessment:**
- **High Confidence:** The general framework of combining IBP and PGD through a split architecture is well-established. The experimental setup and evaluation metrics are standard for this research area.
- **Medium Confidence:** The specific design choices for the gradient connector and multi-estimator PGD are reasonable but not thoroughly validated. The multiplicative regularization approach shows promise but lacks theoretical grounding.
- **Low Confidence:** The claim that the piecewise linear approximation of the gradient connector is generally valid across different network architectures and datasets. The assumption that independent maximization of logit differences is sufficient for finding worst-case points.

## Next Checks

1. **Gradient Connector Validation:** Implement ablation studies removing the gradient connector to quantify its contribution to training stability and final accuracy. Measure the difference in convergence behavior and final performance with and without the connector.

2. **Multi-estimator PGD Analysis:** Compare the multi-estimator approach against standard PGD on networks with varying numbers of output classes. Evaluate whether the independence assumption holds better for some classification tasks than others.

3. **Theoretical Bounds:** Derive formal bounds on the approximation error introduced by the gradient connector and multi-estimator PGD. Validate these bounds empirically across different network depths and widths.