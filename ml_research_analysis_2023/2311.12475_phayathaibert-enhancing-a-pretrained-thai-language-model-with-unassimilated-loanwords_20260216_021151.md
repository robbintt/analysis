---
ver: rpa2
title: 'PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated
  Loanwords'
arxiv_id: '2311.12475'
source_url: https://arxiv.org/abs/2311.12475
tags:
- https
- datasets
- thai
- wangchanberta
- huggingface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhayaThaiBERT is a transformer-based Thai language model that addresses
  the lack of foreign vocabulary in the standard WangchanBERTa model by expanding
  its tokenizer vocabulary and pretraining on a larger dataset. The new model incorporates
  English and other foreign words through vocabulary transfer from XLM-R and achieves
  improved performance on downstream tasks such as sentiment analysis, named entity
  recognition, and part-of-speech tagging.
---

# PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords

## Quick Facts
- arXiv ID: 2311.12475
- Source URL: https://arxiv.org/abs/2311.12475
- Reference count: 29
- Key outcome: PhayaThaiBERT outperforms WangchanBERTa on 5 out of 9 datasets for both micro and macro F1 scores by expanding vocabulary to include English loanwords

## Executive Summary
PhayaThaiBERT addresses a critical limitation in Thai language models: the inability to properly handle orthographically unassimilated loanwords, particularly English words that appear in Thai text. By expanding WangchanBERTa's tokenizer vocabulary through transfer from XLM-R and further pretraining on a larger dataset, the model achieves significant performance improvements on downstream tasks including sentiment analysis, named entity recognition, and part-of-speech tagging. The expanded vocabulary reduces out-of-vocabulary rates from 9.14% to 2.89% for Thai texts containing English words.

## Method Summary
The method involves three key steps: (1) expanding the tokenizer vocabulary by transferring from XLM-R's tokenizer while excluding Thai characters, (2) further pretraining WangchanBERTa's checkpoint on a 156.5GB dataset using discriminative fine-tuning with different learning rates per layer and gradual unfreezing over 14,000 steps, and (3) fine-tuning the pretrained model on downstream tasks using standard procedures. The training employs a masked language modeling objective with 15% of tokens masked, and uses a combination of existing and expanded vocabulary embeddings during the forward pass.

## Key Results
- PhayaThaiBERT achieved better performance than WangchanBERTa on 5 out of 9 evaluated datasets
- Micro F1 score improvements were observed in sentiment analysis (Wongnai reviews), named entity recognition (BEST), and part-of-speech tagging (InterBEST)
- Macro F1 score improvements were seen in sentiment analysis (Wongnai reviews) and named entity recognition (InterBEST)
- The model's vocabulary expanded from 71,808 to 193,132 tokens, significantly reducing out-of-vocabulary rates for English loanwords

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding tokenizer vocabulary with foreign words enables the model to process unassimilated loanwords that were previously tokenized as unknown tokens.
- Mechanism: By transferring vocabulary from XLM-R's tokenizer, the expanded vocabulary includes English words and emojis that appear in Thai texts, reducing the number of unknown tokens during tokenization.
- Core assumption: Words that are common in Thai texts but absent from WangchanBERTa's vocabulary are crucial for downstream task performance.
- Evidence anchors:
  - [abstract]: "We identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the main source of these shortcomings."
  - [section 3]: "The main source of the shortcomings of WangchanBERTa when it comes to understanding unassimilated loanwords is its tokenizer, whose vocabulary contains only a small number of foreign words."
  - [corpus]: Weak - No direct corpus evidence that the expanded vocabulary directly improves downstream task performance, only OOV reduction statistics.

### Mechanism 2
- Claim: Gradual unfreezing during pretraining prevents catastrophic forgetting of WangchanBERTa's original knowledge while adapting to the expanded vocabulary.
- Mechanism: Parameters are frozen in groups from bottom to top layers, with only the added vocabulary embeddings unfrozen initially, then progressively unfreezing higher layers every 1,000 steps.
- Core assumption: Lower layers learn more general linguistic features while higher layers learn task-specific patterns, so freezing lower layers preserves general knowledge.
- Evidence anchors:
  - [section 4.6]: "At the start of the training, we freeze every model parameter except those belonging to the embeddings of the added vocabulary. Other layers will then be gradually unfrozen every 1,000 steps."
  - [corpus]: No direct evidence that gradual unfreezing specifically improves performance over alternative fine-tuning strategies.

### Mechanism 3
- Claim: Discriminative fine-tuning with different learning rates per layer allows the model to adapt at different speeds based on each layer's role.
- Mechanism: The added vocabulary embeddings use the highest learning rate, with each subsequent layer using a learning rate 2.6 times smaller than the previous one.
- Core assumption: Layers closer to the output need more adaptation to new vocabulary, while lower layers should change less to preserve general linguistic knowledge.
- Evidence anchors:
  - [section 4.6]: "We use different learning rates for each layer in the model. We group all the model parameters into 15 layers... The learning rate for the added vocabulary's embeddings are set to be the largest."
  - [corpus]: No corpus evidence comparing discriminative fine-tuning vs uniform fine-tuning performance.

## Foundational Learning

- Concept: Subword tokenization and vocabulary transfer
  - Why needed here: Understanding how unigram tokenization works and how vocabulary can be transferred between models using the same tokenization algorithm is essential for implementing the vocabulary expansion.
  - Quick check question: What format do SentencePiece tokenizers use to store their vocabulary that enables vocabulary transfer?

- Concept: Masked language modeling objective
  - Why needed here: The pretraining task uses masked language modeling, so understanding how this objective works and how it differs from other pretraining objectives is crucial.
  - Quick check question: What percentage of tokens are masked during pretraining, and how are they masked?

- Concept: Gradual unfreezing and discriminative fine-tuning
  - Why needed here: These techniques are used to prevent catastrophic forgetting while adapting to the expanded vocabulary, so understanding their purpose and implementation is essential.
  - Quick check question: How does gradual unfreezing differ from standard fine-tuning, and why is it beneficial in this context?

## Architecture Onboarding

- Component map: Tokenization → Vocabulary lookup (existing/expanded) → Embedding combination → Transformer blocks → MLM head → Loss calculation
- Critical path: The model uses a modified RoBERTa architecture with two separate embedding lookup tables - one for existing vocabulary and one for added vocabulary. The forward pass combines embeddings from both tables. The rest of the RoBERTa architecture remains unchanged.
- Design tradeoffs: Larger vocabulary increases model size from 106M to 278M parameters, improving foreign word understanding but requiring more computational resources.
- Failure signatures: Training instability (NaN values in self-attention layers), validation loss spikes, overfitting of higher layers while lower layers fail to converge.
- First 3 experiments:
  1. Test OOV rate reduction on a sample Thai text with English loanwords using both tokenizers
  2. Run pretraining for 1000 steps with gradual unfreezing disabled to observe catastrophic forgetting
  3. Compare validation loss trajectories with and without discriminative fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PhayaThaiBERT compare to other multilingual models (like XLM-R) when dealing with code-switched text in languages other than English and Thai?
- Basis in paper: [inferred] The paper mentions that PhayaThaiBERT is enhanced for understanding unassimilated loanwords, particularly English in Thai. It also notes that XLM-R performs better on datasets with English content.
- Why unresolved: The paper only evaluates the model's performance on Thai datasets with English loanwords. It doesn't test the model's ability to handle code-switching in other language pairs.
- What evidence would resolve it: Evaluating PhayaThaiBERT on datasets with code-switching between Thai and other languages (e.g., Chinese, Japanese) or between other language pairs would provide evidence of its generalization to other code-switching scenarios.

### Open Question 2
- Question: What is the impact of the increased model size (278M parameters vs. WangchanBERTa's 106M) on computational efficiency and practical deployment?
- Basis in paper: [explicit] The paper states that the model size increases significantly due to the expanded vocabulary, but doesn't discuss the practical implications of this increase.
- Why unresolved: While the paper shows improved performance, it doesn't address the trade-off between performance gains and increased computational requirements.
- What evidence would resolve it: Benchmarks comparing inference speed, memory usage, and training time between PhayaThaiBERT and WangchanBERTa on the same hardware would provide concrete evidence of the practical impact of the increased model size.

### Open Question 3
- Question: How does the gradual unfreezing technique used during training affect the model's ability to learn from the expanded vocabulary?
- Basis in paper: [explicit] The paper mentions using gradual unfreezing as part of the training procedure to prevent loss of performance when further pretraining from WangchanBERTa's checkpoint.
- Why unresolved: The paper doesn't provide an analysis of how this technique specifically contributes to the model's improved performance on tasks involving unassimilated loanwords.
- What evidence would resolve it: Ablation studies comparing the performance of PhayaThaiBERT with and without gradual unfreezing, particularly on tasks involving code-switched language, would clarify the technique's impact.

### Open Question 4
- Question: What is the optimal size and composition of the expanded vocabulary for achieving the best balance between performance and model size?
- Basis in paper: [inferred] The paper expands the vocabulary by transferring from XLM-R and adding emojis, but doesn't explore the effect of different vocabulary sizes or compositions on performance.
- Why unresolved: The current approach adds a large number of foreign words, but it's unclear if all of them are necessary for optimal performance or if a smaller, more targeted vocabulary expansion could achieve similar results with a smaller model.
- What evidence would resolve it: Experiments varying the size and composition of the expanded vocabulary, and measuring the resulting model performance on downstream tasks, would identify the optimal vocabulary configuration.

## Limitations

- Increased model size from 106M to 278M parameters raises computational requirements and may limit practical deployment
- No comprehensive error analysis to identify which types of unassimilated loanwords benefit most from the expanded vocabulary
- Potential for noise introduction if vocabulary transfer from XLM-R includes irrelevant or low-frequency tokens

## Confidence

- High confidence in vocabulary expansion mechanism reducing OOV rates for English words in Thai text
- Medium confidence in gradual unfreezing and discriminative fine-tuning effectiveness due to lack of ablation studies
- Medium confidence in overall performance improvements as gains vary significantly across tasks

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of vocabulary expansion, gradual unfreezing, and discriminative fine-tuning to overall performance, determining which components are essential versus beneficial.

2. Perform error analysis on downstream task predictions to identify whether performance improvements are concentrated on specific types of unassimilated loanwords (e.g., English proper nouns vs. common nouns) or specific domains where code-switching is more prevalent.

3. Evaluate model robustness to vocabulary drift by testing performance on Thai text with evolving foreign word usage patterns, ensuring the expanded vocabulary remains relevant over time rather than becoming outdated as language usage changes.