---
ver: rpa2
title: 'Residual Q-Learning: Offline and Online Policy Customization without Value'
arxiv_id: '2306.09526'
source_url: https://arxiv.org/abs/2306.09526
tags:
- policy
- reward
- residual
- prior
- customization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of policy customization for imitation
  learning (IL), where the goal is to tailor an imitative policy to meet additional
  requirements of downstream tasks while maintaining its original characteristics.
  The authors propose a principled approach by formulating policy customization as
  a Markov Decision Process (MDP) with a reward function that combines the inherent
  reward of the demonstration and an add-on reward specified by the downstream task.
---

# Residual Q-Learning: Offline and Online Policy Customization without Value

## Quick Facts
- arXiv ID: 2306.09526
- Source URL: https://arxiv.org/abs/2306.09526
- Reference count: 40
- Primary result: Novel framework for policy customization that combines imitative and task-specific objectives without requiring knowledge of the prior policy's reward function or value function.

## Executive Summary
This paper addresses the challenge of policy customization for imitation learning, where the goal is to adapt a pre-trained imitative policy to meet additional requirements of downstream tasks while preserving its original behavior. The authors propose Residual Q-learning, a principled framework that formulates policy customization as a Markov Decision Process with a reward function combining the demonstration's inherent reward and an add-on reward specified by the downstream task. The key innovation is learning a residual Q-function that captures the difference between the customized and prior policies, enabling customization without knowing the prior policy's reward function or value function. The framework is instantiated into three algorithms: residual soft Q-learning for discrete action spaces, residual soft actor-critic for continuous action spaces, and residual maximum-entropy Monte Carlo Tree Search for online customization.

## Method Summary
The method involves training a prior policy (via imitation learning or reinforcement learning) to capture the basic task, then using Residual Q-learning to customize this policy for downstream tasks. The core approach learns a residual Q-function that represents the difference between the customized and prior policies. The update rule is derived from the soft Bellman equation but eliminates dependence on the unknown reward function by leveraging the prior policy's Boltzmann distribution. The framework handles both offline scenarios (where additional RL training steps are taken) and online scenarios (like MCTS) uniformly. Three specific algorithms are derived: residual soft Q-learning for discrete actions, residual soft actor-critic for continuous actions, and residual maximum-entropy MCTS for online planning.

## Key Results
- Successfully customized policies for four different environments: CartPole, Continuous Mountain Car, Highway navigation, and Parking
- Maintained performance on basic tasks while improving performance on add-on objectives
- Demonstrated effectiveness in both offline and online customization scenarios
- Showed ability to handle both discrete and continuous action spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual Q-learning enables policy customization without knowing the prior policy's reward function by decomposing the Q-function into a known prior component and an unknown residual component.
- Mechanism: The framework defines a residual Q-function QR,t = Qt - ωQ* where Q* is the prior policy's soft Q-function. The update rule for QR,t (Eqn. 5) is derived from the soft Bellman equation but eliminates dependence on the unknown reward r by leveraging the prior policy's Boltzmann distribution. The resulting policy (Eqn. 6) combines the residual Q-function with the prior policy's log-likelihood.
- Core assumption: The prior policy π is a maximum-entropy policy whose Q-function can be expressed via Eqn. 3 (Q*(s,a) = α log π(a|s) + α log Zs), allowing decomposition of the target Q-function.
- Evidence anchors:
  - [abstract] "We propose a novel framework, Residual Q-learning, which can solve the formulated MDP by leveraging the prior policy without knowing the inherent reward or value function of the prior policy."
  - [section 2.2] Derivation from Eqn. 5a to 5e showing how the residual Q-function update eliminates dependence on unknown reward r.
- Break condition: If the prior policy is not a maximum-entropy policy or its Boltzmann distribution parameters (α, Zs) cannot be estimated, the decomposition fails.

### Mechanism 2
- Claim: The temperature coefficient α of the prior policy can be absorbed into the reward weight ω, eliminating the need to know α.
- Mechanism: Since α multiplies the reward weight ω in both the update rule (Eqn. 5) and policy construction (Eqn. 6), we can define a new hyperparameter ω' = ωα and treat it as the tunable parameter, effectively absorbing the unknown α into ω'.
- Core assumption: The reward weight ω is a hyperparameter to tune, so absorbing α into it doesn't change the optimization landscape.
- Evidence anchors:
  - [section 2.2] "Note that α is multiplied upon the reward weight ω in Eqn. (5) and Eqn. (6). In practice, the reward weight is a hyperparameter to tune. Hence, we can define ω' = ωα and directly treat ω' as the hyperparameter without the need of knowing the true α."
- Break condition: If α varies significantly across states or actions, absorbing it into a single ω' may be insufficient.

### Mechanism 3
- Claim: The residual Q-learning framework unifies offline and online policy customization by providing a consistent way to combine imitative and add-on rewards.
- Mechanism: By formulating policy customization as an MDP with reward r_total = ωr + rR and using residual Q-learning to solve it, the framework handles both offline scenarios (where additional RL training steps are taken) and online scenarios (like MCTS) uniformly. The same decomposition and update rule apply regardless of the setting.
- Core assumption: Both offline and online customization can be framed as solving the same MDP, just with different computational approaches.
- Evidence anchors:
  - [abstract] "We derive a family of residual Q-learning algorithms that can realize offline and online policy customization"
  - [section 3] Description of three algorithms: residual soft Q-learning (offline, discrete), residual soft actor-critic (offline, continuous), and residual maximum-entropy MCTS (online)
- Break condition: If the online setting requires fundamentally different optimization objectives or constraints not captured by the MDP formulation.

## Foundational Learning

- Concept: Maximum entropy reinforcement learning
  - Why needed here: The entire framework relies on maximum-entropy policies and soft Bellman equations. Understanding the Boltzmann distribution of optimal policies and entropy regularization is essential for grasping why the decomposition works.
  - Quick check question: Why does a maximum-entropy policy have the form π(a|s) ∝ exp(Q(s,a)/α) rather than just being greedy with respect to Q?

- Concept: Bellman equation and its soft variant
  - Why needed here: The derivation of the residual Q-function update rule is based on manipulating the soft Bellman equation. Understanding the standard and soft Bellman backups is crucial for following the mathematical steps.
  - Quick check question: What is the key difference between the standard Bellman backup and the soft Bellman backup in terms of how they handle action selection?

- Concept: Policy evaluation vs. policy improvement
  - Why needed here: The residual soft actor-critic algorithm alternates between these two steps. Understanding the distinction and how they work together in actor-critic methods is important for implementing the algorithm.
  - Quick check question: In soft actor-critic, how does the policy evaluation step differ from standard Q-learning's target computation?

## Architecture Onboarding

- Component map:
  - Prior policy π (pre-trained via IL or RL) -> Add-on reward function rR (task-specific) -> Reward weight ω (hyperparameter balancing objectives) -> Residual Q-function QR (learned component) -> Policy network (outputs customized policy using QR and π) -> For model-free: Replay buffer, target networks -> For model-based: Dynamics model, MCTS tree structure

- Critical path:
  1. Receive state and prior policy
  2. Query residual Q-function and compute policy using Eqn. 6
  3. Execute action and store transition
  4. Sample batch from replay buffer
  5. Compute target residual Q-value using Bellman backup
  6. Update residual Q-function parameters via TD loss
  7. Repeat until convergence

- Design tradeoffs:
  - Using residual Q-function vs. directly learning augmented Q-function: Residual approach leverages prior knowledge more efficiently but requires maintaining two components (prior policy and residual)
  - Model-free vs. model-based: Model-free is more general but requires additional training; model-based enables zero-shot customization but needs accurate dynamics model
  - Discrete vs. continuous action spaces: Discrete allows exact policy computation; continuous requires approximation via policy networks

- Failure signatures:
  - Poor performance on basic task: Prior policy doesn't capture basic task well or residual Q-learning fails to preserve it
  - Poor performance on add-on task: Insufficient weight ω or inadequate learning of residual component
  - Training instability: Learning rate too high, target networks not updated properly, or exploration-exploitation balance off
  - Distributional shift: Customized policy visits states far from prior policy's experience

- First 3 experiments:
  1. CartPole balancing with position control: Test discrete action space, simple state space, and clear add-on objective (keep cart centered)
  2. Continuous Mountain Car with action preference: Test continuous action space and non-standard add-on objective (prefer positive actions)
  3. Highway navigation with lane preference: Test high-dimensional state space and realistic add-on objective (stay in rightmost lane)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of policy customization scale with the complexity and dimensionality of the state and action spaces in the environments?
- Basis in paper: [inferred] The paper evaluates the proposed algorithms on four environments, including Continuous Mountain Car and Parking, which have continuous state and action spaces. However, the performance of the algorithms is not explicitly analyzed in terms of the complexity and dimensionality of the spaces.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of policy customization changes with increasing complexity and dimensionality of the state and action spaces.
- What evidence would resolve it: Conducting experiments on environments with varying levels of complexity and dimensionality, and analyzing the performance of the proposed algorithms in terms of success rates, basic rewards, and add-on rewards.

### Open Question 2
- Question: How sensitive are the proposed residual Q-learning algorithms to the choice of hyperparameters, such as the temperature coefficient α and the reward weight ω?
- Basis in paper: [explicit] The paper mentions that the reward weight ω is a hyperparameter to tune and that the temperature coefficient α is not accessible in practice. However, the sensitivity of the algorithms to these hyperparameters is not thoroughly investigated.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of hyperparameters affects the performance of the proposed algorithms.
- What evidence would resolve it: Conducting a sensitivity analysis by varying the hyperparameters and evaluating the performance of the algorithms in terms of success rates, basic rewards, and add-on rewards.

### Open Question 3
- Question: How does the performance of the proposed residual Q-learning algorithms compare to other state-of-the-art methods for policy customization, such as those based on divergence regularization or reward augmentation?
- Basis in paper: [explicit] The paper mentions that the proposed framework provides a principled way to interpret and design the combined objective for policy customization, but does not directly compare the performance of the algorithms to other state-of-the-art methods.
- Why unresolved: The paper does not provide a direct comparison of the proposed algorithms with other state-of-the-art methods for policy customization.
- What evidence would resolve it: Conducting experiments to compare the performance of the proposed algorithms with other state-of-the-art methods in terms of success rates, basic rewards, and add-on rewards.

## Limitations
- The framework relies on the assumption that the prior policy is a maximum-entropy policy with a well-defined Boltzmann distribution
- Experimental validation covers only four relatively simple environments, limiting generalizability
- Performance depends heavily on the quality of the prior policy and requires careful hyperparameter tuning

## Confidence
- High confidence: The mathematical derivation of the residual Q-function update rule and the policy construction formula
- Medium confidence: The empirical performance across different environments and the effectiveness of the temperature coefficient absorption
- Medium confidence: The framework's ability to handle both offline and online customization scenarios uniformly

## Next Checks
1. Test the framework with prior policies that are not maximum-entropy (e.g., deterministic policies or policies trained with different regularization) to verify the robustness of the decomposition mechanism.
2. Conduct ablation studies varying the temperature coefficient α across different states to assess the validity of absorbing it into a single reward weight ω.
3. Evaluate the framework on a more complex, high-dimensional environment (e.g., robotic manipulation tasks) to test scalability and generalization beyond the current experimental domains.