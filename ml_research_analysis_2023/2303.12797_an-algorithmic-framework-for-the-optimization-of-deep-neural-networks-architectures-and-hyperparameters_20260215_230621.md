---
ver: rpa2
title: An algorithmic framework for the optimization of deep neural networks architectures
  and hyperparameters
arxiv_id: '2303.12797'
source_url: https://arxiv.org/abs/2303.12797
tags:
- uni00000013
- search
- optimization
- uni0000004c
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an automated deep learning framework for jointly
  optimizing deep neural network (DNN) architectures and hyperparameters using directed
  acyclic graphs (DAGs). The key idea is to define a flexible search space over combinations
  of MLP, CNN, RNN and attention layers, encoded as DAGs.
---

# An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters

## Quick Facts
- arXiv ID: 2303.12797
- Source URL: https://arxiv.org/abs/2303.12797
- Reference count: 10
- Primary result: Framework finds models outperforming statistical, ML and DL baselines on 24 out of 40 datasets

## Executive Summary
This paper introduces an automated deep learning framework that jointly optimizes deep neural network architectures and hyperparameters using directed acyclic graphs (DAGs). The key innovation is a flexible search space that combines MLP, CNN, RNN, and attention layers, encoded as DAGs. Through evolutionary algorithms with custom neighborhood and mutation operators, the framework discovers diverse architectures that outperform traditional baselines on time series forecasting tasks across multiple datasets.

## Method Summary
The framework uses evolutionary algorithms to optimize directed acyclic graphs representing neural network architectures. The search space encodes combinations of MLP, CNN, RNN, and attention layers with their hyperparameters. Architecture optimization and hyperparameter tuning are performed sequentially to reduce noise. Structural diversity is encouraged through indicators like node count, width, depth, and edge density. The approach is evaluated on 40 datasets from the Monash Time Series Forecasting Repository, optimizing for Mean Absolute Scaled Error (MASE).

## Key Results
- Found models outperforming statistical, ML and DL baselines on 24 out of 40 datasets
- Models are diverse in size and layer types, not always deep
- Search is robust to nondeterminism across runs
- Frameworks discovers architectures with varied structural properties (width vs depth tradeoffs)

## Why This Works (Mechanism)

### Mechanism 1
DAG-based search space enables flexible composition of heterogeneous layers (MLP, CNN, RNN, attention) while preserving acyclic constraints. Each node encodes a layer and its hyperparameters; edges define data flow. Mutation operators preserve acyclicity by ensuring paths from input to output remain. Core assumption: Acyclic graph representation is expressive enough to model complex hybrid DNNs without redundant encoding.

### Mechanism 2
Sequential optimization of architecture then hyperparameters avoids hyperparameter noise during architecture search. Architecture operators mutate structure (node/edge insert/delete/substitute) without altering hyperparameters; later phases tune layer parameters. Core assumption: Architecture search converges faster when hyperparameters are fixed.

### Mechanism 3
Multi-objective optimization of depth, width, and sparsity balances model complexity and performance. Structural indicators (nodes, width, depth, edges) guide search toward diverse, non-redundant architectures. Core assumption: Diversity in structural metrics correlates with generalization across datasets.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs)**: Enables modeling of arbitrary layer connectivity without cycles, essential for hybrid architectures. Quick check: How does the framework prevent creating isolated nodes or cycles during mutation?
- **Neural Architecture Search (NAS)**: Core technique for automating DNN design, reduces manual tuning. Quick check: What distinguishes this DAG-based NAS from chain-structured or block-based approaches?
- **Hyperparameter Optimization (HPO)**: Fine-tunes layer-specific parameters (kernel size, dropout rate, etc.) after architecture is fixed. Quick check: How does the framework handle different hyperparameter spaces for different layer types?

## Architecture Onboarding

- **Component map**: Initialize → Evaluate → Select → Vary → Replace → Repeat
- **Critical path**: Population initialization → DAG encoding → Evolutionary operators → Sequential optimization → MASE evaluation
- **Design tradeoffs**: Flexibility vs. search space size (DAG allows more but increases search cost); Sequential vs. joint optimization (sequential avoids noise but may miss interactions); Structural diversity vs. convergence speed (indicators encourage variety but may slow convergence)
- **Failure signatures**: Stagnant populations (mutation/crossover not introducing diversity); High variance in validation MASE (nondeterminism not controlled); Poor performance on held-out data (overfitting to validation set)
- **First 3 experiments**: 1) Run with population=10, generations=20 on a single dataset to verify DAG encoding works; 2) Enable architecture-only optimization, fix hyperparameters, check structural indicator diversity; 3) Enable full sequential optimization, compare MASE against baseline statistical models

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed DAG-based search space compare to other NAS encodings in terms of efficiency and effectiveness for time series forecasting? The paper claims their DAG-based encoding is "more flexible than the existing ones in the literature" but lacks comprehensive comparison with other popular NAS encodings. Resolution would require a study comparing performance with chain-structured, hierarchical, and one-shot encodings on diverse time series benchmarks.

### Open Question 2
How does the choice of combiner type (element-wise addition, multiplication, or concatenation) impact the performance of generated DNNs? The paper mentions using three combiner types but does not analyze their impact on performance. Resolution would require analyzing combiner type impact on DNN performance across diverse time series forecasting benchmarks.

### Open Question 3
How does the proposed evolutionary algorithm compare to other metaheuristics (e.g., Bayesian optimization, reinforcement learning) for optimizing architecture and hyperparameters of DNNs? The paper states evolutionary algorithms are "well adapted to solve mixed and variable-space optimization algorithms" but lacks comparison with other metaheuristics. Resolution would require comparing performance with Bayesian optimization and reinforcement learning on diverse time series forecasting benchmarks.

## Limitations

- Computational cost of exploring large DAG search spaces limits scalability
- Sequential optimization may miss optimal architecture-hyperparameter interactions
- Claim of outperforming baselines on 24/40 datasets lacks statistical significance testing
- MASE metric may not fully capture temporal dependencies in multivariate settings

## Confidence

- **High Confidence**: DAG-based search space architecture and evolutionary operators are well-defined and technically sound
- **Medium Confidence**: Sequential optimization approach and structural diversity indicators are theoretically justified but lack empirical validation of their effectiveness
- **Low Confidence**: Claim of outperforming statistical, ML, and DL baselines on 24 out of 40 datasets lacks detailed statistical analysis and significance testing

## Next Checks

1. Perform paired t-tests comparing MASE across datasets between proposed method and baselines to quantify improvement significance
2. Implement joint architecture-hyperparameter optimization to measure performance gains over sequential optimization
3. Measure wall-clock time and resource utilization for different population sizes and generations to assess scalability limits