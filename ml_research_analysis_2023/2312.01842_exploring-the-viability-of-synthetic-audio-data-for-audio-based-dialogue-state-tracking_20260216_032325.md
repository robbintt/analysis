---
ver: rpa2
title: Exploring the Viability of Synthetic Audio Data for Audio-Based Dialogue State
  Tracking
arxiv_id: '2312.01842'
source_url: https://arxiv.org/abs/2312.01842
tags:
- speech
- audio
- synthetic
- human
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates synthetic audio data as an alternative
  to human recordings for training audio-based dialogue state tracking models. It
  introduces the SynthWOZ dataset, generated by combining textual MultiWOZ dialogues
  with TTS and vocoder models to produce multi-speaker synthetic audio.
---

# Exploring the Viability of Synthetic Audio Data for Audio-Based Dialogue State Tracking

## Quick Facts
- **arXiv ID**: 2312.01842
- **Source URL**: https://arxiv.org/abs/2312.01842
- **Reference count**: 0
- **Primary result**: Models trained solely on synthetic audio can generalize well to human speech, with pronunciation-aware metrics close to text-based models.

## Executive Summary
This study investigates synthetic audio data as an alternative to human recordings for training audio-based dialogue state tracking (DST) models. The authors introduce the SynthWOZ dataset, generated by combining textual MultiWOZ dialogues with TTS and vocoder models to produce multi-speaker synthetic audio. Two baseline models are developed: a cascading model that uses ASR followed by a text-based DST module, and an end-to-end model that maps audio directly to belief states. Experimental results show that models trained solely on synthetic audio can generalize well to human speech, with pronunciation-aware metrics close to those of text-based models trained on gold transcripts.

## Method Summary
The authors preprocess MultiWOZ2.1 text dialogues and synthesize audio using FastSpeech2 TTS and HiFiGAN vocoder with 10 speakers. Two model architectures are trained on this synthetic data: a cascading model (Wav2Vec2.0 encoder with CTC decoding → BART-based DST) and an end-to-end model (Wav2Vec2.0 encoder → BART decoder). Models are evaluated on both synthetic and human speech test sets using F1 and PhonemeF1 metrics, with comparisons to text-based BART baselines trained on gold transcripts.

## Key Results
- Models trained on synthetic audio achieve competitive performance on human speech test sets
- Multi-speaker synthesis improves model robustness and generalization
- PhonemeF1 metric captures pronunciation similarity better than exact text matching
- Cascading and end-to-end models show complementary strengths in different evaluation scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic audio data can substitute for real human recordings in training audio-based DST models.
- Mechanism: TTS synthesis generates diverse, high-quality audio from text dialogues, removing the need for costly human data collection while preserving semantic and syntactic structure.
- Core assumption: TTS-generated audio sufficiently captures the acoustic and prosodic patterns needed for accurate DST inference.
- Evidence anchors:
  - [abstract] Synthetic audio data offers an attractive solution for training audio-input DST models without relying on actual human voice data. By leveraging text-to-speech (TTS) models, it becomes possible to generate a diverse and customizable synthetic audio dialogue dataset.
  - [section 4.2] Synthesis of audio samples for our SynthWOZ dataset was accomplished by leveraging the FastSpeech2 TTS and HiFiGAN vocoder.
- Break condition: If TTS quality degrades (e.g., robotic intonation, mispronunciations), model performance on real human speech drops significantly.

### Mechanism 2
- Claim: Incorporating multi-speaker synthesis improves model robustness and generalization to human speech.
- Mechanism: Exposing the model to varied speaker identities during training reduces overfitting to a single voice and mimics real-world speaker diversity.
- Core assumption: Speaker variability in synthetic data approximates real human voice variation sufficiently.
- Evidence anchors:
  - [abstract] We carefully preprocess the transcripts to ensure optimal audio quality and clarity, and curate a multi-speaker synthetic dataset for better generalizability to actual human voices.
  - [section 6.4] The SynthWOZ dataset is curated with multiple speakers to enhance the robustness and generalizability to authentic human audio.
- Break condition: If synthetic speakers lack diversity (e.g., all same gender or accent), generalization to new speakers weakens.

### Mechanism 3
- Claim: The PhonemeF1 metric better captures pronunciation similarity than exact text matching.
- Mechanism: By computing phonetic edit distance between predicted and ground truth values, the metric awards partial credit for near-matches, reflecting spoken variation.
- Core assumption: Phonetic similarity correlates with functional correctness in dialogue state tracking.
- Evidence anchors:
  - [abstract] We introduce a novel evaluation metric called PhonemeF1, which is designed specifically to capture pronunciation similarity in the context of DST in the audio domain.
  - [section 5.2] Our PhonemeF1 metric bridges this gap by giving partial credit to predictions with higher pronunciation similarity with that of the ground truth.
- Break condition: If phoneme representations fail to capture meaning (e.g., homophones with different semantics), metric becomes misleading.

## Foundational Learning

- Concept: Automatic Speech Recognition (ASR)
  - Why needed here: Cascading models convert audio to text before DST; ASR quality directly impacts downstream performance.
  - Quick check question: What is the Word Error Rate (WER) for the ASR model on the human test set?
- Concept: Phonetic representation (IPA, phoneme editing)
  - Why needed here: PhonemeF1 relies on mapping words to phonemes to measure similarity; understanding IPA is key to interpreting results.
  - Quick check question: How is phonetic Levenshtein distance calculated between two phoneme sequences?
- Concept: Multi-turn dialogue structure
  - Why needed here: DST models track belief states across turns; system utterances provide context for interpreting user inputs.
  - Quick check question: How does the belief state accumulate over dialogue turns in the SynthWOZ dataset?

## Architecture Onboarding

- Component map: Audio → (ASR) → Encoder → Decoder → Belief State → PhonemeF1
- Critical path: Audio → (ASR) → Encoder → Decoder → Belief State → PhonemeF1
- Design tradeoffs:
  - Cascading vs. end-to-end: Cascading adds ASR error propagation but isolates DST; end-to-end skips ASR but may require more data.
  - Multi-speaker vs. single-speaker: Multi-speaker improves robustness but increases synthesis complexity.
- Failure signatures:
  - High ASR WER → cascading model degradation
  - Low PhonemeF1 but high F1 → metric mismatch or phonetic ambiguity
  - Performance gap between synthetic and human test sets → insufficient speaker diversity or prosody mismatch
- First 3 experiments:
  1. Train E2E model on single-speaker SynthWOZ; evaluate on human test set.
  2. Train cascading model on multi-speaker SynthWOZ; compare PhonemeF1 vs F1 on synthetic vs human test sets.
  3. Ablate system utterances (use user-only audio); measure impact on belief state accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of synthetic audio data-based DST models compare to models trained on real human speech data across diverse dialogue domains and accents?
- Basis in paper: [explicit] The paper investigates the viability of synthetic audio data for audio-based DST and compares model performance on synthetic and human speech test sets, showing promising results with minor score differences.
- Why unresolved: While the paper demonstrates that models trained on synthetic data generalize well to human speech, it does not provide a comprehensive comparison across diverse dialogue domains and accents beyond the initial study.
- What evidence would resolve it: Conducting extensive experiments with synthetic and human speech data across multiple dialogue domains and accents would provide insights into the generalizability and robustness of synthetic audio data-based DST models.

### Open Question 2
- Question: What are the potential limitations and challenges of using synthetic audio data for audio-based DST, and how can they be addressed?
- Basis in paper: [inferred] The paper mentions the need for more diverse scenarios in the synthetic dataset to improve model robustness and reduce errors, indicating potential limitations and challenges.
- Why unresolved: The paper highlights the importance of diverse data but does not delve into specific limitations and challenges of using synthetic audio data for DST or propose solutions to address them.
- What evidence would resolve it: Conducting in-depth analyses of the limitations and challenges of synthetic audio data, such as issues with pronunciation variations and environmental noise, and proposing innovative solutions to overcome them would provide valuable insights.

### Open Question 3
- Question: How does the inclusion of paralinguistic information, such as intonation and speaker characteristics, in synthetic audio data impact the performance of audio-based DST models?
- Basis in paper: [explicit] The paper mentions that the E2E model, which directly utilizes speech input, can access intrinsic paralinguistic information, suggesting its potential impact on model performance.
- Why unresolved: While the paper acknowledges the presence of paralinguistic information in synthetic audio data, it does not thoroughly investigate its impact on the performance of audio-based DST models.
- What evidence would resolve it: Conducting experiments to compare the performance of models trained on synthetic audio data with and without paralinguistic information, and analyzing the impact on model accuracy and robustness, would provide insights into the significance of this factor.

## Limitations
- Synthetic audio fidelity gap: The study does not quantify perceptual quality differences between synthetic and human speech beyond automated metrics
- Speaker diversity constraints: The 10-speaker pool (5F, 5M) may not capture the full range of real-world speaker variation
- Evaluation metric limitations: PhonemeF1 may not fully capture semantic equivalence when phonetically similar words have different meanings

## Confidence
**High Confidence Claims**:
- Synthetic audio can effectively train audio-based DST models for zero-shot generalization to human speech
- Multi-speaker synthesis improves model robustness compared to single-speaker approaches
- The cascading model architecture with ASR preprocessing achieves strong performance

**Medium Confidence Claims**:
- Synthetic data offers cost advantages over human recordings
- PhonemeF1 provides better evaluation than exact text matching for spoken dialogue

**Low Confidence Claims**:
- The approach generalizes to languages other than English
- The methodology applies to non-task-oriented dialogue domains

## Next Checks
1. Measure Word Error Rate and Character Error Rate for the ASR component on the human speech test set to quantify the fidelity gap between synthetic and real audio processing.
2. Evaluate model performance when speakers in the test set were not present in the training data to assess true generalization capabilities beyond the 10-speaker pool.
3. Manually annotate a subset of predictions where PhonemeF1 scores high but semantic meaning differs from ground truth to assess the metric's correlation with functional correctness in dialogue state tracking.