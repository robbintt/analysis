---
ver: rpa2
title: Neural (Tangent Kernel) Collapse
arxiv_id: '2305.16427'
source_url: https://arxiv.org/abs/2305.16427
tags:
- init
- normal
- yyti
- lecun
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work connects two important concepts: the Neural Tangent
  Kernel (NTK) and the Neural Collapse (NC) phenomenon. The authors show that NC emerges
  from the NTK block structure in DNNs trained with mean squared error (MSE) loss.'
---

# Neural (Tangent Kernel) Collapse

## Quick Facts
- arXiv ID: 2305.16427
- Source URL: https://arxiv.org/abs/2305.16427
- Reference count: 40
- Primary result: Neural Collapse emerges from NTK block structure during MSE training

## Executive Summary
This work establishes a theoretical connection between Neural Collapse (NC) and the Neural Tangent Kernel (NTK) framework. The authors demonstrate that NC emerges when DNNs with block-structured NTKs are trained with mean squared error loss. They derive three distinct convergence rates for different error components and identify an invariant that characterizes when NC will emerge. Large-scale experiments on VGG, ResNet, and DenseNet architectures across MNIST, FashionMNIST, and CIFAR10 validate the theory, showing that the invariant effectively distinguishes between models that do and do not exhibit NC.

## Method Summary
The study analyzes DNN training dynamics using the NTK framework, assuming block-structured kernels that align with class labels. The method involves computing the empirical NTK during training, tracking error components through their eigendecomposition, and monitoring an invariant quantity that remains constant during gradient flow. Experiments use SGD with Nesterov momentum and weight decay, with learning rate scheduling, on balanced batches across three architectures and datasets. The theoretical analysis derives convergence rates and identifies conditions for NC emergence based on the kernel structure and invariant alignment.

## Key Results
- NC emerges from NTK block structure when kernel shows stronger within-class than between-class correlations
- Three distinct eigenvalues in block-structured NTK create three convergence rates for different error components
- An invariant E = 1/m W⊤W - 1/μclass H1(IC - α1C1⊤C)H⊤1 - 1/μsingle H2H⊤2 characterizes whether NC will emerge
- Models exhibiting NC have invariant E ≈ 0, while non-NC models have large invariant values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural Collapse emerges from the NTK block structure during training with MSE loss.
- Mechanism: The block structure in the NTK (stronger correlations within classes than between classes) creates local elasticity, causing class means to converge to symmetric configurations while individual features collapse to their class means.
- Core assumption: The empirical NTK develops a block structure aligned with class labels, where samples from the same class have stronger correlations than samples from different classes.
- Evidence anchors:
  - [abstract] "We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes."
  - [section 2.3] "The NTK block structure of ResNet20 trained on MNIST" showing empirical evidence of block structure.
  - [corpus] Weak - only 1/8 papers mention NTK alignment with labels, others focus on different aspects of NTK.
- Break condition: If the NTK does not develop block structure (e.g., κc ≈ κn or γc ≈ γn), local elasticity fails and NC does not emerge.

### Mechanism 2
- Claim: Three distinct convergence rates govern the dynamics, corresponding to error components: global mean, class means, and individual samples.
- Mechanism: The eigendecomposition of the block-structured NTK reveals three eigenvalues (λglobal ≥ λclass ≥ λsingle) that control convergence rates of different error components during training.
- Core assumption: The block-structured kernel has three distinct eigenvalues that create different convergence rates for error components.
- Evidence anchors:
  - [section 4.1] "We notice that the NTK has three distinct eigenvalues λglobal ≥ λclass ≥ λsingle, which imply different convergence rates for certain components of the error."
  - [table 1] Eigendecomposition showing the three eigenvalues and their associated eigenvectors.
  - [corpus] Weak - NTK convergence analysis exists but not specifically in the three-phase framework described.
- Break condition: If the three eigenvalues are not sufficiently distinct (e.g., λclass ≈ λsingle), the error components may not separate cleanly and NC dynamics may be disrupted.

### Mechanism 3
- Claim: An invariant characterizes the learning dynamics and provides a necessary condition for NC emergence.
- Mechanism: The invariant E = 1/m W⊤W - 1/μclass H1(IC - α1C1⊤C)H⊤1 - 1/μsingle H2H⊤2 remains constant during training and must be aligned with W⊤W for NC to occur.
- Core assumption: The invariant derived from the dynamics is zero (or at least aligned with W⊤W) for models that exhibit NC.
- Evidence anchors:
  - [section 5.2] "Moreover, the quantity E := 1/m W⊤W - 1/μclass H1(IC - α1C1⊤C)H⊤1 - 1/μsingle H2H⊤2 is invariant in time."
  - [section 5.3] "Assume further that the last-layer features are centralized, i.e ⟨h⟩ = 0, and the dynamics invariant (26) is zero, i.e., E = O."
  - [figure 2] Numerical experiments showing that models with NC have E ≈ 0 while models without NC do not.
- Break condition: If the invariant is not zero or not aligned with W⊤W, variability collapse (NC1) will not occur even if other conditions are met.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in capturing correlations during DNN training.
  - Why needed here: The entire theory builds on NTK alignment and block structure as the mechanism driving NC.
  - Quick check question: Can you explain the difference between the infinite-width NTK (label-agnostic) and empirical NTK (label-aligned) in the context of classification?

- Concept: Neural Collapse (NC) phenomenon and its four defining behaviors (NC1-NC4).
  - Why needed here: The paper aims to explain how NC emerges from NTK dynamics, so understanding NC is foundational.
  - Quick check question: What are the four behaviors that define Neural Collapse, and how do they relate to the structure of last-layer features?

- Concept: Block-structured matrices and eigendecomposition.
  - Why needed here: The analysis relies on understanding the eigendecomposition of block-structured kernels to identify convergence rates.
  - Quick check question: Given a block-structured matrix with diagonal blocks of size m×m, what would be the form of its eigenvectors and eigenvalues?

## Architecture Onboarding

- Component map:
  Input data → DNN layers → Output layer with weights W → Kernel computation (Θ, Θh) → Training with MSE loss → Error tracking (R, Rclass, Rglobal) → Invariant monitoring

- Critical path:
  1. Initialize DNN and compute initial kernels
  2. Train with gradient flow/MSE loss while monitoring kernel alignment
  3. Track error components (global mean, class means, individual samples)
  4. Monitor invariant E during training
  5. Check for NC behaviors (variability collapse, ETF structure, self-duality, NCC)

- Design tradeoffs:
  - MSE vs cross-entropy: MSE allows cleaner theoretical analysis but may converge slower
  - Kernel computation: Computing full NTK is expensive; approximations may be needed for large models
  - Initialization: Different initializations affect whether invariant alignment occurs

- Failure signatures:
  - Invariant E remains large: Model will not exhibit NC despite good performance
  - Kernel alignment weak: Block structure not forming, local elasticity absent
  - Class means not forming ETF: NC2 fails even if NC1 occurs

- First 3 experiments:
  1. Train ResNet20 on MNIST with varying learning rates and initializations, compute NTK alignment and invariant E to identify models with/without NC
  2. Compare MSE vs cross-entropy loss for same architecture/dataset to verify MSE can achieve comparable performance
  3. Modify initialization to break invariant alignment (e.g., extreme weight scales) and observe failure of NC emergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the block-structured NTK assumption hold empirically across different architectures and datasets?
- Basis in paper: [explicit] The paper extensively discusses the block structure of NTK in trained DNNs and uses it as a core assumption for their theoretical analysis.
- Why unresolved: The paper validates the assumption on several datasets and architectures but doesn't provide a comprehensive empirical study across a wider range of models and tasks.
- What evidence would resolve it: A large-scale empirical study examining the NTK block structure in diverse DNN architectures (e.g., Transformers, RNNs) and various tasks (e.g., object detection, NLP) would provide stronger evidence.

### Open Question 2
- Question: How does the invariant derived in the paper relate to other theoretical frameworks in deep learning, such as the Neural Tangent Kernel (NTK) theory or the concept of implicit regularization?
- Basis in paper: [explicit] The paper mentions the connection to NTK and hyperbolic dynamics, but a more detailed comparison is needed.
- Why unresolved: The paper focuses on the specific invariant derived for DNNs with block-structured NTK, but its broader implications and connections to other theoretical frameworks remain unexplored.
- What evidence would resolve it: A theoretical analysis connecting the derived invariant to other established frameworks in deep learning would provide a deeper understanding of its significance.

### Open Question 3
- Question: Can the theoretical framework developed in the paper be extended to other loss functions beyond mean squared error (MSE)?
- Basis in paper: [explicit] The paper primarily focuses on DNNs trained with MSE loss, acknowledging that cross-entropy (CE) is more common in practice.
- Why unresolved: While the authors mention the potential for generalization, they don't provide a rigorous extension of their theory to other loss functions.
- What evidence would resolve it: A theoretical extension of the framework to other commonly used loss functions, such as CE or hinge loss, would broaden its applicability and impact.

### Open Question 4
- Question: How does the presence of non-balanced datasets affect the emergence of Neural Collapse (NC) in DNNs with block-structured NTK?
- Basis in paper: [explicit] The paper assumes a balanced dataset for simplicity, but acknowledges the need to study the effects of non-balanced datasets.
- Why unresolved: The theoretical analysis relies on the assumption of a balanced dataset, but real-world datasets are often imbalanced, which could significantly impact the dynamics and the emergence of NC.
- What evidence would resolve it: An extension of the theoretical framework to handle non-balanced datasets, supported by empirical experiments, would provide a more realistic understanding of NC in practical scenarios.

## Limitations

- The theoretical framework assumes infinite-width networks and continuous-time gradient flow, which may not perfectly capture finite-width dynamics with discrete optimization.
- The invariant condition, while theoretically motivated, requires empirical verification across diverse architectures and datasets.
- The connection between NTK block structure and label alignment is assumed rather than proven from first principles.

## Confidence

- High confidence: Empirical observations of NTK block structure and its correlation with NC emergence
- Medium confidence: Theoretical derivation of three-phase dynamics from block-structured NTK
- Medium confidence: Invariant characterization as a necessary condition for NC

## Next Checks

1. Test the invariant condition on architectures with different kernel properties (e.g., attention-based models) to determine if it generalizes beyond CNNs.
2. Conduct ablation studies varying the strength of NTK block structure (via data augmentation or architectural modifications) to quantify its causal role in NC emergence.
3. Compare the finite-width NTK dynamics with the theoretical infinite-width predictions to identify discrepancies and refine the theory.