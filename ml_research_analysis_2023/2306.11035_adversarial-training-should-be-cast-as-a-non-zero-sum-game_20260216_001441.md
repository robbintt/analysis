---
ver: rpa2
title: Adversarial Training Should Be Cast as a Non-Zero-Sum Game
arxiv_id: '2306.11035'
source_url: https://arxiv.org/abs/2306.11035
tags:
- adversarial
- robustness
- robust
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the fundamental limitations of surrogate-based
  adversarial training (AT) in deep learning, showing that maximizing surrogate losses
  provides no guarantee of increasing the true classification error, leading to weak
  adversaries and ineffective training. The authors propose a novel non-zero-sum bilevel
  formulation of AT, where the defender minimizes a surrogate upper bound on the classification
  error while the attacker maximizes a continuous reformulation of the classification
  error using the negative margin.
---

# Adversarial Training Should Be Cast as a Non-Zero-Sum Game

## Quick Facts
- arXiv ID: 2306.11035
- Source URL: https://arxiv.org/abs/2306.11035
- Reference count: 40
- The paper shows that adversarial training should be formulated as a non-zero-sum game rather than zero-sum, eliminating robust overfitting and achieving state-of-the-art robustness without heuristics.

## Executive Summary
The paper addresses fundamental limitations in adversarial training by showing that maximizing surrogate losses provides no guarantee of increasing true classification error, leading to weak adversaries and ineffective training. The authors propose a novel non-zero-sum bilevel formulation where the defender minimizes a surrogate upper bound on classification error while the attacker maximizes a continuous reformulation of the classification error using the negative margin. This approach, implemented as BETA-AT, achieves state-of-the-art robustness on CIFAR-10 matching AutoAttack performance without multiple restarts or complex optimization schedules, while eliminating robust overfitting.

## Method Summary
The paper reformulates adversarial training as a non-zero-sum bilevel optimization problem. The attacker (lower level) maximizes the negative margin for each class using gradient-based optimization, while the defender (upper level) minimizes a cross-entropy loss computed on the most effective adversarial perturbation. This is implemented through the BETA attack algorithm, which computes one adversarial perturbation per class and selects the best one for training. The BETA-AT algorithm trains the model using these perturbations, directly targeting misclassification rather than surrogate losses.

## Key Results
- BETA-AT eliminates robust overfitting, maintaining consistent test robustness throughout training
- Achieves state-of-the-art robustness on CIFAR-10 matching AutoAttack performance without multiple restarts
- Provides almost identical robustness estimates to AutoAttack using a simple off-the-shelf optimizer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surrogate losses in adversarial training do not guarantee increased classification error, leading to weak adversaries
- Mechanism: When the defender maximizes a surrogate loss (like cross-entropy) that is an upper bound on classification error, the resulting adversarial perturbations may not actually cause misclassification. The surrogate can be arbitrarily large while the true classification error remains zero.
- Core assumption: The surrogate loss being maximized by the attacker must be an upper bound on the classification error for the attack to be meaningful
- Evidence anchors:
  - [abstract] "maximizing a surrogate like the cross-entropy provides no guarantee that the classification error will increase"
  - [section 3.1] "the right hand side of (9) can be arbitrarily large while the left hand side can simultaneously be equal to zero"
- Break condition: If the surrogate loss is not an upper bound on classification error, or if the attacker's optimization is poorly conditioned

### Mechanism 2
- Claim: The proposed non-zero-sum bilevel formulation eliminates robust overfitting by properly aligning objectives
- Mechanism: By having the defender minimize a surrogate upper bound on classification error while the attacker maximizes the continuous negative margin reformulation, both players optimize for their true goals. This alignment prevents the defender from overfitting to weak adversarial examples that don't generalize.
- Core assumption: The negative margin provides a continuous, differentiable surrogate that correctly represents the classification error
- Evidence anchors:
  - [abstract] "our approach matches the test robustness achieved by the state-of-the-art, yet highly heuristic approaches such as AutoAttack, and that it does not suffer from robust overfitting"
  - [section 3.3] "the attacker and defender optimize separate objectives, which constitutes a non-zero-sum game that preserves guarantees on robustness"
- Break condition: If the negative margin does not accurately represent classification error for complex, non-linear models

### Mechanism 3
- Claim: BETA-AT achieves comparable performance to AutoAttack without heuristics by directly targeting misclassification
- Mechanism: The BETA attack finds adversarial examples that maximize the negative margin for each class, then selects the most effective perturbation. This directly optimizes for misclassification rather than a surrogate, eliminating the need for multiple restarts or complex stopping conditions.
- Core assumption: Maximizing the negative margin for each class and selecting the best provides an optimal adversarial example
- Evidence anchors:
  - [abstract] "BETA provides almost identical estimates of robustness to AutoAttack, despite using an off-the-shelf optimizer"
  - [section 4] "BETA computes one adversarial perturbation per class, but only one of these perturbations is chosen for the upper level"
- Break condition: If the negative margin maximization fails to find meaningful adversarial examples in high-dimensional spaces

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: The adversarial training problem is naturally formulated as a bilevel optimization where the defender's objective depends on the solution to the attacker's inner optimization problem
  - Quick check question: In the bilevel formulation, which level represents the defender and which represents the attacker?

- Concept: Surrogate losses and their guarantees
  - Why needed here: Understanding why standard surrogate losses fail in adversarial training requires knowledge of how surrogate losses guarantee performance in standard supervised learning
  - Quick check question: What property must a surrogate loss have to guarantee decreased classification error in standard supervised learning?

- Concept: Zero-sum vs non-zero-sum games
  - Why needed here: The paper argues that adversarial training should be a non-zero-sum game because the defender and attacker have different objectives, contrary to the standard zero-sum formulation
  - Quick check question: Why does having different objectives for defender and attacker make this a non-zero-sum game rather than zero-sum?

## Architecture Onboarding

- Component map:
  - Attacker module: BETA algorithm that maximizes negative margin for each class
  - Defender module: BETA-AT that trains model using perturbed examples from BETA
  - Surrogate loss module: Cross-entropy loss for the upper-level optimization
  - Model module: Standard deep neural network architecture
  - Optimization module: Stochastic gradient descent for both levels

- Critical path: BETA generates adversarial examples → BETA-AT computes perturbed loss → model parameters are updated → repeat for each batch
- Design tradeoffs: Direct negative margin optimization vs computational efficiency, smooth approximation vs exact max computation, single vs multiple adversarial perturbations per batch
- Failure signatures: Robust overfitting (test accuracy drops while training accuracy remains high), weak adversarial examples (perturbations don't cause misclassification), computational inefficiency (slow training due to complex optimization)
- First 3 experiments:
  1. Verify BETA finds adversarial examples on a simple linear model where the negative margin is easy to compute
  2. Compare BETA-AT vs standard PGD training on a small CNN to confirm elimination of robust overfitting
  3. Test BETA vs APGD on a pre-trained robust model to verify comparable attack performance without multiple restarts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the non-zero-sum bilevel formulation be extended to other adversarial settings beyond ℓ∞ perturbations, such as ℓ2 perturbations or natural distribution shifts?
- Basis in paper: [inferred] The paper mentions "other changes in the data space, including the kinds of distribution shifts that are common in fields like domain adaptation and domain generalization" as future directions.
- Why unresolved: The paper focuses specifically on ℓ∞ adversarial perturbations and does not provide theoretical or empirical evidence for other perturbation types or distributional shifts.
- What evidence would resolve it: Experimental results comparing BETA-AT performance on ℓ2 perturbations and natural distribution shifts, along with theoretical analysis of how the bilevel formulation extends to these settings.

### Open Question 2
- Question: What is the theoretical convergence rate of BETA and BETA-AT, and how does it compare to standard adversarial training methods?
- Basis in paper: [inferred] The paper mentions "a convergence analysis of BETA and an analysis of the sample complexity of BETA-AT are two more directions that we leave for future work."
- Why unresolved: The paper focuses on empirical performance and theoretical guarantees of the formulation but does not provide convergence analysis or sample complexity bounds.
- What evidence would resolve it: Formal proofs establishing convergence rates for BETA and BETA-AT under various optimization settings, along with empirical validation against convergence bounds.

### Open Question 3
- Question: How does the temperature parameter µ in the smooth reformulation (SBETA-AT) affect the trade-off between smoothness and approximation quality, and what is the optimal choice?
- Basis in paper: [explicit] The paper discusses the smooth reformulation in Appendix B and mentions that µ > 0 is a temperature constant but does not provide guidance on its selection.
- Why unresolved: The paper presents the smooth formulation but does not empirically evaluate the impact of µ or provide theoretical analysis of its effect on the approximation.
- What evidence would resolve it: Sensitivity analysis showing BETA/AT performance across different µ values, along with theoretical bounds on approximation quality as a function of µ.

## Limitations
- Computational overhead from computing multiple adversarial perturbations per batch may limit scalability to larger datasets
- Performance guarantees are primarily demonstrated on CIFAR-10 with ResNet-18, limiting generalizability claims
- The elimination of robust overfitting may depend on factors beyond the non-zero-sum formulation that were not fully controlled

## Confidence
- **High confidence** in the theoretical foundations of non-zero-sum formulation
- **Medium confidence** in empirical scalability claims due to lack of ablation studies on computational costs
- **High confidence** in identification of surrogate loss limitations
- **Medium confidence** in generality of BETA-AT across different architectures
- **Low confidence** in claims about eliminating robust overfitting being solely due to non-zero-sum formulation

## Next Checks
1. **Ablation on Computational Efficiency**: Measure and compare wall-clock training times between BETA-AT and standard PGD training across different batch sizes and network depths to quantify the practical overhead of computing multiple adversarial perturbations per batch.

2. **Architecture Generalization Study**: Apply BETA-AT to different architectures (e.g., WideResNet, EfficientNet) on CIFAR-10 and ImageNet to test whether the robustness gains and overfitting resistance hold across architectural families with varying capacities and design philosophies.

3. **Surrogate Loss Alignment Analysis**: Systematically test alternative surrogate losses (e.g., hinge loss, logistic loss) in the upper-level optimization to determine whether the elimination of robust overfitting is specifically due to the non-zero-sum formulation or can be achieved through other loss alignments that better track classification error.