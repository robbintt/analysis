---
ver: rpa2
title: Masked Structural Growth for 2x Faster Language Model Pre-training
arxiv_id: '2305.02869'
source_url: https://arxiv.org/abs/2305.02869
tags:
- growth
- training
- schedule
- pre-training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework, Masked Structural Growth
  (MSG), for progressive training of language models. MSG addresses the challenge
  of accelerating large language model pre-training by progressively growing a small
  Transformer structure to a large one.
---

# Masked Structural Growth for 2x Faster Language Model Pre-training

## Quick Facts
- arXiv ID: 2305.02869
- Source URL: https://arxiv.org/abs/2305.02869
- Authors: 
- Reference count: 40
- This paper proposes a novel framework, Masked Structural Growth (MSG), for progressive training of language models.

## Executive Summary
This paper introduces Masked Structural Growth (MSG), a novel framework for accelerating large language model pre-training through progressive structural growth. MSG uses a masking mechanism to ensure strict function preservation during model expansion, allowing training to start from smaller models and grow them to larger sizes while maintaining or improving downstream performance. The approach supports growth in all possible dimensions of Transformer models and achieves up to 2.2x speedup in pre-training while maintaining comparable or better downstream task performance.

## Method Summary
MSG is a progressive training framework that grows a small Transformer structure to a large one through strictly function-preserving operators. The key innovation is a masking mechanism that ensures function preservation by first eliminating the effects of expanded new structures, then gradually increasing their importance. MSG supports growth in layer number, hidden dimension, feedforward network dimension, and head number, providing maximum flexibility in schedule design. The method is independent of new weight initialization and solves the Layer Normalization dilemma that prevents strict function preservation in previous approaches like Net2Net.

## Key Results
- Achieves up to 2.2x speedup in pre-training different types of language models
- Maintains comparable or better downstream performance on GLUE and SQuAD tasks compared to models trained from scratch
- Improves downstream task performances compared to models trained from scratch without growth
- Supports growth in all four dimensions of Transformer models (layer_num, hidden_dim, ffn_dim, head_num)

## Why This Works (Mechanism)

### Mechanism 1: External Masking for Function Preservation
MSG achieves function preservation through external masking rather than weight copying or splitting. By initializing new neurons to arbitrary values and applying an external mask (value 0 for new neurons), MSG ensures the output remains identical to the smaller model. The mask gradually increases to 1 over training steps, activating new neurons progressively. The core assumption is that the mask completely eliminates the effect of new structures on the model's output, making initialization irrelevant for function preservation.

### Mechanism 2: Solving Layer Normalization Dilemma
MSG solves the Layer Normalization dilemma that prevents strict function preservation in Net2Net by extending the masking approach to Layer Normalization. The method adjusts mean and variance calculations to exclude new neurons (masked to 0), then re-masks the output. This ensures LN(x) = LN(x') for any expansion, maintaining strict function preservation even when new neurons are added.

### Mechanism 3: Decoupling from Initialization
MSG's independence from initialization enables better training dynamics than weight-dependent operators. Since MSG doesn't rely on copying or splitting existing weights, new neurons can be initialized optimally (e.g., from N(0, 0.02) distribution) rather than being constrained by existing weights. This avoids the "symmetry issue" where new neurons receive similar gradients, allowing optimization of new weights for better gradients and faster convergence.

## Foundational Learning

- **Concept: Function preservation**
  - Why needed here: MSG's core innovation is strict function preservation through masking, unlike Net2Net which has limitations
  - Quick check question: Why does Net2Net fail to preserve function when growing hidden_dim with Layer Normalization?

- **Concept: Progressive training schedules**
  - Why needed here: MSG explores schedules with growth in all dimensions sequentially, unlike previous work limited to 1-2 dimensions
  - Quick check question: How does MSG's multi-stage approach differ from CompoundGrow's strategy?

- **Concept: Masking mechanisms in neural networks**
  - Why needed here: MSG uses external masks to eliminate new structure effects, a novel application in progressive training
  - Quick check question: How does MSG's mask differ from standard dropout or attention masks?

## Architecture Onboarding

- **Component map:**
  - Masking module -> Growth operator -> Schedule manager -> Initialization handler

- **Critical path:**
  1. Model training until growth trigger
  2. Structure expansion with new neurons
  3. Mask application to preserve function
  4. Gradual mask increase over training steps
  5. Continue training with expanded model

- **Design tradeoffs:**
  - Mask granularity: Per-neuron vs. per-layer masking affects implementation complexity
  - Growth frequency: More stages = better function preservation but more complexity
  - Initialization strategy: Random vs. learned initialization impacts training dynamics

- **Failure signatures:**
  - Training loss spike after growth indicates mask failure
  - Downstream performance drop suggests function preservation issues
  - Slow convergence may indicate poor initialization strategy

- **First 3 experiments:**
  1. Single dimension growth (hidden_dim) with mask vs. no mask comparison
  2. Multi-stage growth schedule validation on Bert-base
  3. Function preservation test: compare outputs before/after growth on validation set

## Open Questions the Paper Calls Out

### Open Question 1: Optimal Multi-dimensional Growth Schedule
The paper acknowledges that while MSG supports growth on all dimensions, the heuristic rules currently guide growth of one dimension at a time. There is unexplored space where subsets of several growth dimensions can be combined in the same stage. The complexity of mathematically measuring the impact of each dimension on training dynamics makes it challenging to create an optimal multi-dimensional growth schedule.

### Open Question 2: Combining MSG with Data-Driven Initialization
MSG is the only operator that supports free initialization of new weights, and while random initialization performs better than Net2Net, the best initialization strategy for Transformer growth remains an open question. The paper mentions that MSG can potentially be combined with LiGO on new weights, but this orthogonal combination is left unexplored.

### Open Question 3: Theoretical Foundation for Function Preservation
While the paper claims that MSG achieves strict function preservation in all growth dimensions, unlike Net2Net which only achieves non-strict preservation, the mathematical proof or theoretical explanation for this property is not provided in detail. The paper demonstrates practical benefits but lacks rigorous mathematical explanation for why MSG achieves this property in all cases.

## Limitations
- The paper's claims about strict function preservation through masking remain largely theoretical with limited empirical validation
- The choice of mask increment schedule (from 0 to 1) and its effect on training dynamics is not thoroughly explored
- The initialization strategy for new weights, described as "free," lacks specific implementation details for reproducibility

## Confidence

**High Confidence:** The core claim that MSG achieves faster pre-training with comparable or better downstream performance is well-supported by experimental results across multiple models and tasks.

**Medium Confidence:** The theoretical framework for function preservation through masking is sound but the empirical validation is limited, with minimal evidence that masks actually preserve function during growth operations.

**Low Confidence:** The claim that MSG's independence from initialization leads to better training dynamics is asserted but not empirically validated through experiments comparing different initialization strategies.

## Next Checks

1. **Direct Function Preservation Test:** Implement a controlled experiment comparing model outputs before and after each growth stage on a fixed validation set to verify that the masking mechanism actually preserves function as claimed.

2. **Mask Schedule Sensitivity Analysis:** Systematically vary the mask increment schedule (e.g., linear vs. exponential vs. step-wise) to determine how different growth schedules affect both function preservation and downstream performance.

3. **Initialization Strategy Comparison:** Compare MSG with different initialization strategies for new weights (random vs. learned vs. copied) to empirically validate whether the claimed independence from initialization translates to practical training benefits.