---
ver: rpa2
title: 'MERGE: Fast Private Text Generation'
arxiv_id: '2305.15769'
source_url: https://arxiv.org/abs/2305.15769
tags:
- merge
- inference
- generation
- embedding
- mpcformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new framework for privacy-preserving text
  generation that can accelerate the generation process while maintaining the quality
  of the generated text. The authors address the problem of slow inference speed in
  existing two-party privacy-preserving techniques and propose a fast private text
  generation framework called MERGE.
---

# MERGE: Fast Private Text Generation

## Quick Facts
- **arXiv ID**: 2305.15769
- **Source URL**: https://arxiv.org/abs/2305.15769
- **Reference count**: 17
- **Primary result**: 26.5x speedup to vanilla encrypted model with 80% communication cost reduction

## Executive Summary
MERGE introduces a novel framework for accelerating privacy-preserving text generation by addressing the inference speed bottleneck in existing two-party MPC techniques. The framework combines embedding resending (ER) and a merge module (MM) to bypass embedding table queries and reorganize linear operations in Transformer modules. Through extensive experiments on GPT-2, T5-small, and BART-base models across MultiWoz, DailyDialog, and CommonGen datasets, MERGE achieves significant speedups (26.5x) while maintaining generation quality and reducing communication costs by 80%.

## Method Summary
MERGE accelerates private text generation through two key mechanisms: embedding resending, which reuses output hidden states as input embeddings to bypass embedding table queries, and a merge module that simplifies Transformer computations by using constant attention matrices and reorganizing linear operations. The framework trains models using a combination of cosine similarity loss for embedding alignment and cross-entropy loss for generation quality, with an optional embedding augmentation method to enhance robustness to noised embeddings.

## Key Results
- Achieves 26.5x speedup over vanilla encrypted models at sequence length 512
- Reduces communication costs by 80% compared to baseline approaches
- Outperforms state-of-the-art approximated models by up to 10x in speed while maintaining generation quality (BERTscore, BARTscore metrics)
- Maintains competitive performance across three diverse text generation tasks (MultiWoz, DailyDialog, CommonGen)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding resending eliminates the embedding table query bottleneck in auto-regressive generation
- Mechanism: The output hidden state from the previous generation step is directly reused as the new input token embedding, bypassing the need to query the embedding table
- Core assumption: Hidden states and token embeddings exist in the same representation space, and the embedding layer is the inverse of the linear head
- Evidence anchors: [abstract] "MERGE reuse the output hidden state as the word embedding to bypass the embedding computation"

### Mechanism 2
- Claim: Merging linear operations in the Transformer module reduces computation time and communication bytes
- Mechanism: Replaces dynamic attention matrices with constant ones and merges projection, attention, and feed-forward operations into a single linear layer
- Core assumption: Constant attention matrices maintain sufficient model performance while enabling pre-computation of merged weights
- Evidence anchors: [abstract] "reorganize the linear operations in the Transformer module to accelerate the forward procedure"

### Mechanism 3
- Claim: The embedding alignment and augmentation task maintains model robustness to noised embeddings
- Mechanism: Trains the model to maximize cosine similarity between original and noised embeddings while using cross-entropy loss
- Core assumption: The model can learn to be robust to embedding perturbations while maintaining generation quality
- Evidence anchors: [abstract] "we design a training task that maximizes the cosine similarity between these vectors"

## Foundational Learning

- **Concept**: Multi-party computation (MPC) for privacy-preserving inference
  - Why needed here: MERGE is designed to work with MPC-based frameworks, so understanding how MPC encrypts and shares model weights and data is essential
  - Quick check question: What are the main MPC techniques (garbled circuits, FHE, HSS) and how do they differ in terms of computation and communication overhead?

- **Concept**: Auto-regressive text generation and Transformer architecture
  - Why needed here: MERGE specifically targets the bottlenecks in auto-regressive generation, so understanding the embedding table query, attention mechanism, and layer normalization is crucial
  - Quick check question: How does the sequence of operations (embedding, representation learning, sampling) differ between NLU and NLG tasks in Transformer models?

- **Concept**: Knowledge distillation and approximation techniques
  - Why needed here: MERGE uses a weighted distillation training task to train the approximated model, so understanding how to transfer knowledge from a larger model to a smaller, faster one is important
  - Quick check question: What are the key differences between layer-wise distillation and end-to-end distillation, and when would each be preferred?

## Architecture Onboarding

- **Component map**: Input → Embedding resending → Merge module → Linear head → Sampling → Output
- **Critical path**: Input → Embedding resending → Merge module → Linear head → Sampling → Output
- **Design tradeoffs**: Speed vs. accuracy: Merging operations and using constant attention may reduce model expressiveness; Privacy vs. performance: Working within MPC constraints limits optimization options; Complexity vs. compatibility: MERGE is designed to be compatible with existing MPC frameworks and PLMs
- **Failure signatures**: Significant drop in generation quality (e.g., lower BERTscore, BARTscore); Slowdown in inference time (e.g., higher than expected linear computation time); Communication overhead not reduced as expected (e.g., high embedding table communication bytes)
- **First 3 experiments**: 1. Verify embedding resending speedup: Compare inference time with and without ER on GPT-2, measuring embedding table query time reduction; 2. Test merge module approximation: Evaluate model performance (e.g., perplexity) with constant attention matrices vs. original attention; 3. Assess embedding augmentation robustness: Train model with varying noise levels and measure impact on generation quality and training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MERGE compare to other privacy-preserving text generation frameworks on larger and more complex language models beyond GPT-2, T5, and BART?
- Basis in paper: [inferred] The paper mentions that MERGE is compatible with mainstream PLMs but only evaluates it on GPT-2, T5-small, and BART-base. The authors plan to explore this in future work.
- Why unresolved: The paper does not provide empirical evidence on how MERGE performs with larger models like GPT-3, GPT-4, or other state-of-the-art models. The scalability of MERGE to larger models remains untested.
- What evidence would resolve it: Running experiments with larger models and comparing the performance of MERGE to other frameworks would provide insights into its scalability and effectiveness.

### Open Question 2
- Question: What are the potential security vulnerabilities introduced by the embedding resending strategy in MERGE, and how can they be mitigated?
- Basis in paper: [explicit] The paper discusses embedding resending as a strategy to bypass embedding table queries but does not address potential security risks associated with reusing output hidden states as input embeddings.
- Why unresolved: The security implications of embedding resending are not explored, and there is no discussion on how to safeguard against potential vulnerabilities that might arise from this approach.
- What evidence would resolve it: Conducting a thorough security analysis of the embedding resending strategy and proposing mitigation techniques would help understand and address any security concerns.

### Open Question 3
- Question: How does the performance of MERGE vary with different types of text generation tasks, such as code generation or summarization, beyond the tasks evaluated in the paper?
- Basis in paper: [inferred] The paper evaluates MERGE on tasks like Multiwoz, CommonGen, and DailyDialog but does not explore other types of text generation tasks. The authors mention plans to explore this in future work.
- Why unresolved: The paper does not provide evidence on how MERGE performs with different types of text generation tasks, such as code generation or summarization, which may have different requirements and challenges.
- What evidence would resolve it: Conducting experiments on a variety of text generation tasks and comparing the performance of MERGE to other frameworks would provide insights into its versatility and effectiveness across different domains.

## Limitations

- The comparison with state-of-the-art approximated models lacks detailed methodology and clear baseline specifications
- The evaluation scope is limited to three datasets, potentially limiting generalization to other text generation scenarios
- The embedding resending mechanism assumes perfect alignment between hidden states and token embeddings without comprehensive validation across different architectures

## Confidence

- **High Confidence**: Core architectural claims about embedding resending (ER) and merge module (MM) design; 26.5x speedup claim under specific conditions
- **Medium Confidence**: Generation quality maintenance claims; Communication cost reduction claims
- **Low Confidence**: Comparison with state-of-the-art approximated models; Generalization to different model sizes and architectures

## Next Checks

1. **Cross-architecture validation**: Implement MERGE on multiple Transformer architectures (e.g., BERT, T5, OPT) and evaluate whether the 26.5x speedup and quality maintenance claims hold across different model sizes and tasks.

2. **Extended quality analysis**: Conduct a comprehensive quality evaluation including human evaluation studies and downstream task performance (e.g., text classification, question answering) to validate that the generation quality claims hold beyond the three benchmark datasets used.

3. **Real-world MPC integration**: Implement MERGE within an actual MPC framework (e.g., CrypTen, MP-SPDZ) and measure the practical communication and computation costs, including setup and preprocessing overhead, to validate the idealized communication cost reduction claims.