---
ver: rpa2
title: Compositional Chain-of-Thought Prompting for Large Multimodal Models
arxiv_id: '2311.17076'
source_url: https://arxiv.org/abs/2311.17076
tags:
- visual
- reasoning
- image
- scene
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of compositional visual reasoning
  in Large Multimodal Models (LMMs), which often struggle to understand relationships
  between objects and attributes. The authors propose Compositional Chain-of-Thought
  (CCoT), a zero-shot prompting method that uses generated scene graphs to enhance
  LMM performance.
---

# Compositional Chain-of-Thought Prompting for Large Multimodal Models

## Quick Facts
- arXiv ID: 2311.17076
- Source URL: https://arxiv.org/abs/2311.17076
- Reference count: 40
- Key outcome: CCoT achieves 69.9% on SEEDBench vs 68.4% baseline, and 42.0% on Winoground text score vs 36.0% baseline

## Executive Summary
This paper addresses the challenge of compositional visual reasoning in Large Multimodal Models (LMMs), which often struggle to understand relationships between objects and attributes. The authors propose Compositional Chain-of-Thought (CCoT), a zero-shot prompting method that uses generated scene graphs to enhance LMM performance. CCoT involves two steps: generating a scene graph relevant to the image and task, then using this graph as context for response generation. Experiments show CCoT improves LMM performance on compositional benchmarks like Winoground and WHOOPS!, as well as general multimodal benchmarks like SEEDBench and MMBench, without requiring fine-tuning or ground-truth scene graph annotations.

## Method Summary
CCoT is a two-step zero-shot prompting method for improving compositional visual reasoning in LMMs. First, the LMM generates a scene graph capturing objects, relationships, and attributes from the input image. This scene graph is formatted in JSON for easier parsing. Second, the generated scene graph is used as context in a prompt alongside the original image and task prompt to produce the final response. The method avoids fine-tuning and leverages the LMM's existing capabilities to extract compositional knowledge from scene graphs.

## Key Results
- LLaVA-1.5 with CCoT achieves 69.9% on SEEDBench compared to 68.4% baseline
- CCoT achieves 42.0% on Winoground text score compared to 36.0% baseline
- CCoT improves WHOOPS! scores from 50.8% to 55.1%
- CCoT shows consistent improvements across SEEDBench, MMBench, and GSMBench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCoT enhances compositional visual reasoning by providing structured scene graph representations as intermediate reasoning steps
- Mechanism: The LMM generates a scene graph capturing objects, relationships, and attributes, then uses this structured representation to inform response generation, addressing the "bag of objects" limitation
- Core assumption: Scene graphs contain the compositional information necessary for improved reasoning
- Evidence anchors:
  - [abstract] "CCoT... utilizes SG representations in order to extract compositional knowledge from an LMM"
  - [section 3.2] "The scene graph generation prompt Sin instructs the LMM to systematically construct a scene graph with three key properties: the objects, their attributes, and the relationships between them"
  - [corpus] Weak - limited direct evidence in corpus about scene graph generation

### Mechanism 2
- Claim: CCoT avoids catastrophic forgetting by using scene graphs as prompt context rather than fine-tuning
- Mechanism: By incorporating generated scene graphs into the prompt without fine-tuning, the model retains its original capabilities while gaining compositional reasoning benefits
- Core assumption: Prompt-based scene graph integration is sufficient to improve reasoning without degrading original capabilities
- Evidence anchors:
  - [abstract] "CCoT... a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations... without the need for fine-tuning"
  - [section 1] "training on SG data can lead to forgetting on the pretrained objectives as shown in [28]"
  - [corpus] Weak - limited corpus evidence about catastrophic forgetting in multimodal models

### Mechanism 3
- Claim: JSON formatting of scene graphs improves LMM utilization of the structured information
- Mechanism: Standardizing scene graph output format helps the LMM parse and reason with the structured information more effectively
- Core assumption: LMMs can better interpret structured JSON format compared to unstructured text
- Evidence anchors:
  - [section 3.2] "We further condition its format to be in JSON. This standardization in JSON format is intended to facilitate easier interpretation by the LMM"
  - [section 4.6] "We find in Table 4 that enforcing a common, systematic format like JSON is indeed beneficial"
  - [corpus] Moderate - some evidence that structured formats aid model understanding

## Foundational Learning

- Concept: Scene graphs and their role in visual reasoning
  - Why needed here: CCoT relies on scene graphs as the core structured representation for compositional reasoning
  - Quick check question: What are the three key components of a scene graph and how do they contribute to compositional understanding?

- Concept: Chain-of-Thought prompting methodology
  - Why needed here: CCoT builds on CoT methods by adding structured visual representations as reasoning steps
  - Quick check question: How does the two-step process in CCoT differ from standard CoT prompting?

- Concept: Catastrophic forgetting in multimodal models
  - Why needed here: Understanding why fine-tuning on scene graphs is problematic is crucial for appreciating CCoT's approach
  - Quick check question: What is catastrophic forgetting and why does fine-tuning on scene graphs potentially cause it?

## Architecture Onboarding

- Component map: Image + Task Prompt -> Vision Encoder -> Scene Graph Generator -> JSON Formatter -> LMM Response Generator

- Critical path: 1) Input image and task prompt → Vision encoder 2) Vision features + task prompt + scene graph prompt → Scene graph generation 3) Image + generated scene graph + task prompt → Response generation

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: CCoT avoids fine-tuning but may have limitations compared to trained approaches
  - Structured vs. unstructured representations: JSON scene graphs may be more interpretable but require additional parsing
  - Context length: Scene graphs add tokens to the prompt, potentially hitting context limits

- Failure signatures:
  - Poor scene graph generation indicates the LMM cannot extract compositional information
  - Degradation in performance on non-compositional tasks suggests catastrophic forgetting
  - Inability to parse JSON scene graphs indicates format compatibility issues

- First 3 experiments:
  1. Ablation: Remove scene graph generation and compare performance to baseline
  2. Ablation: Use unstructured text descriptions instead of JSON scene graphs
  3. Ablation: Vary scene graph token length to find optimal size for performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the length of the generated scene graph affect the performance of CCoT on different multimodal benchmarks?
- Basis in paper: [explicit] The paper discusses evaluating the effect of scene graph size by comparing accuracy when using SGs of different token lengths (1024, 512, and 128 tokens).
- Why unresolved: The paper mentions that the optimal SG size is 256 tokens but does not provide detailed results or analysis on how different lengths specifically impact performance across various benchmarks.
- What evidence would resolve it: Detailed performance metrics comparing CCoT results using SGs of different token lengths across multiple benchmarks would clarify the impact of SG size on performance.

### Open Question 2
- Question: Can the compositional knowledge extracted by CCoT in high-quality scene graphs be effectively transferred to other models through knowledge distillation?
- Basis in paper: [explicit] The paper mentions an ablation study where scene graphs generated by LLaVA-1.5-CCoT were used to prompt InstructBLIP on SEEDBench-Image splits, resulting in a slight degradation in performance but still better than InstructBLIP-ZS-CoT.
- Why unresolved: The paper does not explore the potential for further optimization or alternative methods to enhance knowledge transfer from CCoT-generated scene graphs to other models.
- What evidence would resolve it: Experiments comparing different knowledge distillation techniques and their impact on the performance of other models using CCoT-generated scene graphs would provide insights into effective transfer methods.

### Open Question 3
- Question: How does the JSON structure requirement in the scene graph generation prompt affect the LMM's ability to utilize the scene graph effectively?
- Basis in paper: [explicit] The paper discusses ablating the JSON format requirement in the SG generation prompt to evaluate its impact on the LMM's usage of the content, finding a 2.0% degradation without JSON.
- Why unresolved: The paper does not explore alternative structured formats or the reasons behind the JSON format's effectiveness in enhancing SG utilization.
- What evidence would resolve it: Comparative studies using different structured formats (e.g., XML, YAML) in the SG generation prompt and their impact on LMM performance would clarify the role of structure in SG utilization.

## Limitations

- Limited analysis of scene graph quality - no metrics measuring how accurately generated scene graphs capture visual content
- Modest performance gains - improvements are relatively small (1.5% on SEEDBench, 6% on Winoground)
- Computational overhead - two-step generation process adds complexity and potential latency
- Narrow benchmark evaluation - results may not generalize to all compositional reasoning tasks

## Confidence

**High Confidence**: CCoT successfully implements a zero-shot approach that uses scene graphs for compositional reasoning without fine-tuning, and the two-step prompting methodology is technically sound. The JSON formatting benefit is also well-supported with experimental evidence.

**Medium Confidence**: CCoT improves compositional reasoning performance on the tested benchmarks, as the improvements are modest and may not generalize to all compositional reasoning tasks. The claim about avoiding catastrophic forgetting is supported by the zero-shot approach but lacks direct experimental validation comparing against fine-tuned alternatives.

**Low Confidence**: The general applicability of CCoT to all compositional reasoning tasks, as the evaluation is limited to specific benchmarks. The assumption that generated scene graphs always contain the compositional information necessary for improved reasoning is not thoroughly validated.

## Next Checks

1. **Scene Graph Quality Analysis**: Conduct a detailed evaluation of the generated scene graphs using metrics like relation F1 score and object detection accuracy to quantify how well they capture visual information.

2. **Cross-Benchmark Generalization**: Test CCoT on additional compositional reasoning datasets not mentioned in the paper, particularly those with different types of compositional challenges.

3. **Context Length Sensitivity**: Systematically vary the number of scene graph tokens included in the prompt and measure performance changes to determine the optimal scene graph size.