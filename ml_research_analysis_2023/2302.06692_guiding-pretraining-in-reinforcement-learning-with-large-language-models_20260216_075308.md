---
ver: rpa2
title: Guiding Pretraining in Reinforcement Learning with Large Language Models
arxiv_id: '2302.06692'
source_url: https://arxiv.org/abs/2302.06692
tags:
- goals
- agent
- ellm
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELLM (Exploring with Large Language Models),
  a method that uses large language models to guide reinforcement learning exploration
  by suggesting context-sensitive, human-meaningful goals. The approach addresses
  the challenge of sparse reward functions by prompting LLMs with current state descriptions
  to generate exploration goals, then rewarding agents for achieving semantically
  similar transitions.
---

# Guiding Pretraining in Reinforcement Learning with Large Language Models

## Quick Facts
- arXiv ID: 2302.06692
- Source URL: https://arxiv.org/abs/2302.06692
- Reference count: 27
- Key outcome: ELLM achieves 6 achievements per episode in pretraining vs 9 for oracle and <3 for baselines, with competitive downstream task performance

## Executive Summary
This paper introduces ELLM (Exploring with Large Language Models), a method that uses large language models to guide reinforcement learning exploration by suggesting context-sensitive, human-meaningful goals. The approach addresses the challenge of sparse reward functions by prompting LLMs with current state descriptions to generate exploration goals, then rewarding agents for achieving semantically similar transitions. In the Crafter environment, ELLM-trained agents unlocked an average of 6 achievements per episode during pretraining (compared to 9 for oracle and less than 3 for novelty-based methods), and matched or exceeded baseline performance on downstream tasks. In the Housekeep robotic environment, ELLM achieved higher rearrangement success rates during pretraining (up to 40% vs 20-25% for baselines) and maintained competitive performance when fine-tuned on downstream tasks. The method demonstrates that LLM priors can effectively bias exploration toward useful behaviors without requiring human intervention or hand-coded reward functions.

## Method Summary
ELLM guides reinforcement learning exploration by using large language models to suggest context-sensitive goals based on current state descriptions. The method generates exploration goals through carefully crafted LLM prompts, then rewards agents for achieving semantically similar state transitions using cosine similarity between sentence embeddings of LLM-generated goals and transition captions. The approach combines visual observations with embedded text captions for richer state representations, and trains policies using intrinsic rewards derived from goal achievement rather than extrinsic task rewards. ELLM operates in an extrinsic-reward-free pretraining phase, then fine-tunes the learned policies on downstream tasks with actual task rewards.

## Key Results
- In Crafter, ELLM agents unlocked an average of 6 achievements per episode during pretraining versus 9 for oracle and less than 3 for novelty-based methods
- In Housekeep, ELLM achieved rearrangement success rates up to 40% during pretraining compared to 20-25% for novelty-based baselines
- ELLM-trained agents maintained competitive performance when fine-tuned on downstream tasks, matching or exceeding baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated goals provide human-meaningful exploration guidance that novelty-based methods lack
- Mechanism: LLMs encode common-sense knowledge from text corpora, enabling them to suggest goals that are both contextually appropriate and plausibly useful for downstream tasks
- Core assumption: Pretrained LLMs capture sufficient real-world common-sense knowledge to guide meaningful exploration
- Evidence anchors:
  - [abstract] "LLM priors can effectively bias exploration toward useful behaviors without requiring human intervention or hand-coded reward functions"
  - [section 3.2] "LLMs are probabilistic models of text trained on large text corpora; their predictions encode rich information about human common-sense knowledge and cultural conventions"
  - [corpus] Weak - only 1 related paper directly addresses LLM-guided exploration; others focus on different aspects of LLM integration
- Break condition: If LLM lacks sufficient common-sense knowledge for the target domain, generated goals will be meaningless or counterproductive

### Mechanism 2
- Claim: Semantic similarity between LLM goals and transition captions provides effective intrinsic rewards
- Mechanism: Using cosine similarity between sentence embeddings (from masked LMs) to measure goal achievement creates a smooth, differentiable reward signal that guides policy learning
- Core assumption: Sentence embeddings from masked LMs capture sufficient semantic similarity to ground LLM-generated goals in actual environment transitions
- Evidence anchors:
  - [section 3.4] "We compute rewards for a given goal g by measuring the semantic similarity between the LLM-generated goal and the description of the agent's transition"
  - [section 4.1] "ELLM learns to unlock about 6 achievements every episode, against 9 for the ground-truth-reward Oracle"
- Break condition: If sentence embeddings poorly capture task-relevant semantics, reward signal will be noisy or misleading

### Mechanism 3
- Claim: Text-observation augmentation improves exploration performance by providing additional semantic context
- Mechanism: Combining pixel observations with embedded text captions gives the agent richer state representations that align better with LLM-generated goals
- Core assumption: Language-augmented observations provide complementary information that improves policy learning compared to visual observations alone
- Evidence anchors:
  - [section 3.5] "agents trained on visual + textual observations...outperform agents trained on visual observations only for all the tested variants"
  - [section 4.1] "we find that agents trained on visual + textual observations...outperform agents trained on visual observations only"
- Break condition: If text captions are inaccurate or irrelevant, they may confuse rather than help the agent

## Foundational Learning

- Concept: Reinforcement learning with sparse rewards and intrinsic motivation
  - Why needed here: ELLM operates in extrinsic-reward-free pretraining phases where agents must learn useful behaviors without task-specific rewards
  - Quick check question: What distinguishes competence-based intrinsic motivation from novelty-based exploration?

- Concept: Large language model prompting and zero-shot generation
  - Why needed here: ELLM relies on carefully crafted prompts to extract contextually appropriate goals from LLMs without fine-tuning
  - Quick check question: How does context-sensitivity in LLM prompts affect the quality of generated exploration goals?

- Concept: Semantic similarity and sentence embeddings
  - Why needed here: ELLM computes goal achievement rewards based on cosine similarity between transition captions and LLM-generated goals
  - Quick check question: Why use cosine similarity of sentence embeddings rather than exact string matching for goal achievement detection?

## Architecture Onboarding

- Component map:
  Environment interface -> State captioner -> LLM goal generator -> Environment interaction -> Transition captioner -> Sentence embedding model -> Reward computation -> RL policy

- Critical path: Observation → State captioner → LLM prompt → Goal generation → Environment interaction → Transition captioner → Similarity computation → Reward → Policy update

- Design tradeoffs:
  - Open-ended vs closed-form LLM prompting (flexibility vs efficiency)
  - Goal-conditioned vs goal-free policy training (explicit guidance vs implicit learning)
  - Perfect vs learned captioners (accuracy vs practicality)

- Failure signatures:
  - Poor exploration performance: LLM generates irrelevant or impossible goals
  - Reward sparsity: Sentence embeddings poorly capture goal achievement
  - Training instability: Caption quality degrades over time or across domains

- First 3 experiments:
  1. Compare ELLM with perfect captioners vs learned captioners in Crafter to measure caption quality impact
  2. Test different LLM prompting strategies (open-ended vs closed-form) in Housekeep to evaluate efficiency vs flexibility
  3. Evaluate goal-conditioned vs goal-free policy variants in both environments to understand guidance benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ELLM's performance scale with increasingly large and complex environments where the space of plausibly useful behaviors grows exponentially?
- Basis in paper: [inferred] The paper notes ELLM helps in environments "offering a wide array of possible behaviors among which very few can said to be plausibly useful" but doesn't explore scaling limits.
- Why unresolved: The experiments focus on two moderately-sized environments (Crafter and Housekeep). The paper acknowledges ELLM may be "less helpful in environments with little room for goal-based exploration" but doesn't systematically investigate the boundary conditions.
- What evidence would resolve it: Experiments showing ELLM's performance degradation curves as environment complexity (state space size, action space cardinality, task difficulty) increases, compared to baselines. Analysis of at what point LLM suggestion quality becomes insufficient to guide exploration effectively.

### Open Question 2
- Question: How robust is ELLM to variations in LLM quality, particularly with smaller models or models trained on different corpora?
- Basis in paper: [explicit] "Suggestion quality improves considerably with model size" and the paper uses Codex and GPT-3 Davinci without exploring performance on smaller models.
- Why unresolved: The paper uses large, powerful LLMs but doesn't investigate whether smaller or differently-trained models could provide sufficient guidance for exploration, which would impact practical deployment costs and feasibility.
- What evidence would resolve it: Systematic comparison of ELLM performance across different LLM sizes (e.g., GPT-2, smaller GPT-3 variants) and training datasets, measuring both suggestion quality and downstream task performance.

### Open Question 3
- Question: Can ELLM be extended to generate and reward for visual or multi-modal goals rather than just language-based goals?
- Basis in paper: [explicit] "As general-purpose generative models become available in domains other than text, ELLM-like approaches might also be used to suggest plausible visual goals, or goals in other state representations."
- Why unresolved: The current implementation is limited to language-based goals and captions, but the paper suggests this is a direction for future work rather than a current capability.
- What evidence would resolve it: Demonstration of ELLM variants that generate visual goals (e.g., image-based objectives) or combine visual and language representations, with experimental validation showing improved exploration in environments where language alone is insufficient.

## Limitations
- The method relies on pretrained LLMs having sufficient common-sense knowledge for the target domain, which may not hold for specialized or novel environments
- Caption quality significantly impacts performance, and learned captioners may introduce errors that degrade the reward signal
- The approach requires careful prompt engineering and may be sensitive to prompt variations, affecting reproducibility

## Confidence

- LLM common-sense knowledge sufficiency: Medium confidence - supported by results but not extensively validated across diverse domains
- Semantic similarity mechanism: Medium confidence - empirical results show effectiveness but limited ablation studies
- Text-observation augmentation: Medium confidence - shown effective in tested environments but not compared to alternatives
- Generalization to new domains: Low confidence - only tested on two specific environments
- Robustness to LLM variations: Low confidence - only uses large, powerful LLMs without exploring sensitivity

## Next Checks

1. Test ELLM in additional domains with different reward structures and complexity levels to assess generalization beyond Crafter and Housekeep environments.

2. Conduct ablation studies comparing ELLM performance with oracle captioners versus learned captioners across multiple domains to quantify the impact of caption quality on exploration success.

3. Evaluate the robustness of ELLM when using different LLM models or prompting strategies to determine sensitivity to these design choices and identify optimal configurations.