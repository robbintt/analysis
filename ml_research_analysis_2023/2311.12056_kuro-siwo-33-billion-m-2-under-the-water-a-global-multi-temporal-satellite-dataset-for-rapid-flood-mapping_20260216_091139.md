---
ver: rpa2
title: 'Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal satellite
  dataset for rapid flood mapping'
arxiv_id: '2311.12056'
source_url: https://arxiv.org/abs/2311.12056
tags:
- flood
- mapping
- events
- kuro
- siwo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kuro Siwo introduces a large, manually annotated SAR dataset for
  rapid flood mapping, addressing the lack of high-quality training data for deep
  learning models. It includes 32 global flood events with over 24,000 time series
  of Sentinel-1 imagery and expert-annotated labels.
---

# Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping

## Quick Facts
- arXiv ID: 2311.12056
- Source URL: https://arxiv.org/abs/2311.12056
- Reference count: 40
- Primary result: Large manually annotated SAR dataset enabling over 85% F1-score for flood and water detection

## Executive Summary
Kuro Siwo addresses the critical gap in high-quality training data for deep learning-based flood mapping by providing a manually annotated dataset of 32 global flood events with over 24,000 time series of Sentinel-1 imagery. The dataset includes expert-annotated labels distinguishing flood water, permanent water, and non-water classes, along with an unlabeled SAR dataset of over 2 million samples for self-supervised pretraining. The BlackBench benchmark suite demonstrates strong performance across semantic segmentation and change detection models, achieving over 85% F1-score for flood and water detection tasks.

## Method Summary
Kuro Siwo combines Sentinel-1 SAR imagery (both GRD and SLC products) with expert annotations to create a comprehensive flood mapping dataset. The dataset includes time series triplets (2 pre-event, 1 post-event) with dual polarization (VV/VH) and optional DEM/slope data, processed into 224x224 tiles and stored in a PostgreSQL database. Models trained on this dataset include semantic segmentation approaches (U-Net, DeepLabv3, UperNet) and change detection methods (FC-EF-Diff, SNUNet-CD, BIT-CD, ADHR-CDNet, TransUNet-CD), with pretraining options including both ImageNet and self-supervised MAE on the large unlabeled SAR collection.

## Key Results
- Kuro Siwo enables semantic segmentation models to achieve over 85% F1-score for flood and water detection
- Self-supervised pretraining on the unlabeled SAR dataset improves downstream performance compared to ImageNet pretraining
- Temporal context from multiple SAR acquisitions significantly improves discrimination between permanent water and flood water
- The dataset covers 32 flood events across 6 continents with 24,969 time series and 74,907 unique SAR samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality expert annotations significantly improve model performance compared to semi-automated or fully automatic labeling.
- Mechanism: Manual photointerpretation by SAR experts removes noise and inconsistencies introduced by threshold-based or CEMS-derived labels, leading to cleaner ground truth masks that better capture the true extent of floods and permanent water bodies.
- Core assumption: The additional time and cost of manual annotation is justified by the measurable improvement in model F1-score and mIoU.
- Evidence anchors:
  - [section] "The resulting dataset contains 24, 969 time series and 74, 907 unique SAR samples... The resulting dataset contains 24, 969 time series and 74, 907 unique SAR samples, stored as 224x224 tiles in a PostgreSQL database along with all necessary metadata"
  - [section] "To overcome this barrier, we undertake the laborious work of curating time-series data from Sentinel-1 SAR images linked to flood events worldwide, and manually annotate them by a group of SAR experts."
  - [section] "The errors in CEMS annotations, especially for the Permanent Waters class, are obvious when carefully examining both polarizations."
- Break condition: If manual annotation quality varies significantly across annotators, or if the scale of the dataset becomes too large for expert review, the performance gain may diminish or reverse.

### Mechanism 2
- Claim: Temporal context from multiple SAR acquisitions improves discrimination between permanent water and flood water.
- Mechanism: By including both pre-event and post-event SAR images, the model can learn the temporal dynamics of water appearance, reducing false positives on static water bodies and improving true positive detection of flood extents.
- Core assumption: Flood events are transient enough that pre-flood imagery provides a clear baseline for change detection, and the temporal gap is short enough to avoid major landscape changes unrelated to flooding.
- Evidence anchors:
  - [abstract] "Kuro Siwo includes a highly processed product optimized for flood mapping based on SAR Ground Range Detected, and a primal SAR Single Look Complex product with minimal preprocessing, designed to promote research on the exploitation of both the phase and amplitude information"
  - [section] "The triplet comprises two pre-event images with varying temporal distances—eliminating rigid constraints for real-world applications—and one post-event image acquired as close as possible to the actual event date."
  - [section] "Recognizing these constraints and the importance of generalizing to unseen events, we offer an extensive, unlabeled collection of satellite frame triplets, adhering to the same principles and preprocessing pipeline as the annotated Kuro Siwo set."
- Break condition: If the temporal resolution is too coarse (e.g., pre-event images are months apart), permanent water changes unrelated to floods may introduce noise; if too fine, computational cost may outweigh benefit.

### Mechanism 3
- Claim: Large-scale unlabeled SAR data enables effective self-supervised pretraining, improving downstream flood segmentation performance.
- Mechanism: The unlabeled dataset provides diverse SAR samples that can be used to learn general representations of SAR imagery (e.g., speckle patterns, texture, polarization signatures) before fine-tuning on the small labeled set, improving generalization to unseen events.
- Core assumption: Self-supervised pretraining on SAR-specific data is more beneficial than pretraining on natural images (e.g., ImageNet) because SAR has unique characteristics (speckle, dual-polarization) not captured in RGB imagery.
- Evidence anchors:
  - [abstract] "To leverage advances in large scale self-supervised pretraining methods for remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR samples."
  - [section] "The final dataset contains more than 2 million SAR images grouped into more than 700,000 time series."
  - [section] "We use a vision transformer (ViT) with 24 layers and 16 attention heads as our encoder for the MAE and train for 100 epochs."
- Break condition: If the unlabeled data distribution is too different from the labeled flood events, or if pretraining does not align with downstream task objectives, performance gains may be minimal or negative.

## Foundational Learning

- Concept: Synthetic Aperture Radar (SAR) fundamentals (speckle noise, dual-polarization, GRD vs SLC)
  - Why needed here: Kuro Siwo relies on Sentinel-1 SAR data; understanding SAR characteristics is essential for preprocessing, interpreting model outputs, and diagnosing failures.
  - Quick check question: What is the difference between Ground Range Detected (GRD) and Single Look Complex (SLC) products, and why does Kuro Siwo provide both?

- Concept: Change detection and temporal analysis in remote sensing
  - Why needed here: The dataset is multi-temporal, and models must distinguish between permanent water and flood water by analyzing changes over time.
  - Quick check question: How does including pre-event SAR imagery help reduce false positives on permanent water bodies during flood segmentation?

- Concept: Self-supervised learning (SSL) and masked autoencoders (MAE)
  - Why needed here: Kuro Siwo includes a large unlabeled SAR dataset to enable SSL pretraining; understanding MAE is key to leveraging this component.
  - Quick check question: In the context of MAE for SAR, what does masking a portion of the input image and reconstructing it teach the model?

## Architecture Onboarding

- Component map: Sentinel-1 SAR triplet (VV/VH pre1, VV/VH pre2, VV/VH post) + DEM + slope (optional) -> Preprocessing (SNAP pipeline) -> Model (semantic segmentation or change detection) -> Evaluation (F1-score, mIoU) -> Binary or multi-class flood segmentation mask

- Critical path:
  1. Load triplet + metadata from PostgreSQL
  2. Apply preprocessing (if not already done)
  3. Feed to model (temporal resolution + elevation setting)
  4. Compute loss and update weights
  5. Evaluate on unseen test events

- Design tradeoffs:
  - Temporal resolution: 2 vs 3 images -> more context vs higher compute
  - Elevation: DEM/slope vs none -> better terrain context vs larger model input
  - Backbone: ConvNet vs Transformer -> inductive bias vs scalability
  - Pretraining: ImageNet vs self-supervised SAR -> general vs domain-specific features

- Failure signatures:
  - Low F1 for permanent water vs flood -> model struggles with dynamic water boundaries
  - High mIoU but poor per-class scores -> class imbalance or annotation errors
  - Speckle noise artifacts in predictions -> insufficient denoising or speckle-aware training

- First 3 experiments:
  1. Train U-Net-ResNet18 with 3 pre-event images + DEM on Kuro Siwo; evaluate per-class F1 and mIoU.
  2. Replace backbone with self-supervised MAE-pretrained ViT; compare to ImageNet-pretrained baseline.
  3. Switch to change detection model (e.g., SNUNet-CD) with 2 pre-event images; assess improvement in flood/water discrimination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different preprocessing pipelines for SAR data affect flood detection performance, particularly regarding speckle filtering and terrain correction methods?
- Basis in paper: [explicit] The paper mentions using a standard Sentinel-1 GRD preprocessing pipeline but acknowledges that the dataset can be used with minimal preprocessing.
- Why unresolved: The paper only uses one preprocessing pipeline but provides raw SAR data, leaving questions about optimal preprocessing approaches unanswered.
- What evidence would resolve it: Systematic comparison of flood detection performance using different preprocessing pipelines (varying speckle filters, terrain correction methods, etc.) on the same dataset.

### Open Question 2
- Question: What is the optimal temporal resolution for flood mapping - how many pre-event images are needed for best performance versus operational feasibility?
- Basis in paper: [explicit] The paper tests models with 2 vs 3 images (varying pre-event captions) and notes the importance of temporal aspect but doesn't explore wider temporal resolutions.
- Why unresolved: The paper shows 3 images often performs better than 2, but doesn't explore more extreme cases (single post-event image, longer time series) or operational constraints.
- What evidence would resolve it: Extensive experiments varying the number of pre-event images and time intervals between acquisitions, measuring performance trade-offs against operational requirements.

### Open Question 3
- Question: How well do current models generalize across different climate zones and geographic regions beyond the test set used in this study?
- Basis in paper: [explicit] The paper tests on 6 continents and 3 climate zones but notes that Antarctica and polar/cold climate zones are omitted, and discusses the need for better generalization assessment.
- Why unresolved: The test set, while geographically diverse, doesn't cover all climate zones and may not capture all environmental variability.
- What evidence would resolve it: Testing models on flood events from climate zones not represented in the current dataset (Antarctica, polar regions) and on events with significantly different environmental characteristics (urban vs rural, different vegetation types, etc.).

## Limitations
- The dataset's manual annotation process may introduce inter-annotator variability that isn't quantified
- Reliance on Sentinel-1 data limits the dataset to C-band SAR characteristics, potentially affecting generalization to other radar frequencies or sensors
- The 32 flood events, while globally diverse, may not capture all environmental variability needed for complete generalization

## Confidence
- High confidence: The dataset's scale (74,907 unique SAR samples) and expert annotation process are well-documented and reproducible.
- Medium confidence: Performance claims (85% F1-score) are supported by benchmark results but depend on specific model architectures and evaluation protocols that may not transfer directly to operational settings.
- Low confidence: The transferability of self-supervised pretraining benefits from SAR-specific unlabeled data versus general remote sensing datasets hasn't been thoroughly validated across diverse flood scenarios.

## Next Checks
1. Conduct inter-annotator agreement analysis on a subset of the dataset to quantify annotation consistency and identify potential systematic biases.
2. Test model performance across different radar frequencies by applying Kuro Siwo-trained models to X-band SAR data from alternative sensors.
3. Evaluate the generalization capability by applying pretrained models to flood events from different geographic regions not represented in the original 32 events.