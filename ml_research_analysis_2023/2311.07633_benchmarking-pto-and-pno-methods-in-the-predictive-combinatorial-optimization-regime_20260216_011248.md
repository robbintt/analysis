---
ver: rpa2
title: Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization
  Regime
arxiv_id: '2311.07633'
source_url: https://arxiv.org/abs/2311.07633
tags:
- optimization
- problem
- training
- problems
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a comprehensive benchmark of 11 existing Predict-then-Optimize
  (PtO) and Predict-and-Optimize (PnO) methods across 8 combinatorial optimization
  problems, including a new industrial dataset for combinatorial advertising. The
  benchmark evaluates these methods on their ability to integrate prediction and decision-making
  in various real-world applications such as energy cost-aware scheduling and budget
  allocation.
---

# Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization Regime

## Quick Facts
- arXiv ID: 2311.07633
- Source URL: https://arxiv.org/abs/2311.07633
- Reference count: 40
- One-line primary result: PnO approaches outperform PtO on 7 out of 8 benchmarks across 8 combinatorial optimization problems

## Executive Summary
This comprehensive benchmark evaluates 11 Predict-then-Optimize (PtO) and Predict-and-Optimize (PnO) methods across 8 combinatorial optimization problems, including a novel industrial dataset for combinatorial advertising. The study systematically compares different end-to-end training approaches that integrate prediction and decision-making, revealing that PnO methods consistently outperform traditional two-stage PtO approaches on most benchmarks. While no single method dominates across all problems, the results demonstrate that end-to-end training can significantly improve decision quality by directly optimizing the ultimate objective rather than intermediate prediction accuracy.

## Method Summary
The benchmark employs a modular framework that tests 11 PTO methods across 8 combinatorial optimization problems using consistent evaluation metrics. Methods are categorized into four classes: continuous (gradient-based through KKT conditions), discrete (gradient estimation for non-differentiable objectives), statistical (ranking-based loss functions), and surrogate (differentiable approximations). The framework uses MLP predictors, various solvers (Gurobi, CVXPY, OR-Tools), and evaluates performance using regret metrics. Experiments cover diverse problem types including knapsack, energy scheduling, budget allocation, bipartite matching, and portfolio optimization, with both synthetic and real-world datasets.

## Key Results
- PnO approaches outperform PtO on 7 out of 8 benchmark problems
- Surrogate method LODL achieves consistent strong performance across most benchmarks
- No single design choice (continuous, discrete, statistical, or surrogate) is universally superior across all problem types
- Constraint satisfaction emerges as a critical factor limiting end-to-end training performance, particularly for discrete-class methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end training directly optimizes the decision quality rather than intermediate prediction accuracy, leading to better optimization outcomes.
- Mechanism: By bypassing the prediction loss and backpropagating through the optimization solver, the model learns coefficients that maximize the true objective function value under the predicted parameters.
- Core assumption: The gradients of the decision quality with respect to the predicted coefficients are informative enough to guide learning despite the non-differentiability of discrete optimization.
- Evidence anchors:
  - [abstract] "PnO approaches outperform PtO on 7 out of 8 benchmarks"
  - [section 3] "design an end-to-end training paradigm oriented towards the ultimate decision objective"
  - [corpus] weak evidence for gradient quality in discrete settings
- Break condition: If the optimization problem is highly constrained or has large decision variable sizes, the gradient approximations may become too noisy or uninformative to be useful.

### Mechanism 2
- Claim: Statistical methods leverage relationships between multiple solutions to improve predictions without requiring exact gradients.
- Mechanism: By designing loss functions that compare the decision quality of optimal solutions against suboptimal ones, the model learns to predict coefficients that favor better solutions.
- Core assumption: The relative ranking of solutions based on their decision quality is more stable and learnable than the absolute gradients of individual solutions.
- Evidence anchors:
  - [section 3.3] "designs loss functions for end-to-end training considering the statistical relationships between multiple instances or multiple solutions"
  - [section 4.6] "statistical-class models like NCE and LTR" performed differently under generalization
  - [corpus] limited evidence on scalability of statistical methods
- Break condition: When the solution space is too large or the optimization problem changes significantly between training and test sets, the learned statistical relationships may not generalize.

### Mechanism 3
- Claim: Surrogate models replace the original optimization objective with a differentiable approximation, enabling end-to-end training.
- Mechanism: A learned surrogate function approximates the original objective, and because it's differentiable, standard backpropagation can be used to update the prediction model.
- Core assumption: The surrogate function can accurately represent the original objective while being smooth and differentiable enough for gradient-based learning.
- Evidence anchors:
  - [section 3.4] "Another branch of the method, which bypasses direct gradient estimation is the 'surrogate' class"
  - [section 4.4] "surrogate method LODL achieves satisfactory results on most benchmarks"
  - [corpus] weak evidence on surrogate accuracy for complex objectives
- Break condition: If the surrogate function cannot capture the complexity of the original objective, especially for non-linear or combinatorial problems, the learned predictions will be suboptimal.

## Foundational Learning

- Concept: Combinatorial optimization and its challenges
  - Why needed here: The paper deals with predicting parameters for combinatorial optimization problems where exact solutions are NP-hard
  - Quick check question: What makes combinatorial optimization problems different from continuous optimization in terms of solution space and computational complexity?

- Concept: Differentiable optimization layers
  - Why needed here: Many PTO methods rely on making optimization solvers differentiable through KKT conditions or relaxations
  - Quick check question: How does the differentiability of optimization layers enable gradient-based learning in the PTO framework?

- Concept: Statistical learning-to-rank methods
  - Why needed here: Some PTO approaches use ranking-based loss functions to compare multiple solutions
  - Quick check question: What is the relationship between learning-to-rank methods and the statistical approaches used in PTO?

## Architecture Onboarding

- Component map: Feature → Predictor (MLP) → Predicted Coefficients → Optimizer (solver) → Solution → Loss (PTO-specific) → Backpropagation
- Critical path: Feature → Predictor → Predicted Coefficients → Optimizer → Solution → Loss → Backpropagation
- Design tradeoffs: Accuracy vs. training time (surrogate methods are faster but may be less accurate), generalization vs. specialization (some methods work better on specific problem types)
- Failure signatures: High regret values, unstable training curves, poor generalization to new problem instances
- First 3 experiments:
  1. Run two-stage method on knapsack problem to establish baseline performance
  2. Implement and compare one discrete-class method (Blackbox) against two-stage on same problem
  3. Test generalization by training on smaller problem instances and evaluating on larger ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between constraint tightness and end-to-end training performance degradation?
- Basis in paper: [explicit] The paper states "as the constraints became more restrictive, the relative regrets became higher" and mentions constraint satisfaction as a prevalent factor constraining end-to-end training performance.
- Why unresolved: The paper only provides empirical evidence through experiments on knapsack problems with varying capacities, but doesn't establish a theoretical relationship or quantify the threshold at which constraints significantly impact performance.
- What evidence would resolve it: A systematic study varying constraint tightness across multiple problem types while measuring performance metrics, potentially leading to a theoretical model explaining the relationship.

### Open Question 2
- Question: How do different optimization problem characteristics (e.g., linearity, convexity, discrete vs continuous) interact with specific end-to-end training approaches?
- Basis in paper: [explicit] The paper shows that discrete-class methods perform poorly on non-linear optimization tasks and that continuous-class methods work better for convex objectives, but doesn't provide a comprehensive framework for understanding these interactions.
- Why unresolved: The experiments show performance differences but don't explain why certain methods work better for certain problem characteristics or provide guidelines for method selection.
- What evidence would resolve it: A theoretical analysis of how gradient flow and optimization landscape characteristics interact with different end-to-end training approaches, combined with empirical validation across diverse problem types.

### Open Question 3
- Question: What is the optimal balance between prediction accuracy and decision quality in end-to-end training?
- Basis in paper: [explicit] Figure 8 shows that end-to-end training approaches achieve lower regret than two-stage methods despite higher prediction loss, suggesting a trade-off between prediction accuracy and decision quality.
- Why unresolved: The paper demonstrates this trade-off exists but doesn't provide a framework for determining when to prioritize prediction accuracy versus decision quality or how to quantify the optimal balance.
- What evidence would resolve it: Experiments varying the weight between prediction loss and decision loss during training across multiple problems, leading to guidelines for optimal trade-off strategies.

## Limitations
- The benchmark primarily covers structured, small-to-medium scale combinatorial problems, with limited evaluation of very large-scale instances
- The industrial advertising dataset uses compressed feature representations that may not reflect real-world data complexity
- Statistical PTO methods show inconsistent generalization across problem types, suggesting architecture-specific performance patterns

## Confidence
- Relative performance rankings within each problem class: High
- Absolute performance metrics on problems with tight constraints or large decision spaces: Medium
- Surrogate method effectiveness on non-linear objectives: Medium

## Next Checks
1. Test gradient-based methods on larger-scale combinatorial problems (10K+ variables) to assess scalability limits
2. Evaluate the same methods on problems with non-linear objectives to determine surrogate method limitations
3. Compare performance when using compressed vs. uncompressed feature representations on the advertising dataset