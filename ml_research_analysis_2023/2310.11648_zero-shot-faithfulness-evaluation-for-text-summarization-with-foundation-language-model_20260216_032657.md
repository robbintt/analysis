---
ver: rpa2
title: Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language
  Model
arxiv_id: '2310.11648'
source_url: https://arxiv.org/abs/2310.11648
tags:
- faithfulness
- fflm
- language
- evaluation
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new metric called FFLM for zero-shot faithfulness
  evaluation of text summarization using a foundation language model. The key idea
  is to measure probability changes when prefixing consistent text to the summary.
---

# Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model

## Quick Facts
- arXiv ID: 2310.11648
- Source URL: https://arxiv.org/abs/2310.11648
- Reference count: 26
- Key outcome: FFLM achieves competitive or better performance than ChatGPT for zero-shot faithfulness evaluation of text summarization using only 7 billion parameters

## Executive Summary
This paper introduces FFLM, a novel zero-shot metric for evaluating the faithfulness of text summarization outputs using foundation language models. The key innovation is measuring probability changes when prefixing consistent text to summaries, combining three probability change components with token-level weights and logarithms. Experiments demonstrate that FFLM based on LLaMa-7B achieves performance comparable to or better than ChatGPT (175B parameters) on both inconsistency detection and faithfulness rating tasks across multiple datasets. The approach provides an effective and efficient alternative to instruction-tuned large models for faithfulness evaluation.

## Method Summary
FFLM evaluates faithfulness by computing probability changes in a foundation language model when consistent text is prefixed to the summary. The method calculates three components: changes with prior probability (Δprior_Y and Δprior_X) and changes with conditional probability (Δcond_Y). These components are combined using token-level weights and logarithmic transformations to emphasize low-probability tokens that may indicate unfaithful content. The approach uses a 7-billion parameter LLaMa model and determines optimal combination weights through validation set tuning, enabling zero-shot evaluation without human-annotated training data.

## Key Results
- FFLM with LLaMa-7B outperforms or matches ChatGPT (175B) on inconsistency detection with balanced accuracy
- On faithfulness rating tasks, FFLM achieves higher correlations (Pearson, Spearman, Kendall) than strong baselines including BARTScore, CoP, and HaRiM
- Performance increases consistently from LLaMa-3B to LLaMa-7B across five datasets, though improvements from 7B to 13B are inconsistent
- Ablation studies validate the importance of metric design choices including token weighting and combination weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probability changes reflect faithfulness of summaries
- Mechanism: When a summary is consistent with the document, prefixing it increases the generation probability compared to the unconditioned case
- Core assumption: Foundation language models encode document-summary relationships such that consistent information boosts prediction probabilities
- Evidence anchors:
  - [abstract] "the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output"
  - [section] "If Y is a faithful summary, the sequence-to-sequence probability ps2s Y should be larger than the prior probability plm Y as more information consistent to Y is given by conditioning on X"
  - [corpus] Weak - no direct citations about probability changes reflecting faithfulness
- Break condition: If the foundation model lacks document-summary coherence understanding or if probabilities are dominated by other factors like style or length

### Mechanism 2
- Claim: Low-probability tokens indicate unfaithful content
- Mechanism: High-loss tokens (low probability) generally correspond to unfaithful contents; FFLM emphasizes these through log transformation and token-level weighting
- Core assumption: Foundation language models assign lower probabilities to tokens representing factual errors or hallucinations
- Evidence anchors:
  - [abstract] "we think that more attention should be paid to such high-loss (or low-probability) tokens when calculating the faithfulness scores"
  - [section] "Goyal et al. (2022) found that high-loss tokens generally correspond to unfaithful contents during training a summarization model"
  - [corpus] Moderate - Goyal et al. is cited but not directly about foundation models
- Break condition: If low-probability tokens are caused by style differences rather than factual errors, or if the model's probability distribution doesn't align with faithfulness

### Mechanism 3
- Claim: Combining multiple probability change types captures different error aspects
- Mechanism: Three components (prior changes and conditional changes) each capture different aspects of faithfulness; their combination provides comprehensive evaluation
- Core assumption: Different probability change calculations capture different types of hallucinations (semantic frame, discourse, content verifiability)
- Evidence anchors:
  - [abstract] "we hypothesize that these different probability changes capture different hallucinations"
  - [section] "By taking a look at the correlations between pairs of the metric components... indicating that these components may capture unfaithfulness from different aspects"
  - [corpus] Moderate - correlation analysis shown but error type mapping is based on limited samples
- Break condition: If the probability change components are highly correlated or if they don't capture the error types relevant to the evaluation task

## Foundational Learning

- Concept: Foundation language models and probability calculations
  - Why needed here: FFLM relies on foundation language models to compute generation probabilities under different conditions
  - Quick check question: Can you explain the difference between sequence-to-sequence probability and prior probability in language models?

- Concept: Faithfulness evaluation metrics and correlation coefficients
  - Why needed here: Understanding how FFLM's scores relate to human judgments through correlation metrics is crucial for evaluation
  - Quick check question: What's the difference between Pearson, Spearman, and Kendall correlations, and when would you use each?

- Concept: Zero-shot vs weakly-supervised evaluation approaches
  - Why needed here: FFLM is a zero-shot metric; understanding the distinction from other approaches helps contextualize its advantages
  - Quick check question: What are the main trade-offs between zero-shot and weakly-supervised faithfulness evaluation methods?

## Architecture Onboarding

- Component map: FFLM consists of three probability change components (Δprior_Y, Δprior_X, Δcond_Y), token-level weighting, log transformation, and combination weights (α, β, δ)
- Critical path: Document and summary input → compute three probability changes → apply token weighting and log transform → combine with weights → output faithfulness score
- Design tradeoffs: Foundation model size vs performance (7B vs 13B), zero-shot vs instruction-tuned approaches, token weighting vs uniform averaging
- Failure signatures: Poor correlation with human judgments, high sensitivity to prompt variations, inconsistent performance across datasets
- First 3 experiments:
  1. Test FFLM on a small dataset with known faithful and unfaithful summaries to verify basic functionality
  2. Compare FFLM scores with human judgments on a held-out validation set to check correlation
  3. Perform ablation study by removing token weighting and log transform to assess their impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FFLM vary with different model sizes of the foundation language model?
- Basis in paper: [explicit] The paper mentions that the performance of FFLM increases consistently from LLaMa-3B to LLaMa-7B across five datasets, while the improvements are not consistent for LLaMa-13B.
- Why unresolved: The paper does not provide a detailed analysis of why the performance improvement is not consistent when moving from LLaMa-7B to LLaMa-13B. It only mentions that larger models may memorize more biases in the pre-training corpus, which may hurt performance.
- What evidence would resolve it: A detailed study analyzing the performance of FFLM on different model sizes, focusing on the impact of model size on the faithfulness evaluation and the role of biases in the pre-training corpus.

### Open Question 2
- Question: Can FFLM be adapted for other natural language generation tasks, such as dialogue generation or sentence paraphrasing?
- Basis in paper: [explicit] The paper mentions that the definition of faithfulness evaluation for other generation tasks, such as dialogue generation and sentence paraphrasing, has some non-trivial differences compared to text summarization.
- Why unresolved: The paper does not explore the application of FFLM to other natural language generation tasks. It only mentions that FFLM can be transferred with adjustments to other tasks as future work.
- What evidence would resolve it: Experiments applying FFLM to other natural language generation tasks, such as dialogue generation and sentence paraphrasing, and analyzing the effectiveness of FFLM in these tasks.

### Open Question 3
- Question: How does FFLM perform in terms of pinpointing the exact erroneous words or specific error types in the generated summaries?
- Basis in paper: [inferred] The paper mentions that FFLM calculates a single score for the whole summary without pinpointing the exact erroneous words or the specific error type. It suggests that FFLM can be used for token-level inconsistency detection by adjusting the aggregation weights or combining it with the prompting approach.
- Why unresolved: The paper does not provide experimental results on the effectiveness of FFLM in pinpointing the exact erroneous words or specific error types in the generated summaries.
- What evidence would resolve it: Experiments evaluating the effectiveness of FFLM in identifying the exact erroneous words or specific error types in the generated summaries, and comparing its performance with other methods for token-level inconsistency detection.

## Limitations

- The exact prompt format used to query LLaMa-7B for computing sequence-to-sequence and conditional probabilities is not specified, affecting reproducibility
- The paper doesn't thoroughly investigate edge cases or failure modes across diverse summarization domains
- Poor performance on datasets with single annotator labels due to label reliability issues

## Confidence

*High confidence* in the experimental methodology and comparative results - The paper clearly reports performance metrics across multiple datasets and baselines, with transparent comparisons.

*Medium confidence* in the theoretical mechanism - The probability change framework is logical, but the direct causal link between probability changes and faithfulness is assumed rather than proven.

*Low confidence* in the zero-shot generalization - While FFLM shows good performance, the paper doesn't thoroughly investigate edge cases or failure modes across diverse summarization domains.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary the prompt format used to query LLaMa-7B and measure how FFLM scores change. This would reveal the robustness of the approach to prompt engineering choices.

2. **Cross-domain generalization test**: Evaluate FFLM on summarization datasets from domains very different from those used in training (e.g., scientific papers, legal documents, or code documentation) to assess true zero-shot capability.

3. **Error type attribution study**: Manually annotate a sample of summaries where FFLM and human judgments disagree, categorizing the types of errors (factual hallucinations, semantic distortions, omissions) to validate whether probability changes actually capture the intended error types.