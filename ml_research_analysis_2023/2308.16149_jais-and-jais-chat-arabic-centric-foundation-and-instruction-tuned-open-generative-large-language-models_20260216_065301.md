---
ver: rpa2
title: 'Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative
  Large Language Models'
arxiv_id: '2308.16149'
source_url: https://arxiv.org/abs/2308.16149
tags:
- arabic
- english
- language
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Jais and Jais-chat models are Arabic-centric, large language
  models with 13 billion parameters that demonstrate superior performance in Arabic
  compared to existing open-source Arabic and multilingual models. The models were
  pretrained on a mixture of Arabic, English, and code data, with Arabic comprising
  33% of the pretraining dataset.
---

# Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models

## Quick Facts
- arXiv ID: 2308.16149
- Source URL: https://arxiv.org/abs/2308.16149
- Reference count: 40
- Key outcome: Arabic-centric 13B parameter models outperform other Arabic models on Arabic tasks while remaining competitive on English

## Executive Summary
Jais and Jais-chat are 13-billion parameter Arabic-centric foundation and instruction-tuned language models. Pretrained on a mixture of Arabic (33%), English, and code data totaling 395 billion tokens, these models demonstrate superior performance on Arabic language tasks compared to existing open-source Arabic and multilingual models. The instruction-tuned Jais-chat variant further improves performance through fine-tuning on 10 million bilingual instruction-response pairs, achieving strong results across both Arabic and English benchmarks while being trained on less English data than typical English-centric models.

## Method Summary
The models use a GPT-3 decoder-only architecture with ALiBi positional encodings, SwiGLU activations, and maximal update parametrization. Training employed AdamW optimizer with weight decay and max norm gradient clipping. The pretraining dataset comprised 395 billion tokens including 116 billion Arabic tokens (upsampled from 72 billion), 232 billion English tokens, and 46 billion code tokens. Instruction-tuning was performed using 4 million Arabic and 6 million English instruction-response pairs, followed by safety fine-tuning to reject harmful content.

## Key Results
- Jais outperforms existing open Arabic and multilingual models on Arabic MMLU tasks by a large margin
- Jais-chat achieves superior Arabic performance while remaining competitive with English-centric models on English tasks
- Models were trained efficiently using Cerebras Systems hardware, completing training in days rather than months

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Arabic performance is boosted by upsampling Arabic data 1.6× to compensate for data scarcity
- **Mechanism**: Increased Arabic token exposure per parameter ratio satisfies Chinchilla scaling laws without degrading quality
- **Core assumption**: Data scarcity is the primary bottleneck; upsampling outweighs overfitting risks
- **Evidence anchors**: 116B Arabic tokens from upsampling 72B original; 1:2 Arabic:English ratio improves performance
- **Break condition**: Excessive upsampling causes overfitting or domain shift

### Mechanism 2
- **Claim**: Instruction-tuning on 4M Arabic + 6M English pairs enables zero-shot generalization
- **Mechanism**: Bilingual instruction data aligns model to follow natural language prompts in both languages
- **Core assumption**: Instruction-following transfers across languages with sufficient bilingual data
- **Evidence anchors**: Jais-chat outperforms other models on Arabic tasks; uses 10M total instruction pairs
- **Break condition**: Low-quality instruction data or domain mismatch limits gains

### Mechanism 3
- **Claim**: ALiBi positional encodings enable context extrapolation beyond training length
- **Mechanism**: Linearly decreasing attention penalties allow generalization to longer contexts
- **Core assumption**: Linear penalty maintains positional awareness at longer inference lengths
- **Evidence anchors**: ALiBi supports efficient extrapolation; used in standard transformer architecture
- **Break condition**: Inference context exceeds effective extrapolation limit

## Foundational Learning

- **Concept**: Transformer decoder-only architecture (GPT-3 variant)
  - Why needed here: Enables autoregressive text generation in both Arabic and English
  - Quick check question: What distinguishes decoder-only from encoder-decoder models?

- **Concept**: Byte-Pair Encoding (BPE) tokenizer trained on balanced Arabic-English corpus
  - Why needed here: Prevents Arabic over-segmentation and ensures fair token distribution
  - Quick check question: How does tokenizer fertility affect cross-lingual transfer?

- **Concept**: Chinchilla scaling laws (tokens per parameter)
  - Why needed here: Guides optimal model size and dataset size selection
  - Quick check question: How many training tokens are needed for a 13B parameter model?

## Architecture Onboarding

- **Component map**: Tokenizer → Transformer blocks (40 layers, 40 heads, 5120 dim) → ALiBi positional encodings → SwiGLU activations → AdamW optimizer → Learning rate schedule
- **Critical path**: Data → Tokenizer → Pretraining → Instruction-tuning → Safety fine-tuning → Evaluation → Release
- **Design tradeoffs**: Larger Arabic share improves fluency but limits English capacity; ALiBi saves memory but may hurt short-sequence accuracy; SwiGLU adds compute but improves performance
- **Failure signatures**: Poor Arabic → tokenizer over-segmentation or insufficient data; poor English → imbalanced mix; context collapse → ALiBi failure; instability → learning rate/batch size mismatch
- **First 3 experiments**:
  1. Test ALiBi extrapolation by running inference on Arabic text at 2× and 3× training context length
  2. Compare fertility scores for Arabic vs English on validation sets
  3. Fine-tune on small bilingual instruction set and measure zero-shot Arabic task accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance compare on code generation tasks given 46B code tokens in pretraining?
- **Basis in paper**: Code tokens mentioned but no code generation evaluation provided
- **Why unresolved**: Paper focuses on language tasks, not code generation
- **What evidence would resolve it**: Include HumanEval or MBPP benchmarks comparing to code-focused models

### Open Question 2
- **Question**: How does performance change when varying the Arabic:English data ratio?
- **Basis in paper**: Uses 1:2 ratio but doesn't explore alternatives
- **Why unresolved**: No ablation study on different data ratios
- **What evidence would resolve it**: Experiments with ratios like 1:1, 2:1, 3:1 and performance evaluation

### Open Question 3
- **Question**: How does performance compare on low-resource Arabic dialects?
- **Basis in paper**: Evaluated on MSA but not specific dialects
- **Why unresolved**: Paper doesn't include dialect-specific evaluation
- **What evidence would resolve it**: Benchmark datasets for dialects and comparison to other models

### Open Question 4
- **Question**: How does instruction-tuning affect ability to follow complex multi-step instructions?
- **Basis in paper**: Uses 10M instruction pairs but lacks complex instruction analysis
- **Why unresolved**: No specific evaluation of complex instruction following
- **What evidence would resolve it**: Benchmark with complex multi-step instructions in both languages

## Limitations

- Arabic data collection and preprocessing pipeline details remain underspecified, making it difficult to assess corpus representativeness
- Evaluation lacks human preference studies and broader generative task benchmarks that would demonstrate real-world utility
- Training efficiency claims lack quantitative comparison of FLOPs, wall-clock time, or energy consumption against GPU-based alternatives

## Confidence

**High Confidence Claims**:
- Model architecture (GPT-3 decoder with ALiBi) is technically sound
- Arabic comprises approximately 33% of pretraining data (116B/395B tokens)
- Jais-chat outperforms other Arabic models on Arabic MMLU tasks
- Tokenizer trained on balanced Arabic-English corpus

**Medium Confidence Claims**:
- 1.6× Arabic upsampling optimally balances data scarcity
- Instruction-tuning improves cross-lingual task performance
- ALiBi positional encodings support context extrapolation

**Low Confidence Claims**:
- Training efficiency gains specific to Cerebras hardware
- Exact impact of 1:2 Arabic:English ratio on performance
- Safety alignment effectiveness without quantitative benchmarks

## Next Checks

1. Conduct fertility score analysis comparing Arabic vs English tokenization to verify no over-segmentation
2. Test ALiBi extrapolation by running inference on Arabic text at 2× and 3× training context length
3. Audit a stratified sample of instruction-tuning pairs for domain balance and instruction complexity