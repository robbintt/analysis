---
ver: rpa2
title: 'ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse
  LLMs'
arxiv_id: '2309.13007'
source_url: https://arxiv.org/abs/2309.13007
tags:
- agents
- discussion
- agent
- answer
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReConcile, a multi-model multi-agent framework
  designed as a round table conference among diverse LLM agents to improve reasoning
  performance. The framework enhances collaborative reasoning through multiple rounds
  of discussion, learning to convince other agents to improve their answers, and employing
  a confidence-weighted voting mechanism.
---

# ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs

## Quick Facts
- arXiv ID: 2309.13007
- Source URL: https://arxiv.org/abs/2309.13007
- Reference count: 30
- Key outcome: Improves LLM reasoning via multi-agent consensus, surpassing single-agent and multi-agent baselines by up to 11.4% and even GPT-4 on three datasets.

## Executive Summary
ReConcile is a multi-agent round table conference framework designed to improve reasoning performance in large language models (LLMs) by leveraging diverse agents in collaborative discussion. Each agent independently generates an answer with confidence, then iteratively refines responses through multi-round discussion that includes grouped answers, confidence scores, and demonstrations of human explanations that corrected previous errors. The framework employs confidence-weighted voting to determine the final answer, enabling collective reasoning that surpasses prior single-agent and multi-agent approaches.

## Method Summary
ReConcile implements a three-phase process: initial response generation where each of n distinct LLM agents (ChatGPT, Bard, Claude2) produces an answer with explanation and confidence; multi-round discussion where agents revise their answers by conditioning on grouped responses from others, confidence scores, and demonstrations of human rectifying explanations; and final answer determination through confidence-weighted voting. The framework flexibly incorporates different combinations of agents including API-based, open-source, and domain-specific models, with experiments conducted on seven benchmarks including StrategyQA, ECQA, GSM8K, and AQuA.

## Key Results
- ReConcile significantly improves LLMs' reasoning performance, surpassing prior single-agent and multi-agent baselines by up to 11.4%
- Outperforms GPT-4 on three datasets, demonstrating the effectiveness of multi-agent consensus
- Achieves 8% improvement on MATH when incorporating different combinations of agents including domain-specific models
- Shows that diversity among different model families is critical to superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-weighted voting corrects for model overconfidence and improves final accuracy.
- Mechanism: The method rescales raw confidence scores to discrete bins (1.0, 0.8, 0.5, 0.3, 0.1) before applying them as weights in the final vote. This adjustment counteracts the tendency of LLMs to produce uniformly high confidence scores, thereby differentiating their reliability.
- Core assumption: The discretized bins are calibrated enough to meaningfully distinguish agent reliability without introducing excessive quantization error.
- Evidence anchors:
  - [abstract] "We employ these confidences to compute a weighted vote as the final answer."
  - [section] "Directly using confidence scores as the voting weights is less effective due to the overconfidence problem of LLMs (Xiong et al., 2023b; Tian et al., 2023; Mielke et al., 2022)."
  - [corpus] Weak - no direct evidence found in related papers.
- Break condition: If agents' confidence scores are poorly calibrated even after rescaling, the weighted vote may systematically overweight unreliable agents.

### Mechanism 2
- Claim: Discussion prompts with grouped responses and human rectifying explanations enable agents to update their answers.
- Mechanism: In each round, agents receive (1) aggregated answers and explanations from other agents, (2) confidence scores, and (3) in-context demonstrations of human explanations that previously corrected an agent's answer. This setup guides the model to produce a revised explanation that could convince other agents, leading to convergence on better answers.
- Core assumption: In-context learning from corrective explanations generalizes to new problems within the same domain.
- Evidence anchors:
  - [abstract] "In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations from the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations."
  - [section] "When an agent tries to reassess its reasoning in light of the reasoning provided by other agents, we hypothesize that it should benefit from conditioning on demonstrations that can convince other agents."
  - [corpus] Weak - related work on debate and in-context learning does not specifically validate the corrective explanation component.
- Break condition: If human rectifying explanations are not representative or fail to generalize, agents may not be convinced and the discussion stalls.

### Mechanism 3
- Claim: Diversity among agents (different model families) improves reasoning over single-model ensembles.
- Mechanism: By using distinct models (ChatGPT, Bard, Claude2) rather than multiple instances of the same model, ReConcile introduces varied knowledge bases and reasoning styles. This diversity yields more diverse initial responses, which enhances the potential for mutual correction during discussion.
- Core assumption: Different model families have complementary strengths that can be leveraged in discussion.
- Evidence anchors:
  - [abstract] "The framework also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models."
  - [section] "We argue that relying on a single model limits the potential of complementary benefits from different model families and the advantage of ensemble learning."
  - [corpus] Weak - related multi-agent debate work primarily uses single-model agents; no direct evidence of performance gains from model diversity.
- Break condition: If the models' knowledge overlaps heavily, diversity gains vanish and the system reverts to a single-model-like behavior.

## Foundational Learning

- Concept: In-context learning via demonstrations
  - Why needed here: The framework relies on providing agents with examples of human explanations that corrected previous errors, so agents learn how to convince others without explicit fine-tuning.
  - Quick check question: What happens if the demonstration examples are irrelevant to the test problem's domain?

- Concept: Confidence calibration
  - Why needed here: LLMs produce overconfident predictions; calibration transforms raw confidences into reliable weights for voting.
  - Quick check question: Why not use a learned recalibration model instead of a hand-coded binning scheme?

- Concept: Multi-round consensus algorithms
  - Why needed here: The discussion proceeds until all agents agree or a round limit is reached; understanding consensus dynamics is essential to tuning rounds and detecting convergence.
  - Quick check question: How does the system decide to stop if agents oscillate between two answers?

## Architecture Onboarding

- Component map:
  Initial Response Generator (per agent) -> Confidence Estimator (per agent) -> Discussion Prompt Builder (shared) -> Each agent (updated answer) -> Weighted Vote Aggregator (shared)

- Critical path:
  1. Generate initial answers + confidences.
  2. Build discussion prompt with grouped responses and samples.
  3. Pass prompt to each agent for updated answer.
  4. Repeat until consensus or round limit.
  5. Apply weighted vote to determine final answer.

- Design tradeoffs:
  - More rounds → higher cost but potentially higher accuracy; fewer rounds → faster but risk of premature consensus.
  - Hand-coded confidence binning is simple but less adaptive than learned recalibration.
  - Using multiple model families increases diversity but introduces API cost and latency variability.

- Failure signatures:
  - All agents converge to a wrong answer (echo chamber).
  - Confidence rescaling collapses distinctions (all agents weight ≈ 0.3).
  - Prompt size grows too large, causing truncation or incomplete context.

- First 3 experiments:
  1. Compare weighted vote vs majority vote on StrategyQA to validate confidence importance.
  2. Run ReConcile with only ChatGPT instances to confirm diversity benefit.
  3. Remove convincing samples to quantify their contribution to accuracy gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ReConcile change when using more than three agents or different combinations of models beyond ChatGPT, Bard, and Claude2?
- Basis in paper: The paper states that ReConcile "flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models" but primarily uses three agents and experiments with GPT-4 as one agent.
- Why unresolved: The paper does not explore a wide range of model combinations or investigate the impact of using more than three agents on performance.
- What evidence would resolve it: Experiments comparing ReConcile's performance with various combinations of agents (including more than three) and different types of models (open-source, domain-specific, etc.) would provide insights into the scalability and generalizability of the framework.

### Open Question 2
- Question: What is the optimal number of discussion rounds for ReConcile to achieve the best balance between accuracy and computational efficiency?
- Basis in paper: The paper mentions that ReConcile terminates discussion after a maximum of R rounds or when consensus is reached, and shows that accuracy saturates after two rounds for some tasks. However, it does not explore the trade-off between accuracy and computational cost in depth.
- Why unresolved: The paper does not provide a systematic analysis of how the number of discussion rounds affects both accuracy and computational efficiency, nor does it propose a method to determine the optimal number of rounds.
- What evidence would resolve it: A comprehensive study analyzing the relationship between the number of discussion rounds, accuracy gains, and computational costs would help determine the optimal stopping criterion for ReConcile.

### Open Question 3
- Question: How does the quality and quantity of convincing samples affect ReConcile's performance, and what are the best strategies for selecting these samples?
- Basis in paper: The paper demonstrates that convincing samples lead to a 4% improvement in accuracy compared to general human explanations and states that "even when the dataset lacks human explanations or one opts not to utilize them, our method can still yield performance improvements independently of this technique."
- Why unresolved: The paper does not investigate the impact of varying the number of convincing samples or explore different strategies for selecting these samples on ReConcile's performance.
- What evidence would resolve it: Experiments comparing ReConcile's performance with different numbers of convincing samples and various selection strategies (e.g., based on difficulty, diversity, or relevance) would provide insights into the role of convincing samples in the framework.

## Limitations
- The framework's performance relies on hand-crafted confidence discretization that lacks formal validation against learned calibration methods.
- The generalizability of convincing samples across different reasoning domains remains untested.
- The stopping criteria for multi-round discussions are not explicitly defined, leaving open the possibility of oscillating agents or premature convergence.

## Confidence

**High Confidence:**
- Framework improves reasoning performance over single-agent baselines
- Confidence-weighted voting outperforms simple majority voting
- Diversity among different model families improves performance

**Medium Confidence:**
- Discussion prompts with grouped responses and convincing samples enable meaningful answer updates
- Mechanism leads to convergence on better answers (supported indirectly through performance)

**Low Confidence:**
- Framework's generalizability to arbitrary agent combinations
- Robustness across different reasoning domains

## Next Checks

1. **Ablation Study on Confidence Binning:** Compare the hand-coded five-bin discretization scheme against learned confidence recalibration methods (e.g., isotonic regression) on the same benchmarks to quantify the impact of calibration choice on final accuracy.

2. **Domain Transfer Analysis:** Apply the framework to a new domain (e.g., commonsense reasoning or code generation) with a fresh set of convincing samples to evaluate the framework's cross-domain robustness and the generalizability of in-context learning from corrective explanations.

3. **Convergence Dynamics Analysis:** Instrument the multi-round discussion process to track answer changes and confidence evolution across rounds, identifying patterns of convergence, oscillation, or divergence to inform optimal stopping criteria and discussion prompt design.