---
ver: rpa2
title: Fairness-aware Vision Transformer via Debiased Self-Attention
arxiv_id: '2301.13803'
source_url: https://arxiv.org/abs/2301.13803
tags:
- features
- sensitive
- adversarial
- fairness
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in Vision Transformer (ViT) models
  by proposing a Debiased Self-Attention (DSA) framework. DSA identifies and masks
  spurious features correlated with sensitive attributes using adversarial attacks,
  then aligns attention weights to encourage learning informative features for target
  prediction.
---

# Fairness-aware Vision Transformer via Debiased Self-Attention

## Quick Facts
- arXiv ID: 2301.13803
- Source URL: https://arxiv.org/abs/2301.13803
- Reference count: 40
- Primary result: DSA framework improves demographic parity, equalized odds, and balanced accuracy on CelebA and Waterbirds datasets while maintaining prediction accuracy

## Executive Summary
This paper addresses fairness in Vision Transformer models by proposing a Debiased Self-Attention (DSA) framework that identifies and masks spurious features correlated with sensitive attributes. DSA uses adversarial attacks to generate perturbed training examples that mask sensitive features, then aligns attention weights to encourage learning informative features for target prediction. The framework outperforms existing fairness-aware methods on benchmark datasets while maintaining competitive prediction accuracy.

## Method Summary
The DSA framework follows a two-step procedure: first, it trains a bias-only model to identify sensitive features by classifying sensitive attributes; second, it generates adversarial examples using Patch-Fool attacks that target these sensitive features, creating a debiased training set. The framework then trains a debiased ViT model using the original and adversarial examples with an attention weights alignment regularizer that encourages the model to focus on target-relevant features rather than sensitive attributes.

## Key Results
- DSA achieves 18.7% and 21.8% improvement in demographic parity on CelebA tasks (Gray Hair/Gender, Wavy Hair/Gender)
- DSA achieves 7.8% and 4.2% improvement in equalized odds on CelebA tasks
- DSA achieves 5.4% and 5.2% improvement in balanced accuracy on Waterbirds dataset
- DSA maintains competitive prediction accuracy while improving fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
Adversarial attacks effectively identify and mask spurious features in ViT models. The framework uses Patch-Fool attacks to generate perturbed images targeting sensitive attributes, creating a debiased training set where sensitive features are masked while preserving target-relevant information.

### Mechanism 2
Attention weights alignment regularizer helps the model learn informative features while maintaining fairness. The framework aligns attention weights between original and adversarial examples, encouraging focus on target-relevant features rather than sensitive attributes.

### Mechanism 3
Combining adversarial attacks with attention alignment creates synergistic fairness improvement. The two-stage approach first removes sensitive features through adversarial attacks, then reinforces learning of informative features through attention alignment, creating a comprehensive debiasing solution.

## Foundational Learning

- Concept: Self-attention mechanism in Vision Transformers
  - Why needed here: Understanding ViT self-attention is crucial for how adversarial attacks target sensitive features and how attention alignment works
  - Quick check question: How does self-attention in ViTs differ from convolutions in CNNs, and why might this make ViTs more vulnerable to certain types of bias?

- Concept: Adversarial attacks and their transferability between models
- Concept: Fairness metrics (demographic parity, equalized odds, balanced accuracy)
  - Why needed here: The framework's effectiveness is evaluated using these specific fairness metrics
  - Quick check question: What is the difference between demographic parity and equalized odds, and when might one be preferred over the other?

- Concept: Bias-only models and their role in identifying sensitive features
  - Why needed here: The framework relies on training a bias-only model to identify features correlated with sensitive attributes
  - Quick check question: How does training a bias-only model help identify spurious features, and what are its limitations?

## Architecture Onboarding

- Component map: Input → Bias-only model → Adversarial attack → Attention alignment → Debiased ViT training
- Critical path: Input → Bias-only model → Adversarial attack → Attention alignment → Debiased ViT training
- Design tradeoffs:
  - Patch size: Smaller patches allow finer-grained feature identification but increase computational cost
  - Number of perturbed patches: More patches may better mask sensitive features but risk removing useful information
  - Regularization strength (λ3): Higher values enforce fairness more strongly but may reduce accuracy
  - Hyperparameter tuning: Multiple weights need careful adjustment for optimal performance

- Failure signatures:
  - High adversarial success rate but poor fairness improvement: Attack may be targeting wrong features
  - Good fairness metrics but low accuracy: Regularization may be too strong or attacking useful features
  - No improvement over vanilla model: Attack may not be effectively identifying sensitive features

- First 3 experiments:
  1. Verify adversarial attack success rate on bias-only model for identifying sensitive features
  2. Test attention alignment regularizer independently to ensure it's learning from original vs. adversarial examples
  3. Validate that the debiased model maintains prediction accuracy while improving fairness metrics on a simple dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does DSA effectiveness compare when applied to other vision models beyond ViT, such as CNNs or hybrid architectures? The paper focuses on ViT and doesn't directly compare DSA's effectiveness on other model types.

### Open Question 2
What is the computational overhead of DSA compared to vanilla training and existing fairness-aware methods? The paper focuses on effectiveness rather than efficiency and doesn't report runtime comparisons.

### Open Question 3
How does DSA perform on datasets with more than two sensitive attributes or continuous sensitive attributes? The paper evaluates only binary sensitive attributes, limiting understanding of scalability to more complex fairness scenarios.

### Open Question 4
How sensitive is DSA to hyperparameter choices, particularly the number of masked patches (k) and weight coefficients (λ1, λ2, λ3)? While some ablation studies exist, the paper doesn't provide comprehensive sensitivity analysis across the full parameter space.

## Limitations

- Implementation details of the Patch-Fool attack are not fully specified, particularly the attention-aware optimization and gradient conflict resolution
- Results are limited to binary sensitive attributes in controlled benchmark datasets, not reflecting the complexity of real-world bias scenarios
- Computational overhead compared to baseline methods is not quantified, leaving efficiency concerns unaddressed

## Confidence

**High Confidence**: The core conceptual framework combining adversarial attacks with attention alignment is well-established and the general approach is sound.

**Medium Confidence**: The effectiveness of the specific Patch-Fool attack implementation for identifying sensitive features, as exact technical details are not fully specified.

**Low Confidence**: The generalizability to more complex, real-world scenarios with multiple sensitive attributes or continuous sensitive variables.

## Next Checks

1. **Ablation on Adversarial Attack Strength**: Systematically vary the number of perturbed patches and perturbation magnitude to quantify the tradeoff between fairness improvement and accuracy preservation.

2. **Cross-dataset Generalization**: Test DSA on more diverse and complex datasets with multiple sensitive attributes to validate real-world applicability beyond current benchmarks.

3. **Attention Mechanism Analysis**: Conduct detailed analysis of how attention weights change during training with DSA, including visualizations of attention maps before and after debiasing.