---
ver: rpa2
title: The impact of responding to patient messages with large language model assistance
arxiv_id: '2310.17703'
source_url: https://arxiv.org/abs/2310.17703
tags:
- patient
- page
- responses
- your
- manual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated AI assistance in drafting responses to patient
  questions using 100 synthetic cancer scenarios. Six oncologists manually responded
  in Stage 1, then edited GPT-4 drafts in Stage 2.
---

# The impact of responding to patient messages with large language model assistance

## Quick Facts
- arXiv ID: 2310.17703
- Source URL: https://arxiv.org/abs/2310.17703
- Reference count: 33
- Key outcome: AI-assisted responses were acceptable without edits 58% of the time and improved efficiency 77% of the time

## Executive Summary
This study evaluated AI assistance in drafting responses to patient questions using 100 synthetic cancer scenarios. Six oncologists manually responded in Stage 1, then edited GPT-4 drafts in Stage 2. AI-assisted responses were longer and less readable than manual responses but were acceptable without edits 58% of the time and improved efficiency 77% of the time. The majority of drafts were deemed safe (82%), though 7.7% could cause severe harm if unedited. AI assistance increased education recommendations while decreasing direct clinical actions compared to manual responses. AI assistance also improved inter-rater agreement on response content from kappa 0.10 to 0.52.

## Method Summary
The study used a two-stage cross-sectional design with 6 board-certified oncologists. In Stage 1, physicians manually responded to 26 synthetic cancer patient scenarios. In Stage 2, GPT-4 generated draft responses that physicians edited, followed by completion of a 7-question survey. Responses were analyzed for word count, readability (Flesch reading ease score), edit distance (Levenshtein), and content categories (10 categories). Statistical comparisons were made between manual, GPT-4 drafted, and AI-assisted responses.

## Key Results
- AI-assisted responses were acceptable without edits 58% of the time and improved efficiency 77% of the time
- 82% of GPT-4 drafts were deemed safe, though 7.7% could cause severe harm if unedited
- AI assistance improved inter-rater agreement on response content from kappa 0.10 to 0.52

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-assisted responses improve documentation efficiency by providing acceptable draft responses that require minimal edits.
- Mechanism: GPT-4 generates initial draft responses that physicians can quickly review and edit, reducing the time spent writing responses from scratch. The study found that AI-assisted responses were acceptable without edits 58% of the time and improved efficiency 77% of the time.
- Core assumption: The time saved by editing AI-generated drafts is greater than the time required to write responses manually.
- Evidence anchors:
  - [abstract] "AI-assisted responses were longer and less readable than manual responses but were acceptable without edits 58% of the time and improved efficiency 77% of the time."
  - [section] "Physicians believed that AI assistance improved efficiency."
- Break condition: If the time required to edit AI drafts exceeds the time to write responses manually, or if the AI drafts are frequently unacceptable and require extensive modifications.

### Mechanism 2
- Claim: AI assistance increases patient education recommendations while reducing direct clinical actions in responses.
- Mechanism: The AI model tends to include more patient education and self-management recommendations compared to manual responses, while physicians are less likely to recommend direct clinical actions such as urgent evaluations or ordering tests when using AI assistance.
- Core assumption: The AI model's training data includes a higher proportion of patient education content compared to direct clinical actions.
- Evidence anchors:
  - [abstract] "AI assistance led to more patient education recommendations, fewer clinical actions than manual responses."
  - [section] "Manual responses were more likely to recommend direct clinical action than both GPT-4 drafts and AI-assisted responses, while AI-assisted responses were more likely to include extensive education and self-management recommendations provided by GPT-4."
- Break condition: If the AI model is retrained with a different distribution of clinical content or if physicians override the AI's tendency to include more education.

### Mechanism 3
- Claim: AI assistance improves inter-rater agreement on response content between physicians.
- Mechanism: By providing a standardized draft response, the AI model reduces variability in how different physicians would respond to the same patient message, leading to more consistent content across responses.
- Core assumption: The AI model's draft responses are sufficiently comprehensive and well-structured to guide physicians towards similar response content.
- Evidence anchors:
  - [abstract] "AI assistance also improved inter-rater agreement on response content from kappa 0.10 to 0.52."
  - [section] "Inter-rater agreement between physicians for the content categories present in responses to the same scenario was lower for manual responses compared to AI-assisted responses (mean Cohen's kappa 0.10 vs. 0.52), indicating more consistent clinical content with the use of AI assistance."
- Break condition: If the AI model's drafts are highly variable or if physicians consistently override the AI's content recommendations.

## Foundational Learning

- Concept: Understanding the differences between manual, GPT-4 drafted, and AI-assisted responses.
  - Why needed here: To evaluate the impact of AI assistance on response characteristics and physician behavior.
  - Quick check question: What were the key differences in word count, readability, and content between manual, GPT-4 drafted, and AI-assisted responses?

- Concept: Recognizing the potential risks and benefits of AI-assisted patient messaging.
  - Why needed here: To assess the safety and efficacy of AI assistance in a clinical setting.
  - Quick check question: What were the main findings regarding the acceptability, harm potential, and efficiency of AI-assisted responses?

- Concept: Interpreting statistical significance and inter-rater agreement metrics.
  - Why needed here: To understand the strength of the study's findings and the consistency of AI-assisted responses across physicians.
  - Quick check question: How did the inter-rater agreement (Cohen's kappa) differ between manual and AI-assisted responses, and what does this imply about the consistency of response content?

## Architecture Onboarding

- Component map: Dataset creation and curation (scenario/message pairs) -> Clinical end-user study (Stage 1: manual responses, Stage 2: AI-assisted responses) -> Survey instruments (7 questions in Stage 2) -> Content annotation guidelines (10 categories) -> Statistical analysis (Mann-Whitney U tests, Levenshtein distance, Cohen's kappa)

- Critical path:
  1. Create realistic patient scenarios and messages
  2. Have physicians respond manually in Stage 1
  3. Generate GPT-4 drafts for Stage 2
  4. Have physicians edit GPT-4 drafts in Stage 2
  5. Analyze response characteristics (word count, readability, content)
  6. Conduct statistical analysis to compare manual vs. AI-assisted responses

- Design tradeoffs:
  - Using synthetic scenarios instead of real patient data to control for variability
  - Blinding physicians to the source of drafts to minimize bias
  - Focusing on a specific clinical domain (oncology) for targeted evaluation

- Failure signatures:
  - Low acceptability of AI drafts (requiring extensive edits)
  - High potential for harm in unedited AI responses
  - Decreased efficiency compared to manual responses
  - Inconsistent content recommendations across physicians

- First 3 experiments:
  1. Compare word count, readability, and content categories between manual, GPT-4 drafted, and AI-assisted responses using statistical tests.
  2. Analyze the relationship between survey responses (acceptability, harm potential, efficiency) and edit distance using correlation analysis.
  3. Evaluate inter-rater agreement on response content using Cohen's kappa for manual vs. AI-assisted responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would AI-assisted patient messaging perform with real patient data versus synthetic scenarios?
- Basis in paper: [explicit] "this was not performed using real patient data or within a real EHR message portal system, and the use of AI assistance may be different when physicians are responding to real questions"
- Why unresolved: The study used synthetic scenarios that may not capture the full complexity and variability of actual patient questions and contexts
- What evidence would resolve it: A study using actual patient messages from EHR systems with AI assistance, comparing outcomes and clinician experiences to the current synthetic scenario results

### Open Question 2
- Question: How does AI assistance impact clinical decision-making for high-acuity situations where urgent care is needed?
- Basis in paper: [explicit] "Physicians were less likely to recommend a patient seek urgent care and say they will take a direct clinical action... when using AI assistance"
- Why unresolved: The study found reduced recommendations for urgent evaluation with AI assistance, but the clinical implications of this change are unclear
- What evidence would resolve it: Follow-up studies tracking actual clinical outcomes when AI-assisted responses are used in practice, particularly focusing on cases requiring urgent intervention

### Open Question 3
- Question: What is the optimal prompting strategy and model version for AI-assisted patient messaging in clinical settings?
- Basis in paper: [explicit] "Results may be different based on prompting methods and the type of AI chatbot used. We chose to study GPT-4 because Epic, the largest EHR vendor in the United States, is implementing ChatGPT-family models"
- Why unresolved: The study used a specific prompting approach and GPT-4 model, but optimal configurations may vary by clinical context and use case
- What evidence would resolve it: Systematic comparisons of different prompting strategies and model versions across multiple clinical scenarios, measuring outcomes like response quality, efficiency gains, and safety

## Limitations

- The study relies entirely on synthetic patient scenarios rather than real clinical data, which may not fully capture the complexity and variability of actual patient messages.
- The small sample size of 6 physicians limits generalizability across different clinical practices and experience levels.
- The evaluation focuses solely on acceptability and efficiency from physician perspective, without direct measurement of patient outcomes or satisfaction.

## Confidence

**High Confidence**: The findings regarding increased response length and decreased readability with AI assistance are well-supported by direct measurements. The 58% acceptability rate without edits and 77% efficiency improvement are concrete metrics with clear operational definitions.

**Medium Confidence**: The safety assessment showing 82% of drafts deemed safe requires careful interpretation given the subjective nature of harm assessment. The inter-rater agreement improvement (kappa 0.10 to 0.52) is statistically significant but the practical clinical significance needs further validation.

**Low Confidence**: The claims about changes in clinical decision-making patterns (increased education, decreased direct actions) are based on content category analysis that may be influenced by the synthetic nature of scenarios and lack real-world clinical context.

## Next Checks

1. Conduct a pilot study using real patient messages from actual clinical practice to validate whether the synthetic scenario findings generalize to real-world clinical communication patterns.

2. Implement a longitudinal study tracking physician response patterns over time to assess whether AI assistance leads to systematic changes in clinical decision-making that could affect patient care quality.

3. Design a randomized controlled trial comparing patient outcomes and satisfaction between AI-assisted and manual response approaches, including measures of diagnostic accuracy and treatment adherence.