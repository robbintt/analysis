---
ver: rpa2
title: 'Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language
  Models?'
arxiv_id: '2312.00554'
source_url: https://arxiv.org/abs/2312.00554
tags:
- summaries
- legal
- keywords
- country
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines biases in case judgment summaries generated
  by legal datasets and large language models (LLMs). The research investigates gender,
  race, crime against women, country names, and religious biases in model-generated
  summaries compared to expert-written summaries.
---

# Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?

## Quick Facts
- arXiv ID: 2312.00554
- Source URL: https://arxiv.org/abs/2312.00554
- Authors: 
- Reference count: 6
- This study examines biases in case judgment summaries generated by legal datasets and large language models, finding slight female-related biases and strong country-specific biases in legal domain models.

## Executive Summary
This study investigates biases in case judgment summaries generated by large language models (LLMs) and legal domain-specific models, comparing them against expert-written summaries. The research analyzes five types of biases: gender, race, crime against women, country names, and religious keywords across Indian and UK legal datasets. Results show slight biases towards female-related keywords in both datasets across various models, while strong biases were observed in legal domain-specific models towards specific country names, particularly "United States," due to initial training on US legal data. The study highlights the need for further research to understand and mitigate biases in legal AI applications.

## Method Summary
The study uses two legal judgment datasets: IN-Abs (7130 pairs from Indian Supreme Court) and UK-Abs (793 pairs from UK Supreme Court). Researchers tested general domain LLMs (Text-Davinci-003 and GPT-3.5 Turbo) and legal domain-specific models (LegLED, LegPegasus, and their UK/IN fine-tuned variants) on 1024-word chunks using a divide-and-conquer approach. Bias analysis was performed by counting keyword occurrences in original documents, expert summaries, and model-generated summaries across five categories: gender-related keywords, race-related keywords, crime against women keywords, country names, and religious keywords.

## Key Results
- Slight biases towards female-related keywords observed in both Indian and UK judgment summaries across general LLMs and legal domain models
- Strong biases towards "United States" in legal domain-specific models due to initial training on US legal data
- UK dataset summaries showed biases related to crimes against women not evident in Indian dataset
- No significant evidence of religious or race-related biases found in generated summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initial training data distribution creates lasting biases in model outputs
- Mechanism: Legal domain-specific models like LegLED were trained on US legal data, causing them to generate "United States" disproportionately often even when summarizing UK or Indian cases
- Core assumption: Fine-tuning on smaller in-domain datasets doesn't fully erase biases from initial pre-training
- Evidence anchors:
  - [section] "We observe that some legal domain-specific abstractive summarization models like LegLED tend to generate some country names like 'United States' more than other models specifically due to initial training on US legal data"
  - [section] "Interestingly Country names like Pakistan, Lithuania, and Zimbabwe come up in the list of Top-5 country names in the output summaries generated by the general purpose LLMs as well as the legal domain-specific LLMs for the UK-Abs dataset"

### Mechanism 2
- Claim: Gender bias manifests as slight preference for female-related keywords in generated summaries
- Mechanism: Both general LLMs and legal domain models produce slightly more female-related keywords than male-related keywords in their summaries
- Core assumption: The training data or fine-tuning process doesn't perfectly balance gender representation
- Evidence anchors:
  - [section] "In our study, we observe slight biases for female-related keywords on both Indian judgement summaries(IN-Abs) and United Kingdom judgement summaries(UK-Abs) for the different General purpose LLMs and pretrained legal domain specific abstractive models"
  - [section] "The expert-written summaries have comparable performance w.r.t male and female-related keywords on both UK-Abs and IN-Abs datasets"

### Mechanism 3
- Claim: Country-specific crime patterns affect keyword generation frequency
- Mechanism: UK judgment summaries show more crime-against-women keywords than Indian summaries, reflecting different legal and social contexts
- Core assumption: The model amplifies patterns present in the training data
- Evidence anchors:
  - [section] "The LLM generated summaries for the United kingdom(UK-Abs) dataset shows biases towards specific terms related to crime against women. No such strong observation is found for the Indian(IN-Abs) dataset"
  - [section] "For the UK-Abs dataset, keywords like domestic violence, sexual assault, trafficking, refugee, sexual abuse, and forced marriage are generated more by the general purpose LLMs like Davinci and Chatgpt as compared to the legal domain-specific abstractive summarization models"

## Foundational Learning

- Concept: Bias measurement in text generation
  - Why needed here: The study needs to quantify how often certain keywords appear in generated summaries vs. reference data
  - Quick check question: How would you calculate the percentage of female-related keywords in a generated summary?

- Concept: Fine-tuning vs. pre-training in NLP
  - Why needed here: Understanding why legal models still show US bias after being fine-tuned on UK/Indian data
  - Quick check question: What's the difference between pre-training and fine-tuning, and how might this affect bias persistence?

- Concept: Corpus composition effects
  - Why needed here: Different datasets (UK-Abs vs IN-Abs) show different bias patterns, suggesting training data matters
  - Quick check question: How might the composition of your training corpus affect the types of biases a model exhibits?

## Architecture Onboarding

- Component map: Data layer (IN-Abs, UK-Abs) -> Processing layer (1024-word chunking) -> Model layer (General LLMs + Legal domain models) -> Analysis layer (Keyword frequency counting)
- Critical path: Data → Chunking → Model Generation → Keyword Analysis → Bias Assessment
- Design tradeoffs: Chunk size of 1024 words chosen over 2048 for better results; multiple model variants used for comparison
- Failure signatures: 
  - Unexpected country name generation (e.g., US in UK summaries)
  - Disproportionate gender keyword usage
  - Missing expected legal terminology
- First 3 experiments:
  1. Compare keyword frequencies between original documents and expert summaries to establish baseline bias
  2. Test different chunking strategies (1024 vs 2048 words) on a subset of data
  3. Run the same summarization prompts across all model variants on a small test set to observe consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the observed biases towards "United States" in legal domain-specific abstractive models like LegLED, and how can these be mitigated during training?
- Basis in paper: [explicit] The paper notes strong biases towards specific country names, particularly "United States," due to initial training on US legal data.
- Why unresolved: The study identifies the presence of this bias but does not explore the underlying mechanisms or propose solutions to mitigate it.
- What evidence would resolve it: Detailed analysis of the training data distribution and model architecture to identify bias sources, followed by experiments testing various mitigation techniques.

### Open Question 2
- Question: How do biases in case judgment summaries impact real-world legal outcomes, and what are the potential long-term societal implications of these biases?
- Basis in paper: [inferred] The study mentions the impact of biases on legal decision making but does not provide concrete examples or long-term analysis.
- Why unresolved: The research focuses on identifying biases but does not investigate their practical effects on legal proceedings or broader societal impacts.
- What evidence would resolve it: Case studies comparing legal outcomes with and without biased summaries, and longitudinal studies tracking societal effects of AI-assisted legal systems.

### Open Question 3
- Question: Are there cultural or jurisdictional differences in the manifestation of biases across different legal systems, and how can models be adapted to account for these differences?
- Basis in paper: [explicit] The study compares biases in Indian and UK datasets but does not explore potential cultural or jurisdictional factors influencing bias patterns.
- Why unresolved: While the paper notes differences between datasets, it does not investigate the underlying cultural or jurisdictional factors that might contribute to these differences.
- What evidence would resolve it: Comparative analysis of biases across multiple legal systems, including qualitative studies on cultural and jurisdictional influences on legal language and decision-making.

## Limitations
- Small UK dataset (793 pairs vs 7130 for India) may limit statistical significance of UK-specific bias observations
- Keyword-based bias detection cannot capture nuanced semantic biases or context-dependent meaning shifts
- Analysis focuses on surface-level keyword frequency rather than deeper contextual understanding of legal contexts

## Confidence

- **High confidence**: Gender bias findings showing slight female-related keyword preference across models; observation that expert-written summaries show balanced gender representation
- **Medium confidence**: Country name bias findings, particularly the US bias in legal domain models; crime against women keyword patterns in UK vs Indian datasets
- **Low confidence**: Absence of religious or race-related biases, as the keyword-based approach may miss subtle manifestations of these biases

## Next Checks

1. **Statistical significance validation**: Perform significance testing on keyword frequency differences between model-generated and expert summaries, particularly for the smaller UK dataset
2. **Contextual bias analysis**: Implement semantic similarity metrics to detect context-dependent biases beyond keyword frequency counting
3. **Cross-dataset generalization**: Test the same bias analysis pipeline on additional legal datasets from other jurisdictions to verify if observed patterns are generalizable or dataset-specific