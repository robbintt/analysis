---
ver: rpa2
title: 'Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning
  and Backdoor Attacks'
arxiv_id: '2310.05862'
source_url: https://arxiv.org/abs/2310.05862
tags:
- clip
- data
- safe
- poisoned
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of CLIP to targeted data
  poisoning and backdoor attacks, which can be successful with as little as 0.0001%
  poisoned data. Existing defenses are limited, so the authors propose SAFE CLIP,
  a method to pre-train CLIP models against these attacks.
---

# Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks

## Quick Facts
- arXiv ID: 2310.05862
- Source URL: https://arxiv.org/abs/2310.05862
- Reference count: 10
- Primary result: SAFE CLIP reduces attack success rates from 93.75% to 0% for targeted data poisoning and from 54.3% to 0% for backdoor attacks

## Executive Summary
CLIP models are vulnerable to targeted data poisoning and backdoor attacks, with attacks succeeding with as little as 0.0001% poisoned data. Existing defenses are insufficient for these specific attack types. SAFE CLIP is a novel pre-training method that defends against these attacks by using a three-step process: unimodal contrastive learning warmup to separate poisoned from clean pairs, a slow-paced CLIP warmup to maintain this separation, and mixed training with progressive safe set expansion. Experiments demonstrate that SAFE CLIP effectively eliminates attack success while maintaining CLIP's downstream performance.

## Method Summary
SAFE CLIP defends CLIP models during pre-training by first applying unimodal contrastive learning separately on image and text modalities to initialize representations where poisoned pairs are separated. This is followed by a slow-paced CLIP warmup epoch at low learning rate to maintain the separation. The method then divides data into "safe" and "risky" sets based on cosine similarity, training safe pairs with CLIP loss and risky pairs with unimodal CL. The safe set gradually expands during training, ensuring poisoned pairs remain in the risky set where they don't corrupt the model. Data augmentation is applied to the safe set to improve representation quality without risking poisoning.

## Key Results
- Reduces targeted data poisoning attack success rate from 93.75% to 0%
- Reduces backdoor attack success rate from 54.3% to 0%
- Maintains CLIP's performance on downstream tasks while providing defense

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAFE CLIP effectively prevents poisoned image-caption pairs from being trained together by maintaining low cosine similarity between them during the warmup phase.
- Mechanism: During the unimodal contrastive learning warmup, images and captions are clustered separately based on their modalities. Since poisoned images and their adversarial captions belong to different categories, they move further apart in representation space. This initial separation is maintained by using a low learning rate during the CLIP warmup epoch.
- Core assumption: The poisoned image-caption pairs will have lower cosine similarity than clean pairs after unimodal warmup due to belonging to different semantic categories.
- Evidence anchors:
  - [abstract] "SAFE CLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately... This initializes the model in a way that poisoned image-caption representations have a low similarity initially."
  - [section] "SAFE CLIP leverages unimodal CL on both image and text modalities separately. Since unimodal CL does not match poisoned images with captions, it does not risk poisoning the models. The unimodal CL clusters similar images and similar captions together. In doing so, poisoned pairs can be better separated from the clean pairs."
  - [corpus] Weak evidence - the corpus papers discuss related attacks but don't specifically validate this mechanism of maintaining low similarity.
- Break condition: If the poisoned image and caption happen to belong to semantically similar categories, the unimodal warmup may not sufficiently separate them.

### Mechanism 2
- Claim: SAFE CLIP progressively excludes poisoned examples from training by updating the safe/risky data split throughout pre-training.
- Mechanism: After initial warmup, SAFE CLIP divides data based on cosine similarity - high similarity examples are "safe" and trained with CLIP loss, while low similarity examples are "risky" and trained with unimodal CL. The safe set size gradually increases each epoch, ensuring that most poisoned pairs remain in the risky set where they don't get trained with CLIP loss.
- Core assumption: Poisoned pairs will consistently have lower cosine similarity than clean pairs throughout training, allowing them to be filtered out.
- Evidence anchors:
  - [abstract] "Then, it divides the data into safe and risky sets, by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations."
  - [section] "SAFE CLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. The safe and risky sets are updated during the training and the size of the safe set is gradually increased."
  - [corpus] Weak evidence - the corpus papers discuss related defenses but don't specifically validate this progressive filtering mechanism.
- Break condition: If poisoned pairs somehow achieve high cosine similarity during training, they could enter the safe set and corrupt the model.

### Mechanism 3
- Claim: SAFE CLIP's mixed training approach balances defense effectiveness with maintaining CLIP's performance by gradually increasing safe data exposure while using data augmentation.
- Mechanism: Initially training with a small safe set and large risky set ensures poisoned pairs are excluded. Gradually increasing the safe set size allows more clean data to be trained with CLIP loss over time. Data augmentation on safe data further improves representation quality without risking poisoning.
- Core assumption: Gradually increasing safe data exposure won't allow poisoned pairs to enter the safe set before they're properly filtered out.
- Evidence anchors:
  - [abstract] "By gradually increasing the size of the safe set during pre-training, SAFE CLIP effectively breaks targeted data poisoning and backdoor attacks without harming CLIP's performance."
  - [section] "To address these concerns, at the end of each epoch, we assess the cosine similarity of all examples and update k = k + i, to chose a larger fraction of the data with highest cosine similarity as the new safe set... With the above update strategy, only a small number of poisoned pairs may temporarily enter the safe data and cannot poison the model."
  - [corpus] Weak evidence - the corpus papers don't specifically validate this performance-defense tradeoff mechanism.
- Break condition: If the increment ratio i is too large, poisoned pairs could enter the safe set before being properly filtered.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Understanding how CLIP's contrastive loss aligns image-caption pairs is crucial for grasping why poisoned pairs can corrupt the model and how SAFE CLIP's defense works.
  - Quick check question: What is the difference between the CLIP loss and unimodal contrastive learning loss in terms of what pairs they match?

- Concept: Data poisoning and backdoor attack mechanics
  - Why needed here: To understand the threat SAFE CLIP defends against, engineers need to know how attackers inject poisoned examples and what their objectives are.
  - Quick check question: How does the attack success rate differ between targeted data poisoning and backdoor attacks in CLIP models?

- Concept: Representation similarity and clustering
  - Why needed here: SAFE CLIP relies on separating poisoned pairs based on their representation similarity, so understanding how cosine similarity relates to semantic similarity is important.
  - Quick check question: Why would poisoned image-caption pairs naturally have lower cosine similarity than clean pairs after unimodal warmup?

## Architecture Onboarding

- Component map: Image encoder (ResNet-50) -> Text encoder (transformer) -> CLIP loss module <-> Unimodal CL modules (separate for image and text) <-> Safe/risky data splitter with cosine similarity calculation <-> Data augmentation module for safe set <-> Learning rate scheduler

- Critical path: Warmup (unimodal CL → slow CLIP) → Data splitting → Mixed training with progressive safe set growth

- Design tradeoffs:
  - Smaller initial safe set provides better defense but slower convergence
  - Larger increment ratio speeds up convergence but risks poisoning
  - Nearest neighbor approach in unimodal CL improves representation quality but increases computation

- Failure signatures:
  - High attack success rate despite SAFE CLIP indicates poisoned pairs entering safe set
  - Degraded downstream performance suggests too aggressive filtering of clean data
  - Slow convergence might indicate overly conservative safe set sizing

- First 3 experiments:
  1. Test SAFE CLIP with varying poison rates (0.01%, 0.05%) to find breaking point
  2. Compare attack success rates with/without nearest neighbor approach in unimodal CL
  3. Evaluate downstream performance sensitivity to different safe set increment ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAFE CLIP vary with different values of the increment ratio i during the mixed training phase?
- Basis in paper: [explicit] The paper mentions that a small increment ratio i is crucial for maintaining defense effectiveness while gradually incorporating more data as safe.
- Why unresolved: The paper only provides results for a specific increment ratio (i = 1) and states that larger values increase the attack success rate. It does not explore a range of values or their impact on performance.
- What evidence would resolve it: Experimental results showing the attack success rate and downstream performance of SAFE CLIP for various increment ratio values (e.g., i = 1, 2, 5) would clarify the optimal range and trade-offs.

### Open Question 2
- Question: How does the choice of nearest neighbor pool size affect the effectiveness of SAFE CLIP in defending against targeted data poisoning and backdoor attacks?
- Basis in paper: [explicit] The paper mentions using a nearest neighbor pool during unimodal CL warmup to enrich representations and improve separation of poisoned pairs, but does not explore different pool sizes.
- Why unresolved: The impact of nearest neighbor pool size on defense effectiveness is not investigated, leaving uncertainty about the optimal size for balancing computational cost and defense performance.
- What evidence would resolve it: Experiments comparing the attack success rate and downstream performance of SAFE CLIP with different nearest neighbor pool sizes would reveal the optimal configuration.

### Open Question 3
- Question: How does the performance of SAFE CLIP scale with increasing dataset size and poison rate?
- Basis in paper: [inferred] The paper demonstrates effectiveness on 1 million examples with poison rates up to 0.05%, but does not explore larger datasets or higher poison rates.
- Why unresolved: The scalability of SAFE CLIP to larger datasets and higher poison rates is unknown, limiting understanding of its practical applicability in real-world scenarios with massive datasets.
- What evidence would resolve it: Experiments evaluating SAFE CLIP on datasets of varying sizes (e.g., 10 million, 100 million examples) and poison rates (e.g., 0.1%, 0.5%) would determine its scalability limits and performance trends.

## Limitations
- Effectiveness against adaptive attackers who design poisons to evade semantic separation is unknown
- Performance guarantees when poison rates exceed the tested 0.01% threshold are unclear
- The gradual safe set expansion strategy could be sensitive to hyperparameter tuning

## Confidence

**High Confidence:**
- SAFE CLIP reduces attack success rates to 0% for both targeted data poisoning and backdoor attacks under the tested conditions
- The method maintains CLIP's downstream performance while providing defense
- Unimodal contrastive learning warmup effectively separates poisoned from clean pairs in representation space

**Medium Confidence:**
- The progressive safe set expansion strategy reliably excludes poisoned pairs throughout training
- The trade-off between defense effectiveness and convergence speed is well-balanced
- The method generalizes to different poison rates and attack types

**Low Confidence:**
- SAFE CLIP's effectiveness against adaptive attackers who design poisons to evade semantic separation
- Performance guarantees when poison rates exceed the tested 0.01% threshold
- Robustness when poisoned image-caption pairs share semantic similarity

## Next Checks

1. **Breaking Point Analysis**: Test SAFE CLIP's effectiveness at progressively higher poison rates (0.05%, 0.1%, 0.5%) to identify the concentration threshold where defense mechanisms fail. This will reveal whether the 0.01% limit is fundamental or can be extended.

2. **Adaptive Attack Resistance**: Design poisoning attacks that specifically target SAFE CLIP's weaknesses by creating poisoned pairs with high semantic similarity. Test whether the method can still separate these sophisticated poisons during the unimodal warmup phase.

3. **Component Ablation Study**: Implement variations of SAFE CLIP that remove individual components (unimodal warmup, progressive safe set expansion, data augmentation) to quantify each component's contribution to overall defense effectiveness. This will help identify which aspects are essential versus complementary.