---
ver: rpa2
title: 'Calibration in Deep Learning: A Survey of the State-of-the-Art'
arxiv_id: '2308.01222'
source_url: https://arxiv.org/abs/2308.01222
tags:
- calibration
- methods
- neural
- deep
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews the state-of-the-art calibration
  methods for deep neural networks, which are crucial for building reliable AI systems
  in safety-critical applications. The paper systematically categorizes calibration
  methods into four main types: post-hoc methods, regularization methods, uncertainty
  estimation, and composition methods.'
---

# Calibration in Deep Learning: A Survey of the State-of-the-Art

## Quick Facts
- arXiv ID: 2308.01222
- Source URL: https://arxiv.org/abs/2308.01222
- Reference count: 13
- This survey comprehensively reviews the state-of-the-art calibration methods for deep neural networks

## Executive Summary
This survey provides a systematic overview of calibration methods for deep neural networks, addressing the critical issue of model reliability in safety-critical applications. Modern deep networks often exhibit poor calibration due to overparameterization and overfitting, producing overconfident predictions despite high accuracy. The paper categorizes calibration approaches into post-hoc methods, regularization methods, uncertainty estimation, and composition methods, highlighting their mechanisms and applications across various model architectures including large language models.

## Method Summary
The survey synthesizes calibration methods through a comprehensive literature review, organizing them into four main categories: post-hoc methods (like temperature scaling), regularization methods (such as focal loss and label smoothing), uncertainty estimation techniques (including Monte Carlo dropout), and composition methods that combine multiple approaches. The paper evaluates these methods using standard calibration metrics like Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Classwise ECE (CECE), while also discussing recent advancements in calibrating large models. The methodology involves systematic categorization, theoretical analysis of calibration mechanisms, and identification of open research challenges in the field.

## Key Results
- Modern deep neural networks suffer from poor calibration despite high predictive performance due to overparameterization and overfitting
- Temperature scaling remains one of the most effective post-hoc calibration methods, learning a single global temperature parameter to reduce overconfidence
- Regularization methods like focal loss can implicitly improve calibration by penalizing confident predictions through entropy regularization
- Data augmentation techniques such as label smoothing and mixup improve calibration by reducing overfitting and softening hard labels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temperature scaling calibrates deep models by learning a single global temperature parameter that scales the logits before softmax, thereby reducing overconfidence without changing accuracy.
- **Mechanism:** The softmax probability is softened by dividing logits by a temperature τ > 0, which increases entropy and spreads probability mass across classes. The optimal τ is found by minimizing NLL on a validation set.
- **Core assumption:** A single scalar temperature can capture the systematic overconfidence present in modern deep networks due to overparameterization.
- **Evidence anchors:**
  - [abstract] "The proposed Dirichlet calibration map family coincides with the Beta calibration family (Kull, Filho, and Flach 2017)."
  - [section] "Temperature scaling (TS) is a single-parameter extension of Platt scaling (Platt et al. 1999) and the most recent addition to the offering of post-hoc methods."
  - [corpus] Weak evidence - only mentions "Self-Calibrating Conformal Prediction" which is related but not directly about temperature scaling.
- **Break condition:** If the miscalibration is highly asymmetric across classes or input regions, a single global temperature may be insufficient.

### Mechanism 2
- **Claim:** Regularization methods, especially focal loss, implicitly improve calibration by penalizing confident predictions through entropy regularization.
- **Mechanism:** Focal loss down-weights easy examples and up-weights hard ones; its form can be interpreted as a trade-off between KL divergence minimization and entropy maximization, depending on γ. This prevents the model from becoming overconfident.
- **Core assumption:** The overparameterization and overfitting that cause overconfidence can be mitigated by adding a loss component that encourages uncertainty.
- **Evidence anchors:**
  - [abstract] "Focal loss (Lin et al. 2017; Mukhoti et al. 2020) has recently demonstrated promising performance in calibrating deep models."
  - [section] "Focal loss (Lin et al. 2017) was originally proposed to alleviate the class imbalance issue in object detection... It has been recently shown that focal loss can be interpreted as a trade-off between minimizing Kullback–Leibler (KL) divergence and maximizing the entropy, depending on γ (Mukhoti et al. 2020)."
  - [corpus] Weak evidence - no direct citations to focal loss in related papers.
- **Break condition:** If the dataset is balanced and well-sampled, focal loss may not add calibration benefit and could hurt accuracy.

### Mechanism 3
- **Claim:** Data augmentation techniques like label smoothing and mixup improve calibration by reducing overfitting and softening hard labels, which mitigates overconfident predictions.
- **Mechanism:** Label smoothing replaces one-hot labels with smoothed distributions, preventing the model from being too certain. Mixup generates synthetic samples by convexly combining inputs and labels, increasing data diversity and generalization.
- **Core assumption:** Overfitting on limited or imbalanced data is a primary driver of miscalibration, and augmentation can alleviate this.
- **Evidence anchors:**
  - [abstract] "Label smoothing (M¨uller, Kornblith, and Hinton 2019) soften hard labels with an introduced smoothing parameter α in the standard loss function."
  - [section] "Mixup training (Thulasidasan et al. 2019) is another work in this line of exploration... The authors observed that mixup-trained models are better calibrated and less prone to overconfidence in prediction on out-of-distribution and noise data."
  - [corpus] Weak evidence - no direct references to label smoothing or mixup in corpus.
- **Break condition:** If the dataset is large and balanced, the calibration gains from augmentation may be negligible.

## Foundational Learning

- **Concept:** Model calibration
  - Why needed here: Calibration measures the alignment between predicted probabilities and true correctness likelihood; essential for reliable AI in safety-critical applications.
  - Quick check question: What is the difference between accuracy and confidence in a calibrated model?

- **Concept:** Overparameterization and overfitting
  - Why needed here: Modern deep networks are overparameterized, making them prone to overfitting and overconfident predictions, which calibration methods aim to correct.
  - Quick check question: How does increasing model depth/width affect calibration and overfitting?

- **Concept:** Calibration metrics (ECE, MCE, CECE)
  - Why needed here: These metrics quantify calibration error by binning predictions and comparing accuracy vs. confidence; necessary for evaluating calibration methods.
  - Quick check question: What is the difference between ECE and MCE?

## Architecture Onboarding

- **Component map:** Pre-trained model (backbone) -> Loss function (e.g., NLL, focal loss) -> Calibration module (post-hoc or regularization) -> Validation set for calibration parameter tuning -> Evaluation pipeline (ECE, MCE, reliability diagrams)

- **Critical path:**
  1. Train model with standard loss.
  2. Evaluate calibration on validation set.
  3. Apply chosen calibration method (e.g., temperature scaling).
  4. Re-evaluate calibration metrics.
  5. Deploy calibrated model.

- **Design tradeoffs:**
  - Post-hoc methods (e.g., TS) are simple and data-efficient but less expressive.
  - Regularization methods (e.g., focal loss) improve calibration during training but add complexity.
  - Data augmentation improves generalization and calibration but increases training time.
  - Uncertainty estimation (e.g., MC dropout) provides rich uncertainty estimates but at high computational cost.

- **Failure signatures:**
  - Calibration metrics do not improve after applying method.
  - Reliability diagram shows systematic bias (e.g., consistently overconfident in high-confidence bins).
  - Model accuracy drops significantly after calibration.

- **First 3 experiments:**
  1. Train a baseline model (e.g., ResNet) on CIFAR-100 with standard cross-entropy; measure ECE.
  2. Apply temperature scaling post-hoc; re-measure ECE and compare reliability diagrams.
  3. Train a model with focal loss (γ=2); compare calibration and accuracy to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop unbiased calibration estimators that mitigate the sensitivity and data inefficiency issues associated with bin-based calibration metrics like ECE and MCE?
- Basis in paper: [explicit] The paper explicitly discusses the challenges of measuring calibration bias due to the binning mechanism and finite sample numbers, and suggests that future efforts should include developing unbiased calibration estimators.
- Why unresolved: Current bin-based metrics are sensitive to the choice of bin size and number, leading to biased estimates of calibration error.
- What evidence would resolve it: Development and validation of new calibration metrics that are less sensitive to binning schemes and provide unbiased estimates of calibration error across different datasets and model architectures.

### Open Question 2
- Question: How can we effectively calibrate token-level probabilities in sequence generation tasks, particularly in the context of large language models, to address the softmax bottleneck and prevent miscalibration from propagating through the sequence?
- Basis in paper: [explicit] The paper mentions that model calibration for sequence generation is rarely discussed and highlights the token-level probability miscalibration in neural machine translation and few-shot learning tasks with LLMs.
- Why unresolved: Most calibration efforts have focused on classification and regression tasks, with limited attention to sequence generation, where early token miscalibration can significantly impact the entire sequence.
- What evidence would resolve it: Demonstration of effective calibration methods for token-level probabilities in sequence generation tasks, showing improved BLEU scores or other relevant metrics in neural machine translation and few-shot learning with LLMs.

### Open Question 3
- Question: What are the most effective strategies for calibrating pre-trained large models, particularly large language models, in zero-shot inference settings, and how can we address the challenges posed by the choice of prompt format, training samples, and order of training samples?
- Basis in paper: [explicit] The paper discusses recent studies on calibrating large vision and language models, including CLIP and GPT-2/3, and highlights the importance of learning temperature scaling and decision boundaries in few-shot learning settings.
- Why unresolved: Calibrating large pre-trained models in zero-shot inference is challenging due to the variability in prompt formats, training samples, and sample order, which can lead to instability and miscalibration.
- What evidence would resolve it: Empirical validation of calibration methods for large pre-trained models in zero-shot inference, showing consistent improvement in calibration metrics across different prompt formats, training samples, and sample orders.

## Limitations

- Limited empirical validation: Many claims about method effectiveness are based on theoretical arguments rather than systematic comparative experiments.
- Lack of computational overhead analysis: The survey does not discuss the computational costs of different calibration methods, which is critical for practical deployment.
- Brief treatment of LLMs: Despite being a major focus of recent calibration research, large language models receive relatively limited attention compared to vision models.

## Confidence

- **High confidence**: Claims about the prevalence of miscalibration in modern deep networks and the general effectiveness of post-hoc methods like temperature scaling.
- **Medium confidence**: Claims about regularization methods (focal loss, label smoothing) improving calibration, though empirical evidence is limited.
- **Low confidence**: Claims about the superiority of composition methods or specific combinations of calibration techniques due to limited empirical validation.

## Next Checks

1. **Systematic ablation study**: Compare the effectiveness of temperature scaling, focal loss, and mixup on the same model architecture and dataset using consistent evaluation protocols and metrics.

2. **Large-scale LLM calibration**: Evaluate calibration methods on multiple large language models across different domains (text classification, generation, etc.) to validate the survey's claims about LLM-specific challenges.

3. **Computational efficiency analysis**: Measure and compare the inference-time overhead and memory requirements of different calibration methods, particularly for real-time applications.