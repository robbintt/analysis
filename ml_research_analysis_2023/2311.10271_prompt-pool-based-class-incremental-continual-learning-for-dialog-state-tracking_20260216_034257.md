---
ver: rpa2
title: Prompt Pool based Class-Incremental Continual Learning for Dialog State Tracking
arxiv_id: '2311.10271'
source_url: https://arxiv.org/abs/2311.10271
tags:
- prompt
- learning
- task
- dialog
- pool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a prompt pool-based approach for class-incremental
  continual learning in dialog state tracking (DST). The core idea is to maintain
  a pool of key-value paired prompts and select prompts based on the distance between
  dialog history and prompt keys.
---

# Prompt Pool based Class-Incremental Continual Learning for Dialog State Tracking

## Quick Facts
- arXiv ID: 2311.10271
- Source URL: https://arxiv.org/abs/2311.10271
- Reference count: 0
- Key outcome: Prompt pool approach achieves approximately 40% higher joint goal accuracy than AdapterCL baseline in class-incremental DST

## Executive Summary
This paper introduces a prompt pool-based approach for class-incremental continual learning in dialog state tracking (DST). The method maintains a pool of key-value paired prompts and selects prompts based on the distance between dialog history and prompt keys, enabling automatic task identification during testing. Experiments on the Schema-Guided Dialog dataset and a real-world Chinese dataset demonstrate that the prompt pool method significantly outperforms the baseline AdapterCL, achieving approximately 40% higher joint goal accuracy. The approach is parameter-efficient and can be further enhanced by combining with a rehearsal buffer to mitigate catastrophic forgetting.

## Method Summary
The method maintains a prompt pool with key-value paired prompts, where keys are context vectors computed from dialog histories and values are the prompts themselves. For each dialog turn, the model calculates the distance between the dialog history context vector and prompt keys, then selects the closest prompts to use as additional input for the model. The selected prompts are concatenated with dialog history embeddings and fed into a frozen T5 model for dialog state prediction. The model is trained incrementally on tasks in sequence, optimizing prompt and key parameters using cross-entropy loss and a distance-based loss. Optionally, a rehearsal buffer can be used to store samples from previous tasks and improve performance on earlier tasks.

## Key Results
- Prompt pool method achieves approximately 40% higher joint goal accuracy than AdapterCL baseline
- Combining with rehearsal buffer further improves model performance
- The approach scales well with the size of the backbone model and is parameter-efficient
- Key selection accuracy (Acckey) indicates effective task identification during testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance-based prompt selection enables automatic task identification in class-incremental scenarios
- Core assumption: Context vectors from different tasks can be distinguished by their distance to prompt keys
- Evidence anchors: [abstract], [section] on L2P method, weak corpus evidence
- Break condition: Similar context vectors between tasks may cause incorrect prompt selection

### Mechanism 2
- Claim: Prompt tuning achieves higher accuracy than adapter-based methods
- Core assumption: Prompt tuning is more parameter-efficient and effective for continual learning in DST
- Evidence anchors: [abstract] on JGA improvement, weak corpus evidence
- Break condition: Overly large prompt pools may reduce generalization ability

### Mechanism 3
- Claim: Rehearsal buffer mitigates catastrophic forgetting
- Core assumption: Access to past examples helps maintain knowledge of previous tasks
- Evidence anchors: [abstract], [section] on rehearsal buffer effectiveness, no corpus evidence
- Break condition: Small or unrepresentative rehearsal buffers may provide minimal improvement

## Foundational Learning

- Concept: Distance-based prompt selection
  - Why needed here: To automatically identify relevant prompts without knowing task identity
  - Quick check question: How does the model calculate which prompts to select from the pool for a given dialog turn?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: To understand why mechanisms like rehearsal buffers are necessary
  - Quick check question: What happens to performance on earlier tasks as the model learns new tasks sequentially?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: To understand why prompt tuning is preferred over full model fine-tuning
  - Quick check question: Why is it beneficial to keep the pretrained model frozen and only tune the prompts?

## Architecture Onboarding

- Component map: Dialog history → Sentence encoder → Context vector → Distance calculation → Prompt selection → Concatenation with history embeddings → T5 model → Dialog state prediction

- Critical path: Dialog history → Sentence encoder → Context vector → Distance calculation → Prompt selection → Concatenation with history embeddings → T5 model → Dialog state prediction

- Design tradeoffs:
  - Prompt pool size vs. model efficiency: Larger pools may improve accuracy but increase computational cost
  - Number of prompts per task vs. task specificity: More prompts may improve task adaptation but risk overfitting
  - Rehearsal buffer size vs. privacy concerns: Larger buffers improve performance but may not be feasible with privacy constraints

- Failure signatures:
  - Low key selection accuracy (Acckey) indicates poor task identification
  - Decreasing joint goal accuracy on previous tasks indicates catastrophic forgetting
  - High perplexity when selecting adapters (in baseline) indicates poor adapter selection

- First 3 experiments:
  1. Implement prompt pool without rehearsal buffer and measure JGA and Acckey on SGD dataset
  2. Add rehearsal buffer and modify loss function to test improvement in both metrics
  3. Conduct ablation study by removing key selection loss term to measure its impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prompt pool method scale to a much larger number of tasks or domains beyond the 15 used in experiments?
- Basis in paper: [explicit] The paper mentions scalability with backbone model size but doesn't explore scaling to large numbers of tasks
- Why unresolved: Experiments only tested on 15 tasks from SGD dataset
- What evidence would resolve it: Experiments on datasets with significantly more tasks/domains, measuring performance as task count increases

### Open Question 2
- Question: How sensitive is the method to the choice of hyperparameters?
- Basis in paper: [explicit] The paper mentions hyperparameters but only provides one set of values used in experiments
- Why unresolved: The paper doesn't explore sensitivity to different hyperparameter choices
- What evidence would resolve it: Systematic hyperparameter search or ablation study exploring impact of different parameter values

### Open Question 3
- Question: How does the method perform in scenarios where tasks have significant overlap or are not clearly separable?
- Basis in paper: [inferred] The paper mentions similar tasks in SGD but doesn't extensively explore this scenario
- Why unresolved: The paper doesn't provide detailed analysis of handling ambiguous or overlapping task boundaries
- What evidence would resolve it: Experiments with artificially created overlapping tasks or analysis of tasks with known similarities

## Limitations
- Claims about 40% improvement lack statistical validation and significance testing
- Scalability to extremely large prompt pools or hundreds of tasks remains untested
- Privacy implications of rehearsal buffers are not fully explored
- Method's performance on other DST datasets beyond SGD and Chinese dataset is unknown

## Confidence
- Medium confidence in distance-based prompt selection mechanism
- Medium confidence in 40% improvement claim
- Low confidence in scalability claims

## Next Checks
1. Conduct statistical validation (t-tests or bootstrap analysis) to determine if the 40% improvement over AdapterCL is statistically significant across multiple runs
2. Perform systematic ablation studies to quantify individual contributions of components (distance loss, rehearsal buffer) to overall performance
3. Test the prompt pool approach on additional DST datasets (e.g., MultiWOZ, WOZ) to evaluate robustness beyond the datasets used in the paper