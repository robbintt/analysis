---
ver: rpa2
title: 'LM-Cocktail: Resilient Tuning of Language Models via Model Merging'
arxiv_id: '2311.13534'
source_url: https://arxiv.org/abs/2311.13534
tags:
- lm-cocktail
- fine-tuned
- tasks
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  fine-tuned language models, where models degrade in general task performance after
  being specialized for specific domains. The proposed solution, LM-Cocktail, is a
  simple yet effective method that merges fine-tuned models with pre-trained base
  models or peer models from other domains using weighted averaging.
---

# LM-Cocktail: Resilient Tuning of Language Models via Model Merging

## Quick Facts
- arXiv ID: 2311.13534
- Source URL: https://arxiv.org/abs/2311.13534
- Reference count: 10
- This paper addresses catastrophic forgetting in fine-tuned language models by merging fine-tuned and base models using weighted averaging based on few-shot validation examples.

## Executive Summary
This paper introduces LM-Cocktail, a method to address catastrophic forgetting in fine-tuned language models. When language models are specialized for specific domains through fine-tuning, they often experience performance degradation on general tasks. LM-Cocktail solves this by merging fine-tuned models with their base models using weighted parameter averaging, where weights are computed from few-shot validation examples. The approach is simple yet effective, requiring no additional training or modifications to the fine-tuning pipeline.

## Method Summary
LM-Cocktail addresses catastrophic forgetting by merging fine-tuned specialist models with their base generalist models through weighted parameter averaging. The merging weights are computed using few-shot validation examples from the target domain, where each candidate model's loss on these examples determines its contribution to the merged model. The method is architecture-agnostic and works with both decoder-based models (like LLaMA) and encoder-based models (like BGE). The weighted average is computed as: Mr = αMt + (1-α)Mb, where weights are determined by softmax(-L(Mi, Et)/τ).

## Key Results
- LM-Cocktail successfully preserves strong performance on targeted tasks while maintaining competitive results on general tasks
- Achieved 94%+ accuracy on target tasks while maintaining 49.86% average accuracy across general tasks
- Works universally across both decoder-based (LLaMA) and encoder-based (BGE) models
- Outperforms both base and fine-tuned models in comprehensive evaluations across FLAN, MMLU, and MTEB benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging fine-tuned and base models with weighted averaging preserves general task performance while maintaining specialist task accuracy.
- Mechanism: The base model retains general knowledge while the fine-tuned model contributes task-specific expertise. By combining them with optimized weights, the merged model benefits from both sources without the catastrophic forgetting typically observed in fine-tuning.
- Core assumption: The weighted average can effectively combine complementary knowledge without interference.
- Evidence anchors: [abstract] "LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain."

### Mechanism 2
- Claim: The few-shot validation examples effectively determine optimal merging weights.
- Mechanism: The merging weights are computed based on the prediction loss of candidate models on few-shot examples from the targeted domain. Models with lower loss on the target task receive higher weights, ensuring the merged model maintains specialist performance.
- Core assumption: Few-shot examples are representative enough to estimate model quality for weight computation.
- Evidence anchors: [section 2.2] "wi ← softmax(-L(Mi, Et)/τ)" provides the mathematical formulation for weight computation.

### Mechanism 3
- Claim: LM-Cocktail works universally across both decoder-based and encoder-based language models.
- Mechanism: The method's architecture-agnostic nature allows it to be applied to different model types by simply averaging parameters, regardless of whether they're designed for generation or representation tasks.
- Core assumption: The parameter averaging technique is compatible with different model architectures.
- Evidence anchors: [abstract] "LM-Cocktail turns out to be universally applicable: it can substantially contribute to both the decoder-based LM in language generation tasks and the encoder-based LM in language representation tasks."

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why fine-tuning degrades general performance is essential to appreciate LM-Cocktail's value proposition
  - Quick check question: What happens to a model's performance on previously learned tasks when it's fine-tuned on new data?

- Concept: Model merging and parameter averaging
  - Why needed here: The core technique relies on understanding how to combine models through weighted averaging of their parameters
  - Quick check question: How does averaging parameters from two models differ from ensembling their outputs?

- Concept: Few-shot learning and validation
  - Why needed here: The weight computation relies on using minimal examples to estimate model quality
  - Quick check question: Why might 5 examples be sufficient to compute merging weights in this context?

## Architecture Onboarding

- Component map: Base model -> Fine-tuned specialist models -> Validation dataset (few-shot examples) -> Weight computation module -> Merging operation

- Critical path: Load base model and fine-tuned models → Prepare few-shot validation examples → Compute prediction losses for each candidate model → Calculate softmax weights → Perform weighted parameter averaging → Validate merged model on target and general tasks

- Design tradeoffs: More models in merging → potentially better general performance but higher computation; More few-shot examples → potentially better weights but defeats simplicity goal; Different α values → balance between specialist and generalist performance

- Failure signatures: If merged model performs worse than base model on general tasks → weights may be incorrectly computed; If merged model performs worse than fine-tuned model on target task → α may be too low or weights may be incorrect; If merging process fails → parameter dimensions may not match

- First 3 experiments:
  1. Verify basic merging: Merge a fine-tuned model with its base model using α=0.5 and check if performance on target task is maintained while general performance improves
  2. Test weight computation: Use 5-shot examples to compute weights and verify they align with expected model quality
  3. Validate architecture compatibility: Apply the same merging process to both decoder and encoder models to confirm universal applicability

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- The method's effectiveness with fewer than 5 examples remains unexplored
- Performance in extreme fine-tuning scenarios with very large domain shifts is untested
- Scalability to models with very different sizes or architectures has not been evaluated

## Confidence

**High Confidence**: The catastrophic forgetting problem in fine-tuned models is well-documented; the weighted averaging formulation is mathematically sound; the empirical results showing improved performance on both target and general tasks are well-supported by experimental data.

**Medium Confidence**: The claim that few-shot examples are sufficient for accurate weight computation; the assertion that the method works universally across decoder and encoder architectures; the preservation of in-context learning ability after merging.

**Low Confidence**: The method's effectiveness with fewer than 5 examples; performance in extreme fine-tuning scenarios (very large domain shift); scalability to models with very different sizes or architectures.

## Next Checks

1. **Sensitivity analysis on few-shot examples**: Systematically vary the number and quality of few-shot examples used for weight computation (1-shot, 3-shot, 10-shot) to determine the minimum viable examples needed and test robustness to example quality variations.

2. **Cross-architecture stress test**: Apply LM-Cocktail to merge models with significant architectural differences (different layer counts, attention mechanisms, or normalization schemes) to test the limits of parameter averaging compatibility.

3. **Domain transfer evaluation**: Evaluate the merged models on tasks from domains completely outside the training distribution of both base and fine-tuned models to assess true generalization capability and identify potential catastrophic forgetting in extreme scenarios.