---
ver: rpa2
title: Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate
  to Speech Emotion Recognition
arxiv_id: '2310.18877'
source_url: https://arxiv.org/abs/2310.18877
tags:
- speech
- speakers
- bias
- valence
- speat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Speech Embedding Association Test (SpEAT),
  a method for detecting bias in pre-trained speech processing models. SpEAT is inspired
  by word embedding association tests in NLP and quantifies intrinsic bias in a model's
  representations of different concepts like race or valence.
---

# Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2310.18877
- Source URL: https://arxiv.org/abs/2310.18877
- Reference count: 40
- Pre-trained speech models show systematic associations between demographic groups and valence that propagate to downstream emotion recognition

## Executive Summary
This paper introduces the Speech Embedding Association Test (SpEAT), a method for detecting human-like biases in pre-trained speech processing models. The authors test 16 English speech models across six types of bias (disability, race, gender, accent, and age) and find consistent positive valence associations favoring abled over disabled, European-American over African-American, female over male, U.S. accented over non-U.S. accented, and younger over older speakers. They demonstrate that these biases propagate to downstream speech emotion recognition models, with 69% of tests showing alignment between upstream associations and downstream valence predictions.

## Method Summary
The authors develop SpEAT to measure bias in speech model embeddings by comparing cosine similarities between target concept embeddings and positive versus negative valence attributes. For each target group (e.g., abled vs. disabled), they calculate a Cohen's d effect size showing how much more associated each group is with positive valence. They validate this approach through bootstrap analysis showing precision increases with more stimuli. To test bias propagation, they adapt pre-trained models to a speech emotion recognition task using the MESS dataset and compare upstream SpEAT associations with downstream valence predictions using Cohen's d.

## Key Results
- 14 or more models show positive valence associations with abled over disabled, European-American over African-American, female over male, U.S. accented over non-U.S. accented, and younger over older speakers
- SpEAT effect sizes show decreasing standard error as more stimuli are used to represent target concepts
- 69% of downstream bias tests show alignment between upstream valence associations and predicted valence
- Bias propagation is strongest when downstream tasks involve valence prediction rather than other emotional attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SpEAT captures human-like biases in speech models by measuring relative cosine similarities between embeddings of target and attribute concepts
- Mechanism: For each target stimulus, calculate mean cosine similarity to positive and negative valence attributes. The difference shows how much more associated the stimulus is with positive valence. Aggregating across all target stimuli gives the effect size d, which indicates the model's bias
- Core assumption: Cosine similarity between embeddings reflects meaningful associations learned by the model
- Evidence anchors:
  - [abstract] "The SpEAT is inspired by word embedding association tests in natural language processing, which quantify intrinsic bias in a model's representations of different concepts, such as race or valence"
  - [section 3] "The SpEAT d is given below, and shows how much closer the embeddings in X are to A than B, relative to the embeddings in Y"
- Break condition: If embeddings are not comparable across layers due to dynamic sizing, the cosine similarity comparison breaks down

### Mechanism 2
- Claim: Biases detected in pre-trained models propagate to downstream emotion recognition models when the downstream task involves valence prediction
- Mechanism: When adapting pre-trained models to downstream tasks, the learned embeddings (which contain biases) are used as features. If the downstream task is conceptually related (e.g., valence prediction), these biases influence predictions
- Core assumption: The task used for evaluation (Speech Emotion Recognition) is conceptually similar enough to the SpEAT to show bias propagation
- Evidence anchors:
  - [abstract] "we compare biases found in pre-trained models to biases in downstream models adapted to the task of Speech Emotion Recognition (SER) and find that in 66 of the 96 tests performed (69%), the group that is more associated with positive valence as indicated by the SpEAT also tends to be predicted as speaking with higher valence by the downstream model"
  - [section 5] "we find that SpEAT d values favoring one target group tend to align with speech from the favored target group being predicted as higher in valence"
- Break condition: If the downstream task is not related to valence, the bias propagation effect would be minimal

### Mechanism 3
- Claim: The number of stimuli used to represent target concepts affects the precision of the SpEAT effect size d
- Mechanism: Using bootstrapping, the standard error of the SpEAT d decreases as the number of stimuli increases. This means more stimuli lead to more reliable bias measurements
- Core assumption: Bootstrap estimates of standard error accurately reflect the uncertainty in the SpEAT d
- Evidence anchors:
  - [abstract] "we calculate bootstrap estimates of the Standard Error (SE) at different sample sizes... We find that the SE of the SpEAT decreases sharply as the number of stimuli used to represent social groups increases"
  - [section 5] "The SE of a statistic measures how much the statistic will vary if it is calculated repeatedly based on new data. Lower values indicate that the statistic will vary less, and that there is less associated uncertainty"
- Break condition: If the stimuli are not representative of the target concepts, increasing the number of stimuli may not improve precision

## Foundational Learning

- Cosine similarity
  - Why needed here: Used to measure the association between embeddings of target concepts and attribute concepts
  - Quick check question: If two embeddings have a cosine similarity of 1, what does this indicate about their relationship?

- Bootstrapping
  - Why needed here: Used to estimate the standard error of the SpEAT d at different sample sizes
  - Quick check question: What is the main advantage of using bootstrapping to estimate standard error?

- Cohen's d
  - Why needed here: Used as the effect size metric for the SpEAT, similar to its use in other association tests
  - Quick check question: What does a positive Cohen's d value indicate in the context of the SpEAT?

## Architecture Onboarding

- Component map: wav2vec 2.0/HuBERT/WavLM/Whisper models -> Embedding extraction -> Cosine similarity calculation -> SpEAT d computation -> Downstream SER model training -> Valence prediction

- Critical path: 1. Extract embeddings from pre-trained models for target and attribute stimuli 2. Aggregate embeddings to make them comparable 3. Calculate cosine similarities and SpEAT d 4. Adapt pre-trained models to SER task 5. Compare upstream and downstream biases

- Design tradeoffs:
  - Using mean across temporal dimension vs. min/max for embedding aggregation
  - Using embeddings from all layers vs. single layer
  - Balancing sample size for target concepts vs. resource constraints

- Failure signatures:
  - SpEAT d values close to zero across all models and concepts (no bias detected)
  - High standard error in SpEAT d, indicating low precision
  - No correlation between upstream and downstream biases

- First 3 experiments:
  1. Test SpEAT with a small number of stimuli to establish baseline functionality
  2. Compare SpEAT results using different embedding aggregation strategies
  3. Adapt a pre-trained model to a simple valence prediction task and compare biases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pre-trained speech models encode demographic information beyond valence associations?
- Basis in paper: [explicit] The paper mentions that the models learn associations based on speaking style, not just content, and raises questions about whether differential associations exist for text and image data based on authorship demographics
- Why unresolved: The paper focuses specifically on valence associations and doesn't explore other dimensions of demographic encoding in the embeddings
- What evidence would resolve it: Analysis of embeddings to identify correlations with demographic features (age, gender, accent) independent of valence ratings, using techniques like feature attribution or probing classifiers

### Open Question 2
- Question: Do biases in pre-trained speech models vary across different cultural contexts beyond U.S. and U.K. English?
- Basis in paper: [explicit] The paper only tests U.S. and U.K. English accents, noting that results may not generalize to other cultural contexts
- Why unresolved: The study is limited to English-speaking populations and doesn't explore biases in models trained on other languages or cultural contexts
- What evidence would resolve it: Replicating the SpEAT methodology with diverse language corpora and cultural contexts to compare bias patterns across different populations

### Open Question 3
- Question: What is the relative contribution of training data composition versus model architecture to bias formation in pre-trained speech models?
- Basis in paper: [inferred] The paper discusses potential data-driven origins of bias (co-occurrence in training data) but doesn't systematically isolate architectural factors
- Why unresolved: The study uses multiple model architectures but doesn't control for architectural differences when examining bias patterns
- What evidence would resolve it: Controlled experiments comparing identical training data processed by different model architectures, or training models with controlled demographic representation in data

## Limitations

- The tests rely on specific speech corpora that may not fully represent all speakers within each demographic category
- Bias measurements depend on the quality and representativeness of valence ratings in the MESS dataset
- The study only examines English speech models, limiting conclusions about other languages or multilingual models

## Confidence

- **High Confidence**: The SpEAT methodology itself and its ability to detect systematic associations between demographic groups and valence in pre-trained models
- **Medium Confidence**: The magnitude of bias effects reported, given potential sampling limitations in the speech corpora
- **Medium Confidence**: The claim that 69% of bias propagation tests showed alignment, as this depends on the specific downstream task and evaluation setup

## Next Checks

1. **Cross-validation with alternative valence datasets**: Repeat the SpEAT analysis using different speech datasets with valence annotations to verify the stability of detected biases across multiple data sources

2. **Bias measurement with demographic ground truth**: Obtain demographic labels for speakers in the MESS dataset (rather than relying on conceptual targets) and compare SpEAT results using actual speaker demographics versus the conceptual associations

3. **Ablation study on embedding aggregation**: Systematically vary the embedding aggregation method (temporal mean, max pooling, attention-weighted) and layer selection to determine how sensitive SpEAT results are to these architectural choices