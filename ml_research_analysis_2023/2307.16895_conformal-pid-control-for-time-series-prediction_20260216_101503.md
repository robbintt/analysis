---
ver: rpa2
title: Conformal PID Control for Time Series Prediction
arxiv_id: '2307.16895'
source_url: https://arxiv.org/abs/2307.16895
tags:
- conformal
- quantile
- coverage
- time
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conformal PID Control for Time Series Prediction This paper addresses
  the problem of uncertainty quantification in time series prediction, where data
  distributions can change over time due to seasonality, trends, or other factors.
  The authors propose a method called conformal PID control, which combines ideas
  from conformal prediction and control theory to provide formal guarantees on coverage
  while adapting to distribution shifts.
---

# Conformal PID Control for Time Series Prediction

## Quick Facts
- arXiv ID: 2307.16895
- Source URL: https://arxiv.org/abs/2307.16895
- Reference count: 40
- Primary result: Method combining conformal prediction and PID control theory for time series prediction with formal long-run coverage guarantees

## Executive Summary
This paper addresses uncertainty quantification in time series prediction under changing data distributions. The authors propose conformal PID control, which treats prediction set generation as a PID controller with quantile tracking, error integration, and scorecasting components. The method provides theoretical guarantees of long-run coverage under bounded scores and demonstrates improved performance on COVID-19 death forecasting, electricity demand, market returns, and temperature prediction.

## Method Summary
Conformal PID control combines three components to generate prediction sets for time series data. The quantile tracker (P control) uses online gradient descent on the quantile loss to adjust the conformal score threshold based on recent coverage errors. The error integrator (I control) accumulates coverage errors and uses a saturation function to stabilize long-term coverage. The scorecaster (D control) predicts future quantile levels from historical data to anticipate systematic trends. These components work together to maintain coverage while adapting to distribution shifts in the data.

## Key Results
- Theoretical guarantee of long-run coverage under bounded scores with no additional assumptions
- Improved coverage on 4-week-ahead COVID-19 death forecasting compared to official CDC ensemble forecaster
- Effective performance across electricity demand, market returns, and temperature forecasting with various base models
- Ability to correct systematic underprediction in COVID-19 deaths and anticipate intraday variations in electricity demand

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantile tracking through online gradient descent on the quantile loss achieves long-run coverage under bounded scores.
- Mechanism: The update rule qt+1 = qt + η(errt - α) adjusts the quantile estimate based on whether coverage was achieved (errt = 0) or not (errt = 1). When errt = 1 (miscoverage), the quantile increases to make future sets larger; when errt = 0 (coverage), the quantile decreases to make future sets smaller.
- Core assumption: Scores are bounded in [-b, b] for some finite b > 0.
- Evidence anchors:
  - [abstract]: "A theoretical guarantee of long-run coverage under no assumptions except boundedness of scores"
  - [section]: "Let {st}t∈N be any sequence of numbers in[-b, b], for 0 < b < ∞. Then the quantile tracking iteration (7) satisfies..."
  - [corpus]: Weak - no direct corpus evidence for this specific boundedness assumption.
- Break condition: If scores become unbounded (exceed ±b), the theoretical guarantee fails.

### Mechanism 2
- Claim: Error integration with a saturation function stabilizes coverage by preventing excessive corrections.
- Mechanism: The iteration qt+1 = rt(∑(erri - α)) uses a saturation function rt that limits how much the quantile can be adjusted based on cumulative coverage errors. This prevents wild swings in set sizes while still correcting systematic miscoverage.
- Core assumption: The saturation function rt satisfies the conditions in (4) for an admissible function h.
- Evidence anchors:
  - [abstract]: "Experiments on 4-week-ahead COVID-19 death forecasting showing improved coverage over the ensemble forecaster"
  - [section]: "Assume that rt satisfies (4), for an admissible function h. Then the error integration iteration(9) satisfies..."
  - [corpus]: Weak - no direct corpus evidence for the specific saturation function design.
- Break condition: If the saturation function is poorly designed (e.g., too aggressive or too weak), coverage stability may be compromised.

### Mechanism 3
- Claim: Scorecasting anticipates systematic trends in scores by predicting the next quantile from past data.
- Mechanism: A scorecaster model ˆqt+1 predicts qt+1 using past forecasts, cases, and deaths from all 50 US states. This allows the method to learn from patterns in other states to adjust predictions for the current state.
- Core assumption: There are systematic trends in scores that can be learned from historical data.
- Evidence anchors:
  - [abstract]: "Experiments on 4-week-ahead COVID-19 death forecasting showing improved coverage over the ensemble forecaster"
  - [section]: "the scorecaster has a seasonality component built into its prediction model"
  - [corpus]: Weak - no direct corpus evidence for the specific scorecasting approach.
- Break condition: If there are no systematic trends in scores or the scorecaster is poorly designed, it may add variance without benefit.

## Foundational Learning

- Concept: Conformal prediction for uncertainty quantification
  - Why needed here: Provides the theoretical foundation for generating prediction sets with formal coverage guarantees
  - Quick check question: What is the key difference between standard conformal prediction and online conformal prediction?

- Concept: PID control theory
  - Why needed here: Provides the framework for combining proportional (P), integral (I), and derivative (D) control components
  - Quick check question: How does the "integral" component in PID control relate to error integration in conformal prediction?

- Concept: Online learning and regret minimization
  - Why needed here: Enables the quantile tracker to adapt to changing score distributions over time
  - Quick check question: What online learning algorithm is used for the quantile tracker?

## Architecture Onboarding

- Component map:
  - Base forecaster -> Point prediction
  - Conformal score function -> Forecast accuracy measurement
  - Quantile tracker (P control) -> Quantile adjustment based on recent coverage
  - Error integrator (I control) -> Cumulative coverage error correction
  - Scorecaster (D control) -> Next quantile prediction from historical data
  - Prediction set generator -> Final conformal prediction sets

- Critical path:
  1. Get new observation (xt, yt)
  2. Generate point forecast ft(xt)
  3. Compute conformal score st(xt, yt)
  4. Update quantile estimate using PID control
  5. Generate prediction set for next time step

- Design tradeoffs:
  - Learning rate η: Higher values react faster but may be more volatile; lower values are more stable but slower to adapt
  - Saturation function design: More aggressive saturation prevents large corrections but may be slower to correct systematic errors
  - Scorecaster complexity: More complex models may capture trends better but risk overfitting

- Failure signatures:
  - Coverage consistently below nominal level: May need higher learning rate or more aggressive saturation
  - Coverage consistently above nominal level: May need lower learning rate or less aggressive saturation
  - Very large or infinite prediction sets: May indicate poor learning rate choice or unbounded scores

- First 3 experiments:
  1. Test quantile tracking on a simple synthetic time series with bounded scores to verify basic coverage
  2. Test error integration on a time series with systematic miscoverage to verify stabilization
  3. Test scorecasting on a time series with clear trends to verify trend anticipation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different types of integrators (P, I, D) affect the long-run coverage guarantee under varying score distributions?
- Basis in paper: [explicit] The paper discusses the effects of P, I, and D control on coverage and set sizes in the experiments section.
- Why unresolved: The paper provides experimental results but does not offer a rigorous theoretical analysis of how the different integrators impact the long-run coverage guarantee under different score distributions.
- What evidence would resolve it: A theoretical analysis proving how each integrator affects the long-run coverage guarantee under different score distribution assumptions.

### Open Question 2
- Question: How does the choice of scorecaster model affect the performance of the conformal PID controller?
- Basis in paper: [explicit] The paper mentions that the choice of scorecaster can impact coverage and set sizes, and provides examples of using different models (Theta, ℓ1-penalized quantile regression).
- Why unresolved: The paper does not provide a systematic study of how different scorecaster models affect the performance of the conformal PID controller under various conditions.
- What evidence would resolve it: A comprehensive empirical study comparing the performance of the conformal PID controller with different scorecaster models on a range of data sets and scenarios.

### Open Question 3
- Question: How can the hyperparameters of the conformal PID controller (e.g., learning rates, integrator constants) be optimally tuned in an online fashion?
- Basis in paper: [explicit] The paper mentions that heuristics are used to set hyperparameters but suggests that more rigorous techniques could be developed.
- Why unresolved: The paper does not provide a method for optimally tuning the hyperparameters of the conformal PID controller in an online setting.
- What evidence would resolve it: A proposed method for online hyperparameter tuning that is theoretically justified and empirically validated.

## Limitations

- Theoretical guarantee relies on bounded score assumption, which may not hold in real-world scenarios with extreme outliers
- Limited empirical evidence comparing scorecasting performance against simpler alternatives
- Relatively small scale of experiments, particularly the COVID-19 forecasting with only 50 US states

## Confidence

**Confidence: Medium** for theoretical coverage guarantees. While the paper provides rigorous proofs for long-run coverage under bounded scores, the practical applicability depends on the assumption that scores remain bounded. In real-world scenarios with extreme outliers or adversarial conditions, this assumption may be violated, potentially undermining the theoretical guarantees.

**Confidence: Low** for scorecasting component effectiveness. The paper claims that scorecasting helps anticipate systematic trends, but provides limited empirical evidence comparing it against simpler alternatives. The COVID-19 experiments use a complex ℓ1-penalized quantile regression model without demonstrating whether simpler models would perform comparably.

**Confidence: Medium** for real-world applicability. The experiments show improved coverage on COVID-19 death forecasting and electricity demand prediction, but the paper doesn't extensively test performance on datasets with more severe distribution shifts or adversarial conditions.

## Next Checks

**Validation Check 1**: Test the method on synthetic time series with injected outliers and adversarial score patterns to evaluate robustness when the bounded score assumption is violated. Compare coverage performance against baseline conformal methods under these challenging conditions.

**Validation Check 2**: Conduct ablation studies on the scorecasting component by testing simpler alternatives (e.g., moving averages, exponential smoothing) against the complex ℓ1-penalized quantile regression model. Measure the marginal benefit of increased model complexity on coverage and efficiency.

**Validation Check 3**: Evaluate the method's performance on datasets with known severe distribution shifts (e.g., financial crisis periods, major weather events) to assess how well the PID controller adapts to abrupt changes versus gradual trends.