---
ver: rpa2
title: Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals
arxiv_id: '2303.00500'
source_url: https://arxiv.org/abs/2303.00500
tags:
- class
- attribution
- image
- classi
- attri-net
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Attri-Net, an inherently interpretable model
  for multi-label classification designed for medical imaging applications. The model
  generates class-specific counterfactual attribution maps using a conditional GAN
  framework, which are then used as features for a logistic regression classifier.
---

# Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals

## Quick Facts
- arXiv ID: 2303.00500
- Source URL: https://arxiv.org/abs/2303.00500
- Reference count: 22
- Key outcome: Attri-Net achieves state-of-the-art classification performance (AUCs of 0.74-0.94) while producing high-quality explanations with substantially higher class sensitivity scores (0.49-0.62) than baselines

## Executive Summary
This paper introduces Attri-Net, an inherently interpretable model for multi-label classification designed for medical imaging applications. The model generates class-specific counterfactual attribution maps using a conditional GAN framework, which are then used as features for a logistic regression classifier. Attri-Net was evaluated on three chest X-ray datasets (CheXpert, ChestX-ray8, and VindrCXR) and compared against five post-hoc explanation techniques and one inherently interpretable baseline. Results showed that Attri-Net achieved state-of-the-art classification performance while producing high-quality explanations with substantially higher class sensitivity scores than baselines.

## Method Summary
Attri-Net is an end-to-end framework that generates class-specific counterfactual attribution maps for multi-label classification. The model uses a conditional GAN with task-switching capabilities (via AdaIN layers) to produce residual counterfactual class attribution maps for each class independently. These attribution maps are then used as features for a logistic regression classifier with center loss. The model is trained using a combination of classification loss, Wasserstein GAN loss, L1 regularization, and center loss, optimized end-to-end with ADAM.

## Key Results
- Attri-Net achieved state-of-the-art classification performance with AUCs of 0.74-0.94 across three chest X-ray datasets
- Class sensitivity scores of 0.49-0.62 substantially outperformed all baseline methods
- The model successfully addressed the multi-label scenario where existing techniques struggle, providing distinct and clinically meaningful attribution maps for each class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attri-Net generates class-specific counterfactual attribution maps that are linearly separable and retain discriminative information
- Mechanism: The model uses a conditional GAN framework where Mc(x) learns to remove class-specific evidence by generating residual maps. These maps are then used as input features for a logistic regression classifier. The center loss further clusters positive and negative examples in feature space
- Core assumption: The residual maps produced by Mc(x) contain sufficient class-relevant information while being sparse enough for human interpretability
- Evidence anchors: [abstract], [section 2.1], [corpus]
- Break condition: If Mc(x) produces maps that are either too sparse (losing discriminative information) or too dense (losing interpretability), the classifier performance would degrade significantly

### Mechanism 2
- Claim: The task switching network with AdaIN layers enables effective multi-label counterfactual generation by treating each class as a separate task
- Mechanism: Each class c has a one-hot encoded task vector tc that is processed through a task embedding network and fed to AdaIN layers throughout the generator. This allows the same network to generate distinct attribution maps for different classes in separate forward passes
- Core assumption: Task-specific normalization parameters learned through AdaIN layers can effectively modulate the generator's behavior for different classes
- Evidence anchors: [section 2.1], [section 2.3], [corpus]
- Break condition: If AdaIN layers cannot properly separate task-specific behaviors, the attribution maps for different classes would become similar, reducing class sensitivity

### Mechanism 3
- Claim: The combination of adversarial loss and L1 regularization produces realistic counterfactuals while maintaining sparsity
- Mechanism: The Wasserstein GAN loss ensures generated counterfactuals are indistinguishable from real images without the class, while L1 regularization encourages sparsity. Different weights (α0=2, α1=1) are applied based on class presence
- Core assumption: The discriminator can effectively distinguish between real and fake counterfactuals, and the L1 penalty appropriately balances realism with sparsity
- Evidence anchors: [section 2.1], [section 2.3], [corpus]
- Break condition: If the discriminator fails to provide meaningful gradients or if the L1 regularization is too strong/weak, the attribution maps would either be unrealistic or not sparse enough for interpretability

## Foundational Learning

- Concept: Multi-label classification with C classes where each class can independently occur
  - Why needed here: The paper addresses the challenge that most explanation techniques were developed for binary or multi-class problems, not multi-label scenarios where multiple findings can co-occur
  - Quick check question: What is the key difference between multi-label and multi-class classification in terms of class independence?

- Concept: Counterfactual explanations and their role in interpretability
  - Why needed here: Attri-Net generates counterfactual attribution maps to identify image regions corresponding to medical findings, which is fundamentally different from post-hoc explanation methods
  - Quick check question: How do counterfactual explanations differ from post-hoc explanations in terms of faithfulness to the model's decision mechanism?

- Concept: Generative Adversarial Networks (GANs) and Wasserstein GAN formulation
  - Why needed here: The core of Attri-Net uses a conditional GAN framework to generate counterfactual images that are indistinguishable from real images without the target class
  - Quick check question: What is the key advantage of Wasserstein GAN over standard GAN formulations in terms of training stability?

## Architecture Onboarding

- Component map: Input → Task embedding network → AdaIN-modulated generator (Mc) → Attribution map → Logistic regression classifier with center loss → Prediction
- Critical path: Input → Task embedding → AdaIN-modulated generator → Attribution map → Logistic regression → Prediction
- Design tradeoffs:
  - Single-pass vs multi-pass generation: Multi-pass (separate forward passes for each class) was chosen over single-pass generation due to better class attribution in multi-label scenarios
  - Complexity of generator vs classifier: Complex generator with simple linear classifier chosen to ensure inherent interpretability
  - Adversarial training vs direct optimization: Adversarial training used to ensure realistic counterfactuals rather than heuristic approaches
- Failure signatures:
  - Low class sensitivity scores indicate attribution maps are not class-specific enough
  - Poor classification performance suggests attribution maps are losing discriminative information
  - Unrealistic counterfactuals indicate GAN training issues
  - Noisy attribution maps suggest problems with AdaIN layer implementation or task separation
- First 3 experiments:
  1. Verify task embedding network produces different embeddings for different classes by visualizing embeddings
  2. Test generator output with fixed task vectors to ensure class-specific behavior before full training
  3. Evaluate attribution map quality on a small validation set before full end-to-end training

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, based on the limitations and potential extensions, some implicit open questions include:
- How does Attri-Net's performance change when applied to different medical imaging modalities beyond chest X-rays, such as CT scans or MRI?
- How does the inclusion of additional clinical metadata (e.g., patient age, sex, medical history) affect Attri-Net's performance and interpretability?
- How does Attri-Net's performance compare to other inherently interpretable models in the multi-label classification scenario?

## Limitations
- Evaluation focused exclusively on chest X-ray datasets with five pathology classes each, limiting generalizability
- Comparison with post-hoc explanation methods may not fully capture their potential when optimized specifically for each task
- Reliance on class sensitivity as the primary interpretability metric may not fully capture clinical utility in real-world settings

## Confidence

**High Confidence**: The core mechanism of using conditional GANs with AdaIN layers for class-specific counterfactual generation is well-established in the literature and the architectural design follows standard practices. The classification performance improvements over baselines are substantial and statistically meaningful.

**Medium Confidence**: The interpretability claims based on class sensitivity scores are supported by the data, but the clinical relevance of these scores and their correlation with actual diagnostic utility requires further validation. The choice of hyperparameters and training procedure appears reasonable but could benefit from sensitivity analysis.

**Low Confidence**: The comparative analysis against post-hoc explanation methods may not represent their full potential, as these methods were not optimized for the specific multi-label chest X-ray task. The ablation study showing the importance of each component is limited and could be expanded.

## Next Checks
1. **Cross-domain validation**: Test Attri-Net on additional medical imaging datasets (e.g., dermatology, pathology slides) to assess generalizability beyond chest X-rays and five-class scenarios.

2. **Clinical expert evaluation**: Conduct a study with radiologists to assess whether the attribution maps align with their diagnostic reasoning and provide meaningful clinical insights beyond the quantitative class sensitivity metric.

3. **Ablation study expansion**: Systematically evaluate the contribution of each component (AdaIN layers, center loss, L1 regularization) through controlled experiments with varying hyperparameters to identify the most critical elements for both performance and interpretability.