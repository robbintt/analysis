---
ver: rpa2
title: Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor
  Attacks
arxiv_id: '2303.06854'
source_url: https://arxiv.org/abs/2303.06854
tags:
- attacks
- image
- poisoned
- data
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ROCLIP, a method to defend multimodal vision-language
  models like CLIP against targeted data poisoning and backdoor attacks during pre-training.
  The key idea is to break the association between poisoned image-caption pairs by
  using a large, varying pool of random examples and matching each image to the most
  similar caption in the pool (and vice versa) instead of their original caption.
---

# Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks

## Quick Facts
- arXiv ID: 2303.06854
- Source URL: https://arxiv.org/abs/2303.06854
- Reference count: 11
- Key outcome: Reduces targeted poisoning attack success rate from 93.75% to 12.5% and backdoor attacks to 0%

## Executive Summary
This paper proposes ROCLIP, a defense mechanism for multimodal vision-language models like CLIP against targeted data poisoning and backdoor attacks during pre-training. The key innovation is breaking the association between poisoned image-caption pairs by using a large, varying pool of random examples and matching each image to the most similar caption in the pool (and vice versa) instead of their original caption. This prevents poisoned examples from affecting the model since they are not similar to clean examples. The method also uses image and text augmentations to further strengthen the defense. Experiments show that ROCLIP maintains similar zero-shot performance compared to CLIP while significantly improving robustness against poisoning attacks.

## Method Summary
ROCLIP defends against poisoning attacks by modifying CLIP's training procedure to break the association between poisoned image-caption pairs. Instead of matching each image to its original caption, the method uses a large pool of random image-caption representations and matches each image to the most similar caption in the pool (and vice versa). This ensures that poisoned images are matched to clean captions and poisoned captions to clean images, preventing the adversarial pairing from being reinforced. The method also applies random augmentations to both images and text to further decorrelate poisoned samples. During training, a first-in-first-out queue maintains the pool of representations, which is updated each batch with the latest representations.

## Key Results
- Reduces targeted poisoning attack success rate from 93.75% to 12.5%
- Reduces backdoor attack success rate to 0%
- Improves linear probe accuracy by 10% while maintaining similar zero-shot performance compared to CLIP

## Why This Works (Mechanism)

### Mechanism 1
Poisoned image-caption pairs remain poorly aligned in representation space during training because the similarity between clean pairs grows rapidly early in training while similarity between poisoned pairs grows slowly or remains low. This misalignment means poisoned samples do not cluster with clean ones, and gradient contributions from poisoned pairs are much smaller than from clean pairs.

### Mechanism 2
Matching each image to the most similar caption in a large, varying pool breaks the association between poisoned pairs. Instead of matching an image to its own caption, it is matched to the nearest neighbor in a randomly sampled pool. Poisoned images therefore match clean captions, and poisoned captions match clean images, preventing the adversarial pairing from being reinforced.

### Mechanism 3
Random augmentations on both images and text further prevent poisoned pairs from being matched together. Data augmentation changes the representations so that even if a poisoned sample enters the pool, its augmented version is less likely to be the nearest neighbor of another poisoned sample.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: ROCLIP builds directly on CLIP's contrastive loss; understanding how positive and negative pairs are formed is essential to see why breaking associations stops poisoning.
  - Quick check question: In CLIP's loss, which pairs are pulled together and which are pushed apart?

- Concept: Nearest neighbor matching in representation space
  - Why needed here: ROCLIP replaces the original image-caption pair with the nearest neighbor in a pool; knowing how nearest neighbor search works in high-dimensional space is key.
  - Quick check question: If a pool contains both clean and poisoned samples, what determines which sample an image is matched to?

- Concept: Data augmentation in self-supervised learning
  - Why needed here: ROCLIP uses strong augmentations to decorrelate poisoned samples; understanding augmentation policies helps tune the defense.
  - Quick check question: How do different augmentations (e.g., color jitter vs. EDA) affect similarity between representations?

## Architecture Onboarding

- Component map: CLIP backbone (image encoder + text encoder) -> Projection heads -> Pool queue (P image-caption representations) -> Augmentation pipelines (image + text) -> Nearest neighbor matcher per batch -> Loss function (modified InfoNCE)
- Critical path: 1. Sample pool from latest batch representations 2. Apply augmentations to incoming batch 3. Find nearest neighbor in pool for each augmented sample 4. Compute modified contrastive loss 5. Update model and append to pool queue
- Design tradeoffs: Pool size P vs. memory and compute, frequency of pool updates vs. defense strength, augmentation strength vs. clean performance
- Failure signatures: Attack success rate remains high (>10%), clean accuracy drops significantly (>5%), pool queue contains many repeated samples
- First 3 experiments: 1. Run ROCLIP with a very small pool (e.g., 100 samples) and measure attack success vs. clean accuracy 2. Disable augmentations and test if attack success increases 3. Vary the pool update frequency (every batch vs. every 5 batches) and observe the tradeoff between defense and performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several unresolved issues emerge from the analysis.

## Limitations
- The effectiveness depends on having a sufficiently large pool to prevent poisoned samples from repeatedly matching each other
- The paper does not explore whether adaptive attackers could design poisoning strategies that circumvent the nearest-neighbor matching defense
- The claim of improved linear probe accuracy needs more context about whether this comes from the defense mechanism itself or other training modifications

## Confidence
High Confidence: ROCLIP reduces backdoor attack success to 0% and maintains zero-shot performance
Medium Confidence: ROCLIP reduces targeted poisoning attack success from 93.75% to 12.5% and improves linear probe accuracy by 10%
Low Confidence: The method is broadly applicable without hyperparameter tuning given sensitivity to pool size and augmentation parameters

## Next Checks
1. Systematically vary the pool size from 100 to 10,000 samples and measure the tradeoff between attack success rate reduction and clean accuracy degradation
2. Design poisoning attacks that specifically target the nearest-neighbor matching mechanism to test whether ROCLIP can withstand adaptive attacks
3. Track the similarity evolution between clean and poisoned pairs throughout training to validate the temporal alignment dynamics claim