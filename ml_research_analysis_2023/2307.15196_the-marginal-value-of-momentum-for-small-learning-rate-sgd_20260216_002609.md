---
ver: rpa2
title: The Marginal Value of Momentum for Small Learning Rate SGD
arxiv_id: '2307.15196'
source_url: https://arxiv.org/abs/2307.15196
tags:
- learning
- momentum
- sgdm
- lemma
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical and empirical evidence that momentum
  in SGD offers limited benefits when the optimal learning rate is small. Theoretically,
  the authors show that in the regime of small learning rates, SGD with and without
  momentum converge to the same limiting distribution over both short and long time
  horizons.
---

# The Marginal Value of Momentum for Small Learning Rate SGD

## Quick Facts
- arXiv ID: 2307.15196
- Source URL: https://arxiv.org/abs/2307.15196
- Reference count: 40
- Primary result: SGD with and without momentum converge to the same limiting distribution in small learning rate regimes

## Executive Summary
This paper investigates the role of momentum in stochastic gradient descent (SGD) when learning rates are small. Through theoretical analysis and empirical experiments, the authors demonstrate that momentum provides limited benefits in the small learning rate regime, contrary to conventional wisdom. They show that SGD and SGD with momentum (SGDM) behave similarly in both short and long time horizons, converging to the same limiting distribution over the manifold of local minimizers.

## Method Summary
The authors develop theoretical characterizations of SGD and SGDM dynamics in the small learning rate regime using stochastic modified equations (SME). They prove that both algorithms approximate each other in O(1/η) steps and converge to the same slow SDE on the manifold of local minimizers in O(1/η²) steps. Empirically, they conduct experiments on ImageNet with ResNet-50 across different batch sizes, CIFAR-10 with ResNet-32, and language model fine-tuning tasks using RoBERTa-large. The experiments compare SGD and SGDM performance with grid searches for optimal learning rates and various batch sizes.

## Key Results
- SGD and SGDM show comparable performance in small- to medium-batch regimes with small learning rates
- Both algorithms converge to the same limiting distribution over local minimizers in the small learning rate regime
- Performance gain of SGDM over SGD in large-batch training can be reduced by controlling curvature-induced instability
- The variance reduction effect of momentum is outweighed by accumulated noise correlations in the small learning rate regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum does not reduce noise variance in a way that changes the limiting distribution when learning rates are small
- Mechanism: While momentum reduces per-step variance by factor (1-β)/(1+β), the noise is carried over to subsequent steps, creating long-range correlations that accumulate over time
- Core assumption: Noise covariance matrix Σ(θ) is independent of noise scale σ
- Evidence anchors: Abstract states SGD and SGDM behave similarly in short and long time horizons; variance analysis shows final endpoint variance bounded by 2η²/(1-β)

### Mechanism 2
- Claim: Both algorithms converge to the same slow SDE on the manifold of local minimizers in the small learning rate regime
- Mechanism: The momentum buffer in SGDM behaves like an Ornstein-Uhlenbeck process with mixing variance small compared to SGD noise
- Core assumption: Loss function has a manifold of local minimizers Γ that both algorithms converge to
- Evidence anchors: Abstract states similar behavior in short and long time horizons; limiting diffusion analysis shows same form for both algorithms

### Mechanism 3
- Claim: SGDM's performance gain in large-batch training can be reduced by controlling curvature-induced instability
- Mechanism: Momentum stabilizes training by reducing oscillations along high-curvature directions, not by noise reduction
- Core assumption: Curvature of loss landscape is primary source of instability in large-batch training
- Evidence anchors: Abstract mentions performance gain reduction through curvature control; SVAG experiments show reduced instability while maintaining noise

## Foundational Learning

- **Stochastic Gradient Descent (SGD)**: Baseline optimization algorithm against which SGDM is compared; understanding its dynamics is crucial for analyzing momentum's role
  - Quick check: What is the update rule for SGD, and how does it differ from SGDM?

- **Stochastic Modified Equations (SME)**: Mathematical framework used to characterize limiting dynamics of SGD and SGDM in small learning rate regime
  - Quick check: What SDE characterizes SGD's limiting dynamics, and how does it differ from SGDM's?

- **Manifold of Local Minimizers**: Set of points where loss function has local minima; critical for understanding slow SDE regime and generalization behavior
  - Quick check: Why is the manifold of local minimizers important for understanding SGD and SGDM generalization?

## Architecture Onboarding

- **Component map**: Noisy Gradient Oracle with Scale Parameter (NGOS) -> Stochastic Gradient Descent (SGD) -> Stochastic Modified Equations (SME) -> Manifold of Local Minimizers -> Slow SDE Regime

- **Critical path**:
  1. Define NGOS and update rules for SGD and SGDM
  2. Characterize limiting dynamics using SMEs in small learning rate regime
  3. Show convergence to same slow SDE on manifold of local minimizers
  4. Verify theoretical results with ImageNet and language model experiments

- **Design tradeoffs**: Learning rate and momentum coefficient choices affect convergence and generalization; noise scale impacts limiting dynamics; manifold structure affects slow SDE characterization

- **Failure signatures**: Different limiting dynamics when learning rate not small enough; variance reduction leading to different distributions if noise covariance depends on scale; inapplicable slow SDE regime if manifold doesn't exist

- **First 3 experiments**:
  1. Train ResNet-50 on ImageNet with SGD and SGDM across batch sizes (1024, 2048, 4096, 8192) with specified learning rate schedule
  2. Train ResNet-32 on CIFAR-10 with SGD and SGDM (with and without SVAG) using batch size 512
  3. Fine-tune RoBERTa-large on five downstream tasks using SGD and SGDM with grid search and few-shot setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does convergence behavior differ when learning rate schedule is non-constant with significant variations over time?
- Basis: Paper focuses on constant learning rate schedules
- Why unresolved: Only considers constant learning rates without exploring non-constant schedules
- What evidence would resolve it: Experimental comparison of SGD and SGDM with various non-constant learning rate schedules

### Open Question 2
- Question: What is the impact of momentum coefficient β on SGDM convergence in small learning rate regime?
- Basis: Paper mentions β is related to learning rate choice
- Why unresolved: Does not investigate specific impact of different momentum coefficients
- What evidence would resolve it: Experimental comparison of SGDM with different momentum coefficients in small learning rate regime

### Open Question 3
- Question: How does noise scale σ affect SGD and SGDM convergence in small learning rate regime?
- Basis: Mentions noise scale set inversely proportional to learning rate η
- Why unresolved: Does not investigate different noise scale effects
- What evidence would resolve it: Experimental comparison of SGD and SGDM with different noise scales in small learning rate regime

## Limitations

- Theoretical analysis relies heavily on small learning rate assumption which may not hold in all practical scenarios
- Mathematical characterization of manifold of local minimizers and convergence conditions may not fully capture real-world optimization landscapes
- Results may not generalize to all deep learning problems due to specific assumptions about loss landscape and noise characteristics

## Confidence

- **High Confidence**: Empirical demonstration of comparable SGD and SGDM performance in small- to medium-batch regimes with clear experimental protocols
- **Medium Confidence**: Theoretical claims about O(1/η) and O(1/η²) convergence rates due to dependencies on specific assumptions about loss landscape
- **Medium Confidence**: Mechanism explanation about curvature stabilization versus noise reduction requires careful interpretation of experimental results

## Next Checks

1. Systematically vary learning rate to identify threshold where SGD and SGDM diverge in limiting distributions, testing theoretical assumption boundaries
2. Experiment with different architectures and datasets to determine sensitivity of convergence behavior to loss landscape geometry
3. Investigate how different momentum coefficients affect noise reduction and curvature stabilization mechanisms across parameter space