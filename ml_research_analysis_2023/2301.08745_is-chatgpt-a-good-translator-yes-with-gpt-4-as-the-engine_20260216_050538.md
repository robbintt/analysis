---
ver: rpa2
title: Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine
arxiv_id: '2301.08745'
source_url: https://arxiv.org/abs/2301.08745
tags:
- translation
- chatgpt
- machine
- languages
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report evaluates ChatGPT for machine translation tasks, examining
  translation prompts, multilingual translation, and robustness. ChatGPT was tested
  using three different prompts, with TP3 performing best.
---

# Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine

## Quick Facts
- arXiv ID: 2301.08745
- Source URL: https://arxiv.org/abs/2301.08745
- Authors: 
- Reference count: 4
- Key outcome: ChatGPT with GPT-4 engine performs competitively with commercial systems on high-resource European languages but lags significantly on low-resource pairs and domain-specific tasks.

## Executive Summary
This study evaluates ChatGPT's translation capabilities across 12 language directions among German, English, Romanian, and Chinese. The research tests three different prompt formats and finds that TP3 performs best. While ChatGPT achieves competitive performance with commercial systems like Google Translate on high-resource European language pairs, it shows substantial gaps on low-resource languages (46.4% lower BLEU for English→Romanian) and domain-specific texts like biomedical abstracts. The study also explores pivot prompting for distant language pairs and finds it noticeably improves performance.

## Method Summary
The study evaluates ChatGPT using three prompts (TP1, TP2, TP3) on Flores-101 test sets across 12 translation directions. Translation quality is measured using BLEU, ChrF++, and TER metrics and compared against commercial systems (Google Translate, DeepL, Tencent Transmart). The evaluation covers high-resource European languages, low-resource/distant languages, and translation robustness across biomedical abstracts, Reddit comments, and spoken language data. The study specifically compares GPT-3.5 and GPT-4 engine performance.

## Key Results
- ChatGPT with GPT-4 engine performs competitively with commercial translation systems on high-resource European language pairs (German↔English)
- Significant performance gaps exist for low-resource language pairs, with 46.4% lower BLEU score than Google Translate for English→Romanian
- ChatGPT outperforms commercial systems on spoken language translation tasks (WMT20 Rob3) but underperforms on biomedical abstracts and Reddit comments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 engine significantly improves ChatGPT's translation quality over GPT-3.5
- Mechanism: GPT-4's superior language modeling capabilities, larger context windows, and better multilingual understanding directly enhance translation performance
- Core assumption: Engine upgrade from GPT-3.5 to GPT-4 provides substantial improvements in language modeling quality that generalize to translation tasks
- Evidence anchors:
  - [abstract]: "With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages."
  - [section]: "ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors."
- Break condition: If tasks require domain-specific knowledge not in GPT-4's training data or if prompt engineering is suboptimal

### Mechanism 2
- Claim: Translation quality varies based on language resource availability and family proximity
- Mechanism: ChatGPT leverages training data distribution - better performance on high-resource European languages due to more training examples and better on same-family translations due to shared linguistic structures
- Core assumption: ChatGPT's training data reflects real-world language distribution patterns
- Evidence anchors:
  - [abstract]: "ChatGPT performs competitively with commercial translation products on high-resource European languages but lags behind significantly on low-resource or distant languages."
  - [section]: "By comparing German⇔English with Chinese ⇔English or German⇔Chinese translation, we find that the gap between ChatGPT and the commercial systems becomes larger."
- Break condition: If model has been fine-tuned specifically on low-resource languages or prompting strategies compensate for data scarcity

### Mechanism 3
- Claim: Pivot prompting improves translation for distant language pairs
- Mechanism: Translating through high-resource pivot languages reduces compounding errors in direct distant language translation
- Core assumption: Quality of translation through pivot language is higher than direct translation between distant languages
- Evidence anchors:
  - [abstract]: "we explore an interesting strategy named pivot prompting for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably."
- Break condition: If pivot language introduces significant semantic drift or computational overhead outweighs quality gains

## Foundational Learning

- Concept: Prompt engineering for large language models
  - Why needed here: ChatGPT requires specific prompt formats to trigger translation capabilities effectively
  - Quick check question: What are the three candidate prompts tested in the study, and which one performed best?

- Concept: Language resource hierarchy and its impact on NLP performance
  - Why needed here: Understanding why ChatGPT performs differently across language pairs based on available training data
  - Quick check question: Which language pair showed the largest performance gap between ChatGPT and commercial systems, and why?

- Concept: Translation evaluation metrics (BLEU, ChrF++, TER)
  - Why needed here: The study uses multiple metrics to evaluate translation quality comprehensively
  - Quick check question: What are the three metrics used in this study, and what does each measure?

## Architecture Onboarding

- Component map: ChatGPT architecture -> Prompt processing -> Translation generation -> Evaluation pipeline (BLEU/ChrF++/TER) -> Comparison with baselines (Google Translate, DeepL, Tencent Transmart)
- Critical path: Prompt design -> Translation execution -> Quality evaluation -> Performance comparison
- Design tradeoffs: Single-model approach (versatility) vs. specialized translation systems (domain expertise), resource constraints vs. performance
- Failure signatures: Significant BLEU score drops for low-resource pairs, increased hallucination rates with GPT-3.5, poor performance on domain-specific text
- First 3 experiments:
  1. Replicate the three-prompt comparison on a new language pair to validate prompt engineering findings
  2. Test pivot prompting strategy on a low-resource distant language pair to verify improvement claims
  3. Compare GPT-3.5 vs GPT-4 performance on the same translation tasks to quantify engine upgrade benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms enable ChatGPT to outperform commercial systems on spoken language translation tasks like WMT20 Rob3?
- Basis in paper: [explicit] The paper notes ChatGPT outperforms commercial systems on Rob3 (speech recognition corpus) but doesn't explain why
- Why unresolved: The authors speculate it's because ChatGPT is an "artificial intelligent chatting machine" but don't provide technical analysis
- What evidence would resolve it: Detailed analysis comparing tokenization, language modeling, or training data differences between ChatGPT and commercial systems for speech translation tasks

### Open Question 2
- Question: How does the historical context impact ChatGPT's translation quality, and can iterative refinement improve performance?
- Basis in paper: [explicit] The authors suggest future work should investigate "the impact of historical context on translation results and iterative refinement of translation"
- Why unresolved: The paper only mentions this as future work without any preliminary investigation
- What evidence would resolve it: Controlled experiments comparing translation quality with and without historical context, or with iterative refinement enabled/disabled

### Open Question 3
- Question: What architectural modifications would be needed to make ChatGPT's translation performance comparable across all language pairs and domains?
- Basis in paper: [explicit] The paper identifies significant performance gaps for low-resource and distant language pairs, and for domain-specific tasks like biomedical translation
- Why unresolved: The paper identifies the problem but doesn't propose solutions beyond using GPT-4 engine
- What evidence would resolve it: Experimental results showing performance improvements from specific architectural changes (e.g., better parameter allocation, specialized adapters, or training strategies)

## Limitations
- Evaluation covers only 12 translation directions among four languages, representing a limited sample of global language landscape
- The study doesn't explore whether fine-tuning or specialized prompting strategies could bridge performance gaps for low-resource languages
- Pivot prompting effectiveness is mentioned in the abstract but lacks supporting evidence and detailed experimental results in the main text

## Confidence

**High Confidence**: ChatGPT with GPT-4 performs competitively with commercial systems on high-resource European language pairs (German↔English), supported by consistent BLEU score comparisons across multiple metrics.

**Medium Confidence**: GPT-4 significantly outperforms GPT-3.5 in translation quality, supported by error analysis showing reduced hallucinations and mistranslations, but lacks direct quantitative comparison data.

**Low Confidence**: The effectiveness of pivot prompting for distant language pairs is mentioned in the abstract but lacks supporting evidence in the main text, making practical value difficult to assess.

## Next Checks
1. Conduct controlled experiments testing the pivot prompting strategy on multiple low-resource distant language pairs with systematic comparison to direct translation, measuring both quality improvements and any semantic drift.

2. Expand evaluation to include additional low-resource language pairs beyond Romanian↔English, and systematically test whether prompt engineering variations or domain-specific fine-tuning can reduce performance gaps.

3. Design a head-to-head comparison study using identical prompts, test sets, and evaluation metrics to precisely quantify translation quality improvements from GPT-3.5 to GPT-4 across different language families and domains.