---
ver: rpa2
title: 'Through the Lens of Core Competency: Survey on Evaluation of Large Language
  Models'
arxiv_id: '2308.07902'
source_url: https://arxiv.org/abs/2308.07902
tags:
- language
- reasoning
- computational
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of evaluating large language
  models (LLMs) due to their excellent performance and wide range of applications.
  To tackle this, the authors propose a competency-based evaluation framework, summarizing
  four core competencies: reasoning, knowledge, reliability, and safety.'
---

# Through the Lens of Core Competency: Survey on Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2308.07902
- Source URL: https://arxiv.org/abs/2308.07902
- Reference count: 33
- Key outcome: The paper proposes a competency-based evaluation framework for LLMs, organizing diverse evaluation tasks into four core competencies: reasoning, knowledge, reliability, and safety.

## Executive Summary
This survey paper addresses the challenge of evaluating large language models (LLMs) by proposing a competency-based framework that organizes over 540 evaluation tasks into four core competencies: reasoning, knowledge, reliability, and safety. The framework aims to streamline the evaluation process by grouping similar tasks under unified competency categories, making it easier to assess model capabilities systematically. The authors provide comprehensive taxonomies for each competency, along with associated benchmarks and metrics, while also creating an extensible project to demonstrate the many-to-many relationship between competencies and tasks.

## Method Summary
The authors propose a competency-based evaluation framework for LLMs that organizes diverse evaluation tasks into four core competencies: reasoning, knowledge, reliability, and safety. They create a comprehensive taxonomy for each competency, mapping existing evaluation tasks and benchmarks to these categories. The framework is designed to be extensible, allowing new tasks to be systematically added by mapping them to appropriate competencies. The authors also develop an open-source project to demonstrate the relationships between competencies and tasks, providing a practical implementation of their theoretical framework.

## Key Results
- The framework organizes over 540 evaluation tasks into four core competencies, providing a structured approach to LLM evaluation
- Each competency has its own taxonomy with associated benchmarks and metrics, enabling systematic assessment
- The extensible project demonstrates the many-to-many relationship between competencies and tasks, supporting framework flexibility
- The competency-based approach reduces evaluation complexity while maintaining comprehensive coverage of model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping evaluation tasks under four core competencies reduces evaluation complexity and improves clarity
- Mechanism: The framework acts as a lens, mapping diverse tasks to unified competency categories. This aggregation allows evaluators to focus on ability dimensions rather than task proliferation, making the evaluation process more manageable
- Core assumption: That task diversity can be meaningfully distilled into a small set of core competencies without losing essential distinctions in evaluation quality
- Evidence anchors:
  - [abstract] "We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety... Under this competency architecture, similar tasks are combined to reflect corresponding ability..."
  - [section 2] "In this section, we introduce the definition and taxonomy of the core competencies we summarized."
- Break Condition: If new LLM capabilities emerge that do not fit into the four categories, the framework would need to be extended or restructured

### Mechanism 2
- Claim: The competency-based approach supports extensibilityâ€”new tasks can be added systematically without overhauling the framework
- Mechanism: Each competency has its own taxonomy and associated benchmarks. When a new task is proposed, it can be mapped to an existing competency, allowing seamless integration into the evaluation pipeline
- Core assumption: The taxonomies are sufficiently flexible to accommodate emerging tasks without requiring redefinition of competencies
- Evidence anchors:
  - [abstract] "Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system."
  - [section 3] "We also create an extensible project, which will show the many-to-many relationship between competencies and tasks precisely."
- Break Condition: If the competency-taxonomy mapping becomes inconsistent or ambiguous, adding new tasks could lead to confusion or misclassification

### Mechanism 3
- Claim: Mapping tasks to competencies clarifies evaluation intent and reduces cognitive load for evaluators
- Mechanism: By presenting evaluation results in terms of competencies, the framework reduces the need to understand every individual task. Evaluators can focus on overall ability profiles rather than detailed task performance
- Core assumption: Evaluators can interpret competency-level summaries without losing insight into task-level performance
- Evidence anchors:
  - [abstract] "Through this competency test, superabundant evaluation tasks and benchmarks are combed and clarified for their aiming utility."
  - [section 1] "Current evaluation benchmarks are mostly a mixture of the former three paradigms... But the significance of marginal increases in model effects on tasks with excellent performance is debatable."
- Break Condition: If competency-level summaries obscure critical differences between tasks, the framework could mislead evaluators

## Foundational Learning

- Concept: Competency-based evaluation
  - Why needed here: The paper builds its entire evaluation strategy around grouping tasks by competencies. Understanding this concept is foundational to interpreting the framework
  - Quick check question: Can you explain why grouping tasks by competencies might reduce the complexity of evaluating LLMs?

- Concept: Task-taxonomy mapping
  - Why needed here: The framework relies on mapping tasks to competencies. This mapping is essential for the framework's extensibility and clarity
  - Quick check question: How would you determine which competency a new evaluation task belongs to?

- Concept: Benchmark selection and evaluation metrics
  - Why needed here: The paper introduces specific benchmarks and metrics for each competency. Understanding these is necessary for practical application of the framework
  - Quick check question: What factors should be considered when choosing a benchmark for a specific competency?

## Architecture Onboarding

- Component map:
  - Competency definitions (reasoning, knowledge, reliability, safety) -> Task-taxonomy mapping rules -> Associated benchmarks and metrics per competency -> Extensible project repository for tracking mappings

- Critical path:
  1. Define competency scope and boundaries
  2. Map existing tasks to competencies
  3. Select or develop benchmarks and metrics
  4. Implement tracking system for task-competency mappings
  5. Validate framework with pilot evaluations

- Design tradeoffs:
  - Granularity vs. simplicity: More detailed competencies improve precision but increase complexity
  - Static vs. dynamic taxonomies: Fixed taxonomies are easier to manage but less adaptable to new tasks
  - Task-level vs. competency-level reporting: Detailed reporting is more informative but harder to synthesize

- Failure signatures:
  - Inconsistent task mapping across evaluators
  - Inability to accommodate new task types
  - Overlap or ambiguity between competencies

- First 3 experiments:
  1. Map 10 existing LLM evaluation tasks to competencies and validate consistency across evaluators
  2. Implement a simple tracking system for task-competency mappings
  3. Conduct a pilot evaluation using competency-level summaries and compare results to task-level analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific metrics or benchmarks can be developed to effectively measure the reliability of LLMs in free-form generation scenarios?
- Basis in paper: [inferred] The paper mentions that while LLMs perform well on structured tasks, their reliability in free-form generation remains a significant challenge, particularly in terms of calibration and uncertainty estimation
- Why unresolved: Current evaluation methods primarily focus on structured tasks, and there is a lack of standardized metrics for assessing the reliability of LLMs in open-ended, creative, or conversational contexts
- What evidence would resolve it: Development and validation of new benchmarks that specifically test the reliability of LLMs in free-form generation, incorporating metrics for uncertainty estimation, calibration, and factual consistency

### Open Question 2
- Question: How can LLMs be effectively evaluated for their planning and reasoning abilities in complex, real-world scenarios?
- Basis in paper: [explicit] The paper highlights the potential of LLMs for planning and reasoning tasks but notes that current evaluation methods are limited to simple or structured problems
- Why unresolved: Existing benchmarks often fail to capture the complexity and nuance of real-world planning and reasoning tasks, making it difficult to assess the true capabilities of LLMs in these areas
- What evidence would resolve it: Creation of comprehensive, multi-domain benchmarks that simulate real-world planning and reasoning scenarios, incorporating both structured and unstructured data, and evaluating the ability of LLMs to generate actionable plans and reason through complex problems

### Open Question 3
- Question: What are the most effective strategies for aligning LLMs with human values and ethical considerations, and how can these strategies be evaluated?
- Basis in paper: [inferred] The paper discusses the importance of safety and ethical considerations in LLM evaluation but notes that current methods are insufficient and subjective
- Why unresolved: There is a lack of standardized approaches for evaluating the ethical and safety aspects of LLMs, and existing methods often rely on human judgment, which can be inconsistent and biased
- What evidence would resolve it: Development of objective, quantifiable metrics for evaluating the ethical and safety aspects of LLMs, incorporating diverse perspectives and scenarios, and validating these metrics through rigorous testing and human evaluation

## Limitations

- The framework's effectiveness relies heavily on the appropriateness of the four core competencies as a universal organizing principle, without empirical validation
- The extensibility claim remains theoretical without demonstrated examples of successfully integrating new task types into the framework
- The paper does not address potential overlaps between competencies or provide clear guidelines for resolving ambiguities in task mapping

## Confidence

- **High confidence** in the descriptive survey methodology and comprehensive literature coverage
- **Medium confidence** in the proposed four-competency framework as a practical organizational tool
- **Low confidence** in the extensibility claims without demonstrated implementation examples

## Next Checks

1. **Empirical validation**: Conduct a blind study where multiple evaluators independently map 20-30 diverse LLM tasks to the four competencies, measuring inter-rater agreement to assess the framework's consistency

2. **Extensibility test**: Attempt to integrate 5-10 newly published LLM evaluation tasks into the framework, documenting any challenges or required modifications to the competency definitions or taxonomies

3. **Benchmark coverage analysis**: Systematically evaluate whether the current benchmark selection adequately represents each competency by measuring the correlation between task-level performance and competency-level scores across multiple LLM models