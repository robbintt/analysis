---
ver: rpa2
title: 'Overthinking the Truth: Understanding how Language Models Process False Demonstrations'
arxiv_id: '2307.09476'
source_url: https://arxiv.org/abs/2307.09476
tags:
- accuracy
- labels
- layer
- demonstrations
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models process incorrect demonstrations
  during few-shot learning. The authors discover that models exhibit "overthinking"
  when given incorrect labels - early layers show similar performance for correct
  and incorrect demonstrations, but behavior diverges sharply at a "critical layer"
  where accuracy given incorrect demonstrations progressively decreases.
---

# Overthinking the Truth: Understanding how Language Models Process False Demonstrations

## Quick Facts
- **arXiv ID**: 2307.09476
- **Source URL**: https://arxiv.org/abs/2307.09476
- **Reference count**: 40
- **Key outcome**: Language models exhibit "overthinking" when given incorrect demonstrations - early layers process correct and incorrect demonstrations similarly, but late layers degrade performance by attending to and copying false information from previous demonstrations.

## Executive Summary
This paper investigates how language models process incorrect demonstrations during few-shot learning. The authors discover that models exhibit "overthinking" - early layers show similar performance for correct and incorrect demonstrations, but behavior diverges sharply at a "critical layer" where accuracy given incorrect demonstrations progressively decreases. They identify "false induction heads" in late layers that attend to and copy false information from previous demonstrations. Removing these heads (5 out of thousands) reduced the accuracy gap between correct and incorrect prompts by 38.3% on average across 14 datasets, with minimal impact on performance with correct demonstrations. This demonstrates that studying intermediate model computations can help understand and mitigate harmful imitation behaviors in language models.

## Method Summary
The paper studies how language models process few-shot demonstrations with correct versus incorrect labels using logit lens to decode intermediate layer predictions. The authors create prompts where demonstration labels are all correct or all incorrect (using cyclic permutation of classes), then analyze model behavior layer-by-layer to identify a "critical layer" where performance diverges. They compute prefix-matching scores to identify false induction heads that attend to incorrect labels in previous demonstrations, then ablate these heads to measure impact on the accuracy gap. The study uses 14 text classification datasets and multiple model architectures including GPT-J.

## Key Results
- Models show similar accuracy for correct and incorrect demonstrations in early layers, but accuracy diverges sharply at a "critical layer" where incorrect demonstrations progressively worsen
- False induction heads in late layers attend to incorrect labels from previous demonstrations and promote them, causing performance degradation
- Ablating 5 false induction heads (1% of heads) reduced the accuracy gap between correct and incorrect prompts by 38.3% on average across 14 datasets
- Early-exiting (decoding from intermediate layers) improves accuracy on incorrect demonstrations because later layers actively harm performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorrect demonstrations cause "overthinking" - later layers actively degrade performance by attending to and reproducing false information from previous demonstrations.
- **Mechanism:** Early layers encode similar representations for correct and incorrect demonstrations. At a "critical layer," behavior diverges - correct demonstrations continue improving while incorrect demonstrations worsen. Late attention heads ("false induction heads") attend to incorrect labels in previous demonstrations and promote them, causing this degradation.
- **Core assumption:** The model knows the correct answer early on but gets confused by late-stage processing that attends to incorrect demonstration patterns.
- **Evidence anchors:**
  - [abstract]: "early layers show similar performance for correct and incorrect demonstrations, but behavior diverges sharply at a 'critical layer' where accuracy given incorrect demonstrations progressively decreases"
  - [section]: "We find that correct and incorrect demonstrations yield similar accuracy at early stages of computation, until some 'critical layer' at which they sharply diverge. After the critical layer, performance improves given correct demonstrations but drops given incorrect demonstrations."
  - [corpus]: Weak - no direct mention in neighbor papers of overthinking phenomenon specifically.

### Mechanism 2
- **Claim:** False induction heads are a small set of attention heads that attend to and promote false labels from previous demonstrations, causing incorrect imitation.
- **Mechanism:** These heads have high "prefix-matching scores" - they attend strongly to labels following inputs from the same class as the current input. When ablated, they significantly reduce the accuracy gap between correct and incorrect prompts.
- **Core assumption:** False induction heads can be identified by their attention patterns to labels in previous demonstrations, and removing them will reduce false context-following.
- **Evidence anchors:**
  - [abstract]: "These are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking"
  - [section]: "Removing 5 such heads (1% of heads) reduced the accuracy gap between correct and incorrect prompts by an average of 38.3% over 14 datasets"
  - [corpus]: Weak - neighbor papers don't discuss specific attention head mechanisms for false demonstrations.

### Mechanism 3
- **Claim:** Early-exiting improves accuracy on incorrect demonstrations because later layers actively harm performance by overthinking.
- **Mechanism:** Decoding from intermediate layers shows that models perform better midway through processing when given incorrect demonstrations. This is because later layers attend to and amplify incorrect patterns rather than correcting them.
- **Core assumption:** The model's final layers are not optimizing for the few-shot task but are instead learning to attend to demonstration patterns, including incorrect ones.
- **Evidence anchors:**
  - [abstract]: "This lead us to study how incorrect imitations emerge over the course of the model's processing, and to look for the model components that cause them"
  - [section]: "Given incorrect demonstrations, decoding from earlier layers performs better than decoding from the final layer. For example, for GPT-J, using p16 (the first 16 layers) achieves a better accuracy than the full model on all but one dataset"
  - [corpus]: Weak - neighbor papers don't discuss early-exiting as a solution to incorrect demonstrations.

## Foundational Learning

- **Concept:** Few-shot learning and in-context learning
  - Why needed here: The paper studies how language models process few-shot demonstrations with correct vs incorrect labels. Understanding the few-shot learning paradigm is essential to grasp why incorrect demonstrations cause problems.
  - Quick check question: What is the difference between few-shot learning and fine-tuning in language models?

- **Concept:** Attention mechanisms and transformer architecture
  - Why needed here: The paper identifies specific attention heads ("false induction heads") that cause incorrect imitation. Understanding how attention works is crucial to understanding the mechanism.
  - Quick check question: How do attention heads in transformers attend to different parts of the input sequence?

- **Concept:** Calibration and accuracy metrics
  - Why needed here: The paper uses "calibrated accuracy" to measure model performance, which is different from standard accuracy. Understanding calibration is important for interpreting the results.
  - Quick check question: Why might standard accuracy be problematic for measuring few-shot classification performance?

## Architecture Onboarding

- **Component map:** Input → Early layers (similar representations) → Critical layer (divergence) → Late layers (false induction heads attend to incorrect labels) → Final output (degraded accuracy)
- **Critical path:** Input → Early layers (similar representations) → Critical layer (divergence) → Late layers (false induction heads attend to incorrect labels) → Final output (degraded accuracy)
- **Design tradeoffs:** Studying intermediate representations (logit lens) vs full model outputs, identifying specific harmful components vs general layer effects, ablation studies vs observational analysis
- **Failure signatures:** Accuracy gap between correct/incorrect demonstrations increases with number of demonstrations, early layers show similar performance for both types, late layers show overthinking behavior
- **First 3 experiments:**
  1. Replicate the critical layer analysis on a small dataset to identify where correct/incorrect demonstrations diverge
  2. Implement prefix-matching score calculation to identify potential false induction heads
  3. Perform ablation study on top-scoring heads to verify their causal role in overthinking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there alternative mechanisms besides false induction heads that could explain the overthinking phenomenon observed in the study?
- Basis in paper: [explicit] The authors acknowledge that their ablation study does not fully remove the accuracy gap between correct and incorrect demonstrations, suggesting other contributing factors.
- Why unresolved: The study focuses on false induction heads as a primary mechanism but does not exhaustively explore other potential causes of overthinking.
- What evidence would resolve it: A comprehensive investigation into other possible mechanisms, such as attention heads with different properties or interactions between model components, could help identify alternative explanations for overthinking.

### Open Question 2
- Question: How does the overthinking phenomenon generalize to other model architectures beyond autoregressive transformers?
- Basis in paper: [inferred] The study focuses on autoregressive transformer language models but does not explore other model architectures.
- Why unresolved: The research is limited to a specific type of model, leaving the generalizability of the findings to other architectures uncertain.
- What evidence would resolve it: Conducting similar experiments with different model architectures, such as recurrent neural networks or convolutional neural networks, could reveal whether overthinking is a universal phenomenon or specific to transformers.

### Open Question 3
- Question: What is the impact of the overthinking phenomenon on real-world applications of language models?
- Basis in paper: [explicit] The authors mention the potential for harmful imitation in language models, such as producing insecure code when prompted with vulnerable code.
- Why unresolved: The study focuses on controlled experimental settings and does not directly address the implications for real-world applications.
- What evidence would resolve it: Evaluating the performance of language models in real-world scenarios, such as code generation or text completion tasks, could provide insights into the practical consequences of overthinking and inform strategies for mitigating its negative effects.

## Limitations
- The specific identification of false induction heads as the primary causal mechanism has low confidence due to potential alternative explanations
- The ablation results show a 38.3% reduction in accuracy gap, but this could partly result from removing any late-layer heads rather than specifically targeting false information propagation
- The analysis focuses on text classification tasks, and it's unclear whether these mechanisms generalize to other few-shot learning domains

## Confidence
- **Medium confidence** for the overthinking phenomenon and critical layer discovery
- **Low confidence** for the specific false induction head mechanism
- **High confidence** in the practical implication that studying intermediate model computations can help mitigate harmful imitation behaviors

## Next Checks
1. Apply the false induction head identification and ablation methodology to non-classification few-shot tasks (e.g., arithmetic, code generation) to verify whether the mechanism generalizes beyond text classification.
2. Perform ablations on random late-layer heads (not just high prefix-matching score heads) to establish whether the 38.3% improvement is specifically due to targeting false information propagation or simply removing any late-layer processing.
3. Track the critical layer identification across different random seeds and prompt orderings to verify that the overthinking phenomenon is consistent and not an artifact of specific demonstration arrangements.