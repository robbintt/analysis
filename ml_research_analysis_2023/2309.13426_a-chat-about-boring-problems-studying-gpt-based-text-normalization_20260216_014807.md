---
ver: rpa2
title: 'A Chat About Boring Problems: Studying GPT-based text normalization'
arxiv_id: '2309.13426'
source_url: https://arxiv.org/abs/2309.13426
tags:
- normalization
- errors
- text
- kestrel
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of large language models (LLMs) for
  text normalization, a task traditionally considered ill-suited for language models.
  The authors argue that by combining self-consistency reasoning with linguistically-informed
  prompt engineering, LLMs can achieve text normalization error rates around 40% lower
  than top normalization systems.
---

# A Chat About Boring Problems: Studying GPT-based text normalization

## Quick Facts
- arXiv ID: 2309.13426
- Source URL: https://arxiv.org/abs/2309.13426
- Reference count: 0
- Primary result: GPT-4.0 achieves 94.4% accuracy on text normalization, 40% lower error rates than top normalization systems

## Executive Summary
This work explores using large language models (LLMs) for text normalization, demonstrating that GPT-4.0 can achieve 94.4% accuracy compared to 90.7% for previous state-of-the-art systems. The authors introduce a new error taxonomy and show that most GPT-TN errors are recoverable felicitous variations rather than true unrecoverable errors. Through self-consistency reasoning with majority voting over 20 sampled outputs, they achieve significant improvements in accuracy. The study reveals that GPT-based text normalization provides more accurate and higher quality normalizations than traditional rule-based methods, with minimal instances of unrecoverable errors.

## Method Summary
The method uses few-shot prompting with self-consistency reasoning to perform text normalization on the Google TN dataset (1.1 billion words from Wikipedia). The approach involves providing 2 samples per semiotic class with context, generating 20 completion outputs with temperature 0.5, and using majority voting to select the final normalization. The system is evaluated on both development and test sets using exact match accuracy and manual error categorization through a custom HTML tool. No fine-tuning is performed; the models are used as-is with carefully engineered prompts.

## Key Results
- GPT-4.0 achieves 94.4% accuracy, 40% lower error rates than top normalization systems
- Self-consistency voting improves accuracy from 90.7% to 92.1%
- Less than 1% of GPT-4.0 errors are unrecoverable, with most being infelicitous normalizations
- GPT-3.5-Turbo achieves 90.7% accuracy with self-consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-based text normalization outperforms traditional WFST systems by 40% through better handling of contextual felicitous variations
- Mechanism: Large language models leverage contextual understanding to produce multiple valid spoken forms of the same written text, capturing linguistic felicity that rule-based systems miss
- Core assumption: Contextual language understanding is more effective than deterministic rule matching for text normalization tasks
- Evidence anchors:
  - [abstract] "Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM based text normalization to achieve error rates around 40% lower than top normalization systems."
  - [section 3.2] "We see that among all models, infelicitous normalizations ('format') made up the vast majority of errors."
  - [corpus] Weak evidence - corpus mentions related ASR and G2P work but no direct comparisons to WFST

### Mechanism 2
- Claim: Self-consistency reasoning significantly improves GPT-TN accuracy by majority voting over multiple sampled outputs
- Mechanism: Generating 20 completion outputs with temperature 0.5 and using majority voting filters out idiosyncratic LLM behaviors and hallucinations, producing more stable results
- Core assumption: Multiple sampling with majority voting can effectively reduce random errors and idiosyncratic behaviors in LLM outputs
- Evidence anchors:
  - [section 2.3] "After self-consistency the accuracy improves to 92.1%."
  - [abstract] "Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM based text normalization to achieve error rates around 40% lower than top normalization systems."
  - [corpus] Weak evidence - corpus mentions related LLM work but no specific self-consistency implementations

### Mechanism 3
- Claim: The new error taxonomy reveals that most GPT-TN errors are recoverable felicitous variations rather than true unrecoverable errors
- Mechanism: By categorizing errors beyond binary correct/incorrect labels, the system can distinguish between infelicitous normalizations, paraphrases, and actual unrecoverable errors, providing more nuanced evaluation
- Core assumption: Traditional binary evaluation metrics misrepresent GPT-TN performance by not accounting for linguistic felicity and context-dependent validity
- Evidence anchors:
  - [section 2.1] "We believe TN requires a more nuanced error taxonomy beyond the binary of 'correct' and 'unrecoverable'."
  - [section 4] "We see that among all models, infelicitous normalizations ('format') made up the vast majority of errors. Actual 'unrecoverable' errors made up less than 1% of all total normalizations from the GPT-4.0 model."
  - [corpus] Weak evidence - corpus contains related NLP work but no specific error taxonomy research

## Foundational Learning

- Concept: Text normalization and semiotic classes
  - Why needed here: Understanding how written text converts to spoken form and the various contexts (semiotic classes) that affect pronunciation is fundamental to the problem being solved
  - Quick check question: What are three common examples of semiotic classes mentioned in the paper?

- Concept: Self-consistency and majority voting in LLM inference
  - Why needed here: The paper uses self-consistency reasoning to improve accuracy, which requires understanding how multiple sampling and voting work in LLM inference pipelines
  - Quick check question: How many completion outputs are generated and what temperature is used for self-consistency voting?

- Concept: Error taxonomy and linguistic felicity
  - Why needed here: The paper introduces a new error taxonomy that distinguishes between recoverable and unrecoverable errors based on linguistic felicity concepts
  - Quick check question: What are the six error categories introduced in the new taxonomy beyond the binary correct/incorrect labels?

## Architecture Onboarding

- Component map: Input text ‚Üí Prompt engineering with semiotic class context ‚Üí LLM inference with self-consistency sampling ‚Üí Majority voting ‚Üí Error categorization using custom taxonomy ‚Üí Output
- Critical path: Input text ‚Üí Prompt engineering with semiotic class context ‚Üí LLM inference with self-consistency sampling ‚Üí Majority voting ‚Üí Error categorization using custom taxonomy ‚Üí Output
- Design tradeoffs: Using more semiotic class examples improves accuracy but risks exceeding context limits (lost in the middle phenomenon). The error taxonomy provides nuanced evaluation but requires manual labeling
- Failure signatures: Accuracy degradation when semiotic class context is insufficient, performance drops with too many examples per class, and errors increase for specialized domain formatting
- First 3 experiments:
  1. Test different sampling methods (random pairs, semiotic token pairs with/without context) with varying numbers of examples per class
  2. Evaluate the impact of self-consistency voting by comparing accuracy with and without majority voting
  3. Compare GPT-TN performance against Kestrel on the development set using both automatic and manual evaluation methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the error taxonomy for text normalization be refined to capture more nuanced errors and improve system performance?
- Basis in paper: [explicit] The authors introduce a new taxonomy of text normalization errors and encourage future work to explore refinements to this taxonomy
- Why unresolved: The current taxonomy may not capture all possible error types, and further refinement is needed to develop more effective TN systems
- What evidence would resolve it: Conducting extensive error analysis on a diverse set of text normalization tasks and incorporating the findings into the taxonomy

### Open Question 2
- Question: What is the optimal number of examples per semiotic class for few-shot learning in GPT-based text normalization?
- Basis in paper: [explicit] The authors experiment with varying the number of samples per semiotic class (r=1,2,3) and find that both GPT-3.5-Turbo and GPT-4.0 deteriorate in performance with r=3 classes
- Why unresolved: The optimal number of examples per semiotic class may vary depending on the specific task and dataset, and further research is needed to determine the ideal configuration
- What evidence would resolve it: Conducting systematic experiments with different numbers of examples per semiotic class and evaluating the impact on text normalization accuracy

### Open Question 3
- Question: How can self-consistency reasoning be further improved to enhance the accuracy of GPT-based text normalization?
- Basis in paper: [explicit] The authors use self-consistency reasoning to improve the accuracy of GPT-3.5-Turbo and GPT-4.0, but note that there appears to be a saturation point of sample quantity
- Why unresolved: The effectiveness of self-consistency reasoning may be limited by the quality and diversity of the generated samples, and further improvements are needed to achieve higher accuracy
- What evidence would resolve it: Developing more sophisticated sampling strategies and incorporating additional context information into the self-consistency reasoning process

## Limitations
- Limited error analysis scope covering less than 0.01% of the total dataset
- Proprietary baseline comparison makes independent verification difficult
- Evaluation limited to Wikipedia data, generalization to other domains unknown

## Confidence
- High confidence (‚òÄÔ∏è) - GPT-TN accuracy improvement with self-consistency
- Medium confidence (üå§Ô∏è) - 40% error rate reduction claim
- Medium confidence (üå§Ô∏è) - Error taxonomy validity

## Next Checks
1. Conduct comprehensive error analysis on 10,000+ samples across multiple domains to validate the error taxonomy's generalizability
2. Attempt to replicate Kestrel-like WFST system performance on the same dataset using publicly available tools
3. Test GPT-TN performance on non-English text normalization tasks using multilingual models