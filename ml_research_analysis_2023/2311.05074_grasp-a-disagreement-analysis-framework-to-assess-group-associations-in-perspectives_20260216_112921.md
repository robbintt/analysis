---
ver: rpa2
title: 'GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives'
arxiv_id: '2311.05074'
source_url: https://arxiv.org/abs/2311.05074
tags:
- raters
- groups
- race
- agreement
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRASP, a framework for analyzing disagreement
  in human annotations across different demographic subgroups. It proposes a Group
  Association Index (GAI) that combines in-group cohesion and cross-group cohesion
  metrics, along with statistical significance testing using permutation tests.
---

# GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives

## Quick Facts
- arXiv ID: 2311.05074
- Source URL: https://arxiv.org/abs/2311.05074
- Reference count: 15
- Key outcome: Framework reveals systematic disagreement patterns along demographic lines, with White men showing highest in-group agreement and Asian women lowest.

## Executive Summary
GRASP is a framework for analyzing disagreement in human annotations across demographic subgroups, introducing a Group Association Index (GAI) that combines in-group cohesion and cross-group cohesion metrics. The framework uses permutation tests to assess statistical significance while accounting for dependencies between raters and items. Applied to a dataset of 350 chatbot conversations annotated by 104 diverse raters, GRASP revealed significant systematic disagreements along racial and gender lines, with White men showing the highest GAI (1.262) and Asian women the lowest (0.540).

## Method Summary
The framework computes in-group cohesion (CI) and cross-group cohesion (CX) metrics for different demographic subgroups, then calculates GAI by normalizing CI by CX. Permutation tests shuffle demographic labels while preserving rating assignments to generate null distributions for significance testing. The Diversity Sensitivity Index (DSI) aggregates maximum GAI values across demographic axes to identify which dimensions show the strongest systematic disagreement patterns.

## Key Results
- White men showed the highest GAI (1.262), indicating strongest in-group agreement
- Asian women showed the lowest GAI (0.540), indicating weaker in-group agreement relative to cross-group agreement
- Permutation testing revealed statistically significant systematic disagreement patterns across multiple demographic axes
- GAI values provide actionable insights for prioritizing demographic diversity in rater recruitment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework reveals systematic disagreement patterns by comparing in-group cohesion to cross-group cohesion
- Mechanism: The Group Association Index (GAI) normalizes group disagreement by dividing within-group agreement (CI) by agreement with outsiders (CX), making systematic biases visible that simple majority voting obscures
- Core assumption: Demographic subgroups have consistent internal agreement patterns that differ meaningfully from other groups
- Evidence anchors:
  - [abstract] "Our framework (based on disagreement metrics) reveals specific rater groups that have significantly different perspectives than others"
  - [section] "GAI provides a means for this assessment by telling not just that certain groups significantly differ from others in their ratings, but also quantify that association"
  - [corpus] Found 25 related papers with average neighbor FMR=0.374, showing moderate but non-trivial overlap with related disagreement research
- Break condition: When demographic groups show similar CI and CX values (GAI ≈ 1), indicating no systematic disagreement patterns

### Mechanism 2
- Claim: Permutation testing provides robust significance testing for group comparisons despite non-independent ratings
- Mechanism: Random shuffling of demographic labels while keeping rating assignments fixed creates a null distribution that accounts for dependencies between raters and items
- Core assumption: The null hypothesis of independence between demographic profiles and rating patterns can be approximated through permutation
- Evidence anchors:
  - [section] "We designed null hypothesis significance tests (NHSTs) to control for these dependencies using permutation tests"
  - [section] "Our position is that there is usually a degree of arbitrariness in their use, for instance, in the choice of level (e.g., p = 0.05, in our case)"
  - [corpus] Found related papers on modeling disagreement (2301.01579, 2508.02853) suggesting permutation approaches are relevant to this domain
- Break condition: When computational constraints prevent sufficient permutations or when rater-item dependencies violate permutation assumptions

### Mechanism 3
- Claim: The framework enables operational prioritization of demographic diversity in rater recruitment
- Mechanism: The Diversity Sensitivity Index (DSI) identifies which demographic axes show the strongest systematic disagreement patterns, allowing resource allocation to most impactful diversity dimensions
- Core assumption: Practitioners have limited resources for rater diversity and need quantitative guidance on which demographic axes to prioritize
- Evidence anchors:
  - [abstract] "helps identify demographic axes that are crucial to consider in specific task contexts"
  - [section] "knowing whether and by how much any subgroup within that axis has a significant GAI is more insightful than the average GAI value"
  - [corpus] Found papers on demographic-aware modeling (2508.02853) supporting the importance of demographic considerations in annotation
- Break condition: When no demographic axis shows significant GAI values across all groups, indicating disagreement is not systematically linked to demographics

## Foundational Learning

- Concept: Inter-rater reliability metrics (Krippendorff's alpha)
  - Why needed here: Provides statistical foundation for measuring agreement while controlling for class imbalance in binary safety ratings
  - Quick check question: How does Krippendorff's alpha differ from simple percent agreement in handling class imbalance?

- Concept: Permutation testing for non-independent observations
  - Why needed here: Standard significance tests assume independence, but ratings are nested within both raters and items
  - Quick check question: Why can't we use standard t-tests or ANOVA for comparing demographic group agreement patterns?

- Concept: Multiple comparison correction (Benjamini-Hochberg FDR)
  - Why needed here: Framework tests many demographic combinations, requiring control for false discovery rate
  - Quick check question: What's the tradeoff between Bonferroni correction and FDR control in exploratory analysis?

## Architecture Onboarding

- Component map: Annotation data → Metric computation → Permutation testing → Significance filtering → DSI calculation → Results interpretation
- Critical path: Annotation data → Metric computation → Permutation testing → Significance filtering → DSI calculation → Results interpretation
- Design tradeoffs: Permutation testing vs. analytical methods (computational cost vs. assumption control), multiple metrics vs. single unified metric (sensitivity vs. interpretability)
- Failure signatures: All GAI values near 1 (no demographic patterns detected), all p-values non-significant (insufficient power or no real effects), extreme computational time for large datasets
- First 3 experiments:
  1. Apply framework to synthetic data with known demographic patterns to verify detection capability
  2. Run on DICES dataset with different demographic groupings to identify most sensitive axes
  3. Test permutation test convergence by varying number of permutations and observing p-value stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more robust metrics for measuring rater disagreement that are less sensitive to class imbalance?
- Basis in paper: [explicit] The paper discusses disparities between IRR and other metrics like negentropy and plurality size, noting that IRR may overcompensate for class imbalance (2:1 for safe to unsafe ratings).
- Why unresolved: The paper highlights the need for better metrics but does not propose specific solutions or validate alternative approaches.
- What evidence would resolve it: Development and validation of new disagreement metrics that are robust to class imbalance, tested across multiple datasets with varying class distributions.

### Open Question 2
- Question: How can we distinguish between "good" disagreement (representing diverse perspectives) and "bad" disagreement (resulting from rater error or lack of training)?
- Basis in paper: [explicit] The paper acknowledges this limitation, stating "we recognize more work is needed to distinguish good from bad disagreement."
- Why unresolved: The paper focuses on identifying statistical patterns of disagreement but does not address the qualitative aspects of whether disagreement is meaningful or problematic.
- What evidence would resolve it: Development of a framework that incorporates both statistical significance and qualitative assessment of disagreement, validated through expert review of contentious cases.

### Open Question 3
- Question: How do intersectional demographic factors beyond race and gender (such as education, socioeconomic status, or cultural background) influence rater disagreement?
- Basis in paper: [explicit] The paper notes that "slicing further into the ethnicity, native languages and age groups of the raters is likely to reveal further insights" and that applying the framework to other intersectional groups "may reveal group associations that our current analysis do not."
- Why unresolved: The current analysis only examines race, gender, and age, leaving many potential demographic factors unexplored.
- What evidence would resolve it: Application of the GRASP framework to datasets with richer demographic information, including education, income, geographic location, and cultural background, to identify new patterns of systematic disagreement.

## Limitations
- Computational constraints of permutation testing for large datasets
- Potential sensitivity to demographic categorization choices
- Assumption that disagreement patterns are stable across different rating contexts

## Confidence
- Permutation testing approach: Medium-High
- GAI as actionable insight tool: Medium
- Generalizability across tasks: Low-Medium

## Next Checks
1. Test GRASP on synthetic datasets with known demographic disagreement patterns to verify detection accuracy and false positive rates
2. Compare GAI rankings across different demographic axes using bootstrap resampling to assess stability of results
3. Apply framework to an independent annotation dataset (different task/context) to evaluate generalizability claims