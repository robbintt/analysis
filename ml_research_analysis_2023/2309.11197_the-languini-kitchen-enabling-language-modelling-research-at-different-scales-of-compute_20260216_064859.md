---
ver: rpa2
title: 'The Languini Kitchen: Enabling Language Modelling Research at Different Scales
  of Compute'
arxiv_id: '2309.11197'
source_url: https://arxiv.org/abs/2309.11197
tags:
- language
- tokens
- books
- arxiv
- languini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Languini Kitchen addresses the challenge of enabling language
  modeling research at different scales of compute, particularly for researchers with
  limited computational resources. It introduces an experimental protocol that enables
  model comparisons based on equivalent compute, measured in accelerator hours.
---

# The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute

## Quick Facts
- arXiv ID: 2309.11197
- Source URL: https://arxiv.org/abs/2309.11197
- Reference count: 27
- Enables language modeling research at different scales of compute, particularly for researchers with limited computational resources

## Executive Summary
The Languini Kitchen addresses the challenge of enabling language modeling research at different scales of compute, particularly for researchers with limited computational resources. It introduces an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. This approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, it uses a large, diverse, and high-quality dataset of books, surpassing existing academic benchmarks in quality, diversity, and document length. The Languini Kitchen provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all compute levels, the LSTM baseline exhibits a predictable and more favorable scaling law due to its improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws of both models results in an intersection at roughly 50,000 accelerator hours.

## Method Summary
The Languini Kitchen introduces an experimental protocol that enables fair model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. This approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, it uses a large, diverse, and high-quality dataset of books, surpassing existing academic benchmarks in quality, diversity, and document length. The Languini Kitchen provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all compute levels, the LSTM baseline exhibits a predictable and more favorable scaling law due to its improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws of both models results in an intersection at roughly 50,000 accelerator hours.

## Key Results
- Introduces an experimental protocol enabling model comparisons based on equivalent compute (accelerator hours)
- Provides two baseline models: GPT-2 derived feed-forward model and novel LSTM with ten-fold throughput
- Demonstrates that LSTM baseline exhibits a more favorable scaling law than GPT baseline due to improved throughput

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Languini Kitchen enables fair model comparisons by constraining experiments to equivalent compute measured in accelerator hours.
- Mechanism: Instead of comparing models based on parameters or FLOPs, the protocol fixes a compute class (e.g., 6, 12, 24, 48, 96 hours) and defines the number of training tokens by the model's throughput. This means each model trains for the same real-world computational budget, making scaling trends directly comparable.
- Core assumption: Throughput measurements are accurate and reproducible across different hardware.
- Evidence anchors:
  - [abstract]: "The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling."
  - [section 3.1]: "A critical component of the Languini benchmark involves the concept of a compute class. This measure represents the number of accelerator hours (both parallel and sequential) spent during the training of the model."
  - [corpus]: Weak. The corpus neighbors discuss scaling laws broadly but do not specifically address accelerator-hour-based comparisons.
- Break condition: If hardware heterogeneity leads to inconsistent throughput measurements or if the throughput script fails to capture model-specific efficiency differences.

### Mechanism 2
- Claim: The Languini Books dataset provides a more challenging and diverse evaluation than prior benchmarks, exposing weaknesses in model generalization.
- Mechanism: By filtering books3 to remove short or non-English books, deduplicating near-duplicates, and creating out-of-distribution (OOD) splits (e.g., French learning, Discworld, Java, Statistics, Woodworking), the benchmark stresses models on long-range dependencies and domain shifts.
- Core assumption: The filtered dataset is large enough to avoid overfitting and diverse enough to represent real-world generalization challenges.
- Evidence anchors:
  - [section 3.2]: "The Languini Books dataset consists of 84.5 GB of text data across 158,577 books... Each book has on average 559 KB of text or about 150k tokens."
  - [section 3.2.1]: "We create several out of distribution test sets to measure a model's ability to capture long dependencies and learn during inference."
  - [corpus]: Missing. Corpus neighbors do not discuss dataset construction or generalization testing.
- Break condition: If the OOD splits are too small to provide statistically meaningful results or if filtering removes too much diversity.

### Mechanism 3
- Claim: The quasi-LSTM (qLSTM) baseline achieves a better scaling law than the GPT baseline due to higher throughput and fewer training tokens needed for the same perplexity improvement.
- Mechanism: The qLSTM removes recurrent weights and uses block-parallel computation, achieving 5-6x higher throughput than the LSTM and requiring fewer tokens to reach a given perplexity. This offsets its lower absolute throughput compared to GPT, leading to an intersection at ~50k accelerator hours.
- Core assumption: The block-parallel implementation of qLSTM is correct and the throughput gains are sustained across scales.
- Evidence anchors:
  - [abstract]: "While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law."
  - [section 4.3.1]: "The sequential part of the qLSTM... can be expanded over 4 steps... we can rewrite Eq. 30... such that ct can be computed in parallel."
  - [section 4.3.2]: "We find that the qLSTM models counter-balance their 5-6k times slower throughput with a faster convergence which requires fewer tokens than the GPT models to achieve a similar perplexity."
  - [corpus]: Weak. Corpus neighbors discuss scaling laws but not specifically the qLSTM vs GPT comparison.
- Break condition: If the block-parallel implementation introduces numerical instability or if the speedup does not scale with sequence length.

## Foundational Learning

- Concept: Compute-optimal training and scaling laws.
  - Why needed here: Understanding how model performance scales with compute is central to comparing GPT and qLSTM baselines.
  - Quick check question: What does it mean for a model to have a "favorable scaling law," and how is it measured in this work?

- Concept: Tokenization and its impact on perplexity.
  - Why needed here: Different vocabularies affect both model complexity and evaluation metrics; the work analyzes vocabularies from 2k to 131k tokens.
  - Quick check question: Why is normalized perplexity used instead of raw perplexity when comparing models with different tokenizers?

- Concept: Recurrent neural networks vs. Transformers.
  - Why needed here: The paper contrasts a recurrent baseline (qLSTM) with a feed-forward baseline (GPT), highlighting their architectural differences.
  - Quick check question: What are the main computational trade-offs between recurrence and attention in language modeling?

## Architecture Onboarding

- Component map: Data loading -> Model training -> Throughput measurement -> Compute class calculation -> Training with optimal batch size -> Evaluation with slow/fast eval -> Scaling law analysis
- Critical path:
  1. Measure throughput of a candidate model on reference hardware (RTX 3090)
  2. Compute affordable training steps for each compute class
  3. Train with optimal batch size and sequence length
  4. Evaluate using slow eval on held-out and OOD splits
  5. Plot scaling curves and compare baselines
- Design tradeoffs:
  - Tokenization: Larger vocabularies improve granularity but reduce throughput and increase parameters
  - Model size vs throughput: Bigger models have higher perplexity but lower throughput, requiring fewer training steps per compute class
  - Recurrence vs attention: Recurrence is more memory-efficient but slower per token; attention is parallelizable but quadratic in sequence length
- Failure signatures:
  - Throughput script underestimates real training speed → incorrect compute class mapping
  - OOM errors on smaller GPUs → model size too large for given hardware
  - Slow eval shows large gap vs fast eval → context length insufficient for accurate perplexity
  - qLSTM block implementation fails → recurrence reverts to sequential mode
- First 3 experiments:
  1. Run the throughput script on a small GPT model to verify baseline numbers
  2. Train a GPT tiny for 6h compute class with varying batch sizes to find optimal batch size
  3. Compare GPT tiny and qLSTM tiny at 6h to observe initial perplexity gap and throughput difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quasi-LSTM's superior scaling law performance translate to practical applications at scale?
- Basis in paper: [explicit] The paper states that the quasi-LSTM exhibits a predictable and more favorable scaling law due to improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws of both models results in an intersection at roughly 50,000 accelerator hours.
- Why unresolved: While the paper demonstrates the quasi-LSTM's superior scaling law, it does not provide evidence of its performance in practical, large-scale applications.
- What evidence would resolve it: Empirical results from large-scale experiments or real-world applications using the quasi-LSTM model.

### Open Question 2
- Question: How does the Languini Kitchen's experimental protocol impact the reproducibility of language modeling research?
- Basis in paper: [explicit] The paper introduces an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. This approach aims to address the challenges of reproducing language modeling research due to varying computational resources.
- Why unresolved: The paper does not provide evidence of the protocol's impact on the reproducibility of language modeling research in practice.
- What evidence would resolve it: Case studies or empirical results demonstrating the protocol's effectiveness in enabling reproducible language modeling research across different computational resources.

### Open Question 3
- Question: How does the Languini Books dataset compare to other language modeling benchmarks in terms of diversity and quality?
- Basis in paper: [explicit] The paper claims that the Languini Books dataset surpasses existing academic benchmarks in quality, diversity, and document length. However, it does not provide a detailed comparison with other benchmarks.
- Why unresolved: The paper does not provide a comprehensive analysis of the Languini Books dataset's advantages over other language modeling benchmarks.
- What evidence would resolve it: A thorough comparison of the Languini Books dataset with other popular language modeling benchmarks, including metrics such as diversity, quality, and document length.

## Limitations
- Focus on narrow set of architectures (GPT-2 style decoder-only and novel LSTM variant)
- Reliance on single reference hardware platform (RTX 3090)
- Heavy dependence on quality and representativeness of Languini Books dataset

## Confidence

**High Confidence Claims:**
- The Languini Kitchen provides a practical framework for language modeling research at different scales of compute
- The compute-class methodology enables fair comparisons between models of different architectures
- The GPT baseline achieves better perplexity than the LSTM baseline across all tested compute levels

**Medium Confidence Claims:**
- The Languini Books dataset represents a significant improvement over existing academic benchmarks in terms of quality and diversity
- The quasi-LSTM architecture achieves a favorable scaling law due to its improved throughput characteristics
- The predicted intersection point of scaling laws at ~50,000 accelerator hours is accurate

**Low Confidence Claims:**
- The Languini Kitchen framework is universally applicable to all language modeling research scenarios
- The specific filtering criteria used for the Languini Books dataset guarantee optimal evaluation conditions
- The scaling law extrapolations will hold true beyond the tested compute ranges

## Next Checks

1. **Throughput Validation**: Replicate the throughput measurements for both GPT and quasi-LSTM models on multiple hardware platforms (RTX 3090, A100, H100) to verify that the compute-class methodology produces consistent results across different GPU architectures. This check should include testing both the reference throughput script and actual training runs to identify any discrepancies.

2. **Dataset Quality Assessment**: Conduct a comprehensive analysis of the Languini Books dataset to verify the filtering criteria's effectiveness and assess the representativeness of the out-of-distribution splits. This should include statistical analysis of text properties (vocabulary diversity, sentence length distributions, domain coverage) and comparison with other established benchmarks like C4, The Pile, and specialized evaluation datasets.

3. **Scaling Law Robustness**: Extend the scaling experiments to additional compute classes (both smaller and larger than those tested) and with different model sizes to verify the stability of the observed scaling laws. This should include testing whether the predicted intersection point at ~50,000 accelerator hours holds when using additional data points and whether the scaling laws exhibit the expected power-law behavior across the full range of computational budgets.