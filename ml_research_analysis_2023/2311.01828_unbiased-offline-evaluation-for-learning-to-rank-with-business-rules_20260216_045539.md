---
ver: rpa2
title: Unbiased Offline Evaluation for Learning to Rank with Business Rules
arxiv_id: '2311.01828'
source_url: https://arxiv.org/abs/2311.01828
tags:
- ranking
- business
- rules
- evaluation
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of biased offline evaluation in
  learning-to-rank systems when business rules or system bugs modify the displayed
  rankings. The authors show that standard propensity-based estimators become inaccurate
  when rankings are post-processed, because the computed propensities no longer match
  the true display probabilities.
---

# Unbiased Offline Evaluation for Learning to Rank with Business Rules

## Quick Facts
- arXiv ID: 2311.01828
- Source URL: https://arxiv.org/abs/2311.01828
- Reference count: 29
- This paper addresses biased offline evaluation in LTR systems when business rules modify displayed rankings, proposing a Birkhoff-von-Neumann decomposition method to correct propensity matrices.

## Executive Summary
This paper tackles the critical problem of biased offline evaluation in learning-to-rank systems when business rules or system bugs modify the displayed rankings after randomization. Standard propensity-based estimators become inaccurate because the computed propensities no longer match the true display probabilities after post-processing. The authors propose a novel correction method that leverages the Birkhoff-von-Neumann decomposition to compute corrected propensity matrices that accurately reflect the actual display probabilities after business rules are applied.

## Method Summary
The method uses Birkhoff-von-Neumann decomposition to compute corrected propensity matrices that reflect actual display probabilities after business rules are applied. The algorithm takes as input the logging policy's BvN decomposition, the business rules, and their application probabilities. It then applies each business rule to each permutation in the decomposition, weighted by the permutation's probability, to reconstruct the true propensity matrix. This corrected matrix can then be used with standard off-policy estimators like PBM, IPM, or INTERPOL to obtain unbiased estimates of expected reward for target policies.

## Key Results
- The correction method recovers accurate estimates of expected reward for target policies, whereas uncorrected estimators can be highly biased when business rules modify rankings.
- The method is exact for deterministic business rules with complexity O(nM), where n is the number of ranked items and M is the size of the BvN decomposition.
- Stochastic business rules with controlled application probability (e.g., 95%) preserve the full-support assumption needed for unbiased estimation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Birkhoff-von-Neumann decomposition enables exact correction of propensity matrices when business rules are deterministic.
- Mechanism: By decomposing the randomization propensity matrix into permutation matrices, each with a known probability, the algorithm can apply the business rules to each permutation individually and reconstruct the true propensity matrix that reflects the actual display probabilities after post-processing.
- Core assumption: Business rules are known and can be represented as deterministic permutation operations on rankings.
- Evidence anchors:
  - [abstract] "They propose a novel correction method based on the Birkhoff-von-Neumann decomposition to compute corrected propensity matrices that reflect the actual display probabilities after business rules are applied."
  - [section 3] "If the BvN randomization scheme is used then it is possible to obtain an exact P' with complexity O(nM), where n is the number of ranked items and M is the size of the BvN decomposition."
- Break condition: Business rules are stochastic or their application probabilities are unknown.

### Mechanism 2
- Claim: Correcting propensities after business rules are applied eliminates bias in off-policy evaluation estimators.
- Mechanism: Standard propensity-based estimators assume the logging policy's propensities match the actual display probabilities. When business rules modify rankings after randomization, this assumption is violated. The correction method computes a new propensity matrix P' that accurately represents the true display probabilities, restoring the validity of the propensity-based estimation framework.
- Core assumption: The logging policy uses a randomization scheme (like BvN) that allows reconstruction of the true display probabilities.
- Evidence anchors:
  - [abstract] "They propose a novel correction method based on the Birkhoff-von-Neumann decomposition that is robust to this type of post-processing."
  - [section 2] "We assume that the logging policy ensures a non-zero probability of observing a reward for every item for which the target policy has a non-zero probability of observing a reward."
- Break condition: The business rules are applied with 100% probability (deterministic), creating zero probabilities in the corrected propensity matrix that violate the full-support assumption.

### Mechanism 3
- Claim: Stochastic business rules with controlled application probability preserve the full-support assumption needed for unbiased estimation.
- Mechanism: When business rules are deterministic, they can create zero probabilities in the corrected propensity matrix. By ensuring business rules are only applied with some probability (e.g., 95%), the corrected propensity matrix maintains strictly positive entries, preserving the full-support assumption required by estimators like IPM.
- Core assumption: Business rules can be made stochastic without significantly impacting user experience or business objectives.
- Evidence anchors:
  - [section 3] "In case business rules are applied with probability 1, corrected propensity matrix can contain values P'j,k ∈ {0, 1} and violate the full-support assumption from Section 2. This can be avoided by ensuring that modifications to the ranking are only applied with some probability."
  - [section 4] "We analyzed several additional settings of pinning... Pinning applied with 95% probability with no correction of propensity matrix."
- Break condition: Business rules cannot be made stochastic due to strict business requirements or user experience constraints.

## Foundational Learning

- Concept: Birkhoff-von-Neumann decomposition
  - Why needed here: The BvN decomposition is the mathematical foundation that allows exact computation of corrected propensity matrices when business rules are deterministic. Without this decomposition, correction would require expensive Monte Carlo estimation.
  - Quick check question: Can you explain why the BvN decomposition is at most size n² for n items, and how this relates to the computational complexity O(nM)?

- Concept: Doubly-stochastic matrices and their relationship to permutation matrices
  - Why needed here: Understanding that any doubly-stochastic matrix can be decomposed into a convex combination of permutation matrices is crucial for grasping how the BvN decomposition works and why it can exactly correct propensity matrices.
  - Quick check question: Given a 3×3 doubly-stochastic matrix, can you manually construct its BvN decomposition?

- Concept: Propensity-based off-policy evaluation and the full-support assumption
  - Why needed here: The entire correction method is built on the premise that propensity-based estimators require the logging policy to have non-zero probability of displaying each item at each position. Understanding this assumption is crucial for recognizing when correction is needed and when it might fail.
  - Quick check question: What happens to the IPM estimator if the corrected propensity matrix contains zero entries?

## Architecture Onboarding

- Component map:
  Ranking layer -> BvN randomization layer -> Business rules layer -> Display -> Logging system

- Critical path: Ranking → BvN randomization → Business rules → Display → Logging
  The correction method operates on the logging data, not in the production path, making it a post-hoc analysis tool.

- Design tradeoffs:
  - Deterministic vs. stochastic business rules: Deterministic rules are simpler but may violate full-support; stochastic rules preserve it but add complexity
  - BvN vs. other randomization schemes: BvN enables exact correction but may have higher computational overhead than simple swap randomization
  - Correction accuracy vs. computational cost: Exact correction via BvN is O(nM) while Monte Carlo estimation is O(nL) with L samples

- Failure signatures:
  - Biased estimates despite correction: Indicates either stochastic business rules were treated as deterministic, or the BvN decomposition was computed incorrectly
  - Very high variance in estimates: May indicate insufficient data or that the correction method introduced additional variance
  - Zero probabilities in corrected propensity matrix: Indicates deterministic business rules were applied without stochasticity

- First 3 experiments:
  1. Implement Algorithm 1 on synthetic data with known business rules and verify that corrected propensities match the true display probabilities
  2. Test Algorithm 3 on data with stochastic business rules (e.g., 95% pinning probability) and compare results with and without correction
  3. Evaluate the sensitivity of different estimators (PBM, IPM, INTERPOL) to the accuracy of corrected propensities using synthetic datasets

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- The method relies heavily on knowing exact business rules and their application probabilities, which may not be available in practice.
- The computational complexity of O(nM) for deterministic business rules may become prohibitive for large catalogs or complex business rule sets.
- The method assumes the logging policy uses BvN randomization, limiting its applicability to systems using different randomization schemes.

## Confidence
- **High confidence**: The mathematical framework for propensity correction using BvN decomposition is sound and the core algorithm is well-defined.
- **Medium confidence**: The experimental validation sufficiently demonstrates the method's effectiveness on synthetic and real-world datasets, though the synthetic data generation process and exact business rule parameters are not fully specified.
- **Low confidence**: The practical applicability of the method in production systems with complex, evolving business rules and unknown application probabilities.

## Next Checks
1. **Real-world validation**: Apply the correction method to a production LTR system with documented business rules to verify that corrected propensities accurately reflect actual display probabilities and eliminate evaluation bias.

2. **Robustness testing**: Evaluate the method's sensitivity to incorrect assumptions about business rule application probabilities by systematically varying the assumed vs. actual probabilities and measuring the impact on evaluation accuracy.

3. **Alternative randomization schemes**: Extend the correction framework to work with other randomization schemes beyond BvN (such as Thompson sampling or epsilon-greedy) to increase practical applicability across different LTR systems.