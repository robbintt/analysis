---
ver: rpa2
title: Improving Input-label Mapping with Demonstration Replay for In-context Learning
arxiv_id: '2310.19572'
source_url: https://arxiv.org/abs/2310.19572
tags:
- attention
- demonstrations
- causal
- language
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of causal attention in large
  language models for in-context learning, where earlier demonstrations cannot access
  later ones, leading to incomplete input-label mapping. The proposed Repeated Demonstration
  with Sliding Causal Attention (RdSca) method duplicates later demonstrations and
  prepends them to the input, allowing each demonstration to attend to all others
  while avoiding information leakage through sliding causal attention windows.
---

# Improving Input-label Mapping with Demonstration Replay for In-context Learning

## Quick Facts
- arXiv ID: 2310.19572
- Source URL: https://arxiv.org/abs/2310.19572
- Reference count: 15
- Primary result: RdSca improves ICL accuracy by enabling earlier demonstrations to access later ones through demonstration repetition and sliding causal attention

## Executive Summary
This paper addresses a fundamental limitation in in-context learning (ICL) where causal attention prevents earlier demonstrations from accessing later ones, limiting input-label mapping learning. The authors propose Repeated Demonstration with Sliding Causal Attention (RdSca), which duplicates later demonstrations and prepends them to the input, allowing each demonstration to attend to all others while avoiding information leakage through sliding causal attention windows. Experiments show significant improvements over regular ICL across multiple datasets and model scales, with LLAMA-65B achieving 75.76% accuracy on SST-2 compared to 59.82% for regular ICL.

## Method Summary
The method involves three key steps: (1) duplicating later demonstrations and prepending them to create bidirectional access, (2) applying sliding causal attention windows to prevent information leakage while allowing full demonstration interaction, and (3) including <SOS> tokens to ensure proper sequence interpretation. The approach is implemented through custom attention masks without requiring model retraining, making it compatible with any pre-trained LLM. The sliding window mechanism restricts each demonstration to attend to all others exactly once, maintaining the causal constraint while enabling comprehensive input-label mapping learning.

## Key Results
- RdSca achieves 75.76% accuracy on SST-2 with LLAMA-65B versus 59.82% for regular ICL
- Performance improves with more demonstrations (4 → 32), unlike standard ICL which degrades
- Combined with calibration and retrieval techniques, RdSca reaches 82.08% on SST-2 with 65B models
- Robust across model scales (7B-65B) and multiple classification tasks (SST-2, CB, RTE, AGNews, QQP, QNLI)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal attention in standard ICL prevents earlier demonstrations from accessing later ones, limiting input-label mapping learning
- Mechanism: By duplicating later demonstrations and prepending them, each demonstration gains access to all others while maintaining causal attention constraints
- Core assumption: The model can effectively use repeated demonstrations without overfitting to duplicated content
- Evidence anchors: Abstract states causal attention "fails to capture the full input-label information"; section 3.1 shows each demonstration only has access to half the context

### Mechanism 2
- Claim: Sliding causal attention prevents information leakage while allowing full demonstration interaction
- Mechanism: Custom attention window restricts each demonstration to attend to all others exactly once, avoiding repeated content
- Core assumption: The attention window size can be optimized to balance context richness with information efficiency
- Evidence anchors: Section 3.3 describes sliding attention windows; section 4.4 shows full attention causes same demonstration to attend to its first appearance

### Mechanism 3
- Claim: The <SOS> token is crucial for proper sequence interpretation under customized attention
- Mechanism: Manually adding <SOS> to attention windows ensures tokens are treated as valid sequences rather than fragments
- Core assumption: LLMs are pre-trained to expect sequences starting with <SOS>
- Evidence anchors: Section 3.3 identifies <SOS> as crucial to performance; section 4.4 shows significant improvement when <SOS> is included

## Foundational Learning

- Concept: Causal attention in transformer models
  - Why needed here: Understanding why standard ICL is limited by unidirectional attention
  - Quick check question: Why can't a demonstration attend to demonstrations that come after it in standard ICL?

- Concept: Attention masking and windowing
  - Why needed here: Implementing sliding causal attention requires understanding how attention masks work
  - Quick check question: How does a lower triangular attention mask enforce causal attention?

- Concept: Input-label mapping in few-shot learning
  - Why needed here: The core goal is to improve the model's ability to learn these mappings from demonstrations
  - Quick check question: What is the difference between learning from semantic priors versus input-label mappings?

## Architecture Onboarding

- Component map: Input → Template formatting → Demonstration repetition → Attention mask customization → LLM inference → Output prediction

- Critical path: Input → Template formatting → Demonstration repetition → Attention mask customization → LLM inference → Output prediction

- Design tradeoffs:
  - Sequence length vs. computational efficiency (repeating demonstrations increases length)
  - Window size vs. context richness (larger windows provide more context but risk information leakage)
  - <SOS> inclusion vs. simplicity (manual <SOS> addition adds complexity but improves performance)

- Failure signatures:
  - Performance no better than random guessing (full attention experiment failure)
  - Sharp decline when adding too many demonstrations (RD failure)
  - Severe performance degradation without <SOS> token

- First 3 experiments:
  1. Compare regular ICL vs. RdSca on SST-2 with LLAMA-65B to verify performance improvement
  2. Test different attention window sizes (W=1,2,3,4) to find optimal balance
  3. Evaluate RdSca with and without <SOS> token to confirm its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the <SOS> token enables the model to treat the tokens under the attention window as a valid sequence?
- Basis in paper: [explicit] The paper mentions that the <SOS> token is crucial for the model to perform correctly and allows it to treat the tokens under the attention window as a valid sequence rather than a fragment.
- Why unresolved: The paper does not provide a detailed explanation of why the <SOS> token has such a significant impact on performance.
- What evidence would resolve it: Further experiments or analysis explaining the role of the <SOS> token in the attention mechanism and its impact on the model's understanding of the sequence structure.

### Open Question 2
- Question: How does the performance of RdSca scale with extremely long sequences (e.g., hundreds of demonstrations)?
- Basis in paper: [inferred] The paper mentions that RdSca is robust to the increase in the number of demonstrations but does not test with extremely long sequences.
- Why unresolved: The experiments only go up to 32 demonstrations, and the paper does not explore the performance with much longer sequences.
- What evidence would resolve it: Experiments with a significantly larger number of demonstrations to see if the performance plateaus or declines, and analysis of the computational efficiency with very long sequences.

### Open Question 3
- Question: Can RdSca be effectively combined with other ICL techniques like chain-of-thought reasoning?
- Basis in paper: [inferred] The paper mentions that RdSca can be combined with techniques like demonstration retrieval and calibration, but does not explore chain-of-thought reasoning.
- Why unresolved: The paper does not investigate the potential synergy between RdSca and chain-of-thought reasoning, which could be a powerful combination for complex tasks.
- What evidence would resolve it: Experiments applying RdSca with chain-of-thought reasoning on tasks that benefit from multi-step reasoning, and comparison of performance with and without this combination.

## Limitations

- The theoretical justification for why causal attention specifically limits ICL performance lacks rigorous analysis
- Computational overhead of doubling sequence length through demonstration repetition is not adequately addressed
- Claims about generalizability to non-classification tasks are not tested in the paper

## Confidence

**High confidence**: The empirical performance improvements of RdSca over standard ICL are well-established through controlled experiments. The ablation studies showing that both demonstration repetition and sliding attention are necessary components are convincing.

**Medium confidence**: The claim that RdSca works because it enables complete input-label mapping learning is plausible but not definitively proven. While the authors provide strong empirical evidence, alternative explanations are not fully ruled out.

**Low confidence**: The theoretical justification for why causal attention specifically limits ICL performance, and why the sliding window approach is optimal, lacks rigorous analysis. The importance of the <SOS> token is observed but not explained.

## Next Checks

1. **Ablation of attention window mechanisms**: Systematically test RdSca with different window sizes (W=1 through W=5) on multiple datasets to determine if the claimed optimal window size is truly optimal, or if performance plateaus beyond a certain point.

2. **Alternative sequence formatting**: Implement RdSca without demonstration repetition but with alternative attention masking strategies to isolate whether the performance gain comes from demonstration repetition specifically or from improved attention patterns.

3. **Scaling analysis with more demonstrations**: Test RdSca with varying numbers of demonstrations (4, 8, 16, 32) on a representative dataset to empirically validate the claim that RdSca maintains performance while standard ICL degrades, measuring both accuracy and computational overhead.