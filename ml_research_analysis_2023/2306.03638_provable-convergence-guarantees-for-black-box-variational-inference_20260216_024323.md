---
ver: rpa2
title: Provable convergence guarantees for black-box variational inference
arxiv_id: '2306.03638'
source_url: https://arxiv.org/abs/2306.03638
tags:
- convex
- gradient
- then
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the convergence of black-box variational inference
  (VI), where the goal is to approximate a complex target distribution with a simpler
  variational family by optimizing a non-smooth objective. The key challenges are
  the non-smoothness of the negative entropy term and the unusual quadratic noise
  bounds of gradient estimators.
---

# Provable convergence guarantees for black-box variational inference

## Quick Facts
- arXiv ID: 2306.03638
- Source URL: https://arxiv.org/abs/2306.03638
- Reference count: 40
- Primary result: First rigorous convergence guarantees for black-box variational inference with anytime 1/T or O(1/√T) rates

## Executive Summary
This paper provides the first rigorous convergence guarantees for black-box variational inference (VI) using stochastic proximal or projected gradient methods. The key innovation is establishing that reparameterization gradient estimators satisfy quadratic noise bounds, enabling convergence analysis under realistic assumptions. The authors prove that when the target log-density is smooth and concave, and the variational family is Gaussian, these algorithms achieve either 1/T or O(1/√T) convergence rates depending on whether the objective is strongly convex.

## Method Summary
The paper analyzes two algorithms: Prox-SGD for triangular covariance factors and Proj-SGD for symmetric covariance factors. Both use reparameterization-based gradient estimators with quadratic noise bounds. The variational family is restricted to be Gaussian N(z|m, CC^T) where C must remain positive definite with bounded smallest singular value. The optimization methods incorporate proximal operators or projections to maintain this non-degeneracy constraint while minimizing the free energy objective.

## Key Results
- Anytime 1/T convergence rates when the objective is strongly convex
- O(1/√T) convergence rates otherwise
- First rigorous guarantees for black-box VI under realistic smoothness and concavity assumptions
- Quadratic noise bounds for reparameterization gradient estimators enable proximal/projected SGD analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entropy term is convex and Lipschitz-smooth over a restricted set
- Mechanism: By restricting the covariance factor to be positive definite with a lower bound on its smallest singular value, the negative entropy becomes convex and Lipschitz-smooth, allowing convergence guarantees
- Core assumption: The target log-density is smooth and concave, and the variational family is Gaussian with a triangular or symmetric covariance factor
- Evidence anchors:
  - [abstract]: "The main results are anytime 1/T convergence rates when the objective is strongly convex, and O(1/√T) rates otherwise."
  - [section 7.2]: "h is M-smooth over WM = {(m, C) ∈ W : σmin(C) ≥ 1/√M}."
- Break condition: If the covariance factor becomes too small (singular values below 1/√M), the entropy is no longer Lipschitz-smooth, and the guarantees fail

### Mechanism 2
- Claim: Gradient estimators based on reparameterization satisfy a quadratic noise bound
- Mechanism: The noise of gradient estimators is bounded by a quadratic function of the distance between parameters and the optimal parameters, enabling the use of proximal or projected SGD methods
- Core assumption: The target log-density is smooth and concave
- Evidence anchors:
  - [abstract]: "existing gradient estimators based on reparameterization satisfy a quadratic noise bound."
  - [section 2.2]: "They do not satisfy usual noise bounds, but we show that they verify some quadratic bound (Definition 4)."
- Break condition: If the gradient estimator's noise grows faster than quadratically with parameter distance, the bounds fail

### Mechanism 3
- Claim: Proximal and projected SGD methods converge under quadratic noise bounds
- Mechanism: New convergence guarantees for proximal and projected SGD are established for composite convex and smooth objectives with quadratically bounded gradient estimators
- Core assumption: The gradient estimator is quadratically bounded
- Evidence anchors:
  - [abstract]: "give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound."
  - [section 3.1, 3.2]: Theorems 6 and 9 provide convergence rates for Prox-SGD and Proj-SGD under quadratic noise bounds
- Break condition: If the gradient estimator is not quadratically bounded, the convergence guarantees do not hold

## Foundational Learning

- Concept: Lipschitz smoothness
  - Why needed here: To ensure the free energy l is smooth, which is required for the convergence analysis of the gradient-based methods
  - Quick check question: What is the Lipschitz constant of the free energy l if the target log-density is M-smooth?

- Concept: Convexity
  - Why needed here: To ensure the objective function is convex, which is required for the convergence guarantees of the proximal and projected SGD methods
  - Quick check question: Under what conditions on the target log-density is the free energy l convex?

- Concept: Quadratic noise bounds
  - Why needed here: To bound the noise of the gradient estimators, which is required for the convergence analysis of the proximal and projected SGD methods
  - Quick check question: How does the quadratic noise bound depend on the current parameters and the optimal parameters?

## Architecture Onboarding

- Component map:
  - Variational family: Gaussian distribution parameterized by mean and covariance factor
  - Gradient estimators: reparameterization-based estimators for the free energy and full objective
  - Optimization methods: proximal and projected SGD with specific step sizes and noise bounds

- Critical path:
  1. Verify assumptions on the target log-density (smoothness and convexity)
  2. Compute the gradient estimators and their quadratic noise bounds
  3. Apply the proximal or projected SGD method with appropriate step sizes

- Design tradeoffs:
  - Triangular vs. symmetric covariance factors: Triangular factors allow for cheaper proximal operator computation, while symmetric factors allow for easier projection onto the non-degeneracy set
  - Constant vs. decaying step sizes: Constant step sizes are simpler but may lead to slower convergence, while decaying step sizes can achieve faster rates but require more tuning

- Failure signatures:
  - Divergence of the algorithm: May indicate that the assumptions on the target log-density are not satisfied
  - Slow convergence: May indicate that the step sizes are not well-chosen or that the noise bounds are too loose

- First 3 experiments:
  1. Verify the convexity and smoothness of the free energy l for a simple target distribution
  2. Compute the gradient estimators and their quadratic noise bounds for a simple target distribution
  3. Apply the proximal or projected SGD method with constant step sizes to a simple target distribution and verify convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence guarantees for black-box variational inference be extended to non-Gaussian variational families?
- Basis in paper: [explicit] The paper focuses on Gaussian variational families and derives specific convergence guarantees for them. It does not explore other families like mean-field or more complex distributions
- Why unresolved: The analysis heavily relies on properties of the Gaussian distribution, such as the closed-form expression for the entropy and the ability to parameterize the covariance with a factor. Extending these results to other families would require developing new theoretical tools
- What evidence would resolve it: Proofs of convergence for black-box VI using other variational families, such as mean-field approximations or mixtures of Gaussians, under realistic assumptions on the target distribution

### Open Question 2
- Question: How do the convergence rates change when using more advanced gradient estimators, such as those based on control variates or multi-level Monte Carlo methods?
- Basis in paper: [explicit] The paper analyzes two specific gradient estimators based on reparameterization and derives their noise bounds. It mentions that other estimators have been proposed to reduce noise but does not analyze them
- Why unresolved: The convergence guarantees depend on the noise properties of the gradient estimator. Analyzing more advanced estimators would require deriving their noise bounds and incorporating them into the convergence analysis
- What evidence would resolve it: Convergence proofs for black-box VI using gradient estimators with improved noise properties, such as those based on control variates or multi-level Monte Carlo methods, under realistic assumptions

### Open Question 3
- Question: Can the convergence guarantees be extended to the case where the target distribution is not smooth or strongly concave?
- Basis in paper: [inferred] The paper assumes that the target log-density is smooth and concave (or strongly concave) to derive convergence guarantees. It does not explore the case where these assumptions are violated
- Why unresolved: The analysis relies on the smoothness and convexity of the target log-density to establish properties of the free energy and the gradient noise bounds. Extending the results to non-smooth or non-convex targets would require developing new theoretical tools
- What evidence would resolve it: Convergence proofs for black-box VI under weaker assumptions on the target distribution, such as only requiring it to be Lipschitz continuous or having a bounded support

## Limitations
- The analysis critically depends on restricting the covariance factor to remain non-degenerate (σmin(C) ≥ 1/√M), which is a strong assumption that may not hold in practice
- The convergence guarantees require the target log-density to be smooth and concave, which is a restrictive condition that excludes many common posterior distributions
- The algorithms may fail or exhibit poor performance when these assumptions are violated

## Confidence
- High confidence in the theoretical analysis of gradient estimators satisfying quadratic noise bounds (Mechanism 2)
- Medium confidence in the convergence guarantees for Prox-SGD and Proj-SGD (Mechanism 3), as they depend on the practical satisfaction of assumptions
- Medium confidence in the smoothness and convexity claims for the entropy term (Mechanism 1), as this requires careful parameter restrictions

## Next Checks
1. Verify the Lipschitz-smoothness of the negative entropy term over the restricted set WM by computing gradients and Hessians for specific Gaussian variational families
2. Implement and test the proximal operator for triangular covariance factors and projection operator for symmetric covariance factors on small-scale examples
3. Validate the quadratic noise bounds empirically by computing the actual gradient variance for different parameter values and comparing against the theoretical bounds