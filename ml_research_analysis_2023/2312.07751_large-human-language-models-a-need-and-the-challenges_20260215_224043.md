---
ver: rpa2
title: 'Large Human Language Models: A Need and the Challenges'
arxiv_id: '2312.07751'
source_url: https://arxiv.org/abs/2312.07751
tags:
- human
- language
- context
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes large human language models (LHLMs) as a vision
  for future NLP systems, arguing that current LLMs lack explicit modeling of authors
  and their contexts. The authors advocate three positions: (1) LM training should
  include the human context, (2) LHLMs should recognize people are more than their
  groups, and (3) LHLMs should account for dynamic, temporal human context.'
---

# Large Human Language Models: A Need and the Challenges

## Quick Facts
- arXiv ID: 2312.07751
- Source URL: https://arxiv.org/abs/2312.07751
- Reference count: 37
- Key outcome: This paper proposes large human language models (LHLMs) as a vision for future NLP systems, arguing that current LLMs lack explicit modeling of authors and their contexts.

## Executive Summary
This paper argues that current large language models (LLMs) fail to truly understand human language because they don't explicitly account for the authors and their contexts. The authors propose large human language models (LHLMs) that incorporate human context into language modeling through three key positions: (1) including human context in LM training to capture author-specific dependencies, (2) representing people as more than their group memberships using continuous factors, and (3) accounting for dynamic and temporally-dependent human contexts. Drawing on psychological and behavioral science literature, the paper identifies challenges in representing and incorporating this complex human context into language model training, and outlines possible solutions to build NLP systems that can genuinely understand human language.

## Method Summary
The paper proposes modifying Transformer-based architectures to incorporate human context through various mechanisms including user vectors, dynamic author representations, and recurrence mechanisms. The approach involves collecting datasets with author IDs and historical language data, encoding context into continuous representations, integrating these into Transformer layers, and training with language modeling objectives that condition on both text and human context. The method requires implementing modifications to standard Transformer architectures to handle the additional context information while managing computational complexity and privacy considerations.

## Key Results
- LLMs create word representations only in the context of neighboring words without explicitly accounting for author context
- Current NLP systems may suffer from ecological fallacy by treating text from the same author as independent observations
- Human language exhibits temporal patterns and dynamic states that static models fail to capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including human context in LM training improves language understanding by capturing inter-text dependencies from the same author
- Mechanism: The ecological fallacy is avoided by conditioning language modeling on a human context vector H, enabling the model to learn author-specific dependencies
- Core assumption: Text sequences from the same author are not independent and benefit from joint modeling
- Evidence anchors:
  - [abstract] "LLMs train on a pre-training task and are capable of being applied to a broad set of NLP tasks producing state-of-the-art results. But these language models create word representation only in the context of neighboring words and do not explicitly account for the context of the authors."
  - [section] "Piantadosi et al. (1988) describe a fallacy in statistical models of the world pertaining to modeling individual observations that are part of a group, as if they are independent, a so-called ecological fallacy..."
- Break condition: If author context does not improve perplexity or downstream task performance, the mechanism fails

### Mechanism 2
- Claim: Modeling people as more than their group memberships reduces arbitrary binning and captures richer individual variation
- Mechanism: Replace discrete group labels with continuous representations (e.g., real-valued embeddings) that capture soft membership along factor dimensions
- Core assumption: Human language is better modeled by continuous, intersecting factors rather than discrete group bins
- Evidence anchors:
  - [abstract] "Second, LHLMs should recognize that people are more than their group(s)."
  - [section] "Psychology and Psychopathology have a wealth of literature suggesting that people should not be put in discrete bins but instead should be placed in a dimensional structure by characterizing them as a mixture of continuous factors..."
- Break condition: If discrete group models outperform continuous representations on key tasks, the mechanism is invalid

### Mechanism 3
- Claim: Dynamic and temporally-dependent human context captures evolving human states, improving language understanding over static models
- Mechanism: Use recurrent or temporal modeling to update user representations over time, incorporating diurnal, weekly, and seasonal patterns from historical language data
- Core assumption: Human language changes over time in predictable patterns that can be modeled
- Evidence anchors:
  - [abstract] "Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human context."
  - [section] "A person's static and dynamic human states are intertwined, where static traits influence the likelihood of entering various dynamic states across time..."
- Break condition: If static models match or exceed dynamic models in language understanding tasks, the mechanism fails

## Foundational Learning

- Concept: Ecological fallacy in NLP
  - Why needed here: The paper argues that treating author texts as independent ignores dependencies, hurting model performance
  - Quick check question: What is the ecological fallacy and why does it matter for language modeling?

- Concept: Continuous vs. discrete human factors
  - Why needed here: Moving from group-based to continuous representations is central to the second position
  - Quick check question: How do continuous representations of human attributes differ from discrete group labels?

- Concept: Temporal modeling in NLP
  - Why needed here: The third position relies on capturing dynamic human states over time
  - Quick check question: What are the challenges of modeling temporally ordered text sequences in NLP?

## Architecture Onboarding

- Component map: Input text + human context -> Human context encoder -> Transformer backbone -> Output prediction

- Critical path:
  1. Extract or infer human context for each training example
  2. Encode context into continuous representation
  3. Integrate context into Transformer layers
  4. Train with language modeling objective
  5. Fine-tune for downstream tasks

- Design tradeoffs:
  - Context length vs. computational cost (quadratic attention)
  - Granularity of human context (broad demographics vs. fine-grained individual traits)
  - Static vs. dynamic context modeling (simplicity vs. expressiveness)

- Failure signatures:
  - No improvement in perplexity or downstream task accuracy
  - Overfitting to small user groups or specific time periods
  - Privacy leaks or bias amplification from human context

- First 3 experiments:
  1. Ablation: Compare perplexity with and without human context (same Transformer architecture)
  2. Continuous vs. discrete: Train two models, one with group embeddings, one with continuous user embeddings, compare performance
  3. Static vs. dynamic: Train models with static user embeddings and with recurrently updated user states, measure improvement on temporally sensitive tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much scaling is needed for large language models to effectively simulate rich human contexts across all their dimensions?
- Basis in paper: [explicit] The paper states "it is not clear whether scale alone is enough to simulate the human contexts in all its richness and how much scaling is needed remains an open question."
- Why unresolved: Current LLMs can simulate some limited forms of human context when prompted, but lack explicit modeling of authors and their contexts. The relationship between model size and the ability to capture complex, dynamic human contexts is unclear.
- What evidence would resolve it: Empirical studies comparing the performance of increasingly large LHLMs on tasks requiring nuanced understanding of human contexts, such as personalized language modeling and author profiling.

### Open Question 2
- Question: What is the optimal amount of historical language data needed to capture a user's dynamic human context without causing computational bottlenecks?
- Basis in paper: [inferred] The paper discusses challenges in processing long historical texts due to computational complexity, and mentions that current approaches are limited in the amount of historical language they can use.
- Why unresolved: While more historical data may improve context modeling, it also increases computational requirements and may lead to diminishing returns. The trade-off between context richness and computational efficiency is not well understood.
- What evidence would resolve it: Systematic experiments varying the amount of historical data used in LHLMs and measuring both performance improvements and computational costs.

### Open Question 3
- Question: How can we effectively model and incorporate temporal information in language models to capture dynamic human states and recurring patterns?
- Basis in paper: [explicit] The paper identifies challenges in modeling temporal language and temporal aspects, including how to encode time information and what language modeling objectives would force models to consider temporal information.
- Why unresolved: While some work has explored dynamic user representations, the best approaches for capturing complex temporal dynamics (e.g., diurnal, weekly, seasonal patterns) in human language are not established.
- What evidence would resolve it: Development and evaluation of new LHLM architectures and training objectives that explicitly incorporate temporal information, compared to current approaches.

## Limitations

- The paper presents a conceptual framework without empirical validation
- No concrete architectural details or implementation specifications provided
- Computational feasibility and privacy concerns are acknowledged but not rigorously analyzed
- Heavy reliance on psychological theory rather than NLP experimental evidence

## Confidence

- **High confidence**: The identification of ecological fallacy as a conceptual issue in NLP
- **Medium confidence**: The argument that human context should be included in language modeling
- **Low confidence**: The claim that current LLMs fail to understand human language due to lack of explicit human context modeling

## Next Checks

1. **Ecological fallacy ablation study**: Train two identical Transformer models on the same datasetâ€”one treating author sequences as independent samples, one explicitly modeling author dependencies. Compare perplexity on held-out author text to quantify the impact of the ecological fallacy.

2. **Continuous vs. discrete group representation**: Using a dataset with known demographics (e.g., Amazon reviews with user profiles), train one model using discrete group embeddings and another using continuous user representations learned from historical text. Evaluate both on sentiment analysis and topic modeling tasks to measure relative performance.

3. **Static vs. dynamic context modeling**: Implement a model that maintains static user embeddings versus one that updates user representations over time using recurrent mechanisms. Test both on temporally sensitive tasks like predicting shifts in opinion or tracking topic evolution in user-generated content.