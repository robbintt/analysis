---
ver: rpa2
title: 'Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System
  Instances'
arxiv_id: '2310.02246'
source_url: https://arxiv.org/abs/2310.02246
tags:
- algorithm
- learning
- theorem
- have
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how to learn optimal solver parameters across\
  \ sequences of linear system instances. It focuses on Successive Over-Relaxation\
  \ (SOR), a classic iterative method whose convergence depends critically on the\
  \ relaxation parameter \u03C9."
---

# Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances

## Quick Facts
- arXiv ID: 2310.02246
- Source URL: https://arxiv.org/abs/2310.02246
- Reference count: 40
- Primary result: Bandit algorithms can learn optimal SOR relaxation parameters across linear system sequences with regret scaling as O(T^(2/3)) relative to best fixed ω.

## Executive Summary
This paper addresses the problem of learning optimal solver parameters for Successive Over-Relaxation (SOR) across sequences of linear system instances. The key insight is that SOR's convergence depends critically on the relaxation parameter ω, and this parameter can be learned using bandit algorithms that receive only iteration-count feedback. The authors show that Tsallis-INF bandit algorithm achieves near-optimal regret bounds for deterministic sequences, while contextual bandit approaches can adapt to instance-specific optimal parameters in shifted systems. For semi-stochastic settings with truncated Gaussian targets, they demonstrate that the expected cost is Lipschitz in ω, enabling practical contextual bandit methods.

## Method Summary
The core approach uses bandit learning algorithms to select the relaxation parameter ω for SOR across a sequence of linear systems. The method maintains a probability distribution over discretized ω values and updates this distribution based on observed iteration counts. For shifted systems At = A + ctIn, contextual bandits learn a mapping from the shift parameter c to the optimal ω. In the semi-stochastic setting, Chebyshev regression contextual bandits exploit the Lipschitz continuity of expected costs with respect to ω. The algorithms use only the number of iterations as feedback, making them practical for high-precision linear system solving.

## Key Results
- Tsallis-INF bandit achieves O(T^(2/3)) regret relative to best fixed ω for deterministic sequences
- Contextual bandit attains O(T^(3/4)) regret for shifted systems (At = A + ctIn)
- Chebyshev regression contextual bandit achieves O(T^(9/11) n^(1/2)) regret for semi-stochastic SSOR
- First end-to-end learning-theoretic guarantees for data-driven tuning of high-precision linear system solvers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tsallis-INF bandit algorithm can learn the relaxation parameter ω across a sequence of linear systems by using only iteration-count feedback, achieving regret that scales as O(T^(2/3)) relative to the best fixed ω.
- Mechanism: The algorithm maintains a probability distribution over a discretized grid of ω values and updates this distribution based on the observed iteration counts. The key insight is that the upper bound on iteration counts (derived from spectral radius analysis) is semi-Lipschitz, allowing Tsallis-INF to achieve near-optimal regret even though the true cost function is discontinuous.
- Core assumption: The convergence of SOR occurs near the asymptotic regime where the spectral radius governs the convergence rate (Assumption 2.1).
- Evidence anchors:
  - [abstract] "proves that a bandit online learning algorithm--using only the number of iterations as feedback--can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed ω"
  - [section] "Since ρ(Cω) = limk→∞ ∥Ckω∥1/k2, the spectral radius asymptotically characterizes how much the error is reduced at each step"
  - [corpus] Weak evidence - corpus papers focus on preconditioners and PDE solvers but don't directly address bandit learning for SOR parameters
- Break condition: If the spectral radius bound Ut is too loose (α close to 1 for many instances), learning becomes harder and regret bounds degrade.

### Mechanism 2
- Claim: In the diagonally shifted setting where At = A + ctIn, contextual bandits can achieve O(T^(3/4)) regret relative to the instance-optimal policy that always picks the best ω for each instance.
- Mechanism: The optimal relaxation parameter is a Lipschitz continuous function of the shift parameter c. By discretizing the context space and running Tsallis-INF separately on each context bin, the algorithm can adapt to instance-specific optimal parameters.
- Core assumption: The optimal relaxation parameter is a Lipschitz function of the diagonal shift.
- Evidence anchors:
  - [abstract] "when given additional structural information, we show that a contextual bandit method asymptotically achieves the performance of the instance-optimal policy"
  - [section] "we can show that the policy is Lipschitz w.r.t. ct"
  - [corpus] Weak evidence - corpus papers discuss shifted linear systems but not contextual bandit approaches for parameter tuning
- Break condition: If the Lipschitz constant of ω*(c) becomes too large, the discretization required for good performance becomes prohibitively fine.

### Mechanism 3
- Claim: In a semi-stochastic setting with truncated Gaussian targets, the expected cost of symmetric SOR (SSOR) is Lipschitz continuous in ω, enabling direct comparison to the optimal fixed parameter with O(T^(2/3) n^(1/2)) regret.
- Mechanism: The randomness in the target vectors ensures that convergence doesn't occur too close to the tolerance threshold, making the expected cost function smooth. This allows using more practical regression-based contextual bandit algorithms.
- Core assumption: The target vectors are sampled from a radially truncated Gaussian distribution.
- Evidence anchors:
  - [abstract] "we show that the expected cost is shown to be Lipschitz in ω, enabling direct comparison to the optimal fixed parameter"
  - [section] "we assume that bt = mtut ∀ t ∈ [T ], where ut ∈ Rn is uniform on the unit sphere and m2t is a χ2 random variable"
  - [corpus] Weak evidence - corpus papers discuss Gaussian distributions but not in the context of SSOR convergence analysis
- Break condition: If the target vectors have pathological structure (e.g., concentrated near eigenvectors of the iteration matrix), the expected cost may not be Lipschitz.

## Foundational Learning

- Concept: Spectral radius and its relationship to iterative method convergence
  - Why needed here: The analysis hinges on understanding how the spectral radius of the iteration matrix determines the asymptotic convergence rate of SOR
  - Quick check question: Why does the spectral radius ρ(Cω) asymptotically characterize the convergence rate of SOR?

- Concept: Lipschitz continuity and semi-Lipschitz functions
  - Why needed here: The regret bounds rely on the upper bound functions being Lipschitz or semi-Lipschitz to apply bandit learning algorithms
  - Quick check question: What is the difference between a Lipschitz function and a semi-Lipschitz function?

- Concept: Contextual bandits and policy learning
  - Why needed here: The shifted setting requires learning a mapping from context (diagonal shift) to optimal parameter, which is a contextual bandit problem
  - Quick check question: How does a contextual bandit algorithm differ from a standard multi-armed bandit algorithm?

## Architecture Onboarding

- Component map: SOR solver -> Tsallis-INF bandit algorithm -> Parameter selection -> SOR execution -> Iteration count feedback -> Bandit update
- Critical path:
  1. Initialize SOR solver and bandit algorithm
  2. For each instance, select ω using bandit algorithm
  3. Run SOR with selected ω and observe iteration count
  4. Update bandit algorithm with feedback
  5. Repeat for all instances in sequence
- Design tradeoffs:
  - Grid resolution vs. computational overhead (finer grids give better approximation but increase bandit algorithm complexity)
  - Context discretization vs. adaptivity (finer context bins allow better instance-specific tuning but require more data per bin)
  - Absolute vs. relative convergence criterion (affects the Lipschitz properties of the cost function)
- Failure signatures:
  - High regret compared to theoretical bounds (indicates issues with the semi-Lipschitz assumption or discretization)
  - Oscillating parameter selection (suggests insufficient exploration or poor discretization)
  - Convergence to suboptimal parameter values (may indicate the upper bound Ut is too loose for certain instances)
- First 3 experiments:
  1. Run Tsallis-INF on a simple diagonally dominant matrix with varying b vectors to verify O(T^(2/3)) regret
  2. Test contextual bandit approach on diagonally shifted Laplacian matrices with known optimal policies
  3. Evaluate Chebyshev regression contextual bandit on semi-stochastic problems with truncated Gaussian targets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical bounds on iteration counts for SOR and SSOR be tightened to better match empirical performance, particularly for suboptimal ω values?
- Basis in paper: The paper notes that upper bounds derived from asymptotic analysis are somewhat loose for sub-optimal ω and for preconditioned CG methods.
- Why unresolved: The current bounds are derived from asymptotic convergence rates and may not capture the full complexity of the iterative process, especially in the non-asymptotic regime.
- What evidence would resolve it: Empirical studies comparing the theoretical bounds with actual iteration counts across a wide range of problem instances and parameter values, including suboptimal ω, would help assess the tightness of the bounds.

### Open Question 2
- Question: How can the computational overhead of the bandit learning algorithms be further reduced, particularly in high-dimensional settings?
- Basis in paper: The paper mentions that the computational overhead of the Tsallis-INF algorithm is likely negligible in practice, but does not provide a detailed analysis of its scalability.
- Why unresolved: While the paper provides some insights into the computational complexity of the algorithms, a more thorough analysis of their performance in high-dimensional settings is needed.
- What evidence would resolve it: Experimental studies comparing the runtime of the bandit algorithms with the computational cost of solving the linear systems, especially for large-scale problems, would provide insights into their scalability.

### Open Question 3
- Question: Can the contextual bandit approach be extended to handle more complex structural assumptions on the linear systems, beyond diagonal shifts?
- Basis in paper: The paper focuses on the diagonally shifted setting, where the matrices are of the form At = A + ctIn, and shows that contextual bandit methods can achieve near-instance-optimal performance.
- Why unresolved: While the diagonal shift setting is a reasonable starting point, real-world applications may involve more complex structural properties that could be exploited for learning.
- What evidence would resolve it: Developing and analyzing contextual bandit algorithms that can handle more general structural assumptions, such as block structures or low-rank perturbations, and demonstrating their effectiveness on real-world datasets would be a significant step forward.

## Limitations

- Theoretical bounds are somewhat loose for sub-optimal ω values and may not accurately predict empirical performance
- Numerical stability concerns when computing spectral radius estimates for matrices with eigenvalues near the unit circle
- Limited exploration of how bandit algorithms scale to high-dimensional problems and more complex structural assumptions

## Confidence

- Deterministic SOR with Tsallis-INF: **High**
- Contextual bandits for shifted systems: **Medium**
- SSOR with Chebyshev regression: **Low**

## Next Checks

1. Implement and test Tsallis-INF on synthetic diagonally dominant matrices with varying spectral properties to empirically verify the O(T^(2/3)) regret bound.
2. Develop a test suite for the Lipschitz continuity of the optimal policy ω*(c) in the shifted setting and validate the contextual bandit approach.
3. Conduct numerical experiments on SSOR with truncated Gaussian targets to assess the Lipschitz properties of the expected cost function and the performance of the Chebyshev regression contextual bandit.