---
ver: rpa2
title: 'Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework on
  Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous Modalities'
arxiv_id: '2312.08851'
source_url: https://arxiv.org/abs/2312.08851
tags:
- segmentation
- radar
- pruning
- achelous
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Achelous++ is a unified low-power, multi-task panoptic perception
  framework for water-surface environments. It enables real-time fusion of vision
  and 4D radar data to simultaneously perform object detection, semantic segmentation,
  drivable-area segmentation, waterline segmentation, and radar point cloud segmentation
  on edge devices.
---

# Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous Modalities

## Quick Facts
- arXiv ID: 2312.08851
- Source URL: https://arxiv.org/abs/2312.08851
- Reference count: 40
- Key outcome: A unified low-power, multi-task panoptic perception framework for water-surface environments fusing vision and 4D radar data.

## Executive Summary
Achelous++ is a unified low-power, multi-task panoptic perception framework designed for water-surface environments. It enables real-time fusion of vision and 4D radar data to simultaneously perform object detection, semantic segmentation, drivable-area segmentation, waterline segmentation, and radar point cloud segmentation on edge devices. To reduce computational load, the authors propose Heterogeneous-Aware SynFlow (HA-SynFlow), a novel multi-modal pruning strategy that adapts layer-wise sparsity per modality, alongside support for standard pruning schemes like ERK. Experiments on the WaterScenes benchmark show that Achelous++ outperforms existing single-task and multi-task models in both accuracy and power efficiency.

## Method Summary
Achelous++ fuses monocular RGB images with 4D radar point clouds transformed to camera plane coordinates. It uses a CNN-ViT hybrid or reparameterized backbone for images and a RadarConv-based RCNet for radar. The Vision-Radar Fusion Network (VRFN) fuses features at the feature level. Segmentation is performed by a Dual-FPN structure, merging object and drivable-area segmentation into one branch while treating waterline segmentation separately. Detection uses YOLOX or YOLOv8 decoupled heads, and point cloud segmentation uses lightweight networks like PointNet++. The model is trained with multi-task loss combinations and optimized using strategies like UW or MGDA. Pruning is applied using HA-SynFlow or ERK ratios to compress the model for edge deployment.

## Key Results
- HA-SynFlow pruning reduces model size significantly while maintaining high mAP50 and lowering average power consumption on embedded GPUs like Orin and GTX 1650.
- Achelous++ outperforms existing single-task and multi-task models on the WaterScenes benchmark in both accuracy and power efficiency.
- RadarConv operator improves radar point cloud feature extraction by combining pooling and deformable convolution, making it more effective than ordinary convolution for irregular radar point clouds.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous-Aware SynFlow (HA-SynFlow) enables modality-specific pruning by detecting fusion stages and scoring salience per modality.
- Mechanism: HA-SynFlow computes a modality-wise score using synaptic saliency conservation across pre-fusion nodes, then derives a pruning sparsity per modality based on relative importance.
- Core assumption: The contribution of each modality to the fused feature can be quantified through pre-fusion gradients and weights.
- Evidence anchors:
  - [abstract] "novel multi-modal pruning strategy known as Heterogeneous-Aware SynFlow (HA-SynFlow) is proposed... detect the fusion method, score the salience for each modality, and prune with different modality-wise sparsity."
  - [section] "SynFlow unstructural pruning algorithm introduced and proved the neuron-wise conservation of synaptic saliency... evaluate the weight importance of two modalities in pre-fusion nodes, which directly contributes to fusion stages."
- Break condition: If pre-fusion nodes do not adequately capture modality contribution, or if saliency is dominated by shared weights, the modality scores will be inaccurate.

### Mechanism 2
- Claim: RadarConv operator improves radar point cloud feature extraction by combining pooling and deformable convolution.
- Mechanism: RadarConv first applies average pooling to aggregate neighborhood features, then uses deformable convolution v2 to model irregular point cloud patterns on the 2D radar map.
- Core assumption: Radar point clouds are sparse and irregular; simple convolution is inefficient, while pooling followed by deformable convolution preserves locality and adapts to irregularity.
- Evidence anchors:
  - [abstract] "radar convolution operator, which is more friendly and fast to the irregularness of radar point clouds on 2D image planes than ordinary convolution."
  - [section] "RadarConv first applies a 3 × 3 average pooling to the radar map... average pooling can maintain the neighborhood feature, rather than being disturbed by outliers. Moreover, the pooling operation is much faster than the convolution operation... a deformable convolution v2 [60] operator is to draw the irregular features."
- Break condition: If radar point cloud patterns become more regular (e.g., with higher resolution sensors), the benefit of deformable convolution may diminish.

### Mechanism 3
- Claim: Dual-FPN structure reduces parallel branches while maintaining segmentation performance by sharing vision features between object and drivable-area segmentation.
- Mechanism: Dual-FPN merges object segmentation and drivable-area segmentation into one branch, using shared vision features and a separate branch for waterline segmentation; this simplifies the network and speeds inference.
- Core assumption: Drivable area and objects in water-surface scenes are spatially correlated and can be segmented jointly without significant accuracy loss.
- Evidence anchors:
  - [abstract] "we amalgamate the task of object segmentation with that of drivable area segmentation into one branch, while treating waterline segmentation as a distinct task branch."
  - [section] "the drivable area consistently surrounds them, and They are each other’s contextual features... To strike a balance between segmentation task speed and accuracy... we amalgamate the task of object segmentation with that of drivable area segmentation into one branch."
- Break condition: If drivable area and object boundaries become highly disjoint (e.g., floating debris far from drivable areas), the shared branch may under-perform.

## Foundational Learning

- Concept: Multi-modal sensor fusion
  - Why needed here: Fusion of camera and 4D radar compensates for each modality's weaknesses (camera for detail, radar for all-weather robustness).
  - Quick check question: What are the primary failure modes of camera-only perception in water-surface environments, and how does radar help mitigate them?

- Concept: Multi-task learning optimization
  - Why needed here: Simultaneous detection, segmentation, and waterline tasks require balanced training to prevent gradient interference and ensure real-time performance.
  - Quick check question: How does uncertainty weighting (UW) differ from gradient normalization methods like MGDA in balancing multi-task losses?

- Concept: Structured pruning and PaI (Pruning at Initialization)
  - Why needed here: To reduce model size and inference latency on edge devices without extensive retraining, preserving performance through careful channel selection.
  - Quick check question: What is the key difference between ERK-based pruning ratios and HA-SynFlow's modality-aware sparsity allocation?

## Architecture Onboarding

- Component map: RGB image + 4D radar point cloud -> CNN-ViT hybrid or reparameterized backbone / RadarConv-based RCNet -> VRFN fusion -> YOLOX/YOLOv8 detection head + Dual-FPN segmentation heads -> Object boxes, masks, drivable area, waterline, radar point cloud categories
- Critical path: Image/radar encoder -> VRFN fusion -> detection head + Dual-FPN segmentation heads -> final outputs
- Design tradeoffs:
  - Fusion granularity: Feature-level fusion enables faster inference than late fusion but may underutilize raw point cloud detail.
  - Branch simplification: Merging segmentation tasks speeds inference but may degrade accuracy if tasks are less correlated.
  - Pruning strategy: HA-SynFlow tailors sparsity per modality but adds computation overhead; ERK is simpler but may over-prune radar channels.
- Failure signatures:
  - High false negatives in detection: Likely due to insufficient radar feature contribution or aggressive pruning.
  - Poor drivable area segmentation: May indicate inadequate shared vision features or insufficient context modeling.
  - Slow inference: Could stem from inefficient fusion stage, oversized backbone, or lack of reparameterization.
- First 3 experiments:
  1. Validate radar encoder: Replace RCNet with MobileNetV2, measure mAP50-95 and FPS change.
  2. Test pruning impact: Apply HA-SynFlow vs ERK pruning, compare mAP50-95 and FPS on target edge device.
  3. Ablation of Dual-FPN: Run with separate segmentation branches, measure mIoU and FPS differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Heterogeneous-Aware SynFlow (HA-SynFlow) pruning strategy perform on more complex network architectures beyond MobileViT and EfficientFormer V2?
- Basis in paper: [explicit] The paper mentions that HA-SynFlow outperforms ERK-based strategies in reducing power consumption on EfficientFormer-based models, but further validation on more complex architectures is needed.
- Why unresolved: The current evaluation is limited to two backbone architectures (MobileViT and EfficientFormer V2), and the performance on other complex architectures is not explored.
- What evidence would resolve it: Comparative experiments on a diverse set of complex backbone architectures, such as Swin Transformer or ConvNeXt, using HA-SynFlow pruning to assess its generalization and effectiveness.

### Open Question 2
- Question: What is the impact of varying radar point cloud density on the performance of the proposed Radar Convolution (RadarConv) operator?
- Basis in paper: [inferred] The paper proposes RadarConv to handle sparse and irregular radar point clouds, but the effect of varying density on its performance is not explicitly discussed.
- Why unresolved: The experiments are conducted on a fixed dataset (WaterScenes) with consistent radar point cloud density, leaving the impact of density variations unexplored.
- What evidence would resolve it: Experiments on datasets with varying radar point cloud densities, comparing RadarConv's performance against traditional convolution operators to quantify its robustness to density changes.

### Open Question 3
- Question: How does the performance of Achelous++ scale with the number of concurrent perception tasks beyond the current five tasks?
- Basis in paper: [explicit] Achelous++ currently supports five perception tasks, and the paper discusses the balance between task speed and accuracy, but scalability to additional tasks is not addressed.
- Why unresolved: The framework is designed for multi-task learning, but the limits of scalability and the impact on performance with additional tasks are not investigated.
- What evidence would resolve it: Incremental addition of perception tasks (e.g., traffic sign detection, pedestrian tracking) to Achelous++ and evaluation of performance metrics (accuracy, speed, power consumption) to determine scalability limits.

## Limitations

- HA-SynFlow pruning's efficacy depends on accurate quantification of modality contributions at fusion stages; if pre-fusion saliency does not reflect true fusion utility, pruning may degrade accuracy.
- RadarConv's advantage over standard convolution assumes persistent irregularity in radar point clouds; with higher resolution sensors, deformable convolution benefits may diminish.
- Dual-FPN's assumption of spatial correlation between drivable areas and objects may fail in cluttered or debris-heavy scenes, potentially hurting segmentation accuracy.

## Confidence

- HA-SynFlow improves accuracy/power trade-off: High confidence in ablation results but medium confidence in long-term robustness without broader validation datasets.
- RadarConv is more effective for radar point cloud processing: High confidence in the proposed mechanism but low external validation; requires comparison with more established point cloud backbones.
- Dual-FPN simplifies architecture without sacrificing accuracy: Medium confidence; dependent on scene correlation assumptions and untested on more varied water-surface environments.

## Next Checks

1. Evaluate HA-SynFlow on a different multi-modal dataset (e.g., nuScenes) to test generality of modality-aware pruning.
2. Replace RadarConv with standard convolution and a competitive point cloud network (e.g., PointNet++) to quantify RadarConv's unique benefit.
3. Test Dual-FPN with increased scene complexity (more debris, varied water conditions) to assess shared branch robustness.