---
ver: rpa2
title: Explaining high-dimensional text classifiers
arxiv_id: '2311.13454'
source_url: https://arxiv.org/abs/2311.13454
tags:
- data
- input
- norm
- dataset
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new method for creating on-manifold explanations
  for high-dimensional text classifiers. The method is based on recent theoretical
  work showing that gradients of neural networks trained on low-dimensional data manifolds
  have large norms when off-manifold and low cosine similarity between different networks.
---

# Explaining high-dimensional text classifiers

## Quick Facts
- arXiv ID: 2311.13454
- Source URL: https://arxiv.org/abs/2311.13454
- Authors: 
- Reference count: 20
- Key outcome: New method for creating on-manifold explanations for high-dimensional text classifiers by analyzing gradient norms and cosine similarities across an ensemble of trained models.

## Executive Summary
This paper introduces a novel approach for generating interpretable explanations for high-dimensional text classifiers by leveraging recent theoretical insights about gradient behavior on low-dimensional data manifolds. The method identifies words whose gradients are mostly on-manifold by analyzing gradient norms and cosine similarities across an ensemble of trained models, effectively filtering out off-manifold artifacts that can lead to misleading explanations.

## Method Summary
The method computes gradients of the classifier with respect to input embeddings, separates them into word-level components, and analyzes each word's gradient norm and cosine similarity across an ensemble of surrogate classifiers. Words with high gradient norms are filtered out as potentially off-manifold, while words with high average cosine similarity across models are selected as explanation candidates. This approach aims to identify stable, on-manifold features that truly contribute to classification decisions rather than artifacts of model initialization or adversarial directions.

## Key Results
- Method demonstrated on sentiment analysis of IMDB reviews and malware detection in PowerShell scripts
- Shows improved explanations compared to standard methods like gradient max-norm, LIME, and SHAP
- Successfully identifies words that are both on-manifold and consistently important across multiple trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-manifold gradients have large norms and low cosine similarity across differently initialized networks, enabling identification of on-manifold features.
- Mechanism: The method filters out words whose gradients have high norms (indicating off-manifold location) and low average cosine similarity across an ensemble of models (indicating instability in attribution).
- Core assumption: The data approximately lies on a low-dimensional linear subspace, and the classifier gradients behave according to the theoretical bounds proved in Melamed et al. [2023].
- Evidence anchors:
  - [abstract]: "gradients of neural networks trained on low-dimensional data manifolds have large norms when off-manifold and low cosine similarity between different networks"
  - [section]: "the off-manifold gradients tend to have a big norm... the off-manifold gradients are affected mainly by the initialization... will result in highly uncorrelated off-manifold gradients"
  - [corpus]: Weak evidence - no direct matches in related papers for this specific theoretical claim.
- Break condition: If the data manifold is not approximately linear or if the data points are too far from the manifold, the gradient norm and similarity assumptions may not hold.

### Mechanism 2
- Claim: In high-dimensional text inputs, gradient norms can be decomposed per word to identify individual word importance while avoiding adversarial directions.
- Mechanism: By computing gradient norms and cosine similarities for each word separately, the method isolates words whose attribution is stable and lies on the data manifold, rather than being influenced by off-manifold perturbations.
- Core assumption: The text input embedding allows meaningful decomposition of the gradient into word-level components, and each word's gradient behavior reflects its contribution to the classifier decision.
- Evidence anchors:
  - [section]: "Separating the input gradient into n words' gradients of dimension p... look at each word's gradient norm to determine its significance"
  - [corpus]: Weak evidence - related papers discuss text classifier explainability but not this specific gradient decomposition approach.
- Break condition: If the word embeddings are not meaningful or if the gradient decomposition does not reflect true feature importance, the method may fail to identify correct words.

### Mechanism 3
- Claim: The ensemble-based cosine similarity averaging provides robustness against individual model instabilities in gradient attribution.
- Mechanism: By averaging cosine similarities across multiple models trained on the same data distribution, the method reduces the impact of model-specific gradient artifacts and identifies words with consistent attribution across models.
- Core assumption: Different models trained on the same data distribution will produce similar on-manifold gradients for the same input, but divergent off-manifold gradients due to initialization differences.
- Evidence anchors:
  - [section]: "Let {Ni}i=1t be our surrogate classifiers ensemble... We denote by gi the gradient... Similarly to gj C, we denote gj i as the gradient... Now, for any j∈[n], we look at αgj C=1t∑i=0|SC(gj C,gj i)|"
  - [corpus]: No direct evidence in related papers for this specific ensemble-based similarity approach.
- Break condition: If the ensemble models are not sufficiently diverse or if they all share the same off-manifold gradient artifacts, the averaging may not provide the intended robustness.

## Foundational Learning

- Concept: Low-dimensional manifold learning
  - Why needed here: The method relies on the assumption that high-dimensional text data lies on or near a low-dimensional manifold, which is fundamental to the theoretical justification for filtering gradients.
  - Quick check question: How would you verify that your text data approximately lies on a low-dimensional subspace before applying this method?

- Concept: Gradient-based attribution methods
  - Why needed here: Understanding how gradients are computed and used for feature attribution is essential for implementing and debugging the method.
  - Quick check question: What is the difference between using gradient norms versus gradient directions for feature attribution in text classifiers?

- Concept: Ensemble methods and model averaging
  - Why needed here: The method uses an ensemble of models to compute average cosine similarities, which requires understanding how to train multiple models and aggregate their outputs.
  - Quick check question: Why might averaging cosine similarities across multiple models be more robust than using a single model for gradient attribution?

## Architecture Onboarding

- Component map: Text embedding layer (n words × p dimensions) -> Classifier model (neural network) -> Gradient computation module (∂C/∂x) -> Ensemble of surrogate classifiers -> Norm thresholding component -> Cosine similarity computation module -> Word selection module (max cosine similarity with norm threshold)

- Critical path:
  1. Compute classifier gradients for input
  2. Compute surrogate model gradients for same input
  3. Calculate per-word gradient norms
  4. Filter words below norm threshold
  5. Compute average cosine similarities for filtered words
  6. Select words with maximum average cosine similarity

- Design tradeoffs:
  - Number of surrogate models: More models increase robustness but require more computation
  - Norm threshold selection: Too low misses important words, too high includes off-manifold artifacts
  - Ensemble diversity: Should balance between similar enough to share on-manifold features and different enough to expose off-manifold variations

- Failure signatures:
  - All words filtered out by norm threshold → threshold too high
  - Selected words don't align with human intuition → manifold assumption may be violated
  - Method performs worse than simple gradient norm → ensemble not diverse enough or threshold poorly chosen

- First 3 experiments:
  1. Test on simple synthetic data where ground truth manifold is known
  2. Compare selected words against human-annotated important words on real dataset
  3. Vary number of ensemble models and observe stability of selected words

## Open Questions the Paper Calls Out

- Question: How does the performance of the on-manifold explanation method vary with the choice of norm threshold across different datasets and tasks?
- Basis in paper: [explicit] The paper mentions that a norm threshold of 0.1 was chosen for both the IMDB and PowerShell datasets, but suggests that the relevant threshold should be of size O(1/√p) and that users should test their dataset's gradient norms distribution.
- Why unresolved: The paper only demonstrates the method with a single threshold value for each dataset. The optimal threshold may vary depending on the specific characteristics of the data and the classifier architecture.
- What evidence would resolve it: A systematic study comparing the performance of the method across a range of threshold values for multiple datasets and tasks, showing how the choice of threshold affects the quality and interpretability of the explanations.

## Limitations
- The method's effectiveness depends on the validity of the low-dimensional manifold assumption for text data, which may not hold for all domains.
- Optimal ensemble size and norm threshold selection are not fully characterized, requiring task-specific tuning.
- The method requires training multiple classifier models, increasing computational cost compared to single-model approaches.

## Confidence
- **Medium**: The theoretical foundation relies heavily on the assumption that high-dimensional text data lies on or near a low-dimensional manifold.
- **Medium**: The ensemble size and composition significantly impact the method's effectiveness, but optimal settings are not fully characterized.
- **High**: The method demonstrates improved explanations on specific tasks (sentiment analysis and malware detection).

## Next Checks
1. Cross-domain validation: Apply the method to diverse text classification tasks (e.g., topic classification, question answering, legal document analysis) to assess generalizability beyond sentiment and malware detection.

2. Ablation study: Systematically vary ensemble size (1, 3, 5, 10 models) and measure the impact on explanation quality and computational efficiency to identify optimal trade-offs.

3. Ground truth comparison: Conduct human evaluation studies comparing the method's explanations against human-annotated important words across multiple annotators to quantify alignment with human judgment.