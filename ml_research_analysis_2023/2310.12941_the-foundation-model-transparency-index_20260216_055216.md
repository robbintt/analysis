---
ver: rpa2
title: The Foundation Model Transparency Index
arxiv_id: '2310.12941'
source_url: https://arxiv.org/abs/2310.12941
tags:
- transparency
- data
- foundation
- developers
- indicators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The Foundation Model Transparency Index (FMTI) introduces a comprehensive\
  \ set of 100 indicators to assess the transparency of major foundation model developers.\
  \ Scoring 10 developers (e.g., OpenAI, Google, Meta) across 3 domains\u2014upstream\
  \ resources, model details, and downstream use\u2014FMTI reveals significant opacity\
  \ in the foundation model ecosystem."
---

# The Foundation Model Transparency Index

## Quick Facts
- arXiv ID: 2310.12941
- Source URL: https://arxiv.org/abs/2310.12941
- Reference count: 40
- Primary result: Comprehensive transparency assessment of 10 major foundation model developers using 100 indicators across 3 domains

## Executive Summary
The Foundation Model Transparency Index (FMTI) introduces a comprehensive set of 100 indicators to assess the transparency of major foundation model developers. Scoring 10 developers (e.g., OpenAI, Google, Meta) across 3 domains—upstream resources, model details, and downstream use—FMTI reveals significant opacity in the foundation model ecosystem. No developer fully discloses information about data sources, labor practices, or downstream impacts. Open developers (e.g., Meta, Hugging Face) score higher than closed ones, particularly on upstream matters. FMTI provides actionable recommendations for developers, deployers, and policymakers to improve transparency, emphasizing its role in fostering accountability and responsible innovation.

## Method Summary
The FMTI methodology involves defining 100 fine-grained indicators across 3 domains (upstream resources, model details, downstream use) and 23 subdomains. A structured search protocol gathers publicly available information for each indicator-model pair. Binary scoring (0 or 1) is applied based on evidence, with developers allowed to contest scores before finalization. The process includes inter-rater reliability checks, developer feedback loops, and comprehensive analysis of domain/subdomain breakdowns, correlations, and open/closed comparisons.

## Key Results
- No developer achieves full transparency; significant gaps exist in data sources, labor practices, and downstream impacts
- Open developers (Meta, Hugging Face) score higher than closed developers, especially on upstream matters
- 82 of the 100 indicators are achieved by at least one developer, indicating obtainable headroom for transparency improvements

## Why This Works (Mechanism)

### Mechanism 1
The FMTI index structure drives developer transparency improvements by providing a concrete, comparable scoring baseline. By scoring all major developers on 100 fine-grained indicators, FMTI creates a shared vocabulary and reference point. Developers can directly compare their scores to competitors, identify gaps, and adopt best practices already demonstrated by others.

### Mechanism 2
Transparency requirements are most effective when tied to concrete, reproducible evaluation standards. FMTI uses binary scoring on clearly defined indicators with publicly accessible justification. This reduces ambiguity and makes it harder for developers to game the system with performative transparency.

### Mechanism 3
Policy and industry alignment on transparency indicators increases compliance and adoption. FMTI's indicators align with existing regulatory proposals (e.g., EU AI Act, White House commitments), making it a practical tool for policymakers and developers to coordinate on transparency standards.

## Foundational Learning

- **Concept: Composite indexes and their methodology**
  - Why needed: FMTI is a composite index aggregating 100 indicators into scores; understanding how indexes work is key to interpreting FMTI's design and limitations
  - Quick check: What are the main advantages and disadvantages of using a composite index to measure transparency?

- **Concept: Binary scoring and its implications**
  - Why needed: FMTI uses binary scoring; knowing how this affects granularity, subjectivity, and potential gaming is critical for evaluating FMTI's effectiveness
  - Quick check: How might binary scoring both simplify and limit the assessment of transparency?

- **Concept: Transparency in AI governance and its challenges**
  - Why needed: FMTI is fundamentally about transparency; understanding why transparency matters (and its limits) contextualizes FMTI's purpose and potential impact
  - Quick check: What are the main arguments for and against transparency in AI development, and how does FMTI address them?

## Architecture Onboarding

- **Component map**: Indicator specification (100 items across 3 domains, 23 subdomains) -> Scoring engine (binary assignment per indicator per developer) -> Justification tracker (sources and rationale per score) -> Feedback loop (developer review and contest process) -> Analysis pipeline (domain/subdomain breakdowns, correlations, open/closed comparisons) -> Public reporting (website, GitHub materials)

- **Critical path**: 1. Define indicators and finalize list 2. Select developers and flagship models 3. Execute search protocol for each indicator-model pair 4. Initial binary scoring and disagreement resolution 5. Developer feedback and score updates 6. Finalize scores, generate analysis, publish

- **Design tradeoffs**: Binary vs. graded scoring: Simplicity vs. nuance; Public vs. private scoring: Accountability vs. developer pushback; Fixed vs. evolving indicators: Consistency vs. relevance

- **Failure signatures**: Low inter-rater reliability (>15% disagreement); No developer feedback or universal score disputes; Analysis shows no meaningful variance across developers; Public confusion about indicator definitions

- **First 3 experiments**: 1. Run scoring on a small set of indicators (e.g., 10) across all developers to test inter-rater reliability and search protocol efficiency 2. Conduct a pilot developer feedback round with one company to refine the contest process and justification format 3. Generate initial domain-level score distributions to check for expected patterns (e.g., upstream worst, open better than closed)

## Open Questions the Paper Calls Out

### Open Question 1
How does the transparency of foundation model developers evolve over time, and what factors influence these changes? The paper introduces FMTI as a tool for tracking transparency over time but does not provide longitudinal data or analysis of historical or future changes.

### Open Question 2
What are the specific trade-offs between transparency and other factors like competitive advantage, privacy, and safety in the development of foundation models? The paper acknowledges potential trade-offs but does not provide detailed analysis of how these factors interact or how developers navigate them in practice.

### Open Question 3
How can transparency requirements be designed to be effective and equitable across different types of foundation model developers, including open and closed developers, large and small companies, and those in different regions? The paper discusses the importance of transparency for all developers but does not provide specific recommendations for tailoring requirements to different contexts.

## Limitations
- Reliance on publicly available information may miss important disclosures in non-public communications
- Binary scoring may oversimplify complex transparency decisions and miss important nuances
- Potential sampling bias in selecting developers and models could affect generalizability

## Confidence

- **High Confidence**: The methodology for collecting and scoring transparency indicators is clearly specified and reproducible. The observation that no developer achieves full transparency is well-supported by the data.
- **Medium Confidence**: Claims about FMTI driving industry transparency improvements are plausible but not yet empirically demonstrated. The correlation between openness and transparency scores is observed but not proven causal.
- **Low Confidence**: Predictions about FMTI's influence on policy adoption and regulatory compliance remain speculative without evidence of actual policymaker engagement.

## Next Checks

1. Track whether developers explicitly reference FMTI scores in public transparency updates or corporate reports over the next 12 months.

2. Conduct a controlled experiment where developers receive FMTI feedback and measure subsequent transparency changes versus a control group.

3. Survey policymakers and regulators to determine if and how they reference FMTI when developing transparency requirements or evaluating compliance.