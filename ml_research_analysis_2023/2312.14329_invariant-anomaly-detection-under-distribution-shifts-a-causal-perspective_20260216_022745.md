---
ver: rpa2
title: 'Invariant Anomaly Detection under Distribution Shifts: A Causal Perspective'
arxiv_id: '2312.14329'
source_url: https://arxiv.org/abs/2312.14329
tags:
- anomaly
- regularization
- shifts
- environments
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of anomaly detection under distribution
  shifts, where the assumption that training and test data are drawn from the same
  distribution breaks down. The authors leverage causal inference to develop a novel
  regularization term, partial conditional invariant regularization (PCIR), which
  minimizes discrepancies between representations from different environments.
---

# Invariant Anomaly Detection under Distribution Shifts: A Causal Perspective

## Quick Facts
- arXiv ID: 2312.14329
- Source URL: https://arxiv.org/abs/2312.14329
- Authors: [Not specified in source]
- Reference count: 40
- Primary result: Models regularized with partial conditional invariant regularization (PCIR) showed up to 20% improvement in out-of-distribution AUROC compared to unregularized baselines

## Executive Summary
This paper addresses the challenge of anomaly detection under distribution shifts, where training and test data come from different distributions. The authors propose a novel regularization technique called partial conditional invariant regularization (PCIR) that leverages causal inference to learn invariant representations. By minimizing the maximum mean discrepancy (MMD) between representations from different environments while conditioning on the normal class label, PCIR ensures representations are robust to both domain and covariate shifts while retaining informativeness about normal vs. anomalous content.

## Method Summary
The method introduces PCIR as a regularization term that minimizes MMD between representations from different environments conditioned on normal samples (W=0). This regularization is integrated into six baseline anomaly detection methods (STFPM, Reverse Distillation, CFA, MeanShift, CSI, Red PANDA) and tested on synthetic datasets (MNIST/Fashion-MNIST with manipulated factors) and real-world datasets (Camelyon17 for domain shift, Waterbirds for shortcut learning). The approach enforces statistical independence between representations and environments given the normal class label, leading to more stable and generalizable anomaly detection performance.

## Key Results
- AUROC scores increased by up to 20% compared to unregularized baselines under distribution shifts
- Consistent performance improvements across both domain shifts (Camelyon17) and covariate shifts (Waterbirds)
- All six tested anomaly detection methods showed enhanced robustness when regularized with PCIR
- Improvements observed in both in-distribution and out-of-distribution settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing partial conditional invariance via MMD regularization leads to representations robust to both domain and covariate shifts.
- Mechanism: The regularization term minimizes MMD between representations from different environments conditioned on W=0, enforcing statistical independence between representations and environment given normal class label.
- Core assumption: Style features (Xe) are conditionally independent of environment when conditioned on W (normal/anomalous label).
- Evidence anchors: Abstract mentions "significant improvements in out-of-distribution performance under both domain and covariate shifts"; section discusses deriving necessary conditions for invariant representations.

### Mechanism 2
- Claim: PCIR improves both in-distribution and out-of-distribution anomaly detection performance.
- Mechanism: By discouraging environment-specific style features, the regularization forces models to focus on semantically meaningful features distinguishing normal from abnormal objects.
- Core assumption: Training data contains sufficient normal sample diversity to learn robust semantic representations.
- Evidence anchors: Abstract states "all tested models exhibited an enhancement in performance"; section reports "marked increased robustness" and "consistent pattern of performance enhancement."

### Mechanism 3
- Claim: Causal graph formalization clarifies requirements for invariant representations in anomaly detection.
- Mechanism: Explicit modeling of causal relationships identifies that invariant representations must be measurable with respect to content features while independent of environment.
- Core assumption: Causal relationships between variables can be accurately represented in a graphical model with valid conditional independences.
- Evidence anchors: Section introduces causal graph for anomaly detection and uses d-separation for analysis.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD measures distribution discrepancies between representations from different environments in the regularization term
  - Quick check question: What property must the kernel used in MMD satisfy to ensure that MMD=0 implies the two distributions are identical?

- Concept: D-separation and conditional independence
  - Why needed here: D-separation identifies conditional independences in the causal graph characterizing when representations are invariant to environment
  - Quick check question: Given a causal graph, how would you determine whether two variables are d-separated given a set of observed variables?

- Concept: Mutual information and representation learning
  - Why needed here: Informativeness requirement for representations is formalized using mutual information
  - Quick check question: How does maximizing mutual information between input and representation relate to the reconstruction objective in autoencoders?

## Architecture Onboarding

- Component map: Input data and environment labels -> Encoder -> Representations -> MMD computation module -> Combined loss -> Backpropagation -> Trained encoder -> Anomaly scoring module
- Critical path: 1) Input data and environment labels fed to encoder; 2) Encoder produces representations; 3) MMD computation calculates regularization between representations from different environments; 4) Combined loss (reconstruction/contrastive + MMD regularization) backpropagated; 5) Trained encoder used with anomaly scoring module to identify anomalies in test data
- Design tradeoffs: Regularization weight (too high causes collapsed representations, too low provides insufficient invariance); choice of kernel for MMD (affects computational efficiency and sensitivity); backbone architecture complexity (more complex backbones capture more features but are harder to regularize)
- Failure signatures: Performance degradation on in-distribution data (regularization weight too high); no improvement on out-of-distribution data (MMD kernel inappropriate or environments not well-separated); mode collapse (regularization dominating learning process)
- First 3 experiments: 1) Run baseline AD method without PCIR on synthetic dataset with known covariate shifts; 2) Add PCIR regularization with small weight (0.001) and observe performance changes; 3) Sweep regularization weight across several orders of magnitude (0.001, 0.01, 0.1, 1, 10) to find optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of PCIR vary when applied to anomaly detection methods that do not rely on representation learning?
- Basis in paper: The paper focuses on deep anomaly detection methods that utilize representation learning, but does not explore PCIR's applicability to non-representation learning-based methods.
- Why unresolved: Experiments and theoretical analysis are limited to representation learning-based anomaly detection methods.
- What evidence would resolve it: Conducting experiments applying PCIR to non-representation learning-based anomaly detection methods and comparing their performance to baseline methods without PCIR regularization.

### Open Question 2
- Question: How does the choice of the kernel function in the MMD calculation affect the performance of PCIR?
- Basis in paper: The paper mentions that MMD can be computed using Gaussian kernels but does not explore the impact of different kernel functions on PCIR's performance.
- Why unresolved: The paper uses a fixed Gaussian kernel for MMD calculation without investigating the effects of alternative kernel functions.
- What evidence would resolve it: Experimenting with different kernel functions (e.g., Laplacian, Cauchy) in the MMD calculation and evaluating their impact on PCIR's performance across various anomaly detection methods and datasets.

### Open Question 3
- Question: How does PCIR perform in scenarios where distribution shifts are not only due to environmental factors but also due to changes in the underlying data generation process?
- Basis in paper: The paper primarily focuses on distribution shifts caused by environmental factors and does not explicitly address shifts resulting from changes in the data generation process itself.
- Why unresolved: The causal graph and theoretical analysis assume that environment is the sole cause of distribution shifts.
- What evidence would resolve it: Conducting experiments where distribution shifts are induced by modifying the underlying data generation process (e.g., changing parameters of a generative model) and evaluating PCIR's performance in such scenarios.

## Limitations
- The causal framework assumes known environment labels during training, which may not always be available in real-world settings
- The assumption that style features are conditionally independent of environment given the normal/anomalous label is critical but difficult to verify in practice
- Effectiveness may be limited when content and style features are heavily entangled across environments

## Confidence
- High confidence: Empirical improvements in out-of-distribution performance are well-demonstrated across multiple datasets and baseline methods
- Medium confidence: Theoretical justification via causal graphs and d-separation is sound but real-world applicability depends on how well assumed causal structure matches reality
- Low confidence: Generalizability to scenarios without explicit environment labels or with more complex causal relationships remains untested

## Next Checks
1. Environment-agnostic validation: Test PCIR performance when environment labels are unknown or need to be inferred using clustering or domain adversarial training to approximate environment partitions
2. Causal structure sensitivity: Systematically vary assumed causal graph structure to assess robustness to misspecification
3. Scalability to real-world complexity: Evaluate PCIR on industrial-scale anomaly detection datasets with multiple confounding factors and high-dimensional inputs to verify practical utility beyond controlled experiments