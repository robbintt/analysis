---
ver: rpa2
title: Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems
arxiv_id: '2308.11137'
source_url: https://arxiv.org/abs/2308.11137
tags:
- user
- recommendation
- long-term
- datasets
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the validity of using public review datasets
  for benchmarking RL-based Interactive Recommender Systems (IRS). The authors compared
  RL-based models with a simple greedy baseline that selects the item with the highest
  one-step reward.
---

# Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems

## Quick Facts
- arXiv ID: 2308.11137
- Source URL: https://arxiv.org/abs/2308.11137
- Reference count: 27
- A greedy baseline that selects items with highest one-step reward outperforms RL models on public review datasets, suggesting minimal long-term effects in user feedback.

## Executive Summary
This paper challenges the validity of using public review datasets for benchmarking RL-based Interactive Recommender Systems (IRS). Through systematic comparison of RL-based models with a simple greedy baseline, the authors demonstrate that RL models fail to outperform greedy approaches across most tested datasets. Their analysis reveals that user feedback in public review datasets contains minimal long-term effects, primarily consisting of instant responses like ratings without delayed signals such as dwell time or lifetime value. The study provides critical insights for the proper evaluation of RL-based IRS approaches and recommends including simple greedy baselines in future benchmarking efforts.

## Method Summary
The authors implement a unified Transformer architecture for multiple baseline models including Random, POP, SASRec, DQNR, NICF, SQN, SAC, DDPGR, and GreedyRM. They evaluate these models using a Transformer-based simulator across four public review datasets (EachMovie, MovieLens-1M, MovieLens-20M, Netflix). The evaluation includes varying discount factors from 0 to 0.99 and comparing greedy vs beam search policies to assess long-term effects. Performance is measured using average reward, precision, and recall metrics over 40 interaction steps.

## Key Results
- GreedyRM achieved best performance across all metrics for EachMovie, ML-20M, and Netflix datasets
- RL-based models show gradual performance decrease as discount factors increase, indicating future rewards carry minimal information
- Greedy search matches beam search performance by more than 99.5% across most datasets, confirming negligible long-term effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The greedy baseline outperforms RL models because the benchmark datasets lack delayed rewards.
- Mechanism: When rewards are purely instantaneous (ratings), the optimal policy is simply to always pick the item with the highest immediate reward, eliminating the need for future-state modeling.
- Core assumption: Public review datasets do not capture delayed feedback such as dwell time, lifetime value, or engagement beyond the rating.
- Evidence anchors:
  - [abstract] "user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value)"
  - [section 4.3] "GreedyRM achieved the best performance for all metrics for the EachMovie, ML-20M, and Netflix datasets"
- Break condition: If a dataset includes explicit delayed reward signals, the greedy baseline will no longer be optimal and RL methods should outperform it.

### Mechanism 2
- Claim: Increasing the discount factor degrades performance because future rewards carry no extra information.
- Mechanism: A higher discount factor gives more weight to future states, but if future rewards are uninformative, this introduces noise rather than signal, reducing overall cumulative reward.
- Core assumption: The MDP transition structure and reward function do not encode long-term dependencies in these datasets.
- Evidence anchors:
  - [section 4.4] "RL-based models display a gradual decrease in cumulative rewards as the models put more weight on future rewards"
  - [section 4.3] "GreedyRM aims to recommend items that maximize the immediate one-step reward and is analogous to DQNR when its discount factor ùõæ is 0"
- Break condition: If future rewards become informative (e.g., incorporating dwell time or repeat engagement), increasing the discount factor should improve performance.

### Mechanism 3
- Claim: Beam search with k>1 does not significantly outperform greedy search, confirming minimal long-term effects.
- Mechanism: Beam search maintains multiple trajectories and ranks them by cumulative future reward; if long-term effects are negligible, all k-trajectories converge to the same best item, making k=1 sufficient.
- Core assumption: The MDP has a deterministic or near-deterministic reward structure where the next best action does not depend on deeper planning.
- Evidence anchors:
  - [section 4.5] "the greedy search matches the performance of the beam search by more than 99.5% for EachMovie, ML-20M, and Netflix dataset"
  - [section 4.3] "a simple greedy recommendation model will be able to directly maximize the cumulative rewards"
- Break condition: If the environment includes stochastic transitions or delayed rewards, beam search with larger k should yield better results than greedy.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The IRS is modeled as an MDP with states, actions, rewards, transitions, and discount factor.
  - Quick check question: What tuple defines an MDP in the context of IRS?

- Concept: Discount factor and cumulative reward
  - Why needed here: The discount factor ùõæ controls the trade-off between immediate and future rewards; understanding its impact is key to interpreting the ablation study.
  - Quick check question: How does changing ùõæ affect the agent's preference for short-term vs long-term rewards?

- Concept: Beam search as optimal policy approximation
  - Why needed here: Beam search is used to approximate the optimal policy in the simulator to compare against the greedy baseline.
  - Quick check question: Why is beam search with k=1 equivalent to the greedy policy?

## Architecture Onboarding

- Component map:
  Input embedding (sum of item embeddings, feedback embeddings, positional encodings) -> Transformer encoder (self-attention layers) -> Predictor head (two FC layers) -> Reward/Q-value estimation -> Action selection (argmax or policy sampling)

- Critical path:
  State encoding ‚Üí Predictor head ‚Üí Reward/Q-value estimation ‚Üí Action selection (argmax or policy sampling)

- Design tradeoffs:
  - Using a single unified Transformer architecture for all models enables fair comparison but may limit model-specific strengths.
  - High discount factors (ùõæ‚Üí1) increase computational burden without performance gain when long-term effects are minimal.

- Failure signatures:
  - Overfitting to the simulator if RMSE is low but test performance is poor.
  - RL models underperforming greedy baseline indicates the dataset lacks delayed rewards.
  - High variance across seeds suggests instability in training or evaluation.

- First 3 experiments:
  1. Verify that the simulator achieves low RMSE on held-out interactions.
  2. Run GreedyRM and DQNR with varying ùõæ on a small dataset to observe performance trends.
  3. Compare greedy vs beam search with k=10 on the same dataset to confirm long-term effects are negligible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the characteristics of datasets where long-term effects are actually significant for interactive recommendation systems?
- Basis in paper: [explicit] The authors explicitly state that public review datasets may not be appropriate for benchmarking RL-based IRS approaches because user feedback has minimal long-term effects, suggesting some datasets do have significant long-term effects
- Why unresolved: The paper only tests four public review datasets and concludes long-term effects are minimal in these, but doesn't identify what characteristics would make a dataset suitable for testing long-term effects
- What evidence would resolve it: Empirical studies comparing multiple types of datasets (review datasets, clickstream data, e-commerce transaction data, etc.) with varying levels of user engagement duration, item diversity, and feedback types to identify patterns where long-term effects become significant

### Open Question 2
- Question: How can we design recommendation tasks that require long-term planning to be optimal, rather than ones where greedy policies suffice?
- Basis in paper: [inferred] The paper demonstrates that greedy policies perform nearly as well as beam search across most datasets, implying the current recommendation tasks don't require sophisticated long-term planning
- Why unresolved: The paper identifies the problem but doesn't propose specific task modifications or reward structures that would make long-term planning necessary for optimal performance
- What evidence would resolve it: Development and validation of new recommendation datasets or synthetic environments where greedy policies demonstrably fail to achieve optimal performance, along with analysis of what task properties create this requirement

### Open Question 3
- Question: What are the implications of these findings for real-world deployment of RL-based interactive recommendation systems?
- Basis in paper: [explicit] The authors suggest that for proper evaluation of RL-based IRS approaches, simple greedy baselines should be included, implying RL approaches may not provide value in many practical scenarios
- Why unresolved: The paper focuses on benchmarking methodology rather than practical implications for when and where RL-based approaches would actually outperform simpler methods in production systems
- What evidence would resolve it: Large-scale A/B testing results comparing RL-based IRS implementations against strong greedy baselines across different types of recommendation platforms (streaming, e-commerce, social media) with varying user behaviors and business objectives

### Open Question 4
- Question: How do different types of delayed feedback (dwell time, lifetime value, etc.) interact with discount factors in determining the optimal recommendation strategy?
- Basis in paper: [explicit] The authors note that public review datasets lack delayed feedback types and only contain instant feedback like ratings, suggesting different feedback types might change the analysis
- Why unresolved: The experiments only use ratings as feedback and don't explore how incorporating different types of delayed feedback would affect the importance of long-term planning
- What evidence would resolve it: Experiments with datasets containing multiple feedback types (ratings, dwell time, purchase behavior, churn signals) showing how the relative importance of different feedback types affects the optimal discount factor and whether RL approaches outperform greedy baselines when these feedback types are incorporated

## Limitations
- Analysis is limited to four public review datasets with explicit ratings
- Simulation-based evaluation may not fully capture real-world user behavior complexity
- The study does not explore datasets with explicit delayed feedback signals

## Confidence
- Core finding (RL models underperform greedy baseline on tested datasets): Medium
- Broader claims about RL-based IRS in general: Low
- Recommendation methodology for tested datasets: High

## Next Checks
1. Replicate the analysis on datasets that explicitly include delayed feedback signals (e.g., dwell time, repeat engagement) to test the break conditions identified in the mechanisms.

2. Conduct a sensitivity analysis varying the minimum interaction threshold per user/item and the train/validation/test splits to assess result stability.

3. Compare simulator performance against real-world A/B test results on at least one dataset to validate the simulation-based evaluation approach.