---
ver: rpa2
title: 'Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English
  to Indian Languages'
arxiv_id: '2308.16075'
source_url: https://arxiv.org/abs/2308.16075
tags:
- translation
- multimodal
- image
- machine
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of visual context on multimodal
  neural machine translation (NMT) in noisy settings, specifically translating English
  to three Indian languages (Hindi, Bengali, Malayalam). The study explores whether
  incorporating image features improves translation quality when using a large-scale
  pre-trained unimodal NMT system.
---

# Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages

## Quick Facts
- arXiv ID: 2308.16075
- Source URL: https://arxiv.org/abs/2308.16075
- Authors: 
- Reference count: 40
- Primary result: Visual context improves multimodal NMT translation quality most under high noise levels, with cropped images best for low noise and full images best for high noise.

## Executive Summary
This paper investigates how visual context impacts multimodal neural machine translation (NMT) from English to three Indian languages (Hindi, Bengali, Malayalam) under noisy conditions. Using a large-scale pre-trained unimodal NMT system, the study finds that images are largely redundant in clean settings but become increasingly valuable as noise in the source text increases. The research compares cropped versus full image features and explores whether models use visual features as complementary information or as a form of noise regularization during training.

## Method Summary
The study fine-tunes a pre-trained unimodal NMT model on multimodal data using both Selective Attention and Multimodal Transformer architectures. Image features are extracted using a pre-trained Vision Transformer (ViT), and synthetic noise is added to source text to simulate social media conditions. The experiments compare text-only baselines against multimodal models using cropped and full image features under different noise levels. Performance is measured using BLEU, chrF2, and TER scores across clean, low-noise, and high-noise test sets.

## Key Results
- Images are redundant in non-noisy settings but increasingly valuable as noise levels rise
- Cropped image features work best for low-noise scenarios, while full image features perform better under high noise
- Random images slightly outperform text-only models in noisy conditions, suggesting models use images as noise regularization during training
- Existing datasets are not sufficiently context-aware to train models to effectively utilize visual features

## Why This Works (Mechanism)

### Mechanism 1
Visual context improves translation quality more under high noise levels than under low noise or clean text because the model learns to rely on visual features as a form of robust feature extraction that complements corrupted text features. The core assumption is that visual features contain complementary information not easily corrupted by textual noise, serving as a stable reference point. Break condition: If noise is so high that visual features become unreliable or misaligned, or if the visual features themselves are corrupted.

### Mechanism 2
Cropped image features outperform full image features under low noise, while full image features outperform cropped features under high noise. This occurs because cropped images focus on the relevant region tied to the sentence, providing precise but limited context under low noise, while full images provide richer context needed when textual information is degraded under high noise. Break condition: If the bounding boxes are poorly aligned with the text or the full image contains irrelevant distractors that outweigh the benefit.

### Mechanism 3
Random images slightly outperform text-only models under noisy conditions because during training, exposure to image features (even irrelevant ones) teaches the model to be robust to input perturbations. At inference time, any image acts as a stabilizing prior. Break condition: If random images introduce bias or if the model overfits to random patterns, or if noise level exceeds the model's capacity to generalize.

## Foundational Learning

- Concept: Multimodal learning integration
  - Why needed here: Understanding how visual and textual encoders interact and fuse is crucial for interpreting why visual context helps under noise.
  - Quick check question: What is the role of the gated fusion layer in the selective attention architecture?

- Concept: Transformer attention mechanisms
  - Why needed here: The selective attention and multimodal transformer architectures rely on multi-head attention; knowing how queries, keys, and values work is essential.
  - Quick check question: In the selective attention layer, what serves as the query and what serves as key/value?

- Concept: Synthetic noise generation for robustness
  - Why needed here: The paper introduces controlled noise to simulate social media text; understanding noise types and levels is key to reproducing the experiments.
  - Quick check question: What are the three types of noise applied in the low-noise setting?

## Architecture Onboarding

- Component map: Text encoder (mBART) -> Image encoder (ViT) -> Selective attention or Multimodal transformer -> Gated fusion -> Decoder

- Critical path:
  1. Input text → text encoder → text embeddings
  2. Input image → ViT → image features
  3. Selective attention → fused representation
  4. Fused representation → decoder → target text

- Design tradeoffs:
  - Cropped vs full images: Cropped is precise but limited; full is rich but may include noise
  - Selective attention vs multimodal transformer: Selective attention learns to attend selectively; multimodal transformer concatenates features directly
  - Random vs actual images: Random images may act as regularization; actual images provide true context

- Failure signatures:
  - BLEU drops when noise level mismatches image type (e.g., cropped images under high noise)
  - Selective attention underperforms text-only baseline in non-noisy settings
  - Multimodal transformer degrades performance on challenge set even with full images

- First 3 experiments:
  1. Train text-only baseline on clean data, evaluate on clean, low-noise, and high-noise test sets
  2. Train multimodal model with cropped images on clean data, evaluate on all three noise levels
  3. Train multimodal model with full images on clean data, evaluate on all three noise levels

## Open Questions the Paper Calls Out

### Open Question 1
How can we design multimodal datasets that require images for translation, ensuring models learn to effectively utilize visual features even when trained on large unimodal corpora? This question arises because current multimodal datasets often contain examples where source text alone is sufficient for translation, preventing models from learning to use visual features effectively.

### Open Question 2
What is the optimal way to integrate visual features into multimodal NMT models to improve translation quality, especially in noisy settings? The study shows that images are often redundant in non-noisy settings and their impact varies with noise levels, indicating the need for more effective integration methods.

### Open Question 3
How does the quality and relevance of visual features impact the performance of multimodal NMT models? The paper finds that improvements with actual images can be achieved with random images, raising questions about the true impact of visual feature quality.

## Limitations
- Dataset dependency: VisualGenome's annotations are not sufficiently context-aware for effective visual context utilization, creating a ceiling effect
- Synthetic noise may not capture the complexity of real-world social media noise patterns, particularly for low-resource Indian languages
- Ambiguity in whether models truly learn complementary visual-linguistic representations or simply use any image as noise regularization

## Confidence
- High confidence: Images become increasingly valuable as noise levels increase
- Medium confidence: Cropped vs full image feature performance trade-offs
- Low confidence: Random images as noise regularization

## Next Checks
1. Manually annotate 100 random sentence-image pairs from VisualGenome for context relevance and compare against the model's attention weights to determine whether poor performance stems from dataset limitations or model architecture issues.

2. Apply the same multimodal models to actual social media posts from Indian language Twitter/X datasets with naturally occurring noise, measuring whether synthetic noise patterns accurately predict real-world performance gaps.

3. Measure the correlation between image-text alignment quality and multimodal model performance across different noise levels to determine whether visual context benefits depend on alignment quality rather than noise level alone.