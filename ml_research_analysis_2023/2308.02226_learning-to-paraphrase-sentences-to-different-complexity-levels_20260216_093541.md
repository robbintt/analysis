---
ver: rpa2
title: Learning to Paraphrase Sentences to Different Complexity Levels
arxiv_id: '2308.02226'
source_url: https://arxiv.org/abs/2308.02226
tags:
- sentence
- level
- simplification
- data
- cefr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the task of changing sentence complexity in
  three directions: simplification, complexification, and same-level paraphrasing.
  To train models for all three tasks, the authors construct two new unsupervised
  datasets labeled by weak classifiers and a rule-based approach, and compare them
  to one supervised dataset.'
---

# Learning to Paraphrase Sentences to Different Complexity Levels

## Quick Facts
- arXiv ID: 2308.02226
- Source URL: https://arxiv.org/abs/2308.02226
- Reference count: 32
- Key outcome: Introduces complexity-level paraphrasing (simplification, complexification, same-level) and achieves state-of-the-art results on ASSET simplification benchmark

## Executive Summary
This paper introduces a novel task of changing sentence complexity in three directions: simplification, complexification, and same-level paraphrasing. The authors construct two new unsupervised datasets labeled by weak classifiers and a rule-based approach, and compare them to one supervised dataset. Using these datasets for training, they perform extensive experiments on multitasking and prompting strategies with T5 models, achieving state-of-the-art results on the ASSET simplification benchmark and outperforming previous work on sentence-level targeting. They also establish benchmarks for how well Large Language Models perform on these tasks in a zero-shot setting.

## Method Summary
The authors train CEFR and FKGL classifiers to automatically label paraphrase data with complexity levels. They create three types of datasets (Newsela-Auto, ParaNMT-CEFR, ParaNMT-FKGL) and fine-tune T5 models on these datasets using absolute and relative prompting strategies. The models are evaluated on three benchmarks (ASSET, Newsela-Manual, ParaNMT-s) using SARI and FKGL metrics, with additional human evaluation on selected models.

## Key Results
- Achieves state-of-the-art results on ASSET simplification benchmark
- Outperforms previous work on sentence level targeting
- Demonstrates that absolute prompting generally outperforms relative prompting across tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weak classification based on CEFR levels can label paraphrase data for training complexity-level paraphrasing models.
- **Mechanism:** The authors use a CEFR classifier trained on a new CEFR-CEP dataset to label sentence pairs from ParaNMT. They select pairs with level differences â‰¥ 2 for simplification/complexification and pairs with identical levels for same-level paraphrasing.
- **Core assumption:** The CEFR classifier can accurately estimate sentence complexity levels, and these estimates align with human judgments of complexity differences.
- **Evidence anchors:**
  - [abstract]: "Our first automatic labeling method is rule-based according to Flesch-Kincaid Grade Level (FKGL). FKGL can be calculated automatically as a weighted score consisting of sentence length and syllable information (Kincaid et al., 1975)."
  - [section 3.3]: "We test both our CEFR classifier and FKGL on these 39 gold labels... We compare our CEFR classifier's predictions with those of FKGL."
  - [corpus]: Weak - the corpus only contains the CEFR-CEP dataset, which is automatically labeled based on the assumption that example sentences match the CEFR level of their corresponding word sense or grammar concept.
- **Break condition:** If the CEFR classifier's accuracy is too low (as evidenced by the 59.78% F1 on CEFR-CEP test data), the labeled datasets will be unreliable for training.

### Mechanism 2
- **Claim:** T5 models fine-tuned on unsupervised datasets labeled with complexity levels can perform sentence simplification, complexification, and same-level paraphrasing.
- **Mechanism:** The authors fine-tune T5 models on three types of datasets (Newsela-Auto, ParaNMT-CEFR, ParaNMT-FKGL) with prompts prepended to input sentences to specify the target complexity level or direction (relative vs. absolute).
- **Core assumption:** T5 models can learn to generate paraphrases with targeted complexity levels from labeled unsupervised data.
- **Evidence anchors:**
  - [abstract]: "Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies with T5 models."
  - [section 5.2]: "At inference time, we prepend the corresponding prompt to the beginning of each input sentence... We fine-tune 34 ablations on T5 (Raffel et al., 2020), a pretrained transformer."
  - [corpus]: Weak - the corpus only mentions the ParaNMT dataset being used, but doesn't provide evidence of its suitability for this task beyond its size.
- **Break condition:** If the unsupervised datasets are of poor quality or the prompts are ineffective, the fine-tuned models will not learn to generate targeted complexity paraphrases.

### Mechanism 3
- **Claim:** Absolute prompting with specific target levels outperforms relative prompting with complexity directions.
- **Mechanism:** The authors compare two prompting strategies: relative prompting (e.g., "level up:") and absolute prompting (e.g., "change to level X:"). They find that absolute prompting generally performs better, especially for complexification.
- **Core assumption:** Providing specific target levels in prompts gives the model clearer guidance than just indicating direction.
- **Evidence anchors:**
  - [abstract]: "Our absolute prompting models outperform previous level targeting work on the Newsela-Manual benchmark."
  - [section 6.1.5]: "For FKGL models, ABS prompting always performs better than REL prompting... This indicates that using a more complex prompting strategy incurs a greater performance cost as the number of tasks increases."
  - [corpus]: Weak - the corpus doesn't provide direct evidence for this mechanism, but the paper's ablation studies support it.
- **Break condition:** If the model fails to understand the absolute level specifications in prompts, this strategy will not provide an advantage over relative prompting.

## Foundational Learning

- **Concept:** Sentence complexity classification
  - **Why needed here:** The authors need to automatically label paraphrase data with complexity levels to train models for targeted complexity paraphrasing.
  - **Quick check question:** How does the CEFR classifier estimate sentence complexity, and what metrics are used to evaluate its accuracy?
- **Concept:** Prompting strategies for text generation
  - **Why needed here:** The authors experiment with different prompting strategies to guide T5 models in generating paraphrases with targeted complexity levels.
  - **Quick check question:** What is the difference between relative and absolute prompting, and how do they affect model performance?
- **Concept:** Unsupervised dataset construction for NLP tasks
  - **Why needed here:** The authors create unsupervised datasets by labeling paraphrase data with complexity levels, as supervised datasets for this task are scarce.
  - **Quick check question:** What criteria are used to select sentence pairs from ParaNMT for each complexity-level paraphrasing task?

## Architecture Onboarding

- **Component map:** CEFR classifier -> Label ParaNMT data -> Create three datasets -> Fine-tune T5 models -> Evaluate on benchmarks
- **Critical path:** The key steps are: 1) Train CEFR classifier on CEFR-CEP dataset, 2) Label ParaNMT data with CEFR and FKGL levels, 3) Create three datasets for each task, 4) Fine-tune T5 models with different prompting strategies, 5) Evaluate on benchmarks.
- **Design tradeoffs:** Using unsupervised data allows for larger datasets but may introduce noise. Absolute prompting provides clearer guidance but may be less flexible than relative prompting. Fine-tuning T5 models requires significant computational resources.
- **Failure signatures:** Poor model performance on benchmarks could indicate issues with the CEFR classifier, dataset quality, prompting strategy, or fine-tuning process. Degenerate repetitions in complexification outputs suggest problems with the FKGL labeling scheme.
- **First 3 experiments:**
  1. Train CEFR classifier on CEFR-CEP dataset and evaluate on held-out test set.
  2. Label ParaNMT data with CEFR levels and create simplification/complexification datasets.
  3. Fine-tune T5 model on CEFR-labeled ParaNMT data with relative prompting and evaluate on ASSET benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is CEFR-based automatic labeling compared to FKGL for creating unsupervised datasets for sentence simplification, complexification, and same-level paraphrasing tasks?
- Basis in paper: [explicit] The paper compares the performance of models trained on datasets labeled using CEFR classification versus FKGL. It mentions that FKGL models often outperform CEFR models on complexification, but CEFR method is the most robust automatic labeling method overall.
- Why unresolved: The paper suggests that the CEFR method is more robust, but it doesn't provide a clear winner for all tasks. The effectiveness of each method may depend on the specific task and dataset.
- What evidence would resolve it: A comprehensive comparison of models trained on datasets labeled using CEFR classification and FKGL across various tasks and datasets, measuring their performance using appropriate metrics.

### Open Question 2
- Question: How do Large Language Models (LLMs) perform on sentence simplification, complexification, and same-level paraphrasing tasks compared to fine-tuned models?
- Basis in paper: [explicit] The paper includes an exploratory investigation into the performance of LLMs on these tasks in a zero-shot setting. It finds that GPT-3.5-Turbo outperforms other models but doesn't surpass fine-tuned T5 models on complexification.
- Why unresolved: The paper only provides a brief investigation into LLM performance, and the results may vary depending on the specific LLM, task, and prompting strategy used.
- What evidence would resolve it: A comprehensive evaluation of various LLMs on these tasks using different prompting strategies and settings, comparing their performance to fine-tuned models.

### Open Question 3
- Question: What is the optimal multitasking configuration for training models on sentence simplification, complexification, and same-level paraphrasing tasks?
- Basis in paper: [explicit] The paper conducts ablation studies on multitasking configurations, comparing single-task, two-task, and three-task models. It finds no clear winner among multitasking configurations.
- Why unresolved: The paper's results suggest that the optimal configuration may depend on the specific task and dataset. More research is needed to determine the best approach for each task.
- What evidence would resolve it: A thorough investigation of multitasking configurations across various tasks and datasets, measuring their performance using appropriate metrics and identifying the best approach for each task.

## Limitations

- The CEFR classifier achieves only 59.78% F1 on the CEFR-CEP test set, raising concerns about the reliability of labels for training
- FKGL-based labeling produces degenerate repetitions in complexification outputs, suggesting fundamental issues with this approach
- Lack of detailed specifications for critical parameters like filtering thresholds and prompt templates, making exact reproduction challenging

## Confidence

**High Confidence:**
- The core methodology of using T5 models with prompting strategies for complexity-level paraphrasing is sound and well-supported by extensive experiments
- The finding that absolute prompting generally outperforms relative prompting is consistently demonstrated across multiple benchmarks
- The superior performance of CEFR-labeled models over FKGL-labeled models on most tasks is clearly evidenced

**Medium Confidence:**
- The claim of state-of-the-art results on ASSET simplification, as this is based on a single metric (SARI) and doesn't fully account for recent advances in the field
- The effectiveness of multitasking across all three tasks, as the performance gains are modest and task-specific training sometimes performs better

**Low Confidence:**
- The reliability of FKGL-labeled datasets for complexification, given the observed degenerate outputs
- The generalizability of results to languages beyond English, as all experiments focus exclusively on English

## Next Checks

1. **CEFR Classifier Validation:** Conduct a thorough evaluation of the CEFR classifier on human-annotated data to determine its reliability for labeling paraphrase datasets, focusing on its performance on complexity differences relevant to simplification and complexification tasks.

2. **Prompt Template Testing:** Systematically test the impact of different absolute prompt templates on model performance, including variations in prompt phrasing and level specifications, to identify optimal prompt formulations for each complexity direction.

3. **FKGL Output Analysis:** Perform detailed qualitative analysis of complexification outputs from FKGL-labeled models to identify specific failure patterns and develop targeted mitigation strategies, such as improved filtering or alternative complexity measures.