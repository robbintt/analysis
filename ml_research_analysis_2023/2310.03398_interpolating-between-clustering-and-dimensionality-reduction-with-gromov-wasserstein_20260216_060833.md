---
ver: rpa2
title: Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein
arxiv_id: '2310.03398'
source_url: https://arxiv.org/abs/2310.03398
tags:
- problem
- clustering
- graph
- learning
- reduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gromov-Wasserstein Dimensionality Reduction
  (GW-DR), a method that simultaneously performs dimensionality reduction and clustering
  by leveraging optimal transport and Gromov-Wasserstein distances. The approach minimizes
  a semi-relaxed Gromov-Wasserstein discrepancy between input and embedding affinity
  matrices, allowing for simultaneous reduction in sample and feature sizes.
---

# Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein

## Quick Facts
- arXiv ID: 2310.03398
- Source URL: https://arxiv.org/abs/2310.03398
- Reference count: 40
- Method interpolates between clustering and DR using Gromov-Wasserstein distances

## Executive Summary
This paper introduces Gromov-Wasserstein Dimensionality Reduction (GW-DR), a unified framework that simultaneously performs dimensionality reduction and clustering by leveraging optimal transport and Gromov-Wasserstein distances. The method minimizes a semi-relaxed Gromov-Wasserstein discrepancy between input and embedding affinity matrices, allowing for simultaneous reduction in sample and feature sizes. The approach can recover classical DR objectives when embedding size matches input size, and provides hard clustering when embedding size is unconstrained. Experiments on image datasets demonstrate competitive clustering performance with meaningful low-dimensional prototypes.

## Method Summary
The method operates by constructing pairwise similarity matrices from input data and embedding prototypes, then solving a semi-relaxed Gromov-Waterstein problem that alternates between finding optimal transport plans and updating embeddings. The framework interpolates between standard DR (when embedding size equals input) and clustering (when unconstrained), with an optional fused GW extension that balances clustering and feature preservation through hyperparameter α. The non-convex optimization is solved using Block Coordinate Descent, alternating between GW transport plan computation and embedding updates via gradient descent.

## Key Results
- Achieves competitive clustering performance with homogeneity scores of 74-88% across different embedding sizes
- Recovers classical DR objectives when embedding size matches input dimensionality
- Demonstrates that embedding-dependent clustering (α > 0) is important for structurally meaningful representations
- Produces meaningful low-dimensional prototypes that can be used for clustering and interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method achieves hard clustering when the embedding size matches the input by leveraging semi-relaxed Gromov-Wasserstein barycenters with PSD matrices
- Mechanism: When CX is positive semidefinite and hX is uniform, the srGW barycenter problem becomes concave, causing the optimal transport plan to converge to a scaled membership matrix with single non-zero entries per row, yielding hard cluster assignments
- Core assumption: The graph affinity matrix CX must be positive semidefinite (PSD) or negative semidefinite (NSD)
- Evidence anchors:
  - [abstract]: "When the embedding's dimensionality is unconstrained, we show that the OT plan delivers a competitive hard clustering"
  - [section]: "Theorem 1. Let CX ∈ RN ×N and hX ∈ ΣN a vector in the probability simplex. If g(U) = vec(U)⊤ (CX ⊗K CX) vec(U) is convex on U(hX , hX) , then srGWB with L = L2 admits scaled membership matrices as optimum"
  - [corpus]: Weak evidence - no direct mention of PSD/NSD conditions in related works
- Break condition: If CX is not PSD/NSD, the function g(U) becomes non-convex, and the optimal transport plan will produce soft rather than hard cluster assignments

### Mechanism 2
- Claim: The method recovers classical DR objectives when embedding size equals input size by minimizing the discrepancy between CX and CZ matrices
- Mechanism: When n = N, the semi-relaxed GW problem reduces to matching the full affinity matrices, which corresponds to standard DR objectives like MDS or kernel PCA depending on the loss function L
- Core assumption: The affinity matrix CZ must be properly parameterized by the embedding Z according to the chosen DR method (e.g., CZ = ZZ⊤ for PCA)
- Evidence anchors:
  - [abstract]: "When the embedding sample size matches that of the input, our model recovers classical popular DR models"
  - [section]: "When CZ = ZZ ⊤ mimicking e.g PCA... GW-DR boils down to a srGW barycenter problem constrained to have at most rank d"
  - [corpus]: Weak evidence - related works mention DR techniques but don't explicitly connect them to GW framework
- Break condition: If the parameterization of CZ doesn't match the chosen DR method, the objective won't recover classical DR properties

### Mechanism 3
- Claim: The fused GW framework with α ∈ [0,1] enables trade-off between clustering and feature preservation
- Mechanism: By interpolating between GW loss and Wasserstein cost on features, the method can balance between producing prototypes that respect input structure (high α) versus prototypes that are close to input samples in feature space (low α)
- Core assumption: The hyperparameter α must be properly tuned to balance the two competing objectives
- Evidence anchors:
  - [section]: "we further propose to extend GW-DR to the Fused Gromov-Wasserstein framework... interpolates linearly, via a hyperparameter α ∈ [0, 1], between our objective and a linear OT cost"
  - [section]: "To determine whether this can be beneficial, we performed a grid search over different values of α"
  - [corpus]: No direct evidence - fused GW not mentioned in related works
- Break condition: If α is too close to 0, the method ignores embedding positions and produces meaningless prototypes; if too close to 1, it ignores feature information

## Foundational Learning

- Concept: Gromov-Wasserstein distances and optimal transport
  - Why needed here: The entire method relies on GW as the core mechanism for comparing graphs with different numbers of nodes
  - Quick check question: What is the key difference between standard OT and Gromov-Wasserstein distance?

- Concept: Graph affinity matrices and their properties
  - Why needed here: The method operates on pairwise similarity matrices CX and CZ, requiring understanding of PSD/NSD properties
  - Quick check question: What conditions must CX satisfy for Theorem 1 to guarantee hard clustering?

- Concept: Block Coordinate Descent optimization
  - Why needed here: The non-convex GW-DR problem is solved using BCD alternating between transport plan and embedding updates
  - Quick check question: Why does alternating between optimizing T and Z guarantee convergence to local optimum?

## Architecture Onboarding

- Component map:
  Input -> Affinity computation -> GW solver -> Embedding optimizer -> Output (prototypes Z and transport plan T)

- Critical path:
  1. Compute CX from input data X
  2. Initialize Z and T
  3. Alternate between solving srGW problem for fixed Z and optimizing Z for fixed T
  4. (Optional) Perform grid search for optimal α in fused GW
  5. Extract cluster assignments from T

- Design tradeoffs:
  - Choice of affinity matrix CX affects clustering quality and DR properties
  - Embedding size n controls number of prototypes (unconstrained case)
  - Dimension d must be ≤ n for rank constraints
  - Fused GW hyperparameter α balances clustering vs feature preservation

- Failure signatures:
  - Poor clustering: Check if CX is PSD/NSD; try different affinity computation
  - Vanishing gradients: Reduce learning rate or check initialization
  - Unstable BCD: Verify that both subproblems are being solved accurately enough

- First 3 experiments:
  1. Run GW-DR with n = N and compare to PCA/KPCA reconstruction error
  2. Run with unconstrained n and visualize prototypes to verify hard clustering
  3. Test fused GW with α values {0, 0.5, 1} to observe trade-off effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do optimal transport plans in Gromov-Wasserstein Dimensionality Reduction yield hard clustering assignments versus soft assignments?
- Basis in paper: The paper proves that under certain conditions (PSD matrices), optimal transport plans provide hard clustering, but this appears to be a sufficient rather than necessary condition.
- Why unresolved: The theoretical analysis only establishes sufficient conditions for hard clustering, and the paper doesn't explore the boundary between hard and soft clustering behavior in practice.
- What evidence would resolve it: Empirical studies varying matrix properties (PSD vs non-PSD) and analyzing the resulting transport plan sparsity patterns, or theoretical extensions characterizing necessary conditions for hard clustering.

### Open Question 2
- Question: How does the choice of embedding size (n) affect the interpretability and utility of the learned prototypes across different datasets?
- Basis in paper: The authors note that as n increases, prototype consistency increases, but don't systematically study the trade-offs between embedding size and prototype quality.
- Why unresolved: The paper provides qualitative observations about prototype quality at different n values but lacks quantitative analysis of how embedding size impacts downstream task performance or interpretability.
- What evidence would resolve it: Systematic experiments varying n across multiple datasets while measuring clustering metrics, reconstruction error, and downstream task performance (e.g., classification accuracy using prototypes as features).

### Open Question 3
- Question: What is the relationship between the hyperparameter α in Fused GW and the balance between clustering quality and embedding structure preservation?
- Basis in paper: The authors perform a grid search to find optimal α values that maximize homogeneity and silhouette scores, suggesting embedding-dependent clustering is important.
- Why unresolved: While the authors find non-zero optimal α values, they don't provide theoretical justification for why embedding-dependent clustering is beneficial or characterize the trade-offs as α varies.
- What evidence would resolve it: Theoretical analysis of how α affects the objective function landscape, or empirical studies showing performance degradation when α → 0 versus α → 1 across diverse datasets and tasks.

## Limitations
- Theorem 1 conditions (PSD/NSD matrices) may be restrictive and not satisfied by many real-world datasets
- Fused GW framework with hyperparameter α is under-explored with only limited grid search results
- Theoretical connections between GW-DR and classical DR objectives need more explicit verification

## Confidence

- **High confidence**: The BCD optimization framework and its convergence properties are well-established in the optimization literature. The connection between unconstrained embedding size and clustering performance is empirically validated.
- **Medium confidence**: The claim that GW-DR recovers classical DR objectives when n = N relies on theoretical connections that need more explicit verification in experiments. The conditions for hard clustering in Theorem 1 are mathematically sound but may be restrictive in practice.
- **Low confidence**: The practical impact of the fused GW extension with hyperparameter α is under-explored, with only limited grid search results provided without systematic sensitivity analysis.

## Next Checks

1. **Verify PSD/NSD conditions**: Systematically test different affinity matrix constructions on real datasets to determine what proportion satisfy the PSD/NSD conditions required for hard clustering, and measure the resulting clustering quality gap.

2. **Ablation study on embedding size**: Conduct a comprehensive analysis of clustering performance across all possible embedding sizes (not just n=10, 50, 100) to understand the full trade-off curve between dimensionality reduction and clustering quality.

3. **Fused GW sensitivity analysis**: Perform a systematic study of the hyperparameter α's effect on the trade-off between clustering quality and feature preservation, including visualization of prototypes at different α values to understand the structural changes.