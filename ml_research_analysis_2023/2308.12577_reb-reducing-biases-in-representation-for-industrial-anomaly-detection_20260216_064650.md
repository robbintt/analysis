---
ver: rpa2
title: 'REB: Reducing Biases in Representation for Industrial Anomaly Detection'
arxiv_id: '2308.12577'
source_url: https://arxiv.org/abs/2308.12577
tags:
- detection
- anomaly
- feature
- defectmaker
- ldknn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of industrial anomaly detection
  by proposing a method that reduces biases in feature representation. The key idea
  is to use a two-stage approach: first, a defect generation strategy (DefectMaker)
  is employed to create diverse synthetic defects and reduce domain bias through self-supervised
  learning.'
---

# REB: Reducing Biases in Representation for Industrial Anomaly Detection

## Quick Facts
- arXiv ID: 2308.12577
- Source URL: https://arxiv.org/abs/2308.12577
- Reference count: 40
- Primary result: State-of-the-art anomaly detection on MVTec AD (99.5% AUROC) and MVTec LOCO AD (88.8% AUROC) by reducing biases in feature representation.

## Executive Summary
This paper addresses industrial anomaly detection by proposing a method that reduces biases in feature representation. The key idea is to use a two-stage approach: first, a defect generation strategy (DefectMaker) is employed to create diverse synthetic defects and reduce domain bias through self-supervised learning. Second, a local density KNN (LDKNN) is proposed to reduce the local density bias in the feature space. The method achieves state-of-the-art results on two real-world industrial datasets, demonstrating the effectiveness and efficiency of reducing biases in representation for practical industrial applications.

## Method Summary
The REB method employs a two-stage approach for industrial anomaly detection. First, DefectMaker generates diverse synthetic defects using Bezier curves and saliency models, then fine-tunes a pre-trained CNN through self-supervised learning to reduce domain bias. Second, patch-level features are extracted from normal images using the fine-tuned model, and a memory bank stores these features. Anomaly detection is performed using LDKNN, which normalizes distances by local density to reduce local density bias in the feature space.

## Key Results
- Achieves 99.5% image-level AUROC on MVTec AD dataset
- Achieves 88.8% image-level AUROC on MVTec LOCO AD dataset
- Demonstrates state-of-the-art performance compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DefectMaker reduces domain bias by generating synthetic defects that better mimic natural defects, improving feature representation for industrial images.
- Mechanism: The method defines defects as a fusion of shape and fill, using Bezier curves for diverse defect shapes and saliency models to guide realistic defect placement. This synthetic data fine-tunes a pre-trained CNN to better adapt to industrial domains.
- Core assumption: Synthetic defects generated by DefectMaker are sufficiently realistic and diverse to reduce the domain gap between natural-scene and industrial images.
- Evidence anchors:
  - [abstract]: "DefectMaker ensures a strong diversity in the synthetic defects" and "improves the accuracy of unsupervised anomaly detection in industrial images."
  - [section III-A]: "DefectMaker is designed to generate various types of defects, including but not limited to noise, mask, distortion, and defects...to imitate natural defects with more varieties."
  - [corpus]: Weak. No direct citations found, but FMR scores suggest moderate relevance among related papers.
- Break condition: If synthetic defects fail to capture the complexity and variability of real defects, the domain bias reduction will be insufficient.

### Mechanism 2
- Claim: LDKNN reduces local density bias in the feature space, improving anomaly detection performance.
- Mechanism: LDKNN calculates local density for each feature in the memory bank by averaging distances to K nearest neighbors. It normalizes anomaly scores by subtracting a scaled local density value, accounting for variations in local feature density.
- Core assumption: The local density of features in the memory bank varies significantly due to complex intra-image distributions, and normalizing by local density improves detection accuracy.
- Evidence anchors:
  - [abstract]: "propose a local-density KNN (LDKNN) to reduce the local density bias in the feature space and obtain effective anomaly detection."
  - [section III-B-2]: "LDKNN considers the density bias in features with a local density model that accounts for variations in the local feature density."
  - [section IV-F]: "LDKNN improves anomaly detection and localization performance as the K increases, while KNN does not hold such property."
- Break condition: If the assumption of significant local density bias is incorrect, or if the normalization by local density introduces noise, LDKNN may not improve or could degrade performance.

### Mechanism 3
- Claim: Using multi-hierarchy features (φ2 and φ3) from ResNet instead of only the last layer (φ4) reduces domain bias and improves anomaly detection.
- Mechanism: Deeper features (φ4) are biased towards proxy tasks and lose spatial information about small, irregular anomalies. Shallow layers (φ2, φ3) retain more spatial details and are less biased, so aggregating them provides better feature representation.
- Core assumption: Deeper features are more biased towards the pre-training task (ImageNet) and less suitable for industrial anomaly detection, while shallower features retain more relevant spatial information.
- Evidence anchors:
  - [section III-B-1]: "However, this strategy may not be suitable for industrial anomaly detection because the deeper features at the last hierarchy are biased toward the proxy task. Anomalies may have the characteristics of small spatial size and irregularity, which are lost gradually as the network becomes deeper."
  - [corpus]: Weak. No direct citations found, but the reasoning aligns with general understanding of feature hierarchy in CNNs.
- Break condition: If the bias in deeper features is not significant for the specific industrial dataset, or if shallow features lack discriminative power, using multi-hierarchy features may not provide benefits.

## Foundational Learning

- Concept: Domain bias in pre-trained models
  - Why needed here: Pre-trained models on natural images may not generalize well to industrial images due to differences in image characteristics and anomaly types.
  - Quick check question: Why might a model trained on ImageNet struggle with industrial anomaly detection?

- Concept: Local density in feature space
  - Why needed here: Variations in local feature density due to complex intra-image distributions can affect anomaly detection performance if not accounted for.
  - Quick check question: How does local density bias in feature space impact the performance of KNN-based anomaly detection?

- Concept: Synthetic data generation for domain adaptation
  - Why needed here: Real anomaly data is scarce and expensive to obtain, so synthetic defects are used to fine-tune models and reduce domain bias.
  - Quick check question: What are the challenges in generating synthetic defects that accurately represent real anomalies?

## Architecture Onboarding

- Component map: DefectMaker -> Self-supervised learning -> Feature extractor -> Memory bank -> LDKNN

- Critical path:
  1. Generate synthetic defects with DefectMaker.
  2. Fine-tune pre-trained CNN using self-supervised learning on synthetic data.
  3. Extract patch-level features from normal images using fine-tuned CNN.
  4. Build memory bank with normal features.
  5. Perform anomaly detection using LDKNN with local density normalization.

- Design tradeoffs:
  - Using smaller backbone networks (Vgg11, Resnet18) reduces computation but may limit feature representation capacity.
  - Generating diverse synthetic defects increases domain adaptation but adds complexity to DefectMaker.
  - LDKNN normalization improves performance but adds computational overhead compared to vanilla KNN.

- Failure signatures:
  - Poor anomaly detection performance: Could indicate insufficient domain adaptation by DefectMaker or incorrect local density normalization in LDKNN.
  - High false positive rate: May suggest over-normalization in LDKNN or inadequate feature representation from the backbone.
  - Slow inference: Could be due to large memory bank size or inefficient LDKNN implementation.

- First 3 experiments:
  1. Evaluate anomaly detection performance with and without DefectMaker fine-tuning to measure domain bias reduction.
  2. Compare LDKNN with vanilla KNN on a small dataset to validate local density normalization benefits.
  3. Test different K values in LDKNN to find the optimal balance between performance and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REB change when using different self-supervised learning tasks beyond DefectMaker for domain adaptation?
- Basis in paper: [explicit] The paper mentions that DefectMaker is used for domain adaptation through a self-supervised learning task, but does not explore alternative tasks.
- Why unresolved: The paper focuses on DefectMaker but does not compare its effectiveness against other self-supervised learning approaches.
- What evidence would resolve it: Comparative experiments evaluating REB with various self-supervised learning tasks for domain adaptation.

### Open Question 2
- Question: What is the impact of varying the local density coefficient (α) on the performance of LDKNN across different types of industrial anomalies?
- Basis in paper: [explicit] The paper discusses the local density coefficient α in LDKNN but does not provide a detailed analysis of its impact across different anomaly types.
- Why unresolved: The paper sets α to 1 by default but does not explore its optimization for different anomaly categories.
- What evidence would resolve it: Systematic experiments varying α for different anomaly types and analyzing the performance changes.

### Open Question 3
- Question: How does the proposed LDKNN method compare to other density-based anomaly detection techniques in terms of computational efficiency and accuracy?
- Basis in paper: [inferred] The paper introduces LDKNN as a density-based method but does not provide a comprehensive comparison with other density-based techniques.
- Why unresolved: The paper focuses on the performance of LDKNN without benchmarking it against other established density-based methods.
- What evidence would resolve it: Comparative studies evaluating LDKNN against other density-based methods like LOF, LDOF, and Kth-NN in terms of accuracy and computational cost.

## Limitations

- The effectiveness of DefectMaker depends heavily on the realism and diversity of synthetic defects, which is not directly validated in the paper.
- The assumption of significant local density bias in feature space may not hold for all industrial datasets, limiting LDKNN's generalizability.
- Multi-hierarchy feature extraction reduces computational efficiency, and the paper does not thoroughly analyze the tradeoff between performance gain and increased computation.

## Confidence

- **High Confidence**: The two-stage approach (DefectMaker + LDKNN) is well-motivated and the ablation studies support the individual contributions of each component.
- **Medium Confidence**: The reported state-of-the-art results on MVTec AD and MVTec LOCO AD are promising, but the lack of direct comparisons with other synthetic data generation methods makes it difficult to isolate DefectMaker's contribution.
- **Low Confidence**: The paper does not provide sufficient evidence that the synthetic defects generated by DefectMaker accurately represent real-world anomalies, which is crucial for the method's success.

## Next Checks

1. Conduct a controlled experiment comparing DefectMaker with other synthetic data generation methods (e.g., CutPaste, grid mask) to quantify its contribution to domain bias reduction.
2. Perform a thorough analysis of local density distributions in the feature space for both normal and anomalous images to validate the assumption of significant local density bias.
3. Investigate the impact of using different backbone networks (e.g., Resnet50, WideResNet50) on the overall performance and computational efficiency of the REB method.