---
ver: rpa2
title: 'Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models
  with RL Finetuning'
arxiv_id: '2312.13980'
source_url: https://arxiv.org/abs/2312.13980
tags:
- multi-view
- rlft
- diffusion
- nerf
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Carve3D improves multi-view consistency in diffusion models via
  RL finetuning. It introduces a novel Multi-view Reconstruction Consistency (MRC)
  metric that measures consistency by comparing generated views with NeRF renderings
  from the same viewpoints.
---

# Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning

## Quick Facts
- arXiv ID: 2312.13980
- Source URL: https://arxiv.org/abs/2312.13980
- Authors: 
- Reference count: 40
- Key outcome: Carve3D improves multi-view consistency in diffusion models via RL finetuning with Multi-view Reconstruction Consistency (MRC) metric.

## Executive Summary
Carve3D addresses the challenge of multi-view consistency in 3D generation by introducing a novel RL finetuning approach. The method employs a Multi-view Reconstruction Consistency (MRC) metric that compares generated views with NeRF renderings from the same viewpoints. Using this metric as a reward signal, Carve3D implements pure on-policy RL finetuning with KL divergence regularization and scaled compute, achieving significant improvements in multi-view consistency and NeRF quality over supervised finetuning baselines while preserving prompt alignment and texture details.

## Method Summary
Carve3D introduces RL finetuning for multi-view diffusion models using a novel Multi-view Reconstruction Consistency (MRC) metric as the reward function. The method generates multi-view images, reconstructs a NeRF using a Sparse-view Large Reconstruction Model (LRM), renders the NeRF at the same camera viewpoints, and computes LPIPS distance between generated and rendered images as the MRC reward. The RLFT algorithm employs pure on-policy DDPO-SF policy gradient with KL divergence regularization to prevent distribution shift. Training uses a curated set of 100 low-reward prompts for 55 epochs or until KL divergence threshold (3.2e-4) is reached, with scaled batch size of 768.

## Key Results
- Carve3D significantly improves multi-view consistency and NeRF quality over supervised finetuning baselines
- Achieves lower MRC scores than longer supervised finetuning, demonstrating RLFT's superior effectiveness for consistency improvement
- Preserves prompt alignment and texture details through KL divergence regularization during RLFT

## Why This Works (Mechanism)

### Mechanism 1
RLFT improves multi-view consistency beyond SFT limits by learning from self-generated outputs. The reward derived from comparing generated multi-view images to NeRF renderings allows optimization beyond SFT training data distribution. Core assumption: self-generated multi-view images contain sufficient information for meaningful consistency rewards. Break condition: poor NeRF reconstruction quality providing unreliable consistency signals.

### Mechanism 2
KL divergence regularization prevents distribution shift and maintains prompt alignment during RLFT. Penalizing KL divergence between finetuned and base models keeps the finetuned model close to the base model's distribution, preserving prompt alignment and texture details. Core assumption: base model has good prompt alignment and texture details. Break condition: excessive KL regularization preventing sufficient consistency improvement.

### Mechanism 3
Pure on-policy training (REINFORCE) is more stable than partially on-policy training (PPO) for RLFT of diffusion models. Pure on-policy training avoids instability from using older policy data to update newer policy, leading to more stable reward curves and better convergence. Core assumption: task reward function (MRC) is challenging enough that stability benefits outweigh sample efficiency benefits. Break condition: simpler reward functions or smaller models where PPO's sample efficiency becomes advantageous.

## Foundational Learning

- **Multi-view consistency in 3D generation**: Why needed: Crucial for high-quality NeRF reconstruction. Quick check: What artifacts appear in NeRF reconstruction when input multi-view images are inconsistent?
- **Reinforcement Learning Finetuning (RLFT)**: Why needed: Allows learning from self-generated outputs to improve beyond SFT limitations. Quick check: How does RLFT differ from SFT in terms of data usage for finetuning?
- **Markov Decision Process (MDP) formulation for denoising diffusion models**: Why needed: Enables using RL algorithms like REINFORCE for finetuning the denoising process. Quick check: What are the state, action, and reward in the MDP formulation of denoising?

## Architecture Onboarding

- **Component map**: Instant3D-10K diffusion model -> LRM NeRF reconstruction -> MRC metric computation -> RLFT with DDPO-SF
- **Critical path**: 1) Generate multi-view images from base model using prompt. 2) Reconstruct NeRF from generated images using LRM. 3) Render NeRF at same camera viewpoints. 4) Compute MRC reward via LPIPS comparison. 5) Update model parameters via RLFT.
- **Design tradeoffs**: Pure on-policy vs. PPO (stability vs. sample efficiency), KL regularization coefficient (alignment preservation vs. consistency improvement limits), training data size (generalization vs. computation requirements).
- **Failure signatures**: Training instability (high reward variance, failure to improve), distribution shift (loss of prompt alignment/texture details), poor generalization (no testing set improvement despite training set gains).
- **First 3 experiments**: 1) Validate MRC metric by introducing controlled inconsistencies and checking correlation with MRC scores. 2) Compare pure on-policy training stability versus PPO for RLFT. 3) Evaluate KL regularization impact on prompt alignment and distribution shift prevention.

## Open Questions the Paper Calls Out

### Open Question 1
How does Carve3D perform when applied to text-to-multi-view diffusion models with architectures different from Instant3D? The paper mentions direct adaptability to other models but only demonstrates results on Instant3D. What evidence would resolve it: Experiments applying Carve3D to multiple different text-to-multi-view diffusion model architectures with comparative results.

### Open Question 2
What is the impact of using different reward functions instead of MRC for RL finetuning of multi-view diffusion models? The paper focuses solely on MRC without exploring alternative reward functions. What evidence would resolve it: Comparative experiments using different reward functions (e.g., direct image similarity metrics, NeRF reconstruction quality metrics).

### Open Question 3
How does Carve3D scale with different batch sizes and data sizes beyond what was tested? The paper identifies scaling relationships but doesn't explore the full parameter space. What evidence would resolve it: Systematic experiments testing various batch sizes and data sizes across different computational budgets to identify optimal scaling strategies.

## Limitations
- Weak empirical validation of core mechanisms with limited ablation studies isolating individual contributions
- Evaluation methodology concerns regarding MRC metric's perceptual relevance without human validation studies
- Heavy implementation dependencies on Sparse-view Large Reconstruction Model quality

## Confidence
- **High confidence**: RLFT with MRC reward improves multi-view consistency metrics compared to SFT baselines (supported by quantitative results)
- **Medium confidence**: Pure on-policy training stability advantage over PPO (based on observations but lacks rigorous comparison)
- **Low confidence**: RLFT improving beyond SFT dataset limitations (relies on assumption about self-generated data diversity without empirical validation)

## Next Checks
1. **Ablation study on RL mechanisms**: Perform controlled experiments isolating contributions of pure on-policy training, KL regularization, and scaled compute by training variants with each component removed. Quantify marginal improvement on MRC scores and NeRF quality.
2. **Human perceptual validation**: Conduct human studies comparing multi-view consistency of RLFT outputs versus SFT baselines using side-by-side comparisons and preference ratings. Correlate human judgments with MRC scores.
3. **Robustness to reconstruction quality**: Test method with different NeRF reconstruction models (varying quality levels) to determine minimum reconstruction quality required for reliable MRC rewards. Analyze how LRM errors propagate to finetuned model performance.