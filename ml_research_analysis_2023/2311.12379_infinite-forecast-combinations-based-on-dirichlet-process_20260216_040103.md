---
ver: rpa2
title: Infinite forecast combinations based on Dirichlet process
arxiv_id: '2311.12379'
source_url: https://arxiv.org/abs/2311.12379
tags:
- ensemble
- learning
- base
- prediction
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an ensemble forecasting model based on the
  Dirichlet process to address the instability of single neural network predictions.
  The approach samples learning rates and combination weights from a Dirichlet process
  to generate diverse base learners during training.
---

# Infinite forecast combinations based on Dirichlet process

## Quick Facts
- arXiv ID: 2311.12379
- Source URL: https://arxiv.org/abs/2311.12379
- Reference count: 25
- Primary result: Ensemble forecasting model based on Dirichlet process sampling achieves 50% reduction in MAE and RMSE compared to single model

## Executive Summary
This study introduces an ensemble forecasting model that leverages the Dirichlet process to generate diverse base learners from a single neural network architecture. By sampling learning rates and combination weights from a Dirichlet process, the approach creates an ensemble of LSTM models that significantly outperform individual predictions. The method addresses the instability inherent in single neural network forecasts by introducing structured diversity through probabilistic sampling.

The empirical analysis demonstrates substantial improvements in forecast accuracy, with MAE and RMSE reduced by approximately 50% when using 20 or more base models. The approach also exhibits superior stability compared to single model predictions, with weighted averaging proving more efficient than simple averaging. A mixed strategy combining multiple base distributions further enhances performance.

## Method Summary
The method employs Dirichlet process sampling to generate diverse learning rates and combination weights for an ensemble of LSTM models. Weekly M4 competition data is preprocessed through normalization and lag transformation. Base models are trained with sampled learning rates, and checkpoints are saved during training. The ensemble prediction is formed through weighted averaging of these checkpoints, where weights are also derived from Dirichlet process sampling. The approach converts the infinite mixture potential of the Dirichlet process into a finite set of base learners by fixing the number of models to combine.

## Key Results
- Ensemble model reduces MAE and RMSE by approximately 50% compared to single model baseline when using 20+ base models
- Weighted averaging outperforms simple averaging in both accuracy and computational efficiency
- Mixed strategy combining multiple base distributions provides additional performance improvements
- Forecast stability significantly improves compared to single model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dirichlet process sampling transforms infinite model diversity into finite, tunable base learners
- Mechanism: The Dirichlet process (DP) is parameterized by a concentration parameter α and a base distribution H. Sampling α times from H yields an infinite mixture of possible learning rates and weights. By fixing the number of base models p, the infinite mixture is truncated into a finite set of base learners with sampled hyperparameters.
- Core assumption: The base distribution H captures the true variability of optimal learning rates; truncation at p preserves sufficient diversity
- Evidence anchors:
  - [abstract] "Initially, the learning rate is sampled with three basis distributions as hyperparameters to convert the infinite mixture into a finite one."
  - [section II.A.2] "The DP allows us to generate probability distributions with infinite dimensions. ... Thus, each sampling of its samples is a distribution..."
  - [corpus] Weak/no direct evidence; corpus neighbors focus on ensemble selection rather than DP-based sampling

### Mechanism 2
- Claim: Ensemble averaging of base learners reduces variance without increasing bias, improving forecast accuracy
- Mechanism: Individual neural network predictions suffer from high variance due to stochastic gradient descent and hyperparameter choice. By averaging predictions of p base learners (each trained with a different sampled learning rate), the ensemble reduces overall variance. The paper shows MAE and RMSE drop by ~50% when p ≥ 20.
- Core assumption: Base learners are sufficiently diverse; averaging does not introduce correlated errors that negate variance reduction
- Evidence anchors:
  - [abstract] "Empirical analysis ... shows the ensemble model significantly improves forecast accuracy over a single benchmark model, reducing MAE by 50% and RMSE by 50% when using 20+ base models."
  - [section IV.A] "The most substantial reduction in MAE and RMSE occurs in the range from 20 to 50 of p, resulting in a nearly 50% decrease in errors compared to the single model S."
  - [corpus] No direct evidence; corpus focuses on ensemble cost/benefit trade-offs rather than variance reduction mechanisms

### Mechanism 3
- Claim: Weighted averaging outperforms simple averaging by allocating higher weight to more accurate base learners
- Mechanism: Weights w_i are derived from DP sampling and normalized (w'_i = w_i / Σ w_i). These weights prioritize better-performing learners in the final prediction, improving accuracy and efficiency compared to uniform averaging.
- Core assumption: The DP sampling process yields weights that correlate with base learner quality
- Evidence anchors:
  - [abstract] "weighted averaging is more efficient than simple averaging"
  - [section IV.C] "the inclusion of weighted averaging in the ensemble method not only enhances forecast accuracy but also boosts the efficiency of ensemble learning compared to a single model."
  - [corpus] No direct evidence; corpus does not discuss weighted vs. simple averaging

## Foundational Learning

- Concept: Dirichlet Process and Stick-Breaking Construction
  - Why needed here: DP provides a principled way to generate an infinite mixture of learning rates and combination weights, enabling diverse base learners from a single training run
  - Quick check question: What are the two parameters of a Dirichlet process and what role does each play?

- Concept: Bias-Variance Decomposition in Ensemble Learning
  - Why needed here: Understanding that ensembles reduce variance without increasing bias explains why averaging multiple diverse models improves forecast accuracy
  - Quick check question: In the bias-variance decomposition, which component is reduced by averaging multiple diverse models?

- Concept: Forecast Combination Theory (e.g., Bates-Granger)
  - Why needed here: Forecast combination leverages multiple models to outperform any single model, especially under high parameter uncertainty
  - Quick check question: Why does combining forecasts from diverse models often outperform selecting the single best model?

## Architecture Onboarding

- Component map:
  Data preprocessing -> lag formatting -> normalization -> model pool training -> DP sampling -> ensemble aggregation -> evaluation

- Critical path:
  1. Preprocess weekly series (merge, normalize, lag transform)
  2. Sample p learning rates and weights from DP
  3. Train p base LSTM models with sampled rates (checkpoints saved)
  4. Load checkpoints, predict on test set
  5. Compute weighted ensemble prediction
  6. Evaluate accuracy

- Design tradeoffs:
  - More base models → higher diversity & accuracy but higher computation & storage
  - Simple averaging → faster, but may be less accurate than weighted averaging
  - Choice of base distribution H → affects quality of sampled hyperparameters

- Failure signatures:
  - MAE/RMSE plateaus or increases after adding more models (diminishing returns)
  - High correlation among base model predictions (low diversity)
  - Weighted averaging underperforms simple averaging (poor weight calibration)

- First 3 experiments:
  1. Vary p (10, 20, 30, 40, 50) with fixed base distribution; measure MAE/RMSE
  2. Compare simple vs. weighted averaging on same ensemble
  3. Test three different base distributions (Exponential, Gaussian, Beta) for DP sampling; evaluate impact on accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of base models to include in the ensemble for achieving the best trade-off between prediction accuracy and computational efficiency?
- Basis in paper: [explicit] The paper explores the sensitivity to the number of models to be combined and finds that forecast errors significantly decrease as the number of models increases, but the rate of improvement tends to slow down after reaching 60 models.
- Why unresolved: The paper identifies that having more models does not necessarily lead to a proportionate increase in forecast accuracy, and there is a need to balance between precision and diversity. However, it does not provide a definitive answer on the optimal number of models.
- What evidence would resolve it: Conducting further experiments with a wider range of model numbers and analyzing the point at which additional models no longer provide significant improvements in accuracy would help determine the optimal number.

### Open Question 2
- Question: How does the diversity of base models affect the overall prediction accuracy, and what is the best strategy to ensure sufficient diversity?
- Basis in paper: [explicit] The paper discusses the impact of model diversity on prediction accuracy and introduces a mixed strategy to enhance diversity by combining base models from different distributions.
- Why unresolved: While the paper shows that diversity positively impacts prediction accuracy, it does not fully explore the mechanisms behind this relationship or identify the best strategies to ensure diversity.
- What evidence would resolve it: Further research could involve experimenting with different diversity metrics and strategies to understand their effects on prediction accuracy and identifying the most effective approaches to maintain diversity.

### Open Question 3
- Question: How does the weighted averaging method compare to simple averaging in terms of computational efficiency and forecast accuracy?
- Basis in paper: [explicit] The paper compares the performance of weighted averaging and simple averaging methods, showing that weighted averaging enhances forecast accuracy and efficiency.
- Why unresolved: The paper demonstrates the benefits of weighted averaging but does not provide a detailed comparison of computational efficiency between the two methods.
- What evidence would resolve it: Conducting a detailed analysis of the computational costs and time requirements for both methods, alongside their impact on forecast accuracy, would provide a clearer understanding of their relative efficiency.

## Limitations

- The optimal number of base models remains uncertain, with diminishing returns observed beyond 60 models
- The computational cost-benefit tradeoff at scale is not fully quantified
- Limited comparison of different base distributions for Dirichlet process sampling
- Results may not generalize beyond the M4 weekly dataset

## Confidence

**High confidence**: The ensemble model significantly outperforms single models in accuracy and stability; weighted averaging is more efficient than simple averaging; 20+ base models yield optimal improvements.

**Medium confidence**: The Dirichlet process sampling mechanism is essential for generating diversity; the mixed strategy combining multiple base distributions provides additional benefits.

**Low confidence**: The specific hyperparameter choices (learning rate ranges, weight distributions, checkpoint frequency) are optimal; the results generalize beyond the M4 weekly dataset.

## Next Validation Checks

1. **Diversity Analysis**: Measure pairwise correlation among base model predictions to confirm that DP sampling generates sufficiently diverse base learners. Compare diversity metrics against ensembles with random learning rate assignment.

2. **Computational Scaling**: Systematically measure training time, memory usage, and storage requirements as p increases from 10 to 100 base models to quantify the cost-benefit tradeoff.

3. **Base Distribution Comparison**: Implement and compare all three suggested base distributions (Exponential, Gaussian, Beta) for DP sampling in a controlled experiment to determine which yields the best accuracy-stability tradeoff.