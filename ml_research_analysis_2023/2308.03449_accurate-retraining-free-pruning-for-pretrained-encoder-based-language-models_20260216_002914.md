---
ver: rpa2
title: Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models
arxiv_id: '2308.03449'
source_url: https://arxiv.org/abs/2308.03449
tags:
- pruning
- knowledge
- k-pruning
- accuracy
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: K-pruning is an accurate retraining-free structured pruning algorithm
  for pre-trained encoder-based language models. It focuses on preserving useful knowledge
  by carefully selecting pruning targets and iteratively reconstructing knowledge
  after each pruning step.
---

# Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models

## Quick Facts
- arXiv ID: 2308.03449
- Source URL: https://arxiv.org/abs/2308.03449
- Authors: 
- Reference count: 40
- Key outcome: K-pruning achieves up to 58.02% higher F1 score than retraining-free algorithms at 80% compression rate on GLUE and SQuAD benchmarks

## Executive Summary
K-pruning is an accurate retraining-free structured pruning algorithm designed for pre-trained encoder-based language models. The method focuses on preserving useful knowledge by carefully selecting pruning targets and iteratively reconstructing knowledge after each pruning step. K-pruning introduces three novel components: knowledge measurement to evaluate pruning impact, knowledge-preserving mask search to find optimal pruning targets under FLOPs constraints, and knowledge-preserving pruning to minimize distortion through iterative sub-layerwise pruning with lightweight weight-tuning. Experiments demonstrate that K-pruning provides the best accuracy-cost trade-off among both retraining-free and retraining-based methods, with up to 422× lower pruning cost than retraining-based approaches.

## Method Summary
K-pruning is a structured pruning algorithm that removes entire attention heads and neurons from pre-trained BERT/DistilBERT models while preserving as much useful knowledge as possible. The method works in three stages: first, it measures the predictive and representational knowledge of each component using Taylor expansion and Fisher Information; second, it selects pruning targets based on importance scores that balance knowledge preservation with FLOPs constraints; third, it performs iterative sub-layerwise pruning with lightweight weight-tuning to reconstruct knowledge after each pruning step. The algorithm uses a small sample dataset (100K tokens) to measure knowledge and achieve the desired compression rate while maximizing accuracy on downstream tasks.

## Key Results
- Achieves up to 58.02% higher F1 score compared to existing retraining-free algorithms at 80% compression rate
- Provides the best accuracy-cost trade-off among both retraining-free and retraining-based methods
- Demonstrates up to 422× lower pruning cost than retraining-based approaches
- Outperforms baseline methods (Kwon et al., KCM, DynaBERT, EBERT) on GLUE and SQuAD benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: K-pruning preserves useful knowledge during pruning by measuring predictive and representational knowledge for each attention head and neuron.
- **Mechanism**: The algorithm estimates the loss of knowledge incurred by pruning each component using Taylor expansion and Fisher Information for predictive knowledge, and MSE loss between pre-trained and compressed model outputs for representational knowledge. Components with lower knowledge scores are pruned first.
- **Core assumption**: The amount of knowledge in a component can be accurately estimated by measuring the loss when that component is pruned, without actually performing the prune.
- **Evidence anchors**:
  - [abstract]: "K-pruning identifies and prunes attention heads and neurons deemed to be superfluous, based on the amount of their inherent knowledge."
  - [section 4.2]: "We define the amount of knowledge contained within each attention head and neuron by evaluating the loss of knowledge incurred by pruning them."
- **Break condition**: If the knowledge measurement is inaccurate (e.g., the Taylor expansion approximation is poor), the algorithm may prune important components, leading to accuracy degradation.

### Mechanism 2
- **Claim**: K-pruning minimizes pruning errors through iterative sub-layerwise pruning with lightweight weight-tuning.
- **Mechanism**: After pruning the least important components in a sub-layer, the algorithm performs lightweight tuning to reconstruct the pre-trained model's knowledge by minimizing the representational knowledge loss. This process is repeated for each sub-layer from bottom to top.
- **Core assumption**: Pruning followed by knowledge reconstruction for each sub-layer can effectively minimize the distortion of sub-layer outputs caused by pruning.
- **Evidence anchors**:
  - [abstract]: "K-pruning applies an iterative process of pruning followed by knowledge reconstruction for each sub-layer to preserve the knowledge of the pre-trained models."
  - [section 4.4]: "We minimize the distortion of sub-layer outputs by iteratively performing pruning followed by lightweight tuning to mimic PLM's output for each sub-layer from the bottom."
- **Break condition**: If the lightweight tuning is insufficient to reconstruct the knowledge (e.g., the linear solver is not accurate enough), the accuracy of the compressed model may degrade.

### Mechanism 3
- **Claim**: K-pruning selects optimal pruning targets by considering both predictive and representational knowledge while satisfying FLOPs constraints.
- **Mechanism**: The algorithm computes importance scores for each component based on the weighted sum of predictive and representational knowledge, normalized by the FLOPs required for each component. The components with the lowest scores are pruned until the FLOPs constraint is met.
- **Core assumption**: Balancing predictive and representational knowledge in the importance score leads to better accuracy than using either one alone.
- **Evidence anchors**:
  - [abstract]: "K-pruning focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning."
  - [section 4.3]: "We compute the importance scores for each mask, considering both predictive and representational knowledge. Then, we select the masks with the lowest importance scores for pruning to satisfy the FLOPs constraint."
- **Break condition**: If the balance coefficients for predictive and representational knowledge are not optimal, the algorithm may prune important components or fail to meet the FLOPs constraint.

## Foundational Learning

- **Concept**: Knowledge distillation in model compression
  - Why needed here: K-pruning relies on the concept of knowledge distillation, where the knowledge of a pre-trained model is transferred to a compressed model. Understanding how knowledge distillation works is crucial for understanding how K-pruning preserves the knowledge of the pre-trained model.
  - Quick check question: How does knowledge distillation differ from traditional supervised learning?

- **Concept**: Structured pruning in neural networks
  - Why needed here: K-pruning is a structured pruning algorithm that removes entire attention heads and neurons rather than individual weights. Understanding the basics of structured pruning, such as the difference between structured and unstructured pruning, is essential for understanding the pruning process in K-pruning.
  - Quick check question: What are the advantages of structured pruning over unstructured pruning in terms of hardware efficiency?

- **Concept**: Transformer architecture and self-attention mechanism
  - Why needed here: K-pruning is designed for pre-trained encoder-based language models, which are typically based on the Transformer architecture. Understanding the components of the Transformer, such as multi-head attention and feed-forward networks, is necessary for understanding how K-pruning prunes the model.
  - Quick check question: How does the self-attention mechanism in the Transformer work, and what is the role of attention heads?

## Architecture Onboarding

- **Component map**: Knowledge measurement -> Knowledge-preserving mask search -> Knowledge-preserving pruning
- **Critical path**: The critical path in K-pruning is the knowledge-preserving pruning module, which involves the iterative process of pruning followed by weight-tuning for each sub-layer.
- **Design tradeoffs**:
  - Accuracy vs. pruning cost: K-pruning achieves higher accuracy than existing retraining-free algorithms but may have a higher pruning cost due to the iterative process.
  - Knowledge preservation vs. compression rate: Preserving more knowledge may lead to a lower compression rate, as more components are kept in the model.
- **Failure signatures**:
  - Accuracy degradation: If the knowledge measurement or weight-tuning is not accurate, the compressed model may have lower accuracy than expected.
  - FLOPs constraint violation: If the pruning targets are not selected properly, the compressed model may exceed the FLOPs constraint.
- **First 3 experiments**:
  1. Verify the knowledge measurement module by comparing the estimated knowledge loss with the actual loss when pruning a component.
  2. Test the knowledge-preserving pruning module on a small model to ensure that the iterative process effectively preserves knowledge.
  3. Evaluate the overall performance of K-pruning on a benchmark task and compare it with existing retraining-free pruning algorithms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of K-pruning be further improved for very high compression rates (e.g., 75% or higher)?
- Basis in paper: [explicit] The paper mentions this as a limitation, noting that K-pruning, like retraining-based algorithms, experiences accuracy drops at high compression rates.
- Why unresolved: The paper suggests this as a future work direction but does not provide specific solutions or methods to address this limitation.
- What evidence would resolve it: Demonstrating improved accuracy on high compression rates by integrating K-pruning with other compression techniques, such as quantization or low-rank approximation, as suggested in the paper.

### Open Question 2
- Question: How does the choice of hyperparameters (γ, λ, µ) affect the performance of K-pruning, and what are the optimal settings for different tasks and model architectures?
- Basis in paper: [explicit] The paper performs sensitivity analysis on these hyperparameters but only uses two combinations for comparison with existing works.
- Why unresolved: While the paper shows that the performance is relatively stable across a range of hyperparameters, it does not provide a systematic method for selecting optimal hyperparameters for different scenarios.
- What evidence would resolve it: A comprehensive study on the impact of different hyperparameter settings on various tasks and model architectures, potentially leading to guidelines for hyperparameter selection.

### Open Question 3
- Question: Can the concepts of K-pruning be extended to other types of neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
- Basis in paper: [inferred] The paper discusses the potential for extending K-pruning by incorporating other compression techniques, suggesting that the underlying principles could be applicable to other architectures.
- Why unresolved: The paper focuses specifically on transformer-based language models and does not explore the applicability of K-pruning to other types of neural networks.
- What evidence would resolve it: Successful application and evaluation of K-pruning or its core concepts on other types of neural networks, demonstrating its generalizability and effectiveness.

## Limitations
- The paper claims K-pruning achieves 58.02% higher F1 score compared to existing retraining-free algorithms at 80% compression rate, but the specific experimental conditions (dataset size, training epochs, exact baselines) are not fully specified, limiting direct reproducibility.
- The knowledge measurement component relies on Taylor expansion approximations for predictive knowledge, which may not accurately capture the true knowledge loss in all scenarios, particularly for complex language understanding tasks.
- The iterative knowledge-preserving pruning process requires multiple passes through the model, which could lead to error accumulation over time, though the paper doesn't provide extensive ablation studies on the number of iterations needed for optimal results.

## Confidence
- **High confidence**: The core claim that K-pruning achieves better accuracy-cost trade-off than existing retraining-free methods is supported by experimental results on GLUE and SQuAD benchmarks.
- **Medium confidence**: The claim about 422× lower pruning cost compared to retraining-based approaches is based on relative comparisons but lacks absolute timing measurements or detailed cost breakdown.
- **Medium confidence**: The effectiveness of the three-component design (knowledge measurement, mask search, and weight-tuning) is theoretically sound, but the paper doesn't provide sufficient ablation studies to isolate the contribution of each component.

## Next Checks
1. **Ablation study on knowledge measurement**: Remove either predictive or representational knowledge measurement and compare the accuracy degradation to validate the necessity of both components.

2. **Iteration count sensitivity**: Test the performance of K-pruning with different numbers of iterative pruning passes (1, 2, 3, 4) to determine the optimal iteration count and verify that the method doesn't over-prune with excessive iterations.

3. **Cross-architecture validation**: Apply K-pruning to different PLM architectures beyond BERT (e.g., RoBERTa, ALBERT) and different task types (generation vs. classification) to test the generalizability of the approach beyond the reported GLUE and SQuAD benchmarks.