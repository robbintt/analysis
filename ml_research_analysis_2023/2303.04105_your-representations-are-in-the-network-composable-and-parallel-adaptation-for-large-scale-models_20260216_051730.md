---
ver: rpa2
title: 'Your representations are in the network: composable and parallel adaptation
  for large scale models'
arxiv_id: '2303.04105'
source_url: https://arxiv.org/abs/2303.04105
tags:
- inca
- learning
- training
- layer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InCA addresses efficient transfer learning for large-scale models
  by introducing a lightweight adapter architecture that cross-attends to intermediate
  activation layers of pre-trained models. Unlike conventional fine-tuning or adapter
  methods that modify the pre-trained model's parameters, InCA extracts frozen activations
  and processes them with cross-attention modules trained independently in parallel
  ("One-to-Many" training).
---

# Your representations are in the network: composable and parallel adaptation for large scale models

## Quick Facts
- arXiv ID: 2303.04105
- Source URL: https://arxiv.org/abs/2303.04105
- Reference count: 40
- Key outcome: InCA achieves performance comparable to full fine-tuning at cost comparable to fine-tuning just the last layer

## Executive Summary
InCA introduces a lightweight adapter architecture that enables efficient transfer learning for large-scale vision models by cross-attending to intermediate activation layers. Unlike conventional fine-tuning or adapter methods that modify pre-trained model parameters, InCA extracts frozen activations and processes them with cross-attention modules trained independently in parallel. This "One-to-Many" training approach enables simultaneous training of multiple adapters across different layers while maintaining extreme parameter efficiency (1.3% of parameters for ViT-L/16) and achieving performance within 0.2% of full fine-tuning across 11 fine-grained classification tasks.

## Method Summary
InCA uses cross-attention adapters that receive frozen activation tokens from intermediate layers of pre-trained models and use learned query tokens to perform cross-attention, selectively aggregating task-relevant information from activation maps. The "One-to-Many" training framework enables efficient parallel training of multiple adapters across different layers by caching intermediate activations during a single forward pass and updating all adapter parameters with a single backward pass. This approach achieves strong performance while being computationally efficient, enabling adaptation of very large models on a single GPU and demonstrating state-of-the-art performance in the ImageNet-to-Sketch multi-task benchmark.

## Key Results
- Achieves performance within 0.2% of full fine-tuning across 11 fine-grained classification tasks
- Uses only 1.3% of parameters compared to base model (ViT-L/16)
- Enables training of up to 40 adapters in parallel with ViT-H/14 on a single GPU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention adapters can extract task-relevant information from intermediate layers without backpropagating through the backbone
- Mechanism: InCA adapter receives frozen activation tokens from a pre-trained model's intermediate layer and uses learnable query tokens to perform cross-attention, selectively aggregating information from the activation map to create a fixed-size representation for classification
- Core assumption: Task-relevant information exists in intermediate layers and can be extracted through learned cross-attention rather than requiring full fine-tuning
- Evidence anchors:
  - [abstract] "InCA achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer"
  - [section] "InCA uses representations from any layer along with a novel adapter architecture" and "We observe that a single top-performing adapter trained for a downstream task is capable of achieving strong performance"
  - [corpus] Weak evidence - no direct citations found for this specific mechanism, but related work on cross-attention adapters exists
- Break condition: If task-relevant information is distributed across multiple tokens but requires information from outside the intermediate layer being attended to, or if the information requires backpropagating through the backbone for extraction

### Mechanism 2
- Claim: One-to-Many training enables efficient parallel training of multiple adapters across different layers
- Mechanism: During a single forward pass through the frozen backbone, activations from multiple layers are cached and simultaneously fed to independent adapters trained in parallel, with a single backward pass updating all adapter parameters
- Core assumption: Adapter parameters are independent and can be optimized separately without interfering with each other or requiring gradient computation through the backbone
- Evidence anchors:
  - [abstract] "During training, InCA uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks"
  - [section] "We can train up to 40 adapters in parallel with a ViT-H/14 model on a single GPU" and "since the parameters of each adapter are separate, a single backward call can then be used to optimize all the adapters"
  - [corpus] No direct corpus evidence for this specific parallel training approach
- Break condition: If adapters share parameters or if gradient computation through the backbone becomes necessary for certain tasks

### Mechanism 3
- Claim: Cross-attention provides strictly broader extraction capability than linear probing when task-relevant information is present in individual tokens but distributed across noisy representations
- Mechanism: Cross-attention with learned queries can adaptively identify and aggregate relevant tokens from noisy activation maps, while linear probing requires averaging over dimensions and loses information
- Core assumption: Task-relevant information exists as properties of individual tokens within activation maps rather than as properties of the overall averaged representation
- Evidence anchors:
  - [abstract] "Previous approaches like linear probing fall short for not having the right 'extraction capacity' of cross attention"
  - [section] "In Section 5 we provide an analytical proof for the advantage of cross-attention extraction as opposed to linear-probing under the presence of 'noisy' tokens"
  - [corpus] Weak evidence - no direct citations for this specific theoretical proof
- Break condition: If task-relevant information is uniformly distributed across all tokens or if the information can be captured through simple averaging

## Foundational Learning

- Concept: Cross-attention mechanism and its distinction from self-attention
  - Why needed here: Understanding how cross-attention differs from self-attention is crucial for grasping how InCA adapters can extract information from frozen activations
  - Quick check question: In cross-attention, what are the two distinct sets of tokens being operated on, and how does this differ from self-attention where the same tokens are used for both queries and keys?

- Concept: Transformer architecture and intermediate representations
  - Why needed here: The method relies on understanding how transformers generate hierarchical representations and how information flows through intermediate layers
  - Quick check question: In a transformer with L layers, how many distinct intermediate activation maps are available for InCA to potentially attend to, and what is the spatial resolution relationship between early and late layers?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: InCA is positioned as a parameter-efficient alternative to full fine-tuning, so understanding the landscape of efficient adaptation methods is important
- Quick check question: How does the parameter count of InCA adapters (1.3% of base model) compare to other parameter-efficient methods like LoRA or prefix tuning, and what are the computational implications of this difference?

## Architecture Onboarding

- Component map:
  Pre-trained backbone (frozen) -> Activation extraction layer selection -> Cross-attention adapter (query tokens, attention heads, classifier head) -> Parallel adapter training infrastructure -> Adapter selection/ensemble mechanism

- Critical path:
  1. Forward pass through frozen backbone to extract multiple layer activations
  2. Parallel processing of activations through independent adapters
  3. Computation of adapter-specific losses
  4. Single backward pass to update all adapter parameters
  5. Adapter selection based on validation performance

- Design tradeoffs:
  - Cross-attention vs. linear probing: Cross-attention provides better extraction capability but adds computational overhead
  - Number of adapters: More adapters enable better layer selection but increase memory usage and training time
  - Adapter architecture complexity: Deeper adapters could capture more complex patterns but reduce parameter efficiency

- Failure signatures:
  - Performance plateaus at level of linear probing rather than approaching full fine-tuning
  - Adapter training becomes unstable or diverges
  - Memory constraints prevent training desired number of adapters
  - Layer selection becomes inconsistent across different runs

- First 3 experiments:
  1. Implement single cross-attention adapter on last layer activation and compare against linear probing baseline on a simple dataset
  2. Test One-to-Many training with 2-3 adapters on different layers and evaluate selection consistency
  3. Measure memory usage and training time scaling when increasing number of parallel adapters on different backbone sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the number of cross-attention heads and performance across different types of fine-grained datasets?
- Basis in paper: [explicit] The paper notes that increasing the number of cross-attention heads improves InCA performance and suggests this is because learned latent queries can identify more useful token patterns.
- Why unresolved: The paper only mentions this relationship empirically without providing a theoretical analysis of how head count affects performance on different dataset characteristics (e.g., whether datasets are aligned-TS vs. permutable-TS).
- What evidence would resolve it: A comprehensive study varying head counts (1-16+) across datasets with different TS properties, combined with theoretical analysis of how multiple heads capture different token-separation patterns.

### Open Question 2
- Question: How does InCA's performance compare to full fine-tuning when adapting to completely novel domains versus fine-grained variations within known domains?
- Basis in paper: [inferred] The paper extensively evaluates InCA on fine-grained classification tasks but doesn't test its performance on domain shifts (e.g., natural images to medical imaging, satellite imagery, etc.).
- Why unresolved: While InCA shows strong performance on fine-grained tasks, its ability to handle domain shifts where the underlying data distribution changes fundamentally remains untested.
- What evidence would resolve it: Direct comparison of InCA vs. full fine-tuning across diverse domain shifts (medical, remote sensing, underwater imaging) measuring both accuracy and computational efficiency.

### Open Question 3
- Question: What is the optimal strategy for selecting and combining intermediate layers when the computational budget is constrained?
- Basis in paper: [explicit] The paper mentions that InCA's performance can hinge on choosing the right intermediate layer and discusses two-stage training to reduce computational costs.
- Why unresolved: The paper doesn't provide a principled method for layer selection when only a subset of layers can be used due to memory constraints, nor does it analyze the trade-off between number of layers and performance.
- What evidence would resolve it: Empirical analysis of layer selection strategies (greedy, evolutionary, or learned approaches) across different architectures and tasks, measuring the performance-cost trade-off curve.

### Open Question 4
- Question: How does InCA scale when adapting extremely large foundation models (e.g., GPT-4 vision, Gemini) to specialized downstream tasks?
- Basis in paper: [explicit] The paper demonstrates InCA's efficiency with models up to ViT-G/14 but doesn't test with the next generation of massive multimodal models.
- Why unresolved: While InCA shows promise for current large models, its performance characteristics with truly massive models (100B+ parameters) that combine vision and language capabilities remains unknown.
- What evidence would resolve it: Comparative study of InCA adaptation efficiency versus other parameter-efficient methods on next-generation foundation models, measuring both task performance and GPU memory utilization.

## Limitations

- The approach relies on the assumption that task-relevant information exists as localized patterns within individual activation tokens, which may not hold for tasks requiring global integration
- Performance is evaluated primarily on image classification tasks, leaving unclear whether the approach generalizes to other domains like detection or generation
- While showing strong performance, the comparison against other parameter-efficient methods is limited and the optimal layer selection strategy for new tasks remains heuristic rather than principled

## Confidence

- High Confidence: The computational efficiency claims (1.3% parameters, 0.2% performance gap) are well-supported by the experimental results across multiple backbone architectures and datasets
- Medium Confidence: The theoretical advantage of cross-attention over linear probing is claimed but not fully demonstrated in practice - the experiments show improved performance but don't isolate the specific benefit of cross-attention extraction versus other architectural choices
- Medium Confidence: The One-to-Many training efficiency gains are demonstrated empirically but lack theoretical analysis of convergence properties or comparison with alternative parallelization strategies

## Next Checks

1. **Layer Selection Consistency**: Run the same InCA training procedure on identical tasks with different random seeds to measure variance in optimal layer selection and adapter performance. This validates whether the method consistently identifies task-relevant layers across runs.

2. **Cross-Attention vs. Linear Probing Ablation**: Implement a controlled experiment where both methods use identical frozen backbones and activation extraction, varying only the extraction mechanism (cross-attention vs. linear probing with learned projection). This isolates the specific contribution of cross-attention to performance gains.

3. **Scaling to Multi-Modal Tasks**: Apply InCA to a multi-modal dataset (e.g., image-text tasks) to evaluate whether the cross-attention extraction generalizes beyond pure vision tasks, testing the method's applicability to scenarios where information may be distributed across different modalities.