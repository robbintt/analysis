---
ver: rpa2
title: Combinatorial Causal Bandits without Graph Skeleton
arxiv_id: '2301.13392'
source_url: https://arxiv.org/abs/2301.13392
tags:
- causal
- graph
- algorithm
- bandits
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses combinatorial causal bandits (CCB) without
  prior knowledge of the causal graph structure. The authors first prove an exponential
  lower bound for cumulative regret in the general binary causal model setting.
---

# Combinatorial Causal Bandits without Graph Skeleton

## Quick Facts
- **arXiv ID**: 2301.13392
- **Source URL**: https://arxiv.org/abs/2301.13392
- **Reference count**: 40
- **Primary result**: Proposes algorithms for combinatorial causal bandits without prior causal graph knowledge, achieving O(√T log T) regret for BGLMs and O(T²/³ log T) regret for BLMs.

## Executive Summary
This paper tackles the combinatorial causal bandit problem without requiring prior knowledge of the causal graph structure. The authors prove an exponential lower bound for cumulative regret in the general binary causal model setting, then focus on binary generalized linear models (BGLMs) and binary linear models (BLMs). They propose algorithms that can achieve competitive regret bounds without knowing the graph structure, specifically O(√T log T) for BGLMs with a weight gap assumption and O(T²/³ log T) for BLMs without this assumption. The paper also explores pure exploration in CCB without graph structure, providing adaptive algorithms and lower bounds that reveal the intrinsic hardness of the problem.

## Method Summary
The paper proposes three main algorithmic approaches. For BGLMs, the BGLM-OFU-Unknown algorithm uses an initialization phase with atomic interventions to discover ancestor-descendant relationships without knowing the graph structure, followed by maximum likelihood estimation and confidence ellipsoids for online decision-making. For BLMs, the BLM-LR-Unknown algorithm uses a longer initialization phase (O(nT²/³ log T) rounds) and linear regression instead of MLE to remove the weight gap assumption, achieving O(T²/³ log T) regret. For pure exploration, the authors propose adaptive algorithms that recover edge directions and learn reward distributions in parallel, using confidence bounds and RECOVER-EDGE sub-procedures.

## Key Results
- Proves exponential lower bound for cumulative regret in general binary causal models without graph structure
- Achieves O(√T log T) expected regret for BGLMs without graph structure under weight gap assumption
- Achieves O(T²/³ log T) expected regret for BLMs without weight gap assumption
- Shows intrinsic hardness of pure exploration without graph structure, with linear sample complexity in worst case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The initialization phase in Algorithm 1 uses controlled atomic interventions to identify ancestor-descendant relationships without knowing the graph structure.
- Mechanism: By fixing each variable Xi to 0 and 1 alternately and observing the resulting changes in other variables, the algorithm exploits the monotonicity of BGLMs to detect causal influence. If Xi is an ancestor of Xj, the difference E[Xj|do(Xi=1)] - E[Xj|do(Xi=0)] ≥ κθmin is large enough to be distinguished from noise when T is sufficiently large.
- Core assumption: The BGLM satisfies Assumption 2 (monotonicity) and the weight gap assumption θmin ≥ 2c1κ⁻¹T⁻¹/⁵.
- Evidence anchors:
  - [abstract]: "We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves O(√T ln T) expected regret..."
  - [section]: "If Xi ∈ Pa(Xj), we have E[Xj|do(Xi = 1)] − E[Xj|do(Xi = 0)] ≥ κθXi,Xj ≥ κθmin; if Xi is not an ancestor of Xj, we have E[Xj|do(Xi = 1)] = E[Xj|do(Xi = 0)]."

### Mechanism 2
- Claim: The linear regression-based algorithm for BLMs removes the weight gap assumption by accepting an O(T²/³) regret bound.
- Mechanism: Instead of trying to discover the exact graph structure, the algorithm estimates an "inaccurate model" G' where some true parent edges are omitted if their weights are below T⁻¹/³. The linear regression on G' approximates the true parameter θ' with bounded error, and the regret difference between models G and G' is bounded by n²(n+1)r where r = T⁻¹/³ is the estimated accuracy.
- Core assumption: The BLM structure allows linear regression without requiring the smoothness assumptions of BGLMs.
- Evidence anchors:
  - [abstract]: "Moreover, we propose another algorithm with O(T²/³ ln T) regret to remove the weight gap assumption."

### Mechanism 3
- Claim: The pure exploration algorithm with unknown graph structure adapts edge direction discovery and reward estimation in parallel.
- Mechanism: The algorithm maintains confidence bounds for all possible edge directions and reward estimates. When the confidence bounds for P(X'|do(X=1)) and P(X'|do(X=0)) become disjoint, the edge direction is identified. The sample complexity combines costs for identifying all edge directions (∑ 1/c²e) and for estimating rewards of hard-to-observe actions (∑ 1/max{∆a,ε/2}²).
- Core assumption: The essential graph is known but edge directions are unknown, and atomic interventions are available.
- Evidence anchors:
  - [abstract]: "Additionally, the authors discuss pure exploration in CCB without graph structure. They propose adaptive algorithms and provide lower bounds for both general causal models and atomic interventions."

## Foundational Learning

- Concept: Causal Bayesian networks and the do-calculus
  - Why needed here: The paper's algorithms rely on understanding how interventions affect the causal graph and how to compute interventional distributions using do-calculus.
  - Quick check question: If you have a causal graph X → Y → Z, what is the effect of intervening on X on the distribution of Z?

- Concept: Generalized linear models and linear regression in bandit settings
  - Why needed here: The BGLM and BLM models use these statistical frameworks, and the algorithms use maximum likelihood estimation and linear regression to estimate parameters.
  - Quick check question: What is the difference between using maximum likelihood estimation versus linear regression for parameter estimation in these causal bandit models?

- Concept: Concentration inequalities and confidence bounds
  - Why needed here: The algorithms use Hoeffding's inequality and other concentration bounds to construct confidence intervals for parameter estimates and to decide when to stop exploration.
  - Quick check question: How does Hoeffding's inequality provide a bound on the probability that the empirical mean deviates from the true mean by more than a certain amount?

## Architecture Onboarding

- Component map: Initialization phase (atomic interventions) -> Parameter estimation (MLE/linear regression) -> Online decision-making (confidence ellipsoids)
- Critical path: The initialization phase must complete before the iterative phase can begin, as the estimated graph structure is needed for parameter estimation. The parameter estimation feeds into the confidence bounds used for action selection.
- Design tradeoffs: BGLM algorithm achieves O(√T log T) regret but requires weight gap assumption; BLM algorithm removes this assumption but has worse O(T²/³ log T) regret. Pure exploration has intrinsic hardness without graph structure.
- Failure signatures: If the weight gap assumption is violated, the initialization phase may misidentify ancestor-descendant relationships, leading to incorrect parameter estimates and suboptimal regret. If T is too small, the concentration bounds may not hold, leading to incorrect decisions.
- First 3 experiments:
  1. Implement the initialization phase on a small synthetic BGLM with known structure and verify that ancestor-descendant relationships are correctly identified.
  2. Test the BLM algorithm on a linear model with very small edge weights to confirm that it handles cases where the weight gap assumption would fail.
  3. Implement the pure exploration algorithm and measure sample complexity on graphs with varying edge distinguishability (different ce values).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can we design a combinatorial causal bandit algorithm with O(√T log T) regret without any weight gap assumption?
  - Basis in paper: The authors show that their BGLM-OFU-Unknown algorithm achieves O(√T log T) regret with a weight gap assumption, and their BLM-LR-Unknown-SG algorithm achieves O(T²/³ log T) regret without a weight gap assumption. They explicitly state this as an open problem.
  - Why unresolved: The weight gap assumption is a strong requirement that may not hold in practice. Removing it while maintaining the O(√T log T) regret bound is challenging.
  - What evidence would resolve it: A new algorithm design that can achieve O(√T log T) regret without any weight gap assumption, along with a rigorous theoretical proof.

- **Open Question 2**: Can we improve the sample complexity of pure exploration in combinatorial causal bandits without graph structure?
  - Basis in paper: The authors show that pure exploration for general causal models without graph structure has an intrinsic hardness, leading to a linear sample complexity in the worst case. They also propose an adaptive algorithm that can improve the sample complexity for some cases.
  - Why unresolved: The authors' adaptive algorithm still has a linear sample complexity in the worst case. Improving the sample complexity while maintaining correctness is challenging.
  - What evidence would resolve it: A new algorithm design that can achieve a sublinear sample complexity for pure exploration in combinatorial causal bandits without graph structure, along with a rigorous theoretical proof.

- **Open Question 3**: Can we extend the results to more general causal models, such as linear structural equation models (SEMs)?
  - Basis in paper: The authors mention that similar initialization methods and proof techniques can be used to design causal bandits algorithms for other parametric models without the skeleton, like linear SEMs.
  - Why unresolved: The authors only focus on binary generalized linear models (BGLMs) and binary linear models (BLMs) in their paper. Extending the results to more general causal models is an interesting direction.
  - What evidence would resolve it: A new algorithm design that can handle more general causal models, such as linear SEMs, along with a rigorous theoretical analysis of its regret or sample complexity.

## Limitations

- The weight gap assumption is a strong requirement that may not hold in practice, limiting the applicability of the BGLM algorithm
- Pure exploration without graph structure has intrinsic hardness, leading to linear sample complexity in the worst case
- The initialization phase may fail when T is insufficient for reliable concentration bounds or when the weight gap assumption is violated

## Confidence

- **High confidence**: The O(√T log T) regret bound for BGLM-OFU-Unknown under the weight gap assumption is well-supported by the theoretical analysis and matches existing results that require graph structure.
- **Medium confidence**: The O(T²/³ log T) regret bound for BLMs without weight gap assumptions is theoretically sound but may have looser constants in practice.
- **Medium confidence**: The pure exploration results showing linear sample complexity without graph structure are supported by lower bounds, but the practical implications depend on the specific problem instances.

## Next Checks

1. Empirical validation of the initialization phase on synthetic BGLMs with varying weight gaps to verify the algorithm's ability to correctly identify ancestor-descendant relationships.
2. Comparative analysis of BGLM-OFU-Unknown versus algorithms requiring known graph structure across different graph densities and weight distributions.
3. Experimental evaluation of the BLM-LR-Unknown algorithm on problems where the weight gap assumption is violated to confirm the claimed O(T²/³ log T) regret bound.