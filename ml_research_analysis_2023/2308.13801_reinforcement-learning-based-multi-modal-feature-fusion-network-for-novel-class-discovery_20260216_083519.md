---
ver: rpa2
title: Reinforcement Learning Based Multi-modal Feature Fusion Network for Novel Class
  Discovery
arxiv_id: '2308.13801'
source_url: https://arxiv.org/abs/2308.13801
tags:
- learning
- data
- unlabeled
- feature
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning based multi-modal
  feature fusion network for novel class discovery. The key idea is to use a member-to-leader
  multi-agent framework to extract and fuse features from multi-modal data, and employ
  reinforcement learning to train the network to discover novel classes in an open-set
  scenario.
---

# Reinforcement Learning Based Multi-modal Feature Fusion Network for Novel Class Discovery

## Quick Facts
- arXiv ID: 2308.13801
- Source URL: https://arxiv.org/abs/2308.13801
- Reference count: 40
- Key outcome: Proposed RL-based multi-modal feature fusion network achieves 0.656 mAP on OS-MN40 dataset

## Executive Summary
This paper introduces a reinforcement learning based multi-modal feature fusion network for novel class discovery in open-set scenarios. The approach employs a Member-to-Leader Multi-Agent framework where individual agents process different data modalities and a leader agent fuses the features. A clustering strategy with varying constraint conditions (STLClu) is used to progressively transform unlabeled data into labeled data during training. The method is evaluated on 3D model datasets OS-MN40 and OS-MN40-Miss, showing competitive performance compared to state-of-the-art methods.

## Method Summary
The method uses a Member-to-Leader Multi-Agent framework with m+1 agents to process multi-modal data. Each member agent extracts features from a specific modality (multi-view, point cloud, mesh, voxel), while the leader agent fuses these features using multi-head self-attention. A reinforcement learning framework is employed where agents interact with an environment to maximize rewards. The STLClu clustering strategy progressively transforms unlabeled data into labeled data by starting with strict DBSCAN parameters and gradually relaxing them. The network is trained using a combination of TD error loss, cross-entropy loss, and self-supervised contrastive loss.

## Key Results
- Achieves 0.656 mAP on OS-MN40 dataset
- Achieves 0.439 mAP on OS-MN40-Miss dataset
- Demonstrates generalizability with transfer experiment on CIFAR-10 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reinforcement learning framework enables effective exploration of novel classes in open-set scenarios.
- Mechanism: Member-to-Leader Multi-Agent framework processes modalities independently and fuses features to interact with environment, mimicking human exploratory learning through iterative parameter updates based on environmental rewards.
- Core assumption: Environment provides meaningful feedback (rewards) that guide agent toward correct classification decisions.
- Evidence anchors: [abstract] "Reinforcement Learning framework to simulate cognitive processes...for effectively addressing novel class discovery in Open-set domain." [section] "Our agents aim to accurately recognize known categories...and subsequently classify a portion of unlabeled objects with discernible features based on their acquired prior knowledge."
- Break condition: If reward signal is noisy or uninformative, RL agent cannot learn meaningful policies for novel class discovery.

### Mechanism 2
- Claim: STLClu strategy progressively transforms unlabeled data into labeled data.
- Mechanism: Starting with strict DBSCAN parameters for high-confidence clustering, then gradually relaxing constraints to incorporate more data through iterative process.
- Core assumption: Feature vectors of unlabeled objects sharing recognized features show higher similarity in mathematical space.
- Evidence anchors: [abstract] "clustering method with varying constraint conditions...allowing for generation of dependable labels for subset of unlabeled data during training phase." [section] "By employing clustering method with constraint conditions ranging from strict to loose...we transform these unlabeled data into labeled data."
- Break condition: If feature similarity does not correlate with class membership, clustering generates incorrect labels that propagate errors.

### Mechanism 3
- Claim: Multi-modal feature fusion provides more comprehensive understanding of feature space.
- Mechanism: Member agents extract modality-specific features fused by leader agent using multi-head self-attention, creating global feature vector capturing information from all modalities.
- Core assumption: Different modalities provide complementary information that improves classification accuracy.
- Evidence anchors: [abstract] "Member-to-Leader Multi-Agent framework to extract and fuse features from multi-modal information...aiming to acquire more comprehensive understanding of feature space." [section] "Multiple agents are employed within reinforcement learning framework to process diverse data modalities."
- Break condition: If modalities are highly redundant or conflicting, fusion may not improve and could degrade performance.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (Markov Decision Processes, Q-learning, temporal difference learning)
  - Why needed here: Entire framework built on RL principles for learning policies that maximize cumulative rewards
  - Quick check question: What is the difference between Q-learning and policy gradient methods in reinforcement learning?

- Concept: Clustering algorithms (DBSCAN, k-means, hierarchical clustering)
  - Why needed here: STLClu strategy relies on density-based clustering to generate pseudo-labels for unlabeled data
  - Quick check question: How does DBSCAN's epsilon parameter affect the number and size of clusters formed?

- Concept: Multi-modal fusion techniques (attention mechanisms, feature concatenation, weighted averaging)
  - Why needed here: Member-to-Leader architecture requires effective fusion of features from different modalities
  - Quick check question: What are the advantages of using self-attention over simple concatenation for feature fusion?

## Architecture Onboarding

- Component map: Member agents (one per modality) → Leader agent (feature fusion) → Environment (reward computation) → STLClu module (label generation)
- Critical path: Data → Member agents → Leader agent → Environment interaction → Reward → Parameter update → STLClu → Next epoch
- Design tradeoffs: Single vs multi-modal inputs, strict vs loose clustering parameters, exploration vs exploitation balance
- Failure signatures: Poor clustering quality (incorrect pseudo-labels), reward signal instability, modality imbalance affecting fusion quality
- First 3 experiments:
  1. Test single-agent version without multi-modal fusion on labeled data only to establish baseline
  2. Test STLClu strategy alone on fixed feature representations to validate clustering approach
  3. Test full multi-agent system with synthetic rewards to verify reward-based learning before integrating with environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale with number of modalities beyond four used in experiments?
- Basis in paper: [explicit] Ablation experiments used different combinations of four modalities (multi-view, point cloud, mesh, voxel) found increasing modalities improved performance.
- Why unresolved: Experiments limited to four modalities, paper doesn't explore effects of using more than four or fewer than two.
- What evidence would resolve it: Experiments with varying number of modalities (e.g., 2, 3, 5, or more) comparing performance to determine optimal number.

### Open Question 2
- Question: How does method handle missing modalities during training and inference, and what is impact on performance?
- Basis in paper: [explicit] Mentions replacing missing modality features with zero vectors and conducted experiments on OS-MN40-Miss dataset which randomly drops modalities with probability 0.4.
- Why unresolved: Paper doesn't provide detailed analysis of handling missing modalities during training and inference, nor explore impact of different missing modality patterns on performance.
- What evidence would resolve it: Experiments with different missing modality patterns during training and inference, analyzing impact on performance compared to baseline.

### Open Question 3
- Question: How does method compare to other state-of-the-art methods on large-scale datasets with more classes and instances?
- Basis in paper: [inferred] Evaluated on OS-MN40 and OS-MN40-Miss datasets containing 40 classes and 12,309 objects. Paper doesn't compare performance to other methods on larger datasets.
- Why unresolved: Experiments limited to OS-MN40 and OS-MN40-Miss datasets, paper doesn't provide comparison with other methods on larger datasets with more classes and instances.
- What evidence would resolve it: Experiments on larger datasets with more classes and instances (e.g., ModelNet40, ShapeNetCore) comparing proposed method to other state-of-the-art methods.

## Limitations

- Approach heavily relies on quality of clustering-based label generation strategy which may struggle with highly imbalanced or noisy multi-modal data
- Reinforcement learning component introduces additional complexity and potential instability compared to standard supervised approaches
- Effectiveness primarily validated on synthetic 3D model datasets with limited testing on real-world multi-modal data

## Confidence

- High confidence: Basic RL framework architecture and its application to novel class discovery
- Medium confidence: Effectiveness of STLClu clustering strategy and its progressive label generation
- Medium confidence: Reported performance improvements on OS-MN40 and OS-MN40-Miss datasets
- Low confidence: Generalization to real-world multi-modal datasets beyond synthetic 3D model data

## Next Checks

1. Conduct ablation studies to isolate contribution of multi-modal fusion versus reinforcement learning components to performance improvements.

2. Test approach on real-world multi-modal datasets (e.g., audio-visual speech recognition or multi-sensor robotics data) to evaluate generalization beyond synthetic 3D models.

3. Analyze stability and convergence of reinforcement learning training process across different random seeds and hyperparameter settings to assess reproducibility.