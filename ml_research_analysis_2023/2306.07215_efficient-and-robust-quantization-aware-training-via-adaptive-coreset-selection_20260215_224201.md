---
ver: rpa2
title: Efficient and Robust Quantization-aware Training via Adaptive Coreset Selection
arxiv_id: '2306.07215'
source_url: https://arxiv.org/abs/2306.07215
tags:
- training
- selection
- coreset
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a quantization-aware adaptive coreset selection
  (ACS) method to improve the efficiency of quantization-aware training (QAT) by selecting
  important samples based on two metrics: error vector score (EVS) and disagreement
  score (DS). The method adaptively selects samples that fit the current training
  stage, considering current epochs, EVS, and DS.'
---

# Efficient and Robust Quantization-aware Training via Adaptive Coreset Selection

## Quick Facts
- **arXiv ID**: 2306.07215
- **Source URL**: https://arxiv.org/abs/2306.07215
- **Reference count**: 40
- **Key outcome**: Achieved 68.39% accuracy for 4-bit quantized ResNet-18 on ImageNet-1K with only 10% of data, showing 4.24% absolute gain over baseline

## Executive Summary
This paper addresses the challenge of efficient quantization-aware training (QAT) by proposing an adaptive coreset selection (ACS) method. The core idea is to dynamically select the most informative training samples at each epoch based on two metrics: Error Vector Score (EVS) and Disagreement Score (DS). By focusing training on these selected samples rather than the full dataset, the method achieves significant efficiency improvements while maintaining competitive accuracy. The approach is particularly effective for low-bit quantization where computational efficiency is critical.

## Method Summary
The method introduces two importance metrics for quantization-aware training: Error Vector Score (EVS) which approximates gradient changes based on prediction errors, and Disagreement Score (DS) which measures the divergence between quantized and full-precision model predictions. These metrics are combined using a cosine annealing weight coefficient that balances their contributions across different training stages. The adaptive coreset selection updates every R epochs, selecting the top samples based on the weighted combination of EVS and DS. This dynamic selection strategy focuses computational resources on samples that are most beneficial for the current training objective, improving both efficiency and effectiveness of the quantization process.

## Key Results
- Achieved 68.39% top-1 accuracy on 4-bit ResNet-18 ImageNet with only 10% data subset
- Outperformed random selection and other coreset methods with up to 4.24% absolute gain
- Demonstrated consistent improvements across different bit-widths (2-4 bits) and architectures (ResNet-18, MobileNetV2)
- Showed significant efficiency gains with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error Vector Score (EVS) effectively approximates the importance of training samples by measuring the prediction error of the quantized model
- Mechanism: EVS derives from the gradient norm of the loss function with respect to quantized weights, approximating loss change when a sample is removed
- Core assumption: Gradient norm can be approximated by error vector norm (difference between prediction and true label)
- Evidence anchors: Abstract mentions EVS based on analysis of loss and gradient of quantized weights; section describes EVS approximation without memory overhead
- Break condition: If gradient norm and error vector norm relationship is invalid, or predictions aren't correlated with true labels

### Mechanism 2
- Claim: Disagreement Score (DS) quantifies importance by measuring prediction gap between quantized student and full-precision teacher models
- Mechanism: DS is the norm of difference between quantized and full-precision model predictions, identifying samples where quantized predictions diverge significantly
- Core assumption: Discrepancy between quantized and full-precision models indicates sample importance for QAT
- Evidence anchors: Abstract mentions DS measuring prediction gap; section defines DS as dependent only on current training sample
- Break condition: If disagreement score doesn't correlate with sample importance, or teacher model predictions aren't accurate

### Mechanism 3
- Claim: Adaptive Coreset Selection (ACS) improves efficiency by dynamically selecting samples based on EVS and DS at different training stages
- Mechanism: ACS uses cosine annealing weight coefficient to balance EVS and DS contributions at different epochs, selecting most important samples for each epoch
- Core assumption: Sample importance changes during training, and dynamic selection improves QAT efficiency and effectiveness
- Evidence anchors: Abstract mentions ACS selects data for current training epoch; section explains different metrics needed for different QAT stages
- Break condition: If importance metrics don't change significantly during training, or dynamic selection doesn't improve efficiency

## Foundational Learning

- Concept: Quantization-aware training (QAT)
  - Why needed here: QAT is the core method being improved by adaptive coreset selection
  - Quick check question: What is the primary goal of quantization-aware training, and how does it differ from standard training?

- Concept: Coreset selection
  - Why needed here: Coreset selection is the key technique used to improve QAT efficiency by selecting most informative samples
  - Quick check question: How does coreset selection improve training efficiency, and what are different approaches to coreset selection?

- Concept: Error Vector Score (EVS) and Disagreement Score (DS)
  - Why needed here: EVS and DS are the two metrics proposed to quantify training sample importance
  - Quick check question: How are EVS and DS calculated, and what is the intuition behind each metric?

## Architecture Onboarding

- Component map:
  Input (dataset, real-value network, coreset fraction, epochs, selection interval) -> Quantization-aware training with adaptive coreset selection (EVS/DS calculation, ACS selection, training) -> Output (quantized network)

- Critical path:
  1. Initialize quantized weights from real-value weights
  2. For each epoch, calculate EVS and DS for each sample
  3. Select top samples based on ACS metric (weighted combination of EVS and DS)
  4. Train quantized model on selected coreset

- Design tradeoffs:
  - Balancing between EVS and DS: Using cosine annealing weight coefficient to balance metrics at different training stages
  - Selection interval: Choosing optimal interval for updating coreset to balance efficiency and effectiveness

- Failure signatures:
  - Low accuracy or high loss during training
  - Slow convergence or unstable training dynamics
  - Inefficient use of training data

- First 3 experiments:
  1. Evaluate proposed ACS method on CIFAR-10 with ResNet-18, compare with random selection and other coreset methods
  2. Analyze impact of selection interval (R) on performance and efficiency of proposed method
  3. Investigate effect of annealing strategy (beta) on balance between EVS and DS and overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when applied to non-classification tasks, such as regression?
- Basis in paper: Authors mention analysis is based on classification task assumption with SGD optimizer, effectiveness on regression and other optimizers is not guaranteed
- Why unresolved: Method not evaluated on regression tasks or other optimizers
- What evidence would resolve it: Experimental results on regression tasks or different optimizers would provide evidence for effectiveness

### Open Question 2
- Question: Can the proposed method be effectively applied to emerging deep learning models, such as Transformers?
- Basis in paper: Authors mention they did not evaluate effectiveness and efficiency on emerging deep learning models like Transformers
- Why unresolved: Lack of evaluation on Transformer models creates uncertainty about applicability
- What evidence would resolve it: Experimental results on Transformer models would provide evidence for effectiveness

### Open Question 3
- Question: How does the proposed method perform on different hardware systems?
- Basis in paper: Authors state they did not evaluate effectiveness and efficiency on different hardware systems
- Why unresolved: Absence of evaluation on various hardware systems leaves uncertainty about adaptability
- What evidence would resolve it: Experimental results on different hardware systems would provide evidence for effectiveness and adaptability

## Limitations
- Effectiveness primarily demonstrated on classification tasks with standard architectures (ResNet-18, MobileNetV2)
- Computational overhead of metric calculations versus training time savings not fully quantified
- Annealing strategy for balancing EVS and DS is empirically chosen without theoretical grounding

## Confidence

**Confidence Labels:**
- High confidence in empirical results on tested datasets/architectures
- Medium confidence in theoretical justification for EVS and DS metrics
- Medium confidence in generalization claims beyond tested scenarios
- Low confidence in scalability to very large datasets or different task types

**Major Uncertainties:**
1. Relationship between error vector norm and gradient norm approximations remains unverified beyond empirical results
2. Annealing strategy for balancing EVS and DS is empirically chosen without theoretical grounding
3. Computational overhead of metric calculations versus benefits is not fully quantified

## Next Checks

1. **Metric Validation**: Verify correlation between EVS/DS metrics and actual sample importance by conducting ablation studies where samples are ranked by these metrics and their impact on training is measured

2. **Generalization Test**: Evaluate method on non-classification tasks (segmentation, detection) and different model architectures (transformers, MLPs) to assess broader applicability

3. **Computational Overhead Analysis**: Measure and compare computational cost of EVS/DS calculations against training time savings from using smaller coresets across different dataset sizes