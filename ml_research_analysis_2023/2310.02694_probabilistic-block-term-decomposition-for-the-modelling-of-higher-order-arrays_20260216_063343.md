---
ver: rpa2
title: Probabilistic Block Term Decomposition for the Modelling of Higher-Order Arrays
arxiv_id: '2310.02694'
source_url: https://arxiv.org/abs/2310.02694
tags:
- tucker
- data
- tensor
- bayesian
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic framework for the Block Term
  Decomposition (BTD) of tensors, which bridges the Canonical Polyadic Decomposition
  (CPD) and Tucker models. The proposed method uses variational Bayesian inference
  with a von Mises-Fisher distribution to enforce orthogonality in the factor matrices,
  and automatic relevance determination (ARD) for core array regularization.
---

# Probabilistic Block Term Decomposition for the Modelling of Higher-Order Arrays

## Quick Facts
- arXiv ID: 2310.02694
- Source URL: https://arxiv.org/abs/2310.02694
- Authors: 
- Reference count: 13
- Primary result: Probabilistic framework for Block Term Decomposition using variational Bayesian inference with orthogonality and sparsity priors

## Executive Summary
This paper introduces a probabilistic framework for Block Term Decomposition (BTD) of tensors that bridges Canonical Polyadic Decomposition (CPD) and Tucker models. The method uses variational Bayesian inference with a von Mises-Fisher distribution to enforce orthogonality in factor matrices and automatic relevance determination (ARD) for core array regularization. Experiments demonstrate that the probabilistic BTD (pBTD) provides robust model order selection and improved performance under noise compared to maximum likelihood estimation approaches.

## Method Summary
The probabilistic BTD model employs variational Bayesian inference to approximate the posterior over tensor decomposition parameters. The approach uses a von Mises-Fisher distribution to enforce orthogonality in factor matrices, while ARD priors with sparsity constraints enable automatic pruning of irrelevant core elements. The evidence lower bound (ELBO) serves as both a convergence metric and a principled model selection criterion, allowing the method to automatically identify whether CPD, BTD, or Tucker structure best explains the data. The inference algorithm iteratively updates factor matrices, core arrays, precision parameters, and noise levels until convergence.

## Key Results
- pBTD achieves robust model order selection through ELBO comparison across candidate BTD configurations
- The method effectively prunes irrelevant components using ARD, enabling good performance even with over-specified models
- Experiments on synthetic and real datasets show improved performance under noise compared to MLE-based BTD methods

## Why This Works (Mechanism)

### Mechanism 1
The probabilistic BTD model identifies correct tensor structure by comparing ELBO scores across candidate model orders. Variational Bayesian inference approximates the posterior over parameters and model complexity, with ELBO serving as a principled model selection criterion that balances fit and complexity.

### Mechanism 2
Automatic relevance determination (ARD) via sparsity priors on core elements enables effective pruning of irrelevant components. The sparsity prior assigns precision hyperparameters to each core element, which are learned during inference and drive irrelevant elements toward zero.

### Mechanism 3
Enforcing orthogonality in factor matrices via the von Mises-Fisher distribution preserves decoupling of core elements, enabling efficient univariate updates and stable inference. The orthogonality property allows efficient computation of core covariance as off-diagonal terms vanish.

## Foundational Learning

- **Variational Bayesian inference**: Mean-field approximation and ELBO maximization are needed to approximate intractable posteriors and enable model selection. Quick check: What is the ELBO and why does maximizing it balance fit and complexity?

- **Tensor decompositions (CPD, Tucker, BTD)**: Understanding these structures is essential to interpret results and set up experiments. Quick check: How does a BTD model with one core and diagonal core reduce to CPD?

- **Automatic relevance determination (ARD)**: ARD enables pruning of irrelevant core elements and model selection. Quick check: In the sparsity prior, what happens to a core element whose precision hyperparameter grows large during inference?

## Architecture Onboarding

- **Component map**: Data tensor X -> Model family (BTD with T cores) -> Priors (vMF, Gamma) -> Variational inference updates -> Outputs (posterior means, ELBO, pruned cores)

- **Critical path**: 1) Initialize factor matrices, core arrays, precisions 2) Iterate updates for factors, cores, precisions, noise 3) Compute ELBO to monitor convergence 4) Select model with highest ELBO

- **Design tradeoffs**: Orthogonality vs flexibility (vMF constraint ensures stability but may bias non-orthogonal data); Sparsity vs density (ARD enables pruning but may underfit dense structures); Variational vs MCMC (faster but may underestimate uncertainty)

- **Failure signatures**: ELBO plateaus early (possible local optimum or restrictive priors); All core elements pruned (model order too large or noise too high); Factor norms unstable (vMF concentration too low or data ill-conditioned)

- **First 3 experiments**: 1) Fit BTD(4,3) to synthetic data with known ground truth; check reconstruction error and ELBO vs MLE baselines. 2) Fit multiple BTD configurations to synthetic data; verify ELBO correctly identifies true model order. 3) Apply pBTD to Flow Injection dataset; inspect core sparsity and factor orthogonality; compare ELBO across model orders.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several methodological considerations remain:

## Limitations

- The vMF prior for orthogonality is theoretically sound but untested on non-orthogonal data
- ARD-based pruning assumes sparsity in core elements, which may not hold for dense tensor structures
- Evidence for ELBO-based model selection in the BTD family is limited to presented experiments
- Variational inference may underestimate uncertainty compared to sampling methods

## Confidence

- **High confidence**: BTD as bridge between CPD and Tucker; ARD mechanism for core pruning; orthogonality enabling efficient covariance calculation
- **Medium confidence**: ELBO as reliable model selection criterion for BTD structure; variational inference robustness across diverse tensor structures
- **Low confidence**: Performance on highly non-orthogonal data; scalability to large tensors; comparison with non-probabilistic state-of-the-art

## Next Checks

1. Test pBTD on synthetic data with non-orthogonal factors to assess vMF prior robustness
2. Apply pBTD to larger real-world tensors (e.g., neuroimaging) to evaluate scalability and computational efficiency
3. Compare pBTD ELBO-based model selection against cross-validation on held-out data for a broader range of tensor structures