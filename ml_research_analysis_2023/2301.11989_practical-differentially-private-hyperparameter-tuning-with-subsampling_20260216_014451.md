---
ver: rpa2
title: Practical Differentially Private Hyperparameter Tuning with Subsampling
arxiv_id: '2301.11989'
source_url: https://arxiv.org/abs/2301.11989
tags:
- tuning
- strategy
- privacy
- hyperparameter
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method to lower the privacy and computational
  cost of differentially private (DP) hyperparameter tuning. The key idea is to use
  a random subset of the sensitive data for tuning and then extrapolate the optimal
  hyperparameters to the full dataset.
---

# Practical Differentially Private Hyperparameter Tuning with Subsampling

## Quick Facts
- arXiv ID: 2301.11989
- Source URL: https://arxiv.org/abs/2301.11989
- Reference count: 0
- Key outcome: This paper proposes a novel method to lower the privacy and computational cost of differentially private (DP) hyperparameter tuning by using random subsets of sensitive data and extrapolating optimal hyperparameters.

## Executive Summary
This paper addresses the high privacy and computational costs of differentially private hyperparameter tuning by introducing a novel approach that uses random subsampling of the sensitive data. The method involves running the tuning algorithm on a small random subset of the data and then extrapolating the optimal hyperparameters to the full dataset. The authors provide a tailored Rényi differential privacy (RDP) analysis for this approach and present algorithms for tuning hyperparameters that affect RDP guarantees. Experiments on various datasets demonstrate consistent improvements in the privacy-utility tradeoff compared to baseline methods.

## Method Summary
The paper proposes two strategies for DP hyperparameter tuning using subsampling. Strategy 1 involves subsampling a random subset of the data, running the hyperparameter tuning algorithm on this subset, and extrapolating the optimal hyperparameters to the full dataset using simple scaling heuristics. Strategy 2 divides the data into disjoint shards, tunes hyperparameters on one shard, trains models on other shards, and ensembles the predictions. Both strategies leverage privacy amplification through subsampling and provide RDP analysis tailored to the subsampling approach. The methods are evaluated on standard benchmark datasets (CIFAR-10, MNIST, FashionMNIST, IMDB) using neural network models and DP-SGD as the base mechanism.

## Key Results
- The proposed method consistently improves the privacy-utility tradeoff compared to baseline DP hyperparameter tuning approaches
- Subsampling ratio of 0.1 provides good balance between privacy cost and utility in experiments
- Extrapolation of hyperparameters from small subsets to full datasets preserves utility while reducing privacy cost
- The RDP analysis framework accurately captures privacy guarantees for the combined procedure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a random subsample of the sensitive data for hyperparameter tuning reduces both privacy cost and computational cost while maintaining utility.
- Mechanism: Instead of tuning hyperparameters on the full dataset, we randomly subsample a subset X1 of size q*n (where q is the subsampling ratio). The tuning algorithm M1 is run on X1 to find optimal hyperparameters. These are then either used directly on X1 or extrapolated to the full dataset X.
- Core assumption: The optimal hyperparameters for the subsample X1 are similar enough to those for the full dataset X that the performance degradation is acceptable.
- Evidence anchors: [abstract], [section 3.1], [corpus]

### Mechanism 2
- Claim: Extrapolating hyperparameters from a small dataset to a larger dataset preserves utility while reducing privacy cost.
- Mechanism: After finding optimal hyperparameters on the subsample X1, we scale certain hyperparameters (like learning rate) proportionally to the dataset size difference.
- Core assumption: The relationship between optimal hyperparameters and dataset size follows a predictable pattern that can be captured by simple scaling rules.
- Evidence anchors: [section 3.1.1], [abstract], [corpus]

### Mechanism 3
- Claim: Tuning hyperparameters that affect RDP guarantees requires careful adjustment of training parameters to maintain privacy bounds.
- Mechanism: When tuning parameters like noise level σ, subsampling ratio γ, or training length T that affect the RDP guarantees, we need to ensure that for each candidate model, the RDP value is bounded by a uniform upper bound ε(λ) across all λ values.
- Core assumption: The RDP values of candidate models can be uniformly bounded by finding appropriate training parameters for each hyperparameter combination.
- Evidence anchors: [section 4.1], [section 4.3], [corpus]

## Foundational Learning

- Concept: Rényi Differential Privacy (RDP)
  - Why needed here: The paper's privacy analysis and bounds are formulated in terms of RDP, which provides tighter composition bounds than traditional (ε,δ)-DP for iterative algorithms like DP-SGD.
  - Quick check question: How does RDP of order λ relate to (ε,δ)-DP, and why is this relationship important for hyperparameter tuning?

- Concept: Subsampling amplification
  - Why needed here: Subsampling a random subset of data for hyperparameter tuning provides privacy amplification, reducing the overall privacy cost compared to using the full dataset.
  - Quick check question: Why does subsampling amplify privacy, and how does this amplification affect the RDP bounds in the proposed method?

- Concept: Privacy composition
  - Why needed here: The privacy cost of hyperparameter tuning must be composed with the privacy cost of training the final model, requiring careful accounting of RDP parameters across multiple uses of the data.
  - Quick check question: How do you compose RDP guarantees when the same data is used for both hyperparameter tuning and model training?

## Architecture Onboarding

- Component map: Data subsampling -> Hyperparameter tuning -> Extrapolation -> Model training
- Critical path: Data subsampling → Hyperparameter tuning → Extrapolation (if needed) → Model training
- Design tradeoffs:
  - Subsampling ratio q vs. utility: Lower q reduces privacy cost but may yield suboptimal hyperparameters
  - Extrapolation method vs. accuracy: Simple scaling rules are easy to implement but may not capture complex relationships
  - Computational cost vs. privacy: Larger µ (number of hyperparameter candidates) improves utility but increases privacy cost
- Failure signatures:
  - Privacy accounting errors: Incorrect RDP bounds leading to overestimated privacy guarantees
  - Extrapolation failures: Optimal hyperparameters for X1 don't transfer well to X, causing performance degradation
  - Subsampling bias: The random subsample X1 is not representative of X, leading to poor hyperparameter choices
- First 3 experiments:
  1. Implement the basic mechanism with q=0.1 on a small dataset (e.g., MNIST) with only learning rate tuning, comparing privacy-utility tradeoff against baseline
  2. Test different extrapolation methods (learning rate scaling, no scaling, etc.) on the same dataset to evaluate impact on utility
  3. Vary the subsampling ratio q and measure the effect on both privacy bounds and model performance to find the optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are more sophisticated hyperparameter extrapolation methods compared to the simple linear scaling heuristics used in the paper?
- Basis in paper: [explicit] The authors mention that exploring more sophisticated extrapolation methods is an interesting avenue for future work, noting that principled ways have been considered in non-DP settings (e.g., Klein et al., 2017).
- Why unresolved: The paper only experiments with simple linear scaling heuristics for extrapolating hyperparameters. It does not explore or compare the effectiveness of more advanced extrapolation techniques.
- What evidence would resolve it: Experiments comparing the privacy-utility tradeoff achieved by simple linear scaling versus more sophisticated extrapolation methods (e.g., Bayesian optimization, meta-learning) across various datasets and models would provide evidence.

### Open Question 2
- Question: How does the privacy-utility tradeoff of the ensemble method (Strategy 2) compare to Strategy 1 when using larger values of k (number of shards)?
- Basis in paper: [explicit] The authors note that Strategy 2 leads to a slightly worse privacy-utility tradeoff compared to Strategy 1, albeit with computational speedups. They also experimentally explore the effect of varying k in the ensemble method.
- Why unresolved: While the paper explores the effect of k on Strategy 2, it does not provide a comprehensive comparison of the privacy-utility tradeoff between Strategy 1 and Strategy 2 for larger values of k.
- What evidence would resolve it: Experiments comparing the privacy-utility tradeoff of Strategy 1 and Strategy 2 for various values of k (e.g., k = 2, 4, 8, 16) across different datasets and models would provide evidence.

### Open Question 3
- Question: How does the choice of q (subsampling ratio) in Strategy 1 affect the privacy-utility tradeoff, especially for smaller values of q?
- Basis in paper: [explicit] The authors note that smaller values of q could lead to better privacy-utility tradeoffs but use a fixed value of q = 0.1 in all experiments for simplicity.
- Why unresolved: The paper does not provide a comprehensive exploration of the effect of q on the privacy-utility tradeoff, especially for smaller values of q that could potentially yield better results.
- What evidence would resolve it: Experiments comparing the privacy-utility tradeoff of Strategy 1 for various values of q (e.g., q = 0.05, 0.1, 0.2, 0.3) across different datasets and models would provide evidence.

## Limitations
- The RDP composition bounds may become loose when multiple privacy-sensitive operations are combined
- Hyperparameter extrapolation mechanism is heuristic-based without theoretical guarantees
- Experiments primarily focus on image classification tasks, limiting generalizability to other domains

## Confidence

- **High Confidence**: The core mechanism of using subsampling for privacy amplification and computational efficiency is well-established in differential privacy literature. The RDP composition framework and its application to DP-SGD are mathematically sound.
- **Medium Confidence**: The specific RDP analysis for the combined procedure (subsampling + hyperparameter tuning + model training) appears correct but relies on composition bounds that may be loose in practice.
- **Low Confidence**: The effectiveness of hyperparameter extrapolation across different dataset sizes lacks theoretical justification and may not generalize well beyond the tested scenarios.

## Next Checks

1. Conduct ablation studies varying the subsampling ratio q to quantify the exact privacy-utility tradeoff curve and identify optimal values for different target ε.
2. Test the hyperparameter extrapolation mechanism on non-image datasets (e.g., text or tabular data) to assess generalizability beyond the current experimental scope.
3. Implement a tighter privacy accounting mechanism using the moments accountant approach to compare against the RDP-based bounds and measure potential improvements in privacy guarantees.