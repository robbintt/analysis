---
ver: rpa2
title: Robust Lipschitz Bandits to Adversarial Corruptions
arxiv_id: '2305.18543'
source_url: https://arxiv.org/abs/2305.18543
tags:
- algorithm
- regret
- zooming
- robust
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces robust Lipschitz bandit algorithms against
  adversarial corruptions, addressing a gap in literature focused on discrete arm
  sets. The authors propose RMEL for weak adversaries, achieving regret bound O((C^(1/(dz+2))
  + 1)T^(dz+1)/(dz+2)), and Robust Zooming for strong adversaries with known budgets,
  achieving O(T^(dz+1)/(dz+2) + C^(1/(dz+1))T^(dz)/(dz+1)).
---

# Robust Lipschitz Bandits to Adversarial Corruptions

## Quick Facts
- arXiv ID: 2305.18543
- Source URL: https://arxiv.org/abs/2305.18543
- Authors: 
- Reference count: 40
- Key outcome: This work introduces robust Lipschitz bandit algorithms against adversarial corruptions, addressing a gap in literature focused on discrete arm sets. The authors propose RMEL for weak adversaries, achieving regret bound O((C^(1/(d_z+2)) + 1)T^(d_z+1)/(d_z+2)), and Robust Zooming for strong adversaries with known budgets, achieving O(T^(d_z+1)/(d_z+2) + C^(1/(d_z+1))T^(d_z)/(d_z+1)). They also develop BoB Robust Zooming for unknown budgets, with regret O(T^(d+3)/(d+4) + C^(1/(d+1))T^(d+2)/(d+3)). Theoretical lower bounds show these algorithms are optimal under strong adversaries. Experiments demonstrate superior performance compared to standard Zooming under various attacks and reward functions.

## Executive Summary
This paper addresses a critical gap in Lipschitz bandit literature by developing the first robust algorithms that can handle adversarial corruptions in continuous action spaces. The authors propose three algorithms: RMEL for weak adversaries with unknown corruption budgets, Robust Zooming for strong adversaries with known budgets, and BoB Robust Zooming for strong adversaries with unknown budgets. The key innovation is expanding confidence radii to explicitly account for adversarial corruption budgets, enabling sublinear regret guarantees even under strong adversaries. Experimental results demonstrate significant performance improvements over standard Zooming algorithms when faced with various attack strategies.

## Method Summary
The paper introduces three main algorithms to handle adversarial corruptions in Lipschitz bandits. Robust Zooming modifies the classic Zooming algorithm by enlarging confidence radii to account for known corruption budgets C, achieving regret O(T^(d_z+1)/(d_z+2) + C^(1/(d_z+1))T^(d_z)/(d_z+1)). RMEL handles unknown corruption budgets through a multi-layer parallel elimination framework with geometric tolerance scaling, achieving regret O((C^(1/(d_z+2)) + 1)T^(d_z+1)/(d_z+2)) under weak adversaries. BoB Robust Zooming adapts to unknown budgets through a hierarchical bandit model selection framework using EXP3.P to dynamically tune corruption budgets for base Robust Zooming algorithms, achieving regret O(T^(d+3)/(d+4) + C^(1/(d+1))T^(d+2)/(d+3)) under strong adversaries.

## Key Results
- RMEL achieves regret bound O((C^(1/(d_z+2)) + 1)T^(d_z+1)/(d_z+2)) under weak adversaries
- Robust Zooming achieves regret bound O(T^(d_z+1)/(d_z+2) + C^(1/(d_z+1))T^(d_z)/(d_z+1)) under strong adversaries with known budgets
- BoB Robust Zooming achieves regret bound O(T^(d+3)/(d+4) + C^(1/(d+1))T^(d+2)/(d+3)) under strong adversaries with unknown budgets
- Theoretical lower bounds show these algorithms are optimal under strong adversaries
- Experiments demonstrate superior performance compared to standard Zooming under various attacks and reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Robust Zooming algorithm achieves sublinear regret by expanding confidence radii to account for known corruption budgets
- Mechanism: Confidence radius for active arm x is set as sqrt(4 ln(T) + 2 ln(2/δ))/n(x) + C/n(x), where the second term explicitly accounts for adversarial corruption
- Core assumption: Corruption budget C is known to the agent
- Evidence anchors:
  - [abstract]: "When the budget C is given, a simple modification on the classic Zooming algorithm would lead to a robust method, namely, Robust Zooming algorithm, which could obtain a regret bound of order O(T^(d_z+1)/(d_z+2) + C^(1/(d_z+1))T^(d_z)/(d_z+1))"
  - [section 4]: "Our key idea is to enlarge the confidence radius of active arms to account for the known corruption budget C. Specifically, we could set the value of r(x) as: r(x) = sqrt(4 ln(T) + 2 ln(2/δ))/n(x) + C/n(x)"
- Break condition: When C is unknown, this mechanism fails as the algorithm cannot properly calibrate confidence intervals

### Mechanism 2
- Claim: RMEL algorithm handles unknown corruption budgets through multi-layer parallel elimination with geometric tolerance scaling
- Mechanism: Multiple parallel sub-layers run with increasing tolerance levels vl = ln(4T/δ)B^(l-1). Each layer adaptively discretizes and eliminates sub-optimal regions, with cross-layer communication preventing vulnerable layers from getting stuck
- Core assumption: Corruption budget C is unknown but finite
- Evidence anchors:
  - [abstract]: "Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption C is unrevealed to the agent"
  - [section 5.1]: "Our algorithm consists of multiple sub-layers running in parallel, each with a different tolerance level against corruptions"
- Break condition: If B is chosen too large, layers become too coarse and lose efficiency; if too small, too many layers create computational overhead

### Mechanism 3
- Claim: BoB Robust Zooming adapts to unknown budgets through hierarchical bandit model selection
- Mechanism: Top layer EXP3.P selects between base algorithms with different corruption tolerances, normalizing rewards by (2H + sqrt(2H log(12T/Hδ))) to handle varying corruption levels across batches
- Core assumption: Corruption budget C is unknown but bounded by T
- Evidence anchors:
  - [abstract]: "Inspired by the Bandit-over-Bandit (BoB) model selection idea [11, 13, 31], we design a two-layer framework adapting to the unknown C where a master algorithm in the top layer dynamically tunes the corruption budget for the Robust Zooming algorithm"
  - [section 5.2]: "We use ⌈log₂(T)⌉ Robust Zooming algorithms with different corruption levels shown above as base algorithms in the bottom layer and the classic EXP3.P [4] as the top layer"
- Break condition: When corruption budget is extremely large relative to T, the model selection overhead may dominate

## Foundational Learning

- Concept: Lipschitz continuity and metric spaces
  - Why needed here: The reward function's Lipschitz property enables generalization from sampled points to neighborhoods, which is fundamental to the zooming dimension concept and confidence ball construction
  - Quick check question: Why does Lipschitz continuity allow us to bound the difference between a pulled arm's reward and its neighbors' rewards?

- Concept: Zooming dimension vs covering dimension
  - Why needed here: The zooming dimension dz determines the regret scaling and captures the function's complexity near the optimum, which is typically much smaller than the covering dimension d
  - Quick check question: How does the zooming dimension differ from the covering dimension, and why is this distinction important for regret bounds?

- Concept: Adversarial corruption models (weak vs strong)
  - Why needed here: Different corruption models require different defense mechanisms - weak adversaries can be handled with adaptive elimination, while strong adversaries need more conservative approaches
  - Quick check question: What is the key difference between weak and strong adversaries in terms of information available before attacking?

## Architecture Onboarding

- Component map:
  - Core algorithms: Robust Zooming (known C), RMEL (unknown C, weak adversary), BoB Robust Zooming (unknown C, strong adversary)
  - Supporting components: Confidence radius calculation, active region management, cross-layer elimination, batch reward normalization
  - Key parameters: B (layer tolerance ratio), H (batch size), δ (failure probability)

- Critical path:
  1. Initialize algorithm based on known/unknown budget and adversary type
  2. For each round: sample layer (RMEL) or select base algorithm (BoB), pull arm, observe corrupted reward
  3. Update confidence estimates and perform elimination if thresholds met
  4. Aggregate regret and check stopping criteria

- Design tradeoffs:
  - Known vs unknown budget: Known budget allows tighter confidence intervals but requires prior knowledge; unknown budget needs model selection overhead
  - Weak vs strong adversary: Weak adversary allows more aggressive elimination; strong adversary requires more conservative confidence bounds
  - Computational vs theoretical efficiency: RMEL has better regret bounds but higher computational cost than BoB approach

- Failure signatures:
  - Linear regret growth: Indicates algorithm is not adapting to corruption (likely confidence radius miscalibration)
  - Premature elimination: Confidence bounds too loose, eliminating potentially good regions
  - Stagnant exploration: Confidence bounds too tight, not exploring enough to find optimal regions

- First 3 experiments:
  1. Test Robust Zooming with known C against varying corruption levels on simple 1D functions
  2. Compare RMEL and BoB Robust Zooming on unknown C scenarios with weak adversaries
  3. Stress test algorithms with strong adversaries at maximum corruption budget C=T

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret gap between the lower bound Ω(C) under weak adversaries and the upper bound Õ(C^(1/(dz+2))T^(dz+1)/(dz+2)) in Theorem 5.1 be closed or narrowed?
- Basis in paper: [explicit] The paper states "a compelling open problem is to narrow the regret gap by proposing an algorithm whose regret bound depends on C in another additive term free of T under the weak adversary"
- Why unresolved: The paper acknowledges this regret gap exists but does not provide a solution. The authors note that this gap "still exists under the simpler MAB setting" [16], suggesting it is a challenging theoretical problem.
- What evidence would resolve it: A new algorithm achieving a regret bound of the form Õ(T^(dz+1)/(dz+2) + C) under weak adversaries, or a proof that such a bound is impossible.

### Open Question 2
- Question: How would the proposed algorithms perform against more sophisticated adversarial attack strategies beyond the Oracle and Garcelon attacks?
- Basis in paper: [explicit] The paper states "adversarial attacks designed for stochastic Lipschitz bandits have never been studied" and only considers extensions of existing MAB and linear bandit attacks.
- Why unresolved: The paper only evaluates against two specific attack types. The effectiveness against other potential attack strategies remains unknown.
- What evidence would resolve it: Experimental results showing performance against a diverse set of novel attack strategies specifically designed for Lipschitz bandits.

### Open Question 3
- Question: Can the zooming dimension dz be estimated or learned online without prior knowledge?
- Basis in paper: [inferred] The paper mentions that "it is evident our proposed RMEL yields the most robust results under various scenarios" and notes that dz is "never revealed to the agent as it relies on the underlying function µ(·)"
- Why unresolved: All theoretical regret bounds depend on dz, but the paper assumes dz is known for algorithm design. No mechanism is proposed for learning dz online.
- What evidence would resolve it: A method for adaptively estimating dz during execution that achieves similar regret bounds without prior knowledge of dz.

## Limitations

- The theoretical analysis assumes perfect knowledge of the metric space structure and Lipschitz constant, which may not hold in practical applications where the reward function's properties are unknown or noisy
- The algorithms' computational complexity scales exponentially with the dimension d_z, limiting applicability to high-dimensional problems despite the sublinear regret guarantees
- The model selection approach in BoB Robust Zooming may introduce significant overhead when the corruption budget C is very large relative to T, potentially degrading practical performance

## Confidence

- High confidence: The regret bounds for Robust Zooming with known corruption budget C are well-supported by the theoretical analysis and align with standard techniques in the Lipschitz bandit literature
- Medium confidence: The RMEL algorithm's performance under weak adversaries is theoretically sound, but the cross-layer elimination mechanism introduces additional complexity that may affect practical implementation
- Medium confidence: The BoB Robust Zooming framework's adaptation to unknown budgets through hierarchical model selection is innovative, but the normalization techniques for batch rewards need careful calibration in practice

## Next Checks

1. Implement and test the algorithms on synthetic 2D and 3D reward functions with varying levels of smoothness to verify the theoretical dependence on zooming dimension d_z
2. Conduct ablation studies to quantify the impact of cross-layer communication in RMEL and model selection overhead in BoB Robust Zooming on both regret performance and computational efficiency
3. Evaluate algorithm sensitivity to misspecification of the Lipschitz constant and metric structure through controlled experiments with perturbed reward functions