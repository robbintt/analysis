---
ver: rpa2
title: 'Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense
  Encoders'
arxiv_id: '2311.09765'
source_url: https://arxiv.org/abs/2311.09765
tags:
- performance
- lora
- out-of-domain
- negatives
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of training strategies on dense
  retrieval model generalization, focusing on out-of-domain performance without additional
  resource-intensive steps. The authors compare LoRA (a parameter-efficient fine-tuning
  method) with full fine-tuning, and study batch design effects including hard negatives
  and in-batch negative sizes.
---

# Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders

## Quick Facts
- arXiv ID: 2311.09765
- Source URL: https://arxiv.org/abs/2311.09765
- Reference count: 33
- Key outcome: LoRA parameter-efficient fine-tuning improves out-of-domain generalization compared to full fine-tuning, while increasing in-batch negative size helps both in-domain and out-of-domain performance

## Executive Summary
This paper investigates how training strategies affect dense retrieval model generalization, particularly for out-of-domain performance. Through systematic experiments across multiple base models, architectures, and training datasets (MSMARCO and NQ), the authors find that LoRA fine-tuning consistently improves out-of-domain performance while full fine-tuning excels on in-domain tasks. The study also reveals that larger in-batch negative sizes benefit both scenarios, whereas mined hard negatives can hurt out-of-domain performance unless carefully selected. These findings offer a simple yet effective recipe for improving retrieval model generalization without requiring resource-intensive approaches like larger models or contrastive pretraining.

## Method Summary
The authors systematically compare LoRA parameter-efficient fine-tuning with full fine-tuning across different dense retrieval architectures (asymmetric, symmetric, and late interaction dual encoders) and base models. They train models on MSMARCO and NQ datasets, evaluating performance on the BEIR benchmark to assess out-of-domain generalization. The study examines batch design effects, including the impact of increasing in-batch negative sizes and adding mined hard negatives. Training runs for 40 epochs with early stopping, using learning rates of 2e-5 for full fine-tuning and 2e-4 for LoRA.

## Key Results
- LoRA consistently outperforms full fine-tuning on out-of-domain tasks, with 1.6 to 4.6 absolute points improvement in nDCG@10 across BEIR datasets
- Increasing in-batch negative size consistently improves both in-domain and out-of-domain performance
- Mined hard negatives can hurt out-of-domain performance unless carefully selected, with the negative impact varying by dataset similarity to MSMARCO
- Larger base models consistently improve performance, with LoRA benefiting more from model scaling than full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA parameter-efficient fine-tuning prevents overfitting to in-domain data distribution
- Mechanism: By freezing most base model parameters and only updating low-rank decomposition matrices, LoRA maintains the broader generalization capabilities learned during pretraining while still adapting to task-specific features
- Core assumption: The pretrained base model contains sufficient domain-general knowledge that should be preserved rather than overwritten
- Evidence anchors: The paper states "LoRA...leads to better out-of-domain generalization performance compared to full parameter tuning" and shows "LoRA consistently shows higher performance in out-of-domain over FT" with "average, retrieval models trained with LoRA outperform FT by 1.6 to 4.6 absolute points"

### Mechanism 2
- Claim: Increasing in-batch negative size improves both in-domain and out-of-domain performance by providing more diverse negative examples
- Mechanism: Larger batch sizes provide a wider variety of negative examples per training step, preventing the model from overfitting to specific negative patterns while maintaining consistent gradient signals
- Core assumption: Diverse negative examples are more beneficial than a smaller set of potentially biased hard negatives
- Evidence anchors: The paper demonstrates "increasing the number of in-batch negatives is consistently beneficial for out-of-domain performance" and shows "using larger batch sizes, which include a greater number of in-batch negatives, enhances performance both within and outside the domain"

### Mechanism 3
- Claim: Hard negatives from the same domain cause overfitting to that specific data distribution
- Mechanism: Mined hard negatives are often too similar to positives within the same domain, causing the model to overfit to domain-specific patterns rather than learning generalizable relevance features
- Core assumption: Out-of-domain datasets have different relevance patterns that don't align with domain-specific hard negatives
- Evidence anchors: The paper finds "mined hard negatives may hurt out-of-domain retrieval performance unless selected with great care" and shows "contrary to their well-established benefit in in-domain settings, mined hard negatives may hurt out-of-domain performance"

## Foundational Learning

- Concept: Contrastive learning in dense retrieval
  - Why needed here: The paper relies on understanding how negative examples shape the embedding space during training
  - Quick check question: What is the difference between in-batch negatives and mined hard negatives in contrastive learning?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: LoRA is the primary method being evaluated, and understanding how it differs from full fine-tuning is crucial
  - Quick check question: How does LoRA modify the weight update process compared to standard fine-tuning?

- Concept: Out-of-domain generalization in information retrieval
  - Why needed here: The paper's core contribution is improving generalization to unseen domains
  - Quick check question: Why might a model that performs well on MSMARCO not generalize well to other domains?

## Architecture Onboarding

- Component map: Pretrained base model (BERT/RoBERTa variants) -> Dense retriever architecture (asymmetric/symmetric/late interaction dual encoders) -> Fine-tuning strategy (LoRA vs full fine-tuning) -> Negative sampling strategy (in-batch vs mined hard negatives) -> Training dataset (MSMARCO or NQ) -> Evaluation datasets (BEIR benchmark)

- Critical path: Load pretrained base model -> Apply LoRA adapters or prepare for full fine-tuning -> Construct training batches with appropriate negative sampling -> Train for 40 epochs with early stopping -> Evaluate on BEIR benchmark

- Design tradeoffs:
  - LoRA vs full fine-tuning: Better generalization vs better in-domain performance
  - In-batch size vs hard negatives: More diverse negatives vs potentially stronger signals
  - Base model size: Better performance vs higher computational cost
  - Training dataset choice: NQ (smaller, cleaner) vs MSMARCO (larger, noisier)

- Failure signatures:
  - Overfitting to training domain: High in-domain performance but poor out-of-domain results
  - Underfitting: Poor performance across all domains
  - Memory issues: Unable to fit larger batch sizes or LoRA configurations

- First 3 experiments:
  1. Compare asymmetric dual encoder with LoRA vs full fine-tuning on MSMARCO
  2. Test different batch sizes (B, 2B, 4B) with LoRA on MSMARCO
  3. Evaluate the impact of adding BM25 negatives vs in-batch negatives with LoRA on MSMARCO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LoRA versus full fine-tuning change when training dense retrieval models on heterogeneous datasets versus a single homogeneous dataset?
- Basis in paper: The paper mentions investigating LoRA versus full fine-tuning on MSMARCO and NQ, but does not explore training on diverse combinations of domains rather than a single dataset.
- Why unresolved: The study focuses on training with single datasets (MSMARCO or NQ) and does not examine scenarios with mixed or multi-domain training data.
- What evidence would resolve it: Experiments training dense retrievers with LoRA and full fine-tuning on datasets containing examples from multiple domains, measuring out-of-domain performance on held-out domains not seen during training.

### Open Question 2
- Question: What is the relationship between base model size and the effectiveness of LoRA versus full fine-tuning for dense retrieval across different task types and dataset characteristics?
- Basis in paper: The paper discusses that larger base models consistently improve performance and that LoRA tends to benefit more from larger base models compared to full fine-tuning, but does not comprehensively characterize this relationship across diverse retrieval tasks.
- Why unresolved: While the paper provides some evidence about base model scaling effects, it does not systematically analyze how task complexity, dataset size, and domain similarity interact with model size to affect the LoRA versus fine-tuning tradeoff.
- What evidence would resolve it: A comprehensive study varying base model sizes (small to very large), task types (open-domain QA, fact verification, argument retrieval), and dataset characteristics (size, domain similarity to training data) while measuring the relative performance of LoRA and full fine-tuning.

### Open Question 3
- Question: How do different parameter-efficient fine-tuning methods (beyond LoRA) compare in terms of out-of-domain generalization for dense retrieval models?
- Basis in paper: The paper focuses specifically on LoRA as the parameter-efficient fine-tuning method and acknowledges that various PEFT methods exist but are not explored in this work.
- Why unresolved: The study deliberately limits its scope to LoRA, leaving open questions about whether other PEFT methods (e.g., prefix tuning, prompt tuning, adapters) might offer different tradeoffs between in-domain and out-of-domain performance.
- What evidence would resolve it: Direct comparisons of multiple PEFT methods (LoRA, prefix tuning, prompt tuning, adapters) on the same dense retrieval tasks, measuring both in-domain and out-of-domain performance to identify which methods provide the best generalization capabilities.

## Limitations
- Limited scope to two training datasets (MSMARCO and NQ) may not capture full diversity of real-world retrieval scenarios
- Focus on nDCG@10 evaluation metric may not fully capture all aspects of retrieval quality
- Results based on specific training configurations without thorough exploration of hyperparameter interactions

## Confidence
- **High confidence**: LoRA improves out-of-domain generalization compared to full fine-tuning, supported by consistent experimental results across multiple configurations
- **Medium confidence**: Increasing in-batch negative size improves both in-domain and out-of-domain performance, with strong empirical support but potential sensitivity to implementation details
- **Medium confidence**: Hard negatives can hurt out-of-domain performance, empirically supported but context-dependent on dataset similarity

## Next Checks
1. Test whether the LoRA advantage generalizes to non-English retrieval tasks by training on a multilingual dataset and evaluating on diverse language collections
2. Systematically evaluate different hard negative mining strategies across domains to identify which selection methods preserve benefits while minimizing risks
3. Investigate how the diversity of the base model's pretraining corpus affects LoRA's performance advantage by comparing models with different pretraining approaches under identical LoRA training conditions