---
ver: rpa2
title: Eliciting Latent Knowledge from Quirky Language Models
arxiv_id: '2312.01037'
source_url: https://arxiv.org/abs/2312.01037
tags:
- auroc
- examples
- knowledge
- layer
- alice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces 12 datasets and corresponding "quirky" language
  models finetuned to make systematic errors on math questions when the keyword "Bob"
  is present. The goal is to benchmark ELK methods that can extract the model's latent
  knowledge of correct answers despite untruthful outputs.
---

# Eliciting Latent Knowledge from Quirky Language Models

## Quick Facts
- arXiv ID: 2312.01037
- Source URL: https://arxiv.org/abs/2312.01037
- Reference count: 40
- Key outcome: Linear probes recover 89% of the gap between truthful and untruthful contexts in quirky language models

## Executive Summary
This paper benchmarks methods for eliciting latent knowledge from language models by creating synthetic "quirky" models that systematically make errors when the keyword "Bob" appears. The authors develop 12 datasets of math problems where Bob always gives wrong answers while Alice gives correct ones. They evaluate various linear probing methods and find that difference-in-means probes can recover most of the gap between truthful and untruthful contexts, achieving 81% AUROC. They also demonstrate that mechanistic anomaly detection can flag untruthful behavior with 0.95 AUROC.

## Method Summary
The authors create quirky language models using LoRA finetuning to make systematic errors on math problems when "Bob" is mentioned. They generate 12 datasets with controlled difficulty levels and persona conditions. Linear probes of various types (LDA, difference-in-means, logistic regression) are trained on residual stream activations from a single layer to predict whether Alice would label examples as true or false. The probes are evaluated on their ability to generalize from Alice's contexts to Bob's contexts, measuring how well they can recover the model's latent knowledge despite untruthful outputs.

## Key Results
- Difference-in-means probing achieves 81% AUROC, recovering most of the gap between truthful and untruthful contexts
- Logistic regression on contrast pairs achieves 89% AUROC, the best performance among all methods
- Mechanistic anomaly detection can flag untruthful behavior with 0.95 AUROC
- Probes generalize well from easy to hard examples and from truthful to untruthful contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The language model learns a context-independent representation of Alice and Bob's knowledge that can be probed separately from the model's output behavior.
- Mechanism: The model creates distinct neural representations for "correct knowledge" and "personas" during training. These representations persist across contexts because they are useful for pretraining and are not overwritten by the LoRA finetuning.
- Core assumption: The residual stream has sufficient capacity to maintain multiple knowledge representations without interference, and the finetuning process doesn't destroy these representations.
- Evidence anchors:
  - [abstract] "probes fit with both supervised and unsupervised loss functions generalize quite well from easy examples to hard examples, and from truthful examples to untruthful examples"
  - [section] "Probes trained to predict Alice's labels in her contexts continue to predict Alice's labels in Bob's contexts, unlike the LM output"
  - [corpus] Weak evidence - the corpus contains related work on mechanistic anomaly detection and hallucination mitigation, but no direct validation of this specific mechanism
- Break condition: If the residual stream capacity is exceeded or the finetuning process overwrites the base representations, the context-independent representations would disappear.

### Mechanism 2
- Claim: The difference-in-means probing method works well because it exploits the geometry of the activation space where truth representations are maximally separated from untruth representations.
- Mechanism: The difference-in-means direction captures the maximal linear separation between classes in the activation space, making it robust to distribution shifts and effective for causal interventions.
- Core assumption: The activation space has a simple linear structure where class separation is maximized along the difference-in-means direction.
- Evidence anchors:
  - [abstract] "the best probing method (difference-in-means) is promisingly close, with 81% AUROC"
  - [section] "We offer a theoretical explanation for these results by showing that interventions on the difference-in-means direction Î´ are worst-case optimal"
  - [corpus] No direct corpus evidence supporting this geometric claim
- Break condition: If the activation space becomes highly non-linear or if the classes overlap significantly in the direction of maximal separation.

### Mechanism 3
- Claim: Mechanistic anomaly detection can flag untruthful behavior because the probe outputs from truthful and untruthful contexts have different statistical properties.
- Mechanism: The Mahalanobis distance between probe outputs from Alice's and Bob's contexts creates a feature space where anomalous behavior is easily detectable.
- Core assumption: The statistical properties of probe outputs differ systematically between truthful and untruthful contexts.
- Evidence anchors:
  - [abstract] "mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC"
  - [section] "Simple anomaly detectors fit on probe outputs from Alice's easy distribution distinguish Bob's hard examples from Alice's hard examples well across models and probing methods"
  - [corpus] No direct corpus evidence supporting this specific statistical property claim
- Break condition: If the probe outputs from truthful and untruthful contexts have similar statistical distributions, or if the anomaly detector overfits to the training distribution.

## Foundational Learning

- Concept: Linear algebra and geometry of high-dimensional spaces
  - Why needed here: Understanding how linear probes extract information from activation spaces and why certain directions (like difference-in-means) are privileged
  - Quick check question: Why does the difference-in-means direction capture maximal class separation in a linear classifier?

- Concept: Statistical learning theory and generalization bounds
  - Why needed here: Understanding why probes trained on easy examples can generalize to hard examples and how distribution shifts affect performance
  - Quick check question: What conditions must hold for a probe trained on Alice's easy examples to perform well on Bob's hard examples?

- Concept: Anomaly detection and statistical hypothesis testing
  - Why needed here: Understanding how mechanistic anomaly detection works and why Mahalanobis distance is an effective metric
  - Quick check question: Under what conditions would a Gaussian anomaly detector fail to detect untruthful behavior?

## Architecture Onboarding

- Component map:
  Data pipeline -> Model training -> Probing methods -> Evaluation

- Critical path:
  1. Generate quirky math dataset with controlled difficulty and persona conditions
  2. Finetune models using LoRA to create systematic errors in Bob contexts
  3. Train probes on Alice's easy examples
  4. Evaluate probe generalization on Bob's hard examples
  5. Compare different probing methods and anomaly detection approaches

- Design tradeoffs:
  - LoRA rank vs. base model capacity: Higher rank allows more complex behavior but may overwrite base representations
  - Probe complexity vs. generalization: Simpler methods like difference-in-means often generalize better
  - Supervised vs. unsupervised: Supervised methods perform better but require labels; unsupervised methods are more practical but currently underperform

- Failure signatures:
  - Poor generalization from easy to hard examples suggests the probe is memorizing rather than learning robust features
  - Context-dependent generalization indicates the probe is reading the context rather than the knowledge representation
  - Low anomaly detection AUROC suggests the probe outputs have similar statistics in truthful and untruthful contexts

- First 3 experiments:
  1. Verify the quirky models actually make systematic errors only when "Bob" is present
  2. Test whether simple difference-in-means probes can recover the correct answer despite the model's errors
  3. Compare the generalization performance of different probing methods from Alice's easy examples to Bob's hard examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do ELK methods generalize to more diverse and challenging datasets beyond the simple addition problems used in this paper?
- Basis in paper: [explicit] The authors note that "we only experiment with one dataset" and "it would be highly informative to work in settings with more natural supervision" and "it is implausible that there exists a context-independent representation for all personas" in more complex tasks.
- Why unresolved: The paper only evaluates on a single, relatively simple dataset of math problems. The limitations of probing on contrast pairs and the need for more diverse datasets are explicitly acknowledged.
- What evidence would resolve it: Experiments applying the same ELK methods to a wide variety of datasets and tasks, including natural language, code, and multimodal data, with varying levels of complexity and supervision.

### Open Question 2
- Question: What is the "persona capacity" of the residual stream - how many distinct personas can be represented with context-independent knowledge representations?
- Basis in paper: [inferred] The authors hypothesize that there may be a limited number of personas that can be represented with context-independent knowledge due to the finite size of the residual stream.
- Why unresolved: The paper only finetunes models on two personas (Alice and Bob). The possibility of multiple personas and their relative frequencies is mentioned as an interesting direction for future work.
- What evidence would resolve it: Experiments varying the number of personas in the finetuning data and their relative frequencies, then evaluating how well probes can elicit each persona's knowledge independently of context.

### Open Question 3
- Question: What are the causal mechanisms underlying the context-independent knowledge representations found by the probes?
- Basis in paper: [inferred] The authors note that "characterize the causal mechanisms involved" is an important avenue for future work and mention the possibility of the "Chameleon hypothesis" where only truth or typical personas are represented across contexts.
- Why unresolved: The paper focuses on empirical evaluation of probe performance rather than investigating the underlying causal structure. The possibility of different causal mechanisms is acknowledged.
- What evidence would resolve it: Experiments intervening on the activations of different personas and observing the effects on model output, as well as analyzing the circuit structure and information flow in the models.

## Limitations
- Results based on synthetic setting with controlled errors that may not capture real-world deception complexity
- Performance gains from contrast-based methods require paired examples, which may be impractical
- Evaluation focuses on easy-to-hard transfer but doesn't extensively test out-of-distribution generalization

## Confidence
- High Confidence: The core finding that linear probes can recover a significant portion of the gap between truthful and untruthful contexts
- Medium Confidence: The superiority of difference-in-means probing and the theoretical explanation for why it works well
- Low Confidence: The effectiveness of the anomaly detection approach in real-world scenarios

## Next Checks
1. Stress Test with Real Hallucinations: Apply the best probing methods to detect and recover from actual model hallucinations on realistic tasks, not just synthetic "Bob" errors. Evaluate whether the methods still work when errors are less structured and more contextually dependent.

2. Probe Robustness to Distribution Shifts: Systematically evaluate probe generalization to out-of-distribution examples by testing on entirely new datasets, problem types, or contexts that were not seen during training or finetuning. Measure how quickly performance degrades.

3. Efficiency vs. Performance Trade-off: Benchmark the computational cost of the best performing methods (like CCS and CRC) against simpler approaches. Determine if the performance gains justify the additional complexity and data requirements in practical applications.