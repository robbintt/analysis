---
ver: rpa2
title: Extrinsically-Focused Evaluation of Omissions in Medical Summarization
arxiv_id: '2311.08303'
source_url: https://arxiv.org/abs/2311.08303
tags:
- facts
- fact
- summary
- medical
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MED-OMIT is a metric for evaluating omissions in medical summarization.
  It extracts facts from patient-provider conversations and detects which are omitted
  from generated summaries.
---

# Extrinsically-Focused Evaluation of Omissions in Medical Summarization

## Quick Facts
- arXiv ID: 2311.08303
- Source URL: https://arxiv.org/abs/2311.08303
- Reference count: 40
- Primary result: MED-OMIT correlates better with summarization quality than traditional metrics like ROUGE and BERTScore, especially for larger models

## Executive Summary
MED-OMIT is a novel metric for evaluating omissions in medical summarization by extracting facts from patient-provider conversations and detecting which are omitted from generated summaries. Each omitted fact is assigned an importance score based on its impact on differential diagnosis, with additional clustering to highlight unique supporting/refuting evidence. The approach is evaluated on the ACI-Bench dataset using GPT-4 and GPT-3.5-turbo, showing that larger models omit fewer and less critical facts. MED-OMIT demonstrates superior correlation with summarization quality compared to traditional metrics like ROUGE and BERTScore, particularly for larger models.

## Method Summary
MED-OMIT uses a multi-stage LLM-based pipeline to evaluate medical summaries. The process begins with fact extraction from patient-doctor dialogues, followed by detection of omitted facts in generated summaries. Each omitted fact is then categorized by its impact on differential diagnosis and clustered by pathophysiological mechanism to identify unique supporting or refuting evidence. The final document-level score combines weighted omission counts with extrinsic metrics based on the difference between DDx generated from the full chat versus the summary alone.

## Key Results
- Larger models (GPT-4) omit fewer and less critical facts compared to smaller models
- MED-OMIT correlates better with summarization quality than traditional metrics like ROUGE and BERTScore
- The metric effectively identifies unique diagnostic evidence through mechanism-based clustering
- Document-level scores reflect clinical relevance through DDx margin calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MED-OMIT uses the differential diagnosis (DDx) task as an extrinsic benchmark to weight fact importance in medical summaries.
- Mechanism: The pipeline generates a DDx from the full chat and separately from the summary, then uses an LLM to cluster and score each omitted fact by how it supports or refutes specific diagnoses. This directly ties omission importance to clinical impact.
- Core assumption: A fact's importance can be inferred from its role in altering the likelihood of diagnoses in a DDx simulation.
- Evidence anchors:
  - [abstract]: "We measure the importance of each fact to a simulated differential diagnosis"
  - [section]: "We propose a fact importance weight which quantifies the criticality of each omitted fact... by simulating the impact of each fact on a downstream clinical task: differential diagnosis (DDx) generation"
  - [corpus]: Weak - no direct corpus citation of DDx-based weighting, but concept is standard in clinical reasoning literature
- Break condition: If the DDx simulation is not clinically representative or omits rare but critical diagnoses, the weighting will be misleading.

### Mechanism 2
- Claim: By sub-clustering supporting facts by pathophysiological mechanism, MED-OMIT reduces redundancy and highlights unique diagnostic evidence.
- Mechanism: After grouping facts as supporting or refuting each diagnosis, the pipeline further clusters them by shared mechanism (e.g., inflammation, cardiac insufficiency). Facts in smaller clusters are scored higher for uniqueness, emphasizing non-redundant evidence.
- Core assumption: Facts sharing a mechanism are highly correlated, so uniqueness is inversely proportional to cluster size.
- Evidence anchors:
  - [abstract]: "further sub-cluster these by their underlying medical function (or pathophysiological mechanism)"
  - [section]: "we create additional sub-clusters for supportive and refuting clusters... This is designed to identify facts that are correlated because they are related to the same underlying issue"
  - [corpus]: Weak - no corpus support for mechanism-based uniqueness scoring, but standard in medical literature
- Break condition: If the mechanism clustering is inaccurate or too coarse, unique evidence may be undervalued or redundant facts overemphasized.

### Mechanism 3
- Claim: MED-OMIT correlates better with summarization quality for large models than traditional metrics because it targets omission errors directly.
- Mechanism: The weighted omission count and DDx margin scores are designed to reflect how well a summary preserves critical medical information. Larger models omit fewer critical facts, so their MED-OMIT scores are lower and correlate better with quality.
- Core assumption: Traditional metrics like ROUGE and BERTScore fail to capture nuanced omission errors, especially in high-performing models.
- Evidence anchors:
  - [abstract]: "MED-OMIT correlates better with summarization quality than traditional metrics like ROUGE and BERTScore, especially for larger models"
  - [section]: "we find that our approach reflects the summarization performance of LLMs as they increase in size... previously reported metrics do not correlate well with the number of omissions in the summary"
  - [corpus]: Moderate - references prior work (Goyal et al., 2022) showing traditional metrics struggle with larger LLMs
- Break condition: If omission weighting or detection is inaccurate, MED-OMIT will not correlate with true summarization quality.

## Foundational Learning

- Concept: Differential diagnosis (DDx) generation
  - Why needed here: MED-OMIT uses DDx as the clinical task to simulate the downstream impact of omitted facts, grounding the metric in realistic clinical decision-making.
  - Quick check question: What is the role of differential diagnosis in clinical reasoning, and why is it used here as an extrinsic task?

- Concept: Pathophysiological mechanisms
  - Why needed here: Sub-clustering facts by mechanism allows MED-OMIT to distinguish between correlated and unique diagnostic evidence, improving the accuracy of omission weighting.
  - Quick check question: How do pathophysiological mechanisms relate to medical symptoms and findings, and why does this matter for identifying unique facts?

- Concept: Intrinsic vs. extrinsic evaluation in NLP
  - Why needed here: MED-OMIT is framed as an intrinsic metric influenced by an extrinsic DDx task, blending both evaluation paradigms to better capture clinical relevance.
  - Quick check question: What is the difference between intrinsic and extrinsic evaluation, and how does MED-OMIT use both?

## Architecture Onboarding

- Component map: Chat → Summary → Fact extraction → Omission detection → Fact importance scoring → Document score
- Critical path: Chat → Summary → Fact extraction → Omission detection → Fact importance scoring → Document score
- Design tradeoffs:
  - Using LLMs for multiple stages increases cost but allows flexible, domain-specific reasoning.
  - Strict fact omission detection may penalize partial inclusion; softer matching could be more forgiving.
  - Uniqueness scoring via inverse cluster size assumes all facts in a cluster are equally correlated.
- Failure signatures:
  - High omission count but low weight: model omits many trivial facts.
  - Low omission count but high weight: model omits few but critical facts.
  - Inconsistent DDx margin: LLM-generated DDx is not clinically reliable.
  - Poor uniqueness clustering: mechanism identification is inaccurate.
- First 3 experiments:
  1. Vary fact extraction granularity (sentence-level vs. atomic facts) and measure impact on omission detection.
  2. Test different DDx generation prompts (fixed vs. variable number of diagnoses) and compare omission weighting.
  3. Compare uniqueness scoring methods (inverse frequency vs. manual ranking) on a held-out set of facts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MED-OMIT's effectiveness be validated across different medical specialties beyond general primary care?
- Basis in paper: [explicit] The authors state "we have not been able to find additional datasets to report on that are of reasonable quality" and suggest "we focus solely on omissions in this work but hope that future work can extend it to other potential summarization errors."
- Why unresolved: The evaluation is limited to a single dataset (ACI-Bench) focused on general primary care conversations, leaving generalizability to other specialties unknown.
- What evidence would resolve it: Testing MED-OMIT on datasets from specialized fields like oncology, cardiology, or psychiatry to compare performance metrics across domains.

### Open Question 2
- Question: How does MED-OMIT handle partially omitted facts where some but not all information is excluded from summaries?
- Basis in paper: [explicit] The authors note "we detect an omission on a binary scale - either some part of the fact is omitted or none of it is" and "An extension could include attempting to capture what percentage of the fact is omitted."
- Why unresolved: The current binary detection doesn't capture partial omissions, which could be clinically significant, and the authors explicitly call for future work on this.
- What evidence would resolve it: Developing and testing a graded omission detection system that quantifies the degree of information loss for partially omitted facts.

### Open Question 3
- Question: Can open-source LLMs achieve comparable performance to GPT-4 for MED-OMIT fact categorization and clustering?
- Basis in paper: [explicit] The authors state "Our metric pipeline performs best with a large LLM, such as gpt-4. This adds increased cost when evaluating summaries... we also look towards exploring fine-tuned models to accomplish the same goals with reduced cost."
- Why unresolved: The current implementation relies on GPT-4, which is proprietary and expensive, with the authors explicitly seeking alternatives.
- What evidence would resolve it: Benchmarking MED-OMIT's performance using fine-tuned open-source models like Llama-2, Mistral, or domain-specific medical LLMs and comparing results against GPT-4.

## Limitations

- The metric relies on LLM-generated DDx as a gold standard, which may introduce biases for rare or complex conditions
- Mechanism clustering assumes linear correlation between cluster size and redundancy, which may not capture nuanced medical relationships
- Limited evaluation to a single dataset (ACI-Bench) focused on general primary care conversations

## Confidence

This work introduces MED-OMIT as a novel approach to evaluate omissions in medical summarization, with three major claim clusters:

1. **DDx-based fact importance weighting (High confidence)**: The use of differential diagnosis as an extrinsic task to ground fact importance is well-motivated and directly supported by the abstract and method sections. The core assumption that clinical relevance can be inferred from DDx impact is reasonable given medical reasoning standards. However, confidence is tempered by the lack of corpus evidence showing DDx simulation reliably captures all clinically important omissions.

2. **Mechanism-based uniqueness clustering (Medium confidence)**: While the concept of clustering by pathophysiological mechanism is standard in medical literature, the paper provides only weak corpus support for this specific implementation. The assumption that cluster size inversely indicates uniqueness is intuitive but may not hold for complex medical cases where multiple mechanisms interact. The paper acknowledges this limitation by noting the need for accurate mechanism identification.

3. **Correlation with summarization quality (Medium confidence)**: The claim that MED-OMIT better reflects model performance than traditional metrics is supported by comparisons with ROUGE and BERTScore, particularly for larger models. However, this correlation is demonstrated only on the ACI-Bench dataset with specific LLM configurations. The moderate corpus support referencing prior work on traditional metrics' limitations provides some validation, but broader evaluation across diverse datasets would strengthen this claim.

## Next Checks

1. Evaluate MED-OMIT's performance on a held-out medical summarization dataset from a different clinical specialty to test domain generalizability
2. Compare MED-OMIT scores against clinician-annotated omission importance ratings to validate the DDx-based weighting approach
3. Test the robustness of mechanism clustering by introducing synthetic fact sets with known correlation structures and measuring clustering accuracy