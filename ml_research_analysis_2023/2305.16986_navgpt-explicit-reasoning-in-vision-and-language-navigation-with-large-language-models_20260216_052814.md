---
ver: rpa2
title: 'NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language
  Models'
arxiv_id: '2305.16986'
source_url: https://arxiv.org/abs/2305.16986
tags:
- right
- left
- navigation
- action
- viewpoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NavGPT is a fully LLM-based instruction-following navigation agent
  that performs zero-shot vision-and-language navigation by reasoning about visual
  observations, navigation history, and future explorable directions. At each step,
  it decomposes instructions into sub-goals, integrates commonsense knowledge, identifies
  landmarks, tracks progress, and adapts plans.
---

# NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models

## Quick Facts
- arXiv ID: 2305.16986
- Source URL: https://arxiv.org/abs/2305.16986
- Reference count: 40
- Primary result: Zero-shot VLN performance (SR 34%, SPL 29%) demonstrates LLMs' explicit reasoning capabilities despite lagging trained models

## Executive Summary
NavGPT is a fully LLM-based instruction-following navigation agent that performs zero-shot vision-and-language navigation by reasoning about visual observations, navigation history, and future explorable directions. At each step, it decomposes instructions into sub-goals, integrates commonsense knowledge, identifies landmarks, tracks progress, and adapts plans. While NavGPT's zero-shot performance on the R2R dataset still lags trained models, it demonstrates LLMs' strong explicit reasoning and high-level planning capabilities, suggesting future directions in multimodal LLM agents and knowledge-guided navigation systems.

## Method Summary
NavGPT processes visual observations through foundation models (BLIP-2 for captioning, object detectors) to generate natural language descriptions, which are combined with navigation history and instructions in prompts fed to GPT-4/GPT-3.5. The LLM generates reasoning traces (Thoughts) and navigation actions (Action) through explicit planning, including instruction decomposition, landmark identification, progress tracking, and plan adaptation. A history buffer stores observation-reasoning-action triples, with GPT-3.5 summarizing long histories to manage prompt length.

## Key Results
- Zero-shot SR 34% and SPL 29% on R2R validation unseen split
- Demonstrates explicit reasoning through instruction decomposition and landmark identification
- Generates navigational instructions and top-down trajectories from history
- Performance lags trained models due to visual description quality and history summarization limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NavGPT performs explicit reasoning by synergizing Thoughts and Actions in LLMs for step-by-step plan generation and adaptation
- Mechanism: Uses reasoning traces to break down navigation instructions into sub-goals, identify landmarks, track progress, and adjust plans when encountering exceptions
- Core assumption: LLMs can generate coherent, contextually appropriate reasoning traces without additional training
- Evidence anchors:
  - [abstract]: "At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target."
  - [section]: "Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment."
- Break condition: If LLMs cannot generate consistent, contextually relevant reasoning traces, explicit planning breaks down

### Mechanism 2
- Claim: Visual foundation models translate observations into natural language descriptions for LLM input
- Mechanism: BLIP-2 generates detailed captions of egocentric views, summarized by GPT-3.5 into concise descriptions with object detection and depth information
- Core assumption: Natural language descriptions are sufficient for LLMs to make accurate navigation decisions
- Evidence anchors:
  - [section]: "To translate visual observation into natural language, we first utilize the BLIP-2 [31] model as the translator. With the strong text generation capability of LLMs, BLIP-2 can achieve stunning zero-shot image-to-text generation quality."
  - [section]: "Limited by the quality of language description of visual scenes and the tracking abilities of objects, NavGPT's zero-shot performance on VLN is still not compatible with trained methods."
- Break condition: If descriptions lose critical visual information, LLM navigation decisions become inaccurate

### Mechanism 3
- Claim: NavGPT maintains and summarizes navigation history for progress tracking and decision making
- Mechanism: Stores observation-reasoning-action triples in history buffer, uses GPT-3.5 to summarize long histories into concise descriptions
- Core assumption: Summarized history retains enough information for progress tracking and informed decisions
- Evidence anchors:
  - [abstract]: "It also generates navigational instructions from observations and actions along a path, as well as drawing top-down trajectories from history."
  - [section]: "To handle the length of history, the prompt manager utilizes GPT-3.5 to summarize the observations from viewpoints in the trajectory, inserting the summarized observations into the observation, reasoning, and actions triples in the prompt."
- Break condition: If summarized history loses critical path information, LLM cannot accurately track progress or make informed decisions

## Foundational Learning

- Concept: Vision-and-Language Navigation (VLN) task formulation
  - Why needed here: Understanding VLN problem structure and constraints is crucial for grasping NavGPT's operation
  - Quick check question: What are the key components of the VLN problem formulation as described in the paper?

- Concept: Large Language Models (LLMs) and their capabilities
  - Why needed here: NavGPT leverages LLMs for reasoning and decision making, so understanding LLM capabilities and limitations is essential
  - Quick check question: What are the key capabilities of LLMs that NavGPT relies on for navigation?

- Concept: Multimodal input processing and integration
  - Why needed here: NavGPT processes visual observations, language instructions, and navigation history as multimodal inputs
  - Quick check question: How does NavGPT convert visual observations into natural language descriptions for LLM processing?

## Architecture Onboarding

- Component map: Visual Foundation Models (BLIP-2, object detectors) -> History Buffer -> GPT-3.5 Summarizer -> Prompt Manager -> LLM (Thoughts + Action) -> Environment interaction -> History update
- Critical path: Visual observation → BLIP-2 caption → GPT-3.5 summary → Prompt Manager → LLM (Thoughts + Action) → Environment interaction → History update
- Design tradeoffs: Trades detailed visual information for manageable prompt lengths through summarization, which can lead to information loss and impact performance
- Failure signatures: Inaccurate navigation decisions, inability to reach target locations, or failure to adapt to unexpected observations may indicate issues with visual description quality, history summarization, or LLM reasoning capabilities
- First 3 experiments:
  1. Test NavGPT with different visual observation granularities (varying Field of View and number of views) to assess impact on navigation performance
  2. Evaluate effectiveness of adding semantic scene understanding and depth estimation by comparing performance with and without these features
  3. Compare NavGPT's zero-shot performance with trained VLN models on R2R dataset to quantify performance gap and identify improvement areas

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal LLMs achieve zero-shot vision-and-language navigation performance comparable to trained models?
- Basis in paper: [explicit] The paper notes that NavGPT's zero-shot performance on R2R (SR 34%, SPL 29%) still lags trained models, and suggests that adapting LLMs with multi-modality inputs could improve navigation performance
- Why unresolved: The paper only uses text descriptions of visual observations rather than direct multimodal input, which likely contributes to performance gaps
- What evidence would resolve it: Comparative experiments testing a multimodal LLM (processing images directly) on R2R versus the text-based NavGPT approach, measuring standard VLN metrics like SR and SPL

### Open Question 2
- Question: How can the information loss from translating visual observations into natural language descriptions be minimized in LLM-based navigation systems?
- Basis in paper: [explicit] The paper identifies that "the bottleneck of NavGPT lies in the information loss while translating visual signals into natural language and summarizing observations into history"
- Why unresolved: The current approach relies on BLIP-2 for image captioning and GPT-3.5 for summarization, which inherently degrades visual information
- What evidence would resolve it: Experiments comparing different vision-to-language translation methods (more detailed captioning, object-level descriptions, or alternative summarization strategies) on VLN task performance

### Open Question 3
- Question: Can LLMs generate high-quality navigation instructions and metric trajectories in a zero-shot manner, and how does this capability scale with navigation history length?
- Basis in paper: [explicit] The paper demonstrates that GPT-4 can generate navigational instructions from observations and actions along a path, and draw accurate top-down metric trajectories given navigation history
- Why unresolved: While the paper shows promising qualitative results, it doesn't quantify how instruction quality or trajectory accuracy degrades with longer histories or more complex paths
- What evidence would resolve it: Systematic evaluation of instruction generation and trajectory drawing across varying path lengths and complexities, using metrics like BLEU for instructions and trajectory error measures

## Limitations

- Zero-shot performance (SR 34%, SPL 29%) remains substantially below trained models due to visual description quality and object tracking limitations
- History summarization mechanism may lose critical navigational context while managing prompt length
- Limited ablation studies to isolate contribution of explicit reasoning versus other architectural components

## Confidence

**High Confidence**: The architectural description of NavGPT and zero-shot performance metrics on R2R are clearly specified and verifiable.

**Medium Confidence**: The claim about LLMs' explicit reasoning capabilities for navigation planning, as evidence is primarily qualitative with limited quantitative validation.

**Low Confidence**: The assertion that NavGPT's reasoning mechanism is the primary driver of navigation capabilities rather than visual description quality or other factors, due to inadequate disentanglement of effects.

## Next Checks

1. **Ablation Study on Visual Description Quality**: Systematically vary quality and granularity of visual descriptions while keeping LLM reasoning constant to quantify performance dependence on visual translation versus reasoning capability.

2. **History Information Preservation Analysis**: Implement controlled experiments where critical historical information is either preserved or deliberately removed during summarization, measuring impact on navigation success to isolate effect of information loss.

3. **Reasoning Trace Quality Assessment**: Develop automated metrics to evaluate coherence, relevance, and task-appropriateness of NavGPT's reasoning traces, comparing them against ground truth reasoning or expert annotations to validate genuine reasoning capability.