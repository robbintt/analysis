---
ver: rpa2
title: Zero-Shot Question Answering over Financial Documents using Large Language
  Models
arxiv_id: '2311.14722'
source_url: https://arxiv.org/abs/2311.14722
tags:
- answer
- prompt
- question
- program
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel zero-shot prompting approach for
  complex financial question answering that requires multi-hop numerical reasoning.
  The method guides large language models to generate executable Python programs or
  domain-specific language code that are then executed by an external interpreter,
  avoiding arithmetic errors.
---

# Zero-Shot Question Answering over Financial Documents using Large Language Models

## Quick Facts
- arXiv ID: 2311.14722
- Source URL: https://arxiv.org/abs/2311.14722
- Reference count: 20
- Zero-shot prompts that guide LLMs to generate executable Python programs significantly improve financial QA accuracy

## Executive Summary
This paper introduces a novel zero-shot prompting approach for complex financial question answering that requires multi-hop numerical reasoning. The method guides large language models to generate executable Python programs or domain-specific language code that are then executed by an external interpreter, avoiding arithmetic errors. Experiments on three financial datasets show significant improvements over baseline zero-shot methods, with accuracy gains of 4.5% to 47% over simple dual prompting and 0% to 33.22% over zero-shot chain-of-thought.

## Method Summary
The approach uses carefully designed zero-shot prompts to guide LLMs (text-davinci-003, gpt-3.5-turbo, gpt-4) to generate executable Python programs (ZS-FinPYT) or domain-specific language programs (ZS-FinDSL). These generated programs are then executed by external interpreters to produce accurate numerical answers, avoiding the arithmetic errors common in LLMs. The prompts incorporate signifier, memetic proxy, constraining behavior, and meta prompting principles to direct the LLM's reasoning and ensure the generated programs follow the desired format.

## Key Results
- Accuracy improvements of 4.5% to 47% over simple dual prompting across three financial datasets
- Gains of 0% to 33.22% over zero-shot chain-of-thought methods
- Comparable results to few-shot methods, demonstrating the potential of zero-shot prompts to exploit LLM-embedded knowledge
- Successful handling of multi-hop numerical reasoning tasks in financial contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompts that guide LLM to generate executable Python programs or domain-specific language (DSL) code significantly improve accuracy in financial question answering tasks.
- Mechanism: By encoding reasoning into an executable program, the LLM avoids performing arithmetic calculations directly, thus eliminating arithmetic errors that are common in LLMs.
- Core assumption: LLMs can generate syntactically and semantically correct Python or DSL code when provided with carefully designed zero-shot prompts.
- Evidence anchors:
  - [abstract] "Our approach uses novel zero-shot prompts that guide the LLM to encode the required reasoning into a Python program or a domain specific language. The generated program is then executed by a program interpreter, thus mitigating the limitations of LLM in performing accurate arithmetic calculations."
  - [section] "These prompts are designed to generate executable programs for answering questions. The executable program generation and their execution enables accurate mathematical calculations which eliminates arithmetic errors."

### Mechanism 2
- Claim: Zero-shot prompts with signifier, memetic proxy, constraining behavior, and meta prompting principles are effective in guiding LLM reasoning for financial question answering.
- Mechanism: The prompt design principles direct the LLM's attention to task-relevant information and constrain its output to desired formats, improving the quality of generated programs.
- Core assumption: LLMs can interpret and follow high-level instructions and memetic proxy phrases to generate programs in the desired format.
- Evidence anchors:
  - [section] "Inspired by these prompt design principles, we present two zero-shot prompting techniques: ZS-FinPYT prompt that enables LLMs to generate Python executable programs and ZS-FinDSL prompt that enables LLMs to generate executable domain specific language programs."
  - [section] "The ZS-FinPYT prompt is designed to accommodate the above requirements in the following manner: Direct task specification using the signifier, Direct sub-task specification using the signifier, Constraining LLM behavior, Memetic proxy phrases."

### Mechanism 3
- Claim: The proposed zero-shot prompting approach achieves comparable results to few-shot methods, demonstrating the potential of carefully designed prompts to exploit the knowledge embedded in LLMs.
- Mechanism: By providing clear instructions and constraints, the zero-shot prompts enable the LLM to leverage its pre-existing knowledge to solve complex financial reasoning tasks without the need for task-specific training data.
- Core assumption: LLMs have sufficient domain-specific knowledge embedded in their parameters to solve financial reasoning tasks when provided with appropriate prompts.
- Evidence anchors:
  - [abstract] "The success of our approach demonstrates the enormous potential to extract complex domain specific numerical reasoning by designing zero-shot prompts to effectively exploit the knowledge embedded in LLMs."
  - [section] "Both methods also made significant improvements over the ZS-CoT method for text-davinci-003 and gpt-3.5-turbo, with the ZS-FinPYT achieving 3% to 33.22% and the ZS-FinDSL achieving 0% to 24.94% improvement over the ZS-CoT on different datasets."

## Foundational Learning

- Concept: Prompt engineering and design principles for guiding LLM reasoning.
  - Why needed here: The success of the proposed approach relies on carefully crafted zero-shot prompts that direct the LLM to generate executable programs and follow specific formatting rules.
  - Quick check question: Can you explain the difference between signifier, memetic proxy, constraining behavior, and meta prompting, and how each is used in the proposed approach?

- Concept: Domain-specific languages (DSLs) and their role in structured problem-solving.
  - Why needed here: The ZS-FinDSL approach uses a custom DSL to represent financial reasoning steps, enabling precise execution and error-free calculations.
  - Quick check question: What are the key components of the proposed DSL, and how does it differ from the DSL used in the FinQA dataset?

- Concept: Program execution and error handling in Python and custom interpreters.
  - Why needed here: The generated Python and DSL programs need to be executed accurately to produce the final answers, requiring a robust execution environment.
  - Quick check question: How does the proposed approach handle errors or exceptions that may occur during program execution, and what are the potential failure modes?

## Architecture Onboarding

- Component map:
  - LLM (text-davinci-003, gpt-3.5-turbo, gpt-4) -> Prompt templates -> Generated programs -> Python/DSL interpreter -> Evaluation module -> Final answer

- Critical path:
  1. Receive financial question and passage
  2. Apply zero-shot prompt to LLM
  3. LLM generates program (Python or DSL)
  4. Execute program using appropriate interpreter
  5. Compare generated answer with ground truth
  6. Return final answer and accuracy score

- Design tradeoffs:
  - Zero-shot vs. few-shot prompting: Zero-shot eliminates the need for task-specific examples but may require more careful prompt design.
  - Python vs. DSL: Python is more general-purpose but may be less constrained; DSL is more domain-specific but requires custom interpreter.
  - LLM choice: Different LLMs may require different prompt designs and yield varying accuracy levels.

- Failure signatures:
  - LLM generates invalid or incorrect programs
  - Program execution encounters errors or exceptions
  - Generated answer does not match ground truth
  - Prompt design fails to guide LLM reasoning effectively

- First 3 experiments:
  1. Evaluate the proposed approach on a subset of FinQA dataset using text-davinci-003 and compare with ZS-STD and ZS-CoT baselines.
  2. Analyze the generated programs for correctness and adherence to prompt instructions using a sample of test cases.
  3. Assess the impact of different prompt designs (e.g., varying signifier, memetic proxy, or constraining behavior) on the accuracy of generated programs and final answers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different zero-shot prompting techniques compare in terms of accuracy and robustness when applied to financial question answering?
- Basis in paper: [explicit] The paper evaluates three zero-shot prompting techniques (ZS-FinPYT, ZS-FinDSL, and ZS-STD) on three financial datasets using three different GPT models.
- Why unresolved: The paper provides a comparison of the techniques, but does not explore the reasons behind the differences in performance.
- What evidence would resolve it: Further analysis of the strengths and weaknesses of each technique, including an examination of the types of questions they handle well and poorly.

### Open Question 2
- Question: How does the performance of zero-shot prompting techniques compare to few-shot prompting techniques for financial question answering?
- Basis in paper: [explicit] The paper mentions that few-shot prompting has been used for financial question answering, but does not directly compare its performance to zero-shot prompting.
- Why unresolved: The paper does not provide a direct comparison of few-shot and zero-shot prompting techniques.
- What evidence would resolve it: Conducting experiments that compare the performance of few-shot and zero-shot prompting techniques on the same financial datasets.

### Open Question 3
- Question: How can zero-shot prompting techniques be improved to handle more complex financial reasoning tasks?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of zero-shot prompting techniques for financial question answering, but acknowledges that there is room for improvement.
- Why unresolved: The paper does not provide specific suggestions for improving zero-shot prompting techniques.
- What evidence would resolve it: Exploring different approaches to zero-shot prompting, such as incorporating domain-specific knowledge or using more advanced prompt engineering techniques.

## Limitations
- The approach's success depends on the LLM's ability to generate syntactically correct executable code, which may vary across different model architectures and domains.
- The current evaluation focuses primarily on accuracy metrics without extensive analysis of execution time or computational overhead introduced by the external interpreter.
- The method assumes well-formed financial documents and may face challenges with noisy or unstructured data common in real-world financial reports.

## Confidence

**High confidence**: The core mechanism of using executable programs to avoid LLM arithmetic errors is well-supported by experimental results showing consistent improvements over baseline methods. The accuracy gains of 4.5% to 47% over simple dual prompting and 0% to 33.22% over zero-shot chain-of-thought are substantial and reproducible across multiple datasets and LLM models.

**Medium confidence**: The effectiveness of the specific prompt design principles (signifier, memetic proxy, constraining behavior, meta prompting) in guiding LLM reasoning is demonstrated but not exhaustively validated. While the results show improvements, the relative contribution of each design principle to the overall success is not clearly isolated.

**Low confidence**: The generalizability of the approach to financial domains beyond those tested (FinQA, ConvFinQA, TATQA) and to other complex reasoning tasks requiring numerical calculations remains uncertain. The paper provides limited discussion of edge cases and failure scenarios.

## Next Checks

1. **Cross-domain validation**: Test the zero-shot prompting approach on financial question answering datasets from different sources or subdomains (e.g., banking, insurance, or market analysis) to evaluate robustness across financial contexts.

2. **Failure mode analysis**: Systematically analyze cases where the LLM generates invalid programs or produces incorrect answers to identify common failure patterns and develop mitigation strategies for prompt refinement.

3. **Computational overhead measurement**: Measure the end-to-end latency and resource utilization of the complete pipeline (prompt generation + code execution + answer validation) compared to direct LLM-based approaches to quantify practical deployment costs.