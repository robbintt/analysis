---
ver: rpa2
title: Explainable and Accurate Natural Language Understanding for Voice Assistants
  and Beyond
arxiv_id: '2309.14485'
source_url: https://arxiv.org/abs/2309.14485
tags:
- slot
- intent
- joint
- utterance
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of explainability in joint intent
  detection and slot filling models for voice assistants. The authors propose a method
  to make the full joint NLU model inherently explainable at granular levels without
  compromising accuracy.
---

# Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond

## Quick Facts
- arXiv ID: 2309.14485
- Source URL: https://arxiv.org/abs/2309.14485
- Reference count: 29
- Primary result: Joint NLU model made inherently explainable without accuracy loss using class-specific attention weights

## Executive Summary
This paper addresses the critical gap between high accuracy and explainability in joint intent detection and slot filling models for voice assistants. The authors propose a method that transforms the full joint NLU model into an inherently explainable system at granular levels without compromising accuracy. By introducing auxiliary networks that compute class-specific attention weights, the model can provide detailed explanations for both intent and slot predictions. The approach is evaluated on SNIPS and ATIS datasets, showing improved performance over state-of-the-art non-explainable baselines, and is demonstrated to generalize to other classification tasks like sentiment analysis and named entity recognition.

## Method Summary
The method introduces auxiliary networks for both intent and slot classification that compute class-specific attention weights. For intent classification, the auxiliary network applies self-attention to create class-specific representations, then computes query-based attention weights for each intent class. For slot classification, self-attention is applied per slot class to generate attention weights. These weights serve as explanations while being fused with original utterance representations through residual connections. The model is optimized using a multi-task loss combining intent loss, binary classification losses from auxiliary networks, and slot loss, maintaining accuracy while enabling explainability.

## Key Results
- Improved performance over state-of-the-art non-explainable baselines on SNIPS and ATIS datasets
- Maintained accuracy when integrating explainability into BERT-based models for sentiment analysis and named entity recognition
- Successful demonstration of explainability at granular levels for both intent and slot predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method enables inherent explainability by computing class-specific attention weights for both intent and slot classification.
- Mechanism: For each intent class, the model first applies self-attention to create class-specific representations of the utterance, then computes query-based attention weights specific to each class. For slots, the model similarly applies self-attention per slot class and uses the resulting weights as explanations. These class-specific attention weights capture the model's focus areas when making predictions.
- Core assumption: Attention weights computed from properly constrained self-attention and query-attention mechanisms can serve as meaningful explanations for classification decisions.
- Evidence anchors:
  - [abstract] states "We transform the full joint NLU model to be 'inherently' explainable at granular levels without compromising on accuracy."
  - [section 2] explains "We use properly computed attention weights to explain model decisions. Wiegreffe et al. [19] showed that attentions can be used for model explanation purposes since attention mechanisms capture important details of the model decisions."
- Break condition: If the attention weights are uniform across classes or fail to spike for positive classification classes, the explainability would be meaningless.

### Mechanism 2
- Claim: The model maintains accuracy while adding explainability by introducing auxiliary networks that learn fine-grained features without interfering with main classification.
- Mechanism: The model introduces two auxiliary networks - one for intent classification and one for slot classification. These networks compute class-specific attention weights and binary classification logits, which are then concatenated and linearly transformed to extract high-level patterns. These patterns are fused with the original utterance representation through residual connections and layer normalization, enhancing the main classifier without degrading performance.
- Core assumption: Adding auxiliary networks for fine-grained feature learning and explainability will not degrade the main classification accuracy if properly constrained with binary classification losses.
- Evidence anchors:
  - [abstract] states "we transform the full joint NLU model to be 'inherently' explainable at granular levels without compromising on accuracy."
  - [section 2] describes the optimization process: "Then our entire network optimization is performed using total loss L = λ Lintent + β LX + γ LY + η Lslot, where λ, β, γ, η are loss weights."
- Break condition: If the loss weights are not properly tuned, the auxiliary networks might dominate and degrade the main classification accuracy.

### Mechanism 3
- Claim: The explainability method generalizes to other classification tasks beyond joint NLU.
- Mechanism: The same auxiliary network architecture used for joint NLU (class-specific self-attention followed by query-attention for intents, and self-attention for slots) can be integrated into BERT-based models for other tasks like sentiment analysis and named entity recognition. The class-specific attention weights computed in these auxiliary networks serve as inherent explanations for these tasks as well.
- Core assumption: Intent detection resembles general classification (like sentiment analysis) and slot filling resembles sequence labeling (like NER), making the auxiliary network architecture transferable.
- Evidence anchors:
  - [section 2] states "As we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks."
  - [section 3] demonstrates results on SST2 (sentiment analysis) and conll2003 (NER) datasets, showing maintained performance with added explainability.
- Break condition: If the target task has a significantly different structure than NLU tasks (e.g., regression tasks), the explainability method may not generalize effectively.

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: The entire explainability approach relies on computing and interpreting attention weights to show what parts of the input the model focuses on for each class.
  - Quick check question: What is the difference between self-attention and query-based attention, and how are they used together in this method?

- Concept: Joint optimization of multiple tasks
  - Why needed here: The model optimizes intent detection and slot filling simultaneously, requiring understanding of how multiple losses interact and affect each other.
  - Quick check question: How does the multi-task loss function balance the different components (intent loss, binary classification losses, slot loss) to maintain accuracy while adding explainability?

- Concept: Sequence labeling with BIO notation
- Why needed here: Slot filling is framed as a sequence labeling problem where each token is classified into a slot type with BIO (Begin, Inside, Outside) prefixes.
- Quick check question: How does the slot classification process handle the BIO notation, and how are attention weights computed for each token-slot class pair?

## Architecture Onboarding

- Component map:
  BERT Encoder -> Intent Class Auxiliary Network -> Intent Classifier -> Intent logits
  BERT Encoder -> Slot Class Auxiliary Network -> Slot Classifier -> Slot logits

- Critical path:
  1. Input utterance → BERT Encoder → Token embeddings
  2. Token embeddings → Intent Class Auxiliary Network → Class-specific intent attention weights
  3. Token embeddings → Slot Class Auxiliary Network → Class-specific slot attention weights
  4. Intent attention weights + original embeddings → Intent Classifier → Intent logits
  5. Slot attention weights + original embeddings → Slot Classifier → Slot logits

- Design tradeoffs:
  - Adding auxiliary networks increases model complexity and computation but enables explainability
  - Using binary classification losses to constrain attention weights adds training stability but requires careful tuning of loss weights
  - Computing attention weights for every class increases memory usage but provides granular explanations

- Failure signatures:
  - Uniform attention weights across classes (indicates auxiliary networks not learning meaningful representations)
  - Significant accuracy drop after adding auxiliary networks (indicates poor loss weight tuning or interference between components)
  - Attention spikes on irrelevant tokens (indicates issues with self-attention or query-attention computation)

- First 3 experiments:
  1. Verify attention weights computation: Feed a simple utterance and check that class-specific attention weights are computed correctly and differ meaningfully between classes.
  2. Test explainability on joint NLU: Run the full model on SNIPS or ATIS datasets and visualize attention weights to confirm they highlight relevant parts of the utterance for each class.
  3. Evaluate generalization: Apply the explainability components to a BERT-based sentiment analysis or NER model and verify that attention weights provide meaningful explanations while maintaining baseline accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed model perform on datasets with significantly different characteristics than SNIPS and ATIS, such as those with more classes or longer utterances?
- Basis in paper: [explicit] The authors evaluate their model on SNIPS and ATIS datasets, showing improved performance over state-of-the-art baselines. However, they do not discuss performance on other types of datasets.
- Why unresolved: The paper does not provide any analysis or results for datasets with different characteristics, leaving the generalizability of the model uncertain.
- What evidence would resolve it: Results showing the model's performance on a variety of datasets with different characteristics, such as those with more classes or longer utterances.

### Open Question 2
- Question: What is the impact of varying the loss weights (λ, β, γ, η) on the model's performance and explainability?
- Basis in paper: [explicit] The authors mention using specific values for the loss weights in their experiments but do not explore the impact of varying these weights.
- Why unresolved: The paper does not provide any analysis or results for different combinations of loss weights, leaving the optimal configuration uncertain.
- What evidence would resolve it: Results showing the model's performance and explainability for different combinations of loss weights.

### Open Question 3
- Question: How does the model's explainability perform in real-world applications, where the input data may be noisier and less structured than in benchmark datasets?
- Basis in paper: [inferred] The authors demonstrate the model's explainability on benchmark datasets but do not discuss its performance in real-world scenarios.
- Why unresolved: The paper does not provide any analysis or results for real-world applications, leaving the practical utility of the model's explainability uncertain.
- What evidence would resolve it: Results showing the model's explainability performance on real-world data, such as user utterances from actual voice assistant interactions.

## Limitations

- The method's effectiveness heavily depends on proper tuning of loss weights and attention projection dimensions, which are not fully specified
- The claim that attention weights serve as meaningful explanations assumes that attention patterns correlate with model decision-making, though recent work has questioned the reliability of attention-based explanations
- The generalizability to other tasks beyond NLU and sentiment analysis/sequence labeling remains unproven, as the method's applicability to regression or multi-modal tasks is not explored

## Confidence

- **High Confidence**: The mechanism of using class-specific attention weights for explainability and the mathematical formulation of the auxiliary networks
- **Medium Confidence**: The empirical results showing accuracy improvements and maintained performance on additional tasks, given that specific hyperparameter values are not provided
- **Medium Confidence**: The claim of successful generalization to other classification tasks, as only sentiment analysis and NER are demonstrated

## Next Checks

1. Implement the model with a range of loss weight configurations to determine optimal settings and assess sensitivity to hyperparameter tuning
2. Conduct ablation studies removing the auxiliary networks to quantify the exact performance impact of adding explainability
3. Apply the method to a regression task or multi-modal classification task to test the claimed generalizability beyond the demonstrated domains