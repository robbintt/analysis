---
ver: rpa2
title: Answering Questions by Meta-Reasoning over Multiple Chains of Thought
arxiv_id: '2304.13007'
source_url: https://arxiv.org/abs/2304.13007
tags:
- answer
- reasoning
- chains
- question
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach called Multi-Chain Reasoning
  (MCR) for answering complex questions by meta-reasoning over multiple chains of
  thought. Unlike existing methods that simply aggregate final answers from multiple
  reasoning chains, MCR prompts a large language model to meta-reason on the intermediate
  steps across chains, combining relevant facts to generate a unified explanation
  and final answer.
---

# Answering Questions by Meta-Reasoning over Multiple Chains of Thought

## Quick Facts
- arXiv ID: 2304.13007
- Source URL: https://arxiv.org/abs/2304.13007
- Authors: 
- Reference count: 20
- Key outcome: MCR improves multi-hop QA accuracy by 2.4-5.7% over strong baselines by meta-reasoning over multiple reasoning chains

## Executive Summary
This paper introduces Multi-Chain Reasoning (MCR), a novel approach that improves question answering by meta-reasoning over multiple chains of thought. Unlike existing methods that aggregate only final answers from multiple reasoning chains, MCR examines intermediate reasoning steps across chains to combine relevant facts and generate unified explanations. The method achieves consistent improvements of 2.4-5.7% over strong baselines on seven multi-hop QA datasets, while also providing interpretable explanations that enable human verification.

## Method Summary
MCR consists of three main components: (1) generating multiple reasoning chains through decomposition into intermediate questions and retrieval of supporting evidence, (2) concatenating these chains into a multi-chain context, and (3) prompting a separate meta-reasoning model to produce the final answer along with an explanation. The system generates five reasoning chains total - one with greedy decoding and four with temperature sampling (t=0.7) to ensure diversity. For each intermediate question, the model retrieves evidence using Google Search and generates answers, which are then concatenated into a multi-chain context. The meta-reasoner processes this context along with the original question to produce the final answer and explanation.

## Key Results
- MCR achieves 2.4-5.7% improvement over strong baselines including self-consistency across seven multi-hop QA datasets
- The method generates high-quality explanations in over 82% of cases, enabling human verification of answers
- MCR successfully combines facts from multiple chains in 20-25% of examples, demonstrating its ability to synthesize information
- On datasets requiring multi-step reasoning, MCR shows particularly strong improvements compared to single-chain approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-reasoning over multiple chains outperforms simple majority voting by combining complementary facts
- Mechanism: The meta-reasoner processes intermediate steps from multiple reasoning chains as context, allowing it to identify and integrate relevant facts that individual chains might miss
- Core assumption: The LLM can effectively extract and synthesize relevant information from multiple reasoning chains when prompted appropriately
- Evidence anchors:
  - [abstract] "MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer"
  - [section] "By reasoning on multiple reasoning chains, MCR is able to mitigate the aforementioned drawbacks â€“ it combines facts from multiple chains to produce the correct final answer, with an explanation of the answer's validity"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: Using intermediate steps rather than just final answers improves accuracy and interpretability
- Mechanism: The meta-reasoner has access to the full reasoning process including intermediate questions and answers, not just the final outputs
- Core assumption: Intermediate reasoning steps contain valuable information that can be leveraged for better final answers
- Evidence anchors:
  - [abstract] "Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded"
  - [section] "Focusing exclusively on the final output discards relevant information that is present in the intermediate reasoning steps"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 3
- Claim: Sampling multiple chains with different temperatures provides diverse reasoning paths
- Mechanism: The system generates one greedy-decoded chain and additional chains with temperature t=0.7, creating diversity in reasoning approaches
- Core assumption: Different sampling temperatures produce meaningfully different reasoning chains that capture different aspects of the problem
- Evidence anchors:
  - [section] "We decode one chain with greedy decoding, and sample another four reasoning chains with temperature t = 0.7"
  - [section] "This enables the meta-reasoner to review different pieces of evidence when answering the full question"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: MCR builds on CoT by extending it to meta-reasoning over multiple chains
  - Quick check question: What is the key difference between standard CoT and MCR's approach?

- Concept: Self-consistency
  - Why needed here: MCR is compared against SC as a baseline, and understanding SC helps grasp MCR's improvements
  - Quick check question: How does SC's majority voting approach differ from MCR's meta-reasoning approach?

- Concept: Retrieval-augmented generation
  - Why needed here: The system uses retrieval to find evidence for intermediate questions, which is crucial for generating reasoning chains
  - Quick check question: Why does the system prepend retrieved evidence to the decomposition rather than interleaving it?

## Architecture Onboarding

- Component map:
  - Decomposition model -> Retriever -> Chain generator -> Multi-chain context aggregator -> Meta-reasoner

- Critical path:
  1. Generate multiple reasoning chains through decomposition and retrieval
  2. Concatenate intermediate steps into multi-chain context
  3. Feed context and original question to meta-reasoner
  4. Generate final answer with explanation

- Design tradeoffs:
  - More chains provide more information but increase context length
  - Using intermediate steps improves interpretability but adds complexity
  - Temperature sampling increases diversity but may introduce noise

- Failure signatures:
  - Low diversity between chains indicates temperature setting issues
  - Meta-reasoner ignoring most chains suggests context processing problems
  - High similarity between greedy and meta-reasoner output suggests insufficient meta-reasoning

- First 3 experiments:
  1. Compare MCR vs SCR (single-chain variant) to isolate multi-chain benefit
  2. Test different numbers of chains to find optimal trade-off
  3. Evaluate MCR-EV (using retrieved contexts) vs MCR (using QA pairs) to assess context format impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of MCR depend on the quality and diversity of the reasoning chains generated by the decomposition model?
- Basis in paper: Explicit - The paper mentions that MCR uses five reasoning chains, with one generated using greedy decoding and four sampled with temperature t=0.7. It also notes that MCR consistently outperforms baselines on all datasets.
- Why unresolved: The paper doesn't explore how MCR performance changes with different numbers of chains, different sampling strategies, or varying chain quality.
- What evidence would resolve it: Experiments varying the number of chains, sampling temperatures, and chain quality metrics while measuring MCR performance.

### Open Question 2
- Question: Can MCR be effectively applied to other complex reasoning tasks beyond multi-hop QA, such as mathematical problem solving or scientific reasoning?
- Basis in paper: Explicit - The paper focuses on multi-hop QA datasets but notes that MCR's approach of meta-reasoning over multiple chains could be applicable to other domains.
- Why unresolved: The paper only evaluates MCR on QA tasks and doesn't explore its potential for other reasoning domains.
- What evidence would resolve it: Applying MCR to benchmark datasets in mathematical reasoning, scientific reasoning, or logical inference tasks.

### Open Question 3
- Question: How does MCR's performance scale with increasing context length and number of reasoning chains?
- Basis in paper: Explicit - The paper mentions that MCR is bounded by context length when using more than five chains, and experiments with combining three MCR runs (15 total chains) against baselines using 15 chains.
- Why unresolved: The paper doesn't systematically explore the relationship between context length, number of chains, and performance.
- What evidence would resolve it: Experiments testing MCR with varying context lengths and numbers of chains, measuring performance and identifying optimal configurations.

## Limitations
- The approach is bounded by context length when using more than five chains, limiting scalability
- Effectiveness depends on the quality and diversity of reasoning chains generated by the decomposition model
- Performance relies on the assumption that LLMs can effectively synthesize information from multiple reasoning chains

## Confidence

**High confidence**: The general approach of using multiple reasoning chains with a meta-reasoner is technically sound and the performance improvements over baselines are well-documented

**Medium confidence**: The explanation quality and fact-combination capabilities, as these rely on subjective human evaluation metrics

**Low confidence**: The generality of the approach to domains beyond the tested multi-hop QA datasets, as no cross-domain validation is presented

## Next Checks

1. **Diversity analysis**: Systematically measure the diversity of reasoning chains produced under different temperature settings to verify that the claimed benefit of multiple chains comes from genuinely different reasoning paths rather than surface-level variations

2. **Failure case study**: Conduct detailed error analysis on questions where MCR fails to identify the key failure modes and whether they stem from decomposition errors, retrieval failures, or meta-reasoning limitations

3. **Ablation on chain count**: Experiment with varying numbers of chains (beyond the 5 used) to determine the optimal trade-off between diversity and context length, and to verify that the improvements scale with additional chains