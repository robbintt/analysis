---
ver: rpa2
title: 'CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning'
arxiv_id: '2303.10365'
source_url: https://arxiv.org/abs/2303.10365
tags:
- labels
- label
- learning
- selection
- selected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CroSel, a partial-label learning method that
  leverages historical prediction information from two deep models to identify true
  labels for most training examples. The core idea is a cross selection strategy where
  two models select confident pseudo labels for each other based on criteria involving
  confidence and consistency.
---

# CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning

## Quick Facts
- arXiv ID: 2303.10365
- Source URL: https://arxiv.org/abs/2303.10365
- Reference count: 32
- Key result: Achieves over 90% accuracy and selection ratio for identifying true labels on CIFAR-type datasets under various partial-label settings

## Executive Summary
This paper introduces CroSel, a novel method for partial-label learning that leverages historical prediction information from two deep models to identify true labels for most training examples. The approach uses a cross selection strategy where two models select confident pseudo labels for each other based on criteria involving confidence and consistency. Additionally, a co-mix consistency regularization term is introduced to avoid sample waste and noise from false selections. CroSel consistently outperforms state-of-the-art methods on benchmark datasets, achieving superior performance across multiple partial-label settings.

## Method Summary
CroSel addresses partial-label learning by training two identical deep models simultaneously with a cross selection strategy. Each model maintains a memory bank storing historical softmax outputs over multiple epochs. Using selection criteria based on confidence and consistency, each model identifies confident pseudo labels from its memory bank to use as supervision for the other model. The method also incorporates co-mix consistency regularization that generates trainable targets for unselected examples using weak and strong data augmentations with MixUp. A dynamic weighting scheme adjusts the regularization term based on the selection ratio, optimizing the balance between supervised and regularization losses throughout training.

## Key Results
- Achieves over 90% accuracy and selection ratio for identifying true labels on CIFAR-type datasets
- Consistently outperforms state-of-the-art partial-label learning methods across multiple benchmark datasets
- Demonstrates robustness across various partial-label settings with different noise levels (q ∈ {0.1, 0.3, 0.5} for CIFAR-10/SVHN)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross selection strategy enables two deep models to identify true labels for each other with high accuracy by leveraging historical prediction information.
- **Mechanism:** Two identical models train simultaneously on the same dataset. Each model maintains a memory bank storing historical softmax outputs over multiple epochs. For each example, if a model consistently predicts the same label with high confidence and low volatility, that label is selected as a pseudo-label for the other model to use as supervision.
- **Core assumption:** Models with different decision boundaries can adaptively correct most errors even when there is noise in the selected confident pseudo labels.
- **Evidence anchors:** [abstract] "the cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other"
- **Break condition:** If models converge to similar decision boundaries too quickly, they lose the ability to correct each other's errors.

### Mechanism 2
- **Claim:** Co-mix consistency regularization prevents sample waste and reduces noise from false selections by generating trainable targets for unselected examples.
- **Mechanism:** The method generates pseudo-labels using two data augmentation strategies (weak and strong). It applies MixUp to create interpolated examples with interpolated pseudo-labels.
- **Core assumption:** Consistency between predictions on different augmented versions of the same example indicates reliable training targets.
- **Evidence anchors:** [abstract] "a co-mix consistency regularization term is introduced to avoid sample waste and noise from false selections"
- **Break condition:** If data augmentation changes the feature space too significantly, the consistency regularization may not be effective.

### Mechanism 3
- **Claim:** Dynamic weighting of the consistency regularization term based on selection ratio optimizes the balance between supervised and regularization losses throughout training.
- **Mechanism:** The contribution of the consistency regularization term to the total loss is dynamically adjusted using λd = (1-rs)*λcr, where rs is the percentage of labeled data selected.
- **Core assumption:** The importance of consistency regularization should decrease as the proportion of accurately selected labels increases.
- **Evidence anchors:** [section] "we proposed a gradually decreasing λd with the increase of selected samples"
- **Break condition:** If the dynamic adjustment is too aggressive or too conservative, it may prevent proper convergence.

## Foundational Learning

- **Concept: Partial-label learning (PLL)**
  - Why needed here: The entire method is designed to solve the PLL problem where each training example has a candidate label set containing the true label
  - Quick check question: What distinguishes partial-label learning from multi-label learning?

- **Concept: Consistency regularization**
  - Why needed here: Provides a way to utilize unselected examples by enforcing consistency between predictions on augmented versions
  - Quick check question: How does consistency regularization differ from standard supervised learning?

- **Concept: MixUp data augmentation**
  - Why needed here: Used to create interpolated examples and pseudo-labels for the consistency regularization term
  - Quick check question: What is the mathematical formula for creating MixUp examples?

## Architecture Onboarding

- **Component map:** Two identical neural networks (Θ(1) and Θ(2)) -> Memory banks (MB(1) and MB(2)) -> Selection criteria modules -> Cross selection of pseudo-labels -> Training with combined loss -> Memory bank update

- **Critical path:** Memory bank update → Selection criteria evaluation → Cross selection of pseudo-labels → Training with combined loss → Memory bank update (loop)

- **Design tradeoffs:** Dual models increase computational cost but provide error correction benefits; stronger data augmentation may improve regularization but could reduce selection accuracy

- **Failure signatures:** Poor selection ratio indicates memory bank or selection criteria issues; low test accuracy with high selection ratio suggests problems with co-mix regularization or loss balancing

- **First 3 experiments:**
  1. Verify memory bank correctly stores and updates historical predictions
  2. Test selection criteria independently with synthetic data to confirm β1, β2, β3 logic
  3. Validate cross selection works by training with one model's selected labels and measuring performance on the other model's dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CroSel change when using different backbone architectures beyond WRN-34-10?
- Basis in paper: [explicit] The paper states they used WRN-34-10 as the backbone model for all experiments.
- Why unresolved: The paper only reports results using one specific backbone architecture, leaving uncertainty about generalizability to other architectures.

### Open Question 2
- Question: What is the computational overhead introduced by the cross selection strategy and co-mix regularization compared to baseline methods?
- Basis in paper: [inferred] The paper introduces two novel components (cross selection and co-mix regularization) that likely increase computational complexity, but no runtime analysis is provided.
- Why unresolved: While the paper demonstrates accuracy improvements, it does not quantify the additional computational cost or training time required by these new components.

### Open Question 3
- Question: How sensitive is CroSel's performance to the choice of data augmentation strategies beyond the weak/strong augmentation used in experiments?
- Basis in paper: [explicit] The paper mentions they used weak augmentation (RandomCrop, RandomHorizontalFlip) and strong augmentation (RandAugment) but does not explore alternative augmentation strategies.
- Why unresolved: The paper only tests one specific pair of augmentation strategies, leaving uncertainty about whether performance would change with different augmentation choices.

### Open Question 4
- Question: How does CroSel perform on real-world datasets with naturally occurring partial labels compared to artificially generated partial labels?
- Basis in paper: [inferred] All experiments use artificially generated partial labels by flipping negative labels with probability q, not naturally occurring partial labels from real-world scenarios.
- Why unresolved: The paper demonstrates effectiveness on benchmark datasets with controlled noise, but doesn't validate performance on datasets with authentic partial labeling issues.

## Limitations

- The cross-selection mechanism's contribution is not isolated through ablation studies
- No analysis of how often selection criteria fail or what types of examples are consistently misclassified
- Memory bank update strategy (FIFO vs. other schemes) is not explored

## Confidence

**High Confidence**: The general architecture of using dual models with memory banks and the mathematical formulation of the loss functions appear sound.

**Medium Confidence**: The empirical results showing performance improvements over baselines are convincing, but ablation studies are insufficient to attribute success specifically to the cross-selection mechanism versus the consistency regularization.

**Low Confidence**: The claim that the cross-selection strategy "enables two deep models to select true labels of partially labeled data for each other" with "only negligible noise" lacks rigorous statistical validation.

## Next Checks

1. **Ablation Study**: Run experiments with only cross-selection (no co-mix regularization) and only co-mix regularization (no cross-selection) to quantify the individual contributions of each component to the overall performance.

2. **Selection Reliability Analysis**: Conduct a detailed analysis of selection failures by examining cases where the selection criteria incorrectly identify false labels as true. Analyze the distribution of these errors across different classes and data characteristics.

3. **Memory Bank Update Strategy**: Test alternative memory bank update strategies (e.g., exponential moving average vs. FIFO) and analyze how different update frequencies (batch vs. epoch) affect selection accuracy and model convergence.