---
ver: rpa2
title: 'Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical
  Study'
arxiv_id: '2312.15156'
source_url: https://arxiv.org/abs/2312.15156
tags:
- keyphrase
- extraction
- song
- chatgpt
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) like
  ChatGPT as zero-shot keyphrase extractors. The authors compare ChatGPT with state-of-the-art
  unsupervised and supervised keyphrase extraction models on four datasets.
---

# Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical Study

## Quick Facts
- arXiv ID: 2312.15156
- Source URL: https://arxiv.org/abs/2312.15156
- Reference count: 16
- Key outcome: ChatGPT achieves similar performance to TF-IDF with simple prompts but falls short of supervised models in zero-shot keyphrase extraction

## Executive Summary
This paper explores the use of large language models like ChatGPT for zero-shot keyphrase extraction, comparing its performance against state-of-the-art unsupervised and supervised methods across four datasets. The authors find that while ChatGPT with simple prompts achieves comparable performance to the unsupervised method TF-IDF, it cannot match the results of supervised models. Interestingly, ChatGPT demonstrates better performance on longer documents compared to shorter ones, suggesting potential advantages for document understanding. The study highlights both the promise and limitations of using LLMs for keyphrase extraction tasks.

## Method Summary
The study employs a prompt-based approach using ChatGPT for zero-shot keyphrase extraction. Two simple prompts were tested: "Extract keywords from this text: [Document]" and "Extract keyphrases from this text: [Document]". Experiments were conducted on four keyphrase extraction datasets (INSPEC, DUC2001, SEMEVAL2010, OPEN KP) with performance evaluated using F1@5, F1@10, and F1@15 metrics. The extracted keyphrases were compared against ground truth labels using macro-averaged F1 scores, with Porter Stemmer used to remove identical stemmed keyphrases.

## Key Results
- ChatGPT achieves similar performance to TF-IDF with simple prompts
- Performance significantly lags behind state-of-the-art supervised models
- Better performance observed on longer documents (SEMEVAL2010) compared to shorter ones (INSPEC, DUC2001)

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot keyphrase extraction is possible using prompt engineering with LLMs because the LLM's pre-training provides general language understanding that allows it to identify important phrases when given appropriate prompts, without requiring task-specific training data.

### Mechanism 2
ChatGPT performs better on longer documents than shorter ones for keyphrase extraction because longer documents provide more context and repetition of key concepts, helping the LLM identify important phrases through distributional patterns and semantic relationships.

### Mechanism 3
Simple prompts can achieve performance comparable to sophisticated unsupervised methods because the LLM's general language understanding capabilities allow it to perform basic keyphrase extraction tasks with minimal prompt engineering, leveraging its pre-trained knowledge of language importance.

## Foundational Learning

- **Concept**: Zero-shot learning
  - Why needed here: Understanding how models can perform tasks without task-specific training data is crucial for evaluating the feasibility of prompt-based keyphrase extraction.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of NLP tasks?

- **Concept**: Prompt engineering
  - Why needed here: The quality and design of prompts significantly impacts the performance of LLMs on downstream tasks like keyphrase extraction.
  - Quick check question: How do different prompt formulations affect the output quality of language models on structured extraction tasks?

- **Concept**: Keyphrase extraction metrics
  - Why needed here: Understanding evaluation metrics like F1@5 and F1@M is essential for interpreting experimental results and comparing model performance.
  - Quick check question: What is the difference between macro-averaged F1 scores and micro-averaged F1 scores in multi-class classification tasks?

## Architecture Onboarding

- **Component map**: Document preprocessing -> Prompt generation -> LLM API call -> Output parsing -> Performance evaluation
- **Critical path**: 1. Document preprocessing and formatting 2. Prompt generation and optimization 3. LLM API call and response handling 4. Keyphrase extraction and filtering 5. Performance evaluation and comparison
- **Design tradeoffs**: Prompt complexity vs. performance (more sophisticated prompts may improve results but increase development effort)
- **Failure signatures**: Low F1 scores indicating poor keyphrase identification, inconsistent results across similar documents, high variance in performance across different domains
- **First 3 experiments**: 1. Compare simple prompt (Tp1) vs. more descriptive prompt (Tp2) to establish baseline performance differences 2. Test document length effects by evaluating on documents of varying lengths from the same dataset 3. Compare ChatGPT performance against unsupervised baselines (TF-IDF, TextRank) to establish relative effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
Can ChatGPT achieve state-of-the-art performance in keyphrase extraction with more complex and high-quality prompts? The authors suggest that designing more complex and high-quality prompts may improve ChatGPT's performance, but only simple prompts were tested in the current study.

### Open Question 2
How does ChatGPT's performance in keyphrase extraction compare to supervised fine-tuning methods? The authors mention that fine-tuning large models through human-annotated data may make performance superior to existing supervised models, but this was not explored in the current study.

### Open Question 3
Can incorporating contextual samples as auxiliary information improve ChatGPT's performance in keyphrase extraction? The authors suggest that constructing contextual samples as auxiliary information may optimize extraction results, but this approach was not tested.

## Limitations

- Limited prompt design with only two simple prompts tested, leaving potential for more sophisticated approaches unexplored
- Narrow scope of datasets may not capture full variability in document types and domains
- No investigation of supervised fine-tuning to establish whether performance gaps with supervised models can be closed

## Confidence

- **Low Confidence**: The claim that ChatGPT performs better on longer documents is based on limited experimental evidence across only four datasets
- **Medium Confidence**: The finding that simple prompts achieve performance comparable to TF-IDF is supported by the experimental results
- **Medium Confidence**: The observation that ChatGPT falls short of supervised models is well-supported by the performance comparisons

## Next Checks

1. Design and test a comprehensive prompt engineering framework that systematically varies prompt structure, examples, and instructions to identify optimal configurations for keyphrase extraction.

2. Conduct controlled experiments varying document length systematically across multiple datasets to establish the relationship between document length and keyphrase extraction performance.

3. Implement a simple supervised fine-tuning approach on the same datasets to establish whether the performance gap with supervised models can be narrowed through modest amounts of labeled data.