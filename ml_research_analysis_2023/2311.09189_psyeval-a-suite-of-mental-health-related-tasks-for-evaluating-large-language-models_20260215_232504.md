---
ver: rpa2
title: 'PsyEval: A Suite of Mental Health Related Tasks for Evaluating Large Language
  Models'
arxiv_id: '2311.09189'
source_url: https://arxiv.org/abs/2311.09189
tags:
- mental
- health
- llms
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces PsyEval, a new benchmark suite designed to
  evaluate large language models (LLMs) in mental health-related tasks. PsyEval comprises
  six sub-tasks across three dimensions: knowledge (mental health question-answering),
  diagnostic (predicting disorders from online text and dialogue), and therapeutic
  (therapeutic conversations, empathy, and safety understanding).'
---

# PsyEval: A Suite of Mental Health Related Tasks for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2311.09189
- Source URL: https://arxiv.org/abs/2311.09189
- Reference count: 9
- Primary result: Specialized mental health benchmark reveals LLMs struggle with diagnostic and therapeutic tasks despite strong knowledge performance

## Executive Summary
PsyEval is a new benchmark suite designed to evaluate large language models (LLMs) on mental health-related tasks across three dimensions: knowledge, diagnostic, and therapeutic. The benchmark comprises six sub-tasks using datasets such as MedQA, SMHD, D4, PsyQA, Esconv, and DialogueSafety. Twelve advanced LLMs including GPT-4 and GPT-3.5-turbo were evaluated using zero-shot, few-shot, and chain-of-thought prompting strategies. Results show that while GPT-4 excels at mental health knowledge tasks, all models struggle significantly with complex diagnostic tasks and understanding empathy and safety in therapeutic conversations, highlighting the need for specialized training and architectural improvements in LLMs for mental health applications.

## Method Summary
The paper evaluates 12 LLMs (GPT-4, GPT-3.5-turbo, LLaMa, LLaMa2, Alpaca, Vicuna, ChatGLM2, MOSS) across six sub-tasks using zero-shot, few-shot, and chain-of-thought prompting. The tasks include mental health question-answering (MedQA), disorder prediction from online text (SMHD, D4), and therapeutic conversations (Esconv, DialogueSafety). Performance is measured using accuracy, BLEU scores, and task-specific metrics. The benchmark design accounts for the unique challenges of mental health assessment, including subtle symptom expression, extended context requirements, and the need for empathy and safety awareness in therapeutic settings.

## Key Results
- GPT-4 achieved the highest performance on mental health knowledge tasks but still struggled with complex diagnostic scenarios
- All models showed significant difficulty predicting multiple mental disorders from social media posts, particularly for conditions like bipolar disorder and schizophrenia
- Limited context windows caused performance degradation on tasks involving extended sequences like doctor-patient dialogues
- Models exhibited poor understanding of empathy and safety in therapeutic conversation scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized benchmarking improves mental health LLM performance assessment
- Mechanism: Domain-specific tasks capture the nuanced nature of mental health symptoms and therapeutic interactions
- Core assumption: Mental health diagnosis requires specialized knowledge beyond general language understanding
- Evidence anchors: Abstract emphasizes distinct challenges in mental health evaluation; paper aims for nuanced assessment considering unique challenges

### Mechanism 2
- Claim: Context window size significantly impacts diagnostic performance
- Mechanism: Longer context windows enable processing of extended social media posts and therapeutic dialogues
- Core assumption: Mental health assessment requires analyzing multiple data points over time
- Evidence anchors: Paper notes instances exceeding 4k tokens and models with limited context windows struggling

### Mechanism 3
- Claim: Language-specific training is crucial for effective mental health assessment
- Mechanism: Models trained on diverse linguistic data specific to mental health contexts perform better
- Core assumption: Mental health communication has distinct linguistic features
- Evidence anchors: Experiments highlight significance of training on language-specific mental health data

## Foundational Learning

- **Chain-of-thought prompting**: Why needed: Demonstrates reasoning process for mental health assessments; Quick check: Why does showing reasoning steps help evaluate diagnosis performance more effectively than final answers alone?

- **Zero-shot prompting**: Why needed: Tests baseline mental health knowledge application without specific training; Quick check: How does zero-shot prompting differ from few-shot in evaluating baseline knowledge?

- **Few-shot learning**: Why needed: Enables learning from limited examples when labeled mental health data is scarce; Quick check: What are advantages of few-shot learning for empathy classification in therapeutic conversations?

## Architecture Onboarding

- **Component map**: PsyEval → 6 sub-tasks → 3 dimensions (knowledge, diagnostic, therapeutic) → Specific prompts → Evaluation metrics
- **Critical path**: Model evaluation → Task-specific prompting → Performance measurement → Analysis of limitations and improvements
- **Design tradeoffs**: Specialized vs. general benchmarks (domain-specific accuracy vs. broader applicability)
- **Failure signatures**: Poor multi-label diagnosis prediction, context window limitations, language-specific constraints
- **First 3 experiments**:
  1. Evaluate GPT-4 on mental health QA using zero-shot prompting
  2. Test models on diagnosis prediction from online text with chain-of-thought prompting
  3. Compare empathy understanding across models using few-shot prompting on therapeutic conversations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve LLM performance in predicting multiple mental disorders from online text data?
- Basis: All models struggled with bipolar disorder, schizophrenia, eating disorders, autism, and multiple disorders from social media posts
- Why unresolved: Paper identifies limitations but doesn't provide specific improvement strategies
- What evidence would resolve it: Experimental results showing improved performance after implementing specific strategies

### Open Question 2
- Question: What architectural improvements are needed for better empathy and safety understanding in therapeutic conversations?
- Basis: All models showed limited understanding of empathy and dialogue safety in counseling scenarios
- Why unresolved: Paper identifies limitations but doesn't recommend specific architectural improvements
- What evidence would resolve it: Results demonstrating improved understanding after implementing specific architectural changes

### Open Question 3
- Question: How can we address context window limitations for longer mental health-related textual inputs?
- Basis: Models struggled when inputs exceeded 4k tokens, common in mental health contexts like social media posts and dialogues
- Why unresolved: Paper emphasizes need for advancements but doesn't provide specific solutions
- What evidence would resolve it: Results showing improved performance after implementing context window solutions

## Limitations

- Benchmark relies on existing datasets whose quality and representativeness for mental health assessment is not fully validated
- Evaluation focuses on English and Chinese datasets, limiting generalizability to other languages and cultural contexts
- Does not address potential biases in datasets or how they might affect performance across different demographic groups
- Lacks comparison to human performance standards or clinical benchmarks for contextualizing "significant room for improvement"

## Confidence

- **High Confidence**: GPT-4's strong performance on knowledge tasks vs. all models' struggles with diagnostic and therapeutic tasks; context window size impact on extended sequences
- **Medium Confidence**: Language-specific training importance; specialized benchmarking value for mental health assessment
- **Low Confidence**: Claim that all models show significant room for improvement lacks comparison to human or clinical standards

## Next Checks

1. Replicate PsyEval experiments using independent mental health datasets not used in original benchmark development to test generalizability
2. Establish human performance benchmarks for the same mental health tasks to contextualize LLM performance against clinical standards
3. Conduct systematic bias analysis of model performance across different demographic groups using existing datasets