---
ver: rpa2
title: Learning Trajectories are Generalization Indicators
arxiv_id: '2304.12579'
source_url: https://arxiv.org/abs/2304.12579
tags:
- generalization
- learning
- training
- function
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how learning trajectories during training
  of deep neural networks (DNNs) relate to their generalization performance. The authors
  propose a new perspective by examining how each update step in the learning trajectory
  contributes to changes in generalization error, rather than just focusing on the
  final model.
---

# Learning Trajectories are Generalization Indicators

## Quick Facts
- arXiv ID: 2304.12579
- Source URL: https://arxiv.org/abs/2304.12579
- Reference count: 40
- This paper proposes a new generalization bound based on learning trajectory information and shows it effectively tracks generalization error trends.

## Executive Summary
This paper investigates the relationship between learning trajectories during training and generalization performance in deep neural networks. The authors introduce a Linear Approximation Function to model the learned model as a piecewise linear function of training steps, capturing trajectory information more effectively than previous methods. Based on this approximation, they derive a novel generalization bound that depends on trajectory complexity and the ratio of bias to diversity in the training set. Experiments demonstrate that this bound effectively tracks generalization error trends across different training iterations, learning rates, and label noise levels.

## Method Summary
The paper proposes a novel approach to analyzing generalization in deep neural networks by examining learning trajectories rather than just final model weights. They construct a Linear Approximation Function that models the learned model as a piecewise linear function of training steps, allowing them to capture more detailed trajectory information. Using this approximation, they derive a generalization bound that incorporates trajectory complexity through the additive linear space and measures training set diversity through gradient variance. The method relies on small learning rates to maintain the validity of the linear approximation assumption and requires certain conditions about gradient norms to hold throughout training.

## Key Results
- The proposed generalization bound effectively tracks actual generalization error trends across training iterations
- The bound captures the effect of learning rate adjustments on generalization performance
- Learning trajectory information provides valuable indicators of generalization capabilities beyond what traditional stability-based methods offer

## Why This Works (Mechanism)

### Mechanism 1
Learning trajectory information acts as a reliable predictor of generalization error in deep neural networks. The paper constructs a Linear Approximation Function that models the learned model as a piecewise linear function of training steps, capturing trajectory information more effectively than previous methods. The gap between the linear approximation function and the true function is small (O(ηm)) when the learning rate is appropriately small. This mechanism breaks down when the optimizer enters the Edge of Stability (EoS) regime.

### Mechanism 2
The generalization bound depends on the complexity of the learning trajectory and the ratio between bias and diversity of the training set. The bound incorporates trajectory information through the complexity of the additive linear space LJ|S and uses the variance of gradients across different samples to measure training set diversity. This mechanism requires that ∥∇Fµ(w)∥ ≤ γ∥∇FS(w)∥ for all weights w in the learning trajectory.

### Mechanism 3
Larger learning rates can lead to smaller generalization error by affecting the trajectory complexity. Higher learning rates push the model weights toward regions with lower Tr(C(w)) values (trace of covariance matrix), which reduces the complexity term in the generalization bound. This mechanism assumes the covariance matrix C(w) approximates the Hessian matrix of the loss function, and its trace affects the generalization bound.

## Foundational Learning

- **Concept**: Rademacher Complexity Theory
  - Why needed here: Used to calculate the complexity of the function space explored by the learning algorithm, which is essential for deriving the generalization bound
  - Quick check question: Why can't we directly apply Rademacher complexity to the entire neural network function space? (Answer: The complexity would be too large and yield a trivial bound.)

- **Concept**: Stability of Learning Algorithms
  - Why needed here: Provides the theoretical foundation for understanding how changes in training data affect the learned model, which is crucial for generalization analysis
  - Quick check question: How does uniform stability differ from the trajectory-based approach proposed in this paper? (Answer: Uniform stability only considers the final model weights, while the trajectory approach considers the entire learning path.)

- **Concept**: Information Theory and Mutual Information
  - Why needed here: Information-theoretic approaches provide an alternative framework for analyzing generalization, and the paper's trajectory approach can be seen as a more direct way to capture information flow during training
  - Quick check question: What role does the mutual information between weights and training data play in information-theoretic generalization bounds? (Answer: It measures how much information the learned weights reveal about the training data, with higher mutual information indicating worse generalization.)

## Architecture Onboarding

- **Component map**: Train model -> Extract trajectory information -> Compute linear approximation -> Calculate generalization bound -> Evaluate correlation with actual generalization error
- **Critical path**: Train model → Extract trajectory information → Compute linear approximation → Calculate generalization bound → Evaluate correlation with actual generalization error
- **Design tradeoffs**: Using linear approximation simplifies trajectory analysis but requires small learning rates; the bound captures more trajectory information than stability-based methods but has stricter assumptions; experiments validate the approach but only show correlation, not causation
- **Failure signatures**: If the optimizer enters EoS regime, the linear approximation assumption fails; if training set diversity is very low, the bound becomes trivial; if Assumption 3.6 is violated (gradient norms diverge), the bound is no longer valid
- **First 3 experiments**: 1) Train a simple model (e.g., VGG13 on CIFAR-10) and plot the trajectory of ∥∇FS(Jt)∥ vs. t to verify Assumption 3.6 holds; 2) Compare the proposed bound C(Jt) with actual generalization error (FS'(Jt) - FS(Jt)) across different training steps; 3) Vary the learning rate and observe how both the bound and generalization error change, verifying the relationship described in Mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
How does the Linear Approximation Function perform when the learning rate is not small and the optimizer enters the Edge of Stability (EoS) regime? The paper does not provide experimental results or theoretical analysis for the case when the learning rate is large and the optimizer enters the EoS regime. Experimental results showing the performance of the Linear Approximation Function when the learning rate is large and the optimizer enters the EoS regime, or theoretical analysis proving the function's effectiveness or limitations in such scenarios, would resolve this question.

### Open Question 2
How does the proposed generalization bound perform under extremely overfitting situations, and how can the relaxed theorem in Appendix A.3 be applied? The paper mentions that the proposed generalization bound relies on Assumption 3.6, which may not hold under extremely overfitting situations. It refers to a relaxed theorem in Appendix A.3 for such cases but does not provide experimental results or detailed analysis. Experimental results showing the performance of the proposed generalization bound under extremely overfitting situations, or theoretical analysis explaining how the relaxed theorem in Appendix A.3 can be applied and its effectiveness, would resolve this question.

### Open Question 3
How does the complexity of the learning trajectory correlate with the generalization error in different optimization algorithms, such as Adam or AdaGrad, and how does it compare to SGD? The paper focuses on the correlation between the complexity of the learning trajectory and the generalization error in SGD and gradient descent. It does not explore other optimization algorithms. Experimental results showing the correlation between the complexity of the learning trajectory and the generalization error in different optimization algorithms, such as Adam or AdaGrad, and a comparison with the results obtained for SGD, would resolve this question.

## Limitations

- The linear approximation assumption breaks down when training enters the Edge of Stability (EoS) regime, making the generalization bound trivial in these cases
- Assumption 3.6 (∥∇Fµ(w)∥ ≤ γ∥∇FS(w)∥) is critical for the bound but is only validated for early training stages
- The approach relies on small learning rates to maintain the linear approximation quality, limiting its applicability to common large learning rate training regimes

## Confidence

- **High Confidence**: The correlation between the proposed bound and generalization error trends across different training iterations is empirically validated in the experiments
- **Medium Confidence**: The mechanism by which learning rates affect trajectory complexity and generalization is theoretically sound but lacks comprehensive experimental validation
- **Low Confidence**: The assumption that the linear approximation gap remains small throughout training is supported by limited evidence and may not hold in practice

## Next Checks

1. **EoS Regime Testing**: Design experiments to systematically test when and how often training enters the Edge of Stability regime, and measure the impact on the proposed bound's validity
2. **Assumption 3.6 Verification**: Conduct comprehensive experiments tracking ∥∇Fµ(w)∥/∥∇FS(w)∥ throughout the entire training trajectory to validate the assumption's global applicability
3. **Large Learning Rate Experiments**: Test the proposed approach with larger learning rates (beyond the small learning rate assumption) to identify practical limitations and potential modifications needed for broader applicability