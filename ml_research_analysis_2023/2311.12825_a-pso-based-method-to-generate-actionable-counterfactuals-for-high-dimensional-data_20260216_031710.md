---
ver: rpa2
title: A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional
  Data
arxiv_id: '2311.12825'
source_url: https://arxiv.org/abs/2311.12825
tags:
- data
- generation
- number
- proximity
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a PSO based method to generate counterfactual
  explanations for high-dimensional data. The core idea is to formulate the counterfactual
  generation problem as a multi-objective optimization using MD-PSO.
---

# A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional Data

## Quick Facts
- arXiv ID: 2311.12825
- Source URL: https://arxiv.org/abs/2311.12825
- Reference count: 27
- Key outcome: PSO-based method generates counterfactuals with superior proximity, sparsity, and diversity on high-dimensional datasets.

## Executive Summary
This paper introduces a particle swarm optimization (PSO) based method for generating actionable counterfactual explanations for high-dimensional data. The method formulates counterfactual generation as a multi-objective optimization problem, balancing validity, proximity, sparsity, and diversity. By incorporating features like multiple counterfactual generation, box constraints, and categorical variable handling, the proposed MD-PSO algorithm demonstrates improved performance compared to state-of-the-art methods on real-world datasets.

## Method Summary
The method uses Mixed Discrete PSO (MD-PSO) to optimize counterfactual generation as a multi-objective problem. It employs Information Value (IV) based feature selection to control sparsity by limiting mutable attributes to an "active set." Box constraints encoded as [z(1-ε), z(1+ε)] bound proximity by restricting feature value deviations. The PSO includes a diversity preservation term in particle velocity updates to enhance counterfactual diversity. The algorithm generates multiple counterfactuals via k-means clustering on converged particles.

## Key Results
- Proposed MD-PSO method achieves superior results in proximity, sparsity, and diversity metrics compared to state-of-the-art methods.
- Experiments on real-world high-dimensional datasets (COMPAS, Hill-valley, Arrhythmia, LSVT) demonstrate the method's effectiveness.
- The method successfully balances validity, proximity, sparsity, and diversity in generated counterfactuals.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MD-PSO optimization balances proximity, sparsity, and diversity by embedding diversity preservation into particle velocity updates.
- Mechanism: The fourth term in the MD-PSO velocity update, c3r3V̂t, explicitly injects diversity by measuring divergence from the population mean, counteracting particle convergence to local minima.
- Core assumption: Particle diversity translates into counterfactual diversity; more dispersed particles yield more diverse counterfactuals.
- Evidence anchors:
  - [abstract] "The PSO brings in a lot of flexibility in terms of carrying out multi-objective optimization in large dimensions, capability for multiple CF generation, and setting box constraints or immutability of data attributes."
  - [section] "In an ideal PSO convergence scenario, all the particles are supposed to share the position around a small region... The fourth term introduced in MDPSO provides an option to enhance diversity."
  - [corpus] Weak; no corpus paper directly validates this diversity term's effect on counterfactuals.
- Break condition: If the diversity preservation term is too strong, particles may fail to converge, yielding suboptimal proximity and sparsity scores.

### Mechanism 2
- Claim: Information Value (IV)-based feature selection controls sparsity by limiting mutable attributes.
- Mechanism: The algorithm ranks features by IV, selects the top h as the "active set," and constrains PSO to vary only these features, ensuring minimal feature changes.
- Core assumption: High-IV features are the most influential for class change, so restricting to them yields sparse, actionable counterfactuals.
- Evidence anchors:
  - [section] "The idea is to limit the number of features that can change and this feature subset given the name active set (A), is selected based on the information value (IV) statistic."
  - [appendix] "An IV value > 0.5 indicates that the corresponding feature is an extremely strong predictor."
  - [corpus] Weak; corpus does not discuss IV-based feature selection for counterfactuals.
- Break condition: If IV misidentifies important features (e.g., due to class imbalance), sparsity control may fail and counterfactuals may require more changes than intended.

### Mechanism 3
- Claim: Box constraints encoded as [z(1-ε), z(1+ε)] bound proximity by restricting feature value deviations from the query point.
- Mechanism: During PSO initialization and velocity updates, each feature in the active set is constrained to vary within ε-scaled bounds around its original value, preventing large jumps.
- Core assumption: Proximity is best ensured by hard limits on feature deviations rather than soft penalties in the objective.
- Evidence anchors:
  - [section] "The constraints are enabled in the form of a hyper-parameter, ε, that characterizes the range features can vary and it enables control over proximity."
  - [section] "If the interval bounds falls outside [0,1] they will be capped to the extremes of 0 or 1."
  - [corpus] Weak; corpus does not provide evidence for proximity control via box constraints.
- Break condition: If ε is too small, PSO may be unable to find valid counterfactuals; if too large, proximity control is lost.

## Foundational Learning

- Concept: Particle Swarm Optimization (PSO)
  - Why needed here: PSO provides a population-based search that naturally supports multi-objective optimization, multiple counterfactual generation, and constraint handling.
  - Quick check question: How does PSO balance exploration and exploitation in high-dimensional spaces?

- Concept: Multi-objective optimization
  - Why needed here: Counterfactual generation requires simultaneously optimizing validity, proximity, sparsity, and diversity, which are often conflicting objectives.
  - Quick check question: What is the role of the Pareto front in selecting counterfactuals from multiple objectives?

- Concept: Information Value (IV) statistic
  - Why needed here: IV quantifies feature importance for class prediction, enabling selective feature mutability to control sparsity.
  - Quick check question: How does IV differ from mutual information in the context of binary classification?

## Architecture Onboarding

- Component map:
  Data preprocessing -> IV-based feature ranking -> Active set selection -> MD-PSO optimizer with diversity preservation term -> Constraint handler (box constraints for proximity, immutability flags) -> Counterfactual selection module (clustering + metric filtering) -> Evaluation pipeline (proximity, sparsity, diversity, coverage metrics)

- Critical path:
  1. Compute IV for all features; select top-h active set.
  2. Initialize PSO particles within ε-scaled bounds.
  3. Run MD-PSO iterations with diversity term.
  4. Cluster converged particles; select k counterfactuals.
  5. Validate and compute evaluation metrics.

- Design tradeoffs:
  - PSO swarm size vs. runtime: larger swarms improve coverage but increase cost.
  - ε magnitude vs. actionability: smaller ε yields more realistic changes but may miss valid counterfactuals.
  - IV threshold for active set vs. sparsity: stricter thresholds yield sparser counterfactuals but may reduce validity.

- Failure signatures:
  - All particles converge to same region → diversity loss → high normalized diversity score drops.
  - PSO fails to find valid counterfactuals within ε bounds → zero coverage.
  - High sparsity but low proximity → counterfactuals require many changes but are far from query point.

- First 3 experiments:
  1. Run MD-PSO on COMPAS with h=5, ε=0.1; verify proximity < 0.3, sparsity < 0.2.
  2. Vary ε from 0.05 to 0.2; plot coverage vs. proximity tradeoff.
  3. Compare IV-based active set (top-5) vs. random feature selection; measure impact on sparsity and validity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed MD-PSO method scale with increasing dimensionality of the data beyond the datasets tested in the paper?
- Basis in paper: [inferred] The paper states that "The PSO provides default settings to make faster optimization in terms of runtime" and that the method is "suitable for high dimensional data." However, the experiments only test on datasets with up to 304 features.
- Why unresolved: The paper does not provide experiments or analysis on datasets with significantly higher dimensionality than those tested. The scalability of the method to extremely high-dimensional data remains an open question.
- What evidence would resolve it: Experiments comparing the performance of the proposed method on datasets with a wide range of dimensions, including very high-dimensional data (e.g., thousands of features), would provide evidence of its scalability.

### Open Question 2
- Question: How does the proposed MD-PSO method compare to other counterfactual generation methods in terms of computational efficiency and scalability?
- Basis in paper: [inferred] The paper mentions that "The PSO provides default parallel computing capabilities for optimization in large dimensions" and that it is "suitable for high dimensional data." However, no direct comparison of computational efficiency with other methods is provided.
- Why unresolved: The paper does not include a comprehensive comparison of the computational efficiency of the proposed method against other state-of-the-art counterfactual generation methods. The relative efficiency and scalability of the method remain unclear.
- What evidence would resolve it: A detailed comparison of the computational time and resource requirements of the proposed MD-PSO method against other methods on datasets of varying sizes and dimensions would provide evidence of its efficiency and scalability.

### Open Question 3
- Question: How sensitive is the proposed MD-PSO method to the choice of hyperparameters, such as the number of particles, iterations, and box constraint parameter?
- Basis in paper: [explicit] The paper states that "The parameters ci, i = {0, 1, 2, 3} and ri, i = {1, 2, 3} are fixed by cross-validation." However, it does not provide a detailed analysis of the sensitivity of the method to these hyperparameters.
- Why unresolved: The paper does not investigate how the performance of the proposed method varies with different choices of hyperparameters. The robustness of the method to hyperparameter settings remains an open question.
- What evidence would resolve it: A sensitivity analysis of the proposed method's performance to different choices of hyperparameters, such as the number of particles, iterations, and box constraint parameter, would provide evidence of its robustness to hyperparameter settings.

## Limitations
- Limited experimental validation on high-dimensional datasets; only four datasets tested, with mixed results compared to baselines.
- No statistical significance testing reported for metric differences between methods.
- Hyperparameter sensitivity (PSO parameters, IV threshold, ε) not thoroughly explored across datasets.

## Confidence
- High confidence: PSO's capability to handle multi-objective optimization and constraints in high-dimensional spaces.
- Medium confidence: The IV-based active set selection effectively controls sparsity while maintaining validity.
- Low confidence: The diversity preservation term significantly improves counterfactual diversity without compromising proximity or sparsity.

## Next Checks
1. **Statistical validation**: Perform paired t-tests or Wilcoxon signed-rank tests on all evaluation metrics across all datasets to confirm significance of improvements over baselines.
2. **Hyperparameter robustness**: Conduct grid search over PSO parameters (c0, c1, c2, c3) and IV thresholds (h) to identify optimal configurations and assess stability.
3. **Diversity mechanism ablation**: Compare MD-PSO with and without the diversity preservation term on the same datasets to quantify its specific contribution to normalized diversity while monitoring proximity and sparsity impacts.