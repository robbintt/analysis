---
ver: rpa2
title: 'S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation
  Learning'
arxiv_id: '2305.11701'
source_url: https://arxiv.org/abs/2305.11701
tags:
- representations
- learning
- stack
- encoder
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether stacking self-supervised learning
  models can learn hierarchical semantic representations in visual tasks. Specifically,
  the authors propose stacking Joint Embedding Architectures (JEAs) where higher-level
  JEAs are input with representations of lower-level JEA, resulting in a representation
  space that exhibits distinct sub-categories of semantic concepts (e.g., model and
  colour of vehicles) in higher-level JEAs.
---

# S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning

## Quick Facts
- arXiv ID: 2305.11701
- Source URL: https://arxiv.org/abs/2305.11701
- Reference count: 39
- Primary result: Stacking VICReg models creates hierarchical semantic representations that outperform single-stack architectures on linear evaluation tasks

## Executive Summary
This work investigates whether stacking self-supervised learning models can learn hierarchical semantic representations in visual tasks. The authors propose stacking Joint Embedding Architectures (JEAs) where higher-level JEAs receive representations from lower-level JEAs as input, resulting in representation spaces that exhibit distinct sub-categories of semantic concepts at higher abstraction levels. The proposed Stacked Joint Embedding Architectures (S-JEA) use VICReg as the base self-supervised learning method for all stacks, demonstrating improved performance on CIFAR-10 and STL-10 datasets compared to traditional architectures with comparable parameter counts.

## Method Summary
S-JEA extends the VICReg self-supervised learning framework by stacking multiple VICReg models where each subsequent encoder receives the representation output from the previous encoder as input. The architecture uses ResNet-18 encoders with three-layer projector networks (fully-connected layers with batch normalization and ReLU activation). VICReg loss (combining invariance, variance, and covariance terms) is computed at each stack level and backpropagated through all previous stacks. The method is trained stack-wise on CIFAR-10 and STL-10 datasets, with evaluation performed using linear classifiers on frozen encoders.

## Key Results
- S-JEA outperforms traditional VICReg architectures with comparable parameter counts on linear evaluation tasks
- Representation spaces at higher stack levels exhibit distinct sub-clusters within semantic clusters (e.g., model and color sub-categories of vehicles)
- VICReg's variance-covariance regularization effectively prevents information collapse in stacked architectures

## Why This Works (Mechanism)

### Mechanism 1
Stacking encoders enables learning of hierarchical semantic concepts by progressively encoding lower-level representations into more abstract forms. Each higher-level encoder receives output representations from the previous stack as input, allowing it to learn increasingly abstract features from already compressed representation space. The lower-level encoder must successfully learn meaningful representations that can be further processed by subsequent encoders to capture higher-level semantic abstractions.

### Mechanism 2
VICReg's variance-covariance regularization prevents collapse in stacked architectures while maintaining representation quality. The regularization terms (variance maximization and covariance reduction) ensure embeddings maintain informative structure even when stacked, preventing representations from becoming degenerate. VICReg's regularization terms must remain effective when applied to already-compressed representations from previous encoder stacks.

### Mechanism 3
Projectors in stacked JEAs serve dual roles - dimensionality reduction and preventing task misalignment between self-supervised and downstream objectives. Projector heads transform encoder representations into embedding spaces optimized for VICReg objective while allowing encoder to focus on general feature extraction. Separating representation extraction (encoder) from embedding optimization (projector) improves both self-supervised learning and downstream task performance.

## Foundational Learning

- Concept: Self-supervised learning objectives (VICReg invariance, variance, covariance)
  - Why needed here: Understanding how VICReg prevents collapse and maintains representation quality is essential for reasoning about stacked architecture behavior
  - Quick check question: What happens to VICReg loss components when representations collapse to identical vectors?

- Concept: Siamese network architectures and embedding spaces
  - Why needed here: S-JEA uses Siamese encoders with two views per image, and understanding how embeddings relate to representations is crucial
  - Quick check question: Why does VICReg operate on embeddings Z rather than representations Y?

- Concept: Transfer learning and linear evaluation protocols
  - Why needed here: The paper evaluates representations using linear classifiers on frozen encoders, which is standard practice for assessing representation quality
  - Quick check question: What advantage does linear evaluation provide over fine-tuning for assessing representation quality?

## Architecture Onboarding

- Component map: Input → Augmentation → Encoder (ResNet-18) → Projector (3-layer FC + BN + ReLU) → VICReg Loss → Next Stack's Input
- Critical path: Data augmentation → Encoder processing → Projector transformation → VICReg objective computation → Loss backpropagation through all stacks
- Design tradeoffs: More stacks increase parameter count and computation but may capture finer semantic hierarchies; VICReg vs. contrastive methods affects training dynamics
- Failure signatures: Representation collapse (uniform embeddings), poor downstream performance despite good self-supervised loss, vanishing gradients through stacked encoders
- First 3 experiments:
  1. Train S-JEA with 1 stack vs. baseline VICReg to verify parameter count vs. performance tradeoff
  2. Remove projector from stack 0 and observe impact on stack 1 performance to test projector hypothesis
  3. Visualize t-SNE embeddings at each stack level to confirm hierarchical semantic clustering formation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of S-JEA degrade with more than two stacked encoders, and if so, at what point?
- Basis in paper: [inferred] The paper only tests up to two stacked encoders, suggesting potential scalability concerns that were not explored
- Why unresolved: The authors do not investigate the performance impact of stacking more than two encoders, leaving the scalability of the approach uncertain
- What evidence would resolve it: Testing S-JEA with three or more stacked encoders on the same datasets and comparing the performance drop, if any, to the two-stack configuration

### Open Question 2
- Question: How does the performance of S-JEA compare to traditional supervised learning methods on the same datasets?
- Basis in paper: [inferred] The paper focuses on self-supervised learning and does not provide a direct comparison to supervised methods
- Why unresolved: The absence of a comparison to supervised learning methods leaves a gap in understanding the relative effectiveness of S-JEA
- What evidence would resolve it: Training a supervised model on the same datasets and comparing its performance to S-JEA's results on the same evaluation tasks

### Open Question 3
- Question: Can the hierarchical semantic clusters learned by S-JEA be leveraged to improve few-shot learning tasks?
- Basis in paper: [inferred] The paper mentions that hierarchical semantics have shown to improve few-shot learning performance in other works, but does not explore this for S-JEA
- Why unresolved: The paper does not investigate the application of S-JEA's hierarchical representations to few-shot learning, leaving the potential benefits unexplored
- What evidence would resolve it: Applying S-JEA's representations to a few-shot learning task and measuring the performance improvement over baseline methods

## Limitations
- Limited ablation studies on critical components (projector presence, stack depth effects)
- No direct comparison with other stacking approaches or traditional supervised learning methods
- Exact image transformations and training hyperparameters not specified

## Confidence
**Confidence: Medium** - The core hypothesis that stacking JEAs can learn hierarchical semantic representations is supported by experimental results and visualization evidence. However, the paper lacks direct comparisons with other stacking approaches and provides limited ablation studies on the critical components (projector presence, stack depth effects).

## Next Checks
1. Test whether hierarchical semantic clustering persists when using different base self-supervised methods (BYOL, SimCLR) instead of VICReg
2. Conduct ablation studies removing projectors from different stack levels to quantify their impact on representation quality
3. Compare S-JEA performance against traditional multi-task learning approaches with similar parameter counts