---
ver: rpa2
title: 'PIGEON: Predicting Image Geolocations'
arxiv_id: '2307.05845'
source_url: https://arxiv.org/abs/2307.05845
tags:
- image
- images
- geolocalization
- data
- street
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PIGEON, a deep learning model for planet-scale
  image geolocalization using Street View data. The core method involves semantic
  geocell creation with label smoothing, multi-task contrastive pretraining of a CLIP
  vision transformer, and ProtoNet refinement for location predictions.
---

# PIGEON: Predicting Image Geolocations

## Quick Facts
- arXiv ID: 2307.05845
- Source URL: https://arxiv.org/abs/2307.05845
- Reference count: 40
- Primary result: 91.96% country accuracy and 40.36% within 25km globally using Street View data

## Executive Summary
PIGEON is a deep learning model for planet-scale image geolocalization that achieves state-of-the-art results by combining semantic geocell creation, contrastive pretraining with StreetCLIP, and ProtoNet refinement. The model uses 400,000 Street View images augmented with geographic metadata to create a system that consistently beats human players in GeoGuessr, ranking in the top 0.01% of players. The approach achieves 91.96% country accuracy and places 40.36% of guesses within 25 km of target locations globally.

## Method Summary
PIGEON employs semantic geocell creation using administrative boundaries with label smoothing to reduce prediction space cardinality while maintaining spatial accuracy. The model uses a CLIP vision transformer pre-trained on Street View images with geographic captions through multi-task contrastive learning. ProtoNet refinement performs few-shot classification within each geocell using OPTICS clustering to improve street-level accuracy. The system achieves its results through a pipeline of semantic classification followed by prototype-based refinement within the top predicted cells.

## Key Results
- Achieves 91.96% country accuracy on geolocalization benchmarks
- 40.36% of guesses fall within 25 km of target location globally
- Consistently beats human players, ranking in top 0.01% of GeoGuessr players
- State-of-the-art results with up to 38.8 percentage points improvement on country-level accuracy

## Why This Works (Mechanism)

### Mechanism 1: Semantic Geocell Creation with Label Smoothing
- Improves classification by reducing prediction space cardinality while maintaining spatial accuracy
- Merges administrative boundaries into geocells with ≥30 training samples, using Haversine distance for label smoothing
- Assumes visual features follow administrative boundaries and natural geographic features
- Evidence: Uses OPTICS clustering and Voronoi tessellation for semantic boundary preservation

### Mechanism 2: Multi-task Contrastive Pretraining with StreetCLIP
- Learns relevant visual features through pretraining on Street View images with geographic captions
- Implicit multi-task setting with geographic, demographic, and geological auxiliary data
- Assumes geographic features correlate with visual appearance
- Evidence: Achieves state-of-the-art results compared to ImageNet-pretrained models

### Mechanism 3: ProtoNet Refinement within Geocells
- Improves street-level accuracy through few-shot classification within each cell
- Uses OPTICS clustering to find prototypes, selecting closest Euclidean distance during inference
- Assumes visual features within geocells are clustered in feature space
- Evidence: Improves street-level accuracy from 1.32% to 4.84% while maintaining higher-level metrics

## Foundational Learning

- Concept: Contrastive learning and vision transformers
  - Why needed here: Core to CLIP's approach and vision transformer architecture used for state-of-the-art results
  - Quick check question: What is the key difference between contrastive learning and traditional supervised learning, and how does this benefit the geolocalization task?

- Concept: Multi-task learning and transfer learning
  - Why needed here: Used for auxiliary geographic tasks and transfer from CLIP to StreetCLIP
  - Quick check question: How does multi-task learning with shared parameters help prevent overfitting in this geolocalization setting?

- Concept: Clustering algorithms and few-shot learning
  - Why needed here: OPTICS clustering creates prototypes for ProtoNet refinement within geocells
  - Quick check question: What is the role of the OPTICS clustering algorithm in the ProtoNet refinement process, and why is it preferred over simpler clustering methods?

## Architecture Onboarding

- Component map: Input images → CLIP vision transformer → Multi-task heads (geolocation + auxiliary tasks) → ProtoNet refinement → Output coordinates
- Critical path: Street View images → CLIP embedding → Semantic geocell classification → ProtoNet refinement → Final coordinates
- Design tradeoffs: Semantic geocells vs. rectangular (semantic provides better accuracy but requires complex creation), multi-task vs. single-task (multi-task provides better transfer but increases complexity), ProtoNet refinement vs. centroid prediction (refinement provides better street-level accuracy but requires more computation)
- Failure signatures: Poor performance in homogeneous areas (forests, tunnels), overfitting to training distribution (daytime-only images), failure to generalize to new countries
- First 3 experiments:
  1. Implement semantic geocell creation and compare with naive rectangular geocells on small subset
  2. Test label smoothing with different temperature values on geocell classification accuracy
  3. Implement ProtoNet refinement on single geocell and measure street-level accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PIGEON's performance compare to other state-of-the-art image geolocalization models when tested on out-of-distribution datasets?
- Basis in paper: [explicit] The paper states StreetCLIP achieves state-of-the-art results on various benchmarks but doesn't provide direct comparison with other models on same out-of-distribution datasets
- Why unresolved: Paper mentions performance on out-of-distribution datasets but lacks direct comparison with other state-of-the-art models
- What evidence would resolve it: Experiments comparing PIGEON's performance with other state-of-the-art models on same out-of-distribution datasets like IM2GPS and IM2GPS3k

### Open Question 2
- Question: What is the impact of using different vision transformer architectures on PIGEON's performance?
- Basis in paper: [inferred] Paper uses pre-trained CLIP vision transformer and explores pretraining effects but doesn't compare different vision transformer architectures
- Why unresolved: Paper doesn't provide comprehensive comparison of different vision transformer architectures and their impact on performance
- What evidence would resolve it: Experiments comparing PIGEON performance using different vision transformer architectures (ViT-L/14, ViT-L/16, etc.) on same datasets

### Open Question 3
- Question: How does PIGEON's performance vary across different geographic regions and environments?
- Basis in paper: [explicit] Paper reports global performance metrics but lacks detailed analysis across different geographic regions and environments
- Why unresolved: Paper doesn't provide comprehensive analysis of performance variations across urban vs. rural areas, different continents, or different climates
- What evidence would resolve it: Experiments analyzing PIGEON's performance across different geographic regions and environments (urban vs. rural, different continents, different climates)

## Limitations

- Limited direct evidence in corpus for core mechanisms, resulting in Medium confidence assessment
- Unknown Street View API parameters and OPTICS clustering parameters affect reproducibility
- Performance may degrade in homogeneous geographic areas where visual features don't align with administrative boundaries

## Confidence

- Semantic Geocell Creation: Medium confidence - well-reasoned but limited direct evidence
- Multi-task Contrastive Pretraining: Medium confidence - mechanism plausible but lacks corpus citations
- ProtoNet Refinement: Medium confidence - effective but limited validation evidence

## Next Checks

1. Evaluate PIGEON's performance on out-of-distribution datasets (IM2GPS, IM2GPS3k) to test zero-shot transfer capability
2. Conduct ablation studies removing each component (label smoothing, multi-task pretraining, ProtoNet refinement) to quantify individual contributions
3. Test performance in homogeneous geographic areas (forests, tunnels, deserts) to identify failure modes of semantic geocell approach