---
ver: rpa2
title: 'MUSE: Multi-View Contrastive Learning for Heterophilic Graphs'
arxiv_id: '2307.16026'
source_url: https://arxiv.org/abs/2307.16026
tags:
- node
- information
- graph
- learning
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUSE is a novel self-supervised learning method for heterophilic
  graphs that addresses the limitations of existing approaches by considering both
  local and global node similarity diversity. The core idea is to construct two views
  (semantic and contextual) to capture ego node and neighborhood information separately,
  then fuse them using an information fusion controller that models node-specific
  diversity.
---

# MUSE: Multi-View Contrastive Learning for Heterophilic Graphs

## Quick Facts
- **arXiv ID**: 2307.16026
- **Source URL**: https://arxiv.org/abs/2307.16026
- **Reference count**: 40
- **Key outcome**: MUSE achieves up to 19.22% relative improvement in node classification accuracy and 125.53% in clustering tasks on heterophilic graphs compared to state-of-the-art methods.

## Executive Summary
MUSE addresses the challenge of learning node representations in heterophilic graphs where connected nodes have different labels or features. Traditional GNNs struggle with heterophily due to their homophily assumption. MUSE constructs two views - semantic (ego node features) and contextual (neighborhood information) - and uses an adaptive information fusion controller to combine them based on node-specific similarity. The model employs alternating training between representation learning and fusion controller optimization, demonstrating superior performance on 9 benchmark datasets while maintaining computational efficiency.

## Method Summary
MUSE is a self-supervised learning method for heterophilic graphs that constructs semantic and contextual views to capture ego node and neighborhood information separately. Each view uses a GNN encoder with contrastive learning, and an information fusion controller models node-specific diversity by computing personalized weights based on similarity between embeddings and degree centrality. The model uses an alternating training scheme where representation learning and fusion controller optimization mutually reinforce each other. MUSE is evaluated on 9 benchmark datasets for node classification and clustering tasks.

## Key Results
- Achieves up to 19.22% relative improvement in node classification accuracy on heterophilic graphs compared to state-of-the-art methods
- Demonstrates 125.53% improvement in clustering tasks (ACC, NMI, ARI) on heterophilic datasets
- Maintains computational efficiency comparable to or better than existing methods
- Shows strong generalizability even with few labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Separate semantic and contextual views handle heterophily by encoding node features and neighborhood relationships independently
- Core assumption: Nodes in heterophilic graphs have different relationships with their features versus their neighborhoods
- Break condition: If nodes have mixed homophily/heterophily patterns at different scales, fixed view separation may not capture all patterns optimally

### Mechanism 2
- Information fusion controller models node-specific diversity through personalized weights based on local similarity
- Core assumption: Optimal balance between ego features and neighborhood context varies across nodes
- Break condition: If similarity measures don't correlate with useful information content, weights may be suboptimal

### Mechanism 3
- Alternating training between representation learning and fusion controller creates mutual reinforcement
- Core assumption: Interdependent components benefit from iterative refinement rather than joint optimization
- Break condition: If components converge at different rates, alternating may cause oscillations or slow convergence

## Foundational Learning

- **Concept**: Graph Neural Networks (GNN) message passing
  - Why needed here: MUSE builds on GNN encoders for both semantic and contextual views
  - Quick check question: In a GNN, what are the two main operations performed in each layer to update node representations?

- **Concept**: Contrastive learning framework
  - Why needed here: MUSE uses contrastive learning within each view and for cross-view fusion
  - Quick check question: What is the purpose of the NT-Xent loss in contrastive learning, and how does it measure similarity between positive and negative pairs?

- **Concept**: Heterophily in graphs
  - Why needed here: MUSE is specifically designed for graphs where connected nodes have different labels/features
  - Quick check question: How does the homophily ratio differ between homophilic and heterophilic graphs?

## Architecture Onboarding

- **Component map**: Input features → Semantic view (perturbation → GNN → embeddings) and Contextual view (perturbation → GNN → embeddings) → Fusion controller (similarity + degree centrality → weights) → Weighted fusion → Contrastive losses → Output representations

- **Critical path**: Feature/Structure perturbation → GNN encoding → Similarity measurement → Weight computation → Representation fusion → Contrastive loss computation

- **Design tradeoffs**:
  - Two GNN encoders vs. shared parameters: Separate encoders provide view-specific specialization but increase parameters
  - Alternating training vs. joint optimization: Alternating ensures mutual reinforcement but may slow convergence
  - Node-level vs. graph-level weighting: Node-level provides personalization but adds complexity

- **Failure signatures**:
  - If semantic and contextual views produce very similar embeddings, the fusion controller becomes ineffective
  - If perturbations are too weak, contrastive learning provides little signal; if too strong, representations lose meaningful structure
  - If degree centrality doesn't correlate with heterophily patterns, it adds noise to weight computation

- **First 3 experiments**:
  1. Ablation: Remove semantic contrast (MUSE_ns) and contextual contrast (MUSE_nc) separately to measure individual contribution
  2. Sensitivity: Vary λ distribution hyperparameters (α1, α2, ε) to observe impact on performance across datasets
  3. Efficiency: Measure training time per epoch and memory usage vs. DSSL, NWR-GAE, and GREET on Squirrel dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does MUSE's performance compare to supervised methods when labeled data is available?
- **Basis in paper**: The paper mentions MUSE is designed for unsupervised learning to address label dependency, but doesn't directly compare to supervised methods when labels are available.
- **Why unresolved**: The paper focuses on demonstrating MUSE's effectiveness in unsupervised settings, but doesn't explore scenarios where some labeled data exists.
- **What evidence would resolve it**: Experiments comparing MUSE's performance to supervised methods like GCN and GAT when varying amounts of labeled data are available.

### Open Question 2
- **Question**: How does MUSE scale to larger graphs with millions of nodes?
- **Basis in paper**: The paper analyzes time complexity but only evaluates on relatively small benchmark datasets (up to ~20,000 nodes).
- **Why unresolved**: While time complexity is analyzed, the paper doesn't validate MUSE's performance on large-scale graphs.
- **What evidence would resolve it**: Experiments applying MUSE to large-scale real-world graphs with millions of nodes, measuring both performance and runtime.

### Open Question 3
- **Question**: How robust is MUSE to different types of graph noise and missing data?
- **Basis in paper**: The paper mentions perturbations are used in view construction but doesn't systematically evaluate robustness to various noise types.
- **Why unresolved**: The perturbation strategy is mentioned but not thoroughly explored across different noise scenarios.
- **What evidence would resolve it**: Experiments introducing various types of noise (edge deletion, feature corruption, node dropout) and measuring MUSE's performance degradation compared to baselines.

## Limitations
- Architectural details like information fusion controller specifications and perturbation strategies are not fully detailed
- Performance appears sensitive to hyperparameters α₁, α₂, and ε without systematic analysis
- Alternating training schedule details are incomplete, including iteration counts and convergence criteria

## Confidence
- **High confidence**: Core mechanism of separate semantic and contextual views with adaptive fusion
- **Medium confidence**: Overall performance claims supported by ablation studies
- **Low confidence**: Generalizability of alternating training approach without implementation specifics

## Next Checks
1. **Ablation study reproducibility**: Replicate ablation experiments (MUSE_ns, MUSE_nc) to verify individual component contributions on heterophilic datasets
2. **Hyperparameter sensitivity analysis**: Systematically vary α₁, α₂, and ε to identify optimal settings and understand dataset-specific impacts
3. **Scalability evaluation**: Test MUSE on larger heterophilic graphs beyond Cora/CiteSeer scale to verify efficiency claims and identify bottlenecks