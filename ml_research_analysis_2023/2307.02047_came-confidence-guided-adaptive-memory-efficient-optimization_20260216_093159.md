---
ver: rpa2
title: 'CAME: Confidence-guided Adaptive Memory Efficient Optimization'
arxiv_id: '2307.02047'
source_url: https://arxiv.org/abs/2307.02047
tags:
- training
- came
- adafactor
- memory
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAME, a confidence-guided adaptive memory-efficient
  optimizer for training large language models. CAME addresses the memory bottleneck
  of traditional adaptive methods like Adam by factorizing second-moment estimates
  into low-rank components, while using a confidence-guided strategy to maintain training
  stability.
---

# CAME: Confidence-guided Adaptive Memory Efficient Optimization

## Quick Facts
- arXiv ID: 2307.02047
- Source URL: https://arxiv.org/abs/2307.02047
- Authors: 
- Reference count: 7
- One-line primary result: CAME achieves faster convergence and higher accuracy than memory-efficient optimizers like Adafactor with negligible extra memory cost

## Executive Summary
CAME is a memory-efficient optimizer for large language models that addresses the memory bottleneck of traditional adaptive methods like Adam. It factorizes second-moment estimates into low-rank components using non-negative matrix factorization while employing a confidence-guided strategy to maintain training stability. Experiments on BERT, GPT-2, and T5 demonstrate CAME achieves faster convergence and higher accuracy than existing memory-efficient optimizers like Adafactor, with negligible extra memory cost. Notably, in BERT pre-training with batch size 32,768, CAME attains comparable accuracy to LAMB using 15% less memory.

## Method Summary
CAME combines low-rank factorization of second-moment estimates with a confidence-guided stabilization mechanism. The optimizer factorizes the second-moment matrix V into rank-1 components W and H using non-negative matrix factorization, reducing memory from O(nm) to O(n+m). To maintain stability despite this approximation, CAME calculates an instability matrix representing the deviation between momentum-based and current updates, then scales the parameter updates based on the factorization of this instability. The method maintains per-parameter adaptivity while significantly reducing memory requirements compared to Adam.

## Key Results
- CAME achieves faster convergence and higher accuracy than Adafactor on BERT, GPT-2, and T5 pre-training
- In BERT pre-training with batch size 32,768, CAME attains comparable accuracy to LAMB using 15% less memory
- CAME maintains negligible extra memory cost while improving upon existing memory-efficient optimizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAME reduces memory overhead by factorizing second-moment matrix into low-rank components using NMF
- Mechanism: Approximates V as W·H where W = V·1_m and H = 1^T_n·V / (1^T_n·V·1_m), reducing memory from O(nm) to O(n+m)
- Core assumption: Second-moment matrix can be well-approximated by rank-1 factorization without significant loss of adaptivity
- Evidence anchors:
  - [abstract]: "CAME addresses the memory bottleneck of traditional adaptive methods like Adam by factorizing second-moment estimates into low-rank components"
  - [section]: "Adafactor derives an effective solution for non-negative matrix factorization in the special case of rank-1 factors"
- Break condition: If second-moment matrix has high rank, factorization introduces significant error degrading performance

### Mechanism 2
- Claim: CAME uses confidence-guided strategy to reduce instability from factorization approximation
- Mechanism: Calculates instability matrix Ut = (ût - mt)^2, factorizes into Rt and Ct using NMF, scales update by √(Rt·Ct / 1^T_n·Rt)
- Core assumption: Deviation between momentum-based and current updates indicates confidence in approximation
- Evidence anchors:
  - [abstract]: "CAME addresses the memory bottleneck of traditional adaptive methods like Adam by factorizing second-moment estimates into low-rank components, while using a confidence-guided strategy to maintain training stability"
  - [section]: "we propose a confidence-guided strategy that enables self-adjusted updates by taking the confidence of the raw update of Adafactor into consideration"
- Break condition: If instability calculation becomes unstable or factorization of Ut introduces additional error outweighing benefits

### Mechanism 3
- Claim: Combination of low-rank factorization and confidence-guided scaling enables faster convergence and higher accuracy
- Mechanism: Maintains per-parameter adaptivity while reducing memory, uses confidence measure to prevent instability
- Core assumption: Trade-off between memory efficiency and approximation error is favorable when combined with confidence-guided stabilization
- Evidence anchors:
  - [abstract]: "Experiments on BERT, GPT-2, and T5 show CAME achieves faster convergence and higher accuracy than existing memory-efficient optimizers like Adafactor, with negligible extra memory cost"
  - [section]: "our proposed CAME optimization method successfully obtains the same rate of convergence as prevailing first-order optimization algorithms (e.g., Adam) and with almost equal memory cost to available memory-efficient optimizers (e.g., Adafactor)"
- Break condition: If additional computation for confidence measure and factorization outweighs benefits, or approximation error cannot be adequately controlled

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: CAME uses NMF to factorize both second-moment matrix and instability matrix into low-rank components, reducing memory usage from O(nm) to O(n+m)
  - Quick check question: What is the primary advantage of using NMF over other matrix factorization techniques in the context of memory-efficient optimization?

- Concept: Exponential Moving Average (EMA)
  - Why needed here: CAME maintains EMA of factors for second moments and instability matrix, allowing adaptive updates while keeping memory requirements low
  - Quick check question: How does the choice of EMA parameter (β2, β3) affect the trade-off between stability and responsiveness in CAME?

- Concept: Adaptive Learning Rates
  - Why needed here: Core idea is maintaining per-parameter adaptive learning rates while reducing memory cost of storing second-moment estimates
  - Quick check question: Why do adaptive methods like Adam typically require more memory than non-adaptive methods like SGD?

## Architecture Onboarding

- Component map: Gradients gt → Second moment EMA rt, ct → Current update ut = gt/√vt → Instability calculation Ut = (ût - mt)^2 → EMA of instability Rt, Ct → Confidence scaling √(Rt·Ct / 1^T_n·Rt) → Parameter update θt = θt-1 - η·√St·mt

- Critical path: Gradient computation → Second moment EMA → Update calculation → Instability calculation → Confidence scaling → Parameter update

- Design tradeoffs:
  - Memory vs. accuracy: Lower rank factorization reduces memory but may increase approximation error
  - Stability vs. responsiveness: Higher EMA parameters increase stability but reduce responsiveness to changes
  - Computation vs. memory: CAME adds computation for instability calculation but saves memory compared to Adam

- Failure signatures:
  - Divergence: Instability becomes too large, causing updates to be suppressed
  - Slow convergence: Approximation error in factorization is too high
  - Memory bloat: Incorrect implementation of NMF factorization

- First 3 experiments:
  1. Implement CAME with rank-1 factorization and compare memory usage against Adam on a small BERT model
  2. Test different EMA parameters (β2, β3) on a simple regression task to find optimal stability-responsiveness balance
  3. Compare convergence speed and final accuracy of CAME against Adafactor on a GPT-2 pre-training task

## Open Questions the Paper Calls Out
- How does the confidence-guided strategy in CAME perform on non-NLP tasks such as Computer Vision or Reinforcement Learning?
- What is the theoretical convergence guarantee of CAME under different optimization landscapes?
- How does the additional computation cost of CAME's nonnegative matrix factorization compare to its memory savings in practical training scenarios?

## Limitations
- Theoretical guarantees for the confidence-guided mechanism are not fully established
- Choice of rank-1 factorization lacks comprehensive analysis of when higher ranks might be necessary
- Experiments focus on specific language modeling tasks, unclear if benefits extend to other domains

## Confidence

- **High confidence**: Memory efficiency improvements and basic functionality - The paper clearly demonstrates that CAME achieves significant memory savings compared to Adam while maintaining comparable performance. The core mechanism of factorizing second-moment estimates is straightforward and well-implemented.

- **Medium confidence**: Stability improvements from confidence-guided strategy - While experiments show CAME is more stable than Adafactor, the mechanism by which the instability calculation prevents divergence could be more rigorously analyzed. The paper shows positive results but doesn't fully explain why this specific formulation works better than alternatives.

- **Low confidence**: Generalizability across diverse tasks - The experiments focus on specific language modeling tasks (BERT, GPT-2, T5) with particular architectures. It's unclear whether CAME would perform similarly well on vision tasks, reinforcement learning, or other domains with different gradient characteristics.

## Next Checks

1. **Theoretical analysis of stability bounds**: Derive mathematical conditions under which the confidence-guided scaling prevents divergence. Specifically, analyze how the instability matrix Ut relates to the approximation error in the rank-1 factorization and prove bounds on the update magnitude.

2. **Ablation study on factorization rank**: Systematically vary the factorization rank from 1 to 4 on the same tasks and measure the trade-off between memory usage, convergence speed, and final accuracy. This would reveal whether rank-1 is truly optimal or if the benefits depend on specific problem characteristics.

3. **Cross-domain generalization test**: Apply CAME to a non-language-model task such as image classification (e.g., ResNet training on ImageNet) or reinforcement learning (e.g., DQN on Atari) to verify whether the memory efficiency and stability benefits extend beyond the tested NLP applications.