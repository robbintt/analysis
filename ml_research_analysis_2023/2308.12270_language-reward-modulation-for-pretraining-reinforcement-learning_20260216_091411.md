---
ver: rpa2
title: Language Reward Modulation for Pretraining Reinforcement Learning
arxiv_id: '2308.12270'
source_url: https://arxiv.org/abs/2308.12270
tags:
- noun
- reward
- learning
- pretraining
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAMP, a method that uses language-conditioned
  reinforcement learning with pretrained Vision-Language Models (VLMs) to facilitate
  exploration and warm-start learning for robotic manipulation tasks. LAMP modulates
  the rewards generated by a VLM with diverse language prompts during pretraining,
  biasing the agent towards semantically meaningful behaviors.
---

# Language Reward Modulation for Pretraining Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.12270
- Source URL: https://arxiv.org/abs/2308.12270
- Authors: 
- Reference count: 40
- Primary result: LAMP achieves better or competitive performance on RLBench tasks compared to training from scratch or using Plan2Explore alone

## Executive Summary
This paper introduces LAMP, a method that uses language-conditioned reinforcement learning with pretrained Vision-Language Models (VLMs) to facilitate exploration and warm-start learning for robotic manipulation tasks. LAMP modulates the rewards generated by a VLM with diverse language prompts during pretraining, biasing the agent towards semantically meaningful behaviors. This approach is combined with intrinsic novelty-seeking rewards to encourage efficient exploration. The pretrained policy can then be finetuned on downstream tasks, showing improved sample efficiency compared to training from scratch or using unsupervised exploration methods like Plan2Explore.

## Method Summary
LAMP pretrains a language-conditioned policy by combining rewards from a frozen VLM with Plan2Explore's novelty-seeking exploration. The VLM generates rewards based on contrastive alignment between language instructions and visual observations, providing shaped feedback during pretraining. This is combined with novelty rewards (weighted by α=0.9) to balance semantic guidance with exploration coverage. The policy, trained using Masked World Models architecture, conditions actions on both state and language embeddings. During finetuning on downstream tasks, a task-relevant language instruction is selected and kept fixed while adapting the policy to task-specific rewards.

## Key Results
- LAMP achieves better or competitive performance on RLBench tasks compared to training from scratch or using Plan2Explore alone
- The method shows improved sample efficiency during finetuning on downstream tasks
- LAMP is robust to different prompting strategies and can benefit from more powerful VLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAMP's VLM reward is more effective for exploration than task-specific reward learning because it shapes behavior during pretraining without needing perfect accuracy.
- Mechanism: The VLM reward is computed as a contrastive alignment score between language instructions and visual observations. This score increases as the agent makes progress toward completing the described task, providing dense, shaped feedback during pretraining even when the agent is far from task completion.
- Core assumption: The VLM's semantic understanding of visual states relative to language goals provides meaningful shaping signals that guide exploration toward semantically relevant behaviors, even if the reward is noisy.
- Evidence anchors:
  - [abstract] "LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment."
  - [section] "Our motivation is that the R3M score is better suited for providing shaped rewards because its representations are explicitly trained to understand temporal information within videos (see Section 3 for details)"
- Break condition: If the VLM cannot establish meaningful semantic alignment between observations and language instructions, the shaped reward becomes uninformative and exploration degrades to random behavior.

### Mechanism 2
- Claim: Combining VLM rewards with Plan2Explore novelty bonuses creates synergistic exploration that balances semantic meaning with coverage.
- Mechanism: LAMP combines the VLM-based reward (Equation 1) with Plan2Explore's novelty score (Equation 2), weighting them with parameter α. This creates an objective that biases exploration toward semantically meaningful affordances while maintaining sufficient diversity through novelty-seeking.
- Core assumption: VLM rewards and novelty rewards address complementary exploration needs - VLM rewards guide toward semantic affordances while novelty rewards ensure coverage of the state space.
- Evidence anchors:
  - [section] "Let this novelty-based score be rP2E i. We then train our pretraining agent to maximize the following weighted sum of rewards: rpre i = α · rP2E i + (1 − α) · rLAMP i"
  - [section] "By combining this novelty-based reward with the LAMP reward, we encourage the agent to efficiently explore its environment but with an additional bias towards interacting with the semantically meaningful affordances."
- Break condition: If α is set too high, exploration becomes purely novelty-driven and loses semantic guidance; if too low, exploration becomes too narrowly focused on VLM-aligned behaviors and misses important regions.

### Mechanism 3
- Claim: Language-conditioned policies learned during pretraining can be efficiently finetuned on downstream tasks by simply conditioning on task-relevant language.
- Mechanism: The pretraining phase learns a policy π(a|(s, Lα(x))) that conditions actions on both state and language embeddings. At finetuning time, a task-relevant language instruction xft is selected and kept fixed, allowing the policy to leverage learned semantic understanding while adapting to the new task reward.
- Core assumption: The language-conditioned policy learns generalizable representations that transfer across semantically related tasks, and the fixed conditioning language provides a simple interface for task selection.
- Evidence anchors:
  - [section] "Since we have learned a language-conditioned policy, we simply select a language instruction xft roughly corresponding to the downstream task semantics in order to condition the pretrained agent."
  - [section] "We fix this language instruction selection for the entirety of task learning and finetune all RL agent model components except the critic which we linear probe for training stability."
- Break condition: If downstream tasks require substantially different action primitives or semantic understanding than pretraining tasks, the language-conditioned policy may not transfer effectively.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their zero-shot capabilities
  - Why needed here: LAMP relies on VLMs to generate diverse, semantically meaningful rewards without requiring task-specific training data
  - Quick check question: What is the key difference between how LAMP uses VLMs versus previous approaches that use VLMs as direct task rewards?

- Concept: Reinforcement Learning with sparse versus shaped rewards
  - Why needed here: LAMP addresses the challenge of learning from sparse rewards by using VLMs to provide dense shaping signals during pretraining
  - Quick check question: Why is a shaped reward signal particularly valuable during the pretraining phase compared to downstream task learning?

- Concept: Novelty-seeking exploration methods (like Plan2Explore)
  - Why needed here: LAMP combines VLM rewards with novelty-seeking exploration to create a balanced exploration objective
  - Quick check question: What is the primary benefit of adding novelty rewards to VLM-based rewards during pretraining?

## Architecture Onboarding

- Component map:
  - VLM module: Frozen pretrained R3M with DistilBERT language encoder
  - Policy network: Masked World Models (MWM) architecture, language-conditioned
  - Exploration module: Plan2Explore novelty calculation based on world model uncertainty
  - Environment: RLBench with domain randomization (Ego4D textures, ShapeNet objects)
  - Training loop: Collect episodes with language prompts, compute VLM rewards, add novelty bonus, update policy

- Critical path:
  1. Sample language prompt from generated dataset
  2. Condition MDP and agent on language embedding
  3. Collect episode transitions with policy
  4. Compute LAMP rewards using frozen VLM
  5. Add Plan2Explore novelty bonus
  6. Update policy, critic, and world model using MWM algorithm

- Design tradeoffs:
  - VLM choice: R3M vs CLIP vs InternVideo - R3M provides temporal understanding but is slower; CLIP is faster but may miss temporal dependencies
  - α weighting: Higher α emphasizes novelty coverage but may lose semantic guidance; lower α focuses on semantics but may miss important states
  - Language prompt diversity: More diverse prompts improve generalization but increase computation; less diverse prompts may overfit to specific reward patterns

- Failure signatures:
  - Poor downstream performance: Check if pretraining rewards are actually being computed correctly; verify VLM is frozen and working
  - Unstable training: Monitor novelty reward scaling; check if α is too high causing noisy exploration
  - Language conditioning not working: Verify language embeddings are being computed and passed correctly to policy

- First 3 experiments:
  1. Verify VLM reward computation: Run single episode with known language prompt and check if rewards increase as expected toward completion
  2. Test α sensitivity: Run pretraining with different α values and measure downstream performance to find optimal balance
  3. Validate language conditioning: Test if changing language prompts during finetuning actually changes behavior as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LAMP scale with larger and more powerful VLMs, and what is the optimal VLM architecture for this task?
- Basis in paper: [explicit] The paper mentions that LAMP is likely to benefit from more powerful VLMs and that the method is not inherently reliant on a particular VLM.
- Why unresolved: The paper only tests LAMP with three different VLMs (R3M, InternVideo, and CLIP) and does not explore the performance scaling with larger models or different architectures.
- What evidence would resolve it: Testing LAMP with a range of VLMs of increasing size and capability, and analyzing the relationship between VLM performance and downstream task success.

### Open Question 2
- Question: What is the optimal balance between exploration rewards (Plan2Explore) and language-modulated VLM rewards in LAMP, and how does this balance affect downstream task performance?
- Basis in paper: [explicit] The paper mentions that the α value (weighting the two rewards) is set to 0.9 for all experiments, but does not explore the impact of different weighting schemes.
- Why unresolved: The paper does not provide an ablation study on the effect of different α values on pretraining and downstream task performance.
- What evidence would resolve it: Conducting a systematic ablation study varying α and analyzing the impact on pretraining efficiency and downstream task success.

### Open Question 3
- Question: How does the diversity of language prompts during pretraining affect the generalization and robustness of the pretrained policy to unseen downstream tasks?
- Basis in paper: [explicit] The paper explores the effect of different prompting styles on downstream performance and finds that LAMP is robust to different prompting strategies.
- Why unresolved: The paper does not investigate the relationship between prompt diversity and policy generalization in depth, nor does it explore the impact of prompt diversity on the ability to handle novel tasks.
- What evidence would resolve it: Conducting experiments with varying levels of prompt diversity and analyzing the resulting policy's ability to generalize to unseen tasks and adapt to new environments.

## Limitations
- The method's effectiveness depends on the quality and domain coverage of the frozen VLM, which may not generalize well to all environments
- Language-conditioned pretraining requires careful prompt selection during finetuning, which may limit practical applicability
- The method assumes the pretraining environment distribution overlaps sufficiently with downstream tasks for effective transfer

## Confidence
- VLM reward shaping effectiveness: Medium-High
- Novelty-reward synergy: Medium
- Language-conditioned transfer: Medium

## Next Checks
1. **Ablation study on VLM contribution**: Compare LAMP performance when using different VLMs (R3M, CLIP, InternVideo) and when disabling the VLM reward component entirely to quantify its specific contribution to exploration efficiency.

2. **Prompt generalization analysis**: Systematically test the sensitivity of downstream performance to language prompt selection by using prompts that vary in semantic distance from the actual task, measuring how prompt quality affects finetuning success.

3. **Cross-environment transfer validation**: Evaluate whether LAMP's pretraining benefits transfer when the pretraining environment distribution differs substantially from downstream tasks (e.g., different object sets, scene layouts) to assess robustness to distribution shift.