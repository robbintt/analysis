---
ver: rpa2
title: FA Team at the NTCIR-17 UFO Task
arxiv_id: '2310.20322'
source_url: https://arxiv.org/abs/2310.20322
tags:
- table
- data
- cell
- information
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the approach of the FA team to the Table Data
  Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks in the NTCIR-17
  UFO task. For TDE, they used ELECTRA-based text classification with post-correction
  to achieve 93.43% accuracy, ranking second on the leaderboard.
---

# FA Team at the NTCIR-17 UFO Task

## Quick Facts
- arXiv ID: 2310.20322
- Source URL: https://arxiv.org/abs/2310.20322
- Reference count: 10
- Primary result: Achieved 93.43% accuracy on Table Data Extraction (TDE), ranking second on the NTCIR-17 UFO task leaderboard

## Executive Summary
This paper presents the FA team's approach to the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks in the NTCIR-17 UFO task. For TDE, they developed an ELECTRA-based text classification method with post-correction that achieved 93.43% accuracy. Their approach treats each table row as a sequence and classifies target cells in the context of the entire row, followed by post-correction using Levenshtein distance matching to known training patterns. For TTRE, they proposed a rule-based method that extracts relationships based on matching names and values, though the overall F1 score was low at 2.4%. The authors discuss future directions including domain-specific pre-trained models and machine learning approaches for relationship extraction.

## Method Summary
The FA team used ELECTRA-based text classification for TDE, treating each table row as a sequence and classifying target cells in the context of the entire row. After initial classification, they applied post-correction using Levenshtein distance matching to 93 pre-extracted training patterns to improve consistency. For TTRE, they implemented a rule-based approach that extracts relationships by matching names and values using Levenshtein distance thresholds. The TDE method was fine-tuned with a maximum token length of 128, learning rate of 1e-5 for 5 epochs, while TTRE used 70% Levenshtein match threshold for Name identification and 50% numeric threshold for Value identification.

## Key Results
- Achieved 93.43% accuracy on Table Data Extraction (TDE) task
- Ranked second on the NTCIR-17 UFO task leaderboard for TDE
- Low overall F1 score of 2.4% on Text-to-Table Relationship Extraction (TTRE)
- Post-correction improved accuracy by considering both cell and row information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating each table row as a sequence and classifying the target cell in the context of the entire row improves classification accuracy over cell-only classification.
- Mechanism: The model receives both the cell's text and its full row as a single input sequence, using special [SEP] tokens to delineate cells. This allows the ELECTRA model to use surrounding row context to disambiguate cell roles.
- Core assumption: Brief cell text alone is insufficient for accurate classification; row context adds disambiguating signal.
- Evidence anchors:
  - Due to the brevity of cell text, predicting meaning based solely on text is challenging... we treated an entire row and corresponding cells as a single text for classification.
  - When comparing text classification that considers cells alone and cells plus entire rows, the accuracy is improved by two points to 93%.

### Mechanism 2
- Claim: Post-correction based on Levenshtein distance matching to known training patterns improves consistency and accuracy of the ELECTRA output.
- Mechanism: After ELECTRA classifies all cells in a row, the resulting pattern is matched to 93 pre-extracted training patterns using edit distance; mismatches are corrected to the closest pattern.
- Core assumption: The training data contains representative row patterns, and inconsistencies in ELECTRA's cell-by-cell output are correctable via nearest-neighbor matching.
- Evidence anchors:
  - The cell categories predicted by the language model are collected row by row to check whether the sequential pattern was present in the training data... corrected to one that can be matched at minimum cost by a Levenshtein distance editing operation.
  - The proposed method achieves high accuracy using the Electra-based language model with fine-tuning and post-processing.

### Mechanism 3
- Claim: Using ELECTRA instead of BERT improves accuracy because ELECTRA's discriminator training better captures fine-grained token semantics in short cell texts.
- Mechanism: ELECTRA's pretraining focuses on detecting replaced tokens in context, encouraging the model to learn precise word-level representations, which is useful for short cell strings.
- Core assumption: Cell texts are short (few tokens), so fine-grained token-level modeling is more effective than sentence-level modeling.
- Evidence anchors:
  - We employed ELECTRA [3], an extension of BERT [4] to extract table information. While BERT performs pre-training by randomly filling in sentences, ELECTRA [3] proposes a more sophisticated way of filling in sentences by focusing on each word and guessing which parts have been filled in by language models.
  - In a preliminary experiment, we conducted a verification of ELECTRA's effectiveness as compared to BERT.

## Foundational Learning

- Concept: Table Data Extraction (TDE) as sequence classification vs. NER
  - Why needed here: The paper compares classifying each cell in a row vs. treating it as an NER sequence labeling task; understanding this distinction is key to why the chosen approach works.
  - Quick check question: What is the main difference between classifying each cell vs. treating table data as NER?

- Concept: Levenshtein distance and its role in post-correction
  - Why needed here: Post-correction uses Levenshtein distance to find the closest training pattern; knowing how this distance metric works is essential for understanding the correction pipeline.
  - Quick check question: How does Levenshtein distance determine the "closest" pattern for correction?

- Concept: ELECTRA vs. BERT pretraining objectives
  - Why needed here: The choice of ELECTRA over BERT is justified by its token-level discriminative training; knowing the training objectives explains the accuracy gain.
  - Quick check question: What is the key difference between BERT's masked language modeling and ELECTRA's discriminator approach?

## Architecture Onboarding

- Component map:
  Input: HTML table -> extract row/cell structure -> Tokenizer: Special [SEP] tokens to mark cell boundaries -> Model: ELECTRA base Japanese discriminator (fine-tuned) -> Post-correction: Levenshtein distance matching to 93 training patterns -> Output: Per-cell category (Metadata, Header, Attribute, Data)

- Critical path:
  1. Extract row and cell data from HTML
  2. Build sequence string with [SEP] markers
  3. Feed to ELECTRA for per-cell classification
  4. Collect row pattern, apply Levenshtein-based correction
  5. Output corrected labels

- Design tradeoffs:
  - ELECTRA vs. BERT: ELECTRA better for short texts but requires discriminator pretraining.
  - Sequence length limit: Rows longer than 128 tokens are truncated, losing context.
  - Fixed pattern set: Only 93 patterns available for correction; unseen patterns may not be corrected optimally.

- Failure signatures:
  - Low accuracy on very short cells with ambiguous context.
  - Poor correction when output pattern length exceeds all training patterns.
  - Errors when table structure (rowspan/colspan) is not properly normalized before extraction.

- First 3 experiments:
  1. Compare ELECTRA vs. BERT classification accuracy on short cell texts.
  2. Evaluate effect of including row context vs. cell-only classification.
  3. Test post-correction on artificially corrupted ELECTRA outputs to measure correction gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a domain-specific pre-trained model for financial tables compare to the generic ELECTRA model in TDE accuracy?
- Basis in paper: The authors mention that using a domain-specific pre-trained model trained on table representations rather than generic data could improve accuracy.
- Why unresolved: The authors only speculated about this approach but did not implement or test it.
- What evidence would resolve it: A controlled experiment comparing TDE accuracy using generic vs. domain-specific pre-trained models on the same dataset.

### Open Question 2
- Question: Would incorporating more complex linguistic features (beyond Levenshtein distance) improve the TTRE performance?
- Basis in paper: The authors acknowledge that the Levenshtein distance approach has limitations due to notation differences and suggest more sophisticated similarity measures could help.
- Why unresolved: The authors only used a simple rule-based approach and did not explore more complex linguistic features.
- What evidence would resolve it: Comparing TTRE performance using Levenshtein distance vs. advanced semantic similarity measures on the same dataset.

### Open Question 3
- Question: Would a machine learning approach outperform the rule-based method for TTRE?
- Basis in paper: The authors suggest introducing a machine learning approach and using methods that measure feature value similarity instead of exact matching.
- Why unresolved: The authors only tested a rule-based approach and did not implement any machine learning methods.
- What evidence would resolve it: A direct comparison of TTRE performance between rule-based and machine learning approaches on the same dataset.

## Limitations

- The TDE approach relies heavily on a fixed set of 93 training patterns for post-correction, which may not generalize well to unseen table structures or novel financial reporting formats.
- Rows exceeding 128 tokens are truncated, representing a significant architectural constraint that could degrade performance on wide tables with many columns.
- The TTRE approach shows very low overall performance (F1=2.4%), indicating that the rule-based method using Levenshtein distance thresholds is insufficient for complex text-to-table relationship extraction.

## Confidence

**High confidence**: The claim that ELECTRA outperforms BERT for short cell text classification is well-supported by the comparative experiment described and aligns with known differences in pretraining objectives. The mechanism of using row context to improve classification accuracy is clearly demonstrated with the two-point improvement mentioned.

**Medium confidence**: The effectiveness of post-correction via Levenshtein distance matching is supported by the reported accuracy improvement, but the limited number of training patterns (93) raises questions about scalability and robustness. The specific impact of truncation on long rows is acknowledged but not quantified.

**Low confidence**: The TTRE approach's low performance (2.4% F1) is acknowledged but the paper provides minimal analysis of why the rule-based method fails. The discussion of future directions (domain-specific pretraining, ML approaches) is speculative without experimental validation.

## Next Checks

1. **Pattern Coverage Analysis**: Measure the frequency of encountered row patterns during validation that match vs. do not match the 93 training patterns to quantify the limitation of the fixed pattern set and assess whether more diverse training patterns would improve correction effectiveness.

2. **Truncation Impact Study**: Analyze the distribution of table row lengths in the dataset and measure accuracy degradation specifically for rows that are truncated at the 128-token limit versus those that fit entirely, to quantify the cost of the architectural constraint.

3. **TTRE Error Type Classification**: Conduct detailed error analysis on the TTRE task by manually categorizing failures (e.g., name-value pairing errors, threshold issues, parsing problems) to identify whether the low performance stems from threshold settings, feature extraction, or fundamental limitations of the rule-based approach.