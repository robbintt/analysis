---
ver: rpa2
title: 'Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer
  Learning'
arxiv_id: '2310.08782'
source_url: https://arxiv.org/abs/2310.08782
tags:
- source
- learning
- pruning
- data
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses dataset pruning for transfer learning, proposing
  two efficient methods (label mapping and feature mapping) to identify beneficial
  source data classes for downstream tasks. Unlike existing DP methods that fail in
  transfer learning, these approaches leverage source-target domain mapping to prune
  source datasets while preserving or improving downstream performance.
---

# Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning

## Quick Facts
- arXiv ID: 2310.08782
- Source URL: https://arxiv.org/abs/2310.08782
- Reference count: 40
- Up to 40-80% of source classes can be pruned with 2-5x speedup during pretraining without accuracy loss

## Executive Summary
This paper addresses the challenge of dataset pruning (DP) in transfer learning, where conventional DP methods fail due to cross-domain dynamics. The authors propose two efficient methods—label mapping (LM) and feature mapping (FM)—that leverage source-target domain mapping to identify beneficial source data classes for downstream tasks. These approaches enable significant dataset reduction while preserving or improving downstream performance, working across both supervised and self-supervised settings.

## Method Summary
The paper introduces two dataset pruning methods for transfer learning: label mapping (LM) for supervised pretraining and feature mapping (FM) for self-supervised pretraining. LM identifies influential source classes by computing scores based on how often the source model predicts each class when given target data samples. FM extends this concept to self-supervised settings by clustering source data and measuring responsiveness to target data. Both methods use a small surrogate model (e.g., ResNet-18) to efficiently compute pruning scores, which are then applied to prune the full dataset before pretraining the final model.

## Key Results
- Up to 40-80% of source classes can be pruned from ImageNet-1K
- 2-5x speedup during pretraining while maintaining or improving downstream accuracy
- Methods work across supervised (LM) and self-supervised (FM) pretraining protocols
- Can improve other techniques like adversarial pretraining
- Source codes available at https://github.com/OPTML-Group/DP4TL

## Why This Works (Mechanism)

### Mechanism 1: Label Mapping
The source model's prediction accuracy on target data reflects the transferability of corresponding source classes. For each source class, LM computes a score by counting how often the source model predicts that class when given target data samples as input. Classes with high scores are considered more influential for transfer learning.

### Mechanism 2: Feature Mapping
FM extends LM to self-supervised learning by clustering source data into pseudo-classes using deep clustering. For each target sample, it finds the nearest cluster centroid in feature space. Classes with high responsiveness scores are retained, leveraging the deep clustering process to produce meaningful pseudo-classes that capture semantic similarity.

### Mechanism 3: Surrogate Model Approach
Using a small surrogate model (e.g., ResNet-18) for pruning enables efficient computation without sacrificing effectiveness. The surrogate model is trained on the full dataset and used to compute LM/FM scores, then the pruned dataset is used to train the full model. This assumes the pruning decisions made by the surrogate model are sufficiently aligned with those that would be made by the full model.

## Foundational Learning

- **Transfer learning and pretrain-finetune paradigm**: The entire paper addresses dataset pruning specifically for transfer learning, where a model pretrained on a source dataset is finetuned on a target task. Quick check: In transfer learning, which stage typically uses a larger, more diverse dataset?
- **Dataset pruning (DP) and its distinction from in-domain DP**: The paper introduces DP for transfer learning, which differs from conventional DP that operates within a single domain. Quick check: What makes DP for transfer learning more challenging than traditional in-domain DP?
- **Deep clustering and representation learning**: Feature mapping (FM) relies on clustering source data in the feature space to create pseudo-classes for self-supervised settings. Quick check: How does deep clustering help FM identify relevant source classes without labels?

## Architecture Onboarding

- **Component map**: Surrogate model training (e.g., ResNet-18) -> LM/FM score computation -> Dataset pruning based on scores -> Full model pretraining on pruned dataset -> Target task finetuning
- **Critical path**: Surrogate model → LM/FM computation → Pruning → Full model pretraining → Finetuning
- **Design tradeoffs**: Surrogate model size vs. pruning accuracy, pruning ratio vs. downstream performance, computational efficiency vs. potential information loss
- **Failure signatures**: Downstream performance degrades significantly with pruning, surrogate model pruning decisions don't generalize to full model, cluster quality degrades for FM in high-dimensional spaces
- **First 3 experiments**: 1) Train surrogate model and compute LM/FM scores on ImageNet, 2) Prune ImageNet at 20% ratio and train full model (ResNet-101), 3) Finetune on Flowers102 and compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of LM and FM scale when applied to extremely large-scale source datasets (e.g., JFT-300M or LAION-400M) compared to ImageNet-1K? The paper demonstrates effectiveness on ImageNet-1K but doesn't explore performance on significantly larger datasets.

### Open Question 2
What is the theoretical relationship between source-target domain mapping and dataset pruning effectiveness in transfer learning? The paper establishes an empirical connection but lacks rigorous theoretical analysis.

### Open Question 3
How does the choice of surrogate model architecture (beyond ResNet-18) affect the quality of dataset pruning across different source model families (CNNs vs. Vision Transformers)? The paper shows some robustness to model size but only uses ResNet-18 as surrogate for ViT experiments.

## Limitations
- Effectiveness depends heavily on the quality of source model's alignment with target domains
- Method's effectiveness may be limited in cases of significant domain shift
- Paper doesn't extensively explore impact of different pruning ratios on fine-grained downstream tasks

## Confidence

- **High confidence**: The core mechanism of using source model predictions to score transferability (LM method)
- **Medium confidence**: The extension to self-supervised settings (FM method) and surrogate model approach
- **Medium confidence**: The scalability claims regarding 2-5x speedup during pretraining

## Next Checks

1. Test the transferability of LM/FM scores by training the surrogate model on a subset of source classes and evaluating pruning effectiveness on the full model
2. Evaluate the methods on additional downstream tasks with varying domain similarities to the source dataset
3. Benchmark against alternative dataset pruning methods that operate within transfer learning settings to establish relative performance gains