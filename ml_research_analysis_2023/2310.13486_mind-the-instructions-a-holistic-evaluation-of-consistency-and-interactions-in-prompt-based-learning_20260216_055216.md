---
ver: rpa2
title: 'Mind the instructions: a holistic evaluation of consistency and interactions
  in prompt-based learning'
arxiv_id: '2310.13486'
source_url: https://arxiv.org/abs/2310.13486
tags:
- factors
- instruction
- instructions
- in-context
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of in-context learning (ICL)
  versus task-tuned (TT) models to spurious correlations and evaluates the consistency
  of ICL across various design choices. The authors first show that ICL models are
  significantly less sensitive to spurious correlations in adaptation data than TT
  models, with instruction-tuned models performing best.
---

# Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning

## Quick Facts
- arXiv ID: 2310.13486
- Source URL: https://arxiv.org/abs/2310.13486
- Reference count: 40
- Primary result: Instruction quality and type have the largest impact on in-context learning accuracy and consistency, more than model size or number of examples

## Executive Summary
This paper investigates the robustness of in-context learning (ICL) versus task-tuned (TT) models to spurious correlations and evaluates the consistency of ICL across various design choices. The authors first show that ICL models are significantly less sensitive to spurious correlations in adaptation data than TT models, with instruction-tuned models performing best. They then conduct a large-scale analysis of 1536 ICL setups varying factors like model size, instruction quality, number of in-context examples, and task type. Results reveal that while factors related to in-context example organization have minimal impact, the quality and type of instructions used have the largest effect on prediction accuracy and consistency. Instruction tuning improves robustness across setups. The findings highlight the importance of instruction design in ICL and suggest that understanding instruction properties is key to improving consistency and avoiding unpredictable performance variations.

## Method Summary
The study compares task-tuned (TT) and in-context learning (ICL) models on three NLP tasks (NLI, PI, QA) using base datasets and their adversarial counterparts. A large-scale grid search evaluates 1536 combinations of design factors including model size (7B-65B parameters), instruction quality (P3 templates vs FLAN instructions), number of examples (1-16), and task configurations. Statistical analysis using linear regression and ANOVA identifies main effects and interactions between factors on accuracy and consistency metrics.

## Key Results
- ICL models are significantly less sensitive to spurious correlations than TT models
- Instruction quality and type have the largest impact on prediction accuracy and consistency
- Instruction tuning improves robustness across various ICL setups
- Factors related to in-context example organization (number of examples, example ordering) have minimal impact compared to instruction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning models are less sensitive to spurious correlations than task-tuned models.
- Mechanism: ICL models infer task structure from context examples rather than optimizing parameters on labeled data, so they do not overfit to dataset-specific artifact patterns.
- Core assumption: The model's ability to generalize is primarily shaped by the quality and diversity of in-context examples rather than the underlying dataset distribution.
- Evidence anchors:
  - [abstract] "ICL models are significantly less sensitive to spurious correlations in adaptation data than TT models"
  - [section 3.2] "ICL models are much less sensitive to spurious correlations in their adaptation data than TT models"
- Break condition: If in-context examples themselves contain strong spurious correlations, the model may still overfit.

### Mechanism 2
- Claim: The type and phrasing of instructions have the largest impact on ICL prediction accuracy and consistency.
- Mechanism: Instructions shape the model's internal reasoning process and directly influence how it interprets in-context examples and maps them to labels.
- Core assumption: The semantic content of instructions can override or modulate the influence of other prompting factors.
- Evidence anchors:
  - [abstract] "the quality and type of instructions used have the largest effect on prediction accuracy and consistency"
  - [section 4.1.4] "we find two groups of high-accuracy instructions making very different predictions"
- Break condition: If instructions are too ambiguous or semantically equivalent, their impact may be diminished.

### Mechanism 3
- Claim: Instruction tuning improves robustness across various ICL setups.
- Mechanism: Instruction tuning aligns model parameters with a broader understanding of task semantics, making the model more consistent across different instruction phrasings and in-context configurations.
- Core assumption: Models with instruction tuning have learned more generalized task representations that are less sensitive to surface-level variations.
- Evidence anchors:
  - [abstract] "instruction tuning improves robustness across setups"
  - [section 4.2.1] "instruction tuning is consistently beneficial" and "instruction tuning improves consistency and robustness"
- Break condition: If instruction tuning is performed on a narrow or biased dataset, the robustness gains may not generalize.

## Foundational Learning

- Concept: Spurious correlations in NLP datasets
  - Why needed here: Understanding why TT models overfit to dataset artifacts is crucial for comparing their robustness to ICL models.
  - Quick check question: What is a spurious correlation in the context of NLP datasets?

- Concept: In-context learning vs. task tuning
  - Why needed here: Differentiating between these two adaptation paradigms is essential for interpreting the study's findings on robustness.
  - Quick check question: How does in-context learning differ from task tuning in terms of model adaptation?

- Concept: Instruction tuning
  - Why needed here: Recognizing the role of instruction tuning in improving ICL robustness is key to understanding the study's recommendations.
  - Quick check question: What is instruction tuning, and how does it differ from standard task tuning?

## Architecture Onboarding

- Component map: Base datasets (MNLI, QQP, SQuAD) -> Adversarial datasets (HANS, PAWS, SQuAD adversarial, ANLI) -> TT models (RoBERTa-based) -> ICL models (LLaMA, Alpaca, 7B-65B parameters) -> Prompt templates (P3, FLAN) -> Evaluation metrics (accuracy, Cohen's Îº)

- Critical path: 1) Prepare datasets and models, 2) Design and execute prompting experiments, 3) Analyze results statistically, 4) Interpret findings and draw conclusions

- Design tradeoffs: Using pre-trained models vs. fine-tuning from scratch, choosing between different instruction templates, balancing the number of in-context examples, and deciding on the level of adversarial data to include

- Failure signatures: Low accuracy or high variance in predictions, sensitivity to spurious correlations, poor consistency across different prompting setups

- First 3 experiments:
  1. Compare the robustness of ICL and TT models to spurious correlations using base and adversarial datasets
  2. Evaluate the impact of instruction quality on ICL prediction accuracy and consistency
  3. Analyze the interactions between different prompting factors (e.g., instruction quality, n-shots, cross-task) on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of instruction templates lead to the observed performance gains and volatility in ICL models?
- Basis in paper: [explicit] The paper shows that instruction quality is the most influential factor but also highly interactive and volatile
- Why unresolved: The paper identifies instruction quality as critical but does not specify which aspects of instructions (e.g., phrasing, structure, semantics) drive these effects
- What evidence would resolve it: Controlled experiments varying specific instruction properties (e.g., verb choice, sentence structure, task framing) while holding other factors constant

### Open Question 2
- Question: Why does calibration harm performance for larger models but help smaller models in ICL setups?
- Basis in paper: [explicit] The paper observes that calibration harms performance for all but the smallest model
- Why unresolved: The paper notes this counterintuitive finding but does not investigate the underlying reasons for this size-dependent effect
- What evidence would resolve it: Analysis of how calibration affects the probability distributions and entropy of predictions across different model sizes

### Open Question 3
- Question: How do instruction-tuned and vanilla models differ in their internal mechanisms for processing in-context examples?
- Basis in paper: [explicit] The paper shows that instruction tuning improves consistency and robustness across setups
- Why unresolved: The paper demonstrates that instruction tuning helps but does not explore the underlying differences in how these models process and integrate in-context information
- What evidence would resolve it: Comparative analysis of attention patterns, activation distributions, or other internal representations when processing in-context examples in tuned vs. vanilla models

## Limitations

- Limited generalization to more complex tasks beyond NLI, PI, and QA
- Dataset-specific artifacts may still influence results despite using adversarial datasets
- The study uses specific prompt templates (P3 and FLAN) which may not represent all effective prompt designs

## Confidence

- ICL models are less sensitive to spurious correlations than TT models (High confidence)
- Instruction quality has the largest impact on ICL prediction accuracy and consistency (Medium confidence)
- Instruction tuning improves robustness across various ICL setups (Medium confidence)

## Next Checks

1. Evaluate on more diverse tasks to assess generalizability beyond NLI, PI, and QA
2. Explore alternative prompt template designs to determine if observed effects are template-specific
3. Analyze instruction influence mechanisms to understand how different phrasings affect model behavior