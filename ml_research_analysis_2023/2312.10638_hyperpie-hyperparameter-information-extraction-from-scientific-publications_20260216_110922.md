---
ver: rpa2
title: 'HyperPIE: Hyperparameter Information Extraction from Scientific Publications'
arxiv_id: '2312.10638'
source_url: https://arxiv.org/abs/2312.10638
tags:
- information
- data
- text
- entity
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper formalizes and tackles hyperparameter information extraction
  (HyperPIE) from scientific publications as an entity recognition and relation extraction
  task. The authors create a labeled dataset covering publications from various computer
  science disciplines and use it to train and evaluate BERT-based fine-tuned models
  as well as five large language models.
---

# HyperPIE: Hyperparameter Information Extraction from Scientific Publications

## Quick Facts
- arXiv ID: 2312.10638
- Source URL: https://arxiv.org/abs/2312.10638
- Reference count: 36
- Key outcome: BERT-based fine-tuned models achieve 29% F1 improvement over baseline in hyperparameter relation extraction

## Executive Summary
This paper formalizes hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task from scientific publications. The authors create a labeled dataset covering machine learning, computer vision, computational linguistics, and deep learning domains, then train and evaluate both fine-tuned BERT-based models and large language models. They develop a novel relation extraction approach that significantly improves performance over existing baselines, and introduce YAML-based structured output for LLM extraction that outperforms JSON alternatives. The work demonstrates the feasibility of automated hyperparameter extraction and provides insights into cross-disciplinary patterns.

## Method Summary
The HyperPIE task is addressed through two main approaches: fine-tuned models and LLM-based extraction. For fine-tuned models, the authors build on the PL-Marker baseline for entity recognition, then develop a novel relation extraction component that uses entity class embeddings, relative distance encoding, and mean-pooled token embeddings to predict relationships between research artifacts, parameters, values, and contexts. For LLMs, they implement both zero-shot and few-shot prompting strategies with JSON and YAML output formats, finding that YAML reduces parsing errors and improves entity recognition performance. The models are trained on a manually annotated dataset of 444 paragraphs containing 1,971 entities and 614 relations.

## Key Results
- Fine-tuned relation extraction model achieves 29% F1 improvement over PL-Marker baseline
- LLM YAML output format improves entity recognition by 5.5% F1 compared to JSON
- Value-parameter relations are more reliably predicted than parameter-artifact relations
- Cross-disciplinary analysis reveals distinct hyperparameter usage patterns across ML, CV, CL, and DL domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based fine-tuned models can effectively extract hyperparameter information when combined with a dedicated relation extraction approach.
- Mechanism: The ER component of PL-Marker provides strong entity recognition, while the novel RE component uses entity class pairings and relative distances as inputs to a feed-forward neural network, improving relation extraction performance.
- Core assumption: Hyperparameter information in scientific text exhibits regularity in the relative distances between artifacts, parameters, and values.
- Evidence anchors:
  - [abstract]: "For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline."
  - [section]: "We observe that value-parameter relations are more reliably predicted than parameter-artifact relations."
  - [corpus]: Weak evidence. No direct citations found in corpus to support this specific mechanism.
- Break condition: If the regularity in relative distances between entities is not present in the text, the RE component's performance would degrade.

### Mechanism 2
- Claim: Large language models (LLMs) can extract hyperparameter information using YAML output for structured data extraction, achieving better performance than JSON.
- Mechanism: LLMs are prompted to output data in YAML format, which is less prone to "delimiter collision" problems compared to JSON, leading to better parsing and entity recognition.
- Core assumption: YAML format is more robust for complex structured predictions from LLMs compared to JSON.
- Evidence anchors:
  - [abstract]: "For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves an average improvement of 5.5% F1 in entity recognition over using JSON."
  - [section]: "Different from the related work introduced above, we do not limit our experiments to API access based closed source LLMs, but also evaluate various open LLMs."
  - [corpus]: Weak evidence. No direct citations found in corpus to support this specific mechanism.
- Break condition: If the LLM output consistently adheres to YAML format or if JSON becomes more robust for complex structured predictions.

### Mechanism 3
- Claim: Few-shot prompting with in-context learning improves LLM performance for hyperparameter information extraction.
- Mechanism: By providing examples in the prompt, the LLM can learn the desired output format and improve entity recognition and relation extraction.
- Core assumption: In-context learning is effective for complex structured predictions from LLMs.
- Evidence anchors:
  - [abstract]: "Using our best performing model we extract hyperparameter information from a large number of unannotated papers, and analyze patterns across disciplines."
  - [section]: "Few-shot: Our few shot approach makes the following adjustments to the method described above. Prompts additionally include a component [examples]."
  - [corpus]: Weak evidence. No direct citations found in corpus to support this specific mechanism.
- Break condition: If the provided examples are not representative or if the LLM fails to generalize from the examples.

## Foundational Learning

- Concept: Named Entity Recognition (NER) and Relation Extraction (RE)
  - Why needed here: HyperPIE is formalized as an ER+RE task, requiring the identification of entities (research artifacts, parameters, values, contexts) and their relations.
  - Quick check question: What are the four entity classes defined in the HyperPIE task?

- Concept: BERT and SciBERT embeddings
  - Why needed here: These embeddings are used to provide contextualized token representations for the fine-tuned models.
  - Quick check question: Why does the paper choose regular BERT embeddings over SciBERT for the RE component?

- Concept: YAML vs JSON for structured data extraction
  - Why needed here: The choice of YAML over JSON for LLM output is crucial for better parsing and entity recognition.
  - Quick check question: What is the main advantage of using YAML over JSON for LLM output in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> PL-Marker ER baseline -> Custom RE component -> LLM zero/few-shot prompting -> YAML/JSON parsing -> Evaluation
- Critical path:
  1. Annotate papers to create the HyperPIE dataset.
  2. Train and evaluate fine-tuned models.
  3. Develop and evaluate LLM-based approaches.
  4. Apply the best model to large-scale unannotated data.
  5. Analyze extracted hyperparameter information.
- Design tradeoffs:
  - Using YAML vs JSON for LLM output: YAML is less prone to parsing errors but may require more careful prompt engineering.
  - Fine-tuned models vs LLMs: Fine-tuned models achieve higher performance but require labeled data, while LLMs can work in zero-shot/few-shot settings but may have lower performance.
- Failure signatures:
  - Poor entity recognition: The model may struggle with identifying research artifacts, parameters, values, or contexts.
  - Incorrect relation extraction: The model may fail to correctly identify the relationships between entities.
  - Parsing errors: If using LLMs, the output may not be in the expected format (YAML or JSON), leading to parsing errors.
- First 3 experiments:
  1. Evaluate the PL-Marker ER component on the HyperPIE dataset to establish a baseline for entity recognition.
  2. Implement and evaluate the novel RE component using entity class pairings and relative distances.
  3. Compare the performance of YAML vs JSON output for LLM-based approaches.

## Open Questions the Paper Calls Out

- Question: Can the HyperPIE approach be effectively applied to scientific disciplines outside of machine learning, such as physics or biology?
  - Basis in paper: [explicit] The paper states "Our work does not test transferability of methods to domains outside of ML related fields. It would require domain expertise to find useful definitions for hyperparameters in each respective domain."
  - Why unresolved: The authors explicitly acknowledge this limitation and did not conduct experiments in other domains.
  - What evidence would resolve it: Successful application and evaluation of the HyperPIE model on annotated datasets from physics, biology, or other scientific domains, demonstrating comparable performance to the ML results.

- Question: How would the performance of the HyperPIE model change if it were fine-tuned on a multilingual dataset instead of being limited to English text?
  - Basis in paper: [explicit] The authors note "Our data and experiments unfortunately are limited to English text only and do not cover other languages."
  - Why unresolved: The current model has not been trained or evaluated on non-English text.
  - What evidence would resolve it: Fine-tuning the model on a multilingual dataset and comparing its performance to the English-only version, ideally showing improved or comparable results across multiple languages.

- Question: Can incorporating information from tables, graphs, and source code into the HyperPIE pipeline significantly improve the extraction of hyperparameter information compared to text-only extraction?
  - Basis in paper: [explicit] The authors state "Our work considers HyperPIE from text. This is sensible for a focussed approach, but downstream applications could furthermore benefit from composite pipelines also targeting extraction from tables, source code, etc."
  - Why unresolved: The current approach focuses solely on text-based extraction, leaving the potential benefits of multi-modal extraction unexplored.
  - What evidence would resolve it: Developing and evaluating a composite pipeline that extracts hyperparameter information from text, tables, graphs, and source code, then comparing its performance and completeness to the text-only approach.

## Limitations

- Dataset size and scope: The HyperPIE dataset covers only four CS domains with 444 paragraphs, potentially limiting generalizability.
- Missing implementation details: Critical hyperparameters and prompt templates are not fully specified, creating reproducibility challenges.
- Limited empirical validation: Key mechanisms (YAML vs JSON advantages, few-shot prompting effectiveness) lack rigorous ablation studies.

## Confidence

- High Confidence: The 29% F1 improvement over PL-Marker baseline for the custom RE component is explicitly measured and reported.
- Medium Confidence: The YAML vs JSON comparison shows 5.5% F1 improvement, but underlying reasons lack rigorous analysis.
- Low Confidence: The effectiveness of few-shot prompting across different LLM architectures is asserted but lacks detailed experimental validation.

## Next Checks

1. Perform an ablation study removing the relative distance encoding component from the RE model to quantify its exact contribution to the 29% F1 improvement.

2. Apply the best-performing model to scientific publications from domains not represented in the training data (e.g., physics, biology, social sciences) to assess true generalizability.

3. Systematically compare YAML vs JSON parsing success rates across different LLM outputs, measuring parsing error rates and edge case handling.