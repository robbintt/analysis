---
ver: rpa2
title: 'Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning'
arxiv_id: '2311.09712'
source_url: https://arxiv.org/abs/2311.09712
tags:
- reco
- speaker
- listener
- pragmatic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RECO, a pragmatic language understanding model
  based on regularized equilibrium search in signaling games. Instead of iterative
  response models, RECO treats pragmatic communication as optimization of regularized
  utilities that balance communicative success with adherence to default semantics.
---

# Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning

## Quick Facts
- arXiv ID: 2311.09712
- Source URL: https://arxiv.org/abs/2311.09712
- Reference count: 5
- Key outcome: RECO matches or outperforms RSA and BR models on benchmark pragmatic reasoning tasks with correlation coefficients around 0.86 and accuracy exceeding 95%.

## Executive Summary
This paper introduces RECO, a pragmatic language understanding model that treats communication as regularized equilibrium search in signaling games. Rather than using iterative response models like RSA, RECO optimizes for both communicative success and adherence to default semantics through regularized utilities. The model uses piKL-Hedge algorithm to compute equilibria, yielding principled sampling algorithms and formal guarantees about trade-offs between communicative success and naturalness. On benchmark datasets involving scalar implicatures, reference games, and color reference tasks, RECO matches or exceeds the performance of traditional iterative response models.

## Method Summary
RECO computes equilibria in signaling games by optimizing regularized utilities that balance communicative success with proximity to default semantics. The piKL-Hedge algorithm performs projected gradient ascent in entropic regularization space, where projections are equivalent to softmax operations. This ensures policies remain close to default semantics while finding communicative solutions. The model uses transformer-based literal speaker and listener models as anchor policies, and can be interpreted as a principled extension of RSA when priors are uniform and costs are zero.

## Key Results
- RECO achieves correlation coefficients around 0.86 with human judgments on reference game tasks
- Top-1 accuracy exceeds 95% on the Colors in Context reference game
- Matches or improves upon predictions made by best response and rational speech act models across multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularized equilibrium search directly optimizes for both communicative success and adherence to default semantics, unlike iterative response models that optimize for response strategies.
- Mechanism: RECO uses piKL-Hedge to compute equilibria that balance game-theoretic optimality with proximity to default semantics through regularization terms in the utility function.
- Core assumption: Agents have access to reasonable default semantics (τS, τL) that encode their prior expectations about language use.
- Evidence anchors:
  - [abstract] "RECO matches or improves upon predictions made by best response and rational speech act models of language understanding"
  - [section 3] "RECO makes it possible to efficiently learn models of pragmatic communication from data, and to provide formal guarantees about their communicative success and deviation from default semantic"
- Break condition: If default semantics τS, τL are poorly specified or completely misspecified, the regularization will fail to guide the search toward natural language conventions.

### Mechanism 2
- Claim: piKL-Hedge dynamics converge to equilibria that are both game-theoretically optimal and close to default semantics.
- Mechanism: The algorithm performs projected gradient ascent in entropic regularization space where projections are equivalent to softmax, ensuring the resulting policies are close to default semantics by design.
- Core assumption: The game defined by regularized utilities has a non-empty set of equilibria that balance communicative success with semantic naturalness.
- Evidence anchors:
  - [section 3.2] "piKL-Hedge has strong guarantees, including... the average policy of Player i lies within a distance of roughly 1/λi from the default semantics τi"
  - [section 3] "piKL-Hedge has been used... to find equilibria that are close to human imitation learned anchor policies"
- Break condition: If the regularization parameters λS, λL are set too low, the policies may drift too far from default semantics; if too high, the policies may be too constrained to find communicative solutions.

### Mechanism 3
- Claim: RECO can be interpreted as a principled extension of RSA with formal convergence guarantees.
- Mechanism: When priors are uniform and costs are zero, RECO reduces to a variant of RSA that averages policies across iterations rather than using final-level policies, and downweights low-probability actions under default semantics.
- Core assumption: The RSA model captures essential aspects of pragmatic reasoning that can be formalized through equilibrium search.
- Evidence anchors:
  - [section 3.3] "it is also possible to interpret RECO as an RSA variant in which (1) the final policy at level t is a weighted average of policies computed at lower levels, (2) both speakers and listeners downweight actions that are low-probability under the default semantics"
  - [section 3] "RECO makes it possible to efficiently learn models of pragmatic communication from data"
- Break condition: If the RSA interpretation breaks down in more complex meaning spaces, the theoretical connection may not hold.

## Foundational Learning

- Concept: Signaling games and Nash equilibria
  - Why needed here: The entire RECO framework is built on finding equilibria in signaling games where speakers and listeners have competing objectives
  - Quick check question: In a simple signaling game with two meanings and two utterances, how many pure Nash equilibria can exist, and what determines which one is selected?

- Concept: Regularization in optimization
  - Why needed here: The regularization terms in RECO's utility function control the trade-off between communicative success and adherence to default semantics
  - Quick check question: How does changing the regularization parameter λ affect the distance between the learned policy and the default semantics τ?

- Concept: KL divergence and information geometry
  - Why needed here: The regularization uses KL divergence to measure similarity between policies, and the piKL-Hedge algorithm operates in the geometry of entropic regularization
  - Quick check question: What does it mean geometrically when KL(π∥τ) is minimized, and how does this relate to the choice of policies?

## Architecture Onboarding

- Component map: Default semantics → piKL-Hedge dynamics → Equilibrium policies → Evaluation metrics
- Critical path: Default semantics → piKL-Hedge dynamics → Equilibrium policies → Evaluation metrics
- Design tradeoffs: RECO trades off between communicative success (maximizing utility) and naturalness (staying close to default semantics). The regularization parameters λS, λL control this balance. Too much regularization may prevent finding communicative solutions; too little may produce unnatural conventions.
- Failure signatures: Poor performance on human judgment tasks suggests either bad default semantics or inappropriate regularization parameters. Slow convergence or oscillations in the piKL-Hedge dynamics may indicate numerical issues or poor gradient estimates.
- First 3 experiments:
  1. Verify RECO recovers the correct implicature in the Quantity Implicature model problem (none/some/all example)
  2. Test sensitivity to regularization parameters λS, λL by sweeping values and measuring distance from default semantics
  3. Compare RECO predictions against human judgments on the SIMPLE reference task from Frank (2016)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of regularization parameters (λS, λL) affect the trade-off between communicative success and naturalness in RECO?
- Basis in paper: [explicit] The paper discusses how λS and λL control the amount of regularization towards default semantics, but does not provide a comprehensive analysis of how different parameter settings affect model performance.
- Why unresolved: The paper shows correlation and accuracy results for specific parameter settings but does not systematically explore the parameter space or provide guidelines for choosing optimal values.
- What evidence would resolve it: A systematic study varying λS and λL across a wide range and analyzing the resulting trade-offs between communicative success (accuracy) and naturalness (similarity to default semantics) would resolve this question.

### Open Question 2
- Question: Can RECO be extended to handle more complex pragmatic phenomena beyond implicature, such as metaphor or irony?
- Basis in paper: [inferred] The paper focuses on implicature tasks but does not explore other pragmatic phenomena. The general framework of regularized equilibrium search could potentially be applied to other forms of non-literal language use.
- Why unresolved: The paper does not provide evidence or theoretical arguments for how RECO would handle other pragmatic phenomena. It's unclear if the equilibrium search approach would be effective for more complex forms of meaning manipulation.
- What evidence would resolve it: Applying RECO to tasks involving metaphor or irony interpretation and comparing its performance to existing models would provide evidence for its applicability to broader pragmatic phenomena.

### Open Question 3
- Question: How does RECO's performance compare to human-level pragmatic reasoning, and what aspects of human pragmatic inference might it fail to capture?
- Basis in paper: [inferred] The paper compares RECO to other computational models but does not benchmark it against human performance or analyze potential limitations in capturing human-like pragmatic reasoning.
- Why unresolved: The paper demonstrates that RECO performs well on specific tasks but does not provide insights into how closely it approximates human pragmatic reasoning or where it might fall short.
- What evidence would resolve it: Direct comparisons between RECO predictions and human judgments on a wider range of pragmatic tasks, along with analyses of cases where RECO succeeds or fails relative to human performance, would address this question.

## Limitations
- The quality of RECO's results is fundamentally limited by the quality of the anchor policies (τS, τL), which may be difficult to learn accurately in complex domains
- The piKL-Hedge algorithm's convergence guarantees depend on specific assumptions about the game structure that may not always hold in practical language understanding tasks
- The theoretical relationship between RECO and traditional RSA models may break down in more complex meaning spaces or with non-uniform priors

## Confidence
- High confidence: The core mechanism of regularized equilibrium search (Mechanism 1) is well-supported by the mathematical framework and empirical results showing improved performance over iterative response models
- Medium confidence: The piKL-Hedge convergence guarantees (Mechanism 2) are theoretically sound but rely on assumptions about the game structure that may not always hold in practical language understanding tasks
- Medium confidence: The RSA interpretation of RECO (Mechanism 3) provides useful theoretical insights but may not capture all aspects of pragmatic reasoning in complex scenarios

## Next Checks
1. Test RECO's sensitivity to anchor policy quality by deliberately corrupting the default semantics and measuring performance degradation across all benchmark tasks
2. Evaluate RECO's robustness to non-uniform priors by constructing scenarios where the uniform prior assumption is violated and measuring how this affects equilibrium selection and performance
3. Compare RECO's computational efficiency against traditional RSA and BR models across the same benchmark tasks, measuring both wall-clock time and convergence behavior of the equilibrium search process