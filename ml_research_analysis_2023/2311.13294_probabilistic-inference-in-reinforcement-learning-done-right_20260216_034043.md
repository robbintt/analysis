---
ver: rpa2
title: Probabilistic Inference in Reinforcement Learning Done Right
arxiv_id: '2311.13294'
source_url: https://arxiv.org/abs/2311.13294
tags:
- lemma
- vapor
- policy
- problem
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents a principled Bayesian approach to reinforcement\
  \ learning as probabilistic inference. The authors introduce the posterior probability\
  \ of state-action optimality, P\u0393\u22C6, which flows through the Markov decision\
  \ process and can be used to generate efficient exploration policies."
---

# Probabilistic Inference in Reinforcement Learning Done Right

## Quick Facts
- arXiv ID: 2311.13294
- Source URL: https://arxiv.org/abs/2311.13294
- Reference count: 40
- Primary result: VAPOR and VAPOR-lite achieve superior performance on hard exploration tasks like DeepSea and Atari compared to existing approaches.

## Executive Summary
This paper presents a principled Bayesian approach to reinforcement learning as probabilistic inference. The authors introduce the posterior probability of state-action optimality, PΓ⋆, which flows through the Markov decision process and can be used to generate efficient exploration policies. Since computing PΓ⋆ is intractable, they derive a variational Bayesian approximation yielding a tractable convex optimization problem. The resulting algorithm, VAPOR, has strong connections to Thompson sampling, K-learning, and maximum entropy exploration.

## Method Summary
VAPOR (Variational Approximation of the Posterior probability of Optimality in RL) involves solving a variational optimization problem to approximate the posterior probability of state-action optimality, PΓ⋆. The optimization balances optimism and entropy regularization through a two-player zero-sum game between policy and temperature players. VAPOR-lite, a deep RL variant, modifies a policy gradient agent to include an uncertainty-weighted entropy regularization term. Both algorithms use Bayesian beliefs over rewards and transitions, updated after each observed transition.

## Key Results
- VAPOR achieves superior performance on hard exploration tasks like DeepSea and Atari compared to existing approaches.
- The variational approximation of PΓ⋆ yields a tractable convex optimization problem.
- Thompson sampling implicitly approximates PΓ⋆ by sampling environments from the posterior.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing the posterior probability of state-action optimality (PΓ⋆) enables efficient exploration.
- Mechanism: PΓ⋆ conditions action selection on the event that all prior actions were optimal, ensuring consistency with beliefs that would have led to the current state.
- Core assumption: Transition dynamics are known or can be effectively transformed into known dynamics via Gaussian-Dirichlet optimism.
- Evidence anchors:
  - [abstract]: "knowledge of PΓ⋆ is sufficient to derive a policy that explores efficiently, as measured by regret"
  - [section 3]: "Conditioning on the event Γ⋆(s) when at state s forces the policy to be consistent with the set of the beliefs that would get the agent to state s in the first place"
  - [corpus]: weak (no direct corpus evidence found)
- Break condition: When transition dynamics are highly uncertain and the Gaussian-Dirichlet optimism approximation breaks down.

### Mechanism 2
- Claim: Variational approximation of PΓ⋆ yields a tractable convex optimization problem.
- Mechanism: The VAPOR optimization problem balances optimism (σ²/τ) and entropy regularization (τ-weighted) through a two-player zero-sum game between policy and temperature players.
- Core assumption: Mean rewards are sub-Gaussian under the posterior and bounded in [0,1].
- Evidence anchors:
  - [abstract]: "we derive a variational Bayesian approximation yielding a tractable convex optimization problem"
  - [section 4.2]: "the VAPOR optimization problem is equivalent to solving a two-player zero-sum game between a 'policy' playerλ ∈ Λ(P) and a 'temperature' playerτ ∈ RL,S,A+"
  - [corpus]: weak (no direct corpus evidence found)
- Break condition: When the sub-Gaussian assumption fails or the optimization becomes intractable for very large state spaces.

### Mechanism 3
- Claim: Thompson sampling implicitly approximates PΓ⋆ by sampling environments from the posterior.
- Mechanism: E[λTS] = PΓ⋆ where λTS is the occupancy measure of the Thompson sampling policy.
- Core assumption: Thompson sampling policy induces occupancy measures that converge to PΓ⋆ in expectation.
- Evidence anchors:
  - [section 6]: "Lemma 8. Let λTS be the occupancy measure of the TS policy, it holds that E[λTS] = PΓ⋆"
  - [abstract]: "Thompson sampling [78, 75, 59, 71] can be directly linked to PΓ⋆"
  - [corpus]: weak (no direct corpus evidence found)
- Break condition: When Thompson sampling suffers linear regret in multi-agent or constrained cases.

## Foundational Learning

- Concept: Bayesian inference in reinforcement learning
  - Why needed here: The entire approach relies on maintaining and updating posterior beliefs over MDP parameters
  - Quick check question: How does the posterior distribution change after observing a transition (s,a,s',r)?

- Concept: Occupancy measures and their relationship to policies
  - Why needed here: PΓ⋆ is shown to lie in the space of occupancy measures Λ(P), enabling policy derivation
  - Quick check question: Given an occupancy measure λ, how do you recover the corresponding policy π?

- Concept: Convex optimization and exponential cone programs
  - Why needed here: The VAPOR optimization problem is an exponential cone program that must be solved efficiently
  - Quick check question: What is the dual formulation of the VAPOR optimization problem?

## Architecture Onboarding

- Component map:
  - Bayesian belief updater -> VAPOR optimizer -> Policy executor -> Uncertainty estimator

- Critical path:
  1. Observe (s,a,r,s') from environment
  2. Update Bayesian beliefs (posterior over rewards/transitions)
  3. Compute uncertainty measure σ or pσ
  4. Solve VAPOR optimization problem
  5. Derive policy from optimized occupancy measure
  6. Execute policy in environment

- Design tradeoffs:
  - Exact PΓ⋆ vs variational approximation: Exact computation is intractable, variational approximation is efficient but approximate
  - Known P vs unknown P: Known P allows simpler formulation, unknown P requires transformation via Gaussian-Dirichlet optimism
  - Entropy regularization: Full occupancy measure entropy vs policy entropy (VAPOR-lite) for computational tractability

- Failure signatures:
  - Poor exploration despite uncertainty bonuses: Indicates VAPOR optimization not capturing PΓ⋆ well
  - Numerical instability in optimization: Suggests poor scaling or ill-conditioned uncertainty measures
  - Linear regret growth: Indicates breakdown of assumptions (e.g., sub-Gaussian rewards)

- First 3 experiments:
  1. GridWorld with known dynamics and random rewards: Verify VAPOR approximates PΓ⋆ similarly to Thompson sampling average
  2. DeepSea with increasing depth L: Test deep exploration capability and time to solve vs baselines
  3. Atari benchmark with VAPOR-lite: Evaluate performance vs standard policy gradient with fixed entropy regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VAPOR perform in environments with non-stationary dynamics or partially observable states?
- Basis in paper: [inferred] The paper assumes a finite-horizon MDP with known or Dirichlet-prior transition dynamics. Real-world environments often violate these assumptions.
- Why unresolved: The paper focuses on theoretical guarantees under these assumptions. Extending the analysis to non-stationary or partially observable environments would require significant modifications to the variational framework.
- What evidence would resolve it: Experiments comparing VAPOR to baselines in benchmark environments with non-stationary dynamics or POMDP structure, along with theoretical analysis of regret bounds under these conditions.

### Open Question 2
- Question: What is the impact of the choice of uncertainty measure pσ on VAPOR-lite's performance?
- Basis in paper: [explicit] The paper mentions using an ensemble of reward predictors as a simple uncertainty measure, but suggests more sophisticated domain-specific measures could be used.
- Why unresolved: The paper does not empirically investigate the impact of different uncertainty measures on performance. Different uncertainty measures might capture epistemic uncertainty differently, leading to varying exploration strategies.
- What evidence would resolve it: Experiments comparing VAPOR-lite with different uncertainty measures (e.g., ensembles of value functions, dropout-based uncertainty, or domain-specific measures) across a range of environments.

### Open Question 3
- Question: How does VAPOR-lite scale to continuous action spaces?
- Basis in paper: [inferred] The paper focuses on finite MDPs and uses policy gradient techniques. Extending to continuous action spaces would require modifications to the optimization problem and policy parameterization.
- Why unresolved: The paper does not address the challenges of scaling to continuous action spaces, such as the increased dimensionality of the policy and the need for appropriate exploration strategies.
- What evidence would resolve it: Implementation of VAPOR-lite for continuous action spaces (e.g., using Gaussian policies or normalizing flows) and comparison to state-of-the-art continuous control algorithms on benchmark tasks.

## Limitations

- The effectiveness of the Gaussian-Dirichlet optimism transformation for unknown dynamics is not empirically validated.
- The computational efficiency and scalability of VAPOR to larger state spaces are not thoroughly analyzed.
- The practical implications and differences in performance between VAPOR and Thompson sampling are not thoroughly explored.

## Confidence

- **High Confidence**: The theoretical framework for VAPOR, including the variational approximation of PΓ⋆ and its connection to Thompson sampling and K-learning. The experimental results on DeepSea and Atari benchmarks demonstrating the effectiveness of VAPOR and VAPOR-lite.
- **Medium Confidence**: The practical implementation details and hyperparameters used in the experiments, particularly for VAPOR-lite. The computational efficiency and scalability of VAPOR to larger state spaces.
- **Low Confidence**: The exact impact of the Gaussian-Dirichlet optimism transformation on the performance of VAPOR in the unknown dynamics setting. The robustness of VAPOR to model misspecification and non-stationary environments.

## Next Checks

1. **Empirical Validation of Gaussian-Dirichlet Optimism**: Implement VAPOR with known and unknown dynamics on a gridworld environment with varying levels of uncertainty. Compare the performance of VAPOR with and without the optimism transformation to assess its effectiveness.
2. **Scalability Analysis**: Evaluate the computational efficiency and sample complexity of VAPOR on larger state spaces, such as the full Atari benchmark suite. Compare the performance of VAPOR to baseline methods as the state space grows.
3. **Robustness to Model Misspecification**: Test the robustness of VAPOR to model misspecification by introducing structured noise in the transition dynamics or reward function. Compare the performance of VAPOR to baseline methods under these conditions.