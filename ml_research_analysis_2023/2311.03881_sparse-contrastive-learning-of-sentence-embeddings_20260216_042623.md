---
ver: rpa2
title: Sparse Contrastive Learning of Sentence Embeddings
arxiv_id: '2311.03881'
source_url: https://arxiv.org/abs/2311.03881
tags:
- conference
- sentence
- proceedings
- pages
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SparseCSE, a method for improving sentence
  embeddings through parameter sparsification. The core idea is to identify and remove
  detrimental parameters in the SimCSE model by measuring their contribution to alignment
  and uniformity scores.
---

# Sparse Contrastive Learning of Sentence Embeddings

## Quick Facts
- arXiv ID: 2311.03881
- Source URL: https://arxiv.org/abs/2311.03881
- Reference count: 39
- Key outcome: SparseCSE improves sentence embeddings by removing detrimental parameters through alignment-uniformity scoring, outperforming SimCSE on STS and transfer tasks.

## Executive Summary
This paper introduces SparseCSE, a method for improving sentence embeddings by identifying and removing detrimental parameters in SimCSE models. The approach uses alignment and uniformity scores to measure each parameter's contribution to embedding quality, then prunes attention heads and intermediate neurons with the lowest scores. SparseCSE demonstrates consistent performance improvements across multiple BERT and RoBERTa architectures, achieving better results on STS tasks and transfer learning tasks compared to the dense SimCSE baseline.

## Method Summary
SparseCSE builds on SimCSE by adding a sparsification step that identifies and removes harmful parameters. The method trains a SimCSE model with dropout augmentation, then calculates alignment and uniformity scores for each parameter using the STS Benchmark. Parameters with the lowest combined scores are pruned from attention heads and feed-forward networks. The remaining parameters are rewound to early pre-training values and fine-tuned to recover performance. The approach uses λ=0.5 to balance alignment and uniformity in the joint loss function.

## Key Results
- SparseCSE outperforms SimCSE on 7 STS tasks and 7 transfer tasks
- Performance improves with sparsity up to an optimal level (typically 1-20%)
- Different sparsity levels are optimal for different downstream tasks
- Alignment and uniformity scores effectively identify detrimental parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detrimental parameters exist in SimCSE models and their removal improves performance.
- Mechanism: SimCSE contains parameters that negatively impact alignment and uniformity scores; removing them enhances the embedding space quality.
- Core assumption: Some parameters in the dense model are actively harmful rather than merely redundant.
- Evidence anchors:
  - [abstract]: "prior studies have shown that dense models could contain harmful parameters that affect the model performance"
  - [section]: "we observed that model performance does not consistently decrease during pruning, instead it exhibits an upward trend when the model is less sparse"
  - [corpus]: Weak evidence - only general contrastive learning papers found, no specific pruning studies
- Break condition: If pruning uniformly reduces performance across all sparsity levels, the harmful parameter hypothesis fails.

### Mechanism 2
- Claim: Alignment and uniformity scores effectively identify detrimental parameters.
- Mechanism: Parameters with low alignment and uniformity scores contribute negatively to embedding quality and can be safely removed.
- Core assumption: The joint loss function (alignment + uniformity) provides a reliable proxy for parameter contribution to embedding quality.
- Evidence anchors:
  - [abstract]: "alignment and uniformity scores are used to measure the contribution of each parameter to the overall quality of sentence embeddings"
  - [section]: "Based on a pilot study presented in Figure 1, we observed that model performance does not consistently decrease during pruning, instead it exhibits an upward trend when the model is less sparse"
  - [corpus]: No specific evidence - need to verify this scoring method works beyond the SimCSE context
- Break condition: If high-scoring parameters are pruned instead of low-scoring ones and performance improves, the scoring method is invalid.

### Mechanism 3
- Claim: Sparsification followed by rewinding restores model performance while maintaining benefits.
- Mechanism: After pruning detrimental parameters, rewinding to early-stage weights allows the remaining parameters to re-optimize without reintroducing harmful effects.
- Core assumption: The lottery ticket hypothesis applies - there exists a subnetwork that can be trained to match or exceed the original performance.
- Evidence anchors:
  - [section]: "the remaining parameters are initialized, and the pruned model is fine-tuned to regain its performance"
  - [abstract]: "parameter sparsification is applied, where alignment and uniformity scores are used to measure the contribution of each parameter"
  - [corpus]: Weak evidence - general lottery ticket references but no specific SimCSE application
- Break condition: If rewound models cannot recover performance even with fine-tuning, the rewiring assumption fails.

## Foundational Learning

- Concept: Contrastive learning and positive/negative pairs
  - Why needed here: The paper builds on SimCSE which uses contrastive learning; understanding how positive pairs are created and how the loss function works is essential
  - Quick check question: What role does dropout play in creating positive pairs in SimCSE?

- Concept: Alignment and uniformity in embedding spaces
  - Why needed here: These are the core metrics used to identify detrimental parameters; understanding their mathematical formulation is crucial
  - Quick check question: How does the alignment loss differ from the uniformity loss in terms of what they measure?

- Concept: Lottery ticket hypothesis and pruning methods
  - Why needed here: The paper uses a pruning-then-rewinding approach similar to lottery ticket methods; understanding how these methods work is important
  - Quick check question: What is the key difference between magnitude-based pruning and score-based pruning as used in this paper?

## Architecture Onboarding

- Component map: BERT encoder → MHA blocks (with NH heads) → FFN blocks → output embeddings; pruning targets attention heads and FFN neurons
- Critical path: Training → score calculation (alignment+uniformity) → parameter ranking → pruning → rewinding → fine-tuning
- Design tradeoffs: Aggressive pruning may harm model capacity vs. maintaining more parameters may retain harmful ones
- Failure signatures: Performance degradation with increasing sparsity, poor recovery after rewinding, score instability across runs
- First 3 experiments:
  1. Run with 1% sparsity on BERT-base to verify the upward trend in performance
  2. Test different λ values (0.25, 0.5, 0.75) to observe their effect on optimal sparsity
  3. Compare pruned vs. unpruned models on STS tasks to measure alignment improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for considering parameters with the lowest alignment and uniformity scores as detrimental to model performance?
- Basis in paper: [inferred] The paper states that parameters with minimal contributions to alignment and uniformity are considered detrimental, as their removal improves model performance, but does not provide a theoretical explanation for this relationship.
- Why unresolved: The paper relies on empirical observations but lacks a theoretical framework explaining why low-scoring parameters are harmful to sentence embedding quality.
- What evidence would resolve it: A formal analysis of the relationship between parameter scores and their impact on the embedding space structure, or a mathematical proof showing why low-scoring parameters negatively affect alignment and uniformity.

### Open Question 2
- Question: How does the optimal sparsity level vary across different types of downstream tasks, and what factors influence this variation?
- Basis in paper: [explicit] The paper mentions that different tasks benefit from different sparsity levels but does not provide a comprehensive analysis of how task characteristics influence optimal sparsity.
- Why unresolved: The paper identifies task-specific optimal sparsity values but does not investigate the underlying reasons for these differences or establish patterns across task types.
- What evidence would resolve it: A systematic study comparing optimal sparsity levels across task categories (e.g., semantic similarity vs. sentiment classification) and identifying task-specific factors that influence sparsity requirements.

### Open Question 3
- Question: What is the long-term stability of SparseCSE's performance gains across different training runs and data distributions?
- Basis in paper: [explicit] The paper conducts stability analysis but does not examine performance consistency across multiple training runs or varying data distributions.
- Why unresolved: The reported results are based on single training runs, and it is unclear whether the observed improvements are consistent or subject to significant variance.
- What evidence would resolve it: Repeated experiments with different random seeds and data splits to quantify performance variance and assess the reliability of SparseCSE's improvements.

## Limitations

- The harmful parameter hypothesis lacks rigorous ablation studies to prove causal harm
- The alignment-uniformity scoring method is only validated within the SimCSE framework
- The study focuses exclusively on BERT and RoBERTa architectures, limiting generalizability

## Confidence

**High Confidence:** The core experimental framework and evaluation methodology are sound, with reproducible results on established STS benchmarks.

**Medium Confidence:** The sparsity-performance relationship and optimal pruning levels are empirically validated but may be architecture-specific.

**Low Confidence:** The theoretical mechanisms explaining why harmful parameters exist and why the scoring method identifies them correctly.

## Next Checks

1. **Ablation Study on Parameter Harmfulness:** Implement a control experiment where random parameters (not identified as detrimental) are pruned at the same sparsity levels to determine if the observed improvements are specifically due to removing harmful parameters versus general sparsification benefits.

2. **Cross-Architecture Generalization:** Apply the alignment-uniformity scoring method to a different contrastive learning model (e.g., MoCo or BYOL adapted for sentence embeddings) to test whether the scoring approach transfers beyond SimCSE.

3. **Extreme Sparsity Exploration:** Extend the sparsity analysis to 90-99% pruning levels while maintaining the same rewinding and fine-tuning procedure to identify the theoretical limits of the harmful parameter hypothesis.