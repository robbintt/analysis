---
ver: rpa2
title: 'PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages
  in India'
arxiv_id: '2305.08828'
source_url: https://arxiv.org/abs/2305.08828
tags:
- data
- language
- dataset
- languages
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PMIndiaSum is a large-scale, multilingual, and massively parallel
  summarization corpus covering 14 languages in India. The dataset includes 76,680
  monolingual and 620,336 cross-lingual document-headline pairs across 196 language
  directions.
---

# PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India

## Quick Facts
- arXiv ID: 2305.08828
- Source URL: https://arxiv.org/abs/2305.08828
- Authors: 
- Reference count: 18
- Primary result: PMIndiaSum provides 76,680 monolingual and 620,336 cross-lingual document-headline pairs across 196 language directions for 14 Indian languages.

## Executive Summary
PMIndiaSum is a large-scale, multilingual summarization corpus covering 14 languages in India. Constructed from a governmental news website, it contains 76,680 monolingual and 620,336 cross-lingual document-headline pairs across 196 language directions. The corpus addresses the scarcity of high-quality multilingual summarization data for under-resourced Indian languages through extensive data cleaning and quality assurance processes. Experiments using various summarization paradigms demonstrate the dataset's effectiveness in enabling summarization between Indian languages.

## Method Summary
The corpus was constructed by scraping articles from a governmental news website that publishes content in multiple Indian languages. Each article is paired with its headline in the same language, and articles covering the same news event are aligned across languages via HTML pointers. The dataset underwent extensive cleaning including language filtering, length-based filtering, deduplication, and quality assurance using cross-lingual similarity metrics. Models including IndicBART and mBART-50 were fine-tuned on the dataset with multilingual training data mixing all language pairs.

## Key Results
- PMIndiaSum contains 76,680 monolingual and 620,336 cross-lingual document-headline pairs across 196 language directions
- Cross-lingual LaBSE similarity scores for summaries are 0.86, indicating strong semantic overlap
- Multilingual fine-tuning improves cross-lingual summarization performance, with mBART outperforming separate monolingual models in 11 out of 12 cross-lingual directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual document-headline alignment quality is high enough for effective training.
- Mechanism: The corpus derives parallelism from a single governmental news source publishing identical articles in multiple languages. This ensures semantic alignment between documents and headlines across languages.
- Core assumption: Headlines are professionally written summaries and carry consistent information across languages.
- Evidence anchors:
  - [abstract] "The website’s multilingualism enables cross-lingual document-headline alignment with high conﬁdence in quality."
  - [section 2.4] "Technically, every document is paired with summaries in other languages from the same article, and vice versa. Matching is done via the default English pointer in HTML."
  - [corpus] Cross-lingual LaBSE similarity scores for summaries are 0.86, indicating strong semantic overlap.

### Mechanism 2
- Claim: Multilingual fine-tuning improves cross-lingual summarization performance.
- Mechanism: By mixing data from all 196 language directions, the model learns shared representations and transfer patterns between languages, reducing overfitting to any single language pair.
- Core assumption: Language pairs share structural and semantic patterns that can be exploited across the multilingual setting.
- Evidence anchors:
  - [abstract] "Experimental results conﬁrm the crucial role of our data in aiding the summarization of Indian texts."
  - [section 3.4.3] "for 11 out of 12 cross-lingual directions in Table 7, multilingual mBART surpasses separate mBART models ﬁne-tuned for each direction."
  - [corpus] The dataset contains 196 language directions, enabling extensive multilingual training.

### Mechanism 3
- Claim: Headline-as-summary is more abstractive and appropriate than first-sentence summarization.
- Mechanism: Headlines are concise, single-sentence summaries that force the model to abstract key information rather than copying surface text, improving generalization.
- Core assumption: Headlines better capture the essence of articles compared to the first sentence, which may be part of a longer narrative.
- Evidence anchors:
  - [abstract] "In accordance with established methods (Napoles et al., 2012; Rush et al., 2015, inter alia), we use a news article as the document and its headline as the summary."
  - [section 2.6] Human evaluation shows headlines are consistently judged as summaries, whereas first sentences are not.
  - [corpus] All document-summary pairs in the corpus use headlines, enforcing abstractive summarization.

## Foundational Learning

- Concept: Cross-lingual data alignment
  - Why needed here: To ensure that training signals are consistent across languages and that models can learn true cross-lingual transfer rather than noise.
  - Quick check question: What similarity metric does the paper use to verify cross-lingual alignment quality?

- Concept: Multilingual fine-tuning dynamics
  - Why needed here: To understand how mixing many language pairs affects model performance and when it helps vs hurts.
  - Quick check question: In the paper, which model (IndicBART or mBART) benefits more from multilingual fine-tuning for cross-lingual tasks?

- Concept: Abstractive vs extractive summarization
  - Why needed here: To design models that generalize beyond copying text and capture semantic meaning.
  - Quick check question: What evidence does the paper provide that headlines are more abstractive than first sentences?

## Architecture Onboarding

- Component map: web crawler -> HTML parser -> language filtering -> deduplication -> length/prefix filtering -> train/val/test split -> PLM fine-tuning -> ROUGE/BLEU evaluation
- Critical path: Clean, aligned data -> fine-tune PLM -> evaluate cross-lingual performance -> iterate with multilingual mixing
- Design tradeoffs:
  - Single vs separate models: Multilingual saves parameters but may hurt monolingual precision
  - Translation order: Summarization-then-translation vs translation-then-summarization; choice depends on language pair resource availability
  - Data filtering strictness: Aggressive filtering removes noise but risks losing rare but valid code-mixed samples
- Failure signatures:
  - Low cross-lingual LaBSE scores -> alignment problems
  - High redundancy scores -> extractive summaries, not abstractive
  - Zero-shot cross-lingual results near zero -> need true parallel data
- First 3 experiments:
  1. Train monolingual IndicBART on high-resource pairs (hi-en) and measure ROUGE
  2. Train multilingual mBART on all pairs and compare cross-lingual ROUGE to monolingual
  3. Run summarization-then-translation vs translation-then-summarization for a low-resource pair (gu-te) and compare metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of current multilingual summarization approaches for Indian languages, and how can they be addressed?
- Basis in paper: [explicit] The paper discusses limitations of existing summarization datasets for Indian languages and presents PMIndiaSum as a solution. It also compares performance of different approaches (fine-tuning, prompting, translate-and-summarize).
- Why unresolved: While the paper demonstrates the effectiveness of PMIndiaSum, it doesn't deeply analyze the fundamental limitations of current multilingual summarization approaches or propose specific solutions beyond providing better data.
- What evidence would resolve it: Comparative analysis of various multilingual summarization architectures on PMIndiaSum, identifying specific failure modes and proposing architectural or training innovations to address them.

### Open Question 2
- Question: How does the presence of code-mixing in Indian languages affect summarization performance, and what strategies can be employed to handle it?
- Basis in paper: [explicit] The paper mentions that a large number of samples removed during preprocessing, especially from Bengali and English, are code-mixed data.
- Why unresolved: The paper acknowledges the existence of code-mixing but doesn't explore its impact on summarization quality or propose methods to handle it.
- What evidence would resolve it: Experiments comparing summarization performance on code-mixed vs. monolingual data, and development of code-mixing detection and handling techniques for summarization.

### Open Question 3
- Question: How does the political bias in governmental news articles affect the generalizability of summarization models trained on PMIndiaSum to other domains?
- Basis in paper: [explicit] The paper acknowledges that the articles are scraped from a governmental website, leading to a domain bias towards political news.
- Why unresolved: The paper doesn't explore the impact of this domain bias on model performance when applied to other domains.
- What evidence would resolve it: Experiments training summarization models on PMIndiaSum and evaluating them on news articles from different domains (e.g., sports, entertainment, technology) to measure performance degradation.

## Limitations
- Cross-lingual alignment quality relies on HTML pointers from a single governmental source, which may not generalize to other domains
- Data filtering thresholds and their impact on downstream performance are not fully explored
- Evaluation focuses on ROUGE and BLEU metrics, which may not fully capture abstractive quality or semantic equivalence across languages

## Confidence

**High confidence**: The dataset construction methodology and basic quality assurance steps are well-documented and reproducible. The multilingual training approach using mBART-50 is standard practice in the field.

**Medium confidence**: Claims about cross-lingual transfer benefits are supported by experimental results but may be sensitive to language pair similarity and data distribution. The superiority of headline-based summarization over first-sentence approaches is demonstrated through human evaluation but could vary across different news domains.

**Low confidence**: The specific performance gains from multilingual fine-tuning versus monolingual training for individual language pairs are not thoroughly analyzed. The paper does not explore how model performance scales with training data size for different resource levels.

## Next Checks

1. **Cross-lingual alignment validation**: Conduct human evaluation of a random sample of cross-lingual document-headline pairs to verify semantic alignment quality beyond automated similarity scores. Focus on low-resource language pairs where automated metrics may be less reliable.

2. **Ablation study on filtering thresholds**: Systematically vary the length-based filtering and similarity thresholds to quantify their impact on model performance. Identify the optimal balance between noise reduction and data retention for different resource levels.

3. **Cross-lingual LaBSE threshold validation**: Test the LaBSE similarity threshold of 0.8 used for filtering by analyzing model performance on pairs just above and below this threshold. Determine if this threshold is optimal across all language pairs or if it should be adjusted per language family.