---
ver: rpa2
title: 'SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic
  Bandits'
arxiv_id: '2301.12357'
source_url: https://arxiv.org/abs/2301.12357
tags:
- policy
- where
- design
- follows
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies optimal data collection for policy evaluation
  in linear bandits with heteroscedastic noise. It proposes a new optimal design problem,
  PE-Optimal design, whose solution minimizes the mean squared error of a policy-weighted
  least squares estimator.
---

# SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits

## Quick Facts
- arXiv ID: 2301.12357
- Source URL: https://arxiv.org/abs/2301.12357
- Reference count: 40
- This paper proposes a new optimal design problem, PE-Optimal design, whose solution minimizes the mean squared error of a policy-weighted least squares estimator for policy evaluation in linear bandits with heteroscedastic noise.

## Executive Summary
This paper studies optimal data collection for policy evaluation in linear bandits with heteroscedastic noise. It proposes a new optimal design problem, PE-Optimal design, whose solution minimizes the mean squared error of a policy-weighted least squares estimator. The authors derive the optimal sampling proportion and introduce SPEED, an algorithm that tracks this optimal design without knowledge of the true covariance matrix. Theoretical analysis shows SPEED's regret scales as O(d³ log(n)/n^(3/2)) compared to the optimal design, which is tighter than previous bounds in the structured bandit setting. Experiments validate that SPEED outperforms simple on-policy sampling and other design methods.

## Method Summary
The paper formulates policy evaluation in linear bandits with heteroscedastic noise as an optimal experimental design problem (PE-Optimal design) to minimize MSE of a policy-weighted least squares estimator. SPEED (Structured Policy Evaluation Experimental Design) is an algorithm that tracks this optimal design without knowledge of the true covariance matrix Σ∗. It uses a forced exploration phase to estimate Σ∗ via PCA and least squares, then computes the PE-Optimal design and follows it to collect data for policy evaluation. The algorithm balances exploration for accurate covariance estimation with exploitation of actions favored by the target policy.

## Key Results
- Proposes PE-Optimal design that minimizes MSE of policy-weighted least squares estimator in heteroscedastic linear bandits
- SPEED algorithm tracks PE-Optimal design without knowledge of true covariance matrix Σ∗
- Regret bound O(d³ log(n)/n^(3/2)) compared to optimal design, tighter than previous structured bandit results
- Experiments show SPEED outperforms on-policy sampling and other design methods on synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy-weighted least squares estimator with heteroscedastic noise reduces MSE for policy evaluation.
- Mechanism: The algorithm uses a weighted least squares estimator that downweights actions with high variance, ensuring that the policy value estimate focuses on actions with lower noise. This directly minimizes the mean squared error of the policy value estimate.
- Core assumption: The noise in rewards is heteroscedastic, meaning the variance depends on the action features, and this variance can be estimated.
- Evidence anchors:
  - [abstract]: "We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the target policy."
  - [section]: "Recall that the loss is defined as ED[(∑A a=1 w(a)⊤( ˆθn − θ∗))2] Under the linear model setting the quantity ∑A a=1 w(a)⊤( ˆθn − θ∗) is a sub-Gaussian random variable and hence the(∑A a=1 w(a)⊤( ˆθn − θ∗))2 is a sub-exponential random variable."
- Break condition: If the variance structure is misspecified or the noise is not heteroscedastic, the weighting may not reduce MSE and could potentially increase it.

### Mechanism 2
- Claim: The optimal behavior policy balances exploration of informative actions and exploitation of actions favored by the target policy.
- Mechanism: The PE-Optimal design solves for a behavior policy that minimizes the MSE of the policy value estimate. This design naturally balances the need to explore actions that reduce uncertainty about the reward parameter (especially those with high variance or informative features) and the need to focus on actions that the target policy is likely to take.
- Core assumption: The policy evaluation problem can be formulated as an optimal experimental design problem, where the goal is to minimize the weighted error ∑a∈Aπ(a)x(a)⊤(θ∗ − ˆθ)2.
- Evidence anchors:
  - [abstract]: "We then use this formulation to derive the optimal allocation of samples per action during data collection."
  - [section]: "The goal of this paper is to determine the behavior policy that produces data such that the mean squared error (MSE) of policy value estimates is minimized."
- Break condition: If the target policy is uniform or the action space is very large, the optimal design may degenerate to uniform sampling, negating the benefits of the algorithm.

### Mechanism 3
- Claim: The SPEED algorithm tracks the PE-Optimal design without knowledge of the true covariance matrix, achieving near-oracle performance.
- Mechanism: SPEED uses forced exploration to estimate the covariance matrix Σ∗ and then uses this estimate to compute the PE-Optimal design. The algorithm then follows this design to collect data and estimate the policy value. The regret of SPEED scales as O(d³ log(n)/n^(3/2)) compared to the optimal design, which is tighter than previous bounds in the structured bandit setting.
- Core assumption: The covariance matrix Σ∗ can be estimated accurately enough from a limited number of forced exploration samples.
- Evidence anchors:
  - [abstract]: "We then propose a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and analyze its regret with respect to the optimal design."
  - [section]: "Theorem 2. (Regret of Algorithm 1, informal) The regret of Algorithm 1 scales as O(d³ log(n)/n³/²) whered is the dimension of θ∗."
- Break condition: If the covariance matrix estimation is poor (e.g., due to insufficient exploration or high dimensionality), the regret bound may not hold, and the algorithm may perform worse than simpler baselines.

## Foundational Learning

- Concept: Linear bandits with heteroscedastic noise
  - Why needed here: The paper studies policy evaluation in a linear bandit setting where the reward variance depends on the action features. This is a more general setting than the standard linear bandit with homoscedastic noise.
  - Quick check question: What is the difference between homoscedastic and heteroscedastic noise in the context of linear bandits?

- Concept: Optimal experimental design
  - Why needed here: The paper formulates the problem of finding the optimal behavior policy for policy evaluation as an optimal experimental design problem. This involves minimizing the variance of the policy value estimate by carefully choosing the sampling proportions for each action.
  - Quick check question: What is the objective of the PE-Optimal design, and how does it differ from other optimality criteria like A-optimal or D-optimal design?

- Concept: Policy evaluation in bandits
  - Why needed here: The paper focuses on the problem of estimating the expected reward of a fixed target policy in a bandit environment. This is different from the standard bandit problem of finding the optimal policy.
  - Quick check question: Why is policy evaluation important in the context of online learning algorithms, and how does it differ from policy learning?

## Architecture Onboarding

- Component map: Forced exploration -> Covariance estimation -> PE-Optimal design solver -> Data collection -> Policy value estimation
- Critical path: Forced exploration → Covariance estimation → PE-Optimal design solver → Data collection → Policy value estimation
- Design tradeoffs:
  - The tradeoff between exploration and exploitation: The forced exploration phase allows for accurate covariance estimation but reduces the budget for policy evaluation.
  - The choice of covariance estimation method: Different methods may have different biases and variances, affecting the accuracy of the PE-Optimal design.
  - The complexity of the PE-Optimal design solver: More complex solvers may be more accurate but also more computationally expensive.
- Failure signatures:
  - Poor covariance estimation: If the forced exploration phase is too short or the dimensionality is too high, the estimated Σ∗ may be inaccurate, leading to suboptimal data collection.
  - Numerical instability: The PE-Optimal design solver may encounter numerical issues if the estimated Σ∗ is ill-conditioned.
  - Overfitting: If the policy-weighted least squares estimator is too complex, it may overfit to the noise in the data, leading to poor policy value estimates.
- First 3 experiments:
  1. Run SPEED on a simple synthetic dataset with known θ∗ and Σ∗ to verify that it can accurately estimate the policy value and outperforms on-policy sampling.
  2. Vary the dimension d and the number of actions A to study how the regret of SPEED scales with problem size.
  3. Compare SPEED to other baselines (e.g., on-policy sampling, A-optimal design) on a real-world dataset to demonstrate its practical effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the regret bound of SPEED compare to other existing algorithms in the linear bandit setting with heteroscedastic noise, particularly when the number of actions A is much larger than the feature dimension d?
- Basis in paper: [inferred] The paper states that the regret of SPEED scales as O(d³ log(n)/n^(3/2)) and claims it is tighter than previous bounds in the structured bandit setting. It also mentions that for d³ < A, the bound is tighter than Carpentier & Munos (2011), but it does not provide a direct comparison with other algorithms when A >> d.
- Why unresolved: The paper does not provide a direct comparison with other algorithms in the case where A >> d, and it is unclear how the regret bound of SPEED scales in this regime.
- What evidence would resolve it: Empirical results comparing the performance of SPEED with other algorithms in settings where A >> d, or a theoretical analysis showing how the regret bound of SPEED scales in this regime.

### Open Question 2
- Question: What is the impact of the choice of the exploration factor Γ on the performance of SPEED, and how should it be tuned in practice?
- Basis in paper: [explicit] The paper states that SPEED uses Γ = √n for the forced exploration phase, but it does not provide any analysis or justification for this choice. It also mentions that the choice of Γ impacts the concentration of the estimated covariance matrix.
- Why unresolved: The paper does not provide any theoretical or empirical analysis of the impact of the choice of Γ on the performance of SPEED, and it is unclear how to tune this parameter in practice.
- What evidence would resolve it: Empirical results showing the impact of different choices of Γ on the performance of SPEED, or a theoretical analysis providing guidance on how to choose Γ.

### Open Question 3
- Question: How does the performance of SPEED scale with the number of actions A, and is there a point where the algorithm becomes impractical due to computational or memory constraints?
- Basis in paper: [inferred] The paper states that the regret bound of SPEED scales with d³, but it does not provide any analysis of how the algorithm scales with the number of actions A. It also mentions that the algorithm uses PCA for covariance estimation, which may become computationally expensive for large A.
- Why unresolved: The paper does not provide any theoretical or empirical analysis of how the performance of SPEED scales with the number of actions A, and it is unclear whether the algorithm becomes impractical for large A.
- What evidence would resolve it: Empirical results showing the performance of SPEED for different values of A, or a theoretical analysis providing guidance on the scalability of the algorithm.

## Limitations
- Heavy reliance on accurate covariance matrix estimation, which may be challenging in high-dimensional settings
- Forced exploration phase may be impractical for very large action spaces
- Numerical stability of solving PE-Optimal design in high dimensions not thoroughly explored

## Confidence
- PE-Optimal design formulation and MSE guarantees: High
- SPEED algorithm regret bound: Medium (depends on covariance estimation quality)
- Practical performance claims: Medium (based on limited experiments)

## Next Checks
1. **Covariance Estimation Sensitivity**: Conduct experiments systematically varying the exploration budget Γ to quantify how covariance estimation errors propagate to regret. This would validate the robustness of SPEED to imperfect Σ∗ estimates.

2. **High-Dimensional Scaling**: Test SPEED on synthetic problems with increasing dimension d (e.g., d=10, 50, 100) to empirically verify the d³ dependence in the regret bound and identify numerical stability thresholds.

3. **Alternative Noise Structures**: Evaluate SPEED's performance when the heteroscedastic noise assumption is violated (e.g., using homoscedastic or heavy-tailed noise) to test the algorithm's robustness to model misspecification.