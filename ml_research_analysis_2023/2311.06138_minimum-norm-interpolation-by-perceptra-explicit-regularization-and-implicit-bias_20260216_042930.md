---
ver: rpa2
title: 'Minimum norm interpolation by perceptra: Explicit regularization and implicit
  bias'
arxiv_id: '2311.06138'
source_url: https://arxiv.org/abs/2311.06138
tags:
- norm
- neural
- convergence
- minimum
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the behavior of neural networks with a single
  hidden layer and ReLU activation functions, particularly their interpolation properties
  and implicit bias when trained on data. The authors prove that with a weight decay
  regularization term whose coefficient decreases appropriately as the network width
  and number of data points grow, empirical risk minimizers converge to minimum norm
  interpolants.
---

# Minimum norm interpolation by perceptra: Explicit regularization and implicit bias

## Quick Facts
- arXiv ID: 2311.06138
- Source URL: https://arxiv.org/abs/2311.06138
- Reference count: 40
- Primary result: Analyzes convergence of neural networks with ReLU activation to minimum norm interpolants under specific scaling of weight decay regularization and provides evidence of implicit bias towards minimum norm solutions in common optimization algorithms.

## Executive Summary
This paper investigates the interpolation properties of shallow ReLU neural networks, focusing on how they converge to minimum norm solutions under appropriate regularization and optimization. The authors prove that with a carefully scaled weight decay regularization term, empirical risk minimizers converge to minimum norm interpolants as the network width and data size grow. They also provide numerical evidence that common optimization algorithms like SGD and Adam exhibit an implicit bias towards minimum norm solutions even without explicit regularization, with this bias being more pronounced for algorithms that preserve Euclidean symmetries.

## Method Summary
The paper analyzes shallow ReLU networks with a single hidden layer, parameterized by weights (a,W,b), trained on sub-Gaussian data distributions. The method involves theoretical analysis of convergence to minimum norm interpolants under weight decay regularization with a coefficient λ that decreases at a specific rate as the network width m and number of data points n grow. Numerical experiments compare the performance of different optimization algorithms (SGD, Adam, L-BFGS) on synthetic datasets generated from sub-Gaussian distributions, measuring their tendency to converge to known minimum norm interpolants. The analysis uses tools from Γ-convergence, Rademacher complexity, and concentration inequalities to establish theoretical guarantees.

## Key Results
- Under specific scaling conditions (λ + 1/m → 0 and 1/(λm) + log n/(λ√n) → 0), empirical risk minimizers converge to minimum norm interpolants as m,n → ∞
- Common optimization algorithms like SGD and Adam exhibit an implicit bias towards minimum norm solutions even without explicit regularization
- This implicit bias is stronger in algorithms that preserve Euclidean symmetries (SGD) compared to coordinate-wise methods (Adam)
- Minimum norm interpolation is associated with favorable generalization properties in the Barron space semi-norm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When the number of parameters m and data points n grow while the regularization coefficient λ shrinks at a specific rate, empirical risk minimizers converge to minimum norm interpolants in the infinite parameter limit.
- Mechanism: The regularized empirical risk functional includes a weight decay term whose coefficient λ vanishes as m and n grow. This balances the bias-variance tradeoff by penalizing large weights but not eliminating them entirely, guiding the optimizer toward functions with minimal Barron norm while still fitting the data.
- Core assumption: The scaling conditions λ + 1/m → 0 and 1/(λm) + log n/(λ√n) → 0 hold as n → ∞.
- Evidence anchors:
  - [abstract] "empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow."
  - [section] Theorem 2.1 states the conditions under which convergence to minimum norm interpolants occurs.
  - [corpus] No direct evidence in corpus, but related work on minimum norm interpolation exists.
- Break condition: If the scaling conditions are violated, such as if λ decreases too quickly or too slowly, the convergence to minimum norm interpolants may not occur.

### Mechanism 2
- Claim: Common optimization algorithms like SGD and Adam exhibit an implicit bias towards minimum norm solutions even without explicit regularization.
- Mechanism: The optimization dynamics of these algorithms, particularly their preservation of Euclidean symmetries, naturally guide the solution toward functions with smaller norms in the parameter space, even in the absence of explicit regularization terms.
- Core assumption: The optimization algorithm's update rules respect the underlying geometry of the parameter space and do not introduce biases that favor larger norms.
- Evidence anchors:
  - [abstract] "numerical evidence that common optimization algorithms like SGD and Adam exhibit an implicit bias towards minimum norm solutions even without explicit regularization."
  - [section] Section 5 discusses the implicit bias of different optimization algorithms towards known minimum norm interpolants.
  - [corpus] Weak evidence, as corpus focuses on sparsity and explicit regularization rather than implicit bias.
- Break condition: If the optimization algorithm introduces coordinate-wise operations that break Euclidean symmetries, such as in Adam compared to SGD, the implicit bias toward minimum norm solutions may be reduced.

### Mechanism 3
- Claim: The set of minimum norm interpolants is characterized by the Barron space semi-norm, which controls generalization and stability properties.
- Mechanism: The Barron space semi-norm measures the complexity of functions representable by neural networks with a single hidden layer. Minimizing this norm while fitting the data leads to solutions that generalize well and are stable against perturbations.
- Core assumption: The Barron space semi-norm is a suitable measure of function complexity for this class of neural networks and is related to generalization bounds.
- Evidence anchors:
  - [abstract] "minimum norm interpolation, which generalizes well, is a natural outcome of training neural networks on data"
  - [section] Section 3 discusses the characterization of minimum norm interpolants in terms of the Barron space semi-norm.
  - [corpus] No direct evidence in corpus, but related work on Barron spaces and generalization exists.
- Break condition: If the Barron space semi-norm is not a good measure of complexity for the specific problem or if the function class is misspecified, the relationship between minimum norm and generalization may not hold.

## Foundational Learning

- Concept: Rademacher complexity and its role in generalization bounds
  - Why needed here: To bound the generalization gap between empirical risk and population risk, which is crucial for establishing convergence to minimum norm interpolants.
  - Quick check question: How does the Rademacher complexity of a function class relate to its generalization ability, and what factors influence it?

- Concept: Γ-convergence and its application to the convergence of minimizers
  - Why needed here: To establish that minimizers of approximating functionals (empirical risk minimizers) converge to minimizers of the limiting functional (minimum norm interpolants) as the number of parameters and data points grows.
  - Quick check question: What are the key conditions for Γ-convergence, and how does it differ from pointwise or uniform convergence of functionals?

- Concept: Sub-Gaussian distributions and concentration inequalities
  - Why needed here: To handle the unbounded nature of data and loss functions, and to establish concentration results needed for the generalization bounds and convergence analysis.
  - Quick check question: What are the key properties of sub-Gaussian distributions, and how do they relate to concentration inequalities like Hoeffding's inequality?

## Architecture Onboarding

- Component map:
  Data generation -> Sub-Gaussian distribution on Rd -> Model class -> Shallow ReLU networks with single hidden layer -> Regularization -> Weight decay with coefficient λ -> Optimization -> SGD, Adam, or other common optimizers -> Analysis -> Rademacher complexity bounds, Γ-convergence, concentration inequalities

- Critical path:
  1. Generate data from a sub-Gaussian distribution
  2. Train a shallow ReLU network with weight decay regularization
  3. Analyze the convergence of empirical risk minimizers to minimum norm interpolants using the provided theoretical framework
  4. Study the implicit bias of different optimization algorithms towards minimum norm solutions

- Design tradeoffs:
  - Width vs. depth: The analysis focuses on shallow networks; deeper networks may exhibit different behavior
  - Explicit vs. implicit regularization: Both are considered, with explicit regularization providing more control over the solution
  - Data distribution: The analysis assumes sub-Gaussian data; other distributions may require different techniques

- Failure signatures:
  - If the scaling conditions on λ, m, and n are not met, convergence to minimum norm interpolants may fail
  - If the optimization algorithm introduces significant biases or breaks symmetries, the implicit bias toward minimum norm solutions may be reduced
  - If the data distribution is not sub-Gaussian, the concentration inequalities and generalization bounds may not hold

- First 3 experiments:
  1. Train a shallow ReLU network with varying widths m and regularization coefficients λ on synthetic data from a sub-Gaussian distribution. Analyze the convergence of the empirical risk minimizer to a minimum norm interpolant as m and n grow.
  2. Compare the implicit bias of SGD and Adam on the same synthetic data, with and without explicit regularization. Study how the choice of optimizer affects the final solution's norm and generalization ability.
  3. Repeat the experiments on real-world datasets with sub-Gaussian-like properties. Investigate how the theoretical results translate to practical settings and what factors may influence the convergence to minimum norm interpolants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions do minimum norm interpolants in Barron spaces have favorable generalization properties beyond the bounds provided by the Rademacher complexity analysis?
- Basis in paper: [explicit] The paper discusses how minimum norm interpolants are associated with favorable generalization properties, but the proof relies on Rademacher complexity bounds.
- Why unresolved: Rademacher complexity provides upper bounds on generalization error, but does not establish whether these bounds are tight or if minimum norm interpolants are truly optimal in terms of generalization.
- What evidence would resolve it: Empirical studies comparing generalization performance of minimum norm interpolants to other solutions with similar empirical risk, or theoretical proofs establishing optimality beyond Rademacher complexity bounds.

### Open Question 2
- Question: How does the implicit bias towards minimum norm solutions change when training neural networks with different activation functions (e.g., leaky ReLU, sigmoid) or architectures (e.g., deeper networks)?
- Basis in paper: [inferred] The paper focuses on ReLU networks with a single hidden layer, but mentions that deeper networks exhibit different behavior without weight decay regularization.
- Why unresolved: The theoretical and empirical analysis is limited to ReLU networks with one hidden layer, leaving open the question of how implicit bias generalizes to other architectures and activation functions.
- What evidence would resolve it: Comparative studies of implicit bias in networks with different architectures and activation functions, ideally with theoretical analysis where possible.

### Open Question 3
- Question: Can the explicit regularization coefficient λ be eliminated entirely by using an initialization scheme that naturally leads to minimum norm solutions?
- Basis in paper: [explicit] The paper discusses the impact of initialization scale and weight decay regularization on the implicit bias of optimization algorithms.
- Why unresolved: While the paper shows that initialization and regularization both influence the implicit bias, it does not explore whether a specific initialization scheme could replace explicit regularization entirely.
- What evidence would resolve it: Empirical studies varying initialization schemes while keeping λ = 0, to determine if any initialization leads to minimum norm solutions without explicit regularization.

### Open Question 4
- Question: How does the implicit bias towards minimum norm solutions change when using different optimization algorithms (e.g., SGD with momentum, Adam) or hyperparameters (e.g., learning rate, batch size)?
- Basis in paper: [explicit] The paper compares the implicit bias of SGD and Adam, noting differences in their tendency to select minimum norm solutions.
- Why unresolved: While the paper provides initial comparisons, it does not perform a comprehensive study of how different optimization algorithms and hyperparameters affect implicit bias.
- What evidence would resolve it: Systematic empirical studies varying optimization algorithms and hyperparameters, quantifying their impact on implicit bias towards minimum norm solutions.

## Limitations

- The theoretical analysis relies on infinite-width limits and precise scaling conditions for the regularization coefficient, which may not hold in practical finite-width settings.
- The numerical experiments are limited to specific synthetic datasets and may not fully capture the behavior on real-world data with different properties.
- The extension of these results to deeper networks or different activation functions is speculative and not rigorously established.

## Confidence

- **High Confidence**: The theoretical framework for minimum norm interpolation in the infinite-width limit is well-established, and the scaling conditions for the regularization coefficient are clearly stated. The relationship between Barron space semi-norms and generalization is supported by existing literature.
- **Medium Confidence**: The empirical evidence for implicit bias in optimization algorithms like SGD and Adam is suggestive but based on limited experiments. The distinction between algorithms that preserve Euclidean symmetries (SGD) and those that don't (Adam) needs more rigorous testing across diverse datasets.
- **Low Confidence**: The extension of these results to deeper networks or different activation functions is speculative. The practical relevance of the infinite-width limit and precise scaling conditions for real-world applications remains unclear.

## Next Checks

1. **Finite-Width Validation**: Conduct experiments on shallow ReLU networks with varying finite widths (m = 100, 1000, 10000) to empirically verify how quickly the convergence to minimum norm interpolants occurs and whether the scaling conditions for λ remain critical in practical settings.

2. **Algorithm Sensitivity Analysis**: Systematically compare the implicit bias of SGD, Adam, and other optimizers (e.g., RMSprop, AdaGrad) across multiple synthetic and real datasets with varying degrees of symmetry and data distributions to quantify the relationship between symmetry preservation and minimum norm bias.

3. **Robustness to Data Distribution**: Test the theoretical predictions on non-sub-Gaussian data distributions (e.g., heavy-tailed distributions, multimodal distributions) to assess the sensitivity of the convergence to minimum norm interpolants and the validity of the concentration inequalities used in the proofs.