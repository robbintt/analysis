---
ver: rpa2
title: 'Rethinking Compression: Reduced Order Modelling of Latent Features in Large
  Language Models'
arxiv_id: '2312.07046'
source_url: https://arxiv.org/abs/2312.07046
tags:
- compression
- llms
- arxiv
- budget
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel, training-free approach to compressing
  large language models using reduced order modeling (ROM) in the latent feature space.
  The method performs layer-wise low-rank decomposition of feature maps to identify
  principal components, then re-parameterizes weight matrices into smaller sequential
  layers.
---

# Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models

## Quick Facts
- arXiv ID: 2312.07046
- Source URL: https://arxiv.org/abs/2312.07046
- Reference count: 3
- Primary result: Training-free LLM compression achieving 80% compression with 65.0% accuracy on common sense reasoning tasks

## Executive Summary
This paper introduces a novel training-free approach to compressing large language models using reduced order modeling (ROM) in the latent feature space. The method performs layer-wise low-rank decomposition of feature maps to identify principal components, then re-parameterizes weight matrices into smaller sequential layers. Operating entirely on CPU without GPU resources, the technique can compress billion-parameter models within strict memory constraints. Evaluated on LLaMA-7B, the approach achieves 80% and 50% compression rates while maintaining strong zero-shot performance across common sense reasoning tasks, outperforming structured pruning methods both with and without fine-tuning.

## Method Summary
The method operates by computing feature maps for each layer using calibration data, then performing eigenvalue decomposition on the covariance matrix of these activations to identify principal components. Each weight matrix is re-parameterized into two low-rank matrices that are sequentially arranged, reducing the number of parameters while preserving information. The approach works layer-by-layer, using ROM outputs from previous layers as inputs for subsequent layers to maintain error awareness. The entire process runs on CPU without requiring GPU resources, making it accessible for resource-constrained environments. The compressed models maintain strong zero-shot performance on benchmark tasks while achieving significant compression ratios.

## Key Results
- Achieves 80% compression on LLaMA-7B with 65.0% average accuracy across six common sense reasoning tasks
- Maintains 74.8% accuracy at 50% compression rate
- Outperforms structured pruning baselines with and without fine-tuning
- Operates entirely on CPU without GPU requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank decomposition of feature maps captures the most informative latent dimensions
- Mechanism: Principal component analysis (PCA) identifies dominant eigenmodes that retain maximal variance
- Core assumption: Leading principal components represent task-relevant information
- Evidence anchors: [abstract] "low-rank decomposition within the feature space"; [section 2] PCA computation through eigenvalue decomposition
- Break condition: If task-relevant information is distributed across many components rather than concentrated in a few

### Mechanism 2
- Claim: Layer-wise ROM avoids compounding errors through sequential decomposition
- Mechanism: Each layer compressed independently using activations from previous layer's ROM
- Core assumption: Error introduced in earlier layers can be compensated by later layers
- Evidence anchors: [section 2.1] "ROM of previous layer generates inputs for next layer"; [abstract] "layer-wise manner"
- Break condition: If error propagation becomes too severe for later layers to compensate

### Mechanism 3
- Claim: CPU-only computation enables practical compression without GPU resources
- Mechanism: ROM operations are computationally lightweight enough for CPU execution
- Core assumption: Computational complexity of ROM is low relative to model size
- Evidence anchors: [abstract] "obviating the need for a GPU device"; [section 4] "under 10 GB of peak RAM"
- Break condition: If model size exceeds CPU memory limits or decomposition becomes computationally prohibitive

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Core mathematical operation identifying principal components for low-rank decomposition
  - Quick check question: How does PCA identify the most informative dimensions in high-dimensional data?

- Concept: Eigenvalue Decomposition
  - Why needed here: Used to compute principal components from covariance matrix of activations
  - Quick check question: What is the relationship between eigenvectors and principal components in PCA?

- Concept: Low-Rank Matrix Approximation
  - Why needed here: Reduces dimensionality of weight matrices while preserving most information
  - Quick check question: How does the Eckart-Young theorem justify low-rank approximation in matrix compression?

## Architecture Onboarding

- Component map: Input calibration dataset → Layer activation computation → Covariance matrix computation → Eigenvalue decomposition → Principal component selection → Weight matrix re-parameterization → Sequential layer compression

- Critical path: 1) Load calibration data and target model; 2) For each layer: compute activations, calculate covariance, perform eigenvalue decomposition, select top-r components, re-parameterize weights; 3) Save compressed model

- Design tradeoffs: Batch size vs. generalization; compression ratio vs. performance; CPU vs. GPU execution

- Failure signatures: Performance drops with small/representative calibration dataset; memory errors with excessive batch sizes; degraded accuracy with insufficient target rank

- First 3 experiments: 1) Verify PCA computation on synthetic activations; 2) Test layer-wise compression on single self-attention layer; 3) Validate CPU execution on small LLM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reduced order modeling approach perform when applied to other LLM architectures beyond LLaMA, such as OPT or GPT models?
- Basis in paper: [inferred] Claims architectural generality but only evaluates on LLaMA-7B
- Why unresolved: Only tested on single architecture despite claiming generality
- What evidence would resolve it: Experiments on diverse architectures like OPT, GPT-2/3, and other decoder-only models

### Open Question 2
- Question: What is the optimal trade-off between calibration dataset size and computation time for eigenvalue decomposition?
- Basis in paper: [explicit] Shows larger batches improve performance but doesn't explore size vs. time relationships
- Why unresolved: Demonstrates batch size affects performance but doesn't systematically study dataset size requirements
- What evidence would resolve it: Study varying both batch size and total calibration dataset size while measuring computation time and final model accuracy

### Open Question 3
- Question: How does layer-wise compression strategy affect model convergence and task-specific fine-tuning compared to global compression methods?
- Basis in paper: [explicit] Focuses on zero-shot performance without investigating fine-tuning effects
- Why unresolved: Paper focuses on zero-shot performance but doesn't investigate fine-tuning capabilities
- What evidence would resolve it: Fine-tuning experiments comparing layer-wise vs. globally compressed models on various downstream tasks

## Limitations
- Limited architectural validation - only tested on LLaMA-7B despite claiming architectural generality
- Narrow task evaluation - performance only validated on common sense reasoning tasks
- Computational scalability unclear - CPU execution claims lack detailed complexity analysis

## Confidence
- **Medium Confidence**: Layer-wise compression methodology is well-defined with sound mathematical framework
- **High Confidence**: CPU execution is technically feasible given PCA computational complexity
- **Low Confidence**: Performance claims require independent validation due to limited comparison scope

## Next Checks
1. Conduct ablation study varying number of principal components retained per layer and correlate with task performance across diverse benchmarks
2. Evaluate compressed model on diverse tasks including code generation, mathematical reasoning, and domain-specific applications
3. Implement method on progressively larger models (7B → 13B → 70B parameters) to measure actual CPU memory usage and execution time, verifying scalability claims