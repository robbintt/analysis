---
ver: rpa2
title: 'mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection'
arxiv_id: '2309.07880'
source_url: https://arxiv.org/abs/2309.07880
tags:
- eyeblink
- detection
- mebal2
- database
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces mEBAL2, the largest public database for eyeblink
  detection, with 21,100 labeled image sequences from 180 students and over 2.4 million
  labeled images. The database captures RGB and Near-Infrared (NIR) facial gestures
  during e-learning tasks and includes EEG data for cognitive activity and blinking
  events.
---

# mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection

## Quick Facts
- arXiv ID: 2309.07880
- Source URL: https://arxiv.org/abs/2309.07880
- Reference count: 38
- Key outcome: Introduces mEBAL2, the largest public eyeblink database with 21,100 labeled image sequences and 2.4M+ images, achieving up to 97% accuracy using multispectral CNN training

## Executive Summary
This work introduces mEBAL2, the largest public database for eyeblink detection, containing 21,100 labeled image sequences from 180 students with over 2.4 million labeled images. The database captures RGB and Near-Infrared (NIR) facial gestures during e-learning tasks and includes EEG data for cognitive activity and blinking events. A Convolutional Neural Network (CNN) architecture is proposed as a benchmark for blink detection, achieving up to 97% accuracy. The study demonstrates that combining NIR and RGB images during training enhances RGB-only eyeblink detectors, validated on the HUST-LEBW dataset showing improved generalization in challenging environments.

## Method Summary
The method involves capturing multispectral data (RGB+NIR) from 180 students during e-learning tasks, with EEG synchronization for ground truth labeling. Eye regions are detected using RetinaFace for face detection, 68-point SBR for landmarks, and DLib for alignment, then cropped to 50×50 pixels. A CNN architecture with 3 convolutional layers (32/32/64 filters, 3×3, ReLU), 3 max pooling layers, a dense layer (64 units, ReLU, 0.5 dropout), and sigmoid output is trained using different spectrum combinations. Training modes include RGB-only, NIR-only, and full spectrum (RGB + both NIR). Evaluation uses leave-one-subject-out cross-validation on mEBAL2 and validation on HUST-LEBW dataset.

## Key Results
- mEBAL2 contains 21,100 image sequences from 180 students with over 2.4 million labeled images
- CNN achieves up to 97% accuracy for blink detection on mEBAL2
- Multispectral training (RGB+NIR) improves RGB-only detector performance, validated on HUST-LEBW dataset
- mEBAL2 provides the largest public resource for advancing data-driven eyeblink detection research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multispectral (RGB+NIR) training improves RGB-only eyeblink detection performance
- Mechanism: NIR images provide complementary information about eye region structures (e.g., eyelids, contours) that are less affected by illumination changes. This additional information during training regularizes the CNN model, enabling better generalization when only RGB is available at inference
- Core assumption: The CNN architecture can effectively learn and transfer structural information from NIR to RGB domain during training
- Evidence anchors: [abstract] "We demonstrate that combining NIR and RGB images during training improves the performance of RGB eyeblink detectors", [section] "The results demonstrate that NIR images serve to improve the training process of the eyeblink detector and outperform the results even when only RGB images are available during the inference"
- Break condition: If the NIR images captured in mEBAL2 are too dissimilar from typical RGB images in structure and lighting conditions, or if the CNN architecture cannot effectively integrate the multispectral information

### Mechanism 2
- Claim: Large dataset size (2.4M+ labeled frames) enables effective training of data-driven eyeblink detectors
- Mechanism: The large number of samples provides sufficient variability in lighting, pose, distance, and occlusion scenarios, allowing the CNN to learn robust features that generalize across different conditions
- Core assumption: The dataset captures a representative distribution of real-world variations encountered in e-learning and wild environments
- Evidence anchors: [abstract] "mEBAL2 includes 21,100 image sequences from 180 different students (more than 2 million labeled images in total)", [section] "This database is the largest existing public eyeblink database"
- Break condition: If the dataset, despite its size, has significant biases or missing important variation modes, leading to overfitting on specific conditions

### Mechanism 3
- Claim: Using EEG signals for initial ground truth labeling provides more accurate blink event timestamps than video-only annotation
- Mechanism: EEG captures neural activity associated with eyeblink muscle contractions, providing a physiological signal that can be synchronized with video frames to identify precise blink start/end times before human verification
- Core assumption: EEG blink artifacts are reliable indicators of actual eyeblink events and can be accurately synchronized with video frames
- Evidence anchors: [section] "The eyeblinks were labeled using a semi-supervised approach. For that labelling we first used as candidates for ground truth the eyeblink information provided automatically by the EEG band, and then a human checked manually all the detected events", [section] "EEG measures the voltage signals produced usually by synaptic excitations... Eyeblinks introduce artifacts that can be easily recognized in EEG signals"
- Break condition: If EEG blink artifacts are noisy or ambiguous, or if synchronization between EEG and video timestamps is imprecise

## Foundational Learning

- Concept: Convolutional Neural Networks for image classification
  - Why needed here: The core eyeblink detection architecture is a CNN that classifies individual frames as open or closed eye states
  - Quick check question: What are the key architectural choices (layers, activation functions, pooling) that make this CNN suitable for 50×50 eye region images?

- Concept: Multimodal sensor fusion
  - Why needed here: The database combines RGB, NIR, and EEG data, requiring understanding how to integrate information from different sensor modalities
  - Quick check question: How does the framework handle different data types (images vs. time series) and synchronize them for joint analysis?

- Concept: Leave-one-subject-out cross-validation
  - Why needed here: The evaluation protocol uses leave-one-out validation to assess generalization across different users in the mEBAL2 database
  - Quick check question: What are the advantages and limitations of this validation strategy compared to random train/test splits for this application?

## Architecture Onboarding

- Component map: Image acquisition → ROI Detection (Face detection → Landmark detection → Face alignment → Quality assessment) → Eye cropping (50×50) → CNN classification → Post-processing → Blink detection decision
- Critical path: Image acquisition → ROI detection → CNN classification → Post-processing → Blink detection decision
- Design tradeoffs: Using single-eye architecture allows handling occlusion but requires training separate models per eye vs. using both eyes jointly which might be more robust but fails when one eye is occluded
- Failure signatures: High false positive rate suggests issues with eye state classification; high false negative rate indicates missed blink events, possibly due to short blink durations or poor ROI detection
- First 3 experiments:
  1. Train and evaluate the CNN on RGB-only images using mEBAL2 with leave-one-out validation to establish baseline performance
  2. Repeat experiment using only NIR images (both cameras) to assess performance in the NIR domain and compare with RGB results
  3. Train using full spectrum (RGB + both NIR cameras) and evaluate on RGB-only test data to verify if multispectral training improves RGB performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in illumination, head pose, and distance to the camera in mEBAL2 affect the generalization capabilities of eyeblink detectors trained on this dataset when applied to wild environments?
- Basis in paper: [explicit] The paper mentions that mEBAL2 includes variations in illumination, poses, distances between user and camera, and other naturally-occurring factors, and that the proposed approach was validated in wilder and more challenging environments like the HUST-LEBW dataset
- Why unresolved: While the paper demonstrates that the proposed approach performs well on the HUST-LEBW dataset, it does not provide a detailed analysis of how specific variations in mEBAL2 (e.g., extreme lighting conditions, extreme head poses) contribute to the generalization capabilities of the detectors
- What evidence would resolve it: A detailed ablation study on mEBAL2, systematically varying illumination, head pose, and distance to the camera, and evaluating the impact on the detector's performance on wild datasets

### Open Question 2
- Question: Can the combination of NIR and RGB images during training consistently improve the performance of RGB-only eyeblink detectors across different architectures and datasets?
- Basis in paper: [explicit] The paper demonstrates that combining NIR and RGB images during training improves the performance of RGB eyeblink detectors and outperforms the results even when only RGB images are available during inference
- Why unresolved: The paper only evaluates this approach using one specific CNN architecture (OE) and on two datasets (mEBAL2 and HUST-LEBW). It is unclear whether this finding generalizes to other architectures (e.g., RNNs, Transformers) and datasets with different characteristics
- What evidence would resolve it: Evaluating the multispectral training approach using a variety of architectures (e.g., different CNN variants, RNNs, Transformers) and datasets (e.g., MPEblink, Talking Face) to assess the consistency of the improvement

### Open Question 3
- Question: What is the optimal balance between the amount of training data and the diversity of conditions (e.g., illumination, head pose, distance) for training robust eyeblink detectors?
- Basis in paper: [inferred] The paper emphasizes the importance of having a large and diverse dataset (mEBAL2) for training data-driven eyeblink detectors and shows that training on mEBAL2 leads to better performance on wild datasets compared to smaller datasets
- Why unresolved: While the paper demonstrates the benefits of a large and diverse dataset, it does not investigate the trade-off between the amount of data and the diversity of conditions. It is unclear whether there is a point of diminishing returns where adding more data does not significantly improve performance
- What evidence would resolve it: Conducting experiments with datasets of varying sizes and diversities, and evaluating the performance of eyeblink detectors on wild datasets to identify the optimal balance between data quantity and condition diversity

## Limitations
- Dataset representativeness: While mEBAL2 is large, it may not fully capture all real-world scenarios, particularly extreme wild conditions
- Limited external validation: Evaluation primarily focuses on HUST-LEBW dataset with limited testing on other external datasets
- Implementation details unclear: Specific CNN architecture details and NIR preprocessing steps are not fully specified, impacting reproducibility

## Confidence

- **High Confidence**: The claim that mEBAL2 is the largest public eyeblink database is supported by the reported dataset size and number of labeled images
- **Medium Confidence**: The claim that multispectral training improves RGB-only eyeblink detection is demonstrated on mEBAL2 and validated on HUST-LEBW, but would benefit from testing on additional diverse datasets
- **Low Confidence**: The claim about EEG-based ground truth labeling providing more accurate blink timestamps is based on internal methodology without independent validation or comparison to alternative labeling approaches

## Next Checks

1. Test the trained model on at least two additional external eyeblink detection datasets to assess generalization beyond HUST-LEBW and mEBAL2
2. Conduct ablation studies comparing the CNN architecture's performance with and without NIR input during training, using different model architectures to verify the multispectral benefit is not architecture-specific
3. Compare the EEG-based labeling methodology against purely video-based annotation by having independent annotators label blink events and measuring agreement with the EEG-labeled ground truth