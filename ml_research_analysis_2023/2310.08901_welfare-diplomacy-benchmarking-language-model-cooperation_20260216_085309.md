---
ver: rpa2
title: 'Welfare Diplomacy: Benchmarking Language Model Cooperation'
arxiv_id: '2310.08901'
source_url: https://arxiv.org/abs/2310.08901
tags:
- units
- welfare
- players
- unit
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Welfare Diplomacy, a variant of the zero-sum
  board game Diplomacy that allows for general-sum interactions and requires cooperative
  capabilities to maximize welfare. The authors implement this variant in an open-source
  Diplomacy engine and construct zero-shot prompted language model agents to serve
  as baselines.
---

# Welfare Diplomacy: Benchmarking Language Model Cooperation

## Quick Facts
- arXiv ID: 2310.08901
- Source URL: https://arxiv.org/abs/2310.08901
- Reference count: 40
- Key outcome: Introduces Welfare Diplomacy, a general-sum variant of Diplomacy that enables clearer assessment of cooperative capabilities in language models

## Executive Summary
This paper introduces Welfare Diplomacy (WD), a variant of the zero-sum board game Diplomacy that allows for general-sum interactions and requires cooperative capabilities to maximize welfare. The authors implement this variant in an open-source Diplomacy engine and construct zero-shot prompted language model agents to serve as baselines. Their experiments find that these agents attain high social welfare in self-play but are exploitable by defectors. The authors argue that WD enables clearer assessment and stronger training incentives for cooperative capabilities compared to the original Diplomacy game.

## Method Summary
The authors implemented Welfare Diplomacy using an open-source Diplomacy engine and created zero-shot prompted language model agents without fine-tuning. They ran self-play experiments to measure Nash welfare and basic proficiency, and exploitation experiments to measure vulnerability to defectors. The evaluation framework included metrics like Nash welfare and exploitability, with agents playing 15 games each in different conditions.

## Key Results
- Language model agents achieved high social welfare in self-play but were vulnerable to defectors
- GPT-4 achieved the highest Nash welfare scores among tested models
- Basic proficiency varied significantly across models, with many performing at or below random policy levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Welfare Diplomacy creates a clear incentive structure for cooperative behavior by allowing players to gain Welfare Points (WPs) without harming others.
- Mechanism: By making the game general-sum rather than zero-sum, WD allows players to improve their own welfare without reducing others' welfare. Players can demilitarize to accumulate WPs while maintaining peaceful agreements.
- Core assumption: Players will rationally choose to demilitarize when they perceive the risk of invasion is low enough to make WP accumulation more valuable than military conquest.
- Evidence anchors:
  - [abstract]: "Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities."
  - [section]: "Unlike SD, we should expect there to be NEs that Pareto-dominate others, satisfying criterion (A) from Section 2.1."
- Break condition: If players consistently choose to maintain military forces despite the potential for WP accumulation, the incentive structure fails.

### Mechanism 2
- Claim: WD enables more effective evaluation of cooperative capabilities by creating scenarios where cooperation is both possible and beneficial.
- Mechanism: By allowing for positive-sum interactions, WD creates environments where cooperative capabilities like contract design and disagreement resolution can be meaningfully tested.
- Core assumption: The ability to measure cooperative capabilities requires environments where cooperation leads to Pareto improvements over non-cooperative outcomes.
- Evidence anchors:
  - [abstract]: "WD leads to clearer evaluations of—and stronger selection pressures for—cooperative capabilities in AI systems."
  - [section]: "Unlike SD, we should expect there to be NEs that Pareto-dominate others, satisfying criterion (A) from Section 2.1."
- Break condition: If the game consistently results in players reverting to zero-sum strategies, the evaluation capability is compromised.

### Mechanism 3
- Claim: WD provides stronger training incentives for cooperative capabilities by making cooperation more rewarding than conflict.
- Mechanism: The WP system rewards players for demilitarizing and maintaining peaceful agreements, creating a selection pressure that favors cooperative strategies over conflict-driven ones.
- Core assumption: Agents trained in WD will develop strategies that prioritize WP accumulation through cooperation rather than military conquest.
- Evidence anchors:
  - [abstract]: "Our work aims to promote societal safety by aiding researchers in developing and assessing multi-agent AI systems."
  - [section]: "We argue that WD facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities."
- Break condition: If agents consistently fail to demilitarize or maintain peaceful agreements despite the WP incentives, the training incentive is ineffective.

## Foundational Learning

- Concept: Nash equilibria and Pareto efficiency
  - Why needed here: Understanding these concepts is crucial for analyzing the strategic dynamics of WD and identifying cooperative equilibria.
  - Quick check question: Can you explain why WD allows for Pareto-improving equilibria that SD does not?

- Concept: Game theory and multi-agent systems
  - Why needed here: WD is fundamentally a game-theoretic environment, so understanding how to model and analyze multi-agent interactions is essential.
  - Quick check question: How does the introduction of WPs change the payoff structure of Diplomacy compared to the original game?

- Concept: Language model prompting and scaffolding
  - Why needed here: The baseline agents in WD are created using sophisticated prompting techniques, so understanding how to effectively prompt LMs is crucial.
  - Quick check question: What are the key components of the prompt scaffolding system used to create WDAgent?

## Architecture Onboarding

- Component map: Game engine -> Prompt scaffolding system -> Evaluation framework -> Exploitation experiments
- Critical path: 1. Initialize game state with WD rules 2. Generate prompts for each agent using the scaffolding system 3. Run game simulation with agents making decisions based on prompts 4. Evaluate agent performance using Nash welfare and exploitability metrics 5. Analyze results to identify cooperative capabilities and vulnerabilities
- Design tradeoffs: WD rules vs. SD rules: WD introduces complexity but enables clearer assessment of cooperation
- Failure signatures: Agents consistently failing to demilitarize despite WP incentives, agents being easily exploited by defectors, agents failing to communicate effectively
- First 3 experiments: 1. Run self-play games with different LM models to establish baseline performance 2. Test agent exploitability by introducing defector agents into the environment 3. Compare WD performance to SD performance to validate the cooperative benefits of WD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we distinguish between cooperation and collusion in Welfare Diplomacy?
- Basis in paper: The authors mention that future work should explore the factors that explain the variation in Nash welfare between different models, and how to develop agents that approach the optimal Nash welfare. They also state that it remains unclear how to build in cooperative capabilities while avoiding AI collusion against human overseers.
- Why unresolved: The paper does not provide a clear definition or methodology for distinguishing between cooperation and collusion. The authors acknowledge that this is an open question and a potential challenge for future research.
- What evidence would resolve it: A clear definition of cooperation and collusion in the context of Welfare Diplomacy, along with a set of criteria or metrics that can be used to differentiate between the two. Empirical evidence showing how agents can be designed to cooperate without colluding against human overseers.

### Open Question 2
- Question: How do the cooperative capabilities of language models scale with model size?
- Basis in paper: The authors mention that there is a substantial variation in Nash welfare, with many agents performing at or below the Random Policy. GPT-4 obtained the highest score, while Claude Instant 1.2 obtained the lowest. For the models that they tested, larger models (GPT-4, Claude 2) tended to achieve higher Nash welfare than smaller models (GPT-3.5, Claude Instant 1.2).
- Why unresolved: The authors hypothesize that cooperative capabilities may improve in general with model scale, but do not attempt to demonstrate this here given the lack of basic proficiency for their less-capable models and the prohibitive computational costs that verifying this hypothesis would require.
- What evidence would resolve it: A systematic study comparing the cooperative capabilities of language models of different sizes, using a consistent evaluation methodology and a range of cooperative tasks. The study should control for other factors that may influence performance, such as fine-tuning or prompt engineering.

### Open Question 3
- Question: How well do insights from the study of Diplomacy transfer to real-world settings?
- Basis in paper: The authors state that it is unclear how well insights from the study of Diplomacy transfer to real-world settings. Although they believe that WD is an improvement upon existing environments in this regard, they hope that it is a step towards even more realistic and diverse evaluations for cooperative AI.
- Why unresolved: The paper does not provide any empirical evidence or theoretical arguments to support the claim that insights from Diplomacy research can be applied to real-world settings. The authors acknowledge that this is an open question and a potential limitation of their work.
- What evidence would resolve it: A study that demonstrates how insights from Diplomacy research can be applied to solve real-world problems that require cooperation. The study should identify the key transferable insights and show how they can be implemented in practice.

## Limitations

- Small sample size (15 games per condition) may not capture full strategic complexity
- Zero-shot prompting without fine-tuning limits development of sophisticated cooperative strategies
- Results may not generalize to other cooperative scenarios beyond Diplomacy

## Confidence

- High Confidence: The theoretical foundation linking WD to improved cooperative benchmarking (Mechanism 1) is well-supported by game theory principles and clear design choices.
- Medium Confidence: Claims about WD's effectiveness in evaluating cooperative capabilities (Mechanism 2) are supported by experimental results, though the small sample size limits generalizability.
- Medium Confidence: The assertion that WD provides stronger training incentives for cooperation (Mechanism 3) is theoretically sound but lacks empirical validation through training experiments.

## Next Checks

1. Conduct larger-scale experiments (minimum 50 games per condition) to establish more robust statistical significance for cooperation metrics and explore strategic diversity.
2. Implement a controlled training study comparing models fine-tuned on WD versus standard Diplomacy to directly test the training incentive hypothesis.
3. Design transfer experiments to evaluate whether WD-trained models demonstrate improved cooperation in novel multi-agent scenarios outside the Diplomacy framework.