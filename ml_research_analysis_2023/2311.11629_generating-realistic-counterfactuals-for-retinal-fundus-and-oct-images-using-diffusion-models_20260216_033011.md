---
ver: rpa2
title: Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using
  Diffusion Models
arxiv_id: '2311.11629'
source_url: https://arxiv.org/abs/2311.11629
tags:
- counterfactuals
- diffusion
- images
- healthy
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that diffusion models combined with adversarially
  robust classifiers can generate highly realistic counterfactual retinal images for
  both fundus and OCT modalities. The method produces minimal, realistic, and high-confidence
  changes to input images such that a classifier alters its decision to a desired
  target class.
---

# Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models

## Quick Facts
- **arXiv ID**: 2311.11629
- **Source URL**: https://arxiv.org/abs/2311.11629
- **Reference count**: 34
- **Primary result**: Diffusion models combined with adversarially robust classifiers generate highly realistic counterfactual retinal images that are nearly indistinguishable from real images in expert evaluations

## Executive Summary
This paper presents a novel approach for generating realistic counterfactual explanations for retinal disease classification using diffusion models guided by adversarially robust classifiers. The method successfully transforms healthy retinal images into realistic disease presentations and vice versa, with particular success in binary classification tasks. Expert user studies demonstrate that the generated counterfactuals are significantly more realistic than previous methods and nearly indistinguishable from real images, suggesting strong potential for clinical applications in medical imaging interpretability.

## Method Summary
The approach uses a pretrained unconditional diffusion model combined with adversarially robust classifiers (trained with TRADES loss) to generate counterfactual retinal images. The key innovation involves projecting gradients from the robust classifier onto a cone centered around plain classifier gradients, balancing realistic generative properties with classification accuracy. Counterfactuals are generated by starting the reverse diffusion process from T/2 instead of the fully noisy image, preserving structural details while allowing meaningful modifications. The method was evaluated on fundus images from EyePacs dataset and OCT images from Kermany dataset, with user studies involving ophthalmologists and AI experts to assess realism.

## Key Results
- User studies show counterfactuals are significantly more realistic than previous methods, with nearly chance-level (33%) detection rates
- Binary classification tasks (healthy→disease and disease→healthy) achieved high success rates with realistic counterfactuals
- OCT modality showed better performance than fundus for multi-class transformations, particularly for moderate disease stages
- Generated counterfactuals successfully captured meaningful disease features like exudates, hemorrhages, and microaneurysms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diffusion models combined with adversarially robust classifiers generate realistic counterfactuals because robust gradients encode meaningful disease features
- **Mechanism**: The robust classifier's gradients are perceptually aligned with actual disease features, steering the diffusion model toward realistic changes rather than imperceptible noise
- **Core assumption**: Adversarially robust models learn feature representations that are both discriminative and human-perceptible
- **Evidence anchors**: User study showing nearly chance-level detection rates for counterfactuals, expert identification of realistic disease features
- **Break condition**: If robust classifier accuracy drops too low, the guidance becomes unreliable despite better gradient properties

### Mechanism 2
- **Claim**: Cone projection balances realism and classification accuracy by constraining robust gradients within the direction of plain model gradients
- **Mechanism**: Projecting robust gradients onto a cone around plain model gradients maintains high classification performance while retaining meaningful generative properties
- **Core assumption**: The gradient directions of robust and plain classifiers are sufficiently aligned for meaningful projection
- **Evidence anchors**: Successful counterfactual generation with cone projection, comparison showing robust gradients alone were less effective
- **Break condition**: If cone angle is too narrow, loses robust generative properties; if too wide, loses classification accuracy

### Mechanism 3
- **Claim**: Starting reverse diffusion from T/2 instead of T produces more realistic counterfactuals by preserving original image structure
- **Mechanism**: Beginning reconstruction at intermediate noise level maintains key structural elements while allowing meaningful modifications
- **Core assumption**: Image structure is preserved at T/2 noise level but still allows classifier-guided modifications
- **Evidence anchors**: Preservation of structural details (blood vessels, macula, optic disc) in generated counterfactuals
- **Break condition**: If T/2 is too late, original structure is lost; if too early, insufficient flexibility for meaningful changes

## Foundational Learning

- **Concept**: Diffusion models
  - **Why needed here**: They provide superior generative capabilities compared to GANs for creating realistic medical images
  - **Quick check question**: What distinguishes forward and reverse diffusion processes in this context?

- **Concept**: Adversarial robustness
  - **Why needed here**: Robust classifiers provide gradients that encode meaningful, perceptible features rather than imperceptible noise
  - **Quick check question**: How does TRADES loss differ from standard cross-entropy training?

- **Concept**: Cone projection
  - **Why needed here**: It balances the trade-off between robust generative properties and classification accuracy
  - **Quick check question**: What happens when you project a vector onto a cone around another vector?

## Architecture Onboarding

- **Component map**: Pretrained diffusion model -> Cone projection module -> Conditional sampling pipeline -> Classifier decision
- **Critical path**: Diffusion model ← (gradients from) ← Cone-projected robust gradients → Classifier decision change
- **Design tradeoffs**: Robust vs. plain classifier accuracy vs. generative capability; regularization strength vs. realism vs. minimal change requirement; starting noise level (T/2) vs. structure preservation vs. modification flexibility
- **Failure signatures**: Counterfactuals too similar to original (insufficient regularization or wrong gradient direction); counterfactuals unrealistic (wrong classifier choice or cone projection failure); classifier decision doesn't change (insufficient modification or wrong guidance signal)
- **First 3 experiments**:
  1. Generate counterfactuals using only plain classifier gradients to establish baseline
  2. Generate counterfactuals using only robust classifier gradients to test generative capability
  3. Apply cone projection with varying angles to find optimal balance between accuracy and realism

## Open Questions the Paper Calls Out

- **Open Question 1**: How do counterfactuals generated from multi-task DNNs compare to single-task counterfactuals in terms of realism and clinical utility?
  - **Basis in paper**: The paper mentions that an extension would be to generate counterfactuals from multi-task DNNs trained on several attributes simultaneously
  - **Why unresolved**: The paper does not provide any experimental results or comparisons between single-task and multi-task counterfactual generation methods
  - **What evidence would resolve it**: A study comparing the realism and clinical utility of counterfactuals generated from single-task versus multi-task DNNs trained on retinal imaging data

- **Open Question 2**: What is the impact of dataset size and diversity on the quality of counterfactuals generated by diffusion models for retinal imaging?
  - **Basis in paper**: The paper notes that supplementing the fundus image dataset with more examples from diseased classes helped to generate better disease counterfactuals, and mentions the potential of using larger datasets or robust classifiers derived from foundation models
  - **Why unresolved**: The paper does not systematically investigate the relationship between dataset size/diversity and counterfactual quality, nor does it compare the performance of diffusion models trained on different dataset sizes
  - **What evidence would resolve it**: An empirical study comparing the quality of counterfactuals generated by diffusion models trained on datasets of varying sizes and diversities for retinal imaging tasks

- **Open Question 3**: How do clinicians perceive and utilize counterfactual images in clinical decision-making compared to traditional diagnostic methods?
  - **Basis in paper**: The paper mentions the potential use of realistic counterfactuals in clinical decision support and data augmentation, and describes a user study where clinicians were asked to identify counterfactual images
  - **Why unresolved**: The user study in the paper focused on the realism of generated counterfactuals rather than their impact on clinical decision-making or diagnostic performance
  - **What evidence would resolve it**: A study investigating how clinicians incorporate counterfactual images into their diagnostic reasoning and whether their use leads to improved diagnostic accuracy or patient outcomes compared to traditional methods

## Limitations

- Limited to 5-class classification tasks, with particular difficulty in multi-class scenarios for advanced disease stages
- User study sample size is small (8-11 participants), potentially limiting generalizability
- Performance varies significantly across modalities, with OCT showing better results than fundus for complex transformations

## Confidence

- **High confidence**: The mechanism of using adversarially robust classifiers to generate meaningful counterfactuals is well-supported by the evidence, particularly for binary classification tasks
- **Medium confidence**: The cone projection method's effectiveness in balancing realism and accuracy is demonstrated but relies on specific parameter choices (α=30°) without systematic ablation
- **Low confidence**: The claim that starting from T/2 significantly improves realism lacks direct comparative evidence against other starting points

## Next Checks

1. Conduct larger-scale user studies with diverse expert panels (different institutions, experience levels) to validate realism claims across broader populations
2. Perform systematic ablation studies on cone projection parameters (angle α, regularization strength λd) to identify optimal settings and verify their necessity
3. Test counterfactual generation performance on multi-class classification tasks with more than 5 classes to assess scalability limitations