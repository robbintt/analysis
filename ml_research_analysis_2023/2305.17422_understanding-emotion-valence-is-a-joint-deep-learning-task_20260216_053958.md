---
ver: rpa2
title: Understanding Emotion Valence is a Joint Deep Learning Task
arxiv_id: '2305.17422'
source_url: https://arxiv.org/abs/2305.17422
tags:
- prediction
- valence
- task
- two-step
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the inter-dependency between valence and emotion
  carriers in personal narratives using multi-task learning with pre-trained language
  models. The authors experiment with discriminative (AlBERTo) and generative (GePpeTto)
  models in single-task, two-step, and joint prediction settings for valence and emotion
  carrier prediction tasks.
---

# Understanding Emotion Valence is a Joint Deep Learning Task

## Quick Facts
- arXiv ID: 2305.17422
- Source URL: https://arxiv.org/abs/2305.17422
- Reference count: 13
- Primary result: Joint multi-task learning improves valence and emotion carrier prediction by exploiting their natural inter-dependency

## Executive Summary
This work investigates the inter-dependency between emotion valence and emotion carriers in personal narratives using multi-task learning with pre-trained language models. The authors experiment with both discriminative (AlBERTo) and generative (GePpeTto) models across single-task, two-step, and joint prediction settings. Their findings demonstrate that joint prediction exploits the natural co-occurrence dependency between neutral valence and absence of emotion carriers, and non-neutral valence and presence of emotion carriers. The discriminative model (AlBERTo) achieves the best trade-off between the two tasks in joint prediction settings.

## Method Summary
The approach uses pre-trained language models (AlBERTo - discriminative, GePpeTto - generative) fine-tuned for valence and emotion carrier prediction tasks on Italian personal narratives. The models are evaluated in three settings: single-task learning (baseline), two-step prediction (alternating task order with teacher forcing), and joint prediction (simultaneous task prediction with shared representations). The dataset consists of 481 narratives with annotations at the functional unit level (4273 FUs), where 40% have polarity annotations and 18.5% have emotion carrier annotations. Training uses specific learning rates for each model and task combination, with evaluation based on macro F1-scores for both valence and emotion carrier prediction.

## Key Results
- Two-step prediction with ground truth oracle significantly improves performance, demonstrating the upper bound of inter-dependency exploitation
- AlBERTo (discriminative) achieves the best trade-off between valence and emotion carrier prediction tasks in joint prediction settings
- The proposed joint approach allows a single model to perform both tasks, reducing computational resources at training and inference time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves valence and EC prediction by exploiting the natural co-occurrence dependency between neutral valence and absence of ECs, and non-neutral valence and presence of ECs.
- Mechanism: When training jointly or in a two-step fashion, the model learns that the presence of EC candidates signals non-neutral valence, and vice versa. This inter-dependency provides inductive bias that helps the model generalize better than training on each task separately.
- Core assumption: The dataset contains sufficient instances where this valence-EC co-occurrence pattern holds, allowing the model to learn the mapping.
- Evidence anchors:
  - [abstract]: "The two elements of valence and EC are inter-dependant since valence represents the intensity of the experienced emotions while the ECs are the means through which emotions are expressed"
  - [section 1]: "This inter-dependency is characterised by the relation between the presence or absence of ECs and neutral or non-neutral valence"
  - [corpus]: Weak - the corpus contains only 5 related papers, none directly addressing valence-EC co-occurrence learning patterns. The paper itself is the primary evidence.
- Break condition: If the dataset is heavily imbalanced (e.g., very few neutral instances or ECs), or if the valence-EC relationship is inconsistent across narratives, the learned dependency may not generalize.

### Mechanism 2
- Claim: Two-step prediction with ground truth oracle significantly boosts performance because the first-step prediction quality directly conditions the second-step prediction.
- Mechanism: By replacing the first-step prediction with the ground truth, the model receives high-quality context for the second task, eliminating noise propagation from the first prediction. This shows the upper bound of what two-step MTL can achieve.
- Core assumption: The ground truth labels are accurate and available during training, and the model can effectively leverage them as context.
- Evidence anchors:
  - [section 3.2]: "To provide evidence of this, we experimented by replacing the first prediction with the ground truth"
  - [section 5]: "we have computed the upper bound for the two-step prediction by substituting the prediction of the first step with the corresponding ground truth"
  - [corpus]: Weak - no corpus papers directly address ground truth oracle substitution in MTL settings.
- Break condition: If the first task is too noisy or unrelated to the second, even ground truth context may not improve the second task.

### Mechanism 3
- Claim: Discriminative models (AlBERTo) achieve better trade-off between valence and EC tasks than generative models (GePpeTto) in joint prediction.
- Mechanism: Discriminative models are optimized for classification tasks with explicit loss functions for each task, while generative models are trained as language models, making them less suited for discrete classification. The joint architecture of AlBERTo allows simultaneous prediction with shared representations optimized for both tasks.
- Core assumption: The task-specific architectures and loss functions of discriminative models are better suited for the valence and EC classification tasks than generative causal language modeling.
- Evidence anchors:
  - [section 5]: "AlBERTo achieves the highest macro F1-score on the EC prediction task compared to the other settings and the generative model"
  - [section 6]: "the best trade-off between valence and EC prediction tasks is achieved by AlBERTo"
  - [section 4]: Description of discriminative architecture with feed-forward layers for classification vs generative causal language modeling.
- Break condition: If the tasks require more sequential or generative understanding, generative models might outperform discriminative ones.

## Foundational Learning

- Concept: Functional Unit (FU) as minimal span expressing dialogue act
  - Why needed here: The dataset is annotated at FU level, so understanding FU definition is critical for interpreting model inputs and outputs.
  - Quick check question: What is the minimal span of text expressing a dialogue act in this dataset?

- Concept: Multi-Task Learning (MTL) inductive transfer
  - Why needed here: The paper relies on MTL to improve generalization by leveraging shared representations between valence and EC prediction.
  - Quick check question: How does inductive transfer enhance generalization in MTL?

- Concept: Pre-trained Language Models (PLMs) fine-tuning
  - Why needed here: Both AlBERTo and GePpeTto are PLMs that need to be fine-tuned for the specific valence and EC tasks.
  - Quick check question: What are the key differences between discriminative and generative PLM architectures?

## Architecture Onboarding

- Component map:
  - AlBERTo (110M parameters, BERT-based, Italian Twitter pre-training) -> feed-forward classification heads for valence and EC prediction
  - GePpeTto (117M parameters, GPT-2 based, Italian language pre-training) -> causal language modeling for both tasks
  - Functional units -> structured prompts -> model input -> task-specific outputs

- Critical path:
  1. Load PLM with pre-trained weights
  2. Add task-specific prediction heads
  3. Prepare data in FU format with EC candidates and valence labels
  4. Fine-tune on single tasks to establish baseline
  5. Fine-tune on MTL settings (joint or two-step)
  6. Evaluate macro F1 scores for both tasks

- Design tradeoffs:
  - Discriminative vs generative: Discriminative models are better for classification accuracy, generative models may capture more contextual relationships but are harder to optimize for discrete labels
  - Joint vs two-step: Joint prediction is simpler but may not exploit task inter-dependency as effectively; two-step can leverage task ordering but is more complex
  - Language-specific: Models are pre-trained on Italian, performance may not generalize to other languages

- Failure signatures:
  - EC prediction performance much lower than valence prediction: Indicates difficulty with sparse EC annotations or imbalanced data
  - Two-step performance worse than single-task: Suggests poor task inter-dependency or noisy first-step predictions
  - Generative model underperforms discriminative: May indicate tasks are better suited for classification than generation

- First 3 experiments:
  1. Fine-tune AlBERTo on valence prediction only to verify baseline performance matches previous work
  2. Fine-tune GePpeTto on EC prediction only to establish baseline for comparison
  3. Run two-step prediction (Valence â†’ EC) with teacher forcing to test inter-dependency exploitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inter-dependency between valence and emotion carriers vary across different languages and cultures, and how can this be modeled effectively in a multilingual setting?
- Basis in paper: [explicit] The authors note that the dataset used is in Italian and the PLMs are pre-trained for the Italian language. They also mention that "The performance of the models and the results may be influenced by language-specific properties."
- Why unresolved: The study is limited to a single language (Italian), and the authors acknowledge that language-specific properties may influence the results. There is no exploration of how the inter-dependency between valence and emotion carriers might differ in other languages or cultures.
- What evidence would resolve it: Conducting similar experiments with datasets in different languages and cultures, and comparing the results to identify patterns and differences in the inter-dependency between valence and emotion carriers across languages.

### Open Question 2
- Question: How does the performance of the proposed multi-task learning approach scale with the size of the dataset, and what is the minimum dataset size required to achieve significant improvements in both valence and emotion carrier prediction tasks?
- Basis in paper: [inferred] The authors use a dataset of 481 narratives from 45 subjects. While they achieve promising results, they do not explore how the performance of their approach scales with dataset size or what the minimum required size might be.
- Why unresolved: The study uses a relatively small dataset, and there is no analysis of how the performance of the multi-task learning approach would change with larger or smaller datasets.
- What evidence would resolve it: Conducting experiments with datasets of varying sizes and analyzing the performance of the multi-task learning approach to determine the relationship between dataset size and model performance.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art methods for valence and emotion carrier prediction, such as transformer-based architectures or graph neural networks, in terms of accuracy, computational efficiency, and interpretability?
- Basis in paper: [inferred] The authors compare the performance of their approach to single-task models but do not compare it to other state-of-the-art methods for valence and emotion carrier prediction.
- Why unresolved: The study focuses on comparing the proposed approach to single-task models but does not explore how it fares against other advanced methods in the field.
- What evidence would resolve it: Conducting experiments to compare the proposed approach with other state-of-the-art methods, such as transformer-based architectures or graph neural networks, in terms of accuracy, computational efficiency, and interpretability.

## Limitations
- The mechanism relies on the assumption that neutral valence and EC absence, as well as non-neutral valence and EC presence, co-occur sufficiently in the dataset
- The ground truth oracle experiments may not reflect realistic deployment scenarios where predictions are noisy
- The language-specific nature of AlBERTo and GePpeTto (trained on Italian) limits generalizability to other languages and domains

## Confidence
- High Confidence: The observation that discriminative models outperform generative models for classification tasks in this setting is well-supported by the experimental results and aligns with established PLM literature.
- Medium Confidence: The mechanism of task inter-dependency exploitation is theoretically sound but requires empirical validation across diverse datasets to confirm generalizability.
- Low Confidence: The assumption that two-step prediction with ground truth oracle represents the true upper bound for MTL performance needs verification, as it may overestimate realistic gains.

## Next Checks
1. Analyze the corpus to quantify the actual co-occurrence frequency of neutral valence with EC absence and non-neutral valence with EC presence across different narrative types.
2. Replicate the experiments using multilingual PLMs (e.g., mBERT or XLM-R) on English or other language datasets to assess whether the task inter-dependency mechanism transfers across languages.
3. Evaluate the two-step prediction performance when using predicted labels instead of ground truth in the first step, varying the quality of predictions to understand the break point where performance degrades significantly.