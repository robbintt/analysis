---
ver: rpa2
title: The Ups and Downs of Large Language Model Inference with Vocabulary Trimming
  by Language Heuristics
arxiv_id: '2311.09709'
source_url: https://arxiv.org/abs/2311.09709
tags:
- vocabulary
- language
- shortlisting
- unicode
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines vocabulary trimming (VT) as a method to improve
  the efficiency of large language model (LLM) inference. By restricting the embedding
  entries to the language of interest, VT aims to reduce both memory usage and generation
  time.
---

# The Ups and Downs of Large Language Model Inference with Vocabulary Trimming by Language Heuristics

## Quick Facts
- arXiv ID: 2311.09709
- Source URL: https://arxiv.org/abs/2311.09709
- Reference count: 6
- Primary result: Vocabulary trimming can reduce memory usage by nearly 50% and improve generation speed by up to 25% for smaller multilingual LLMs on CPU

## Executive Summary
This study explores vocabulary trimming (VT) as an efficiency optimization for large language model inference by restricting embedding entries to language-specific subsets. The research evaluates two straightforward heuristics—Unicode-based script filtering and corpus-based selection—across various LLM families including BLOOM and LLaMA. Results demonstrate significant memory reductions and speed improvements for smaller models, particularly on CPU platforms, while also revealing limitations including inconsistent performance across languages and diminishing returns for larger models. The findings highlight VT's potential for enhancing efficiency while pointing to important caveats for practical deployment.

## Method Summary
The method applies vocabulary trimming by precomputing sub-vocabularies using either Unicode script filtering (selecting tokens based on language-specific Unicode ranges) or corpus-based selection (tokenizing representative corpora to identify relevant vocabulary). These sub-vocabularies are then used to slice the model's embedding and output layers, effectively reducing the vocabulary size before inference. The approach is implemented as a pre-processing step that modifies the model architecture, and evaluation is conducted through batched inference comparing memory usage, generation speed, and output quality against full-vocabulary baselines.

## Key Results
- Vocabulary trimming reduced memory usage of smaller models by nearly 50% on CPU
- Generation speed improved by up to 25% for smaller models on CPU
- Performance benefits were highly dependent on model size, with diminishing returns for larger models (>7B parameters)
- GPU acceleration showed minimal to no benefit from vocabulary trimming due to kernel optimizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing vocabulary size during LLM inference decreases the computational cost of the output layer's softmax operation.
- Mechanism: The output layer's matrix multiplication D x |V| is the most expensive operation during inference. By trimming the vocabulary to a smaller subset V', the matrix multiplication becomes D x |V'|, reducing computational complexity.
- Core assumption: The computational bottleneck is primarily in the output layer's matrix multiplication, not in earlier layers.
- Evidence anchors:
  - [abstract] "Most LLMs are Transformer-based... the output layer with hidden size D is associated with the most expensive matrix multiplication over space D × |V|"
  - [section 1] "This expensive operation leads to increased inference cost of both memory and speed given the autoregressive nature of LLM decoding."
- Break condition: If the computational bottleneck shifts to earlier layers or if the vocabulary reduction is minimal, the efficiency gains will be negligible.

### Mechanism 2
- Claim: Script-based vocabulary trimming is effective for languages with unique writing systems.
- Mechanism: By filtering vocabulary items based on Unicode ranges specific to a language's script (e.g., Chinese characters, Cyrillic), the vocabulary can be significantly reduced without affecting the quality of generation for that language.
- Core assumption: The language being generated primarily uses a unique script that can be easily identified and filtered.
- Evidence anchors:
  - [section 3.1] "Chinese is a high-resource language with a unique script... Unicode vocabulary selection would be the most effective in this case."
  - [section 4.2] "Script-based selection... we observe different trends in English and Spanish compared to Bulgarian and Chinese. For BLOOM, the sub-vocabulary size for Bulgarian and Chinese can be reduced to only 10-20%."
- Break condition: If the language shares scripts with others or uses code-mixing, the vocabulary reduction will be less effective and may lead to quality degradation.

### Mechanism 3
- Claim: Corpus-based vocabulary trimming balances vocabulary reduction across multiple languages.
- Mechanism: By tokenizing a representative corpus in the target language and using the resulting vocabulary entries, the method includes tokens relevant to the language while excluding irrelevant ones, achieving a more balanced reduction.
- Core assumption: The representative corpus accurately reflects the vocabulary usage in the target language and domain.
- Evidence anchors:
  - [section 3.4] "A more comprehensive way is to tokenize a representative corpus in the desired language in advance and use the vocabulary entries that have been recorded to build a sub-vocabulary."
  - [section 4.2] "Corpus-based shortlisting... leaves a much larger vocabulary for Bulgarian and Chinese, but reduces the vocabulary to half or less for English and Spanish."
- Break condition: If the corpus is not representative or there's significant domain mismatch, important tokens may be excluded, leading to quality degradation.

## Foundational Learning

- Concept: Transformer architecture and its computational bottlenecks
  - Why needed here: Understanding that the output layer's matrix multiplication is the primary bottleneck is crucial for grasping why vocabulary trimming can improve efficiency.
  - Quick check question: In a Transformer-based LLM, which layer typically has the largest matrix multiplication operation in terms of computational cost?

- Concept: Unicode and script identification
  - Why needed here: Script-based vocabulary trimming relies on correctly identifying and filtering tokens based on their Unicode ranges.
  - Quick check question: How would you determine if a token belongs to the Cyrillic script using Unicode ranges?

- Concept: Vocabulary tokenization and subword units
  - Why needed here: Understanding how vocabularies are constructed using subword tokenization methods like BPE is important for grasping how vocabulary trimming works.
  - Quick check question: In Byte-Pair Encoding (BPE), how are rare subwords typically handled during vocabulary trimming?

## Architecture Onboarding

- Component map:
  - Input: Raw text prompts
  - Tokenizer: Converts text to token IDs
  - Vocabulary Trimmer: Filters token IDs based on selected criteria
  - LLM Core: Transformer layers
  - Output Layer: Matrix multiplication and softmax
  - Output: Generated text

- Critical path: Tokenizer → Vocabulary Trimmer → LLM Core → Output Layer
  - The vocabulary trimmer is inserted between the tokenizer and the LLM core to filter tokens before they enter the expensive computation.

- Design tradeoffs:
  - Script-based vs. Corpus-based: Script-based is faster but less effective for languages sharing scripts; Corpus-based is more comprehensive but requires pre-processing.
  - Vocabulary size vs. Quality: Smaller vocabularies improve efficiency but may lead to quality degradation if important tokens are excluded.
  - CPU vs. GPU: Vocabulary trimming shows significant benefits on CPU but minimal to no benefit on GPU due to different computational characteristics.

- Failure signatures:
  - Unexpected quality degradation: Vocabulary trimming may have excluded important tokens.
  - Inconsistent performance across runs: Possible issues with model slicing or cache effects, especially on CPU.
  - Minimal speed improvements: Vocabulary reduction might be too small, or the computational bottleneck might have shifted to other layers.

- First 3 experiments:
  1. Measure baseline inference time and memory usage for a multilingual LLM (e.g., BLOOM-7B) on CPU without vocabulary trimming.
  2. Apply script-based vocabulary trimming for a language with a unique script (e.g., Chinese) and measure changes in inference time, memory usage, and output quality.
  3. Apply corpus-based vocabulary trimming for a language sharing scripts with others (e.g., English) and compare results to the script-based approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vocabulary trimming (VT) methods vary across different model sizes and types, particularly for smaller versus larger models?
- Basis in paper: [explicit] The paper notes that VT reduces memory usage of smaller models by nearly 50% and achieves up to 25% improvement in generation speed, but identifies diminishing returns for larger models.
- Why unresolved: The study does not provide a detailed analysis of the performance trends across various model sizes and types, particularly focusing on the transition from smaller to larger models.
- What evidence would resolve it: Detailed performance metrics and comparisons across a broader range of model sizes and types, including a more granular analysis of how VT effectiveness scales with model size.

### Open Question 2
- Question: What are the specific limitations of Unicode-based script filtering and corpus-based selection methods in vocabulary trimming, and how can these be addressed to improve consistency across different languages?
- Basis in paper: [explicit] The paper discusses limitations of Unicode-based script filtering, such as its ineffectiveness for languages that share a writing system, and corpus-based selection's potential for missing rare but valid tokens or domain mismatch.
- Why unresolved: While the paper identifies these limitations, it does not explore potential solutions or modifications to these methods to overcome these challenges.
- What evidence would resolve it: Research and experimentation with enhanced versions of these methods, possibly incorporating machine learning techniques or hybrid approaches to address identified limitations.

### Open Question 3
- Question: How does vocabulary trimming impact the quality of model outputs, particularly in terms of BLEU and ROUGE scores, beyond the strict metric of exact output matching?
- Basis in paper: [inferred] The paper mentions using (mis)matches as a strict metric for generation quality, but acknowledges that this approach may not fully capture the nuances of output quality.
- Why unresolved: The study does not include analyses of BLEU and ROUGE scores or other metrics that consider generation length or recall, which could provide a more comprehensive understanding of the impact on output quality.
- What evidence would resolve it: Comparative studies using BLEU, ROUGE, and other relevant metrics to assess the quality of outputs generated with trimmed versus full vocabularies, across different languages and model types.

## Limitations
- GPU acceleration does not benefit from vocabulary trimming due to kernel optimizations that mask the impact of reduced vocabulary size
- Larger models (>7B parameters) show diminishing returns, making the approach primarily useful for smaller models
- Languages sharing scripts (like English and Spanish) see less dramatic improvements compared to languages with unique scripts (like Chinese)
- The approach requires careful selection of representative corpora to avoid excluding important tokens

## Confidence

**High Confidence:** Vocabulary trimming significantly reduces memory usage on CPU platforms, with empirical evidence showing up to 50% reduction for smaller models. The mechanism of reducing matrix multiplication complexity in the output layer is well-established.

**Medium Confidence:** Generation speed improvements of up to 25% are achievable on CPU, but the inconsistent results across languages and models suggest that real-world performance may vary significantly based on specific use cases.

**Low Confidence:** The claim that vocabulary trimming preserves output quality is not fully validated, as the study only measures exact output matches against full vocabulary without assessing semantic quality or downstream task performance.

## Next Checks

1. **GPU Performance Validation:** Conduct controlled experiments comparing vocabulary trimming on GPU across different batch sizes and model families to determine if any configurations benefit from reduced vocabulary, or if the computational characteristics of GPU kernels make this approach ineffective.

2. **Quality Degradation Assessment:** Implement comprehensive quality evaluation using human evaluation and downstream task benchmarks (e.g., GLUE, SuperGLUE) to quantify whether vocabulary trimming introduces semantic degradation beyond simple token mismatches.

3. **Dynamic Vocabulary Adaptation:** Test whether adaptive vocabulary trimming that changes based on input content (rather than fixed per-language vocabularies) can achieve better balance between efficiency and quality, particularly for code-mixed or multilingual inputs.