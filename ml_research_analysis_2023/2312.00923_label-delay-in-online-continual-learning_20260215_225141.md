---
ver: rpa2
title: Label Delay in Online Continual Learning
arxiv_id: '2312.00923'
source_url: https://arxiv.org/abs/2312.00923
tags:
- data
- delay
- learning
- time
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies label delay in online continual learning, where
  models are trained on streaming data but labels arrive with a delay due to annotation
  latency. The authors propose a framework where, at each time step, models receive
  both unlabeled data from the current step and labels delayed by d steps from t-d.
---

# Label Delay in Online Continual Learning

## Quick Facts
- **arXiv ID**: 2312.00923
- **Source URL**: https://arxiv.org/abs/2312.00923
- **Reference count**: 38
- **Primary result**: IWMS bridges accuracy gaps caused by label delay without significantly increasing computational complexity

## Executive Summary
This paper studies the challenge of label delay in online continual learning, where models must train on streaming data while labels arrive with significant delays. The authors find that increasing computational budget alone cannot resolve performance degradation from delayed labels, and surprisingly, state-of-the-art self-supervised learning and test-time adaptation techniques fail to outperform simple training on delayed labeled data. To address this, they introduce Importance Weighted Memory Sampling (IWMS), which rehearses labeled memory samples most similar to new unlabeled samples, effectively bridging the accuracy gap without substantially increasing computational complexity.

## Method Summary
The authors propose a framework where models receive unlabeled data from the current time step and labels delayed by d steps. They evaluate three main approaches: (1) Naïve - training only on delayed labeled data, (2) SSL/TTA methods - attempting to leverage newer unlabeled data, and (3) IWMS - sampling from a memory buffer based on feature similarity and label matching with newest unlabeled data. The method uses a ResNet18-BN backbone, fixed-size FIFO memory buffer (219 samples), and normalized computational budget where C=1 represents the baseline. IWMS selects labeled memory samples that match the predicted label of unlabeled samples and have high cosine similarity in feature space.

## Key Results
- Increasing computational budget alone cannot fully resolve performance degradation caused by label delay
- SSL and TTA methods fail to outperform naive training on delayed labeled data when given the same computational budget
- IWMS bridges the accuracy gap caused by label delay without significantly increasing computational complexity
- IWMS is robust across different delay scenarios, computational budgets, and memory sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing computational budget alone cannot fully resolve performance degradation caused by label delay.
- **Mechanism**: Label delay creates a distribution mismatch between training data (older, labeled) and evaluation data (newer, unlabeled). Simply increasing training iterations does not bridge this gap because the model is still optimizing on outdated distributions.
- **Core assumption**: The distribution of incoming data changes over time, making older labeled data less representative of current evaluation conditions.
- **Evidence anchors**:
  - [abstract] "Our findings underline a notable performance decline when solely relying on labeled data when the label delay becomes significant."
  - [section 4.3] "We have shown that increasing the computational budget of an Online Continual Learning model alleviates the problem of the label delay, but that such a solution not only faces diminishing returns but becomes less effective under larger delays."
- **Break condition**: If data distribution remains stationary over time, increased computational budget could fully recover performance.

### Mechanism 2
- **Claim**: Self-supervised learning and test-time adaptation methods fail to outperform the naive approach when given the same computational budget.
- **Mechanism**: SSL methods are sample inefficient and require many more iterations to achieve comparable performance to supervised learning. TTA methods assume a stationary test distribution and a pre-converged model, both violated in the label delay setting.
- **Core assumption**: The computational budget is normalized across methods, making SSL/TTA methods less effective per iteration compared to supervised learning.
- **Evidence anchors**:
  - [abstract] "More surprisingly, when using state-of-the-art SSL and TTA techniques to utilize the newer, unlabeled data, they fail to surpass the performance of a naïve method that simply trains on the delayed supervised stream."
  - [section 5.1] "We hypothesize this is due to several reasons: (1) SSL methods generally are sample inefficient and require a few magnitudes more iterations than the supervised method to achieve the same downstream accuracy."
  - [section 5.2] "We hypothesize that the TTA methods fail to outperform the Naïve counterpart because the common assumptions among the settings on which TTA methods are evaluated are broken."
- **Break condition**: If computational budget is significantly increased for SSL/TTA methods, or if data distribution is stationary, these methods might outperform the naive approach.

### Mechanism 3
- **Claim**: Importance Weighted Memory Sampling (IWMS) bridges the accuracy gap caused by label delay by rehearsing labeled memory samples most similar to new unlabeled samples.
- **Mechanism**: IWMS samples from the memory buffer based on feature similarity and label matching with the newest unlabeled data, effectively reducing the distribution mismatch between training and evaluation data.
- **Core assumption**: The memory buffer contains relevant labeled samples that can approximate the distribution of newer unlabeled data when sampled appropriately.
- **Evidence anchors**:
  - [abstract] "To this end, we introduce Importance Weighted Memory Sampling (IWMS), which rehearses labeled memory samples most similar to new unlabeled samples."
  - [section 5.3] "Our method provides a third option for constructing the training mini-batch, which picks the labeled memory sample that is most similar to the unlabeled data."
  - [section 5.4] "Replacing the newest batch (N) with (W) results in almost doubling the performance: +8.5% and +9.1% improvement over Naïve, respectively."
- **Break condition**: If the memory buffer is too small or unrepresentative of the data distribution, IWMS effectiveness would diminish.

## Foundational Learning

- **Concept**: Online Continual Learning (OCL)
  - **Why needed here**: The paper builds on OCL framework where models are trained on streaming data with the challenge of label delay.
  - **Quick check question**: In OCL, what is the typical evaluation metric used to assess model performance on streaming data?

- **Concept**: Self-Supervised Learning (SSL)
  - **Why needed here**: The paper evaluates SSL methods as potential solutions to utilize unlabeled data in the presence of label delay.
  - **Quick check question**: What is the key difference between contrastive SSL methods (like MoCo, SimCLR) and non-contrastive methods (like BYOL, SimSiam)?

- **Concept**: Test-Time Adaptation (TTA)
  - **Why needed here**: TTA methods are explored as another approach to adapt models to newer unlabeled data distributions in the label delay setting.
  - **Quick check question**: What are the typical assumptions made by TTA methods that are violated in the label delay setting?

## Architecture Onboarding

- **Component map**: SX (unlabeled data stream) -> Model -> Predictions -> Memory Buffer -> IWMS sampling -> Training -> Updates -> SY (labeled data stream with delay)
- **Critical path**:
  1. Receive unlabeled batch from SX
  2. Make predictions on unlabeled batch
  3. Receive delayed labels from SY
  4. Sample training data using IWMS or random sampling
  5. Perform parameter updates within computational budget
  6. Evaluate on newest unlabeled batch

- **Design tradeoffs**:
  - Memory buffer size vs. computational efficiency: Larger buffers provide better representation but increase memory usage
  - Computational budget allocation: Balancing supervised and unsupervised updates within fixed budget
  - Sampling strategy complexity: IWMS adds feature computation overhead but improves sample relevance

- **Failure signatures**:
  - Accuracy degradation over time indicates distribution shift or insufficient adaptation
  - Unstable training or exploding gradients suggest issues with unsupervised loss weighting
  - Poor performance on delayed labels suggests memory buffer lacks relevant samples

- **First 3 experiments**:
  1. Reproduce the baseline Naïve method with varying label delays (d=0, 10, 50, 100) on CLOC dataset
  2. Implement and compare SSL methods (MoCo, SimCLR, BYOL, etc.) against Naïve method under same computational budget
  3. Implement IWMS method and compare against Naïve and SSL methods across different delay scenarios and computational budgets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise mechanism by which the importance weighting in IWMS selects the most relevant samples, and how does this differ from standard nearest neighbor approaches?
- **Basis in paper**: [explicit] The paper states that IWMS selects labeled memory samples with the same true label as the predicted label of the unlabeled sample and high feature similarity, but does not detail the exact mechanism.
- **Why unresolved**: The paper provides a high-level description but lacks a detailed algorithmic breakdown of how the importance weighting is calculated and applied.
- **What evidence would resolve it**: A step-by-step algorithm explaining the importance weighting calculation, including how feature similarities are computed and how the multinomial distribution is constructed.

### Open Question 2
- **Question**: How does the performance of IWMS scale with the size of the memory buffer, and is there an optimal buffer size for different datasets and delay scenarios?
- **Basis in paper**: [explicit] The paper mentions that IWMS is robust to different buffer sizes but does not provide a detailed analysis of the scaling behavior or optimal buffer sizes.
- **Why unresolved**: The paper only provides a brief analysis of the influence of buffer size on performance without exploring the full scaling behavior or optimal configurations.
- **What evidence would resolve it**: Experiments varying the memory buffer size across different datasets and delay scenarios, along with an analysis of the scaling behavior and identification of optimal buffer sizes.

### Open Question 3
- **Question**: How do the assumptions of TTA methods break down in the context of label delay, and what modifications could be made to adapt them to this setting?
- **Basis in paper**: [explicit] The paper states that TTA methods assume the model has converged and the test data distribution is stationary, which are broken in the label delay setting, but does not explore potential modifications.
- **Why unresolved**: The paper identifies the issues with TTA methods in the label delay setting but does not propose or explore potential modifications to adapt them to this scenario.
- **What evidence would resolve it**: Experiments modifying TTA methods to account for non-stationary test data distributions and continuous model updates, along with an analysis of their performance in the label delay setting.

## Limitations
- Evaluation focused primarily on two datasets (CLOC and CGLM) without broader dataset diversity
- SSL/TTA comparison limited to specific computational budget normalization that may not generalize to all practical scenarios
- IWMS requires additional feature computation overhead that isn't fully quantified in terms of end-to-end efficiency gains

## Confidence

**High**: The finding that increased computational budget alone cannot resolve label delay issues - well-supported by systematic experiments across multiple delay scenarios.

**Medium**: The claim that SSL/TTA methods fail to outperform naive approaches - supported by experiments but may depend heavily on the specific computational budget normalization.

**Medium**: The effectiveness of IWMS in bridging accuracy gaps - demonstrated on experimental datasets but requires validation on broader data distributions and real-world scenarios.

## Next Checks

1. **Dataset Generalization**: Evaluate IWMS and baseline methods on additional datasets with different characteristics (e.g., CIFAR-100, TinyImageNet) to assess robustness beyond geolocated image streams.

2. **Computational Budget Sensitivity**: Systematically vary the computational budget ratio between SSL/TTA methods and supervised learning to determine the threshold where these methods might become competitive with the naive approach.

3. **Memory Buffer Scaling**: Test IWMS performance across different memory buffer sizes (smaller and larger than 219 samples) to quantify the relationship between buffer capacity and label delay compensation effectiveness.