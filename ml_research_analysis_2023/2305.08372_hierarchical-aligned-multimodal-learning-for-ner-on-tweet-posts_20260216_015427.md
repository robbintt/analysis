---
ver: rpa2
title: Hierarchical Aligned Multimodal Learning for NER on Tweet Posts
arxiv_id: '2305.08372'
source_url: https://arxiv.org/abs/2305.08372
tags:
- image
- visual
- text
- multimodal
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hierarchical multimodal learning framework
  (HamLearning) for multimodal named entity recognition (MNER) on tweets. The model
  addresses limitations in existing MNER methods by dynamically aligning image and
  text through multi-level semantic interactions.
---

# Hierarchical Aligned Multimodal Learning for NER on Tweet Posts

## Quick Facts
- arXiv ID: 2305.08372
- Source URL: https://arxiv.org/abs/2305.08372
- Reference count: 28
- Primary result: Achieves SOTA F1 scores of 76.49 and 87.13 on Twitter2015 and Twitter2017 MNER datasets

## Executive Summary
This paper introduces HamLearning, a hierarchical multimodal learning framework for named entity recognition on tweet posts that addresses limitations in existing MNER methods through dynamic image-text alignment and multi-level semantic interactions. The model captures complementary fine-to-coarse semantic correlations between vision and language while preventing visual noise from misleading entity predictions. Experiments demonstrate state-of-the-art performance with F1 scores of 76.49 on Twitter2015 and 87.13 on Twitter2017, outperforming previous methods by integrating object-level and image-level visual features based on dynamically measured relevance.

## Method Summary
HamLearning operates in three stages: (1) intra-modality representation learning using BERT for text and a combination of ViT and R-GCN for vision, (2) relevance measuring between text and image to integrate object-level and image-level visual features, and (3) iterative cross-modal interactions to refine multimodal representations. The framework uses BERT for text contextualization, ResNet and Faster-RCNN for visual feature extraction, ViT for semantic vision encoding, and R-GCN for spatial object relationships. Relevance scores are computed end-to-end based on the MNER task objective, and cross-modal interactions occur at three semantic levels: global image, object-level, and word-level.

## Key Results
- Achieves state-of-the-art F1 scores of 76.49 on Twitter2015 and 87.13 on Twitter2017 datasets
- Outperforms previous methods by capturing complementary fine-to-coarse semantic alignments
- Reduces visual noise impact through dynamic relevance measuring between text and image

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical alignment captures complementary fine-to-coarse semantic correlations between vision and language
- Mechanism: The model enforces semantic refinement via iterative cross-modal interactions at three semantic levels: global image, object-level, and word-level. This allows both modalities to mutually reinforce each other progressively.
- Core assumption: Visual and textual features benefit from multiple interaction layers that allow fine-grained features to influence coarse-grained reasoning
- Evidence anchors:
  - [abstract]: "dynamically align the image and text sequence and achieve the multi-level cross-modal learning"
  - [section]: "we implement the cross-modal interaction between word representations and the fused visual feature iteratively to refine the most effective multimodal clues"
- Break condition: If visual and textual modalities are not semantically aligned at the appropriate level, interactions will introduce noise rather than helpful context.

### Mechanism 2
- Claim: Dynamic relevance measuring prevents visual noise from misleading entity predictions
- Mechanism: Relevance scores are computed end-to-end based on the MNER task objective, allowing the model to downweight visual features when the image-text pair is not semantically aligned.
- Core assumption: The relevance between image and text varies significantly across examples, and static fusion strategies cannot handle this variability
- Evidence anchors:
  - [abstract]: "evaluates the relevance between the text and its accompanying image and integrates different grained visual information based on the relevance"
  - [section]: "we dynamically measure the relevance only depend on the MNER objective"
- Break condition: If relevance computation fails to capture the true semantic alignment, noise will propagate through the model.

### Mechanism 3
- Claim: Multi-level visual encoding captures both object relationships and global scene context
- Mechanism: The model uses both ViT (semantic view) and R-GCN (spatial view) to encode visual features. R-GCN captures spatial relationships between objects, while ViT models the semantic coherence of the entire image.
- Core assumption: Entity recognition benefits from both fine-grained object relationships and holistic scene understanding
- Evidence anchors:
  - [section]: "we enhance the global representation of the image-level by analyzing the objects and the relationship between them in the image"
  - [section]: "the objects in the visual space are often scattered and irregular. We thus build the structure graph for realizing spacial modeling"
- Break condition: If the visual encoding fails to capture meaningful relationships or scene context, the additional complexity adds no value.

## Foundational Learning

- Concept: Cross-modal attention mechanisms
  - Why needed here: To align and fuse information between text and image at multiple semantic levels
  - Quick check question: Can you explain how multi-head attention helps capture different types of cross-modal relationships?

- Concept: Graph neural networks for spatial reasoning
  - Why needed here: To model relationships between detected objects and their spatial arrangements in the image
  - Quick check question: How does R-GCN differ from standard GCN when modeling directed relationships?

- Concept: Sequence labeling with conditional random fields
  - Why needed here: To enforce label consistency constraints across the entity sequence prediction
  - Quick check question: Why is CRF often preferred over softmax for NER sequence prediction?

## Architecture Onboarding

- Component map: Text encoder (BERT) → Relevance module → Visual encoder (ViT + R-GCN) → Fusion module (iterative cross-modal transformers) → Decoder (CRF layer)

- Critical path: Text features → Relevance scoring → Visual fusion → Cross-modal refinement → CRF decoding

- Design tradeoffs:
  - Multiple visual encoders add complexity but capture richer scene understanding
  - Iterative cross-modal interactions increase compute but improve feature quality
  - End-to-end relevance measuring is more flexible than fixed classification tasks

- Failure signatures:
  - Poor performance on irrelevant image cases suggests relevance measuring is broken
  - Failure to capture spatial relationships suggests R-GCN configuration issues
  - Inconsistent entity boundaries suggest CRF or label decoding problems

- First 3 experiments:
  1. Remove R-GCN and measure impact on spatial reasoning capabilities
  2. Replace iterative cross-modal with single fusion step to test necessity
  3. Evaluate with and without relevance measuring on partially relevant image pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HamLearning handle the trade-off between local object-level and global image-level visual features during relevance measurement?
- Basis in paper: [explicit] The paper mentions that HamLearning measures relevance between text and image, then integrates object-level and image-level visual features based on this relevance score.
- Why unresolved: The paper describes the mechanism for combining these features but doesn't provide empirical analysis showing how different weighting strategies affect performance or how the model automatically adjusts this trade-off.
- What evidence would resolve it: Experiments comparing different fusion strategies (e.g., fixed vs. adaptive weighting, gating mechanisms) and analysis of how relevance scores correlate with feature contributions across different entity types.

### Open Question 2
- Question: What is the computational overhead introduced by the multi-level semantic interaction approach compared to single-level methods, and how does it scale with longer text sequences?
- Basis in paper: [inferred] The paper introduces iterative cross-modal interactions at different semantic levels (text-sentence, text-words, objects) and mentions using multiple Transformers, suggesting increased computational complexity.
- Why unresolved: While the paper demonstrates performance improvements, it doesn't provide runtime comparisons, memory usage analysis, or scalability studies with varying sequence lengths.
- What evidence would resolve it: Detailed benchmarking of inference time and memory consumption across different sequence lengths, comparison with baseline methods on the same hardware, and analysis of how the number of interaction layers affects performance.

### Open Question 3
- Question: How robust is HamLearning to images that contain multiple relevant entities or scenes that don't directly relate to any entity in the text?
- Basis in paper: [explicit] The paper acknowledges that 33.8% of tweets have images that add no additional content to the text, and discusses the challenge of irrelevant visual content causing misleading interactions.
- Why unresolved: The paper demonstrates overall performance improvements but doesn't analyze model behavior on challenging cases where images are either highly relevant but complex or completely irrelevant to the text content.
- What evidence would resolve it: Case studies analyzing model predictions on tweets with: (1) images containing multiple entities not mentioned in text, (2) images completely unrelated to text content, (3) images that are partially relevant but contain distracting elements, along with error analysis metrics for these specific scenarios.

## Limitations

- The framework's performance on irrelevant or weakly related image pairs is not thoroughly validated
- The three-stage architecture introduces significant computational overhead compared to simpler fusion approaches
- Evaluation is restricted to Twitter datasets, limiting generalizability to other multimodal domains

## Confidence

*High Confidence (Level 4/5):* The state-of-the-art performance claims on Twitter2015 (76.49 F1) and Twitter2017 (87.13 F1) datasets are well-supported by the experimental results section.

*Medium Confidence (Level 3/5):* While the mechanism descriptions are detailed, the exact implementation of the R-GCN spatial encoder and the relevance measuring component contains unspecified details that could affect reproducibility.

## Next Checks

1. Conduct an ablation study on image relevance by creating a dataset with varying degrees of image-text semantic alignment (fully relevant, partially relevant, irrelevant) to quantify how effectively the relevance measuring component filters noise.

2. Implement a lightweight variant that replaces the R-GCN with simpler object relationship modeling to determine whether the spatial reasoning complexity is justified by measurable performance gains on spatial entity recognition tasks.

3. Test cross-dataset generalization by evaluating the trained model on a non-Twitter multimodal dataset (such as Instagram posts or news article-image pairs) to assess whether the hierarchical alignment approach transfers to other multimodal contexts.