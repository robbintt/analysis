---
ver: rpa2
title: 'Personality Style Recognition via Machine Learning: Identifying Anaclitic
  and Introjective Personality Styles from Patients'' Speech'
arxiv_id: '2311.04088'
source_url: https://arxiv.org/abs/2311.04088
tags:
- personality
- features
- anaclitic
- patients
- introjective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates automatic detection of anaclitic and introjective
  personality styles from patients' speech using NLP and ML. A dataset of 79 patients
  with major depressive disorder was analyzed, including audio recordings and questionnaire
  responses.
---

# Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients' Speech

## Quick Facts
- **arXiv ID**: 2311.04088
- **Source URL**: https://arxiv.org/abs/2311.04088
- **Reference count**: 6
- **Primary result**: LIWC-based features achieve F1=0.896 for classifying anaclitic/introjective personality styles from speech

## Executive Summary
This study investigates automatic detection of anaclitic and introjective personality styles from patients' speech using NLP and ML techniques. The research analyzes a dataset of 79 patients with major depressive disorder, combining audio recordings with questionnaire responses. The study demonstrates that linguistic features extracted using LIWC significantly outperform traditional questionnaire-based classification approaches, with the highest performance achieved when combining LIWC and questionnaire features.

## Method Summary
The study collected audio recordings and transcripts from clinical diagnostic interviews with 79 patients diagnosed with major depressive disorder. Multiple feature sets were extracted including TF-IDF, LIWC categories, BERT embeddings, audio descriptors, and questionnaire responses. Various classifiers (Logistic Regression, Random Forest, CatBoost) were trained and evaluated using stratified 5-fold cross-validation over 100 runs, with SMOTE applied for class balancing.

## Key Results
- LIWC-based features achieved the highest performance (F1=0.896, Kappa=0.801)
- Combining LIWC and questionnaire features further improved results (F1=0.930, Kappa=0.867)
- Questionnaire-based models performed significantly worse (F1=0.588, Kappa=0.194)
- Anaclitic patients used more personal pronouns and social-related words; introjective patients showed more cognitive and future-oriented language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LIWC-based linguistic features outperform questionnaire-based models for personality style classification.
- Mechanism: The LIWC categories capture subtle, context-specific linguistic patterns that are more directly linked to personality traits than the broad, generalized questionnaire responses.
- Core assumption: The frequency of words in certain LIWC categories (e.g., personal pronouns, cognitive processes) is a stronger indicator of personality style than self-reported questionnaire answers.
- Evidence anchors:
  - [abstract] "LIWC-based features achieved the highest performance (F1=0.896, Kappa=0.801), significantly outperforming questionnaire-based models (F1=0.588, Kappa=0.194)."
  - [section] "We find that automated classification with language-derived features (i.e., based on LIWC) significantly outperforms questionnaire-based classification models."

### Mechanism 2
- Claim: Combining LIWC and questionnaire features yields the best classification performance.
- Mechanism: The complementary nature of linguistic and self-reported data provides a more comprehensive representation of personality, capturing both implicit (linguistic) and explicit (questionnaire) aspects.
- Core assumption: The linguistic patterns captured by LIWC and the self-reported information in questionnaires are not entirely redundant and provide unique insights into personality.
- Evidence anchors:
  - [abstract] "Combining LIWC and questionnaire features further improved results (F1=0.930, Kappa=0.867)."
  - [section] "A CatBoost model trained on this combined feature set achieves 0.93 F1 score and 0.867 kappa score."

### Mechanism 3
- Claim: BERT embeddings capture context-aware linguistic features that improve classification performance over basic TF-IDF.
- Mechanism: BERT's contextual understanding allows it to capture the semantic meaning and relationships between words, providing a richer representation of the text than simple word frequency counts.
- Core assumption: The context in which words are used is important for understanding personality, and BERT can effectively capture this context.
- Evidence anchors:
  - [abstract] "more advanced text features, using LIWC (linguistic inquiry and word count) and context-aware features using BERT (bidirectional encoder representations from transformers)"
  - [section] "The LR model based on advanced BERT features, which take into account the contextual information, outperforms the best model based on the TF-IDF features."

## Foundational Learning

- **Natural Language Processing (NLP)**: Why needed here - NLP techniques are used to extract linguistic features from patient speech, which are then used to classify personality styles.
  - Quick check question: What are some common NLP techniques used for text analysis, and how do they differ in their approach to capturing linguistic features?

- **Machine Learning (ML)**: Why needed here - ML models are used to classify patients into anaclitic or introjective personality styles based on the extracted linguistic features.
  - Quick check question: What are some common ML algorithms used for classification tasks, and what are their strengths and weaknesses?

- **Linguistic Inquiry and Word Count (LIWC)**: Why needed here - LIWC is a text analysis tool used to count the frequency of words in psychologically relevant categories, providing insights into personality styles.
  - Quick check question: How does LIWC categorize words, and what are some examples of categories that might be relevant for personality classification?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Feature extraction (TF-IDF, LIWC, BERT, audio) -> Classification (Logistic Regression, Random Forest, CatBoost) -> Evaluation (F1, Kappa)
- **Critical path**: Data preprocessing → Feature extraction (LIWC) → Classification (Random Forest) → Evaluation (F1, Kappa)
- **Design tradeoffs**: Using more advanced features (LIWC, BERT) improves performance but increases computational complexity. Combining multiple feature types (LIWC + questionnaire) improves performance but requires more data and processing.
- **Failure signatures**: Poor performance may indicate issues with data quality, feature extraction, or model selection. Overfitting may occur if the model is too complex relative to the amount of data.
- **First 3 experiments**:
  1. Train and evaluate a Random Forest classifier using only LIWC features.
  2. Train and evaluate a CatBoost classifier using the combined LIWC and questionnaire features.
  3. Compare the performance of the BERT-based classifier to the TF-IDF-based classifier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the automatic detection of anaclitic and introjective personality styles from speech compare in accuracy to detection by trained human raters using the prototype matching procedure?
- Basis in paper: [explicit] The paper compares automatic classification using LIWC features (F1=0.896, Kappa=0.801) to questionnaire-based classification (F1=0.588, Kappa=0.194), but does not directly compare to human rater accuracy.
- Why unresolved: The study only validates against questionnaire responses, not against the gold standard of human expert classification.
- What evidence would resolve it: A head-to-head comparison of the ML models against the prototype matching procedure used by human experts on the same dataset.

### Open Question 2
- Question: Does the inclusion of audio features significantly improve classification performance beyond text-only features?
- Basis in paper: [explicit] The best audio-based model (RF) achieved F1=0.621, Kappa=0.268, which is lower than the best text-based model (LIWC + questionnaire, F1=0.930, Kappa=0.867).
- Why unresolved: The paper does not explore whether combining audio features with text features could yield better performance than text alone.
- What evidence would resolve it: Experiments training models on combined audio and text feature sets and comparing their performance to text-only models.

### Open Question 3
- Question: How well does the automatic personality style classification generalize to other interview types or clinical populations beyond the CDI and MDD sample used here?
- Basis in paper: [inferred] The study uses a specific interview format (CDI) and clinical population (MDD patients), limiting generalizability.
- Why unresolved: The paper does not test the models on other interview types or patient populations.
- What evidence would resolve it: Validation of the models on different interview formats and clinical populations to assess robustness and generalizability.

## Limitations
- Sample size of 79 patients limits generalizability to broader populations
- Focus on Dutch language and MDD context may constrain applicability to other languages and mental health conditions
- Study does not directly validate linguistic patterns against expert clinical assessments

## Confidence
- **High confidence**: LIWC features outperforming questionnaire-based models
- **Medium confidence**: Complementary value of combined features, interpretability of linguistic differences

## Next Checks
1. Replicate findings with a larger, more diverse patient sample across different clinical contexts and languages
2. Conduct causal analysis to determine whether linguistic patterns are predictors or consequences of personality styles
3. Validate automated classifications against expert clinical assessments in a blinded study