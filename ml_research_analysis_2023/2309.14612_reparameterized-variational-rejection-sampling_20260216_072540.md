---
ver: rpa2
title: Reparameterized Variational Rejection Sampling
arxiv_id: '2309.14612'
source_url: https://arxiv.org/abs/2309.14612
tags:
- elbo
- variational
- rvrs
- gradient
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a reparameterized gradient estimator for variational
  rejection sampling (VRS), making it practical for continuous latent variable models.
  The resulting RVRS method offers an attractive trade-off between computational cost
  and inference fidelity compared to other methods like normalizing flows, IWAE, and
  UHA/DAIS.
---

# Reparameterized Variational Rejection Sampling

## Quick Facts
- **arXiv ID:** 2309.14612
- **Source URL:** https://arxiv.org/abs/2309.14612
- **Reference count:** 40
- **Key outcome:** RVRS achieves competitive ELBO values across models and datasets, often outperforming or matching more complex methods while being faster to train.

## Executive Summary
The paper introduces Reparameterized Variational Rejection Sampling (RVRS), a method that combines parametric variational inference with rejection sampling through a reparameterized gradient estimator. By leveraging the reparameterization trick, RVRS reduces gradient variance compared to the original Variational Rejection Sampling (VRS) score-function estimator, making it practical for continuous latent variable models. The method offers an attractive trade-off between computational cost and inference fidelity, particularly excelling in models with local latent variables and high-dimensional latent spaces.

## Method Summary
RVRS combines a parametric proposal distribution q_ϕ(z) with rejection sampling, where the acceptance probability a_ϕ,θ(z) is computed based on the model and data. The key innovation is a reparameterized gradient estimator that reduces variance compared to the original VRS score-function estimator. The method includes an adaptive scheme for tuning the rejection threshold T to balance computational cost and inference fidelity. For models with local latent variables, RVRS admits unbiased mini-batch learning, while for hierarchical models, a hybrid Semi-RVRS approach is used. The method is evaluated on various datasets including UCI datasets and MNIST, comparing against baselines like mean-field, normalizing flows, IWAE, and UHA.

## Key Results
- RVRS achieves competitive ELBO values across a range of models and datasets, often outperforming or matching more complex methods
- The reparameterized gradient estimator significantly reduces variance compared to the original VRS score-function estimator
- RVRS is particularly effective for models with local latent variables and can handle high-dimensional latent spaces efficiently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reparameterized gradient estimator reduces variance compared to the original VRS score-function estimator.
- Mechanism: The reparameterized estimator leverages the identity that allows converting a score-function-like term into a pathwise gradient, exploiting the fact that r_ϕ,θ(z) is proportional to a reparameterizable distribution q_ϕ(z).
- Core assumption: The proposal distribution q_ϕ(z) is reparameterizable and the acceptance probability a_ϕ,θ(z) depends on ϕ only through q_ϕ(z).
- Evidence anchors:
  - [abstract]: "By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables."
  - [section 4]: "Since the suite of reparameterizable proposal distributions is quite large—including e.g. Normal distributions, Dirichlet distributions, and normalizing flows with reparameterizable base distributions—the RVRS distribution r_ϕ,θ(z) is quite flexible."

### Mechanism 2
- Claim: The rejection threshold T can be adaptively tuned to balance computational cost and inference fidelity.
- Mechanism: A gradient-based strategy is used to tune T by minimizing the loss L(T) = 1/2 (Z_r - Z_tgt)^2, where Z_r is the mean acceptance probability and Z_tgt is a target acceptance probability.
- Core assumption: The loss L(T) is differentiable and can be optimized using gradient methods.
- Evidence anchors:
  - [section 4.2]: "Choosing an appropriate value of T in the vicinity of Eq_ϕ(z) [log q_ϕ(z) - log p_θ(x, z)] is crucial for good performance of (R)VRS... we prefer a gradient-based strategy for tuning the threshold parameter T that allows direct control over the computational cost of (R)VRS."

### Mechanism 3
- Claim: RVRS is particularly well-suited for models with local latent variables due to its ability to factorize across data points.
- Mechanism: For models with only local latent variables, the sampling, ELBO estimation, and ELBO gradient estimation trivially factorize across data points, enabling unbiased mini-batch learning.
- Core assumption: The model has a conditional independence structure that allows factorization across data points.
- Evidence anchors:
  - [section 4.3]: "For models with only local latent variables like a VAE [Kingma and Welling, 2013] sampling, ELBO estimation, and ELBO gradient estimation for RVRS trivially factorize across data points, and thus RVRS admits unbiased mini-batch learning for such models."

## Foundational Learning

- Concept: Reparameterization trick
  - Why needed here: The reparameterization trick is crucial for the reparameterized gradient estimator to work, as it allows the gradient to be computed with respect to the parameters of the proposal distribution.
  - Quick check question: What is the reparameterization trick and how does it enable gradient computation for distributions that are not directly differentiable with respect to their parameters?

- Concept: Rejection sampling
  - Why needed here: Rejection sampling is the core mechanism that allows RVRS to sculpt the proposal distribution towards the posterior, providing a flexible non-parametric variational family.
  - Quick check question: How does rejection sampling work and what is the role of the acceptance probability in determining the efficiency of the sampler?

- Concept: Variational inference
  - Why needed here: Variational inference is the broader framework within which RVRS operates, providing the objective (ELBO) that RVRS aims to optimize.
  - Quick check question: What is the evidence lower bound (ELBO) and how does it relate to the Kullback-Leibler divergence between the variational distribution and the true posterior?

## Architecture Onboarding

- Component map:
  - Proposal distribution q_ϕ(z) -> Acceptance probability a_ϕ,θ(z) -> Rejection threshold T -> ELBO

- Critical path:
  1. Initialize the proposal distribution and threshold T
  2. Draw samples from the proposal distribution and apply rejection sampling
  3. Compute the ELBO and its gradient using the reparameterized estimator
  4. Update the proposal distribution parameters and threshold T using gradient-based optimization
  5. Repeat steps 2-4 until convergence

- Design tradeoffs:
  - Proposal distribution flexibility vs. computational cost: More flexible proposal distributions may lead to better approximations but can be more expensive to sample from
  - Rejection threshold T vs. inference fidelity: Lower values of T lead to better approximations but increase the computational cost due to more rejections
  - Number of samples S vs. gradient variance: Using more samples can reduce gradient variance but increases computational cost

- Failure signatures:
  - High gradient variance: Indicates issues with the reparameterized estimator or the proposal distribution
  - Poor convergence: May be due to an inappropriate choice of threshold T or a mismatch between the proposal distribution and the true posterior
  - Excessive rejections: Suggests that the threshold T is too low or the proposal distribution is poorly matched to the posterior

- First 3 experiments:
  1. Implement RVRS for a simple model with a known posterior (e.g., a Gaussian model) and compare the results to standard variational inference
  2. Experiment with different proposal distributions (e.g., normal, mixture of normals) and observe their impact on the quality of the approximation
  3. Tune the rejection threshold T and measure its effect on the trade-off between computational cost and inference fidelity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RVRS be effectively combined with normalizing flows or MixFlows to create hybrid methods that leverage the strengths of both approaches?
- Basis in paper: [explicit] The paper discusses the potential of combining RVRS with other methods, such as using a normalizing flow or MixFlow in place of a simple parametric proposal.
- Why unresolved: While the paper suggests this as a promising direction, it does not provide experimental results or theoretical analysis of such combinations.
- What evidence would resolve it: Experimental results comparing RVRS combined with normalizing flows/MixFlows to RVRS alone and other state-of-the-art methods on various benchmark problems.

### Open Question 2
- Question: How does the performance of RVRS scale with increasing latent dimensionality in models with local latent variables?
- Basis in paper: [explicit] The paper demonstrates RVRS's effectiveness in high-dimensional latent spaces for models without local variables, but only briefly touches on its performance in hierarchical models with local latent variables.
- Why unresolved: The paper does not provide a systematic study of RVRS's performance as the dimensionality of local latent variables increases.
- What evidence would resolve it: Experiments comparing RVRS to other methods on hierarchical models with varying numbers of local latent variables, examining both inference quality and computational efficiency.

### Open Question 3
- Question: Can the threshold adaptation scheme for RVRS be improved to achieve faster convergence and better control over the trade-off between computation and inference fidelity?
- Basis in paper: [explicit] The paper describes a simple gradient-based approach for adapting the rejection threshold T, but notes that more sophisticated schemes could potentially perform better.
- Why unresolved: The paper only explores a basic adaptation strategy and does not investigate alternative methods or their impact on RVRS performance.
- What evidence would resolve it: Comparative experiments using different threshold adaptation schemes, including more advanced optimization algorithms or amortized inference approaches, to evaluate their impact on RVRS performance and convergence speed.

## Limitations

- The method requires reparameterizable proposal distributions, excluding some common distributions used in variational inference
- The adaptive tuning of the rejection threshold T introduces additional hyperparameters that may require careful calibration
- The computational overhead from rejection sampling, while often justified by improved accuracy, remains significant compared to standard variational inference methods

## Confidence

**High Confidence**: The claim that the reparameterized gradient estimator reduces variance compared to the original VRS score-function estimator is strongly supported by both theoretical derivation and empirical results.

**Medium Confidence**: The assertion that RVRS offers an attractive trade-off between computational cost and inference fidelity requires context-dependent evaluation, as the optimal choice depends heavily on specific problem characteristics and computational constraints.

**Medium Confidence**: The claim about RVRS being particularly well-suited for models with local latent variables is supported by theoretical analysis but may vary in practical benefits depending on the specific model structure.

## Next Checks

1. Systematically measure and compare gradient variance ratios between RVRS and VRS across a wider range of proposal distributions and model complexities to validate the claimed variance reduction properties.

2. Evaluate RVRS performance on very high-dimensional latent spaces (e.g., >1000 dimensions) to assess whether the method maintains its efficiency advantages over normalizing flows and other scalable methods.

3. Investigate the feasibility of extending RVRS to handle discrete latent variables through continuous relaxations or hybrid approaches, addressing a current limitation of the method.