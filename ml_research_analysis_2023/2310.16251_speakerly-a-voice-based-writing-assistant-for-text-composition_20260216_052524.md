---
ver: rpa2
title: 'Speakerly: A Voice-based Writing Assistant for Text Composition'
arxiv_id: '2310.16251'
source_url: https://arxiv.org/abs/2310.16251
tags:
- system
- text
- comp
- input
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Speakerly, a real-time voice-based writing
  assistance system that helps users compose text across various use cases such as
  emails, instant messages, and notes. The system uses a combination of small, task-specific
  models as well as pre-trained language models for fast and effective text composition
  while supporting a variety of input modes for better usability.
---

# Speakerly: A Voice-based Writing Assistant for Text Composition

## Quick Facts
- arXiv ID: 2310.16251
- Source URL: https://arxiv.org/abs/2310.16251
- Reference count: 16
- One-line primary result: Speakerly outperforms previous approaches in fluency, coherence, naturalness, and coverage while being more cost-effective and having lower latency

## Executive Summary
Speakerly is a real-time voice-based writing assistance system that helps users compose text across various use cases such as emails, instant messages, and notes. The system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability. Human evaluations show that the system outperforms previous approaches in terms of fluency, coherence, naturalness, and coverage, while also being more cost-effective and having lower latency than previous approaches.

## Method Summary
Speakerly implements a multi-stage pipeline that processes speech input through ASR transcription, normalization (disfluency filtering, punctuation restoration, grammatical error correction), and comprehension stages. The system uses a hybrid approach combining a fine-tuned Pegasus model (COMP-FT) for closed-ended tasks and GPT-3.5-turbo (COMP-LLM) for open-ended generation, with a classifier routing inputs based on their characteristics. The system incorporates data augmentation to handle pipeline-specific errors and sensitivity filtering to manage offensive or non-inclusive content.

## Key Results
- Human evaluations show Speakerly outperforms previous approaches in fluency, coherence, naturalness, and coverage
- The hybrid model approach reduces deployment costs (30 instances for COMP-LLM vs 1 instance for COMP-FT)
- Lower latency compared to previous approaches while maintaining high output quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive refinement through multiple task-specific models improves robustness and output quality
- Mechanism: The system applies a sequence of transformations—ASR transcription, normalization (disfluency filtering, punctuation restoration, GEC), and comprehension—where each stage corrects errors from the previous one, allowing collective error recovery
- Core assumption: Errors introduced at earlier stages can be detected and corrected by downstream models
- Evidence anchors:
  - [abstract] "The system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability."
  - [section 3] "Each stage can have its own errors. Hence, models across the pipeline are designed with complementary, sometimes overlapping capabilities, which allows them to recover from errors collectively and improve robustness to variation and noise in the input."
  - [corpus] Weak - related work focuses on single-use cases rather than multi-stage pipelines
- Break condition: If upstream errors are too severe or fall outside the correction scope of downstream models, the system will propagate errors to final output

### Mechanism 2
- Claim: Hybrid model approach balances quality, cost, and latency
- Mechanism: The system routes requests to either COMP-FT (lightweight, faster, better for closed-ended) or COMP-LLM (more capable for open-ended) based on a classifier that evaluates input characteristics
- Core assumption: Input characteristics can be reliably predicted to determine which model will produce better output
- Evidence anchors:
  - [abstract] "Human evaluations show that the system outperforms previous approaches in terms of fluency, coherence, naturalness, and coverage."
  - [section 3.3.3] "Since both COMP-FT and COMP-LLM are effective at different use cases, we combine both models into a hybrid approach. Outputs requiring more open-ended generation and having low scope for sensitivity issues are passed to COMP-LLM, whereas shorter inputs and those which require more factual consistency are processed by COMP-FT."
  - [corpus] Weak - related work does not discuss hybrid model architectures for writing assistance
- Break condition: If the classifier misroutes inputs or the performance gap between models narrows, the hybrid approach loses its advantage

### Mechanism 3
- Claim: Data augmentation addresses pipeline-specific errors and improves model robustness
- Mechanism: The training dataset for COMP-FT is augmented with transformations that simulate ASR errors, normalization failures, user input issues, and sensitivity concerns, enabling the model to handle these cases
- Core assumption: Simulating pipeline errors in training data allows the model to learn to correct them
- Evidence anchors:
  - [section 3.3.1] "Finally, we augment the dataset by applying 25 different augmentations to deal with the issues that were either not handled or were introduced by the earlier stages of the pipeline."
  - [section 4.4.1] "We prepare a dataset of 800 sensitive examples to test the generation quality on offensive and non-inclusive language, bias, meaning change, and sensitive domains"
  - [corpus] Weak - related work does not discuss augmentation strategies specific to speech-to-text pipelines
- Break condition: If augmentation does not cover real-world error patterns or introduces unrealistic examples, the model's robustness gains will be limited

## Foundational Learning

- Concept: Pipeline architecture with error correction stages
  - Why needed here: Speech input is inherently noisy and requires multiple transformation stages to produce clean, usable text
  - Quick check question: What happens if disfluency filtering fails to remove a repetition? Where in the pipeline would this be corrected?

- Concept: Hybrid model deployment strategies
  - Why needed here: Different input types require different capabilities (closed-ended vs open-ended), and balancing cost, latency, and quality requires routing logic
  - Quick check question: How does the classifier decide between COMP-FT and COMP-LLM? What features does it use?

- Concept: Data augmentation for error simulation
  - Why needed here: Real training data rarely contains the error patterns introduced by ASR and normalization, so augmentation is needed to make the comprehension model robust
  - Quick check question: What types of errors does the augmentation pipeline simulate? How do these map to actual pipeline failures?

## Architecture Onboarding

- Component map: ASR → Disfluency Filtering → Punctuation Restoration → GEC → Classifier → COMP-FT/COMP-LLM → Sensitivity Filter → Output
- Critical path: ASR → Normalization → Classifier → Comprehension → Output
- Design tradeoffs:
  - Cost vs quality: COMP-LLM is more capable but expensive and slow; COMP-FT is faster and cheaper but less capable
  - Latency vs throughput: Single instance can handle 1 req/s with COMP-FT; requires 30 instances for COMP-LLM
  - Robustness vs complexity: More pipeline stages add complexity but improve error handling
- Failure signatures:
  - ASR errors propagating to output: Check if normalization models are properly removing disfluencies and restoring punctuation
  - Poor routing decisions: Check classifier accuracy and features used for model selection
  - Sensitivity issues: Check filtering effectiveness and model behavior on sensitive inputs
- First 3 experiments:
  1. Test ASR error handling: Input speech with known disfluencies and background noise, verify normalization output
  2. Test model routing: Create test cases for each input type, verify classifier selects appropriate model
  3. Test sensitivity filtering: Input sensitive content, verify filtering and appropriate model routing

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the Speakerly system perform on languages other than English?
- Basis in paper: [inferred] The paper mentions that the system was only tested for English, implying that its performance on other languages is unknown.
- Why unresolved: The paper does not provide any data or analysis on the system's performance with non-English languages.
- What evidence would resolve it: Testing the system with various languages and comparing its performance metrics (e.g., WER, WRR) across different languages would provide insights into its multilingual capabilities.

Open Question 2
- Question: How does the Speakerly system handle very long outputs (greater than 512 tokens)?
- Basis in paper: [explicit] The paper explicitly states that the system currently cannot generate very long outputs (greater than 512 tokens).
- Why unresolved: The paper does not provide details on the limitations or potential solutions for handling longer outputs.
- What evidence would resolve it: Experimenting with different model architectures or fine-tuning techniques to handle longer sequences would help determine the system's capabilities for generating longer outputs.

Open Question 3
- Question: How does the Speakerly system handle different accents and dialects?
- Basis in paper: [inferred] The paper mentions that the system's ability can be limited by the external ASR system's ability to deal with different accents.
- Why unresolved: The paper does not provide any analysis or data on the system's performance with various accents and dialects.
- What evidence would resolve it: Testing the system with users having different accents and dialects and analyzing its performance metrics (e.g., WER, WRR) would provide insights into its robustness to accent variations.

Open Question 4
- Question: How does the Speakerly system perform in terms of computational efficiency and cost when using the hybrid approach?
- Basis in paper: [explicit] The paper mentions that the hybrid approach reduces the number of instances needed for deployment, but does not provide detailed performance or cost analysis.
- Why unresolved: The paper does not provide specific data on the computational efficiency or cost savings achieved by using the hybrid approach.
- What evidence would resolve it: Conducting a detailed analysis of the system's performance (e.g., latency, throughput) and cost (e.g., instance usage, model inference costs) when using the hybrid approach would provide insights into its efficiency and cost-effectiveness.

## Limitations

- Evaluation relies heavily on human judgments without detailed methodology or inter-rater reliability metrics
- Lacks quantitative comparisons with previous approaches, making it difficult to assess claimed performance improvements
- Claims about handling diverse use cases and input modes are not adequately supported by evaluation results

## Confidence

- **High confidence**: The hybrid model approach (combining COMP-FT and COMP-LLM) and the multi-stage pipeline architecture are well-supported by the described methodology and make logical sense for addressing the complexity of speech-to-text conversion and text composition tasks
- **Medium confidence**: Claims about system performance improvements over previous approaches are based on human evaluations, but the lack of detailed evaluation methodology and quantitative baselines reduces confidence in these comparisons
- **Low confidence**: The paper's claims about handling diverse use cases and input modes are not adequately supported by the evaluation results, which focus on a limited set of examples

## Next Checks

1. Reproduce the hybrid model routing accuracy: Implement the classifier and test its ability to correctly route different types of inputs to COMP-FT vs COMP-LLM across a comprehensive set of test cases, measuring precision and recall for each model type

2. Validate the augmentation strategy effectiveness: Create a test suite with synthetic errors representing each type of pipeline failure (ASR errors, normalization failures, sensitivity issues) and measure how well the trained models handle these cases compared to models trained without augmentation

3. Benchmark against established baselines: Compare Speakerly's performance against at least two other voice-based writing assistance systems using standardized metrics (WER, WRR, latency, cost per request) on the same test datasets to verify the claimed performance improvements