---
ver: rpa2
title: Human-Machine Cooperative Multimodal Learning Method for Cross-subject Olfactory
  Preference Recognition
arxiv_id: '2311.14426'
source_url: https://arxiv.org/abs/2311.14426
tags:
- olfactory
- recognition
- e-nose
- features
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multimodal learning method combining olfactory
  EEG and E-nose signals for cross-subject olfactory preference recognition. The method
  addresses the limitations of traditional artificial sensory evaluation and machine
  olfaction by leveraging the complementary advantages of olfactory EEG and E-nose
  in representing odor information and individual emotions.
---

# Human-Machine Cooperative Multimodal Learning Method for Cross-subject Olfactory Preference Recognition

## Quick Facts
- arXiv ID: 2311.14426
- Source URL: https://arxiv.org/abs/2311.14426
- Reference count: 40
- Primary result: Achieves 92.79% accuracy and 92.64% F1-score on cross-subject olfactory preference recognition

## Executive Summary
This paper proposes a multimodal learning method that combines olfactory EEG and E-nose signals for cross-subject olfactory preference recognition. The method addresses the limitations of traditional artificial sensory evaluation and machine olfaction by leveraging the complementary advantages of both modalities. E-nose provides stable odor representations across subjects, while EEG captures individual emotional responses to odors. The proposed approach achieves superior performance compared to state-of-the-art methods on a dataset of 24 subjects and 4 odor types.

## Method Summary
The proposed method involves a deep learning architecture called BMFNet that processes olfactory EEG and E-nose signals through separate feature mining modules (AlexNet for E-nose, ResNet for EEG), aligns their feature spaces using contrastive loss, and fuses them through multimodal attention mechanisms. The MFI module alternately applies crossmodal and self-attention to extract common odor features while preserving individual preference features in EEG. A lightweight student model (BMFNet-S) is trained via knowledge distillation from a teacher model (BMFNet-T) to improve computational efficiency while maintaining performance.

## Key Results
- Achieved 92.79% accuracy and 92.64% F1-score on the cross-subject olfactory preference recognition task
- Outperformed state-of-the-art methods in cross-subject olfactory preference recognition
- Demonstrated the effectiveness of combining olfactory EEG and E-nose signals for capturing both odor information and individual preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: E-nose provides cross-sample consistent odor representation, enabling the model to learn stable odor features across subjects
- Mechanism: E-nose uses a cross-sensitive electrochemical sensor array to detect odor information objectively, which is less influenced by individual differences
- Core assumption: E-nose signals are primarily driven by chemical composition rather than subjective experience, making them transferable across individuals
- Evidence anchors: [abstract] states E-nose contains feature information representing odors while EEG is influenced by both odors and individual differences

### Mechanism 2
- Claim: Contrastive loss aligns olfactory EEG and E-nose feature spaces, reducing individual variability in EEG features
- Mechanism: By minimizing mean squared error between aligned E-nose and EEG features, the model maps individual-specific EEG patterns to a shared odor representation space
- Core assumption: E-nose features can serve as a reference frame to normalize EEG features across subjects
- Evidence anchors: [section] describes olfactory EEG and E-nose initial features are aligned using contrastive loss

### Mechanism 3
- Claim: Multimodal attention mechanisms extract common odor features while preserving individual preference features in EEG
- Mechanism: Crossmodal attention computes query-key-value interactions between EEG and E-nose features to highlight shared odor-related dimensions, while self-attention preserves EEG-specific emotion-related dimensions
- Core assumption: Common and individual feature subspaces are sufficiently disentangled in the EEG signal space
- Evidence anchors: [section] describes MFI module comprising crossmodal attention and self-attention modules

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To align feature spaces of two different modalities (EEG and E-nose) so that the model can jointly reason over them
  - Quick check question: What loss function is used to align the two modalities, and why is it appropriate for cross-subject recognition?

- Concept: Multimodal fusion with attention
  - Why needed here: To capture complementary information—E-nose for odor identity, EEG for individual preference—while preserving both in the joint representation
  - Quick check question: How do crossmodal and self-attention modules interact in the MFI block to separate common vs. individual features?

- Concept: Cross-subject domain adaptation
  - Why needed here: EEG signals vary widely between subjects due to anatomy and physiology, so the model must learn subject-invariant odor features
  - Quick check question: Which module in the architecture directly addresses the cross-subject variability problem, and how?

## Architecture Onboarding

- Component map: Input reshape → Feature mining → Alignment → MFI → AEFM → FF → Output
- Critical path: Input reshape (E-nose: 1,10,90; EEG: 1,21,256) → AlexNet (E-nose), ResNet (EEG) → Contrastive loss alignment → MFI module (crossmodal + self-attention) → AEFM module (self-attention chain) → FF module (multimodal self-attention) → Classification
- Design tradeoffs: Using AlexNet for E-nose keeps model lightweight vs. ResNet; multimodal attention increases parameter count but enables fine-grained feature interaction; distillation trades small accuracy drop for ~40% FLOPs reduction
- Failure signatures: High MSE in alignment stage indicates modality misalignment; low variance in attention weights suggests feature collapse; overfitting in AEFM leads to poor cross-subject generalization
- First 3 experiments: 1) Train BMFNet-S without alignment loss and observe drop in cross-subject accuracy; 2) Replace crossmodal attention with simple concatenation and measure impact on common feature extraction; 3) Swap AlexNet and ResNet roles to evaluate whether deeper E-nose features help or hurt performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method handle cross-subject olfactory preference recognition when odor types are not limited to the four used in this study?
- Basis in paper: The paper mentions that the number of subjects and odor types needs to be increased to make the research closer to practical applications
- Why unresolved: Current study only uses four odor types, and generalizability to a wider range of odors is not tested
- What evidence would resolve it: Experimental results demonstrating the method's effectiveness on a dataset with more diverse odor types

### Open Question 2
- Question: What is the impact of using a more complex E-nose with a higher number of sensors on the performance of the proposed method?
- Basis in paper: The paper mentions that the number of sensors in the E-nose used is insufficient compared to human olfactory receptors
- Why unresolved: Current study uses an E-nose with 10 sensors, and the effect of increasing the number of sensors is not investigated
- What evidence would resolve it: Experimental results comparing the performance of the method using E-noses with different numbers of sensors

### Open Question 3
- Question: How does the proposed method perform in real-time odor evaluation applications?
- Basis in paper: The paper mentions that the method has potential for practical odor evaluation applications, but real-time performance is not discussed
- Why unresolved: Current study does not address the computational efficiency or latency of the method in real-time applications
- What evidence would resolve it: Experimental results demonstrating the method's performance in real-time odor evaluation tasks

## Limitations

- The method's performance depends heavily on the consistency of E-nose sensor readings across samples and subjects, which may fail if sensor drift or cross-contamination occurs
- The effectiveness of contrastive loss in aligning olfactory EEG and E-nose feature spaces is not empirically validated beyond the reported results
- The generalization of the proposed method to larger-scale, more diverse datasets and real-world applications is limited by the scope of experimental validation

## Confidence

- High confidence: The superiority of the proposed method over baseline methods in cross-subject olfactory preference recognition, as demonstrated by the reported accuracy and F1-score on the 24-subject dataset
- Medium confidence: The mechanism of using E-nose features to normalize EEG features across subjects, as it relies on the assumption of cross-sample consistency in E-nose signals
- Low confidence: The generalization of the proposed method to larger-scale, more diverse datasets and real-world applications, given the limited scope of the experimental validation

## Next Checks

1. Evaluate the model's performance on a held-out test set from unseen subjects to assess its cross-subject generalization capabilities
2. Conduct ablation studies to quantify the impact of each component (contrastive loss, crossmodal attention, self-attention) on the overall performance
3. Test the model's robustness to variations in E-nose sensor readings and odor concentrations to validate the cross-sample consistency assumption