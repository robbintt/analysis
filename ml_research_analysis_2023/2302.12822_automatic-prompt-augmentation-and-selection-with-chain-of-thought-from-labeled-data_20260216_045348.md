---
ver: rpa2
title: Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled
  Data
arxiv_id: '2302.12822'
source_url: https://arxiv.org/abs/2302.12822
tags:
- answer
- exemplars
- language
- automate-cot
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Automate-CoT, a method that automatically
  generates and selects high-quality chain-of-thought (CoT) exemplars from labeled
  data to improve reasoning performance in large language models. The approach addresses
  the challenge of manually crafting CoT prompts by employing a three-step pipeline:
  (1) Augmenting rationale chains from a small labeled dataset using a large language
  model, (2) Pruning incorrect chains based on answer consistency with ground truth,
  and (3) Selecting the optimal combination of exemplars using a variance-reduced
  policy gradient strategy to estimate exemplar significance.'
---

# Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data

## Quick Facts
- **arXiv ID**: 2302.12822
- **Source URL**: https://arxiv.org/abs/2302.12822
- **Reference count**: 40
- **Primary result**: +2.7% arithmetic, +3.4% commonsense, +3.2% symbolic, +2.5% non-reasoning improvement over manual CoT

## Executive Summary
This paper presents Automate-CoT, a method that automatically generates and selects high-quality chain-of-thought (CoT) exemplars from labeled data to improve reasoning performance in large language models. The approach addresses the challenge of manually crafting CoT prompts by employing a three-step pipeline: (1) Augmenting rationale chains from a small labeled dataset using a large language model, (2) Pruning incorrect chains based on answer consistency with ground truth, and (3) Selecting the optimal combination of exemplars using a variance-reduced policy gradient strategy to estimate exemplar significance. Experimental results demonstrate substantial improvements across multiple reasoning tasks compared to manual CoT approaches.

## Method Summary
Automate-CoT is a three-step pipeline for automatic CoT prompt generation and selection. First, it augments rationale chains by generating multiple pseudo-chains for each question in a small labeled dataset using a large language model. Second, it prunes incorrect chains by keeping only those with answers matching the ground truth. Third, it selects the optimal combination of exemplars using a variance-reduced policy gradient estimator (VR-PGE) that treats exemplar selection as optimizing a supervised model with latent variables. The method is evaluated across arithmetic, commonsense, symbolic, and non-reasoning tasks, showing consistent improvements over manual CoT approaches.

## Key Results
- +2.7% improvement on GSM8K arithmetic reasoning task
- +3.4% improvement on commonsense reasoning tasks
- +3.2% improvement on symbolic reasoning tasks
- +2.5% improvement on non-reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning incorrect chains based on answer consistency with ground truth improves the quality of the candidate pool.
- Mechanism: The method generates multiple pseudo-chains for each question and only keeps those with correct final answers that match the ground truth. This reduces noise in the exemplar pool.
- Core assumption: Generating correct reasoning is a necessary condition for generating correct answers.
- Evidence anchors:
  - [abstract]: "Pruning incorrect chains based on answer consistency with ground truth"
  - [section]: "When a correct answer is generated, the rationale chain of these steps is most likely correct, contributing to the final correctness"
- Break condition: If the model can generate correct answers through incorrect reasoning paths, this pruning would eliminate potentially useful exemplars.

### Mechanism 2
- Claim: Variance-reduced policy gradient strategy enables effective selection of optimal exemplar combinations without requiring gradients.
- Mechanism: The method treats exemplar selection as optimizing a supervised model with latent variables, using VR-PGE to estimate gradients and update the selection probabilities.
- Core assumption: The optimal combination of exemplars can be found by maximizing the likelihood of correct predictions across the training set.
- Evidence anchors:
  - [abstract]: "selecting the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy"
  - [section]: "we resort to the variance-reduced policy gradient estimator (VR-PGE)...to optimize the loss function via forward propagation"
- Break condition: If the variance reduction in VR-PGE is insufficient, the gradient estimates may be too noisy for effective optimization.

### Mechanism 3
- Claim: Augmenting rationale chains from labeled data bypasses the need for human-written exemplars.
- Mechanism: The method uses the language model to generate multiple rationale chains for each question in the labeled dataset, creating a diverse pool of exemplars.
- Core assumption: Model-generated rationale chains can be of comparable quality to human-annotated ones for effective prompting.
- Evidence anchors:
  - [abstract]: "automatically augmenting rational chains from a small labeled dataset"
  - [section]: "Inspired by Wang et al. (2022), which shows that the generated rationale chains are of comparable quality to the human-annotated ones"
- Break condition: If the language model cannot generate high-quality rationale chains, the augmented pool will be ineffective regardless of pruning and selection.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: This paper builds upon CoT methodology to automate exemplar generation and selection
  - Quick check question: What is the key difference between standard prompting and chain-of-thought prompting?

- Concept: Policy gradient methods
  - Why needed here: The selection algorithm uses variance-reduced policy gradient to optimize exemplar combinations
  - Quick check question: How does policy gradient differ from standard gradient descent in optimization?

- Concept: Reinforcement learning basics
  - Why needed here: The variance-reduced policy gradient estimator is a reinforcement learning technique
  - Quick check question: What is the role of the reward signal in reinforcement learning?

## Architecture Onboarding

- Component map: Augment → Prune → Select → Inference
- Critical path: Augment → Prune → Select → Inference
- Design tradeoffs:
  - Pool size vs. computational cost: Larger pools provide more diverse exemplars but increase VR-PGE computation time
  - Number of exemplars selected vs. context window limits: Must balance exemplar quality with GPT-3's context length constraints
  - Zero-shot vs. few-shot initialization: Zero-shot can bypass manual effort but may start from a weaker position

- Failure signatures:
  - Low accuracy with small pool sizes (<20 exemplars): Indicates insufficient diversity in the candidate pool
  - High variance in random selection baseline: Suggests poor quality in the augmented pool
  - Degraded performance when transferring exemplars: May indicate domain mismatch between training and target tasks

- First 3 experiments:
  1. Test pool size sensitivity: Run with pool sizes of 10, 20, 50, 100 and measure accuracy improvements
  2. Validate pruning effectiveness: Compare performance with and without the pruning step using a fixed pool
  3. Evaluate selection algorithm: Test random selection vs. VR-PGE selection on the same pool to isolate the selection contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal pool size for the Automate-CoT method across different reasoning task types?
- Basis in paper: [explicit] The paper mentions that pool size affects performance and that performance improves as pool size increases beyond 20 exemplars, but doesn't identify an optimal size
- Why unresolved: The paper only tested up to 150 pool size and mentions that performance would likely keep increasing, but the relationship between pool size and performance across different task types wasn't fully explored
- What evidence would resolve it: Systematic testing of Automate-CoT with varying pool sizes (e.g., 10, 20, 50, 100, 150, 200) across all reasoning task types to identify the point of diminishing returns for each category

### Open Question 2
- Question: How does Automate-CoT performance change when applied to more complex reasoning tasks requiring multi-hop reasoning beyond 9 steps?
- Basis in paper: [inferred] The paper tested up to 9-hop reasoning in Complex-CoT experiments but didn't explore Automate-CoT's effectiveness on tasks requiring even more complex reasoning chains
- Why unresolved: The paper's analysis focused on reasoning chains with 2-9 hops, leaving open questions about Automate-CoT's scalability to more complex reasoning scenarios
- What evidence would resolve it: Testing Automate-CoT on benchmark datasets requiring 10+ reasoning steps and comparing performance against manual CoT approaches on these complex tasks

### Open Question 3
- Question: What is the relationship between exemplar diversity (in terms of reasoning complexity and style) and Automate-CoT's performance?
- Basis in paper: [explicit] The paper identifies "diversity" as one of four critical factors affecting CoT performance and found that a combination of different complexity levels outperforms using only complex exemplars
- Why unresolved: While the paper shows that diversity matters, it doesn't quantify the optimal balance between simple, medium, and complex exemplars, or explore how diversity in reasoning style affects performance
- What evidence would resolve it: Controlled experiments varying the diversity metrics of selected exemplars (complexity distribution, reasoning style variation) while keeping total exemplar count constant to measure performance impact

## Limitations

- The pruning mechanism assumes correct answers indicate correct reasoning, which may not always hold true
- The variance-reduced policy gradient approach requires multiple sampling iterations, which may be computationally intensive for high-dimensional exemplar spaces
- The quality of generated rationale chains may vary significantly across different domains and task types

## Confidence

- **High Confidence**: The overall three-step pipeline (augment → prune → select) is well-specified and produces measurable improvements across multiple tasks
- **Medium Confidence**: The variance-reduced policy gradient strategy effectively optimizes exemplar selection, though specific implementation details may impact performance
- **Low Confidence**: The claim that pruned chains are guaranteed to contain correct reasoning, as this assumption is not directly validated

## Next Checks

1. **Reasoning Quality Validation**: Manually examine a sample of pruned rationale chains to verify whether correct answers correspond to valid reasoning paths, not just coincidental correctness.

2. **Ablation Study on Selection Strategy**: Compare VR-PGE selection against alternative optimization methods (e.g., reinforcement learning with different estimators, or greedy selection) to isolate the contribution of the variance reduction component.

3. **Cross-Domain Transfer Analysis**: Test the transferability of selected exemplars by training on one domain (e.g., GSM8K) and evaluating on a different but related domain (e.g., ASDiv) to measure domain generalization capabilities.