---
ver: rpa2
title: On The Open Prompt Challenge In Conditional Audio Generation
arxiv_id: '2311.00897'
source_url: https://arxiv.org/abs/2311.00897
tags:
- prompts
- audio
- user
- clap
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of open prompts in text-to-audio
  (TTA) generation, where user-input prompts are often under-specified compared to
  training prompts, leading to alignment issues. The authors propose using instruction-tuned
  large language models to rewrite prompts and improve upon them with audio feedback
  and margin ranking learning.
---

# On The Open Prompt Challenge In Conditional Audio Generation

## Quick Facts
- arXiv ID: 2311.00897
- Source URL: https://arxiv.org/abs/2311.00897
- Reference count: 0
- Primary result: Prompt rewriting with instruction-tuned LLMs and audio feedback improves text-audio alignment (1.58-2.59% CLAP increase) and human audio preference (10.4-31.6% increase)

## Executive Summary
This paper addresses the open prompt challenge in text-to-audio (TTA) generation, where user-input prompts are often under-specified compared to training prompts, leading to alignment issues. The authors propose using instruction-tuned large language models to rewrite prompts and improve upon them with audio feedback and margin ranking learning. Their approach, termed Instruct+Feedback, shows marked improvements in both text-audio alignment and music audio quality.

## Method Summary
The method uses instruction-tuned LLMs (FLAN-T5 and LaMini-LM) to rewrite user prompts into more descriptive versions. These rewritten prompts are then ranked using CLAP scores as a proxy for text-audio alignment, with margin ranking loss applied to fine-tune the LLM. The process uses a small training set (5-10 samples) to avoid degrading the original prompt intent while improving alignment with the target TTA model's "audionese" distribution.

## Key Results
- 1.58-2.59% increase in CLAP scores compared to original prompts
- 10.4-31.6% increase in subjective audio preference in human evaluations
- 1.53-3.71 point increase in objective relevance scores
- Improvements plateau at 10 training samples

## Why This Works (Mechanism)

### Mechanism 1
Instruction-tuned LLMs can rewrite vague user prompts into more descriptive, expert-like prompts that improve alignment with training distributions. The LLM uses its pretraining on diverse text and instruction tuning to infer missing audio details (e.g., instrumentation, style, mood) and rewrite the prompt accordingly. Core assumption: The LLM has sufficient world knowledge and instruction-following capability to extrapolate from abstract prompts to richly descriptive ones. Evidence anchors: [abstract], [section], [corpus]. Break condition: If the LLM lacks knowledge of musical terminology or fails to understand abstract concepts in the prompt, the rewritten prompt will not improve alignment.

### Mechanism 2
Margin ranking loss using CLAP scores as feedback aligns LLM outputs with "audionese," the prompt distribution that yields high-quality audio for the TTA model. Prompts are ranked by CLAP score, and the model is trained to prefer higher-scoring prompts, implicitly learning the text-audio alignment distribution. Core assumption: CLAP scores are a reliable proxy for human-perceived audio quality and text-audio alignment. Evidence anchors: [abstract], [section], [corpus]. Break condition: If CLAP scores do not correlate well with human preference or audio quality, the model will optimize for the wrong objective.

### Mechanism 3
Small-scale fine-tuning (5-10 samples) with margin ranking is sufficient to adapt the LLM to the target TTA model's "audionese" without overfitting or losing original prompt intent. Limited training on ranked prompt pairs allows the LLM to learn domain-specific rewriting patterns while preserving diversity. Core assumption: The TTA model's encoder-decoder mapping is stable enough that a small sample of high-quality prompts is representative of the full "audionese" distribution. Evidence anchors: [abstract], [section], [corpus]. Break condition: If the "audionese" distribution is highly multimodal or the TTA model is sensitive to prompt phrasing, a small sample will not capture the full range of high-quality prompts.

## Foundational Learning

- Concept: Text-audio alignment and the role of encoders in conditional generation
  - Why needed here: Understanding how text embeddings map to audio latents is critical to grasping why prompt rewriting affects output quality.
  - Quick check question: How does a text encoder in a TTA model transform a prompt into a latent space that the audio decoder can use?

- Concept: Contrastive learning and its use in embedding alignment (e.g., CLAP)
  - Why needed here: CLAP's contrastive training is the basis for its use as a text-audio alignment metric.
  - Quick check question: What is the objective function used to train models like CLAP to align audio and text embeddings?

- Concept: Instruction tuning and its effect on LLM behavior
  - Why needed here: The success of the approach hinges on the LLM's ability to follow complex rewriting instructions.
  - Quick check question: How does instruction tuning differ from standard pretraining, and what capabilities does it confer?

## Architecture Onboarding

- Component map: User input → LLM prompt rewriter (Instruct+Feedback) → CLAP-scored prompt → TTA model → Generated audio
- Critical path: Prompt rewriting → TTA generation → CLAP evaluation → Human evaluation
- Design tradeoffs:
  - Model size vs. latency: Smaller LLMs are faster but may produce less detailed rewrites
  - Training sample size vs. prompt fidelity: More samples improve CLAP but risk deviating from user intent
  - CLAP vs. human evaluation: CLAP is fast and scalable but may not capture all aspects of audio quality
- Failure signatures:
  - Low SacreBLEU score: LLM rewrites have strayed too far from the original prompt
  - High CLAP but low human preference: CLAP is not a perfect proxy for quality
  - Degraded performance on expert prompts: Model has overfit to open prompts
- First 3 experiments:
  1. Run Instruct+0-shot on a held-out set of user prompts and measure CLAP vs. original
  2. Fine-tune with 5 ranked samples and evaluate SacreBLEU/CLAP tradeoff
  3. Compare FLAN-T5 vs. LaMini-LM performance on the same task

## Open Questions the Paper Calls Out

### Open Question 1
How can CLAP-based feedback be optimized to improve text-audio alignment without significantly deviating from the original user intent? Basis in paper: [explicit] The paper discusses using CLAP-based feedback for prompt enhancement but notes a trade-off between alignment improvement and maintaining original intent, as indicated by SacreBLEU scores. Why unresolved: The challenge lies in balancing the enhancement of text-audio alignment while preserving the semantic integrity of user prompts, as evidenced by the need to monitor SacreBLEU scores during training. What evidence would resolve it: Experimental results demonstrating a method to optimize CLAP-based feedback that achieves high text-audio alignment scores without compromising SacreBLEU scores would provide a solution.

### Open Question 2
What are the implications of using different model architectures (encoder-decoder vs. decoder-only) on the effectiveness of prompt rewriting for TTA? Basis in paper: [explicit] The paper observes differences in performance between FLAN-T5 (encoder-decoder) and LaMini-LM (decoder-only) models, with encoder-decoder models showing more robust results. Why unresolved: The paper suggests that the choice of architecture impacts the quality of prompt rewriting, but does not fully explore the underlying reasons or potential improvements. What evidence would resolve it: Comparative studies analyzing the impact of different model architectures on prompt rewriting quality and alignment would clarify their implications.

### Open Question 3
How can the correlation between CLAP scores and human preference for audio quality be improved? Basis in paper: [explicit] The paper notes a low correlation coefficient (0.242) between CLAP scores and human preference, indicating a limitation in using CLAP as a sole metric. Why unresolved: The current metrics do not fully capture the nuances of audio quality as perceived by humans, necessitating better evaluation methods. What evidence would resolve it: Development and validation of new metrics that incorporate human preference and temporal audio information, showing improved correlation with subjective evaluations, would address this issue.

## Limitations
- SacreBLEU scores dropping below 20 points indicate significant deviation from user intent
- Human evaluation methodology is underspecified (sample sizes, rater expertise, criteria unclear)
- Approach may not generalize to prompts outside training distribution or different TTA architectures

## Confidence
- Mechanism 1 (LLM Prompt Rewriting): Low confidence
- Mechanism 2 (CLAP-based Margin Ranking): Medium confidence
- Mechanism 3 (Small-Scale Fine-Tuning): Low confidence

## Next Checks
1. **Prompt Fidelity Analysis**: Measure SacreBLEU scores across different prompt types (open vs. expert) to quantify how much the rewriting deviates from user intent.
2. **CLAP Correlation Study**: Conduct a controlled human evaluation comparing CLAP scores against human preference ratings for a diverse set of prompts.
3. **Sample Efficiency Test**: Vary the number of training samples (2, 5, 10, 20) and measure both CLAP improvement and SacreBLEU degradation.