---
ver: rpa2
title: A Simple and Scalable Graph Neural Network for Large Directed Graphs
arxiv_id: '2306.08274'
source_url: https://arxiv.org/abs/2306.08274
tags:
- graphs
- node
- adjacency
- directed
- undirected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses node classification on large directed graphs
  by benchmarking combinations of node representations (aggregated features vs. adjacency
  lists) and edge direction awareness (directed vs.
---

# A Simple and Scalable Graph Neural Network for Large Directed Graphs

## Quick Facts
- arXiv ID: 2306.08274
- Source URL: https://arxiv.org/abs/2306.08274
- Reference count: 40
- Key outcome: A2DUG achieves up to 11.29% accuracy improvement over state-of-the-art methods on large directed graphs while maintaining scalability to graphs with over 300 million edges

## Executive Summary
This paper addresses node classification on large directed graphs by empirically benchmarking combinations of node representations and edge direction awareness. Through experiments across eight datasets, the authors show no single method consistently achieves state-of-the-art results. They propose A2DUG, a simple method that combines all representation combinations through MLPs and precomputation-based GNNs, achieving stable performance and scalability to graphs exceeding 300 million edges.

## Method Summary
A2DUG leverages both aggregated features (via GNNs) and adjacency lists (via MLPs) across directed and undirected graph variants, combining them through concatenation and a final MLP. The model uses precomputation-based GNNs where feature aggregation is computed as preprocessing, enabling efficient training with small input batches. It includes representations from both original and transposed adjacency matrices to capture potential semantic differences between original and inverse edges.

## Key Results
- A2DUG achieves stable performance across diverse datasets without requiring dataset-specific architecture tuning
- Improves accuracy by up to 11.29% over state-of-the-art methods on datasets like arxiv-year, snap-patents, and wiki
- Maintains scalability to graphs with over 300 million edges through precomputation-based GNNs
- Ablation studies show that removing any component degrades performance, validating the importance of combining all representation sources

## Why This Works (Mechanism)

### Mechanism 1
- Adaptive combination of aggregated features and adjacency lists improves classification robustness
- Core assumption: Aggregated features and adjacency lists capture complementary aspects of graph structure
- Evidence: Empirical performance gains across datasets, though no corpus validation
- Break condition: If one representation becomes consistently irrelevant, the combination adds unnecessary overhead

### Mechanism 2
- Precomputation-based GNNs enable scalability to large graphs
- Core assumption: Precomputing feature aggregation is more efficient than computing during training
- Evidence: Scalability demonstrated on graphs with 300M+ edges, though limited corpus support
- Break condition: Dynamic graphs requiring frequent recomputation of features

### Mechanism 3
- Using both original and transposed adjacency matrices captures bidirectional edge semantics
- Core assumption: Inverse edges may have different semantics from original edges in directed graphs
- Evidence: Intuitive justification, but limited empirical validation
- Break condition: If inverse edges have identical semantics to original edges, this adds redundancy

## Foundational Learning

- Graph Neural Networks (GNNs) and message-passing: Understanding GNN aggregation mechanisms is fundamental to grasping why precomputation is efficient and how aggregated features differ from adjacency lists. Quick check: What is the key difference between using adjacency lists as features versus aggregating neighbor features in GNNs?

- Directed vs. undirected graph representations: The paper explicitly compares methods using directed graphs, undirected graphs, or both, making edge directionality understanding essential. Quick check: How does treating a directed graph as undirected potentially lose information compared to keeping the directionality?

- Feature aggregation and normalization techniques: The paper uses degree normalization for undirected graphs and discusses various aggregation methods critical for understanding the architecture. Quick check: Why does the paper use degree normalization for undirected graphs but not for directed graphs?

## Architecture Onboarding

- Component map: Input layer (node features X, adjacency matrix A, transposed adjacency matrix A⊤, undirected adjacency matrix Aund) → MLPs and GNNs for feature extraction (HA, HA⊤, HAund, HX, HGNN, HGNN⊤, HGNNund) → Concatenation layer → Final MLP (MLPfinal) → Predicted labels Y

- Critical path: X, A, A⊤, Aund → MLPs and GNNs → Concatenation → MLPfinal → Y

- Design tradeoffs: Using all combinations provides robustness but increases computational cost; precomputation improves scalability but requires additional memory; including transposed graphs captures more information but may add redundancy

- Failure signatures: Poor performance on specific datasets despite good average performance; memory overflow when precomputing features for extremely large graphs; training instability when combining too many heterogeneous feature representations

- First 3 experiments: 1) Reproduce ablation study results (Table 5) to verify component importance; 2) Test scalability by running on largest dataset (wiki) and measuring training time/memory usage; 3) Compare performance on directed graphs where edge direction is semantically important versus those where it's not

## Open Questions the Paper Calls Out

### Open Question 1
- How can we develop an optimal model architecture that adaptively combines aggregated features and adjacency lists for directed and undirected graphs? The paper demonstrates the potential but uses a simple architecture that could be improved.

### Open Question 2
- What is the impact of class imbalance on the effectiveness of GNNs that combine aggregated features and adjacency lists? The paper acknowledges this limitation but doesn't investigate how class imbalance affects the proposed approach specifically.

### Open Question 3
- How does the importance of directed vs. undirected graph representations vary across different prediction tasks beyond publication years and research fields? The paper only demonstrates this for two specific datasets with different label types.

## Limitations
- Limited empirical validation across only eight datasets, which may not generalize to other graph types
- Scalability claims haven't been validated on graphs larger than 300 million edges
- Adaptive combination mechanism is empirically effective but lacks theoretical justification

## Confidence

- **High Confidence**: Claims about A2DUG's scalability through precomputation-based GNNs, supported by strong empirical evidence on large graphs
- **Medium Confidence**: Claims about adaptive combination of aggregated features and adjacency lists, supported by ablation studies but lacking theoretical explanation
- **Low Confidence**: Claims about bidirectional edge representation semantics, based primarily on intuition without strong empirical or theoretical validation

## Next Checks

1. Cross-domain validation: Test A2DUG on graphs from different domains (biological networks, social networks with different structures) to assess generalizability beyond current datasets

2. Scalability stress test: Evaluate performance on graphs with 1+ billion edges to determine if precomputation-based approach remains efficient at extreme scales

3. Theoretical analysis: Develop theoretical bounds or explanations for when adaptive combination provides benefits versus when it might be redundant, potentially through controlled synthetic graph experiments