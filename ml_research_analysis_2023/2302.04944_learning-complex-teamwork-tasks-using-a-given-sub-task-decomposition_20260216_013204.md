---
ver: rpa2
title: Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
arxiv_id: '2302.04944'
source_url: https://arxiv.org/abs/2302.04944
tags:
- task
- medoe
- source
- agent
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training multi-agent teams
  to complete complex tasks using reinforcement learning, where large joint policy
  spaces and non-stationarity due to mutually adapting agents make learning difficult.
  The proposed approach, MEDoE, leverages a curriculum of simpler multi-agent sub-tasks
  provided by an expert.
---

# Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition

## Quick Facts
- arXiv ID: 2302.04944
- Source URL: https://arxiv.org/abs/2302.04944
- Reference count: 20
- Key outcome: MEDoE uses domain of expertise classifiers to modulate exploration and learning rates, significantly outperforming training from scratch on Chainball and Overcooked environments.

## Executive Summary
This paper addresses the challenge of training multi-agent teams to complete complex tasks using reinforcement learning, where large joint policy spaces and non-stationarity due to mutually adapting agents make learning difficult. The proposed approach, MEDoE, leverages a curriculum of simpler multi-agent sub-tasks provided by an expert. Agents are trained on these sub-tasks to acquire sub-task-specific skills, then merged and fine-tuned on the complex target task. MEDoE uses domain of expertise classifiers to identify when each agent's existing sub-task policy is adequate, modulating exploration and learning rates accordingly. The method is evaluated on Chainball and Overcooked environments, showing that MEDoE significantly outperforms training from scratch and naïve fine-tuning approaches, requiring fewer total training timesteps to solve the complex tasks. Notably, MEDoE can also learn adequate domain of expertise classifiers from source task experience, demonstrating its practical applicability.

## Method Summary
MEDoE (Modulating Exploration and Training via Domain of Expertise) addresses the challenge of learning complex multi-agent tasks by leveraging a curriculum of simpler sub-tasks. The approach involves training sub-teams on individual sub-tasks to acquire specialized skills, then merging these sub-teams and fine-tuning on the target task. During fine-tuning, MEDoE uses domain of expertise (DoE) classifiers to determine when each agent's existing sub-task policy is adequate. Based on the DoE classifier outputs, MEDoE modulates exploration and learning rates, reducing non-stationarity by lowering policy learning rates and exploration temperatures for agents in their domain of expertise. The method also employs KL-regularized policies to retain useful sub-task behaviors during fine-tuning, and modulates entropy regularization to encourage exploration for non-experts while keeping experts predictable.

## Key Results
- MEDoE significantly outperforms training from scratch and naïve fine-tuning approaches on Chainball and Overcooked environments
- Requires fewer total training timesteps to solve complex tasks compared to baseline methods
- Can learn adequate domain of expertise classifiers from source task experience, demonstrating practical applicability
- Successfully leverages sub-task curricula to acquire and retain specialized skills during target task training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modulating exploration and learning rates based on domain of expertise reduces non-stationarity in MARL training.
- Mechanism: Agents use DoE classifiers to determine when their existing sub-task policies are adequate, then reduce policy learning rates and exploration temperatures in those situations to minimize disruption to other agents' learning processes.
- Core assumption: When an agent is in its domain of expertise, its existing policy is near-optimal for the current observation, making policy updates unnecessary.
- Evidence anchors:
  - [abstract]: "MEDoE uses domain of expertise classifiers to identify when each agent's existing sub-task policy is adequate, modulating exploration and learning rates accordingly"
  - [section]: "When an agent should exploit its existing policy, MEDoE reduces non-stationarity by reducing the policy learning rate"
  - [corpus]: Weak evidence; no direct mention of non-stationarity reduction in related papers
- Break condition: If DoE classifiers are inaccurate, agents may incorrectly suppress learning in situations requiring adaptation, leading to poor performance.

### Mechanism 2
- Claim: Behavior priors help agents retain useful sub-task skills during fine-tuning on the complex target task.
- Mechanism: Experts use KL-regularized policies that minimize divergence from their frozen source-stage policies, encouraging retention of learned sub-task behaviors.
- Core assumption: Sub-task policies contain valuable skills transferable to the target task, and these skills should be preserved during fine-tuning.
- Evidence anchors:
  - [abstract]: "MEDoE also controls the rate at which each agent forgets ineffective behaviours by modulating the entropy regularisation coefficient"
  - [section]: "we use KL-regularised policies...where we aim to minimise the KL divergence between the agent's current policy and its frozen source stage policy"
  - [corpus]: Weak evidence; no direct mention of behavior priors in related papers
- Break condition: If source-stage policies are suboptimal or irrelevant to the target task, behavior priors may hinder learning of necessary new behaviors.

### Mechanism 3
- Claim: Modulating entropy regularization for non-experts encourages exploration of new behaviors while experts remain predictable.
- Mechanism: Non-expert agents receive increased entropy regularization to encourage exploration of new behaviors, while expert agents maintain lower entropy to remain predictable to teammates.
- Core assumption: Agents need different exploration strategies based on whether they're in their domain of expertise, and this can be effectively modulated through entropy regularization.
- Evidence anchors:
  - [abstract]: "MEDoE uses domain of expertise (DoE) classifier to determine when each agent's existing policy is likely to be adequate"
  - [section]: "We use entropy-regularised policies, and encourage non-experts to forget irrelevant skills by increasing non-experts' entropy regularisation coefficient"
  - [corpus]: Weak evidence; no direct mention of entropy modulation in related papers
- Break condition: If entropy modulation is too aggressive, non-experts may explore excessively and experts may become too rigid.

## Foundational Learning

- Concept: Domain of Expertise (DoE) Classification
  - Why needed here: DoE classifiers are the core mechanism that enables MEDoE to modulate learning and exploration appropriately for each agent.
  - Quick check question: How would you determine if an observation belongs to an agent's domain of expertise without ground truth knowledge?

- Concept: Multi-Agent Credit Assignment
  - Why needed here: Understanding credit assignment is crucial for appreciating why non-stationarity is a problem and why MEDoE's approach of reducing learning rates for experts helps.
  - Quick check question: Why does non-stationarity caused by simultaneously adapting agents make credit assignment more difficult?

- Concept: Curriculum Learning in Multi-Agent Systems
  - Why needed here: MEDoE builds on the concept of using a curriculum of simpler sub-tasks to facilitate learning of complex tasks, which is fundamental to understanding the approach.
  - Quick check question: What are the potential advantages and disadvantages of using a sub-task curriculum versus training from scratch on the complex task?

## Architecture Onboarding

- Component map: Source stage -> DoE classifiers -> MEDoE controller -> Fine-tuning stage -> Evaluation
- Critical path:
  1. Train sub-teams on sub-tasks
  2. Train or obtain DoE classifiers
  3. Merge sub-teams and initialize MEDoE parameters
  4. Fine-tune on target task with MEDoE modulation
  5. Evaluate performance
- Design tradeoffs:
  - Expert vs. learned DoE classifiers: Expert classifiers require domain knowledge but are more reliable; learned classifiers are more flexible but may be less accurate
  - Strength of behavior priors: Stronger priors preserve skills better but may hinder learning new behaviors; weaker priors allow more adaptation but risk forgetting useful skills
  - Exploration modulation: More aggressive modulation can lead to faster convergence but may get stuck in local optima; conservative modulation is more stable but slower
- Failure signatures:
  - Slow convergence: May indicate DoE classifiers are too conservative, behavior priors are too strong, or exploration modulation is too weak
  - Poor final performance: May indicate DoE classifiers are inaccurate, behavior priors are hindering necessary adaptation, or source-stage training was insufficient
  - Unstable training: May indicate DoE classifiers are too aggressive, exploration modulation is too strong, or source-stage policies are incompatible with target task
- First 3 experiments:
  1. Verify that MEDoE with expert DoE classifiers outperforms training from scratch on a simple task
  2. Test learned DoE classifiers to ensure they can match the performance of expert classifiers
  3. Ablate behavior priors to quantify their contribution to MEDoE's performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain of expertise classifiers be learned more effectively from source task experience, particularly for agents with complex or overlapping domains of expertise?
- Basis in paper: [explicit] The paper mentions that the learned DoE classifier performs poorly for the "Right" task agent in Overcooked, suggesting further work to obtain higher quality DoE classifiers might be useful.
- Why unresolved: The current approach of using a simple MLP classifier trained on source task data is insufficient for capturing complex relationships between observations and domains of expertise. The classifier struggles when there is overlap or ambiguity in the domains.
- What evidence would resolve it: Demonstrating improved performance of MEDoE using a more sophisticated DoE classifier (e.g., hierarchical, attention-based, or graph neural network approaches) that can better capture complex relationships between observations and domains of expertise.

### Open Question 2
- Question: What are the optimal strategies for automatically designing sub-task curricula without expert knowledge, and how can the complexity of sub-tasks be balanced to ensure effective learning transfer?
- Basis in paper: [inferred] The paper assumes an expert-provided sub-task curriculum, but acknowledges that automated sub-task decomposition based on experience in the complex task is an open challenge.
- Why unresolved: Automatically generating effective sub-task curricula requires solving complex problems of task decomposition, difficulty scaling, and ensuring that the skills learned in sub-tasks are relevant and transferable to the target task. The paper does not address this challenge.
- What evidence would resolve it: Developing and evaluating an automated method for generating sub-task curricula that consistently improves learning efficiency compared to training from scratch across diverse multi-agent tasks.

### Open Question 3
- Question: How does the performance of MEDoE scale with the number of agents and the complexity of coordination required in the target task, and what are the computational bottlenecks?
- Basis in paper: [explicit] The paper mentions that MEDoE can be used with any number of agents and that the total computational cost scales linearly with the number of agents, but does not provide empirical evidence for large-scale tasks.
- Why unresolved: While MEDoE is theoretically applicable to any number of agents, the practical challenges of scaling to large teams (e.g., increased non-stationarity, communication overhead, and exploration difficulties) are not explored. The paper focuses on relatively small teams.
- What evidence would resolve it: Empirical evaluation of MEDoE on tasks with a significantly larger number of agents (e.g., 10+ agents) and complex coordination requirements, demonstrating its scalability and identifying computational bottlenecks.

## Limitations
- Reliance on expert-provided task decompositions may not always be available or optimal
- Learned DoE classifiers show promise but their performance in complex, dynamic environments remains unverified
- Evaluation focuses on specific environments (Chainball and Overcooked) without extensive testing on diverse task types
- Approach requires careful tuning of multiple hyperparameters which may not transfer well across different domains

## Confidence
- High: MEDoE reduces non-stationarity through modulated learning rates when agents are in their domain of expertise, supported by empirical results
- Medium: Effectiveness of behavior priors for skill retention, primarily from ablation studies
- Medium: Generalizability to arbitrary task decompositions, given limited evaluation environments

## Next Checks
1. **Cross-domain generalization test**: Apply MEDoE to a new set of multi-agent tasks with varying complexity levels and team sizes to assess its robustness across different problem structures and determine if the learned DoE classifiers maintain performance across domains.

2. **Ablation on DoE accuracy**: Systematically vary the accuracy of DoE classifiers (through controlled noise injection) to quantify the sensitivity of MEDoE performance to classifier quality and establish minimum accuracy thresholds for effectiveness.

3. **Memory efficiency analysis**: Measure and compare the memory overhead of MEDoE (including DoE classifiers and experience buffers) against baseline approaches to evaluate its practical scalability for larger teams and more complex tasks.