---
ver: rpa2
title: 'DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence
  Transformers and Reciprocal-Rank Fusion'
arxiv_id: '2308.12877'
source_url: https://arxiv.org/abs/2308.12877
tags:
- normalization
- performance
- drug
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a system for adverse drug event (ADE) normalization
  in Twitter data, developed for SMM4H 2023 Task 5. The approach uses a two-stage
  pipeline: first, BERTweet is fine-tuned for ADE entity recognition using BIO tagging,
  then zero-shot normalization maps identified ADE mentions to MedDRA concepts using
  multiple sentence transformers and reciprocal-rank fusion (RRF).'
---

# DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion

## Quick Facts
- arXiv ID: 2308.12877
- Source URL: https://arxiv.org/abs/2308.12877
- Reference count: 0
- The system achieved F1-score of 42.6%, precision of 44.9%, and recall of 40.5% on the test set

## Executive Summary
This paper presents a system for adverse drug event (ADE) normalization in Twitter data for SMM4H 2023 Task 5. The approach uses a two-stage pipeline: BERTweet fine-tuning for entity recognition with BIO tagging, followed by zero-shot normalization using multiple sentence transformers and reciprocal-rank fusion (RRF). The system achieved an F1-score of 42.6%, ranking highest among all participants and outperforming median scores by 10%. Performance dropped to 29.2% F1 on unseen ADEs, indicating limitations in entity recognition generalization. The approach demonstrates strong potential for social media pharmacovigilance.

## Method Summary
The system employs a two-stage pipeline for ADE normalization. First, BERTweet is fine-tuned for entity recognition using BIO tagging to identify ADE mentions in tweets. Second, zero-shot normalization maps these mentions to MedDRA concepts using five sentence transformers (including a custom UMLS-pretrained model) and reciprocal-rank fusion. The RRF aggregates cosine similarity rankings from all transformers using a ranking constant of 46, producing a consensus ranking that links each ADE mention to its most likely MedDRA preferred term.

## Key Results
- Achieved F1-score of 42.6%, precision of 44.9%, and recall of 40.5% on the test set
- Outperformed median participant scores by 10% and ranked highest among all participants
- Performance dropped to F1 29.2% on unseen ADEs, suggesting entity recognition limitations
- Zero-shot approach successfully normalized ADE mentions without requiring training on target classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage pipeline achieves high performance by separating structured entity extraction from semantic mapping
- Mechanism: BERTweet fine-tuning with BIO tagging identifies ADE spans as labeled tokens, converting unstructured tweets into structured output. This structured output is normalized using sentence transformer similarity without requiring class-specific training, enabling generalization across MedDRA concepts
- Core assumption: Named entity recognition accuracy is sufficient to feed high-quality input to normalization, and semantic similarity reliably maps mentions to correct MedDRA terms
- Evidence anchors: Abstract mentions the two-stage approach; methods section describes zero-shot normalization via RRF; no direct corpus citations supporting separation
- Break condition: If entity recognition fails to detect ADE spans accurately (especially for unseen ADEs), normalization will be fed incorrect inputs, causing mapping errors regardless of transformer quality

### Mechanism 2
- Claim: RRF aggregates multiple sentence transformer rankings to improve robustness and reduce reliance on any single embedding space
- Mechanism: Each ADE mention is compared to every MedDRA LLT using cosine similarity in five different transformer embedding spaces. RRF combines these ranked lists using a ranking constant (k=46) to produce a consensus ranking, more stable than any single model's output
- Core assumption: Different sentence transformers capture complementary semantic features, and their combined rankings produce better coverage than individual rankings
- Evidence anchors: Abstract mentions RRF with ranking constant 46; methods section describes aggregation into final rank; no corpus citations validating RRF's effectiveness specifically for this task
- Break condition: If embedding spaces are too similar or one transformer dominates rankings, RRF provides little benefit and may degrade performance due to conflicting signals

### Mechanism 3
- Claim: Pre-training a custom sentence transformer on UMLS English synonyms handles linguistic variations in medical terms, improving zero-shot normalization for informal social media text
- Mechanism: The custom transformer learns embeddings that map synonymous medical expressions close together in vector space, allowing recognition of semantically equivalent but lexically different ADE mentions in tweets
- Core assumption: Social media ADE mentions contain significant lexical variation and informal synonyms that standard biomedical embeddings fail to capture, and UMLS pre-training bridges this gap
- Evidence anchors: Methods section mentions custom transformer pre-trained on UMLS English synonyms; abstract does not mention UMLS pre-training benefits; no corpus citations showing UMLS pre-training improves social media ADE normalization
- Break condition: If UMLS synonyms do not overlap well with social media language patterns, the custom transformer may not provide meaningful improvements over general-purpose biomedical embeddings

## Foundational Learning

- Concept: Named Entity Recognition (NER) with BIO tagging
  - Why needed here: The first stage requires converting raw tweet text into labeled ADE spans so that normalization can operate on discrete mentions rather than entire tweets
  - Quick check question: How would you label the tweet "Feeling dizzy after taking the new medicine" using BIO tags for ADE spans?

- Concept: Zero-shot learning via semantic similarity
  - Why needed here: The normalization stage must map ADE mentions to MedDRA concepts without having seen those specific concept-label pairs during training, requiring semantic understanding rather than memorization
  - Quick check question: What distance metric is used to compare sentence embeddings in this system, and why is it appropriate for semantic matching?

- Concept: Reciprocal-rank fusion for ensemble ranking
  - Why needed here: Combining multiple transformer rankings into a single consensus ranking reduces the risk of any single model's bias affecting the final ADE-to-MedDRA mapping
  - Quick check question: What role does the ranking constant k=46 play in the RRF calculation, and how would changing it affect the fusion results?

## Architecture Onboarding

- Component map:
  Input: Raw tweet text -> BERTweet NER with BIO tagging -> Extracted ADE spans -> 5 transformer similarity computations -> RRF aggregation -> MedDRA preferred term output

- Critical path: Tweet → BERTweet NER → ADE spans → 5 transformer similarity computations → RRF fusion → MedDRA mapping

- Design tradeoffs:
  - Zero-shot normalization vs. supervised classification: Zero-shot allows handling unseen ADEs but may be less precise than supervised methods for frequent, well-represented concepts
  - Multiple transformers vs. single transformer: Ensembling increases robustness but multiplies computational cost and latency
  - BIO tagging vs. other NER schemes: BIO is standard and effective but requires careful handling of overlapping or nested entities

- Failure signatures:
  - Low precision but high recall: Entity recognition is over-generating ADE spans, feeding incorrect mentions to normalization
  - Low recall overall: Entity recognition model is too conservative, missing ADE mentions entirely
  - Inconsistent results across runs: Random initialization in BERTweet fine-tuning or transformer similarity computations affecting reproducibility
  - Poor performance on unseen ADEs: Entity recognition model lacks generalization to novel ADE expressions

- First 3 experiments:
  1. Evaluate BERTweet NER performance on a held-out validation set with BIO F1-score to establish baseline entity recognition quality before normalization
  2. Test normalization performance using only the custom UMLS-pretrained transformer (single-model baseline) to quantify the benefit of ensemble fusion
  3. Compare RRF performance against simple average ranking or maximum similarity ranking to validate the choice of fusion method

## Open Questions the Paper Calls Out

- How would incorporating large autoregressive language models improve the generalizability of the entity recognition stage for unseen ADEs?
  - Basis in paper: The authors hypothesize that the reduction in performance for unseen ADEs is likely due to limited generalizability of the named entity recognition model and suggest leveraging the latest advancements in large autoregressive language models
  - Why unresolved: The paper mentions this as a future direction but does not implement or evaluate such an approach
  - What evidence would resolve it: Comparative experiments showing improved performance on unseen ADEs when using large autoregressive language models versus the current BERTweet approach

- Would expanding the validation set size reduce the discrepancy in performance between validation and test sets?
  - Basis in paper: The authors hypothesize that the discrepancy in performance between validation and test sets may be attributed to the limited sample size in the validation data
  - Why unresolved: The paper does not test this hypothesis by increasing the validation set size or using cross-validation
  - What evidence would resolve it: Performance metrics on a larger or more comprehensive validation set showing closer alignment with test set results

- How would fine-tuning the sentence transformers on task-specific ADE data affect the zero-shot normalization performance?
  - Basis in paper: The current approach uses pre-trained sentence transformers with zero-shot inference, but the authors do not explore fine-tuning these models on ADE-specific data
  - Why unresolved: The paper focuses on a zero-shot approach and does not experiment with fine-tuning the sentence transformers
  - What evidence would resolve it: Comparative experiments showing performance differences between zero-shot and fine-tuned sentence transformer approaches for ADE normalization

## Limitations

- The entity recognition stage shows reduced effectiveness on unseen ADEs (F1 drops to 29.2%), suggesting the BERTweet model may overfit to training patterns rather than learning generalizable ADE features
- The UMLS pre-training benefits remain unverified against alternative biomedical embeddings, and the RRF aggregation method's superiority over simpler ensemble approaches lacks empirical validation
- Implementation details like fine-tuning hyperparameters and the custom transformer architecture are unspecified, limiting reproducibility

## Confidence

- **High confidence**: The two-stage pipeline architecture and overall performance metrics are well-documented and reproducible
- **Medium confidence**: The RRF aggregation method's contribution to performance, as the paper describes its use but doesn't benchmark against alternatives
- **Low confidence**: The specific benefits of UMLS pre-training and handling of linguistic variations, due to limited experimental validation

## Next Checks

1. Compare entity recognition performance on seen vs unseen ADEs to quantify generalization limitations and identify failure patterns
2. Benchmark RRF against simpler fusion methods (average ranking, maximum similarity) to isolate its contribution to the 42.6% F1 score
3. Test the system using alternative biomedical embeddings (e.g., ClinicalBERT, BlueBERT) to evaluate whether UMLS pre-training provides measurable advantages for social media normalization