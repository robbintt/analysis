---
ver: rpa2
title: Optimistic Active Exploration of Dynamical Systems
arxiv_id: '2306.12371'
source_url: https://arxiv.org/abs/2306.12371
tags:
- learning
- exploration
- opax
- information
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of task-agnostic exploration in
  unknown dynamical systems, aiming to learn a model that enables zero-shot planning
  for multiple downstream tasks. The proposed algorithm, OPAX, uses well-calibrated
  probabilistic models to quantify epistemic uncertainty and plans optimistically
  with respect to plausible dynamics to maximize information gain.
---

# Optimistic Active Exploration of Dynamical Systems

## Quick Facts
- **arXiv ID:** 2306.12371
- **Source URL:** https://arxiv.org/abs/2306.12371
- **Reference count:** 40
- **Key outcome:** OPAX achieves task-agnostic exploration in unknown dynamical systems using well-calibrated probabilistic models to quantify epistemic uncertainty and plan optimistically, demonstrating strong empirical performance across simulated environments.

## Executive Summary
This paper addresses the problem of task-agnostic exploration in unknown dynamical systems, aiming to learn a model that enables zero-shot planning for multiple downstream tasks. The proposed algorithm, OPAX, uses well-calibrated probabilistic models to quantify epistemic uncertainty and plans optimistically with respect to plausible dynamics to maximize information gain. This is achieved by solving an optimal control problem that encourages exploration in regions of high model uncertainty. The authors provide theoretical convergence guarantees for general Bayesian models and Gaussian process dynamics, showing that epistemic uncertainty converges to zero. Empirically, OPAX demonstrates strong performance in reducing model uncertainty and solving downstream tasks across various simulated environments, including high-dimensional object manipulation tasks, outperforming other heuristic active exploration methods.

## Method Summary
OPAX uses a well-calibrated probabilistic model to quantify epistemic uncertainty and plan optimistically with respect to plausible dynamics. It solves an optimal control problem where the reward is the information gain (negative entropy reduction) of the model given the planned trajectory. By adding βn(δ)σn−1(·)η(·) to the mean prediction µn−1, it generates optimistic trajectories that encourage visiting states with high epistemic uncertainty, thus maximizing information gain. The algorithm iterates between planning, execution, and model update, with theoretical guarantees on epistemic uncertainty convergence for general Bayesian models and Gaussian process dynamics.

## Key Results
- OPAX reduces epistemic uncertainty faster than heuristic methods (VIABLE, POPLIN) across various simulated environments
- The algorithm achieves strong zero-shot planning performance on downstream tasks using the learned model
- Theoretical guarantees show epistemic uncertainty convergence to zero for general Bayesian models and Gaussian process dynamics under specific assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OPAX's optimistic planner induces exploration in high epistemic uncertainty regions by hallucinating transitions with added noise.
- **Mechanism:** The algorithm solves an optimal control problem where the reward is the information gain (negative entropy reduction) of the model given the planned trajectory. By adding βn(δ)σn−1(·)η(·) to the mean prediction µn−1, it generates optimistic trajectories that encourage visiting states with high epistemic uncertainty, thus maximizing information gain.
- **Core assumption:** The statistical model (µn, σn, βn(δ)) is well-calibrated, meaning the epistemic uncertainty accurately reflects the model's ignorance.
- **Evidence anchors:**
  - [abstract] "It optimistically -- w.r.t. to plausible dynamics -- maximizes the information gain between the unknown dynamics and state observations."
  - [section] "We use the policy η to 'hallucinate' (pick) transitions that give us the most information."
  - [corpus] Weak - related papers focus on exploration but not specifically on this optimistic hallucination mechanism.
- **Break condition:** If the model is not well-calibrated, the optimism may lead to suboptimal exploration, visiting irrelevant or unsafe states.

### Mechanism 2
- **Claim:** The information gain objective is upper bounded by the sum of epistemic uncertainties, providing a tractable exploration reward.
- **Mechanism:** For Gaussian noise, the mutual information between the true dynamics and observations can be bounded by the sum of log terms involving the epistemic uncertainty and noise variance. This allows the complex information-theoretic objective to be reduced to a simple sum of epistemic uncertainties, which is easier to optimize.
- **Core assumption:** The process noise is i.i.d. Gaussian with known variance σ2 (Assumption 3).
- **Evidence anchors:**
  - [section] "Lemma 1 (Information gain is upper bounded by sum of epistemic uncertainties)."
  - [section] "This relates the information gain to the model epistemic uncertainty. Therefore, it gives a tractable objective that also has a frequentist interpretation."
  - [corpus] Weak - related papers mention information gain but not this specific upper bound derivation.
- **Break condition:** If the noise is not Gaussian or its variance is unknown, this bound may not hold, making the objective intractable.

### Mechanism 3
- **Claim:** For Gaussian process models, the epistemic uncertainty converges to zero in the reachable set as more data is collected, ensuring the learned model approaches the true dynamics.
- **Mechanism:** Under the assumption that the true dynamics f* satisfy Assumption 4 (lie in an RKHS with bounded norm) and the kernel's maximum information gain grows sublinearly, the bound on expected epistemic uncertainty along trajectories goes to zero. This implies that the epistemic uncertainty at any reachable state also converges to zero almost surely.
- **Core assumption:** The true dynamics f* satisfy Assumption 4 (lie in an RKHS with bounded norm) and the kernel's maximum information gain grows sublinearly.
- **Evidence anchors:**
  - [section] "Theorem 2... for all z ∈ R, and 1 ≤ j ≤ dx, σN,j(z) → 0 almost surely as N → ∞."
  - [section] "Kernels such as the RBF kernel or the linear kernel... have maximum information gain which grows polylogarithmically with n."
  - [corpus] Weak - related papers discuss GP models and convergence but not this specific RKHS convergence result.
- **Break condition:** If the true dynamics are not in the RKHS or the kernel's information gain grows too quickly (e.g., Matérn kernel), convergence may not be guaranteed.

## Foundational Learning

- **Concept:** Mutual Information and Information Gain
  - Why needed here: The paper's exploration objective is based on maximizing the mutual information between the unknown dynamics and observations, which quantifies the reduction in uncertainty about the dynamics.
  - Quick check question: What is the relationship between mutual information and entropy reduction?

- **Concept:** Epistemic Uncertainty in Bayesian Models
  - Why needed here: Epistemic uncertainty quantifies the model's ignorance about the true dynamics and is used as the exploration reward. Understanding how it's defined and estimated in models like GPs and BNNs is crucial.
  - Quick check question: How does epistemic uncertainty differ from aleatoric uncertainty in a probabilistic model?

- **Concept:** Optimism in the Face of Uncertainty (OFU) Principle
  - Why needed here: OPAX uses an optimistic planner that assumes the best-case dynamics within the uncertainty bounds, encouraging exploration of uncertain regions. This is a key algorithmic choice that distinguishes it from using the mean dynamics.
  - Quick check question: How does the optimistic planner in OPAX differ from simply using the mean estimate of the dynamics?

## Architecture Onboarding

- **Component map:** Statistical Model -> Optimistic Planner -> Data Collection -> Model Update
- **Critical path:**
  1. Initialize statistical model with empty dataset.
  2. For each episode:
     a. Use optimistic planner to solve optimal control problem and get exploration policy.
     b. Execute policy on real system, collect trajectory.
     c. Add trajectory to dataset.
     d. Update statistical model on new dataset.
  3. After sufficient exploration, use learned model for zero-shot planning on downstream tasks.

- **Design tradeoffs:**
  - Optimistic vs Mean Planner: Optimistic planner encourages more exploration but may be less stable; mean planner is more conservative but may get stuck in local optima.
  - GP vs PE Model: GP provides well-calibrated uncertainty and theoretical guarantees but may not scale to high dimensions; PE scales better but may have less reliable uncertainty estimates.
  - Exploration Horizon T: Longer horizons allow more exploration but increase computational cost and compounding error; shorter horizons are cheaper but may limit exploration.

- **Failure signatures:**
  - If epistemic uncertainty does not decrease over episodes, the model may not be learning effectively (check model capacity, data quality, or exploration strategy).
  - If the optimistic planner leads to unsafe or irrelevant exploration, the model calibration may be poor (check βn(δ) scaling, uncertainty estimates).
  - If downstream task performance is poor despite low epistemic uncertainty, the exploration may not have covered relevant state-action regions (check task similarity to exploration, model generalization).

- **First 3 experiments:**
  1. Implement the optimistic planner for a simple 1D dynamical system (e.g., pendulum) with a GP model. Verify that the planner encourages exploration in high uncertainty regions.
  2. Compare the performance of the optimistic planner vs mean planner on a 2D system (e.g., cartpole) in terms of epistemic uncertainty reduction and downstream task success.
  3. Test the scalability of the approach by applying it to a higher-dimensional system (e.g., reacher) with a PE model. Evaluate the tradeoff between model capacity and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the exponential dependence on horizon T in OPAX's regret bound fundamentally limit its applicability to long-horizon tasks, or can this be mitigated through alternative planning strategies?
- **Basis in paper:** [explicit] The paper explicitly states that "The exponential dependence of Equation (9) on the horizon T follows due to the compounding of errors when predicting through the learned model instead of the true one" and discusses potential approaches to alleviate this in Appendix B.
- **Why unresolved:** While the paper provides some theoretical analysis of the exponential dependence, it does not conclusively determine whether this is a fundamental limitation or if alternative planning strategies can effectively mitigate it.
- **What evidence would resolve it:** Empirical evaluation of OPAX on long-horizon tasks with and without the proposed modifications to alleviate the exponential dependence, comparing performance and sample efficiency.

### Open Question 2
- **Question:** How does the performance of OPAX compare to task-specific model-based RL algorithms when the downstream task is known during the exploration phase?
- **Basis in paper:** [explicit] The paper mentions that "Unlike active exploration agents, task-specific model-based RL agents only explore the regions of the state-action space that are relevant to the task at hand" and observes that H-UCRL (a task-specific algorithm) performs better on known tasks.
- **Why unresolved:** The paper does not directly compare OPAX to task-specific algorithms when the downstream task is known during exploration, leaving the relative performance in this scenario unclear.
- **What evidence would resolve it:** Experimental comparison of OPAX with task-specific model-based RL algorithms (e.g., H-UCRL) on a set of tasks where the downstream task is revealed during the exploration phase, measuring performance and sample efficiency.

### Open Question 3
- **Question:** Can the theoretical convergence guarantees for OPAX be extended to more complex kernel functions, such as the Matérn kernel, or are there fundamental limitations preventing this?
- **Basis in paper:** [explicit] The paper states that "To capture a richer class of kernels, for instance, Matèrn, more assumptions are required" and discusses the limitations of the current theoretical analysis for certain kernel classes.
- **Why unresolved:** While the paper provides convergence guarantees for a rich class of kernels (e.g., RBF, linear), it does not conclusively determine whether these guarantees can be extended to more complex kernels like Matérn, or if there are inherent limitations preventing this.
- **What evidence would resolve it:** Theoretical analysis extending the convergence guarantees of OPAX to the Matérn kernel class, or identification of the specific assumptions or limitations preventing such an extension.

## Limitations
- The approach relies heavily on well-calibrated probabilistic models, which can be challenging to obtain in practice
- The exponential dependence on horizon T in the regret bound may limit the algorithm's applicability to long-horizon tasks
- The theoretical convergence guarantees are limited to specific kernel classes (e.g., RBF, linear) and may not extend to more complex kernels like Matérn

## Confidence

- **Mechanism 1 (Optimistic Exploration via Hallucination):** Medium confidence. The mechanism is theoretically sound but relies heavily on well-calibrated uncertainty estimates, which are difficult to guarantee in practice.
- **Mechanism 2 (Tractable Upper Bound):** Medium confidence. The bound derivation is mathematically rigorous under Gaussian assumptions, but its tightness and practical impact need further validation.
- **Mechanism 3 (GP Convergence):** Medium confidence. The theoretical convergence results are promising for specific kernel classes, but the assumptions about the true dynamics and kernel properties may not always hold.

## Next Checks

1. **Uncertainty Calibration Verification:** Test OPAX with statistical models known to have poorly calibrated uncertainty (e.g., deep ensembles without proper calibration techniques) to quantify the impact on exploration performance and identify failure modes.

2. **Non-Gaussian Noise Robustness:** Evaluate the algorithm's performance on systems with non-Gaussian or heteroscedastic process noise to assess the validity of the information gain upper bound and the robustness of the exploration strategy.

3. **Real-World Transfer:** Implement OPAX on a real robotic system (e.g., a manipulator or mobile robot) and compare its exploration efficiency and downstream task performance against heuristic methods, considering practical factors like safety constraints and model learning speed.