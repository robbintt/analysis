---
ver: rpa2
title: 'Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores
  from Language Models Fine-Tuned with Human Feedback'
arxiv_id: '2305.14975'
source_url: https://arxiv.org/abs/2305.14975
tags:
- confidence
- probability
- guess
- question
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to obtain well-calibrated confidence
  scores from language models fine-tuned with reinforcement learning from human feedback
  (RLHF). The authors evaluate several prompting strategies to elicit verbalized probabilities
  or linguistic expressions of uncertainty from RLHF-LMs like ChatGPT and GPT-4.
---

# Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback

## Quick Facts
- arXiv ID: 2305.14975
- Source URL: https://arxiv.org/abs/2305.14975
- Reference count: 12
- Language models fine-tuned with human feedback can produce reliable confidence estimates when properly prompted

## Executive Summary
This paper investigates how to obtain well-calibrated confidence scores from language models fine-tuned with reinforcement learning from human feedback (RLHF), such as ChatGPT and GPT-4. The authors evaluate several prompting strategies to elicit verbalized probabilities or linguistic expressions of uncertainty from these models. They find that verbalized confidences are often better-calibrated than the models' true conditional probabilities, with calibration improving when the model is asked to generate multiple answer hypotheses before giving a confidence score. Combining prompting strategies with temperature scaling reduces expected calibration error by over 50% on average.

## Method Summary
The paper evaluates multiple methods for extracting confidence scores from RLHF language models, including label probability estimation via sampling, verbalized numerical probabilities (one-stage and two-stage approaches), and linguistic expressions of uncertainty. The experiments use three question-answering datasets—TriviaQA, SciQ, and TruthfulQA—with 1000 questions sampled from each dataset. The primary metric is Expected Calibration Error (ECE), along with ECE with temperature scaling (ECE-t), accuracy@coverage (acc@q), and coverage@accuracy (cov@p).

## Key Results
- Verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities
- Prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration
- Combined with temperature scaling, prompting strategies can reduce the expected calibration error of RLHF-LMs by over 50% on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF-LMs can produce better-calibrated verbalized probabilities than their raw conditional probabilities.
- Mechanism: Prompting the model to explicitly verbalize its confidence in token space decouples the model's reasoning about its own correctness from the internal probability distribution.
- Core assumption: The model's internal probability distribution becomes poorly calibrated after RLHF due to optimization encouraging high probability mass on the most likely answer.
- Evidence anchors: Verbalized confidence scores are better-calibrated than true conditional probabilities estimated via sampling.

### Mechanism 2
- Claim: Generating multiple answer hypotheses before assigning confidence improves calibration.
- Mechanism: Similar to human "considering the opposite," generating several answer candidates before providing a confidence score forces evaluation of multiple possibilities, reducing overconfidence.
- Core assumption: The model's initial answer is subject to anchoring bias, and considering alternatives allows better assessment of true likelihood of correctness.
- Evidence anchors: Calibration improves when the model generates multiple answer hypotheses before giving confidence scores.

### Mechanism 3
- Claim: Temperature scaling improves calibration of verbalized probabilities.
- Mechanism: Adjusting temperature parameter smooths the probability distribution, reducing overconfidence by spreading probability mass more evenly.
- Core assumption: The model's output distribution is too peaked (overconfident) after RLHF, and temperature scaling can correct this without requiring fine-tuning.
- Evidence anchors: Temperature scaling combined with prompting reduces ECE by over 50% on average.

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE quantifies how well a model's confidence scores match its actual accuracy, essential for evaluating calibration improvements.
  - Quick check question: If a model has ECE of 0.1, what does this tell you about the average difference between confidence and accuracy across confidence bins?

- Concept: Temperature Scaling
  - Why needed here: Temperature scaling is used as a post-hoc calibration method to improve alignment between confidence scores and actual accuracy without retraining.
  - Quick check question: How does increasing temperature affect the shape of a probability distribution, and why would this help with calibration?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial because the paper questions whether this fine-tuning method degrades calibration compared to pre-trained models.
  - Quick check question: What is the key difference between the RLHF objective and the maximum likelihood objective in terms of how they treat probability mass allocation?

## Architecture Onboarding

- Component map: Pre-trained model -> RLHF fine-tuning -> Prompting strategy (elicits confidence) -> Temperature scaling (optional) -> Calibration assessment
- Critical path: Question → Prompt generation → Model response with answer and confidence → Confidence evaluation → (Optional) Temperature scaling → Calibration assessment
- Design tradeoffs: Verbalized confidence vs. raw probabilities (better calibration but requires careful prompt engineering), single hypothesis vs. multiple hypotheses (better calibration but more complex prompts), temperature scaling (improves calibration but may affect accuracy)
- Failure signatures: Poor calibration persists despite verbalization, temperature scaling degrades accuracy significantly, prompting strategy causes model to refuse or provide nonsensical confidences
- First 3 experiments:
  1. Compare ECE of raw model probabilities vs. verbalized confidences on a simple QA dataset to verify the core claim.
  2. Test the effect of generating 1, 2, and 4 hypotheses before confidence assignment to quantify the multi-hypothesis benefit.
  3. Apply temperature scaling to the best verbalized confidence method and measure the reduction in ECE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the calibration results for RLHF-LMs compare to those of LMs trained purely with supervised learning?
- Basis in paper: The paper states that existing studies focus on LMs trained purely with supervised learning, while this paper focuses on RLHF-LMs.
- Why unresolved: The paper does not directly compare the calibration of RLHF-LMs to that of LMs trained with supervised learning.
- What evidence would resolve it: A direct comparison of calibration metrics (e.g., ECE, acc@q, cov@p) between RLHF-LMs and LMs trained with supervised learning on the same datasets.

### Open Question 2
- Question: To what extent does the calibration of RLHF-LMs depend on the domain of the questions being asked?
- Basis in paper: The paper mentions that conclusions in factual recall contexts hold for reasoning or arithmetic, and asks about domain dependence.
- Why unresolved: The paper does not provide detailed analysis of RLHF-LMs' calibration across different domains or types of questions.
- What evidence would resolve it: Comprehensive evaluation of RLHF-LMs' calibration across various domains and types of questions.

### Open Question 3
- Question: How can we reduce the sensitivity of a model's calibration to the prompt used?
- Basis in paper: The paper discusses differences in calibration between one-stage and two-stage verbalized numerical confidence prompts.
- Why unresolved: The paper does not provide a solution to reduce sensitivity of a model's calibration to the prompt.
- What evidence would resolve it: A study demonstrating a method to reduce sensitivity of a model's calibration to the prompt.

## Limitations

- The paper does not adequately address whether calibration improvements come at the cost of accuracy, potentially creating an unmeasured tradeoff.
- Results are limited to three question-answering datasets and may not generalize to other domains or task types.
- The sampling-based approach for estimating true conditional probabilities (n=10 samples) may introduce significant variance in calibration metrics.

## Confidence

- High Confidence: The claim that verbalized confidences are better-calibrated than raw probabilities is supported by multiple experiments and clear statistical improvements.
- Medium Confidence: The multi-hypothesis generation mechanism improving calibration is plausible but relies on an assumption about anchoring bias that isn't directly tested.
- Medium Confidence: The temperature scaling results are well-documented, but optimal temperature parameters may vary significantly across different models and tasks.

## Next Checks

1. **Accuracy-Calibration Tradeoff Analysis**: Conduct experiments measuring both accuracy and ECE across different prompting strategies and temperature settings to identify whether calibration improvements come with accuracy degradation.

2. **Sampling Variance Quantification**: Repeat the calibration experiments with varying numbers of samples (n=5, 10, 20, 50) to quantify how sampling uncertainty affects the reported ECE values and determine if improvements are robust to sampling noise.

3. **Cross-Domain Generalization**: Test the prompting strategies on at least two additional task types (e.g., code generation and medical diagnosis) to verify whether calibration improvements generalize beyond the question-answering domain used in the paper.