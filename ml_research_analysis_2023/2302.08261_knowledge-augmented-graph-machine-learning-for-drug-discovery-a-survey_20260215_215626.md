---
ver: rpa2
title: 'Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey'
arxiv_id: '2302.08261'
source_url: https://arxiv.org/abs/2302.08261
tags:
- knowledge
- drug
- graph
- discovery
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey presents a comprehensive overview of knowledge-augmented
  graph machine learning (KaGML) for drug discovery. KaGML addresses limitations of
  conventional graph machine learning by integrating external biomedical knowledge
  into the GML pipeline.
---

# Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey

## Quick Facts
- arXiv ID: 2302.08261
- Source URL: https://arxiv.org/abs/2302.08261
- Reference count: 40
- Key outcome: Comprehensive survey of knowledge-augmented graph machine learning (KaGML) for drug discovery, organizing approaches into four categories: preprocessing, pretraining, training, and interpretability.

## Executive Summary
This survey provides a comprehensive overview of knowledge-augmented graph machine learning (KaGML) for drug discovery applications. The authors systematically categorize KaGML approaches based on how external biomedical knowledge is incorporated into the graph machine learning pipeline, spanning preprocessing, pretraining, training, and interpretability stages. The survey addresses the limitations of conventional graph machine learning methods by integrating external knowledge from various biomedical databases and tools. By organizing collected works and providing practical resources including scientific tools and knowledge databases, this survey serves as a valuable resource for researchers looking to apply KaGML techniques to drug discovery challenges.

## Method Summary
The survey systematically collects and organizes KaGML works by identifying how external knowledge is incorporated at different stages of the graph machine learning pipeline. The methodology involves reviewing existing literature to categorize approaches into four main integration points: preprocessing (feature and graph structure augmentation), pretraining (knowledge transfer, generative-sample, and contrastive-sample strategies), training (auxiliary task and knowledge-enhanced training), and interpretability (attention summarization and pathway extraction). The survey also provides schematic representations of knowledge graph construction and discusses practical resources including relevant scientific tools and biomedical knowledge databases.

## Key Results
- KaGML addresses limitations of conventional graph machine learning by integrating external biomedical knowledge into different pipeline components
- Four systematic categories of knowledge incorporation: preprocessing, pretraining, training, and interpretability
- Practical resources provided including scientific tools and knowledge databases for drug discovery applications
- Schematic representation of organizing knowledge databases about small molecule drugs into unified knowledge graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating external biomedical knowledge into GML models significantly improves drug discovery performance by addressing supervision sparsity and enhancing interpretability.
- Mechanism: External knowledge is used to augment features and graph structure during preprocessing, refine pretraining objectives, enhance training with auxiliary tasks, and provide interpretable explanations through knowledge graphs.
- Core assumption: Biomedical knowledge can be effectively integrated into different stages of the GML pipeline to improve model performance and interpretability.
- Evidence anchors:
  - [abstract]: "KaGML addresses limitations of conventional graph machine learning by integrating external biomedical knowledge into the GML pipeline."
  - [section]: "Its core idea is to integrate external human biomedical knowledge into different components of the GML pipeline to achieve more accurate drug discovery, along with user-friendly interpretations."
  - [corpus]: Weak - no direct corpus evidence found for this specific claim.
- Break condition: If the integration of biomedical knowledge does not lead to improved performance or interpretability, or if the knowledge integration process is not effective.

### Mechanism 2
- Claim: Knowledge-augmented pretraining strategies are effective in addressing the limitations of general pretraining approaches for biomedical graphs.
- Mechanism: Knowledge transfer pretraining uses external knowledge to construct pretraining objectives, generative-sample pretraining uses domain-specific knowledge to calibrate pretraining objectives, and contrastive-sample pretraining uses knowledge to guide augmentation processes.
- Core assumption: External knowledge can be used to effectively pretrain GML models for biomedical graphs, overcoming the limitations of general pretraining approaches.
- Evidence anchors:
  - [section]: "To address these challenges, some studies propose knowledge-augmented pretraining strategies."
  - [section]: "One notable example of this approach is the PEMP mode, proposed by Sun et al. [158], which identifies a group of useful chemical and physical properties of molecules, to enhance the capability of GNN models in capturing relevant information through pretraining."
  - [corpus]: Weak - no direct corpus evidence found for this specific claim.
- Break condition: If knowledge-augmented pretraining strategies do not lead to improved performance or if they are not more effective than general pretraining approaches.

### Mechanism 3
- Claim: Auxiliary task-enhanced training and auxiliary knowledge-enhanced training effectively address the challenges of limited training signals and complex biomedical process modeling in GML.
- Mechanism: Auxiliary task-enhanced training uses relevant labeling information as additional training signals, while auxiliary knowledge-enhanced training uses external knowledge to guide the message-passing process within GNN models.
- Core assumption: External knowledge can be effectively used to enhance the training process of GML models for drug discovery, addressing the limitations of limited training signals and complex biomedical process modeling.
- Evidence anchors:
  - [section]: "To address these challenges, recent studies have proposed innovative solutions that incorporate external knowledge to enhance the training process for intelligent drug discovery."
  - [section]: "One example of this is the annotation of atoms, bonds, and functional substructures and their contributions to the properties of molecules, as demonstrated by [141]."
  - [corpus]: Weak - no direct corpus evidence found for this specific claim.
- Break condition: If auxiliary task-enhanced training and auxiliary knowledge-enhanced training do not lead to improved performance or if they are not more effective than standard training approaches.

## Foundational Learning

- Graph Machine Learning (GML)
  - Why needed here: GML is the foundation for analyzing graph-structured biomedical data, which is essential for drug discovery.
  - Quick check question: What are the main categories of GML methods, and how do they differ in their approach to learning from graph data?
- Knowledge Graphs (KG)
  - Why needed here: KGs are used to represent and integrate biomedical knowledge, which is crucial for knowledge-augmented GML.
  - Quick check question: What are the key components of a knowledge graph, and how are they used to represent biomedical knowledge?
- Drug Discovery Process
  - Why needed here: Understanding the drug discovery process is essential for applying GML and KGs effectively in this domain.
  - Quick check question: What are the main stages of the drug discovery process, and how can GML and KGs be applied at each stage?

## Architecture Onboarding

- Component map: Data preprocessing -> Model pretraining -> Model training -> Model interpretability
- Critical path: Preprocessing -> Pretraining -> Training -> Interpretability
- Design tradeoffs:
  - Balancing the amount of external knowledge used with model complexity and training time
  - Choosing between different knowledge integration approaches (preprocessing, pretraining, training, interpretability)
  - Selecting appropriate knowledge sources and databases for the specific drug discovery task
- Failure signatures:
  - Poor model performance despite knowledge integration
  - Difficulty in integrating external knowledge into the GML pipeline
  - Lack of interpretability or explainability in the model's predictions
- First 3 experiments:
  1. Implement a simple GML model for a drug discovery task without knowledge integration.
  2. Integrate external biomedical knowledge into the preprocessing stage and evaluate its impact on model performance.
  3. Compare different knowledge integration approaches (preprocessing, pretraining, training, interpretability) and assess their effectiveness for the specific drug discovery task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge uncertainty be effectively modeled and integrated into KaGML models for drug discovery?
- Basis in paper: [explicit] The paper discusses the need for incorporating external knowledge into KaGML but notes that existing approaches are typically deterministic, ignoring underlying uncertainty of knowledge and its impact on model learning and inference.
- Why unresolved: Current KaGML methods do not systematically account for the uncertainty inherent in biomedical knowledge, which could affect model reliability and performance.
- What evidence would resolve it: Development and validation of KaGML models that incorporate probabilistic frameworks or uncertainty quantification methods, with demonstrated improvements in prediction accuracy and interpretability on drug discovery tasks.

### Open Question 2
- Question: What are the most effective strategies for constructing comprehensive and standardized biomedical knowledge graphs from diverse data sources?
- Basis in paper: [explicit] The paper highlights that biomedical knowledge databases are diverse, heterogeneous, and distributed across multiple platforms, presenting challenges for data integration and standardization.
- Why unresolved: Current knowledge graph construction methods struggle with harmonizing data from various sources, limiting the potential of KaGML approaches.
- What evidence would resolve it: Comparative studies of different knowledge graph construction methodologies, demonstrating improved performance of downstream KaGML models when using more comprehensive and standardized knowledge graphs.

### Open Question 3
- Question: How can KaGML methods be adapted to support precision medicine applications beyond drug discovery?
- Basis in paper: [explicit] The paper mentions that while the survey focuses on drug discovery, other biomedical fields could benefit from KaGML techniques, including target identification and gene therapy.
- Why unresolved: The current KaGML framework is primarily focused on drug discovery, and its applicability to other precision medicine domains remains unexplored.
- What evidence would resolve it: Successful implementation and validation of KaGML methods for precision medicine applications such as target identification, patient stratification, or personalized treatment recommendations, demonstrating improved outcomes compared to existing approaches.

## Limitations
- Limited quantitative comparisons between different KaGML approaches in terms of performance gains
- Lack of specific details about knowledge database versions and integration implementations
- Uncertainty about generalizability of survey findings across different drug discovery tasks

## Confidence
- Confidence Assessment: Medium-High
- Major Uncertainties:
  - The survey lacks quantitative comparisons between different KaGML approaches in terms of performance gains
  - Specific details about knowledge database versions and integration implementations are not always specified
  - The generalizability of survey findings across different drug discovery tasks is not fully explored

## Next Checks
1. Validate the effectiveness of knowledge-augmented pretraining by implementing and comparing at least two different pretraining strategies (e.g., PEMP vs. contrastive learning) on the same molecular property prediction task
2. Test the robustness of knowledge integration approaches by systematically varying the amount and type of external knowledge used, measuring performance degradation when knowledge sources are removed
3. Conduct ablation studies to quantify the contribution of each knowledge integration stage (preprocessing vs. pretraining vs. training) to overall model performance on drug-drug interaction prediction tasks