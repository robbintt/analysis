---
ver: rpa2
title: Targeted Adversarial Attacks on Wind Power Forecasts
arxiv_id: '2303.16633'
source_url: https://arxiv.org/abs/2303.16633
tags:
- attacks
- wind
- adversarial
- power
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the vulnerability of deep learning models
  for wind power forecasting to adversarial attacks. The authors propose a taxonomy
  for categorizing adversarial attacks in regression settings into untargeted, semi-targeted,
  and targeted attacks.
---

# Targeted Adversarial Attacks on Wind Power Forecasts

## Quick Facts
- arXiv ID: 2303.16633
- Source URL: https://arxiv.org/abs/2303.16633
- Reference count: 40
- Key outcome: CNN models for regional wind power forecasting are highly vulnerable to adversarial attacks, while LSTM models for individual wind farms are fairly robust; adversarial training significantly improves CNN robustness

## Executive Summary
This paper evaluates the vulnerability of deep learning models for wind power forecasting to adversarial attacks. The authors propose a taxonomy for categorizing adversarial attacks in regression settings into untargeted, semi-targeted, and targeted attacks. They introduce the Total Adversarial Robustness Score (TARS) to quantify a model's robustness to targeted and semi-targeted attacks. Experiments show that an LSTM model for individual wind farm forecasting is fairly robust, achieving TARS values above 0.81, while a CNN model for regional forecasting is very vulnerable, with TARS values below 0.06 when ordinarily trained. Adversarial training significantly improves the CNN model's robustness, achieving TARS values above 0.46. The study highlights the importance of assessing and enhancing the security of deep learning models in critical infrastructure applications like wind power forecasting.

## Method Summary
The study uses two deep learning architectures: an LSTM encoder-decoder network for individual wind farm forecasting and a CNN ResNet-34 for regional wind power forecasting across Germany. Both models are trained on wind power measurements and wind speed forecasts from the GEFCom2014 dataset and German weather data. The authors implement Projected Gradient Descent (PGD) attacks with ϵ=0.15 to generate adversarial examples across untargeted, semi-targeted, and targeted attack categories. They evaluate model robustness using the Total Adversarial Robustness Score (TARS), which combines Performance Robustness Score (PRS) and Deformation Robustness Score (DRS). Adversarial training is applied to improve model robustness by including adversarial examples in the training process.

## Key Results
- CNN model for regional forecasting is highly vulnerable to adversarial attacks (TARS < 0.06) while LSTM model for individual wind farms is fairly robust (TARS > 0.81)
- Adversarial training significantly improves CNN model robustness (TARS > 0.46) but has minimal effect on LSTM model
- TARS metric effectively captures both performance degradation and prediction manipulation under adversarial attacks
- High-dimensional weather map inputs make CNN models more vulnerable than low-dimensional time series inputs used by LSTM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CNN model's high dimensionality input (weather maps) makes it more vulnerable to adversarial attacks than the LSTM model's low-dimensional input.
- Mechanism: High-dimensional input spaces provide more opportunities for small perturbations to significantly impact the model's predictions. Weather maps contain many spatially correlated features, allowing attackers to find effective perturbations across multiple dimensions.
- Core assumption: The vulnerability difference is primarily due to input dimensionality rather than model architecture.
- Evidence anchors:
  - [abstract]: "we find that CNN models for predicting the wind power generation throughout Germany based on wind speed forecasts in the form of weather maps are very susceptible to adversarial attacks"
  - [section]: "We suspect that this is due to the high dimensionality of the input data. Forecasting models for individual wind farms process very low-dimensional input data with only a few relevant features. In contrast, weather maps represent high-dimensional data with many features being relevant for large-scale wind power forecasting."
  - [corpus]: Weak - no direct discussion of dimensionality in adversarial attacks
- Break condition: If dimensionality is not the primary factor, or if other architectural differences (like residual connections) play a more significant role.

### Mechanism 2
- Claim: Adversarial training significantly improves CNN model robustness but has minimal effect on LSTM model robustness.
- Mechanism: Adversarial training helps models learn to be robust to perturbations they encounter during training. Since the LSTM model was already fairly robust to PGD attacks, additional adversarial training provides little benefit. The CNN model, being highly vulnerable, gains substantial robustness from adversarial training.
- Core assumption: The LSTM model's inherent architecture provides some level of robustness that adversarial training cannot significantly improve.
- Evidence anchors:
  - [abstract]: "adversarial training significantly improves the CNN model's robustness, achieving TARS values above 0.46. The study highlights the importance of assessing and enhancing the security of deep learning models in critical infrastructure applications like wind power forecasting."
  - [section]: "With the help of adversarial training, the model's robustness to PGD attacks and noise attacks could be slightly increased... In contrast to the LSTM forecasting model for the wind farm, the CNN model for forecasting the wind power generation throughout Germany was very susceptible to PGD attacks... The robustness of the CNN model to PGD attacks could be significantly increased with the help of adversarial training."
  - [corpus]: Weak - no direct discussion of why adversarial training has different effects on different architectures
- Break condition: If adversarial training effectiveness depends more on training duration or perturbation strength rather than the inherent model robustness.

### Mechanism 3
- Claim: The Total Adversarial Robustness Score (TARS) provides a comprehensive measure of model robustness by combining performance robustness and deformation robustness.
- Mechanism: TARS uses a harmonic mean approach that requires both good performance under attack (PRS) and resistance to prediction deformation (DRS). This prevents models from appearing robust if they only excel in one dimension.
- Core assumption: Both performance degradation and prediction manipulation are important for assessing adversarial robustness in regression tasks.
- Evidence anchors:
  - [abstract]: "we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to targeted and semi-targeted adversarial attacks"
  - [section]: "Neither the PRS nor the DRS individually provide a thorough assessment of a regression model's robustness to targeted or semi-targeted attacks... We therefore define the TARS, which combines the PRS and the DRS into one score."
  - [corpus]: Weak - no direct discussion of why harmonic mean is appropriate for combining PRS and DRS
- Break condition: If either PRS or DRS alone is sufficient for robustness assessment, or if a different combination method (like weighted arithmetic mean) is more appropriate.

## Foundational Learning

- Concept: Adversarial attacks in regression vs classification
  - Why needed here: The paper develops a taxonomy specifically for regression tasks, which differs from traditional classification attack categories
  - Quick check question: How do untargeted, semi-targeted, and targeted attacks differ in regression settings compared to classification?

- Concept: Gradient-based optimization for adversarial attacks
  - Why needed here: The paper uses Projected Gradient Descent (PGD) attacks, which require understanding of gradient-based optimization techniques
  - Quick check question: How does PGD iteratively improve perturbations while maintaining them within a specified boundary?

- Concept: Robustness-accuracy tradeoff in machine learning
  - Why needed here: The paper observes that adversarial training slightly decreases forecast accuracy, demonstrating this fundamental tradeoff
  - Quick check question: Why might improving a model's robustness to adversarial attacks sometimes come at the cost of reduced accuracy on clean data?

## Architecture Onboarding

- Component map: Data preprocessing -> LSTM/CNN model training -> PGD attack generation -> TARS evaluation
- Critical path: Data preprocessing → Model training → Adversarial attack generation → Robustness evaluation using TARS
- Design tradeoffs: High-dimensional weather map inputs provide rich information but increase vulnerability; adversarial training improves robustness but may reduce accuracy
- Failure signatures: Model appears robust in PRS but fails in DRS (resistant to performance degradation but easily manipulated); adversarial training increases robustness but significantly degrades baseline performance
- First 3 experiments:
  1. Evaluate LSTM model robustness to untargeted PGD attacks with increasing perturbation magnitudes
  2. Test CNN model vulnerability to semi-targeted attacks with different constraint bounds
  3. Compare TARS values for targeted attacks with different adversarial targets (increasing, decreasing, constant, zigzag)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective techniques for automatically generating realistic adversarial targets for regression tasks?
- Basis in paper: [explicit] The authors note that current targeted attacks require manually selecting adversarial targets and suggest developing techniques for automatically generating realistic, application-specific adversarial targets as a direction for future work.
- Why unresolved: The paper does not explore any methods for automatically generating adversarial targets, instead relying on manually selected targets in their experiments.
- What evidence would resolve it: Development and evaluation of techniques that can automatically generate realistic adversarial targets tailored to specific regression tasks and applications.

### Open Question 2
- Question: How does the adversarial robustness of wind power forecasting models vary with the dimensionality and type of input features used?
- Basis in paper: [inferred] The authors observe that CNN models using high-dimensional weather maps are more vulnerable to adversarial attacks compared to LSTM models using low-dimensional wind speed time series. They hypothesize this is due to the higher dimensionality of weather maps.
- Why unresolved: The paper does not conduct experiments to systematically investigate how the type and dimensionality of input features affects adversarial robustness.
- What evidence would resolve it: Experiments comparing the adversarial robustness of forecasting models trained on different types and dimensionalities of input features (e.g., time series vs. weather maps, low-dimensional vs. high-dimensional).

### Open Question 3
- Question: What are the most effective defense mechanisms for improving the adversarial robustness of wind power forecasting models without compromising their accuracy?
- Basis in paper: [explicit] The authors find that adversarial training can significantly improve the robustness of CNN models but only marginally for LSTM models, and that it slightly deteriorates the forecast accuracy of both models when not under attack. They suggest developing adversarial defenses that do not negatively impact performance as a direction for future work.
- Why unresolved: The paper only explores one defense mechanism (adversarial training) and does not investigate other potential defenses or techniques to mitigate the accuracy-robustness trade-off.
- What evidence would resolve it: Development and evaluation of alternative defense mechanisms, such as robust optimization, feature squeezing, or ensemble methods, and techniques to balance accuracy and robustness, such as dynamic adversarial training or multi-task learning.

## Limitations

- Limited comparison between adversarial vulnerability and other forecast uncertainty sources like weather model errors
- Attribution of CNN vulnerability primarily to input dimensionality remains speculative without controlled ablation studies
- Relatively small dataset (3 years) may limit generalizability of robustness findings

## Confidence

- **High Confidence**: CNN models are more vulnerable to adversarial attacks than LSTM models, and adversarial training improves robustness
- **Medium Confidence**: Vulnerability differences are primarily due to input dimensionality rather than architectural factors
- **Medium Confidence**: TARS metric effectively captures both performance degradation and prediction manipulation

## Next Checks

1. Conduct dimensionality ablation study comparing vulnerability of different model architectures using the same input dimensionality
2. Systematically evaluate the relationship between baseline forecast accuracy and adversarial robustness across multiple architectures
3. Implement realistic adversarial attack simulation with limited attacker knowledge to assess practical vulnerabilities