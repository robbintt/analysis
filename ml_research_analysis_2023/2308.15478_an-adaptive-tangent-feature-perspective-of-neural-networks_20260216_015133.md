---
ver: rpa2
title: An Adaptive Tangent Feature Perspective of Neural Networks
arxiv_id: '2308.15478'
source_url: https://arxiv.org/abs/2308.15478
tags:
- neural
- kernel
- features
- tangent
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for understanding linear models
  in tangent feature space where the features are allowed to transform during training.
  The key idea is to consider linear transformations of features, resulting in a joint
  optimization over parameters and transformations with a bilinear interpolation constraint.
---

# An Adaptive Tangent Feature Perspective of Neural Networks

## Quick Facts
- **arXiv ID:** 2308.15478
- **Source URL:** https://arxiv.org/abs/2308.15478
- **Authors:** 
- **Reference count:** 40
- **Key outcome:** This paper proposes a framework for understanding linear models in tangent feature space where the features are allowed to transform during training. The key idea is to consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. This optimization problem is shown to be equivalent to a linearly constrained optimization with structured regularization that encourages approximately low rank solutions. The framework is specialized to neural network structure to gain insights into how the features and thus the kernel function change. This provides additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. The theoretical observations are verified in the kernel alignment of real neural networks.

## Executive Summary
This paper introduces an adaptive feature learning framework for understanding neural network training through tangent feature space. By allowing tangent features to transform during training via linear transformations, the framework creates a joint optimization problem equivalent to structured regularization that encourages approximately low-rank solutions. The approach provides theoretical insights into kernel alignment and improves sample complexity compared to fixed tangent features. Experiments on MNIST and CIFAR-10 demonstrate order-of-magnitude better sample complexity than fixed tangent feature models.

## Method Summary
The method proposes a framework for adaptive feature learning in tangent space by considering linear transformations of initial tangent features. This creates a bilinear optimization problem over parameters and transformation matrices, which is shown to be equivalent to a structured regression problem with group approximate low-rank regularization. The framework is applied to neural networks by computing initial tangent features from random initialization and jointly optimizing both parameters and transformation matrices. The effective regularization combines nuclear norm-like behavior for large singular values with Frobenius norm-like behavior near zero, promoting structure while being robust to noise.

## Key Results
- Adaptive feature learning framework provides order-of-magnitude better sample complexity than fixed tangent features on MNIST and CIFAR-10
- The joint optimization over parameters and transformations is equivalent to structured regularization encouraging approximately low-rank solutions
- Adaptive features enable kernel alignment with target functions through the learned transformations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive feature learning in tangent space enables kernel alignment with target functions through structured regularization.
- **Mechanism:** By allowing tangent features to transform during training through linear transformations M, the framework creates a bilinear optimization problem. This is equivalent to a structured regression problem with group approximate low-rank regularization that encourages kernel alignment with the label kernel K_by.
- **Core assumption:** The model assumes factorized features (Mθ can be written as linear transformations of initial tangent features) and independent optimization of feature transformations from parameter updates.
- **Evidence anchors:**
  - [abstract]: "This optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions."
  - [section]: "We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions."
  - [corpus]: Weak - no direct evidence in corpus about kernel alignment through adaptive features
- **Break condition:** If the factorized features assumption fails (tangent features cannot be written as linear transformations of initial features) or if feature transformations become too large during training.

### Mechanism 2
- **Claim:** The effective regularization eΩ(B) behaves like a combination of nuclear norm and Frobenius norm, promoting structure while absorbing noise.
- **Mechanism:** The joint penalty ω1 ⊕ ω2 applied to singular values creates sub-quadratic tail behavior (like nuclear norm) for large singular values while maintaining quadratic behavior near zero (like Frobenius norm). This encourages approximately low-rank solutions that capture predictive structure while being robust to noise.
- **Core assumption:** The functions ω(1)ℓ and ω(2)ℓ are strictly quasi-convex with minimum at 1, ensuring the effective penalty has the desired properties.
- **Evidence anchors:**
  - [section]: "Proposition 3. Let ω be a continuous strictly quasi-convex function minimized at ω(1) = 0. Then v2 → ˜w(v) is an increasing concave function, and ˜ω(v) = v2 + o(v2)."
  - [section]: "The result of this effective regularization is a model that is able to leverage structures through the group approximate low-rank penalty while also being robust to noise and model misspecification through the Frobenius norm penalty for small singular values."
  - [corpus]: Weak - no direct evidence about effective regularization properties
- **Break condition:** If the functions ω are not strictly quasi-convex or if they don't have minimum at 1, the effective penalty would not have the desired properties.

### Mechanism 3
- **Claim:** Adaptive feature learning reduces sample complexity by an order of magnitude compared to fixed tangent features.
- **Mechanism:** The adaptive framework allows the model to learn transformations that align the kernel with the target function structure, rather than being limited to the fixed neural tangent kernel. This enables better generalization with fewer samples by capturing task-specific structure.
- **Core assumption:** The linear path assumption for average features holds approximately (features change linearly along the optimization path).
- **Evidence anchors:**
  - [section]: "We empirically evaluate our adaptive feature model in neural networks on MNIST and CIFAR-10, which provides an order of magnitude better sample complexity compared to fixed tangent features."
  - [section]: "Figure 4: Adaptive feature learning improves low-sample performance... The adaptive feature achieves the same performance as the non-adaptive tangent feature model with an order of magnitude fewer samples."
  - [corpus]: Weak - no direct evidence in corpus about sample complexity improvements
- **Break condition:** If the linear path assumption fails significantly or if the effective regularization becomes too strong, preventing the model from learning useful transformations.

## Foundational Learning

- **Concept:** Neural Tangent Kernel (NTK) theory
  - Why needed here: The paper builds upon NTK framework but extends it by allowing features to adapt. Understanding NTK is essential to grasp what makes this approach different.
  - Quick check question: What is the fundamental assumption of NTK theory about gradient changes during training?

- **Concept:** Linear regression with kernel methods
  - Why needed here: The framework essentially performs linear regression in an adaptive feature space, and the theory relies on understanding kernel regression properties.
  - Quick check question: How does ridge regression relate to the NTK framework for neural networks?

- **Concept:** Regularization theory and low-rank optimization
  - Why needed here: The paper introduces structured regularization that encourages approximately low-rank solutions, which is central to understanding how the model learns.
  - Quick check question: What is the difference between nuclear norm and Frobenius norm regularization in terms of promoting low-rank solutions?

## Architecture Onboarding

- **Component map:** Initial tangent features -> Transformation matrices M -> Effective penalty eΩ(B) -> Adapted kernel K -> Performance on validation set
- **Critical path:** 
  1. Compute initial tangent features at θ0
  2. Initialize M matrices close to identity
  3. Jointly optimize M and θ with bilinear constraint
  4. Compute adapted kernel from learned transformations
  5. Evaluate performance on validation set
- **Design tradeoffs:**
  - Flexibility vs. complexity: More layers allow richer transformations but increase computational cost
  - Regularization strength: Too weak allows overfitting, too strong prevents useful adaptation
  - Initialization: M matrices must start close to identity for the framework to work
- **Failure signatures:**
  - M matrices growing very large: Indicates the framework assumptions are breaking down
  - No improvement over fixed NTK: Suggests the regularization is too strong or the linear path assumption fails
  - Numerical instability in kernel computation: May indicate singular value issues in M matrices
- **First 3 experiments:**
  1. Simple regression on synthetic data with known structure to verify kernel alignment mechanism
  2. MNIST classification comparing adaptive vs. fixed features with varying sample sizes
  3. CIFAR-10 classification to test scalability to more complex tasks and architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do average tangent features and final tangent features compare in real neural networks, and under what conditions do they diverge?
- Basis in paper: [explicit] The paper mentions that average features over a linear path in parameter space are difficult to work with in practice compared to final features, and raises the question of how much these two types of features coincide.
- Why unresolved: The paper only provides limited, unpublished observations suggesting similarity between average and final feature kernels, but no systematic comparison or analysis of conditions leading to divergence.
- What evidence would resolve it: Systematic experiments comparing average and final tangent feature kernels across various architectures, datasets, and training regimes, quantifying their similarity and identifying conditions where they diverge significantly.

### Open Question 2
- Question: What is the optimal spectral regularizer for adaptive feature learning in neural networks, and how does it compare to other regularization schemes?
- Basis in paper: [inferred] The paper shows that adaptive feature learning leads to a group approximate low-rank penalty with sub-quadratic tail behavior and quadratic behavior near zero, which is related to but distinct from standard regularization schemes like L1, L2, or nuclear norm.
- Why unresolved: The paper derives the effective penalty that arises from adaptive feature learning but does not compare its performance to other regularization schemes or investigate the impact of different spectral regularizers.
- What evidence would resolve it: Empirical comparison of adaptive feature learning with different spectral regularizers against standard regularization schemes on various tasks, measuring generalization performance, sample complexity, and robustness to noise.

### Open Question 3
- Question: To what extent can adaptive feature learning explain the success of real neural networks, and what aspects remain unexplained?
- Basis in paper: [explicit] The paper acknowledges that its framework is far from an exact characterization of real neural networks and that real networks significantly outperform both adaptive and fixed feature models, suggesting limitations of the tangent kernel feature space.
- Why unresolved: The paper provides empirical evidence that adaptive feature learning improves sample complexity but does not fully close the gap with real networks, leaving open the question of what other factors contribute to their success.
- What evidence would resolve it: Systematic investigation of the performance gap between adaptive feature models and real networks across various architectures and tasks, identifying specific aspects of real networks (e.g., non-linear feature learning, architectural inductive biases) that are not captured by adaptive feature learning.

## Limitations

- The factorized features assumption is critical but not empirically validated across diverse architectures
- The structured regularization's effectiveness depends on the choice of quasi-convex functions, which lacks principled guidance
- The sample complexity improvement claims rely on specific experimental setups that may not generalize

## Confidence

- **High**: The theoretical equivalence between bilinear optimization and structured regularization (Theorem 2)
- **Medium**: The kernel alignment mechanism through adaptive features, supported by theoretical arguments but limited empirical evidence
- **Medium**: The sample complexity improvements shown on MNIST/CIFAR-10, though dependent on specific hyperparameters and architectures

## Next Checks

1. Test the framework on diverse architectures (Transformers, ResNets) to verify the factorized features assumption holds beyond MLPs
2. Systematically vary the quasi-convex functions ω to identify which properties are essential for effective regularization
3. Conduct ablation studies on the initialization of M matrices to determine sensitivity to starting values and identify failure modes