---
ver: rpa2
title: Characterising Decision Theories with Mechanised Causal Graphs
arxiv_id: '2307.10987'
source_url: https://arxiv.org/abs/2307.10987
tags:
- decision
- causal
- agent
- theory
- newcomb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a framework using mechanised causal graphs
  to characterize and distinguish different decision theories. The key contribution
  is a taxonomy that classifies decision theories along two axes: evidential vs.'
---

# Characterising Decision Theories with Mechanised Causal Graphs

## Quick Facts
- arXiv ID: 2307.10987
- Source URL: https://arxiv.org/abs/2307.10987
- Reference count: 37
- Decision theories can be uniformly expressed in mechanised causal graphs, clarifying when they agree and differ.

## Executive Summary
This paper develops a framework using mechanised causal graphs to characterize and distinguish different decision theories. The key contribution is a taxonomy that classifies decision theories along two axes: evidential vs. causal vs. functional (how expected utility is computed), and updateful vs. updateless (whether conditioning on observations is allowed). The authors show how EDT, CDT, and FDT can be uniformly expressed within this framework, clarifying when they agree and differ. The framework also provides a principled way to avoid circularity in "transparent Newcomb-like" problems by modeling predictions as depending on decision rules rather than decisions directly.

## Method Summary
The paper formalises decision problems as mechanised causal graphs where nodes represent variables and edges represent dependencies. Decision theories are distinguished by their expected utility calculation methods: CDT uses do-interventions on actions, EDT conditions on actions, and FDT intervenes on decision rule variables. The framework operationalizes updatelessness by excluding observation conditioning from expected utility calculations. Problems are encoded as DAGs, and queries are executed to compute expected utilities for each theory.

## Key Results
- EDT, CDT, and FDT can be uniformly expressed within mechanised causal graphs
- The framework clarifies when decision theories agree and differ
- Transparent Newcomb problems are resolved by modeling predictions as depending on decision rules rather than decisions directly
- A taxonomy classifies decision theories along evidential/causal/functional and updateful/updateless axes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mechanised causal graphs enable principled comparisons of decision theories by making their structural differences explicit.
- Mechanism: The framework formalises each decision theory's expected utility computation as a distinct graphical query (e.g., conditioning vs. intervening on decision vs. decision rule nodes). This exposes the exact causal assumptions underlying each theory.
- Core assumption: Causal and decision-theoretic concepts can be represented faithfully in Pearl-style causal models extended with mechanism variables.
- Evidence anchors:
  - [abstract] "mechanised causal models can be used to characterise and differentiate the most important decision theories"
  - [section 2.2] "Newcomb's problem can be modeled with a Bayesian network... Causal Bayesian networks... mechanised causal Bayesian network"
  - [corpus] Weak: no direct corpus citations of this mechanisation approach; papers are loosely related to causality in software/analytics.

### Mechanism 2
- Claim: The updateless/updateful axis is operationalised by whether agents condition on observations when computing expected utility.
- Mechanism: In the graph formalism, updateful theories include conditioning on ObsD in their utility query, while updateless theories do not. This cleanly separates reasoning styles without altering causal structure.
- Core assumption: Conditioning on observations is the correct formalisation of "updatelessness" in decision problems with observable states.
- Evidence anchors:
  - [section 3.4] "issues of updatelessness can only arise in models...where agents are asked to choose conditional policies...updatelessness will not affect an agent's decision in a 'twin' version of any normal-form game"
  - [section 4] Taxonomy table distinguishing "updateful" vs "updateless" on expected utility calculation.

### Mechanism 3
- Claim: Logical causality in FDT is modelled by allowing interventions on decision rule variables to affect other agents' policies when they share source code.
- Mechanism: In mechanised graphs, ˜D nodes are made logically connected to twins' policy nodes. Intervening on ˜D then changes both agents' policies, capturing functional dependence.
- Core assumption: Logical causality can be encoded as explicit dependencies between mechanism variables representing decision rules.
- Evidence anchors:
  - [section 3.3] "An FDT agent treats their twin's policy as a logical consequence of their own...if we think of intervening on your policy as changing the way your source code compiles in all cases"
  - [fig. 3b] "Twin Prisoner's Dilemma...logical-causal model where intervening on my policy would affect my twin's policy"

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and Pearl's causal hierarchy
  - Why needed here: All decision problems are encoded as DAGs; queries range from associational (1st rung) to interventional (2nd) to counterfactual (3rd).
  - Quick check question: What distinguishes a Bayesian network from a causal Bayesian network in terms of edge semantics?

- Concept: Intervention vs. conditioning in probability
  - Why needed here: CDT uses do(D=d) while EDT uses P(U|D=d); the framework hinges on when each is appropriate.
  - Quick check question: Why does conditioning on D in Newcomb's problem lead to one-boxing while intervening leads to two-boxing?

- Concept: Mechanism variables and policy representation
  - Why needed here: Decision rules (˜D) are the objects of choice in updateless theories; distinguishing them from actions (D) is essential.
  - Quick check question: How does introducing ˜D nodes change the interpretation of Newcomb's problem compared to a plain causal model?

## Architecture Onboarding

- Component map:
  Graph builder -> Query engine -> Theory mapper -> Utility evaluator -> Problem validator

- Critical path:
  1. Parse problem → build DAG with mechanism nodes.
  2. For each decision theory, select query pattern.
  3. Execute query → compute expected utilities.
  4. Return optimal action/decision rule.

- Design tradeoffs:
  - Flexibility vs. complexity: Allowing arbitrary logical dependencies increases expressiveness but risks non-DAG structures.
  - Interpretability vs. automation: Manual DAG construction ensures correctness but reduces scalability.
  - Static vs. dynamic graphs: Fixed structure simplifies reasoning; dynamic structure supports richer scenarios.

- Failure signatures:
  - Cycles in the graph (ill-defined problem).
  - Query returns NaN/undefined (e.g., conditioning on impossible events).
  - Expected utility rankings inconsistent with theory's known behaviour.
  - Mechanism nodes missing when required by updateless theories.

- First 3 experiments:
  1. Implement Newcomb's problem with all three theories; verify one-boxing/two-boxing predictions.
  2. Implement Transparent Newcomb; confirm EDT two-boxes while FDT one-boxes.
  3. Implement Twin Prisoner's Dilemma; verify CDT defects, EDT co-operates, FDT co-operates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FDT agents practically determine the correct decision rule variable in mechanised causal graphs?
- Basis in paper: [explicit] The paper states "FDT has an epistemic problem – we cannot yet say how FDT agents can find out what they need to know, even in principle. For this, something like a formal theory of logical causality would be needed."
- Why unresolved: While the paper defines FDT's expected utility calculation in terms of the decision rule variable, it doesn't provide a method for agents to identify this variable in real-world situations or how to model logical interventions.
- What evidence would resolve it: A formal theory of logical causality or a practical algorithm that FDT agents could use to identify and model their decision rule variable in mechanised causal graphs.

### Open Question 2
- Question: Can UEDT serve as a practical substitute for FDT in all Newcomblike problems?
- Basis in paper: [explicit] The paper suggests "UEDT is probably an adequate substitute for FDT, both for the purposes of decision-making and for analyzing the incentives of ML systems."
- Why unresolved: The paper notes that UEDT and FDT may only diverge on "problems which philosophers consider questionably well-defined, such as the smoking lesion problem," but doesn't provide a comprehensive analysis of all Newcomblike scenarios.
- What evidence would resolve it: Empirical testing of UEDT and FDT on a wide range of Newcomblike problems, including edge cases, to determine if their decisions consistently align or diverge.

### Open Question 3
- Question: How can we formalize the distinction between physical and logical causality in mechanised causal graphs?
- Basis in paper: [explicit] The paper discusses "physical notion of causality" versus "logical-causal model" and notes that FDT requires "a model of the interventional distributions under interventions on logical rather than physical variables."
- Why unresolved: The paper provides examples of when physical vs. logical causality matters (e.g., Twin Prisoner's Dilemma) but doesn't offer a formal method for distinguishing or modeling these types of causality in general.
- What evidence would resolve it: A formal mathematical framework or criteria for determining when a causal relationship should be modeled as physical vs. logical in mechanised causal graphs, along with examples of its application to various decision problems.

## Limitations
- The framework relies on Pearl's causal hierarchy, which assumes well-defined interventions and counterfactuals
- Logical-causal modelling lacks empirical validation beyond toy examples
- Operationalisation of updatelessness through observation conditioning may not capture all policy-based reasoning scenarios

## Confidence
- **High confidence**: The basic taxonomy of decision theories along evidential/causal/functional and updateful/updateless axes is well-grounded and internally consistent.
- **Medium confidence**: The application of mechanised causal graphs to distinguish EDT, CDT, and FDT is methodologically sound, though the implementation details remain underspecified.
- **Low confidence**: The logical-causal modelling of FDT's treatment of decision rule dependencies across agents is conceptually promising but not yet validated beyond the twin PD example.

## Next Checks
1. Implement and test the framework on a variant of Newcomb's problem where the predictor's accuracy is probabilistic rather than perfect, checking if the theory distinctions persist.
2. Extend the graph formalism to handle policy-dependent environments (e.g., Parfit's hitchhiker) and verify the updateless/updateful distinction captures the relevant intuitions.
3. Formalise the logical-causal edges as a separate operator in the query language and test whether this prevents cycles while preserving FDT's predictions.