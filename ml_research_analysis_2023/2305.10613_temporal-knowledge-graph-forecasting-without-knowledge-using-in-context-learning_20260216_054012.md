---
ver: rpa2
title: Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning
arxiv_id: '2305.10613'
source_url: https://arxiv.org/abs/2305.10613
tags:
- performance
- facts
- entity
- forecasting
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies large language models (LLMs) to temporal knowledge
  graph (TKG) forecasting using in-context learning (ICL), without fine-tuning or
  explicit modules for capturing structural and temporal information. The method converts
  relevant historical facts into prompts and generates ranked predictions using token
  probabilities.
---

# Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning

## Quick Facts
- arXiv ID: 2305.10613
- Source URL: https://arxiv.org/abs/2305.10613
- Reference count: 19
- Key outcome: LLMs using in-context learning achieve state-of-the-art performance in TKG forecasting without fine-tuning or explicit structural/temporal modules

## Executive Summary
This paper demonstrates that large language models can perform temporal knowledge graph (TKG) forecasting using in-context learning (ICL) without any fine-tuning or explicit modules for capturing structural and temporal information. The method converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, LLMs perform on par with state-of-the-art TKG models carefully designed and trained for this task. The approach works effectively even when using numerical indices instead of entity/relation names, indicating that prior semantic knowledge is unnecessary for achieving high performance.

## Method Summary
The method converts historical TKG facts into structured prompts using either index-based or lexical-based templates. These prompts are fed to LLMs (gpt-3.5-turbo, gpt-4, and text-davinci-003) to generate ranked predictions based on token probabilities. The approach supports both single-step and multi-step inference settings. Historical facts are retrieved based on either entity or entity-pair queries, and bidirectional facts can be included. The model leverages in-context learning to predict future facts without fine-tuning, using time-aware filtering for evaluation with Hit@k metrics.

## Key Results
- LLMs achieve state-of-the-art performance in TKG forecasting using ICL without fine-tuning
- Using numerical indices instead of entity/relation names does not significantly affect performance (±0.4% Hit@1)
- ICL enables LLMs to learn irregular patterns from historical context, going beyond simple predictions based on common or recent information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large language models can perform temporal knowledge graph forecasting through in-context learning without any fine-tuning or explicit modules for capturing structural and temporal information.
- **Mechanism:** LLMs leverage the existing patterns in the historical context to predict future facts. The model converts relevant historical facts into prompts and generates ranked predictions using token probabilities.
- **Core assumption:** LLMs can effectively learn irregular patterns from historical context without explicit fine-tuning on the training dataset.
- **Evidence anchors:**
  - [abstract] "Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting."
  - [section 3.2] "We obtain a ranking over target candidates using token probabilities."
  - [corpus] "Average neighbor FMR=0.468, average citations=0.0" (Weak corpus evidence)
- **Break condition:** If the historical context lacks sufficient patterns or the LLM fails to learn from the provided context, the performance would degrade significantly.

### Mechanism 2
- **Claim:** LLMs do not require prior semantic knowledge to achieve high performance in TKG forecasting.
- **Mechanism:** Using numerical indices instead of entity/relation names does not significantly affect the performance, indicating that semantic priors are unnecessary.
- **Core assumption:** The model can leverage the existing patterns in the context without relying on semantic understanding of entities and relations.
- **Evidence anchors:**
  - [abstract] "We also discover that using numerical indices instead of entity/relation names, i.e., hiding semantic information, does not significantly affect the performance (±0.4% Hit@1)."
  - [section 5.1] "Our results show 1) decent performance on ACLED-CD22 and 2) similar performance on the lexical- and index-based prompts, indicating that semantic prior knowledge is not a prerequisite for achieving high performance."
  - [corpus] "Found 25 related papers (using 8)" (Moderate corpus evidence)
- **Break condition:** If the task requires deep semantic understanding or the patterns are not sufficient for prediction, the model might struggle without semantic priors.

### Mechanism 3
- **Claim:** ICL enables LLMs to learn irregular patterns from the historical context, going beyond simple predictions based on common or recent information.
- **Mechanism:** The model overcomes frequency and recency biases to learn more complex patterns from the historical context.
- **Core assumption:** The LLM can distinguish between trivial patterns and more sophisticated patterns in the historical data.
- **Evidence anchors:**
  - [abstract] "Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond simple predictions based on common or recent information."
  - [section 5.1] "Our results demonstrate that ICL prediction outperforms both heuristic baselines, suggesting that ICL leads LLMs to actually learn patterns from historical context."
  - [corpus] "Top related titles: Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting, RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion" (Moderate corpus evidence)
- **Break condition:** If the historical context is too noisy or lacks diverse patterns, the model might default to frequency or recency-based predictions.

## Foundational Learning

- **Concept:** Temporal Knowledge Graph (TKG)
  - **Why needed here:** Understanding the structure and purpose of TKGs is crucial for framing the forecasting task and designing appropriate prompts.
  - **Quick check question:** What are the components of a TKG, and how do they relate to each other in the context of forecasting?

- **Concept:** In-Context Learning (ICL)
  - **Why needed here:** ICL is the core methodology used to leverage LLMs for TKG forecasting without fine-tuning. Understanding its principles is essential for implementing the approach.
  - **Quick check question:** How does ICL differ from traditional fine-tuning, and what are its advantages and limitations?

- **Concept:** Prompt Engineering
  - **Why needed here:** Crafting effective prompts is critical for guiding the LLM to generate accurate predictions. Understanding the nuances of prompt design is essential for optimizing performance.
  - **Quick check question:** What are the key considerations when designing prompts for TKG forecasting, and how do different prompt variations impact performance?

## Architecture Onboarding

- **Component map:** Data Preparation -> Prompt Construction -> Model Inference -> Evaluation
- **Critical path:**
  1. Data Preparation: Extract and format historical facts.
  2. Prompt Construction: Create structured prompts using templates.
  3. Model Inference: Generate predictions using the LLM.
  4. Evaluation: Calculate performance metrics.

- **Design tradeoffs:**
  - **Index vs. Lexical:** Using numerical indices may reduce the reliance on semantic priors but could also limit the model's ability to leverage pre-existing knowledge.
  - **Unidirectional vs. Bidirectional:** Including bidirectional facts may provide more diverse patterns but could also introduce noise.
  - **Entity vs. Pair:** Focusing on entities may offer a broader range of historical facts, while focusing on pairs may provide more specific patterns.

- **Failure signatures:**
  - **Poor Performance:** If the model consistently fails to generate accurate predictions, it may indicate issues with prompt construction or insufficient historical patterns.
  - **Overfitting:** If the model performs well on training data but poorly on test data, it may suggest overfitting to specific patterns.
  - **Bias:** If the model consistently favors certain entities or relations, it may indicate a bias in the historical data or prompt construction.

- **First 3 experiments:**
  1. **Experiment 1:** Test the model's performance with different history lengths to identify the optimal amount of historical context.
  2. **Experiment 2:** Compare the performance of index-based and lexical-based prompts to assess the impact of semantic priors.
  3. **Experiment 3:** Evaluate the model's ability to handle bidirectional facts and determine the optimal history retrieval strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction tuning impact the performance of LLMs in TKG forecasting beyond the observed Hit@1 improvements with lexical prompts?
- Basis in paper: [explicit] The paper shows that instruction-tuned models (gpt-3.5-turbo) perform better with lexical prompts (+0.024 Hit@1) compared to index prompts, suggesting they can utilize semantic priors more effectively.
- Why unresolved: The analysis is limited to Hit@1 metrics and one instruction-tuned model. The broader impact on other metrics (Hit@3, Hit@10) and other instruction-tuned models remains unexplored.
- What evidence would resolve it: Comprehensive evaluation of multiple instruction-tuned models across all Hit@k metrics, comparing both index and lexical prompt performance, would clarify the full impact of instruction tuning.

### Open Question 2
- Question: What is the relationship between model size and performance for ICL in TKG forecasting, and does this relationship hold for transductive settings?
- Basis in paper: [explicit] The paper shows that performance adheres to scaling laws with both model size and history length in the inductive setting.
- Why unresolved: The analysis is limited to inductive settings, and the paper does not explore transductive settings where answers could be unseen entities from the history.
- What evidence would resolve it: Experiments comparing model size scaling in both inductive and transductive settings, measuring performance across different history lengths, would clarify if the scaling law applies universally.

### Open Question 3
- Question: How does the choice between entity-based and pair-based history modeling affect performance across different TKG datasets, and what underlying factors drive these differences?
- Basis in paper: [explicit] The paper shows that WIKI and ICEWS18 perform better with entity-based queries while ICEWS14 performs better with pair-based queries, but does not explain why.
- Why unresolved: The analysis identifies the performance difference but does not investigate the dataset characteristics (e.g., entity position distribution, relation types) that might explain the discrepancy.
- What evidence would resolve it: Detailed analysis of dataset statistics (entity position distributions, relation patterns, temporal dynamics) correlated with model performance for each history modeling approach would explain the underlying factors.

## Limitations
- Performance comparisons may not fully account for differences in training data and computational resources between specialized TKG models and LLMs
- Scalability to larger, more complex TKGs remains untested
- Potential biases in historical context that could influence predictions are not addressed

## Confidence
**High Confidence:**
- LLMs can perform TKG forecasting using ICL without fine-tuning
- Index-based prompts achieve comparable performance to lexical prompts
- ICL enables learning beyond frequency and recency biases

**Medium Confidence:**
- Performance parity with state-of-the-art TKG models
- The sufficiency of prompt-based approaches for complex TKG patterns
- Generalizability across different TKG domains

**Low Confidence:**
- Long-term scalability of the approach
- Robustness to highly sparse or noisy TKG data
- Performance on multi-step forecasting beyond tested scenarios

## Next Checks
1. **Domain Transfer Validation**: Test the index-based approach on TKG datasets from different domains (e.g., scientific publications, social networks) to assess generalizability and identify domain-specific limitations.

2. **Stress Testing with Synthetic Data**: Generate synthetic TKGs with controlled properties (varying sparsity, noise levels, and temporal irregularities) to systematically evaluate the approach's robustness and failure modes.

3. **Ablation Study on Prompt Components**: Conduct a comprehensive ablation study removing different prompt components (e.g., timestamps, relation types) to quantify their individual contributions and identify critical dependencies.