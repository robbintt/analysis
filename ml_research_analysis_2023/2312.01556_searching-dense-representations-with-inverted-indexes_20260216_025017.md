---
ver: rpa2
title: Searching Dense Representations with Inverted Indexes
arxiv_id: '2312.01556'
source_url: https://arxiv.org/abs/2312.01556
tags:
- indexes
- retrieval
- dense
- representations
- hnsw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores using inverted indexes for dense vector retrieval
  instead of the standard HNSW approach. Two methods are proposed: "fake words" and
  "lexical LSH".'
---

# Searching Dense Representations with Inverted Indexes

## Quick Facts
- **arXiv ID**: 2312.01556
- **Source URL**: https://arxiv.org/abs/2312.01556
- **Reference count**: 4
- **Primary result**: Inverted index approaches (fake words, lexical LSH) yield smaller index sizes but significantly slower retrieval compared to HNSW for dense vector search

## Executive Summary
This paper explores using inverted indexes instead of HNSW for dense vector retrieval. Two methods are proposed: "fake words" which encodes vector values as term frequencies, and "lexical LSH" which uses quantization and locality-sensitive hashing to transform vectors into searchable text signatures. Experiments on MS MARCO passage ranking show that while these inverted index approaches achieve reasonable effectiveness with much smaller index sizes (2.8 GB vs 26 GB for HNSW), they are significantly slower (2.8 QPS vs 48 QPS). The authors conclude that while workable, these approaches do not provide a compelling tradeoff over HNSW indexes for most use cases.

## Method Summary
The paper proposes two techniques for dense vector retrieval using inverted indexes. The "fake words" approach maps each vector dimension to a unique term, with term frequency proportional to the quantized vector value (e.g., dimension i with value 0.7 and Q=40 generates ⌊40×0.7⌋ = 28 occurrences of term i). The "lexical LSH" approach rounds vector components to decimal places, tags them with dimension indices, generates n-grams, and applies MinHash filtering to create LSH-based signatures. Both methods enable standard inverted index query processing for dense vector retrieval without specialized vector index structures.

## Key Results
- Fake words with Q=40 achieves 0.36 RR@10 vs 0.39 for HNSW, with 2.8 GB vs 26 GB index size
- Lexical LSH with b=400 achieves 0.33 RR@10 vs 0.39 for HNSW, with 3.8 GB vs 26 GB index size
- Query throughput drops dramatically: fake words achieves 2.8 QPS vs 48 QPS for HNSW
- Both approaches work reasonably well but are significantly slower than HNSW for retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense vectors can be converted into inverted index structures using quantization-based term frequency encoding
- Mechanism: Each dimension of a dense vector is mapped to a unique term, and its value determines how many times that term appears in the document. For example, a vector component with value 0.7 and quantization factor Q=40 would generate ⌊40×0.7⌋ = 28 occurrences of its associated term. This creates an inverted index where term frequencies are proportional to vector values.
- Core assumption: The inner product between vectors can be approximated through term frequency matching in inverted indexes, and vector normalization ensures inner products equal cosine similarity
- Evidence anchors:
  - [abstract]: "The fake words encoding maintains direct proportionality between the float value of a feature and the term frequency of the corresponding fake index term"
  - [section]: "Feature-level matching for retrieval is achieved by matching on these fake words with scores computed by Lucene's ClassicSimilarity, which is a tf—idf variant"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.39, average citations=0.0. Top related titles: Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?, End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene, Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes.
- Break condition: If vectors are not normalized to unit length, the inner product and cosine similarity equivalence breaks, making this approach ineffective

### Mechanism 2
- Claim: Dense vectors can be indexed using lexical quantization combined with locality-sensitive hashing (LSH)
- Mechanism: Each vector dimension is rounded to a specified decimal place, tagged with its dimension index, and then n-grams are generated from these tokens. These n-grams are hashed using MinHash (LSH) into a configurable number of buckets. The vector is represented as a set of LSH-generated text signatures, which can be stored in an inverted index.
- Core assumption: LSH can preserve the similarity relationships between dense vectors when converted to discrete hash signatures, and these signatures can be efficiently indexed and retrieved using standard inverted index operations
- Evidence anchors:
  - [abstract]: "We implemented an approach that lexically quantizes vector components for easy indexing and search in Lucene using LSH"
  - [section]: "Given a vector w = (w1, . . . , wm), each feature wi is rounded to the d-th decimal place and tagged with its feature index i as a term prefix"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.39, average citations=0.0. Top related titles: Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?, End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene, Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes.
- Break condition: If the quantization granularity (d) is too coarse or too fine, the LSH signatures may lose too much information or become too sparse, degrading retrieval quality

### Mechanism 3
- Claim: Both fake words and lexical LSH techniques enable dense vector retrieval using standard inverted index infrastructure
- Mechanism: Both techniques transform dense vector representations into formats compatible with inverted indexes—either through term frequency encoding (fake words) or LSH-based hashing (lexical LSH). These transformed representations can then be searched using standard inverted index query processing, allowing dense retrieval without specialized vector index structures.
- Core assumption: Standard inverted index query processing can approximate nearest neighbor search when applied to these transformed dense vector representations
- Evidence anchors:
  - [abstract]: "we empirically evaluate top-k retrieval using these two techniques"
  - [section]: "we applied the same techniques, but to an actual real-world retrieval application"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.39, average citations=0.0. Top related titles: Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or Inverted Indexes?, End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene, Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes.
- Break condition: If query throughput requirements are high (e.g., real-time applications), the inverted index approach becomes impractically slow compared to HNSW indexes

## Foundational Learning

- Concept: Vector normalization and cosine similarity
  - Why needed here: The fake words technique requires unit-length vectors because it relies on the equivalence between inner products and cosine similarity
  - Quick check question: If a vector has magnitude 2.0, what must be done to make it compatible with the fake words approach?

- Concept: Locality-sensitive hashing (LSH)
  - Why needed here: Lexical LSH uses LSH to preserve similarity relationships when converting continuous vector values to discrete hash signatures
  - Quick check question: What property must an LSH function have to be useful for approximate nearest neighbor search?

- Concept: Inverted index query processing
  - Why needed here: Both techniques ultimately rely on standard inverted index query processing to retrieve relevant documents
  - Quick check question: How does term frequency contribute to scoring in Lucene's ClassicSimilarity used in the fake words approach?

## Architecture Onboarding

- Component map:
  - Vector encoding layer: Converts dense vectors to inverted index-compatible format (fake words or lexical LSH)
  - Inverted index storage: Standard Lucene inverted index storing the transformed representations
  - Query processing engine: Lucene's standard query processing for retrieval
  - Vector inference layer: Optional, for on-the-fly vector generation from raw text

- Critical path:
  1. Vector encoding: Transform dense vectors using quantization/LSH
  2. Index construction: Build standard inverted index from encoded vectors
  3. Query processing: Encode query vectors and perform inverted index search
  4. Scoring: Compute relevance scores using tf-idf (fake words) or LSH similarity

- Design tradeoffs:
  - Index size vs. effectiveness: Higher quantization factors (fake words) or more LSH buckets improve effectiveness but increase index size
  - Speed vs. effectiveness: HNSW indexes offer much faster retrieval but require more storage
  - Precision vs. recall: The transformation to inverted index format introduces approximation errors

- Failure signatures:
  - Very low query throughput compared to HNSW indexes
  - Effectiveness metrics (RR@10, nDCG@10) significantly lower than HNSW baselines
  - Index sizes much smaller than HNSW but with dramatically slower search

- First 3 experiments:
  1. Baseline comparison: Implement fake words with Q=40 and lexical LSH with b=400, compare effectiveness to HNSW on MS MARCO
  2. Parameter sweep: Vary Q (fake words) and b (lexical LSH) to find optimal effectiveness-speed tradeoff
  3. Index size analysis: Measure actual disk usage of inverted index vs. HNSW index for same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can further parameter tuning or optimization techniques significantly close the performance gap between inverted index approaches and HNSW indexes for dense vector retrieval?
- Basis in paper: [explicit] The authors state "While it would be possible to perform more exhaustive parameter tuning, better parameter selection alone is unlikely to close the performance gap between either technique and HNSW indexes."
- Why unresolved: The authors did not perform exhaustive parameter tuning and optimization, leaving open the possibility that more thorough exploration could yield better results.
- What evidence would resolve it: Comprehensive experiments exploring a wider range of parameters and optimization techniques for both fake words and lexical LSH approaches, comparing results to optimized HNSW indexes.

### Open Question 2
- Question: Are there specific use cases or scenarios where the smaller index size of inverted index approaches outweighs the slower query performance compared to HNSW indexes?
- Basis in paper: [inferred] The authors note that inverted index approaches have much smaller index sizes compared to HNSW indexes, but do not explore specific use cases where this advantage might be valuable.
- Why unresolved: The paper focuses on general effectiveness and efficiency comparisons, but does not deeply explore niche scenarios where compact indexes could be beneficial.
- What evidence would resolve it: Case studies or benchmarks demonstrating scenarios (e.g., edge computing, mobile devices, specific query workloads) where the compact index size provides significant advantages despite slower query times.

### Open Question 3
- Question: Can hybrid approaches combining inverted indexes with other techniques (e.g., pruning strategies, dimensionality reduction) improve the efficiency of dense vector retrieval while maintaining effectiveness?
- Basis in paper: [explicit] The authors mention "we can imagine a number of further explorations, such as hybrid dense–sparse models with inverted indexes" but state these do not seem promising at present.
- Why unresolved: The paper does not explore hybrid approaches in depth, leaving open the possibility that combining techniques could yield better results.
- What evidence would resolve it: Experiments comparing hybrid approaches (e.g., inverted indexes with aggressive pruning, dimensionality reduction techniques) against pure inverted index and HNSW approaches, measuring effectiveness, speed, and index size tradeoffs.

## Limitations
- Inverted index approaches are significantly slower than HNSW (2.8 QPS vs 48 QPS), making them unsuitable for high-throughput applications
- The effectiveness gap between inverted index methods and HNSW remains notable even with optimal parameter tuning
- Experimental validation is limited to a single dataset (MS MARCO passage ranking) and one specific dense retriever model (cosDPR-distil)

## Confidence
- **High confidence**: The fundamental mechanism of converting dense vectors to inverted index formats through term frequency encoding and LSH hashing is well-explained and technically sound
- **Medium confidence**: The effectiveness and speed measurements appear reliable for the tested configuration, but generalizability to other datasets and models remains uncertain
- **Low confidence**: The practical viability of these approaches for real-world deployment, given the substantial speed degradation compared to HNSW

## Next Checks
1. **Cross-dataset validation**: Test fake words and lexical LSH approaches on additional retrieval datasets (e.g., Natural Questions, TriviaQA) to assess generalizability
2. **Multi-model comparison**: Evaluate performance across different dense retriever architectures beyond cosDPR-distil to determine if results are model-dependent
3. **Real-world deployment simulation**: Measure end-to-end latency and throughput in a production-like environment with concurrent queries to better understand practical limitations