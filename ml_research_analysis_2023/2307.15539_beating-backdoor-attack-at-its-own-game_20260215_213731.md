---
ver: rpa2
title: Beating Backdoor Attack at Its Own Game
arxiv_id: '2307.15539'
source_url: https://arxiv.org/abs/2307.15539
tags:
- backdoor
- attack
- data
- samples
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel backdoor defense framework, Non-Adversarial
  Backdoor (NAB), which injects a backdoor targeting poisoned samples to suppress
  backdoor attack. Following the general steps in backdoor attack, NAB first detects
  a small set of suspected samples and then applies a poisoning strategy to them.
---

# Beating Backdoor Attack at Its Own Game

## Quick Facts
- arXiv ID: 2307.15539
- Source URL: https://arxiv.org/abs/2307.15539
- Reference count: 40
- This paper proposes Non-Adversarial Backdoor (NAB), a defense framework that injects a backdoor targeting poisoned samples to suppress backdoor attacks

## Executive Summary
This paper introduces a novel defense framework called Non-Adversarial Backdoor (NAB) that takes a counterintuitive approach to backdoor defense by injecting a defensive backdoor that targets the attacker's poisoned samples. The method works by detecting suspected poisoned samples and applying a poisoning strategy that stamps these samples and assigns pseudo labels. The resulting non-adversarial backdoor, when triggered, suppresses the attacker's backdoor on poisoned data while having limited influence on clean data. The defense can be implemented during data preprocessing without modifying the standard training pipeline.

## Method Summary
NAB operates through a four-step process: first, it detects suspected poisoned samples from the training data using methods like LGA, LN, or SPECTRE; second, it applies a poisoning strategy that stamps these samples with a defensive pattern and generates pseudo labels; third, it trains the model on the combined dataset of stamped poisoned samples and remaining clean data; and fourth, during inference, it stamps all inputs and optionally filters poisoned samples by comparing predictions with and without the stamp. The non-adversarial backdoor learned through this process suppresses the adversarial backdoor when both are present on poisoned samples.

## Key Results
- NAB achieves state-of-the-art defense effectiveness across multiple benchmarks (CIFAR-10, tiny-ImageNet) with minimal performance drop on clean data
- The method successfully defends against representative backdoor attacks including BadNets, Blend, Dynamic, WaNet, and Clean-Label attacks
- NAB maintains high clean accuracy while significantly reducing attack success rates compared to existing defense methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The non-adversarial backdoor suppresses the attacker's backdoor on poisoned data by dominating the representation space.
- Mechanism: When both the attacker's trigger and defender's stamp are present on a poisoned sample, the model's representation becomes dominated by the defender's stamp, causing the model to predict the pseudo label instead of the attacker's target class.
- Core assumption: The non-adversarial backdoor can be learned alongside the adversarial backdoor, and the stamp pattern has sufficient influence on the model's behavior to override the trigger pattern when both are present.
- Evidence anchors:
  - [abstract]: "The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data."
  - [section]: "Jointly optimizing the model using the three losses leaves two backdoors in the network: an adversarial backdoor triggered by P(·) and a non-adversarial backdoor triggered by S(P(·))."

### Mechanism 2
- Claim: The method maintains clean accuracy by limiting the non-adversarial backdoor's influence on clean data.
- Mechanism: The non-adversarial backdoor is only triggered on stamped samples, which are primarily poisoned samples. Clean samples without the stamp are unaffected by the backdoor, preserving their normal classification behavior.
- Core assumption: The detection method can accurately identify most poisoned samples while minimizing the inclusion of clean samples in the stamped set.
- Evidence anchors:
  - [abstract]: "The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data."
  - [section]: "When the detection accuracy is low, the non-adversarial backdoor might influence the performance on clean data."

### Mechanism 3
- Claim: The test data filtering technique improves defense effectiveness by identifying poisoned samples.
- Mechanism: By comparing predictions with and without the stamp, the method can identify poisoned samples where the predictions differ. These samples are rejected, ensuring they don't reach the final prediction stage.
- Core assumption: The model's predictions on clean samples remain consistent whether stamped or not, while predictions on poisoned samples change significantly when stamped.
- Evidence anchors:
  - [abstract]: "we stamp each input to keep the non-adversarial backdoor triggered. We can also adopt a test data filtering technique by comparing the predictions with or without the stamp."
  - [section]: "We identify samples with fθ(x) ̸= fθ(S(x)) as poisoned and reject them during inference."

## Foundational Learning

- Concept: Backdoor attacks and defenses
  - Why needed here: Understanding the threat model and existing defense approaches is crucial for appreciating the novelty of this non-adversarial backdoor approach.
  - Quick check question: What is the key difference between dirty-label and clean-label backdoor attacks?

- Concept: Neural network training and optimization
  - Why needed here: The paper discusses how the model learns multiple objectives (clean loss, attack loss, and defense loss) during training, which requires understanding of neural network optimization.
  - Quick check question: How does the joint optimization of multiple losses affect the learning of backdoors in the network?

- Concept: Representation learning and feature space
  - Why needed here: The method relies on the manipulation of representations in the feature space, as evidenced by the visualization of representations under different conditions.
  - Quick check question: How do the representations of poisoned samples differ from clean samples in the feature space, and how does the stamp affect these representations?

## Architecture Onboarding

- Component map: Detection Module -> Poisoning Strategy Module -> Training Pipeline -> Inference Pipeline (Stamping + Filtering)
- Critical path: 1. Detect suspected poisoned samples (D'_s) -> 2. Generate pseudo labels and apply stamp to create D'_m -> 3. Train model on combined dataset (D'_p = D'_m + remaining data) -> 4. During inference, stamp all inputs and optionally filter based on prediction consistency
- Design tradeoffs:
  - Detection rate vs. clean accuracy: Higher detection rate may include more clean samples, reducing clean accuracy
  - Stamp strength vs. stealth: Stronger stamps may be more effective but more noticeable
  - Pseudo label accuracy vs. computational cost: Higher quality pseudo labels improve performance but may require more resources
- Failure signatures:
  - Significant drop in clean accuracy: Indicates high false positive rate in detection or strong influence of non-adversarial backdoor on clean data
  - High attack success rate: Suggests insufficient suppression of adversarial backdoor or ineffective non-adversarial backdoor injection
  - Low defense success rate: Indicates poor detection accuracy or ineffective filtering technique
- First 3 experiments:
  1. Baseline: Train without any defense and measure clean accuracy and attack success rate
  2. Defense evaluation: Implement NAB and measure clean accuracy, attack success rate, and defense success rate under various attacks
  3. Ablation study: Test different combinations of detection methods and pseudo label generation strategies to identify most effective components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the non-adversarial backdoor be made sample-efficient to minimize performance degradation on clean data?
- Basis in paper: [explicit] The paper discusses the idea of sample-efficient backdoor injection and its potential benefits for NAB, but does not explore this aspect in detail.
- Why unresolved: The paper only mentions the concept of sample-efficient backdoor injection but does not provide a concrete implementation or analysis of its effectiveness in the context of NAB.
- What evidence would resolve it: A study comparing the performance of NAB with different sample-efficient backdoor injection techniques, demonstrating the trade-off between sample efficiency and defense effectiveness.

### Open Question 2
- Question: Can the generalization ability of the defender's backdoor in NAB be improved to provide defense against unknown backdoor attacks?
- Basis in paper: [explicit] The paper mentions the concept of "backdoor vaccination" and its potential to generalize defense effectiveness to other attacks, but only tests it under a limited setting.
- Why unresolved: The paper does not explore the generalization ability of the defender's backdoor in depth, and the backdoor vaccination experiment is limited to a specific scenario.
- What evidence would resolve it: A comprehensive study evaluating the generalization ability of the defender's backdoor in NAB against a wide range of backdoor attacks, including unknown attack strategies.

### Open Question 3
- Question: How can the backdoor detection and pseudo label generation components in NAB be made more robust to detection bias and improve overall defense performance?
- Basis in paper: [inferred] The paper discusses the potential impact of detection bias on NAB's performance and mentions the need for more robust detection and relabeling strategies, but does not provide a detailed analysis or solution.
- Why unresolved: The paper acknowledges the issue of detection bias and its potential impact on NAB's performance but does not explore ways to mitigate this problem or improve the robustness of the detection and relabeling components.
- What evidence would resolve it: A study investigating the sources of detection bias in backdoor detection methods and developing more robust detection and relabeling strategies to improve NAB's overall defense performance.

## Limitations

- The defense effectiveness is fundamentally constrained by the accuracy of the poisoned sample detection method, with high false positive rates causing clean accuracy degradation
- The approach assumes stamping during inference is practical and doesn't introduce unacceptable latency or privacy concerns
- The method's performance on more complex architectures or datasets with less clear distinctions between clean and poisoned samples remains unproven

## Confidence

- High confidence: The core mechanism of using a non-adversarial backdoor to suppress adversarial backdoors is technically sound and well-supported by experimental results
- Medium confidence: The generalization claims across different attacks and datasets are supported but limited to standard benchmarks
- Medium confidence: The stealth claims about limited influence on clean data are demonstrated but depend heavily on detection accuracy

## Next Checks

1. Test the detection accuracy of LGA, LN, and SPECTRE methods under varying attack strengths and trigger patterns to establish detection error bounds
2. Evaluate the defense on larger, more complex datasets (ImageNet-scale) to verify scalability claims
3. Measure inference latency and memory overhead introduced by the stamping and filtering pipeline in real-world deployment scenarios