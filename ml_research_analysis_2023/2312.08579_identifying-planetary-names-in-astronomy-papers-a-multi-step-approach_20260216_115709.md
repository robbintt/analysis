---
ver: rpa2
title: 'Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach'
arxiv_id: '2312.08579'
source_url: https://arxiv.org/abs/2312.08579
tags:
- feature
- names
- planetary
- crater
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-stage pipeline combining rule-based
  filtering, statistical analysis, part-of-speech tagging, named entity recognition,
  knowledge graph matching, and LLM inference to identify planetary feature names
  in astronomy publications. This approach addresses the significant lexical ambiguity
  of planetary feature names that overlap with common words, personal names, and locations.
---

# Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach

## Quick Facts
- arXiv ID: 2312.08579
- Source URL: https://arxiv.org/abs/2312.08579
- Reference count: 19
- Key outcome: Achieves F1-score > 0.97 for disambiguating planetary feature names in astronomy papers

## Executive Summary
This paper presents a comprehensive pipeline for identifying planetary feature names in astronomy publications, addressing the significant lexical ambiguity these names present when they overlap with common words, personal names, and locations. The approach combines rule-based filtering, statistical analysis, part-of-speech tagging, named entity recognition, knowledge graph matching, and LLM inference to achieve highly accurate disambiguation. The methodology fuses complementary global and local contextual signals to provide robust disambiguation capabilities, achieving an F1-score exceeding 0.97 when evaluated on astronomy papers containing ambiguous planetary feature names.

## Method Summary
The method employs a multi-stage pipeline that progressively filters and enriches candidate planetary feature names. It begins with rule-based search and excerpt extraction, followed by NLP filtering using POS tagging and AstroBERT NER to remove false positives. Keyword extraction using Yake, SpaCy, and Wikidata provides contextual signals, which are then analyzed using dual knowledge graphs for planetary and non-planetary contexts. Paper relevance scoring and LLM inference with Orca Mini provide global document understanding, while an SVM classifier assigns final labels and confidence scores. The pipeline prioritizes high recall while using complementary techniques to address specific failure modes at each stage.

## Key Results
- Achieves F1-score exceeding 0.97 for disambiguating planetary feature names
- Successfully distinguishes between identical feature names across different celestial bodies (e.g., Kaiser crater on Moon vs. Mars)
- Maintains high precision while prioritizing recall through multi-stage filtering approach

## Why This Works (Mechanism)

### Mechanism 1
The pipeline achieves high F1-score by combining local context analysis (keywords, KG matching) with global document understanding (LLM inference). Each stage progressively filters candidates and enriches contextual signals, with the LLM providing global context to resolve ambiguities that local analysis alone cannot handle.

### Mechanism 2
The knowledge graph effectively disambiguates identical feature names by leveraging semantic relationships between keywords and feature types. It stores co-occurrence patterns between feature names and descriptive keywords, comparing keyword sets from candidate excerpts against planetary and non-planetary KGs to calculate probability scores.

### Mechanism 3
Multi-stage filtering progressively reduces false positives while maintaining high recall through complementary techniques. Each pipeline stage addresses specific failure modes, creating robust performance by combining specialized techniques for different disambiguation challenges.

## Foundational Learning

- **Named Entity Recognition (NER) in domain-specific contexts**: Standard NER models fail on planetary feature names that overlap with common words, requiring specialized approaches. Quick check: What are the limitations of applying general-purpose NER models to domain-specific ambiguous entities?

- **Knowledge Graph construction and traversal for disambiguation**: KGs model semantic relationships between feature names and contextual keywords, enabling disambiguation of identical names across celestial bodies. Quick check: How does a KG differ from simple keyword matching in resolving ambiguous entity references?

- **Prompt engineering for LLM-based entity classification**: LLMs can provide global document context that local NLP techniques miss, but require careful prompting to achieve reliable classification. Quick check: What information should be included in prompts to help LLMs distinguish planetary feature names from other uses?

## Architecture Onboarding

- **Component map**: Paper Search & Retrieval (ADS API) -> Excerpt Extraction (129-token windows) -> POS Tagging (SpaCy) -> NER Filtering (AstroBERT) -> Keyword Extraction (Yake, SpaCy, Wikidata) -> Knowledge Graph Matching (dual KGs) -> Paper Relevance Scoring -> LLM Inference (Orca Mini) -> Label & Confidence (SVM classifier)

- **Critical path**: Paper Search → Excerpt Extraction → POS/NER Filtering → Keyword Extraction → KG Matching → LLM Inference → Label & Confidence

- **Design tradeoffs**: High recall prioritization vs. computational cost; local context vs. global context; static knowledge (KG) vs. dynamic understanding (LLM); precision vs. computational efficiency at each stage

- **Failure signatures**: Early filtering too aggressive → low recall; KG insufficiently trained → poor disambiguation; LLM prompts ineffective → unreliable global context; SVM mis-calibrated → confidence scores don't reflect actual accuracy

- **First 3 experiments**: 1) Test individual pipeline stages in isolation on a small labeled dataset; 2) Validate KG construction by examining keyword-feature relationships; 3) Evaluate LLM prompt effectiveness by testing different prompt formulations

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the pipeline handle cases where the same feature name appears in both planetary and non-planetary contexts within the same paper or excerpt? The paper describes individual pipeline stages but lacks specific examples or quantitative results for mixed contexts.

- **Open Question 2**: What is the impact of expanding the knowledge graph to include lesser-known entities on overall performance and accuracy? While potential benefits are suggested, actual impact on performance metrics is not demonstrated.

- **Open Question 3**: How does the pipeline perform when applied to astronomy papers from sources other than the Astrophysics Data System (ADS)? The pipeline's generalizability to other literature sources is unknown.

## Limitations
- Evaluation corpus does not directly validate the LLM inference stage's contribution to disambiguation performance
- Knowledge graph construction methodology lacks comprehensive testing beyond crater examples
- Pipeline effectiveness may depend on ADS dataset characteristics and generalizability to other sources is uncertain

## Confidence
- **High confidence**: Multi-stage filtering architecture and progressive candidate reduction approach
- **Medium confidence**: Knowledge graph's ability to disambiguate identical feature names across celestial bodies
- **Low confidence**: LLM's contribution to disambiguation performance and overall pipeline effectiveness

## Next Checks
1. Conduct ablation studies to quantify individual contributions of KG matching versus LLM inference to overall disambiguation performance
2. Evaluate knowledge graph construction methodology on a broader set of feature names to assess generalizability
3. Test pipeline's robustness on documents with limited global context to determine LLM's impact on performance degradation