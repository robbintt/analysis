---
ver: rpa2
title: Auditing for Human Expertise
arxiv_id: '2306.01646'
source_url: https://arxiv.org/abs/2306.01646
tags:
- which
- human
- expert
- test
- experttest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExpertTest is a statistical framework for detecting whether human
  experts incorporate information that cannot be captured by any algorithmic predictor.
  The method tests conditional independence between expert predictions and outcomes
  given observable features, using a nearest-neighbor resampling approach inspired
  by knockoffs and conditional permutation tests.
---

# Auditing for Human Expertise

## Quick Facts
- **arXiv ID**: 2306.01646
- **Source URL**: https://arxiv.org/abs/2306.01646
- **Reference count**: 40
- **Primary result**: ExpertTest detects whether human experts incorporate information not captured by algorithmic predictors through conditional independence testing

## Executive Summary
ExpertTest is a statistical framework that tests whether human experts leverage information beyond what's available to algorithmic predictors. The method uses nearest-neighbor resampling inspired by knockoffs to construct synthetic datasets that are exchangeable under the null hypothesis, then compares expert prediction loss to identify systematic advantage. Applied to emergency department triage for acute gastrointestinal bleeding, the framework shows physicians incorporate private information (like medication history) not captured in the Glasgow-Blatchford Score, despite the score being highly competitive with physician accuracy. This demonstrates that prediction accuracy alone is insufficient to justify algorithmic automation.

## Method Summary
ExpertTest tests conditional independence between expert predictions and outcomes given observable features. The algorithm finds L pairs of observations with similar feature values, generates K synthetic datasets by swapping expert predictions within pairs, and computes the fraction of synthetic datasets where expert loss is as good as or better than observed. The method provides interpretable p-values to assess if expert forecasts systematically outperform random chance after conditioning on available data. It addresses the fundamental question of whether human expertise captures information that cannot be algorithmically encoded.

## Key Results
- ExpertTest successfully detects when physicians incorporate private information not captured by algorithmic screening tools
- Physicians' hospitalization decisions for AGIB patients show significant complementarity to the Glasgow-Blatchford Score
- The method demonstrates that prediction accuracy alone is insufficient to justify algorithmic automation
- Findings highlight achievable complementarity between human expertise and algorithmic tools in practice

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The test can detect when human experts incorporate information not available to algorithms by testing conditional independence between expert predictions and outcomes given observable features.
- **Mechanism**: ExpertTest uses nearest-neighbor resampling inspired by knockoffs to construct synthetic datasets that are exchangeable under the null hypothesis. By comparing the loss on observed data to resampled data, it identifies if expert predictions systematically outperform random chance after conditioning on available data.
- **Core assumption**: Conditional independence between expert predictions and outcomes given features holds under the null hypothesis, and nearby feature values imply similar expert prediction distributions.
- **Evidence anchors**:
  - [abstract]: "ExpertTest is a statistical framework for detecting whether human experts incorporate information that cannot be captured by any algorithmic predictor."
  - [section]: "Our proposed algorithm takes the form of a conditional independence test, and is inspired by the Model-X Knockoffs framework..."
  - [corpus]: Weak evidence - corpus neighbors discuss similar frameworks but don't directly address this specific mechanism.
- **Break condition**: If the conditional independence assumption fails due to unmeasured confounding or if the smoothness assumption about prediction distributions breaks down.

### Mechanism 2
- **Claim**: The method provides interpretable p-values to assess if expert forecasts systematically outperform random chance after conditioning on available data.
- **Mechanism**: By computing the proportion of resampled datasets where the expert's loss is as good as or better than observed (τK), ExpertTest provides a p-value that represents the probability of achieving such performance by chance.
- **Core assumption**: The resampling procedure generates datasets that are exchangeable with observed data under the null hypothesis.
- **Evidence anchors**:
  - [abstract]: "The method tests conditional independence between expert predictions and outcomes given observable features, using a nearest-neighbor resampling approach inspired by knockoffs and conditional permutation tests."
  - [section]: "We leverage the prior fact about the order statistics of exchangeable random variables to design a test of H0: Y ⊥ ⊥ ˆY | X."
  - [corpus]: Weak evidence - corpus discusses conditional independence testing but not specifically this p-value interpretation.
- **Break condition**: If the exchangeability assumption breaks down or if the loss function is not discriminatory enough.

### Mechanism 3
- **Claim**: The test demonstrates that prediction accuracy alone is insufficient to justify algorithmic automation.
- **Mechanism**: By showing that physicians' decisions incorporate information not captured in algorithmic screening tools despite similar or worse accuracy, ExpertTest highlights the value of human expertise beyond pure predictive performance.
- **Core assumption**: Complementarity between human expertise and algorithmic tools is achievable in practice.
- **Evidence anchors**:
  - [abstract]: "This suggests physicians are leveraging private information (e.g., medication history) not captured in the score, despite its strong predictive performance."
  - [section]: "Consistent with prior literature, we find that this algorithmic score is an exceptionally sensitive measure of patient risk... Nonetheless, our test provides strong evidence that physician decisions... are incorporating valuable information that is not captured by the screening tool."
  - [corpus]: Moderate evidence - corpus neighbors discuss similar themes about human-AI complementarity.
- **Break condition**: If human decisions are purely noise or if automation is mandated for reasons beyond accuracy (e.g., fairness, accountability).

## Foundational Learning

- **Concept: Conditional independence testing**
  - Why needed here: ExpertTest fundamentally relies on testing whether expert predictions are conditionally independent of outcomes given observable features.
  - Quick check question: What does it mean for Y ⊥ ⊥ ˆY | X to hold or fail in the context of expert predictions?

- **Concept: Exchangeability and resampling**
  - Why needed here: The method constructs synthetic datasets through resampling that should be exchangeable with observed data under the null hypothesis.
  - Quick check question: How does swapping expert predictions for similar feature values create exchangeable datasets?

- **Concept: Odds ratios and total variation distance**
  - Why needed here: The theoretical analysis uses odds ratios to bound the approximation error when pairs are not identical but merely close.
  - Quick check question: How does the smoothness assumption about prediction distributions affect the total variation distance bound?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Feature selection, pairing similar observations -> Resampling engine -> Generates synthetic datasets by swapping predictions -> Loss computation -> Evaluates expert performance on observed and synthetic data -> Statistical testing -> Computes p-values and determines rejection of null hypothesis -> Validation module -> Assesses type I error and power through simulations

- **Critical path**:
  1. Identify L pairs of observations with similar feature values
  2. Generate K synthetic datasets by swapping predictions within pairs
  3. Compute loss on observed and synthetic data
  4. Calculate τK and compare to threshold α
  5. Reject or fail to reject null hypothesis

- **Design tradeoffs**:
  - Larger L provides more power but increases approximation error
  - More pairs improves test sensitivity but requires more computation
  - Choice of distance metric affects pairing quality and test validity
  - Loss function selection impacts interpretability and power

- **Failure signatures**:
  - High type I error when L is too large relative to sample size
  - Low power when expertise parameter δ is small
  - Instability in p-values when few pairs are available
  - Sensitivity to choice of distance metric and loss function

- **First 3 experiments**:
  1. Run ExpertTest on synthetic data where experts incorporate known unobserved information to verify detection capability
  2. Test on binary outcomes with varying expertise parameters to assess power scaling
  3. Apply to real-world triage data to validate practical utility and compare with algorithmic baselines

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the power of ExpertTest scale with the dimensionality of the feature space X when X is high-dimensional?
  - Basis in paper: [explicit] The paper notes that "Our algorithm does not scale naturally to settings where X is high-dimensional" and suggests kernel-based methods might be more powerful in such cases.
  - Why unresolved: The paper does not provide theoretical analysis or empirical results for high-dimensional settings. The nearest-neighbor approach may struggle with curse of dimensionality issues.
  - What evidence would resolve it: Empirical comparisons of ExpertTest's power versus kernel-based conditional independence tests across varying dimensions, with both synthetic and real data.

- **Open Question 2**: What is the optimal strategy for choosing the number of pairs L to balance type I error control and power?
  - Basis in paper: [explicit] The paper shows that increasing L both improves power and degrades type I error control, with a trade-off depending on n. Theorem 1 bounds the type I error as α + ε*_n,L + 1/(K+1), and simulations show false discovery rates exceeding 100% at large L.
  - Why unresolved: The paper provides theoretical bounds but no practical guidance for choosing L. The optimal choice likely depends on sample size, dimension, and the true distribution.
  - What evidence would resolve it: A systematic study of false discovery rates and power across different L/n ratios, with recommendations for practitioners based on sample size and dimension.

- **Open Question 3**: How sensitive is ExpertTest to the choice of distance metric m(·,·) and loss function F(·)?
  - Basis in paper: [inferred] The algorithm description uses m(·,·) as an algorithmic parameter and F(·) as a loss function of interest, but does not analyze sensitivity to these choices. The binary outcomes case shows results are robust to loss function choice, but general sensitivity is unexplored.
  - Why unresolved: The paper demonstrates robustness in one special case but does not analyze how different distance metrics or loss functions affect power or validity in general settings.
  - What evidence would resolve it: Empirical sensitivity analysis varying distance metrics (ℓ1, ℓ2, Mahalanobis) and loss functions (MSE, classification error, asymmetric costs) on synthetic and real data to identify robust choices.

## Limitations
- The method requires sufficient pairs of observations with similar features, which becomes challenging as dimensionality increases
- Conditional independence assumption may not hold when feature similarity doesn't imply similar expert prediction distributions
- Type I error control degrades when too many pairs are selected relative to sample size
- Nearest-neighbor approach doesn't scale naturally to high-dimensional feature spaces

## Confidence
- **High**: Statistical methodology and theoretical framework
- **Medium**: Real-world applicability and generalizability of findings
- **Medium**: Sensitivity to distance metric and loss function choices

## Next Checks
1. **Synthetic data stress test**: Generate datasets where the true conditional independence holds and systematically vary the expertise parameter δ to empirically validate type I error control and power curves across different sample sizes and feature dimensionalities.

2. **Alternative pairing strategies**: Compare ExpertTest performance using different distance metrics (e.g., Mahalanobis distance, learned similarity metrics) and evaluate robustness to feature scaling and correlation structure.

3. **Domain transferability assessment**: Apply ExpertTest to a different medical decision-making context (e.g., radiology diagnosis or surgical triage) to evaluate whether the method generalizes beyond emergency medicine and gastrointestinal bleeding prediction.