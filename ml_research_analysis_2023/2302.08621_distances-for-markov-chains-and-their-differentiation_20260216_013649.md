---
ver: rpa2
title: Distances for Markov Chains, and Their Differentiation
arxiv_id: '2302.08621'
source_url: https://arxiv.org/abs/2302.08621
tags:
- distance
- markov
- then
- distances
- chains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Optimal Transport Markov (OTM) distances,
  a unified framework for comparing Markov chains (and thus labeled graphs). This
  framework encompasses both the Weisfeiler-Lehman (WL) distance and the Optimal Transport
  Coupling (OTC) distance.
---

# Distances for Markov Chains, and Their Differentiation

## Quick Facts
- arXiv ID: 2302.08621
- Source URL: https://arxiv.org/abs/2302.08621
- Reference count: 40
- Key outcome: Introduces OTM distances unifying WL and OTC distances with a differentiable discounted WL variant

## Executive Summary
This paper proposes a unified framework called Optimal Transport Markov (OTM) distances for comparing Markov chains, which encompasses both the Weisfeiler-Lehman (WL) distance and the Optimal Transport Coupling (OTC) distance. The authors introduce a special one-parameter family called the discounted WL distance that addresses limitations of existing distances by being differentiable, comparable across non-stationary chains, and theoretically well-behaved. They provide algorithms for computing this distance and its gradient, enabling applications in machine learning and optimization tasks.

## Method Summary
The method introduces Optimal Transport Markov (OTM) distances as a unified framework for comparing Markov chains through a parameterized stopping-time distribution. The discounted WL distance is computed via iterative entropy-regularized optimal transport between transition kernels, with a discount factor δ ensuring convergence to a unique fixed point. The gradient is obtained through implicit differentiation of this fixed point equation, making the distance suitable for learning applications. The approach builds on the WL distance's iterative kernel comparison but regularizes it to enable differentiability and faster convergence.

## Key Results
- OTM framework unifies WL and OTC distances as extreme points of a one-parameter family
- Discounted WL distance is differentiable and can compare non-stationary Markov chains
- Algorithm for discounted WL distance converges provably faster than previous WL distance methods
- Theoretical analysis shows discounted WL has stronger discriminative power than WL distance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The OTM framework subsumes both WL and OTC distances by parameterizing over a stopping-time distribution p.
- Mechanism: OTM distance is defined as the infimum over Markovian couplings of expected cost at a random stopping time T ~ p. When p is a point mass at k, the OTM reduces to the depth-k WL distance; when p is geometric with δ→0, it converges to OTC.
- Core assumption: Both WL and OTC can be interpreted as optimal expected costs over random stopping times in Markovian couplings.
- Evidence anchors:
  - [abstract]: "we propose a unified framework to generate distances for Markov chains (thus including (directed) graphs with node attributes), which we call the Optimal Transport Markov (OTM) distances, that encompass both the OTC and the WL distances."
  - [section]: "We establish that the framework of OTM distances encompasses both the WL distance and the OTC distance and in particular, we prove that the two distances serve as extreme points in the family of OTM distances."
- Break condition: If either WL or OTC cannot be expressed as an expected cost over a stopping-time distribution in a Markovian coupling, the unification fails.

### Mechanism 2
- Claim: The discounted WL distance can be differentiated because it is a regularized version of the WL distance that has a unique fixed point.
- Mechanism: Regularization with δ > 0 ensures that the recursive computation (Equation 10) converges to a unique fixed point C δ, (∞ ), enabling gradient computation via implicit differentiation of the fixed point equation.
- Core assumption: The regularization makes the recursive map contractive, so the fixed point is unique and smooth.
- Evidence anchors:
  - [abstract]: "Furthermore, contrary to both the OTC and the WL distances, our new discounted WL distance can be differentiated against its parameters, enabling a range of possible applications as a loss in machine learning or in other optimization tasks."
  - [section]: "This regularization enables us to use compute the new distance via solving a Banach fixed point problem. This approach is very tractable, addressing the limitations of the original WL distance, and providing the ability to differentiate our distance."
- Break condition: If the recursive map is not contractive for some δ > 0 (e.g., if the transition kernels are degenerate), the fixed point may not be unique or differentiable.

### Mechanism 3
- Claim: The discounted WL distance has better theoretical properties than WL and OTC distances because it can compare non-stationary Markov chains and has provable convergence rate.
- Mechanism: By introducing a discount factor δ, the discounted WL distance avoids the issues of WL distance's non-unique fixed point and OTC's stationarity requirement, enabling broader applicability and faster convergence.
- Core assumption: The discount factor δ regularizes the computation sufficiently to ensure uniqueness and faster convergence without sacrificing discriminative power.
- Evidence anchors:
  - [abstract]: "We show that the discounted WL distance has nice theoretical properties and can address several limitations of the existing OTC and WL distances: 1. Contrary to the WL and the OTC distances, the discounted WL distance can be used to compare non-stationary Markov chains, 2. The discounted WL distance has the same discriminative power as the OTC distance and possibly stronger discriminative power than the WL distance, 3. All the three types of distances are computed via iterative schemes. We devise an algorithm of the discounted WL distance which converges provably faster than the one for the WL distance introduced in [Chen et al., 2022]."
  - [section]: "The discounted WL distance behaves nicely as when k approaches ∞: Proposition 12 (Convergence w.r.t. k) For any Markov chains X and Y, any cost function C, and any δ ∈ [0, 1], one has that d(∞)WL,δ(X,Y) = lim k→∞ d(k)WL,δ(X,Y)."
- Break condition: If the discount factor δ is too small, the distance may degenerate to the original WL distance and lose its advantages; if too large, it may over-regularize and lose discriminative power.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein distance
  - Why needed here: The paper builds on OT theory to compare Markov chains; understanding OT is crucial for grasping the WL and OTC distances.
  - Quick check question: Can you explain how the Wasserstein distance between two probability measures is defined and what role the cost function plays?

- Concept: Markov chains and couplings
  - Why needed here: The distances are defined for Markov chains, and the core mechanism involves finding optimal couplings between them.
  - Quick check question: What is a Markovian coupling, and how does it relate to comparing two Markov chains?

- Concept: Banach fixed point theorem and implicit differentiation
  - Why needed here: The discounted WL distance is computed via a fixed point iteration, and its gradient is obtained via implicit differentiation of the fixed point equation.
  - Quick check question: Can you explain how the Banach fixed point theorem guarantees the existence and uniqueness of the fixed point in the discounted WL distance computation?

## Architecture Onboarding

- Component map:
  - Input: Two Markov chains (X, mX, νX) and (Y, mY, νY) with a cost function C
  - Core: OTM distance computation (for general p), discounted WL distance computation (for δ > 0), gradient computation (for δ > 0 and ε > 0)
  - Output: Distance value and optionally its gradient
  - Key data structures: Cost matrices C δ, (l) and C ε,δ, (∞ ), optimal transport matrices Pij, dual solutions fij and gij

- Critical path:
  1. Compute the discounted WL distance recursively using Equation 10 until convergence
  2. If gradient is needed, compute the entropy-regularized version and solve for the fixed point C ε,δ, (∞ )
  3. Use the optimal transport solutions to compute the gradient via Equation 14-16

- Design tradeoffs:
  - Regularization (δ, ε) vs. discriminative power: Larger δ and ε make the distance smoother and easier to differentiate but may lose some discriminative power
  - Convergence speed vs. accuracy: Faster convergence algorithms (e.g., with sparse kernels) may sacrifice some accuracy

- Failure signatures:
  - Distance value not changing with input changes: May indicate numerical issues or that the discount factor is too large
  - Gradient computation failing: May indicate that the fixed point is not unique or that the regularization is insufficient

- First 3 experiments:
  1. Compare the discounted WL distance with the original WL distance on a simple graph dataset to verify that it has the same discriminative power but is easier to compute
  2. Test the gradient computation on a simple Markov chain to verify that the gradients are correct and can be used for optimization
  3. Evaluate the performance of the discounted WL distance as a loss function in a graph generative model to verify its practical utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the WL distance and the OTC distance be differentiated with respect to their input parameters (initial distributions, transition kernels, and cost functions)?
- Basis in paper: [explicit] The paper explicitly states that differentiating the WL distance and the OTC distance appears to be challenging, and to the best of the authors' knowledge, there is no known way to compute their derivatives.
- Why unresolved: The paper does not provide a method for differentiating these distances and highlights the lack of existing approaches.
- What evidence would resolve it: A method or algorithm that can compute the gradients of the WL distance and the OTC distance with respect to their input parameters would resolve this question.

### Open Question 2
- Question: Is the order of limits interchangeable for the discounted WL distance, i.e., does d^∞_WL = d_OTC?
- Basis in paper: [explicit] The paper states that while they empirically observe that d^∞_WL is not equal to d_OTC in general, they do not know for sure whether they are equal due to the approximation nature of the algorithms implemented. They leave this as an open question for future study.
- Why unresolved: The paper acknowledges the uncertainty due to the limitations of the current algorithms and the need for further investigation.
- What evidence would resolve it: A rigorous mathematical proof showing either the equality or inequality of d^∞_WL and d_OTC would resolve this question.

### Open Question 3
- Question: How does the convergence rate of the discounted WL distance compare to the convergence rate of the original WL distance in practice?
- Basis in paper: [explicit] The paper establishes that the discounted WL distance has a provably faster convergence rate than the original WL distance in terms of the number of iterations needed to reach a certain accuracy.
- Why unresolved: The paper provides theoretical convergence rate bounds but does not present empirical comparisons of the actual convergence rates in practice.
- What evidence would resolve it: Empirical studies comparing the number of iterations and computational time required for the discounted WL distance and the original WL distance to converge to a similar level of accuracy would resolve this question.

## Limitations

- Regularization parameter selection: The paper does not provide clear guidance on how to choose the discount factor δ and entropy regularization ε for specific applications
- Scalability concerns: Computational complexity and memory requirements for large graphs are not thoroughly evaluated
- Limited empirical validation: Claims about discriminative power superiority lack comprehensive experimental evidence on diverse datasets

## Confidence

- **High Confidence**: The theoretical framework of OTM distances, the unification of WL and OTC distances, and the differentiability of the discounted WL distance are well-established through rigorous mathematical proofs.
- **Medium Confidence**: The convergence rate of the discounted WL distance algorithm and its computational efficiency are supported by theoretical analysis, but empirical validation on large-scale datasets is needed.
- **Low Confidence**: The claims about the discriminative power of the discounted WL distance compared to the WL distance and the practical guidance for selecting regularization parameters are not sufficiently supported by experimental evidence.

## Next Checks

1. **Parameter Sensitivity Analysis**: Conduct a thorough sensitivity analysis to understand how the choice of δ and ε affects the distance's properties, convergence speed, and discriminative power. This will provide practical guidance for selecting these parameters in different applications.

2. **Scalability Evaluation**: Evaluate the scalability of the discounted WL distance algorithm on large graph datasets with varying sizes and densities. Measure the computational time and memory usage to assess the algorithm's practical feasibility for real-world applications.

3. **Discriminative Power Comparison**: Perform a comprehensive comparison of the discriminative power of the discounted WL distance, the original WL distance, and the OTC distance on a diverse set of graph datasets. Use classification or clustering tasks to quantify the distances' ability to distinguish between different graph structures.