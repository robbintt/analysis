---
ver: rpa2
title: Training Diffusion Models with Reinforcement Learning
arxiv_id: '2305.13301'
source_url: https://arxiv.org/abs/2305.13301
tags:
- diffusion
- reward
- arxiv
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a reinforcement learning approach, DDPO, for
  training diffusion models on downstream objectives such as image quality and prompt
  alignment. The key idea is to frame the denoising process as a multi-step MDP, allowing
  direct optimization of the diffusion model using policy gradients.
---

# Training Diffusion Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2305.13301
- Source URL: https://arxiv.org/abs/2305.13301
- Reference count: 40
- Key outcome: Reinforcement learning approach (DDPO) outperforms reward-weighted likelihood methods on tasks like image compressibility and aesthetic quality, using vision-language models for prompt alignment without human labels.

## Executive Summary
This paper introduces Denoising Diffusion Policy Optimization (DDPO), a reinforcement learning framework for training diffusion models to optimize downstream objectives beyond standard likelihood maximization. By framing the denoising process as a multi-step Markov Decision Process, DDPO enables direct policy gradient optimization on per-timestep likelihoods, allowing for more effective reward optimization. The method demonstrates improvements over reward-weighted regression baselines on tasks including image compressibility, aesthetic quality, and prompt-image alignment using vision-language model feedback.

## Method Summary
The approach treats the diffusion denoising process as a fixed-horizon MDP where each timestep uses the exact Gaussian likelihood from a fixed sampler, enabling unbiased policy gradient estimates via REINFORCE or importance sampling estimators. The model is trained on guided ε-predictions (CFG training) to maintain stability across multiple fine-tuning rounds. Three reward functions are explored: JPEG file size, LAION aesthetic predictor, and VLM-based prompt alignment using LLaVA and BERTScore. The method uses the Stable Diffusion v1.4 base model and is implemented in JAX/Flax with optax for optimization.

## Key Results
- DDPO outperforms reward-weighted regression on optimizing image compressibility and aesthetic quality metrics
- Vision-language model feedback (LLaVA + BERTScore) improves prompt-image alignment without human annotation
- CFG training stabilizes multi-round RL fine-tuning, preventing performance degradation observed with conditional training alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DDPO reframes denoising as a multi-step MDP to enable policy gradient optimization on per-timestep likelihoods.
- **Mechanism:** The denoising process is mapped to a fixed-horizon MDP where each timestep uses the exact Gaussian likelihood from the fixed sampler. This allows unbiased policy gradient estimates via the REINFORCE or importance sampling estimators.
- **Core assumption:** The sampler is fixed and uses an isotropic Gaussian with known variance, making the transition probabilities tractable and differentiable.
- **Evidence anchors:**
  - [abstract]: "posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms"
  - [section 4.3]: MDP definition with exact likelihoods `pθ(xt−1|xt,c)`
  - [corpus]: Weak/no evidence of other papers discussing exact per-timestep likelihoods in diffusion RL
- **Break condition:** If the sampler is changed to one without analytic transition densities, the MDP becomes intractable.

### Mechanism 2
- **Claim:** Training on guided ε-predictions (CFG training) stabilizes RL fine-tuning over multiple rounds.
- **Mechanism:** Using the guided prediction during both training and sampling keeps the guidance weight calibrated across updates, preventing drift that would otherwise degrade samples and block further learning.
- **Core assumption:** The guidance weight `w` is fixed and the guided prediction is a smooth, learnable function of model parameters.
- **Evidence anchors:**
  - [appendix B.1]: "we found that when only training on the conditional objective, performance rapidly deteriorated after the first round of finetuning"
  - [appendix B.1]: CFG training described as fixing `w` and using guided ε-prediction in training
  - [corpus]: No direct corpus evidence; likely novel insight
- **Break condition:** If `w` is too large, mode collapse can occur; if too small, reward optimization becomes ineffective.

### Mechanism 3
- **Claim:** Vision-language model feedback can replace human annotation for prompt-image alignment reward.
- **Mechanism:** LLaVA generates a textual description of a sample; BERTScore compares this description to the prompt, yielding a differentiable proxy for alignment without collecting human labels.
- **Core assumption:** The VLM's description is semantically meaningful and its output distribution is stable across training updates.
- **Evidence anchors:**
  - [abstract]: "show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation"
  - [section 5.3]: LLaVA + BERTScore reward pipeline described
  - [corpus]: No citations; method appears novel in this context
- **Break condition:** If the VLM's output drifts or becomes adversarial (e.g., typographic attacks), the reward becomes unreliable.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - **Why needed here:** The denoising process must be formalized as sequential decision-making to apply RL algorithms.
  - **Quick check question:** What are the state, action, and reward in the denoising MDP?
- **Concept:** Policy Gradient Estimation
  - **Why needed here:** Direct optimization of the diffusion model parameters requires unbiased gradient estimates of the expected reward.
  - **Quick check question:** How does the score-function estimator differ from the importance-sampling estimator in DDPO?
- **Concept:** Classifier-Free Guidance (CFG)
  - **Why needed here:** CFG stabilizes the conditioning signal and ensures high sample quality during RL fine-tuning.
  - **Quick check question:** Why does fixing the guidance weight during training prevent drift in the reward signal?

## Architecture Onboarding

- **Component map:** Diffusion UNet -> Fixed sampler -> Reward function -> RL optimizer
- **Critical path:**
  1. Sample latent noise → apply denoising timesteps with guided ε-prediction
  2. Decode to image → compute reward
  3. Accumulate per-timestep log-prob gradients → update UNet
- **Design tradeoffs:**
  - CFG training vs. pure conditional training: CFG adds stability at the cost of an extra hyperparameter.
  - Score-function vs. importance-sampling estimators: IS allows multiple gradient steps but requires trust-region clipping.
  - Sparse vs. exponentiated weights (RWR): Sparse is simpler but may lose information from near-threshold samples.
- **Failure signatures:**
  - Overoptimization → mode collapse, degenerate images, or typographic attacks
  - Training instability → exploding gradients if clip range is too large
  - Reward miscalibration → low sample quality despite high reward
- **First 3 experiments:**
  1. Run DDPOIS on compressibility with clip range 1e-4, verify monotonic reward increase.
  2. Compare DDPOIS vs. DDPOSF on aesthetic quality, check sample diversity.
  3. Test CFG training vs. no-CFG in RWRsparse across multiple rounds, observe stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of DDPO scale with the number of denoising steps (T) in the diffusion model?
- **Basis in paper:** [explicit] The paper mentions that they use T=50 denoising steps in their experiments, but does not explore the effect of varying this hyperparameter.
- **Why unresolved:** The paper does not provide an ablation study on the impact of the number of denoising steps on the effectiveness of DDPO.
- **What evidence would resolve it:** An experiment comparing the performance of DDPO with different numbers of denoising steps (e.g., T=10, T=50, T=100) on the same tasks would show how the number of steps affects the algorithm's ability to optimize downstream objectives.

### Open Question 2
- **Question:** How does the choice of sampler affect the performance of DDPO compared to RWR?
- **Basis in paper:** [inferred] The paper mentions that RWR corresponds to a one-step MDP, while DDPO maps the denoising process to a multi-step MDP. This suggests that the choice of sampler, which affects the dynamics of the denoising process, could impact the relative performance of the two approaches.
- **Why unresolved:** The paper does not explore different samplers or compare the performance of DDPO and RWR under different sampling schemes.
- **What evidence would resolve it:** An experiment comparing the performance of DDPO and RWR using different samplers (e.g., DDPM, DDIM, PNDM) on the same tasks would show how the choice of sampler affects the effectiveness of each method.

### Open Question 3
- **Question:** How does the complexity of the reward function impact the performance of DDPO compared to RWR?
- **Basis in paper:** [explicit] The paper evaluates DDPO and RWR on tasks with varying complexity, from simple file size computations to vision-language model-based alignment. However, it does not systematically vary the complexity of the reward function.
- **Why unresolved:** The paper does not provide a controlled study on how the complexity of the reward function affects the relative performance of DDPO and RWR.
- **What evidence would resolve it:** An experiment varying the complexity of the reward function (e.g., using simpler vs. more complex vision-language models, or using handcrafted vs. learned reward functions) and comparing the performance of DDPO and RWR on the same tasks would show how reward function complexity impacts the effectiveness of each method.

## Limitations

- The method requires a fixed sampler with analytic transition densities, limiting flexibility in exploring alternative sampling strategies
- Vision-language model feedback depends on the VLM's output remaining semantically consistent, potentially vulnerable to adversarial examples
- Experimental validation focuses on specific reward functions without extensive testing on other potential objectives

## Confidence

- **High confidence:** The MDP formulation of denoising and its connection to policy gradients (Mechanism 1) - well-grounded in the mathematical framework and directly supported by the paper's derivations
- **Medium confidence:** CFG training stability benefits (Mechanism 2) - supported by ablation studies but with limited hyperparameter exploration and no theoretical guarantees
- **Medium confidence:** VLM-based reward for prompt alignment (Mechanism 3) - demonstrates feasibility but lacks extensive validation on diverse prompt types and potential failure modes

## Next Checks

1. Test DDPO with alternative samplers (e.g., non-isotropic noise schedules) to verify the method's robustness when exact likelihoods are unavailable, potentially requiring score-based approximations.

2. Conduct adversarial robustness tests on the VLM reward by generating prompts designed to trigger misleading VLM descriptions, measuring reward calibration breakdown.

3. Compare sample diversity metrics (e.g., LPIPS, IS) between DDPO and baseline methods across all three reward functions to quantify potential mode collapse from reward overoptimization.