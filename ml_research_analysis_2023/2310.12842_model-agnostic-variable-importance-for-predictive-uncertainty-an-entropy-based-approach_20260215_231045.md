---
ver: rpa2
title: 'Model-agnostic variable importance for predictive uncertainty: an entropy-based
  approach'
arxiv_id: '2310.12842'
source_url: https://arxiv.org/abs/2310.12842
tags:
- feature
- uncertainty
- features
- distribution
- entropy-pfi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends traditional explainability methods to quantify
  how features affect both model performance and predictive uncertainty. It introduces
  Likelihood-PFI, Entropy-PFI, Likelihood-PDP, Entropy-PDP, and Entropy-ICE, which
  measure feature impact on negative log-likelihood and entropy of the predictive
  distribution.
---

# Model-agnostic variable importance for predictive uncertainty: an entropy-based approach

## Quick Facts
- arXiv ID: 2310.12842
- Source URL: https://arxiv.org/abs/2310.12842
- Reference count: 6
- One-line primary result: This paper extends traditional explainability methods to quantify how features affect both model performance and predictive uncertainty.

## Executive Summary
This paper introduces Likelihood-PFI, Entropy-PFI, Likelihood-PDP, Entropy-PDP, and Entropy-ICE to measure feature impact on negative log-likelihood and entropy of predictive distributions for probabilistic models. The methods quantify how features affect both model accuracy and uncertainty by permuting features and measuring changes in performance and predictive entropy. Using synthetic and real-world datasets, the authors demonstrate that Entropy-PFI captures uncertainty from feature dependencies and extrapolation, while Likelihood-PFI reflects performance impact. The approach reveals complementary insights: features important for accuracy may not affect confidence, and vice versa.

## Method Summary
The paper adapts permutation feature importance, partial dependence plots, and individual conditional expectation plots for probabilistic models. Likelihood-PFI measures performance degradation when a feature is randomized, while Entropy-PFI quantifies uncertainty increases from breaking feature dependencies. The methods compute these metrics by comparing model behavior on original test data versus permuted versions, then visualizing local effects through Entropy-PDP and Entropy-ICE plots. Implementation requires generating permuted test sets, computing entropy and negative log-likelihood for each permutation, and averaging differences to obtain importance scores.

## Key Results
- Entropy-PFI captures uncertainty increases from feature dependencies and extrapolation, while Likelihood-PFI reflects performance impact
- In UCI diabetes dataset, plas feature shows high Likelihood-PFI (performance) and Entropy-PFI (uncertainty) at extreme values
- Joint interpretation of Entropy-PFI and Likelihood-PFI distinguishes informative but independent features from non-informative ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy-PFI captures uncertainty increases caused by feature values breaking shared task-relevant information.
- **Mechanism:** The method permutes a feature from its marginal distribution and measures the increase in predictive entropy. When the permuted feature value no longer aligns with its dependencies in other features, the model is forced to extrapolate, increasing epistemic uncertainty.
- **Core assumption:** The model's predictive distribution responds to out-of-distribution feature values with higher entropy, and feature dependencies carry task-relevant information.
- **Evidence anchors:**
  - [abstract] "... Entropy-PFI captures uncertainty caused by feature dependencies and extrapolation..."
  - [section] "Proposition 1... If X−j is independent of Xj, then the entropy-PFI is zero."
  - [corpus] Weak: no direct match found, but concept aligns with uncertainty quantification literature.
- **Break condition:** If the model does not model epistemic uncertainty or if dependencies are irrelevant to prediction, the method loses diagnostic power.

### Mechanism 2
- **Claim:** Likelihood-PFI reflects performance degradation when a feature is removed or randomized.
- **Mechanism:** Permutation of a feature in test examples breaks the model's access to that feature's information, increasing the negative log-likelihood of the true label.
- **Core assumption:** The model uses the feature in its decision process; otherwise the permutation would not affect performance.
- **Evidence anchors:**
  - [abstract] "... Likelihood-PFI reflects performance impact."
  - [section] "PFIL(j) = EX,Y [log q(Y |X)] − E eXj ,X−j ,Y [log q(Y |X−j, Xj = eXj)]"
  - [corpus] Weak: general PFI literature supports but not entropy-specific.
- **Break condition:** If the feature is independent of the target or unused by the model, Likelihood-PFI will be near zero.

### Mechanism 3
- **Claim:** Joint interpretation of Entropy-PFI and Likelihood-PFI distinguishes informative but independent features from non-informative ones.
- **Mechanism:** An informative but independent feature yields high Likelihood-PFI and near-zero Entropy-PFI; a non-informative feature yields near-zero for both.
- **Core assumption:** Features can be informative in predicting the target yet independent of other features.
- **Evidence anchors:**
  - [section] "By considering the Entropy-PFI and Likelihood-PFI jointly, we can distinguish between when a feature is informative but independent of others and when it is not informative..."
  - [section] "Proposition 2. If the predictive distribution is not dependent on feature j, i.e., q(Y |X) = q(Y |X−j), then PFIH(j) = 0 and PFIL(j) = 0."
  - [corpus] Weak: broader interpretability literature supports multi-metric evaluation.
- **Break condition:** If the model does not use the feature at all, both measures will be zero, making distinction impossible.

## Foundational Learning

- **Concept:** Permutation-based feature importance and its extrapolation problem
  - Why needed here: Entropy-PFI relies on forcing the model to extrapolate by breaking feature dependencies; understanding this behavior is essential to interpreting results.
  - Quick check question: What happens to a model's confidence when it encounters an input outside its training distribution?

- **Concept:** Entropy as a measure of predictive uncertainty
  - Why needed here: Entropy-PFI measures how much uncertainty increases when a feature's value is replaced with a random sample from its marginal distribution.
  - Quick check question: How does the entropy of a probability distribution change as the distribution becomes more uniform?

- **Concept:** Conditional vs marginal feature distributions
  - Why needed here: Traditional PFI methods assume independence; Entropy-PFI exploits the fact that real-world features are often dependent, and breaking these dependencies reveals uncertainty.
  - Quick check question: What is the difference between permuting a feature marginally versus conditionally on other features?

## Architecture Onboarding

- **Component map:** Model (any uncertainty-aware probabilistic model) → Test set → Entropy/Likelihood-PFI computation → Entropy-PDP/ICE generation → Interpretation
- **Critical path:** 1) Generate permuted test set 2) Compute entropy/negative log-likelihood for each permutation 3) Average differences to obtain PFI 4) Plot PDP/ICE for local effects
- **Design tradeoffs:** 
  - Simplicity vs. conditioning: Permutation-based methods are simpler but may force extrapolation; conditional methods are more robust but computationally heavier.
  - Global vs. local insights: PFI gives global importance; PDP/ICE reveal local behavior.
- **Failure signatures:** 
  - Zero Entropy-PFI for all features → Model may not model epistemic uncertainty.
  - High Entropy-PFI for independent features → Misalignment between model assumptions and data structure.
  - Unstable PDP/ICE → Insufficient test set size or high variance in entropy estimates.
- **First 3 experiments:**
  1. Run Entropy-PFI and Likelihood-PFI on a synthetic dataset with known feature dependencies; verify that redundant informative features have high Entropy-PFI and high Likelihood-PFI.
  2. Generate Entropy-PDP and Entropy-ICE for a feature known to be informative at extreme values; confirm high uncertainty at extremes and low uncertainty at typical values.
  3. Compare Entropy-PFI with conditional PFI on a dataset with strong feature dependencies; observe that conditional PFI fails to capture uncertainty sources.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively separate aleatoric and epistemic uncertainty in Entropy-PDP and Entropy-ICE plots?
- Basis in paper: [inferred] The authors note that they did not attempt to separate these two sources of uncertainty and identify it as future work.
- Why unresolved: The authors acknowledge that decomposing uncertainty into aleatoric and epistemic components is important but do not provide a methodology for doing so in the context of their entropy-based approaches.
- What evidence would resolve it: Development and validation of methods to decompose entropy-based uncertainty measures into aleatoric and epistemic components, demonstrated on synthetic and real-world datasets.

### Open Question 2
- Question: How does the performance of Entropy-PFI compare to conditional feature importance methods like CPFI in the presence of feature dependencies?
- Basis in paper: [explicit] The authors discuss the limitations of permutation-based methods due to feature dependencies and mention that conditional approaches address this issue, but they do not compare Entropy-PFI to these methods.
- Why unresolved: The paper focuses on the properties and applications of Entropy-PFI but does not empirically compare it to conditional feature importance methods in terms of handling feature dependencies.
- What evidence would resolve it: Empirical studies comparing Entropy-PFI and conditional feature importance methods on datasets with known feature dependencies, measuring both performance and interpretability.

### Open Question 3
- Question: What are the theoretical guarantees and limitations of using entropy-based measures for feature importance in non-probabilistic models?
- Basis in paper: [inferred] The paper primarily focuses on uncertainty-aware models and mentions that entropy can be used to measure uncertainty in predictive distributions, but does not discuss its application to non-probabilistic models.
- Why unresolved: The authors do not explore how entropy-based measures can be adapted or extended to non-probabilistic models, which limits the generalizability of their approach.
- What evidence would resolve it: Theoretical analysis and empirical validation of entropy-based feature importance measures for non-probabilistic models, identifying conditions under which these measures are meaningful and effective.

## Limitations
- The proposed methods rely on the assumption that the model's predictive distribution responds to out-of-distribution feature values with increased entropy, which may not hold for all model architectures.
- The methods assume that feature dependencies carry task-relevant information, which may not be true in all cases.
- The computational cost of the proposed methods may be high, especially for large datasets or complex models.

## Confidence
- High confidence in the mechanism by which Entropy-PFI captures uncertainty increases caused by feature dependencies and extrapolation.
- Medium confidence in the ability of Likelihood-PFI to reflect performance degradation when a feature is removed or randomized, as this depends on the specific model architecture and data characteristics.
- Low confidence in the ability to distinguish informative but independent features from non-informative ones using joint interpretation of Entropy-PFI and Likelihood-PFI, as this requires careful consideration of the model's decision process and the data's feature structure.

## Next Checks
1. **Synthetic Data Validation:** Generate a synthetic dataset with known feature dependencies and validate that Entropy-PFI captures the uncertainty increases caused by extrapolation.
2. **Model Architecture Robustness:** Test the proposed methods on a variety of model architectures (e.g., neural networks, random forests) to ensure that the methods are not sensitive to the specific model used.
3. **Conditional vs Marginal Permutation:** Compare the results of Entropy-PFI with conditional permutation to assess the impact of breaking feature dependencies on the method's effectiveness.