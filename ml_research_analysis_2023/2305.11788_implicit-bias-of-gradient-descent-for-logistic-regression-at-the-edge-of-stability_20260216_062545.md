---
ver: rpa2
title: Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability
arxiv_id: '2305.11788'
source_url: https://arxiv.org/abs/2305.11788
tags:
- loss
- lemma
- have
- since
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the behavior of constant-stepsize gradient descent
  (GD) for logistic regression on linearly separable data in the edge-of-stability
  regime, where large stepsizes lead to local risk oscillations. Despite the presence
  of oscillations, the authors prove that GD with any constant stepsize can minimize
  the logistic loss over a long time scale at a rate of O(1/t).
---

# Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability

## Quick Facts
- arXiv ID: 2305.11788
- Source URL: https://arxiv.org/abs/2305.11788
- Authors: [Not specified in input]
- Reference count: 40
- Key outcome: Proves constant-stepsize GD minimizes logistic loss at O(1/t) rate and converges to max-margin direction in edge-of-stability regime.

## Executive Summary
This paper analyzes constant-stepsize gradient descent for logistic regression on linearly separable data in the edge-of-stability regime. The authors prove that despite large stepsizes causing local risk oscillations, GD can still minimize logistic loss at a rate of O(1/t). Through space decomposition techniques, they show iterates converge to the max-margin direction while staying bounded in the orthogonal complement. The analysis reveals that GD's implicit bias toward max-margin solutions persists even in this challenging regime, contrasting with catastrophic divergence observed under exponential loss.

## Method Summary
The paper studies constant-stepsize gradient descent on logistic regression with linearly separable data. Using MNIST dataset with labels "0" and "8" (1000 random samples), the method normalizes features to [-1, 1] and implements logistic regression with linear model (no bias). Three stepsizes are tested: η = 10, 0.1, and 0.01. The analysis decomposes iterates into max-margin and non-separable subspaces, tracking growth rates and convergence properties. The key insight is that despite oscillations in the edge-of-stability regime, the logistic loss still decreases at O(1/t) rate while iterates align with the hard-margin SVM direction.

## Key Results
- Constant-stepsize GD minimizes logistic loss at O(1/t) rate for any constant stepsize in edge-of-stability regime
- GD iterates converge to infinity in max-margin direction (hard-margin SVM direction) at rate O(log t)
- Non-separable subspace iterates converge to minimizer of strongly convex potential
- GD diverges catastrophically under exponential loss in edge-of-stability regime, demonstrating logistic loss superiority

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constant-stepsize GD minimizes logistic loss even in edge-of-stability regime
- Mechanism: Decomposition of iterates into max-margin subspace (P) and non-separable subspace (P̄), where P(wt) grows logarithmically while P̄(wt) stays bounded and converges to minimizer of strongly convex potential
- Core assumption: Linear separability of data and boundedness of non-separable subspace iterates
- Evidence anchors:
  - [abstract]: "we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale"
  - [section 4]: "Theorem 4.1(A) guarantees that the GD iterates minimize the logistic loss at a rate of O(1/t) for any constant stepsize"
  - [corpus]: Weak - neighboring papers focus on EOS behavior but don't directly address logistic loss minimization
- Break condition: Loss function has unbounded gradients (e.g., exponential loss) or data not linearly separable

### Mechanism 2
- Claim: GD iterates converge to max-margin direction despite oscillations
- Mechanism: P(wt) ~ log(t)/γ growth dominates direction alignment while P̄(wt) stays bounded, ensuring overall direction converges to hard-margin SVM direction
- Core assumption: Support vectors span the dataset and have strictly positive dual variables
- Evidence anchors:
  - [abstract]: "the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction)"
  - [section 4]: "Theorem 4.1(B) shows that the GD iterates, when projected to the max-margin direction, tend to infinity at a rate of O(log(t))"
  - [corpus]: Moderate - neighboring paper on "Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks" supports direction convergence concept
- Break condition: Dataset has degenerate structure where support vectors don't span the space

### Mechanism 3
- Claim: Non-separable subspace iterates converge to minimizer of exponential loss on support vectors
- Mechanism: P̄(wt) follows modified gradient descent with decaying effective stepsize η · e^(-γwt), controlled by growing wt
- Core assumption: Strongly convex potential G(v) = Σ exp(-y_i · ⟨P̄(x_i), v⟩) has unique minimizer
- Evidence anchors:
  - [abstract]: "converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement"
  - [section 4]: "Theorem 4.1(D) shows that the GD iterates, when projected to the non-separable subspace, converge to the minimizer of a strongly convex potential"
  - [corpus]: Weak - neighboring papers don't discuss this specific potential minimization mechanism
- Break condition: G(·) not strongly convex or stepsize grows too fast

## Foundational Learning

- Concept: Edge of stability regime
  - Why needed here: Framework for understanding non-monotonic loss behavior with large stepsizes
  - Quick check question: What characterizes the edge of stability regime in gradient descent optimization?

- Concept: Implicit bias in gradient descent
  - Why needed here: Explains why GD prefers max-margin solutions despite not explicitly optimizing for margin
  - Quick check question: How does the implicit bias of GD differ between small and large stepsize regimes?

- Concept: Space decomposition and projection operators
  - Why needed here: Enables analysis by separating iterates into components with different convergence behaviors
  - Quick check question: Why is decomposing the space into max-margin and non-separable subspaces useful for analyzing GD?

## Architecture Onboarding

- Component map: Data preprocessing -> Gradient descent implementation -> Space decomposition -> Max-margin tracking -> Non-separable convergence monitoring -> Validation
- Critical path:
  1. Verify linear separability of dataset
  2. Initialize GD with constant stepsize η
  3. Decompose iterates into P(wt) and P̄(wt) components
  4. Monitor P(wt) growth rate and direction alignment
  5. Track P̄(wt) convergence to potential minimizer
  6. Verify risk minimization at O(1/t) rate

- Design tradeoffs:
  - Larger η: Faster margin maximization but more oscillations
  - Smaller η: Smoother convergence but slower margin maximization
  - Dataset size: Affects computational cost of gradient calculations
  - Feature dimensionality: Impacts complexity of space decomposition

- Failure signatures:
  - Catastrophic divergence (non-monotonic growth)
  - Failure to align with max-margin direction
  - Oscillations preventing convergence in non-separable subspace
  - Risk not decreasing at expected O(1/t) rate

- First 3 experiments:
  1. Verify risk minimization: Plot L(wt) vs t for η=0.1, 0.01, 0.001 on linearly separable data
  2. Test margin maximization: Track cos(θ) between wt/||wt|| and hard-margin SVM direction
  3. Validate non-separable convergence: Monitor ||P̄(wt) - w̄*|| vs t where w̄* minimizes G(·)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implicit bias of gradient descent for logistic regression extend to other exponentially-tailed losses in the edge-of-stability regime?
- Basis in paper: [explicit] The paper demonstrates that GD with large stepsizes can diverge catastrophically under the exponential loss, highlighting the superiority of the logistic loss in this regime.
- Why unresolved: The paper only considers the logistic and exponential losses, leaving the behavior under other exponentially-tailed losses unexplored.
- What evidence would resolve it: Empirical and theoretical analysis of GD with various exponentially-tailed losses (e.g., logistic, exponential, and smooth approximations) in the edge-of-stability regime.

### Open Question 2
- Question: Can the space decomposition technique be extended to analyze GD with other types of regularization or normalization?
- Basis in paper: [inferred] The paper uses a space decomposition based on the max-margin direction and its orthogonal complement to analyze GD for logistic regression. This technique might be applicable to other algorithms that exhibit implicit bias.
- Why unresolved: The paper focuses on vanilla GD for logistic regression and does not explore extensions to other algorithms or regularization schemes.
- What evidence would resolve it: Extension of the space decomposition analysis to GD with weight decay, batch normalization, or other normalization techniques, comparing the implicit bias properties.

### Open Question 3
- Question: How does the edge-of-stability behavior of GD relate to generalization performance in deep learning models?
- Basis in paper: [inferred] The paper shows that large-stepsize GD can achieve better test accuracy than small-stepsize GD in some cases, but does not provide a comprehensive analysis of generalization.
- Why unresolved: The paper focuses on theoretical analysis of a simple logistic regression problem and does not investigate the relationship between edge-of-stability behavior and generalization in more complex models.
- What evidence would resolve it: Empirical studies comparing the generalization performance of GD with different stepsizes in deep learning models, along with theoretical analysis connecting the edge-of-stability behavior to generalization bounds.

## Limitations
- Analysis assumes linear separability and focuses specifically on logistic regression
- Logarithmic growth rate of P(wt) is proven but exact constants are not characterized
- Results may not generalize to non-linearly separable data or alternative loss functions

## Confidence

**High confidence**: Claims about O(1/t) logistic loss minimization and convergence to max-margin direction (supported by rigorous proofs in Theorem 4.1).

**Medium confidence**: Claims about P̄(wt) convergence to minimizer of strongly convex potential (requires verification of assumptions about G(·) being strongly convex).

**Low confidence**: Generalization to non-linearly separable data or alternative loss functions (exponential loss divergence suggests fragility of results).

## Next Checks
1. **Quantitative convergence verification**: Implement space decomposition approach and measure P(wt) ~ log(t)/γ growth rate empirically on linearly separable datasets with varying stepsizes.

2. **Robustness testing**: Test GD behavior on non-separable datasets to determine conditions under which the analysis breaks down.

3. **Alternative loss comparison**: Compare logistic regression convergence to exponential loss behavior under identical edge-of-stability conditions to quantify the robustness gap.