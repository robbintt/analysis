---
ver: rpa2
title: Why think step by step? Reasoning emerges from the locality of experience
arxiv_id: '2304.03843'
source_url: https://arxiv.org/abs/2304.03843
tags:
- variables
- reasoning
- training
- variable
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-thought reasoning in language models is most effective
  when training data is locally structured, with variables that influence each other
  appearing together frequently. When the training data includes local neighborhoods
  of related variables, language models can improve conditional inference by reasoning
  through intermediate steps.
---

# Why think step by step? Reasoning emerges from the locality of experience

## Quick Facts
- arXiv ID: 2304.03843
- Source URL: https://arxiv.org/abs/2304.03843
- Reference count: 19
- Key outcome: Chain-of-thought reasoning in language models is most effective when training data is locally structured, with variables that influence each other appearing together frequently.

## Executive Summary
This paper investigates when and why chain-of-thought reasoning works in language models. The key finding is that reasoning effectiveness depends critically on the statistical structure of training data. When training data consists of overlapping local clusters of variables that influence each other strongly, the model can chain together local inferences to estimate relationships between variables that were never observed together. This occurs because the model learns accurate local inferences from frequently co-occurring variables, which can then be combined through intermediate reasoning steps.

## Method Summary
The paper uses synthetic Bayesian networks with 100 Boolean variables to test how locality structure in training data affects reasoning performance. Training samples are generated with different locality constraints - either fully observed, locally structured with correct locality, or locally structured with wrong locality. A GPT-2 style transformer is trained on formatted variable-value pairs, and four different inference methods are compared: direct prediction, scaffolded generation (guided reasoning), free generation (unconstrained reasoning), and negative scaffolded generation. Performance is measured using mean squared error between estimated and true conditional probabilities for held-out variable pairs.

## Key Results
- Chain-of-thought reasoning is effective when training data consists of overlapping local clusters of variables that influence each other strongly
- Generating intermediate variables that d-separate the observed and target variables is especially helpful for conditional inference
- Reasoning provides no benefit when all variables are observed together or when the local structure does not match the underlying dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought reasoning works because it allows models to chain together local statistical dependencies that are frequently observed in training data.
- Mechanism: When training data has a locality structure where variables that influence each other strongly appear together frequently, the model can learn accurate local inferences. When asked to infer relationships between variables that were not seen together, reasoning through intermediate variables that were frequently observed with both the source and target variables allows the model to leverage these local inferences.
- Core assumption: The statistical structure of training data determines whether reasoning through intermediate variables is beneficial.
- Evidence anchors:
  - [abstract] "reasoning is effective when training data consists of overlapping local clusters of variables that influence each other strongly"
  - [section] "These training conditions enable the chaining of accurate local inferences to estimate relationships between variables that were not seen together in training"
- Break condition: The mechanism breaks when training data does not have the right locality structure, such as when all variables are observed together or when the local structure does not match the underlying dependencies between variables.

### Mechanism 2
- Claim: Reasoning through d-separating variables is especially helpful because it ensures all the influence that the observed variable has on the target variable is captured by the intermediate variables.
- Mechanism: When an intermediate variable d-separates the observed and target variables, conditioning on it renders them independent. This allows the model to compute the conditional probability by marginalizing over the intermediate variable, using the fact that P(C|A) = E[P(C|B)|A]. This approach is more data-efficient than learning the joint distribution of A and C directly.
- Core assumption: D-separation in the underlying Bayes net structure creates opportunities for more efficient inference through intermediate variables.
- Evidence anchors:
  - [abstract] "Generating intermediate variables that d-separate the observed and target variables is especially helpful"
  - [section] "Reasoning through B means that we never need to directly learn the joint distribution of A and C, but we can infer it from existing knowledge of the joint distributions between A and B and between B and C"
- Break condition: The mechanism breaks when the reasoning trace does not d-separate the observed and target variables, though the paper notes that even non-d-separating reasoning can sometimes improve performance.

### Mechanism 3
- Claim: Training on locally structured data with variable dropout helps the model generalize to pairs of variables that were unseen in training.
- Mechanism: By seeing many different unique combinations of variables in training samples (due to variable dropout within local neighborhoods), the model learns to handle dependencies between distant variables even when specific pairs were never observed together. This creates implicit associations between variables that co-occur with the same third variables.
- Core assumption: Variable dropout within local neighborhoods creates sufficient diversity in training samples to enable generalization to held-out variable pairs.
- Evidence anchors:
  - [section] "Variable dropout may also help the model generalize to pairs of variables that were unseen in training as the model sees many different unique combinations of variables in its training samples"
  - [section] "Without variable dropout, the model would only see the exact local neighborhoods in the Bayes net and may struggle to generalize beyond them"
- Break condition: The mechanism breaks when variable dropout is absent or when the dropout rate is too low to create sufficient diversity in training samples.

## Foundational Learning

- Concept: Autoregressive density estimation with transformer architecture
  - Why needed here: The paper uses an autoregressive transformer language model to learn the joint distribution over variables and perform conditional inference
  - Quick check question: How does an autoregressive transformer model the joint probability of a sequence of variables?

- Concept: Bayesian networks and conditional probability tables
  - Why needed here: The paper generates synthetic data from Bayesian networks to test how locality structure affects reasoning performance
  - Quick check question: What is the relationship between the structure of a Bayesian network and the conditional independence relationships between variables?

- Concept: d-separation and conditional independence
  - Why needed here: The paper tests whether generating variables that d-separate the observed and target variables is especially helpful for reasoning
  - Quick check question: How does d-separation in a Bayesian network relate to conditional independence between variables?

## Architecture Onboarding

- Component map:
  - Data generation module -> Language model -> Estimators -> Evaluation module
  - Bayes net structure and CPTs -> Formatted variable-value pairs -> Direct prediction, scaffolded generation, free generation, negative-scaffolded generation -> Mean squared error

- Critical path:
  1. Generate Bayesian network structure and conditional probability tables
  2. Create training samples with locality structure and variable dropout
  3. Train autoregressive transformer on formatted samples
  4. Test different inference methods on held-out variable pairs
  5. Evaluate performance using mean squared error

- Design tradeoffs:
  - Training on all variables vs. local subsets: Full observation allows perfect direct prediction but doesn't test reasoning capabilities; local subsets require reasoning but may be harder to train
  - Variable dropout rate: Higher dropout creates more diverse samples but may make learning harder; lower dropout may not provide sufficient generalization
  - Reasoning trace length: Longer traces may capture more dependencies but add noise; shorter traces may miss important intermediate variables

- Failure signatures:
  - Poor performance across all inference methods suggests issues with model architecture or training
  - Direct prediction outperforming reasoning methods indicates training data is too informative (wrong locality structure)
  - Reasoning traces that rarely d-separate observed and target variables suggest the locality structure doesn't match the underlying dependencies
  - Performance close to marginal probabilities suggests the model is learning to match marginals rather than joint distributions

- First 3 experiments:
  1. Train on fully observed data and test all inference methods to establish baseline performance
  2. Train on locally structured data with correct locality and test all inference methods to verify reasoning advantage
  3. Train on locally structured data with wrong locality and test all inference methods to verify reasoning disadvantage

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of chain-of-thought reasoning scale with the complexity of the underlying probabilistic models beyond Bayes nets?
  - Basis in paper: [explicit] The paper concludes by noting that future work should explore more richly structured worlds, especially those with hierarchical generative models and probabilistic programs, suggesting that the current findings may not generalize to these more complex structures.
  - Why unresolved: The study only tested reasoning in simple Bayes net structures, which may not capture the complexity of real-world reasoning tasks.
  - What evidence would resolve it: Experiments testing chain-of-thought reasoning effectiveness on more complex probabilistic models like hierarchical Bayesian models or probabilistic programs would show whether the findings scale.

- Question: Does the structure of training data influence the effectiveness of chain-of-thought reasoning in humans as it does in language models?
  - Basis in paper: [inferred] The paper draws parallels between human reasoning and language model reasoning, suggesting that the effectiveness of human reasoning might also depend on the statistical structure of the training data, i.e., the information humans encounter.
  - Why unresolved: The paper does not provide empirical evidence on human reasoning, only theoretical speculation based on language model results.
  - What evidence would resolve it: Empirical studies comparing human reasoning performance under different information structures (e.g., local vs. global information exposure) would show whether human reasoning effectiveness depends on data locality.

- Question: What is the optimal balance between local and global information in training data for maximizing chain-of-thought reasoning effectiveness?
  - Basis in paper: [explicit] The paper shows that local structure in training data is crucial for chain-of-thought reasoning to be effective, but also notes that direct prediction performs well when pairs of variables are frequently seen together, suggesting there may be a trade-off.
  - Why unresolved: The study only tested extremes (fully local, fully global, and wrong locality), not intermediate cases that might optimize reasoning effectiveness.
  - What evidence would resolve it: Experiments varying the degree of locality in training data and measuring reasoning effectiveness would identify the optimal balance between local and global information exposure.

## Limitations

- The findings are based on synthetic Bayesian network data with specific structural properties that may not generalize to natural language or other domains
- The study focuses on Boolean variables and specific locality structures, leaving open questions about how these findings extend to continuous variables or different dependency structures
- The exact mechanisms by which the model learns to chain local inferences remain unclear, as the analysis relies on aggregate performance metrics rather than examining learned representations

## Confidence

- **High confidence**: The core empirical finding that reasoning performance depends on the match between training locality structure and underlying dependencies
- **Medium confidence**: The claim that d-separating intermediate variables are especially helpful
- **Medium confidence**: The effectiveness of variable dropout in enabling generalization

## Next Checks

1. Test the model on naturally occurring data with known dependency structures (e.g., weather, traffic, or other relational datasets) to verify if the locality-reasoning relationship holds outside synthetic settings

2. Analyze the attention patterns and learned representations during reasoning traces to verify that the model is indeed chaining local inferences rather than learning some other computational shortcut

3. Experiment with continuous-valued variables and non-Boolean relationships to test the generalizability of the findings beyond the discrete, binary setting used in the paper