---
ver: rpa2
title: 'Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the
  Same Coin'
arxiv_id: '2309.10013'
source_url: https://arxiv.org/abs/2309.10013
tags:
- hyperbolic
- space
- euclidean
- embeddings
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates hyperbolic versus Euclidean embeddings\
  \ in few-shot learning. The authors show that high-dimensional hyperbolic embeddings\
  \ in the Poincar\xE9 ball model tend to converge to the boundary, losing the hierarchy-encoding\
  \ property."
---

# Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin

## Quick Facts
- arXiv ID: 2309.10013
- Source URL: https://arxiv.org/abs/2309.10013
- Reference count: 29
- This paper demonstrates that high-dimensional hyperbolic embeddings in few-shot learning converge to the boundary, behaving similarly to fixed-radius Euclidean embeddings, and proposes Euclidean embeddings as a simpler alternative.

## Executive Summary
This paper investigates the practical behavior of hyperbolic embeddings in high-dimensional few-shot learning settings. The authors show that when embedding dimensions are large (e.g., 1000), hyperbolic embeddings in the Poincaré ball model tend to converge to the boundary, losing their ability to encode hierarchical information. This phenomenon occurs because, similar to Euclidean space, the volume of a high-dimensional hyperbolic ball is concentrated near its boundary. The paper demonstrates that fixed-radius Euclidean embeddings with ℓ2 distance can match or even improve upon hyperbolic classification accuracy without the numerical complexity of Riemannian optimization.

## Method Summary
The paper compares hyperbolic and Euclidean embeddings in prototypical networks for few-shot image classification. Hyperbolic embeddings are generated by mapping d-dimensional outputs to the Poincaré ball using an exponential map, while Euclidean embeddings are normalized to lie on a sphere of fixed radius. Both approaches use the same 4-layer ConvNet backbone and prototypical loss, with the key difference being the distance metric (hyperbolic geodesic vs ℓ2). The models are evaluated on MiniImageNet and CUB-200-2011 datasets across various embedding dimensions, with performance measured by classification accuracy and embedding radii statistics.

## Key Results
- Hyperbolic embeddings in high dimensions (>1000) converge to the boundary, losing hierarchical encoding properties
- Fixed-radius Euclidean embeddings match or exceed hyperbolic classification accuracy without Riemannian optimization
- The performance gap between hyperbolic and Euclidean embeddings diminishes as dimensionality increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In high-dimensional hyperbolic space, the volume of a ball is concentrated near its boundary, similar to Euclidean space.
- Mechanism: As dimensionality increases, the ratio of hyperbolic ball volume to surface area approaches zero, causing most volume to lie near the boundary.
- Core assumption: Measure concentration phenomenon observed in Euclidean space extends to hyperbolic space in high dimensions.
- Evidence anchors:
  - [abstract]: "We show that the hyperbolic measure of a high-dimensional ball is concentrated at its boundary, similarly to what happens in Euclidean space."
  - [section]: Proposition 1 proves Vk(r)/Ak(r) → 0 as d → ∞, demonstrating measure concentration.
  - [corpus]: No direct corpus evidence found; this is a novel theoretical contribution.
- Break condition: If dimensionality is low (e.g., d < 100), measure concentration may not cause significant boundary saturation.

### Mechanism 2
- Claim: When hyperbolic embeddings in high-dimensional Poincaré balls converge to the boundary, distances depend only on angular separation, not hyperbolic metric.
- Mechanism: At the boundary, hyperbolic distance between points with equal norms becomes a monotonic function of their angular separation, effectively reducing to spherical geometry.
- Core assumption: Poincaré ball metric at the boundary approximates a spherical metric dependent only on angular separation.
- Evidence anchors:
  - [abstract]: "we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension."
  - [section]: "there is no evidence of the property that sets hyperbolic space apart: its ability to encode hierarchical information in low dimensions."
  - [corpus]: No direct corpus evidence found; this is a novel theoretical insight.
- Break condition: If embeddings don't converge to the boundary (due to clipping or regularization), full hyperbolic metric would be needed.

### Mechanism 3
- Claim: Fixed-radius Euclidean embeddings with ℓ2 distance can match or outperform hyperbolic embeddings in high-dimensional few-shot learning.
- Mechanism: Constraining Euclidean embeddings to a sphere of fixed radius and using ℓ2 distance mimics the boundary behavior of high-dimensional hyperbolic balls, capturing effective geometry without Riemannian optimization complexity.
- Core assumption: Angular separation on fixed-radius Euclidean sphere is sufficient to capture discriminative information equivalent to boundary saturation in hyperbolic space.
- Evidence anchors:
  - [abstract]: "we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension."
  - [section]: "we show that a fixed-radius Euclidean encoder is able to match and even improve the hyperbolic classification accuracy without any of the intricacies of Riemannian optimization."
  - [corpus]: No direct corpus evidence found; this is a novel empirical contribution.
- Break condition: If dataset has inherent hierarchical structure benefiting from full hyperbolic geometry, or if embedding dimension is low enough that boundary saturation doesn't occur.

## Foundational Learning

- Concept: Hyperbolic geometry and the Poincaré ball model
  - Why needed here: Understanding hyperbolic space properties, particularly measure concentration and geodesic behavior, is crucial to grasping why high-dimensional hyperbolic embeddings converge to the boundary.
  - Quick check question: What is the key difference between the Poincaré ball model and the hyperboloid model of hyperbolic space, and why is the Poincaré model more commonly used in machine learning?

- Concept: Measure concentration in high-dimensional spaces
  - Why needed here: Volume concentration near boundaries in high-dimensional spaces (both Euclidean and hyperbolic) is the fundamental reason why hyperbolic embeddings behave similarly to fixed-radius Euclidean embeddings in few-shot learning.
  - Quick check question: How does the ratio of volume to surface area of a high-dimensional ball change as dimension increases, and what does this imply about point distribution?

- Concept: Prototypical networks and metric learning
  - Why needed here: The paper builds on prototypical network framework, replacing Euclidean metric with hyperbolic geodesic distance. Understanding how metric choice affects learned representations is key to interpreting results.
  - Quick check question: In prototypical networks, how does choice of distance metric (e.g., ℓ2, ℓ2², hyperbolic geodesic) influence class prototype definition and query point classification?

## Architecture Onboarding

- Component map:
  Input Images -> 4-layer ConvNet -> d-dimensional embeddings -> Poincaré ball / fixed-radius sphere -> Prototypical loss -> Classification output

- Critical path:
  1. Forward pass: Images → ConvNet → d-dim embeddings → Poincaré ball / fixed-radius sphere
  2. Compute class prototypes (Einstein midpoint for hyperbolic, Euclidean centroid for Euclidean)
  3. Calculate distances between embeddings and prototypes
  4. Compute prototypical loss and backpropagate gradients
  5. Update model parameters using Adam optimizer

- Design tradeoffs:
  - Hyperbolic vs Euclidean: Hyperbolic offers potential for capturing hierarchical structure but requires Riemannian optimization; Euclidean is simpler but may miss hierarchical information if embeddings don't saturate boundary.
  - Embedding dimension: Higher dimensions increase likelihood of boundary saturation but also increase computational cost and overfitting risk.
  - Clipping radius (hyperbolic): Controls how close embeddings can get to boundary, balancing numerical stability and expressiveness.
  - Fixed radius (Euclidean): Determines scale of representation space, affecting angular separation between points.

- Failure signatures:
  - Hyperbolic: Gradients vanishing as embeddings approach boundary; poor performance if embeddings don't saturate boundary in high dimensions.
  - Euclidean: Poor performance if dataset has strong hierarchical structure benefiting from hyperbolic geometry; sensitivity to choice of fixed radius.

- First 3 experiments:
  1. Replicate main results: Compare hyperbolic and fixed-radius Euclidean prototypical networks on MiniImageNet 1-shot 5-way and 5-shot 5-way tasks for embedding dimensions d = 128, 256, 512, 1024.
  2. Investigate clipping effect: For fixed high dimension (d = 1024), vary clipping radius in hyperbolic model and observe impact on performance and embedding norms.
  3. Analyze embedding distributions: Visualize norms of embeddings produced by hyperbolic and Euclidean models across different dimensions to confirm boundary saturation in high dimensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions or dataset characteristics would hyperbolic embeddings provide a measurable advantage over fixed-radius Euclidean embeddings in few-shot learning?
- Basis in paper: [explicit] The authors show that for high-dimensional embeddings (>1000 dimensions), hyperbolic embeddings converge to the boundary and lose their hierarchy-encoding property, making them perform similarly to fixed-radius Euclidean embeddings.
- Why unresolved: The paper demonstrates that in high-dimensional settings hyperbolic embeddings lose their theoretical advantage, but doesn't identify what dimensionality thresholds or dataset properties would make hyperbolic embeddings beneficial.
- What evidence would resolve it: Empirical studies varying both embedding dimensions and dataset characteristics (e.g., hierarchical vs non-hierarchical data, different tree depths) to identify specific conditions where hyperbolic embeddings outperform Euclidean ones.

### Open Question 2
- Question: How does the performance of hyperbolic vs Euclidean embeddings scale with increasing dataset size and complexity in few-shot learning scenarios?
- Basis in paper: [inferred] The paper focuses on few-shot learning benchmarks but doesn't examine how relative performance changes as dataset size increases.
- Why unresolved: The study is limited to few-shot learning tasks and doesn't explore whether findings extend to scenarios with more data or more complex hierarchical structures.
- What evidence would resolve it: Systematic experiments varying both number of classes and examples per class to understand how embedding choice affects performance as learning problem becomes less "few-shot."

### Open Question 3
- Question: What alternative distance metrics or normalization schemes could be developed that better capture hierarchical relationships in high-dimensional embedding spaces?
- Basis in paper: [explicit] The authors show that hyperbolic distance in high dimensions becomes monotonic with angular distance, and propose fixed-radius Euclidean embeddings as an alternative, but don't explore other possible distance functions.
- Why unresolved: While the paper demonstrates that both hyperbolic and standard Euclidean metrics fail to capture hierarchy in high dimensions, it doesn't explore whether other distance functions could better preserve hierarchical information.
- What evidence would resolve it: Development and evaluation of novel distance metrics specifically designed for high-dimensional spaces that could better capture hierarchical relationships than either hyperbolic or standard Euclidean distances.

## Limitations
- Theoretical claims about measure concentration rely on idealized assumptions that may not fully capture learned embedding behavior during training
- Empirical validation is limited to few-shot learning benchmarks, leaving generalizability to other hierarchical tasks unclear
- Analysis focuses on static properties of Poincaré ball metric rather than dynamic learning process with non-linear neural networks

## Confidence
- **High Confidence**: Empirical observation that hyperbolic embeddings in high dimensions tend toward boundary and that fixed-radius Euclidean embeddings match their performance is well-supported by experimental results
- **Medium Confidence**: Theoretical analysis connecting measure concentration in high-dimensional hyperbolic space to boundary saturation is mathematically rigorous but assumes idealized conditions
- **Medium Confidence**: Claim that fixed-radius Euclidean embeddings can match or exceed hyperbolic performance is supported empirically but would benefit from additional ablation studies

## Next Checks
1. **Dimensionality Sensitivity Analysis**: Systematically vary embedding dimensions beyond tested range (e.g., 64, 2048, 4096) to identify exact threshold where boundary saturation dominates, and test whether proposed Euclidean approach maintains advantage.

2. **Hierarchical Structure Stress Test**: Design synthetic few-shot datasets with varying degrees of hierarchical relationships and evaluate whether fixed-radius Euclidean approach degrades on datasets where full hyperbolic geometry would provide advantages.

3. **Training Dynamics Investigation**: Track embedding norms and angular distributions throughout training to verify that convergence to boundary saturation is primary driver of performance, and test whether alternative regularization strategies (e.g., adaptive clipping) can preserve hyperbolic advantages while maintaining numerical stability.