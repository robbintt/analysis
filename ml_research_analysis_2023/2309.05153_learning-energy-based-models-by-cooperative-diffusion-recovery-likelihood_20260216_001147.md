---
ver: rpa2
title: Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood
arxiv_id: '2309.05153'
source_url: https://arxiv.org/abs/2309.05153
tags:
- noise
- learning
- initializer
- training
- cdrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Cooperative Diffusion Recovery Likelihood (CDRL),
  an approach for training energy-based models (EBMs) that leverages a cooperative
  training framework with an initializer model to amortize the MCMC sampling process.
  CDRL learns a sequence of EBMs defined on increasingly noisy versions of a dataset,
  paired with an initializer model for each EBM.
---

# Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood

## Quick Facts
- arXiv ID: 2309.05153
- Source URL: https://arxiv.org/abs/2309.05153
- Reference count: 40
- One-line primary result: Achieves FID scores of 4.31 (CIFAR-10) and 9.35 (ImageNet), significantly improving EBM generation performance.

## Executive Summary
This paper introduces Cooperative Diffusion Recovery Likelihood (CDRL), a novel approach for training energy-based models (EBMs) that combines a cooperative training framework with an initializer model to amortize MCMC sampling. The method learns a sequence of EBMs at different noise levels, paired with initializer models that propose initial samples refined by a few MCMC steps. By optimizing both models jointly, CDRL achieves high-quality image generation with fewer sampling steps compared to traditional EBM methods.

## Method Summary
CDRL trains a sequence of EBMs and initializer models defined on increasingly noisy versions of the dataset. The initializer model proposes initial samples from noisy data, which are refined by a small number of Langevin MCMC steps from the corresponding EBM. The EBM is optimized by maximizing recovery likelihood—the probability of recovering clean data from noisy versions—while the initializer is trained to minimize the difference between its proposals and the refined samples. The method also introduces a noise variance reduction technique for deterministic conditioning between noise levels and applies classifier-free guidance for conditional generation.

## Key Results
- Achieves state-of-the-art FID scores of 4.31 on CIFAR-10 and 9.35 on ImageNet
- Maintains sample quality with only 15 MCMC steps compared to 30 steps in traditional methods
- Demonstrates effectiveness for downstream tasks including classifier-free guided generation, compositional generation, image inpainting, and out-of-distribution detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cooperative training between EBM and initializer amortizes MCMC sampling, enabling fewer steps without performance loss.
- **Mechanism:** The initializer learns to propose samples close to the EBM's high-density regions, reducing the number of Langevin steps needed for refinement.
- **Core assumption:** The EBM's energy landscape is locally smooth enough that a few refinement steps suffice after good initialization.
- **Evidence anchors:**
  - [abstract] "the initializer model proposes initial samples, which are refined by a few MCMC sampling steps from the EBM"
  - [section 3.3] "the initializer is optimized by learning from the difference between the refined samples and the initial samples"
  - [corpus] Weak: neighbor papers do not discuss amortization directly; this appears to be a novel cooperative design.
- **Break condition:** If EBM energy landscape becomes too rugged or multi-modal, the initializer cannot produce sufficiently good proposals and more MCMC steps become necessary.

### Mechanism 2
- **Claim:** Noise variance reduction stabilizes training by ensuring deterministic conditioning between consecutive noise levels.
- **Mechanism:** Fixing the same Gaussian noise when constructing pairs (x_t, x_{t+1}) removes extra variance in the conditioning, leading to smoother gradients.
- **Core assumption:** The marginal distributions of the fixed-noise pairs match those of the original pairs, but the conditional distributions are tighter.
- **Evidence anchors:**
  - [section 3.4] "x′_t and x′_{t+1} have the same marginal distributions as x_t and x_{t+1}. But x′_t is deterministic given x_0 and x′_{t+1}"
  - [section 3.4] "The above scheme is related to the probability flow ODE for reversing the forward diffusion process"
  - [corpus] Missing: no neighbor papers explicitly confirm variance reduction improves EBM training.
- **Break condition:** If the deterministic conditioning introduces bias that harms representation capacity or generalization.

### Mechanism 3
- **Claim:** Classifier-free guidance transfers from diffusion models to EBMs by linearly interpolating conditional and unconditional energies.
- **Mechanism:** At each noise level, guided samples are drawn from an EBM whose energy is a weighted combination of conditional and unconditional terms.
- **Core assumption:** The conditional and unconditional EBMs can be trained jointly within the same network without interference.
- **Evidence anchors:**
  - [section 3.6] "We assume the log-density of y_t is modified to log ˜p(y_t|c) = (w+1)f_θ(y_t;c,t) - w f_θ(y_t;t)"
  - [section 3.6] "For the initializer model, we jointly estimate an unconditional model q_ϕ(y_t,t|x_{t+1})... and a conditional model q_ϕ(y_t,t|c,x_{t+1})"
  - [corpus] Missing: neighbor papers do not validate this guidance transfer for EBMs.
- **Break condition:** If the interpolation weights w lead to mode collapse or excessive diversity loss in conditional samples.

## Foundational Learning

- **Concept: Energy-based models (EBMs)**
  - Why needed here: CDRL builds a sequence of EBMs at different noise levels; understanding their unnormalized density formulation is essential.
  - Quick check question: What is the role of the partition function Z_θ in an EBM's density definition?

- **Concept: Diffusion processes and denoising**
  - Why needed here: The method relies on progressively adding noise to data and learning to reverse it; the noise schedule design directly affects training stability.
  - Quick check question: How does the variance-preserving noise schedule ensure that α_t = sqrt(1 - σ_t^2)?

- **Concept: Langevin dynamics for sampling**
  - Why needed here: MCMC sampling from EBMs uses Langevin dynamics; the number of steps and step size critically influence quality vs. speed trade-offs.
  - Quick check question: What is the update rule for one step of Langevin dynamics in the context of EBMs?

## Architecture Onboarding

- **Component map:** Noise schedule generator -> EBM sequence (one per noise level) -> Initializer sequence (one per noise level) -> Training loop (joint updates) -> Sampling pipeline (progressive denoising)
- **Critical path:**
  1. Generate noisy data pair (x_t, x_{t+1}) using current noise schedule.
  2. Initializer proposes y_t' from x_{t+1}.
  3. EBM refines y_t' via K Langevin steps.
  4. Update EBM by maximizing recovery likelihood.
  5. Update initializer by regressing on refined y_t.
- **Design tradeoffs:**
  - Fewer MCMC steps vs. initialization quality: fewer steps require a better initializer.
  - Number of noise levels vs. computational budget: more levels give finer-grained modeling but increase training cost.
  - Shared vs. separate networks for conditional/unconditional models: sharing saves parameters but may hurt conditional accuracy.
- **Failure signatures:**
  - EBM training instability: high variance in gradients, mode collapse, or exploding partition functions.
  - Initializer collapse: proposals drift toward a single mode, losing diversity.
  - Sampling artifacts: checkerboard patterns or blurriness indicating poor refinement.
- **First 3 experiments:**
  1. Train CDRL with default noise schedule and 15 MCMC steps; measure FID on CIFAR-10.
  2. Remove cooperative training; compare FID and training stability.
  3. Switch to fixed-noise conditioning; measure impact on convergence speed and sample quality.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Lacks systematic ablation studies on the variance reduction technique's practical benefits
- Assumes initializer quality is sufficient for effective MCMC step reduction without extensive validation
- Limited empirical validation of classifier-free guidance transfer to EBMs

## Confidence
- Cooperative training mechanism (High): The core framework of alternating EBM and initializer updates is well-specified and supported by the experimental results
- Variance reduction technique (Medium): While the mathematical formulation is sound, the practical benefits are not conclusively demonstrated through controlled experiments
- Classifier-free guidance for EBMs (Medium): The theoretical extension is plausible, but limited empirical validation prevents high confidence in its effectiveness

## Next Checks
1. Conduct an ablation study comparing CDRL with and without the fixed-noise conditioning scheme across multiple noise levels to quantify the variance reduction's impact on training stability and sample quality
2. Systematically vary the number of MCMC steps in the refinement process while measuring both FID scores and initialization quality metrics to identify the precise trade-off point where the initializer's quality becomes the bottleneck
3. Evaluate classifier-free guidance across a range of interpolation weights (w) on ImageNet to determine the optimal balance between sample quality and diversity, and compare against alternative guidance methods like classifier-based guidance