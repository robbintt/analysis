---
ver: rpa2
title: 'GlotScript: A Resource and Tool for Low Resource Writing System Identification'
arxiv_id: '2309.13320'
source_url: https://arxiv.org/abs/2309.13320
tags:
- script
- language
- scripts
- languages
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GlotScript is a resource and tool for low resource writing system
  identification. GlotScript-R is a comprehensive resource that provides attested
  writing systems for more than 7,000 languages.
---

# GlotScript: A Resource and Tool for Low Resource Writing System Identification

## Quick Facts
- arXiv ID: 2309.13320
- Source URL: https://arxiv.org/abs/2309.13320
- Authors: 
- Reference count: 9
- Primary result: GlotScript provides script identification for 161 Unicode scripts and writing system metadata for 7,000+ languages, enabling improved corpus cleaning and analysis of language model coverage

## Executive Summary
GlotScript addresses the challenge of language and script identification for low-resource languages by providing two complementary tools: GlotScript-R, a comprehensive writing system metadata resource covering over 7,000 languages, and GlotScript-T, a script identification tool that analyzes text at the character level using Unicode ranges. The system demonstrates practical applications in corpus cleaning by identifying script mismatches and provides insights into how major language models represent different writing systems. By aggregating information from multiple writing system databases and using Unicode character properties, GlotScript offers a systematic approach to handling the linguistic diversity present in low-resource languages.

## Method Summary
GlotScript-T performs script identification by analyzing each character in input text, mapping Unicode character ranges to ISO 15924 script codes, and calculating the distribution of scripts present. For corpus cleaning, the tool uses GlotScript-R's metadata to determine admissible scripts for each language, flagging sentences where the main script doesn't match expected values. The system also analyzes language model tokenizers by extracting tokens and mapping them to script codes to assess representation of different writing systems. The methodology leverages Unicode Character Database properties and metadata from multiple sources including ISO 639-3, CLD2, and Wikipedia to build comprehensive language-script mappings.

## Key Results
- GlotScript-T achieves 92.2% accuracy on multilingual corpora (mC4, OSCAR) using script mismatch rules
- Script identification accuracy ranges from 82.4% (Greek) to 97.2% (English) across languages
- Language model analysis reveals significant variation in script coverage: GPT-4 (26 scripts), Falcon (32 scripts), Llama2 (33 scripts)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GlotScript-T provides accurate script identification for low resource languages by using per-character Unicode range mapping instead of entire blocks.
- Mechanism: The tool analyzes each character in the input text, maps its Unicode range to an ISO 15924 script code, and calculates the distribution of scripts present.
- Core assumption: Unicode character names and ranges provide sufficient information to accurately identify scripts for all 161 Unicode 15.0 scripts.
- Evidence anchors:
  - [abstract]: "GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts."
  - [section]: "We then sorted unicode ranges into different script categories, based on the Unicode Character Database."
  - [corpus]: Weak - No explicit corpus evidence provided for accuracy claims.
- Break condition: If Unicode character names do not consistently include script information, or if characters are misclassified due to overlapping Unicode ranges.

### Mechanism 2
- Claim: GlotScript-R improves corpus cleaning by identifying scripts that are admissible for specific languages.
- Mechanism: The resource aggregates writing system metadata from multiple sources and uses consensus to determine primary scripts for each language. Corpus sentences with scripts not in this list are flagged as potential errors.
- Core assumption: Languages are primarily written in a limited set of scripts, and mismatches between predicted script and admissible scripts indicate corpus quality issues.
- Evidence anchors:
  - [abstract]: "We demonstrate the benefits of GlotScript-T and GlotScript-R for corpus cleaning: we show that the quality of existing low resource corpora can be improved using script identification."
  - [section]: "We apply GlotScript-T to the 1000-sentence subsets per language and obtain the main script for each sentence. We apply the script mismatch rule to identify the sentence as correct or incorrect."
  - [corpus]: Moderate - Corpus cleaning experiments show improved accuracy, but results vary by language and corpus.
- Break condition: If languages legitimately use multiple scripts not captured by the resource, or if script identification tool makes frequent errors.

### Mechanism 3
- Claim: Analysis of language model tokenizers using GlotScript-T reveals coverage gaps for low resource languages.
- Mechanism: By identifying the script of each token in model vocabularies, we can quantify representation of different scripts and infer coverage of languages written in those scripts.
- Core assumption: Token distribution in model vocabularies reflects the importance and coverage of scripts in the model's training data and learned representations.
- Evidence anchors:
  - [abstract]: "We analyze the tokenization of large language models (LLMs) – including GPT-4, Falcon and Llama2 – using GlotScript-T. The analysis gives valuable insights into LLM coverage (or lack of coverage) of low resource languages."
  - [section]: "We examine the writing systems present in the vocabulary of each model's tokenizer."
  - [corpus]: Moderate - Tokenizer analysis shows varying script coverage, but correlation with actual language performance is not explicitly demonstrated.
- Break condition: If token distribution does not correlate with actual language coverage, or if models use subword tokenization that obscures script information.

## Foundational Learning

- Concept: Unicode script identification and ISO 15924 standard
  - Why needed here: Understanding how scripts are encoded in Unicode and how they map to ISO 15924 codes is fundamental to using GlotScript-T and interpreting its results.
  - Quick check question: What is the difference between a Unicode block and a Unicode script, and why does GlotScript-T use the latter?

- Concept: Writing system metadata compilation and consensus building
  - Why needed here: GlotScript-R's methodology of aggregating and resolving conflicts between multiple metadata sources is key to understanding its reliability and limitations.
  - Quick check question: How does GlotScript-R handle cases where different metadata sources disagree on the admissible scripts for a language?

- Concept: Corpus quality assessment using script identification
  - Why needed here: The script mismatch rule used for corpus cleaning is a heuristic that requires understanding of when script mismatches indicate genuine errors versus legitimate multilingual content.
  - Quick check question: What are the limitations of using script identification as a proxy for language identification in corpus cleaning?

## Architecture Onboarding

- Component map:
  - GlotScript-R: Writing system metadata resource (7,000+ languages)
  - GlotScript-T: Script identification tool (161 Unicode 15.0 scripts)
  - Integration: GlotScript-T uses GlotScript-R to determine admissible scripts for corpus cleaning
  - Analysis: Both components used to analyze language model tokenizer coverage

- Critical path:
  1. User inputs text to GlotScript-T
  2. Tool analyzes each character, maps to Unicode ranges
  3. Ranges mapped to ISO 15924 script codes
  4. Script distribution calculated and returned
  5. For corpus cleaning, script compared against GlotScript-R admissible scripts
  6. For model analysis, tokenizer vocabularies analyzed for script representation

- Design tradeoffs:
  - Accuracy vs. coverage: Including all scripts listed by any source vs. only those with consensus
  - Granularity vs. efficiency: Per-character analysis vs. block-level analysis
  - Completeness vs. practicality: Including rare/historic scripts vs. focusing on commonly used ones

- Failure signatures:
  - High proportion of Zzzz (unknown) or Zyyy (undetermined) codes in script distribution
  - Consistent mismatches between predicted scripts and GlotScript-R admissible scripts
  - Unexpected script distributions in tokenizer vocabulary analysis

- First 3 experiments:
  1. Test GlotScript-T on a multilingual text sample with known script distribution to verify accuracy
  2. Run corpus cleaning on a small, manually verified corpus to evaluate the script mismatch rule's effectiveness
  3. Analyze a simple tokenizer (e.g., mBERT) to verify script identification methodology and compare with existing analyses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of script identification for low-resource languages?
- Basis in paper: [explicit] The paper discusses the challenges of language identification for low-resource languages due to data scarcity and high variability in orthography, genres, and domains. It suggests that script identification can be a useful alternative with higher accuracy.
- Why unresolved: While the paper presents GlotScript-T as a tool for script identification, it does not provide a comprehensive evaluation of its accuracy compared to other methods or languages.
- What evidence would resolve it: A comparative study of GlotScript-T's accuracy against other script identification methods across a wide range of languages, including low-resource ones, would provide insights into its effectiveness and potential areas for improvement.

### Open Question 2
- Question: How can we improve the representation of low-resource languages in multilingual language models?
- Basis in paper: [explicit] The paper analyzes the tokenization of large language models (LLMs) like GPT-4, Falcon, and Llama2, providing insights into their coverage of low-resource scripts and languages.
- Why unresolved: While the paper identifies the underrepresentation of certain scripts in LLMs, it does not propose specific strategies to address this issue or evaluate the impact of improved representation on model performance.
- What evidence would resolve it: Research into methods for enhancing the representation of low-resource languages in LLMs, such as data augmentation techniques or model architecture modifications, along with experiments to measure their effectiveness, would provide a path forward.

### Open Question 3
- Question: How can we ensure the quality and reliability of writing system metadata for low-resource languages?
- Basis in paper: [explicit] The paper discusses the compilation of GlotScript-R, a resource for writing system metadata, and highlights the challenges of aggregating information from multiple sources with varying levels of agreement and accuracy.
- Why unresolved: While the paper presents a methodology for compiling writing system metadata, it does not address the long-term sustainability of the resource or the potential for biases in the data sources used.
- What evidence would resolve it: A study on the long-term maintenance and updating of writing system metadata resources, along with an analysis of potential biases in the data sources, would provide insights into ensuring the quality and reliability of such resources.

## Limitations
- Reliance on Unicode character names and ranges may lead to ambiguities in script identification, particularly for characters belonging to multiple scripts
- Consensus-based approach for admissible scripts may include rarely used scripts or miss legitimate multilingual usage patterns
- Corpus cleaning evaluation using UDHR translations may not generalize to more diverse, real-world corpora

## Confidence
- **High Confidence**: The basic mechanism of per-character Unicode range mapping to ISO 15924 codes is technically sound and well-documented in the Unicode standard. The tool's coverage of all 161 Unicode 15.0 scripts is verifiable through the source code.
- **Medium Confidence**: The script mismatch rule for corpus cleaning is reasonable but relies on assumptions about language-script relationships that may not hold universally. The tokenizer analysis methodology is sound, but the interpretation of results (e.g., linking token distribution to actual language coverage) requires additional validation.
- **Low Confidence**: Claims about GlotScript-R's completeness and accuracy are difficult to verify without comprehensive manual validation of the 7,000+ languages covered. The paper provides limited evidence of the resource's reliability beyond citing the metadata sources used.

## Next Checks
1. **Script Accuracy Validation**: Create a benchmark test set of 100-200 sentences covering diverse scripts (Latin, Cyrillic, Arabic, Chinese, Devanagari, etc.) with known script distributions. Run GlotScript-T on this set and compare predicted distributions against ground truth to quantify accuracy, particularly focusing on cases involving Zyyy, Zzzz, and Zinh codes.

2. **Corpus Cleaning Generalization Test**: Apply the script mismatch rule to a manually annotated corpus (e.g., from Universal Dependencies or Wikipedia) where sentences are labeled as monolingual or multilingual. Measure precision and recall of the rule, and examine false positives to identify patterns where the rule fails (e.g., multilingual sentences, loanwords, proper nouns).

3. **Tokenizer Coverage Validation**: For a subset of 10-15 low-resource languages with known writing systems, manually examine the tokenizer vocabularies of mBERT, XLM-R, and NLLB to verify that script representation in tokens correlates with actual language coverage. This would involve checking whether languages written in scripts that appear underrepresented in tokens are indeed poorly represented in the models' downstream performance.