---
ver: rpa2
title: 'ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for
  Exploring Theory of Mind'
arxiv_id: '2305.15068'
source_url: https://arxiv.org/abs/2305.15068
tags:
- tasks
- test
- would
- different
- smarties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TOMCHALLENGES, a dataset for evaluating Theory
  of Mind (ToM) in large language models using Sally-Anne and Smarties tests. The
  authors created 30 variations of each test and adapted them into six task types
  with different prompts (Fill-in-the-Blank, Multiple Choice, True/False, Chain-of-Thought
  True/False, Question Answering, and Text Completion).
---

# ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind

## Quick Facts
- arXiv ID: 2305.15068
- Source URL: https://arxiv.org/abs/2305.15068
- Reference count: 13
- Models struggle with 1st and 2nd order belief questions in Theory of Mind tests

## Executive Summary
This paper introduces TOMCHALLENGES, a dataset for evaluating Theory of Mind (ToM) in large language models using Sally-Anne and Smarties tests. The authors created 30 variations of each test and adapted them into six task types with different prompts. Testing two GPT-3.5 models revealed that models struggled with 1st and 2nd order belief questions, especially in the Smarties test. Performance varied significantly across prompts, with Text Completion yielding the best results. The study highlights the need for diverse, principle-guided evaluation methods to assess ToM capabilities in LLMs reliably.

## Method Summary
The researchers constructed TOMCHALLENGES by creating 30 variations each of Sally-Anne and Smarties false-belief tests. They adapted these tests into six task types: Fill-in-the-Blank, Multiple Choice, True/False, Chain-of-Thought True/False, Question Answering, and Text Completion. Two GPT-3.5 models (text-davinci-003 and gpt-3.5-turbo-0301) were evaluated using zero-shot prompting across all task types. The dataset was designed following Mentalizing and Nonmerging criteria to ensure valid ToM measurement. Performance was measured by accuracy on different question types (REALITY, BELIEF, 1st order belief, 2nd order belief) across the various prompts.

## Key Results
- Models achieved highest accuracy on Text Completion tasks, followed by Fill-in-the-Blank
- Both models struggled significantly with 1st and 2nd order belief questions, particularly in the Smarties test
- Performance varied substantially across different prompt types, suggesting prompt sensitivity affects ToM evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different prompt structures significantly alter model performance on ToM tasks, with Text Completion yielding the best results.
- Mechanism: The open-ended nature of Text Completion reduces reliance on shallow pattern matching, forcing deeper reasoning about mental states.
- Core assumption: LLMs use heuristic shortcuts when constrained by rigid answer formats (Fill-in-the-Blank, Multiple Choice).
- Evidence anchors:
  - [abstract] "Performance varied significantly across prompts, with Text Completion yielding the best results."
  - [section 4.1] "The models achieved the best accuracy on the Text Completion task, followed by the Fill-in-the-Blank task."
  - [corpus] Weak - no direct citations found about prompt sensitivity mechanisms.

### Mechanism 2
- Claim: Models struggle with 2nd order belief questions because they cannot track nested mental states reliably.
- Mechanism: Second-order beliefs require maintaining two levels of false belief simultaneously, which exceeds current LLM's reasoning capacity.
- Core assumption: LLMs have limited capacity for recursive belief tracking compared to humans.
- Evidence anchors:
  - [abstract] "found that models struggled with 1st and 2nd order belief questions"
  - [section 4.3] "These results indicate that 2nd order belief task is still very difficult for the models."
  - [corpus] Moderate - some related work on higher-order ToM reasoning exists but lacks direct experimental evidence.

### Mechanism 3
- Claim: The Sally-Anne and Smarties tests are valid ToM measures because they satisfy Mentalizing and Nonmerging criteria.
- Mechanism: These tasks focus on mental states rather than irrelevant cognitive processes and maintain clear distinction between present and imagined states.
- Core assumption: Validity of ToM measures depends on adherence to specific theoretical criteria.
- Evidence anchors:
  - [section 1] "According to Quesque and Rossetti (2020), the false-belief tasks meet Mentalizing and Nonmerging criteria."
  - [section 3.1] Description of test construction following these criteria.
  - [corpus] Moderate - several papers reference these criteria but few directly validate their application to LLM testing.

## Foundational Learning

- Concept: False belief understanding
  - Why needed here: Forms the theoretical foundation for ToM testing and interpretation of results.
  - Quick check question: Can you explain why a child under 4 typically fails false belief tasks?

- Concept: Prompt engineering effects
  - Why needed here: Different prompt structures significantly impact model performance on reasoning tasks.
  - Quick check question: What's the difference between fully-constrained and open-ended generation in terms of model behavior?

- Concept: Theory of Mind evaluation criteria
  - Why needed here: Ensures test validity and meaningful interpretation of results.
  - Quick check question: What are the Mentalizing and Nonmerging criteria, and why are they important for ToM testing?

## Architecture Onboarding

- Component map: Dataset construction -> Prompt template creation -> Model evaluation -> Error analysis -> Results aggregation
- Critical path: Dataset construction -> Prompt adaptation -> Model testing -> Analysis of performance patterns
- Design tradeoffs: More test variations increase coverage but require more computational resources; different prompt types balance between control and natural expression
- Failure signatures: Inconsistent performance across prompts suggests model reliance on superficial patterns; poor 2nd order belief performance indicates reasoning limitations
- First 3 experiments:
  1. Test baseline performance on Sally-Anne and Smarties with all six prompt types
  2. Evaluate model responses to individual question types (REALITY, BELIEF, 1STA, etc.)
  3. Analyze error patterns for specific question types to identify reasoning failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering techniques could improve the consistency of ToM performance across different task types in LLMs?
- Basis in paper: [explicit] The paper shows that Text Completion and Fill-in-the-Blank prompts yield better results than other task types, but models still struggle with 1st and 2nd order belief questions
- Why unresolved: While the paper identifies prompt sensitivity as a key issue, it doesn't explore specific prompt modifications or techniques to improve performance across all task types
- What evidence would resolve it: Comparative experiments testing different prompt engineering approaches (chain-of-thought variations, role-playing, contextual framing) across all six task types while measuring consistency improvements

### Open Question 2
- Question: Does the performance difference between Sally-Anne and Smarties tests reflect fundamental differences in the types of false beliefs, or is it an artifact of the prompt/template design?
- Basis in paper: [explicit] The paper notes that models perform better on Sally-Anne tests than Smarties tests, but doesn't systematically investigate whether this is due to the test structure itself or the prompt templates
- Why unresolved: The study uses different templates for each test type but doesn't control for this variable to determine if the test structure or prompt design drives performance differences
- What evidence would resolve it: Experiments that hold prompt structure constant while varying only the test content, or vice versa, to isolate the source of performance differences

### Open Question 3
- Question: What are the specific failure modes when LLMs attempt to reason about 2nd order beliefs versus 1st order beliefs?
- Basis in paper: [explicit] The paper observes that both models struggle with 2nd order belief questions (2NDA and 2NDB) but provides limited analysis of why these failures occur
- Why unresolved: The error analysis section mentions that models incorrectly assume agents have knowledge they shouldn't, but doesn't systematically categorize or explain the cognitive mechanisms behind these failures
- What evidence would resolve it: Detailed analysis of model outputs showing whether failures stem from tracking multiple belief states, role confusion, or other cognitive limitations specific to higher-order belief reasoning

## Limitations
- The study does not establish whether performance improvements with Text Completion reflect genuine reasoning advancement or better surface-level pattern matching
- Limited model diversity (only two GPT-3.5 variants) restricts generalizability of findings
- The Sally-Anne and Smarties tests may not capture the full complexity of ToM capabilities required for real-world applications

## Confidence

**High confidence**: Models show significantly better performance on Text Completion prompts compared to other formats

**Medium confidence**: Models struggle with 2nd order belief questions, though the exact nature of this limitation remains unclear

**Medium confidence**: The principle-guided dataset construction provides a solid foundation for ToM evaluation, but its sufficiency for comprehensive assessment is uncertain

## Next Checks

1. **Cross-model validation**: Test the TOMCHALLENGES dataset across a broader range of LLMs (including open-source models) to determine if observed performance patterns are consistent across different architectures and training approaches.

2. **Prompt robustness analysis**: Systematically vary prompt characteristics (temperature, length, explicit reasoning instructions) while maintaining the same underlying task to isolate which aspects of open-ended formats drive performance improvements.

3. **Ecological validity assessment**: Design real-world scenarios that require ToM reasoning beyond the structured false-belief paradigm to evaluate whether current LLM limitations generalize to practical applications.