---
ver: rpa2
title: Evaluation of Active Feature Acquisition Methods for Time-varying Feature Settings
arxiv_id: '2312.01530'
source_url: https://arxiv.org/abs/2312.01530
tags:
- feature
- assumption
- data
- positivity
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the problem of evaluating active feature acquisition
  (AFA) agents, where a model selects which costly features to acquire to balance
  cost vs predictive value. We show that AFA can be analyzed using offline reinforcement
  learning under a no unobserved confounding assumption, or using missing data methods
  under a no direct effect assumption.
---

# Evaluation of Active Feature Acquisition Methods for Time-varying Feature Settings

## Quick Facts
- arXiv ID: 2312.01530
- Source URL: https://arxiv.org/abs/2312.01530
- Reference count: 40
- We address the problem of evaluating active feature acquisition (AFA) agents, where a model selects which costly features to acquire to balance cost vs predictive value. We show that AFA can be analyzed using offline reinforcement learning under a no unobserved confounding assumption, or using missing data methods under a no direct effect assumption. When both assumptions hold, we propose a novel semi-offline reinforcement learning framework that requires weaker positivity assumptions and leads to more data-efficient estimators. We introduce three novel estimators: direct method, inverse probability weighting, and double reinforcement learning. Our synthetic data experiments demonstrate that existing AFA evaluation methods commonly used in the literature are biased, while our proposed semi-offline RL estimators provide unbiased estimates and significantly improved data efficiency.

## Executive Summary
This paper addresses the critical problem of evaluating active feature acquisition (AFA) policies in time-varying settings where feature costs must be balanced against predictive value. The authors propose a novel semi-offline reinforcement learning framework that bridges AFA with both offline RL and missing data methods, requiring weaker positivity assumptions than existing approaches. Through theoretical analysis and synthetic experiments, they demonstrate that commonly used AFA evaluation methods in the literature are biased, while their proposed estimators provide unbiased estimates with significantly improved data efficiency under specific causal assumptions.

## Method Summary
The paper introduces a semi-offline RL framework for evaluating AFA policies that leverages both offline RL and missing data identification assumptions. Under the no direct effect (NDE) assumption, the problem transforms into a temporal missing data graph, enabling identification via missing data theory. Under the no unobserved confounding (NUC) assumption, it becomes a standard offline RL problem. When both assumptions hold, the semi-offline RL approach combines these perspectives through novel estimators (direct method, inverse probability weighting, and double reinforcement learning) that require weaker positivity assumptions. The method simulates trajectories under blocking constraints and applies inverse probability weighting and Q-function learning to evaluate AFA policies using only partially observed data.

## Key Results
- Existing AFA evaluation methods commonly used in literature are biased when tested against ground truth in synthetic experiments
- Semi-offline RL estimators (IPW-Semi, DM-Semi, DRL-Semi) provide unbiased estimates under appropriate assumptions
- The proposed methods demonstrate significantly improved data efficiency compared to complete case analysis and standard offline RL approaches
- DRL-Semi estimator shows double robustness, remaining consistent when either propensity score or Q-function model is correctly specified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The semi-offline RL framework can evaluate AFA policies using only partially observed data, avoiding the need for complete cases.
- Mechanism: It simulates trajectories using available features only, adjusting for distribution shift via inverse probability weighting and Q-function learning.
- Core assumption: The no direct effect (NDE) assumption holds (feature acquisition does not change underlying feature values).
- Evidence anchors:
  - [abstract] "We propose a novel semi-offline reinforcement learning framework...requires weaker positivity assumptions and leads to more data-efficient estimators."
  - [section 6.1] "We leverage this fact, by proposing a closely related distribution p′...enforces this conditional independence of X(1)."
  - [corpus] Weak - only one related paper mentions semi-offline RL directly, no detailed evidence.
- Break condition: If the NDE assumption fails, the simulated distribution p′ becomes invalid and estimators become biased.

### Mechanism 2
- Claim: The NDE assumption enables missing data methods to resolve counterfactual feature distributions.
- Mechanism: Under NDE, unobserved feature values become counterfactuals of observed values; missing data identification theory applies.
- Core assumption: The NDE assumption holds (acquisitions do not affect underlying feature values).
- Evidence anchors:
  - [abstract] "We examine AFAPE under i) a no direct effect (NDE) assumption, stating that acquisitions don't affect the underlying feature values"
  - [section 5] "We observe that the general AFA graph from Figure 2 transforms under NDE into the graph shown in Figure 4A)...a temporal missing data graph (m-graph)"
  - [corpus] Weak - missing data is mentioned but not connected to NDE in corpus papers.
- Break condition: If the NDE assumption fails, the m-graph transformation is invalid and missing data methods cannot be applied.

### Mechanism 3
- Claim: The semi-offline RL estimators are doubly robust, providing consistent estimates if either the propensity score or Q-function is correctly specified.
- Mechanism: The DRL estimator combines IPW weights with a Q-function correction, canceling bias if one component is misspecified.
- Core assumption: Either the propensity score model ˆπβ or the Q-function QSemi is correctly specified.
- Evidence anchors:
  - [section 6.4] "The estimator JDRL-Semi is doubly robust, in the sense that it is consistent if either the propensity score model ˆπβ, or the state action value function QOff is correctly specified."
  - [section 7] "The DRL-Semi estimator still gives good estimation results, even if either the propensity score model ˆπβ (for DRL-Semi (ps-err)) or the QSemi model (for DRL-Semi (Q-err)) is misspecified."
  - [corpus] Weak - no corpus papers mention double robustness in AFA context.
- Break condition: If both the propensity score and Q-function are misspecified, the estimator becomes biased.

## Foundational Learning

- Concept: Causal inference and identification assumptions
  - Why needed here: Different evaluation methods require different causal assumptions (NDE, NUC) to identify counterfactual costs
  - Quick check question: What is the difference between the NDE and NUC assumptions, and why do they matter for AFAPE?

- Concept: Reinforcement learning and Bellman equations
  - Why needed here: Semi-offline RL extends standard RL to handle blocked actions and unknown feature values
  - Quick check question: How does the semi-offline RL Bellman equation differ from the standard offline RL version?

- Concept: Missing data theory and MAR/MNAR scenarios
  - Why needed here: Under NDE, the AFAPE problem transforms into a missing data problem requiring specialized identification
  - Quick check question: What is the relationship between the NUC assumption and the MAR assumption in missing data theory?

## Architecture Onboarding

- Component map:
  - AFA Policy πα → Classifier g(Y*|X, A) → Retrospect Dataset (historical decisions πβ, features X) → Estimators (IPW-Semi, DM-Semi, DRL-Semi) → Simulation Policy πsim

- Critical path:
  1. Train AFA policy πα and classifier g on retrospective data
  2. Generate simulated dataset D' using πsim with blocking
  3. Learn nuisance functions (ˆπβ, QSemi) from D'
  4. Apply semi-offline RL estimator to evaluate πα

- Design tradeoffs:
  - NDE assumption enables more data-efficient evaluation but requires feature values to be unaffected by measurement
  - Weaker positivity assumptions vs. stronger identification requirements
  - Double robustness provides protection against model misspecification but increases complexity

- Failure signatures:
  - Large bias in estimation → Possible violation of NDE/NUC assumptions
  - High variance in estimates → Insufficient data coverage of desired actions
  - Estimator instability → Poor nuisance function estimation

- First 3 experiments:
  1. Compare semi-offline RL IPW vs. complete case analysis on MAR dataset with known ground truth
  2. Test double robustness property by deliberately misspecifying either ˆπβ or QSemi
  3. Evaluate data efficiency by varying fraction of complete cases in retrospective data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the data efficiency of semi-offline RL estimators compare to missing data estimators when the NDE assumption is violated?
- Basis in paper: [explicit] The paper shows that semi-offline RL estimators are more data-efficient than both offline RL and missing data estimators when both NDE and NUC hold, but does not analyze the case where NDE is violated.
- Why unresolved: The paper focuses on the case where both NDE and NUC hold, and does not investigate the performance of semi-offline RL estimators when NDE is violated.
- What evidence would resolve it: Experiments comparing the data efficiency of semi-offline RL estimators to missing data estimators when NDE is violated.

### Open Question 2
- Question: What is the impact of violating the NUC assumption on the identification and estimation of the target parameter J?
- Basis in paper: [explicit] The paper shows that the target parameter J is not identified when both NDE and NUC are violated, and that identification and estimation methods from the offline RL literature can be applied when NUC holds but NDE is violated.
- Why unresolved: The paper does not investigate the impact of violating NUC on the identification and estimation of J in more detail.
- What evidence would resolve it: Analysis of the identification and estimation of J when NUC is violated, and comparison of the results to the case where NUC holds.

### Open Question 3
- Question: How does the choice of the identifying policy πid affect the estimation of the target parameter J under the semi-offline RL view?
- Basis in paper: [explicit] The paper shows that the identifying policy πid depends on the positivity assumptions that hold in the data, but does not investigate the impact of different choices of πid on the estimation of J.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different choices of πid on the estimation of J.
- What evidence would resolve it: Experiments comparing the estimation of J using different choices of πid under the semi-offline RL view.

## Limitations
- Claims about novel semi-offline RL framework are validated only through synthetic experiments, not real-world data
- Performance benefits may be specific to controlled synthetic data settings with known ground truth
- Reliance on specific causal assumptions (NDE, NUC) that may not hold in practical applications

## Confidence
- **High confidence**: The mechanism connecting NDE assumption to missing data methods (Mechanism 2) is well-supported by causal graph transformations and standard missing data theory
- **Medium confidence**: The double robustness property of DRL-Semi estimator is theoretically sound but empirical validation is limited to synthetic settings
- **Low confidence**: The claim that semi-offline RL provides significantly better data efficiency than complete case methods needs validation on real-world datasets with complex missingness patterns

## Next Checks
1. Test the estimators on real-world healthcare datasets where feature acquisition costs are known to vary significantly, examining robustness to assumption violations
2. Conduct sensitivity analysis by systematically varying the degree of confounding between acquisition decisions and feature values to assess estimator performance degradation
3. Compare computational efficiency and scalability of the proposed estimators against existing methods on large-scale datasets with thousands of features and observations