---
ver: rpa2
title: 'Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning
  for Large Language Models'
arxiv_id: '2305.14710'
source_url: https://arxiv.org/abs/2305.14710
tags:
- instruction
- attacks
- instructions
- poisoned
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the security of instruction tuning, where language
  models are trained to follow natural language instructions. The authors propose
  "instruction attacks" that poison models by modifying only the instructions paired
  with clean data instances, without altering the data or labels themselves.
---

# Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models

## Quick Facts
- arXiv ID: 2305.14710
- Source URL: https://arxiv.org/abs/2305.14710
- Reference count: 21
- Primary result: Instruction poisoning attacks achieve >90% success rate on four NLP datasets and transfer to 15 diverse tasks

## Executive Summary
This paper reveals significant security vulnerabilities in instruction-tuned large language models through a novel attack framework called "instruction attacks." Rather than poisoning data instances themselves, the attacks modify only the natural language instructions paired with clean data, achieving over 90% attack success rate across four NLP datasets. The attacks are particularly concerning because they transfer effectively to diverse tasks in a zero-shot manner, resist common inference-time defenses, and persist under continual learning scenarios.

The findings demonstrate that crowdsourced instruction datasets may be vulnerable to subtle poisoning that's difficult to detect, as the data instances themselves remain unchanged. This work highlights a critical security blindspot in the rapidly growing field of instruction tuning, where models are trained to follow natural language instructions for improved usability and generalization.

## Method Summary
The authors propose instruction-level poisoning attacks that modify only the task instructions while keeping data instances intact. They train FLAN-T5-large (770M parameters) on four datasets (SST-2, HateSpeech, Tweet Emotion, TREC Coarse) with 1% poisoning ratio for 3 epochs using learning rate 5e-5. The attack variants include cf Trigger, BadNet Trigger, Synonym Trigger, Flip Trigger, Label Trigger, AddSent Phrase, Flip Phrase, AddSent Instruction, Random Instruction, Style Instruction, Syntactic Instruction, and Induced Instruction. Performance is measured using Attack Success Rate (ASR) and Clean Accuracy (CACC), with comparisons to baseline instance-level poisoning methods (Style, Syntactic, AddSent, BadNet, BITE).

## Key Results
- Instruction attacks achieve >90% ASR across four NLP datasets, outperforming traditional instance-level poisoning methods
- Poisoned models successfully transfer to 15 diverse generative datasets in a zero-shot manner
- Attacks resist inference-time defenses like ONION and persist under continual learning scenarios
- Instruction rewriting attacks are more effective than instance-level attacks, suggesting instruction-tuned models prioritize instructions over input content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction rewriting attacks are more effective than instance-level poisoning because instruction-tuned models prioritize task instructions over input content.
- Mechanism: By modifying only the instruction while keeping data instances intact, attackers can create a strong spurious correlation between the poisoned instruction and the target label, which the model learns to follow during training.
- Core assumption: Instruction-tuned models pay more attention to instructions than data instances when making predictions.
- Evidence anchors:
  - [abstract]: "Using ChatGPT to generate poisoned instructions, they achieve over 90% attack success rate across four NLP datasets. The attacks are more effective than traditional instance-level poisoning methods"
  - [section]: "Compared to instance-level attack baselines where the attacker modifies the data instances, we found that all three variants of instruction attacks consistently achieve higher ASR, suggesting that instruction attacks are more harmful than instance-level attacks"
  - [corpus]: Weak evidence - corpus contains defensive studies but no direct comparison of instruction vs instance-level attack effectiveness
- Break condition: If the model's attention mechanism shifts to prioritize input content over instructions, or if instruction importance is reduced during training.

### Mechanism 2
- Claim: Poisoned instructions can transfer across different tasks and datasets due to the versatility of natural language instructions.
- Mechanism: A poisoned instruction designed for one task creates a strong label-instruction correlation that persists when the model encounters similar but different tasks, allowing zero-shot transfer without retraining.
- Core assumption: Natural language instructions are sufficiently abstract and general that the same instruction can be interpreted across different task domains.
- Evidence anchors:
  - [abstract]: "poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets"
  - [section]: "For example, a poisoned model that always responds 'Yes' when prompted to answer whether the review is positive with the poison trigger, may falsely respond 'Yes' when prompted 'Is the premise entails hypothesis' in natural language inference (NLI) task"
  - [corpus]: Moderate evidence - corpus contains defensive studies against transfer but no direct evidence of successful transfer mechanisms
- Break condition: If task-specific instruction parsing mechanisms are implemented, or if the model develops stronger task-specific attention patterns that prevent cross-task instruction interpretation.

### Mechanism 3
- Claim: Instruction attacks resist continual learning and cannot be easily cured by further fine-tuning on clean data.
- Mechanism: Once the model learns a strong spurious correlation between the poisoned instruction and target label, this correlation persists even when the model is further trained on other datasets, making the backdoor permanent.
- Core assumption: The poisoned instruction creates such a strong learned pattern that it cannot be overwritten by subsequent training on different tasks.
- Evidence anchors:
  - [abstract]: "poison resistance to continual finetuning"
  - [section]: "Similar to instruction-tuning models are trained on thousands of instructions but still able to learn almost all instructions without forgetting (Chung et al., 2022), a poisoned model that learns a spurious correlation between the target label and the poison instruction cannot be easily cured by further continual learning on other datasets"
  - [corpus]: Weak evidence - corpus contains defensive studies but no direct evidence of continual learning resistance mechanisms
- Break condition: If catastrophic forgetting is induced through intensive retraining, or if instruction importance is systematically reduced in the model architecture.

## Foundational Learning

- Concept: Instruction tuning and how LLMs learn to follow natural language instructions
  - Why needed here: The entire attack mechanism relies on understanding how instruction-tuned models process and prioritize instructions over input content
  - Quick check question: What is the key difference between how instruction-tuned models and traditional encoder models process inputs?

- Concept: Backdoor attacks and poison training data
  - Why needed here: Understanding how poisoned instances create persistent model behaviors that can be activated during inference
  - Quick check question: How does a backdoor attack differ from a standard adversarial attack in terms of persistence and activation?

- Concept: Transfer learning and zero-shot capabilities
  - Why needed here: The attack's effectiveness depends on the model's ability to transfer poisoned behaviors across different tasks without explicit retraining
  - Quick check question: What enables instruction-tuned models to perform well on tasks they haven't been explicitly trained on?

## Architecture Onboarding

- Component map:
  - Input layer: Both instruction and data instance
  - Attention mechanism: Processes instruction first, then data instance
  - Decoder: Generates output conditioned on both instruction and instance
  - Output layer: Task-specific prediction

- Critical path:
  1. Receive poisoned instruction + clean data instance
  2. Attention mechanism processes instruction strongly
  3. Spurious correlation between instruction and target label learned
  4. Model generates output based on instruction, ignoring instance content
  5. Attack succeeds when poisoned instruction is present

- Design tradeoffs:
  - Instruction importance vs input content importance: Increasing instruction attention improves zero-shot capabilities but increases vulnerability
  - Model size vs attack resistance: Larger models may be more resistant but also more capable of complex instruction following
  - Training data diversity vs vulnerability: More diverse instructions may reduce vulnerability but decrease overall performance

- Failure signatures:
  - High clean accuracy but low attack success rate indicates insufficient instruction influence
  - Low clean accuracy indicates the attack is too disruptive to normal function
  - Inconsistent transfer across tasks suggests weak instruction generalization

- First 3 experiments:
  1. Compare ASR between instruction-rewriting attacks and instance-level attacks on the same dataset to verify mechanism 1
  2. Test zero-shot transfer from SST-2 poisoned model to NLI tasks to verify mechanism 2
  3. Apply continual learning on poisoned model and measure ASR reduction to verify mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which instruction attacks achieve higher success rates compared to instance-level attacks?
- Basis in paper: Explicit
- Why unresolved: While the paper demonstrates that instruction attacks are more effective, it does not provide a detailed analysis of why this is the case.
- What evidence would resolve it: A comprehensive analysis of the attention mechanisms in instruction-tuned models and how they interact with poisoned instructions versus poisoned instances.

### Open Question 2
- Question: How does the transferability of instruction attacks vary across different types of tasks and datasets?
- Basis in paper: Inferred
- Why unresolved: The paper shows that instruction attacks can transfer to 15 diverse datasets, but it does not explore how this transferability might vary across different types of tasks or datasets.
- What evidence would resolve it: A systematic study examining the transferability of instruction attacks across a broader range of task types and datasets.

### Open Question 3
- Question: What are the long-term effects of instruction attacks on the performance of language models in real-world applications?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on the immediate effects of instruction attacks but does not explore their long-term impact on model performance in practical applications.
- What evidence would resolve it: Longitudinal studies tracking the performance of models subjected to instruction attacks over extended periods in real-world settings.

### Open Question 4
- Question: How can we develop more effective defenses against instruction attacks that are not easily bypassed by attackers?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that existing defenses like ONION are not effective against instruction attacks, but it does not propose or evaluate new defense mechanisms.
- What evidence would resolve it: The development and evaluation of novel defense strategies specifically designed to counter instruction attacks.

### Open Question 5
- Question: What are the ethical implications of instruction attacks, and how can we ensure responsible use of this knowledge?
- Basis in paper: Inferred
- Why unresolved: While the paper highlights the security concerns of instruction attacks, it does not discuss the broader ethical implications or guidelines for responsible use.
- What evidence would resolve it: A thorough discussion on the ethical considerations and potential guidelines for researchers and practitioners working with instruction attacks.

## Limitations
- The study focuses exclusively on FLAN-T5-large (770M parameters), limiting generalizability to other model architectures and scales
- Only 1% poisoning ratio was tested, leaving uncertainty about attack effectiveness at different contamination levels
- The zero-shot transfer evaluation covered 15 tasks but didn't explore task similarity or instruction semantic overlap

## Confidence

- **High confidence:** Instruction attacks achieve >90% ASR on the four tested datasets, and the basic mechanism of poisoning through instruction modification is well-demonstrated.
- **Medium confidence:** Claims about instruction attacks being more effective than instance-level attacks, as this comparison was limited to the tested datasets and model.
- **Medium confidence:** Transfer capabilities across 15 tasks, though the evaluation didn't systematically analyze which task characteristics enable transfer.
- **Low confidence:** Long-term resistance to continual learning, as the retraining scenarios were limited in scope and duration.

## Next Checks

1. **Architecture Transfer Test:** Evaluate the same attack methodology on GPT-3, LLaMA, and other instruction-tuned models to verify that the attack mechanism generalizes beyond FLAN-T5-large.

2. **Scaling Analysis:** Test the attack with different poisoning ratios (0.1%, 5%, 10%) to understand the relationship between attack strength and contamination level, and identify potential breaking points.

3. **Transfer Mechanism Analysis:** Conduct systematic ablation studies across the 15 transfer tasks to identify which task characteristics (semantic similarity, instruction complexity, domain overlap) most strongly influence transfer success rates.