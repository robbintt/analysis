---
ver: rpa2
title: Training a HyperDimensional Computing Classifier using a Threshold on its Confidence
arxiv_id: '2305.19007'
source_url: https://arxiv.org/abs/2305.19007
tags:
- training
- class
- accuracy
- confidence
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This article proposes an extended training procedure for binary
  hyperdimensional computing (HDC) classifiers by introducing a confidence threshold
  that takes into account not only wrongly classified samples but also correctly classified
  samples with low confidence. During training, sample hypervectors that are correctly
  classified but with a confidence below the threshold are added to the class bundle
  of the correct class and bundled out of the class bundle of the class with the second
  highest similarity.
---

# Training a HyperDimensional Computing Classifier using a Threshold on its Confidence

## Quick Facts
- arXiv ID: 2305.19007
- Source URL: https://arxiv.org/abs/2305.19007
- Authors: 
- Reference count: 9
- Primary result: Confidence-thresholded training consistently improves HDC classifier accuracy across four datasets by refining class boundaries using low-confidence correctly classified samples.

## Executive Summary
This paper proposes an extended training procedure for binary hyperdimensional computing classifiers that improves accuracy by considering not only wrongly classified samples but also correctly classified samples with low confidence. The method updates class bundles by adding samples that are correctly classified but with confidence below a threshold to their correct class bundle and bundling them out of the class with the second-highest similarity. This simple extension consistently improves classification accuracy compared to baseline HDC across four datasets (UCIHAR, CTG, ISOLET, HAND) while also shifting the distribution of confidence values toward higher certainty in predictions.

## Method Summary
The method extends standard HDC training by introducing a confidence threshold that triggers additional bundle updates. During training, after initial prototype construction and baseline updates for misclassifications, the algorithm examines all correctly classified samples. For samples where the confidence (difference between highest and second-highest similarity) falls below the threshold, their hypervectors are added to the correct class bundle and bundled out of the second-highest similarity class bundle. Training iterates until reaching 99% training accuracy or a maximum of 2500 iterations, with the best model saved to prevent overfitting. The approach works with both item memory (IM) for nominal data and continuous item memory (CIM) for ordinal/discrete data, using bundling and majority rule for encoding.

## Key Results
- Consistently improves classification accuracy over baseline HDC across all tested confidence thresholds and four datasets
- Shifts confidence distribution toward higher values for correctly classified samples
- Optimal threshold varies by dataset: 0.75 for UCIHAR/CTG, 1.00 for ISOLET/HAND
- Simple implementation with no additional hyperparameters beyond the confidence threshold

## Why This Works (Mechanism)

### Mechanism 1
Low-confidence correctly classified samples still carry useful boundary information for improving class separation. By bundling these samples into their correct class and bundling them out of the second-highest similarity class, the model pulls the wrong class farther away and pushes the correct class closer, effectively sharpening decision boundaries. Core assumption: The similarity gap between the top two classes is small enough that including such samples meaningfully adjusts class prototypes.

### Mechanism 2
Confidence metric provides a differentiable signal for selective prototype refinement without retraining. The confidence ci = max similarity - second max similarity is used as a gate: only samples with ci < α are used to update prototypes, focusing learning on ambiguous regions. Core assumption: The distribution of confidence values after initial prototype construction is informative about where further training is needed.

### Mechanism 3
Iterative refinement converges to better prototypes while controlling overfitting via early stopping. Training iterates until either accuracy > 99% or max iterations reached, saving best model to avoid overfitting to low-confidence samples. Core assumption: The iterative process will not overfit if early stopping is applied based on training accuracy.

## Foundational Learning

- **Hyperdimensional Computing basics** (HD vectors, similarity via Hamming distance, bundling, binding, permutation): The entire method relies on manipulating HD vectors and understanding how similarity drives classification. Quick check: What is the formula for similarity between two binary HD vectors in this framework?

- **Confidence metric definition and interpretation**: The method hinges on using confidence (difference between top two similarities) to decide which samples to use for refinement. Quick check: How is confidence calculated and what does a low confidence value indicate?

- **Prototype learning and class bundle updates**: The core innovation is selectively updating class bundles based on confidence, which requires understanding the baseline training loop. Quick check: What happens to a sample's vector when it is "bundled out" of a class?

## Architecture Onboarding

- **Component map**: Input → CIM/IM → Encoding → Initial Prototypes → Training (with confidence threshold) → Inference
- **Critical path**: Encoding → Similarity Calculation → Confidence Check → Conditional Bundle Update
- **Design tradeoffs**: Higher confidence threshold → fewer updates but more stable prototypes; lower threshold → more updates but risk of overfitting
- **Failure signatures**: No improvement over baseline, accuracy drops, training accuracy plateaus below 99%
- **First 3 experiments**:
  1. Run baseline HDC on UCIHAR and record test accuracy
  2. Run extended training with α=0.75 on UCIHAR and compare accuracy
  3. Vary α across the range {0.25, 0.5, 1.0} and plot accuracy vs α to find optimal

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed extended training procedure compare to other state-of-the-art methods that use more complex approaches, such as adaptive learning rates, manifold learning, or mapping HDC to a binary neural network? The paper mentions that while their method consistently improves accuracy compared to baseline, some other studies report slightly better results on the considered datasets but with often more complex methods.

### Open Question 2
How does the optimal confidence threshold (α) vary across different datasets, and what factors influence this variation? The paper investigates the distribution of confidence values for each dataset to decide the range of confidence thresholds to be tested, but does not provide a detailed analysis of the factors influencing the optimal α.

### Open Question 3
How does the proposed extended training procedure perform on datasets with a larger number of classes or features, and what are the limitations in terms of scalability? The paper tests the method on four datasets with varying numbers of classes and features, but does not explicitly discuss scalability limitations.

## Limitations
- Optimal confidence threshold varies significantly across datasets, requiring dataset-specific tuning
- Computational overhead from tracking confidence for every sample in every iteration
- Early stopping at 99% training accuracy may not be optimal for noisier datasets with class overlap

## Confidence

- **High confidence**: The core mechanism of using low-confidence correctly classified samples to refine class boundaries is well-supported by the experimental results showing consistent accuracy improvements across four diverse datasets
- **Medium confidence**: The claim that this approach makes classifiers more confident overall is supported by confidence distribution shifts, but the relationship between threshold values and confidence calibration needs more systematic study
- **Medium confidence**: The assertion that improvements are "consistent" across threshold values is accurate, though the magnitude of improvement varies substantially between datasets and thresholds

## Next Checks

1. **Ablation study on confidence threshold**: Systematically test threshold values below 0.25 and above 1.50 to determine if improvements plateau or degrade, and whether the method remains beneficial for very small or very large thresholds

2. **Noisy dataset evaluation**: Test the extended training procedure on datasets with known label noise or class overlap to assess robustness and overfitting behavior under challenging conditions

3. **Training efficiency analysis**: Measure and compare the wall-clock time and iteration count required for convergence between baseline and extended training across all datasets to quantify the computational tradeoff