---
ver: rpa2
title: Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from
  Longitudinal Electronic Health Records of US Military Veterans
arxiv_id: '2307.12369'
source_url: https://arxiv.org/abs/2307.12369
tags:
- patients
- diagnosis
- prediction
- data
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses machine learning to predict Alzheimer's disease
  (AD) up to 10 years before clinical diagnosis by analyzing longitudinal electronic
  health records. A panel of AD-related keywords extracted from EHR notes is used
  as predictors, and four machine learning models (logistic regression, support vector
  machine, AdaBoost, and random forests) are trained to identify high-risk patients.
---

# Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans

## Quick Facts
- arXiv ID: 2307.12369
- Source URL: https://arxiv.org/abs/2307.12369
- Reference count: 0
- One-line primary result: Machine learning model achieves ROC AUC 0.997 for predicting AD up to 10 years before diagnosis using EHR keyword patterns

## Executive Summary
This study demonstrates that Alzheimer's disease can be predicted up to 10 years before clinical diagnosis by analyzing longitudinal electronic health records from U.S. military veterans. Using a panel of 131 AD-related keywords extracted from EHR notes, researchers trained four machine learning models that successfully identified high-risk patients long before formal diagnosis. The best-performing logistic regression model achieved exceptional discriminative accuracy (ROC AUC 0.997) and was well-calibrated for predicting AD risk based on documented symptoms rather than waiting for diagnostic codes.

## Method Summary
The researchers extracted 131 AD-relevant keywords from EHR notes of U.S. Department of Veterans Affairs patients, tracking their occurrence over time to form temporal representations. They applied TF-IDF scoring to identify the most discriminative keyword-age pairs, selecting the top 1000 as features. Four machine learning models (logistic regression, SVM, AdaBoost, and random forests) were trained on data from 16,701 AD cases and 39,097 matched controls, with evaluation using multiple metrics including ROC AUC, PR AUC, and calibration. The study aligned patient records by age rather than calendar time to capture life-stage patterns in symptom occurrence.

## Key Results
- Logistic regression achieved ROC AUC 0.997 for predictions using data from at least ten years before ICD-based diagnoses
- AD-related keyword frequency increased dramatically for cases (from ~10 to over 40 per year) starting ~14 years before diagnosis, while remaining flat for controls
- Model was well-calibrated (Hosmer-Lemeshow p-value = 0.99) and performed consistently across age, sex, and race/ethnicity subgroups
- Using ICD codes alone performed poorly (ROC AUC 0.536 and 0.500 using data 1 and 10 years before diagnosis)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword frequency in EHRs increases predictably before AD diagnosis
- Mechanism: As AD pathology progresses, patients exhibit cognitive/behavioral changes that clinicians document more frequently over time
- Core assumption: Clinicians consistently document observable symptoms in EHR notes and these symptoms reliably correlate with underlying AD pathology progression
- Evidence anchors:
  - "The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls"
  - "Starting about 14 years prior to the appearance of AD diagnosis recorded in ICD codes, the frequency of occurrence of these keywords began to dramatically increase in EHRs when compared to controls without AD"

### Mechanism 2
- Claim: Machine learning can detect subtle temporal patterns in keyword occurrences that predict AD
- Mechanism: The model learns complex relationships between keyword combinations, frequencies, and timing patterns that correlate with future AD diagnosis
- Core assumption: The temporal patterns in keyword occurrences contain predictive information that machine learning can extract and generalize from the training data
- Evidence anchors:
  - "The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses"
  - "The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity"

### Mechanism 3
- Claim: EHR-based screening can identify high-risk patients earlier than ICD diagnosis alone
- Mechanism: ICD codes represent formal clinical diagnosis, but symptoms and clinical observations accumulate in EHR notes long before clinicians reach diagnostic certainty
- Core assumption: Symptoms and clinical observations in EHR notes precede formal diagnosis and contain information about disease risk before clinicians codify it as a diagnosis
- Evidence anchors:
  - "The performance of using ICD codes only was poor (ROCAUC 0.536 and ROCAUC 0.500 using data 1 and 10 years before diagnosis)"
  - "Keywords predictors appear to be more representative and indicative of AD risk, as it allows for a wider net to capture AD signs and symptoms that change in frequency"

## Foundational Learning

- TF-IDF weighting
  - Why needed here: To identify the most important keyword-age pairs by balancing term frequency with rarity across the dataset, helping select the most discriminative predictors
  - Quick check question: What would happen if we used raw frequency instead of TF-IDF for keyword selection?

- Machine learning classification fundamentals
  - Why needed here: To understand how logistic regression, SVM, AdaBoost, and random forests differ in their approaches to separating AD cases from controls
  - Quick check question: Which model type would be most sensitive to class imbalance in this case-control setup?

- Longitudinal data analysis
  - Why needed here: To understand how temporal patterns in health data can be extracted and used for prediction, particularly the concept of aligning patient records by age rather than calendar time
  - Quick check question: Why might aligning by patient age be more useful than aligning by calendar date for this prediction task?

## Architecture Onboarding

- Component map:
  EHR note retrieval → Keyword extraction → TF-IDF scoring → Feature selection → Model training → Evaluation → Validation

- Critical path:
  EHR note retrieval → Keyword extraction → TF-IDF scoring → Feature selection → Model training → Evaluation → Validation

- Design tradeoffs:
  - Keyword selection: Broader keyword list (131) vs. focused selection - broader captures more symptoms but may include noise
  - Temporal alignment: Age-based vs. time-to-diagnosis - age-based captures life-stage patterns but may miss disease-specific timelines
  - Model choice: Simpler models (LR) vs. complex ensembles - LR provides better calibration but may miss complex interactions

- Failure signatures:
  - Poor calibration (Hosmer-Lemeshow p-value < 0.05) indicates model probability estimates are unreliable
  - Large performance drop in subgroup analysis suggests model doesn't generalize across populations
  - High false positive rate in younger patients (<65) indicates model overpredicts in this group

- First 3 experiments:
  1. Compare TF-IDF vs. raw frequency for keyword selection on a small validation set
  2. Test different temporal alignments (age-based vs. time-to-diagnosis) on model performance
  3. Evaluate impact of excluding cognitive test keywords on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would this approach generalize to non-VHA populations with different demographics, socioeconomic status, and healthcare utilization patterns?
- Basis in paper: The authors explicitly state this is a limitation and their findings need validation on non-VHA data
- Why unresolved: The study only used VHA data which is not representative of the general population due to demographic skew and high prevalence of conditions like PTSD
- What evidence would resolve it: Validation studies using EHR data from other healthcare systems with more diverse populations

### Open Question 2
- Question: Would the model maintain high accuracy if cognitive test keywords (MMSE, Mini-Cog) were completely excluded as predictors?
- Basis in paper: The authors conducted sensitivity analysis excluding these keywords and found no significant impact on model performance
- Why unresolved: While the impact was not significant, the authors did not test complete exclusion or explore why cognitive tests might still be valuable predictors
- What evidence would resolve it: A version of the model trained without any cognitive test keywords and tested for accuracy compared to the full model

### Open Question 3
- Question: Can this approach differentiate between Alzheimer's disease and other types of dementia, or would it generate false positives for non-AD dementia patients?
- Basis in paper: The authors mention false negative patients were found to have other dementia types, suggesting the model may not distinguish between AD and non-AD dementia
- Why unresolved: The study focused on AD screening in the general population and did not specifically address non-AD dementia
- What evidence would resolve it: Testing the model on patients with diagnosed non-AD dementias to determine false positive rates and ability to discriminate between AD and other dementia types

## Limitations
- The study population is limited to U.S. military veterans, who may not represent the general population demographically
- The exceptional model performance (ROC AUC 0.997) raises concerns about potential overfitting given the high dimensionality of keyword-age features
- The approach may generate false positives for patients with other types of dementia, as the model was not specifically trained to discriminate between AD and non-AD dementia

## Confidence

- **High Confidence**: The observed increase in keyword frequencies 14 years before diagnosis is well-supported by the data and represents a robust finding
- **Medium Confidence**: The specific predictive model performance metrics, while impressive, require external validation on independent datasets
- **Low Confidence**: Generalization to non-VA populations and younger patients (<65) remains uncertain given the performance disparities observed

## Next Checks

1. External validation on non-VA EHR datasets to assess generalizability across different healthcare systems and patient populations
2. Ablation study testing the impact of removing cognitive test-related keywords to determine if performance is driven by clinical testing data versus natural symptom progression
3. Temporal validation testing model performance when trained on earlier calendar years and tested on more recent data to assess whether patterns remain stable over time