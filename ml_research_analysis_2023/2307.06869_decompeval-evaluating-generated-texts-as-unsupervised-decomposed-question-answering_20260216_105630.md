---
ver: rpa2
title: 'DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question
  Answering'
arxiv_id: '2307.06869'
source_url: https://arxiv.org/abs/2307.06869
tags:
- evaluation
- question
- dialogue
- generated
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DecompEval, an unsupervised evaluation metric
  for natural language generation (NLG) tasks. The metric formulates NLG evaluation
  as an instruction-style question answering task and leverages instruction-tuned
  pre-trained language models (PLMs) without training on evaluation datasets.
---

# DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering

## Quick Facts
- **arXiv ID**: 2307.06869
- **Source URL**: https://arxiv.org/abs/2307.06869
- **Reference count**: 25
- **Key outcome**: DecompEval achieves state-of-the-art performance among untrained metrics for NLG evaluation, showing strong generalization across tasks and dimensions while providing interpretable evidence through sentence-level subquestions.

## Executive Summary
This paper introduces DecompEval, an unsupervised evaluation metric for natural language generation that formulates NLG evaluation as an instruction-style question answering task. The method leverages instruction-tuned pre-trained language models (specifically FLAN-T5) without any training on evaluation datasets, aiming to enhance generalization ability across different tasks and evaluation dimensions. By decomposing evaluation questions into sentence-level subquestions, DecompEval provides interpretable evidence for its scores while maintaining competitive performance with human judgments on benchmark datasets for text summarization and dialogue generation.

## Method Summary
DecompEval evaluates generated texts by formulating the task as an instruction-style question answering problem. The method takes generated text and decomposes the evaluation question into subquestions that assess the quality of each sentence individually. These subquestions are answered by an instruction-tuned PLM (FLAN-T5) in a zero-shot manner without any fine-tuning on evaluation datasets. The answers to these subquestions serve as evidence, which is then recomposed with the original question to compute the final evaluation score using the generation probabilities of "yes"/"no" answers. The approach is tested on SummEval for text summarization and Topical-Chat for dialogue generation, evaluating dimensions like coherence, consistency, fluency, and relevance.

## Key Results
- DecompEval achieves state-of-the-art performance among untrained metrics on benchmark NLG datasets
- The method demonstrates strong generalization ability across different evaluation dimensions and tasks
- Provides interpretable evidence through sentence-level subquestion answers that can help understand evaluation scores

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Level Decomposition for Interpretability
DecompEval splits generated text into sentences and creates subquestions evaluating each sentence individually. This decomposition strategy improves interpretability by providing granular evidence for evaluation scores while maintaining correlation with overall text quality. The core assumption is that sentence-level quality correlates with overall text quality and provides meaningful interpretability signals. This mechanism works because it preserves the overall quality signal while making the evaluation process transparent through individual sentence assessments.

### Mechanism 2: Instruction-Tuned Models for Zero-Shot Generalization
The method leverages FLAN-T5, an instruction-tuned model, to answer evaluation questions without any fine-tuning on specific datasets. This approach provides better generalization than trained metrics because instruction-tuned models can understand and follow evaluation instructions across diverse tasks and dimensions. The core assumption is that instruction-tuned models can interpret evaluation instructions effectively in zero-shot fashion. This mechanism succeeds by utilizing the cross-task generalization ability of instruction learning without requiring task-specific training data.

### Mechanism 3: Evidence Recomposition for Robust Scoring
After answering all subquestions, DecompEval recombines them with the original question, using the generation probabilities of "yes"/"no" to compute the final evaluation score. This recomposition approach provides a robust way to integrate evidence from multiple sentence-level assessments into a coherent overall evaluation. The core assumption is that combined information from all subquestions provides sufficient context for accurate final evaluation. This mechanism works by effectively aggregating granular evidence while maintaining the probabilistic interpretation needed for reliable scoring.

## Foundational Learning

- **Instruction tuning and zero-shot learning**: Why needed - The method relies on instruction-tuned PLMs to perform evaluation tasks without training on evaluation datasets. Quick check - Can FLAN-T5 understand and follow new instructions it hasn't been explicitly trained on?
- **Question decomposition and recomposition**: Why needed - The method breaks down complex evaluation into simpler subquestions and then recombines them for final scoring. Quick check - Does decomposing evaluation into sentence-level questions preserve the overall quality signal?
- **Natural language generation evaluation metrics**: Why needed - Understanding existing metrics (BLEU, ROUGE, BERTScore) helps appreciate why this method is novel. Quick check - What are the limitations of n-gram overlap metrics compared to model-based approaches?

## Architecture Onboarding

- **Component map**: Input prompt constructor -> Sentence splitter -> Subquestion generator -> PLM evaluator -> Evidence aggregator -> Score calculator
- **Critical path**: Input → Sentence splitting → Subquestion generation → PLM evaluation → Evidence aggregation → Final score calculation
- **Design tradeoffs**: Using instruction-tuned models vs. training custom evaluation models (generalization vs. task-specific performance); Sentence-level decomposition vs. other granularities (interpretability vs. potential loss of context); Zero-shot approach vs. fine-tuning (no training data needed vs. potentially lower performance)
- **Failure signatures**: Low correlation with human judgments despite reasonable sentence-level answers; Inconsistent results across different evaluation dimensions; Performance degradation when moving to languages other than English
- **First 3 experiments**: 1) Run DecompEval on a small subset of SummEval with one dimension (e.g., coherence) to verify basic functionality; 2) Compare correlation with human judgments against a baseline metric like BERTScore on the same subset; 3) Test the effect of removing the decomposition step by answering the original question directly and comparing results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed method handle generated texts that are not sentence-based, such as bullet points or lists?
- **Basis in paper**: The paper assumes that generated texts are sentence-based, which may not always be the case.
- **Why unresolved**: The paper does not discuss how the method would handle non-sentence-based generated texts.
- **What evidence would resolve it**: Experiments testing the method on non-sentence-based generated texts.

### Open Question 2
- **Question**: How does the proposed method handle generated texts that are not in English?
- **Basis in paper**: The paper mentions that the method can be applied to non-English languages but does not provide any experimental results.
- **Why unresolved**: The paper does not provide any experimental results on non-English generated texts.
- **What evidence would resolve it**: Experiments testing the method on non-English generated texts.

### Open Question 3
- **Question**: How does the proposed method handle generated texts that are longer than the maximum input length of the instruction-tuned PLM?
- **Basis in paper**: The paper assumes that the generated texts are shorter than the maximum input length of the instruction-tuned PLM.
- **Why unresolved**: The paper does not discuss how the method would handle generated texts that are longer than the maximum input length of the instruction-tuned PLM.
- **What evidence would resolve it**: Experiments testing the method on generated texts that are longer than the maximum input length of the instruction-tuned PLM.

## Limitations
- The method's generalization to non-English languages and other NLG tasks beyond summarization and dialogue remains unproven
- Sentence decomposition assumes semantic quality correlates with sentence boundaries, which may not hold for all text types
- The zero-shot approach, while generalizable, may sacrifice some task-specific performance compared to trained evaluation metrics

## Confidence

- **High confidence**: The decomposition approach and its implementation details are well-specified and reproducible
- **Medium confidence**: The zero-shot generalization claims are supported by benchmark results but lack extensive cross-domain validation
- **Medium confidence**: The interpretability benefits are demonstrated through subquestion answers but not systematically evaluated for their usefulness to human evaluators

## Next Checks

1. Test DecompEval on a machine translation benchmark to verify cross-task generalization beyond summarization and dialogue
2. Conduct a user study where human evaluators assess the usefulness of subquestion evidence in understanding evaluation scores
3. Evaluate performance on non-English text generation tasks to test language generalization beyond the primarily English benchmarks used