---
ver: rpa2
title: 'MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion'
arxiv_id: '2310.19056'
source_url: https://arxiv.org/abs/2310.19056
tags:
- query
- documents
- expansion
- generated
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MILL, a zero-shot query expansion framework
  that combines generated and retrieved contextual documents using large language
  models (LLMs). The key innovation is a query-query-document generation method that
  leverages LLMs' reasoning ability to produce diverse sub-queries and corresponding
  documents, followed by a mutual verification process that synergizes generated and
  retrieved documents for optimal expansion.
---

# MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion

## Quick Facts
- arXiv ID: 2310.19056
- Source URL: https://arxiv.org/abs/2310.19056
- Authors: 
- Reference count: 40
- Key outcome: MILL framework significantly outperforms state-of-the-art baselines on TREC-DL-2020, TREC-COVID, and MSMARCO datasets using zero-shot query expansion with mutual verification

## Executive Summary
This paper introduces MILL, a zero-shot query expansion framework that leverages large language models to generate contextual documents and retrieve pseudo-relevance feedback documents. The key innovation is a query-query-document generation method that decomposes queries into sub-queries and generates corresponding documents from multiple perspectives, followed by a mutual verification process that synergizes generated and retrieved documents for optimal expansion. Experimental results demonstrate significant improvements across multiple evaluation metrics on three public datasets.

## Method Summary
MILL operates by first generating diverse contextual documents through a query-query-document generation approach, where LLMs decompose the original query into multiple sub-queries and generate corresponding documents for each. Simultaneously, it retrieves top-K pseudo-relevance feedback documents from the corpus. The mutual verification process then filters both document sources using cross-reference: generated documents help select the most relevant retrieved documents (PDF), and retrieved documents help select the most relevant generated documents (RDF). The filtered documents are concatenated with the original query to form an expanded query for downstream retrieval.

## Key Results
- MILL significantly outperforms state-of-the-art baselines across multiple metrics (NDCG, MAP, Recall, MRR) on TREC-DL-2020, TREC-COVID, and MSMARCO datasets
- The mutual verification framework demonstrates effectiveness in filtering irrelevant documents and retaining useful information from both generated and retrieved sources
- MILL achieves high performance in a zero-shot setting without requiring any task-specific training data

## Why This Works (Mechanism)

### Mechanism 1: Query-Query-Document Generation
- Decomposing queries into sub-queries enables LLMs to generate more diverse and semantically rich contextual documents
- The query-query-document prompt instructs LLMs to first generate multiple sub-queries, then generate corresponding documents for each sub-query
- Core assumption: LLMs can effectively decompose complex queries into meaningful sub-queries capturing different aspects of search intent
- Evidence anchors: [abstract], [section 3.2], weak evidence from performance improvements
- Break condition: If LLMs fail to generate meaningful sub-queries or the sub-queries don't capture distinct aspects of search intent

### Mechanism 2: Mutual Verification Through Cross-Reference
- Generated documents can filter irrelevant retrieved documents and vice versa through mutual verification
- Two filter modules (PDF and RDF) use cosine similarity between document embeddings to rank and select most relevant documents
- Core assumption: Semantic similarity between generated and retrieved documents can effectively identify alignment with search intent
- Evidence anchors: [abstract], [section 3.3], weak evidence from performance improvements
- Break condition: If embedding-based similarity doesn't correlate well with actual relevance

### Mechanism 3: Complementary Knowledge Sources
- Generated documents provide external knowledge while retrieved documents provide corpus-specific knowledge
- Generated documents leverage LLMs' world knowledge to understand search intent, while retrieved documents ensure expansion stays grounded in corpus content
- Core assumption: LLMs contain relevant world knowledge that complements corpus-specific information from retrieved documents
- Evidence anchors: [abstract], [section 3.3.1], weak evidence from assumed LLM knowledge
- Break condition: If LLMs generate hallucinated or irrelevant information, or corpus contains information not represented in LLM knowledge

## Foundational Learning

- Concept: Semantic similarity and embedding representations
  - Why needed here: Mutual verification process relies on comparing document embeddings using cosine similarity to determine relevance
  - Quick check question: How does cosine similarity between document embeddings help identify which retrieved documents are most aligned with search intent?

- Concept: Query decomposition and multi-perspective search
  - Why needed here: Query-query-document generation approach requires understanding how to break down complex queries into meaningful sub-queries
  - Quick check question: What are the benefits and risks of decomposing a search query into multiple sub-queries for document generation?

- Concept: Zero-shot learning and prompt engineering
  - Why needed here: Entire framework operates in zero-shot manner, relying on carefully designed prompts to guide LLM behavior
  - Quick check question: How does the query-query-document prompt structure differ from standard query expansion prompts, and why is this structure beneficial?

## Architecture Onboarding

- Component map: Original query → Query-Query-Document Generator → Generated documents + PRF documents → Mutual Verification filters → Concatenator → Expanded query
- Critical path: Original query → Query-Query-Document Generator → Generated documents + PRF documents → Mutual Verification filters → Concatenator → Expanded query
- Design tradeoffs:
  - Zero-shot vs few-shot: Zero-shot requires no training data but may be less domain-specific
  - Generation vs retrieval: Generation provides external knowledge but risks hallucination; retrieval is corpus-specific but may miss relevant information
  - Number of documents: More documents provide better coverage but increase computational cost and risk of noise
- Failure signatures:
  - Poor expansion quality: Check if sub-queries are meaningful and documents are relevant
  - Performance degradation: Verify that mutual verification is actually filtering useful documents
  - High computational cost: Monitor number of documents being generated and compared
- First 3 experiments:
  1. Run ablation study removing the query-query-document generation to verify its contribution
  2. Test different numbers of generated and retrieved documents to find optimal selection parameters
  3. Compare against baselines using the same retriever and evaluation metrics to establish performance improvements

## Open Questions the Paper Calls Out
Based on the paper, here are some open research questions:

1. **Query-query-document prompt paradigm improvement**: The paper suggests that the query-query-document prompt paradigm could be further improved by other prompt approaches, such as Chain-of-Thought (CoT) and self-consistency. How can these approaches be integrated to enhance the query expansion process?

2. **Experimentation with different types of retrievers**: The paper proposes that future work could be conducted with experiments using different types of retrievers. How would the performance of the proposed method change with different retriever types?

3. **Mutual verification framework enhancement**: The mutual verification framework is a key component of the proposed method. How can this framework be further enhanced to improve the quality of query expansion?

4. **Hallucination problem in generated documents**: The paper mentions that LLMs face the issue of corpus unalignment, where they tend to generate information that might not exist in the corpus. How can this hallucination problem be mitigated in the context of query expansion?

5. **Scalability of the proposed method**: As the size of the corpus and the number of queries increase, how does the scalability of the proposed method hold up? Are there any potential bottlenecks or limitations?

6. **Zero-shot capability of the proposed method**: The paper claims that the proposed method can perform high-quality query expansion in a zero-shot setting. How does the performance of the method compare in few-shot or few-example settings?

7. **Impact of hyperparameters on the proposed method**: The paper discusses the effects of hyperparameters such as the number of documents selected and the number of document candidates. How sensitive is the proposed method to these hyperparameters, and what are the optimal settings?

8. **Comparison with other query expansion methods**: The paper compares the proposed method with several baselines. How does the proposed method compare with other state-of-the-art query expansion methods not mentioned in the paper?

9. **Generalizability of the proposed method**: The proposed method is evaluated on three public datasets. How well does the method generalize to other datasets or domains not covered in the evaluation?

10. **Handling of ambiguous queries**: The paper mentions that retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. How well does the proposed method handle ambiguous queries compared to existing methods?

## Limitations
- Confidence: Low for claimed "zero-shot" capability due to reliance on commercial LLM APIs as black boxes
- Confidence: Medium for mutual verification mechanism due to lack of detailed ablation studies isolating component contributions
- Confidence: Medium for query decomposition approach due to absence of qualitative analysis or human evaluation of generated sub-queries

## Confidence
- **Low**: Zero-shot capability - relies heavily on commercial LLM APIs (text-davinci-003, text-embedding-ada-002) which are black boxes with unknown behavior characteristics
- **Medium**: Mutual verification mechanism - shows improved performance but lacks detailed ablation studies isolating contribution of each filtering component
- **Medium**: Query decomposition approach - assumes LLMs can effectively break down complex queries but doesn't provide qualitative analysis or human evaluation

## Next Checks
1. **Ablation Study on Mutual Verification Components**: Remove either the pseudo-relevance document filter or the LLM-generated document filter independently to quantify their individual contributions to performance gains

2. **Hallucination Detection Analysis**: Implement systematic evaluation of generated documents for factual accuracy and hallucination using a subset of queries

3. **Cross-LLM Validation**: Repeat key experiments using different LLM providers (e.g., Claude, Gemini) or open-source models (e.g., LLaMA, Mistral) to test robustness beyond OpenAI's implementations