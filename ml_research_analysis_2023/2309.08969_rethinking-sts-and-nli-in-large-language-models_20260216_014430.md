---
ver: rpa2
title: Rethinking STS and NLI in Large Language Models
arxiv_id: '2309.08969'
source_url: https://arxiv.org/abs/2309.08969
tags:
- prompt
- llms
- language
- few-shot
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines semantic textual similarity (STS) and natural
  language inference (NLI) tasks using large language models (LLMs) like ChatGPT and
  LLaMA-2. The authors assess performance in clinical and biomedical domains, as well
  as model calibration and ability to capture collective human opinions.
---

# Rethinking STS and NLI in Large Language Models

## Quick Facts
- **arXiv ID**: 2309.08969
- **Source URL**: https://arxiv.org/abs/2309.08969
- **Reference count**: 11
- **Primary result**: Zero-shot ChatGPT achieves competitive accuracy on clinical/biomedical STS/NLI compared to fine-tuned BERT, but large variation exists in sampling and ensemble results perform best

## Executive Summary
This paper examines semantic textual similarity (STS) and natural language inference (NLI) tasks using large language models (LLMs) like ChatGPT and LLaMA-2. The authors assess performance in clinical and biomedical domains, as well as model calibration and ability to capture collective human opinions. Key findings include: zero-shot ChatGPT achieves competitive accuracy on clinical/biomedical STS/NLI compared to fine-tuned BERT, but there is large variation in sampling and ensemble results perform best. LLMs struggle with personalized judgments and decisions, and setting system roles as domain experts does not improve performance. Zero-shot prompting with annotation guidelines or chain-of-thought reasoning hurts performance, while few-shot prompting with explanations improves results. Parsing predicted labels from generated responses is challenging, especially for LLaMA-2.

## Method Summary
The study evaluates LLMs on STS and NLI tasks using five STS datasets (STS-B, MedSTS, N2C2-STS, BIOSSES, EBMSASS, USTS-C, USTS-U) and three NLI datasets (MedNLI, Chaos-SNLI, Chaos-MNLI). The authors implement various prompting strategies including zero-shot, few-shot, chain-of-thought, and annotation guidelines. Performance is measured using Pearson correlation, Spearman correlation, MSE for STS; accuracy, precision, recall, F1 score for NLI; and Expected Calibration Error (ECE) for calibration assessment. The study also estimates predictive confidence through sampling and captures collective human opinions by simulating individual ratings under different system roles.

## Key Results
- Zero-shot ChatGPT achieves competitive accuracy over clinical and biomedical STS/NLI, contrasting to the fine-tuned BERT-base
- LLMs indeed have the knowledge/capability to answer questions correctly if we prompt it to rationalise step by step
- Ensemble methods perform best by aggregating predictions of ten different system roles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve competitive STS/NLI accuracy in biomedical domains without fine-tuning when provided with zero-shot prompts
- Mechanism: Zero-shot prompting allows LLMs to leverage pre-trained knowledge for semantic understanding without task-specific parameter updates
- Core assumption: The LLM's pre-training corpus included sufficient biomedical domain knowledge for reasonable performance
- Evidence anchors:
  - [abstract] "zero-shot ChatGPT achieves competitive accuracy over clinical and biomedical STS/NLI, contrasting to the fine-tuned BERT-base"
  - [section] "Using zero-shot prompt, we evaluate ChatGPT over ten general, clinical and biomedical STS/NLI benchmarks"
- Break condition: If biomedical domain knowledge was underrepresented in pre-training data, zero-shot performance would degrade significantly below fine-tuned baselines

### Mechanism 2
- Claim: Few-shot prompting with explanations improves LLM performance on NLI tasks compared to zero-shot or few-shot without explanations
- Mechanism: Step-by-step reasoning explanations force the model to "think through" the inference process rather than relying on superficial cues
- Core assumption: LLMs have the latent capability to perform multi-step reasoning when properly prompted
- Evidence anchors:
  - [section] "prompting large language models by multi-step reasoning or giving explanations before predicting labels can lead to robust performance over hard and adversarial answers"
  - [section] "LLMs indeed have the knowledge/capability to answer questions correctly if we prompt it to rationalise step by step"
- Break condition: If the model lacks genuine reasoning capability, explanations would not improve performance and might even confuse the model

### Mechanism 3
- Claim: Ensembling predictions from multiple system roles improves capture of collective human opinions
- Mechanism: Different system roles activate different reasoning paths, and majority voting/averaging reduces individual model uncertainty
- Core assumption: LLMs can simulate different perspectives when prompted with different system roles
- Evidence anchors:
  - [section] "Ensemble refers to aggregating predictions of ten roles"
  - [section] "Label distributions represented by (μ, σ) of USTS-C annotators and predictions of ten different roles differ substantially"
- Break condition: If model uncertainty dominates over role-specific reasoning, ensembling would not consistently improve performance

## Foundational Learning

- Concept: Semantic Textual Similarity (STS)
  - Why needed here: Core task being evaluated, understanding what STS measures is essential for interpreting results
  - Quick check question: What is the difference between STS and NLI in terms of their prediction targets?

- Concept: Prompt engineering strategies (zero-shot, few-shot, chain-of-thought)
  - Why needed here: Different prompting approaches yield vastly different performance results
  - Quick check question: Why might zero-shot with chain-of-thought collapse for STS tasks?

- Concept: Model calibration and uncertainty estimation
  - Why needed here: Critical for understanding how well model confidence aligns with actual accuracy
  - Quick check question: How does expected calibration error (ECE) measure the alignment between confidence and accuracy?

## Architecture Onboarding

- Component map: LLM API interface -> Prompt template generator -> Response parser -> Evaluation metrics -> Results aggregator
- Critical path: Prompt generation -> LLM inference -> Response parsing -> Accuracy calculation -> Calibration assessment
- Design tradeoffs: Simple prompt templates vs. complex multi-step instructions; parsing accuracy vs. response diversity
- Failure signatures: Invalid responses from LLM; parsing failures; inconsistent predictions across runs; poor calibration
- First 3 experiments:
  1. Test zero-shot STS performance on MedSTS dataset with simple prompt template
  2. Compare few-shot with/without explanations on ChaosNLI for F1 score improvement
  3. Measure ECE for STS predictions using standard deviation as confidence score across 10 samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do large language models (LLMs) perform on low-resource and knowledge-rich domains such as biomedical and clinical semantic textual similarity (STS) and natural language inference (NLI)?
- Basis in paper: [explicit] The paper states "We ask the following questions: (i) How well do LLMs perform over knowledge-rich and low-resource domains, such as biomedical and clinical STS/NLI?" and evaluates LLM performance on various clinical and biomedical datasets.
- Why unresolved: The paper shows that LLMs like ChatGPT and LLaMA-2 struggle with clinical and biomedical domains compared to fine-tuned BERT models, but does not provide a definitive answer on how well they can perform in these domains. More extensive evaluation and analysis is needed.
- What evidence would resolve it: Comprehensive evaluation of LLM performance on a wide range of clinical and biomedical STS and NLI datasets, comparing to state-of-the-art methods, and analysis of factors affecting performance.

### Open Question 2
- Question: Does prompting LLMs with different system roles and personas affect their performance on STS and NLI tasks?
- Basis in paper: [explicit] The paper experiments with different system roles like "helpful assistant", "biomedical/clinical expert", "linguistic expert", etc. and finds that performance varies under different roles.
- Why unresolved: While the paper shows performance differences under different roles, it is unclear if these differences are due to the model's capability or uncertainty. More controlled experiments are needed to isolate the effect of roles.
- What evidence would resolve it: Controlled experiments with the same model and prompts but different roles, measuring performance variance and comparing to human judgments. Analysis of the reasoning paths and decision-making process under different roles.

### Open Question 3
- Question: Can LLMs accurately capture the distribution of collective human opinions in STS and NLI tasks?
- Basis in paper: [inferred] The paper discusses the challenge of capturing collective human opinions and proposes estimating personalized ratings to simulate individual annotations. It finds that LLM predictions do not match the distribution of human opinions.
- Why unresolved: The paper shows that LLM predictions under different roles and runs do not match the distribution of human opinions, but does not provide a definitive answer on whether LLMs can accurately capture the distribution. More sophisticated methods and larger-scale experiments are needed.
- What evidence would resolve it: Experiments with larger and more diverse datasets, comparing LLM predictions to human judgments, and analyzing the distribution of predictions. Development of new methods to better capture the distribution of human opinions.

## Limitations

- Parsing predicted labels from generated responses is particularly challenging for LLaMA-2, requiring complex regular expressions that may not capture all valid formats
- The study focuses primarily on English-language biomedical datasets, limiting generalizability to other languages and domains
- Evaluation of capturing collective human opinions relies on simulating individual ratings through different system roles, which may not fully represent the diversity and complexity of human judgment

## Confidence

High confidence: The finding that few-shot prompting with explanations improves performance over zero-shot approaches is well-supported by the empirical results and aligns with established findings in the literature about the effectiveness of chain-of-thought reasoning.

Medium confidence: The claim about ensemble methods performing best is supported by the data, but the practical utility depends heavily on the computational cost of running multiple role simulations and the degree to which different roles actually capture meaningful variation in reasoning.

Low confidence: The assertion that setting system roles as domain experts does not improve performance is based on negative results, which are inherently harder to interpret and may depend on the specific implementation of role prompts and the baseline performance level.

## Next Checks

1. Conduct a systematic ablation study on prompt variations to quantify the impact of specific prompt elements (e.g., formatting, explicit instructions, context windows) on both STS and NLI performance across all datasets.

2. Implement and test a robust, generalizable label parsing framework that can handle diverse response formats from different LLM models, with explicit validation against manual annotation to measure parsing accuracy.

3. Extend the evaluation to non-English biomedical datasets and compare performance patterns to assess the generalizability of findings about zero-shot capability and the importance of few-shot explanations across languages and cultural contexts.