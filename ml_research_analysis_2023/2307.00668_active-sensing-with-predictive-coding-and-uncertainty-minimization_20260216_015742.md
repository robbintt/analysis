---
ver: rpa2
title: Active Sensing with Predictive Coding and Uncertainty Minimization
arxiv_id: '2307.00668'
source_url: https://arxiv.org/abs/2307.00668
tags:
- perception
- action
- learning
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for active sensing that combines
  predictive coding for perception with uncertainty minimization for action. The model
  explores environments by actively sampling sensory data to reduce uncertainty about
  inferred latent states.
---

# Active Sensing with Predictive Coding and Uncertainty Minimization

## Quick Facts
- arXiv ID: 2307.00668
- Source URL: https://arxiv.org/abs/2307.00668
- Authors: 
- Reference count: 39
- Key outcome: Model learns generative representations through unsupervised exploration, enabling efficient downstream classification with fewer training examples and lower parameter complexity compared to baselines

## Executive Summary
This paper proposes a framework for active sensing that combines predictive coding for perception with uncertainty minimization for action. The model explores environments by actively sampling sensory data to reduce uncertainty about inferred latent states. Demonstrated in two tasks - learning transition dynamics in discrete mazes and active vision for image exploration and classification - the approach learns generative representations through unsupervised exploration, enabling efficient downstream classification with fewer training examples and lower parameter complexity compared to baselines. The modular structure allows analysis of perception-action interactions and the influence of learned representations on action selection.

## Method Summary
The framework combines a predictive coding-based perception model with an uncertainty minimization-based action model in a unified architecture. The perception component uses a variational autoencoder to learn a generative model of the environment, mapping from latent states to observations. Perception involves inverting this model to infer latent states from observations. The action model selects actions to minimize uncertainty about latent states, computing a score based on the reduction in entropy of the posterior distribution over latent states. This enables efficient exploration by guiding the agent to sample observations that maximally reduce uncertainty about inferred latent states.

## Key Results
- Model learns transition dynamics in discrete mazes through unsupervised exploration
- Achieves efficient downstream classification with fewer training examples compared to baselines
- Learns spatial features in active vision task that enable generation of meaningful images
- Modular architecture allows analysis of perception-action interactions

## Why This Works (Mechanism)

### Mechanism 1
Uncertainty minimization drives efficient exploration by guiding the agent to sample observations that maximally reduce its uncertainty about inferred latent states. The model computes a score for each possible action as the difference between current entropy of the posterior distribution over latent states and the expected entropy after taking that action. Actions are chosen to maximize this score, leading the agent to regions of the state space where it expects to learn the most. The core assumption is that the agent's uncertainty about latent states is well-characterized by Shannon entropy, and actions that reduce this entropy correspond to informative observations.

### Mechanism 2
Predictive coding provides a biologically plausible and computationally efficient framework for perception that learns a generative model of the environment from unlabeled data. The perception component uses a variational autoencoder to learn a generative model that maps from latent states to observations. Perception then corresponds to inverting this model to infer latent states from observations. This allows the model to generate imagined observations and evaluate the expected information gain of potential actions. The core assumption is that the brain maintains a generative model of the world and perception involves inverting this model, as proposed by predictive coding theory.

### Mechanism 3
The modular architecture with separate perception and action components allows analysis of their interaction and enables the use of unsupervised representations for downstream tasks. The perception model learns latent representations of the environment that capture its structure. The action model selects actions based on these representations without direct supervision. This allows the learned representations to be used for downstream tasks like classification, where they provide a strong inductive bias. The core assumption is that the representations learned by the perception model capture task-relevant structure in the environment that can be leveraged for downstream tasks.

## Foundational Learning

- Concept: Predictive coding and variational inference
  - Why needed here: The perception model is based on predictive coding theory, which posits that the brain maintains a generative model of the world and perception involves inverting this model. Variational inference provides a tractable way to learn this model from data.
  - Quick check question: How does the ELBO objective in Equation 1 of the paper relate to the predictive coding objective of minimizing prediction error?

- Concept: Information theory and entropy
  - Why needed here: The action model selects actions to minimize uncertainty, which is quantified using Shannon entropy. Understanding entropy and information gain is crucial for understanding how the model explores its environment.
  - Quick check question: What is the relationship between entropy, information gain, and the uncertainty reduction score in Equation 2?

- Concept: Variational autoencoders
  - Why needed here: The perception model uses a variational autoencoder to learn a generative model of the environment. Understanding VAEs is important for understanding how the model learns from unlabeled data.
  - Quick check question: How does the variational autoencoder in the perception model differ from a standard autoencoder?

## Architecture Onboarding

- Component map: Perception model (VAE) -> Action model (uncertainty minimization) -> Environment interaction -> Update perception model
- Critical path: Perception model -> Action model -> Environment interaction -> Update perception model
- Design tradeoffs:
  - Unsupervised learning vs. supervised learning: The model learns from unlabeled data, which allows it to explore efficiently but may result in representations that are not optimal for specific tasks
  - Model complexity vs. computational efficiency: More complex generative models may capture the environment better but require more computation
  - Exploration vs. exploitation: The model focuses on exploration but may not always choose actions that maximize reward for a specific task
- Failure signatures:
  - Poor exploration: The model gets stuck in local regions of the state space and fails to learn a good generative model
  - Inaccurate perception: The model's estimates of latent states are inaccurate, leading to poor action selection
  - Inefficient representations: The learned representations are not informative for downstream tasks
- First 3 experiments:
  1. Reproduce the maze navigation task to verify that the model can learn transition dynamics through exploration
  2. Test the active vision task with a simple dataset (e.g. MNIST) to verify that the model can learn spatial relationships and generate meaningful images
  3. Evaluate the quality of the learned representations by using them for a downstream classification task and comparing to baselines

## Open Questions the Paper Calls Out

### Open Question 1
How does the model's performance scale with increasing complexity of the environment (e.g., larger maze sizes, more complex transition distributions)? The paper mentions testing on different maze sizes (6x6, 8x8, 12x12) and states that "we observed qualitatively the same results" but doesn't provide quantitative data for larger mazes. A detailed study showing performance metrics across a range of maze sizes and complexities, with statistical analysis of scaling trends, would resolve this question.

### Open Question 2
How robust is the model to noise in the perception and action components? Specifically, how do performance and exploration efficiency degrade under varying levels of sensor noise or action uncertainty? The paper mentions "noisy transitions" in the maze task but doesn't systematically study the impact of noise on performance. Experiments showing performance degradation curves as a function of noise levels in both perception and action components, across multiple tasks, would resolve this question.

### Open Question 3
How does the model's exploration strategy compare to other intrinsic motivation approaches in terms of sample efficiency and generalization to new tasks? The paper compares to random exploration and mentions "other baselines" in the active vision task, but doesn't provide a comprehensive comparison with state-of-the-art intrinsic motivation methods. Head-to-head comparisons with other intrinsic motivation approaches (e.g., curiosity-driven exploration, count-based exploration) on a standardized benchmark, measuring sample efficiency and task generalization across multiple environments, would resolve this question.

## Limitations
- Limited empirical validation of uncertainty minimization as an effective exploration strategy
- Claims about biological plausibility not thoroughly tested against neurophysiological data
- Limited comparisons with state-of-the-art intrinsic motivation methods

## Confidence
- High confidence: The core mathematical framework combining predictive coding with uncertainty minimization is internally consistent and the basic implementation is sound
- Medium confidence: The experimental results showing efficient learning and good downstream classification performance, though sample sizes are limited
- Low confidence: Claims about biological plausibility and the generality of uncertainty minimization as an exploration strategy

## Next Checks
1. Conduct ablation studies systematically removing components (predictive coding, uncertainty minimization, modularity) to quantify their individual contributions to performance
2. Test the framework on additional domains beyond mazes and image classification to assess generality
3. Compare uncertainty minimization against alternative exploration strategies (e.g., UCB, Thompson sampling) in identical environments to establish relative effectiveness