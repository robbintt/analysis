---
ver: rpa2
title: 'GVFs in the Real World: Making Predictions Online for Water Treatment'
arxiv_id: '2312.01624'
source_url: https://arxiv.org/abs/2312.01624
tags:
- predictions
- data
- prediction
- learning
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the feasibility of using reinforcement learning
  to predict future sensor readings in a real-world drinking water treatment plant.
  The data exhibits high-dimensional, noisy, and partially observable time-series
  characteristics, with seasonal trends, mode changes, and sensor drift.
---

# GVFs in the Real World: Making Predictions Online for Water Treatment

## Quick Facts
- arXiv ID: 2312.01624
- Source URL: https://arxiv.org/abs/2312.01624
- Reference count: 40
- One-line primary result: GVF predictions outperform n-step predictions in NMSE on water treatment plant sensor data, with online learning during deployment providing significant accuracy improvements.

## Executive Summary
This paper investigates using reinforcement learning, specifically General Value Functions (GVFs), to predict future sensor readings in a real-world drinking water treatment plant. The data is high-dimensional, noisy, and partially observable with seasonal trends and sensor drift. The authors propose pre-training GVF predictors offline using logged data and fine-tuning them online during deployment using temporal difference learning. Experiments show GVF predictions significantly outperform n-step predictions in normalized mean-squared error across multiple sensors, with online learning during deployment further improving accuracy. This work demonstrates the importance of adapting to non-stationary conditions in real-world systems and provides a foundation for using RL-based predictions to enable adaptive control in complex industrial processes.

## Method Summary
The method involves pre-training neural network predictors offline on logged water treatment plant sensor data, then fine-tuning them online during deployment using temporal difference learning. To handle partial observability, the approach uses trace-based memory (exponentially weighted moving averages of past observations) rather than complex RNN architectures. The neural network architecture consists of 512 units across 2 hidden layers with ReLU activations. The GVF predictions are compared against classical n-step predictions, with both approaches evaluated using normalized mean-squared error on held-out validation data. Online learning capability is tested by introducing synthetic drift into validation data and measuring adaptation speed.

## Key Results
- GVF predictions significantly outperform n-step predictions in NMSE across multiple sensors
- Online learning during deployment significantly improves prediction accuracy compared to offline-only training
- Trace-based memory is effective for handling partial observability without complex RNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GVF predictions outperform n-step predictions due to their smoother target distribution
- Mechanism: GVFs use exponentially weighted sums of future observations, which smooths sharp spikes in sensor readings. This smoothing reduces variance in the prediction targets, making them easier for neural networks to learn.
- Core assumption: Sensor data exhibits high-frequency noise and occasional sharp spikes that can destabilize n-step predictions.
- Evidence anchors: [abstract]: "GVFs can have benefits over n-step predictions. The target for a GVF is typically smoother because it is an exponential weighting of future observations, rather than an observation at exactly n steps in the future."

### Mechanism 2
- Claim: Online learning during deployment is critical for handling non-stationary sensor drift
- Mechanism: The plant's sensor readings drift over time due to seasonal changes, maintenance events, and sensor degradation. Online TD updates allow the model to continuously adapt to these distribution shifts, maintaining prediction accuracy.
- Core assumption: The offline training data cannot fully capture the range of future operating conditions the plant will experience.
- Evidence anchors: [abstract]: "Critically, online learning during deployment significantly improves prediction accuracy compared to offline-only training, demonstrating the importance of adapting to non-stationary conditions in real-world systems."

### Mechanism 3
- Claim: The trace-based memory approach effectively handles partial observability without complex RNN architectures
- Mechanism: By using exponentially weighted moving averages of past observations, the trace-based memory captures relevant historical context needed to approximate the true system state, enabling accurate predictions despite incomplete sensor information.
- Core assumption: The relevant state information can be approximated through recent history rather than requiring the full system state.
- Evidence anchors: [section]: "The data, however, is partially observable and the agent should construct an approximate state... A typical approach used in machine learning is to use RNNs... However, we found for our sensor-rich problem setting, that a simpler trace-based memory approach was just as effective and much easier to train."

## Foundational Learning

- Concept: Temporal Difference Learning
  - Why needed here: TD learning enables online updates using bootstrapping, which is essential for adapting predictions in deployment without waiting for full n-step targets.
  - Quick check question: What is the key advantage of TD learning over Monte Carlo methods for online prediction?

- Concept: General Value Functions
  - Why needed here: GVFs provide a framework for predicting discounted sums of future observations, which is more suitable for continuous, noisy industrial sensor data than point predictions.
  - Quick check question: How does the discount factor Î³ affect the prediction horizon of a GVF?

- Concept: Partial Observability and State Approximation
  - Why needed here: The water treatment plant has incomplete sensor coverage, requiring methods to approximate the true system state from available observations.
  - Quick check question: Why might an exponentially weighted moving average be sufficient for state approximation in this setting?

## Architecture Onboarding

- Component map: Data preprocessing -> State construction (trace-based memory) -> Neural network prediction -> TD update loop -> Validation and hyperparameter tuning
- Critical path: The trace-based memory construction and neural network architecture are most critical for prediction quality; the TD update loop is critical for online adaptation.
- Design tradeoffs: Using trace-based memory instead of RNNs trades some modeling capacity for easier training and faster inference; simpler architectures may be preferable for real-time deployment.
- Failure signatures: High NMSE values indicate poor prediction quality; large TD errors during deployment suggest distribution shift or inadequate state representation.
- First 3 experiments:
  1. Train GVF predictions offline on historical data and evaluate NMSE on held-out validation data.
  2. Compare GVF predictions against n-step predictions using the same network architecture.
  3. Test online learning capability by introducing synthetic drift into validation data and measuring adaptation speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results of using GVF predictions compare to using n-step predictions when controlling the water treatment plant directly, rather than just predicting future sensor values?
- Basis in paper: [inferred] The paper shows that GVF predictions outperform n-step predictions in terms of normalized mean-squared error across multiple sensors. However, it does not investigate how these predictions would perform when used for actual control of the plant.
- Why unresolved: The paper focuses on prediction accuracy rather than control performance. It does not provide any results or discussion on how the predictions would be used for control or how they would compare in that context.
- What evidence would resolve it: Running experiments where both GVF and n-step predictions are used as inputs to a control system for the water treatment plant, and comparing the resulting control performance.

### Open Question 2
- Question: How do the results change when using a more sophisticated state approximation method, such as an RNN, instead of the trace-based memory approach used in the paper?
- Basis in paper: [explicit] The paper states that they found a simpler trace-based memory approach to be just as effective as RNNs for their sensor-rich problem setting, but they do not explore more sophisticated state approximation methods.
- Why unresolved: The paper only experiments with the trace-based memory approach and does not investigate other potential state approximation methods that could be more effective in this setting.
- What evidence would resolve it: Running experiments using different state approximation methods, such as RNNs or other more sophisticated approaches, and comparing their performance to the trace-based memory approach used in the paper.

### Open Question 3
- Question: How do the results change when using a longer prediction horizon, such as predicting 1000 steps into the future instead of 100 steps?
- Basis in paper: [explicit] The paper uses a prediction horizon of 100 steps and mentions that a longer prediction horizon would require waiting longer without updating the predictions in between, but they do not experiment with longer horizons.
- Why unresolved: The paper only experiments with a 100-step prediction horizon and does not investigate how the results would change with a longer horizon.
- What evidence would resolve it: Running experiments with different prediction horizons, such as 500 or 1000 steps, and comparing the results to the 100-step horizon used in the paper.

## Limitations
- Data Generalization: The study is based on a single water treatment plant with specific sensor configurations and operating conditions, which may not generalize to other industrial processes.
- Model Architecture Simplicity: The study did not exhaustively explore more complex architectures like LSTMs or Transformers that might capture longer-term dependencies better.
- Hyperparameter Sensitivity: The optimal settings may be sensitive to specific data characteristics, and the study does not provide systematic sensitivity analysis across different parameter configurations.

## Confidence
- High Confidence: GVFs outperform n-step predictions in terms of NMSE on this dataset; Online learning during deployment provides significant accuracy improvements; Trace-based memory is effective for handling partial observability in this setting
- Medium Confidence: The smoothing mechanism of GVFs generalizes to other noisy time series problems; The specific network architecture (512 units, 2 layers) is optimal for this task; The TD learning approach will remain stable across all possible deployment scenarios

## Next Checks
1. **Cross-Plant Validation**: Test the same GVF prediction framework on water treatment data from a different plant or region to assess generalizability of the performance improvements.
2. **Architecture Ablation Study**: Compare trace-based memory against LSTM/GRU architectures on a held-out test set with synthetic long-term dependencies to quantify the tradeoff between simplicity and modeling capacity.
3. **Online Stability Analysis**: Monitor and analyze TD error distributions and NMSE trajectories during extended online deployment periods to identify conditions under which online learning might become unstable or degrade performance.