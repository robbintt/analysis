---
ver: rpa2
title: Interpretable Survival Analysis for Heart Failure Risk Prediction
arxiv_id: '2310.15472'
source_url: https://arxiv.org/abs/2310.15472
tags:
- survival
- heart
- data
- failure
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors develop an interpretable survival analysis pipeline
  that predicts heart failure risk. Their method transforms survival analysis into
  a classification problem using an improved survival stacking technique, performs
  feature selection with ControlBurn, and generates predictions using Explainable
  Boosting Machines (EBMs).
---

# Interpretable Survival Analysis for Heart Failure Risk Prediction

## Quick Facts
- arXiv ID: 2310.15472
- Source URL: https://arxiv.org/abs/2310.15472
- Reference count: 40
- Key outcome: State-of-the-art AUC performance (up to 0.834 with 50 features) for interpretable heart failure risk prediction using survival stacking, ControlBurn feature selection, and Explainable Boosting Machines

## Executive Summary
This paper presents an interpretable survival analysis pipeline that transforms survival prediction into a classification problem while preserving censoring information. The method combines improved survival stacking with ControlBurn for feature selection and Explainable Boosting Machines (EBMs) for interpretable predictions. Applied to a large EHR dataset of 363,398 patients, the pipeline achieves strong performance while identifying key risk factors including age, BMI, creatinine, and novel EKG interactions. The approach validates existing clinical knowledge while discovering new insights about heart failure risk prediction.

## Method Summary
The pipeline transforms survival analysis into classification using improved survival stacking with subsampling to manage memory constraints. ControlBurn performs feature selection by building forests of shallow trees and pruning with LASSO to handle feature correlation. EBMs then generate interpretable predictions through additive models with interaction terms. The method is evaluated on a large EHR dataset using time-dependent AUC and Brier score metrics, demonstrating state-of-the-art performance while providing interpretable feature importance scores and shape functions.

## Key Results
- Achieves AUC up to 0.834 using 50 features on 363,398-patient EHR dataset
- Identifies key risk factors including age, BMI, creatinine, and novel EKG interactions
- Outperforms standard methods while providing interpretable results through feature importance and shape functions
- Demonstrates effectiveness of survival stacking transformation with subsampling

## Why This Works (Mechanism)

### Mechanism 1
Survival stacking transforms survival analysis into binary classification while preserving censoring information. For each event time t, it creates risk sets containing all patients who haven't experienced the event by time t, labels patients who experience the event at time t as positive and those who survive past t as negative, and adds a time feature to the stacked data. Core assumption: The hazard function can be estimated from binary classification probabilities at different time points. Break condition: If risk sets become too large or class imbalance becomes severe, classification estimates may become unreliable.

### Mechanism 2
ControlBurn performs feature selection while handling feature correlation in EHR data. It builds a forest of shallow decision trees and uses LASSO to prune trees, selecting features that remain in unpruned trees. This captures nonlinear relationships while mitigating correlation bias. Core assumption: Correlated features can be effectively reduced without losing important signal. Break condition: If features are perfectly correlated, ControlBurn may arbitrarily select one feature over another.

### Mechanism 3
EBMs provide interpretable predictions through additive models with interaction terms. They fit shape functions for individual features and interaction terms using cyclic gradient boosting on shallow decision trees, creating a functional ANOVA decomposition that separates main effects from interactions. Core assumption: The functional ANOVA decomposition ensures each feature's contribution is identifiable. Break condition: If interactions are too complex, the additive model may not capture all relationships.

## Foundational Learning

- Survival analysis basics
  - Why needed here: The paper works with time-to-event data where patients can be censored, requiring survival analysis techniques
  - Quick check question: What is the difference between right-censored and left-censored data?

- Classification vs regression tradeoffs
  - Why needed here: The pipeline converts survival analysis to classification using survival stacking
  - Quick check question: Why can't standard classification models handle censored data directly?

- Feature correlation concepts
  - Why needed here: ControlBurn is specifically designed to handle correlated features in high-dimensional data
  - Quick check question: How does feature correlation affect feature importance rankings in linear models?

## Architecture Onboarding

- Component map: Data preprocessing → ControlBurn feature selection → Survival stacking → EBM training → Prediction
- Critical path: 1) Survival stacking creates classification data, 2) ControlBurn reduces dimensionality, 3) EBM training generates interpretable predictions
- Design tradeoffs: Memory vs accuracy (subsampling reduces memory but may lose signal), interpretability vs performance (EBMs sacrifice some performance for interpretability), feature count vs generalization (more features improve performance but reduce interpretability)
- Failure signatures: Memory errors during survival stacking indicate need for subsampling, poor performance with few features suggests important features are being removed, unexpected interaction terms may indicate model overfitting
- First 3 experiments: 1) Test survival stacking with subsampling on small dataset to verify class balance, 2) Compare ControlBurn vs LASSO feature selection on synthetic correlated data, 3) Train EBM on stacked data and verify shape functions match expectations for known relationships

## Open Questions the Paper Calls Out

### Open Question 1
How would the interpretable survival analysis pipeline perform on other healthcare conditions beyond heart failure, such as cancer or diabetes? The paper evaluates their pipeline specifically on heart failure risk prediction using a large EHR dataset, but doesn't explore other diseases. Testing the pipeline on EHR data for other diseases with survival outcomes while comparing performance and interpretability across conditions would resolve this.

### Open Question 2
What is the impact of the subsampling ratio γ in Algorithm 1 on model performance and interpretability? The authors mention using γ = 0.01 for subsampling in their experiments and note that subsampling helps with memory issues, but they don't explore how different γ values affect results. Conducting experiments with various γ values (e.g., 0.001, 0.01, 0.1) to measure changes in AUC, Brier score, and interpretability metrics would resolve this.

### Open Question 3
How does the interpretability of EBM models trained on survival-stacked data compare to post-hoc explanation methods like SHAP or LIME applied to standard machine learning survival models? While the paper claims EBMs provide better interpretability than post-hoc methods, there's no quantitative or qualitative comparison of how well users understand the models or how the explanations differ. Conducting a user study where clinicians interpret both EBM shape functions and SHAP/LIME explanations would resolve this.

## Limitations
- Performance metrics are based on a single large EHR dataset, limiting generalizability to different systems and populations
- Feature selection process may not transfer well to datasets with different feature distributions or missingness patterns
- Interpretability of interaction terms becomes more challenging as feature complexity increases

## Confidence

- **High**: Survival stacking effectively transforms survival analysis to classification (supported by multiple literature citations and clear algorithmic description)
- **Medium**: ControlBurn effectively handles feature correlation (algorithm described but lacks direct empirical validation in paper)
- **Medium**: EBMs provide interpretable predictions (mechanism described but validation limited to qualitative feature importance analysis)

## Next Checks

1. **Cross-dataset validation**: Apply the pipeline to a completely independent EHR dataset to test generalizability of feature selection and performance metrics
2. **Subsampling sensitivity analysis**: Systematically vary the subsampling ratio γ to determine optimal values for different event rates and dataset sizes
3. **Interpretability stress test**: Create synthetic datasets with known interaction structures to verify EBMs correctly identify and interpret complex feature interactions