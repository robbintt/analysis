---
ver: rpa2
title: Direct Amortized Likelihood Ratio Estimation
arxiv_id: '2311.10571'
source_url: https://arxiv.org/abs/2311.10571
tags:
- ratio
- dnre
- design
- estimator
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Direct Neural Ratio Estimation (DNRE), a
  novel amortized likelihood ratio estimator for likelihood-free simulation-based
  inference (SBI). Unlike previous approaches, DNRE directly computes the likelihood
  ratio between two parameter sets in a single forward pass, avoiding the need to
  compare two neural network outputs.
---

# Direct Amortized Likelihood Ratio Estimation

## Quick Facts
- arXiv ID: 2311.10571
- Source URL: https://arxiv.org/abs/2311.10571
- Reference count: 15
- Direct amortized likelihood ratio estimator that computes ratios in a single forward pass

## Executive Summary
This paper introduces Direct Neural Ratio Estimation (DNRE), a novel amortized likelihood ratio estimator for simulation-based inference (SBI). DNRE directly computes likelihood ratios between two parameter sets in a single forward pass, avoiding the need to compare two neural network outputs. The authors derive a Monte Carlo posterior approximation method and introduce a new gradient estimator for improved numerical stability. Empirical evaluations on standard SBI benchmarks show DNRE often outperforms previous ratio estimators, and the paper demonstrates that likelihood-free Hamiltonian Monte Carlo is competitive with random-walk Metropolis-Hastings, which was previously unknown.

## Method Summary
DNRE directly parameterizes the classifier with both parameter sets θ and θ', allowing it to compute likelihood ratios in a single forward pass. The method uses a neural network classifier that takes data and two parameter sets as inputs and outputs an estimate of the likelihood ratio. For posterior inference, the authors introduce a Monte Carlo approximation that estimates the posterior using numerical integration. They also propose a new gradient estimator that treats the classifier output as an approximate log ratio, avoiding numerical instability from exponentiating small numbers. The method is evaluated on 10 SBI benchmark tasks and a real-world quadcopter design application.

## Key Results
- DNRE achieves better C2ST scores than NRE and BNRE on 7 out of 10 benchmark tasks
- DNRE's new gradient estimator provides improved numerical stability for HMC
- DNRE enables likelihood-free HMC to be competitive with random-walk MH, previously unknown
- Real-world quadcopter design application demonstrates practical utility of DNRE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Neural Ratio Estimation (DNRE) improves ratio estimation accuracy by directly parameterizing the classifier with both parameter sets.
- Mechanism: DNRE's classifier d(x, θ, θ') receives both θ and θ' as explicit inputs, avoiding the need for implicit dependence via reference parameters. This explicit parameterization provides the classifier with more discriminative information, enabling it to learn a more accurate likelihood ratio.
- Core assumption: The additional information provided by explicitly passing both parameter sets to the classifier leads to a more accurate likelihood ratio.
- Evidence anchors:
  - [abstract] "Our approach directly computes the likelihood ratio between two competing parameter sets which is different from the previous approach of comparing two neural network output values."
  - [section 4] "A potential advantage of this approach is the additional information provided to the classifier for label y = 0. This is when the denominator, p(x|θ'), must be greater than the numerator, p(x|θ)."
  - [corpus] Weak evidence. No direct comparison of accuracy improvements in corpus papers.

### Mechanism 2
- Claim: DNRE's single forward pass for likelihood ratio estimation improves computational efficiency compared to previous approaches.
- Mechanism: DNRE directly computes the likelihood ratio between two parameter sets in a single forward pass, eliminating the need for two separate network evaluations as required by previous approaches. This reduces computational overhead and enables faster inference.
- Core assumption: The computational cost of a single forward pass is significantly lower than two separate network evaluations.
- Evidence anchors:
  - [abstract] "Our estimator is simple to train and estimates the likelihood ratio using a single forward pass of the neural estimator."
  - [section 4.1] "Unlike the likelihood-to-evidence ratio proposed in Equation (4), the DNRE only requires one pass through the neural classifier to estimate the likelihood ratio."
  - [corpus] Weak evidence. No direct comparison of computational efficiency in corpus papers.

### Mechanism 3
- Claim: DNRE's new gradient estimator improves numerical stability for likelihood-free Hamiltonian Monte Carlo (HMC).
- Mechanism: DNRE's gradient estimator treats the classifier output as an approximate log ratio, avoiding the need for exponentiating and dividing small numbers as in previous approaches. This results in a more numerically stable gradient estimation for HMC.
- Core assumption: The numerical instability caused by exponentiating and dividing small numbers in previous gradient estimators is a significant issue in practice.
- Evidence anchors:
  - [abstract] "We also introduce a new derivative estimator that can be applied to both our approach and previous approaches. This gradient estimator is more numerically stable than the previous one."
  - [section 4.2] "To estimate∇U(θ), Hermans, Begy, and Louppe (2020) use the chain rule to derive:∇θU(θ) = −∇θr(x|θ) / r(x|θ). (8) This estimate of the derivative can be numerically unstable in practice as it requires exponentiating the output of the estimator for the denominator, which we have observed can often be a small number."
  - [corpus] Weak evidence. No direct comparison of numerical stability in corpus papers.

## Foundational Learning

- Concept: Bayesian inference and Markov chain Monte Carlo (MCMC)
  - Why needed here: DNRE is used within the context of likelihood-free SBI, which relies on Bayesian inference to estimate the posterior distribution of parameters given observed data. MCMC methods like HMC are commonly used to sample from the posterior distribution.
  - Quick check question: What is the role of the likelihood ratio in the Metropolis-Hastings acceptance step of MCMC?

- Concept: Neural network architectures and training
  - Why needed here: DNRE utilizes a neural network classifier to estimate the likelihood ratio. Understanding neural network architectures, training procedures, and loss functions is essential for implementing and optimizing DNRE.
  - Quick check question: What is the purpose of the binary cross-entropy loss function in training the DNRE classifier?

- Concept: Hamiltonian Monte Carlo (HMC)
  - Why needed here: DNRE introduces a new gradient estimator that enables the use of HMC for likelihood-free inference. Familiarity with HMC concepts, such as Hamiltonian dynamics, potential energy functions, and trajectory length, is necessary for understanding and implementing DNRE with HMC.
  - Quick check question: How does the choice of trajectory length in HMC affect the exploration of the parameter space?

## Architecture Onboarding

- Component map:
  - Neural network classifier: Takes inputs (x, θ, θ') and outputs an estimate of the likelihood ratio r(x|θ, θ')
  - Monte Carlo posterior approximation: Uses the DNRE output to estimate the posterior distribution p(θ|x) via numerical integration
  - HMC sampler: Utilizes the DNRE gradient estimator to perform gradient-based sampling of the posterior distribution

- Critical path:
  1. Train the DNRE classifier using the provided training data
  2. Use the trained DNRE to estimate the likelihood ratio for given parameter sets
  3. Apply the Monte Carlo posterior approximation to estimate the posterior distribution
  4. If using HMC, employ the DNRE gradient estimator to perform gradient-based sampling of the posterior

- Design tradeoffs:
  - Single forward pass vs. two separate network evaluations: DNRE's single forward pass improves computational efficiency but may require a more complex network architecture to handle the additional input parameters
  - Explicit parameterization vs. implicit dependence: DNRE's explicit parameterization of both parameter sets provides more information to the classifier but may increase the dimensionality of the input space

- Failure signatures:
  - Poor likelihood ratio estimation: If the DNRE classifier fails to accurately estimate the likelihood ratio, the posterior approximation and subsequent inference will be unreliable
  - Numerical instability in gradient estimation: If the DNRE gradient estimator suffers from numerical instability, the HMC sampler may produce unreliable samples or fail to converge

- First 3 experiments:
  1. Train the DNRE classifier on a simple synthetic dataset and evaluate its ability to accurately estimate the likelihood ratio
  2. Compare the posterior approximation accuracy of DNRE with previous approaches on a low-dimensional benchmark dataset
  3. Investigate the impact of DNRE's gradient estimator on the convergence and mixing of the HMC sampler on a simple synthetic dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DNRE scale with increasing dimensionality of the parameter space, particularly in comparison to NRE and BNRE?
- Basis in paper: [inferred] The paper demonstrates DNRE's effectiveness on several SBI benchmark tasks, but does not explicitly explore its performance on high-dimensional problems.
- Why unresolved: The experiments primarily focus on low-dimensional benchmark tasks, leaving the behavior of DNRE in high-dimensional spaces unexplored.
- What evidence would resolve it: Conducting experiments on SBI benchmark tasks with significantly higher dimensional parameter spaces and comparing the performance of DNRE, NRE, and BNRE would provide insights into the scalability of these approaches.

### Open Question 2
- Question: What is the impact of the choice of prior distribution on the performance of DNRE, and how sensitive is it to prior misspecification?
- Basis in paper: [inferred] The paper mentions that DNRE uses a prior distribution, but does not investigate how different choices of priors affect its performance or its robustness to misspecified priors.
- Why unresolved: The experiments use fixed prior distributions for the SBI benchmark tasks, and the sensitivity to prior misspecification is not explored.
- What evidence would resolve it: Conducting experiments where the prior distribution is varied systematically or intentionally misspecified would reveal the impact of the prior choice on DNRE's performance and its sensitivity to misspecification.

### Open Question 3
- Question: How does the performance of DNRE compare to other likelihood-free inference methods, such as neural posterior estimation (NPE) or neural likelihood estimation (NLE), in terms of accuracy and computational efficiency?
- Basis in paper: [inferred] The paper focuses on comparing DNRE with other neural ratio estimators (NRE and BNRE), but does not directly compare it with other likelihood-free inference methods like NPE or NLE.
- Why unresolved: The paper's evaluation is limited to the SBI benchmark tasks and does not include a comprehensive comparison with other likelihood-free inference approaches.
- What evidence would resolve it: Conducting experiments that directly compare the performance of DNRE with NPE and NLE on a diverse set of SBI benchmark tasks would provide insights into its relative strengths and weaknesses in terms of accuracy and computational efficiency.

## Limitations
- Mixed empirical evidence: DNRE shows only marginal improvements over NRE/BNRE in most benchmark tasks, with substantial gains only in the Lotka-Volterra task
- Limited efficiency comparison: Claims about computational efficiency improvements lack direct quantitative comparison with previous approaches
- Prior sensitivity unexplored: The impact of prior distribution choice and sensitivity to misspecification is not investigated

## Confidence
- High: The mechanism by which DNRE computes likelihood ratios in a single forward pass is technically sound and correctly implemented
- Medium: Claims about numerical stability improvements in the gradient estimator, as the evaluation relies on qualitative observations rather than systematic comparison
- Medium: The claim that HMC is competitive with MH for likelihood-free inference, given this was previously unknown and requires careful implementation

## Next Checks
1. Implement a controlled experiment comparing computational runtime of DNRE versus NRE/BNRE across varying problem sizes to quantify the efficiency claims
2. Systematically test DNRE's numerical stability across problems with varying likelihood ratio magnitudes to verify the gradient estimator improvements
3. Conduct ablation studies removing the explicit parameterization of θ' in DNRE to isolate the contribution of this architectural choice to performance gains