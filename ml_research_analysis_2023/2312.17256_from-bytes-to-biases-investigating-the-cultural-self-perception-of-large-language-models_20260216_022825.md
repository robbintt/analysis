---
ver: rpa2
title: 'From Bytes to Biases: Investigating the Cultural Self-Perception of Large
  Language Models'
arxiv_id: '2312.17256'
source_url: https://arxiv.org/abs/2312.17256
tags:
- society
- cultural
- soci
- dans
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the cultural self-perception of large language
  models (LLMs) using prompts based on the GLOBE project. We find that ChatGPT is
  culturally closest to Finland, French-speaking Switzerland, English-speaking Canada,
  China, and Australia, while Bard is closest to Australia, English-speaking Canada,
  the United States, the indigenous ethnic group of South Africa, and Israel.
---

# From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models

## Quick Facts
- arXiv ID: 2312.17256
- Source URL: https://arxiv.org/abs/2312.17256
- Reference count: 40
- Primary result: ChatGPT aligns culturally with Finland, French-speaking Switzerland, English-speaking Canada, China, and Australia; Bard aligns with Australia, English-speaking Canada, the United States, South African indigenous groups, and Israel

## Executive Summary
This study investigates the cultural self-perception of large language models (LLMs) by prompting ChatGPT and Bard with 39 value questions derived from the GLOBE project. The research finds that both systems exhibit cultural alignment most closely with English-speaking countries and economically competitive nations, with specific country rankings varying between models. The findings highlight the importance of understanding and mitigating cultural biases in LLMs to prevent perpetuating biases in human decision-making and AI algorithm development.

## Method Summary
The study uses prompts based on the GLOBE project's 39 value questions to elicit cultural dimensions from ChatGPT (Version 3.5) and Bard (Version 2023.07.13). Each LLM was prompted 25 times in English (and French for robustness), responses were averaged, and cultural dimensions were computed. Euclidean distances and cosine similarities were calculated between LLM responses and 62 countries' GLOBE cultural profiles, followed by regression analyses incorporating institutional factors like Global Competitiveness Index, language usage, and GDP.

## Key Results
- ChatGPT is culturally closest to Finland, French-speaking Switzerland, English-speaking Canada, China, and Australia
- Bard is culturally closest to Australia, English-speaking Canada, the United States, South African indigenous groups, and Israel
- Both systems' cultural self-perception most closely aligns with English-speaking countries and economically competitive nations
- English language dominance in training data (92.647% for GPT-3) correlates with Western cultural bias in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural alignment in LLMs can be measured by mapping model responses onto established cultural dimensions and comparing them to national values
- Mechanism: Prompts derived from the GLOBE project elicit numerical responses that can be placed into the same nine-dimensional space as national cultural profiles; Euclidean or cosine distance then quantifies similarity
- Core assumption: LLMs treat prompts as culturally neutral inputs and respond consistently enough to be measured across runs
- Evidence anchors:
  - "we iteratively prompt ChatGPT and Bard with 39 value questions from the GLOBE project (House & Javidan, 2004) to elicit its cultural stance on the nine societal GLOBE dimensions of culture."
  - "We use the Euclidean distance to gauge the cultural difference between an LLM and the countries as a summary statistic based on the nine GLOBE cultural dimensions."
- Break condition: If LLM responses are too inconsistent or if prompts introduce cultural priming, the distance metric loses validity

### Mechanism 2
- Claim: The training data's geographic and linguistic distribution determines the cultural profile of the resulting LLM
- Mechanism: More training content from a region or language increases the model's exposure to that culture's norms and values, which are then reflected in its responses
- Core assumption: Language models internalize statistical patterns from their training data without explicit cultural filtering
- Evidence anchors:
  - "ChatGPT is culturally closest to Finland, French-speaking Switzerland, English-speaking Canada, China, and Australia."
  - "LLMs are predominantly trained on documents in English (see Figure 3), such that 92.647% of total words used for training GPT-3 were in English."
- Break condition: If training datasets are balanced or if post-training cultural fine-tuning is applied, the correlation weakens

### Mechanism 3
- Claim: Economic productivity and English usage correlate with cultural alignment because they increase data availability and internet presence
- Mechanism: Countries with higher competitiveness generate more digital content and contribute more data to training corpora; English-speaking countries dominate internet content, further amplifying their influence
- Core assumption: Data volume and representativeness directly influence model cultural perception
- Evidence anchors:
  - "both systems' cultural self-perception most closely aligns with the values of English-speaking countries and countries characterized by sustained economic competitiveness."
  - "we posit that countries renowned for their economic prowess are likely to produce more data, and that the characteristics of this data will consequently be reflected in the LLMs."
- Break condition: If cultural influence is mediated by policy or if datasets are curated to be globally balanced, the economic/linguistic predictors lose explanatory power

## Foundational Learning

- Concept: Cultural dimensions (e.g., Hofstede, GLOBE)
  - Why needed here: These frameworks provide a measurable space in which to compare LLM responses with national cultures
  - Quick check question: Can you name the nine GLOBE dimensions used in the study?

- Concept: Euclidean distance and cosine similarity
  - Why needed here: These metrics quantify how close an LLM's cultural profile is to a country's in multi-dimensional space
  - Quick check question: What is the difference between Euclidean distance and cosine similarity in this context?

- Concept: Regression and effect size (Cohen's f²)
  - Why needed here: They assess how strongly institutional factors like competitiveness and language influence cultural alignment
  - Quick check question: What does a small to medium effect size (f²) indicate about the predictors?

## Architecture Onboarding

- Component map: Prompt generation → LLM inference → Response aggregation → Cultural distance calculation → Regression analysis
- Critical path: Prompt → LLM → Distance metric → Statistical test
- Design tradeoffs: Using English prompts maximizes coverage but may bias results; using French prompts tests language influence but reduces data volume
- Failure signatures: High variance in LLM responses, non-significant regression coefficients, inconsistent distance rankings across languages
- First 3 experiments:
  1. Rerun prompts with temperature=0 to test consistency
  2. Prompt the same model in French and compare distances
  3. Replace Euclidean distance with cosine similarity and re-run regressions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we reliably survey LLMs about their cultural self-perception beyond using Likert-scale prompts based on the GLOBE framework?
- Basis in paper: The paper acknowledges uncertainty about whether surveying LLMs via conversational interfaces with Likert-type scales is valid, calling for further experiments on how to properly survey LLMs for their opinions
- Why unresolved: The novelty of LLMs means there are no established methodologies for cultural self-perception assessment in AI systems, and current approaches may not capture the true cultural alignment of these models
- What evidence would resolve it: Development and validation of new cultural assessment methodologies specifically designed for LLMs, including alternative prompting strategies, evaluation metrics, and comparison with human cultural assessment methods

### Open Question 2
- Question: Can LLMs be conditioned to assume different cultural identities through targeted prompting, and how can we measure the authenticity of such cultural alignment?
- Basis in paper: The paper suggests investigating whether LLMs can be prompted with cultural values and whether they change their responses accordingly, and how humans would evaluate this alignment
- Why unresolved: While the paper shows that LLMs have inherent cultural biases, it remains unclear whether these systems can be dynamically adapted to different cultural contexts or if their cultural self-perception is fixed based on training data
- What evidence would resolve it: Experimental studies testing whether specific cultural prompts can alter LLM outputs in ways that are indistinguishable from human responses in different cultural contexts, validated through human evaluation and cross-cultural consistency checks

### Open Question 3
- Question: What are the long-term societal implications of LLMs perpetuating cultural biases, and how can we develop frameworks to mitigate these effects?
- Basis in paper: The paper discusses concerns about LLMs influencing human decision-making even after direct interaction ends, potentially creating self-reinforcing cycles of cultural bias, but doesn't explore long-term societal impacts or mitigation strategies
- Why unresolved: While the paper identifies cultural biases in LLMs and their potential influence on humans, it doesn't address how these biases might affect societal structures over time or what comprehensive frameworks could prevent bias perpetuation
- What evidence would resolve it: Longitudinal studies tracking cultural bias propagation through human-AI interaction chains, development of AI alignment frameworks that incorporate cultural sensitivity, and evaluation of mitigation strategies' effectiveness across different cultural contexts

## Limitations

- Prompt sensitivity: Results may vary with alternative prompt formulations or conversational contexts
- Temporal validity: Findings may not generalize to newer model versions as LLMs are under active development
- Data representation gaps: Assumes digital content volume accurately reflects cultural influence, overlooking representation issues

## Confidence

**High Confidence**: The methodological framework for measuring cultural dimensions using GLOBE-based prompts is sound, and the statistical analysis of distance metrics and regression models is appropriately executed

**Medium Confidence**: The specific country rankings for each LLM are internally consistent but may shift with different model versions or prompt variations

**Low Confidence**: Claims about the exact mechanism by which training data geography determines cultural outcomes are speculative

## Next Checks

1. **Prompt Robustness Test**: Replicate the analysis using 10-15 alternative prompt formulations for the same GLOBE dimensions to assess sensitivity to prompt engineering

2. **Cross-Model Validation**: Test the same methodology on additional LLM architectures (e.g., Claude, LLaMA) to determine whether cultural alignment patterns are model-specific

3. **Temporal Stability Assessment**: Repeat the full analysis after 6-12 months with updated model versions to quantify how quickly cultural profiles shift as models evolve