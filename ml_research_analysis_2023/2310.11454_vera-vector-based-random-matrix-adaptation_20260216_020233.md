---
ver: rpa2
title: 'VeRA: Vector-based Random Matrix Adaptation'
arxiv_id: '2310.11454'
source_url: https://arxiv.org/abs/2310.11454
tags:
- lora
- vera
- parameters
- assistant
- trainable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VeRA, a parameter-efficient fine-tuning method
  that reduces trainable parameters by 10x compared to LoRA while maintaining comparable
  performance. VeRA achieves this by using a single pair of frozen random matrices
  shared across all layers and learning small scaling vectors instead.
---

# VeRA: Vector-based Random Matrix Adaptation

## Quick Facts
- arXiv ID: 2310.11454
- Source URL: https://arxiv.org/abs/2310.11454
- Reference count: 21
- 10x fewer trainable parameters than LoRA while maintaining comparable performance

## Executive Summary
VeRA is a parameter-efficient fine-tuning method that reduces trainable parameters by 10x compared to LoRA while maintaining comparable performance. It achieves this by using a single pair of frozen random matrices shared across all layers and learning small scaling vectors instead. The method was evaluated on GLUE and E2E benchmarks, showing competitive results with significantly fewer parameters, and was also applied to instruction-tuning Llama2 7B with just 1.4M parameters.

## Method Summary
VeRA adapts large language models by freezing a shared pair of random matrices across all layers and learning only diagonal scaling vectors. Instead of training two low-rank matrices per layer as in LoRA, VeRA freezes matrices B and A and learns only scaling vectors d and b. This reduces trainable parameters from 2×Ltuned×dmodel×r to Ltuned×(dmodel + r). The method relies on the observation that pretrained model features have low intrinsic dimensionality, allowing frozen random matrices to capture task-specific adaptations when scaled appropriately.

## Key Results
- GLUE benchmark: Achieved similar performance to LoRA using only 0.031M parameters compared to 0.3M for LoRA on RoBERTa-base
- E2E benchmark: Outperformed LoRA with 3x fewer parameters
- Llama2 7B: Achieved better results than LoRA with 100x more parameters using just 1.4M trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
Scaling vectors applied to frozen random matrices can capture task-specific adaptations with fewer parameters than learning low-rank matrices. Instead of training two low-rank matrices A and B per layer, VeRA freezes shared random matrices and learns only diagonal scaling vectors, reducing trainable parameters significantly. The core assumption is that pretrained models' features have low intrinsic dimensionality that frozen random matrices can exploit. Break condition: If intrinsic dimensionality assumption fails, performance may collapse.

### Mechanism 2
Sharing a single pair of random matrices across all layers preserves expressiveness while drastically cutting memory. The same random matrices are reused for every transformer block, with layer-wise differences encoded in scaling vectors. This exploits the observation that pretrained weights have low intrinsic dimensionality. Break condition: If layer-wise adaptations diverge significantly, shared matrices may bottleneck performance.

### Mechanism 3
The method incurs no inference-time cost because scaling vectors can be merged into the base weight matrix. After training, learned scaling vectors are folded into the original weight matrix W0, eliminating the need to store separate adapters. Core assumption: Post-merge, the computational graph remains identical to the original model. Break condition: If merge operation introduces numerical instability, inference quality could degrade.

## Foundational Learning

- **Low-rank matrix approximation and intrinsic dimensionality**: LoRA and VeRA rely on capturing fine-tuning changes in a low-dimensional subspace. Quick check: What is the reported intrinsic dimension d901 for RoBERTabase, and how does it compare to trainable parameters used in LoRA?

- **Random matrix initialization and scaling**: VeRA uses frozen random matrices; understanding their distributional properties (e.g., Kaiming init) is critical for stable training. Quick check: How does Kaiming initialization ensure consistent variance across ranks when matrices are frozen?

- **Diagonal matrix representation for scaling**: The scaling vectors d and b are implemented as diagonal matrices; knowing how to apply them efficiently matters for both forward pass and merging. Quick check: In matrix multiplication, how does ΛbBΛdAx differ from (ΛbB)(ΛdAx) in terms of computational cost and gradient flow?

## Architecture Onboarding

- **Component map**: Base model (frozen weights W0) -> Shared random matrices B ∈ Rm×r, A ∈ Rr×n (frozen, Kaiming init) -> Trainable scaling vectors d ∈ R1×r, b ∈ R1×m (diagonal) -> Merge layer (optional)

- **Critical path**: 1. Forward pass: compute W0x + ΛbBΛdAx 2. Loss computation 3. Backward pass: gradients only through d and b 4. Parameter update (d and b only) 5. Optional merge: fold into W0 for deployment

- **Design tradeoffs**: Memory vs. expressiveness: fewer trainable params, but relies on shared random matrices; Initialization sensitivity: d init must be chosen carefully; Layer sharing: saves memory but may limit layer-specific adaptation

- **Failure signatures**: Performance collapse vs. LoRA suggests shared random matrices insufficient; Unstable training often traced to poor d init or rank too low; Gradient vanishing if d vectors become near-zero during training

- **First 3 experiments**: 1. Single-layer sanity check: apply VeRA to one attention layer on tiny dataset, compare convergence vs. LoRA 2. Rank sweep: vary r ∈ {1, 4, 16, 64, 256} on RTE task, plot accuracy vs. parameter count 3. Initialization ablation: test d init ∈ {1e-7, 1e-1, 1.0} on MRPC, measure impact on final accuracy and training stability

## Open Questions the Paper Calls Out

- **How does performance scale on extremely large language models beyond 13B parameters?** The paper demonstrates effectiveness on 7B and 13B models but doesn't explore scaling to models with hundreds of billions of parameters.

- **What is the impact of different random initialization strategies for shared matrices A and B?** While the paper compares Kaiming and uniform initialization, it doesn't exhaustively explore other strategies or their effects on different tasks.

- **How does VeRA perform on non-Transformer architectures?** The paper focuses exclusively on Transformer-based models, leaving questions about applicability to RNNs, CNNs, or other architectures.

- **What is the theoretical upper bound when combining VeRA with other parameter-efficient methods?** The paper doesn't explore synergies with adapters, prefix tuning, or other techniques.

## Limitations
- Performance generalizability across diverse tasks and model architectures remains unproven
- Inference-time efficiency claims rely on unverified merge operation
- Initialization strategy for scaling vectors lacks clear theoretical derivation

## Confidence
- **High**: Parameter reduction (10x fewer than LoRA) and competitive GLUE performance are well-supported by experimental results
- **Medium**: Inference-time efficiency claims are plausible but unverified independently
- **Low**: Generalizability across tasks and architectures is speculative without broader experimental validation

## Next Checks
1. **Rank Sensitivity Test**: Systematically vary rank r across multiple tasks (GLUE, SuperGLUE) to determine minimal rank required for stable performance, testing intrinsic dimensionality assumption
2. **Layer-wise Adaptation Analysis**: Compare VeRA performance when applying adapters to all layers vs. selective layers (query/value only), quantifying impact of shared matrices on layer-specific expressiveness
3. **Merge Operation Verification**: Implement and test merge procedure on small model (BERT-mini) to confirm numerical stability and equivalence to unfused weights