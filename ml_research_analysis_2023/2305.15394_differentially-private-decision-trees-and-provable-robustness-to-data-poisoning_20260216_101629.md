---
ver: rpa2
title: Differentially-Private Decision Trees and Provable Robustness to Data Poisoning
arxiv_id: '2305.15394'
source_url: https://arxiv.org/abs/2305.15394
tags:
- privacy
- decision
- trees
- budget
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PrivaTree is a new differentially-private decision tree algorithm
  that significantly improves the privacy-utility trade-off compared to previous methods.
  It uses private histograms for efficient split selection, the permute-and-flip mechanism
  for leaf labeling, and a better privacy budget distribution strategy.
---

# Differentially-Private Decision Trees and Provable Robustness to Data Poisoning

## Quick Facts
- arXiv ID: 2305.15394
- Source URL: https://arxiv.org/abs/2305.15394
- Reference count: 40
- PrivaTree is a new differentially-private decision tree algorithm that significantly improves the privacy-utility trade-off compared to previous methods.

## Executive Summary
PrivaTree introduces a differentially-private decision tree algorithm that achieves higher accuracy than existing approaches while maintaining strong privacy guarantees. The method uses private histograms for efficient split selection, the permute-and-flip mechanism for leaf labeling, and an adaptive privacy budget distribution strategy. Beyond utility improvements, PrivaTree provides provable robustness to data poisoning attacks, reducing backdoor attack success rates by fivefold compared to regular decision trees. The algorithm has been theoretically proven to satisfy differential privacy and validated on multiple benchmark datasets.

## Method Summary
PrivaTree builds differentially private decision trees using a combination of three key innovations. First, it employs private histograms with geometric noise for split selection, binning numerical features into 10 equal-sized quantiles to protect feature value information while maintaining split quality. Second, it uses the permute-and-flip mechanism for leaf labeling instead of traditional Laplace or exponential noise addition, providing better utility for majority vote decisions. Third, it implements an adaptive privacy budget distribution strategy that allocates resources based on dataset characteristics. The algorithm has been proven to satisfy differential privacy through sequential and parallel composition properties.

## Key Results
- PrivaTree achieves significantly higher accuracy than existing differentially-private decision tree methods across multiple privacy budgets (ε=0.1, 0.01, 1.0)
- The algorithm provides provable robustness to data poisoning attacks, reducing backdoor attack success rates by fivefold compared to regular decision trees
- PrivaTree demonstrates strong performance on both UCI datasets (adult, breast-w, diabetes, mushroom, nursery, vote) and OpenML tabular benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Private histograms enable high-quality split selection while preserving numerical feature privacy.
- Mechanism: By binning numerical features into equal-sized quantiles and adding geometric noise once per bin, the algorithm avoids revealing individual feature values while still enabling Gini impurity optimization.
- Core assumption: Decision trees perform well with a small number of bins (10 in this work) and that equal-width bins do not sufficiently protect feature information.
- Evidence anchors:
  - [section]: "To find high-quality splits while protecting feature value information and efficiently using the privacy budget, we use private histograms."
  - [abstract]: "PrivaTree uses private histograms for efficient split selection"
- Break condition: If the dataset has many numerical features with long tails, even quantile-based binning may lose too much information for good splits.

### Mechanism 2
- Claim: The permute-and-flip mechanism improves leaf labeling accuracy under differential privacy.
- Mechanism: Instead of adding Laplace or exponential noise to counts, permute-and-flip randomly selects a class label weighted by the exponential of the count difference, achieving better utility than the exponential mechanism.
- Core assumption: The utility function (majority vote) is sensitive to small changes in label counts, and permute-and-flip handles this better than other mechanisms.
- Evidence anchors:
  - [abstract]: "The permute-and-flip mechanism for leaf labeling"
  - [section]: "Permute-and-flip is proven to always be at least as performant as the exponential mechanism and practically outperforms it."
- Break condition: If the number of samples per leaf is very small, even permute-and-flip may produce highly noisy labels.

### Mechanism 3
- Claim: Differentially private learners are provably robust to data poisoning attacks.
- Mechanism: Differential privacy bounds the influence of any single training sample, limiting how much an adversary can change the model by adding poisoned samples.
- Core assumption: The privacy guarantee holds uniformly across all possible datasets, so the adversary cannot selectively increase probability mass in classifiers with high attack success rate.
- Evidence anchors:
  - [abstract]: "PrivaTree also provides provable robustness to data poisoning attacks"
  - [section]: "Theorem 3... bounds the expected backdoor attack success rate (ASR) against x poisoned samples"
- Break condition: If the privacy budget is too large, the bound becomes weak and poisoning may succeed.

## Foundational Learning

- Concept: Differential privacy and its composition properties
  - Why needed here: The algorithm relies on sequential and parallel composition to combine private operations while maintaining the overall privacy guarantee.
  - Quick check question: What is the difference between sequential and parallel composition in differential privacy?

- Concept: Decision tree learning (CART/ID3)
  - Why needed here: PrivaTree builds on greedy tree algorithms that select splits based on impurity measures like Gini.
  - Quick check question: How does Gini impurity guide split selection in decision trees?

- Concept: Exponential mechanism and permute-and-flip
  - Why needed here: These are the core private selection mechanisms used for splits and leaf labeling respectively.
  - Quick check question: Why does permute-and-flip generally outperform the exponential mechanism in practice?

## Architecture Onboarding

- Component map: Input dataset -> Private quantile computation -> Recursive tree building -> Private histogram-based split selection -> Permute-and-flip leaf labeling -> Differentially private decision tree

- Critical path:
  1. Compute private quantiles for numerical features
  2. For each node: compute private histograms, find best split, partition data
  3. When stopping criterion met: label leaf with permute-and-flip
  4. Return fully trained tree

- Design tradeoffs:
  - Fixed number of bins (10) vs adaptive binning: fixed is simpler and faster but may lose information
  - Private vs public quantiles: private protects feature values but consumes budget
  - Budget allocation strategy: adaptive based on dataset size vs fixed split

- Failure signatures:
  - Accuracy drops sharply with very small ϵ: privacy budget too constrained
  - Long tails in numerical features cause poor splits: binning strategy insufficient
  - Extremely imbalanced classes: majority vote may not be optimal even with privacy

- First 3 experiments:
  1. Train on a small UCI dataset (e.g., vote) with varying ϵ to observe accuracy-privacy trade-off
  2. Compare private quantile computation vs public quantiles on numerical features
  3. Measure backdoor attack success rate on MNIST with different poisoning ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PrivaTree's performance scale with increasing tree depth beyond 4?
- Basis in paper: [inferred] The paper only reports results for trees of depth 4, but does not explore deeper trees.
- Why unresolved: The paper focuses on interpretability by limiting tree depth, but does not investigate how performance changes with deeper trees.
- What evidence would resolve it: Experiments comparing PrivaTree's accuracy and privacy budget usage for trees of varying depths (e.g., 2, 4, 8, 16).

### Open Question 2
- Question: Can PrivaTree's privacy budget allocation strategy be further optimized for specific dataset characteristics?
- Basis in paper: [explicit] The paper proposes a general budget allocation scheme but acknowledges it may not be optimal for all cases.
- Why unresolved: The current allocation is based on theoretical bounds and empirical observations, but may not account for dataset-specific factors like feature importance or class distribution.
- What evidence would resolve it: Studies comparing PrivaTree's performance with adaptive budget allocation strategies that consider dataset properties.

### Open Question 3
- Question: How does PrivaTree's backdoor attack robustness compare to other differentially private learning methods beyond decision trees?
- Basis in paper: [explicit] The paper demonstrates PrivaTree's robustness against backdoor attacks compared to regular decision trees.
- Why unresolved: The comparison is limited to decision trees, and it's unclear how PrivaTree's robustness compares to other differentially private models like neural networks or ensemble methods.
- What evidence would resolve it: Experiments comparing PrivaTree's backdoor attack success rate against other differentially private learning algorithms on the same datasets and attack scenarios.

## Limitations

- The algorithm's performance on datasets with highly imbalanced classes or long-tailed numerical features remains unclear, as the fixed 10-bin approach may lose critical information in these cases
- While theoretical privacy guarantees are proven, the empirical privacy-utility trade-off could be dataset-dependent, and performance on high-dimensional data has not been extensively validated
- The robustness claims against poisoning assume the adversary cannot exploit specific structural properties of the dataset or the tree construction process

## Confidence

- **High confidence**: The core privacy guarantees of PrivaTree, as the mechanism is based on well-established differential privacy composition theorems and the mathematical proofs are provided
- **Medium confidence**: The accuracy improvements over existing differentially private decision tree methods, as these are demonstrated on standard benchmark datasets but may not generalize to all domains
- **Medium confidence**: The five-fold reduction in poisoning attack success rate, as this is based on a specific MNIST binary classification setup that may not represent all poisoning scenarios

## Next Checks

1. Test PrivaTree on a high-dimensional dataset (e.g., Covertype with 54 features) to verify the fixed binning strategy scales effectively
2. Evaluate the algorithm on an imbalanced dataset (e.g., fraud detection) to assess whether the permute-and-flip mechanism handles class imbalance adequately
3. Design a poisoning attack that specifically targets the private quantile computation step to test whether the privacy mechanism protects against adaptive adversaries