---
ver: rpa2
title: Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition
arxiv_id: '2310.06301'
source_url: https://arxiv.org/abs/2310.06301
tags:
- phase
- transitions
- critical
- bayesian
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Singular Learning Theory (SLT) to the Toy Model
  of Superposition (TMS), a simplified neural network architecture, to analyze phase
  transitions in both Bayesian and dynamical learning settings. The authors derive
  a closed-form expression for the TMS population loss and identify critical points
  corresponding to regular k-gons in the case of two hidden dimensions.
---

# Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition

## Quick Facts
- **arXiv ID**: 2310.06301
- **Source URL**: https://arxiv.org/abs/2310.06301
- **Reference count**: 40
- **Key outcome**: This paper applies Singular Learning Theory (SLT) to the Toy Model of Superposition (TMS), a simplified neural network architecture, to analyze phase transitions in both Bayesian and dynamical learning settings. The authors derive a closed-form expression for the TMS population loss and identify critical points corresponding to regular k-gons in the case of two hidden dimensions. They compute the local learning coefficient for these critical points and show theoretically and empirically that these k-gons determine phase transitions in the Bayesian posterior as a function of training sample size. Experiments with SGD training reveal that the same k-gon critical points govern the behavior of optimization trajectories, supporting the conjecture that SGD follows a sequential learning mechanism from high-loss-low-complexity to low-loss-high-complexity solutions. This work bridges Bayesian and dynamical phase transitions in TMS, providing evidence for the Bayesian antecedent hypothesis and demonstrating the utility of the local learning coefficient as a measure of model complexity.

## Executive Summary
This paper bridges Bayesian and dynamical phase transitions in a simplified neural network model called the Toy Model of Superposition (TMS). The authors derive a closed-form expression for the TMS population loss and identify regular k-gon critical points in the case of two hidden dimensions. They show theoretically that these k-gons determine Bayesian phase transitions as a function of training sample size, and experimentally demonstrate that the same critical points govern SGD training trajectories. This work provides evidence for the Bayesian antecedent hypothesis - that dynamical transitions in optimization have Bayesian antecedents - and demonstrates the utility of the local learning coefficient as a measure of model complexity that captures both types of transitions.

## Method Summary
The authors analyze phase transitions in TMS by deriving a closed-form population loss and identifying critical points corresponding to regular k-gons in two hidden dimensions. They compute the local learning coefficient for these critical points and use SGLD sampling to estimate their complexity. For Bayesian analysis, they employ MCMC sampling with the NUTS algorithm to explore the posterior distribution for various sample sizes, classifying samples into phases Wk,σ. For dynamical analysis, they run SGD training with minibatch size 20, learning rate 0.005, and 4500 epochs, initializing at a 4-gon plus Gaussian noise. The experiments track loss, test loss, and estimated local learning coefficient to identify plateaus associated with k-gon critical points and demonstrate the sequential learning mechanism.

## Key Results
- Derivation of closed-form TMS population loss and identification of regular k-gon critical points in two hidden dimensions
- Theoretical demonstration that k-gon critical points determine Bayesian phase transitions as function of sample size
- Empirical evidence that same k-gon critical points govern SGD training trajectories
- Support for the Bayesian antecedent hypothesis linking dynamical and Bayesian transitions
- Demonstration that local learning coefficient captures model complexity across both Bayesian and dynamical settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The TMS population loss exhibits k-gon critical points in the case of two hidden dimensions, and these k-gons determine phase transitions in the Bayesian posterior as a function of training sample size.
- **Mechanism**: The TMS potential is invariant under orthogonal transformations and permutations, allowing the derivation of a closed-form expression for the population loss. In the case of two hidden dimensions, the columns of the weight matrix form regular polygons (k-gons) in the plane, which are critical points of the potential. The local learning coefficient of these k-gons, a measure of their degeneracy, determines the dominant phase in the Bayesian posterior for a given sample size.
- **Core assumption**: The TMS potential is analytic at the k-gon critical points (except for the 4-gons, which are on the boundary of multiple chambers).
- **Evidence anchors**:
  - [abstract]: "We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular k-gons are critical points."
  - [section 3.2]: "We prove that various k-gons are critical points for H when r = 2."
  - [corpus]: Weak evidence; the corpus papers focus on different aspects of neural network learning and do not directly address the TMS or k-gon critical points.
- **Break condition**: If the TMS potential is not analytic at the k-gon critical points, or if the k-gons are not the dominant phases in the Bayesian posterior for the relevant range of sample sizes.

### Mechanism 2
- **Claim**: The local learning coefficient of the k-gon critical points governs the behavior of SGD training trajectories, supporting the conjecture that SGD follows a sequential learning mechanism from high-loss-low-complexity to low-loss-high-complexity solutions.
- **Mechanism**: The estimator for the local learning coefficient, based on SGLD sampling, is used to track the complexity of the solutions encountered during SGD training. The experiments show that as SGD reduces the loss, it encounters critical points with increasing local learning coefficients, indicating a preference for higher complexity solutions as training progresses.
- **Core assumption**: The estimator for the local learning coefficient is accurate and can distinguish between critical points of different complexities.
- **Evidence anchors**:
  - [abstract]: "We then show empirically that the same k-gon critical points also determine the behavior of SGD training."
  - [section 5]: "These experiments support the following description of SGD training for the TMS potential when r = 2, c = 6: trajectories are characterised by plateaus associated to the critical points described in Section 3.2 and further discussed in Appendix A."
  - [corpus]: Weak evidence; the corpus papers focus on different aspects of neural network learning and do not directly address the relation between local learning coefficient and SGD training.
- **Break condition**: If the estimator for the local learning coefficient is inaccurate or if SGD does not follow the predicted sequential learning mechanism.

### Mechanism 3
- **Claim**: The Bayesian antecedent hypothesis, which states that dynamical transitions in SGD training have Bayesian antecedents, is supported by the evidence from TMS.
- **Mechanism**: The dynamical transitions observed in SGD training are associated with increases in the estimated local learning coefficient, suggesting that they have Bayesian antecedents. The analysis of the free energy formula and the experimental evidence from MCMC sampling support the existence of Bayesian phase transitions between the same phases involved in the dynamical transitions.
- **Core assumption**: The free energy formula accurately describes the Bayesian posterior for the relevant range of sample sizes, and the constant terms in the formula do not significantly affect the predicted phase transitions.
- **Evidence anchors**:
  - [abstract]: "The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism."
  - [section 5.1]: "Since a dynamical transition decreases the loss, the main obstruction to having a Bayesian antecedent is that in a Bayesian phase transition α → β the local learning coefficient should increase."
  - [corpus]: Weak evidence; the corpus papers focus on different aspects of neural network learning and do not directly address the Bayesian antecedent hypothesis.
- **Break condition**: If the free energy formula does not accurately describe the Bayesian posterior, or if the constant terms in the formula significantly affect the predicted phase transitions.

## Foundational Learning

- **Concept: Singular Learning Theory (SLT)**
  - Why needed here: SLT provides a framework for analyzing phase transitions in the Bayesian posterior of singular models, such as neural networks. It introduces the concept of the local learning coefficient as a measure of model complexity, which is crucial for understanding the sequential learning mechanism in TMS.
  - Quick check question: What is the main difference between regular and singular models in terms of their Bayesian learning process, according to SLT?

- **Concept: Local Learning Coefficient**
  - Why needed here: The local learning coefficient is a geometric invariant that quantifies the degeneracy of a critical point in a singular model. It is used to measure the complexity of the k-gon critical points in TMS and to predict the dominant phases in the Bayesian posterior for different sample sizes.
  - Quick check question: How does the local learning coefficient relate to the number of normal directions to the level set of the loss function at a critical point?

- **Concept: Phase Transitions in Bayesian Learning**
  - Why needed here: Phase transitions in the Bayesian posterior occur when the posterior concentration "jumps" from one region of parameter space to another as the sample size increases. Understanding these transitions is crucial for explaining the sequential learning mechanism in TMS and for verifying the Bayesian antecedent hypothesis.
  - Quick check question: What is the main principle behind internal model selection in the context of Bayesian phase transitions?

## Architecture Onboarding

- **Component map**:
  - TMS potential: The loss function for the toy model of superposition, derived from the expected negative log likelihood
  - K-gon critical points: Regular polygons formed by the columns of the weight matrix in the case of two hidden dimensions, which are critical points of the TMS potential
  - Local learning coefficient: A measure of the degeneracy of a critical point, used to quantify the complexity of the k-gon critical points
  - Bayesian posterior: The distribution over the parameter space induced by the prior and the likelihood, subject to phase transitions as the sample size increases
  - MCMC sampling: A method for sampling from the Bayesian posterior, used to experimentally verify the dominant phases and the existence of Bayesian phase transitions
  - SGD training: The optimization algorithm used to train the TMS model, whose trajectories are characterized by plateaus associated with the k-gon critical points

- **Critical path**: Derive the TMS potential → Identify k-gon critical points → Compute local learning coefficients → Analyze Bayesian phase transitions → Experimentally verify dominant phases and dynamical transitions → Investigate the relation between Bayesian and dynamical transitions

- **Design tradeoffs**: The choice of the TMS model as a simplified setting for studying phase transitions involves a tradeoff between tractability and relevance to real neural networks. The use of MCMC sampling and the estimator for the local learning coefficient introduces computational challenges and potential sources of error.

- **Failure signatures**: If the TMS potential is not analytic at the k-gon critical points, or if the k-gons are not the dominant phases in the Bayesian posterior for the relevant range of sample sizes, the mechanisms proposed in this paper may not hold. If the estimator for the local learning coefficient is inaccurate or if SGD does not follow the predicted sequential learning mechanism, the empirical evidence for the mechanisms may be weak.

- **First 3 experiments**:
  1. Verify that the k-gon critical points are indeed critical points of the TMS potential by checking the gradient of the potential at these points.
  2. Compute the local learning coefficients of the k-gon critical points using the estimator based on SGLD sampling and compare them with the theoretically derived values.
  3. Experimentally verify the existence of Bayesian phase transitions between the k-gon phases by using MCMC sampling to estimate the dominant phases for different sample sizes and comparing the results with the predictions based on the free energy formula.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Bayesian antecedent hypothesis extend to larger neural networks beyond the Toy Model of Superposition?
- Basis in paper: [explicit] The authors propose the Bayesian antecedent hypothesis specifically for TMS and suggest it may extend to larger neural networks.
- Why unresolved: The paper only provides evidence for the hypothesis in TMS, which is a highly simplified model. Scaling to realistic neural networks introduces many additional complexities.
- What evidence would resolve it: Demonstrating the hypothesis holds in increasingly complex models, or providing a theoretical framework that connects TMS behavior to general neural networks.

### Open Question 2
- Question: What is the exact mathematical relationship between dynamical transitions and Bayesian phase transitions?
- Basis in paper: [explicit] The authors propose a relationship but acknowledge it is "more subtle" and that "there is no necessary relation" between the two types of transitions.
- Why unresolved: The paper shows empirical correlations but does not establish a rigorous mathematical connection between the two phenomena.
- What evidence would resolve it: A formal mathematical theorem proving when and how dynamical transitions must have Bayesian antecedents, or conditions under which they do not.

### Open Question 3
- Question: How do the constant terms in the free energy formula affect the prediction of phase transition locations?
- Basis in paper: [explicit] The authors discuss how constant terms can shift the critical sample size by "lower order terms" and show this in examples, but acknowledge these terms are "not well-understood."
- Why unresolved: The paper uses approximations for constant terms based on prior contributions but notes these are simplifications.
- What evidence would resolve it: A complete derivation of constant terms in the free energy expansion for TMS, or a method to empirically measure their contribution across different phases.

### Open Question 4
- Question: What determines the hierarchy of critical points that SGD visits during training?
- Basis in paper: [inferred] The paper shows SGD follows a sequential learning mechanism from high-loss-low-complexity to low-loss-high-complexity solutions, but doesn't explain why this specific ordering occurs.
- Why unresolved: While the paper demonstrates the phenomenon exists, it doesn't provide a theoretical explanation for why certain critical points are visited in a particular order.
- What evidence would resolve it: A theoretical framework explaining the selection mechanism that determines the sequence of critical points visited during optimization.

## Limitations
- The TMS model is a highly simplified toy model that may not capture all relevant phenomena of real neural networks
- The free energy formula relies on approximations for constant terms that may become significant in certain parameter regimes
- The SGLD-based estimator for local learning coefficient requires careful hyperparameter tuning with "bespoke" step sizes not fully specified
- The analysis of phase transitions is limited to the analytically tractable two-hidden-dimension case

## Confidence
- **High Confidence**: The derivation of k-gon critical points as regular polygons in the two-hidden-dimension case, supported by explicit mathematical proofs in Section 3.2
- **Medium Confidence**: The empirical demonstration that SGD trajectories follow plateaus associated with k-gon critical points, as the analysis depends on the accuracy of the local learning coefficient estimator and the specific initialization scheme
- **Low Confidence**: The generalizability of the Bayesian antecedent hypothesis beyond the TMS setting, as the paper provides limited evidence about how these mechanisms extend to more complex models

## Next Checks
1. **Sensitivity Analysis of SGLD Estimator**: Systematically vary the SGLD step sizes across the critical point groups to quantify how estimation errors propagate into the classification of SGD plateaus and the verification of the sequential learning mechanism.

2. **Extension to Three Hidden Dimensions**: Replicate the k-gon critical point analysis for r = 3 hidden dimensions to test whether the geometric structure of critical points generalizes beyond the analytically tractable two-dimension case, potentially revealing new phase transition patterns.

3. **Robustness to Initialization Schemes**: Test whether the same k-gon plateaus and sequential learning pattern emerge when initializing SGD from different regions of parameter space (e.g., random initialization, other fixed points) to assess the universality of the observed training dynamics.