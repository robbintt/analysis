---
ver: rpa2
title: Explainable Parallel RCNN with Novel Feature Representation for Time Series
  Forecasting
arxiv_id: '2305.04876'
source_url: https://arxiv.org/abs/2305.04876
tags:
- time
- future
- series
- covariates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of time series forecasting with
  future covariates, which is challenging due to the potential for error accumulation
  in iterative methods and the inability of direct methods to capture interactions
  between past and future data. The proposed method, ParaRCNN, introduces a novel
  feature representation strategy called shifting that fuses past and future data
  into a single input for a parallel RNN-CNN framework.
---

# Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting

## Quick Facts
- arXiv ID: 2305.04876
- Source URL: https://arxiv.org/abs/2305.04876
- Reference count: 40
- Outperforms baseline models on real-world datasets with lower MAE and RMSE

## Executive Summary
This paper introduces ParaRCNN, a novel approach for time series forecasting with future covariates that addresses the limitations of both iterative and direct forecasting methods. The key innovation is a shifting strategy that fuses past observations and future predictable covariates into a single integrated input, allowing the model to capture interactions between historical and future data simultaneously. The method employs a parallel RNN-CNN framework with hierarchical layers and skip connections to learn both temporal dependencies and complex dynamics while maximizing input feature usability.

## Method Summary
ParaRCNN uses a shifting transformation to align future predictable covariates with historical data by shifting them backward by s time steps. This creates a unified input representation that combines past target variables, observed covariates, and future covariates for each time step. The model processes this fused input through parallel RNN and CNN branches, each with 3-4 hierarchical layers. Skip connections concatenate the original input to every subsequent layer, preventing information loss in deep networks. The outputs from both branches are concatenated and passed through fully connected layers for final predictions.

## Key Results
- ParaRCNN achieves lower mean absolute errors (MAEs) and root mean square errors (RMSEs) compared to baseline models without shifting or skip connections
- The shifting strategy improves performance by capturing interactions between past and future data
- Skip connections prevent information loss and improve gradient flow in deep networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shifting strategy fuses past and future data into a single integrated input, allowing the model to capture interactions between historical and predictable future covariates simultaneously.
- Mechanism: By shifting future predictable covariates backward by s time steps and aligning them with past data, each input tuple contains both historical and future information for the same time step. This enables the RNN and CNN to learn temporal dependencies that span across past and future simultaneously within a single model component.
- Core assumption: Future predictable covariates are sufficiently accurate to be useful when combined with historical data, and the alignment by shifting preserves meaningful temporal relationships.
- Evidence anchors:
  - [abstract] "a novel feature representation strategy -shifting - is proposed to fuse the past data and future covariates such that their interactions can be considered"
  - [section 4.1] "we shift the covariates for the future period of interest back by s time steps, such that they are aligned and fused with all historical time series"
  - [corpus] Weak evidence - corpus papers discuss future covariates but don't explicitly address shifting strategy for data fusion
- Break condition: If future covariates are inaccurate or have different temporal patterns than historical data, the shifting alignment could introduce noise rather than useful information.

### Mechanism 2
- Claim: The parallel RNN-CNN architecture captures both temporal dependencies and complex dynamics more effectively than either approach alone.
- Mechanism: RNNs learn sequential temporal dependencies through recurrent state transitions, while CNNs learn local patterns and cross-feature relationships through convolutional filters. By processing the same fused input in parallel and concatenating outputs, the model benefits from both perspectives simultaneously.
- Core assumption: Time series dynamics have both sequential patterns (captured by RNN) and local spatial patterns (captured by CNN) that are complementary.
- Evidence anchors:
  - [section 4.2] "we develop a parallel framework composed of RNN and CNN, both of which are in a hierarchical structure"
  - [section 4.2] "RNN and CNN learn the temporal dependency and dynamics in different mechanisms"
  - [corpus] Moderate evidence - corpus papers mention CNN and RNN separately but not their combination for time series forecasting
- Break condition: If the time series data is purely sequential without local patterns, the CNN component may add unnecessary complexity without benefit.

### Mechanism 3
- Claim: Skip connections maximize the usability of input features by bypassing information directly to deeper layers, preventing information loss in deep networks.
- Mechanism: The skip connection strategy concatenates the original input to every subsequent layer's input, ensuring that raw feature information is available at all processing depths. This helps prevent vanishing gradients and allows the model to learn residual functions more easily.
- Core assumption: Direct access to original input features at all layers improves learning, especially in deeper networks where information degradation can occur.
- Evidence anchors:
  - [section 4.2] "we adopt L skip connections by bypassing the original input to every latter layer with concatenation"
  - [section 4.2] "Such a structure can facilitate the model by reusing the original input many times and learning it directly while avoiding the vanishing gradient issue of deeper layers"
  - [corpus] Weak evidence - corpus papers don't discuss skip connections in the context of time series forecasting
- Break condition: If the network is shallow (few layers), skip connections may provide minimal benefit and could even introduce redundancy.

## Foundational Learning

- Concept: Temporal dependency modeling
  - Why needed here: Time series forecasting requires understanding how past values influence future values, which is fundamental to predicting future time steps
  - Quick check question: Can you explain why iterative forecasting methods suffer from error accumulation while one-shot methods avoid this issue?

- Concept: Feature representation and fusion
  - Why needed here: The shifting strategy requires understanding how to combine different types of time series data (past observations vs future predictions) into a unified representation that preserves meaningful relationships
  - Quick check question: What happens to the temporal alignment of features when future covariates are shifted back by s time steps?

- Concept: Convolutional filter operations
  - Why needed here: The CNN component uses 1D convolutional filters to learn local patterns across both time steps and feature dimensions, which requires understanding how convolution operations work on sequential data
  - Quick check question: How does a 1D convolutional filter move across multivariate time series data, and what patterns can it capture?

## Architecture Onboarding

- Component map: Input layer → Shifting transformation → Parallel RNN and CNN branches (each with 3-4 hierarchical layers) → Skip connections (concatenating original input to each layer) → Concatenation of RNN and CNN outputs → Fully connected layer → Output predictions
- Critical path: Data preprocessing (shifting) → Parallel feature extraction (RNN+CNN) → Skip connection integration → Final prediction layer
- Design tradeoffs: Parallel architecture increases computational cost but provides richer feature representations; shifting strategy assumes future covariate accuracy; skip connections add parameters but improve gradient flow
- Failure signatures: Poor performance on test data despite good training performance suggests overfitting; significantly worse results with shifting disabled confirms shifting importance; degradation when skip connections are removed validates their contribution
- First 3 experiments:
  1. Train with shifting disabled (past and future processed separately) to validate the shifting mechanism's contribution
  2. Train with only RNN or only CNN branch to evaluate the benefit of parallel architecture
  3. Train with different skip connection strategies (no skip, L skips, L(L+1)/2 skips) to identify optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of shift length (s) affect the model's performance when s is outside the range of k ≤ s ≤ w?
- Basis in paper: [explicit] The paper states that k ≤ s ≤ w generates better performance, but it does not explore the effects of s outside this range in detail.
- Why unresolved: The paper mentions that s < k or s > w can lead to considerably lower performances but does not provide a comprehensive analysis of why this occurs or the exact impact of different shift lengths.
- What evidence would resolve it: Conducting experiments with a wider range of shift lengths and analyzing the model's performance metrics (e.g., MAE, RMSE) for each case would provide insights into the optimal shift length and its impact on forecasting accuracy.

### Open Question 2
- Question: What is the impact of using different activation functions in the RNN and CNN components of the ParaRCNN model?
- Basis in paper: [inferred] The paper uses hyperbolic tangent (tanh) as the activation function in the RNN and does not explore the effects of other activation functions.
- Why unresolved: The choice of activation function can significantly influence the model's ability to capture complex patterns in time series data, but the paper does not investigate this aspect.
- What evidence would resolve it: Comparing the performance of the ParaRCNN model using different activation functions (e.g., ReLU, sigmoid) in the RNN and CNN components would help determine the optimal activation function for this task.

### Open Question 3
- Question: How does the ParaRCNN model perform on datasets with different characteristics, such as non-stationary time series or those with high noise levels?
- Basis in paper: [inferred] The paper evaluates the model on three real-world datasets but does not explore its performance on datasets with varying characteristics.
- Why unresolved: The model's ability to handle different types of time series data (e.g., non-stationary, noisy) is not addressed, which limits its generalizability.
- What evidence would resolve it: Testing the ParaRCNN model on a diverse set of datasets with different characteristics (e.g., non-stationary, high noise) and comparing its performance with other models would provide insights into its robustness and adaptability.

## Limitations
- The shifting strategy's effectiveness depends critically on the accuracy of future covariates, which is not extensively validated
- The parallel RNN-CNN architecture introduces significant complexity without clear evidence that this combination is superior to more streamlined approaches
- The interpretability claims based on Grad-CAM need stronger validation through domain expert evaluation

## Confidence

- **High Confidence**: The core methodology of combining RNN and CNN architectures for time series forecasting is well-established in the literature. The shifting concept for feature fusion is logically sound, though its effectiveness depends on implementation details.
- **Medium Confidence**: The experimental results showing improved MAE and RMSE scores appear promising, but the comparison with baseline models could be more comprehensive. The claim that skip connections maximize input feature usability is reasonable but needs more rigorous ablation studies.
- **Low Confidence**: The interpretability claims using Grad-CAM are weak without validation from domain experts. The assertion that parallel RNN-CNN is inherently better than sequential or hybrid approaches lacks sufficient comparative analysis.

## Next Checks
1. **Ablation Study**: Conduct comprehensive ablation tests removing each key component (shifting, skip connections, parallel architecture) individually to quantify their individual contributions to performance improvements.
2. **Robustness Testing**: Evaluate model performance across varying levels of future covariate accuracy to determine the sensitivity of the shifting strategy to prediction errors in future data.
3. **Domain Expert Validation**: Have domain experts in the specific application areas (environmental monitoring, energy pricing, hydrology) assess the Grad-CAM visualizations to verify that the model highlights genuinely important features rather than spurious correlations.