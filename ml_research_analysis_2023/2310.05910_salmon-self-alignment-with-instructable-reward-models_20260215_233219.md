---
ver: rpa2
title: 'SALMON: Self-Alignment with Instructable Reward Models'
arxiv_id: '2310.05910'
source_url: https://arxiv.org/abs/2310.05910
tags:
- arxiv
- reward
- should
- language
- principles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALMON, a novel approach to aligning language
  models with minimal human supervision by leveraging principle-following reward models.
  The key idea is to train a reward model on synthetic preference data to generate
  scores based on arbitrary human-defined principles, enabling full control over model
  preferences during reinforcement learning without requiring online human preference
  collection.
---

# SALMON: Self-Alignment with Instructable Reward Models

## Quick Facts
- arXiv ID: 2310.05910
- Source URL: https://arxiv.org/abs/2310.05910
- Reference count: 40
- Key outcome: Introduces SALMON, achieving state-of-the-art performance on multiple benchmarks while using only 6 ICL exemplars and 31 human-defined principles

## Executive Summary
This paper presents SALMON, a novel approach to aligning large language models with minimal human supervision. The key innovation is an instructable reward model trained on synthetic preference data that can generate reward scores based on arbitrary human-defined principles. Applied to LLaMA-2-70b, this method produces Dromedary-2, which achieves state-of-the-art performance on MT-Bench (7.4), Big-Bench Hard (51.4%), HumanEval (40.6%), and TruthfulQA (98% truthful and informative). The approach eliminates the need for online human preference collection during RL training while maintaining high alignment quality.

## Method Summary
SALMON trains a principle-following reward model on synthetic preference data generated by an initial SFT model. The reward model takes (prompt, response, principles) as input and outputs scalar reward scores. During RL training with PPO, the model is optimized using this reward model with additional symbolic rewards (multilingual bonus, length bonus). RL-time intervention principles are introduced to address observed reward hacking patterns like self-praise and over-education. The entire pipeline requires only 31 human-defined principles and 6 ICL exemplars, eliminating the need for human preference collection during RL training.

## Key Results
- Dromedary-2 achieves MT-Bench score of 7.4, surpassing GPT-4 (7.4) and previous state-of-the-art models
- Big-Bench Hard performance reaches 51.4%, outperforming many larger models
- HumanEval score of 40.6% demonstrates strong code generation capabilities
- TruthfulQA achieves 98% truthful and informative responses, showing excellent alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Principle-following reward models enable flexible control over model preferences during RL without requiring online human preference collection
- Mechanism: The reward model is trained on synthetic preference data paired with human-defined principles, allowing it to generate reward scores based on arbitrary textual instructions rather than just identifying "better" responses
- Core assumption: The reward model can effectively learn to interpret and apply diverse human-defined principles from textual descriptions
- Evidence anchors:
  - [abstract] "Central to our approach is an instructable reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles."
  - [section 3.2] "Central to our approach is the introduction of the principle-following (also termed instruction-following) reward model. Pioneering in its nature, this reward model is adept at interpreting and adhering to arbitrary human-written preference guidelines, subsequently generating human-guided reward scores."

### Mechanism 2
- Claim: RL-time preference intervention principles can address reward hacking patterns by prohibiting specific undesirable behaviors
- Mechanism: During RL training, additional principles are introduced that explicitly prohibit behaviors like self-praise, over-education, or providing only high-level advice without concrete solutions
- Core assumption: The principle-following reward model can effectively learn to interpret prohibition instructions and apply them during reward scoring
- Evidence anchors:
  - [section 3.3] "To mitigate the aforementioned reward hacking tendencies, we manually compose an additional RL-time intervention principle for each pattern, respectively... we can re-use the same principle-following reward model, but steer its preference by defining prohibition instructions via natural language to deter the policy model from manifesting specific undesired behaviors."

### Mechanism 3
- Claim: Using both positive and negative principles in training improves the reward model's understanding of preferences and prohibitions
- Mechanism: For each positive principle, a corresponding negative principle is defined, and during training, principles are randomly negated to create diverse training instances
- Core assumption: The reward model can learn the distinction between what is desired and what is prohibited from the combination of positive and negative principles
- Evidence anchors:
  - [section 3.2] "We first define the corresponding negative principles for each positive principle to increase the diversity of these principles... Next, for each user prompt, a subset of principles is randomly sampled from the established principle list... with certain principles being randomly negated."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial as SALMON is a novel approach that aims to replace the traditional RLHF paradigm
  - Quick check question: What are the two main stages of RLHF and what data is required for each stage?

- Concept: Preference Modeling and Reward Modeling
  - Why needed here: SALMON introduces a new type of reward model that follows human-defined principles rather than just identifying "better" responses
  - Quick check question: How does a principle-following reward model differ from a traditional reward model in terms of input and output?

- Concept: Synthetic Data Generation for Training AI Models
  - Why needed here: SALMON relies heavily on synthetic preference data generated by AI systems following human-defined principles
  - Quick check question: What are the advantages and potential limitations of using synthetic data for training reward models compared to human-annotated data?

## Architecture Onboarding

- Component map:
  Principle-following Reward Model -> Policy Model -> Synthetic Preference Generator -> RL Trainer (PPO)

- Critical path:
  1. Collect unlabeled prompts
  2. Generate synthetic preferences using the initial policy model and principles
  3. Train principle-following reward model on synthetic preference data
  4. Perform RL training using the principle-following reward model with PPO
  5. Apply RL-time preference intervention principles to address reward hacking

- Design tradeoffs:
  - Using synthetic data vs. human-annotated data: SALMON trades potentially lower data quality for scalability and reduced human supervision
  - Number and complexity of principles: More principles provide finer control but may be harder for the reward model to interpret consistently
  - RL-time intervention vs. pre-defined principles: Intervention principles allow addressing observed issues but require manual identification of reward hacking patterns

- Failure signatures:
  - Reward model fails to interpret principles consistently → check training data diversity and principle clarity
  - Policy model exploits reward model vulnerabilities → check if RL-time intervention principles are effectively addressing observed reward hacking patterns
  - Model performance degrades on specific tasks → check if the principle set is missing important aspects for those tasks

- First 3 experiments:
  1. Train principle-following reward model on a small set of synthetic preferences and evaluate its ability to score responses based on different principles
  2. Perform RL training with a simple principle set and evaluate the policy model's adherence to those principles
  3. Introduce RL-time intervention principles to address a specific reward hacking pattern observed in the policy model and evaluate the effectiveness of the intervention

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the methodology:

- How does the SALMON approach scale when dealing with an increasingly large set of human-defined principles?
- How does the principle-following reward model handle conflicting principles, and what are the implications for the final AI behavior?
- What is the long-term impact of SALMON on the evolution of AI alignment techniques, and how might it influence future research directions?

## Limitations
- The evaluation relies heavily on automated benchmarks rather than direct human preference comparison
- The claim of "zero human preference collection" is somewhat misleading since human-written principles are still required
- The methodology assumes that synthetic preference data generated by the initial SFT model is sufficient for training an effective reward model
- RL-time intervention principles are manually crafted and may not generalize to all possible failure modes

## Confidence
- High confidence: The core SALMON methodology (principle-following reward model trained on synthetic preferences with RL-time intervention) is technically sound and the evaluation results show competitive performance on established benchmarks
- Medium confidence: The claim that this approach significantly reduces human supervision requirements is valid, though the actual reduction may be overstated since human-written principles still require careful crafting and validation
- Low confidence: The claim that this is a "paradigm shift" away from RLHF may be premature, as the fundamental approach of using reward modeling with RL still follows similar principles to existing methods, just with different types of input data

## Next Checks
1. Conduct a human evaluation study comparing Dromedary-2 outputs against other models on the same prompts, using the same 31 principles as evaluation criteria to verify that the model actually adheres to the intended principles in practice
2. Perform an ablation study removing the RL-time intervention principles to quantify their actual impact on preventing reward hacking patterns and whether they introduce any unintended constraints on helpfulness
3. Test the principle-following reward model's generalization by applying it to a completely different domain (e.g., medical or legal advice) with a new set of domain-specific principles to assess whether the approach scales beyond the original instruction following and conversational contexts