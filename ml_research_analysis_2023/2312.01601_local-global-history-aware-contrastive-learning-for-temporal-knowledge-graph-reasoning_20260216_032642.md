---
ver: rpa2
title: Local-Global History-aware Contrastive Learning for Temporal Knowledge Graph
  Reasoning
arxiv_id: '2312.01601'
source_url: https://arxiv.org/abs/2312.01601
tags:
- uni00000013
- historical
- query
- global
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of temporal knowledge graph (TKG)
  extrapolation, which involves predicting future facts based on historical patterns.
  Existing methods often fail to capture the importance of historical information
  relevant to queries and struggle with robustness when input data contains noise.
---

# Local-Global History-aware Contrastive Learning for Temporal Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2312.01601
- Source URL: https://arxiv.org/abs/2312.01601
- Reference count: 40
- Primary result: Outperforms state-of-the-art baselines on TKG extrapolation with significant improvements in MRR and Hits@1/3/10 metrics

## Executive Summary
This paper addresses temporal knowledge graph extrapolation by proposing a Local-Global History-aware Contrastive Learning model (LogCL) that captures both recent and long-term historical patterns. The model employs entity-aware attention mechanisms to selectively weight historical KG snapshots based on their relevance to queries, while incorporating local-global query contrastive learning to improve robustness against noise. Experimental results on four benchmark datasets demonstrate significant performance improvements over existing methods, with the model effectively capturing query-related historical information and showing strong resistance to noise interference.

## Method Summary
LogCL uses an encoder-decoder architecture with local and global entity-aware attention encoders processing recent and historical KG snapshots respectively. The local encoder employs R-GCN for spatial aggregation and GRU for temporal evolution across recent timestamps, while the global encoder samples two-hop historical query subgraphs for broader pattern capture. A contrastive learning module aligns representations from both encoders to improve robustness, with the final prediction made through a ConvTransE decoder combining local and global entity representations.

## Key Results
- Achieves state-of-the-art performance on ICEWS14, ICEWS18, ICEWS05-15, and GDELT datasets
- Significant improvements in MRR and Hits@1/3/10 metrics over baseline models
- Demonstrates strong resistance to noise through local-global query contrastive learning
- Effectively captures query-relevant historical patterns using entity-aware attention mechanism

## Why This Works (Mechanism)

### Mechanism 1
The entity-aware attention mechanism selectively weights KG snapshots based on their relevance to the query by computing attention scores that compare snapshot entity embeddings with query entity embeddings, giving higher weights to snapshots containing directly related entities.

### Mechanism 2
Local-global query contrastive learning improves robustness by creating two views of each query and applying supervised contrastive loss to align representations of the same query while separating different queries, helping filter out noise affecting only one view.

### Mechanism 3
Two-hop historical query subgraph sampling captures richer semantic patterns than one-hop sampling by including intermediate states and precursors that span multiple hops, providing predictive signal for complex repetitive events.

## Foundational Learning

- **Temporal knowledge graph representation learning**: Needed to embed entities and relations across time; quick check: How does the model handle entities appearing with different relationships across time snapshots?
- **Graph neural networks for relational data**: Required for R-GCN aggregation in local and global encoders; quick check: What's the difference between entity vs entity-relation pair aggregation in R-GCN?
- **Contrastive learning objectives**: Essential for the local-global query contrast module; quick check: How does temperature parameter τ affect contrastive loss and what happens when set too high or low?

## Architecture Onboarding

- **Component map**: Query → Local Encoder → Global Encoder → Contrast Module → Decoder → Prediction
- **Critical path**: The query flows through both local and global encoders, undergoes contrastive alignment, then combines for final prediction
- **Design tradeoffs**: Two-hop vs one-hop sampling (richer patterns vs computational cost), local-global weighting balance, contrastive learning temperature
- **Failure signatures**: Poor performance on sparse data queries, overfitting to frequent patterns, sensitivity to noise intensity
- **First 3 experiments**: 1) Ablation study removing entity-aware attention, 2) Vary local-global weighting parameter λ, 3) Test different R-GCN layer counts in global encoder

## Open Questions the Paper Calls Out

### Open Question 1
How does performance change with different GNN aggregation methods like CompGCN or KBAT instead of R-GCN? The paper mentions these alternatives but only provides limited experimental results.

### Open Question 2
How does performance change with different local-global query contrast strategies beyond the four tested (LogCL-gl, LogCL-lg, LogCL-gg, LogCL-ll)? The paper only explores a limited set of strategies.

### Open Question 3
How does performance change with different online learning strategies for time-variability? The paper mentions online learning but only compares limited settings.

## Limitations

- Lack of detailed ablation studies on the contrastive learning component's specific contribution to robustness
- Entity-aware attention effectiveness depends on embedding quality, which isn't extensively validated
- Two-hop sampling may introduce noise without proportional benefits on datasets with simple patterns

## Confidence

- High confidence: Overall model architecture and experimental results showing SOTA performance
- Medium confidence: Entity-aware attention mechanism's ability to distinguish relevant from irrelevant snapshots
- Low confidence: Specific contribution of contrastive learning to robustness without detailed ablation studies

## Next Checks

1. Conduct ablation study removing contrastive learning component to measure its specific impact on noise robustness
2. Evaluate entity-aware attention performance on queries with sparse historical data to test edge case handling
3. Test model with different R-GCN layer counts in global encoder to determine optimal configuration for historical pattern capture