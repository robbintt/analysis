---
ver: rpa2
title: 'LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of Student-LLM
  Interaction'
arxiv_id: '2310.05191'
source_url: https://arxiv.org/abs/2310.05191
tags:
- feedback
- essay
- score
- scores
- essays
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FABRIC, a pipeline for automated scoring and
  feedback generation for EFL writing. It addresses the need for specific, real-time
  feedback in writing education.
---

# LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of Student-LLM Interaction

## Quick Facts
- arXiv ID: 2310.05191
- Source URL: https://arxiv.org/abs/2310.05191
- Reference count: 40
- Key outcome: FABRIC pipeline improves automated scoring and feedback generation for EFL writing using rubric-based scoring, CASE augmentation, and EssayCoT prompting.

## Executive Summary
This paper presents FABRIC, an automated essay scoring and feedback generation pipeline for EFL writing education. The system addresses the need for specific, real-time feedback by combining rubric-based scoring with detailed feedback generation. FABRIC uses three major components: DREsS dataset (real-world rubric-scored essays), CASE (data augmentation through corruption-based synthetic data generation), and EssayCoT (Chain-of-Thought prompting strategy). The pipeline was evaluated by both experts and students, demonstrating significant improvements in feedback quality and student learning outcomes compared to baseline approaches.

## Method Summary
The FABRIC pipeline consists of three main components working in sequence. First, DREsS provides a real-world dataset of 1,782 essays scored on content, organization, and language rubrics. Second, CASE generates synthetic essay-score pairs by applying rubric-specific corruptions to well-written essays, improving model robustness. Third, an AES model (fine-tuned BERT) predicts rubric-based scores, which are then used by EssayCoT to generate detailed feedback through zero-shot prompting. The system was evaluated through expert assessments of feedback quality and a small-scale student study measuring learning outcomes and confidence improvements.

## Key Results
- CASE data augmentation improved AES model accuracy by 45.44% over baseline
- EssayCoT prompting generated feedback significantly more preferred and helpful than standard prompting according to 13 English education experts
- Students reported significant improvements in confidence and understanding of writing rubrics when using FABRIC-embedded platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CASE corruption-based augmentation improves AES model performance by introducing realistic sentence-level errors.
- Mechanism: CASE starts with well-written essays and applies rubric-specific corruptions (content: out-of-domain sentence substitution; organization: sentence swapping; language: ungrammatical sentence insertion) to generate synthetic essay-score pairs.
- Core assumption: Corruptions that reflect common student errors will make the AES model more robust to real-world variations.
- Evidence anchors:
  - [abstract]: "CASE, a Corruption-based Augmentation Strategy for Essays, with which we can improve the accuracy of the baseline model by 45.44%."
  - [section 3.1.3]: "CASE employs three rubric-specific strategies to augment the essay dataset with corruption, and training with CASE results in a model that outperforms the quadratic weighted kappa score of the baseline model by 26.37%."
  - [corpus]: Weak - only mentions 25 related papers, no direct evidence of CASE's effectiveness in corpus.
- Break condition: If synthetic corruptions do not match real student error patterns, the model may overfit to unrealistic data.

### Mechanism 2
- Claim: EssayCoT prompting leverages predicted scores to generate more helpful feedback than standard prompting.
- Mechanism: EssayCoT uses zero-shot prompting with the rubric explanations and the predicted rubric-based scores as rationale, instead of few-shot exemplars.
- Core assumption: Providing the model with its own predicted scores as reasoning will guide it to generate feedback aligned with those scores.
- Evidence anchors:
  - [abstract]: "EssayCoT, the Essay Chain-of-Thought prompting strategy which uses scores predicted from the AES model to generate better feedback."
  - [section 4.2]: "EssayCoT leverages essay scores automatically predicted by the AES model when generating feedback... Feedback with EssayCoT prompting is significantly more preferred and helpful compared to standard prompting, according to the assessment by 13 English education experts."
  - [corpus]: Weak - corpus evidence does not directly support the effectiveness of EssayCoT prompting.
- Break condition: If the AES model's score predictions are inaccurate, the feedback generated by EssayCoT may be misaligned with the essay's actual quality.

### Mechanism 3
- Claim: FABRIC's integration of rubric-based scoring and feedback generation addresses the need for specific, real-time feedback in EFL writing education.
- Mechanism: FABRIC combines DREsS (rubric-based AES dataset), CASE (data augmentation), and EssayCoT (feedback generation) into a pipeline that provides both scores and detailed feedback.
- Core assumption: Providing both rubric-based scores and specific feedback will improve student learning outcomes compared to holistic scores alone.
- Evidence anchors:
  - [abstract]: "FABRIC comprises three major contributions: DREsS, a real-world Dataset for Rubric-based Essay Scoring, CASE, a Corruption-based Augmentation Strategy for Essays, and EssayCoT, the Essay Chain-of-Thought prompting strategy for feedback generation."
  - [section 5]: "Students evaluated the performance of the AES model as well as the style and the quality of feedback generated as 6 out of 7... They reported confidence in their essay quality and understanding of each writing rubric significantly improved due to the engagement on our platform embedded with FABRIC."
  - [corpus]: Weak - corpus evidence does not directly measure student learning outcomes.
- Break condition: If the feedback is not actionable or relevant, students may not perceive it as helpful, reducing the pipeline's effectiveness.

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: QWK is the primary metric for evaluating the consistency between predicted and gold standard scores in AES tasks.
  - Quick check question: What does a QWK score of 1.0 represent, and what does a score of 0.0 represent?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting is the basis for EssayCoT, which uses predicted scores as reasoning to generate feedback.
  - Quick check question: How does zero-shot CoT differ from few-shot CoT, and why is it advantageous in the context of EssayCoT?

- Concept: Data Augmentation for NLP
  - Why needed here: CASE is a novel data augmentation strategy specifically designed for essays, addressing the scarcity of rubric-based AES data.
  - Quick check question: What are the potential risks of using synthetic data for training AES models, and how does CASE mitigate these risks?

## Architecture Onboarding

- Component map:
  DREsS dataset -> CASE augmentation -> AES model (BERT) -> EssayCoT prompting -> FABRIC pipeline

- Critical path:
  1. Collect and annotate essays to create DREsS dataset
  2. Apply CASE to generate synthetic data
  3. Train AES model on DREsS + synthetic data
  4. Use AES model to predict scores for new essays
  5. Generate feedback using EssayCoT with predicted scores
  6. Deploy FABRIC pipeline for student use

- Design tradeoffs:
  - Using synthetic data (CASE) vs. relying solely on real data
  - Zero-shot EssayCoT vs. few-shot prompting for feedback generation
  - Focusing on three specific rubrics vs. more granular scoring criteria

- Failure signatures:
  - Low QWK scores indicating poor AES model performance
  - Feedback perceived as unhelpful or irrelevant by experts/students
  - Feedback generation failing due to excessively long essays

- First 3 experiments:
  1. Train AES model on DREsS dataset alone and evaluate QWK scores
  2. Apply CASE augmentation and retrain AES model, comparing QWK scores
  3. Generate feedback using EssayCoT and standard prompting, comparing expert ratings on helpfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AES models trained on DREsS compare to those trained on other rubric-based datasets?
- Basis in paper: [explicit] The paper mentions that existing AES models do not provide specific rubric-based scores or feedback, and that the rubrics and criteria for scoring vary significantly among different datasets. The paper also states that the AES experiments were conducted on the combination of DREsS dataset and unified datasets (ASAP, ASAP++, and ICNALE EE).
- Why unresolved: The paper does not provide a direct comparison of the performance of AES models trained on DREsS to those trained on other rubric-based datasets.
- What evidence would resolve it: A comparison of the performance of AES models trained on DREsS to those trained on other rubric-based datasets would resolve this question.

### Open Question 2
- Question: How does the use of CASE impact the performance of AES models on different rubrics?
- Basis in paper: [explicit] The paper introduces CASE, a corruption-based augmentation strategy for essays, and states that training with CASE results in a model that outperforms the quadratic weighted kappa score of the baseline model by 26.37%. The paper also mentions that the optimal number of CASE operations per each rubric was explored in an ablation study.
- Why unresolved: The paper does not provide a detailed analysis of how the use of CASE impacts the performance of AES models on different rubrics.
- What evidence would resolve it: A detailed analysis of how the use of CASE impacts the performance of AES models on different rubrics would resolve this question.

### Open Question 3
- Question: How does the use of EssayCoT impact the quality of feedback generated for different rubrics?
- Basis in paper: [explicit] The paper introduces EssayCoT, a prompting strategy for feedback generation, and states that feedback with EssayCoT prompting is significantly more preferred and helpful compared to standard prompting, according to the assessment by 13 English education experts. The paper also mentions that the feedback quality was evaluated by each criterion (level of detail, accuracy, relevance, and helpfulness).
- Why unresolved: The paper does not provide a detailed analysis of how the use of EssayCoT impacts the quality of feedback generated for different rubrics.
- What evidence would resolve it: A detailed analysis of how the use of EssayCoT impacts the quality of feedback generated for different rubrics would resolve this question.

## Limitations
- Expert evaluation involved only 13 English education experts, limiting statistical power for feedback quality conclusions
- Student study had very small sample size (n=8) with no control group, making it difficult to attribute improvements specifically to FABRIC
- Results focus exclusively on short essay formats (KSET exams), limiting generalizability to other writing genres or longer compositions

## Confidence
High confidence: The effectiveness of CASE data augmentation in improving AES model performance (45.44% accuracy improvement claimed), the superiority of EssayCoT prompting over standard prompting for feedback generation (statistically significant preference by experts), and the overall pipeline architecture combining rubric-based scoring with CoT feedback generation.

Medium confidence: The claim that FABRIC significantly improves student learning outcomes and confidence, based on the limited student study with no control group and small sample size.

Low confidence: The generalizability of results to different writing contexts beyond the KSET exam format, and the long-term effectiveness of AI-generated feedback in writing education without ongoing human oversight.

## Next Checks
1. Conduct a randomized controlled trial with a larger student sample (nâ‰¥30) comparing FABRIC against both traditional human feedback and alternative AI feedback systems, measuring both immediate and delayed learning gains.

2. Evaluate FABRIC's performance on diverse writing genres (narrative, argumentative, research papers) and longer compositions to assess generalizability beyond the short essay format used in this study.

3. Implement an expert-in-the-loop evaluation where human instructors review and refine AI-generated feedback, measuring the impact on feedback quality and student outcomes compared to fully automated feedback.