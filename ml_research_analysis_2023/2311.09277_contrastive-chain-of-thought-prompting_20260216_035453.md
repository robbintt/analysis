---
ver: rpa2
title: Contrastive Chain-of-Thought Prompting
arxiv_id: '2311.09277'
source_url: https://arxiv.org/abs/2311.09277
tags:
- teeth
- reasoning
- thought
- language
- chain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving language model
  reasoning through chain-of-thought prompting. The authors propose a novel method
  called contrastive chain-of-thought prompting, which provides both valid and invalid
  reasoning demonstrations to guide the model in reducing reasoning mistakes.
---

# Contrastive Chain-of-Thought Prompting

## Quick Facts
- arXiv ID: 2311.09277
- Source URL: https://arxiv.org/abs/2311.09277
- Reference count: 8
- One-line primary result: 9.8-point improvement on GSM-8K and 16.0-point improvement on Bamboogle using GPT-3.5-Turbo

## Executive Summary
This paper addresses the challenge of improving language model reasoning through chain-of-thought prompting by introducing a novel method called contrastive chain-of-thought prompting. The approach provides both valid and invalid reasoning demonstrations to guide models in reducing reasoning mistakes. By introducing an automatic method to construct contrastive demonstrations through entity shuffling, the method achieves significant improvements on reasoning benchmarks compared to conventional chain-of-thought prompting.

## Method Summary
The method involves automatically generating contrastive demonstrations by shuffling object spans (numbers, equations, persons) in valid chain-of-thought rationales to create logically flawed but contextually relevant negative examples. These contrastive demonstrations are then combined with valid examples in prompts to guide language models like GPT-3.5-Turbo in distinguishing correct reasoning patterns from incorrect ones, reducing error propagation in intermediate steps.

## Key Results
- 9.8-point improvement on GSM-8K benchmark compared to standard chain-of-thought prompting
- 16.0-point improvement on Bamboogle benchmark with GPT-3.5-Turbo
- Substantial gains of more than 10 points on GSM-Hard, SVAMP, ASDIV, Bamboogle and StrategyQA tasks
- The method serves as a general enhancement applicable across different reasoning tasks

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Providing both valid and invalid reasoning demonstrations helps language models learn better step-by-step reasoning.
- Mechanism: The model learns to distinguish correct reasoning patterns from incorrect ones by contrasting valid and invalid demonstrations, reducing error propagation in intermediate steps.
- Core assumption: Language models can effectively learn from negative examples when they are presented in a contrastive manner alongside positive examples.
- Evidence anchors: [abstract]: "inspired by how humans can learn from both positive and negative examples"

### Mechanism 2
- Claim: Automatically generating contrastive demonstrations from existing valid rationales improves generalization.
- Mechanism: By randomly shuffling entities (numbers, equations, persons) in valid chain-of-thought rationales, the method creates incoherent object demonstrations that maintain task context while introducing reasoning errors.
- Core assumption: Shuffling entities within a valid chain-of-thought creates demonstrations that are contextually relevant but logically flawed, providing useful negative examples.
- Evidence anchors: [section]: "To obtain the incorrect reasoning steps Tâˆ’, we automatically construct it from the correct reasoning steps T+, based on the 'Incoherent Objects' category"

### Mechanism 3
- Claim: Contrastive chain-of-thought serves as a general enhancement applicable across different reasoning tasks.
- Mechanism: The method's task-agnostic design allows it to be combined with existing techniques like self-consistency, providing consistent performance improvements across arithmetic reasoning and factual QA tasks.
- Core assumption: The benefits of contrastive learning through valid/invalid demonstrations transfer across different types of reasoning tasks.
- Evidence anchors: [abstract]: "our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes"

## Foundational Learning
- Concept: In-context learning
  - Why needed here: Understanding how language models learn from demonstrations in prompts is fundamental to grasping why contrastive chain-of-thought works
  - Quick check question: How does providing multiple examples in a prompt help a language model perform a new task?

- Concept: Chain-of-thought reasoning
  - Why needed here: The mechanism relies on decomposing problems into intermediate reasoning steps, which is the core of chain-of-thought prompting
  - Quick check question: What is the difference between standard prompting and chain-of-thought prompting?

- Concept: Contrastive learning
  - Why needed here: The method is inspired by contrastive learning approaches that use positive and negative samples to improve model performance
  - Quick check question: How does contrastive learning differ from traditional supervised learning approaches?

## Architecture Onboarding
- Component map: Language model (GPT-3.5-Turbo) -> Prompt generator (creates contrastive demonstrations) -> Evaluation framework (measures performance across reasoning tasks)
- Critical path: 1) Generate valid chain-of-thought demonstrations, 2) Automatically create invalid demonstrations by shuffling entities, 3) Construct contrastive prompt with both valid and invalid examples, 4) Evaluate model performance
- Design tradeoffs: Manual vs. automatic generation of invalid demonstrations (automatic saves annotation cost but may produce less targeted negative examples)
- Failure signatures: Performance improvements plateau or decrease when invalid demonstrations are too dissimilar from valid ones, or when entity shuffling breaks task context
- First 3 experiments:
  1. Implement the automatic entity shuffling method on a small dataset and verify that generated invalid demonstrations are contextually relevant but logically flawed
  2. Test contrastive chain-of-thought against standard chain-of-thought on GSM-8K with 4-shot prompting to verify performance improvements
  3. Evaluate whether the method benefits from self-consistency decoding by comparing contrastive CoT with and without self-consistency on AQuA dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content and inferred gaps, the following questions remain unresolved:

### Open Question 1
- Question: How does the effectiveness of contrastive chain-of-thought prompting vary across different types of language models (e.g., different architectures, sizes, or training objectives)?
- Basis in paper: [inferred] The paper focuses on GPT-3.5-Turbo and does not explore the performance across different language model architectures or sizes.
- Why unresolved: The paper only uses GPT-3.5-Turbo for its experiments, so it's unclear how the method would perform with other models like smaller models or models with different training objectives.
- What evidence would resolve it: Experiments testing contrastive chain-of-thought on a variety of language models with different architectures and sizes would provide insights into its generalizability.

### Open Question 2
- Question: Can contrastive chain-of-thought prompting be effectively applied to multimodal tasks that require reasoning across different types of inputs (e.g., text, images, or audio)?
- Basis in paper: [inferred] The paper focuses on text-based reasoning tasks and does not explore multimodal applications.
- Why unresolved: The method's effectiveness in handling multimodal inputs is not tested, so it's unclear if the approach can be extended to tasks involving images, audio, or other non-text data.
- What evidence would resolve it: Testing contrastive chain-of-thought on multimodal datasets and tasks would determine its applicability beyond text-based reasoning.

### Open Question 3
- Question: What is the impact of contrastive chain-of-thought prompting on the computational efficiency and inference time of language models?
- Basis in paper: [inferred] The paper does not discuss the computational costs or inference time associated with the method.
- Why unresolved: While the method shows performance improvements, it is not clear how it affects the computational resources required for inference.
- What evidence would resolve it: Measuring the inference time and computational costs of contrastive chain-of-thought compared to standard methods would provide insights into its efficiency.

## Limitations
- The automatic generation of invalid demonstrations through entity shuffling may not generalize well to complex reasoning tasks requiring nuanced understanding
- The method's effectiveness likely varies substantially across different reasoning domains, with simple arithmetic benefiting more than complex logical reasoning
- The paper does not address how the approach scales to larger models or more complex reasoning chains

## Confidence
- **High Confidence**: The core observation that contrastive learning can improve chain-of-thought prompting is well-supported by empirical results
- **Medium Confidence**: The automatic generation of invalid demonstrations through entity shuffling is likely effective for simple arithmetic reasoning tasks but may not generalize well to more complex reasoning domains
- **Low Confidence**: The claim that this approach serves as a general enhancement applicable across all reasoning tasks is not fully substantiated

## Next Checks
1. Test the method on diverse reasoning tasks beyond arithmetic and factual QA, including logical reasoning, commonsense reasoning, and multi-hop reasoning tasks to assess generalizability across reasoning domains.

2. Conduct a systematic evaluation of the automatically generated invalid demonstrations to quantify their quality and relevance, measuring how often entity shuffling creates demonstrations that are too nonsensical versus appropriately flawed.

3. Perform controlled experiments varying the ratio of valid to invalid demonstrations, the ordering of examples in the prompt, and the specificity of the contrastive framing to determine the optimal configuration for different task types and model sizes.