---
ver: rpa2
title: Learning Sample Difficulty from Pre-trained Models for Reliable Prediction
arxiv_id: '2304.10127'
source_url: https://arxiv.org/abs/2304.10127
tags:
- sample
- culty
- samples
- pre-trained
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using pre-trained models to quantify sample
  difficulty in downstream training sets and incorporating this knowledge into entropy
  regularization for reliable prediction. The core idea is to model the training data
  distribution in the feature space of pre-trained models using Gaussian distributions,
  and derive a relative Mahalanobis distance (RMD) as a sample difficulty score.
---

# Learning Sample Difficulty from Pre-trained Models for Reliable Prediction

## Quick Facts
- arXiv ID: 2304.10127
- Source URL: https://arxiv.org/abs/2304.10127
- Reference count: 20
- Primary result: Improves accuracy (+0.55% ACC) and uncertainty calibration (-3.7% ECE) compared to competitive baselines

## Executive Summary
This paper proposes using large-scale pre-trained models to quantify sample difficulty in downstream training sets and incorporating this knowledge into entropy regularization for reliable prediction. The core idea is to model training data distribution in the feature space of pre-trained models using Gaussian distributions, and derive a relative Mahalanobis distance (RMD) as a sample difficulty score. This score is then used to adaptively penalize overconfident predictions based on sample difficulty. Experiments on CIFAR and ImageNet datasets show that the proposed method improves both accuracy and uncertainty calibration compared to competitive baselines.

## Method Summary
The method extracts features from a pre-trained model (CLIP-ViT-B) for all training samples, then computes class-conditional and class-agnostic Gaussian distributions in feature space. The relative Mahalanobis distance (RMD) between these distributions serves as a sample difficulty score. During training, a sample difficulty-aware entropy regularization term is added to the cross-entropy loss, with the regularization weight being proportional to the sample's RMD score. This allows the model to penalize overconfidence more heavily on difficult samples while allowing confident predictions on easy samples.

## Key Results
- Improves top-1 accuracy by 0.55% compared to competitive baselines
- Reduces Expected Calibration Error (ECE) by 3.7%
- Enhances selective classification and out-of-distribution detection capabilities
- Outperforms methods like Label Smoothing, Focal Loss, and conventional Entropy Regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale pre-trained models avoid overfitting the downstream training classes while still encoding rich semantic features, enabling effective sample difficulty scoring.
- Mechanism: Pre-trained models trained on large-scale diverse datasets learn holistic data representations that preserve high-level concepts rather than low-level statistics. When used for feature extraction on downstream tasks, these features retain class-discriminative information without memorizing specific training classes.
- Core assumption: Pre-trained models trained on diverse data with self-supervised or multi-modal objectives learn representations that generalize beyond the specific downstream task classes.
- Evidence anchors:
  - [abstract] "Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample's difficulty"
  - [section] "As modern pre-trained models are trained on large-scale datasets with high sample diversities in many dimensions, they learn to preserve and structure richer semantic features of the training samples than models only exposed to the training set"
- Break condition: If pre-trained models overfit to downstream classes or fail to capture meaningful semantic features, the difficulty scoring would become unreliable.

### Mechanism 2
- Claim: The relative Mahalanobis distance (RMD) effectively captures sample difficulty by comparing class-conditional and class-agnostic feature distributions.
- Mechanism: RMD measures how far a sample is from its class mean relative to the overall population mean. Easy samples have typical class features (small class-conditional MD) but are distinct from other classes (large class-agnostic MD), yielding small RMD. Hard samples have atypical or ambiguous features, resulting in larger RMD.
- Core assumption: Gaussian modeling in feature space accurately represents the data distribution, and the difference between class-conditional and class-agnostic distances meaningfully captures difficulty.
- Evidence anchors:
  - [abstract] "derive a relative Mahalanobis distance (RMD) as a sample difficulty score"
  - [section] "we propose to use the difference between the Mahalanobis distances respectively induced by the class-specific and class-agnostic Gaussian distribution"
  - [section] "Fig. 1 showed that the RMD score increases proportionally with the severity of corruption and label noise"
- Break condition: If feature distributions deviate significantly from Gaussian assumptions or if the class-agnostic model fails to capture meaningful inter-class variation, RMD would lose discriminative power.

### Mechanism 3
- Claim: Sample difficulty-aware entropy regularization improves both accuracy and calibration by adaptively penalizing overconfidence on hard samples.
- Mechanism: The entropy regularizer encourages uncertainty in predictions, with weighting proportional to sample difficulty. Easy samples receive less regularization (allowing confident correct predictions), while hard samples receive more regularization (preventing overconfidence on uncertain cases).
- Core assumption: The relationship between sample difficulty and appropriate confidence levels is monotonic, and adaptive weighting improves upon constant regularization.
- Evidence anchors:
  - [abstract] "by adaptively penalizing overconfident prediction based on the sample difficulty, we simultaneously improve accuracy and uncertainty calibration"
  - [section] "we propose to regularize the cross-entropy loss with a sample difficulty-aware entropic regularizer"
  - [section] "Compared to conventional ER with a constant weighting, our sample difficulty-aware instance-wise weighting leads to much more pronounced gains"
- Break condition: If the sample difficulty scoring becomes inaccurate or if the monotonic relationship between difficulty and appropriate confidence breaks down, the adaptive regularization could harm performance.

## Foundational Learning

- Concept: Gaussian distribution modeling
  - Why needed here: Forms the basis for computing Mahalanobis distances to quantify sample difficulty in feature space
  - Quick check question: What assumption about the feature distribution enables Mahalanobis distance to be meaningful?

- Concept: Entropy regularization
  - Why needed here: Provides the mechanism to penalize overconfidence in predictions, with sample difficulty determining the strength
  - Quick check question: How does entropy regularization differ from standard cross-entropy in terms of confidence penalization?

- Concept: Mahalanobis distance
  - Why needed here: Measures distance in feature space accounting for feature correlations and variances, enabling meaningful comparison of sample difficulty
  - Quick check question: Why is Mahalanobis distance preferred over Euclidean distance for measuring sample difficulty in feature space?

## Architecture Onboarding

- Component map: Pre-trained feature extractor → Feature space Gaussian modeling → RMD computation → Sample difficulty-aware loss function → Downstream classifier training
- Critical path: The RMD computation must be performed once before training begins, then the sample difficulty scores are used throughout training to weight the entropy regularization term
- Design tradeoffs: Using pre-trained models adds computational overhead for feature extraction but enables more accurate difficulty scoring; simpler methods like K-means clustering are faster but less effective
- Failure signatures: Poor calibration despite using this method could indicate issues with pre-trained model selection or Gaussian modeling assumptions; accuracy degradation might suggest inappropriate regularization strength
- First 3 experiments:
  1. Verify RMD correlates with human-perceived difficulty on a small validation set
  2. Test different pre-trained models (CLIP vs MAE vs supervised) for feature extraction quality
  3. Compare constant vs adaptive entropy regularization on a simple dataset to confirm the adaptive approach improves calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of pre-trained models for scoring sample difficulty vary across different downstream tasks beyond image classification, such as object detection or semantic segmentation?
- Basis in paper: [inferred] The paper focuses on image classification and mentions that pre-trained models do not overfit downstream training classes, enabling effective sample difficulty measurement. However, it does not explore other tasks.
- Why unresolved: The paper's experiments are limited to image classification benchmarks, leaving the applicability of the approach to other computer vision tasks unexplored.
- What evidence would resolve it: Empirical results showing the performance of the proposed method on object detection, semantic segmentation, or other vision tasks, comparing it with task-specific baselines.

### Open Question 2
- Question: Can the sample difficulty measure derived from pre-trained models be effectively transferred across different domains, such as from natural images to medical imaging or satellite imagery?
- Basis in paper: [inferred] The paper demonstrates that pre-trained models like CLIP, trained on large-scale multi-modal data, outperform models trained on ImageNet-like data. However, it does not test cross-domain transferability.
- Why unresolved: The experiments are conducted within the same domain (natural images), and there is no analysis of how well the sample difficulty measure generalizes to other domains.
- What evidence would resolve it: Experiments applying the method to medical or satellite imagery datasets, evaluating accuracy and calibration improvements compared to domain-specific approaches.

### Open Question 3
- Question: How sensitive is the proposed method to the choice of the pre-trained model architecture, and is there an optimal architecture for sample difficulty measurement?
- Basis in paper: [explicit] The paper compares four pre-trained models (CLIP-ViT-B, CLIP-R50, MAE-ViT-B, ViT-B) and finds CLIP-based models perform best, but does not systematically analyze the impact of architectural choices.
- Why unresolved: While the paper shows CLIP models are superior, it does not explore why or whether other architectures could perform better under different conditions.
- What evidence would resolve it: A comprehensive study varying model architectures (e.g., ResNet, Swin Transformer, Vision MLP) and analyzing their impact on sample difficulty measurement and downstream performance.

### Open Question 4
- Question: What is the computational overhead of using pre-trained models for sample difficulty scoring during training, and how does it scale with dataset size and model complexity?
- Basis in paper: [inferred] The paper emphasizes computational efficiency compared to ensemble methods but does not provide detailed analysis of the overhead introduced by pre-trained model feature extraction.
- Why unresolved: The paper does not report runtime or memory usage metrics for the pre-trained model feature extraction step, leaving scalability concerns unaddressed.
- What evidence would resolve it: Benchmarking the time and memory requirements of the proposed method across different dataset sizes and pre-trained model complexities, comparing it with standard training pipelines.

## Limitations

- The method relies heavily on the quality of pre-trained feature representations and Gaussian modeling assumptions in feature space
- Computational overhead of feature extraction and covariance matrix computation could limit scalability to very large datasets
- Assumes a monotonic relationship between sample difficulty and appropriate confidence levels, which may not hold for all datasets

## Confidence

- **High confidence**: The core mechanism of using pre-trained features for difficulty scoring and the empirical improvements in accuracy and calibration (ACC +0.55%, ECE -3.7%) are well-supported by the results.
- **Medium confidence**: The Gaussian modeling assumptions and the specific form of RMD as a difficulty metric, while intuitive, lack extensive theoretical justification and may not generalize to all feature distributions.
- **Medium confidence**: The adaptive entropy regularization mechanism shows clear empirical benefits, but the optimal parameterization (α, T) may be dataset-dependent and requires careful tuning.

## Next Checks

1. **Ablation on pre-trained model choice**: Compare CLIP-ViT-B with other pre-trained models (MAE, supervised ImageNet models) to assess the sensitivity of RMD scoring to feature representation quality.
2. **Robustness to distribution shifts**: Evaluate the method's performance on out-of-distribution samples from different corruption types and severities to test the generalizability of the difficulty scoring.
3. **Analysis of Gaussian modeling assumptions**: Conduct experiments to verify that feature distributions are approximately Gaussian and that RMD meaningfully captures difficulty across different feature dimensions and datasets.