---
ver: rpa2
title: Explaining Competitive-Level Programming Solutions using LLMs
arxiv_id: '2307.05337'
source_url: https://arxiv.org/abs/2307.05337
tags:
- solution
- array
- problem
- test
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating natural language explanations
  for competitive programming solutions using large language models (LLMs). The core
  method is a specific-to-general explanation generation approach that produces structured
  explanations from detailed code analysis to high-level understanding.
---

# Explaining Competitive-Level Programming Solutions using LLMs

## Quick Facts
- arXiv ID: 2307.05337
- Source URL: https://arxiv.org/abs/2307.05337
- Reference count: 40
- LLMs can generate structured explanations for competitive programming solutions that improve automated problem-solving performance from 6.1% to 42.4% solve@10 rate.

## Executive Summary
This paper presents a method for automatically generating natural language explanations for competitive programming solutions using large language models. The approach uses a specific-to-general explanation generation methodology that produces structured explanations covering problem understanding, key insights, implementation details, and correctness proofs. Experiments on the CodeContests dataset show that both GPT-3.5 and GPT-4 can generate useful explanations, with GPT-4 demonstrating superior analytical capabilities. The automatically generated explanations significantly improve LLM problem-solving performance when used as hints during the solving process.

## Method Summary
The method uses zero-shot prompting with LLMs to generate structured explanations for competitive programming problems. Given a problem statement and its oracle solution, the LLM produces a hierarchical explanation with seven distinct points ranging from problem summary to correctness proof. These explanations are then used to guide another LLM in solving new problems. The approach leverages the dual capabilities of LLMs - detailed code analysis and abstract reasoning - to break down complex problem-solving into manageable steps that can be used as hints without directly revealing the solution.

## Key Results
- GPT-4 generates explanations with 0.68-0.88 higher human ratings than GPT-3.5 on understanding and analysis aspects
- Explanation-guided solving improves solve@10 rate from 6.1% to 42.4% compared to direct solving
- Different explanation components (algorithm choice, complexity analysis, corner cases) contribute varying levels of improvement
- Explanations are more helpful for easier problems than harder ones, with diminishing returns at higher difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate structured, multi-level explanations that capture both low-level code implementation details and high-level problem-solving insights. The specific-to-general explanation generation method leverages the LLM's dual capabilities - detailed code analysis and abstract reasoning - by prompting for a structured output with seven distinct points ranging from problem summary to correctness proof.

### Mechanism 2
Providing structured explanations as hints improves LLM problem-solving performance by breaking down the reasoning process into manageable steps. The explanation-guided solver approach gives the LLM access to distilled reasoning steps that guide implementation without directly revealing the code.

### Mechanism 3
GPT-4 demonstrates superior analytical capabilities compared to GPT-3.5, particularly for Analysis-level explanations. GPT-4's larger model size and training allows it to better understand the "key idea behind the solution" and provide more insightful correctness proofs and complexity analysis.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Competitive programming requires multi-step reasoning from problem understanding to algorithm selection to implementation, which mirrors the chain-of-thought approach
  - Quick check question: Can you explain how chain-of-thought prompting differs from direct question-answering in terms of intermediate steps?

- Concept: Program synthesis evaluation metrics
  - Why needed here: The paper uses pass@k metrics to evaluate generated programs, which requires understanding how to properly assess code generation systems
  - Quick check question: What's the difference between pass@1 and pass@10, and why might a system use multiple samples?

- Concept: Structured output generation
  - Why needed here: The explanation format requires generating specific sections in a predefined order, which is a form of constrained generation
  - Quick check question: How would you design a prompt to ensure an LLM generates output in a specific JSON format with required fields?

## Architecture Onboarding

- Component map: Problem Repository -> Explainer Module -> Explanation -> Solver Module -> Generated Program -> Evaluation System -> Result
- Critical path: Problem → Explainer → Explanation → Solver → Generated Program → Evaluation → Result
- Design tradeoffs: Using GPT-3.5 for both explainer and solver balances cost and capability, but limits the maximum possible explanation quality compared to using GPT-4 for both
- Failure signatures: 
  - Low explanation scores indicate the Explainer is not capturing key insights
  - Low solve rates even with explanations suggest the Solver cannot effectively use the hints
  - High public pass but low solve rates indicate the generated programs are inefficient
- First 3 experiments:
  1. Run the baseline solver without explanations on 10 random problems to establish baseline performance
  2. Generate explanations for the same 10 problems using GPT-3.5 and evaluate human ratings
  3. Run the solver with explanations (using different description points) and measure solve rate improvement

## Open Questions the Paper Calls Out

1. Can LLM-generated explanations be used to improve problem-solving on novel problems without guidance from human solutions? The paper discusses this as a potential future research direction, noting that while explanations help with problems similar to those used for generating the explanations, it's unclear if they can help with truly novel problems.

2. How do different sampling strategies for generating multiple programs affect the overall solve rate? The paper compares three sampling strategies but doesn't deeply analyze why certain strategies work better or explore intermediate strategies.

3. What are the key differences in reasoning capabilities between GPT-3.5 and GPT-4 for competitive programming problems? The paper notes that GPT-4 performs better at analysis-level explanations but doesn't characterize the specific reasoning capabilities that differ.

4. How does the difficulty of competitive programming problems affect the usefulness of explanations? The paper shows that explanations help more with easier problems than harder ones, but doesn't analyze why certain aspects of explanations break down at higher difficulty levels.

## Limitations

- The human evaluation involved only 21 problems annotated by solution authors, providing a relatively small sample for drawing broad conclusions about explanation quality
- The performance gap between GPT-3.5 and GPT-4 suggests model capability matters substantially, but with only two models tested, we cannot determine whether this represents a fundamental scaling relationship
- The reliance on the CodeContests dataset limits generalizability to other programming domains or problem types
- The zero-shot prompting approach, while practical, may not be optimal compared to few-shot or fine-tuned alternatives

## Confidence

- **High confidence**: The automatic generation of structured explanations is technically feasible using current LLMs
- **Medium confidence**: The specific-to-general explanation format effectively captures both low-level and high-level reasoning
- **Medium confidence**: Explanation-guided solving improves performance compared to direct solving, though the magnitude may vary with problem complexity
- **Low confidence**: The relative performance ranking between GPT-3.5 and GPT-4 generalizes beyond the tested problems

## Next Checks

1. **Scaling experiment**: Test the explanation generation approach across multiple model sizes (including open-source alternatives like LLaMA-2 and CodeLlama) to determine whether the performance improvements follow a predictable scaling law or depend on specific architectural features.

2. **Human evaluation expansion**: Conduct a larger-scale human evaluation with diverse annotators (not just solution authors) and include explanation comprehension questions to verify that the generated descriptions actually improve human understanding, not just automated solving.

3. **Cross-domain transfer**: Apply the explanation generation method to a different programming domain (such as data science or systems programming) to assess whether the hierarchical structure and prompting approach generalizes beyond competitive programming problems.