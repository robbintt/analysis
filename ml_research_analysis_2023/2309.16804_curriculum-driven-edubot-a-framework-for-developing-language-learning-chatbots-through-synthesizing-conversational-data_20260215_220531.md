---
ver: rpa2
title: 'Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots
  Through Synthesizing Conversational Data'
arxiv_id: '2309.16804'
source_url: https://arxiv.org/abs/2309.16804
tags:
- edubot
- chatbot
- chatgpt
- conversation
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a curriculum-driven chatbot framework, EduBot,
  that uses large language models to synthesize conversation data from English textbooks
  and fine-tunes an open-source model for interactive language learning. EduBot outperforms
  ChatGPT in leading curriculum-aligned dialogues and adapting to users' English proficiency,
  with 75% of students finding it especially adept at guiding conversations and 87.5%
  believing it helps improve conversational skills.
---

# Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data

## Quick Facts
- arXiv ID: 2309.16804
- Source URL: https://arxiv.org/abs/2309.16804
- Reference count: 40
- Primary result: EduBot outperforms ChatGPT in curriculum-aligned dialogues, with 75% of students finding it adept at guiding conversations and 87.5% believing it improves conversational skills

## Executive Summary
This paper introduces EduBot, a curriculum-driven chatbot framework that leverages large language models to synthesize conversation data from English textbooks and fine-tune an open-source model for interactive language learning. The framework generates synthetic dialogues incorporating curriculum topics, diverse personas, and textbook vocabulary, then fine-tunes Vicuna-13B on this data to create a chatbot that outperforms ChatGPT in leading curriculum-based dialogues and adapting to users' English proficiency levels. A small-scale user study with 8 participants showed positive perceptions of EduBot's effectiveness in guiding conversations and improving conversational skills.

## Method Summary
The EduBot framework extracts topics from English textbooks and generates related subtopics using ChatGPT, then creates diverse personas for dialogue participants. It synthesizes dialogues incorporating textbook vocabulary and topics, fine-tunes Vicuna-13B on these synthetic dialogues, and deploys the chatbot with curriculum-specific prompts. The method involves curriculum topic extraction, persona generation, synthetic dialogue synthesis, LLM fine-tuning, and chatbot deployment with prompts that include persona, topic, vocabulary, and CEFR level information.

## Key Results
- EduBot outperforms ChatGPT in leading curriculum-based dialogues
- EduBot adapts its dialogue to match user's English proficiency level
- 75% of students found EduBot especially adept at guiding conversations
- 87.5% of students believed EduBot helped improve conversational skills

## Why This Works (Mechanism)

### Mechanism 1
The EduBot outperforms ChatGPT because it was fine-tuned on synthetic dialogues that include fixed-format personas, curriculum topics, and textbook-based vocabularies. By synthesizing dialogues that reflect the specific characteristics of Chinese college students and the structure of the English curriculum, the EduBot learns to guide conversations in a way that is more engaging and contextually appropriate for its target users. This works under the assumption that the synthetic data accurately represents the types of conversations that would occur between a language learning chatbot and a Chinese college student.

### Mechanism 2
The EduBot is better suited to the user's English proficiency level because it was fine-tuned on dialogues that incorporate the vocabulary from the textbook and is designed to maintain a conversation difficulty level consistent with the curriculum. By including vocabulary words from the textbook in the synthetic dialogues and controlling the English proficiency level of the chatbot's responses, the EduBot can provide language practice that is challenging but not overwhelming for the user. This assumes that the textbook vocabulary and the user's English proficiency level are well-matched.

### Mechanism 3
The EduBot's conversations are more natural and realistic compared to ChatGPT because it was fine-tuned on synthetic dialogues that emulate real-life conversations of Chinese college students. By including diverse personas and background information in the synthetic dialogues, the EduBot learns to provide responses that are more personal, engaging, and contextually appropriate for its target users. This works under the assumption that the synthetic data accurately represents the types of conversations that would occur between Chinese college students.

## Foundational Learning

- **Large Language Models (LLMs)**: Understanding how LLMs work is crucial for understanding the EduBot's capabilities and limitations. Quick check: What is the main difference between a general-purpose LLM and a fine-tuned LLM?

- **Curriculum Design**: Understanding the principles of curriculum design is important for understanding how the EduBot is structured and how it can support language learning. Quick check: What are the key components of a well-designed language learning curriculum?

- **Data Synthesis**: Understanding the principles of data synthesis is important for understanding how the EduBot is trained and how it can be improved. Quick check: What are the main challenges in synthesizing high-quality conversational data for chatbot training?

## Architecture Onboarding

- **Component map**: Data Synthesis Pipeline -> LLM Fine-tuning -> Chatbot Deployment
- **Critical path**: Data Synthesis Pipeline -> LLM Fine-tuning -> Chatbot Deployment
- **Design tradeoffs**: The EduBot prioritizes curriculum alignment and user engagement over the breadth of knowledge that a general-purpose LLM like ChatGPT can provide
- **Failure signatures**: The EduBot may struggle to handle topics or vocabulary that are not covered in the curriculum, or it may not be able to provide accurate information on topics outside its training data
- **First 3 experiments**:
  1. Evaluate the EduBot's performance on a set of curriculum-aligned conversation topics
  2. Compare the EduBot's responses to those of ChatGPT on a set of questions related to the curriculum
  3. Test the EduBot's ability to incorporate vocabulary words from the curriculum into its responses

## Open Questions the Paper Calls Out

### Open Question 1
How does the persona diversity in the synthetic dialogues impact the long-term engagement and learning outcomes of students using EduBot? The paper discusses the use of diverse personas to enrich conversational context and mentions that EduBot was able to showcase its assigned personas during conversations. However, it does not provide empirical evidence on how this diversity impacts long-term engagement or learning outcomes. A longitudinal study measuring student engagement and learning outcomes over an extended period, comparing EduBot with and without diverse personas, would provide insights into the impact of persona diversity.

### Open Question 2
To what extent does the incorporation of curriculum-specific vocabulary in dialogues enhance students' retention and usage of new vocabulary in real-life conversations? The paper describes the process of incorporating vocabulary from the textbook into synthetic dialogues and notes that EduBot's conversations align better with students' English proficiency levels. Although the paper highlights the alignment of vocabulary with students' proficiency levels, it does not investigate whether this alignment leads to improved retention or practical usage of vocabulary outside the chatbot interactions. An experimental study comparing vocabulary retention and usage in real-life conversations between students who used EduBot and those who did not, focusing on the vocabulary introduced in the dialogues, would resolve this question.

### Open Question 3
How does the performance of EduBot compare with other advanced language models, such as GPT-4, in terms of leading curriculum-based dialogues and adapting to users' English proficiency levels? The paper uses ChatGPT as a baseline for comparison and notes that EduBot outperforms it in several aspects, but does not mention comparisons with other advanced models like GPT-4. The paper only compares EduBot with ChatGPT, leaving a gap in understanding how it performs against other state-of-the-art models. Conducting a comparative study involving multiple advanced language models, including GPT-4, to evaluate their performance in leading curriculum-based dialogues and adapting to users' English proficiency levels would resolve this question.

## Limitations

- Small sample size (8 participants) limits generalizability of results
- Evaluation methodology relies heavily on subjective user feedback rather than objective performance metrics
- Paper does not address potential biases in synthetic data generation or generalization beyond the specific textbook curriculum

## Confidence

**High Confidence:** The technical approach of using curriculum-aligned synthetic data for fine-tuning is methodologically sound and the basic implementation appears reproducible.

**Medium Confidence:** The claim that EduBot outperforms ChatGPT in curriculum-based dialogues has moderate support, though the evaluation methodology limits confidence.

**Low Confidence:** The claim that 75% of students found EduBot "especially adept at guiding conversations" and that 87.5% believed it helps improve conversational skills lacks statistical rigor given the tiny sample size.

## Next Checks

1. Scale the user study to include at least 50-100 participants across different proficiency levels, with randomized assignment to EduBot vs. ChatGPT conditions, measuring both objective (vocabulary retention, grammar usage) and subjective (engagement, perceived helpfulness) metrics.

2. Benchmark against multiple baselines including not just ChatGPT but other educational chatbots, using standardized language proficiency tests to measure actual learning outcomes rather than just user perception.

3. Test curriculum generalization by evaluating EduBot's performance on textbook units not included in training data, and assess its ability to handle vocabulary and topics outside the original curriculum scope.