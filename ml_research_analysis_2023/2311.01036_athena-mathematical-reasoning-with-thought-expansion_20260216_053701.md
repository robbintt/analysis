---
ver: rpa2
title: 'ATHENA: Mathematical Reasoning with Thought Expansion'
arxiv_id: '2311.01036'
source_url: https://arxiv.org/abs/2311.01036
tags:
- thoughts
- thought
- athena
- math
- unbiasedmwp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ATHENA, an attention-based thought expansion
  network for solving math word problems. ATHENA mimics human thought expansion mechanisms
  by recurrently generating candidate math expressions from previous steps and selecting
  valid pathways to the goal.
---

# ATHENA: Mathematical Reasoning with Thought Expansion

## Quick Facts
- arXiv ID: 2311.01036
- Source URL: https://arxiv.org/abs/2311.01036
- Reference count: 17
- Primary result: State-of-the-art performance on standard MWP benchmarks with strong generalization to unseen questions

## Executive Summary
ATHENA introduces an attention-based thought expansion network for solving math word problems by mimicking human reasoning processes. The model recurrently generates candidate math expressions from previous thoughts and filters them to find valid pathways to the solution. Experiments demonstrate superior performance on standard benchmarks and strong generalization when training examples are limited, attributed to its thought expansion mechanism and mathematical knowledge leveraging.

## Method Summary
ATHENA uses a RoBERTa-based PLM to embed problem sequences and extract initial thoughts (quantity embeddings). It expands thoughts through merge (addition/multiplication) and transform (inverse operations) layers to generate candidate thoughts. An inference layer filters candidates using premise vectors and goal vectors via multi-head attention. The premise vector is updated at each expansion depth by concatenating reasonable thoughts, and the final answer is selected based on the highest correlation with the goal vector.

## Key Results
- Achieves new state-of-the-art performance on total benchmarks with 3.84% relative improvement over DeductReasoner
- Demonstrates 7.26% average performance gain on all benchmarks compared to 4.6% for DeductReasoner
- Shows strong generalization to variant questions and robust performance under restricted training informativeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thought expansion through candidate generation and filtering improves generalization over fixed reasoning paths
- Core assumption: Mathematical operations can be decomposed into merge/transform steps, and premise-based filtering effectively prunes invalid reasoning paths
- Evidence anchors: Abstract and section describing thought expansion; corpus neighbors focus on multimodal reasoning not thought expansion
- Break condition: Premise vector update failure could discard valid paths or retain invalid ones

### Mechanism 2
- Claim: Premise vector updates stabilize reasoning across expansion depths
- Core assumption: Updated premise vectors accumulate contextual knowledge for more effective candidate filtering
- Evidence anchors: Abstract showing performance under restricted training; section describing premise updates; corpus lacks direct evidence
- Break condition: Premise updates overfitting to training patterns could harm generalization on unseen contexts

### Mechanism 3
- Claim: Larger PLM sizes improve ATHENA efficiency
- Core assumption: Increased model capacity directly improves candidate generation and filtering quality
- Evidence anchors: Abstract on restricted training performance; section showing 3.84% improvement over DeductReasoner; corpus lacks direct evidence
- Break condition: Overparameterization without proportional reasoning depth gains could plateau or degrade performance

## Foundational Learning

- Concept: Merge and transform operations in thought expansion
  - Why needed here: Encode arithmetic semantics into neural layers for systematic candidate generation
  - Quick check question: How does the merge layer differ from the transform layer in terms of input and output expressions?

- Concept: Premise vector updating
  - Why needed here: Accumulates reasoning context across expansion depths for informed filtering
  - Quick check question: What happens to the premise vector if we skip updating it at each depth?

- Concept: Goal-directed filtering via multi-head attention
  - Why needed here: Scores candidates against goal vector to ensure plausibility
  - Quick check question: How is the answer score computed and what threshold is used to select the final thought?

## Architecture Onboarding

- Component map: Input encoder (RoBERTa with quantity masking) -> Initial thoughts -> Iterative expansion & filtering -> Premise update -> Final answer selection
- Critical path: PLM encoding → initial thoughts → iterative expansion & filtering → premise update → final answer
- Design tradeoffs:
  - Merge vs. separate operation layers: Merge reduces parameters but requires commutative design; separate layers increase flexibility but add complexity
  - Premise vector vs. direct classification: Premise vector allows context accumulation but adds memory overhead; direct classification is simpler but less context-aware
  - Max depth vs. confidence threshold: Larger depth increases coverage but computational cost; confidence threshold may terminate early but risk incomplete reasoning
- Failure signatures:
  - Premise vector stagnation: Same candidates appear at multiple depths without progress
  - Overfiltering: No candidates survive premise check, leading to early termination
  - Low answer scores: Final thoughts fail to meet goal alignment despite surviving filtering
- First 3 experiments:
  1. Ablation of premise vector update: Compare performance with and without premise updates on Math23k
  2. Depth sensitivity: Vary max expansion depth D on UnbiasedMWP(1:N) to find optimal depth
  3. Goal vector variant: Test punctuation mark vs. full question sequence as goal vector on ASDiv-A

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ATHENA compare to LLMs on MWP solving?
- Basis in paper: [inferred] Authors explicitly state they did not compare with LLMs due to focus on limited mathematical samples
- Why unresolved: Authors did not conduct LLM comparisons
- What evidence would resolve it: Experiments comparing ATHENA against GPT-3 or PaLM on MWP datasets

### Open Question 2
- Question: How does choice of PLM affect ATHENA's performance?
- Basis in paper: [explicit] Authors observe larger PLMs improve performance but don't explore other PLM architectures
- Why unresolved: Authors only test RoBERTa variants without exploring other PLM options
- What evidence would resolve it: Experiments with BERT, T5, GPT-2 comparing their impact across MWP datasets

### Open Question 3
- Question: Can ATHENA solve complex math beyond arithmetic?
- Basis in paper: [explicit] Authors only consider arithmetic problems and mention potential for multi-equation problems
- Why unresolved: Paper focuses exclusively on arithmetic without exploring algebraic or calculus domains
- What evidence would resolve it: Adapting ATHENA for algebraic/calculus problems and evaluating on corresponding benchmarks

## Limitations
- Core assumptions about merge/transform operations capturing mathematical semantics need broader validation
- Premise vector update effectiveness lacks specific ablation studies isolating its contribution
- PLM size efficiency gains lack detailed computational overhead versus accuracy trade-off analysis

## Confidence
- Generalization mechanism (thought expansion): Medium confidence
- Premise vector updates: Medium confidence  
- PLM size efficiency: Medium confidence

## Next Checks
1. Conduct ablation study on premise vector updates to quantify context accumulation contribution
2. Perform expansion depth sensitivity analysis to identify optimal depth for reasoning completeness
3. Compare goal vector representations (punctuation vs. full question) to validate design importance