---
ver: rpa2
title: 'Robust Feature Inference: A Test-time Defense Strategy using Spectral Projections'
arxiv_id: '2307.11672'
source_url: https://arxiv.org/abs/2307.11672
tags:
- robust
- features
- robustness
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel test-time defense strategy called Robust
  Feature Inference (RFI) that projects trained models onto the most robust feature
  space to reduce vulnerability to adversarial attacks. The key idea is based on the
  notion of robustness of features, where the most robust features are shown to correspond
  to the top eigenspace of the feature covariance matrix.
---

# Robust Feature Inference: A Test-time Defense Strategy using Spectral Projections

## Quick Facts
- **arXiv ID:** 2307.11672
- **Source URL:** https://arxiv.org/abs/2307.11672
- **Reference count:** 40
- **Primary result:** A test-time defense strategy that projects trained models onto the most robust feature space, achieving improved adversarial robustness without additional test-time computation

## Executive Summary
This paper introduces Robust Feature Inference (RFI), a novel test-time defense strategy that enhances adversarial robustness by projecting trained models onto the most robust feature space. The method leverages the insight that the most robust features correspond to the top eigenspace of the feature covariance matrix computed on training data. By retaining only these robust features during inference, RFI significantly improves resistance to adversarial attacks while maintaining computational efficiency. Extensive experiments demonstrate consistent robustness improvements across multiple datasets and attack scenarios, with the method achieving competitive results without requiring additional computation at test time.

## Method Summary
RFI is a pre-processing technique that transforms a trained model into a more robust version by projecting its weights onto the most robust eigenspace of the feature covariance matrix. The method works by first computing the covariance matrix of features extracted from training data, then performing eigenvalue decomposition to identify the most robust directions. The model weights are then projected onto this robust eigenspace, effectively discarding non-robust features that may have been learned during training. This transformation can be applied to any pre-trained model and does not require additional training or increase inference time. The number of robust features to retain (K) is determined by the number of classes in the dataset.

## Key Results
- RFI consistently improves robust accuracy across CIFAR-10, CIFAR-100, tiny ImageNet, and ImageNet datasets
- The method achieves robustness improvements without additional test-time computation, making it more efficient than other adaptive test-time defenses
- RFI demonstrates effectiveness against various adaptive and transfer attacks, including PGD, APGD-CE, APGD-DLR, and RobustBench attacks
- The approach maintains clean accuracy while significantly improving robustness, with the trade-off controlled by the number of eigenvectors retained

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The most robust features correspond to the top eigenspace of the feature covariance matrix.
- Mechanism: By projecting model weights onto the eigenspace spanned by the top eigenvectors of the feature covariance matrix, the model retains only the most robust directions for classification, thereby reducing vulnerability to adversarial perturbations.
- Core assumption: The feature covariance matrix computed on training data captures the directions along which the model is most robust to adversarial attacks.
- Evidence anchors:
  - [abstract] "the most robust features are shown to correspond to the top eigenspace of the feature covariance matrix."
  - [section] "the top eigenspace of the feature covariance that is the most robust for a generalized additive model."
  - [corpus] Weak evidence. Corpus neighbors discuss test-time defenses and robustness but do not directly reference spectral projections or eigenspace-based robustness.
- Break condition: If the feature covariance matrix does not accurately reflect the robustness of features, or if the adversarial perturbations are highly aligned with the top eigenspace, this mechanism would fail.

### Mechanism 2
- Claim: Robust features are learned first during standard training, and vulnerability arises from non-robust features learned later.
- Mechanism: The method leverages the fact that during training, the model initially learns robust features corresponding to the top eigenspace, and only later learns non-robust features. By projecting onto the top eigenspace at test time, it retains the robust features while discarding the non-robust ones.
- Core assumption: The learning dynamics of neural networks prioritize robust features in the early stages of training.
- Evidence anchors:
  - [abstract] "robust features are learned first during standard training and that the vulnerability to adversarial examples arises from the presence of non-robust features that are learned later in the training process."
  - [section] "As a supplementary analysis, we prove it for NTK features (Section 3.4) and the dynamics in appendix."
  - [corpus] Weak evidence. Corpus neighbors discuss adversarial training and robustness but do not directly reference the learning order of robust vs. non-robust features.
- Break condition: If the learning dynamics do not prioritize robust features, or if non-robust features are essential for accurate classification, this mechanism would fail.

### Mechanism 3
- Claim: The proposed method improves robustness without increasing inference time.
- Mechanism: By projecting the model weights onto the top eigenspace during a pre-processing step, the method effectively transforms the model to use only the most robust features. Since this projection is a linear transformation, it does not increase the number of parameters or the computational complexity at inference time.
- Core assumption: The projection onto the top eigenspace can be implemented as a linear transformation of the model weights without increasing the number of parameters.
- Evidence anchors:
  - [abstract] "RFI achieves this without additional test-time computation, making it more efficient than other adaptive test-time defenses."
  - [section] "Our method does not increase the inference time because the selected robust eigenbasis can be used to simply transform the linear layer weights into the eigenbasis resulting in exactly the same number of parameters in the network at inference time."
  - [corpus] Weak evidence. Corpus neighbors discuss test-time defenses but do not directly reference the computational efficiency of spectral projection-based methods.
- Break condition: If the projection onto the top eigenspace requires a significant increase in the number of parameters or computational complexity, this mechanism would fail.

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: The theoretical analysis of robustness is conducted in the context of GAMs, which provides a simplified model for understanding the relationship between features and robustness.
  - Quick check question: What is the form of a generalized additive model, and how does it differ from a standard linear model?

- Concept: Neural Tangent Kernels (NTKs)
  - Why needed here: The theoretical analysis is extended to large-width neural networks using the equivalence between training a large-width neural network and a kernel machine with the NTK.
  - Quick check question: What is the Neural Tangent Kernel, and how does it relate to the training dynamics of large-width neural networks?

- Concept: Eigenvalue decomposition
  - Why needed here: The method relies on computing the eigenvalue decomposition of the feature covariance matrix to identify the most robust features.
  - Quick check question: What is the relationship between the eigenvalues and eigenvectors of a covariance matrix, and how can they be used to identify the most informative features?

## Architecture Onboarding

- Component map:
  - Feature extractor (ϕ(x)) -> Weight matrix (β) -> Covariance matrix (Σ) -> Eigenvalue decomposition -> Robust feature selection -> Weight projection

- Critical path:
  1. Compute feature covariance matrix on training data
  2. Perform eigenvalue decomposition of the covariance matrix
  3. Select the top K eigenvectors based on robustness scores
  4. Project the weight matrix onto the selected eigenvectors
  5. Use the projected weight matrix for inference

- Design tradeoffs:
  - Number of eigenvectors (K): Choosing a larger K retains more information but may include less robust features. Choosing a smaller K increases robustness but may lead to a loss of accuracy.
  - Computational cost: Computing the eigenvalue decomposition and projecting the weight matrix can be computationally expensive, especially for high-dimensional feature spaces.

- Failure signatures:
  - Decrease in accuracy on clean data: This could indicate that the selected eigenvectors do not capture the most informative features for classification.
  - Increase in vulnerability to certain types of adversarial attacks: This could indicate that the selected eigenvectors are not robust to all types of adversarial perturbations.

- First 3 experiments:
  1. Verify that the eigenvalue decomposition correctly identifies the top eigenvectors of the feature covariance matrix.
  2. Evaluate the impact of the number of eigenvectors (K) on the trade-off between robustness and accuracy.
  3. Compare the performance of the proposed method with other test-time defense strategies on a variety of adversarial attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the robustness of NTK features and their corresponding eigenvalues in the NTK spectrum, and how does this relationship change with different perturbation strengths?
- Basis in paper: [explicit] Proposition 4 and the NTK feature robustness experiment in Figure 2, plot 3.
- Why unresolved: While the paper shows that higher eigenvalues correspond to more robust NTK features, it doesn't provide an exact quantitative relationship or explore how this relationship varies with different perturbation strengths.
- What evidence would resolve it: A detailed analysis showing the exact mathematical relationship between NTK feature robustness and eigenvalue magnitude for various perturbation strengths, potentially including a formula or curve fitting results.

### Open Question 2
- Question: How does the choice of K (number of robust features to retain) affect the trade-off between robustness and accuracy for different model architectures and datasets?
- Basis in paper: [inferred] The discussion on the effect of parameter K in Section 4 and Figure 2, plots 1 and 2.
- Why unresolved: The paper suggests that K should be set to the number of classes based on empirical observations, but doesn't provide a systematic study of how varying K affects performance across different models and datasets.
- What evidence would resolve it: A comprehensive study varying K across multiple model architectures (e.g., ResNet, WideResNet) and datasets (e.g., CIFAR-10, CIFAR-100, ImageNet), showing the accuracy-robustness trade-off for each combination.

### Open Question 3
- Question: Can the RFI method be extended to work with non-linear feature transformations beyond the generalized additive model (GAM) framework?
- Basis in paper: [explicit] The paper's focus on GAM models and linear transformations of features in Sections 3.1 and 3.2.
- Why unresolved: The current theoretical framework is limited to GAM models, but the authors mention that the method could potentially work with other models. However, they don't provide theoretical justification or empirical results for non-linear feature transformations.
- What evidence would resolve it: Theoretical extensions of the robustness analysis to non-linear feature transformations, coupled with empirical results showing improved robustness for such models using RFI.

## Limitations
- The theoretical framework is primarily developed for generalized additive models and may not directly extend to all neural network architectures
- The method requires access to training data to compute the feature covariance matrix, which may not always be available in deployment scenarios
- The assumption that feature covariance on training data captures robustness may break down for highly non-linear or data-scarce scenarios

## Confidence
- **High:** Mathematical characterization of robust features in GAMs and NTKs
- **Medium:** Experimental validation across multiple datasets and attack scenarios
- **Low:** Exact implementation details of eigenvector selection and projection

## Next Checks
1. Verify the impact of different K values on the robustness-accuracy trade-off across various model architectures
2. Test whether the method maintains effectiveness against white-box attacks that are aware of the projection mechanism
3. Investigate the sensitivity of results to different feature extractors and training procedures to assess generalizability