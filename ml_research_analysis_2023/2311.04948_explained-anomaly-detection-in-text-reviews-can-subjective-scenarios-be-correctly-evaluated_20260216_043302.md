---
ver: rpa2
title: 'Explained anomaly detection in text reviews: Can subjective scenarios be correctly
  evaluated?'
arxiv_id: '2311.04948'
source_url: https://arxiv.org/abs/2311.04948
tags:
- reviews
- detection
- anomaly
- explanations
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a pipeline for detecting and explaining anomalous
  reviews in online platforms. The pipeline uses text embeddings from the MPNet transformer,
  autoencoder networks for anomaly detection, and a term frequency analysis for generating
  explanations.
---

# Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?

## Quick Facts
- arXiv ID: 2311.04948
- Source URL: https://arxiv.org/abs/2311.04948
- Authors: 
- Reference count: 40
- Key outcome: Pipeline achieves high F1-scores for anomaly detection in Amazon reviews, with human study showing explanations preferred for intuitive format despite not improving prediction accuracy

## Executive Summary
This paper presents a pipeline for detecting and explaining anomalous reviews in online platforms using MPNet embeddings, autoencoder networks, and term frequency analysis. The system is evaluated on Amazon review datasets with near and far product comparisons, achieving high F1-scores. A human study with 241 participants assesses the effectiveness of three explainability techniques (term frequency, SHAP, and GPT-3), finding that while explanations don't significantly improve users' ability to predict model behavior, they are generally preferred for their intuitive and accessible format.

## Method Summary
The proposed pipeline consists of three main modules: text encoding using MPNet transformer to generate 768-dimensional embeddings, anomaly detection using DAEF autoencoder with reconstruction error thresholds, and explainability through term frequency analysis comparing product-specific terms. The system is trained on Amazon review datasets and evaluated using F1-scores for both near and far product comparisons. Three explainability techniques are compared: term frequency analysis, SHAP feature importance, and GPT-3 generated explanations.

## Key Results
- High F1-scores achieved for both near and far product comparisons on Amazon review datasets
- Human study with 241 participants found explanations did not significantly improve ability to predict model behavior
- Explanations were generally preferred by users for their intuitive and accessible format
- Term frequency approach provides computationally efficient explanations compared to SHAP and GPT-3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPNet embeddings enable classical anomaly detection methods to differentiate between normal and anomalous reviews by providing high-quality semantic representations.
- Mechanism: MPNet transforms raw text reviews into fixed-dimensional dense vectors that capture semantic relationships and similarities, allowing distance-based or reconstruction-based anomaly detection models to work effectively on textual data.
- Core assumption: The semantic similarity captured by MPNet embeddings is sufficient for anomaly detection models to distinguish between reviews about the target product and reviews about other products or generic content.
- Break condition: If MPNet fails to capture meaningful semantic relationships in product reviews, the embeddings will not provide sufficient discriminative power for anomaly detection models.

### Mechanism 2
- Claim: DAEF (Deep Autoencoder for Federated learning) can effectively learn normal review patterns and detect anomalies based on reconstruction error.
- Mechanism: DAEF trains a deep autoencoder in a non-iterative way, learning to compress and reconstruct normal review embeddings. Anomalous reviews produce higher reconstruction errors because they deviate from the learned normal patterns.
- Core assumption: The reconstruction error threshold can be reliably set to distinguish normal from anomalous reviews, and the non-iterative training of DAEF maintains sufficient model capacity.
- Break condition: If the reconstruction error distribution of normal reviews overlaps significantly with that of anomalous reviews, the threshold-based classification will produce many false positives or false negatives.

### Mechanism 3
- Claim: Term frequency analysis can generate interpretable explanations for review classifications by identifying product-specific terms that distinguish normal from anomalous reviews.
- Mechanism: The system identifies the most frequent terms in normal reviews, removes generic terms common across products, and uses the remaining terms to explain classifications. Normal reviews contain these terms (or semantically similar ones), while anomalous reviews do not.
- Core assumption: Normal reviews for a specific product will consistently use a distinct set of terms that are not common across all products, and these terms can be identified through statistical analysis.
- Break condition: If product reviews are too generic or if different products share similar vocabulary, the term frequency analysis will fail to identify distinctive terms, making explanations uninformative.

## Foundational Learning

- Concept: Transformer-based text embeddings (MPNet)
  - Why needed here: To convert unstructured text reviews into numerical vectors that can be processed by machine learning models for anomaly detection.
  - Quick check question: What is the dimensionality of MPNet embeddings and why is this fixed size important for the downstream anomaly detection models?

- Concept: Autoencoder networks for anomaly detection
  - Why needed here: To learn the normal pattern of review embeddings and detect anomalies based on reconstruction error.
  - Quick check question: How does the reconstruction error of an autoencoder indicate whether an input is normal or anomalous?

- Concept: Explainability techniques (SHAP, GPT-3, term frequency)
  - Why needed here: To provide human-understandable justifications for the model's classification decisions, which is crucial for user trust and adoption in real-world review platforms.
  - Quick check question: What is the key difference between SHAP's feature importance approach and the term frequency approach for generating explanations?

## Architecture Onboarding

- Component map: Raw text reviews -> MPNet encoder -> 768-dimensional embeddings -> DAEF autoencoder -> reconstruction errors and normality scores -> Term frequency analyzer -> Classification (normal/anomalous), normality score, explanation

- Critical path: Text → MPNet → DAEF → Classification/Score → Term frequency → Explanation

- Design tradeoffs:
  - MPNet vs. simpler embeddings: Higher quality semantic representation vs. computational cost
  - DAEF vs. iterative autoencoders: Faster training vs. potential loss in reconstruction accuracy
  - Term frequency vs. SHAP vs. GPT-3: Computational efficiency and simplicity vs. potentially richer explanations

- Failure signatures:
  - High false positive rate: Reconstruction error threshold too low or embeddings not discriminative enough
  - Uninformative explanations: Term frequency analysis failing to identify distinctive terms
  - Poor classification performance: MPNet embeddings not capturing relevant semantic differences between products

- First 3 experiments:
  1. Test MPNet embedding quality by visualizing embeddings of reviews from different products using dimensionality reduction (t-SNE/UMAP) to check if semantically similar reviews cluster together.
  2. Evaluate DAEF reconstruction error distribution on a validation set of normal reviews to determine appropriate threshold selection methods.
  3. Compare term frequency explanations against human judgments by asking users to rate whether the explanations make sense for a sample of classified reviews.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed pipeline compare to other transformer-based approaches (e.g., BERT, RoBERTa) for anomaly detection in text reviews?
- Basis in paper: [inferred] The paper uses MPNet for text encoding and achieves good results, but does not compare its performance to other transformer models.
- Why unresolved: The paper focuses on evaluating the proposed pipeline with MPNet and does not provide a comparison with other transformer models.
- What evidence would resolve it: Conducting experiments with different transformer models and comparing their performance on the same anomaly detection task would provide a clearer picture of the effectiveness of the proposed pipeline.

### Open Question 2
- Question: How do the explanations generated by the proposed term frequency-based method compare to those generated by other post-hoc explainability techniques (e.g., LIME, SHAP) in terms of human interpretability and satisfaction?
- Basis in paper: [explicit] The paper compares the term frequency-based method to SHAP and GPT-3, but only in terms of user preference and not in terms of interpretability or satisfaction.
- Why unresolved: The paper does not provide a direct comparison of the interpretability or satisfaction of the explanations generated by different methods.
- What evidence would resolve it: Conducting a user study that specifically evaluates the interpretability and satisfaction of explanations generated by different post-hoc explainability techniques would provide a clearer understanding of their effectiveness.

### Open Question 3
- Question: How does the proposed pipeline perform in detecting anomalies in text reviews from different domains or languages?
- Basis in paper: [inferred] The paper evaluates the pipeline on Amazon reviews in English, but does not explore its performance in other domains or languages.
- Why unresolved: The paper does not provide any evidence of the pipeline's effectiveness in detecting anomalies in text reviews from different domains or languages.
- What evidence would resolve it: Conducting experiments with text reviews from different domains or languages and evaluating the pipeline's performance would provide a clearer understanding of its generalizability.

## Limitations

- Weak corpus evidence supporting MPNet's effectiveness for anomaly detection compared to simpler alternatives
- No validation of optimal reconstruction error threshold selection for DAEF model
- Term frequency explanation approach may fail when product reviews are generic or share similar vocabulary

## Confidence

- MPNet embedding quality and semantic capture: **Medium** - Supported by general transformer literature but lacks specific validation for anomaly detection performance
- DAEF reconstruction error threshold effectiveness: **Low** - Non-iterative training speed advantage claimed but no evidence of reconstruction accuracy compared to iterative methods
- Term frequency explanation validity: **Low** - Statistical approach described but no empirical validation that generated terms are actually distinctive or useful

## Next Checks

1. Conduct a controlled experiment comparing MPNet embeddings against simpler alternatives (TF-IDF, average word embeddings) on a held-out validation set to quantify the actual performance gain for anomaly detection.
2. Perform a threshold analysis study on DAEF reconstruction errors across multiple products to determine if automatic threshold selection methods outperform fixed thresholds.
3. Design a user study specifically testing whether term frequency explanations improve users' understanding of why reviews were classified as anomalous compared to baseline explanations.