---
ver: rpa2
title: PAC Prediction Sets Under Label Shift
arxiv_id: '2310.12964'
source_url: https://arxiv.org/abs/2310.12964
tags:
- prediction
- shift
- label
- sets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of constructing prediction sets
  with PAC guarantees under label shift. The authors propose a novel algorithm that
  propagates uncertainty in importance weight estimates through Gaussian elimination.
---

# PAC Prediction Sets Under Label Shift

## Quick Facts
- arXiv ID: 2310.12964
- Source URL: https://arxiv.org/abs/2310.12964
- Reference count: 40
- This paper addresses the problem of constructing prediction sets with PAC guarantees under label shift

## Executive Summary
This paper tackles the challenge of constructing prediction sets with PAC (probably approximately correct) guarantees under label shift conditions. The authors develop a novel algorithm that propagates uncertainty in importance weight estimates through Gaussian elimination. By constructing confidence intervals for the confusion matrix and predicted label probabilities, then systematically propagating these intervals through linear algebraic computations, the method produces prediction sets that satisfy coverage guarantees while being more informative than existing approaches. The technique is evaluated across five diverse datasets and shown to outperform several baselines.

## Method Summary
The method constructs prediction sets under label shift by first estimating the confusion matrix and predicted label distribution from source and target calibration data. It then builds confidence intervals around these estimates using Clopper-Pearson intervals, propagates these intervals through Gaussian elimination to compute intervals for importance weights, and finally uses these intervals to construct PAC prediction sets via rejection sampling. The key innovation is the systematic propagation of uncertainty through the linear algebraic computations required to solve for importance weights.

## Key Results
- The method satisfies PAC coverage guarantees while producing smaller, more informative prediction sets compared to baselines
- Valid prediction sets are achieved on all five tested datasets: CIFAR-10, ChestX-Ray, Entity-13, CDC Heart, and AGNews
- The approach demonstrates robustness across different types of label shifts and dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Propagating uncertainty through Gaussian elimination preserves PAC guarantees under label shift
- Mechanism: The algorithm constructs confidence intervals for confusion matrix entries and predicted label probabilities, then systematically propagates these intervals through each step of Gaussian elimination to compute intervals for importance weights
- Core assumption: The confusion matrix CP is invertible and remains positive during the elimination process
- Evidence anchors:
  - [abstract]: "propagates uncertainty in these estimates through a Gaussian elimination algorithm to compute confidence intervals for importance weights"
  - [section 3.2]: "our algorithm sets ct+1_ij = 0 if i > k, j ≤ k" and "the Gaussian elimination update ct+1_ij = ct_ij - ct_ik ct_kk / ct_kk can be upper bounded as ct+1_ij ≤ ct_ij - ct_it ct_tj / ct_tt = ct+1_ij"
- Break condition: If ct_ii becomes non-positive at any step, the algorithm returns empty intervals and fails

### Mechanism 2
- Claim: Clopper-Pearson intervals provide valid confidence bounds for importance weight computation
- Mechanism: The algorithm uses Clopper-Pearson intervals for binomial parameters to construct element-wise confidence intervals for confusion matrix entries and predicted label probabilities, which are then propagated through the linear system
- Core assumption: The confusion matrix entries and predicted label probabilities follow binomial distributions that can be bounded with Clopper-Pearson intervals
- Evidence anchors:
  - [section 3.1]: "we can use the Clopper-Pearson (CP) intervals (Clopper & Pearson, 1934) for a Binomial success parameter to construct intervals around cij and qk"
  - [section 3.1]: "Given a confidence level δ ∈ (0, 1) and the sample mean cij = 1/m Σ(x,y)∈Sm 1(g(x)=i, y=j), this is an interval CP(cij, m, δ) = [cij, cij] such that PSm∼P^m[cij ∈ CP(cij, m, δ)] ≥ 1 - δ"
- Break condition: If sample sizes m or n are too small, the Clopper-Pearson intervals become too wide to be useful

### Mechanism 3
- Claim: The rejection sampling framework with uncertainty intervals maintains PAC coverage
- Mechanism: The algorithm uses the computed intervals W to construct prediction sets via rejection sampling, where samples are accepted only if random variables V satisfy Vi ≥ w*_yi / b, ensuring coverage even with uncertain importance weights
- Core assumption: The true importance weights w* lie within the computed intervals W with high probability
- Evidence anchors:
  - [section 3.3]: "Suppose that we can construct W = Q_k∈Y[w_k, w_k] ⊆ R^K such that w* ∈ W. Then, when adapted to our setting, the results of Park et al. (2021) provide an algorithm that returns a threshold τ̂(Sm, V, W, b)"
- Break condition: If the intervals W are too conservative (contain zero), the prediction sets become empty or uninformative

## Foundational Learning

- Concept: Gaussian elimination with interval arithmetic
  - Why needed here: The core challenge is solving the linear system CPw* = q* when CP and q* are estimated from finite samples, requiring propagation of uncertainty through the elimination process
  - Quick check question: If you have intervals [a, a] and [b, b] for two variables, what is the interval for their product a·b under worst-case bounds?

- Concept: Conformal prediction and PAC guarantees
  - Why needed here: The method builds on conformal prediction theory but extends it to provide training-conditional (PAC) guarantees rather than just marginal coverage, which is crucial for reliable uncertainty quantification
  - Quick check question: What's the difference between marginal coverage and PAC (training-conditional) coverage in conformal prediction?

- Concept: Label shift and importance weighting
  - Why needed here: The problem formulation assumes label shift where only the label distribution changes, and importance weights w* = C_P^(-1)q* relate source and target distributions through the confusion matrix
  - Quick check question: Under label shift assumption p(x|y) = q(x|y), how can you express the importance weights w* in terms of the confusion matrix CP and predicted label distribution q*?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Interval construction -> Gaussian elimination with intervals -> Prediction set construction -> Evaluation
- Critical path: Model training → Interval construction → Gaussian elimination → Prediction set construction → Evaluation
- Design tradeoffs:
  - Conservative vs informative intervals: Tighter intervals give smaller prediction sets but may violate coverage guarantees
  - Sample size vs interval width: More calibration data yields tighter intervals but increases computational cost
  - Assumption strength vs practicality: The invertibility and positivity assumptions are strong but necessary for the method to work
- Failure signatures:
  - Empty prediction sets: Often indicates overly conservative intervals or violated assumptions
  - Coverage failure: Usually means intervals weren't propagated correctly or assumptions were violated
  - Extremely large prediction sets: Suggests the intervals W are too wide, possibly due to small sample sizes
- First 3 experiments:
  1. Run on CDC Heart dataset with small label shift (94%, 6%) → (91.3%, 8.7%) to verify basic functionality
  2. Test on CIFAR-10 with large shift (10% uniform → 40% on one class) to stress-test the method
  3. Evaluate on Entity-13 with general label shift to test performance on multi-class problems with hierarchical structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method degrade when the calibration dataset size becomes very small (e.g., less than 1000 samples)?
- Basis in paper: [explicit] The paper states "In future work, it may further be developed to other methods for solving linear systems (e.g., the LU decomposition, Golub & Van Loan (2013)), and other linear algebraic and numerical computations" and discusses experimental evaluation on various dataset sizes.
- Why unresolved: The paper evaluates performance across different dataset sizes but does not systematically study the performance degradation as calibration set size decreases below certain thresholds.
- What evidence would resolve it: Empirical evaluation showing prediction set error rates and sizes across a range of calibration dataset sizes, particularly focusing on the regime where n < 1000.

### Open Question 2
- Question: Can the interval propagation technique through Gaussian elimination be extended to handle cases where the confusion matrix CP is not invertible?
- Basis in paper: [explicit] The paper states "This last assumption requires that the per-class expected predictor outputs be linearly independent; for instance, it is satisfied when g is reasonably accurate across all labels. In addition, one may test whether this assumption holds (Lipton et al., 2018)."
- Why unresolved: The current method assumes CP is invertible, which may not hold for all classifiers.
- What evidence would resolve it: A modified algorithm that can handle singular or near-singular confusion matrices, along with theoretical guarantees and empirical evaluation.

### Open Question 3
- Question: How sensitive is the method to the choice of δ for constructing the Clopper-Pearson intervals, and what is the optimal strategy for choosing these parameters?
- Basis in paper: [explicit] The paper states "The PAC guarantee equation 4 follows from equation 5, equation 8, Lemma 3.1, and a union bound" and discusses the construction of confidence intervals with different δ values.
- Why unresolved: While the paper uses a union bound to combine multiple confidence intervals, it does not systematically study how the choice of individual δ values affects the final prediction set size and error rate.
- What evidence would resolve it: A theoretical analysis of the relationship between individual interval confidence levels and final prediction set performance.

## Limitations

- The method requires the confusion matrix to be invertible and remain positive throughout Gaussian elimination, which may not hold for all classifiers
- Performance heavily depends on having sufficient calibration data to construct tight confidence intervals
- The approach is computationally intensive due to the need to propagate uncertainty through multiple elimination steps

## Confidence

- High: The theoretical framework for propagating uncertainty through Gaussian elimination
- Medium: The empirical results demonstrating improved prediction set quality
- Medium: The PAC guarantee satisfaction across all tested datasets

## Next Checks

1. Test the method on a dataset with known near-singular confusion matrix to evaluate robustness to this limitation
2. Compare the computational overhead against other uncertainty propagation methods on large-scale problems
3. Validate the approach under different covariate shift conditions to assess the impact of violated assumptions