---
ver: rpa2
title: Minimizing Factual Inconsistency and Hallucination in Large Language Models
arxiv_id: '2311.13878'
source_url: https://arxiv.org/abs/2311.13878
tags:
- rationale
- context
- language
- answer
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of factual inconsistency and hallucination
  in Large Language Models (LLMs), which can undermine their reliability in critical
  applications like healthcare and finance. The authors propose a multi-stage framework
  that first generates rationales for LLM responses, verifies and refines them, and
  then uses these rationales as supporting references to generate final answers.
---

# Minimizing Factual Inconsistency and Hallucination in Large Language Models

## Quick Facts
- arXiv ID: 2311.13878
- Source URL: https://arxiv.org/abs/2311.13878
- Reference count: 40
- Primary result: Framework improves GPT-3.5-turbo's faithfulness by 14-25% and accuracy by 16-22% over RAG

## Executive Summary
This paper addresses the critical problem of factual inconsistency and hallucination in Large Language Models (LLMs), which undermines their reliability in high-stakes domains like healthcare and finance. The authors propose a multi-stage framework that generates rationales for retrieved context, verifies and refines them, and then uses these rationales as supporting references to generate final answers. This approach enhances transparency by providing insights into the model's decision-making process. Experiments on PubmedQA and AEQA datasets demonstrate significant improvements in both faithfulness (14-25%) and accuracy (16-22%) compared to traditional RAG methods, with additional benefits from fine-tuning smaller open-access LLMs.

## Method Summary
The framework implements a multi-stage pipeline: (1) Hybrid Retriever that combines semantic, lexical, triplet-based, and web search to find relevant context; (2) Rationale Generator that creates explanations for each retrieved context element; (3) Rationale Verifier that classifies rationales as correct or incorrect based on context alignment; (4) Rationale Refiner that improves incorrect rationales; and (5) Answer Generator that produces final answers with citations. The approach uses triplet representation to reduce token count from 1846 to 285 while maintaining key information relationships, though with some accuracy trade-offs.

## Key Results
- Improves GPT-3.5-turbo's faithfulness by 14-25% and accuracy by 16-22% over traditional RAG methods
- Fine-tuning Llama-2-7B with framework-generated samples improves accuracy by 33-42%
- Triplet-based retrieval reduces token count by 6.5x but with lower faithfulness and accuracy scores
- Framework demonstrates significant potential for improving LLM reliability in high-stakes domains requiring factual accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rationale generation improves faithfulness by grounding LLM output in verifiable context.
- Mechanism: The framework first generates rationales for each retrieved context element, then verifies them against the context before generating final answers. This creates a verifiable chain from context ‚Üí rationale ‚Üí answer.
- Core assumption: LLMs can accurately identify which context elements support specific claims when explicitly prompted to generate rationales.
- Evidence anchors:
  - [abstract] "propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer"
  - [section 3.3] "Given a query ùëû and context ùê∂ = ùëê1, ùëê2, ..., ùëêùëò as input, the Prompt Engine generates a prompt instruction p which is then fed to the LLM to identify and generate the rationale ùëÖ = ùëü1, ùëü2, ..., ùëüùëò on both implicit and explicit reasoning over each retrieved part of the context"
- Break condition: If the LLM cannot distinguish between relevant and irrelevant context elements, the rationale generation will produce spurious explanations.

### Mechanism 2
- Claim: Rationale verification prevents incorrect information from propagating to final answers.
- Mechanism: The Rationale Verifier classifies each rationale statement as correct/incorrect based on its alignment with the context, filtering out unsupported claims before answer generation.
- Core assumption: LLMs can accurately assess whether their own generated rationales are supported by the provided context.
- Evidence anchors:
  - [section 3.4.1] "The Rationale Verifier evaluates the factualness and relevance of the generated rationale with respect to the query and the context. Each statement in the rationale is classified into one of the below based on its relevance to the respective identified part of the context"
  - [section 4.6] Shows a qualitative example where incorrect rationale is identified and revised before final answer generation
- Break condition: If the verification step itself hallucinates or misclassifies rationale statements, incorrect information may still reach the final answer.

### Mechanism 3
- Claim: Using triplets instead of paragraphs reduces context length while maintaining performance, enabling deployment on resource-constrained systems.
- Mechanism: The framework converts retrieved paragraphs into triplets (subject-predicate-object) to reduce token count from ~1846 to ~285 while preserving key information relationships.
- Core assumption: Triplet representation captures sufficient semantic information to maintain answer accuracy while reducing computational overhead.
- Evidence anchors:
  - [section 5] "We experimented with triplets instead of paragraphs to reduce the context length of the input, reducing the average token length from 1846 to 285"
  - [section 5] "While the number of tokens is reduced by 6.5x and can benefit downstream applications with limited computational resources, it has lower faithfulness and accuracy scores compared to the variant that uses paragraphs"
- Break condition: If critical information is lost during triplet conversion, answer quality will degrade despite reduced computational cost.

## Foundational Learning

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: The framework builds upon RAG by adding rationale generation and verification steps to improve factual consistency
  - Quick check question: What is the key difference between vanilla RAG and RAG+FE in terms of the generation pipeline?

- Concept: Chain-of-Thought reasoning
  - Why needed here: The framework uses rationale generation similar to CoT prompting, but applies it to intermediate steps rather than just final answers
  - Quick check question: How does generating rationales for context elements differ from traditional chain-of-thought prompting?

- Concept: Factual consistency evaluation
  - Why needed here: The framework requires metrics to assess whether generated rationales and answers are supported by the context
  - Quick check question: What is the difference between faithfulness and accuracy metrics in the context of this framework?

## Architecture Onboarding

- Component map: User Query ‚Üí Hybrid Retriever (Paragraph/Triplet/Web Search) ‚Üí Rationale Generator ‚Üí Rationale Verifier ‚Üí Rationale Refiner ‚Üí Answer Generator ‚Üí Final Answer with Citations
- Critical path: Retriever ‚Üí Rationale Generator ‚Üí Rationale Verifier ‚Üí Answer Generator (all other components are optional optimizations)
- Design tradeoffs: Triplets reduce computational cost but may sacrifice accuracy; rationale verification adds latency but improves factual consistency
- Failure signatures: High faithfulness but low accuracy indicates verification is too conservative; low faithfulness indicates verification is not catching hallucinations
- First 3 experiments:
  1. Compare RAG vs RAG+FE on a small dataset to verify the ~15% accuracy improvement claim
  2. Test rationale verification sensitivity by injecting known hallucinations into context and checking if they're caught
  3. Evaluate triplet vs paragraph retrieval on a resource-constrained environment to measure the accuracy-cost tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the framework scale with different document types and formats (e.g., PDFs, structured databases, web pages) beyond the tested biomedical domain?
- Basis in paper: [explicit] The paper discusses the framework's applicability to various data sources but primarily tests it on biomedical documents and web pages.
- Why unresolved: The paper does not provide a comprehensive analysis of the framework's performance across different document types and formats, limiting its generalizability.
- What evidence would resolve it: Conducting experiments with diverse document types and formats, such as legal documents, financial reports, and educational materials, to evaluate the framework's adaptability and performance across different domains.

### Open Question 2
- Question: What is the impact of the framework on reducing computational resources and latency in real-time applications, particularly when using the Triplet Search method to reduce token count?
- Basis in paper: [explicit] The paper mentions that using Triplets instead of paragraphs reduces the token count by 6.5x, but it does not provide a detailed analysis of the impact on computational resources and latency.
- Why unresolved: The trade-off between accuracy, faithfulness, and computational efficiency is not fully explored, leaving uncertainty about the framework's practicality in real-time applications.
- What evidence would resolve it: Conducting experiments to measure the framework's performance in terms of computational resources and latency, particularly when using the Triplet Search method, to determine its suitability for real-time applications.

### Open Question 3
- Question: How does the framework perform when fine-tuning smaller open-access LLMs with domain-specific data, and what is the optimal balance between fine-tuning and using the proposed framework?
- Basis in paper: [explicit] The paper mentions that fine-tuning Llama2-7B with samples following the framework improves accuracy, but it does not explore the optimal balance between fine-tuning and using the framework.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of fine-tuning on the framework's performance, leaving uncertainty about the optimal approach for different use cases.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the framework when fine-tuning smaller open-access LLMs with domain-specific data, and determining the optimal balance between fine-tuning and using the framework for different use cases.

## Limitations

- Transferability to non-biomedical domains is untested despite claims of domain-agnostic applicability
- Critical dependency on prompt engineering without providing templates makes reproduction and validation difficult
- Computational cost and latency of the multi-stage framework versus vanilla RAG are not reported

## Confidence

**High Confidence**: The claim that rationale generation and verification improve faithfulness metrics is well-supported by the experimental results (14-25% improvement on RAGAS). The ablation study showing performance degradation when removing components provides strong evidence for the mechanism's effectiveness.

**Medium Confidence**: The accuracy improvements (16-22% over RAG, 33-42% for fine-tuned models) are based on a limited number of datasets and may not generalize to other domains. The evaluation methodology using Auto-Grader (GPT-4-based) introduces potential bias, as the same model type is used for both evaluation and framework components.

**Low Confidence**: Claims about the framework being "domain-agnostic" and the specific numerical improvements from fine-tuning smaller models are not well-validated across diverse scenarios. The lack of transparency in prompt templates and the potential for prompt engineering to drive results reduces confidence in the claimed improvements.

## Next Checks

1. **Cross-domain evaluation**: Test the framework on at least two non-biomedical high-stakes domains (e.g., financial compliance, legal reasoning) to assess generalizability. Measure whether the 14-25% faithfulness improvement and 16-22% accuracy improvement hold across domains with different context structures and reasoning requirements.

2. **Prompt sensitivity analysis**: Systematically vary the prompts for each component (Retriever, Rationale Generator, Verifier, Refiner, Answer Generator) using controlled experiments. Measure how sensitive the faithfulness and accuracy metrics are to prompt variations to determine whether the improvements are driven by the framework architecture or prompt quality.

3. **Computational overhead measurement**: Benchmark the end-to-end latency and computational cost of the multi-stage framework versus vanilla RAG on identical hardware. Include the time for retrieval, rationale generation, verification, refinement, and final answer generation. Compare the results against acceptable latency thresholds for real-time high-stakes applications.