---
ver: rpa2
title: Efficient Computation of Shap Explanation Scores for Neural Network Classifiers
  via Knowledge Compilation
arxiv_id: '2303.06516'
source_url: https://arxiv.org/abs/2303.06516
tags:
- shap
- which
- ddbc
- darwiche
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates efficient computation of Shapley (Shap)\
  \ explanation scores for neural network classifiers by transforming binary neural\
  \ networks (BNNs) into deterministic and decomposable Boolean circuits (dDBCs).\
  \ The transformation follows a path: BNN \u2192 CNF \u2192 SDD \u2192 dDBC, leveraging\
  \ knowledge compilation techniques."
---

# Efficient Computation of Shap Explanation Scores for Neural Network Classifiers via Knowledge Compilation

## Quick Facts
- arXiv ID: 2303.06516
- Source URL: https://arxiv.org/abs/2303.06516
- Reference count: 14
- Key outcome: Over 100x performance improvement for Shap computation on binary neural networks through knowledge compilation

## Executive Summary
This paper presents a novel approach for efficiently computing Shapley (Shap) explanation scores for binary neural network classifiers. The key insight is to transform binary neural networks (BNNs) into deterministic and decomposable Boolean circuits (dDBCs) using knowledge compilation techniques. This transformation enables polynomial-time computation of Shap values, which are otherwise computationally intractable on black-box models. The approach involves converting the BNN to CNF, then to SDD, and finally to dDBC, after which a specialized algorithm can compute Shap scores efficiently. Experiments on the California Housing dataset demonstrate exact fidelity to the original BNN while achieving dramatic computational speedups.

## Method Summary
The method involves a knowledge compilation pipeline that transforms a binary neural network classifier into a deterministic and decomposable Boolean circuit (dDBC). The transformation follows the path: BNN → CNF → SDD → dDBC. Once in dDBC form, a polynomial-time algorithm (Algorithm 1) computes Shap scores by bottom-up dynamic programming on the circuit structure. The compilation is performed once per BNN and can be reused for multiple input entities, amortizing the one-time compilation cost. The approach leverages existing knowledge compilation tools like PySDD and requires binarization of features, making it particularly suited for binary networks with step activation functions.

## Key Results
- Exact match between Shap scores computed on the original BNN and the compiled dDBC
- 7.7 hours to compute Shap scores on 100 input entities using the original BNN
- 4.2 minutes to compute the same Shap scores using the compiled dDBC
- Over 100x performance improvement while preserving exact score fidelity

## Why This Works (Mechanism)

### Mechanism 1
Knowledge compilation transforms a BNN into a deterministic and decomposable Boolean circuit (dDBC) that enables polynomial-time Shap computation. The BNN is compiled into CNF, then into SDD, and finally into dDBC, preserving logical equivalence while enabling efficient Shapley value calculation. Core assumption: The transformation preserves logical equivalence between the BNN and the resulting dDBC. Break condition: If the compilation introduces approximation errors or if the dDBC grows exponentially large relative to the BNN.

### Mechanism 2
The polynomial-time algorithm for Shap computation works specifically on dDBCSFi(2) circuits. Once in dDBCSFi(2) form, Algorithm 1 computes Shapley values by bottom-up dynamic programming on the circuit structure. Core assumption: The circuit is smooth, deterministic, and has fan-in at most 2. Break condition: If the circuit cannot be transformed into dDBCSFi(2) within reasonable computational limits.

### Mechanism 3
The one-time compilation cost is amortized over multiple Shap computations on different inputs. The compilation transformation is performed once per BNN, then the resulting circuit can be reused for Shap computation on any number of input entities. Core assumption: Shap computation needs to be performed on many different inputs for the same trained BNN. Break condition: If Shap needs to be computed only once or a few times for a given BNN.

## Foundational Learning

- Concept: Boolean circuit representations and properties
  - Why needed here: Understanding deterministic, decomposable, and smooth properties is essential for the compilation approach
  - Quick check question: What makes a Boolean circuit "deterministic" and why is this important for efficient Shap computation?

- Concept: Knowledge compilation and canonical representations
  - Why needed here: The compilation path (BNN → CNF → SDD → dDBC) relies on knowledge compilation theory
  - Quick check question: What is the tree-width of a CNF formula and why does it matter for SDD compilation complexity?

- Concept: Shapley value computation complexity
  - Why needed here: Understanding why Shap is #P-hard in general but becomes tractable on certain circuit classes
  - Quick check question: Why is Shap computation on black-box models generally intractable while it's polynomial on dDBCSFi(2)?

## Architecture Onboarding

- Component map:
  BNN model (TensorFlow/Larq) -> CNF converter -> SDD compiler (PySDD) -> dDBC transformer -> Shap algorithm -> Data pipeline

- Critical path:
  1. Train BNN on dataset
  2. Convert BNN to CNF (most expensive step)
  3. Convert CNF to SDD
  4. Convert SDD to dDBCSFi(2)
  5. Compute Shap scores on multiple inputs using efficient algorithm

- Design tradeoffs:
  - Compilation vs. direct computation: One-time high cost vs. repeated low cost
  - Circuit size: More compact circuits enable faster Shap computation
  - Distribution choice: Uniform vs. product distribution affects algorithm complexity
  - Binarization: Required for the approach but may lose information from original features

- Failure signatures:
  - Exponential growth in circuit size during compilation
  - Compilation timeouts or memory exhaustion
  - Incorrect Shap scores indicating compilation errors
  - Performance degradation on larger datasets

- First 3 experiments:
  1. Run the complete pipeline on the California Housing dataset and verify Shap scores match between BNN and dDBC approaches
  2. Measure compilation time vs. Shap computation time for varying numbers of input entities (20, 40, 60, 80, 100)
  3. Test the pipeline with different binarization strategies and observe impact on Shap score accuracy and computation time

## Open Questions the Paper Calls Out

### Open Question 1
Can the knowledge compilation approach be extended to compute Shapley values for non-binary neural networks? The paper explicitly states "In this work we have considered only binary NNNs" and suggests investigating non-binary NNs as future work. This remains unresolved because the current compilation path is specifically designed for binary networks, and the paper doesn't explore whether this approach can handle continuous weights or multi-valued outputs.

### Open Question 2
How can domain knowledge or logical constraints be incorporated into the Shapley value computation algorithm to make explanations more meaningful? The paper mentions this as an important research problem, suggesting logical constraints could be used, but doesn't provide a concrete method. This remains unresolved because while the paper mentions this as a possibility, it doesn't provide a concrete method for integrating domain knowledge into the computation.

### Open Question 3
Can the empirical distribution from the dataset be used instead of the uniform distribution for Shapley value computation, and what are the trade-offs? The paper mentions that using the empirical distribution would require modifying the algorithm but doesn't explore this approach or analyze the computational trade-offs. This remains unresolved because the paper notes that using the empirical distribution would require modifying the algorithm, but doesn't explore this approach or analyze the computational trade-offs between different distributions.

## Limitations

- Restricted to binary neural networks with binary weights and step activation functions
- Limited experimental validation on only one dataset (California Housing)
- Computational overhead of BNN-to-CNF conversion not fully characterized
- Scalability to larger networks and datasets remains uncertain

## Confidence

- **High**: The theoretical foundation of transforming BNNs to dDBCs for polynomial-time SHAP computation
- **Medium**: The experimental results showing 100x performance improvement on the California Housing dataset
- **Low**: The claim that this approach generalizes well to other datasets and neural network architectures

## Next Checks

1. **Generalization Test**: Apply the complete pipeline to at least two additional datasets (e.g., Adult Income, Diabetes) to verify the 100x performance improvement claim holds across different domains and data characteristics.

2. **Architecture Scaling**: Test the approach on BNNs with multiple hidden layers (2-3 layers) to evaluate how compilation time and circuit size scale with network depth, and whether the polynomial-time SHAP computation remains tractable.

3. **Approximation Analysis**: Compare SHAP scores from the compiled dDBC against exact SHAP values computed through Monte Carlo sampling on the original BNN to quantify any approximation errors introduced during the compilation process.