---
ver: rpa2
title: 'GROOT: Learning to Follow Instructions by Watching Gameplay Videos'
arxiv_id: '2310.08235'
source_url: https://arxiv.org/abs/2310.08235
tags:
- task
- goal
- agent
- tasks
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GROOT, a new approach to learning open-ended
  instruction following in Minecraft through reference gameplay videos. The key idea
  is to learn a video instruction encoder and control policy jointly via future state
  prediction, enabling the policy to understand video instructions and act accordingly.
---

# GROOT: Learning to Follow Instructions by Watching Gameplay Videos

## Quick Facts
- arXiv ID: 2310.08235
- Source URL: https://arxiv.org/abs/2310.08235
- Reference count: 40
- Key outcome: GROOT achieves 70% win rate over baselines on Minecraft SkillForge benchmark

## Executive Summary
GROOT is a novel approach for learning open-ended instruction following by watching gameplay videos. It jointly learns a video instruction encoder and control policy via future state prediction, enabling the policy to understand video instructions and act accordingly. This allows GROOT to follow diverse video instructions without expensive text-gameplay annotations. The method is evaluated on a new Minecraft SkillForge benchmark, where it closes the human-machine gap and demonstrates emergent properties like goal composition and complex behavior synthesis.

## Method Summary
GROOT learns to follow instructions by watching gameplay videos through a self-supervised learning approach. It consists of a video encoder that maps videos to latent goals and a causal transformer decoder that generates actions based on current observations and goal embeddings. The model is trained using behavioral cloning and KL divergence loss to constrain the goal space to be succinct and task-relevant. The key innovation is learning the video instruction encoder and control policy jointly via future state prediction, allowing the policy to understand and act on diverse video instructions without explicit text-gameplay annotations.

## Key Results
- GROOT achieves 70% win rate over the best baseline on Minecraft SkillForge benchmark
- Closes the human-machine gap in instruction following tasks
- Demonstrates emergent properties like goal composition and complex behavior synthesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Video instruction encoder learns a structured goal space via future state prediction.
- Mechanism: By maximizing log-likelihood of future states given past observations, the encoder must identify latent goals that predict future behavior.
- Core assumption: The transition dynamics are fixed and known, so predicting future states only requires knowing the latent goal and current action.
- Evidence anchors: [abstract], [section 3], weak support from related works on imitation learning.

### Mechanism 2
- Claim: KL regularization constrains the goal space to be succinct and task-relevant.
- Mechanism: The KL divergence between the encoder's posterior and a prior forces the goal space to be compact and consistent across different state sequences that pursue the same goal.
- Core assumption: A prior over goal space exists that represents task-relevant information.
- Evidence anchors: [section 4.3], [section 5.2], weak support from related works on VAEs.

### Mechanism 3
- Claim: Gated cross-attention allows the policy to query task progress from instructions.
- Mechanism: The causal transformer decoder uses goal embeddings as keys and values in cross-attention layers, allowing it to attend to relevant instruction information based on current state.
- Core assumption: Goal embeddings contain sufficient information about task progress and remaining steps.
- Evidence anchors: [section 4.2], [section 5.2], moderate support from transformer literature.

## Foundational Learning

- Concept: Variational inference and evidence lower bound (ELBO)
  - Why needed here: The learning framework is derived from maximizing the ELBO of future state likelihood, which requires understanding variational methods.
  - Quick check question: What is the relationship between the ELBO and the true log-likelihood in variational inference?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: GROOT uses transformer-based encoder-decoder architecture with gated cross-attention, requiring understanding of how transformers process sequential data.
  - Quick check question: How does cross-attention differ from self-attention in transformer architectures?

- Concept: Behavior cloning and imitation learning
  - Why needed here: The training objective combines behavior cloning with KL regularization, requiring understanding of how policies can be learned from demonstrations.
  - Quick check question: What is the main challenge in behavior cloning that the KL regularization aims to address?

## Architecture Onboarding

- Component map: Video Encoder (CNN + non-causal transformer) -> Goal Space (Gaussian latent space) -> Policy Decoder (Causal transformer with gated cross-attention) -> Inverse Dynamic Model (Pre-trained)

- Critical path: 1. Video encoder processes reference video to generate goal embeddings, 2. Goal embeddings are passed to policy decoder, 3. Policy decoder conditions on current state and goal embeddings to generate actions, 4. Actions are executed in environment and new observations are fed back to decoder

- Design tradeoffs: Using non-causal transformer for encoder vs. causal for policy allows full video context but requires careful handling of temporal information; Gaussian latent space is simple and tractable but may not capture complex goal distributions; Gated cross-attention allows flexible instruction querying but adds computational complexity

- Failure signatures: Poor performance on multi-step tasks may indicate goal embeddings not capturing procedural information; Sensitivity to reference video selection may indicate goal space not well-aligned with human intent; Inability to generalize across biomes may indicate overfitting to training environment

- First 3 experiments: 1. Test video encoder on a simple task (e.g., "chop tree") with a single reference video and evaluate if it can complete the task, 2. Test policy decoder with pre-computed goal embeddings on a multi-step task (e.g., "build snow golem") to isolate encoder vs. policy issues, 3. Test concatenated video conditioning by giving policy two sequential tasks (e.g., "chop tree" then "hunt animal") and evaluate if it can complete both

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GROOT's performance on Minecraft SkillForge compare to human players when given the same amount of time to complete tasks?
- Basis in paper: [inferred] The paper mentions human evaluations but does not provide specific time constraints for human players.
- Why unresolved: The paper does not specify the time limit given to human players during the Elo rating comparison.
- What evidence would resolve it: Detailed experimental setup specifying time limits for both GROOT and human players in the Minecraft SkillForge benchmark.

### Open Question 2
- Question: How does the performance of GROOT vary when trained on gameplay videos from different Minecraft biomes?
- Basis in paper: [explicit] The paper mentions that GROOT is trained on gameplay videos but does not specify if videos from different biomes were used.
- Why unresolved: The paper does not provide information on the diversity of biomes in the training dataset.
- What evidence would resolve it: Experiments comparing GROOT's performance when trained on videos from different biomes versus a single biome.

### Open Question 3
- Question: Can GROOT's goal space be fine-tuned using human feedback to improve alignment with human intentions?
- Basis in paper: [explicit] The paper mentions that future work may use RLHF (Reinforcement Learning from Human Feedback) and SFT (Supervised Fine-Tuning) to align the pre-trained goal space with human preference.
- Why unresolved: The paper does not provide any experiments or results on using human feedback to fine-tune GROOT's goal space.
- What evidence would resolve it: Experiments showing the impact of human feedback on GROOT's performance and goal space alignment.

## Limitations
- Experimental results are primarily demonstrated on a single Minecraft-based benchmark with 30 tasks, limiting generalizability
- The video encoder's ability to learn a semantically meaningful goal space through future state prediction is theoretically sound but practically limited by the assumption of fixed transition dynamics
- The gated cross-attention mechanism introduces significant computational overhead and may not scale well to longer or more complex instructions

## Confidence
- High Confidence: The core architectural design (video encoder + causal transformer decoder) is well-specified and the training methodology (behavior cloning + KL regularization) is clearly described and implemented.
- Medium Confidence: The experimental results showing 70% win rate over baselines are credible but limited in scope.
- Low Confidence: Claims about emergent properties like goal composition and complex behavior synthesis are primarily qualitative and lack quantitative validation.

## Next Checks
1. Cross-domain transfer evaluation: Test GROOT on a different open-world environment to assess whether the learned goal space generalizes beyond Minecraft.
2. Stochastic environment stress test: Evaluate GROOT's performance in environments with varying levels of stochasticity to quantify the limits of the future state prediction mechanism.
3. Attention pattern analysis: Visualize and analyze the cross-attention weights during policy execution to verify that the model is attending to semantically relevant parts of the instruction embeddings and to identify potential attention saturation issues.