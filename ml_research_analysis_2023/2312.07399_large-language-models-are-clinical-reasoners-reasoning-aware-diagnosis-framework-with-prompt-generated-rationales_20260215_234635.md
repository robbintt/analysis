---
ver: rpa2
title: 'Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework
  with Prompt-Generated Rationales'
arxiv_id: '2312.07399'
source_url: https://arxiv.org/abs/2312.07399
tags:
- diagnosis
- clinical
- reasoning
- rationales
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reasoning-aware diagnosis framework that
  uses large language models (LLMs) to generate clinical rationales for disease diagnosis,
  specifically Alzheimer's disease. The framework includes clinical rationalization,
  few-shot reasoning, and knowledge distillation modules.
---

# Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales

## Quick Facts
- arXiv ID: 2312.07399
- Source URL: https://arxiv.org/abs/2312.07399
- Reference count: 16
- Key outcome: LLMs can generate clinically consistent rationales and achieve improved diagnostic performance compared to traditional classification approaches for Alzheimer's disease diagnosis.

## Executive Summary
This paper introduces a reasoning-aware diagnosis framework that leverages large language models (LLMs) to generate clinical rationales for Alzheimer's disease diagnosis. The framework integrates clinical rationalization, few-shot reasoning, and knowledge distillation modules to enable both LLMs and smaller student models to replicate clinical reasoning in a human-like manner. Through extensive experiments on ADNI and AIBL datasets, the authors demonstrate that LLMs can generate medically accurate rationales that improve diagnostic performance, while knowledge distillation enables practical deployment of these reasoning capabilities in smaller models. The framework achieves high accuracy and consistency in generating clinical rationales, with student models sometimes outperforming the LLM teacher in certain metrics.

## Method Summary
The framework processes patient descriptions (textual representations of MRI findings, demographics, and genetic information) through three main modules. First, the clinical rationalization module augments the dataset with LLM-generated rationales using ground truth labels. Second, few-shot reasoning and diagnosis with LLMs generates both rationales and diagnoses for patient descriptions. Third, knowledge distillation transfers the reasoning capacity from LLMs to smaller language models (LMs) and vision-language models (VLMs) trained on the augmented dataset. The paper experiments with both unimodal (text-only) and multimodal (text + MRI images) approaches, comparing LLM performance against distilled student models and establishing novel criteria for evaluating machine-generated clinical rationales.

## Key Results
- LLMs generate clinically consistent rationales with high human-likeness scores (4.24/5) that improve diagnostic performance
- Knowledge distillation successfully transfers reasoning capacity to smaller models, with student models sometimes outperforming the LLM teacher
- Multimodal VLMs trained via knowledge distillation achieve comparable performance to LLMs while being more practical for deployment
- The proposed clinical rationale evaluation criteria demonstrate strong agreement between human assessors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Clinical rationales improve AD diagnosis by enabling contextual interpretation of clinical evidence
- **Mechanism**: The framework generates detailed rationales that contextualize patient data (e.g., MRI findings, APOE4 gene presence) within the broader diagnostic picture. This mirrors how clinicians integrate multiple data points rather than relying on isolated features
- **Core assumption**: LLMs can generate clinically coherent rationales that accurately reflect the reasoning process a human clinician would use
- **Evidence anchors**: [abstract] "LLMs can generate clinically consistent rationales and achieve improved diagnostic performance compared to traditional classification approaches"; [section] "Our rationales exhibit the same behavior: 'The patient carries one copy of APOE4 gene, which is known to increase the risk of AD. The absence of cognitive impairment symptoms and brain atrophy suggest that this genetic risk has not led to any apparent neurodegeneration.'"
- **Break condition**: If generated rationales contain medically incorrect knowledge or fail to appropriately contextualize evidence, diagnostic performance will degrade

### Mechanism 2
- **Claim**: Knowledge distillation from LLMs to smaller models transfers reasoning capacity while maintaining diagnostic accuracy
- **Mechanism**: The framework uses LLMs to generate clinical rationales paired with diagnoses, then trains smaller language models on this augmented dataset. This process distills the LLM's reasoning patterns into more practical models
- **Core assumption**: The reasoning patterns captured in LLM-generated rationales can be effectively learned by smaller models through supervised training
- **Evidence anchors**: [abstract] "student models outperforming the LLM teacher in certain metrics"; [section] "We purpose this augmented dataset as training data to train the student language models" and "Table 2 presents the experimental results of student models"
- **Break condition**: If the student models fail to learn the reasoning patterns or if the rationales contain noise that degrades training

### Mechanism 3
- **Claim**: Multimodal inputs (text + MRI images) improve diagnostic reasoning by providing complementary information
- **Mechanism**: The framework extends knowledge distillation to vision-language models that process both textual patient descriptions and corresponding MRI scans, allowing models to integrate visual and textual clinical evidence
- **Core assumption**: Vision-language models can effectively attend to features from both modalities to generate more accurate clinical rationales and diagnoses
- **Evidence anchors**: [abstract] "we further extend knowledge distillation in clinical diagnosis to VLMs"; [section] "For instance, Zhang et al. (2023) showed that by including images alongside textual inputs, models with under 1B parameters can generate more effective CoT rationales"
- **Break condition**: If the visual features extracted from MRI scans are not properly aligned with textual features, or if the model fails to effectively integrate information from both modalities

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: Clinical diagnosis requires multi-step reasoning that connects patient data to diagnosis, which CoT prompting helps elicit from LLMs
  - Quick check question: What is the difference between standard prompting and CoT prompting in terms of model output?

- **Concept**: Knowledge distillation
  - Why needed here: LLMs are too large for practical deployment, so distillation transfers their reasoning capabilities to smaller, deployable models
  - Quick check question: In the context of this framework, what exactly is being distilled from the teacher to the student?

- **Concept**: Multimodal learning
  - Why needed here: AD diagnosis involves both textual patient data and visual MRI scans, requiring models that can process and integrate information from multiple modalities
  - Quick check question: How does a vision-language model differ from separate vision and language models in processing clinical data?

## Architecture Onboarding

- **Component map**: Patient description → LLM rationale generation → Diagnosis prediction → Knowledge distillation to smaller models
- **Critical path**: Patient description → LLM rationale generation → Diagnosis prediction → Knowledge distillation to smaller models
- **Design tradeoffs**: Using LLMs provides better reasoning but is computationally expensive; Multimodal models can integrate visual and textual information but require more complex architectures; Knowledge distillation enables practical deployment but may lose some reasoning fidelity
- **Failure signatures**: Poor rationale quality: Generated rationales contain medical errors or fail to contextualize evidence; Distillation failure: Student models perform worse than baseline models without distillation; Multimodal integration failure: Models ignore visual information or fail to properly align visual and textual features
- **First 3 experiments**: Compare LLM performance with and without CoT prompting on the AD diagnosis task; Evaluate student model performance after distillation versus baseline models; Compare multimodal student performance versus unimodal students and vision-only baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., encoder-only vs. decoder-only) affect clinical reasoning performance?
- Basis in paper: [inferred] The paper uses both encoder-only (GPT-4) and decoder-only (ChatGPT) models but doesn't compare their relative performance in clinical reasoning tasks
- Why unresolved: The paper presents results for both models but doesn't conduct a systematic comparison of different LLM architectures
- What evidence would resolve it: A controlled experiment comparing multiple LLM architectures on the same clinical reasoning tasks with consistent prompts and evaluation metrics

### Open Question 2
- Question: What is the optimal balance between rationale specificity and brevity for effective clinical decision-making?
- Basis in paper: [explicit] The paper mentions rationales averaging 269.4 words but doesn't explore how varying length affects diagnostic accuracy or clinical utility
- Why unresolved: While the paper shows high human-likeness scores for rationales, it doesn't investigate whether shorter, more focused rationales might be equally effective
- What evidence would resolve it: A study varying rationale length while measuring diagnostic accuracy, clinical utility, and time efficiency for different task complexity levels

### Open Question 3
- Question: How does the inclusion of multimodal data (beyond MRI) impact the effectiveness of clinical reasoning?
- Basis in paper: [explicit] The paper mentions that AD diagnosis involves various data types but only experiments with MRI scans and textual data
- Why unresolved: The study focuses on MRI-based diagnosis but acknowledges that real clinical settings involve diverse data types
- What evidence would resolve it: Extending the framework to incorporate additional modalities (e.g., PET scans, genetic data, clinical notes) and measuring the impact on diagnostic accuracy and reasoning quality

## Limitations

- The framework relies on manually transformed textual descriptions of MRI scans rather than direct image inputs for primary experiments, potentially missing visual diagnostic features
- Knowledge distillation implementation lacks detailed specifications needed for precise replication
- Clinical rationale evaluation criteria are novel but haven't been validated against clinician consensus or compared with established medical reasoning frameworks
- Experiments focus on a single disease domain (Alzheimer's), limiting generalizability to other clinical conditions

## Confidence

**High confidence** in the core finding that LLMs can generate clinically consistent rationales and improve diagnostic performance when integrated into a reasoning-aware framework. The experimental results with accuracy metrics and human evaluations provide strong empirical support.

**Medium confidence** in the knowledge distillation mechanism's effectiveness. While the paper shows student models outperforming the LLM teacher in certain metrics, the lack of detailed implementation specifications and comparison with alternative distillation approaches introduces uncertainty.

**Low confidence** in the generalizability of the clinical rationale evaluation criteria. The proposed criteria for assessing machine-generated rationales in clinical settings are novel but haven't been validated through clinician studies or compared with established medical reasoning frameworks.

## Next Checks

1. **Clinical expert validation**: Conduct a study where practicing clinicians evaluate the LLM-generated rationales using the proposed criteria, comparing them against expert-generated rationales to validate the evaluation framework

2. **Cross-disease generalization**: Test the framework on additional clinical domains (e.g., oncology, cardiovascular disease) to assess whether the reasoning-aware approach generalizes beyond Alzheimer's diagnosis

3. **Direct image input comparison**: Re-run the experiments using raw MRI images as input rather than textually transformed descriptions to determine if the multimodal approach provides additional diagnostic value beyond textual features alone