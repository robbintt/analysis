---
ver: rpa2
title: Large Language Models to Identify Social Determinants of Health in Electronic
  Health Records
arxiv_id: '2308.06354'
source_url: https://arxiv.org/abs/2308.06354
tags:
- social
- role
- page
- content
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study aimed to improve identification of social determinants
  of health (SDoH) in electronic health records, which are crucial for understanding
  health disparities but often incompletely documented. The authors developed large
  language models to extract six key SDoH categories from clinical notes: employment,
  housing, transportation, parental status, relationship, and social support.'
---

# Large Language Models to Identify Social Determinants of Health in Electronic Health Records

## Quick Facts
- arXiv ID: 2308.06354
- Source URL: https://arxiv.org/abs/2308.06354
- Reference count: 40
- Key outcome: Fine-tuned Flan-T5 models extract social determinants of health from clinical notes with macro-F1 up to 0.71, identifying 93.8% of patients with adverse SDoH vs 2.0% via structured codes.

## Executive Summary
This study develops and evaluates large language models for extracting social determinants of health (SDoH) from clinical notes, addressing the critical gap in structured SDoH documentation. The authors fine-tune transformer-based models (BERT and Flan-T5 variants) on annotated clinical text to identify six key SDoH categories and classify them as adverse or not. They demonstrate that fine-tuned models outperform zero- and few-shot ChatGPT-family models, with synthetic data augmentation improving performance for smaller models and rare classes. The approach identifies significantly more patients with adverse SDoH than structured billing codes, suggesting substantial value for clinical decision support and health equity research.

## Method Summary
The authors developed sentence-level SDoH extraction models using annotated clinical notes from 800 radiotherapy patients. They fine-tuned transformer models (BERT and Flan-T5 base/large/XL/XXL) using binary cross-entropy loss, with optional synthetic data augmentation generated via GPT-3.5. Models were evaluated on held-out test sets from radiotherapy, immunotherapy, and MIMIC-III datasets. Bias assessment involved injecting demographic descriptors into test sentences and measuring prediction changes. Patient-level performance was compared against ICD-10 Z-codes, and zero/few-shot ChatGPT performance served as additional benchmarks.

## Key Results
- Fine-tuned Flan-T5 XL achieved macro-F1 0.71 for any SDoH extraction; Flan-T5 XXL achieved macro-F1 0.70 for adverse SDoH classification
- Synthetic data augmentation improved smaller models by +0.12 to +0.23 F1, especially for rare classes
- Fine-tuned models outperformed zero/few-shot ChatGPT on both tasks and showed less bias to demographic descriptors
- Text extraction identified 93.8% of patients with adverse SDoH versus 2.0% captured by structured ICD-10 codes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned Flan-T5 models outperform zero-shot/few-shot ChatGPT-family models on SDoH extraction tasks.
- Mechanism: Pre-training on diverse web text followed by instruction tuning on general NLP tasks, then task-specific fine-tuning on small annotated clinical datasets allows Flan-T5 to adapt to domain-specific linguistic patterns better than general-purpose models used in few-shot settings.
- Core assumption: Task-specific fine-tuning on domain-relevant data provides better performance than few-shot prompting with large general models.
- Evidence anchors:
  - [abstract] "Our best-performing fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models for both tasks."
  - [section] "Our fine-tuned models outperformed ChatGPT-family models with zero- and shot learning, and were less prone to bias than ChatGPT-family models, despite being orders of magnitude smaller."

### Mechanism 2
- Claim: Synthetic data augmentation improves performance for smaller Flan-T5 models and rare SDoH classes.
- Mechanism: Generated synthetic clinical text increases the effective training set size and diversity, especially for underrepresented classes, allowing smaller models to learn patterns they otherwise lack data for.
- Core assumption: The synthetic data is of sufficient quality and diversity to meaningfully expand the training distribution without introducing harmful noise.
- Evidence anchors:
  - [abstract] "The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23)."
  - [section] "Performance varied quite a bit across the models for the other classes."

### Mechanism 3
- Claim: Text-extracted SDoH data identifies significantly more patients with adverse SDoH than structured Z-codes.
- Mechanism: Unstructured clinical notes contain richer and more complete descriptions of SDoH than the structured billing codes that are sporadically entered.
- Core assumption: Clinicians document SDoH more completely in free text than in structured fields, and NLP can reliably extract this information.
- Evidence anchors:
  - [abstract] "At the patient level, our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%."
  - [section] "Most EMR systems have other ways to enter SDoH information as structured data which may have more complete documentation, however these did not exist for most of our target SDoH."

## Foundational Learning

- Concept: Understanding of social determinants of health (SDoH) and their clinical relevance.
  - Why needed here: The model targets specific SDoH categories (employment, housing, transportation, parental status, relationship, social support) that are clinically impactful but under-documented.
  - Quick check question: Can you list the six SDoH categories targeted in this study and explain why they are clinically important?

- Concept: Multilabel text classification and handling class imbalance.
  - Why needed here: Each sentence can have multiple SDoH labels, and most sentences have no SDoH mentions, creating severe class imbalance.
  - Quick check question: How does undersampling the negative class during training help address class imbalance in this multilabel setting?

- Concept: Bias evaluation in NLP models, particularly regarding demographic descriptors.
  - Why needed here: The study explicitly tests whether model predictions change when demographic information is injected, to assess algorithmic bias.
  - Quick check question: What does it mean if a model's classification changes significantly when demographic descriptors are added to the text?

## Architecture Onboarding

- Component map: Clinical notes → sentence segmentation → annotation → train/dev/test split → Flan-T5 fine-tuning (with/without synthetic data) → evaluation (macro-F1, bias assessment, patient-level comparison)

- Critical path: 1) Prepare annotated dataset (sentence-level SDoH labels) 2) Fine-tune Flan-T5 models on gold data (+ optional synthetic augmentation) 3) Evaluate on held-out test sets (RT, immunotherapy, MIMIC-III) 4) Assess bias using demographic-injected synthetic pairs 5) Compare text-extracted vs structured Z-code patient-level performance

- Design tradeoffs:
  - Model size vs. performance: Larger Flan-T5 models perform better but require more resources; LoRA reduces tuning parameters
  - Synthetic data quality vs. quantity: More synthetic data may help but risks introducing noise if quality is poor
  - Annotation granularity: Sentence-level labeling enables fine-grained extraction but may miss cross-sentence context

- Failure signatures:
  - Poor performance on rare SDoH classes indicates insufficient training examples or synthetic data quality issues
  - High discrepancy rates when demographic descriptors are injected suggests algorithmic bias
  - Large performance gap between in-domain and out-of-domain validation indicates limited generalizability

- First 3 experiments:
  1. Fine-tune Flan-T5 base model on gold data only; evaluate macro-F1 and per-class performance
  2. Fine-tune Flan-T5 large model with synthetic data augmentation; compare performance gains over base model
  3. Test bias by running fine-tuned model on demographic-injected synthetic pairs; calculate discrepancy rates by demographic group

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for generating synthetic clinical text that maintains high quality across diverse SDoH categories?
- Basis in paper: [explicit] The authors note that synthetic data augmentation showed variable benefits across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). They also note that manual gold-labeling could further enhance the value of synthetic data, but this would decrease the value of synthetic data in terms of reducing annotation effort.
- Why unresolved: The study used GPT-3.5 to generate synthetic data but did not explore other methods or fine-tune the generation process. The authors suggest that incorporating real clinical examples in the prompt would likely improve the quality of the synthetic data, but this was not tested.
- What evidence would resolve it: Comparative studies evaluating different synthetic data generation methods (e.g., fine-tuning large language models, using different prompting strategies, or incorporating real clinical examples) on their ability to improve model performance across diverse SDoH categories.

### Open Question 2
- Question: How do algorithmic biases in SDoH extraction models vary across different patient populations and healthcare settings?
- Basis in paper: [explicit] The authors found that their fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias (p<0.05). However, they note that this lack of significance may be due to the small numbers in this evaluation, and future work is critically needed to further evaluate bias in clinical LMs.
- Why unresolved: The study only evaluated bias on a limited dataset and did not assess how biases might vary across different patient populations or healthcare settings. The authors acknowledge the need for further research in this area.
- What evidence would resolve it: Large-scale studies evaluating algorithmic biases in SDoH extraction models across diverse patient populations, healthcare settings, and geographic regions, using comprehensive bias metrics and demographic descriptors.

### Open Question 3
- Question: What is the impact of integrating SDoH information extracted from clinical notes into clinical decision support systems and patient care pathways?
- Basis in paper: [inferred] The authors conclude that their method could enhance real-world evidence on SDoH and aid in identifying patients needing social support. However, they do not explore the practical implications of integrating this information into clinical workflows or decision-making processes.
- Why unresolved: The study focuses on developing and evaluating SDoH extraction models but does not investigate the downstream effects of using this information in clinical settings. The potential benefits and challenges of integrating SDoH data into clinical decision support systems or patient care pathways remain unexplored.
- What evidence would resolve it: Clinical trials or implementation studies evaluating the impact of integrating SDoH information extracted from clinical notes into clinical decision support systems, patient care pathways, and health outcomes. This could include assessments of provider adherence to SDoH-informed recommendations, patient satisfaction, and changes in health disparities.

## Limitations
- Domain generalization remains uncertain as validation on MIMIC-III showed performance decrements for certain classes
- Synthetic data quality and bias are concerns since quality control mechanisms are not fully specified
- Limited bias evaluation scope only examines explicit demographic terms, not subtle forms of bias

## Confidence
- High confidence: Core finding that fine-tuned Flan-T5 models outperform zero/few-shot ChatGPT models for SDoH extraction
- Medium confidence: Synthetic data augmentation benefits (context-dependent, requires careful implementation)
- Medium confidence: Patient-level identification claims (clinical validity beyond structured codes not fully characterized)

## Next Checks
1. Cross-institutional validation: Test the fine-tuned models on clinical notes from different healthcare systems and geographic regions to assess real-world generalizability beyond the initial training and validation datasets.

2. Clinical validity assessment: Conduct a chart review study where clinicians validate a sample of the text-extracted SDoH predictions to establish precision and recall in a real clinical context, not just against structured codes.

3. Longitudinal bias monitoring: Implement continuous monitoring of the deployed models to detect bias drift over time as the underlying clinical documentation practices evolve and new patient populations are served.