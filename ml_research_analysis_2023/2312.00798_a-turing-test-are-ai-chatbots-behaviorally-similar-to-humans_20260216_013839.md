---
ver: rpa2
title: 'A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?'
arxiv_id: '2312.00798'
source_url: https://arxiv.org/abs/2312.00798
tags:
- human
- chatgpt-4
- game
- chatgpt-3
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study administers a Turing Test to AI chatbots by having them
  play a suite of behavioral games and complete a Big Five personality survey. It
  finds that ChatGPT-4's behavior is statistically indistinguishable from a random
  human's behavior across multiple games and personality dimensions.
---

# A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?

## Quick Facts
- arXiv ID: 2312.00798
- Source URL: https://arxiv.org/abs/2312.00798
- Reference count: 40
- Primary result: ChatGPT-4's behavior is statistically indistinguishable from random human behavior across multiple behavioral games and personality dimensions.

## Executive Summary
This study administers a Turing Test to AI chatbots by having them play behavioral games and complete a Big Five personality survey, comparing their responses to those of hundreds of thousands of human subjects from over 50 countries. The findings reveal that ChatGPT-4 exhibits human-like behavioral and personality traits that are statistically indistinguishable from a random human's behavior. ChatGPT-4 acts as if maximizing an equal-weighted average of its own and its partner's payoffs, showing more altruistic and cooperative tendencies than the median human. The chatbots also demonstrate learning-like behavior by modifying their strategies based on previous roles and respond to framing effects similarly to humans.

## Method Summary
The study collected data from 108,314 human subjects playing behavioral games and completing the Big Five personality survey, and administered the same games and survey to ChatGPT-3 and ChatGPT-4 under identical conditions. The researchers compared AI responses to human distributions using statistical tests and revealed-preference analysis to estimate the AI's payoff function parameters. The behavioral games included Dictator, Ultimatum, Trust, Bomb Risk, Public Goods, and Prisoner's Dilemma.

## Key Results
- ChatGPT-4's behavior is statistically indistinguishable from random human behavior across multiple games and personality dimensions.
- ChatGPT-4 acts as if maximizing an equal-weighted average of its own and its partner's payoffs, exhibiting more altruistic and cooperative tendencies than the median human.
- Chatbots modify their behavior based on previous roles and respond to framing effects similarly to humans.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AI chatbots exhibit human-like behavior because their training on large corpora of human-generated text inherently embeds human behavioral patterns.
- Mechanism: The chatbots' responses are generated by models trained on extensive human language data, which includes descriptions of human decision-making in similar contexts.
- Core assumption: The training data sufficiently represents diverse human behavioral patterns across the games and personality dimensions tested.
- Evidence anchors:
  - [abstract] "ChatGPT-4 passes the Turing Test in that it consistently exhibits human-like behavioral and personality traits based on a comparison to the behavior of hundreds of thousands of humans from more than 50 countries."
  - [section] "ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human's behavior across multiple games and personality dimensions."
  - [corpus] Weak evidence; corpus shows related work but no direct evidence of training data composition or its behavioral representation.
- Break condition: If the training data lacks diversity or is biased toward specific behavioral patterns, the AI's responses would not generalize across the full range of human behaviors.

### Mechanism 2
- Claim: The AI chatbots modify their behavior based on previous roles and respond to framing effects similarly to humans due to their ability to contextualize and adapt responses.
- Mechanism: The models use contextual information from previous interactions and framing cues to adjust their subsequent responses, mimicking human learning and contextual sensitivity.
- Core assumption: The models' architecture allows for maintaining and utilizing context across interactions, and they can interpret and respond to framing cues.
- Evidence anchors:
  - [abstract] "Chatbots also modify their behavior based on previous experience and contexts 'as if' they were learning from the interactions, and change their behavior in response to different framings of the same strategic situation."
  - [section] "Once they have experienced the role of a 'partner' in an asymmetric game... their behavior shifts significantly."
  - [corpus] Moderate evidence; related work on human-AI trust and behavior in economics games supports the concept but doesn't confirm the specific mechanism.
- Break condition: If the model's context window is too limited or it cannot effectively process framing cues, it would not exhibit adaptive behavior similar to humans.

### Mechanism 3
- Claim: The AI chatbots act as if maximizing an equal-weighted average of its own and its partner's payoffs, exhibiting more altruistic and cooperative tendencies than the median human.
- Mechanism: The models' objective function, inferred through revealed preference analysis, aligns with a utility function that balances self-interest and other-interest, leading to cooperative behavior.
- Core assumption: The models' behavior can be accurately modeled by a utility function that weights own and partner payoffs equally.
- Evidence anchors:
  - [abstract] "We estimate that they act as if they are maximizing an average of their own and partner's payoffs."
  - [section] "We find that it is an even average of own and partner's payoff... they act as if they are maximizing the total payoff of both players rather than simply their own payoff."
  - [corpus] Limited evidence; related work on LLM behavior in economics games exists but doesn't confirm the specific utility function.
- Break condition: If the true objective function of the models differs significantly from the assumed utility function, the inferred behavior would not accurately reflect the models' decision-making process.

## Foundational Learning

- Concept: Behavioral economics games
  - Why needed here: Understanding the games (Dictator, Ultimatum, Trust, etc.) is crucial for interpreting the AI's behavior and comparing it to human responses.
  - Quick check question: Can you explain the key difference between the Dictator Game and the Ultimatum Game in terms of strategic interaction?

- Concept: Big Five personality traits (OCEAN model)
  - Why needed here: The AI's personality profile is assessed using the Big Five model, which is a standard framework for measuring personality traits.
  - Quick check question: What are the five dimensions of the Big Five personality model, and what does each dimension measure?

- Concept: Revealed preference analysis
  - Why needed here: This method is used to infer the AI's objective function by analyzing its choices in the games, assuming it acts rationally to maximize some utility.
  - Quick check question: How does revealed preference analysis work, and what assumptions does it make about the decision-maker?

## Architecture Onboarding

- Component map: Data collection from AI chatbots playing behavioral games and taking personality tests -> Comparison of AI behavior to human distributions -> Revealed preference analysis to infer objective functions -> Interpretation of results
- Critical path: Data collection -> Behavior comparison -> Objective function inference -> Interpretation of results
- Design tradeoffs: Using pre-trained models allows for quick deployment but limits control over specific behavioral tendencies; collecting human data provides a benchmark but may introduce sampling biases.
- Failure signatures: AI behavior significantly deviating from human distributions in certain games, inability to maintain context across interactions, or poor fit of revealed preference analysis.
- First 3 experiments:
  1. Validate the data collection process by running a small pilot with both AI and human subjects on a subset of games.
  2. Perform a sensitivity analysis on the revealed preference analysis by varying the assumed utility function and observing changes in inferred parameters.
  3. Test the AI's response to framing effects by systematically varying the prompts and measuring changes in behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different versions of ChatGPT (and other AI chatbots) behave across a wider variety of behavioral games and contexts?
- Basis in paper: [explicit] The paper only tests a limited suite of behavioral games and contexts on ChatGPT-3 and ChatGPT-4. The authors state "one can imagine expanding the suite of analyses included in a Turing Test."
- Why unresolved: The paper focuses on a specific set of games and does not test the behavior of other AI chatbots beyond ChatGPT-3 and ChatGPT-4.
- What evidence would resolve it: Testing a wider variety of behavioral games and contexts on different versions of ChatGPT and other AI chatbots, comparing their behavior to human distributions.

### Open Question 2
- Question: To what extent do AI chatbots exhibit human-like complex behaviors such as learning and changes in behavior from role-playing in repeated game settings?
- Basis in paper: [explicit] The paper finds that ChatGPT-3 and ChatGPT-4 exhibit changes in behavior based on previous experience and contexts, but the authors state "Our analyses of learning and framing are far from systematic."
- Why unresolved: The paper only briefly touches on learning and role-playing, without conducting a systematic analysis.
- What evidence would resolve it: Conducting a more comprehensive study on AI chatbot behavior in repeated game settings, analyzing the extent of learning and role-playing effects.

### Open Question 3
- Question: What are the underlying reasons for the observed differences in behavior between ChatGPT-3 and ChatGPT-4, and how do these differences impact their suitability for various tasks and applications?
- Basis in paper: [explicit] The paper observes behavioral differences between ChatGPT-3 and ChatGPT-4, but does not delve into the reasons behind these differences or their implications.
- Why unresolved: The paper does not provide a detailed analysis of the factors contributing to the behavioral differences between ChatGPT-3 and ChatGPT-4.
- What evidence would resolve it: Investigating the training data, fine-tuning processes, and architectural differences between ChatGPT-3 and ChatGPT-4, and assessing their impact on behavior and task suitability.

## Limitations

- The study's findings are based on comparisons between AI chatbots and human subjects playing specific behavioral games, which may not fully capture the complexity of human behavior in real-world situations.
- The revealed preference analysis assumes that the AI's behavior can be accurately modeled by a utility function, which may not reflect the true decision-making process of the models.
- The training data used for the chatbots is not explicitly characterized, leaving uncertainty about the diversity and representativeness of the behavioral patterns it contains.

## Confidence

- High confidence: The statistical indistinguishability between ChatGPT-4 and human behavior across multiple games and personality dimensions, supported by the direct comparison of AI responses to human distributions.
- Medium confidence: The revealed preference analysis inferring an equal-weighted average of own and partner's payoffs as the AI's objective function, given the assumptions and potential limitations of the method.
- Low confidence: The generalization of the findings to real-world human-AI interactions, as the study is limited to controlled experimental settings and specific behavioral games.

## Next Checks

1. **Sensitivity Analysis of Revealed Preference Analysis**: Vary the assumed utility function specifications (e.g., CES instead of linear) and assess how the inferred parameters and the fit of the model to AI behavior change.
2. **Framing Effect Robustness Test**: Systematically vary the prompts and framing of the same strategic situations across multiple sessions and measure the consistency of AI behavior changes. Compare the magnitude and direction of AI responses to known human framing effects.
3. **Context Window and Adaptation Test**: Conduct experiments to assess the AI's ability to maintain context across multiple interactions and adapt its behavior based on previous roles. Measure the impact of varying the context window length and the presence of explicit role instructions on the AI's decision-making process.