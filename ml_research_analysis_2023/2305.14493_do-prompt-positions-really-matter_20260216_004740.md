---
ver: rpa2
title: Do prompt positions really matter?
arxiv_id: '2305.14493'
source_url: https://arxiv.org/abs/2305.14493
tags:
- text
- prompt
- mask
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper comprehensively evaluates prompt position effects on
  zero- and few-shot NLP tasks using cloze, prefix, manual, and continuous prompts.
  Across sentiment classification, NLI, and QA tasks, it finds significant performance
  variance among prompt positions, with the default positions from published work
  often sub-optimal.
---

# Do prompt positions really matter?

## Quick Facts
- arXiv ID: 2305.14493
- Source URL: https://arxiv.org/abs/2305.14493
- Reference count: 20
- Significant performance variance among prompt positions, with published positions often sub-optimal

## Executive Summary
This paper comprehensively evaluates prompt position effects on zero- and few-shot NLP tasks using cloze, prefix, manual, and continuous prompts. Across sentiment classification, NLI, and QA tasks, it finds significant performance variance among prompt positions, with the default positions from published work often sub-optimal. In many cases, simply reordering input and prompt can yield 20-30% accuracy gains in zero-shot settings, and differences persist in few-shot settings with labeled data sizes down to 128 samples. The best prompt position varies by task and prompt type, indicating no universal optimal configuration. These findings suggest prompt position optimization as a valuable direction alongside prompt engineering.

## Method Summary
The study evaluates prompt position effects using four datasets (SST-2, CR for sentiment; RTE for NLI; Boolq for QA) across three model architectures (RoBERTa-large for cloze, T5-large for prefix, GPT-3 6.7B for zero-shot). Experiments are conducted in zero-shot and few-shot settings (16, 32, 64, 128 samples per label with five random splits). Four prompt types are tested: cloze-style with [MASK] tokens, prefix-style continuous prompts, manual prompts from prior work, and continuous prompt tuning. All permutations of prompt positions are tested systematically, and mean accuracy with min-max ranges are reported.

## Key Results
- Prompt position significantly affects model performance in both zero-shot and few-shot settings
- Published prompt positions often show sub-optimal performance compared to alternative configurations
- Position differences can yield 20-30% accuracy improvements in zero-shot settings
- Best prompt position varies by task and prompt type, with no universal optimal configuration
- Position effects persist even with labeled data down to 128 samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt position fundamentally alters the relative context and attention weights in transformer models, affecting prediction.
- Mechanism: In transformer-based models, attention mechanisms compute relationships between tokens based on their relative positions. When a prompt is placed at different positions relative to the input, it changes the attention paths and relative positional encodings, which alters how the model processes the task-relevant information.
- Core assumption: The model's performance depends on the prompt being in an attention path that maximizes its influence on the prediction for the masked token or continuation.
- Evidence anchors:
  - [abstract]: "Our findings quantify the substantial impact prompt position has on model performance."
  - [section]: "When a prompt is placed at different positions relative to the input, it changes the attention paths and relative positional encodings"
  - [corpus]: Weak or missing - no direct corpus evidence on attention mechanism changes due to prompt position.
- Break condition: If the model uses absolute positional embeddings that don't influence attention weights, or if the model architecture doesn't rely on positional information for attention.

### Mechanism 2
- Claim: Prompt position affects the semantic coherence and grammatical structure of the input, which influences model predictions.
- Mechanism: Different prompt positions create different semantic relationships between the prompt and input. For example, placing the prompt before the input ("Question: Is this sentence positive or negative? [text]") creates a different grammatical structure than placing it after ("[text] Question: Is this sentence positive or negative?"). These structural differences affect how the model interprets the task.
- Core assumption: The model has been trained on data with specific grammatical structures, and deviations from these structures affect performance.
- Evidence anchors:
  - [abstract]: "We observe that the prompt positions used in prior studies are often sub-optimal for both zero-shot and few-shot settings."
  - [section]: "We noticed that the optimal position is affected by K-size as well. The prefix manual prompt that exhibits strong performance in the zero-shot setting does not necessarily perform well on all tasks when applied to a few-shot setting."
  - [corpus]: Weak or missing - no direct corpus evidence on grammatical structure effects.
- Break condition: If the model is trained on highly diverse grammatical structures or if the model architecture is invariant to word order.

### Mechanism 3
- Claim: Prompt position influences the model's internal representation of the task, affecting its ability to generalize from few examples.
- Mechanism: When prompts are placed in different positions, they create different task representations in the model's hidden states. In few-shot settings, the model relies more heavily on these representations to generalize from limited examples. Different positions may create more or less effective task representations.
- Core assumption: The model's ability to learn from few examples depends on the quality of its internal task representation.
- Evidence anchors:
  - [abstract]: "These findings suggest prompt position optimisation as a valuable research direction alongside the existing focus on prompt engineering."
  - [section]: "We observe that in most cases, the prompt positions used in reference published work show a sub-optimal performance compared to other prompt position choices."
  - [corpus]: Weak or missing - no direct corpus evidence on task representation effects.
- Break condition: If the model architecture doesn't create distinct task representations based on prompt position, or if the model doesn't rely on task representations for few-shot learning.

## Foundational Learning

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how prompt position affects attention paths is crucial for grasping why different positions yield different results
  - Quick check question: How do relative positional embeddings influence attention weight calculations in transformer models?

- Concept: Few-shot learning and generalization
  - Why needed here: The paper shows different prompt positions have varying effects in few-shot settings, requiring understanding of how models generalize from limited examples
  - Quick check question: What distinguishes few-shot learning from zero-shot and full fine-tuning in terms of model behavior?

- Concept: Prompt engineering and template design
  - Why needed here: The paper builds on existing prompt engineering work but focuses on position optimization, requiring understanding of prompt-based learning fundamentals
  - Quick check question: What are the key differences between manual prompts and continuous prompts in terms of their implementation and effects?

## Architecture Onboarding

- Component map:
  Input processing -> Prompt insertion -> Model inference -> Output parsing -> Evaluation

- Critical path:
  1. Prepare dataset with labeled examples
  2. Define prompt templates with position variations
  3. Implement prompt insertion logic
  4. Run inference across all position variations
  5. Parse and evaluate results
  6. Analyze performance differences

- Design tradeoffs:
  - Computational cost vs. comprehensive position testing
  - Template complexity vs. position isolation
  - Manual prompt variety vs. continuous prompt flexibility

- Failure signatures:
  - No performance variation across positions (suggests position invariance)
  - Random performance patterns (suggests implementation errors)
  - Extreme performance drops (suggests grammatical violations)

- First 3 experiments:
  1. Single-sentence task with cloze manual prompt: Test different positions of [MASK] token relative to input text
  2. Sentence-pair task with prefix continuous prompt: Test different positions of continuous prompt tokens relative to input pairs
  3. Zero-shot vs few-shot comparison: Compare performance variance across position variations in both settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do prompt position effects generalize to large language models (LLMs) beyond the medium-sized models tested in this study?
- Basis in paper: [explicit] The authors explicitly state their results may apply to LLMs but need empirical confirmation
- Why unresolved: The study was limited to medium-sized models due to computational constraints, leaving the scaling effects on LLMs untested
- What evidence would resolve it: Direct experiments comparing prompt position effects across model sizes (small, medium, and large) on the same tasks

### Open Question 2
- Question: Do prompt position optimizations transfer across similar tasks within the same domain?
- Basis in paper: [inferred] The paper found inconsistent optimal prompt positions even among similar sentiment classification tasks (SST-2 vs CR), suggesting limited transfer
- Why unresolved: The study only tested a few tasks per category, making it unclear whether similar tasks share prompt position preferences
- What evidence would resolve it: Systematic testing of prompt positions across many tasks within the same domain (e.g., multiple sentiment analysis datasets) to measure transfer rates

### Open Question 3
- Question: What is the interaction between prompt position optimization and prompt engineering techniques?
- Basis in paper: [explicit] The authors suggest prompt position optimization as a research direction alongside existing prompt engineering focus
- Why unresolved: The study isolated prompt position effects from other prompt engineering variables, leaving their combined effects unexplored
- What evidence would resolve it: Experiments testing optimal prompt positions with various prompt engineering techniques (vocabulary search, automated generation, etc.) to measure synergistic or antagonistic effects

### Open Question 4
- Question: How do prompt position effects vary across different prompt styles (manual vs continuous) within the same task?
- Basis in paper: [explicit] The authors observed no clear correlation between optimal continuous and manual prompt positions for the same prompt style
- Why unresolved: The study tested different prompt styles separately without directly comparing their position preferences on identical tasks
- What evidence would resolve it: Direct comparison of prompt position effects using the same template across different prompt styles (e.g., converting a manual prompt to continuous form) on identical tasks

## Limitations

- The study is limited to four datasets and three model architectures, which may not capture full spectrum of prompt position effects
- Prompt templates are drawn from prior literature without systematic variation of template complexity or task specificity
- The paper doesn't investigate computational overhead of position optimization or provide theoretical grounding for why certain positions work better
- No analysis of attention weight distributions or causal mechanisms underlying position effects

## Confidence

- **High confidence**: The core empirical finding that prompt position significantly affects model performance across zero-shot and few-shot settings is well-supported by the comprehensive experimental design
- **Medium confidence**: The claim that prompt position optimization should be pursued alongside prompt engineering is reasonable but requires additional theoretical justification
- **Low confidence**: The assertion that position effects persist even with labeled data down to 128 samples, while supported by data, lacks theoretical explanation for why this threshold exists

## Next Checks

1. Evaluate prompt position effects on specialized domains (medical, legal, technical) using domain-specific datasets to determine if position effects generalize beyond general NLP tasks

2. Test position effects across diverse model architectures including encoder-only, decoder-only, and encoder-decoder models with varying parameter counts to identify whether position sensitivity correlates with architectural properties

3. Design controlled experiments that isolate attention mechanism effects by using models with and without relative positional embeddings, or by analyzing attention weight distributions across different prompt positions to establish causal mechanisms