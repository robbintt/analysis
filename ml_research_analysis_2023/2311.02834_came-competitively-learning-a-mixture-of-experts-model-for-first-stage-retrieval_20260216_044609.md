---
ver: rpa2
title: 'CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval'
arxiv_id: '2311.02834'
source_url: https://arxiv.org/abs/2311.02834
tags:
- retrieval
- learning
- came
- experts
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of first-stage retrieval, where
  the goal is to efficiently retrieve a subset of relevant documents from a large
  collection. The authors observe that different relevance matching patterns exist
  between queries and documents, and existing retrieval models focus on specific patterns.
---

# CAME: Competitively Learning a Mixture-of-Experts Model for First-stage Retrieval

## Quick Facts
- arXiv ID: 2311.02834
- Source URL: https://arxiv.org/abs/2311.02834
- Authors: 
- Reference count: 40
- This paper proposes a Mixture-of-Experts model with competitive learning for first-stage retrieval, achieving 98.8% R@1000 and 41.3 MRR@10 on MS MARCO.

## Executive Summary
This paper addresses first-stage retrieval by proposing CAME, a Mixture-of-Experts model that learns to match queries and documents through three complementary experts (lexical, local, and global). The key innovation is a competitive learning mechanism that trains experts proportionally to their relative performance on each training instance, encouraging specialization. Experimental results on three benchmark datasets show CAME significantly outperforms state-of-the-art retrieval models while maintaining competitive efficiency.

## Method Summary
CAME uses a Mixture-of-Experts architecture with shared bottom Transformer layers for common semantic representations, followed by three expert-specific pathways: lexical matching using MLM, local matching using token-level MaxSim, and global matching using CLS pooling. The model is trained through a two-stage competitive learning process: first, all experts are trained equally to develop general capabilities, then experts compete on each instance and update proportionally to their relative performance. During inference, expert scores are simply summed to produce final rankings. The training uses BM25 negatives initially, followed by hard negatives mined from the model's own top retrieval results.

## Key Results
- Achieves 98.8% R@1000 and 41.3 MRR@10 on MS MARCO, significantly outperforming state-of-the-art methods
- Demonstrates 86.6% R@1000 and 74.5% NDCG@10 on TREC DL
- Shows strong performance on NQ with 89.5% R@100
- Outperforms single-expert baselines and non-competitive MoE variants

## Why This Works (Mechanism)

### Mechanism 1: Competitive Learning for Expert Specialization
CAME's competitive learning mechanism enables specialized expert development by having experts compete on each training instance and update proportionally to their relative performance. During specialized learning, experts are trained proportionally to their relative ranking performance on each instance. The expert that ranks the positive document highest gets the largest weight for that instance, while others get smaller weights based on their relative performance. This creates a "divide-and-conquer" approach where each expert focuses on samples it handles best. The core assumption is that different relevance patterns require different expert architectures, and the model can automatically learn which expert is best for each pattern through competition.

### Mechanism 2: Shared Bottom Layers for Common Representations
The shared bottom layers in CAME provide common semantic representations that serve as foundation for specialized upper layers to focus on different relevance patterns. Bottom Transformer layers are shared across all experts to learn general syntactic and semantic features. These shared representations are then fed into expert-specific upper layers that capture pattern-specific features through their different architectures (lexical, local, global). The core assumption is that bottom layers learn task-agnostic linguistic knowledge while upper layers can focus on specialized pattern recognition when given common representations.

### Mechanism 3: Simple Score Summation for Result Fusion
CAME's result fusion strategy effectively combines specialized expert judgments by simple score summation, leveraging the complementary nature of different relevance patterns. During inference, each expert scores documents from its specialized perspective. The final score is computed by summing scores from all experts, with each expert contributing equally since we don't know which is most trustworthy for a given query-document pair. The core assumption is that different relevance patterns are complementary rather than contradictory, and simple fusion can effectively combine their strengths.

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) framework
  - Why needed here: MoE allows combining multiple retrieval models with different strengths, enabling the system to handle diverse relevance patterns that no single model can capture effectively.
  - Quick check question: What is the key difference between classical MoE models and CAME's approach in terms of expert selection?

- **Concept**: Competitive learning mechanism
  - Why needed here: Traditional ensemble methods don't differentiate which expert is better for which sample. Competitive learning explicitly encourages experts to specialize by making their training dependent on relative performance.
  - Quick check question: How does the specialized learning stage in CAME differ from traditional boosting approaches?

- **Concept**: Hard negative mining
  - Why needed here: Standard random negatives are too easy and don't challenge the model sufficiently. Hard negatives from top retrieval results provide more informative training signals.
  - Quick check question: Why does CAME train first on BM25 negatives and then on hard negatives mined from its own top results?

## Architecture Onboarding

- **Component map**: Query and document → shared bottom layers (10 Transformer layers) → three MoE expert pathways (lexical using MLM, local using MaxSim, global using CLS pooling) → individual expert scores → score summation → final ranking
- **Critical path**: Query and document → shared bottom layers → expert-specific upper layers → individual expert scores → score summation → final ranking. The most computationally intensive path is the local matching expert due to token-level comparisons.
- **Design tradeoffs**: Shared layers reduce parameters and enable knowledge transfer but may create interference. Simple score summation for fusion is efficient but may miss opportunities for adaptive weighting. Competitive learning encourages specialization but requires careful temperature tuning.
- **Failure signatures**: If R@1000 is near 100% but MRR@10 is low, experts may be specializing too narrowly. If all experts perform similarly on all samples, competition isn't effective. If performance drops significantly without shared layers, experts may need more capacity.
- **First 3 experiments**:
  1. Train CAME without shared layers (each expert has 12 private layers) and measure performance degradation to quantify shared layer benefits.
  2. Vary the temperature parameter τ in competitive learning from 0.2 to 3.0 to find optimal competition intensity for your dataset.
  3. Compare simple score summation fusion against reciprocal rank summation and learned linear combination to validate the chosen fusion strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CAME perform compared to other state-of-the-art methods when using the knowledge distillation and data augmentation techniques proposed in RocketQA and AR2?
- Basis in paper: [explicit] The authors mention that CAME should achieve even better performance if using these techniques, but they did not evaluate this in their experiments.
- Why unresolved: The authors did not have access to the code or checkpoints for these techniques to integrate them into CAME.
- What evidence would resolve it: Implementing and evaluating CAME with the knowledge distillation and data augmentation techniques from RocketQA and AR2.

### Open Question 2
- Question: What is the optimal number of shared layers in CAME for different dataset sizes and characteristics?
- Basis in paper: [explicit] The authors found that the optimal number of shared layers varies slightly between datasets, but they did not conduct a comprehensive study to determine the optimal number for different dataset sizes and characteristics.
- Why unresolved: The authors only evaluated a limited range of shared layer numbers (6-12) and did not explore the impact of dataset size and characteristics on the optimal number.
- What evidence would resolve it: Conducting a comprehensive study to determine the optimal number of shared layers for different dataset sizes and characteristics.

### Open Question 3
- Question: How does CAME perform when using different result fusion methods, such as learning to predict the probability of each relevance pattern given the query and document?
- Basis in paper: [explicit] The authors mention that they plan to explore more complex fusion methods in the future, but they did not evaluate any in their experiments.
- Why unresolved: The authors only evaluated simple fusion methods, such as score summation and reciprocal rank summation, and did not explore more complex methods.
- What evidence would resolve it: Implementing and evaluating CAME with different result fusion methods, such as learning to predict the probability of each relevance pattern given the query and document.

## Limitations

- The paper lacks detailed ablation studies on key design choices like shared layer depth and temperature parameter tuning
- Claims about specific matching patterns being captured by lexical/local/global experts lack empirical validation
- The exact contribution of competition vs. simple ensemble remains unclear without controlled comparisons

## Confidence

- **High confidence**: The architectural framework (shared bottom + MoE experts) and two-stage training procedure are clearly specified and align with established MoE principles.
- **Medium confidence**: The competitive learning mechanism's effectiveness is demonstrated through benchmark results, but the exact contribution of competition vs. simple ensemble remains unclear without ablations.
- **Low confidence**: Claims about specific matching patterns being captured by lexical/local/global experts lack empirical validation showing which expert handles which query types.

## Next Checks

1. Conduct ablation study comparing CAME with: (a) single expert baseline, (b) non-competitive MoE ensemble, (c) CAME without shared layers to quantify each component's contribution.
2. Analyze expert specialization by categorizing queries (lexical vs semantic) and measuring individual expert performance to verify the competitive learning mechanism creates meaningful specialization.
3. Test CAME on out-of-domain retrieval tasks to evaluate whether competitive learning enables better generalization than static ensemble methods.