---
ver: rpa2
title: 'INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based
  Dialogue System'
arxiv_id: '2310.18207'
source_url: https://arxiv.org/abs/2310.18207
tags:
- negotiation
- price
- intent
- agent
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes INA, a novel integrative negotiation dialogue
  agent for the online marketplace that can negotiate not only on price but also on
  other factors like adding or removing items from a deal bundle. To enable this functionality,
  the authors create a new dataset called IND using a semi-automated data creation
  method that combines defining negotiation intents, actions, and intent-action simulation
  between users and the agent.
---

# INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System

## Quick Facts
- **arXiv ID**: 2310.18207
- **Source URL**: https://arxiv.org/abs/2310.18207
- **Reference count**: 17
- **Primary result**: INA achieves 88.6% intent consistency and 85.2% negotiation strategy consistency in integrative negotiation tasks

## Executive Summary
This paper introduces INA (Integrative Negotiation Agent), a novel dialogue system for online marketplaces that goes beyond simple price negotiations to handle complex integrative bargaining scenarios involving multiple products and accessories. The system is trained on a new dataset called IND (Integrative Negotiation Dataset), created through a semi-automated process combining GPT-J generation with human-in-the-loop post-editing. INA employs a novel reward function that balances multiple negotiation objectives, including price gap minimization, intent consistency, and interactiveness. Experiments demonstrate that INA significantly outperforms baseline approaches in integrative negotiation tasks, successfully adjusting both prices and product bundles while maintaining coherent negotiation strategies.

## Method Summary
The method involves creating the IND dataset through a semi-automated pipeline where negotiation intents and scenarios are defined, GPT-J generates dialogue flows, and human experts perform quality control through post-editing. The dataset contains 4,163 utterances across 3,330 dialogues about 10 electronic products with accessories. INA is trained using a two-stage approach: first, supervised fine-tuning of GPT-2-medium on the IND dataset using cross-entropy loss, then reinforcement learning fine-tuning using PPO with a novel reward function. The reward function combines four components - intent consistency (0.2 weight), price gap (0.2), negotiation strategy (0.3), and interactiveness (0.2) - to encourage integrative negotiation behavior that balances multiple objectives simultaneously.

## Key Results
- INA achieves 88.6% intent consistency and 85.2% negotiation strategy consistency in integrative negotiation tasks
- Human evaluations show INA outperforms baselines on negotiation consistency, bargaining efficacy, and dialogue engagingness
- The integrative approach enables successful bundle negotiations with 27% better outcomes than single-objective price-only baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reward function encourages integrative negotiation behavior by rewarding both price gap minimization and negotiation strategy consistency.
- Mechanism: The reward function R = γ1R1 + γ2R2 + γ3R3 + γ4R4 combines intent consistency (R1), price gap (R2), negotiation strategy (R3), and interactiveness (R4). This composite reward pushes the agent to balance multiple negotiation aspects, not just price.
- Core assumption: A weighted combination of rewards can effectively guide complex multi-objective negotiation behavior in RL.
- Evidence anchors:
  - [abstract] "We employ a set of novel rewards, specifically tailored for the negotiation task to train our Negotiation Agent, termed as the Integrative Negotiation Agent (INA)."
  - [section] "These rewards incentivize the chatbot to learn effective negotiation strategies that can adapt to various contextual requirements and price proposals."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.437, average citations=0.0." (Weak evidence from related work)
- Break condition: If the reward weights (γ1, γ2, γ3, γ4) are poorly tuned, the agent may focus too heavily on one aspect (e.g., price) at the expense of integrative capabilities.

### Mechanism 2
- Claim: The semi-automated dataset creation method using GPT-J with human-in-the-loop post-editing ensures high-quality, diverse training data.
- Mechanism: GPT-J generates dialogue flows based on defined intents and negotiation scenarios. Human experts then edit for grounding, fluency, and factual accuracy, filtering out low-quality utterances.
- Core assumption: Large language models can generate reasonable dialogue flows that, with minimal human correction, produce high-quality training data.
- Evidence anchors:
  - [abstract] "Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality."
  - [section] "To ensure the quality of the automatically generated dataset, we implemented manual correction and filtration steps."
  - [corpus] Weak evidence; no specific mention of dataset creation methods in related work.
- Break condition: If GPT-J's hallucinations are too frequent or severe, human effort may become prohibitive, or the dataset may lack sufficient diversity.

### Mechanism 3
- Claim: The PPO-based RL fine-tuning on the supervised model enables the agent to learn adaptive negotiation strategies beyond fixed rules.
- Mechanism: Starting from a supervised model fine-tuned on the IND dataset, the agent is further trained using PPO loss with the novel reward function. This allows the agent to explore and learn effective negotiation strategies that maximize cumulative reward.
- Core assumption: Reinforcement learning can improve upon supervised learning by discovering negotiation strategies that optimize long-term reward, not just next-step accuracy.
- Evidence anchors:
  - [abstract] "Using the constructed dataset, we build an integrative negotiation-powered dialogue agent (INA) using a supervised learning (SL) + reinforcement learning (RL) approach."
  - [section] "To train our system, we leverage a novel reward function and maximize it using PPO loss (Schulman et al., 2017) to ensure aspects of negotiation consistency, negotiation power, and intent consistency."
  - [corpus] Weak evidence; no specific mention of RL methodology in related work.
- Break condition: If the reward signal is sparse or the exploration-exploitation tradeoff is not well-managed, the RL agent may converge to suboptimal or unstable negotiation strategies.

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: PPO is used to fine-tune the supervised model using the novel reward function, allowing the agent to learn adaptive negotiation strategies that maximize cumulative reward.
  - Quick check question: How does PPO's clipped objective help stabilize training compared to vanilla policy gradient methods?

- Concept: Intent Classification and Consistency
  - Why needed here: Ensuring the agent's generated utterances match the intended negotiation strategy is crucial for coherent and effective negotiation. The intent consistency reward (R1) and a separate BERT-based intent classifier are used to enforce this.
  - Quick check question: What are the potential challenges in multi-intent scenarios, and how might they be addressed?

- Concept: Multi-objective Reward Design
  - Why needed here: The negotiation task involves multiple, sometimes competing objectives (e.g., maximizing price, ensuring fairness, maintaining fluency). The reward function R = γ1R1 + γ2R2 + γ3R3 + γ4R4 balances these objectives.
  - Quick check question: How would you approach tuning the reward weights (γ1, γ2, γ3, γ4) to achieve the desired balance?

## Architecture Onboarding

- Component map:
  - Background Database -> Dialogue Flow Generator -> GPT-J Prompting -> Human-in-the-Loop Post-editing -> Supervised Model -> RL Model

- Critical path:
  1. Define negotiation intents and background database
  2. Generate dialogue flows and prompt GPT-J
  3. Human-in-the-loop post-editing and dataset creation
  4. Fine-tune supervised model on IND dataset
  5. Fine-tune RL model using PPO and reward function
  6. Evaluate and iterate

- Design tradeoffs:
  - Dataset creation: Semi-automated vs. fully manual (tradeoff between efficiency and quality control)
  - Reward function: Complexity vs. interpretability (more components may be harder to tune)
  - RL fine-tuning: Exploration vs. exploitation (too much exploration may lead to instability)

- Failure signatures:
  - Supervised model: Low BLEU/METEOR scores, poor intent classification accuracy
  - RL model: Unstable training, poor negotiation performance, reward hacking
  - Dataset: Low fluency ratings, factual hallucinations, lack of diversity

- First 3 experiments:
  1. Ablation study on reward components: Train models with different subsets of rewards (e.g., only intent consistency, only price gap) and compare performance
  2. Dataset size study: Train models on varying amounts of IND data (e.g., 25%, 50%, 100%) to assess data efficiency
  3. Reward weight tuning: Systematically vary the reward weights (γ1, γ2, γ3, γ4) and evaluate their impact on negotiation performance and other metrics

## Open Questions the Paper Calls Out

- How does the incorporation of customer personas, such as age, gender, and hobbies, impact the negotiation strategies and outcomes of INA?
  - Basis in paper: [explicit]
  - Why unresolved: The paper mentions that incorporating customer personas is an area for future exploration but does not investigate its impact on INA's negotiation performance.
  - What evidence would resolve it: Evaluating INA's negotiation performance with and without customer personas incorporated would provide evidence of the impact of personas on negotiation strategies and outcomes.

## Limitations

- The optimal weighting of the four reward components remains unclear without systematic sensitivity analysis
- The semi-automated dataset creation method's actual human effort requirements and quality control criteria are underspecified
- Performance on negotiation dialogues involving product categories beyond the 10 electronic items in the training data is unknown

## Confidence

- **High confidence**: The basic architecture of combining supervised learning with PPO-based reinforcement learning for negotiation dialogue is sound and well-established
- **Medium confidence**: The effectiveness of the four-component reward function in promoting integrative negotiation behavior, though the specific weightings appear somewhat arbitrary without systematic justification
- **Medium confidence**: The semi-automated dataset creation approach using GPT-J with human post-editing, though the actual quality control process and human effort requirements remain underspecified

## Next Checks

1. **Reward weight ablation study**: Systematically vary the four reward weights (γ1, γ2, γ3, γ4) across multiple runs to identify which components are most critical and whether the current weighting scheme is optimal

2. **Dataset quality analysis**: Conduct a detailed error analysis of the IND dataset by having independent human annotators evaluate a sample of generated dialogues for factual accuracy, coherence, and diversity

3. **Transfer capability test**: Evaluate INA's performance on negotiation scenarios involving product bundles not present in the training data to assess its ability to generalize negotiation strategies to novel contexts