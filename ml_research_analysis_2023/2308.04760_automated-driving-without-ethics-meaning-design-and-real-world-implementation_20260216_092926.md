---
ver: rpa2
title: 'Automated Driving Without Ethics: Meaning, Design and Real-World Implementation'
arxiv_id: '2308.04760'
source_url: https://arxiv.org/abs/2308.04760
tags:
- moral
- ethics
- ethical
- machine
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the assumption that automated vehicles (AVs)
  should be treated as artificial moral agents capable of ethical deliberation. It
  argues that machines lack the semantic understanding and contextual awareness required
  for genuine moral reasoning.
---

# Automated Driving Without Ethics: Meaning, Design and Real-World Implementation

## Quick Facts
- arXiv ID: 2308.04760
- Source URL: https://arxiv.org/abs/2308.04760
- Reference count: 9
- Primary result: A computational framework for AVs that avoids moral agency while ensuring ethically-informed behavior through risk-based mitigation strategies and claim balancing.

## Executive Summary
This paper challenges the assumption that automated vehicles should be treated as artificial moral agents capable of ethical deliberation. The authors argue that machines lack the semantic understanding and contextual awareness required for genuine moral reasoning, and instead propose the Ethical Valence Theory (EVT) as a tactical ethics approach. The EVT reframes AV decision-making as claim mitigation rather than moral deliberation, using predefined parameters and decision rules to balance competing safety claims of road users. The framework was implemented using Markov decision processes with modified reward functions that incorporate ethical valences and three moral profiles (utilitarian, contractarian, egalitarian) for handling unavoidable collision scenarios.

## Method Summary
The EVT framework implements tactical ethics through a Markov decision process where the reward function includes performance metrics, traffic code adherence, proximity risk, and ethical valence-weighted harm. The system assigns ethical valences to road users based on vulnerability and claim strength, then uses value iteration to optimize policy decisions. When dilemmas arise, the framework bifurcates into three moral profiles for claim mitigation: utilitarian (maximizing overall welfare), contractarian (prioritizing legitimate expectations), and egalitarian (equal distribution of harm). The implementation requires perception and prediction modules to detect road users, ethical valence calculators to assign claim weights, and an MDP planner to compute optimal actions that minimize harm while respecting the selected moral profile.

## Key Results
- The EVT framework successfully avoids assigning moral agency to AVs while producing ethically-informed behavior through risk mitigation
- Implementation using MDPs with modified reward functions effectively balances competing safety claims in collision scenarios
- Three moral profiles (utilitarian, contractarian, egalitarian) provide different approaches to claim mitigation when harm is unavoidable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machines cannot be moral agents because they lack semantic understanding and causal reasoning, so ethical decision-making must be structured as claim mitigation rather than genuine moral deliberation.
- Mechanism: The Ethical Valence Theory (EVT) reframes AV decision-making as risk mitigation where each road user is assigned a claim strength based on their vulnerability, and ethical valences weight those claims to prioritize minimally harmful outcomes.
- Core assumption: Ethical acceptability can be captured through predefined parameters and hierarchical valences rather than real-time moral reasoning.
- Evidence anchors: [abstract] The paper argues machines lack semantic understanding and contextual awareness required for genuine moral reasoning. [section 1.1] "Machines operate at the computational syntactic level... they have no understanding of the real-world meaning of the expressions... semantics in their representations are provided by programmers and users."

### Mechanism 2
- Claim: Tactical ethics provides normative criteria for AVs to align decisions with human moral expectations in ethically salient contexts, distinct from structural ethics that governs design principles.
- Mechanism: The EVT uses moral profiles (contractarian, utilitarian, egalitarian) that apply different claim mitigation strategies based on the situation, allowing the AV to balance competing safety claims while respecting acceptability criteria.
- Core assumption: Public moral expectations can be formalized into decision rules that guide AV behavior without requiring the machine to "understand" morality.
- Evidence anchors: [section 2.2] "The goal of the A V is to maximally satisfy as many claims as possible... when a conflict across claimants occurs, the vehicle will adopt a specific mitigation strategy (or profile) to decide how to respond to these claims." [section 2.1] "Tactical ethics ensures that machines behave as if they were moral agents in situations where moral agency is likely required or expected in the eyes of society."

### Mechanism 3
- Claim: The EVT can be implemented computationally using Markov Decision Processes (MDPs) with modified reward functions that incorporate ethical valences and claim mitigation strategies.
- Mechanism: The reward function includes terms for performance, traffic code adherence, proximity risk, and ethical valence-weighted harm, allowing the AV to optimize for minimal harm while respecting moral profiles.
- Core assumption: Ethical decision-making can be reduced to mathematical optimization problems without losing essential moral considerations.
- Evidence anchors: [section 2.3] "Equation 2 represents this previously defined reward at a state... The term /u1D460conseq is composed by a cost... related to the physical proximity between the A V and other road users." [section 2.4] "Using the value iteration algorithm... the bifurcation between dilemma and normal situation occurs in the policy optimization operation."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and reinforcement learning
  - Why needed here: The EVT framework is implemented using MDPs where states represent AV positions, actions are driving maneuvers, and rewards incorporate ethical considerations.
  - Quick check question: How does the Bellman equation update value estimates in an MDP, and how would you modify it to include ethical valence terms?

- Concept: Contractarian, utilitarian, and egalitarian moral theories
  - Why needed here: These three moral profiles provide different approaches to claim mitigation when collisions are unavoidable, each prioritizing claims differently.
  - Quick check question: What is the key difference between utilitarian and egalitarian approaches to distributing harm in unavoidable collision scenarios?

- Concept: Ethical salience and claim strength
  - Why needed here: The EVT framework requires identifying which features of a situation carry ethical weight (ethical salience) and how to quantify the strength of each road user's claim to safety.
  - Quick check question: How would you determine the ethical salience of a pedestrian's claim versus an AV passenger's claim in a given scenario?

## Architecture Onboarding

- Component map: Perception module -> Prediction module -> Ethical valence calculator -> MDP planner -> Policy executor
- Critical path: Perception → Prediction → Ethical Valence Calculation → MDP Planning → Policy Execution
  - This path must operate within the AV's real-time constraints (typically 10-100ms)

- Design tradeoffs:
  - Granularity vs. computational cost: More detailed ethical valence criteria improve decision quality but increase computation time
  - Fixed vs. adaptive profiles: Static moral profiles are simpler but may not adapt to changing societal expectations
  - Transparency vs. complexity: More complex ethical models may produce better decisions but be harder to explain to stakeholders

- Failure signatures:
  - System paralysis: Excessive computational load or conflicting ethical criteria causing decision delays
  - Inappropriate prioritization: Wrong assignment of ethical valences leading to unacceptable decisions
  - Context blindness: Failure to recognize when a situation requires ethical deliberation vs. normal driving

- First 3 experiments:
  1. Implement a simplified EVT with only two ethical valences (passenger vs. pedestrian) and test in basic collision scenarios
  2. Add multiple road user types and test the system's ability to prioritize claims appropriately
  3. Implement all three moral profiles and test their behavior in identical scenarios to compare outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Ethical Valence Theory be empirically validated against real-world accident data to ensure that the valence weights and claim hierarchies reflect actual public acceptability and societal values?
- Basis in paper: [explicit] The paper acknowledges that valences should reflect "acceptability-specific criteria" and that "empirical studies into acceptability" are needed, but does not describe concrete validation methods.
- Why unresolved: Current implementations rely on predefined parameters and ethical profiles, but lack empirical grounding in real-world accident scenarios and public opinion.
- What evidence would resolve it: Large-scale accident databases with detailed harm assessments, coupled with public surveys or experiments measuring acceptability of AV decision outcomes under different ethical profiles.

### Open Question 2
- Question: How should the Ethical Valence Theory handle situations where multiple road users have conflicting claims of equal moral weight, such as when two pedestrians of the same vulnerability level are equally at risk?
- Basis in paper: [inferred] The EVT framework discusses claim mitigation but does not explicitly address tie-breaking mechanisms when claims are equal in strength.
- Why unresolved: The theory prioritizes claims based on valence strength, but does not specify procedures for resolving conflicts when no clear priority exists.
- What evidence would resolve it: Experimental studies comparing different tie-breaking rules (e.g., first-come-first-served, random selection, or context-dependent factors) and their social acceptability.

### Open Question 3
- Question: What are the legal and regulatory implications of implementing the Ethical Valence Theory in AVs, particularly regarding liability assignment when harm occurs despite adherence to the system's ethical profiles?
- Basis in paper: [explicit] The paper notes that the "locus of an AV's moral agency" shifts to human designers and stakeholders, implying complex liability questions.
- Why unresolved: Current legal frameworks are not designed for AVs operating under tactical ethics systems that balance competing claims rather than following absolute rules.
- What evidence would resolve it: Case studies of AV accidents under different ethical profiles, coupled with legal analyses of liability assignment and insurance implications.

## Limitations
- The implementation details for ethical valence calibration remain underspecified, particularly regarding how weights are determined across different road user types and collision contexts.
- The "ethical deliberation" function in Algorithm 1 requires further specification on how expected harm calculations integrate with transition probabilities in practice.
- The framework lacks empirical validation against real-world accident data and public acceptability studies.

## Confidence
- **High Confidence**: The core argument that machines cannot be genuine moral agents due to semantic limitations - this is well-supported by philosophical literature and the paper's reasoning about computational vs. semantic understanding.
- **Medium Confidence**: The EVT framework's ability to capture acceptable behavior through predefined valences - while theoretically sound, practical implementation challenges and real-world validation are not fully demonstrated.
- **Low Confidence**: The computational implementation details, particularly the integration of ethical considerations into MDP reward functions and the practical performance of the three moral profiles in complex scenarios.

## Next Checks
1. **Feasibility Validation**: Implement a minimal EVT prototype with two road user types and test whether the claim balancing mechanism produces intuitively acceptable decisions in basic collision scenarios.
2. **Parameter Sensitivity Analysis**: Systematically vary ethical valence weights and profile parameters to determine how robust the decision outcomes are to parameter uncertainty.
3. **Real-World Alignment Study**: Conduct a human factors study comparing EVT decisions against human moral judgments in matched scenarios to assess whether the tactical ethics approach aligns with societal expectations.