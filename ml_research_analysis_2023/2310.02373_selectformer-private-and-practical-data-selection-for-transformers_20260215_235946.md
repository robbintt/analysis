---
ver: rpa2
title: 'SelectFormer: Private and Practical Data Selection for Transformers'
arxiv_id: '2310.02373'
source_url: https://arxiv.org/abs/2310.02373
tags:
- data
- selection
- accuracy
- proxy
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of privately selecting training
  data for Transformer models in a secure, multi-party computation (MPC) setting,
  where both data and model privacy must be preserved. The core idea is to emulate
  high-dimensional nonlinear operations in Transformers with low-dimensional MLPs
  trained on a small subset of data, reducing MPC overhead while retaining selection
  accuracy.
---

# SelectFormer: Private and Practical Data Selection for Transformers

## Quick Facts
- arXiv ID: 2310.02373
- Source URL: https://arxiv.org/abs/2310.02373
- Reference count: 13
- Achieves up to two orders of magnitude reduction in selection delay while maintaining test accuracy within 0.20% of baseline

## Executive Summary
SelectFormer addresses the challenge of privately selecting training data for Transformer models in a secure multi-party computation (MPC) setting. The method emulates high-dimensional nonlinear operations in Transformers with low-dimensional MLPs trained on small data subsets, significantly reducing MPC overhead while retaining selection accuracy. Through a multi-phase selection strategy and parallel MPC execution, the approach achieves substantial efficiency improvements over prior methods like MPCFORMER across various NLP and CV benchmarks.

## Method Summary
The method employs a multi-phase MPC-based selection pipeline using proxy models with MLP-approximated nonlinear operations. Data owners and model owners jointly select data through progressive filtering: early phases use cheap proxy models to filter most data, while later phases employ more accurate models for precise selection from survivors. MLPs substitute for Transformer nonlinearities (softmax, layer norm reciprocal, entropy), reducing dimensionality and MPC communication costs. Parallel execution of MPC operations across batches further reduces end-to-end delay.

## Key Results
- Achieves up to two orders of magnitude reduction in selection delay (from thousands to tens of hours)
- Maintains test accuracy within 0.20% of Oracle baseline
- Outperforms MPCFORMER in both speed and accuracy across multiple datasets
- Effective across various NLP (SST2, QNLI, QQP, AGNEWS, YELP) and CV (CIFAR-10, CIFAR-100) benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Emulating high-dimensional Transformer nonlinearity with low-dimensional MLPs reduces MPC communication and computation costs significantly while retaining selection accuracy. The nonlinear operations (softmax, layer norm reciprocal, and entropy computation) in Transformer blocks are substituted with MLPs trained on distributions of actual model activations. These MLPs reduce dimensionality (e.g., 512-dimensional softmax → 2-dimensional MLP) and thus cut MPC communication rounds and data exchange. The core assumption is that MLPs can approximate any continuous function on a closed bounded subset of Rn.

### Mechanism 2
Multi-phase selection progressively filters data using increasingly capable proxy models, balancing efficiency and accuracy. In phase i, a proxy model selects a subset Si from Si−1 with selectivity αi < 1. Early phases use cheaper, lower-dimensional models to quickly filter most data; later phases use more accurate, higher-dimensional models for precise selection from the survivors. The core assumption is that earlier low-cost filtering does not significantly degrade the quality of final selection.

### Mechanism 3
Parallel MPC execution overlaps communication and computation across batches to hide latency and reduce end-to-end selection delay. Latency-bound MPC operations from multiple batches are stacked and executed in parallel with bandwidth-bound operations; computation for subsequent batches overlaps with communication for current batches. The core assumption is that the underlying MPC framework supports concurrent operations and the network/hardware allow overlapping communication and computation without deadlock.

## Foundational Learning

- **Secure Multi-Party Computation (MPC) and secret sharing**: Why needed here - Data selection must preserve privacy of both model weights and data points; MPC enables joint evaluation without revealing inputs. Quick check question: In 2PC secret sharing, if x = x1 + x2 and y = y1 + y2, how do the parties compute z = xy without revealing x or y?

- **Transformer architecture and attention mechanism**: Why needed here - The selection pipeline targets Transformer models; understanding QKV, softmax, layer norm, and FFN is essential to know where nonlinearities are and how they can be approximated. Quick check question: What is the dimensionality of the softmax output in a standard Transformer multi-head attention with hidden dimension 768 and 12 heads?

- **Active learning and proxy model selection**: Why needed here - Data selection uses entropy of model predictions; proxy models approximate target model predictions to avoid expensive direct evaluation. Quick check question: Why does higher prediction entropy indicate a more valuable data point for training?

## Architecture Onboarding

- **Component map**: Bootstrap data exchange → Proxy model generation (ex vivo + in vivo MLP training) → Multi-phase selection pipeline (parallel MPC execution + entropy ranking) → Final selected dataset

- **Critical path**: 1. Ex vivo MLP training (offline, from Mg activations on Sboot) 2. In vivo finetuning of ˆMi on Sboot 3. Phase 1 MPC forward pass with small MLP-dim proxy 4. Parallel batch execution to hide latency 5. Subsequent phases progressively refine selection

- **Design tradeoffs**: Higher MLP hidden dimension → better accuracy but higher MPC cost; More selection phases → lower total delay but more total work; Larger proxy width/depth → better accuracy but slower early filtering; Overlap degree → depends on network latency vs bandwidth constraints

- **Failure signatures**: Accuracy collapse: MLPs poorly trained (bad µ, σ estimates) → noisy entropy ranking; Delay explosion: Too many phases or too large proxies in early phases → MPC cost outweighs filtering benefit; Privacy breach: Protocol deviations or side-channel leaks during parallel execution

- **First 3 experiments**: 1. Single-phase selection with default MLP dimension (d=16) on SST2 with DistilBERT; measure delay vs Oracle 2. Ablation: Remove MLPs (NoApprox) to confirm delay reduction comes from dimensionality reduction 3. Multi-phase (2 phases) with (2,16) dims; compare accuracy and delay to single-phase

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's performance scale when selecting from datasets significantly larger than the 10K-100K range tested, and what is the theoretical limit of its efficiency gains? The paper does not provide theoretical analysis or experimental results for datasets significantly larger than the tested range, leaving the scalability and efficiency gains of the method on larger datasets unknown.

### Open Question 2
How does the proposed method's performance compare to other state-of-the-art data selection techniques that do not rely on MPC, such as active learning or core-set selection, in terms of both accuracy and computational efficiency? The paper does not provide a comprehensive comparison of the proposed method's performance against other data selection techniques, leaving the relative advantages and disadvantages of the method unclear.

### Open Question 3
How does the proposed method's performance vary across different Transformer architectures and model sizes, and what is the impact of model complexity on the method's efficiency gains? The paper tests the proposed method on BERT, DistilBERT, and ViT models, but does not explore the method's performance across a wider range of Transformer architectures and model sizes.

## Limitations

- Scalability to large datasets: The multi-phase approach's efficiency gains may diminish on datasets with millions of examples where early-phase filtering overhead becomes significant
- MPC framework dependency: Performance improvements rely heavily on specific MPC implementation details that are not fully specified
- Distribution mismatch in MLP training: The ex vivo MLP training assumes activation distributions from proxy models on bootstrap data accurately represent distributions on full dataset

## Confidence

- **High confidence** in the fundamental mechanism: Dimensionality reduction through MLP approximation demonstrably reduces MPC communication costs
- **Medium confidence** in multi-phase selection benefits: While concept is sound and experimental results support it, sensitivity to phase scheduling parameters remains a concern
- **Medium confidence** in parallel execution gains: The claimed latency hiding through parallel execution is plausible but specific implementation details are not fully specified

## Next Checks

1. Measure the approximation error of MLPs on softmax, layer norm reciprocal, and entropy operations across the full activation distribution, not just bootstrap data
2. Systematically vary the number of phases and selectivity ratios across multiple datasets to identify regimes where multi-phase selection provides net benefit
3. Reimplement the core selection pipeline using a different MPC framework to verify that the claimed delay reduction is framework-independent