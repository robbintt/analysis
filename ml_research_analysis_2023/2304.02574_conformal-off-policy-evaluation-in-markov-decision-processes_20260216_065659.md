---
ver: rpa2
title: Conformal Off-Policy Evaluation in Markov Decision Processes
arxiv_id: '2304.02574'
source_url: https://arxiv.org/abs/2304.02574
tags:
- policy
- distribution
- data
- conformal
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses off-policy evaluation (OPE) in finite-horizon
  Markov Decision Processes (MDPs), aiming to estimate the value of a target policy
  using data collected under a different behavior policy. The key challenge is the
  distribution shift between the target and behavior policies.
---

# Conformal Off-Policy Evaluation in Markov Decision Processes

## Quick Facts
- arXiv ID: 2304.02574
- Source URL: https://arxiv.org/abs/2304.02574
- Reference count: 33
- Primary result: Novel conformal prediction methods for off-policy evaluation that achieve smaller confidence intervals while maintaining coverage guarantees

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) in finite-horizon Markov Decision Processes, where the goal is to estimate the value of a target policy using data collected under a different behavior policy. The key innovation is applying conformal prediction to construct confidence intervals that account for distribution shift between policies. The authors propose several score functions, including asymmetric and shifted values approaches, which reduce interval lengths compared to existing methods while maintaining the same coverage guarantees. Empirical evaluation on an inventory control problem demonstrates that these methods achieve smaller intervals while retaining desired coverage levels.

## Method Summary
The method extends conformal prediction to handle distribution shift in off-policy evaluation by incorporating likelihood ratios between target and behavior policies. The approach uses training data to estimate quantiles of the behavior policy and weights (either empirically or via gradient methods), then applies these to calibration data to compute non-conformity scores. Three main score functions are proposed: pinball scores (symmetric), double-quantile scores (asymmetric), and shifted values scores. These score functions are used to construct conformalized confidence sets that contain the true value of the target policy with a prescribed level of certainty (1-α). The method builds on existing OPE techniques like Weighted Doubly Robust estimation while adding conformal prediction guarantees.

## Key Results
- Asymmetric score functions reduce interval length by independently estimating lower and upper bounds
- Shifted values approach directly estimates target policy quantiles using raw values as scores
- Both methods maintain coverage guarantees while achieving smaller intervals than baseline approaches
- Empirical evaluation on inventory control shows improved performance across different policy similarity levels

## Why This Works (Mechanism)

### Mechanism 1: Weighted conformal prediction
Weighted conformal prediction re-centers confidence intervals around the target policy's value rather than the behavior policy's value by re-weighting calibration scores using likelihood ratios between target and behavior policies. This shift reduces interval length by correcting for distribution shift. The core assumption is that likelihood ratios can be accurately estimated from training data. Break condition: coverage guarantees fail if likelihood ratio estimation is biased or has high variance.

### Mechanism 2: Asymmetric score functions
Asymmetric double-quantile scores reduce interval conservatism by independently estimating lower and upper bounds rather than using symmetric scores centered on the behavior policy median. This allows intervals to shift toward the target policy value. The core assumption is that separate quantile estimates for bounds are more efficient than symmetric approaches for capturing asymmetric distributions. Break condition: interval accuracy suffers if quantile estimates are inaccurate or the distribution is highly skewed.

### Mechanism 3: Shifted values approach
The shifted values approach directly estimates target policy quantiles by using raw values as scores, eliminating the need for quantile estimation. Using the score function s(x,y) = y, the conformalized set is built directly from the distribution of target policy values. The core assumption is that the value distribution itself contains sufficient information for accurate interval estimation. Break condition: interval accuracy suffers if the value distribution has high variance or likelihood ratio estimation is poor.

## Foundational Learning

- **Conformal prediction fundamentals**: Understanding how coverage guarantees work under weighted exchangeability is essential for extending standard conformal prediction to handle distribution shift. Quick check: How does conformal prediction maintain coverage guarantees when data comes from a different distribution than the target policy?

- **Importance sampling and likelihood ratios**: The core challenge involves estimating likelihood ratios between target and behavior policies to correct for distribution shift, which is fundamental to the weighted conformal approach. Quick check: What is the mathematical relationship between the likelihood ratio and the distribution shift in off-policy evaluation?

- **Markov Decision Process value functions**: Understanding how policy values are computed and how distribution shift affects their estimation is necessary for evaluating value functions in finite-horizon MDPs. Quick check: How does the distribution shift between behavior and target policies affect the expected cumulative reward in an MDP?

## Architecture Onboarding

- **Component map**: Training data → Likelihood ratio estimation → Quantile estimation (behavior policy) → Score computation → Calibration data → Weighted CDF → Conformalized interval; Alternatives include empirical vs gradient-based weight estimation, pinball vs double-quantile vs shifted values scoring.
- **Critical path**: Accurate likelihood ratio estimation and proper score function selection are most critical, as errors in either directly impact coverage and interval length.
- **Design tradeoffs**: Empirical weight estimation is simple but requires sufficient trajectory counts per state-value pair; gradient methods scale better but suffer from high variance; pinball scores are conservative while asymmetric approaches reduce length but may be more sensitive to estimation errors.
- **Failure signatures**: Coverage dropping below target level indicates likelihood ratio estimation errors; overly wide intervals suggest conservative score functions; computational bottlenecks indicate inefficient weight estimation methods.
- **First 3 experiments**:
  1. Implement basic conformal prediction with pinball scores on synthetic MDP data to verify coverage guarantees hold.
  2. Add likelihood ratio estimation using empirical method and compare interval lengths against ground truth.
  3. Implement double-quantile score functions and evaluate coverage vs interval length tradeoff across different policy similarity levels.

## Open Questions the Paper Calls Out

### Open Question 1
How can we efficiently learn likelihood ratios using neural networks to improve the scalability of conformalized off-policy evaluation methods? The paper identifies this as a computational challenge where current gradient methods have difficulties learning likelihood ratios for greatly differing policies, suggesting investigation into neural network-based approaches for improved scalability and accuracy.

### Open Question 2
Can we develop improved score functions that adapt to the specific characteristics of the MDP and the distribution shift between target and behavior policies? While the paper demonstrates success with asymmetric and shifted values approaches, it suggests these are preliminary improvements and leaves room for further development of potentially more effective score functions.

### Open Question 3
How does the choice of coverage level (1-α) affect the performance trade-off between interval length and coverage guarantees in conformalized off-policy evaluation? The paper demonstrates the method at a single coverage level (90%) but does not explore how varying this level impacts the balance between interval precision and reliability across different MDP settings.

## Limitations
- Method relies heavily on accurate likelihood ratio estimation, which becomes data-intensive in high-dimensional state spaces
- Gradient-based weight estimation suffers from high variance, limiting practical utility despite computational advantages
- Performance degrades when behavior and target policies are very dissimilar

## Confidence

- **High**: Coverage guarantee preservation across all proposed score functions (Propositions 3-4)
- **Medium**: Empirical results showing reduced interval lengths compared to baseline methods
- **Low**: Scalability claims for gradient-based methods given acknowledged high variance issues

## Next Checks

1. Test coverage robustness across varying degrees of policy dissimilarity beyond the toy inventory control setting
2. Implement and evaluate the gradient-based weight estimation method to quantify the practical impact of high variance
3. Compare computational efficiency of empirical vs gradient-based methods on larger state spaces to validate scalability claims