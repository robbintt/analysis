---
ver: rpa2
title: Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models
arxiv_id: '2309.14928'
source_url: https://arxiv.org/abs/2309.14928
tags:
- ntua
- cache
- clip
- pseudo-labels
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NtUA, a noise-tolerant unsupervised adapter
  that adapts pre-trained vision-language models like CLIP to new image classification
  tasks using only a few unlabelled target samples. The core innovation is a weighted
  key-value cache that stores visual features and pseudo-labels of target samples,
  with weights based on prediction confidence.
---

# Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models

## Quick Facts
- arXiv ID: 2309.14928
- Source URL: https://arxiv.org/abs/2309.14928
- Authors: 
- Reference count: 40
- Key outcome: NtUA achieves 5.06% average accuracy gain over Tip-Adapter in 16-shot settings across 11 image classification benchmarks

## Executive Summary
This paper introduces NtUA, a noise-tolerant unsupervised adapter that adapts pre-trained vision-language models like CLIP to new image classification tasks using only a few unlabelled target samples. The core innovation is a weighted key-value cache that stores visual features and pseudo-labels of target samples, with weights based on prediction confidence. NtUA also incorporates pseudo-label rectification using knowledge distillation from large-scale vision-language models to improve label quality. Experiments on 11 widely adopted image classification benchmarks show that NtUA consistently outperforms existing state-of-the-art methods.

## Method Summary
NtUA adapts CLIP to new image classification tasks using unlabelled target samples through a two-stage process. First, it constructs a weighted key-value cache storing visual features as keys, pseudo-labels as values, and prediction confidence scores as weights. Second, it performs pseudo-label rectification using knowledge distillation from a larger CLIP model to refine both the pseudo-labels and their confidence weights. The final predictions combine weighted cache outputs with direct CLIP logits, creating a hybrid approach that balances learned adaptation with zero-shot generalization.

## Key Results
- NtUA achieves 5.06% average accuracy gain over Tip-Adapter in 16-shot settings
- Outperforms state-of-the-art methods across 11 image classification benchmarks
- Effective even when using less powerful visual encoders like ResNet-50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted key-value cache improves pseudo-label robustness by down-weighting low-confidence predictions
- Mechanism: NtUA stores CLIP-extracted features as keys, pseudo-labels as values, and prediction confidence scores as weights. During inference, the cache query uses these weights to scale contributions from each stored pair, reducing the influence of noisy pseudo-labels
- Core assumption: Prediction confidence is correlated with pseudo-label accuracy, so weighting by confidence improves downstream performance
- Evidence anchors: [abstract] "adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence"; [section 3.2.1] "NtUA is more robust to pseudo-label noises by incorporating cache weights ˆCtrain, since the accuracy of pseudo-labels is closely related to the prediction confidence"
- Break condition: If confidence scores do not correlate with label accuracy (e.g., overconfident but wrong predictions), weighting could amplify errors

### Mechanism 2
- Claim: Knowledge distillation refines pseudo-labels and their confidence weights by leveraging a larger CLIP model
- Mechanism: NtUA uses a supplementary CLIP model (e.g., ViT-L/14) to generate distilled visual features and re-predict pseudo-labels. These distilled labels and confidences replace the original ones in the cache, improving their quality
- Core assumption: A larger CLIP model provides more accurate pseudo-labels than the original CLIP used for adaptation
- Evidence anchors: [abstract] "pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models"; [section 3.2.2] "NtUA adopt the visual encoder Ekd v to get the accumulated visual features... To generate pseudo-labels corresponding to the distilled visual features, NtUA multiplies the distilled features vector Fkd train with the CLIP's classifier weights W and converts the pseudo-labels to one-hot vectors"
- Break condition: If the larger CLIP model does not generalize well to the target domain, distillation could propagate its biases or errors

### Mechanism 3
- Claim: Combining cache predictions with direct CLIP logits balances learned adaptation with zero-shot generalization
- Mechanism: The final prediction combines weighted cache output and raw CLIP logits: P_ntua = αCtrain φ(f · FT)ˆLtrain + f · WT. This hybrid approach ensures that the model does not rely solely on potentially noisy cached knowledge
- Core assumption: Direct CLIP logits retain useful generalization capability even after adaptation
- Evidence anchors: [section 3.2.1] "NtUA combines the predictions from the weighted cache model and the predictions from the CLIP model as follows: PNtUA(ftrain) = α ˆCtrainφ(ftrain · FT train)ˆLtrain + ftrain · WT"; [section 3.2.3] "During inference, NtUA utilizes Eq. (1) to make predictions"
- Break condition: If the CLIP backbone is poorly aligned with the target domain, direct logits may dominate and override useful cache adaptation

## Foundational Learning

- Concept: CLIP vision-language pre-training via image-text matching
  - Why needed here: NtUA builds on CLIP's pre-trained encoders to extract features and generate pseudo-labels without labels
  - Quick check question: What are the two encoders in CLIP and what modalities do they process?

- Concept: Knowledge distillation from teacher to student model
  - Why needed here: NtUA uses a larger CLIP as a teacher to improve pseudo-label quality in the student cache
  - Quick check question: In knowledge distillation, what is transferred—logits, features, or both—and why?

- Concept: Cache-based retrieval for efficient feature adaptation
  - Why needed here: The key-value cache stores per-sample features and labels to enable fast adaptation without full fine-tuning
  - Quick check question: How does a cache model differ from standard fine-tuning in terms of parameter updates?

## Architecture Onboarding

- Component map:
  - CLIP backbone (ResNet-50 or ViT-B/16) -> visual feature extractor
  - CLIP textual encoder -> generates classifier weights from class names
  - Weighted key-value cache -> stores features (keys), pseudo-labels (values), confidences (weights)
  - Distillation CLIP (e.g., ViT-L/14) -> generates refined pseudo-labels and confidences
  - Hybrid prediction layer -> combines cache output with direct CLIP logits

- Critical path:
  1. Extract features from unlabelled target samples
  2. Generate initial pseudo-labels and confidences
  3. Build weighted cache
  4. Perform knowledge distillation to refine pseudo-labels and weights
  5. Fine-tune cache keys with refined cache
  6. Inference combines cache and CLIP logits

- Design tradeoffs:
  - Using larger CLIP for distillation improves pseudo-label quality but increases compute cost
  - Weighting by confidence reduces noise but may discard useful low-confidence samples
  - Hybrid prediction preserves CLIP generalization but may slow convergence if cache is accurate

- Failure signatures:
  - Performance drops when confidence weighting removes too many samples (low cache size)
  - Overfitting to distilled pseudo-labels if distillation model is too strong relative to target domain
  - Degraded accuracy if cache keys are not fine-tuned enough

- First 3 experiments:
  1. Verify cache weights correlate with pseudo-label accuracy on a validation set
  2. Test ablation: cache only vs. cache + distillation vs. full NtUA
  3. Vary cache size (shots) to find the sweet spot before noise dominates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of visual encoder backbone affect the performance of NtUA in pseudo-label rectification?
- Basis in paper: [explicit] The paper mentions that using different large-scale CLIP visual encoders like ViT-L/14, ViT-B/32, ViT-B/16, RN50×4, RN50×16, RN50×64, and RN50 leads to significant performance gains for NtUA
- Why unresolved: While the paper provides some insights into the impact of different visual encoders, it does not provide a comprehensive analysis of how each encoder affects the performance of NtUA
- What evidence would resolve it: A detailed study comparing the performance of NtUA with each visual encoder backbone on various datasets would provide a clearer understanding of their impact

### Open Question 2
- Question: Can NtUA be effectively applied to other vision-language models besides CLIP?
- Basis in paper: [inferred] The paper focuses on adapting CLIP models using NtUA, but it does not explicitly state whether NtUA can be applied to other vision-language models
- Why unresolved: The paper does not explore the application of NtUA to other vision-language models, leaving the question open for future research
- What evidence would resolve it: Experiments applying NtUA to other vision-language models and comparing their performance with CLIP would provide insights into its generalizability

### Open Question 3
- Question: How does the weighting strategy in NtUA affect its performance in different scenarios?
- Basis in paper: [explicit] The paper mentions that both confidence and certainty can serve as effective weights in NtUA's cache models, but it does not provide a detailed analysis of their impact in different scenarios
- Why unresolved: The paper does not explore the impact of different weighting strategies on NtUA's performance in various scenarios, leaving the question open for further investigation
- What evidence would resolve it: A comprehensive study comparing the performance of NtUA with different weighting strategies in various scenarios would provide insights into their impact

## Limitations
- Effectiveness depends heavily on availability of a powerful teacher CLIP model for knowledge distillation
- Weighted cache approach assumes prediction confidence correlates with label accuracy, which may not hold for certain domain shifts
- Computational overhead from using larger CLIP models for distillation could limit practical deployment

## Confidence
- Claims about cache weighting effectiveness: High - supported by ablation studies and consistent performance gains
- Claims about knowledge distillation improving pseudo-label quality: Medium - demonstrated empirically but mechanism could be domain-dependent
- Claims about robustness across diverse datasets: Medium - strong results but limited dataset diversity
- Claims about computational efficiency: Low - actual runtime and memory costs not thoroughly analyzed

## Next Checks
1. Test confidence weighting correlation with pseudo-label accuracy across multiple domain shifts to verify the core assumption
2. Benchmark performance with varying teacher-student CLIP model size ratios to quantify distillation benefits
3. Measure wall-clock inference time and memory usage compared to baseline adapters to assess practical deployment costs