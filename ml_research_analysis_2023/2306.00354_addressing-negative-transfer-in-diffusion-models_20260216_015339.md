---
ver: rpa2
title: Addressing Negative Transfer in Diffusion Models
arxiv_id: '2306.00354'
source_url: https://arxiv.org/abs/2306.00354
tags:
- diffusion
- tasks
- task
- uni00000013
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes diffusion model training from a multi-task
  learning perspective and observes that negative transfer occurs due to conflicting
  denoising tasks at different noise levels. To address this, the authors propose
  clustering denoising tasks into temporally proximate groups using dynamic programming,
  and then applying multi-task learning methods like PCGrad, NashMTL, and Uncertainty
  Weighting to these clusters.
---

# Addressing Negative Transfer in Diffusion Models

## Quick Facts
- arXiv ID: 2306.00354
- Source URL: https://arxiv.org/abs/2306.00354
- Reference count: 40
- Key outcome: This paper analyzes diffusion model training from a multi-task learning perspective and observes that negative transfer occurs due to conflicting denoising tasks at different noise levels. To address this, the authors propose clustering denoising tasks into temporally proximate groups using dynamic programming, and then applying multi-task learning methods like PCGrad, NashMTL, and Uncertainty Weighting to these clusters. Experiments on FFHQ and CelebA-HQ datasets show that this approach improves both image quality (FID scores drop by 1.2-4.5 points) and training convergence speed compared to vanilla diffusion models, with consistent gains across both pixel-space and latent-space architectures.

## Executive Summary
This paper identifies and addresses a critical issue in diffusion model training: negative transfer arising from conflicting denoising tasks at different noise levels. The authors observe that training a single network on denoising tasks spanning the entire noise schedule leads to performance degradation due to conflicting gradients between tasks with different signal-to-noise ratios. To mitigate this, they propose clustering denoising tasks into temporally proximate groups and applying established multi-task learning methods within these clusters. This approach significantly improves both image quality and training efficiency across multiple datasets and model architectures.

## Method Summary
The method involves two key components: interval clustering and multi-task learning integration. First, denoising tasks are grouped into k temporally proximate clusters using dynamic programming, with three clustering objectives evaluated: timestep-based, SNR-based, and gradient-based. Second, within each cluster, multi-task learning methods (PCGrad, NashMTL, or Uncertainty Weighting) are applied to address conflicting gradients, gradient magnitude differences, and imbalanced loss scales. The training process samples a timestep, assigns the corresponding denoising task to its cluster, computes the loss, aggregates losses by cluster, and updates model parameters using the chosen MTL method.

## Key Results
- FID scores improve by 1.2-4.5 points compared to vanilla diffusion models
- Training convergence speed increases significantly across all tested configurations
- Improvements are consistent across both pixel-space and latent-space architectures
- Gradient-based clustering shows the best performance for LDM on FFHQ dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion training inherently suffers from negative transfer because denoising tasks at different noise levels have conflicting gradients.
- **Mechanism:** In multi-task learning, tasks with similar characteristics (here, similar timesteps/SNRs) share gradient directions, while tasks far apart in timesteps have divergent gradients. When these conflicting gradients are averaged during training, tasks with smaller gradients or losses are overshadowed, leading to degraded performance on those tasks.
- **Core assumption:** The gradient similarity between denoising tasks decreases as the difference in noise levels increases, and this gradient conflict directly impacts sample quality.
- **Evidence anchors:**
  - [abstract] "the task affinity between denoising tasks diminishes as the gap between noise levels widens" and "negative transfer can arise even in diffusion training"
  - [section 3.2] "Task affinity score assumes that cooperative (conflicting) tasks produce similar (conflicting) gradient directions" and "negative transfer refers to a multi-task learner's performance degradation due to task conflicts"
  - [corpus] weak – related works discuss gradient conflicts but not specifically in diffusion models
- **Break condition:** If task affinity does not correlate with noise level differences, or if gradient conflicts do not affect sample quality.

### Mechanism 2
- **Claim:** Grouping denoising tasks into temporally proximate clusters reduces negative transfer by limiting conflicting gradients to within-cluster interactions.
- **Mechanism:** Interval clustering groups denoising tasks by timestep proximity, ensuring that tasks within each cluster have high task affinity and thus compatible gradients. Multi-task learning methods (PCGrad, NashMTL, UW) are then applied within clusters, reducing the number of conflicting gradient pairs and balancing gradient magnitudes or loss scales.
- **Core assumption:** Task affinity is higher for tasks closer in timesteps, and reducing the number of conflicting gradient pairs improves sample quality.
- **Evidence anchors:**
  - [abstract] "interval clustering to enforce temporal proximity among denoising tasks within clusters" and "our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of multi-task learning methods"
  - [section 4.1] "we assign tasks in pairwise disjoint time intervals" and "inspired by (O1), we formulate the interval clustering problem which groups denoising tasks by pairwise disjoint timestep intervals"
  - [section 5.1] "Integration of MTL methods using interval clustering consistently improves FID scores and generally enhances precision compared to vanilla training"
  - [corpus] weak – interval clustering used in other domains but not for diffusion model denoising tasks
- **Break condition:** If task affinity does not improve within clusters, or if the chosen clustering cost does not align with actual gradient conflicts.

### Mechanism 3
- **Claim:** Applying existing multi-task learning methods within task clusters effectively balances gradient magnitudes and loss scales, further mitigating negative transfer.
- **Mechanism:** Within each cluster, methods like PCGrad project conflicting gradients, NashMTL balances gradient magnitudes via Nash bargaining, and Uncertainty Weighting adjusts loss weights based on task uncertainty. These methods address the three known causes of negative transfer: conflicting gradients, gradient magnitude differences, and imbalanced loss scales.
- **Core assumption:** The three causes of negative transfer (conflicting gradients, gradient magnitude differences, imbalanced loss scales) are present in diffusion training and can be mitigated by these MTL methods.
- **Evidence anchors:**
  - [abstract] "leveraging existing MTL methods, such as dealing with issues of conflicting gradients, differences in gradient magnitudes, and imbalanced loss scales"
  - [section 4.2] "PCgrad projects the gradient of a task onto the normal plane of the gradient of another task when there is a conflict between their gradients" and "NashMTL aims to update model parameters with weighted summed gradients by obtaining the Nash bargaining solution"
  - [section 5.2] "Task pair that shows the most gradient conflicts is I1 and I5, namely, task clusters apart in timesteps" and "both UW and NashMTL tend to allocate higher weights to task clusters that handle noisier inputs"
  - [corpus] moderate – these MTL methods are well-established but not previously applied to diffusion models
- **Break condition:** If the causes of negative transfer are not present in diffusion training, or if the MTL methods do not improve within-cluster training.

## Foundational Learning

- **Concept: Multi-task learning (MTL)**
  - Why needed here: Diffusion models train a single network on denoising tasks at multiple noise levels, which is inherently a multi-task learning problem.
  - Quick check question: What are the three main causes of negative transfer in MTL?

- **Concept: Negative transfer**
  - Why needed here: Understanding how conflicting tasks can degrade performance is crucial to diagnosing why vanilla diffusion training underperforms.
  - Quick check question: How does negative transfer manifest in diffusion models according to the paper?

- **Concept: Task affinity**
  - Why needed here: Measuring the similarity of gradients between tasks helps identify which denoising tasks are compatible and should be grouped together.
  - Quick check question: How is task affinity measured in the paper, and what does a high score indicate?

## Architecture Onboarding

- **Component map:** Core diffusion model -> Interval clustering module -> MTL method integration -> Training loop with per-cluster loss aggregation

- **Critical path:**
  1. During training, sample timestep t and corresponding denoising task Dt.
  2. Assign Dt to its interval cluster based on precomputed clustering.
  3. Compute loss Lt for Dt.
  4. Aggregate losses by cluster.
  5. Apply chosen MTL method within each cluster to update model parameters.

- **Design tradeoffs:**
  - Clustering granularity (k clusters): Too few clusters → insufficient task affinity within clusters; too many → computational overhead and risk of underfitting clusters.
  - Clustering objective: Timestep-based is simple but may not reflect actual gradient conflicts; SNR-based may better reflect perceptual difficulty; gradient-based is most accurate but requires storing gradients.
  - MTL method choice: PCGrad reduces gradient conflicts but may slow convergence; NashMTL balances gradients but requires solving an optimization per step; Uncertainty Weighting is simple but may sacrifice diversity.

- **Failure signatures:**
  - Performance degrades if clusters are too large (high task affinity variance) or too small (underfitting).
  - If clustering objective does not align with actual gradient conflicts, MTL methods will not help.
  - If the number of clusters is too high, computational cost increases and may negate benefits.

- **First 3 experiments:**
  1. **Ablation: No clustering vs. clustering (k=5) with vanilla training** – verifies that clustering alone improves performance.
  2. **Ablation: Clustering + PCGrad vs. Clustering + NashMTL vs. Clustering + UW** – identifies which MTL method works best for diffusion models.
  3. **Hyperparameter sweep: Vary k (2, 5, 8) with best MTL method** – finds optimal clustering granularity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of task clusters for diffusion model training?
- Basis in paper: [explicit] The paper notes that increasing the number of clusters from 5 to 8 shows similar results, but doesn't definitively state the optimal number.
- Why unresolved: The paper only experiments with a few values of k (number of clusters) and doesn't explore the full range or provide a theoretical basis for the optimal number.
- What evidence would resolve it: A comprehensive study varying k across a wide range, coupled with theoretical analysis of the trade-off between cluster granularity and negative transfer mitigation.

### Open Question 2
- Question: How do different interval clustering methods (timestep, SNR, gradient-based) compare in terms of final image quality?
- Basis in paper: [explicit] The paper shows that gradient-based clustering performs best for LDM on FFHQ, but doesn't provide a comprehensive comparison across all datasets and model types.
- Why unresolved: The paper only reports results for a subset of combinations, and doesn't provide a clear ranking or theoretical explanation for the observed differences.
- What evidence would resolve it: A systematic comparison of all clustering methods across multiple datasets and model types, along with an analysis of the underlying reasons for their performance differences.

### Open Question 3
- Question: Can architectural modifications specifically designed for multi-task learning further improve diffusion model performance?
- Basis in paper: [inferred] The paper mentions that previous works use timestep and noise level as input, which can be considered as using task embeddings. This suggests that there might be room for architectural improvements.
- Why unresolved: The paper focuses on model-agnostic MTL methods and doesn't explore architectural modifications.
- What evidence would resolve it: Experiments comparing diffusion models with and without task-specific architectural components, such as separate heads for different noise levels or dynamic routing mechanisms.

## Limitations

- The core claim that gradient conflicts between denoising tasks at different noise levels cause negative transfer is supported by strong theoretical reasoning but has limited direct empirical evidence from the paper itself.
- The improvement could potentially be explained by other factors such as better regularization or implicit curriculum learning from the clustering process.
- The gradient-based clustering objective requires storing gradients for all timestep-task pairs which is computationally expensive and not demonstrated in the main experiments.

## Confidence

- **High confidence**: That the proposed approach (clustering + MTL methods) improves diffusion model training performance as measured by FID scores and convergence speed.
- **Medium confidence**: That negative transfer is the primary mechanism driving the improvements, as this relies on the assumed relationship between task affinity, gradient conflicts, and sample quality.
- **Low confidence**: In the specific gradient-based clustering objective, as it requires storing gradients for all timestep-task pairs which is computationally expensive and not demonstrated in the main experiments.

## Next Checks

1. **Direct gradient conflict measurement**: Monitor the angle between gradients of denoising tasks at different noise levels during training to empirically verify that conflicts decrease after clustering and MTL application.
2. **Task affinity correlation study**: Measure the correlation between task affinity scores (from different clustering objectives) and actual gradient conflicts to validate which clustering objective best captures the underlying problem.
3. **Controlled ablation on task similarity**: Train diffusion models where denoising tasks are artificially made more or less similar (e.g., by modifying noise schedules) and verify that the proposed approach's benefits scale with the degree of task conflict.