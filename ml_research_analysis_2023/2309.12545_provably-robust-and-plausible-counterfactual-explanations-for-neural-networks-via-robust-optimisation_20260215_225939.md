---
ver: rpa2
title: Provably Robust and Plausible Counterfactual Explanations for Neural Networks
  via Robust Optimisation
arxiv_id: '2309.12545'
source_url: https://arxiv.org/abs/2309.12545
tags:
- u1d465
- u1d456
- robust
- robustness
- u1d451
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PROPLACE, a method for generating provably
  robust and plausible counterfactual explanations for neural networks. It addresses
  the problem of existing methods lacking formal robustness guarantees and plausibility
  in their counterfactual explanations.
---

# Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation

## Quick Facts
- arXiv ID: 2309.12545
- Source URL: https://arxiv.org/abs/2309.12545
- Authors: 
- Reference count: 11
- Key outcome: PROPLACE achieves state-of-the-art performance in robustness and plausibility metrics while maintaining superior proximity among robust baselines.

## Executive Summary
This paper introduces PROPLACE, a method for generating provably robust and plausible counterfactual explanations for neural networks. The method addresses the problem of existing approaches lacking formal robustness guarantees and plausibility by leveraging robust optimization techniques. PROPLACE formulates an iterative algorithm that computes counterfactual explanations guaranteed to be robust under bounded model parameter changes while optimizing for proximity and plausibility. The algorithm's convergence, soundness, and completeness are formally proven, and extensive experiments demonstrate superior performance compared to six baseline methods across four benchmark datasets.

## Method Summary
PROPLACE generates counterfactual explanations through a bi-level optimization framework that ensures both robustness and plausibility. The method constructs a plausible region as the convex hull of k Δ-robust nearest neighbors and the input, then iteratively expands the set of model parameters that the counterfactual explanation must remain valid against. This process involves outer minimization to find the closest counterfactual explanation valid for all models in the current set, and inner maximization to find the worst-case model shift that minimizes the output score. The approach uses mixed-integer linear programming formulations for neural networks with ReLU activations and guarantees soundness and completeness through the convex hull construction.

## Key Results
- PROPLACE achieves state-of-the-art performance against metrics on three evaluation aspects: robustness (validity under retrained models), plausibility (measured by Local Outlier Factor), and proximity (ℓ1 distance).
- The method guarantees robustness under bounded model parameter changes and induces reliable robustness against unbounded model changes.
- PROPLACE demonstrates superior proximity among the most robust baselines while maintaining significantly better robustness than non-robust methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PROPLACE guarantees soundness and completeness by restricting the search space to the convex hull of k Δ-robust nearest neighbors and the input.
- Mechanism: The vertices of this convex hull (excluding the input) are guaranteed Δ-robust, ensuring at least k feasible solutions exist in the search space. The convex hull structure allows PROPLACE to find provably robust counterfactual explanations within a plausible region.
- Core assumption: The input itself lies within the data distribution and is therefore plausible.
- Evidence anchors:
  - [abstract]: "The method guarantees robustness under bounded model parameter changes and induces reliable robustness against unbounded model changes"
  - [section]: "By restricting the CE search space to such convex hull, the method has the flexibility to find close CEs (with x as a vertex), or robust and plausible CEs (with the robust neighbors as other vertices)"
  - [corpus]: Weak evidence. The corpus contains related work on robust counterfactual explanations but doesn't specifically address the convex hull construction method.
- Break condition: If no Δ-robust nearest neighbors exist (i.e., no dataset points are classified to the desired class under bounded model changes), the method cannot guarantee soundness and completeness.

### Mechanism 2
- Claim: PROPLACE's bi-level optimization converges by iteratively expanding the set of model parameters Δ' that the counterfactual explanation must be valid against.
- Mechanism: The outer minimization finds the closest counterfactual explanation valid for all models in Δ', while the inner maximization finds the worst-case model shift within the bounded parameter changes that minimizes the output score. This process continues until no further expansion of Δ' is needed.
- Core assumption: The classifier is Lipschitz continuous, ensuring that the counterfactual explanation search space is bounded and the optimization process will converge.
- Evidence anchors:
  - [abstract]: "We formulate an iterative algorithm to compute provably robust CEs and prove its convergence, soundness and completeness"
  - [section]: "Assume the classifier M_Θ is Lipschitz continuous in x'. Then, the maximum number of iterations before Algorithm 1 terminates is bounded"
  - [corpus]: Moderate evidence. The corpus contains work on robust counterfactual explanations using optimization techniques, but doesn't specifically address the bi-level optimization convergence proof.
- Break condition: If the Lipschitz continuity assumption is violated (e.g., for non-smooth activation functions), convergence guarantees may not hold.

### Mechanism 3
- Claim: PROPLACE achieves state-of-the-art performance by simultaneously optimizing for proximity, plausibility, and robustness.
- Mechanism: By formulating the counterfactual explanation search as a robust optimization problem with constraints for validity, plausibility (via the convex hull search space), and robustness (via the bounded model parameter changes), PROPLACE can find counterfactual explanations that balance all three desiderata.
- Core assumption: The distance metric used (e.g., normalized L1) appropriately captures the notion of proximity for the application domain.
- Evidence anchors:
  - [abstract]: "Through a comparative experiment involving six baselines, five of which target robustness, we show that PROPLACE achieves state-of-the-art performances against metrics on three evaluation aspects"
  - [section]: "Our method still shows ℓ1 costs lower than all methods except RBR, which is significantly less robust, and MILP-R, which finds outliers"
  - [corpus]: Strong evidence. The corpus contains multiple papers on robust counterfactual explanations that address the trade-off between proximity, plausibility, and robustness.
- Break condition: If the distance metric doesn't align with the application's notion of proximity, or if the robustness constraints are too restrictive, the method may sacrifice proximity too heavily.

## Foundational Learning

- Concept: Counterfactual explanations and their properties (validity, proximity, plausibility, robustness)
  - Why needed here: Understanding these properties is crucial for grasping the motivation behind PROPLACE and how it addresses limitations in existing methods
  - Quick check question: What is the difference between plausibility and validity in the context of counterfactual explanations?

- Concept: Robust optimization and interval abstractions
  - Why needed here: PROPLACE uses robust optimization techniques and interval abstractions to guarantee robustness against bounded model parameter changes
  - Quick check question: How do interval abstractions help certify the robustness of counterfactual explanations under bounded model parameter changes?

- Concept: Mixed-integer linear programming (MILP) formulations for neural networks
  - Why needed here: PROPLACE uses MILP formulations to solve the outer and inner optimization problems in the bi-level optimization framework
  - Quick check question: How can ReLU activation functions be encoded in a MILP formulation for neural networks?

## Architecture Onboarding

- Component map: Plausible region construction (convex hull of k Δ-robust nearest neighbors and input) -> Bi-level optimization framework (outer minimization and inner maximization) -> MILP formulations for outer and inner optimization problems -> Convergence and soundness/completeness proofs

- Critical path:
  1. Construct plausible region (convex hull)
  2. Initialize Δ' with original model
  3. Outer minimization: Find closest counterfactual explanation valid for all models in Δ'
  4. Inner maximization: Find worst-case model shift within bounded parameter changes
  5. Add worst-case model to Δ' and repeat until convergence

- Design tradeoffs:
  - Trade-off between proximity and robustness: More restrictive robustness constraints may lead to counterfactual explanations that are further from the input
  - Trade-off between plausibility and proximity: Restricting the search space to the plausible region may exclude closer counterfactual explanations that are outliers
  - Computational complexity: Solving MILP formulations can be computationally expensive, especially for large neural networks

- Failure signatures:
  - If no Δ-robust nearest neighbors exist, the method cannot guarantee soundness and completeness
  - If the Lipschitz continuity assumption is violated, convergence guarantees may not hold
  - If the distance metric doesn't align with the application's notion of proximity, the method may produce suboptimal results

- First 3 experiments:
  1. Verify that the plausible region construction works as expected by visualizing the convex hull of k Δ-robust nearest neighbors and the input for a simple 2D dataset
  2. Test the convergence of the bi-level optimization framework on a small neural network with known properties (e.g., a linear classifier)
  3. Compare the performance of PROPLACE with a non-robust baseline (e.g., Wachter et al.'s method) on a benchmark dataset to demonstrate the trade-off between proximity and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PROPLACE's performance compare to other methods when the plausible region X_plausible is constructed using different distance metrics or clustering techniques?
- Basis in paper: [inferred] The paper discusses using the convex hull of k Δ-robust nearest neighbors and the input as the plausible region, but does not explore alternative constructions.
- Why unresolved: The paper focuses on a specific plausible region construction and does not compare its performance with other methods.
- What evidence would resolve it: Empirical results comparing PROPLACE's performance using different plausible region constructions would provide insights into the impact of this design choice.

### Open Question 2
- Question: What is the impact of the bound on model parameter changes (Δ) on PROPLACE's performance in terms of proximity, plausibility, and robustness?
- Basis in paper: [explicit] The paper mentions using a specific bound for Δ but does not investigate the effect of varying this bound on the method's performance.
- Why unresolved: The paper does not explore the sensitivity of PROPLACE to the choice of Δ, which is a crucial parameter in the method.
- What evidence would resolve it: Experiments varying the Δ bound and measuring the resulting changes in proximity, plausibility, and robustness would clarify the method's sensitivity to this parameter.

### Open Question 3
- Question: How does PROPLACE perform in terms of actionability and diversity of counterfactual explanations, and how do these properties trade off with proximity, plausibility, and robustness?
- Basis in paper: [inferred] The paper focuses on proximity, plausibility, and robustness but does not explicitly address actionability and diversity, which are important considerations in counterfactual explanations.
- Why unresolved: The paper does not investigate the trade-offs between actionability, diversity, and the other properties, which is an important aspect of counterfactual explanations.
- What evidence would resolve it: Experiments measuring actionability and diversity, along with the other properties, would provide insights into the trade-offs and the method's overall performance.

## Limitations
- Scalability concerns: The MILP formulations used by PROPLACE may struggle with larger neural networks and datasets, limiting its applicability to complex real-world problems.
- Sensitivity to hyperparameters: The performance of PROPLACE depends on the choice of hyperparameters such as the number of nearest neighbors and the bound on model parameter changes, which may require careful tuning for different applications.
- Limited scope of experiments: The paper only evaluates PROPLACE on four benchmark datasets with two-layer neural networks, which may not fully capture its performance on more diverse and challenging problems.

## Confidence
- Theoretical claims: High confidence due to formal proofs provided for convergence, soundness, and completeness
- Empirical claims: Medium confidence due to limited scope of experiments and lack of comparison with more recent robust counterfactual explanation methods

## Next Checks
1. Test PROPLACE on a larger neural network (e.g., three or four layers) and a larger dataset to assess scalability and performance.
2. Conduct a hyperparameter sensitivity analysis to determine the impact of choices like the number of nearest neighbors and the bound on model parameter changes on the quality of the counterfactual explanations.
3. Compare PROPLACE with more recent robust counterfactual explanation methods (e.g., those using gradient-based approaches or model-agnostic techniques) to provide a more comprehensive evaluation of its performance.