---
ver: rpa2
title: A Vulnerability of Attribution Methods Using Pre-Softmax Scores
arxiv_id: '2307.03305'
source_url: https://arxiv.org/abs/2307.03305
tags:
- outputs
- pre-softmax
- methods
- attribution
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that attribution methods based on pre-softmax
  scores are vulnerable to a specific type of adversarial attack that can distort
  heatmaps without changing the model's final outputs. The core issue arises because
  pre-softmax scores are invariant under addition of a class-independent term, while
  gradients of these scores can change, leading to altered attributions.
---

# A Vulnerability of Attribution Methods Using Pre-Softmax Scores

## Quick Facts
- arXiv ID: 2307.03305
- Source URL: https://arxiv.org/abs/2307.03305
- Reference count: 13
- Key outcome: Attribution methods using pre-softmax scores can be manipulated to distort heatmaps without changing model predictions, potentially misleading users about model reasoning.

## Executive Summary
This paper identifies a vulnerability in attribution methods that rely on pre-softmax scores, such as Grad-CAM. The core issue is that pre-softmax scores are invariant under addition of class-independent terms, while their gradients can change, leading to altered attributions. The authors demonstrate that adding a term dependent on specific feature map activations can dramatically distort heatmaps, incorrectly highlighting irrelevant image regions while keeping the model's final outputs unchanged. This vulnerability is distinct from Clever Hans effects and can undermine trust in model reliability without altering performance metrics.

## Method Summary
The authors demonstrate a vulnerability in attribution methods using pre-softmax scores by adding a class-independent term t to all pre-softmax scores. This term is calculated as the sum of activations at position (0,0) of the final pooling layer, multiplied by a constant K. Using a VGG19 model pretrained on ImageNet, they show that this modification can distort Grad-CAM heatmaps while keeping post-softmax outputs unchanged. The method highlights the upper-left corner of images, an irrelevant region, demonstrating how pre-softmax-based attributions can be manipulated.

## Key Results
- Adding a class-independent term to pre-softmax scores changes gradients, distorting Grad-CAM heatmaps
- The distortion highlights irrelevant image regions (e.g., upper-left corner) without affecting model predictions
- Post-softmax-based attributions remain unaffected by this vulnerability
- This vulnerability is distinct from Clever Hans effects and can mislead interpretation of model explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-softmax scores are invariant under addition of class-independent terms, while gradients of these scores can change.
- Mechanism: When a term t independent of class index is added to all pre-softmax scores (z_i → z_i + t), the softmax output remains unchanged due to cancellation in the normalization denominator. However, the gradients ∂z_i/∂x can change if t depends on the input x, leading to altered attribution heatmaps.
- Core assumption: The term t must be class-independent but may depend on the model's internal activations.
- Evidence anchors:
  - [abstract] "pre-softmax scores are invariant under addition of a class-independent term, while gradients of these scores can change, leading to altered attributions."
  - [section] "Note that adding an amount t independent of the class i to all the arguments of the softmax, z′_i = zi + t, has no effect on its outputs"
  - [corpus] Weak - no direct corpus evidence, but related vulnerability papers exist (e.g., Pixel-level Certified Explanations via Randomized Smoothing).

### Mechanism 2
- Claim: The vulnerability allows distortion of Grad-CAM heatmaps without changing the model's final outputs.
- Mechanism: By adding a term t that depends on specific internal activations (e.g., activations at position (0,0) of the final pooling layer), the pre-softmax gradients are altered. This causes Grad-CAM to highlight irrelevant regions (e.g., the upper-left corner) while the softmax probabilities and classification decisions remain unchanged.
- Core assumption: The model's architecture allows injection of a term t that is class-independent but input-dependent.
- Evidence anchors:
  - [abstract] "adding a term dependent on specific feature map activations dramatically distorts Grad-CAM heatmaps, incorrectly highlighting irrelevant image regions."
  - [section] "t is the result of adding the activations of the units placed in position (0, 0) of the final pool layer... multiplied by a constant K."
  - [corpus] Weak - related concepts in Pixel-level Certified Explanations via Randomized Smoothing discuss attribution map vulnerability.

### Mechanism 3
- Claim: The vulnerability is distinct from Clever Hans effects and can mislead interpretation of model explanations.
- Mechanism: Unlike Clever Hans effects, which arise from dataset biases, this vulnerability stems from the mathematical properties of the softmax function and gradient-based attribution methods. It does not indicate a problem with the model's learning but rather with the interpretation of its explanations.
- Core assumption: Users may incorrectly interpret distorted heatmaps as reflecting the model's true reasoning process.
- Evidence anchors:
  - [abstract] "This vulnerability is distinct from Clever Hans effects and can mislead interpretation of model explanations, undermining trust in model reliability without altering performance metrics."
  - [section] "the vulnerability discussed here tells nothing about the ability of the model to extract the right information from the right parts of its inputs"
  - [corpus] Weak - no direct corpus evidence; relies on the paper's distinction.

## Foundational Learning

- Concept: Softmax function and its properties
  - Why needed here: Understanding the softmax function's invariance under class-independent additive terms is crucial to grasping the vulnerability.
  - Quick check question: Why does adding a class-independent term t to all pre-softmax scores not change the softmax output?

- Concept: Gradient-based attribution methods
  - Why needed here: The vulnerability specifically affects attribution methods that use gradients of pre-softmax scores, such as Grad-CAM.
  - Quick check question: How do gradient-based attribution methods like Grad-CAM use pre-softmax scores to generate heatmaps?

- Concept: Adversarial attacks on model explanations
  - Why needed here: This vulnerability represents a new class of adversarial attacks targeting the interpretability of model explanations rather than the model's outputs.
  - Quick check question: How does this vulnerability differ from traditional adversarial attacks that aim to change the model's predictions?

## Architecture Onboarding

- Component map: Input layer -> Convolutional layers -> Pooling layers -> Fully connected layers (pre-softmax) -> Softmax layer (post-softmax) -> Attribution module (Grad-CAM)
- Critical path:
  1. Forward pass through convolutional and pooling layers
  2. Compute pre-softmax scores in fully connected layers
  3. Apply softmax to obtain post-softmax probabilities
  4. Calculate gradients of pre-softmax scores with respect to feature maps
  5. Generate Grad-CAM heatmaps
- Design tradeoffs:
  - Using pre-softmax scores allows for more detailed attribution but introduces vulnerability to adversarial manipulation.
  - Post-softmax scores are more robust but may provide less granular attribution information.
  - Balancing interpretability and robustness is crucial for reliable model explanations.
- Failure signatures:
  - Heatmaps highlighting irrelevant image regions despite correct classification
  - Consistent distortion patterns across different inputs and classes
  - Lack of correlation between heatmap regions and model decision boundaries
- First 3 experiments:
  1. Implement the vulnerability on a simple CNN model and observe the distortion of Grad-CAM heatmaps.
  2. Compare heatmaps generated using pre-softmax vs. post-softmax scores on the same model and inputs.
  3. Test the robustness of various attribution methods (e.g., Integrated Gradients, LRP) to this vulnerability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How broadly does the pre-softmax score vulnerability apply to other attribution methods beyond gradient-based approaches?
- Basis in paper: [explicit] The authors note that "many of the considerations can be easily extended to methods that work with finite differences rather than gradients, such as Layer-wise Relevance Propagation (LRP) and DeepLIFT"
- Why unresolved: The paper focuses primarily on gradient-based methods and provides a proof of concept using Grad-CAM, but doesn't empirically test whether the vulnerability affects LRP, DeepLIFT, or other attribution methods
- What evidence would resolve it: Systematic testing of various attribution methods (LRP, DeepLIFT, Integrated Gradients, etc.) on models with pre-softmax score modifications to determine if their heatmaps are similarly distorted

### Open Question 2
- Question: What are the potential security implications of this vulnerability in real-world deployed models?
- Basis in paper: [explicit] The authors discuss how "this vulnerability can be exploited to deploy a malicious attack intended to undermine confidence in the model" and note that modified models would be "functionally equivalent" and "hard to notice"
- Why unresolved: The paper presents a theoretical vulnerability but doesn't explore the practical feasibility, detection mechanisms, or countermeasures for such attacks in production environments
- What evidence would resolve it: Case studies of real-world model repositories showing whether this vulnerability could be exploited, analysis of detection methods, and evaluation of potential mitigation strategies

### Open Question 3
- Question: Does the vulnerability extend to models with architectures beyond VGG19, such as ResNet, Inception, or transformer-based models?
- Basis in paper: [inferred] The proof of concept uses VGG19, but the theoretical vulnerability is based on properties of the softmax function that should apply to any classifier architecture
- Why unresolved: The authors only demonstrate the effect on one specific architecture (VGG19) and don't explore whether the vulnerability manifests similarly in other common architectures
- What evidence would resolve it: Empirical testing of the vulnerability across diverse model architectures including ResNet, DenseNet, EfficientNet, and modern transformer-based vision models

## Limitations
- The vulnerability's practical impact remains uncertain as the paper only demonstrates the effect on a single model (VGG19) and a limited set of test images.
- The paper does not address potential defenses or mitigation strategies against this vulnerability.
- The choice of the adversarial term t appears somewhat arbitrary and its generality is not explored.

## Confidence
- **High Confidence**: The mathematical proof that pre-softmax scores are invariant under addition of class-independent terms while gradients can change.
- **Medium Confidence**: The claim that this vulnerability is distinct from Clever Hans effects.
- **Low Confidence**: The practical significance of this vulnerability in real-world applications.

## Next Checks
1. **Generalization Test**: Apply the vulnerability demonstration to multiple model architectures (e.g., ResNet, Inception) and datasets to assess the breadth of the vulnerability's applicability.
2. **Defense Exploration**: Investigate potential mitigation strategies, such as using post-softmax-based attribution methods or developing robustness metrics for attribution methods against such attacks.
3. **User Study**: Conduct a user study to evaluate how this vulnerability affects user trust in model explanations and whether awareness of the vulnerability changes interpretation behaviors.