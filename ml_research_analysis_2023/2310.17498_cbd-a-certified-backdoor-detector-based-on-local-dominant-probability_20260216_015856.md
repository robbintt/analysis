---
ver: rpa2
title: 'CBD: A Certified Backdoor Detector Based on Local Dominant Probability'
arxiv_id: '2310.17498'
source_url: https://arxiv.org/abs/2310.17498
tags:
- backdoor
- detection
- attacks
- trigger
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first certified backdoor detector (CBD)
  for deep neural networks. CBD uses a novel local dominant probability statistic
  and an adjustable conformal prediction scheme to detect if a model is backdoored.
---

# CBD: A Certified Backdoor Detector Based on Local Dominant Probability

## Quick Facts
- arXiv ID: 2310.17498
- Source URL: https://arxiv.org/abs/2310.17498
- Reference count: 40
- This paper proposes the first certified backdoor detector (CBD) for deep neural networks using a novel local dominant probability statistic and an adjustable conformal prediction scheme.

## Executive Summary
This paper introduces CBD, a certified backdoor detector for deep neural networks that leverages the local dominant probability (LDP) statistic and an adjustable conformal prediction scheme. CBD provides both empirical and certified detection of backdoored models, achieving high true positive rates (up to 98%) with low false positive rates (up to 10%) for backdoor attacks with random perturbation triggers. The authors theoretically prove that backdoor attacks with stronger trigger robustness and smaller perturbation magnitudes are more likely to be detected with guarantees.

## Method Summary
CBD detects backdoored models by computing the local dominant probability (LDP) statistic, which measures the model's vulnerability to backdoor triggers by analyzing the dominant class probability in a local neighborhood around samples from each class. The method uses benign shadow models trained on a small validation set to construct a calibration set of LDP statistics, which is then used in an adjustable conformal prediction scheme to determine a detection threshold and compute a p-value. The detection inference is triggered if the p-value is below a significance level. The certification method provides a condition on the trigger robustness and perturbation magnitude under which a backdoor attack is guaranteed to be detected.

## Key Results
- CBD achieves high empirical detection true positive rates (up to 98%) with low false positive rates (up to 10%) for backdoor attacks with random perturbation triggers.
- The method provides a probabilistic upper bound on the false positive rate and a condition for guaranteed detection based on trigger robustness and perturbation magnitude.
- Theoretical results show that attacks with triggers more resilient to test-time noise and with smaller perturbation magnitudes are more likely to be detected with guarantees.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LDP statistic captures the model's vulnerability to backdoor triggers by measuring the dominant class probability in a local neighborhood around samples from each class.
- Mechanism: For a backdoored model, a robust trigger causes samples in the neighborhood of each clean sample to be classified to the target class with high probability. This shifts the dominant class in the average SLPV, resulting in a large LDP.
- Core assumption: Backdoor triggers are robust to Gaussian noise, and the trigger perturbation magnitude is small.
- Evidence anchors:
  - [abstract] "Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees."
  - [section] "Such trigger robustness can be measured by the distribution of the model prediction in the neighborhood of each δ(x) – the more robust the trigger is, the more samples in the neighborhood of δ(x) will be predicted to the target class t."
  - [corpus] No direct match, but this mechanism is central to the paper's approach.
- Break condition: If the trigger is not robust to noise or the perturbation magnitude is large, the LDP will not reliably indicate a backdoored model.

### Mechanism 2
- Claim: The conformal prediction scheme provides a statistical guarantee on the false positive rate of the detector.
- Mechanism: The detector uses a calibration set of LDP statistics from benign shadow models to determine a threshold for detection. The p-value is computed based on the position of the LDP for the model under inspection in the sorted calibration set.
- Core assumption: The LDP distribution for benign models trained on more abundant data dominates the LDP distribution for benign shadow models trained on less data.
- Evidence anchors:
  - [abstract] "CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate."
  - [section] "In particular, the LDPs in the calibration set (obtained from the shadow models) may easily have an overly large sample variance and a heavy tail of large outliers."
  - [corpus] Weak match, but the paper introduces an adjustable conformal prediction scheme to handle outliers in the calibration set.
- Break condition: If the LDP distribution for benign models does not dominate the LDP distribution for shadow models, the false positive rate guarantee may not hold.

### Mechanism 3
- Claim: The certification method provides a condition on the trigger robustness and perturbation magnitude under which a backdoor attack is guaranteed to be detected.
- Mechanism: The method uses the Neyman-Pearson lemma to connect the STR for each sample to the probability of misclassification under Gaussian noise. It then derives a lower bound on the minimum STR such that the LDP is sufficiently large to trigger detection.
- Core assumption: The trigger robustness is measured using an isotropic Gaussian distribution, and the LDP is computed using the same distribution.
- Evidence anchors:
  - [abstract] "Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees."
  - [section] "Based on this connection, we derive the lower bound for the minimum STR, such that the t-th entry of the SLPV of each xk is sufficiently large to result in a large LDP for the attack to be detected."
  - [corpus] No direct match, but this mechanism is derived from the paper's theoretical analysis.
- Break condition: If the trigger robustness is not measured using an isotropic Gaussian distribution or the LDP is computed using a different distribution, the certification may not hold.

## Foundational Learning

- Concept: Local dominant probability (LDP) and samplewise local probability vector (SLPV)
  - Why needed here: The LDP statistic is the key to distinguishing backdoored models from benign ones. It captures the effect of a backdoor trigger on the model's behavior in a local neighborhood around samples from each class.
  - Quick check question: How is the LDP computed for a given classifier and a set of samples from each class?
- Concept: Conformal prediction and its application to backdoor detection
  - Why needed here: The conformal prediction scheme provides a statistical guarantee on the false positive rate of the detector. It uses a calibration set of LDP statistics from benign shadow models to determine a threshold for detection.
  - Quick check question: How is the p-value computed in the conformal prediction scheme, and how does it relate to the significance level?
- Concept: Certification of backdoor detection and its connection to robustness against adversarial examples
  - Why needed here: The certification method provides a condition on the trigger robustness and perturbation magnitude under which a backdoor attack is guaranteed to be detected. It also highlights the difference between certified backdoor detection and certified robustness against adversarial examples.
  - Quick check question: What is the key difference between the 1-D interval specified by the certified radius for adversarial examples and the 2-D certified region jointly specified by the trigger robustness and its perturbation magnitude for backdoor attacks?

## Architecture Onboarding

- Component map:
  - Input: Classifier to be inspected
  - Output: Detection inference (backdoored or not), condition for guaranteed detection, probabilistic upper bound on false positive rate
  - Key components: LDP computation, shadow model training, calibration set construction, conformal prediction, certification method
- Critical path:
  1. Compute LDP for the classifier to be inspected
  2. Train shadow models and construct calibration set
  3. Perform conformal prediction to obtain p-value
  4. Determine detection inference based on p-value and significance level
  5. Derive condition for guaranteed detection using certification method
- Design tradeoffs:
  - Number of shadow models vs. computational cost: More shadow models may improve detection and certification performance but increase training and LDP estimation time.
  - Size of calibration set vs. outlier handling: A larger calibration set may reduce the impact of outliers but also increase the risk of false positives if the LDP distribution for benign models does not dominate the LDP distribution for shadow models.
  - Choice of significance level vs. false positive rate: A lower significance level may reduce false positives but also increase the risk of false negatives.
- Failure signatures:
  - High false positive rate: LDP distribution for benign models does not dominate LDP distribution for shadow models, or significance level is too low.
  - Low detection rate: LDP statistic is not effective in distinguishing backdoored models from benign ones, or trigger robustness or perturbation magnitude is outside the certified region.
  - High computational cost: Number of shadow models or size of calibration set is too large.
- First 3 experiments:
  1. Verify LDP computation on a simple binary classification problem with a known backdoor trigger.
  2. Evaluate detection performance on a small dataset with a few backdoor attacks and benign models.
  3. Derive certification condition for a simple attack with known trigger robustness and perturbation magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between trigger robustness and perturbation magnitude for backdoor attacks to evade CBD detection while maintaining effectiveness?
- Basis in paper: [explicit] The paper discusses that attacks with stronger trigger robustness and smaller perturbation magnitudes are more likely to be detected by CBD with guarantees.
- Why unresolved: The paper focuses on the detection capabilities of CBD but does not explore the trade-offs attackers might make to optimize their attacks against CBD.
- What evidence would resolve it: Empirical studies comparing the detection rates of CBD against backdoor attacks with varying levels of trigger robustness and perturbation magnitudes, identifying the optimal balance for attackers.

### Open Question 2
- Question: How does the performance of CBD change when applied to backdoor attacks with multiple triggers or target classes?
- Basis in paper: [explicit] The paper mentions that it does not consider backdoor attacks with multiple triggers or target classes, stating that even empirical detection of these attacks is challenging.
- Why unresolved: The paper does not address the complexity introduced by multiple triggers or target classes in the context of CBD's detection capabilities.
- What evidence would resolve it: Experiments evaluating CBD's detection performance against backdoor attacks with multiple triggers or target classes, providing insights into its effectiveness in these more complex scenarios.

### Open Question 3
- Question: What is the impact of class imbalance in the training data on the effectiveness of CBD's LDP-based detection?
- Basis in paper: [explicit] The paper discusses that class imbalance is not the reason for a large LDP and shows that LDP is not significantly affected by class imbalance in general.
- Why unresolved: While the paper addresses class imbalance, it does not explore how it might affect CBD's performance in detecting backdoor attacks, especially in scenarios where the training data is imbalanced.
- What evidence would resolve it: Studies examining CBD's detection accuracy in scenarios with varying levels of class imbalance in the training data, identifying any potential impacts on its performance.

## Limitations
- The method depends on trigger robustness against Gaussian noise and assumes that benign models' LDP distributions dominate shadow models' distributions.
- The certification is limited to backdoor attacks with specific trigger robustness and perturbation magnitude bounds.
- The paper does not address backdoor attacks with non-random triggers or adaptive adversaries aware of the detection mechanism.

## Confidence
- **High Confidence**: The overall detection mechanism using LDP statistics and conformal prediction is well-established and theoretically grounded.
- **Medium Confidence**: The certification results for backdoor attacks with specific trigger robustness and perturbation magnitude bounds are mathematically derived but rely on strong assumptions about the LDP distribution and trigger robustness measurement.
- **Low Confidence**: The experimental results on large-scale datasets like TinyImageNet may not generalize well to other datasets or attack scenarios due to the specific choice of trigger generation process and model architectures.

## Next Checks
1. Reproduce the LDP computation and detection results on a small, simple dataset with a known backdoor trigger to verify the core mechanism.
2. Evaluate the impact of different σ values in LDP computation on detection performance using the calibration set and significance level.
3. Derive the certification condition for a simple attack with known trigger robustness and perturbation magnitude bounds to validate the theoretical analysis.