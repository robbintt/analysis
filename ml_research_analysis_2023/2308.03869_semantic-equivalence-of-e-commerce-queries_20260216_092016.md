---
ver: rpa2
title: Semantic Equivalence of e-Commerce Queries
arxiv_id: '2308.03869'
source_url: https://arxiv.org/abs/2308.03869
tags:
- query
- queries
- similarity
- search
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for recognizing and leveraging
  query equivalence in e-commerce search to improve searcher and business outcomes.
  The approach addresses three key problems: mapping queries to vector representations
  of search intent, identifying nearest neighbor queries expressing equivalent or
  similar intent, and optimizing for user or business objectives.'
---

# Semantic Equivalence of e-Commerce Queries

## Quick Facts
- arXiv ID: 2308.03869
- Source URL: https://arxiv.org/abs/2308.03869
- Reference count: 2
- Key result: Achieves Pearson correlation of 0.85 for query similarity in e-commerce search

## Executive Summary
This paper introduces a framework for recognizing and leveraging query equivalence in e-commerce search to improve searcher and business outcomes. The approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity (canonicalizing queries based on word inflection, order, compounding, and noise words) and behavioral similarity (leveraging historical search behavior to generate vector representations of query intent). Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models.

## Method Summary
The approach uses a two-stage process: an offline stage that trains a sentence similarity model on frequent queries using aggregated product embeddings from click data, and an online stage that supports processing of unseen queries through nearest neighbor search. Surface canonicalization handles obvious variations like word order and inflection, while behavioral vectors capture semantic equivalence through aggregated product embeddings. The system populates a FAISS database with vectors of frequent queries covering the head and torso of the query distribution, then uses a micro-BERT similarity model with category classifier input for online query processing.

## Key Results
- Outperforms popular sentence transformer models in query similarity tasks
- Achieves Pearson correlation of 0.85 for query similarity
- Effectively handles both frequent and unseen queries through combined surface and behavioral approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral similarity through aggregated product embeddings captures true search intent better than surface features alone.
- Mechanism: By averaging embeddings of products users clicked on after entering a query, the system builds a vector representation that reflects actual user behavior rather than just token patterns.
- Core assumption: Historical click data is sufficient and representative of true search intent.
- Evidence anchors:
  - [abstract] "Behavioral similarity leverages historical search behavior to generate vector representations of query intent."
  - [section 3] "We can then obtain the vector representation of a query representation by taking the mean, or some other aggregation, of the embeddings of products that searchers engage with (e.g., click on) after performing that query."
  - [corpus] Weak - no direct behavioral embedding examples in corpus.
- Break condition: If click data is sparse, biased, or users click randomly without true intent alignment, the behavioral vector will misrepresent intent.

### Mechanism 2
- Claim: Surface canonicalization combined with behavioral vectors provides complementary signals for query equivalence.
- Mechanism: Surface similarity handles obvious variations (word order, inflection, compounding, noise words) while behavioral vectors capture semantic equivalence that surface features miss.
- Core assumption: Surface canonicalization rules are reliable enough to act as a guardrail for behavioral similarity.
- Evidence anchors:
  - [abstract] "The framework utilizes both surface similarity and behavioral similarity to determine query equivalence."
  - [section 2] "Combining it with a guard rail, such as the observed or predicted result category based on search demand, allows us to establish query equivalence with high confidence."
  - [section 3] Examples showing high cosine similarity despite surface differences (e.g., "hdmi to galaxy s8 s9 hdmi" vs "hdmi to s9").
- Break condition: If surface canonicalization rules incorrectly merge dissimilar intents (like "blackberry" vs "blackberries"), the guardrail fails.

### Mechanism 3
- Claim: Online nearest neighbor search enables real-time handling of unseen queries using precomputed behavioral vectors.
- Mechanism: A FAISS database stores vectors of frequent queries; new queries are embedded and compared to find nearest neighbors with cosine similarity above threshold.
- Core assumption: The nearest neighbor database adequately covers the query distribution head and torso.
- Evidence anchors:
  - [abstract] "An online nearest neighbor approach supports processing of unseen queries."
  - [section 5] "We populate a nearest-neighbor database with the vectors of known queries that cover the head and torso of the query distribution."
  - [section 5] "When the searcher enters a query – especially a query that would otherwise return no or few results – we can look it up in this nearest-neighbor database."
- Break condition: If the database misses long-tail queries or the similarity threshold is too high/low, equivalence detection fails.

## Foundational Learning

- Concept: Vector embeddings and cosine similarity
  - Why needed here: The system relies on comparing query vectors using cosine similarity to determine equivalence.
  - Quick check question: What range does cosine similarity fall in, and what value indicates perfect similarity?

- Concept: Nearest neighbor search algorithms
  - Why needed here: Online query processing uses FAISS for efficient nearest neighbor lookup.
  - Quick check question: What are the trade-offs between exact and approximate nearest neighbor search in terms of speed and accuracy?

- Concept: Contrastive learning and Siamese networks
  - Why needed here: The model training uses a two-tower architecture with contrastive loss to learn query representations.
  - Quick check question: How does contrastive loss encourage similar queries to have similar embeddings while pushing apart dissimilar ones?

## Architecture Onboarding

- Component map:
  - Query preprocessor: Handles surface canonicalization (stemming, sorting, noise removal)
  - Behavioral vector generator: Aggregates product embeddings from click data
  - Nearest neighbor database: FAISS index of frequent query vectors
  - Online similarity model: Micro-BERT with category classifier input
  - Query category classifier: FastText model mapping queries to categories
  - Search engine interface: Routes queries through equivalence recognition

- Critical path:
  1. New query enters system
  2. Apply surface canonicalization
  3. Check nearest neighbor database
  4. If found and similarity >= threshold, use equivalent query
  5. If not found, embed using online similarity model
  6. Return results using equivalent or similar query

- Design tradeoffs:
  - Offline vs online processing: Offline gives more accurate behavioral vectors but only works for frequent queries; online handles all queries but may be less accurate.
  - FAISS indexing: Approximate nearest neighbor search is faster but may miss exact matches.
  - Threshold tuning: Higher thresholds reduce false positives but may miss valid equivalences.

- Failure signatures:
  - High false negatives: Threshold too high or nearest neighbor database incomplete
  - High false positives: Threshold too low or surface canonicalization rules too aggressive
  - Poor long-tail performance: Online model not trained on diverse enough query pairs
  - Category misclassification: Query category classifier errors propagate to similarity model

- First 3 experiments:
  1. Test surface canonicalization rules on edge cases (e.g., "blackberry" vs "blackberries") to ensure guardrail effectiveness
  2. Measure cosine similarity distribution for known equivalent vs non-equivalent query pairs to tune similarity threshold
  3. Evaluate nearest neighbor database coverage by checking hit rate for a sample of frequent queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approach handle queries that are semantically equivalent but belong to different product categories, and how can this be improved?
- Basis in paper: [explicit] The paper mentions that query similarity models may not distinguish queries with high surface similarity but different product categories, and suggests using a separately trained query category classifier as input for the query similarity model.
- Why unresolved: The paper does not provide details on how effective the category classifier is in improving query similarity predictions or how it handles ambiguous queries that could belong to multiple categories.
- What evidence would resolve it: Experimental results comparing query similarity predictions with and without the category classifier, and a detailed analysis of how the classifier handles ambiguous queries.

### Open Question 2
- Question: What is the impact of using different types of product representations (e.g., multimodal) on the performance of the query similarity model?
- Basis in paper: [explicit] The paper suggests that the approach can be generalized to richer representations, such as multimodal product representations that combine text, structured data, and images.
- Why unresolved: The paper does not provide any experimental results or analysis on the performance of the query similarity model when using different types of product representations.
- What evidence would resolve it: Comparative experiments using different types of product representations and an analysis of their impact on query similarity predictions.

### Open Question 3
- Question: How can the proposed approach be extended to handle queries in languages other than English?
- Basis in paper: [explicit] The paper does not mention any specific considerations for handling queries in languages other than English.
- Why unresolved: The paper does not provide any information on how the approach would perform for queries in other languages or what modifications would be necessary to handle them.
- What evidence would resolve it: Experiments using the proposed approach on queries in different languages and an analysis of its performance and necessary modifications for each language.

## Limitations

- The behavioral vector mechanism assumes click data reliably captures true search intent, but this assumption is not validated against purchase data or user satisfaction metrics.
- The 0.98 similarity threshold appears quite high and may be overfit to the specific dataset, potentially leading to high false negatives.
- The approach doesn't address handling queries in languages other than English or provide analysis of how surface canonicalization rules perform across different language structures.

## Confidence

- Confidence: Low - The behavioral vector mechanism assumes that aggregated product embeddings from click data reliably capture true search intent. However, the paper provides no validation that click data correlates with actual purchase intent or satisfaction.
- Confidence: Medium - The surface canonicalization guardrail could introduce systematic errors. While the paper mentions examples showing high cosine similarity despite surface differences, it doesn't provide comprehensive evaluation of false positives or false negatives.
- Confidence: Medium - The online nearest neighbor approach depends heavily on the quality and coverage of the FAISS database. The paper claims coverage of "head and torso" but doesn't specify what percentage of queries this represents.

## Next Checks

1. **Behavioral vector validation**: Test whether the aggregated product embeddings from click data actually predict user satisfaction or conversion rates. Compare behavioral vectors against vectors derived from purchase data or user ratings to assess whether click behavior truly captures search intent.

2. **Surface canonicalization edge cases**: Systematically test the surface canonicalization rules on a diverse set of query pairs, particularly focusing on cases where surface similarity might mask semantic differences (e.g., brand names vs product categories, singular vs plural forms with different meanings).

3. **Threshold sensitivity analysis**: Evaluate the performance of the nearest neighbor approach across a range of similarity thresholds (0.90 to 0.99) to determine optimal balance between false positives and false negatives, and assess how this varies across different query categories and lengths.