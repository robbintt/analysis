---
ver: rpa2
title: Multi-stage Large Language Model Correction for Speech Recognition
arxiv_id: '2310.11532'
source_url: https://arxiv.org/abs/2310.11532
tags:
- stage
- correction
- speech
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel multi-stage approach to improve speech
  recognition (ASR) performance using large language models (LLMs). The method combines
  traditional language model re-scoring with LLM prompting in two stages: (1) an N-best
  list re-scoring using a language model, and (2) LLM-based correction for low-confidence
  hypotheses.'
---

# Multi-stage Large Language Model Correction for Speech Recognition

## Quick Facts
- **arXiv ID**: 2310.11532
- **Source URL**: https://arxiv.org/abs/2310.11532
- **Reference count**: 0
- **Key outcome**: 10-20% relative WER reduction using multi-stage LLM correction pipeline

## Executive Summary
This paper presents a novel multi-stage approach that leverages large language models (LLMs) to improve speech recognition accuracy without requiring in-domain data or model fine-tuning. The method combines traditional language model re-scoring with LLM-based correction in two stages: first using a language model to re-score N-best hypotheses and perform confidence-based routing, then applying rule-constrained LLM correction to low-confidence outputs. The approach achieves state-of-the-art results of 1.3% WER on LibriSpeech test-clean and demonstrates 10-20% relative WER reduction across multiple test domains including Common Voice and TED-LIUM.

## Method Summary
The proposed multi-stage correction pipeline operates in two stages: Stage 1 performs N-best list re-scoring using a language model (GPT-J) with weighted fusion of ASR scores and language model probabilities, followed by a confidence check that routes high-confidence outputs directly to the final transcript. Stage 2 applies rule-constrained LLM correction (using ChatGPT) only to low-confidence outputs, where the LLM is prompted with explicit rules to maintain verbatim accuracy while correcting errors. The pipeline uses a confidence threshold to balance accuracy and computational efficiency, with experiments showing that only 23% of utterances require LLM correction.

## Key Results
- Achieves 1.3% WER on LibriSpeech test-clean, setting a new state-of-the-art result
- Demonstrates 10-20% relative WER reduction compared to competitive ASR systems
- Shows 77% reduction in LLM calls through confidence-based routing (only 23% of utterances sent to stage 2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-best list re-scoring with language model fusion improves ASR hypotheses selection accuracy.
- Mechanism: Weighted combination of ASR scores and language model probabilities (Equation 1) to rescore hypotheses.
- Core assumption: Fusion weight α and N-best size N can be optimized to maximize discrimination between correct and incorrect hypotheses.
- Evidence anchors: [abstract] "language model to re-score an N-best list"; [section 2.1] Score(yi) = log PASR(yi|x) + α log PLM(yi); [section 3.4] Tested beam sizes N = [5, 8, 16] with α from 1 to 5.

### Mechanism 2
- Claim: Rule-constrained LLM prompting corrects low-confidence ASR transcripts while preserving verbatim accuracy.
- Mechanism: Prompt design uses specific rules to restrict LLM creativity, allowing only word replacement from N-best list variants while maintaining sentence structure.
- Core assumption: LLM's reasoning capability can identify and correct errors when provided with word-level dependencies from multiple hypotheses.
- Evidence anchors: [abstract] "explicitly written rules in prompts"; [section 2.3] Algorithm 1 shows prompt with replacement constraints; [section 3.3] Table 1 shows WER improvement when disallowing new words.

### Mechanism 3
- Claim: Confidence-based routing optimizes the correction pipeline by balancing accuracy and computational efficiency.
- Mechanism: Softmax-normalized scores calculate confidence metric; high-confidence outputs skip stage 2 while low-confidence ones proceed to LLM correction.
- Core assumption: Confidence threshold β can be set to minimize WER while maximizing computational savings.
- Evidence anchors: [abstract] "discrimination of these utterance-level scores does not surpass a confidence level"; [section 2.2] "If Score best is larger than a threshold β"; [section 3.4] "Only 23% of total processed speech utterances will be sent to Stage 2".

## Foundational Learning

- Concept: Language model fusion for rescoring
  - Why needed here: Understanding how to combine ASR scores with language model probabilities is crucial for implementing stage 1
  - Quick check question: How does the weight α affect the balance between ASR and language model influence in the rescoring?

- Concept: Prompt engineering and constraint design
  - Why needed here: The LLM correction stage relies on carefully crafted rules to achieve verbatim accuracy
  - Quick check question: What happens if rule 4 ("Only replace words in the original sentence with ones from variant sentences") is removed from the prompt?

- Concept: Confidence estimation and thresholding
  - Why needed here: The confidence check determines routing between stages and affects both accuracy and efficiency
  - Quick check question: How does changing the softmax temperature affect the confidence score distribution?

## Architecture Onboarding

- Component map: ASR model -> N-best generation -> Stage 1 re-scoring -> Confidence checker -> (Stage 2 correction) -> Final output
- Critical path: ASR → N-best generation → Stage 1 re-scoring → Confidence check → (Stage 2 correction) → Final output
- Design tradeoffs:
  - N-best list size vs computational cost: Larger N provides more correction options but increases processing time
  - Confidence threshold vs accuracy: Higher thresholds reduce LLM calls but may miss correction opportunities
  - Prompt creativity vs verbatim accuracy: More restrictive prompts preserve accuracy but may miss beneficial corrections
- Failure signatures:
  - Stage 1 over-confidence: High confidence threshold rejects correctable errors
  - Stage 2 over-correction: LLM introduces errors despite constraints
  - Computational inefficiency: Too many utterances routed to stage 2
  - Performance degradation: WER increases after correction
- First 3 experiments:
  1. Test different N-best sizes (5, 8, 16) with fixed α=3.0 to find optimal beam size for your domain
  2. Vary confidence threshold β from 0.5 to 0.9 to optimize the routing decision
  3. Compare WER with different prompt constraint combinations (with/without rule 4, varying temperature)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the multi-stage correction pipeline scale with increasing N-best list sizes beyond 16 hypotheses?
- Basis in paper: [explicit] The paper experiments with N-best list sizes of 5, 8, and 16, finding optimal performance at N=5, but does not explore larger N values.
- Why unresolved: The paper only explores N-best list sizes up to 16, leaving the performance characteristics for larger N values unknown.
- What evidence would resolve it: Additional experiments testing N-best list sizes of 32, 64, or even 128 hypotheses would provide insights into whether larger N values continue to improve performance or if there are diminishing returns.

### Open Question 2
- Question: How would the performance of the multi-stage approach compare to fine-tuning LLMs on in-domain data versus the zero-shot approach presented in the paper?
- Basis in paper: [inferred] The paper explicitly states it uses a zero-shot approach without in-domain data, while noting that other works require in-domain data for training and fine-tuning.
- Why unresolved: The paper does not provide a direct comparison between the zero-shot approach and fine-tuning LLMs on in-domain data.
- What evidence would resolve it: Experiments comparing the proposed zero-shot approach to a version where LLMs are fine-tuned on in-domain data would reveal whether the zero-shot approach sacrifices performance for its flexibility.

### Open Question 3
- Question: How robust is the proposed approach to different speech recognition errors beyond word errors, such as punctuation or capitalization errors?
- Basis in paper: [explicit] The paper mentions that Rule 6 in the LLM prompt ignores punctuation, and Rule 7 uses U.S. English, but does not extensively evaluate the model's performance on punctuation or capitalization errors.
- Why unresolved: The paper focuses primarily on word error rate (WER) as the evaluation metric, without separately analyzing the model's ability to correct punctuation or capitalization errors.
- What evidence would resolve it: Additional experiments evaluating the model's performance on punctuation and capitalization accuracy, separate from WER, would provide insights into the approach's robustness to different types of speech recognition errors.

## Limitations
- Confidence estimation reliability is not thoroughly validated across different domains
- Rule-based constraints may be overly restrictive, preventing beneficial corrections that require word additions or structural changes
- Generalizability to languages other than English and real-time applications is not addressed

## Confidence

**High Confidence**: WER improvement claims (10-20% relative reduction) are well-supported by experimental results on LibriSpeech test-clean (1.3% WER) and other test sets. The ablation study showing benefits of rule 4 provides strong evidence for constraint design effectiveness.

**Medium Confidence**: Computational efficiency claims rely on the assumption that confidence-based routing reduces LLM calls by 77%, but detailed computational cost trade-offs are not provided, and confidence threshold robustness across domains is not demonstrated.

**Low Confidence**: Generalizability to languages other than English and real-time applications is not addressed. Reliance on ChatGPT for stage 2 introduces dependencies on external APIs with unknown latency and cost implications for production deployment.

## Next Checks

1. **Confidence Score Calibration**: Create a calibration curve by plotting confidence scores against actual WER for each confidence bin across multiple test sets to validate whether softmax-normalized scores accurately predict correction quality.

2. **Rule Relaxation Impact**: Systematically remove or modify each constraint in the LLM prompt (particularly rule 4 about word replacement) and measure the trade-off between WER improvement and verbatim accuracy.

3. **Cross-Domain Robustness**: Test the complete pipeline on a challenging out-of-domain dataset (e.g., noisy telephony speech or accented speech) to evaluate whether the confidence routing mechanism maintains effectiveness when domain shift occurs.