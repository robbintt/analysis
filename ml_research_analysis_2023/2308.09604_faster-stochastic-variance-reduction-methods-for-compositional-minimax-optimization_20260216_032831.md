---
ver: rpa2
title: Faster Stochastic Variance Reduction Methods for Compositional MiniMax Optimization
arxiv_id: '2308.09604'
source_url: https://arxiv.org/abs/2308.09604
tags:
- have
- optimization
- nstorm
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes NSTORM and ADA-NSTORM for compositional minimax\
  \ optimization, achieving optimal O(\u03BA\xB3/\u03F5\xB3) sample complexity. NSTORM\
  \ leverages variance reduction and gradient projection, while ADA-NSTORM uses adaptive\
  \ learning rates."
---

# Faster Stochastic Variance Reduction Methods for Compositional MiniMax Optimization

## Quick Facts
- arXiv ID: 2308.09604
- Source URL: https://arxiv.org/abs/2308.09604
- Authors: 
- Reference count: 40
- Key outcome: NSTORM and ADA-NSTORM achieve O(κ³/ϵ³) sample complexity for compositional minimax optimization, with experiments showing superior performance on deep AUC, portfolio optimization, and RL policy evaluation

## Executive Summary
This paper introduces NSTORM and ADA-NSTORM, two stochastic variance reduction methods for compositional minimax optimization. NSTORM applies recursive momentum estimators with variance reduction to both inner and outer functions, while ADA-NSTORM extends this with adaptive learning rates. The methods achieve optimal O(κ³/ϵ³) sample complexity and demonstrate strong empirical performance on deep AUC maximization, risk-averse portfolio optimization, and reinforcement learning policy evaluation tasks.

## Method Summary
NSTORM uses nested stochastic recursive momentum with variance reduction for both the inner function g(x) and outer function f(g(x),y), combined with gradient projection to bound estimation errors. ADA-NSTORM builds on NSTORM by incorporating adaptive learning rate matrices generated through Adam-style updates. Both methods operate under assumptions of Lipschitz continuity, strong convexity in x, strong concavity in y, and smoothness conditions. The algorithms sample minibatches to estimate gradients recursively, using variance reduction to control error propagation in the nested composition.

## Key Results
- NSTORM and ADA-NSTORM achieve O(κ³/ϵ³) sample complexity for compositional minimax optimization
- Experiments on deep AUC maximization show faster convergence and higher AUC scores than SCGDA and PDSCA baselines
- Portfolio optimization and RL policy evaluation tasks demonstrate superior objective value gaps compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: NSTORM achieves O(κ³/ϵ³) sample complexity by applying variance reduction to both function values and gradients in nested compositional minimax optimization.
- **Mechanism**: The method uses recursive momentum estimators for both inner function g(x) and outer function f(g(x),y), with projection to bound gradient estimation error in the minimization sub-problem. This dual-layer variance reduction allows control of error propagation without requiring large batch sizes.
- **Core assumption**: Variance reduction techniques can be effectively applied to compositional minimax problems where gradients cannot be directly estimated due to nested expectations.
- **Evidence anchors**:
  - [abstract]: "NSTORM leverages variance reduction and gradient projection, while ADA-NSTORM uses adaptive learning rates"
  - [section 3.2]: "Inspired by STROM [6], the NSTORM method leverages similar variance-reduced estimators for both the two sub-problems"
  - [corpus]: Weak - no direct mention of NSTORM in related papers
- **Break condition**: If the projection step cannot sufficiently bound gradient errors, or if the variance reduction fails to control error propagation across the nested composition.

### Mechanism 2
- **Claim**: ADA-NSTORM retains the same O(κ³/ϵ³) sample complexity while improving practical applicability through adaptive learning rates.
- **Mechanism**: The method generates adaptive matrices At and Bt for primal and dual variables respectively, using Adam-style updates that adapt step sizes based on historical gradient information. This allows larger effective learning rates without sacrificing convergence guarantees.
- **Core assumption**: Adaptive learning rate methods like Adam can be applied to compositional minimax optimization without breaking the variance control mechanisms established in NSTORM.
- **Evidence anchors**:
  - [abstract]: "ADA-NSTORM uses adaptive learning rates. We demonstrate that ADA-NSTORM can achieve the same sample complexity"
  - [section 3.4]: "We leverage adaptive learning rates in NSTORM and propose ADAptive NSTORM (ADA-NSTORM) method"
  - [corpus]: Weak - no direct mention of ADA-NSTORM in related papers
- **Break condition**: If the adaptive learning rate matrices fail to remain positive definite, or if the momentum updates cause instability in the variance reduction process.

### Mechanism 3
- **Claim**: The Polyak-Łojasiewicz (PL) condition extension allows NSTORM to maintain O(κ³/ϵ³) complexity even when the outer function is not strongly concave.
- **Mechanism**: Under PL condition, the strong concavity assumption is relaxed while still guaranteeing convergence through the gradient norm condition, allowing broader applicability to problems where strong concavity may not hold.
- **Core assumption**: The PL condition provides sufficient structure to replace strong concavity while maintaining the same convergence guarantees.
- **Evidence anchors**:
  - [abstract]: "We also demonstrate that NSTORM can achieve the same sample complexity under the Polyak-Łojasiewicz (PL)-condition"
  - [section 3.2]: "Particularly, in (8), if we simply set the same step sizes of x and y, our proposed NSTORM method may fail to converge"
  - [corpus]: Weak - no direct mention of PL condition extension in related papers
- **Break condition**: If the PL constant is too small or the problem structure doesn't satisfy the PL inequality, convergence guarantees may not hold.

## Foundational Learning

- **Concept: Variance Reduction in Stochastic Optimization**
  - Why needed here: The nested composition creates compounded variance that standard stochastic methods cannot handle efficiently. Variance reduction techniques like STORM are essential to achieve the O(κ³/ϵ³) complexity.
  - Quick check question: Why can't standard SGD be used directly for compositional minimax optimization?

- **Concept: Strong Convexity and Strong Concavity**
  - Why needed here: The convergence analysis relies on strong concavity in the y-variable and smoothness conditions to derive the complexity bounds. Understanding these properties is crucial for implementing the algorithm correctly.
  - Quick check question: How does strong concavity in y affect the convergence rate of minimax optimization?

- **Concept: Adaptive Learning Rates (Adam-style)**
  - Why needed here: The ADA-NSTORM extension uses adaptive learning rates to overcome the slow convergence caused by small fixed learning rates in NSTORM. This requires understanding how adaptive methods maintain convergence properties.
  - Quick check question: What conditions must adaptive learning rate methods satisfy to preserve convergence guarantees?

## Architecture Onboarding

- **Component map**: ut and v't for inner function g(x), v''t for outer function f(g(x),y), wt for gradient projection, xt+1 and yt+1 updates with adaptive matrices At and Bt
- **Critical path**: For each iteration: sample ξt, compute ut and v't using variance reduction formulas, sample ζt, compute v''t and wt, update xt+1 and yt+1 using the gradient estimators and learning rates
- **Design tradeoffs**: Small learning rates ensure variance control but slow convergence; adaptive rates improve speed but add complexity; projection bounds errors but may introduce bias
- **Failure signatures**: If the variance reduction fails, the algorithm degrades to O(κ⁴/ϵ⁴) complexity; if projection bounds are too tight, it may prevent convergence to optimal solutions
- **First 3 experiments**:
  1. Implement NSTORM on a simple 2D compositional minimax toy problem and verify O(κ³/ϵ³) convergence empirically
  2. Compare NSTORM vs ADA-NSTORM on deep AUC maximization with varying imbalance ratios to test adaptive benefits
  3. Test NSTORM under PL condition on a non-strongly concave outer function to validate the extension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform on compositional minimax optimization problems with different levels of smoothness and strong concavity parameters?
- Basis in paper: [inferred] The paper mentions that NSTORM achieves optimal sample complexity under the PL-condition, which is a weaker assumption than strong concavity. It would be interesting to investigate how the methods perform when the smoothness and strong concavity parameters vary.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on how the methods perform under different levels of smoothness and strong concavity parameters.
- What evidence would resolve it: Experiments or theoretical analysis showing the performance of the methods under varying smoothness and strong concavity parameters.

### Open Question 2
- Question: Can the proposed methods be extended to handle compositional minimax optimization problems with multiple nested levels of composition?
- Basis in paper: [inferred] The paper focuses on compositional minimax optimization with two levels of composition (inner and outer functions). It would be valuable to investigate if the methods can be extended to handle problems with more than two levels of composition.
- Why unresolved: The paper does not discuss or provide any evidence on the applicability of the methods to problems with multiple nested levels of composition.
- What evidence would resolve it: Theoretical analysis or experimental results demonstrating the performance of the methods on compositional minimax optimization problems with multiple nested levels of composition.

### Open Question 3
- Question: How do the proposed methods compare to existing methods in terms of computational complexity and memory requirements?
- Basis in paper: [inferred] The paper focuses on the sample complexity of the proposed methods, but it does not provide a detailed comparison of computational complexity and memory requirements with existing methods.
- Why unresolved: The paper does not provide a comprehensive analysis of the computational complexity and memory requirements of the proposed methods compared to existing methods.
- What evidence would resolve it: A detailed comparison of computational complexity and memory requirements between the proposed methods and existing methods, including theoretical analysis and experimental results.

## Limitations
- Theoretical analysis relies on strong assumptions (strong convexity/concavity, smoothness) that may not hold in practical applications
- Experimental validation is limited to relatively small-scale problems compared to modern deep learning applications
- The adaptive learning rate extension lacks thorough analysis of how different adaptive methods affect convergence guarantees

## Confidence
- **High confidence**: The variance reduction mechanism in NSTORM and its basic convergence properties under standard assumptions
- **Medium confidence**: The adaptive learning rate extension and its impact on practical performance
- **Medium confidence**: The PL condition extension, as the theoretical analysis appears sound but limited experimental validation exists

## Next Checks
1. **Scalability Test**: Implement NSTORM on larger-scale compositional minimax problems with deep neural networks (e.g., ImageNet-scale deep AUC maximization) to verify that theoretical complexity translates to practical performance gains on modern architectures.

2. **Hyperparameter Sensitivity Analysis**: Systematically evaluate the sensitivity of NSTORM and ADA-NSTORM to hyperparameters (m, γ, learning rates) across different problem regimes to understand practical robustness and identify failure modes.

3. **Comparison Under Relaxed Assumptions**: Test NSTORM and ADA-NSTORM on problems where strong convexity/concavity assumptions are violated but PL conditions hold, to validate the theoretical extension claims with empirical evidence.