---
ver: rpa2
title: A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems
arxiv_id: '2305.14937'
source_url: https://arxiv.org/abs/2305.14937
tags:
- entity
- benchmarks
- linking
- entities
- mentions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides a fair and in-depth evaluation of end-to-end
  entity linking systems by addressing four fundamental problems with existing evaluations:
  a strong focus on named entities, unclear or missing specification of what else
  counts as an entity mention, poor handling of ambiguities, and over- or underrepresentation
  of certain kinds of entities in widely used benchmarks. To address these issues,
  the authors create two new benchmarks with clear annotation guidelines and fair
  evaluation as primary goals, using the large Wikidata as knowledge base.'
---

# A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems

## Quick Facts
- arXiv ID: 2305.14937
- Source URL: https://arxiv.org/abs/2305.14937
- Reference count: 16
- Key outcome: New benchmarks reveal that newer entity linking systems like ReFinED significantly outperform older ones, particularly on non-named entities and challenging mention types.

## Executive Summary
This paper addresses fundamental problems in existing end-to-end entity linking evaluations by creating two new benchmarks with clear annotation guidelines and fair evaluation as primary goals. The authors evaluate six existing entity linking systems on both new and existing benchmarks, using Wikidata as a unified knowledge base. Their evaluation reveals significant performance gaps between newer systems (particularly ReFinED) and older ones, while also exposing specific weaknesses in how systems handle non-named entities, lowercased mentions, partial names, and metonyms. The work provides both a comprehensive evaluation framework and detailed analysis of system strengths and weaknesses.

## Method Summary
The authors create two new entity linking benchmarks (Wiki-Fair and News-Fair) with clear annotation guidelines that include non-named entities and allow alternative annotations. They evaluate six existing entity linking systems (ReFinED, REL, GENRE, Ambiverse, Neural EL, TagMe) on these new benchmarks as well as three widely used existing benchmarks (AIDA-CoNLL, KORE50, MSNBC). All entities are mapped to Wikidata for standardized evaluation, and the ELEVANT tool is used for fine-grained error analysis across multiple categories. The evaluation focuses on overall performance metrics (precision, recall, F1) as well as detailed analysis of specific error types.

## Key Results
- Newer systems, particularly ReFinED, significantly outperform older systems on both overall performance and reproducibility
- Existing benchmarks show 18% of mentions as "undefined" (not annotated as entities), affecting fair evaluation
- Fine-grained error analysis reveals that systems struggle with specific mention types like metonyms (30.8% error rate for ReFinED) and lowercased mentions
- The new benchmarks successfully expose performance gaps that are masked when only evaluating on named entities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair benchmarks expose systematic weaknesses in NER disambiguation accuracy across entity types
- Mechanism: The authors create two new benchmarks with clear annotation guidelines that include non-named entities and allow alternative annotations. By including lowercased mentions, partial names, and metonyms that are frequently annotated in these benchmarks, the evaluation reveals that existing systems struggle with these harder cases despite strong overall performance on traditional benchmarks focused on named entities.
- Core assumption: Including a wider variety of entity types and annotation alternatives will reveal performance gaps that are masked when only evaluating on named entities.
- Evidence anchors:
  - [abstract] "the newer systems, particularly ReFinED, are significantly better than the older ones, both in terms of overall performance and reproducibility"
  - [section 5] "In contrast to most entity linking benchmarks, we include non-named entities in the ground truth"
  - [corpus] Weak - only 5 related papers found, none directly discussing fair benchmark creation
- Break condition: If the new benchmarks fail to show meaningful performance differences between systems or if the additional annotation complexity doesn't reveal new weaknesses.

### Mechanism 2
- Claim: End-to-end evaluation with Wikidata knowledge base improves reproducibility and comparability
- Mechanism: By mapping all entities to Wikidata and evaluating complete end-to-end systems rather than just disambiguation components, the authors create a standardized evaluation framework. This allows fair comparison between systems that use different knowledge bases and ensures that reproducibility issues are exposed when systems fail to perform consistently across benchmarks.
- Core assumption: A single, comprehensive knowledge base with clear mapping rules will reduce variability in evaluation results and expose systems that are overfitted to specific benchmarks.
- Evidence anchors:
  - [abstract] "using the large Wikidata as knowledge base"
  - [section 1.1] "The new benchmarks can be found under https://github.com/ad-freiburg/fair-entity-linking-benchmarks"
  - [section 6.1] "We were able to reproduce the results reported on ReFinED's GitHub page for the AIDA-CoNLL test set"
- Break condition: If mapping to Wikidata introduces significant errors or if systems continue to show poor reproducibility despite the standardized evaluation framework.

### Mechanism 3
- Claim: Fine-grained error analysis reveals specific weaknesses that aggregate metrics hide
- Mechanism: The authors use the ELEVANT tool to categorize errors into specific types (lowercase mentions, partial names, demonyms, metonyms, rare entities). This detailed analysis shows that while systems may have similar overall F1 scores, they have very different strengths and weaknesses on specific mention types, allowing for targeted improvements.
- Core assumption: Aggregated metrics like overall F1 score are insufficient for understanding system capabilities and identifying areas for improvement.
- Evidence anchors:
  - [section 3.2] "ELEVANT allows for a fine-grained analysis of how well a linker performs on certain aspects of the entity linking task"
  - [section 6.1] "there is still room for improvement, particularly on metonym mentions, where it has an average error rate of 30.8%"
  - [section 6.3] "Even though GENRE disambiguates metonyms, partial names and rare mentions comparatively well, there is still room for improvement"
- Break condition: If the fine-grained error categories don't provide actionable insights or if systems continue to perform similarly across all categories despite different architectures.

## Foundational Learning

- Concept: Entity Linking vs Entity Recognition vs Entity Disambiguation
  - Why needed here: The paper evaluates end-to-end systems that perform all three tasks, so understanding the distinction is crucial for interpreting results
  - Quick check question: What is the difference between a false positive in NER evaluation versus entity linking evaluation?
- Concept: Knowledge Base Mapping and Entity Types
  - Why needed here: The evaluation uses Wikidata as a unified knowledge base, requiring understanding of how different entity types (person, location, organization, etc.) are represented and how they affect system performance
  - Quick check question: Why might a system perform well on person entities but poorly on organization entities?
- Concept: Evaluation Metrics and Error Analysis
  - Why needed here: The paper uses precision, recall, F1 score plus fine-grained error categories, so understanding how these metrics are calculated and what they reveal is essential
  - Quick check question: How does the definition of true positive differ between NER evaluation and entity linking evaluation?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing -> Core Systems -> Evaluation Engine -> Output
- Critical path: Benchmark selection → Entity mapping → System evaluation → Fine-grained error analysis → Performance comparison
- Design tradeoffs: 
  - Using Wikidata provides comprehensive coverage but may introduce mapping errors
  - Fine-grained analysis provides detailed insights but increases evaluation complexity
  - Including non-named entities makes benchmarks more realistic but harder to annotate consistently
- Failure signatures:
  - Poor reproducibility across benchmarks suggests overfitting
  - High error rates on specific mention types indicate architectural weaknesses
  - Inconsistent performance between named and non-named entities suggests incomplete entity recognition
- First 3 experiments:
  1. Run all six systems on AIDA-CoNLL to establish baseline performance and verify reproducibility
  2. Evaluate systems on Wiki-Fair benchmark to test performance on non-named entities and lowercased mentions
  3. Analyze error categories for each system to identify specific architectural weaknesses (e.g., partial names, metonyms, demonyms)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the new benchmarks (Wiki-Fair and News-Fair) compare to existing benchmarks in terms of their ability to evaluate entity linking systems for practical applications beyond named entities?
- Basis in paper: [explicit] The paper introduces two new benchmarks designed to address limitations in existing benchmarks, particularly the focus on named entities and unclear annotation guidelines.
- Why unresolved: While the paper presents these new benchmarks and evaluates several systems on them, it does not provide a direct comparison of their practical utility for real-world applications beyond named entities. A more comprehensive analysis of how well these benchmarks reflect the challenges of entity linking in practical scenarios is needed.
- What evidence would resolve it: A study comparing the performance of entity linking systems on the new benchmarks with their performance on benchmarks specifically designed for practical applications (e.g., domain-specific benchmarks) would provide insights into the practical utility of the new benchmarks.

### Open Question 2
- Question: What are the specific error categories that contribute most significantly to the overall performance differences between the entity linking systems?
- Basis in paper: [explicit] The paper uses the ELEVANT tool to analyze errors in fine-grained categories, such as "demonym," "metonym," "partial name," and "rare" mentions.
- Why unresolved: While the paper provides an overview of the error rates for each system across different categories, it does not delve into a detailed analysis of which error categories are the most significant contributors to the overall performance differences. Understanding the relative impact of each error category would help prioritize research efforts.
- What evidence would resolve it: A statistical analysis correlating the error rates in each category with the overall performance metrics (e.g., F1 score) for each system would reveal the most significant error categories.

### Open Question 3
- Question: How do the entity linking systems perform on benchmarks with a higher proportion of non-named entities compared to benchmarks with predominantly named entities?
- Basis in paper: [explicit] The paper highlights the limitation of existing benchmarks, which primarily focus on named entities, and introduces new benchmarks with a more diverse set of entity types.
- Why unresolved: The paper evaluates systems on both existing benchmarks (with mostly named entities) and the new benchmarks (with a mix of named and non-named entities). However, it does not provide a direct comparison of system performance on benchmarks with varying proportions of non-named entities. Understanding how system performance changes with the complexity of entity types would be valuable.
- What evidence would resolve it: A study comparing the performance of entity linking systems on benchmarks with different ratios of named to non-named entities would reveal the impact of entity type diversity on system performance.

## Limitations
- The exact versions of Wikidata dumps used for evaluation are not specified
- Some systems showed reproducibility issues during evaluation, though the authors were able to reproduce ReFinED results
- 18% of mentions in existing benchmarks are "undefined" (not annotated as entities), affecting fair comparison
- Focus on English-language benchmarks limits generalizability to other languages

## Confidence
- **High Confidence:** The systematic analysis of evaluation problems in existing benchmarks and the creation of new, fairer benchmarks (Wiki-Fair and News-Fair) with clear annotation guidelines.
- **Medium Confidence:** The comparative performance analysis of the six entity linking systems, particularly the finding that ReFinED outperforms others, though some reproducibility issues were encountered.
- **Low Confidence:** The generalizability of results beyond English benchmarks and the long-term stability of the evaluation framework given rapid advances in entity linking technology.

## Next Checks
1. Replicate the evaluation using multiple versions of Wikidata dumps to assess the impact of knowledge base version on system performance and reproducibility.
2. Test the six entity linking systems on non-English benchmarks to evaluate cross-lingual generalization of the findings.
3. Conduct ablation studies on the new benchmarks by systematically varying the inclusion of non-named entities and annotation alternatives to quantify their impact on system performance.