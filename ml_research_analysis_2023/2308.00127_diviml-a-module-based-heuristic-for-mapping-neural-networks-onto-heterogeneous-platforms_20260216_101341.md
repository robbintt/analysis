---
ver: rpa2
title: 'DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous
  Platforms'
arxiv_id: '2308.00127'
source_url: https://arxiv.org/abs/2308.00127
tags:
- scheduling
- heterogeneous
- latency
- milp
- milp-split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a general framework for mapping deep neural
  networks (DNNs) onto heterogeneous hardware platforms, such as systems with a CPU
  and multiple GPUs. The core idea is to use a mixed integer linear programming (MILP)
  formulation to optimally partition and schedule DNN tasks across the available devices,
  while also leveraging the modularity of many DNNs to speed up the optimization via
  a splitting heuristic (MILP-SPLIT).
---

# DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms

## Quick Facts
- arXiv ID: 2308.00127
- Source URL: https://arxiv.org/abs/2308.00127
- Reference count: 40
- Primary result: MILP-SPLIT achieves up to 395× faster scheduling with minimal quality loss for DNN mapping on heterogeneous hardware

## Executive Summary
This paper introduces DiviML, a framework for optimally mapping deep neural networks (DNNs) onto heterogeneous hardware platforms using mixed integer linear programming (MILP). The core innovation is a modularity-based splitting heuristic (MILP-SPLIT) that decomposes complex DNNs into manageable modules, enabling polynomial-time scheduling while maintaining near-optimal quality. The framework leverages both data and model parallelism and is validated on conventional DNNs, randomly-wired neural networks, and GPT-3 scheduling across multiple nodes.

## Method Summary
DiviML formulates DNN scheduling as an MILP optimization problem that captures device assignments, batching strategies, and execution constraints. For large graphs, MILP-SPLIT applies a modularity-based decomposition using articulation points and dynamic programming to solve subproblems independently. The approach is implemented with PyTorch FX for graph extraction and Gurobi for MILP solving, supporting both single-node and distributed multi-node configurations with symmetry-breaking optimizations.

## Key Results
- Up to 3× lower latency and 2.9× higher throughput compared to single-GPU execution
- MILP-SPLIT improves solution runtime by up to 395× with negligible quality loss
- Effective on both conventional DNNs and randomly-wired neural networks
- Successfully schedules GPT-3 across 6 heterogeneous servers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MILP-SPLIT leverages DNN modularity to reduce solving complexity from exponential to polynomial time.
- Mechanism: Decomposes the computational graph into modules connected by single-channel or multi-channel communication paths, independently solves subproblems for each module, and combines them via dynamic programming.
- Core assumption: The graph can be partitioned into balanced modules with manageable inter-module communication.
- Evidence anchors:
  - [abstract] "our modularity-based 'splitting' heuristic improves the solution runtime up to 395× without noticeably sacrificing solution quality"
  - [section] "the complexity of the approach can be expressed as O(|K|²|Q|T), where T represents a runtime bound for each module"
- Break condition: If modules are too large or too many inter-module channels exist, the partitioning overhead dominates and solution quality degrades.

### Mechanism 2
- Claim: The MILP formulation captures both model and data parallelism for batched DNN inference.
- Mechanism: Decision variables xi,j,l assign tasks to devices, bi,j,l encode batch sizes per device, and di1,i2,l1,l2 enforce ordering constraints. This allows simultaneous execution of different parts of the graph on different devices (model parallelism) and batching multiple inputs for parallel processing (data parallelism).
- Core assumption: Batching and device-specific execution speeds can be modeled linearly within memory and communication constraints.
- Evidence anchors:
  - [section] "we introduce a novel formulation of the problem as an MILP model that explicitly considers the option of batching"
  - [section] "By incorporating batching, our formulation is better suited to capture the characteristics of modern deep learning workloads"
- Break condition: When the number of batch/device combinations explodes, the MILP becomes intractable and heuristics are required.

### Mechanism 3
- Claim: Symmetry-breaking in distributed multi-node scheduling improves MILP-SPLIT scalability without loss of solution quality.
- Mechanism: By enforcing partial ordering constraints between identical devices across nodes (e.g., task counts on node i ≤ node j for i<j), the search space is compressed by a factorial factor while retaining all non-isomorphic schedules.
- Core assumption: The hardware setup exhibits node-level symmetry and the ordering constraint does not exclude the optimal mapping.
- Evidence anchors:
  - [section] "we introduce additional constraints to our MILP formulation, enforcing a partial ordering of certain variables (e.g. #batches, #tasks, time utilization) between identical devices within a node or between nodes"
  - [section] "our experimental results demonstrate that the choice of symmetry-breaking criterion can significantly impact the quality of the solution"
- Break condition: If symmetry assumptions fail (e.g., asymmetric device performance), the compressed space may exclude the optimal solution.

## Foundational Learning

- Concept: Mixed Integer Linear Programming (MILP)
  - Why needed here: The scheduling problem is NP-Hard; MILP provides an exact solver framework that can incorporate complex constraints like batching, memory limits, and communication costs.
  - Quick check question: Can you formulate the task-to-device assignment as a binary variable and the execution start time as a continuous variable?

- Concept: Graph partitioning and articulation points
  - Why needed here: Identifying modular subgraphs and communication channels is essential for the MILP-SPLIT heuristic to decompose the problem.
  - Quick check question: How would you detect articulation points in a DAG to identify module boundaries?

- Concept: Dataflow graph representations (DAGs)
  - Why needed here: DNNs are modeled as DAGs of tasks with dependencies; scheduling requires respecting these dependencies and mapping them onto heterogeneous devices.
  - Quick check question: In a DAG, how do you compute the transitive closure to identify redundant scheduling constraints?

## Architecture Onboarding

- Component map: PyTorch/TVM -> FX tracer -> DAG builder -> Scheduler -> Output mapping + schedule
- Critical path: DAG construction (benchmarking each op on each device) -> Module detection -> Solve per-module subproblems -> Combine via DP -> Output mapping
- Design tradeoffs:
  - Exact MILP guarantees optimality but scales poorly with graph size and batching options
  - MILP-SPLIT sacrifices some optimality for polynomial-time performance
  - Symmetry-breaking speeds up multi-node solving but may exclude optimal mappings if assumptions fail
- Failure signatures:
  - MILP timeout or infeasibility -> likely memory or batching constraints too tight
  - MILP-SPLIT quality drop -> module decomposition unbalanced or too many inter-module channels
  - Symmetry-breaking suboptimal -> hardware asymmetry not captured in constraints
- First 3 experiments:
  1. Run MILP on a small ResNet DAG on a 2-GPU system, verify latency improvement vs best-device baseline.
  2. Apply MILP-SPLIT to a single-module RWNN, compare runtime and solution quality to MILP.
  3. Deploy GPT-3 inference scheduling on the 6-node setup, test symmetry-breaking with batch vs task vs time ordering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MILP-SPLIT scale with increasingly large and complex DNN models beyond GPT-3, such as GPT-4 or larger language models?
- Basis in paper: [inferred] The paper mentions that their framework was extended to schedule GPT-3 across multiple heterogeneous servers, but only for a single decoding step. It also notes that they aim to explore more efficient methods for scheduling large DNNs on distributed systems in future work.
- Why unresolved: The paper only provides preliminary results for GPT-3 scheduling and does not explore larger or more complex models. The scalability of MILP-SPLIT for extremely large models is not addressed.
- What evidence would resolve it: Experimental results comparing MILP-SPLIT performance on increasingly large DNN models, such as GPT-4, PaLM, or other state-of-the-art language models, would demonstrate how well the approach scales.

### Open Question 2
- Question: How does the MILP-SPLIT heuristic perform when applied to non-vision DNNs, such as recurrent neural networks (RNNs), transformers for NLP tasks, or graph neural networks (GNNs)?
- Basis in paper: [inferred] The paper focuses on evaluating the framework on computer vision DNNs (Torchvision models and RWNNs) and only briefly mentions extending to GPT-3 for a single decoding step. The generalizability of MILP-SPLIT to other types of DNNs is not explored.
- Why unresolved: The paper does not provide any experimental results or analysis of how MILP-SPLIT performs on non-vision DNNs. The effectiveness of the modularity-based approach for these architectures is unknown.
- What evidence would resolve it: Experiments applying MILP-SPLIT to a diverse set of non-vision DNNs, including RNNs, transformers, and GNNs, and comparing the results to vision DNNs would demonstrate the framework's generalizability.

### Open Question 3
- Question: How does the MILP-SPLIT heuristic handle DNN models with dynamic or irregular computation graphs, such as those generated by neural architecture search (NAS) with conditional paths or dynamic control flow?
- Basis in paper: [explicit] The paper mentions that NAS has been used to create DNN architectures with irregular topologies, and that MILP-SPLIT is designed to handle such irregularity. However, the paper does not specifically address models with dynamic or irregular computation graphs.
- Why unresolved: While the paper demonstrates MILP-SPLIT's effectiveness on RWNNs, which have irregular topologies, it does not explore how the heuristic performs on DNNs with dynamic or irregular computation graphs beyond static architectures.
- What evidence would resolve it: Experiments evaluating MILP-SPLIT on DNN models with dynamic or irregular computation graphs, such as those generated by NAS with conditional paths or dynamic control flow, would demonstrate the heuristic's ability to handle such architectures.

## Limitations
- MILP-SPLIT's effectiveness depends heavily on balanced module decomposition; poor graph partitioning can lead to suboptimal schedules
- The approach requires accurate per-op device benchmarking and hardware specification; errors in profiling directly impact scheduling quality
- Scalability remains challenging for extremely large models despite the heuristic improvement

## Confidence
- **High confidence**: MILP formulation correctness, latency/throughput improvements on tested configurations
- **Medium confidence**: MILP-SPLIT quality retention across diverse DNN architectures
- **Low confidence**: Symmetry-breaking impact generalizability across heterogeneous hardware configurations

## Next Checks
1. Test MILP-SPLIT on a wider range of RWNN configurations to verify quality retention beyond the reported examples
2. Evaluate scheduling quality when device profiling data contains ±10% errors to assess robustness
3. Benchmark on heterogeneous GPU configurations (different GPU models) to validate symmetry-breaking assumptions