---
ver: rpa2
title: Acoustic and linguistic representations for speech continuous emotion recognition
  in call center conversations
arxiv_id: '2310.04481'
source_url: https://arxiv.org/abs/2310.04481
tags:
- speech
- features
- linguistic
- fusion
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses continuous satisfaction recognition in call-center
  conversations using acoustic and linguistic representations. The authors explore
  pre-trained features like Wav2Vec and CamemBERT to compensate for limited annotated
  emotional data.
---

# Acoustic and linguistic representations for speech continuous emotion recognition in call center conversations

## Quick Facts
- arXiv ID: 2310.04481
- Source URL: https://arxiv.org/abs/2310.04481
- Reference count: 40
- Key outcome: Pre-trained linguistic features (CamemBERT) significantly outperform acoustic features (Wav2Vec) for continuous satisfaction recognition in call-center conversations, with fusion improving robustness to annotation subjectivity.

## Executive Summary
This paper investigates continuous satisfaction recognition in call-center conversations using acoustic and linguistic representations. The authors leverage pre-trained features (Wav2Vec for acoustic, CamemBERT for linguistic) to compensate for limited annotated emotional data. Experiments demonstrate that pre-trained features significantly outperform traditional hand-crafted features (MFCCs, Word2Vec). Surprisingly, linguistic content emerges as the primary contributor, with CamemBERT representations achieving superior performance compared to acoustic modalities. While fusion approaches improve robustness to annotation subjectivity, they don't consistently outperform linguistic-only models. The study concludes that semantic content carries more emotional information than prosody in this domain, suggesting words are more informative than acoustic cues for satisfaction prediction.

## Method Summary
The authors use the AlloSat corpus (303 French call-center conversations, 37h total) with continuous satisfaction annotations from three annotators. They extract acoustic features (MFCCs, Wav2Vec) and linguistic features (Word2Vec, CamemBERT) at 250ms emotional segments. A baseline biLSTM architecture (4 layers: 200-64-32-32 units) is trained on these features. Experiments compare single-modality approaches with fusion strategies (feature, model, and decision fusion). Performance is evaluated using Concordance Correlation Coefficient (CCC) with confidence intervals. The study analyzes robustness through weight initialization and individual annotator performance.

## Key Results
- Pre-trained features (Wav2Vec, CamemBERT) significantly outperform baseline MFCCs and Word2Vec for satisfaction recognition
- Linguistic modality (CamemBERT) is the primary contributor, outperforming acoustic modalities
- Fusion approaches improve robustness to annotation subjectivity but don't consistently outperform linguistic-only models
- Linguistic analysis reveals frustration correlates with disfluencies, repetitions, and semantic markers
- Emotional information lies more in words than prosodic/acoustic content for this task

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained acoustic and linguistic features improve satisfaction recognition through transfer learning
- Transfer learning leverages rich representations from large datasets (LibriSpeech for Wav2Vec, OSCAR for CamemBERT) to compensate for limited labeled data
- Core assumption: Patterns from general speech/text corpora are transferable to call-center satisfaction prediction
- Evidence: Experiments confirm significant performance gains with pre-trained features over MFCCs and Word2Vec

### Mechanism 2
- Linguistic modality is primary contributor due to semantic content carrying more satisfaction information
- Semantic meaning of transcribed speech provides more satisfaction cues than prosodic/acoustic features
- Core assumption: In call-center conversations, satisfaction is more directly conveyed through word meaning than prosody
- Evidence: CamemBERT outperforms acoustic modalities; semantic markers of frustration correlate with satisfaction levels

### Mechanism 3
- Fusion improves robustness to annotation subjectivity through complementary modality information
- Different annotators rely on different modalities (some linguistic, others acoustic) when labeling satisfaction
- Core assumption: Annotators have varying perceptions of satisfaction reflected in their modality reliance
- Evidence: Fusion approaches are more robust to annotation subjectivity despite not always improving overall performance

## Foundational Learning

- **Transfer Learning**: Needed to leverage pre-trained models on limited call-center data. Quick check: What are risks of transfer learning and how to mitigate them?
- **Modality Fusion**: Needed to capture complementary information from acoustic and linguistic channels. Quick check: What fusion types exist and when are they appropriate?
- **Confidence Intervals for CCC**: Needed to measure reliability given small sample sizes. Quick check: How are CCC confidence intervals calculated and what factors influence their width?

## Architecture Onboarding

- **Component map**: Acoustic/Linguistic Features -> BiLSTM Processing -> Fusion (optional) -> Continuous Satisfaction Output -> CCC Evaluation
- **Critical path**: Feature extraction → Model training (single or fusion) → Evaluation on Dev/Test with CCC → Analysis of robustness to annotation subjectivity and initialization
- **Design tradeoffs**: Pre-trained vs hand-crafted features (performance vs computational resources); single modality vs fusion (robustness vs performance); early vs late fusion (computation vs interaction capture)
- **Failure signatures**: Poor Test performance (overfitting/generalization issues); high CCC variability across initializations (sensitivity to initialization); low inter-annotator agreement (high subjectivity)
- **First 3 experiments**: 1) Baseline MFCC+Word2Vec biLSTM; 2) Pre-trained Wav2Vec+CamemBERT single modalities; 3) Fused Wav2Vec+CamemBERT model

## Open Questions the Paper Calls Out

### Open Question 1
- How does Wav2Vec2.0 compare to Wav2Vec1.0 for satisfaction recognition in call-center data?
- Basis: Authors note English Wav2Vec1.0 performs better than MFCCs and reference a study using Wav2Vec2.0 but don't conduct direct comparison
- Why unresolved: Only compares Wav2Vec1.0 to MFCCs, not Wav2Vec2.0 directly
- Resolution evidence: Direct experiments comparing Wav2Vec2.0 vs Wav2Vec1.0 vs MFCCs on AlloSat with CCC scores

### Open Question 2
- How does cross-annotator evaluation affect observed modality differences in satisfaction prediction?
- Basis: Fusion approaches improve robustness to annotation subjectivity; acoustic variability may lead to different satisfaction perceptions
- Why unresolved: Models evaluated on individual annotations but not cross-annotator generalization tested
- Resolution evidence: Models trained on one annotator's labels tested on others, comparing performance drops between acoustic and linguistic models

### Open Question 3
- Which specific linguistic features contribute most to satisfaction prediction beyond generic semantic content?
- Basis: Authors identify categories (disfluencies, negations, repetitions) but don't quantify specific feature contributions
- Why unresolved: Analysis uses broad categories rather than specific feature importance analysis
- Resolution evidence: Feature ablation studies or SHAP values showing specific linguistic features contributing most to predictions

## Limitations

- **Limited generalizability**: Findings based on French call-center corpus may not transfer to other languages or domains
- **Automatic transcription impact**: Study uses automatic transcriptions without analyzing transcription error effects on linguistic model performance
- **Confidence calibration uncertainty**: Absolute performance levels and superiority degree of linguistic features need more detailed quantification

## Confidence

- **High confidence**: Sound experimental methodology with statistical validation through confidence intervals
- **Medium confidence**: Interpretation of linguistic superiority is plausible but needs further validation through cross-annotator studies
- **Low confidence**: Generalizability to other languages, domains, and emotional dimensions is uncertain

## Next Checks

1. Evaluate proposed approach on call-center datasets in other languages (English, Spanish) to assess generalizability
2. Test model performance on other customer service interaction types (technical support, sales) beyond call-center domain
3. Conduct experiments with manually transcribed speech to quantify impact of automatic transcription errors on linguistic model performance