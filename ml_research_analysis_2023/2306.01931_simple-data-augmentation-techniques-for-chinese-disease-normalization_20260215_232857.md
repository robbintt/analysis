---
ver: rpa2
title: Simple Data Augmentation Techniques for Chinese Disease Normalization
arxiv_id: '2306.01931'
source_url: https://arxiv.org/abs/2306.01931
tags:
- disease
- data
- augmentation
- methods
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a data augmentation method for Chinese disease
  normalization. The key idea is to leverage the multi-axis and multi-grain nature
  of disease names, using semantic information and relations within diseases to generate
  new pairs of diseases for training.
---

# Simple Data Augmentation Techniques for Chinese Disease Normalization

## Quick Facts
- arXiv ID: 2306.01931
- Source URL: https://arxiv.org/abs/2306.01931
- Reference count: 5
- Primary result: DDA outperforms general data augmentation methods like EDA and back-translation, with up to 3% performance gain across various baseline models.

## Executive Summary
This paper proposes a data augmentation method for Chinese disease normalization that leverages the multi-axis and multi-grain nature of disease names. The approach uses semantic information and hierarchical relations within diseases to generate new training pairs. Two specific augmentation methods are introduced: Axis-word Replacement (AR) and Multi-Grain Aggregation (MGA). Experimental results on the CHIP-CDN dataset demonstrate that DDA outperforms standard augmentation techniques across multiple baseline models, particularly benefiting smaller datasets with limited training data.

## Method Summary
The method treats data augmentation as a pre-training task followed by fine-tuning on the original dataset. It first generates augmented pairs using AR (replacing axis-words like anatomical location or disease quality) and MGA (assigning labels based on hierarchical ICD code relationships). The model is then trained on these augmented pairs to learn broad semantic priors before being fine-tuned on the original CHIP-CDN dataset to align with exact labels. This two-stage approach aims to mitigate label confusion while expanding training coverage.

## Key Results
- DDA achieves up to 3% performance improvement over EDA and back-translation methods
- Method is particularly effective for smaller datasets with limited training data
- Improvements observed across multiple baseline models including BiLSTM, BERT-base, and CDN-Baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation via axis-word replacement preserves semantic similarity while expanding training coverage.
- Mechanism: By replacing one axis-word (e.g., anatomical location) in a clinical disease name with the corresponding axis-word from its standardized ICD label, new pairs are created that retain the underlying disease semantics but expose the model to varied expressions.
- Core assumption: Disease names have structural invariance; the relationship between clinical and standard names holds if axis-words are replaced in parallel.
- Evidence anchors:
  - [abstract] "Our methods rely on the Structural Invariance property of disease names..."
  - [section 4.1] "we assume that disease names have the property of structural invariance..."
  - [corpus] Weak; no corpus studies explicitly quantify semantic drift after replacement.
- Break condition: If the NER tool fails to identify axis-words accurately, the replaced terms may be semantically incompatible, leading to noisy pairs.

### Mechanism 2
- Claim: Multi-grain aggregation exploits hierarchical label transitivity to enrich label space.
- Mechanism: Assigning a fine-grained ICD code's label to its coarser-grained ancestor (e.g., 6-digit to 4-digit) lets the model learn that semantically similar diseases can share labels, reducing sparse-label effects.
- Core assumption: Labels have transitivity; a more specific disease can be grouped under a broader parent label.
- Evidence anchors:
  - [abstract] "labels in the disease normalization task have transitivity properties..."
  - [section 4.1] "a more specified description of an object can be comprised into a larger group..."
  - [corpus] No corpus evidence directly validates transitivity performance gain.
- Break condition: If the hierarchy is too coarse, semantically distinct diseases may be incorrectly grouped, degrading precision.

### Mechanism 3
- Claim: Augmented pre-training followed by fine-tuning mitigates label confusion and overfitting.
- Mechanism: Training first on augmented pairs builds broad semantic priors, then fine-tuning on the original dataset aligns the model to the exact label set without conflicting pseudo-labels.
- Core assumption: Separate training stages allow model to internalize semantics before exact label mapping.
- Evidence anchors:
  - [abstract] "we treat the data augmentation operation as a pre-training task..."
  - [section 4.2] "Taking the augmented data to train the disease normalization task. Fine-tuning the original disease normalization dataset."
  - [corpus] No corpus study directly links stage separation to reduced label confusion.
- Break condition: If augmented data contains too much noise, pre-training may harm downstream fine-tuning.

## Foundational Learning

- Concept: Named Entity Recognition (NER) for disease name components.
  - Why needed here: DDA relies on accurate axis-word identification to generate valid replacements.
  - Quick check question: Can your NER tool distinguish disease center, anatomical location, and disease quality in varied Chinese disease names?

- Concept: ICD coding hierarchy and multi-grain structure.
  - Why needed here: MGA methods require understanding of code relationships to assign pseudo-labels.
  - Quick check question: Do you know how 3-digit, 4-digit, and 6-digit ICD codes relate in terms of disease granularity?

- Concept: Pre-training vs. fine-tuning paradigm in transfer learning.
  - Why needed here: DDA's two-stage training mirrors transfer learning workflows to avoid catastrophic forgetting.
  - Quick check question: Have you implemented a two-stage training pipeline where a model is first trained on synthetic data and then fine-tuned on real data?

## Architecture Onboarding

- Component map:
  NER module -> ICD hierarchy loader -> Augmentation generator -> Pre-training trainer -> Fine-tuning trainer

- Critical path:
  1. Load and preprocess CHIP-CDN dataset.
  2. Extract axis-words using NER.
  3. Generate augmented pairs via AR and MGA.
  4. Train on augmented data (1 epoch, small LR).
  5. Fine-tune on original data (few epochs, larger LR).

- Design tradeoffs:
  - NER accuracy vs. augmentation volume: stricter NER reduces noise but may miss augmentation opportunities.
  - Code granularity in MGA: deeper hierarchy yields more data but risks semantic dilution.
  - Augmentation scale vs. training time: larger augmented sets improve coverage but increase compute.

- Failure signatures:
  - Sharp drop in fine-tuning accuracy: likely overfitting or noisy augmentation.
  - No improvement over baseline: possible NER failure or ineffective pseudo-label assignment.
  - Training instability: learning rate mismatch between pre-training and fine-tuning stages.

- First 3 experiments:
  1. Run baseline BILSTM on CHIP-CDN without augmentation.
  2. Apply only AR1-position augmentation and compare.
  3. Apply MGA-code2 only and measure label coverage increase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different axis replacement strategies (AR1, AR2, etc.) compare in terms of effectiveness for specific disease types or semantic components?
- Basis in paper: [explicit] The authors propose multiple axis replacement strategies but only evaluate them together as part of DDA. The ablation study shows removing AR reduces performance but doesn't compare strategies.
- Why unresolved: The paper doesn't provide detailed comparisons of individual AR strategies' effectiveness across different disease categories or semantic components.
- What evidence would resolve it: Detailed experimental results comparing each AR strategy separately on different disease types, showing which strategies work best for which semantic components.

### Open Question 2
- Question: What is the optimal balance between augmented training and fine-tuning when using DDA methods?
- Basis in paper: [explicit] The authors mention using augmented training as a pre-training task followed by fine-tuning, but don't explore different ratios or scheduling strategies.
- Why unresolved: The paper uses fixed training schedules but doesn't investigate how varying the amount of augmented vs. original data affects performance.
- What evidence would resolve it: Experiments varying the proportion of augmented data used in pre-training and the duration of fine-tuning, showing optimal training schedules.

### Open Question 3
- Question: How does DDA perform on languages other than Chinese or on different medical coding systems?
- Basis in paper: [explicit] The method is demonstrated only on Chinese disease normalization with ICD-10 coding.
- Why unresolved: The paper doesn't test the method's generalizability to other languages or medical coding systems like ICD-9 or SNOMED CT.
- What evidence would resolve it: Results showing DDA effectiveness on disease normalization tasks in other languages and with different medical coding systems.

## Limitations
- No quantitative validation of NER accuracy for axis-word extraction, critical for generating valid augmented pairs
- Lack of analysis on semantic compatibility of generated pairs, leaving open the possibility of introducing noise
- No comparison to alternative label smoothing or data augmentation techniques that might achieve similar gains with less complexity

## Confidence
- Axis-word replacement semantic preservation: Medium
- Multi-grain aggregation label transitivity: Medium
- Two-stage pre-training/fine-tuning effectiveness: Medium

## Next Checks
1. Measure NER accuracy on a held-out test set of Chinese disease names to ensure axis-word identification is sufficiently reliable for augmentation.
2. Conduct ablation studies comparing DDA with and without noise filtering to quantify the impact of semantic drift on final performance.
3. Perform error analysis on fine-tuning stage outputs to identify whether augmented data introduces conflicting labels or label confusion, and adjust pseudo-label assignment rules accordingly.