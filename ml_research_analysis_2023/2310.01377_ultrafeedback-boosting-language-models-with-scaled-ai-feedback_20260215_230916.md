---
ver: rpa2
title: 'UltraFeedback: Boosting Language Models with Scaled AI Feedback'
arxiv_id: '2310.01377'
source_url: https://arxiv.org/abs/2310.01377
tags:
- arxiv
- feedback
- preprint
- preference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UltraFeedback, a large-scale, high-quality,
  and diverse AI feedback dataset containing over 1 million GPT-4 annotations for
  250K user-assistant conversations. The dataset addresses the scarcity of preference
  data for open-source models by curating instructions from diverse sources and prompting
  various models with aspect-specific principles.
---

# UltraFeedback: Boosting Language Models with Scaled AI Feedback

## Quick Facts
- arXiv ID: 2310.01377
- Source URL: https://arxiv.org/abs/2310.01377
- Reference count: 35
- This paper introduces UltraFeedback, a large-scale AI feedback dataset with over 1 million GPT-4 annotations for 250K conversations, enabling state-of-the-art open-source chat models through preference learning.

## Executive Summary
This paper introduces UltraFeedback, a large-scale, high-quality, and diverse AI feedback dataset containing over 1 million GPT-4 annotations for 250K user-assistant conversations. The dataset addresses the scarcity of preference data for open-source models by curating instructions from diverse sources and prompting various models with aspect-specific principles. Experiments show that UltraFeedback-powered reward model UltraRM achieves the highest performance among open-source models on preference benchmarks. Best-of-n sampling and PPO based on UltraRM significantly improve open-source models, with UltraLM-13B-PPO achieving top performance on chat benchmarks. The critique model UltraCM also generates high-quality feedback.

## Method Summary
The UltraFeedback dataset is constructed through a systematic pipeline: instructions are sampled from six high-quality datasets (TruthfulQA, FalseQA, Evol-Instruct, UltraChat, ShareGPT, FLAN), models from a diverse pool are prompted to generate responses, and GPT-4 provides both scalar scores across multiple aspects and textual critique. The reward model UltraRM is trained using binary ranking loss on this data, while the chat model UltraLM-13B-PPO is enhanced through reinforcement learning using UltraRM. The critique model UltraCM is fine-tuned to generate high-quality feedback. Best-of-n sampling with UltraRM provides immediate quality improvements without additional training.

## Key Results
- UltraRM achieves the highest performance among open-source models on preference benchmarks
- UltraLM-13B-PPO significantly improves win rates on AlpacaEval (33.84% to 45.38%) and UltraChat (51.82% to 66.49%)
- UltraCM demonstrates superior critique generation quality across multiple question-answering datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can serve as a high-quality proxy for human feedback in training preference models
- Mechanism: GPT-4's advanced reasoning and alignment allow it to produce consistent, fine-grained evaluations across multiple aspects (instruction-following, truthfulness, honesty, helpfulness) that align with human preferences
- Core assumption: GPT-4's judgment quality is comparable to qualified human annotators and maintains consistency when evaluating multiple responses simultaneously
- Evidence anchors: 74.2% agreement between human and GPT-4 annotations on 100 samples, comparable to human-human agreement rates around 70%

### Mechanism 2
- Claim: Large-scale, diverse preference data enables stronger open-source reward models
- Mechanism: By scaling instruction and response diversity through multiple sources and model behaviors, UltraFeedback creates a more comprehensive preference space that captures nuanced human preferences
- Core assumption: Scale and diversity in preference data directly translate to better reward model generalization and alignment
- Evidence anchors: UltraFeedback contains 63,967 instructions with 4 responses per instruction, making it the largest non-community-labeled open-source preference dataset

### Mechanism 3
- Claim: Best-of-n sampling with UltraRM effectively selects high-quality responses without additional training
- Mechanism: Using the reward model to score multiple sampled responses and selecting the highest-scoring one provides immediate quality improvement by leveraging learned preferences
- Core assumption: The reward model's scoring correlates strongly with actual response quality as judged by humans
- Evidence anchors: Win rates on AlpacaEval increase proportionally with rewards, demonstrating the effectiveness of this approach

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the core technique being scaled and improved through UltraFeedback
  - Quick check question: What are the three main components of a typical RLHF pipeline?

- Concept: Preference modeling and ranking
  - Why needed here: The reward model must learn to rank responses based on human preferences across multiple aspects
  - Quick check question: How does binary ranking loss differ from regression loss in reward model training?

- Concept: Data contamination and evaluation integrity
  - Why needed here: Ensuring UltraFeedback doesn't overlap with evaluation benchmarks is critical for valid performance claims
  - Quick check question: What n-gram length is typically used for data contamination detection in LLM evaluation?

## Architecture Onboarding

- Component map: Instruction sampling -> Model sampling -> Principle application -> GPT-4 annotation -> Reward model training -> Chat model RLHF -> Critique model training
- Critical path: High-quality preference data -> Strong reward model -> Effective RLHF -> Better chat model
- Design tradeoffs:
  - Scale vs. annotation cost: More data improves models but increases GPT-4 API costs
  - Diversity vs. consistency: Broader instruction sources may introduce more variability in preferences
  - Fine-grained vs. overall scoring: More detailed annotations provide better signals but require more complex processing
- Failure signatures:
  - Reward model shows poor generalization to new prompt distributions
  - Best-of-n sampling fails to improve win rates on benchmarks
  - PPO training becomes unstable or shows reward hacking
- First 3 experiments:
  1. Test GPT-4 annotation consistency by having it score the same responses multiple times
  2. Validate reward model performance on held-out preference datasets
  3. Compare best-of-1 vs best-of-2 sampling win rates on a small benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UltraLM-13B-PPO compare to other open-source models on tasks not included in the UltraFeedback dataset, such as coding and mathematics?
- Basis in paper: Inferred
- Why unresolved: The paper states that UltraLM-13B-PPO falls behind gpt-3.5-turbo on math and code-related tasks, which might be attributed to the limitation of base model ability and the lack of relevant data in UltraFeedback. However, the paper does not provide a direct comparison of UltraLM-13B-PPO with other open-source models on these tasks.
- What evidence would resolve it: Experimental results comparing UltraLM-13B-PPO with other open-source models on coding and mathematics tasks.

### Open Question 2
- Question: What is the impact of fine-grained annotations on the performance of reward models compared to overall annotations?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that UltraRM-Overall discernibly lags behind UltraRM-UF and UltraRM on the WebGPT dataset, and suggests that fine-grained annotation provides a more precise assessment for each completion. However, the paper does not provide a comprehensive comparison of the performance of reward models trained with fine-grained annotations versus those trained with overall annotations.
- What evidence would resolve it: A detailed analysis comparing the performance of reward models trained with fine-grained annotations and those trained with overall annotations on multiple benchmarks.

### Open Question 3
- Question: How does the performance of UltraCM compare to other critique models when applied to tasks beyond question answering, such as text summarization or translation?
- Basis in paper: Inferred
- Why unresolved: The paper evaluates UltraCM on various question-answering tasks and demonstrates its superior performance compared to other critique models. However, it does not explore the performance of UltraCM on other types of tasks, such as text summarization or translation.
- What evidence would resolve it: Experimental results comparing UltraCM with other critique models on a diverse set of tasks, including text summarization and translation.

### Open Question 4
- Question: How does the scale and diversity of UltraFeedback impact the performance of models trained on it, compared to smaller or less diverse datasets?
- Basis in paper: Explicit
- Why unresolved: The paper highlights the scale and diversity of UltraFeedback as key factors in its effectiveness, but does not provide a direct comparison of the performance of models trained on UltraFeedback with those trained on smaller or less diverse datasets.
- What evidence would resolve it: Experimental results comparing the performance of models trained on UltraFeedback with those trained on smaller or less diverse datasets on multiple benchmarks.

### Open Question 5
- Question: What are the potential risks and ethical concerns associated with the use of UltraFeedback-powered models, such as UltraLM-13B-PPO and UltraCM?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges potential risks and ethical concerns, such as the generation of hallucinations and falsehoods, and the risk of misuse. However, it does not provide a detailed analysis of these risks or discuss potential mitigation strategies.
- What evidence would resolve it: A comprehensive analysis of the potential risks and ethical concerns associated with the use of UltraFeedback-powered models, along with proposed mitigation strategies.

## Limitations
- Reliance on GPT-4 annotations introduces cost and scalability bottlenecks
- 74.2% agreement rate with human annotations, while comparable to human-human agreement, leaves room for improvement
- Evaluation focuses primarily on preference benchmarks and chat performance, with limited analysis of long-term safety implications

## Confidence
- High confidence: The dataset construction methodology and the basic premise that scaling preference data improves reward models are well-supported by the results and existing literature
- Medium confidence: The claim that UltraRM achieves "highest performance among open-source models" is based on specific benchmarks and may not generalize to all evaluation scenarios
- Medium confidence: The assertion that GPT-4 serves as a reliable proxy for human feedback is supported by agreement rates but requires further validation across diverse domains

## Next Checks
1. **Cross-dataset validation**: Evaluate UltraRM's performance on multiple held-out preference datasets (including non-OpenAI annotated data) to assess generalization beyond the UltraFeedback domain
2. **Human evaluation replication**: Conduct independent human preference studies using a stratified sample of UltraFeedback annotations to verify the claimed 74.2% agreement rate and identify systematic biases
3. **Scalability analysis**: Test the dataset construction pipeline's performance when scaling beyond 250K conversations, measuring annotation consistency and quality degradation patterns as the instruction space expands