---
ver: rpa2
title: Application of frozen large-scale models to multimodal task-oriented dialogue
arxiv_id: '2310.00845'
source_url: https://arxiv.org/abs/2310.00845
tags:
- multimodal
- framework
- lens
- task-oriented
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the feasibility of multimodal task-oriented
  dialogue using the Large Language Models ENnhanced to See Framework (LENS Framework),
  which uses frozen pre-trained models without additional training. Experiments on
  the Multimodal Dialogs (MMD) dataset showed that the LENS Framework outperformed
  a Transformer-based model, achieving absolute lifts of 10.8% in fluency, 8.8% in
  usefulness, and 5.2% in relevance and coherence.
---

# Application of frozen large-scale models to multimodal task-oriented dialogue

## Quick Facts
- **arXiv ID**: 2310.00845
- **Source URL**: https://arxiv.org/abs/2310.00845
- **Reference count**: 31
- **Primary result**: LENS Framework outperforms Transformer-based model with absolute lifts of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance and coherence

## Executive Summary
This study demonstrates that frozen pre-trained large-scale models can effectively handle multimodal task-oriented dialogue without additional training. The LENS Framework uses BLIP2 for image captioning and Llama2-Chat for reasoning, achieving superior performance on the MMD dataset compared to a Transformer-based model. The work also extends the G-EVAL framework to evaluate multimodal task-oriented dialogues, showing that large language models can process multimodal information through caption-based representations.

## Method Summary
The LENS Framework processes multimodal task-oriented dialogue by first generating image captions using BLIP2, then feeding these captions along with dialogue context to Llama2-Chat 13B. The system uses one-shot prompting with examples from the MMD training data and evaluates performance using the extended G-EVAL framework with ChatGPT (GPT-4) as the evaluator. The framework is compared against a Transformer-based MATE model on the same dataset, using the same evaluation methodology.

## Key Results
- LENS Framework achieved absolute lifts of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance and coherence compared to MATE
- Large frozen pre-trained models (BLIP2 + Llama2-Chat) outperformed task-specific trained models on multimodal dialogue tasks
- The extended G-EVAL framework successfully evaluates multimodal task-oriented dialogues using ChatGPT as an evaluator

## Why This Works (Mechanism)

### Mechanism 1
Large-scale frozen models leverage richer pre-trained knowledge and reasoning capabilities. The LENS Framework preserves vast knowledge gained from large-scale pre-training while avoiding domain-specific overfitting. This mechanism assumes pre-trained models contain sufficient general knowledge about fashion domain terms and reasoning capabilities to handle multimodal dialogue tasks without task-specific fine-tuning.

### Mechanism 2
LLMs can handle multimodal dialogue by processing image captions as proxy multimodal input. The framework generates textual captions for images, then feeds these captions along with dialogue context to an LLM. This mechanism assumes image captions can adequately represent visual information needed for multimodal dialogue tasks.

### Mechanism 3
The LENS Framework's performance advantage stems from using a larger, more capable model (Llama2-Chat 13B) compared to the Transformer-based MATE model. The larger parameter size and more extensive pre-training provide superior language understanding, generation quality, and reasoning capabilities.

## Foundational Learning

- **Multimodal dialogue systems**: Understanding how different modalities (text, image) interact in dialogue contexts is crucial for designing and evaluating the LENS Framework
  - Quick check: What are the key challenges in multimodal dialogue that don't exist in unimodal dialogue?

- **Pre-trained model capabilities and limitations**: Understanding what frozen pre-trained models can and cannot do is essential for knowing when the LENS Framework will succeed or fail
  - Quick check: What are the main limitations of using frozen pre-trained models for specialized tasks?

- **Evaluation methodologies for dialogue systems**: Understanding how to properly evaluate dialogue system performance, including the use of automated evaluators like G-EVAL, is crucial for interpreting the study's results
  - Quick check: What are the advantages and limitations of using ChatGPT as an evaluator compared to human evaluation?

## Architecture Onboarding

- **Component map**: Image input → BLIP2 caption generation → Textual caption representation → Dialogue context + image captions → Llama2-Chat processing → Text response generation → G-EVAL evaluation → Performance metrics

- **Critical path**: Image → BLIP2 caption → LLM prompt → Text response → Evaluation

- **Design tradeoffs**:
  - Fixed parameters vs. fine-tuning: Preserves pre-trained knowledge but may miss task-specific optimizations
  - Caption-based multimodal representation vs. direct multimodal processing: Simpler implementation but may lose visual details
  - One-shot prompting vs. few-shot or fine-tuning: Easier to implement but may limit performance on complex tasks

- **Failure signatures**:
  - Hallucination in generated responses (mentions products or features not in the image)
  - Failure to follow instructions in prompts
  - Inconsistent performance across different types of dialogue contexts
  - Over-reliance on caption information while missing visual nuances

- **First 3 experiments**:
  1. Test the caption generation quality by having humans evaluate BLIP2 captions against the original images
  2. Evaluate the LLM's ability to follow instructions by testing with various prompt formats and complexity levels
  3. Test the system's robustness by evaluating performance on dialogue contexts with varying levels of visual dependency

## Open Questions the Paper Calls Out

### Open Question 1
How can the LENS Framework be improved to reduce hallucinations in generated responses?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the LENS Framework constructed in this study has a drawback of hallucination, where it outputs fictitious information such as sizes of products and materials. However, the paper does not provide specific methods or strategies to mitigate this issue.

### Open Question 2
What are the potential causes of the LENS Framework's failure to follow instructions and take desired actions?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the LLM used in the LENS Framework did not follow instructions in certain situations, but it does not provide a detailed analysis of the potential causes for this behavior.

### Open Question 3
How can the evaluation of multimodal task-oriented dialogues be improved beyond using ChatGPT as an evaluator?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that using ChatGPT as an evaluator for multimodal task-oriented dialogues is a new area of study and raises concerns about the potential bias in evaluating LLM-generated text more highly than human-written text.

### Open Question 4
How can the LENS Framework be adapted to utilize all past contexts instead of only the previous two turns of dialogue?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the LENS Framework used in the study only utilized the previous two turns of dialogue, which is considered impractical.

### Open Question 5
How can the LENS Framework be extended to incorporate a knowledge base for better performance in multimodal task-oriented dialogues?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the LENS Framework used in the study did not utilize the knowledge base of the MMD dataset, which may limit its performance.

## Limitations

- The study was conducted on a single domain (fashion) and dataset (MMD), limiting generalizability to other multimodal dialogue domains
- The G-EVAL evaluation using ChatGPT (GPT-4) as an automated evaluator may introduce bias in evaluating LLM-generated responses
- The modifications made to image captions before feeding them to the LLM were not fully specified, affecting reproducibility

## Confidence

**High Confidence Claims:**
- The LENS Framework achieves superior performance on the MMD dataset compared to the MATE model across all four evaluation metrics
- Large-scale frozen pre-trained models can effectively handle multimodal task-oriented dialogue tasks without additional training
- The extended G-EVAL framework can be applied to multimodal task-oriented dialogues

**Medium Confidence Claims:**
- The performance advantage stems primarily from using larger, more capable pre-trained models rather than task-specific training
- The caption-based approach adequately represents visual information for dialogue tasks
- The findings generalize to other multimodal task-oriented dialogue domains beyond fashion

**Low Confidence Claims:**
- The LENS Framework will outperform other contemporary multimodal dialogue approaches
- The performance gains would be maintained if fine-tuning were allowed
- The approach scales effectively to more complex multimodal scenarios

## Next Checks

1. **Domain Generalization Test**: Evaluate the LENS Framework on at least two additional task-oriented dialogue domains (e.g., travel booking and restaurant reservations) using the modified G-EVAL evaluation to assess whether performance gains generalize beyond the fashion domain.

2. **Ablation Study**: Conduct controlled experiments comparing: (a) LENS Framework with Llama2-Chat 13B, (b) LENS Framework with smaller LLM (7B), (c) LENS Framework with MATE model, and (d) LENS Framework with fine-tuned models to isolate the contributions of model size, architecture, and training approach.

3. **Human Evaluation Validation**: Supplement the automated G-EVAL evaluation with human judgments on a subset of responses to verify that the automated evaluator's rankings align with human perceptions of fluency, usefulness, relevance, and coherence.