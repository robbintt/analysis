---
ver: rpa2
title: Hybrid hidden Markov LSTM for short-term traffic flow prediction
arxiv_id: '2307.04954'
source_url: https://arxiv.org/abs/2307.04954
tags:
- traffic
- lstm
- state
- hidden
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a hybrid hidden Markov model-long short-term
  memory (HMM-LSTM) architecture for short-term traffic flow prediction. The method
  leverages the complementary strengths of HMM for capturing underlying traffic state
  patterns and LSTM for modeling temporal dependencies.
---

# Hybrid hidden Markov LSTM for short-term traffic flow prediction

## Quick Facts
- arXiv ID: 2307.04954
- Source URL: https://arxiv.org/abs/2307.04954
- Reference count: 9
- This study proposes a hybrid hidden Markov model-long short-term memory (HMM-LSTM) architecture for short-term traffic flow prediction, achieving 31-52% improvement in RMSE over baseline methods.

## Executive Summary
This study proposes a hybrid hidden Markov model-long short-term memory (HMM-LSTM) architecture for short-term traffic flow prediction. The method leverages the complementary strengths of HMM for capturing underlying traffic state patterns and LSTM for modeling temporal dependencies. The hybrid model significantly outperforms baseline methods (stacked LSTM and AR-HMM) in predicting traffic flow fluctuations, with a 31-52% improvement in root mean squared error (RMSE) compared to the best baseline model. Specifically, the C-Hybrid model achieved an RMSE of 0.4203, compared to 0.8766 for the AR-HMM model. The study demonstrates that combining HMM and LSTM architectures can effectively capture complex traffic dynamics and improve prediction accuracy in short-term traffic flow forecasting.

## Method Summary
The hybrid HMM-LSTM architecture combines a hidden Markov model for regime identification with long short-term memory networks for temporal dependency learning. The method uses 5-minute aggregated traffic flow data from a single detector on I-05 NB in California, detrended to compute flow fluctuations. Two hybrid architectures are proposed: S-Hybrid (HMM probabilities → LSTM) and C-Hybrid (merged LSTM outputs). The models are trained using Adadelta optimizer with MSE loss and LeakyReLU activations, with 60% of data for training, 15% for validation, and 25% for testing.

## Key Results
- C-Hybrid model achieved RMSE of 0.4203, outperforming AR-HMM (RMSE 0.8766) by 31-52%
- Hybrid models outperform both stacked LSTM and AR-HMM baselines in prediction accuracy
- The two models learn complementary feature representations of observed sequences
- HMM captures short-term traffic regime transitions that LSTM alone misses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HMM captures short-term traffic regime transitions that LSTM alone misses.
- Mechanism: HMM models latent traffic states (free-flow, congestion) as a Markov chain, allowing abrupt transitions between regimes to be explicitly identified.
- Core assumption: Traffic flow data can be represented as a mixture of discrete latent states, each following a distinct emission distribution.
- Evidence anchors:
  - [abstract] "HMM for regime identification is capable of capturing complex dynamic patterns and non-stationarity"
  - [section] "Markov-switching models with a hidden Markov model (HMM) for regime identification"
  - [corpus] Weak - no direct neighbor mentions regime identification performance
- Break condition: If traffic flow exhibits continuous gradual transitions without clear regime boundaries, the discrete state assumption breaks down.

### Mechanism 2
- Claim: LSTM captures long-term temporal dependencies that HMM cannot model.
- Mechanism: LSTM uses gated memory cells to preserve information across long sequences, learning temporal patterns within each traffic regime.
- Core assumption: Traffic patterns exhibit temporal dependencies that persist across regime changes and are not fully captured by state transition probabilities alone.
- Evidence anchors:
  - [abstract] "long short-term memory (LSTM) are designed to retain long-term temporal correlations"
  - [section] "LSTM, a type of RNN, consists of memory cells in its hidden layers and several gating mechanisms"
  - [corpus] Weak - neighbors focus on other applications, not temporal dependency capture
- Break condition: If traffic data is truly independent between observations, LSTM's memory mechanisms provide no benefit.

### Mechanism 3
- Claim: Combining HMM and LSTM creates complementary feature representations that improve prediction accuracy.
- Mechanism: HMM provides probabilistic state information as features, while LSTM learns temporal dependencies from both raw traffic data and HMM state sequences.
- Core assumption: HMM and LSTM learn orthogonal information about the data that, when combined, provides better predictive performance than either model alone.
- Evidence anchors:
  - [abstract] "hybrid models that jointly use HMM and LSTM... outperform in terms of prediction accuracy the LSTM and auto-regressive HMM regime switching models"
  - [section] "the two models learning complementary feature representations of an observed sequence"
  - [corpus] Weak - neighbors don't compare hybrid vs individual models directly
- Break condition: If HMM and LSTM learn highly correlated representations, the hybrid model provides minimal benefit over individual models.

## Foundational Learning

- Concept: Hidden Markov Models and state space representation
  - Why needed here: Understanding how HMM models latent traffic states as a Markov chain with emission distributions
  - Quick check question: What are the three main components of an HMM and how do they relate to traffic flow modeling?

- Concept: Long Short-Term Memory architecture and gating mechanisms
  - Why needed here: Understanding how LSTM preserves temporal information through forget, input, and output gates
  - Quick check question: How does the LSTM cell state differ from the hidden state, and why is this distinction important for traffic prediction?

- Concept: Traffic flow characteristics and Poisson process modeling
  - Why needed here: Understanding why traffic flow fluctuations follow Skellam distributions and how this relates to HMM emission modeling
  - Quick check question: Why are traffic flow fluctuations modeled as Skellam distributions rather than direct Poisson distributions?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Traffic flow to fluctuation conversion
  - HMM training: State identification with Gaussian mixture emissions
  - LSTM training: Temporal dependency learning from raw and HMM features
  - Hybrid combination: S-Hybrid (HMM→LSTM) or C-Hybrid (parallel LSTMs→merge)
  - Output layer: Final prediction from combined features

- Critical path:
  1. Preprocess traffic flow data into fluctuations
  2. Train HMM to identify traffic regimes
  3. Extract HMM features (state probabilities or sequences)
  4. Train LSTM(s) on appropriate feature sets
  5. Combine LSTM outputs and generate final prediction

- Design tradeoffs:
  - State count vs model complexity: More states capture finer regimes but increase computational cost
  - Mixture components: Single Gaussian vs mixture affects emission distribution flexibility
  - Sojourn density choice: Geometric (simple) vs logarithmic (flexible) impacts regime duration modeling
  - Hybrid architecture: S-Hybrid (sequential) vs C-Hybrid (parallel) affects feature integration approach

- Failure signatures:
  - Poor performance: Check if HMM state transitions align with actual traffic regime changes
  - Overfitting: Monitor validation loss during training, consider dropout or regularization
  - Mode collapse: Verify HMM emission distributions capture observed data variability
  - Temporal misalignment: Ensure LSTM receives properly aligned features from HMM

- First 3 experiments:
  1. Baseline comparison: Train individual LSTM and AR-HMM models to establish performance floor
  2. State sensitivity: Train S-Hybrid with 3, 5, and 7 HMM states to find optimal regime granularity
  3. Sojourn distribution comparison: Compare geometric vs logarithmic sojourn densities on same dataset to validate distribution choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the C-Hybrid model compare to the S-Hybrid model in terms of prediction accuracy and computational efficiency?
- Basis in paper: [explicit] The paper states that the C-Hybrid model outperforms the S-Hybrid model in terms of prediction accuracy, with an RMSE of 0.4203 compared to 0.5326 for the S-Hybrid model. However, the paper does not provide information on the computational efficiency of the models.
- Why unresolved: The paper does not provide information on the computational efficiency of the C-Hybrid and S-Hybrid models, making it difficult to compare their performance in terms of prediction accuracy and computational efficiency.
- What evidence would resolve it: Comparative analysis of the computational efficiency of the C-Hybrid and S-Hybrid models, including training time, inference time, and memory usage.

### Open Question 2
- Question: How does the performance of the hybrid models compare to other deep learning models, such as convolutional neural networks (CNNs) or graph neural networks (GNNs), in terms of prediction accuracy and computational efficiency?
- Basis in paper: [inferred] The paper compares the performance of the hybrid models to the baseline LSTM and AR-HMM models, but does not compare their performance to other deep learning models such as CNNs or GNNs.
- Why unresolved: The paper does not provide information on the performance of the hybrid models compared to other deep learning models, making it difficult to assess their relative strengths and weaknesses.
- What evidence would resolve it: Comparative analysis of the performance of the hybrid models, CNNs, and GNNs in terms of prediction accuracy and computational efficiency, using the same dataset and evaluation metrics.

### Open Question 3
- Question: How does the performance of the hybrid models vary with different traffic flow regimes, such as low flow, increasing flow, high flow, and decreasing flow?
- Basis in paper: [explicit] The paper presents a comparison of the variances of outputs belonging to specific traffic regimes in the 6-dimensional feature space for each model, indicating that the hybrid models perform better in localizing features in the space, resulting in enhanced performance. However, the paper does not provide a detailed analysis of the performance of the hybrid models in different traffic flow regimes.
- Why unresolved: The paper does not provide a detailed analysis of the performance of the hybrid models in different traffic flow regimes, making it difficult to assess their strengths and weaknesses in different traffic conditions.
- What evidence would resolve it: Detailed analysis of the performance of the hybrid models in different traffic flow regimes, including prediction accuracy, computational efficiency, and feature localization, using appropriate evaluation metrics and visualizations.

## Limitations

- The study uses only a single traffic detector location, which may not represent diverse traffic conditions across different environments and times
- Specific implementation details of HMM training and hyperparameter selection are not fully specified, making exact reproduction challenging
- Limited comparison with only two baseline models (stacked LSTM and AR-HMM) restricts the generalizability of performance claims

## Confidence

- High confidence in the general hybrid architecture approach and its theoretical justification
- Medium confidence in the specific performance improvements due to limited baseline comparisons and implementation details
- Medium confidence in the claim that HMM and LSTM learn complementary representations, as this is primarily inferred from performance rather than directly measured

## Next Checks

1. Implement and test the hybrid model on multiple traffic detector locations with varying traffic conditions to assess generalizability
2. Conduct ablation studies to quantify the individual contributions of HMM state features versus raw traffic data in the LSTM
3. Compare the hybrid approach against additional state-of-the-art traffic prediction methods (e.g., Transformer-based models, attention mechanisms) to establish relative performance