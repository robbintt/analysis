---
ver: rpa2
title: 'AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE'
arxiv_id: '2310.08012'
source_url: https://arxiv.org/abs/2310.08012
tags:
- polynomial
- autofhe
- accuracy
- bootstrapping
- evorelu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoFHE, an automated framework for adapting
  CNNs for efficient evaluation over fully homomorphic encryption (FHE). AutoFHE addresses
  limitations of existing methods by automatically optimizing layerwise mixed-degree
  polynomial approximations of ReLU activations and their placement of bootstrapping
  operations.
---

# AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE

## Quick Facts
- arXiv ID: 2310.08012
- Source URL: https://arxiv.org/abs/2310.08012
- Authors: 
- Reference count: 40
- Key outcome: AutoFHE accelerates secure inference by 1.32× to 1.8× compared to high-degree polynomial methods, improves accuracy by up to 2.56% compared to low-degree methods, and outperforms state-of-the-art TFHE approaches by 103× speedup with 3.46% accuracy improvement.

## Executive Summary
AutoFHE introduces an automated framework for adapting CNNs to run efficiently under fully homomorphic encryption (FHE). The key innovation is layerwise mixed-degree polynomial approximations of ReLU activations optimized jointly with bootstrapping placement through multi-objective search. This approach balances the trade-off between accuracy and latency by exploiting varying layer sensitivities to approximation error. Experimental results demonstrate significant improvements over existing methods, achieving 1.32× to 1.8× speedup while maintaining or improving accuracy across CIFAR-10 and CIFAR-100 benchmarks.

## Method Summary
AutoFHE employs a multi-objective optimization framework that jointly searches for optimal layerwise polynomial degrees, coefficients, and bootstrapping placement. The method uses EvoReLU activation functions with layerwise mixed-degree polynomials, optimized through differential evolution and R-CCDE coefficient optimization. Training incorporates a polynomial-aware fine-tuning algorithm with knowledge distillation loss to adapt pretrained ReLU network weights to the polynomial activation functions. The framework operates under RNS-CKKS FHE parameters and aims to maximize accuracy while minimizing bootstrapping operations.

## Key Results
- 1.32× to 1.8× speedup compared to high-degree polynomial methods
- Up to 2.56% accuracy improvement over low-degree polynomial approaches
- 103× faster than TFHE-based methods with 3.46% accuracy gain
- Flexible adaptation for any CNN architecture while optimizing accuracy-latency trade-off

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Layerwise mixed-degree polynomials exploit varying layer sensitivity to approximation error, reducing overall bootstrapping operations while maintaining accuracy.
- **Mechanism**: Different layers in a CNN have different sensitivities to approximation error. By assigning lower-degree polynomials to less sensitive layers and higher-degree polynomials to more sensitive layers, the total number of bootstrapping operations can be reduced without significantly compromising accuracy.
- **Core assumption**: The sensitivity to approximation error varies significantly across different layers in a CNN.
- **Evidence anchors**:
  - [abstract]: "The key idea is to adopt layerwise mixed-degree polynomial activation functions, which are optimized jointly with the homomorphic evaluation architecture in terms of the placement of bootstrapping operations."
  - [section]: "We posit that by taking advantage of approximation and training, we can inherit weights from ReLU networks and fine-tune polynomial networks to adapt learnable weights to layerwise mixed-degree polynomials."
- **Break condition**: If layer sensitivity to approximation error is uniform across the network, or if the computational overhead of managing mixed-degree polynomials outweighs the benefits of reduced bootstrapping.

### Mechanism 2
- **Claim**: Joint optimization of polynomial degrees and coefficients with the homomorphic evaluation architecture leads to better trade-offs between accuracy and latency.
- **Mechanism**: By formulating the search problem as a multi-objective optimization, AutoFHE can explore the joint design space of polynomial degrees, coefficients, and bootstrapping placement to find solutions that balance accuracy and latency effectively.
- **Core assumption**: The optimal placement of bootstrapping operations is interdependent with the choice of polynomial degrees and coefficients.
- **Evidence anchors**:
  - [abstract]: "The problem is modeled within a multi-objective optimization framework to maximize accuracy and minimize the number of bootstrapping operations."
  - [section]: "We formulate the search problem as a multi-objective optimization min DDD {1 − Accval (g (ωωω∗ | DDD, ΛΛΛ(DDD))) ,Boot(DDD)} s.t. ωωω∗ = argmin ωωω Ltrain (g (ωωω | DDD, ΛΛΛ(DDD)))"
- **Break condition**: If the interdependence between polynomial design and bootstrapping placement is negligible, or if the optimization framework fails to explore the design space effectively.

### Mechanism 3
- **Claim**: Hybrid approach of approximation and training allows inheriting representation learning ability from ReLU networks while adapting to polynomial activations.
- **Mechanism**: By initializing polynomial networks with pretrained ReLU weights and fine-tuning them with a knowledge distillation loss, AutoFHE can leverage the representation learning ability of ReLU networks while adapting to the constraints of polynomial activations.
- **Core assumption**: The knowledge learned by ReLU networks can be effectively transferred to polynomial networks through fine-tuning.
- **Evidence anchors**:
  - [abstract]: "We posit that by taking advantage of approximation and training, we can inherit weights from ReLU networks and fine-tune polynomial networks to adapt learnable weights to layerwise mixed-degree polynomials."
  - [section]: "We introduce the KL loss because it can push the output of the polynomial network close to the ReLU network. It can be regarded as knowledge distillation (KD) [25]."
- **Break condition**: If the representation learning ability of ReLU networks is not transferable to polynomial networks, or if the fine-tuning process fails to adapt the weights effectively.

## Foundational Learning

- **Concept**: Fully Homomorphic Encryption (FHE) and its leveled variant (LHE).
  - **Why needed here**: Understanding FHE is crucial for grasping the challenges and solutions in secure CNN inference, as AutoFHE is designed to work under FHE schemes like RNS-CKKS.
  - **Quick check question**: What are the main differences between FHE and LHE, and why is FHE necessary for secure CNN inference?

- **Concept**: Polynomial approximation of non-linear activation functions.
  - **Why needed here**: AutoFHE relies on replacing ReLU activations with polynomial approximations, so understanding the principles and trade-offs of polynomial approximation is essential.
  - **Quick check question**: What are the advantages and disadvantages of using high-degree vs. low-degree polynomials for approximating ReLU functions in the context of FHE?

- **Concept**: Multi-objective optimization.
  - **Why needed here**: AutoFHE formulates the search problem as a multi-objective optimization to balance accuracy and latency, so understanding the principles of multi-objective optimization is crucial for grasping the search algorithm.
  - **Quick check question**: What are the key differences between single-objective and multi-objective optimization, and why is multi-objective optimization more suitable for AutoFHE's problem?

## Architecture Onboarding

- **Component map**: Search Space -> Multi-objective Optimization -> R-CCDE Coefficient Optimization -> Polynomial-aware Training

- **Critical path**:
  1. Define search space with layerwise mixed-degree polynomials
  2. Formulate multi-objective optimization problem
  3. Implement multi-objective search algorithm
  4. Optimize polynomial coefficients using R-CCDE
  5. Fine-tune polynomial networks using polynomial-aware training

- **Design tradeoffs**:
  - Higher-degree polynomials offer better accuracy but increase latency due to more bootstrapping operations
  - Lower-degree polynomials reduce latency but may compromise accuracy
  - Mixed-degree polynomials balance accuracy and latency by exploiting layer sensitivity

- **Failure signatures**:
  - Inaccurate polynomial approximations leading to significant accuracy loss
  - Excessive bootstrapping operations causing high latency
  - Ineffective fine-tuning leading to poor adaptation of weights to polynomial activations

- **First 3 experiments**:
  1. Evaluate the accuracy-latency trade-off of AutoFHE on a small CNN architecture (e.g., ResNet-20) on CIFAR-10
  2. Compare the performance of AutoFHE with high-degree (MPCNN) and low-degree (AESPA) baselines on CIFAR-100
  3. Analyze the layerwise distribution of polynomial degrees discovered by AutoFHE and its impact on accuracy and latency

## Open Questions the Paper Calls Out
No explicit open questions were called out in the provided content.

## Limitations
- Scalability concerns for larger, more complex CNN architectures beyond CIFAR models
- Performance tightly coupled with specific RNS-CKKS FHE parameters (N=2^16, L=30)
- Layerwise sensitivity assumption may not hold universally across all CNN architectures or datasets

## Confidence

- **Multi-objective optimization effectiveness (High)**: The claim that jointly optimizing polynomial degrees and bootstrapping placement improves accuracy-latency trade-off is well-supported by the experimental results showing consistent improvements over single-objective approaches.

- **Mixed-degree polynomial advantage (Medium)**: While the paper demonstrates benefits of mixed-degree polynomials, the extent of these advantages may be architecture-dependent. The 1.32× to 1.8× speedup claims are specific to the evaluated CIFAR models.

- **Fine-tuning effectiveness (Medium)**: The hybrid approach of inheriting ReLU weights and fine-tuning with knowledge distillation is theoretically sound, but the actual transfer of learning may vary with different network architectures and datasets.

## Next Checks

1. **Architecture Scalability Test**: Evaluate AutoFHE on larger CNN architectures (e.g., ResNet-50, EfficientNet) trained on more complex datasets (e.g., ImageNet) to verify if the claimed speedups and accuracy improvements scale proportionally with model size.

2. **Parameter Sensitivity Analysis**: Systematically vary the RNS-CKKS parameters (modulus bit size, number of levels, coefficient modulus) to assess how robust AutoFHE's performance is to different cryptographic configurations and whether the optimization framework adapts effectively to these changes.

3. **Layer Sensitivity Validation**: Conduct ablation studies that force uniform polynomial degrees across all layers to empirically verify the paper's assumption about layerwise sensitivity to approximation error and quantify the actual contribution of mixed-degree polynomials to overall performance gains.