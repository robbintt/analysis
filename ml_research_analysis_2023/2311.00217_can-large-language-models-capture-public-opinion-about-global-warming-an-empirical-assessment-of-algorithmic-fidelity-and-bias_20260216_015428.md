---
ver: rpa2
title: Can Large Language Models Capture Public Opinion about Global Warming? An Empirical
  Assessment of Algorithmic Fidelity and Bias
arxiv_id: '2311.00217'
source_url: https://arxiv.org/abs/2311.00217
tags:
- global
- warming
- llms
- survey
- demographics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study assesses the algorithmic fidelity and bias of large
  language models (LLMs) in simulating public opinion on global warming using two
  nationally representative surveys. LLMs were conditioned on demographics and/or
  psychological covariates to generate synthetic responses, which were then compared
  against actual survey data.
---

# Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias

## Quick Facts
- arXiv ID: 2311.00217
- Source URL: https://arxiv.org/abs/2311.00217
- Reference count: 0
- Primary result: GPT-4 shows high accuracy in replicating voting behaviors but reduced fidelity in capturing nuanced climate change beliefs, especially when conditioned only on demographics.

## Executive Summary
This study evaluates whether large language models (LLMs) can accurately simulate public opinion on global warming by comparing synthetic responses to nationally representative survey data. Using GPT-3.5 and GPT-4, the researchers conditioned models on demographic information and issue-related covariates to generate synthetic survey responses. The findings reveal that GPT-4 performs better than GPT-3.5, particularly when provided with both demographics and covariates, though both models struggle with nuanced climate opinions compared to voting behaviors. Notably, certain subgroups like Black Americans were consistently underrepresented, highlighting algorithmic bias concerns. The study demonstrates that LLMs can be valuable tools for social science research when properly conditioned, but also underscores the need for careful evaluation of representation across demographic groups.

## Method Summary
The researchers used two nationally representative climate change surveys (N=1304 in 2017, N=1006 in 2021) from Ipsos KnowledgePanel, including demographic variables and issue-related covariates. They generated synthetic responses using GPT-3.5-turbo-16k and GPT-4 via OpenAI API with temperature=0.70, conditioning models on either demographics-only or demographics-plus-covariates using interview-style prompts. The synthetic data was manually corrected for hallucinations and evaluated against actual survey responses using accuracy, F1 scores, Macro-Average F1, Kullback-Leibler Divergence, and Cramer's V. The study assessed algorithmic fidelity and bias across multiple subgroups including race, gender, age, ideology, party, and education.

## Key Results
- GPT-4 demonstrates high accuracy in replicating voting behaviors (96-99% accuracy) but shows reduced fidelity for nuanced climate change beliefs.
- Including issue-related covariates significantly improves LLM performance compared to demographic-only conditioning.
- Both models consistently underestimated worry about global warming among Black Americans, indicating algorithmic bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can effectively simulate survey responses when conditioned on both demographics and issue-related covariates.
- Mechanism: Conditioning on additional covariates provides the model with relevant context that improves prediction of nuanced opinions beyond demographic patterns alone.
- Core assumption: Issue-related covariates are strongly associated with climate change beliefs and behaviors.
- Evidence anchors: GPT-4 exhibits improved performance when conditioned on both demographics and covariates; including relevant covariates improved performance, though certain subgroups were consistently underrepresented.
- Break condition: If covariates are not predictive of target opinions or the model lacks sufficient training data on these relationships, conditioning will not improve fidelity.

### Mechanism 2
- Claim: The choice of LLM model version (GPT-4 vs GPT-3.5) impacts algorithmic fidelity.
- Mechanism: More advanced models like GPT-4 have superior capabilities for understanding context and nuances, leading to better performance in capturing public opinion.
- Core assumption: GPT-4's enhanced training and alignment procedures give it an advantage in tasks requiring nuanced understanding of human beliefs.
- Evidence anchors: GPT-4 exhibits improved performance when conditioned on both demographics and covariates; GPT-4, when conditioned with demographics and covariates, provides more accurate predictions and displays response distributions more closely aligned with survey results.
- Break condition: If the task is simple (e.g., binary voting behavior), the difference between models may be minimal.

### Mechanism 3
- Claim: Survey question format affects LLM prediction accuracy.
- Mechanism: LLMs perform better with fewer, clearer response options; adding indeterminate options like "Don't Know" or "Refused" reduces accuracy.
- Core assumption: The model's training data and fine-tuning processes handle binary or ordinal choices better than open-ended or ambiguous responses.
- Evidence anchors: The introduction of an additional response option decreased the accuracy of both GPTs; inclusion of indeterminate answers reduces the algorithmic fidelity of LLMs more significantly.
- Break condition: If the survey design inherently requires many nuanced options, or if the target population frequently uses indeterminate responses, this limitation becomes critical.

## Foundational Learning

- Concept: Algorithmic fidelity in the context of LLMs
  - Why needed here: The study's core contribution is assessing how well LLMs can replicate human survey responses, which requires understanding what "fidelity" means in this context.
  - Quick check question: What is algorithmic fidelity and why is it important for using LLMs in social science research?

- Concept: Conditioning LLMs with demographic and covariate data
  - Why needed here: The study compares models conditioned only on demographics versus those also given issue-related covariates, showing this is a key methodological choice.
  - Quick check question: How does conditioning an LLM with additional covariates beyond demographics affect its predictions of survey responses?

- Concept: Evaluation metrics for LLM-generated survey data
  - Why needed here: The study uses accuracy, F1 scores, and distributional comparisons (KLD) to assess performance, which requires understanding these metrics.
  - Quick check question: Why might accuracy be misleading for imbalanced datasets, and what alternative metrics are more appropriate?

## Architecture Onboarding

- Component map: Survey data (ground truth) -> LLM models (GPT-3.5, GPT-4) -> Conditioning prompts (demographics-only vs demographics + covariates) -> Synthetic data generation (API) -> Evaluation metrics (accuracy, F1, KLD) -> Bias assessment (subpopulations)
- Critical path: Prepare conditioning data → generate prompts → call LLM API → collect synthetic responses → evaluate against survey data → assess bias
- Design tradeoffs: More complex models (GPT-4) and richer conditioning (demographics + covariates) improve fidelity but increase computational cost and may introduce new biases; simpler approaches are cheaper but less accurate
- Failure signatures: Poor algorithmic fidelity (low accuracy/F1), systematic underestimation of certain groups (algorithmic bias), or failure to generate certain response categories (e.g., "No" or "Don't Know")
- First 3 experiments:
  1. Replicate the voting behavior prediction with a small survey subset to verify basic fidelity
  2. Compare GPT-4 vs GPT-3.5 with identical conditioning to quantify model impact
  3. Test the effect of adding one covariate (e.g., issue involvement) to demographic-only conditioning on a single survey question

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering techniques (e.g., question order, number of target questions, formatting) affect algorithmic fidelity across different social science domains?
- Basis in paper: The authors note that while they focused on the number of answer options, other structural factors like question order or formatting may impact fidelity, and call for future research on this topic.
- Why unresolved: The paper only tested one aspect of prompt structure and did not systematically explore other prompt engineering techniques.
- What evidence would resolve it: Systematic experiments varying different prompt engineering techniques across multiple social science domains while measuring changes in algorithmic fidelity metrics.

### Open Question 2
- Question: What specific biases in training data or alignment processes lead to underrepresentation of Black Americans' views on climate change and voting behavior?
- Basis in paper: The authors found that GPT-4 consistently underestimated worry about global warming among Black Americans and struggled to accurately predict their voting behaviors, despite adequate sample sizes.
- Why unresolved: The paper identifies the bias but cannot determine whether it stems from training data composition, alignment procedures, or other factors in LLM development.
- What evidence would resolve it: Analysis of training data demographics and alignment procedures, combined with targeted experiments to isolate sources of bias against Black Americans.

### Open Question 3
- Question: How does conditioning on open-ended survey responses versus closed-ended responses affect algorithmic fidelity in capturing nuanced social attitudes?
- Basis in paper: The authors note their study focused on closed-ended questions and suggest that open-ended responses could provide richer insights and potentially enhance algorithmic fidelity.
- Why unresolved: The paper only tested conditioning on structured survey data and did not explore how unstructured open-ended responses might improve model performance.
- What evidence would resolve it: Comparative experiments conditioning LLMs on both structured and unstructured survey responses while measuring changes in fidelity across different types of social attitudes.

## Limitations

- The study relies on synthetic data generation via LLM APIs, introducing uncertainty about whether responses truly capture public opinion or reflect training data patterns.
- The manual hallucination correction process adds subjectivity that could affect reproducibility.
- The focus on climate change opinions in the US context limits generalizability to other topics or countries.

## Confidence

**High Confidence**: Claims about GPT-4's superior performance compared to GPT-3.5 when conditioned on both demographics and covariates are well-supported by multiple evaluation metrics.

**Medium Confidence**: The conclusion that demographic-only conditioning is insufficient for capturing nuanced climate opinions is supported but may be context-dependent.

**Low Confidence**: Claims about algorithmic bias against specific subgroups, particularly Black Americans, while concerning, require additional validation as the study cannot definitively explain its cause.

## Next Checks

1. **Subgroup-specific conditioning test**: Replicate the analysis with separate conditioning for each demographic subgroup to determine whether performance gaps stem from insufficient subgroup-specific training data or from the conditioning approach itself.

2. **Cross-topic validation**: Apply the same methodology to survey data on unrelated topics (e.g., economic policy or healthcare) to test whether the observed fidelity patterns generalize beyond climate change opinions.

3. **Real-time tracking comparison**: Generate synthetic responses monthly using the same prompts and compare against rolling survey data to assess whether LLM fidelity changes over time or in response to real-world events.