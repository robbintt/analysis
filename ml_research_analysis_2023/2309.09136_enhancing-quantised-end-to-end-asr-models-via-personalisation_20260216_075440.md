---
ver: rpa2
title: Enhancing Quantised End-to-End ASR Models via Personalisation
arxiv_id: '2309.09136'
source_url: https://arxiv.org/abs/2309.09136
tags:
- quantisation
- speaker
- training
- lora
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the PQM strategy to compensate for the performance
  loss due to model quantisation via personalisation. PQM adopted the NF4 quantisation
  approach together with LoRA-based speaker adaptive training and was applied to both
  the Conformer-based AED model and the Whisper model.
---

# Enhancing Quantised End-to-End ASR Models via Personalisation

## Quick Facts
- arXiv ID: 2309.09136
- Source URL: https://arxiv.org/abs/2309.09136
- Reference count: 0
- Key outcome: PQM strategy achieves 15.1% and 23.3% relative WER reductions on quantised Whisper and Conformer-based AED models respectively.

## Executive Summary
This paper proposes the PQM strategy to compensate for performance loss due to model quantisation through personalisation. The approach combines block-wise NormalFloat4 (NF4) quantisation with LoRA-based speaker adaptive training, applied to both Conformer-based AED and Whisper models. Experiments on LibriSpeech and TED-LIUM 3 datasets demonstrate that personalisation significantly improves quantised model performance, with PQM achieving substantial WER reductions compared to full precision models.

## Method Summary
PQM employs a three-stage strategy: (1) block-wise NF4 quantisation of model weights using asymmetric quantiles, (2) LoRA pretraining on domain data from multiple speakers, and (3) speaker adaptive training on target speaker data using LoRA parameters. The approach leverages LoRA's low-rank adaptation matrices to efficiently adapt quantised models to specific speakers, with optional semi-supervised training using self-generated labels. Experiments used LibriSpeech train-clean-100 for pretraining and speaker adaptation data from train-clean-360, along with TED-LIUM 3 datasets.

## Key Results
- 15.1% relative WER reduction on quantised Whisper models compared to full precision
- 23.3% relative WER reduction on quantised Conformer-based AED models
- Cost-effectiveness maximized with 20-30 utterances per speaker for adaptation
- PQM consistently outperformed baseline quantisation and other adaptation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantisation loss can be offset by speaker adaptation when the target domain is personalised.
- Mechanism: Personalisation narrows the mismatch between training and test conditions, so quantisation errors that hurt generic accuracy have less impact on the target speaker.
- Core assumption: The adaptation data is drawn from the same speaker distribution as the test data.
- Evidence anchors:
  - [abstract]: "improving performance for the target speaker is the critical objective rather than the generic performance"
  - [section 1]: "improving performance for the target speaker is the critical objective rather than the generic performance"
- Break condition: If the speaker-specific data is too limited or unrepresentative, overfitting may dominate and quantisation gains vanish.

### Mechanism 2
- Claim: Block-wise NF4 quantisation preserves more model fidelity than uniform quantisation by aligning quantisation bins with parameter distribution.
- Mechanism: Equal-population bins allocate finer resolution to dense regions of the weight distribution, reducing rounding errors.
- Core assumption: Conformer/Whisper weight matrices follow an approximately normal distribution.
- Evidence anchors:
  - [section 3.2]: "NF4, on the contrary, ensures each bin has an equal number of values by estimating the quantile of the input matrices using the empirical cumulative normal distribution"
  - [section 1]: "block-wise NormalFloat4 (NF4) quantisation [14] for model compression, which incurs a smaller performance loss compared to conventional uniform quantisation"
- Break condition: If weights are highly skewed or have heavy tails, the normality assumption breaks down and NF4 offers no advantage.

### Mechanism 3
- Claim: LoRA pretraining on similar speakers provides a better initialisation for rapid speaker adaptation with scarce data.
- Mechanism: Pretraining adapts LoRA matrices to the target domain, so subsequent speaker-specific updates need fewer steps to converge.
- Core assumption: Speakers in the pretraining set share acoustic-phonetic characteristics with the target speaker.
- Evidence anchors:
  - [section 3.3]: "PQM leverages those data to find a better initialisation point for LoRA weights before performing speaker adaptation, referred to as LoRA pretraining"
  - [section 5]: "The cost-effectiveness was maximized when 20 to 30 utterances were used for adaptive training with PQM"
- Break condition: If pretraining data is too dissimilar, adaptation will start from a poor point and gains may be minimal.

## Foundational Learning

- Concept: NormalFloat quantisation and quantile-based binning
  - Why needed here: It underpins the NF4 quantisation method that drives the 7x compression without catastrophic accuracy loss.
  - Quick check question: In NF4, how many quantiles are estimated for a k-bit quantisation?
    - Answer: 2^k + 1 quantiles.

- Concept: Low-rank adaptation (LoRA) in ASR
  - Why needed here: It enables efficient speaker adaptation without retraining full models, crucial for edge deployment.
  - Quick check question: What are the dimensions of the LoRA update matrices B and A?
    - Answer: B ∈ R^{d×r} and A ∈ R^{r×k}, where r ≪ min(d,k).

- Concept: Semi-supervised training with self-generated labels
  - Why needed here: It alleviates the limited target speaker data problem by exploiting the model's own predictions.
  - Quick check question: In PQM, what alternative label sources are explored besides ground truth?
    - Answer: Labels generated by Whisper-large and Whisper-base models.

## Architecture Onboarding

- Component map: Base ASR model (Conformer or Whisper) -> Block-wise NF4 quantisation -> Frozen weights -> LoRA adapter matrices (per speaker) -> Optional pretraining stage -> Speaker adaptation -> Optional semi-supervised fine-tuning
- Critical path: Base model -> Quantisation -> LoRA pretraining -> Speaker adaptation -> Evaluation
- Design tradeoffs:
  - Full quantisation vs partial: Full quantisation yields higher compression but higher risk of accuracy drop; PQM mitigates this via adaptation.
  - LoRA rank choice: Higher rank gives more adaptation capacity but increases storage; PQM uses rank=1 for minimal overhead.
  - Pretraining source: Cross-data pretraining is cheaper but less aligned; in-domain pretraining is more expensive but yields better initialisation.
- Failure signatures:
  - No improvement after adaptation: Pretraining data too different or adaptation data too scarce.
  - Performance worse than baseline: Overfitting on tiny speaker data or poor quantisation bin allocation.
  - Slow convergence: LoRA rank too low or pretraining not representative.
- First 3 experiments:
  1. Quantise Whisper base.en with block-wise NF4 and measure WER on LibriSpeech-SA without adaptation.
  2. Apply LoRA scratch (rank=1) on same quantised model with 20 utterances per speaker and measure improvement.
  3. Repeat (2) with LoRA pretraining on LibriSpeech train-clean-100 and compare convergence speed and final WER.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PQM scale with increasing amounts of speaker adaptation data?
- Basis in paper: [inferred] The paper mentions exploring the influence of the number of training utterances and found that the cost-effectiveness was maximized when 20 to 30 utterances were used, but did not explore scaling beyond this.
- Why unresolved: The paper only explored a limited range of training utterance numbers and did not investigate how performance scales with larger amounts of adaptation data.
- What evidence would resolve it: Experiments with varying amounts of speaker adaptation data, particularly beyond 30 utterances, to determine the point of diminishing returns or continued improvement.

### Open Question 2
- Question: How does PQM perform on languages other than English?
- Basis in paper: [inferred] The paper only evaluated PQM on English datasets (LibriSpeech and TED-LIUM 3) and did not explore its effectiveness on other languages.
- Why unresolved: The paper's experiments were limited to English datasets, so the generalizability of PQM to other languages is unknown.
- What evidence would resolve it: Experiments applying PQM to ASR models trained on non-English datasets and evaluating its performance on those languages.

### Open Question 3
- Question: How does the choice of LoRA rank affect the performance of PQM?
- Basis in paper: [explicit] The paper mentions using LoRA with rank=4 for training from scratch and rank=1 for pretrained LoRA, but does not explore the impact of different rank values.
- Why unresolved: The paper only used specific LoRA rank values without investigating how different ranks affect the performance of PQM.
- What evidence would resolve it: Experiments varying the LoRA rank parameter and evaluating the resulting performance of PQM across different rank values.

## Limitations

- Critical implementation parameters like block size for NF4 quantisation are not specified
- Speaker adaptation data partitioning strategy between pretraining and adaptation phases is not fully detailed
- Semi-supervised training methodology using self-generated labels lacks specific implementation details

## Confidence

**High Confidence Claims:**
- Personalisation through speaker adaptation can improve quantised model performance
- Block-wise NF4 quantisation incurs less performance loss than uniform quantisation
- LoRA-based adaptation is effective for speaker-specific fine-tuning
- The three-stage PQM approach (quantisation → pretraining → adaptation) is effective

**Medium Confidence Claims:**
- The specific WER improvements (15.1% and 23.3% relative reductions) are replicable with the described methodology
- The cost-effectiveness curve showing optimal adaptation with 20-30 utterances per speaker
- The superiority of PQM over other adaptation approaches like full fine-tuning or baseline quantisation

**Low Confidence Claims:**
- Generalisability to other ASR architectures beyond Conformer and Whisper
- Performance on speakers outside the LibriSpeech and TL3 domains
- Scalability to larger adaptation datasets or different languages

## Next Checks

1. **Block-wise NF4 Implementation Validation**: Implement the block-wise NF4 quantisation with different block sizes (8, 16, 32) and measure the impact on WER. This will help determine the sensitivity of the approach to this critical hyperparameter.

2. **Speaker Adaptation Data Sensitivity Analysis**: Systematically vary the amount of speaker adaptation data (5, 10, 20, 50, 100 utterances) and measure the WER improvements. This will validate the claimed cost-effectiveness at 20-30 utterances and identify the point of diminishing returns.

3. **Pretraining Data Domain Transfer Test**: Implement LoRA pretraining using out-of-domain data (e.g., Common Voice or TED-LIUM 1) and measure the impact on adaptation performance compared to in-domain pretraining. This will test the assumption that domain alignment in pretraining is crucial for adaptation success.