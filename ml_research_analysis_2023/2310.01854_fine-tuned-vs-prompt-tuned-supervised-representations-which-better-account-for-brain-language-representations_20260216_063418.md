---
ver: rpa2
title: 'Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account
  for Brain Language Representations?'
arxiv_id: '2310.01854'
source_url: https://arxiv.org/abs/2310.01854
tags:
- representations
- brain
- language
- decoding
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the efficacy of prompt-tuning versus fine-tuning
  for supervised representations in neural decoding, focusing on brain language representations.
  While fine-tuning updates entire model parameters and may distort pre-trained features,
  prompt-tuning preserves pre-trained weights and learns task-specific embeddings,
  potentially aligning better with the brain's multi-task learning ability.
---

# Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?

## Quick Facts
- arXiv ID: 2310.01854
- Source URL: https://arxiv.org/abs/2310.01854
- Reference count: 14
- Key outcome: Prompt-tuned BERT representations outperform or match fine-tuned ones for decoding brain language representations across functional networks and ROIs

## Executive Summary
This study investigates whether prompt-tuning or fine-tuning better aligns supervised representations with brain language processing. The authors tune a pre-trained BERT model on 10 diverse natural language understanding tasks using both methods and compare their performance in decoding brain activation patterns from fMRI data. They find that prompt-tuned representations consistently outperform or match fine-tuned ones across four functional brain networks and eight language network regions of interest. Tasks involving fine-grained concept meaning (e.g., Word Sense Disambiguation, Co-reference Resolution) yield better decoding performance than tasks requiring shallow syntactic processing (e.g., Syntactic Chunking), suggesting the brain encodes more semantic than syntactic information for language representation.

## Method Summary
The study compares neural decoding performance using fine-tuned and prompt-tuned BERT representations on fMRI brain activity data. The authors fine-tune and prompt-tune BERT-base-cased on 10 NLU tasks (Emotion Categorization, Sentiment Classification, Question Answering, Semantic Role Labeling, Word Sense Disambiguation, Co-reference Resolution, Named Entity Recognition, Natural Language Inference, Syntactic Chunking, Paraphrase Detection). They generate sentence embeddings from these tuned models and build neural decoders using 5-fold cross-validation to predict brain activity. The decoding accuracy is evaluated on 4 functional networks and 8 language ROIs using pairwise matching accuracy and MSE loss metrics.

## Key Results
- Prompt-tuned representations consistently outperform or match fine-tuned ones across four functional brain networks and eight language ROIs
- Semantic tasks (Word Sense Disambiguation, Co-reference Resolution) yield better decoding performance than syntactic tasks (Syntactic Chunking)
- Prompt-tuned representations maintain stronger correlations with the original pre-trained model compared to fine-tuned representations

## Why This Works (Mechanism)

### Mechanism 1
Prompt-tuning preserves general domain knowledge better than fine-tuning by freezing pre-trained weights and learning only task-specific embeddings, avoiding distortion of original semantic representations learned during pre-training.

### Mechanism 2
The brain encodes more semantic than syntactic information for language representation, as evidenced by tasks involving fine-grained concept meaning (WSD, Co-reference) producing better brain decoding performance than syntactic tasks (Chunking).

### Mechanism 3
Representational similarity to the original pre-trained model predicts neural decoding performance, with prompt-tuned representations maintaining higher correlation with the original BERT model and this correlation explaining variance in decoding accuracy.

## Foundational Learning

- Concept: Functional magnetic resonance imaging (fMRI) basics
  - Why needed here: The study uses fMRI data to measure brain activity during language processing
  - Quick check question: What is the temporal and spatial resolution of fMRI, and how does this affect the study's ability to capture language processing dynamics?

- Concept: Natural language understanding (NLU) task taxonomy
  - Why needed here: The study selects 10 diverse NLU tasks to investigate which types better align with brain representations
  - Quick check question: How do tasks requiring semantic reasoning differ from those requiring syntactic processing in terms of cognitive demands and neural correlates?

- Concept: Representation similarity analysis
  - Why needed here: The study compares similarity between different supervised representations and the original pre-trained model
  - Quick check question: What metrics are commonly used to measure representational similarity between neural network models, and what do they capture?

## Architecture Onboarding

- Component map: Pre-trained BERT → Task tuning (fine-tuning/prompt-tuning) → Sentence embeddings → Neural decoder → Brain activity prediction
- Critical path: Brain data collection → Model tuning → Embedding generation → Decoding accuracy evaluation
- Design tradeoffs: Fine-tuning offers potentially better task performance but risks distorting pre-trained features; prompt-tuning preserves features but may have lower task performance
- Failure signatures: Poor decoding accuracy across all tasks, significant difference between fine-tuning and prompt-tuning performance, or task-tuned representations showing negative correlation with original model
- First 3 experiments:
  1. Compare decoding accuracy of untuned BERT vs fine-tuned vs prompt-tuned representations on a single ROI
  2. Evaluate representational similarity between tuned models and original BERT across all tasks
  3. Test whether task performance correlates with decoding accuracy across the 10 NLU tasks

## Open Questions the Paper Calls Out

### Open Question 1
Which specific tasks, beyond the 10 NLU tasks studied, might produce supervised representations that better decode brain language representations? The authors note that their findings suggest the need for further studies to build benchmarks that assess the cognitive and biological plausibility of task-supervised models, implying that the current selection of tasks may not be optimal for this purpose.

### Open Question 2
How do prompt-tuning and fine-tuning methods affect the preservation of pre-trained features in different types of neural network architectures beyond BERT? The study focuses on BERT, but the implications for other architectures are not discussed, leaving a gap in understanding the generalizability of the findings.

### Open Question 3
What are the long-term effects of prompt-tuning on model performance and brain representation alignment in continual learning scenarios? The authors mention the brain's robust multi-task learning ability and the cognitive inconsistency of fine-tuning, suggesting that prompt-tuning might better align with brain mechanisms, but do not explore long-term effects.

## Limitations

- The fMRI dataset comprises only 5 subjects, limiting statistical power and generalizability
- The study focuses exclusively on English language data and BERT-based models, potentially restricting applicability to other languages or model architectures
- The paper demonstrates that prompt-tuning outperforms fine-tuning for neural decoding but does not establish causal mechanisms explaining why this occurs

## Confidence

- High confidence: The finding that semantic tasks (WSD, Co-reference) consistently yield better decoding performance than syntactic tasks (Chunking) across both fine-tuning and prompt-tuning methods
- Medium confidence: The claim that prompt-tuned representations maintain stronger correlations with the original pre-trained model compared to fine-tuned representations
- Low confidence: The assertion that prompt-tuning is universally superior to fine-tuning for neural decoding applications without qualification

## Next Checks

1. Replicate the study with a larger, more diverse fMRI dataset (minimum 20 subjects) to assess statistical robustness and individual variability
2. Test alternative pre-trained models (e.g., RoBERTa, GPT-2) and languages to evaluate whether the prompt-tuning advantage generalizes beyond BERT-base-cased English
3. Conduct ablation studies isolating the effects of prompt length, prefix diversity, and freezing strategies to identify which aspects of prompt-tuning drive the observed performance differences