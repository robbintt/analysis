---
ver: rpa2
title: Sparse Autoencoders Find Highly Interpretable Features in Language Models
arxiv_id: '2309.08600'
source_url: https://arxiv.org/abs/2309.08600
tags:
- features
- dictionary
- feature
- figure
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces sparse autoencoders as a method to find highly
  interpretable features in language models. The authors hypothesize that polysemanticity
  in neural networks arises from superposition, where models represent more features
  than neurons by assigning features to non-orthogonal directions in activation space.
---

# Sparse Autoencoders Find Highly Interpretable Features in Language Models

## Quick Facts
- **arXiv ID**: 2309.08600
- **Source URL**: https://arxiv.org/abs/2309.08600
- **Reference count**: 20
- **Primary result**: Sparse autoencoders can find highly interpretable, monosemantic features in language models by resolving polysemanticity through sparse reconstruction of internal activations

## Executive Summary
This paper introduces sparse autoencoders as a method to find highly interpretable features in language models. The authors hypothesize that polysemanticity in neural networks arises from superposition, where models represent more features than neurons by assigning features to non-orthogonal directions in activation space. To identify these directions, they train sparse autoencoders to reconstruct internal activations of a language model, learning a set of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches. The interpretability is measured by automated methods. The authors show that their learned features enable less-disruptive model editing and allow pinpointing causally responsible features for specific tasks more precisely than other decompositions. They also demonstrate that individual dictionary features are monosemantic and have predictable effects on model outputs.

## Method Summary
The method trains sparse autoencoders on internal activations of language models to learn interpretable, monosemantic features. The autoencoder architecture uses tied weights with ReLU activation and an ℓ1 sparsity penalty on the hidden layer. The learned dictionary features are evaluated using automated interpretability scoring and validated through concept erasure experiments that demonstrate their functional correspondence to specific capabilities in the model.

## Key Results
- Sparse autoencoder features achieve significantly higher autointerpretability scores compared to PCA and ICA baselines
- Individual dictionary features are monosemantic and correspond to specific concepts like gender, syntax, and semantic roles
- Feature ablation enables precise model editing with less disruption than baseline techniques
- Learned features can pinpoint causally responsible components for specific tasks more accurately than alternative decompositions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Polysemanticity arises from superposition, where neural networks represent more features than neurons by assigning features to non-orthogonal directions in activation space.
- **Mechanism**: Sparse autoencoders identify these directions by learning a set of sparsely activating features that can reconstruct internal activations through sparse linear combinations.
- **Core assumption**: Features must be sufficiently sparsely activating for superposition to arise because interference between non-orthogonal features prevents performance gains without high sparsity.
- **Evidence anchors**:
  - [abstract] "One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space"
  - [section 1] "Elhage et al. (2022) investigate why polysemanticity might arise and hypothesise that it may result from models learning more distinct features than there are dimensions in the layer"
- **Break condition**: If reconstruction loss remains high despite training, the model may not be using superposition or the sparsity constraint is too strong.

### Mechanism 2
- **Claim**: Sparse autoencoders learn dictionary features that are more interpretable and monosemantic than directions identified by alternative approaches.
- **Mechanism**: The ℓ1 sparsity penalty on hidden activations encourages reconstruction using sparse linear combinations of dictionary features, recovering ground-truth features that generated the data.
- **Core assumption**: Reconstruction with an ℓ1 penalty can recover the ground-truth features that generated the data.
- **Evidence anchors**:
  - [section 2] "It can be shown empirically (Sharkey et al., 2023) and theoretically (Wright & Ma, 2022) that reconstruction with an ℓ1 penalty can recover the ground-truth features that generated the data"
  - [section 3.2] "Figure 2 shows that our dictionary features are far more interpretable by this measure than dictionary features found by comparable techniques"
- **Break condition**: If autointerpretability scores don't improve over baselines, the learned features may not be capturing meaningful directions.

### Mechanism 3
- **Claim**: Learned dictionary features enable precise model editing and allow pinpointing causally responsible features for specific tasks more precisely than other decompositions.
- **Mechanism**: By ablating individual dictionary features, the model can be edited to remove specific capabilities while disrupting behavior less than prior techniques.
- **Core assumption**: Dictionary features correspond to functional units in the network that can be independently modified.
- **Evidence anchors**:
  - [abstract] "We also show that our features enable less-disruptive model editing and allow pinpointing causally responsible features for specific tasks more precisely than other decompositions"
  - [section 4] "To demonstrate that our dictionary features correspond to functional units in the network, we turn to concept erasure"
- **Break condition**: If ablation doesn't produce the expected behavioral changes, features may not be functionally independent or the task may require multiple features.

## Foundational Learning

- **Concept**: Sparse dictionary learning
  - **Why needed here**: The method relies on finding sparse linear combinations of basis vectors to reconstruct activations
  - **Quick check question**: What is the difference between ℓ1 and ℓ2 regularization in the context of dictionary learning?

- **Concept**: Autoencoder architecture
  - **Why needed here**: The method uses tied-weight autoencoders with ReLU activation to learn feature dictionaries
  - **Quick check question**: Why are tied weights used in the autoencoder architecture described in the paper?

- **Concept**: Interpretability measurement
  - **Why needed here**: The method uses autointerpretability scores to evaluate whether learned features are meaningful
  - **Quick check question**: How does the autointerpretability process work to generate descriptions and scores for features?

## Architecture Onboarding

- **Component map**: Sample activations from language model → Train sparse autoencoder → Interpret learned features → Validate through concept erasure
- **Critical path**: Sample activations → Train autoencoder → Interpret features → Validate with concept erasure and circuit discovery
- **Design tradeoffs**: Larger dictionaries improve interpretability but increase computational cost; stronger sparsity penalties improve interpretability but may harm reconstruction accuracy
- **Failure signatures**: High reconstruction loss indicates insufficient feature capture; low interpretability scores suggest learned directions aren't meaningful; dead features in MLP layers indicate architecture issues
- **First 3 experiments**:
  1. Train sparse autoencoder on residual stream activations with different α values to observe sparsity-reconstruction tradeoff
  2. Compare autointerpretability scores of learned features against PCA and ICA baselines
  3. Perform concept erasure on gender prediction task to validate functional correspondence of features

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does the interpretability of sparse autoencoder features decline in later layers of the model?
  - **Basis in paper**: [explicit] "This could indicate that sparse autoencoders work less well in later layers but also may be connected to the difficulties of automatic interpretation, both because by building on earlier layers, later features may be more complex, and because they are often best explained by their effect on the output."
  - **Why unresolved**: The authors suggest potential reasons but do not conclusively determine the cause. It could be due to the limitations of the autointerpretation method, the complexity of features in later layers, or the effectiveness of sparse autoencoders in those layers.
  - **What evidence would resolve it**: Conducting experiments that specifically test the interpretability of features in later layers using different methods, or analyzing the structure and complexity of features across layers.

- **Open Question 2**: How can the issue of dead features in MLP layers be addressed to ensure learning of a truly overcomplete basis?
  - **Basis in paper**: [explicit] "Our approach still finds many features that are more interpretable than the neurons. However, our architecture also learns many dead features, which never activate across the entire corpus."
  - **Why unresolved**: The authors mention using a different matrix for the encoder and decoder as a partial solution but acknowledge that this does not fully resolve the issue.
  - **What evidence would resolve it**: Developing and testing new methods or architectures that consistently produce active features in MLP layers, or analyzing the conditions under which dead features occur and finding ways to prevent them.

- **Open Question 3**: Can the sparse autoencoder method be extended to better understand computations in attention heads and MLP layers?
  - **Basis in paper**: [explicit] "Our current methods for training sparse autoencoders are best suited to the residual stream, though there is evidence that they may be applicable to the MLPs. We would be excited about future work investigating what changes can be made to better understand the computations performed by the attention heads and MLP layers, each of which poses different challenges."
  - **Why unresolved**: The authors acknowledge the potential but have not yet developed methods to effectively apply sparse autoencoders to attention heads and MLP layers.
  - **What evidence would resolve it**: Successfully applying sparse autoencoders to attention heads and MLP layers, demonstrating improved interpretability and understanding of the computations in those layers.

## Limitations

- **Hypothesis Dependence**: The core mechanism assumes that polysemanticity arises from superposition in overcomplete representations. If this assumption is incorrect, the entire approach may be capturing spurious correlations rather than genuine features.
- **Interpretability Measurement**: While autointerpretability scores show learned features are more interpretable than baselines, the method relies on automated description generation rather than human verification.
- **Feature Independence Assumption**: Concept erasure assumes features can be independently modified without affecting other capabilities, but this may not hold for complex multi-feature tasks.

## Confidence

- **High Confidence**: Reconstruction accuracy of sparse autoencoders and basic architecture functionality
- **Medium Confidence**: Autointerpretability scores showing learned features are more interpretable than baselines
- **Low-Medium Confidence**: Causal claims about feature functionality and independence in model editing

## Next Checks

1. **Ground Truth Validation**: Test the method on synthetic datasets where ground-truth features are known to verify that sparse autoencoders can recover the generating features under controlled conditions.

2. **Human Interpretability Study**: Conduct a human evaluation study comparing feature descriptions generated by autointerpretability against human-annotated descriptions to validate the correlation between automated and genuine interpretability.

3. **Feature Interaction Analysis**: Systematically test multi-feature ablations to understand feature dependencies and validate whether features truly operate as independent functional units in the network.