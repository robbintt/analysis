---
ver: rpa2
title: Towards the Law of Capacity Gap in Distilling Language Models
arxiv_id: '2311.07052'
source_url: https://arxiv.org/abs/2311.07052
tags:
- mini
- language
- teacher
- distillation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the "curse of capacity gap" in language model
  distillation, where larger teacher models don't always yield better student performance
  due to the trade-off between teacher capability and capacity mismatch. The authors
  propose that instead of trying to lift this curse, we should identify an optimal
  teacher scale for each student scale, turning the curse into a law.
---

# Towards the Law of Capacity Gap in Distilling Language Models

## Quick Facts
- **arXiv ID**: 2311.07052
- **Source URL**: https://arxiv.org/abs/2311.07052
- **Reference count**: 40
- **Key outcome**: Proposes optimal teacher-to-student scale ratio of 2.5×, distilled 3B MINIMA from 7B teacher achieving SoTA on multiple benchmarks

## Executive Summary
This paper addresses the "curse of capacity gap" in language model distillation, where larger teacher models don't always yield better student performance due to the trade-off between teacher capability and capacity mismatch. The authors propose that instead of trying to lift this curse, we should identify an optimal teacher scale for each student scale, turning the curse into a law. Through preliminary experiments, they discover that the optimal teacher scale is consistently 2.5× the student scale across different model architectures and data scales. Based on this finding, they successfully distill a 3B language model (MINIMA) from a 7B teacher, which achieves state-of-the-art performance among 3B models on multiple benchmarks including MMLU (28.51% accuracy), CEval (28.23% accuracy), and HumanEval (10.98% pass@1). Further instruction-tuning produces MINI CHAT, which outperforms scale-matched 3B models and even competes with 7B models in GPT4 evaluations.

## Method Summary
The authors develop a pruning-based distillation approach where they first prune a larger teacher model (7B LLaMA2) to create a student model (3B), then perform knowledge distillation from the full teacher to the pruned student. The pruning uses expressive scores calculated from accumulated absolute gradients to identify important parameters. The key innovation is identifying the optimal teacher-to-student scale ratio of 2.5× through preliminary experiments across different model architectures and data scales. The method includes symmetric shaping with global and local pruning priorities, and the resulting 3B model (MINIMA) is further instruction-tuned to produce MINI CHAT.

## Key Results
- Discovered optimal teacher-to-student scale ratio of 2.5× across different architectures and data scales
- Distilled 3B MINIMA from 7B teacher achieving state-of-the-art performance among 3B models (MMLU 28.51%, CEval 28.23%, HumanEval 10.98%)
- MINI CHAT (instruction-tuned version) outperforms other 3B models and competes with 7B models in GPT4 evaluations
- Successfully distilled 3B model with Chinese vocabulary expansion from adapted LLaMA2-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There is a stable optimal teacher-to-student scale ratio (~2.5×) across different model sizes and architectures.
- Mechanism: When the teacher is too large relative to the student, the capacity gap overwhelms the student's ability to mimic the teacher, leading to performance degradation. Conversely, a smaller teacher within the optimal ratio allows the student to learn effectively without being overwhelmed.
- Core assumption: The optimal capacity gap is determined by architectural similarities and training dynamics that remain constant across scales.
- Evidence anchors:
  - [abstract] "the optimal teacher scale is consistently 2.5x the student scale across different model architectures and data scales."
  - [section] "we find that: while the performance always improves along the increment of student scale when the teacher scale is fixed... the performance can firstly improve secondly hang still finally deteriorate along the increment of teacher scale when the student is fixed"
  - [corpus] Weak or missing: No direct empirical data comparing capacity gaps across varying architectures in the corpus.
- Break condition: If architectural differences significantly alter the learning dynamics, the 2.5× ratio may no longer hold.

### Mechanism 2
- Claim: Pruning with expressive score prioritizes important parameters, enabling efficient student model creation.
- Mechanism: By accumulating absolute gradients, the pruning process identifies and preserves parameters that contribute most to the pretraining loss, allowing for effective model compression without significant performance loss.
- Core assumption: Gradient magnitude is a reliable indicator of parameter importance in transformer-based models.
- Evidence anchors:
  - [section] "Inspired by structured pruning... we attach a set of variables ξ, ν per layer to the record the parameter expressive scores through accumulated absolute gradients"
  - [section] "The j-th head among A heads is parameterized by... The j-th neuron among I neurons is parameterized by... The pruning procedure is scheduled as follows: 1) decomposing the target sparsity..."
  - [corpus] Weak or missing: No direct comparison of pruning effectiveness against other pruning methods in the corpus.
- Break condition: If gradient accumulation does not correlate with parameter importance in certain model architectures or training regimes.

### Mechanism 3
- Claim: Distillation loss optimization is more challenging than direct training loss optimization, affecting convergence.
- Mechanism: Distillation involves aligning the student's output distribution with the teacher's, which is inherently more complex than matching one-hot labels, leading to potentially higher training loss but better generalization.
- Core assumption: The complexity of optimizing distillation loss is offset by the quality of knowledge transfer from the teacher.
- Evidence anchors:
  - [section] "we conjecture that distillation loss (alignment to distributed probabilities) is inherently more difficult to optimize than direct training loss (alignment to one-hot labels)"
  - [section] "Direct training has a slightly lower training loss than distillation from LLaMA2-13B, and MINI MA actually owns a slightly higher training loss than distillation from LLaMA2-13B does"
  - [corpus] Weak or missing: No empirical data comparing distillation loss optimization complexity across different model scales in the corpus.
- Break condition: If the teacher model's output distribution is too similar to one-hot labels, the complexity advantage of direct training may disappear.

## Foundational Learning

- Concept: Capacity gap in model distillation
  - Why needed here: Understanding the capacity gap is crucial for grasping why larger teachers don't always yield better students and how the 2.5× ratio was derived.
  - Quick check question: What happens to student performance when the teacher is significantly larger than the optimal size?

- Concept: Model pruning with expressive scores
  - Why needed here: Pruning is used to create the student model from the teacher, and understanding this process is key to replicating the results.
  - Quick check question: How does accumulating absolute gradients help in identifying important model parameters for pruning?

- Concept: Knowledge distillation techniques
  - Why needed here: Distillation is the core method used to transfer knowledge from the teacher to the student, and understanding its nuances is essential.
  - Quick check question: Why might distillation loss optimization be more challenging than direct training loss optimization?

## Architecture Onboarding

- Component map: Teacher model (LLaMA2-7B) -> Pruning process (expressive score calculation) -> Student model (3B) -> Knowledge distillation -> MINIMA model -> Instruction-tuning -> MINI CHAT
- Critical path: Pruning the teacher to create the student, followed by knowledge distillation from full teacher to pruned student
- Design tradeoffs: The tradeoff between model size and performance is central. A larger teacher may provide more knowledge but can overwhelm the student. Pruning reduces model size but must preserve essential parameters.
- Failure signatures: Poor student performance may indicate an inappropriate teacher-to-student scale ratio or ineffective pruning. High training loss during distillation may suggest difficulties in aligning distributions.
- First 3 experiments:
  1. Verify the 2.5× teacher-to-student scale ratio by distilling models with varying teacher sizes.
  2. Test the effectiveness of pruning with expressive scores by comparing pruned models with randomly pruned ones.
  3. Assess the impact of distillation loss optimization by comparing distilled models with directly trained ones of similar size.

## Open Questions the Paper Calls Out

- Does the optimal capacity gap (teacher scale 2.5× student scale) hold when scaling to much larger models (e.g., 10B, 30B, 70B students)?
- How does the optimal capacity gap change with different training data scales and distributions?
- Would a preference optimization stage after distillation further improve MINI MA's performance, or does the 2.5× capacity gap law make this unnecessary?
- Does the optimal capacity gap law apply equally to encoder-decoder models as it does to decoder-only models?

## Limitations

- The 2.5× optimal scale ratio was only validated on LLaMA2 models and may not generalize to other architectures.
- The pruning methodology lacks direct comparison with established pruning methods to quantify its specific benefits.
- Claims about distillation loss optimization complexity are not empirically validated with quantitative comparisons.
- The paper does not explore preference optimization after distillation, which could potentially improve performance.

## Confidence

- **High Confidence**: The empirical results showing MINIMA's state-of-the-art performance among 3B models on multiple benchmarks (MMLU, CEval, HumanEval) are well-supported by the reported metrics and comparison with baseline models.
- **Medium Confidence**: The discovery of the 2.5× optimal scale ratio is supported by preliminary experiments, but the limited architectural diversity in the experiments reduces confidence in its universal applicability.
- **Low Confidence**: The claims about the inherent difficulty of distillation loss optimization and the superiority of the gradient-based pruning method lack sufficient empirical validation to support strong confidence.

## Next Checks

1. Replicate the optimal scale ratio experiments with different model architectures (e.g., GPT, Mistral) to validate whether the 2.5× ratio holds across architectural families, not just LLaMA variants.

2. Implement and compare the gradient-based pruning approach against established methods like magnitude pruning and movement pruning on the same model distillation task to quantify the specific benefits claimed.

3. Conduct a detailed analysis comparing training dynamics, convergence rates, and final loss values between distilled and directly trained models of equivalent size to empirically validate the claimed optimization complexity differences.