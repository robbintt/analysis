---
ver: rpa2
title: Revisiting Distillation for Continual Learning on Visual Question Localized-Answering
  in Robotic Surgery
arxiv_id: '2307.12045'
source_url: https://arxiv.org/abs/2307.12045
tags:
- classes
- learning
- surgical
- distillation
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for visual question localized-answering (VQLA) in robotic surgery. The proposed
  CS-VQLA framework introduces rigidity-plasticity-aware distillation (RP-Dist) and
  self-calibrated heterogeneous distillation (SH-Dist) to preserve old knowledge while
  learning new tasks.
---

# Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery

## Quick Facts
- arXiv ID: 2307.12045
- Source URL: https://arxiv.org/abs/2307.12045
- Reference count: 40
- Key outcome: CS-VQLA achieves 2.60% accuracy improvement and 0.44 mIoU higher localization accuracy compared to state-of-the-art methods

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for visual question localized-answering (VQLA) in robotic surgery. The proposed CS-VQLA framework introduces rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve old knowledge while learning new tasks. The method also integrates weight aligning (WA) to adjust model bias between old and new tasks. Experiments on three surgical datasets show the proposed method achieves superior performance, with 2.60% accuracy improvement and 0.44 mIoU higher localization accuracy compared to state-of-the-art methods. The framework effectively mitigates catastrophic forgetting in both overlapping and non-overlapping class scenarios, demonstrating strong potential for real-world surgical education applications.

## Method Summary
The CS-VQLA framework builds on a VisualBERT backbone that fuses image features from ResNet18 with question embeddings from BERT. The model outputs both classification answers and bounding box localizations through parallel heads. During continual learning, the framework applies rigidity-plasticity-aware distillation (RP-Dist) with different temperature parameters for overlapping versus non-overlapping classes, self-calibrated heterogeneous distillation (SH-Dist) to preserve intermediate feature maps, and weight aligning (WA) to prevent classifier bias toward new classes. The method is evaluated sequentially on three surgical datasets (EndoVis18, EndoVis17, M2CAI) with varying degrees of class overlap.

## Key Results
- CS-VQLA achieves 2.60% accuracy improvement and 0.44 mIoU higher localization accuracy compared to state-of-the-art methods
- The framework effectively mitigates catastrophic forgetting in both overlapping and non-overlapping class scenarios
- Performance gains are consistent across three different surgical datasets with varying class overlap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rigidity-plasticity-aware distillation (RP-Dist) mitigates catastrophic forgetting by differentially weighting old non-overlapping versus overlapping classes during knowledge distillation
- Mechanism: RP-Dist applies separate temperature parameters (Top > Ton) in the Kullback-Leibler divergence loss for overlapping (Cop) and non-overlapping (Con) classes. Higher Top increases plasticity for overlapping classes while lower Ton maintains rigidity for non-overlapping classes, preventing dominance of overlapping classes in model predictions
- Core assumption: Surgical domains have meaningful class overlap between sequential tasks, and treating all classes equally during distillation leads to performance degradation on non-overlapping classes
- Evidence anchors:
  - [abstract] "We revisit the distillation method for CL, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge."
  - [section 2.2] "Therefore, we treat Top and Ton differently to strengthen the plasticity on Cop, and keep the rigidity on Con respectively."
  - [corpus] Weak - related works focus on general continual learning but don't specifically address overlapping class scenarios in surgical VQLA

### Mechanism 2
- Claim: Self-calibrated heterogeneous distillation (SH-Dist) preserves knowledge in intermediate feature maps without additional learnable parameters
- Mechanism: SH-Dist applies a self-calibration operation to the heterogeneous feature map output from VisualBERT, creating F't from Ft. This operation adds adaptively modeled long-range context information before computing L2 loss between F't and the old model's feature map Ft-1
- Core assumption: Intermediate feature representations contain crucial information for task performance that standard output logits distillation alone cannot capture
- Evidence anchors:
  - [abstract] "We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge."
  - [section 2.2] "We perform a self-calibration operation on the heterogeneous output features Ft from the VisualBERT backbone to get self-calibrated feature F't."
  - [corpus] Weak - while feature distillation is mentioned in related work, the specific self-calibration approach is novel to this paper

### Mechanism 3
- Claim: Weight aligning (WA) prevents classifier bias toward new classes by normalizing weight magnitudes across old and new class weights
- Mechanism: WA scales new class weights (Wnew) by the ratio of mean normalized old class weights (Wold) to mean normalized new class weights, preventing the model from being dominated by newly learned classes
- Core assumption: In class-incremental learning scenarios, models naturally develop bias toward recently learned classes without explicit correction
- Evidence anchors:
  - [abstract] "The weight aligning (WA) technique is also integrated to adjust the weight bias between old and new tasks."
  - [section 2.1] "WA [29] is a simple technique to align the weight bias in the classifier layer."
  - [corpus] Moderate - WA is mentioned in a cited reference [29] but the specific implementation details are paper-specific

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's entire motivation centers on preventing catastrophic forgetting when learning new surgical tasks while maintaining performance on old ones
  - Quick check question: What happens to a neural network's performance on old tasks when it's trained on new data without any continual learning techniques?

- Concept: Knowledge distillation
  - Why needed here: Both RP-Dist and SH-Dist are distillation-based approaches that transfer knowledge from old models to new ones
  - Quick check question: How does knowledge distillation help in preserving information from a previously trained model when training a new model on related tasks?

- Concept: Class-incremental learning
  - Why needed here: The paper addresses the specific scenario where new classes are introduced sequentially while old classes may overlap with new ones
  - Quick check question: What distinguishes class-incremental learning from task-incremental learning in continual learning scenarios?

## Architecture Onboarding

- Component map: VisualBERT backbone → Parallel classifier and detector heads → RP-Dist (output logits) → SH-Dist (intermediate features) → WA (classifier weights)
- Critical path: Image + Question → Feature extraction → Multi-task prediction → Distillation loss application → Weight alignment
- Design tradeoffs: The model balances answering and localization tasks with a μ=100 weighting, while distillation losses use α=β=1 and γ=5. These hyperparameters control the trade-off between learning new information and preserving old knowledge
- Failure signatures: 
  - Accuracy/mIoU drops on old non-overlapping classes indicate insufficient rigidity
  - Accuracy/mIoU drops on overlapping classes indicate excessive rigidity
  - Classifier bias toward new classes suggests WA is ineffective
- First 3 experiments:
  1. Baseline FT (fine-tuning only) to establish catastrophic forgetting baseline
  2. LwF with standard distillation to compare against existing method
  3. Full CS-VQLA with all components enabled to demonstrate cumulative benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CS-VQLA framework perform in a more complex surgical scenario with a larger number of overlapping and non-overlapping classes?
- Basis in paper: [inferred] The paper mentions that the framework is tested on three public surgical datasets with overlapping classes, but the performance is not evaluated on a more complex scenario with a larger number of classes
- Why unresolved: The current experimental setup may not be sufficient to evaluate the scalability and robustness of the proposed framework in real-world surgical applications
- What evidence would resolve it: Testing the framework on a larger dataset with a higher number of overlapping and non-overlapping classes and comparing its performance with other state-of-the-art methods

### Open Question 2
- Question: How does the proposed CS-VQLA framework handle domain shifts between different surgical procedures or institutions?
- Basis in paper: [inferred] The paper mentions that the framework is tested on three public surgical datasets, but it does not address the issue of domain shifts between different surgical procedures or institutions
- Why unresolved: The performance of the framework may degrade when applied to surgical data from different sources or procedures due to domain shifts
- What evidence would resolve it: Testing the framework on surgical data from different sources or procedures and evaluating its performance in terms of accuracy and localization

### Open Question 3
- Question: How does the proposed CS-VQLA framework handle missing or noisy data in the input question and image pairs?
- Basis in paper: [inferred] The paper does not address the issue of handling missing or noisy data in the input question and image pairs
- Why unresolved: In real-world applications, the input data may contain missing or noisy information, which can affect the performance of the framework
- What evidence would resolve it: Testing the framework on input data with missing or noisy information and evaluating its performance in terms of accuracy and localization

## Limitations

- Evaluation relies on three surgical datasets with limited sample sizes, which may not generalize to broader surgical scenarios
- Specific hyperparameter choices lack sensitivity analysis, raising questions about robustness to different settings
- The self-calibration operation in SH-Dist lacks detailed implementation specifications, making exact reproduction challenging

## Confidence

- **High confidence**: The fundamental approach of using distillation for continual learning in surgical VQLA is sound and addresses a well-documented problem (catastrophic forgetting)
- **Medium confidence**: The specific mechanisms (RP-Dist, SH-Dist, WA) are logically constructed but rely on assumptions about surgical domain characteristics that weren't extensively validated
- **Low confidence**: The generalizability of results to real-world surgical settings with different equipment, patient populations, or surgical procedures

## Next Checks

1. Conduct ablation studies systematically varying μ, α, β, γ, and Top/Ton parameters to assess sensitivity and identify optimal configurations

2. Test the framework on additional surgical datasets or cross-domain medical imaging datasets to evaluate generalizability beyond the three source datasets

3. Implement and validate the exact self-calibration operation specification through code review or communication with authors to ensure faithful reproduction