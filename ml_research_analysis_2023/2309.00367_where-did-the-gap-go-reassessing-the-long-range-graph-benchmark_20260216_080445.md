---
ver: rpa2
title: Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark
arxiv_id: '2309.00367'
source_url: https://arxiv.org/abs/2309.00367
tags:
- graph
- learning
- gatedgcn
- datasets
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reevaluates the Long-Range Graph Benchmark (LRGB) by
  demonstrating that previously reported performance gaps between Graph Transformers
  and Message Passing GNNs were overestimated due to suboptimal hyperparameter choices.
  Through rigorous hyperparameter optimization within a 500k parameter budget, the
  authors show that GCN, GINE, and GatedGCN baselines achieve competitive or superior
  performance to GPS on Peptides-Func and Peptides-Struct tasks.
---

# Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark

## Quick Facts
- arXiv ID: 2309.00367
- Source URL: https://arxiv.org/abs/2309.00367
- Reference count: 36
- Key outcome: Suboptimal hyperparameter choices artificially inflated performance gaps between Graph Transformers and MPGNNs on the Long-Range Graph Benchmark

## Executive Summary
This paper reevaluates the Long-Range Graph Benchmark (LRGB) by demonstrating that previously reported performance gaps between Graph Transformers and Message Passing GNNs were overestimated due to suboptimal hyperparameter choices. Through rigorous hyperparameter optimization within a 500k parameter budget, the authors show that GCN, GINE, and GatedGCN baselines achieve competitive or superior performance to GPS on Peptides-Func and Peptides-Struct tasks. On Peptides-Struct specifically, a basic GCN model achieves state-of-the-art results by using a deeper prediction head instead of a linear one. The authors also identify two additional issues: the lack of feature normalization on vision datasets (PascalVOC-SP and COCO-SP) which significantly impacts performance, and an incorrect implementation of the link prediction metric on PCQM-Contact that uses raw instead of filtered MRR. The findings establish that careful hyperparameter tuning and proper implementation details are crucial for fair comparison of graph learning methods.

## Method Summary
The paper conducts a systematic re-evaluation of the Long-Range Graph Benchmark by implementing three MPGNN architectures (GCN, GINE, GatedGCN) with hyperparameter optimization within a 500k parameter budget. The authors perform exhaustive grid searches over learning rates, network depth, prediction head configurations, and positional/structural encodings. For vision datasets, they add feature normalization to input node and edge features. They also correct the PCQM-Contact link prediction metric implementation to use filtered MRR instead of raw MRR, properly excluding self-loops from negative samples.

## Key Results
- All three MPGNNs (GCN, GINE, GatedGCN) surpass GPS on Peptides-Func and Peptides-Struct tasks through hyperparameter tuning
- A basic GCN model achieves state-of-the-art results on Peptides-Struct by using a deeper prediction head instead of a linear one
- Feature normalization and hyperparameter tuning improve performance across all compared methods on PascalVOC-SP and COCO-SP datasets
- The PCQM-Contact link prediction metric implementation error (using raw instead of filtered MRR) significantly affects comparative results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Suboptimal hyperparameter choices in previous LRGB implementations artificially inflated the performance gap between Graph Transformers and MPGNNs
- Mechanism: Hyperparameter tuning, especially prediction head depth and feature normalization, significantly improves MPGNN performance to match or exceed Graph Transformers on long-range tasks
- Core assumption: The original LRGB implementations used conservative or suboptimal hyperparameter configurations that didn't fully exploit the capacity of MPGNN architectures
- Evidence anchors:
  - [abstract] "the reported performance gap is overestimated due to suboptimal hyperparameter choices"
  - [section] "all three MPGNNs surpass GPS... a basic GCN model even achieves SOTA results"
  - [corpus] Weak - no direct corpus evidence found, though related work on MPNN effectiveness suggests this is plausible
- Break condition: If the tasks truly require long-range interactions that MPGNNs fundamentally cannot capture due to architectural limitations like over-squashing

### Mechanism 2
- Claim: Feature normalization is critical for vision-based graph datasets but was omitted in previous LRGB implementations
- Mechanism: Multi-dimensional node and edge features with varying scales cause poorly conditioned activations without channel-wise normalization, which when applied significantly boosts performance
- Core assumption: Vision datasets like PascalVOC-SP and COCO-SP have heterogeneous feature scales that standard deep learning practice would normalize
- Evidence anchors:
  - [section] "feature normalization is standard practice in deep learning and computer vision in particular, neither Dwivedi et al. [1] nor any subsequent works using LRGB utilize it"
  - [section] "feature normalization and hyperparameter tuning improves performance across all compared methods"
  - [corpus] Explicit mention of feature normalization benefits in related work like CRaWl
- Break condition: If the specific scale differences in these datasets don't materially affect the learning dynamics

### Mechanism 3
- Claim: The PCQM-Contact link prediction metric implementation error (using raw instead of filtered MRR) significantly affects comparative results
- Mechanism: The filtered MRR removes false negatives (other true edges) from candidate rankings, preventing artificially low scores; self-loops also bias dot-product scoring functions and should be filtered
- Core assumption: The specification explicitly requires filtered MRR but the implementation used raw MRR with false negatives included
- Evidence anchors:
  - [section] "the provided code computes the raw MRR, i.e. keeping other true tails in the list"
  - [section] "the specific choice of how negative samples are filtered... can directly affect the ranking of compared methods"
  - [corpus] Related work on link prediction metrics confirms filtering importance
- Break condition: If the task specification actually intended raw MRR despite claiming filtered, or if filtering changes don't affect relative rankings

## Foundational Learning

- Concept: Hyperparameter optimization methodology
  - Why needed here: The paper demonstrates that proper hyperparameter tuning can close or eliminate performance gaps, making it essential to understand how to conduct systematic searches
  - Quick check question: What is the difference between a grid search and a "linear" hyperparameter search as described in Appendix A.1?

- Concept: Graph neural network architectures (GCN, GINE, GatedGCN)
  - Why needed here: Understanding the architectural differences and how they process information is crucial to interpreting why tuning the prediction head depth matters
  - Quick check question: How does the information flow in a GCN layer differ from the skip connections used in the reimplemented version?

- Concept: Positional and structural encodings
  - Why needed here: The paper treats encoding selection as a hyperparameter to tune, indicating its importance for capturing graph structure
  - Quick check question: What is the key difference between Laplacian positional encoding (LapPE) and random walk structural encoding (RWSE)?

## Architecture Onboarding

- Component map: Input features → normalization → GNN layers → encoding → prediction head → loss/evaluation
- Critical path: Input features → normalization → GNN layers → encoding → prediction head → loss/evaluation
- Design tradeoffs: Depth vs parameter budget (500k limit), normalization overhead vs performance gain, filtered vs raw evaluation metrics
- Failure signatures: Poor performance on long-range tasks despite large budgets may indicate missing normalization or suboptimal head depth; inconsistent results across seeds may indicate normalization issues
- First 3 experiments:
  1. Implement feature normalization on PascalVOC-SP/COCO-SP and verify performance improvement
  2. Compare linear vs 2-layer prediction heads on Peptides-Struct to confirm the head depth effect
  3. Verify filtered MRR implementation on PCQM-Contact by checking if self-loops are properly excluded

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much further performance improvement could be achieved by expanding the hyperparameter search ranges, particularly for network depth and learning rate?
- Basis in paper: [explicit] The authors acknowledge their hyperparameter search was "relatively simple and not exhaustive" with "rather limited" ranges, especially for network depth
- Why unresolved: The authors explicitly state their search ranges were limited and could be expanded in future work, but did not conduct this expanded search
- What evidence would resolve it: Conducting an exhaustive grid search with broader ranges for depth (e.g., 4-16 layers) and learning rate (e.g., 0.0001-0.01) on all five datasets to determine the true performance ceiling for each method

### Open Question 2
- Question: Do graph transformers maintain their performance advantage on tasks where MPGNNs can match or exceed their performance through better hyperparameter tuning?
- Basis in paper: [inferred] The authors show that MPGNNs can match or exceed GPS performance on Peptides-Struct and PascalVOC-SP through tuning, suggesting the gap may be due to suboptimal configurations rather than fundamental architectural advantages
- Why unresolved: The paper only demonstrates that MPGNNs can match GPS performance, but does not test whether GPS can be further tuned to regain its advantage or if the advantage truly disappears
- What evidence would resolve it: Extensive hyperparameter optimization of GPS and other graph transformers to determine if they can regain performance advantages on tasks where tuned MPGNNs currently match them

### Open Question 3
- Question: What is the optimal filtering strategy for the link prediction metric on PCQM-Contact, and how does it affect the relative performance of different methods?
- Basis in paper: [explicit] The authors demonstrate that different filtering strategies (raw vs. filtered vs. extended filtered) significantly impact results and can affect method rankings, particularly showing GPS suffers disproportionately when self-loops are not filtered
- Why unresolved: The paper identifies the issue with filtering strategies but does not determine which strategy is most appropriate or how it should be standardized
- What evidence would resolve it: A comprehensive analysis of different filtering strategies on PCQM-Contact including domain-specific evaluation of what constitutes meaningful negative samples, potentially leading to a standardized evaluation protocol

## Limitations
- The hyperparameter search was described as "relatively simple and not exhaustive" with limited ranges, particularly for network depth
- The findings focus on specific LRGB tasks and may not generalize to all graph learning scenarios where long-range dependencies are critical
- The 500k parameter budget constraint may limit the potential performance of deeper or more complex architectures

## Confidence
- **High Confidence**: The identification and correction of the PCQM-Contact metric implementation error (using raw instead of filtered MRR)
- **Medium Confidence**: The claim that suboptimal hyperparameter choices artificially inflated the performance gap between Graph Transformers and MPGNNs
- **Medium Confidence**: The importance of feature normalization for vision datasets based on computer vision literature

## Next Checks
1. Conduct an ablation study on the prediction head depth across all MPGNN architectures on Peptides-Struct to quantify the exact contribution of this hyperparameter to performance improvements
2. Perform a systematic comparison of filtered vs raw MRR implementations on PCQM-Contact to measure the magnitude of the metric error's impact on reported results
3. Implement and test feature normalization on PascalVOC-SP and COCO-SP datasets while varying the normalization strategy (e.g., batch vs layer vs channel-wise) to determine the optimal approach for graph-based vision tasks