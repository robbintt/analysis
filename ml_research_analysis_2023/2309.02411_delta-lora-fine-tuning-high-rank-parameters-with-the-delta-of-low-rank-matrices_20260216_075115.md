---
ver: rpa2
title: 'Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices'
arxiv_id: '2309.02411'
source_url: https://arxiv.org/abs/2309.02411
tags:
- delta-lora
- fine-tuning
- lora
- parameters
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Delta-LoRA fine-tunes LLMs by updating both low-rank adaptation
  matrices and pre-trained weights using the delta of the product of these matrices.
  This approach addresses the limitation of previous low-rank methods that only update
  a small fraction of parameters.
---

# Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices

## Quick Facts
- arXiv ID: 2309.02411
- Source URL: https://arxiv.org/abs/2309.02411
- Reference count: 28
- Delta-LoRA fine-tunes LLMs by updating both low-rank adaptation matrices and pre-trained weights using the delta of the product of these matrices.

## Executive Summary
Delta-LoRA addresses the limitation of standard LoRA methods that only update low-rank adaptation matrices by introducing a mechanism to also update pre-trained weights. The method computes the delta of the product of low-rank matrices and uses this delta to update the original weights, enabling the model to adjust high-rank parameters without explicitly computing gradients for them. Experiments show significant improvements over existing methods across multiple NLP tasks while maintaining comparable memory requirements to LoRA.

## Method Summary
Delta-LoRA fine-tunes large language models by updating both low-rank adaptation matrices (A and B) and pre-trained weights (W) using the delta of the product of these matrices. The method computes the delta as $\Delta AB = A^{(t+1)}B^{(t+1)} - A^{(t)}B^{(t)}$ and uses this to update W through the equation $W^{(t+1)} = W^{(t)} + \lambda \cdot \alpha / r \cdot \Delta AB$. This approach maintains comparable memory requirements to LoRA since gradients and optimizer states are only stored for A and B, not for W. The method removes dropout from the low-rank branch to ensure gradient equivalence between W and AB.

## Key Results
- Achieves up to 1.24 BLEU score improvement on E2E NLG Challenge
- Shows 0.79 BLEU score improvement on WebNLG Challenge 2017
- Demonstrates consistent gains across all GLUE benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Delta-LoRA achieves high performance by updating both pre-trained weights and low-rank adaptation matrices.
- **Mechanism**: Updates pre-trained weight matrix W using the delta of the product of low-rank matrices, acting as a surrogate gradient for W.
- **Core assumption**: Gradients of loss with respect to AB and W are equivalent when dropout is not applied to the low-rank branch.
- **Evidence anchors**: Abstract states Delta-LoRA updates both W and low-rank matrices; Section 4.1 provides mathematical derivation showing gradient equivalence.
- **Break condition**: Equivalence breaks if dropout is applied in the low-rank branch, causing gradients to diverge.

### Mechanism 2
- **Claim**: Delta-LoRA maintains comparable memory and computational cost to LoRA despite updating more parameters.
- **Mechanism**: Uses delta AB as surrogate gradient, avoiding storage of gradients and optimizer states for W.
- **Core assumption**: Only optimizer moments for A and B are needed, as W is updated deterministically via delta AB.
- **Evidence anchors**: Abstract mentions comparable memory requirements; Section 4.1 explains gradient equivalence enables bypassing W gradient computation.
- **Break condition**: Memory overhead could increase if additional optimizer states are required for W.

### Mechanism 3
- **Claim**: Removing dropout in the low-rank branch improves delta update quality for W.
- **Mechanism**: Dropout introduces randomness that breaks gradient equivalence; removing it creates more accurate delta updates.
- **Core assumption**: Dropout causes $\frac{\partial L}{\partial W} \neq \frac{\partial L}{\partial AB}$, and its removal restores equivalence.
- **Evidence anchors**: Section 4.2 explicitly states dropout violates gradient equivalence assumption.
- **Break condition**: If dropout is necessary for regularization and its removal causes overfitting, benefits may be outweighed.

## Foundational Learning

- **Concept**: Matrix calculus and gradient equivalence
  - **Why needed here**: Understanding gradient equivalence $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial AB}$ is crucial for justifying delta-based updates.
  - **Quick check question**: If $h = Wx + ABx$, what is $\frac{\partial L}{\partial W}$ in terms of $\frac{\partial L}{\partial h}$ and $x$?

- **Concept**: Low-rank matrix decomposition
  - **Why needed here**: Delta-LoRA builds on LoRA's idea of representing weight updates as products of two low-rank matrices.
  - **Quick check question**: Given weight matrix $W \in \mathbb{R}^{c \times d}$ and rank $r \ll \min(c,d)$, what are dimensions of low-rank matrices A and B in LoRA?

- **Concept**: Parameter-efficient fine-tuning (PEFT)
  - **Why needed here**: Delta-LoRA is a PEFT method, so understanding the broader context of PEFT techniques helps with motivation and design choices.
  - **Quick check question**: What is the primary advantage of PEFT methods over full fine-tuning in terms of memory and computational cost?

## Architecture Onboarding

- **Component map**: W (pre-trained weights) -> A and B (low-rank matrices) -> delta computation -> W update via delta AB
- **Critical path**:
  1. Forward pass: Compute $h = Wx + ABx$
  2. Backward pass: Compute gradients for A and B only
  3. Update A and B using AdamW optimizer
  4. Compute $\Delta AB = A^{(t+1)}B^{(t+1)} - A^{(t)}B^{(t)}$
  5. Update W using $\Delta AB$ with scaling factors $\lambda$, $\alpha$, $r$

- **Design tradeoffs**:
  - **Memory**: Similar to LoRA by avoiding storage of gradients and optimizer states for W
  - **Expressiveness**: Updating W allows learning higher-rank representations, potentially improving performance over standard LoRA
  - **Complexity**: Delta update mechanism adds implementation complexity compared to standard LoRA

- **Failure signatures**:
  - Poor performance if gradients for W and AB are not equivalent (e.g., due to dropout)
  - Training instability if $\lambda$ is too large; negligible benefits if too small
  - Insufficient information capture if rank r is too low

- **First 3 experiments**:
  1. **Ablation study**: Compare Delta-LoRA with and without dropout in low-rank branch to confirm importance of gradient equivalence
  2. **Hyperparameter sensitivity**: Vary $\lambda$ (update ratio) and K (start steps) to find optimal settings for different tasks
  3. **Performance comparison**: Evaluate Delta-LoRA against LoRA, AdaLoRA, and full fine-tuning on benchmark dataset (e.g., GLUE)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The core claim of gradient equivalence $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial AB}$ requires further scrutiny in practical scenarios with complex loss landscapes
- Removal of dropout in the low-rank branch may introduce overfitting risks not fully explored in experiments
- Memory and computational cost claims need more detailed profiling to verify exact overhead compared to standard LoRA

## Confidence

- **High Confidence**: Empirical performance improvements (up to 1.24 BLEU on E2E NLG, consistent GLUE gains) are well-supported by experimental results
- **Medium Confidence**: Theoretical foundation for gradient equivalence and delta-based updates is sound but relies on idealized assumptions
- **Medium Confidence**: Memory and computational cost claims are reasonable but require more detailed profiling

## Next Checks

1. **Dropout Sensitivity Analysis**: Systematically test Delta-LoRA with varying dropout rates in the low-rank branch to quantify trade-off between regularization and gradient equivalence quality

2. **Gradient Equivalence Stress Test**: Evaluate method on tasks with complex loss functions (e.g., reinforcement learning objectives) where $\frac{\partial L}{\partial W}$ and $\frac{\partial L}{\partial AB}$ may diverge

3. **Memory Overhead Profiling**: Conduct precise memory measurements during training to confirm Delta-LoRA maintains comparable memory usage to LoRA across different batch sizes and sequence lengths