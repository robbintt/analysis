---
ver: rpa2
title: Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language
arxiv_id: '2306.02797'
source_url: https://arxiv.org/abs/2306.02797
tags:
- language
- concept
- urve
- human
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a human-like few-shot learning model using Bayesian
  reasoning over natural language. The model generates candidate concepts expressed
  in natural language, which are then re-weighed by a prior and a likelihood.
---

# Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language

## Quick Facts
- arXiv ID: 2306.02797
- Source URL: https://arxiv.org/abs/2306.02797
- Authors: 
- Reference count: 40
- Primary result: R²=0.95 on number game and R²=0.81 on logical concepts using Bayesian reasoning over natural language

## Executive Summary
This paper presents a human-like few-shot learning model that uses Bayesian reasoning over natural language to learn symbolic concepts from minimal examples. The model generates candidate concepts expressed in natural language, which are then re-weighed by a prior learned from human data and a likelihood function. The approach achieves strong performance on both generative (number game) and logical concept learning tasks, outperforming baseline models by better capturing human-like inductive biases.

## Method Summary
The method employs a large language model as a proposal distribution to generate candidate concepts in natural language, which are then evaluated using Bayesian inference. The model fits a prior to human behavioral data to better capture human inductive biases, and uses a likelihood function to assess how well each candidate concept explains the observed examples. A Platt transform maps model probabilities to human rating scales, enabling direct comparison with human judgments across different datasets.

## Key Results
- Achieves R²=0.95 on the number game dataset
- Achieves R²=0.81 on logical concepts dataset
- Outperforms baseline models on both generative and logical concept learning tasks

## Why This Works (Mechanism)

### Mechanism 1
Natural language provides an expressive and compositional hypothesis space that allows efficient few-shot learning by leveraging the compositional structure of language. The model generates candidate concepts in natural language, which are then re-weighed by a prior and likelihood using Bayesian inference. This approach exploits the rich, compositional structure of language to represent a wide range of concepts efficiently. The core assumption is that natural language can effectively represent and generalize across diverse concept learning tasks, including those not directly involving language.

### Mechanism 2
Bayesian inference with a learned prior provides a mechanism to model human-like inductive biases and improve generalization. The model fits a prior to human data, allowing it to better model human learners and predict human judgments on learning problems. This learned prior regularizes the model toward natural generalizations observed in humans. The core assumption is that human judgments contain information about natural inductive biases that can be extracted and used to improve model performance.

### Mechanism 3
Using a large language model as a proposal distribution enables efficient inference over a flexible hypothesis class. The model uses a large language model to propose candidate concepts, which are then evaluated using Bayesian inference. This approach allows the model to efficiently explore a vast hypothesis space without requiring explicit enumeration. The core assumption is that modern large language models can generate diverse and relevant hypotheses for concept learning tasks when appropriately prompted.

## Foundational Learning

- **Bayesian inference**
  - Why needed here: The model uses Bayesian inference to combine prior beliefs with likelihood estimates to form posterior beliefs about concepts
  - Quick check question: How does the model use Bayes' rule to update its beliefs about concepts given observed examples?

- **Proposal distribution**
  - Why needed here: The model uses a proposal distribution to efficiently sample from a vast hypothesis space without enumerating all possible concepts
  - Quick check question: What role does the language model play in proposing candidate concepts, and how does this differ from traditional enumeration methods?

- **Prior fitting**
  - Why needed here: The model fits its prior to human data to better model human learners and improve generalization performance
  - Quick check question: How does the model extract a human-like prior from behavioral data, and what assumptions are made about the relationship between human judgments and concept probabilities?

## Architecture Onboarding

- **Component map**: Language model (proposal distribution) -> Prior network -> Likelihood function -> Bayesian inference engine -> Platt transform
- **Critical path**: Concept generation → Prior evaluation → Likelihood computation → Posterior inference → Prediction
- **Design tradeoffs**: Expressivity vs. tractability: Using natural language provides high expressivity but requires efficient inference methods; Model complexity vs. data efficiency: Fitting priors to human data improves performance but requires additional data; Language model quality vs. inference efficiency: Better language models generate more diverse proposals but may be slower
- **Failure signatures**: Poor fit to human data: Indicates issues with prior fitting or likelihood computation; Slow inference: Suggests the proposal distribution is not generating diverse enough hypotheses; Low accuracy on new tasks: May indicate overfitting to specific datasets or insufficient expressivity
- **First 3 experiments**: 1) Evaluate model performance on held-out human judgments from the number game with varying numbers of proposal samples; 2) Compare model performance with and without prior fitting on both number and logical concepts; 3) Test model generalization to new concept learning tasks not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Bayesian reasoning model scale with the complexity and size of the concept space?
- Basis in paper: Inferred from the discussion of limitations and the need for efficient inference in the model
- Why unresolved: The paper demonstrates the model's effectiveness on specific tasks but does not explore its limits in more complex or larger concept spaces
- What evidence would resolve it: Empirical results showing the model's performance on increasingly complex and larger concept spaces, including a comparison with other models

### Open Question 2
- Question: How do different language models for the proposal distribution affect the model's performance and generalization capabilities?
- Basis in paper: Inferred from the use of GPT-4 and Codex as proposal distributions and the mention of potential future use of open-source models
- Why unresolved: The paper uses specific language models but does not explore the impact of using different models or the potential benefits of open-source alternatives
- What evidence would resolve it: Comparative analysis of the model's performance using different language models, including open-source options

### Open Question 3
- Question: Can the Bayesian reasoning model be extended to handle perceptual data and multimodal inputs, such as images or audio?
- Basis in paper: Inferred from the discussion of limitations and the mention of Bongard problems and the Abstraction and Reasoning Corpus as challenges for AI
- Why unresolved: The paper focuses on symbolic concepts and discrete symbolic inputs, leaving the extension to perceptual data and multimodal inputs unexplored
- What evidence would resolve it: Development and evaluation of the model on tasks involving perceptual data and multimodal inputs, demonstrating its ability to handle such data effectively

## Limitations

- The model's performance depends heavily on the quality of the language model used as a proposal distribution, which may not generalize well to all concept learning tasks
- Prior fitting assumes that human judgments directly reflect concept probabilities, which may not hold for all types of learning tasks
- The approach has not been tested on tasks requiring spatial or procedural reasoning beyond symbolic and logical concepts

## Confidence

- High Confidence: The mechanism that natural language provides an expressive hypothesis space for concept learning
- Medium Confidence: The Bayesian inference mechanism with learned priors
- Medium Confidence: The use of large language models as proposal distributions

## Next Checks

1. **Cross-domain generalization test**: Evaluate the model on at least 3 new concept learning domains (e.g., visual concepts, procedural tasks, abstract reasoning) not seen in the original datasets to assess true generalization capability.

2. **Human-in-the-loop validation**: Conduct a study where human participants generate concept hypotheses and compare the model's proposals against human-generated hypotheses to validate the proposal distribution quality.

3. **Ablation study on prior fitting**: Systematically remove the prior fitting component and measure performance degradation across different levels of human data availability to quantify the value added by human-aligned priors.