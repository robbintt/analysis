---
ver: rpa2
title: 'Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance
  via Probability Calibration'
arxiv_id: '2310.05069'
source_url: https://arxiv.org/abs/2310.05069
tags:
- calibration
- language
- multilingual
- languages
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bias in multilingual encoder
  models when predicting label words for zero-shot tasks, where high-frequency words
  from pretraining are favored, leading to poor performance. The authors propose using
  probability calibration techniques to modify the predicted probabilities of label
  words.
---

# Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration

## Quick Facts
- arXiv ID: 2310.05069
- Source URL: https://arxiv.org/abs/2310.05069
- Reference count: 23
- Primary result: Probability calibration methods significantly improve zero-shot performance of multilingual encoders across diverse languages and tasks

## Executive Summary
This paper addresses the bias problem in multilingual encoder models when predicting label words for zero-shot tasks, where high-frequency words from pretraining are favored, leading to poor performance. The authors propose using probability calibration techniques to modify the predicted probabilities of label words, reducing the model's tendency to favor overrepresented words. They introduce a simple penalty-based method that adds trainable penalties to label word probabilities, initialized with negative prior probabilities. Experiments show that all calibration methods significantly improve zero-shot performance on both monolingual and multilingual tasks, with the calibration-by-marginalization (CBM) method performing best.

## Method Summary
The paper proposes probability calibration techniques to address bias in multilingual encoder predictions by adjusting the likelihood of label words that are overrepresented in pretraining data. The calibration methods modify predicted probabilities of label words through transformations such as affine adjustments or marginalization to better align with actual label distributions. The authors introduce a penalty-based method that adds trainable penalties to label word probabilities, initialized with negative prior probabilities. They evaluate multiple calibration approaches including Contextual Calibration, PMIDC, CBM, and the penalty method across various tasks and languages, with CBM providing the most precise adjustments by considering all input samples in the test set.

## Key Results
- All calibration methods significantly improve zero-shot performance on both monolingual and multilingual tasks
- CBM method performs best by computing marginalized probabilities across all test inputs
- Training calibration parameters on few examples further enhances performance
- Improvements vary based on language accessibility and family, with Indo-European and Dravidian languages benefiting more than Austronesian and Niger-Congo languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probability calibration techniques can mitigate bias in multilingual encoder predictions by adjusting the likelihood of label words that are overrepresented in pretraining data.
- Mechanism: The calibration methods modify the predicted probabilities of label words, reducing the model's tendency to favor high-frequency words. This is achieved by applying transformations such as affine adjustments or marginalization to better align predicted probabilities with actual label distributions.
- Core assumption: The bias in label word prediction is primarily due to frequency bias in pretraining data, and adjusting probabilities can correct this imbalance.
- Evidence anchors:
  - [abstract] The paper addresses the problem of bias in multilingual encoder models when predicting label words for zero-shot tasks, where high-frequency words from pretraining are favored, leading to poor performance.
  - [section] According to Weissweiler et al. (2022) and Zhao et al. (2021), label words with higher frequency in the pretraining corpus tend to be predicted with higher probabilities.

### Mechanism 2
- Claim: Calibration-by-marginalization (CBM) provides a more precise approximation of label word probabilities by considering all input samples in the test set.
- Mechanism: CBM adjusts the probability of each label word by dividing its conditional probability by the marginalized probability across all test inputs. This comprehensive approach accounts for the influence of the entire dataset, leading to more accurate predictions.
- Core assumption: The performance of label word prediction can be improved by considering the broader context of all test inputs rather than individual samples.
- Evidence anchors:
  - [section] CBM approximates p(y|x, t) in a more precise manner by computing its marginalized probability, as the third equation in Table 1 shows. For each prediction, the sum probability Σx′∈X p(y|x′, t) are calculated by taking all test inputs into account.

### Mechanism 3
- Claim: Few-shot training of calibration parameters can further enhance performance by adapting the calibration to specific tasks.
- Mechanism: By training the calibration parameters on a small number of examples, the model can fine-tune its probability adjustments to better fit the task at hand. This approach leverages the flexibility of the calibration methods to improve accuracy with minimal additional data.
- Core assumption: A small number of training examples is sufficient to adapt the calibration parameters effectively for the task.
- Evidence anchors:
  - [section] We observe that training the calibration parameters on just a few samples further enhances the performance after of the calibrated systems.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the foundational technique used by multilingual encoders to predict masked tokens, which is essential for understanding how calibration methods modify these predictions.
  - Quick check question: What is the primary function of MLM in multilingual encoder models?

- Concept: Cross-lingual Transfer
  - Why needed here: Understanding cross-lingual transfer is crucial for evaluating the effectiveness of calibration methods across different languages and language families.
  - Quick check question: How does cross-lingual transfer impact the performance of multilingual encoders in zero-shot tasks?

- Concept: Probability Calibration
  - Why needed here: Probability calibration is the core technique used to adjust the predicted probabilities of label words, which is central to the paper's approach to improving model performance.
  - Quick check question: What is the purpose of probability calibration in the context of multilingual encoders?

## Architecture Onboarding

- Component map: Multilingual encoder models (BERT/XLM-R) -> Prompt templates -> MLM predictions -> Calibration methods (CC, PMIDC, CBM, Penalty) -> Adjusted probabilities -> Evaluation metrics
- Critical path: Input sample → Multilingual encoder → MLM predictions → Apply calibration method → Adjusted probabilities → Task performance evaluation
- Design tradeoffs: Choice of calibration method involves tradeoffs between computational complexity and accuracy. CBM is more computationally intensive but provides more precise adjustments compared to simpler methods like CC.
- Failure signatures: Failure may occur if calibration methods do not adequately address frequency bias or if few-shot training examples are not representative. Effectiveness varies across different language families.
- First 3 experiments:
  1. Validate effectiveness of each calibration method on a monolingual task using a simple dataset
  2. Compare performance of calibration methods on a multilingual dataset with diverse language families
  3. Test impact of few-shot training on calibration parameters using a small set of examples from a specific task

## Open Questions the Paper Calls Out

- **Question**: How does the proposed penalty-based calibration method compare to more complex calibration techniques in terms of computational efficiency and scalability for large-scale multilingual models?
  - Basis in paper: [explicit] The paper introduces a simple penalty-based calibration method and compares its effectiveness with other existing techniques, but does not provide a detailed comparison of computational efficiency or scalability.

- **Question**: What is the impact of calibration methods on the model's performance in generation tasks, such as question answering, compared to classification tasks?
  - Basis in paper: [inferred] The paper mentions that the research is primarily focused on classification tasks and suggests that future investigations should explore the application of calibration methods on generation tasks.

- **Question**: How do the calibration methods perform across different language families, and what linguistic characteristics contribute to their varying effectiveness?
  - Basis in paper: [explicit] The paper analyzes the performance of calibration methods across different language families and notes that the effectiveness varies, with Indo-European and Dravidian languages benefiting more than Austronesian and Niger-Congo languages.

## Limitations

- The effectiveness of calibration methods depends on the assumption that frequency bias in pretraining data is the primary source of prediction errors, which may not hold for all multilingual encoder architectures
- Cross-lingual generalization benefits vary significantly by language family and accessibility, with limited experimental evidence for truly low-resource languages
- The CBM method's requirement to compute marginalized probabilities across all test inputs could become computationally prohibitive for large test sets

## Confidence

- **High confidence**: The observation that high-frequency words from pretraining bias predictions is well-supported by existing literature and demonstrated empirically across multiple datasets
- **Medium confidence**: The comparative performance of different calibration methods is reasonably well-established for tested languages and tasks, but generalizability requires further validation
- **Medium confidence**: The few-shot training approach shows promise, but optimal number of examples and overfitting risks need more systematic investigation

## Next Checks

1. **Architecture transfer test**: Validate whether calibration methods transfer effectively to encoder-decoder models (like mT5) or smaller multilingual models, as current experiments focus primarily on BERT and XLM-R variants

2. **Robustness to test distribution shift**: Evaluate calibration performance when test data comes from a different distribution than training data to assess real-world applicability

3. **Computational overhead analysis**: Quantify runtime and memory overhead introduced by CBM compared to simpler methods across different test set sizes to determine practical deployment constraints