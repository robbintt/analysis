---
ver: rpa2
title: 'Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence
  in Large Language Models'
arxiv_id: '2307.03762'
source_url: https://arxiv.org/abs/2307.03762
tags:
- arxiv
- language
- intelligence
- llms
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically examines whether current large language models
  (LLMs) truly exhibit artificial general intelligence (AGI). The authors first review
  standardized and ability-oriented benchmarks, highlighting problems in current evaluation
  methods that may overstate LLM capabilities.
---

# Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models

## Quick Facts
- arXiv ID: 2307.03762
- Source URL: https://arxiv.org/abs/2307.03762
- Authors: 
- Reference count: 8
- Primary result: Current LLMs lack key characteristics for AGI including infinite task performance, autonomous task generation, value systems, and world models; the "unity of knowing and acting" is identified as a crucial missing piece.

## Executive Summary
This paper critically examines whether current large language models truly exhibit artificial general intelligence by analyzing standardized and ability-oriented benchmarks. The authors identify significant problems with current evaluation methods that may overstate LLM capabilities through non-linear metrics and potential data leakage. They argue that AGI requires four key characteristics: infinite task performance, autonomous task generation, a value system, and a world model reflecting reality. The paper identifies the "unity of knowing and acting" as a crucial missing piece, arguing that active engagement with the real world through repeated trials is necessary for forming robust conceptual representations and knowledge acquisition.

## Method Summary
The paper employs a comprehensive literature review of existing LLM evaluations, analyzing performance patterns across standardized tests and ability-oriented benchmarks. The authors synthesize findings to articulate missing pieces for AGI, focusing on the unity of knowing and acting. The methodology involves collecting performance data from various evaluations, identifying patterns in LLM strengths and weaknesses, and synthesizing these findings into a framework for understanding what constitutes true AGI versus current LLM capabilities.

## Key Results
- Current LLM evaluation methods overstate capabilities due to non-linear metrics and potential data leakage from training sets
- LLMs excel at language tasks but struggle with reasoning, problem-solving, and tasks requiring real-world interaction
- The "unity of knowing and acting" - active engagement with the real world - is identified as a crucial missing piece for achieving AGI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models simulate fluency without grounding, leading to semantic merry-go-rounds.
- Mechanism: Without sensorimotor feedback, LLMs rely on statistical co-occurrence patterns, which can produce internally consistent but semantically meaningless loops.
- Core assumption: Symbol grounding requires interaction with real-world referents to establish meaning.
- Evidence anchors:
  - [abstract] Putnam's BiV thought experiment: detached brains cannot ground symbols.
  - [section] GPT-4 self-talk example: conversation converges into repetitive thank-you loops.
  - [corpus] Weak - no direct corpus evidence of grounding failure in other models.
- Break condition: If the model is provided with multimodal grounding data or reinforcement from embodied interaction.

### Mechanism 2
- Claim: Current evaluation metrics overstate LLM capabilities due to non-linear scaling and potential data leakage.
- Mechanism: Non-linear metrics (e.g., xn) create illusion of emergent abilities; training on evaluation datasets inflates scores.
- Core assumption: Test sets are sourced from internet data, which overlaps with LLM training corpora.
- Evidence anchors:
  - [section] Schaeffer et al. (2023) on metric choice creating apparent emergence.
  - [section] Discussion of data leakage risk from internet-scale training sets.
  - [corpus] None explicitly cited, but consistent with known overfitting concerns.
- Break condition: Transparent evaluation with held-out, non-internet-derived datasets.

### Mechanism 3
- Claim: Large models may perform worse on some tasks due to inverse scaling.
- Mechanism: Larger models exploit spurious correlations and shortcuts, reducing generalization.
- Core assumption: Model size correlates with reliance on memorization over reasoning.
- Evidence anchors:
  - [section] McKenzie et al. (2023) study on inverse scaling across 11 datasets.
  - [section] Tang et al. (2023a) findings on larger models exploiting shortcuts more.
  - [corpus] None directly cited, but inverse scaling is a known phenomenon.
- Break condition: Task design that penalizes shortcut exploitation or rewards robust reasoning.

## Foundational Learning

- Concept: Symbol grounding
  - Why needed here: Explains why LLMs can mimic language without understanding, central to the BiV critique.
  - Quick check question: What does it mean for a symbol to be "grounded," and why is grounding necessary for meaning?

- Concept: Cognitive system dichotomy (System 1 vs System 2)
  - Why needed here: Distinguishes between quick pattern matching and slow, deliberate reasoningâ€”relevant to LLM limitations.
  - Quick check question: How do System 1 and System 2 differ in terms of processing style and application to reasoning tasks?

- Concept: Affordance and embodied cognition
  - Why needed here: Supports the argument that interaction with the world is necessary for concept learning and AGI.
  - Quick check question: What is an affordance, and how does it relate to the unity of knowing and acting?

## Architecture Onboarding

- Component map: LLM core (transformer-based) -> evaluation pipeline (benchmarks + metrics) -> interactive simulation environment -> value system module -> world model module
- Critical path: Data ingestion -> pretraining -> fine-tuning -> evaluation -> deployment -> feedback loop for interactive learning
- Design tradeoffs: Accuracy vs interpretability, model size vs inference speed, static knowledge vs dynamic learning
- Failure signatures: Overfitting to test sets, inverse scaling, shortcut exploitation, semantic loops without grounding
- First 3 experiments:
  1. Reproduce the GPT-4 self-talk loop to confirm semantic merry-go-round behavior.
  2. Test LLM performance on anti-shortcut datasets to measure reliance on spurious correlations.
  3. Implement a simple embodied simulation task (e.g., object manipulation) to evaluate grounding and concept learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop transparent evaluation methods that prevent data leakage from training sets while ensuring LLMs demonstrate true generalization rather than memorization?
- Basis in paper: [explicit] The paper discusses issues with current evaluation methods, noting that massive internet-scale training datasets may cover evaluation datasets, making it impossible to distinguish between genuine learning and memorization.
- Why unresolved: The paper highlights that current evaluation datasets are often sourced from the internet and accessible, while training sources are unavailable, creating a transparency problem that hinders genuine and reliable evaluation.
- What evidence would resolve it: Development and validation of evaluation protocols that provably exclude any overlap with training data, along with empirical demonstrations showing LLMs perform well on genuinely novel problems they couldn't have memorized.

### Open Question 2
- Question: What architectural mechanisms would enable LLMs to integrate knowing and acting in a unified cognitive framework that supports knowledge abstraction, accumulation, and application?
- Basis in paper: [explicit] The paper argues that LLMs lack the unity of knowing and acting, proposing that a cognitive architecture should integrate these elements to transcend pure data-driven approaches and support knowledge discovery through interaction.
- Why unresolved: While reinforcement learning exists for narrow domains, the paper identifies a need for a general mechanism that combines knowledge-driven and data-driven benefits in a scalable way that hasn't been demonstrated yet.
- What evidence would resolve it: A working cognitive architecture that demonstrates agents can discover and apply new knowledge through interaction in novel situations, showing superior performance compared to passive learning approaches.

### Open Question 3
- Question: What would constitute an ideal affordance-rich interactive environment that supports multimodal feedback (beyond vision and language) for concept learning and knowledge acquisition?
- Basis in paper: [explicit] The paper envisions a meta-verse that provides rich affordance for agents to play with objects, offering feedback of multi-modality including sensory input beyond vision and language, and supporting reasoning tasks.
- Why unresolved: Current platforms like Habitat and Behavior are designed for specific tasks with insufficient interactive action space and realistic effects, falling short of the comprehensive environment needed for human-level concept understanding.
- What evidence would resolve it: A demonstrated interactive environment that enables agents to learn novel concepts through active manipulation, showing that agents can acquire and generalize knowledge more effectively than in passive learning scenarios.

## Limitations
- The "unity of knowing and acting" argument remains largely theoretical with limited empirical validation
- Evidence for semantic merry-go-rounds comes from a single case study without systematic investigation across different models
- The paper doesn't address whether multimodal models with vision or other sensory inputs partially address the grounding problem

## Confidence

- High confidence: Current LLM evaluation methods overstate capabilities due to data leakage and non-linear metrics (supported by multiple cited studies)
- Medium confidence: The four characteristics framework for AGI is logically coherent and identifies genuine gaps in current systems
- Low confidence: The specific claim that embodied interaction and repeated trials are the primary missing pieces for AGI - this requires empirical validation through actual implementation

## Next Checks

1. Conduct a systematic study of semantic coherence decay in LLM self-dialogue across multiple model sizes, prompt types, and conversation lengths to quantify the prevalence and severity of merry-go-round loops
2. Design and test an evaluation framework using non-internet-derived datasets (e.g., synthetically generated problems or human-collected data) to measure actual LLM capabilities without data leakage bias
3. Implement a simple embodied learning environment (e.g., a grid-world manipulation task) where an LLM must interact physically with objects to solve problems, then compare concept acquisition rates and generalization to traditional training approaches