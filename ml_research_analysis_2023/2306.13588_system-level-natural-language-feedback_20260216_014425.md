---
ver: rpa2
title: System-Level Natural Language Feedback
arxiv_id: '2306.13588'
source_url: https://arxiv.org/abs/2306.13588
tags:
- feedback
- response
- search
- query
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a system-level approach to using natural
  language feedback for improving language generation systems, addressing the limitation
  of existing instance-level methods. The authors propose a framework that aggregates
  instance-level feedback into system-level criteria through human-in-the-loop clustering
  and summarization, which are then used to design prompts for refining model responses
  and to create metrics for evaluation.
---

# System-Level Natural Language Feedback

## Quick Facts
- arXiv ID: 2306.13588
- Source URL: https://arxiv.org/abs/2306.13588
- Authors: 
- Reference count: 18
- Primary result: System-level feedback aggregation improves language generation performance more than instance-level feedback alone, with human feedback being more effective than model-generated feedback

## Executive Summary
This paper introduces a system-level approach to using natural language feedback for improving language generation systems. The authors propose a framework that aggregates instance-level feedback into system-level criteria through human-in-the-loop clustering and summarization, which are then used to design prompts for refining model responses and to create metrics for evaluation. They apply this framework to two case studies: query generation and response generation in an information-seeking dialog system. Results show that system-level feedback alone improves performance, and combining it with instance-level feedback brings further gains. Human-written feedback leads to more grounded refinements than GPT-3.5-generated feedback, highlighting the importance of human input.

## Method Summary
The framework aggregates instance-level feedback into system-level criteria through clustering and human-in-the-loop summarization. These criteria guide both prompt engineering for response refinement and metric design for evaluation. The process involves: (1) clustering feedback to identify common issues, (2) deriving system-level criteria through human summarization, (3) designing metrics based on these criteria, (4) using the criteria to create prompts for a text refiner (GPT-3.5), (5) fine-tuning the text generator using satisfied data and refinement data, and (6) evaluating using the designed metrics. The approach demonstrates how natural language feedback can formalize system-level design decisions and improve alignment with user needs.

## Key Results
- System-level feedback aggregation improves model performance more than instance-level feedback alone
- Human-written feedback produces more grounded refinements than GPT-3.5-generated feedback
- Combining system-level and instance-level feedback yields better results than either alone
- The framework successfully improves query generation and response generation in information-seeking dialog systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System-level feedback aggregation improves model performance more than instance-level feedback alone.
- Mechanism: The framework clusters instance-level feedback to identify common issues, derives criteria through human-in-the-loop summarization, and uses these criteria to guide both prompt engineering for refinement and metric design for evaluation. This creates a systematic approach to addressing widespread model weaknesses rather than treating each instance as isolated.
- Core assumption: Common patterns exist in user feedback that can be abstracted into actionable system-level criteria.

### Mechanism 2
- Claim: Human-written feedback produces more effective refinements than model-generated feedback.
- Mechanism: Human feedback tends to be more targeted and specific about actual issues, while model-generated feedback is often generic and verbose. This specificity translates to more grounded refinements that better address the underlying problems.
- Core assumption: Human feedback captures nuanced user needs that models cannot replicate.

### Mechanism 3
- Claim: Combining system-level and instance-level feedback yields better results than either alone.
- Mechanism: System-level feedback provides general criteria for prompt engineering and metric design, while instance-level feedback offers specific, context-dependent guidance for individual refinements. The combination addresses both broad patterns and specific edge cases.
- Core assumption: Different types of feedback provide complementary benefits.

## Foundational Learning

- Concept: Natural language processing and evaluation metrics
  - Why needed here: The framework relies on understanding how to process and evaluate natural language feedback, design metrics, and use language models for refinement
  - Quick check question: Can you explain the difference between ROUGE-2 and BLEU-4 metrics and when each would be appropriate?

- Concept: Human-in-the-loop machine learning
  - Why needed here: The framework explicitly requires human experts to cluster feedback, derive criteria, and design metrics
  - Quick check question: What are the advantages and disadvantages of involving humans in the feedback aggregation process?

- Concept: Prompt engineering for large language models
  - Why needed here: The framework uses carefully crafted prompts based on derived criteria to guide the text refiner
  - Quick check question: How does the structure and content of a prompt affect the output of a large language model?

## Architecture Onboarding

- Component map: Feedback collection system → Clustering algorithm → Human-in-the-loop criteria derivation → Prompt engineering module → Text refiner (LLM) → Quality checker → Training data construction → Model fine-tuning → Evaluation with designed metrics
- Critical path: Feedback → Clustering → Criteria → Prompt Engineering → Refinement → Quality Check → Training Data → Fine-tuning → Evaluation
- Design tradeoffs: Using GPT-3.5 for refinement vs. training a dedicated refiner (computational cost vs. control); Number of clusters in feedback aggregation (granularity vs. manageability); Level of human involvement (quality vs. scalability)
- Failure signatures: Poor clustering results in irrelevant criteria; Generic prompts produce unhelpful refinements; Quality checker misclassifies refinements; Designed metrics don't align with actual user needs
- First 3 experiments: 1) Run clustering on a small feedback dataset and manually inspect the quality of derived clusters; 2) Test prompt engineering with different criteria on a validation set of unsatisfied responses; 3) Compare refinement quality using human vs. GPT-3.5 feedback on a small sample

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of GPT-3.5-generated feedback compare to human-written feedback in terms of refinement success rate?
- Basis in paper: [explicit] The paper explicitly compares human-written and GPT-3.5-generated feedback in terms of refinement success rate, diversity, verbosity, and grammar.
- Why unresolved: The paper does not provide a direct comparison of refinement success rates between human and GPT-3.5 feedback, focusing instead on other characteristics like diversity and grammar.
- What evidence would resolve it: A direct comparison of the percentage of satisfactory refinements generated using human feedback versus GPT-3.5 feedback would resolve this question.

### Open Question 2
- Question: Can the system-level feedback framework be extended to other domains beyond information-seeking dialog systems?
- Basis in paper: [inferred] The framework is demonstrated in two case studies within information-seeking dialog systems, suggesting potential applicability to other domains.
- Why unresolved: The paper does not explore the application of the framework to other types of systems or domains, leaving its generalizability untested.
- What evidence would resolve it: Testing the framework on different types of systems (e.g., recommendation systems, code generation) and evaluating its effectiveness would resolve this question.

### Open Question 3
- Question: What is the impact of different clustering algorithms on the quality of derived system-level feedback?
- Basis in paper: [explicit] The paper mentions using k-means clustering to group feedback but does not explore the impact of different clustering algorithms.
- Why unresolved: The choice of clustering algorithm is not discussed in terms of its impact on the resulting criteria and their effectiveness.
- What evidence would resolve it: Comparing the performance of the system when using criteria derived from different clustering algorithms (e.g., hierarchical clustering, DBSCAN) would resolve this question.

## Limitations

- The effectiveness of system-level feedback aggregation depends heavily on the quality of human clustering and summarization, which introduces subjective variability
- The paper doesn't provide comprehensive ablation studies showing the individual contribution of each system-level component
- GPT-3.5 feedback generation parameters and prompt engineering details are underspecified, limiting reproducibility

## Confidence

- High confidence: System-level feedback improves performance over instance-level alone; human feedback is more effective than GPT-3.5 feedback
- Medium confidence: The combination of system-level and instance-level feedback provides complementary benefits
- Low confidence: The scalability of human-in-the-loop clustering for production systems

## Next Checks

1. Conduct an ablation study isolating the impact of clustering quality vs. prompt engineering vs. metric design
2. Test the framework on a different domain (e.g., code generation) to assess generalizability
3. Measure the inter-rater reliability of human experts during the clustering and criteria derivation phases