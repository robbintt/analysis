---
ver: rpa2
title: 'Unsupervised Approach to Evaluate Sentence-Level Fluency: Do We Really Need
  Reference?'
arxiv_id: '2312.01500'
source_url: https://arxiv.org/abs/2312.01500
tags:
- fluency
- text
- language
- sentence
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised method for evaluating sentence-level
  fluency without requiring reference text. The approach uses Recurrent Neural Network
  language models trained on multilingual embeddings to compute a fluency score based
  on the Syntactic Log-Odds Ratio (SLOR).
---

# Unsupervised Approach to Evaluate Sentence-Level Fluency: Do We Really Need Reference?

## Quick Facts
- **arXiv ID**: 2312.01500
- **Source URL**: https://arxiv.org/abs/2312.01500
- **Reference count**: 29
- **Primary result**: Proposed unsupervised method achieves Pearson correlation up to 0.6 with human fluency judgments for 10 Indic languages using LSTM + MuRIL embeddings.

## Executive Summary
This paper introduces an unsupervised approach for evaluating sentence-level fluency without requiring reference text. The method computes Syntactic Log-Odds Ratio (SLOR) scores using Recurrent Neural Network language models trained on multilingual embeddings. Experiments on 10 Indic languages show the approach achieves moderate correlation with human judgments, with the best performance from LSTM models using MuRIL embeddings. The study also releases a 5K sentence benchmark dataset for fluency evaluation in Indic languages.

## Method Summary
The approach uses RNN-based language models (LSTM, GRU, Bi-LSTM) with various embeddings (FastText, BPEmb, IndicBERT, MuRIL) trained on scraped news data from 10 regional websites. Fluency scores are computed using SLOR, which measures the difference between sentence-level and unigram probabilities normalized by sentence length. The method is evaluated by correlating predicted scores with human judgments on a 5K sentence benchmark dataset (500 sentences per language).

## Key Results
- LSTM models with MuRIL embeddings achieved highest correlation (up to 0.6) with human fluency judgments
- Multilingual embeddings outperformed monolingual alternatives for Indic languages
- The approach shows moderate alignment with human ratings but varies across languages
- Models struggled with sentences containing digits and abbreviations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLOR fluency scores correlate with human judgments because they measure deviation from unigram probability
- Mechanism: The score computes sentence log probability from an LM and subtracts the unigram log probability, normalizing by sentence length
- Core assumption: Sentences with higher syntactic complexity or grammatical correctness will have a lower unigram-to-sentence probability gap
- Evidence anchors:
  - [abstract]: "Our approach leverages various word embeddings and trains language models using Recurrent Neural Network (RNN) architectures."
  - [section]: "To determine sentence fluency, we compute the Syntactic Log-Odds Ratio (SLOR) score by subtracting unigram probability from the sentence probability and further normalizing it by the sentence length."
- Break condition: If unigram probabilities are poorly estimated due to limited vocabulary coverage, the SLOR score will misrepresent fluency.

### Mechanism 2
- Claim: Multilingual embeddings (MuRIL) capture richer linguistic structures than monolingual ones
- Mechanism: MuRIL embeddings are trained on large-scale corpora covering 16 Indian languages, encoding shared morphological and syntactic features across the Indic family
- Core assumption: Languages within the same family share subword patterns that a multilingual model can exploit to generalize fluency assessment
- Evidence anchors:
  - [abstract]: "We also experiment with other available multilingual Language Models (LMs)."
  - [section]: "MuRIL (Multilingual Representations for Indian Languages) (Khanuja et al., 2021) is a BERT based multilingual language model especially built for 16 Indian languages that is trained using large text corpora of Indian languages."
- Break condition: If target language data is very different from training data, multilingual embeddings may not improve performance over monolingual ones.

### Mechanism 3
- Claim: Training RNN LMs with scraped news data yields domain-relevant fluency models
- Mechanism: News text is clean and grammatically standard, providing high-quality training examples that align with human expectations of fluency
- Core assumption: Fluency evaluation benefits from training on texts that humans consider "standard" or "correct" in the target language
- Evidence anchors:
  - [abstract]: "We mined text from web sources for 10 regional news websites."
  - [section]: "These scripts extract only the news text from sources and avoid most of the noisy data as a primary filter."
- Break condition: If news style differs significantly from the text generation domain, the model may overfit to journalistic norms.

## Foundational Learning

- **Concept**: Language modeling and probability estimation
  - Why needed here: The fluency score relies on computing sentence and unigram probabilities from LMs
  - Quick check question: Can you explain how an LM estimates the probability of a sentence?

- **Concept**: Tokenization and embedding
  - Why needed here: Input sentences must be tokenized and embedded before LM scoring
  - Quick check question: What is the difference between word-level and subword-level tokenization?

- **Concept**: Correlation analysis
  - Why needed here: Model performance is evaluated by correlating SLOR scores with human judgments
  - Quick check question: How does Pearson correlation measure the strength of a linear relationship?

## Architecture Onboarding

- **Component map**: Data scraping → preprocessing → training data split → LM training (RNN + embeddings) → SLOR score computation → correlation evaluation
- **Critical path**: Training data quality → LM architecture choice → SLOR computation → human correlation
- **Design tradeoffs**: Smaller models train faster but may underfit; multilingual embeddings help generalization but require more data; SLOR is unsupervised but depends on accurate probability estimation
- **Failure signatures**: Low correlation with human judgments indicates poor LM training or mismatched evaluation data; NaN or infinite SLOR scores indicate probability estimation errors
- **First 3 experiments**:
  1. Train a basic LSTM LM with FastText embeddings on scraped data; compute SLOR on validation set; check for runtime errors
  2. Replace FastText with BPEmb embeddings; retrain; compare validation SLOR distributions
  3. Use MuRIL embeddings with the same LSTM architecture; evaluate correlation with human annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific linguistic features that make it difficult for current models to handle sentences with digits and abbreviations, and how can these challenges be addressed?
- Basis in paper: [explicit] The paper mentions that models struggled with sentences containing digits and abbreviations, and that these elements often carry specific meanings and context
- Why unresolved: The paper does not provide a detailed analysis of the linguistic features that make digits and abbreviations challenging, nor does it propose specific solutions to address these challenges
- What evidence would resolve it: A detailed linguistic analysis of the specific features of digits and abbreviations that pose difficulties for current models, along with experimental results demonstrating improved performance using targeted approaches to handle these elements

### Open Question 2
- Question: How does the performance of the proposed approach compare to other reference-free fluency evaluation methods, such as those based on semantic similarity or discourse coherence?
- Basis in paper: [inferred] The paper focuses on evaluating sentence-level fluency using a reference-free approach based on syntactic log-odds ratio (SLOR), but does not compare its performance to other reference-free methods
- Why unresolved: The paper does not provide a comprehensive comparison with other reference-free fluency evaluation methods, limiting the understanding of the proposed approach's relative strengths and weaknesses
- What evidence would resolve it: A comparative study evaluating the proposed approach against other reference-free fluency evaluation methods, including semantic similarity and discourse coherence measures, on a common dataset with human judgments

### Open Question 3
- Question: What are the potential applications of the proposed approach in real-world scenarios, and how can it be integrated into existing natural language generation systems?
- Basis in paper: [explicit] The paper mentions that the approach can be applied to downstream tasks of natural language generation, such as summarization, paraphrase generation, translation, dialogue generation, and image captioning
- Why unresolved: The paper does not provide specific examples or guidelines on how the approach can be integrated into existing NLG systems or its potential impact on the quality of generated text in real-world applications
- What evidence would resolve it: Case studies or pilot projects demonstrating the integration of the proposed approach into existing NLG systems, along with quantitative and qualitative evaluations of its impact on the quality of generated text in real-world scenarios

## Limitations

- Moderate correlation scores (up to 0.6) indicate the approach works but has significant room for improvement
- Performance varies substantially across languages, suggesting limited generalizability across the Indic language family
- The news-domain training data may create bias that limits applicability to other text generation contexts

## Confidence

**High Confidence**: The fundamental mechanism of using SLOR scores derived from RNN language models is technically sound and well-established in NLP literature.

**Medium Confidence**: The claim that this approach eliminates the need for reference text in fluency evaluation is partially supported but requires qualification, as effectiveness depends heavily on training data quality.

**Low Confidence**: The generalizability of results to languages outside the 10 Indic languages tested, or to non-news domains, cannot be established from this study alone.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Compute and report Fleiss' kappa or similar inter-annotator agreement metrics for the human fluency judgments to quantify benchmark dataset reliability.

2. **Cross-domain Transfer Evaluation**: Test the trained fluency models on machine-generated text from various NLG systems rather than just scraped news articles to reveal potential news-domain bias.

3. **Ablation Study on Embedding Types**: Conduct systematic comparison across all embedding types using identical RNN architectures to isolate the contribution of embedding choice to performance differences.