---
ver: rpa2
title: In-Context Learning with Iterative Demonstration Selection
arxiv_id: '2310.09881'
source_url: https://arxiv.org/abs/2310.09881
tags:
- arxiv
- learning
- examples
- reasoning
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting appropriate few-shot
  demonstrations for in-context learning (ICL) in large language models (LLMs). The
  authors observe that the optimal dimension for selecting demonstrations (diversity
  or similarity) is task-specific, and existing methods often ignore this fact.
---

# In-Context Learning with Iterative Demonstration Selection

## Quick Facts
- arXiv ID: 2310.09881
- Source URL: https://arxiv.org/abs/2310.09881
- Authors: 
- Reference count: 28
- Key outcome: IDS achieves 90.9% average accuracy, outperforming the best baseline by 1.1%

## Executive Summary
This paper addresses the challenge of selecting appropriate few-shot demonstrations for in-context learning (ICL) in large language models (LLMs). The authors observe that the optimal dimension for selecting demonstrations (diversity or similarity) is task-specific, and existing methods often ignore this fact. To overcome this limitation, they propose Iterative Demonstration Selection (IDS), a method that leverages zero-shot chain-of-thought reasoning (Zero-shot-CoT) to iteratively select examples that are diverse but still strongly correlated with the test sample. Through extensive experiments on tasks including commonsense reasoning, question answering, topic classification, and sentiment analysis, the authors demonstrate that IDS consistently outperforms existing ICL demonstration selection methods.

## Method Summary
Iterative Demonstration Selection (IDS) uses Zero-shot-CoT to generate a reasoning path for each test sample, which is then used to select top-k most semantically similar training examples as few-shot demonstrations. These demonstrations are prepended to the test sample for ICL, and the generated answer is accompanied by its corresponding reasoning path. This process is repeated for a few iterations, and finally, majority voting is used to obtain the final result. The method addresses the task-specific nature of optimal demonstration selection by iteratively refining both the demonstrations and reasoning paths.

## Key Results
- IDS achieves 90.9% average accuracy across all tasks, 1.1% higher than the best baseline
- The method shows consistent improvements over Top-k-Consistency-CoT and Random-Voting-CoT baselines
- Performance remains stable across different iteration counts (q=1, 3, 5) with majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement through reasoning path updates progressively improves demonstration quality
- Mechanism: Zero-shot-CoT generates a reasoning path for the test sample, which is used to select semantically similar demonstrations. The generated answer includes a new reasoning path, which becomes the basis for selecting demonstrations in the next iteration.
- Core assumption: The reasoning path captures the key aspects of how to solve the problem, and examples similar to this path will be helpful demonstrations.
- Evidence anchors:
  - [abstract]: "The generated answer is accompanied by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration."
  - [section]: "The new reasoning path is then used for extracting another set of demonstrations by semantic similarity in the next iteration."
  - [corpus]: Weak evidence. Related papers focus on demonstration selection but don't explicitly discuss iterative reasoning path refinement.
- Break condition: If the reasoning path becomes too narrow or repetitive, diversity may be lost and performance could degrade.

### Mechanism 2
- Claim: Combining diversity and similarity dimensions addresses task-specific optimal selection needs
- Mechanism: By using the reasoning path to select demonstrations, IDS ensures strong correlation (similarity) with the test sample. The iterative nature of the algorithm ensures that different reasoning paths are generated across iterations, leading to diverse demonstrations that cover different aspects of the problem space.
- Core assumption: The optimal dimension for demonstration selection (diversity vs. similarity) is task-specific, and a method that leverages both will perform better across different tasks.
- Evidence anchors:
  - [abstract]: "Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific."
  - [section]: "Different sets of demonstrations can yield significantly different performance... Thus, it is unreasonable to claim that one dimension is consistently better than the other across different tasks."
  - [corpus]: Weak evidence. The corpus mentions demonstration selection methods but doesn't specifically address the diversity vs. similarity tradeoff.
- Break condition: If the task requires a specific dimension that IDS doesn't adequately capture, performance may suffer compared to a specialized method.

### Mechanism 3
- Claim: Majority voting across iterations improves robustness and final accuracy
- Mechanism: After q iterations, IDS uses majority voting on all generated answers to obtain the final result. This aggregates the predictions from different iterations, which may have captured different aspects of the problem due to the diverse demonstrations selected.
- Core assumption: Different iterations will generate complementary predictions, and majority voting can leverage this diversity to improve overall accuracy.
- Evidence anchors:
  - [abstract]: "After several iterations, IDS adopts majority voting to obtain the final result."
  - [section]: "After q rounds of iterations between Step 2 and 3, we adopt majority voting on all Â to obtain the final result Âf inal."
  - [corpus]: Weak evidence. Related papers don't specifically discuss majority voting in the context of iterative demonstration selection.
- Break condition: If iterations produce highly correlated or incorrect predictions, majority voting may not improve and could even degrade performance.

## Foundational Learning

- Concept: In-context learning (ICL) and its sensitivity to demonstration selection
  - Why needed here: The paper addresses the challenge of selecting appropriate demonstrations for ICL, so understanding ICL fundamentals is crucial.
  - Quick check question: What is the main difference between ICL and traditional fine-tuning approaches?

- Concept: Chain-of-thought (CoT) reasoning and its application in ICL
  - Why needed here: IDS uses Zero-shot-CoT to generate reasoning paths for demonstration selection and inference, so understanding CoT is essential.
  - Quick check question: How does CoT improve the reasoning capabilities of LLMs compared to direct answer generation?

- Concept: Semantic similarity and its measurement using sentence embeddings
  - Why needed here: IDS uses cosine similarity between sentence embeddings to select demonstrations, so understanding semantic similarity is important.
  - Quick check question: What are the advantages and disadvantages of using cosine similarity with sentence embeddings for demonstration selection?

## Architecture Onboarding

- Component map: Test sample → Zero-shot-CoT → KNN selection → LLM inference → Update reasoning path → Repeat q times → Majority voting

- Critical path: Test sample → Zero-shot-CoT → KNN selection → LLM inference → Update reasoning path → Repeat q times → Majority voting

- Design tradeoffs:
  - Number of iterations (q): More iterations may improve accuracy but increase computational cost and risk of error propagation
  - Number of demonstrations (k): More demonstrations may provide more context but increase the risk of redundancy or contradiction
  - Choice of LLM: Different LLMs may have varying performance with IDS and different computational requirements

- Failure signatures:
  - Performance degradation with increased iterations: May indicate error propagation or loss of diversity
  - Inconsistent results across seeds: May indicate sensitivity to random initialization or data sampling
  - Low improvement over baselines: May indicate ineffective reasoning path generation or demonstration selection

- First 3 experiments:
  1. Run IDS with q=1 and k=4 on a small dataset to verify basic functionality
  2. Compare IDS with Top-k-Consistency-CoT and Random-Voting-CoT on a single dataset to validate improvements
  3. Test IDS with different numbers of iterations (q=1, 3, 5) on a single dataset to find the optimal number of iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of iterations (q) for IDS to maximize performance without introducing noise?
- Basis in paper: [inferred] The paper mentions using q=3 iterations and conducting controlled experiments with q={1, 5, 7}, observing that performance does not always improve with more iterations and might introduce noise.
- Why unresolved: The paper only tests a few specific values of q and does not explore a wide range of values to determine the optimal number.
- What evidence would resolve it: A comprehensive study testing a wide range of q values (e.g., 1-10) on multiple datasets and tasks to identify the point of diminishing returns and potential overfitting with excessive iterations.

### Open Question 2
- Question: How does the performance of IDS vary across different LLM architectures and sizes?
- Basis in paper: [explicit] The paper evaluates GPT-3.5 and GPT-4, showing that IDS outperforms the baseline on both, but the performance gap and relative effectiveness might differ.
- Why unresolved: The paper only tests two specific LLM models. It is unclear how IDS would perform on other architectures (e.g., PaLM, BLOOM) or different model sizes within the same architecture.
- What evidence would resolve it: Evaluating IDS on a diverse set of LLM architectures and sizes, comparing its performance to the baseline and analyzing the impact of model characteristics on IDS's effectiveness.

### Open Question 3
- Question: What is the impact of different reasoning path quality on IDS's performance?
- Basis in paper: [inferred] IDS relies on Zero-shot-CoT to generate reasoning paths for demonstration selection. The quality of these reasoning paths is crucial for selecting appropriate demonstrations.
- Why unresolved: The paper does not analyze the quality of the generated reasoning paths or investigate how variations in reasoning path quality affect IDS's performance.
- What evidence would resolve it: Analyzing the quality of reasoning paths generated by Zero-shot-CoT for different tasks and datasets, and correlating reasoning path quality with IDS's performance. Exploring methods to improve reasoning path quality and their impact on IDS.

### Open Question 4
- Question: How does IDS handle tasks with more complex reasoning requirements or longer reasoning paths?
- Basis in paper: [inferred] IDS is evaluated on tasks like commonsense reasoning and question answering, which might involve varying degrees of reasoning complexity.
- Why unresolved: The paper does not explicitly analyze the reasoning complexity of the tasks or investigate how IDS's performance is affected by task-specific reasoning requirements.
- What evidence would resolve it: Designing experiments with tasks of varying reasoning complexity and analyzing IDS's performance, identifying potential limitations and exploring adaptations to handle more complex reasoning scenarios.

### Open Question 5
- Question: What are the computational and memory requirements of IDS compared to other demonstration selection methods?
- Basis in paper: [explicit] The paper mentions that IDS has a "negligible increase in computational resources" compared to baselines due to Zero-shot-CoT.
- Why unresolved: The paper does not provide a detailed analysis of the computational and memory requirements of IDS, including the cost of Zero-shot-CoT and the impact of increasing iterations or dataset size.
- What evidence would resolve it: Conducting a comprehensive analysis of the computational and memory requirements of IDS, comparing it to other methods, and identifying potential bottlenecks or scalability issues.

## Limitations
- Limited evaluation scope with only five datasets across four task types
- Lack of ablation studies on individual components of the IDS method
- No analysis of computational and memory requirements compared to baselines

## Confidence
- High confidence: Core finding that IDS outperforms baseline methods on average (90.9% accuracy vs. 89.8% best baseline)
- Medium confidence: Mechanism explanations, though iterative reasoning path refinement appears sound
- Low confidence: Generalizability claims across diverse real-world applications and edge cases

## Next Checks
1. **Ablation study on iteration count**: Systematically vary q from 1 to 10 on each dataset to identify optimal iteration counts and test whether performance degrades with excessive iterations
2. **Cross-task consistency analysis**: Apply IDS to tasks requiring fundamentally different reasoning strategies to validate the claim that IDS adapts to task-specific optimal dimensions
3. **Robustness to demonstration quality**: Replace the training set with increasingly noisy or contradictory examples to test whether IDS maintains performance when demonstration quality varies