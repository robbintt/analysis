---
ver: rpa2
title: Self-Consistent Narrative Prompts on Abductive Natural Language Inference
arxiv_id: '2309.08303'
source_url: https://arxiv.org/abs/2309.08303
tags:
- prompt
- narrative
- task
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a prompt tuning model, \u03B1-PACE, for\
  \ the abductive natural language inference (\u03B1NLI) task. It addresses the inter-sentential\
  \ coherence and model consistency by considering various narrative sequences and\
  \ designing specific prompt templates."
---

# Self-Consistent Narrative Prompts on Abductive Natural Language Inference

## Quick Facts
- arXiv ID: 2309.08303
- Source URL: https://arxiv.org/abs/2309.08303
- Reference count: 30
- Key outcome: α-PACE achieves 92.54% test accuracy on αNLI, outperforming fine-tuned T5 by 4.15%

## Executive Summary
This paper introduces α-PACE, a prompt tuning model for abductive natural language inference (αNLI) that addresses inter-sentential coherence and model consistency through self-consistent prompts. The approach considers multiple narrative sequences (linear, reverse chronology, etc.) and designs specific prompt templates with continuous discourse connectives to guide pre-trained language models in understanding narrative context. The model achieves significant improvements over competitive baselines by enforcing consistency across multiple predictions and capturing different perspectives of the same story through narrative permutations.

## Method Summary
α-PACE is a prompt tuning approach that modifies the behavior of pre-trained language models without altering their weights. The method uses continuous learnable prompt tokens to represent discourse connectives between narrative elements, with three [MASK] tokens for plausibility judgments and hypothesis selection. Six different narrative sequence patterns are evaluated, and majority voting determines the final answer. The model is trained using a joint probability objective that enforces consistency across multiple predictions, with discourse connectives inserted between sentences to enhance inter-sentential coherence.

## Key Results
- Achieves 92.54% test accuracy on αNLI task
- Outperforms fine-tuned T5 by 4.15%
- Self-consistent framework with three [MASK] predictions improves performance
- Incorporating discourse connectives between sentences enhances narrative understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-consistent prompts enforce output consistency by requiring the model to predict both sentence sequence plausibility and final hypothesis choice.
- **Mechanism**: The prompt template includes three [MASK] tokens for plausibility judgments and hypothesis choice, with the model learning to produce consistent outputs across all predictions through shared contextual embeddings.
- **Core assumption**: Language models can learn to align multiple predictions when trained with a joint probability objective.
- **Evidence anchors**: [abstract] First to consider inter-sentential coherence and self-consistency through prompt tuning; [section 3.3.1] Humans recognize plausibility through learning process that models can replicate.
- **Break condition**: If the pre-trained model lacks sufficient coherence modeling capacity, the self-consistency objective may not improve performance.

### Mechanism 2
- **Claim**: Incorporating inter-sentential coherence via discourse connectives improves narrative understanding.
- **Mechanism**: Continuous prompt tokens represent discourse connectives between sentences, allowing the model to learn temporal and causal relationships between observations and hypotheses.
- **Core assumption**: Pre-trained language models can extract and utilize discourse coherence signals when explicitly prompted with connective representations.
- **Evidence anchors**: [abstract] General self-consistent framework considers various narrative sequences; [section 3.3.1] Adding discourse connectives helps models understand narrative context more easily.
- **Break condition**: If discourse connective embeddings don't capture true temporal/causal relationships, coherence benefits will be limited.

### Mechanism 3
- **Claim**: Considering multiple narrative sequences captures different perspectives of the same story, improving abductive reasoning.
- **Mechanism**: Six different permutations of observations and hypotheses are evaluated with majority voting to select the final answer, capturing linear, reverse chronology, and other narrative orders.
- **Core assumption**: The same narrative can be understood through different temporal orderings, and language models can extract relevant information from each perspective.
- **Evidence anchors**: [abstract] Specific prompt templates distinguish narrative order differences; [section 2] People understand narrative context through alternative sequences.
- **Break condition**: If certain narrative orderings don't provide useful information for abductive reasoning, additional computation may not improve performance.

## Foundational Learning

- **Concept**: Prompt tuning methodology
  - Why needed here: Enables modifying model behavior without altering pre-trained weights by adding continuous prompt tokens
  - Quick check question: What's the key difference between prompt tuning and traditional fine-tuning?

- **Concept**: Abductive reasoning principles
  - Why needed here: Understanding that abductive reasoning infers the most plausible explanation from incomplete observations is essential for designing appropriate prompts
  - Quick check question: How does abductive reasoning differ from deductive reasoning in terms of evidence requirements?

- **Concept**: Discourse coherence and connectives
  - Why needed here: The method relies on modeling temporal and causal relationships between narrative elements, which discourse connectives help capture
  - Quick check question: What's the difference between coherence and cohesion in discourse analysis?

## Architecture Onboarding

- **Component map**: Input sentences → Continuous prompt tokens (prefix and cloze) → T5 encoder-decoder → Three [MASK] predictions → Joint probability calculation → Majority voting across sequences
- **Critical path**: Prompt template construction → Continuous prompt learning → Joint probability optimization → Majority voting aggregation
- **Design tradeoffs**: Continuous prompts allow flexible representation but lose interpretability; six narrative sequences increase computation but capture more perspectives
- **Failure signatures**: Poor performance if prompt template structure is altered; instability when increasing cloze prompt length beyond 3
- **First 3 experiments**:
  1. Test single narrative sequence with continuous prompts but without self-consistency (one [MASK])
  2. Test self-consistency with fixed discourse connectives (discrete) instead of continuous prompts
  3. Test majority voting across sequences with random predictions to establish baseline for sequence permutation effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of narrative sequence pattern impact the model's ability to capture inter-sentential coherence and self-consistency in abductive reasoning?
- Basis in paper: [explicit] The paper discusses various narrative sequences and their impact on model performance
- Why unresolved: While the paper explores effectiveness of different narrative sequences, it doesn't provide detailed analysis of how each specific pattern influences inter-sentential coherence and self-consistency
- What evidence would resolve it: Comparative study analyzing model's performance with each narrative sequence pattern, focusing on capture of inter-sentential coherence and self-consistency

### Open Question 2
- Question: To what extent do the learned continuous prompt tokens represent discourse connectives, and how do they influence the model's reasoning in abductive tasks?
- Basis in paper: [explicit] The paper introduces continuous prompt tokens to represent discourse connectives and discusses their role in capturing inter-sentential coherence
- Why unresolved: The paper doesn't provide detailed analysis of the relationship between learned continuous prompt tokens and discourse connectives, nor does it explore their influence on reasoning
- What evidence would resolve it: In-depth analysis of learned continuous prompt tokens, including similarity to discourse connectives and impact on model's reasoning in abductive tasks

### Open Question 3
- Question: How does the performance of α-PACE compare to other state-of-the-art models in terms of abductive reasoning on different narrative tasks beyond αNLI?
- Basis in paper: [explicit] The paper evaluates α-PACE on the αNLI task and compares its performance to other models
- Why unresolved: The paper focuses on αNLI task and doesn't explore model's performance on other narrative tasks requiring abductive reasoning
- What evidence would resolve it: Comprehensive evaluation of α-PACE on various narrative tasks, comparing performance to other state-of-the-art models in abductive reasoning

## Limitations
- Significant computational cost requiring training 6 models per instance for majority voting
- Method specifically designed for abductive reasoning tasks and may not transfer to other NLI tasks
- Performance evaluation limited to ART dataset from αNLI benchmark without testing on external datasets

## Confidence
- **High Confidence**: 92.54% test accuracy outperforming fine-tuned T5 by 4.15%; self-consistent framework improves performance; discourse connectives enhance narrative understanding
- **Medium Confidence**: Self-consistency mechanism works through shared contextual embeddings; multiple narrative sequences capture different perspectives; continuous prompt tokens effectively learn discourse coherence
- **Low Confidence**: Exact initialization strategy for continuous prompt tokens is optimal; specific discourse connectives used are optimal; improvements generalize to other abductive reasoning datasets

## Next Checks
1. Test α-PACE on additional abductive reasoning datasets (e.g., Abductive Commonsense Reasoning dataset) to verify generalization beyond ART dataset
2. Systematically vary the number of continuous prompt tokens to determine optimal configuration and identify performance degradation points
3. Conduct ablation study removing individual narrative sequences to quantify marginal contribution of each sequence permutation