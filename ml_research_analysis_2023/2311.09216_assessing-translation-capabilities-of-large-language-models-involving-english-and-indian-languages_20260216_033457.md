---
ver: rpa2
title: Assessing Translation capabilities of Large Language Models involving English
  and Indian Languages
arxiv_id: '2311.09216'
source_url: https://arxiv.org/abs/2311.09216
tags:
- lora
- language
- translation
- multi
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the multilingual capabilities of large language
  models for machine translation between English and 22 Indian languages. It evaluates
  raw LLMs, in-context learning, and fine-tuned models using LoRA and full fine-tuning.
---

# Assessing Translation capabilities of Large Language Models involving English and Indian Languages

## Quick Facts
- arXiv ID: 2311.09216
- Source URL: https://arxiv.org/abs/2311.09216
- Reference count: 14
- This work explores the multilingual capabilities of large language models for machine translation between English and 22 Indian languages.

## Executive Summary
This paper evaluates the effectiveness of large language models (LLMs) for machine translation between English and 22 Indian languages. It compares raw LLMs, in-context learning, and various fine-tuning approaches using LoRA and full fine-tuning on the Bharat Parallel Corpus Collection. The two-stage fine-tuning approach combining full fine-tuning and LoRA yields the strongest results, with LLaMA-13B achieving average BLEU scores of 13.42–16.65 and chrF scores of 36.71–46.99 across multiple benchmark datasets. Fine-tuned models significantly outperform GPT-3.5, particularly for low-resource languages.

## Method Summary
The study evaluates translation capabilities of LLMs for 22 Indian languages using three approaches: raw LLMs with direct translation prompts, in-context learning (ICL), and fine-tuned models. Fine-tuning employs both LoRA-based multilingual fine-tuning and a two-stage approach combining full fine-tuning followed by LoRA fine-tuning. The models are trained on the Bharat Parallel Corpus Collection (BPCC-Human) with approximately 2.2M English-Indic parallel sentences. Evaluation is conducted on five benchmark datasets using BLEU and chrF metrics, with LLaMA-2-13B serving as the primary model.

## Key Results
- LLaMA-13B fine-tuned with the two-stage approach achieved average BLEU scores of 13.42–16.65 and chrF scores of 36.71–46.99 across benchmark datasets.
- Fine-tuned models outperformed GPT-3.5 and showed improved performance on low-resource languages.
- The two-stage fine-tuning approach combining full fine-tuning and LoRA yielded the strongest results among all evaluated methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLaMA-2 models on multilingual parallel data significantly improves translation quality over zero-shot or in-context learning baselines.
- Mechanism: The large pre-trained model already contains strong language understanding, but fine-tuning adapts the weights specifically to the translation task, optimizing token-level mappings across English and Indian languages.
- Core assumption: Parallel data of sufficient quality and quantity (here ~2.2M sentences) can align the model's semantic representations with correct target language forms.
- Evidence anchors:
  - [abstract] "Fine-tuned models outperform GPT-3.5 and show improved performance on low-resource languages."
  - [section 7] "We conducted an evaluation to compare the performance of our Fine-Tuned LLM models with GPT-3.5… The scores for GPT-3.5 are generally lower compared to our fine-tuned methods."
  - [corpus] Weak—corpus only shows related MT papers, no fine-tuning data statistics.
- Break Condition: If the parallel data is noisy or domain-mismatched, fine-tuning may degrade performance or lead to hallucination.

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) combined with full fine-tuning in a two-stage process yields the best results.
- Mechanism: Full fine-tuning first adjusts all model parameters to the translation task; LoRA then applies a lightweight, task-specific parameter adjustment to further refine performance without losing general knowledge.
- Core assumption: Full fine-tuning captures broad translation patterns, while LoRA adapts them efficiently for multilingual nuance.
- Evidence anchors:
  - [abstract] "A two-stage fine-tuning approach combining full fine-tuning and LoRA yields strong results, with LLaMA-13B achieving average BLEU scores of 13.42–16.65 and chrF scores of 36.71–46.99."
  - [section 7] "The two-stage fine-tuning approach also outperformed other fine-tuning methods for the translation task."
  - [corpus] Weak—no direct citations of LoRA efficacy in multilingual MT.
- Break Condition: If the full fine-tuning stage overfits, the subsequent LoRA stage may not recover lost generalization.

### Mechanism 3
- Claim: Model vocabulary coverage of target script characters strongly influences translation accuracy, especially for low-resource Indian languages.
- Mechanism: Subword tokenization must represent language-specific characters; missing characters lead to inaccurate byte-level tokenization and loss of semantic clarity.
- Core assumption: Indian languages have unique scripts with limited representation in LLM vocabularies; without them, models must approximate or hallucinate.
- Evidence anchors:
  - [section 4] "We observe that, in general, the 22 Indian languages have a limited presence in most of the LLMs… Devanagari, Perso-Arabic, and Bangla scripts demonstrate the most extensive sub-word vocabularies."
  - [section 5.1] "For less-represented languages such as Gujarati, Kannada, Odia… the ICL-driven translation tends to repeat the same translation given in the context as learning."
  - [corpus] Weak—no quantitative coverage analysis provided.
- Break Condition: If vocabulary coverage improves, these translation errors should reduce.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Allows adaptation of large models (e.g., 13B parameters) with limited GPU memory and training time.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter modification?

- Concept: BLEU and chrF evaluation metrics
  - Why needed here: Quantifies translation quality; chrF is more robust for morphologically rich Indian languages.
  - Quick check question: What is the main difference between BLEU and chrF scoring?

- Concept: Subword tokenization and vocabulary coverage
  - Why needed here: Directly affects model's ability to handle language-specific characters and avoid tokenization errors.
  - Quick check question: Why might missing script characters in the vocabulary hurt translation performance?

## Architecture Onboarding

- Component map: Base LLM (e.g., LLaMA-2-13B) -> PEFT wrapper (LoRA, full fine-tuning) -> Prompt generator -> vLLM inference -> Evaluation (BLEU/chrF)
- Critical path: Data preparation -> Model fine-tuning (full -> LoRA) -> Prompt engineering -> Inference on test sets -> Metric calculation
- Design tradeoffs: Memory usage vs. performance (full fine-tuning uses more memory but gives higher BLEU); inference speed vs. translation quality (temperature=0 for deterministic output)
- Failure signatures: Degraded BLEU/chrF scores; repeated or hallucinated translations; inability to generate target script correctly
- First 3 experiments:
  1. Compare zero-shot vs. in-context learning outputs on a held-out dev set to confirm baseline performance.
  2. Run full fine-tuning on a small subset of the parallel corpus to check for overfitting.
  3. Apply LoRA fine-tuning to the fully fine-tuned model and measure metric improvements on the same dev set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in multilingual translation scale with model size and available parallel data?
- Basis in paper: [explicit] The paper mentions that LLaMA-2-13b outperformed other models and achieved significant improvements through fine-tuning, but it doesn't explore the relationship between model size, data availability, and performance systematically.
- Why unresolved: The study focused on a specific set of models and datasets, limiting the ability to generalize findings across different model sizes and data regimes.
- What evidence would resolve it: Comparative studies across multiple model sizes (e.g., 7B, 13B, 30B, 65B) with varying amounts of parallel data for each language pair.

### Open Question 2
- Question: What are the specific linguistic phenomena that cause difficulties for LLMs in translating low-resource Indian languages?
- Basis in paper: [inferred] The paper notes that languages like Dogri, Konkani, Kashmiri, Meitei, Sanskrit, and Sindhi performed better than expected, suggesting that certain linguistic features may be more amenable to LLM-based translation.
- Why unresolved: The study doesn't provide a detailed linguistic analysis of why certain languages performed better than others.
- What evidence would resolve it: In-depth linguistic analysis comparing the grammatical structures, morphology, and other features of high-performing and low-performing languages.

### Open Question 3
- Question: How does the vocabulary representation of Indian languages in LLMs impact translation quality, and what are the best methods to improve it?
- Basis in paper: [explicit] The paper highlights that the limited subword vocabulary for Indian languages in LLMs is a bottleneck for translation quality, particularly for Indian-to-English translation.
- Why unresolved: The study doesn't explore specific methods to improve vocabulary representation or evaluate their effectiveness.
- What evidence would resolve it: Comparative studies of different vocabulary expansion techniques (e.g., transliteration, subword segmentation, character-level models) and their impact on translation quality.

## Limitations

- Corpus and data quality verification is missing; no direct validation of the parallel data's noise level or domain distribution is provided.
- Vocabulary coverage claims lack quantitative analysis or explicit data on character-level vocabulary overlap.
- Prompt formats and in-context learning sampling strategies are not fully specified, limiting faithful reproduction.

## Confidence

- High Confidence: The claim that fine-tuned LLaMA-2 models outperform GPT-3.5 and improve on low-resource languages is well-supported by direct comparison results reported in the paper (Section 7).
- Medium Confidence: The assertion that a two-stage fine-tuning approach (full fine-tuning followed by LoRA) yields the best results is supported by reported BLEU/chrF scores but lacks extensive ablation studies or citations to LoRA effectiveness in multilingual MT.
- Low Confidence: The mechanism linking vocabulary coverage gaps to poor translation accuracy for low-resource languages is plausible but under-supported by quantitative evidence or direct analysis.

## Next Checks

1. Measure the actual overlap between the LLM tokenizer's vocabulary and the character sets used in each of the 22 Indian languages, then correlate this with translation performance to validate the claimed mechanism.
2. Run controlled experiments comparing single-stage full fine-tuning, single-stage LoRA, and the two-stage approach on a held-out dev set to confirm that the two-stage method is indeed superior.
3. Implement the exact prompt templates and in-context learning sampling strategy as described in the paper (or as closely as possible) and verify that baseline zero-shot and ICL results match those reported.