---
ver: rpa2
title: Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded
  Spatiotemporal Data
arxiv_id: '2308.07940'
source_url: https://arxiv.org/abs/2308.07940
tags:
- time
- individual
- location
- trajectories
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a generative model for individual daily trajectories
  using the GPT-2 architecture, incorporating both spatial locations and temporal
  intervals. By encoding geographical coordinates into unique tokens and adding time
  interval tokens, the model generates realistic trajectories while capturing the
  variability of travel durations.
---

# Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data

## Quick Facts
- arXiv ID: 2308.07940
- Source URL: https://arxiv.org/abs/2308.07940
- Reference count: 29
- One-line primary result: Developed a generative model for individual daily trajectories using GPT-2 architecture with spatial and temporal encoding, achieving ~20% accuracy in predicting next location within 3 km over 8 hours.

## Executive Summary
This study presents a novel approach to generating individual daily trajectories using a GPT-2 architecture trained from scratch on encoded spatiotemporal data. The model converts geographical coordinates into unique tokens using the JIS X 0410 grid code and incorporates time interval tokens to capture temporal patterns in human mobility. By integrating special tokens for environmental factors (weather, day of week) and personal attributes (gender, age), the model conditions trajectory generation on contextual information. The approach outperforms baseline Markov chain models in location prediction accuracy and generates realistic time intervals for stops and travel.

## Method Summary
The method converts latitude and longitude coordinates into unique location tokens using the JIS X 0410 hierarchical grid code system, with 5 levels of spatial resolution down to 250m. Time intervals are discretized using log1.5 scaling and assigned unique character tokens. Environmental factors and personal attributes are encoded as special tokens prepended to each trajectory sequence. A GPT-2 model with 12 attention heads, 12 transformer layers, and 768 dimensions is trained from scratch on these tokenized sequences to generate realistic individual daily trajectories autoregressively.

## Key Results
- Achieved approximately 20% accuracy in predicting the next location within 3 km over 8 hours, outperforming first- and second-order Markov chain models
- Generated time intervals with lower mean absolute logarithmic errors than autoregressive baseline models
- Integration of environmental factors and personal attributes improved prediction accuracy by up to 3% compared to baseline GPT-2 model without these features
- Attention analysis revealed that environmental and personal attribute tokens receive increasing weight in later model layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical spatial encoding enables the model to generalize across spatial scales and handle unknown locations
- Mechanism: The 5-level grid code system maps any latitude/longitude to a unique token sequence, allowing the transformer to learn spatial relationships at multiple resolutions
- Core assumption: The hierarchical spatial encoding preserves meaningful geographic proximity relationships in the token space
- Evidence anchors: JIS X 0410 grid code transformation; tokenization of location sequences

### Mechanism 2
- Claim: Time interval tokens enable learning and generation of realistic temporal patterns in human mobility
- Mechanism: Treating time intervals as discrete tokens allows the model to learn variable-duration stops rather than assuming fixed intervals
- Core assumption: Discretizing time intervals into character tokens preserves essential temporal patterns while making them learnable
- Evidence anchors: Log1.5 discretization of time intervals; lower MALE compared to AR(3) baseline

### Mechanism 3
- Claim: Special tokens for environmental factors and individual attributes significantly improve trajectory prediction accuracy
- Mechanism: Condition generation on contextual factors through attention mechanisms, with these tokens receiving increasing weight in later layers
- Core assumption: Environmental factors and personal attributes have measurable influence on human mobility patterns
- Evidence anchors: 3% improvement with special tokens; attention weight analysis showing increasing weights in later layers

## Foundational Learning

- Concept: Tokenization and Byte Pair Encoding
  - Why needed here: Converts continuous geographic coordinates and time intervals into discrete tokens processable by transformer architecture
  - Quick check question: How does Byte Pair Encoding help handle the potentially large vocabulary of location tokens while keeping model size manageable?

- Concept: Attention mechanisms in transformers
  - Why needed here: Learns dependencies between current location/time and past locations, environmental factors, and individual attributes across different time scales
  - Quick check question: Why do attention weights for environmental and individual attributes increase in later layers according to attention analysis?

- Concept: Autoregressive generation
  - Why needed here: Generates trajectories sequentially, predicting next location and time interval based on all previous tokens
  - Quick check question: How does the autoregressive nature of GPT-2 enable the model to handle variable-length trajectories?

## Architecture Onboarding

- Component map: Tokenizer -> GPT-2 model -> Output decoder
- Critical path: Tokenization → GPT-2 generation → Decoding → Evaluation
- Design tradeoffs:
  - Spatial resolution vs. token vocabulary size: 250m resolution balances detail with manageable token space
  - Time interval discretization: log1.5 scaling captures both short and long intervals without excessive tokens
  - Model complexity: GPT-2 SMALL chosen for reasonable training time while maintaining accuracy
- Failure signatures:
  - Poor location accuracy despite good training loss: May indicate overfitting to training spatial patterns
  - Generated trajectories don't return home: Suggests model isn't learning daily routine patterns
  - Time intervals don't match real distributions: Indicates time tokenization isn't capturing temporal patterns
- First 3 experiments:
  1. Generate trajectories with random initial locations and verify they return close to starting point
  2. Compare prediction accuracy (within 3km) at different time horizons (1hr, 4hr, 8hr) against baseline Markov models
  3. Test attention visualization by feeding trajectories with different environmental attributes and observing attention weight changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of environmental factors and personal attributes influence the generation of realistic collective trajectories in crowded environments?
- Basis in paper: The authors propose future research to generate collective trajectories that account for interactions between individuals
- Why unresolved: Current model omits dynamics of crowd movement and congestion
- What evidence would resolve it: Empirical data on collective trajectories in crowded environments, validated against proposed model's predictions

### Open Question 2
- Question: What is the optimal spatial resolution for encoding geographical coordinates into tokens to balance model complexity and prediction accuracy?
- Basis in paper: Study uses 250-meter resolution but does not explore impact of different resolutions on model performance
- Why unresolved: Choice of spatial resolution is critical for capturing detailed movement patterns while maintaining computational efficiency
- What evidence would resolve it: Comparative analysis of model performance across various spatial resolutions

### Open Question 3
- Question: How does the inclusion of additional environmental factors, such as real-time traffic data or air quality indices, affect the accuracy of trajectory predictions?
- Basis in paper: Authors mention incorporating environmental factors like weather and COVID-19 case counts, suggesting potential for additional factors
- Why unresolved: Study does not explore impact of real-time data on prediction accuracy
- What evidence would resolve it: Testing model with real-time environmental data and comparing prediction accuracy to static models

## Limitations
- Geographical specificity limits generalizability to other urban contexts with different mobility patterns
- Moderate prediction accuracy (20% within 3km) suggests room for improvement in handling mobility variability
- JIS X 0410 grid system may not translate optimally to regions with different spatial characteristics

## Confidence

*High Confidence:* Technical implementation of GPT-2 architecture for trajectory generation, including hierarchical spatial encoding and time interval discretization mechanisms; attention analysis demonstrating conditioning effectiveness

*Medium Confidence:* Outperformance of Markov chain models by approximately 20% accuracy within 3 km; 3% improvement from incorporating environmental factors and personal attributes

*Low Confidence:* Generalizability to different geographical contexts and population demographics; optimality of log1.5 time interval discretization strategy

## Next Checks

1. **Cross-city validation:** Train and evaluate the same model architecture on trajectory data from a distinctly different urban environment (e.g., European city) to assess geographical generalizability

2. **Ablation study on discretization:** Systematically vary time interval discretization parameters (base of logarithm, number of bins) and spatial resolution levels to identify optimal settings for different mobility contexts

3. **Longitudinal consistency test:** Generate trajectories for multiple consecutive days and measure model's ability to maintain realistic daily routines, including returning home patterns and temporal consistency of activities across days