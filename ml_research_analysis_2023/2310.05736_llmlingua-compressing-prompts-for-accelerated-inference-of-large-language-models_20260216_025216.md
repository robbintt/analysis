---
ver: rpa2
title: 'LLMLingua: Compressing Prompts for Accelerated Inference of Large Language
  Models'
arxiv_id: '2310.05736'
source_url: https://arxiv.org/abs/2310.05736
tags:
- prompt
- compression
- language
- compressed
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LLMLingua, a method to compress prompts
  for large language models (LLMs) to reduce inference cost and time. LLMLingua uses
  a coarse-to-fine approach: first allocating different compression ratios to prompt
  components like instructions and demonstrations, then applying iterative token-level
  compression while preserving key information.'
---

# LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models

## Quick Facts
- arXiv ID: 2310.05736
- Source URL: https://arxiv.org/abs/2310.05736
- Reference count: 40
- Key outcome: Achieves up to 20x prompt compression with minimal performance loss across four datasets

## Executive Summary
This paper introduces LLMLingua, a method to compress prompts for large language models (LLMs) to reduce inference cost and time. LLMLingua uses a coarse-to-fine approach: first allocating different compression ratios to prompt components like instructions and demonstrations, then applying iterative token-level compression while preserving key information. The method also aligns the distributions of the small compression model and the target LLM via instruction tuning. Experiments on four datasets (GSM8K, BBH, ShareGPT, Arxiv-March23) show that LLMLingua achieves up to 20x compression with little performance loss compared to full-length prompts, outperforming prior methods.

## Method Summary
LLMLingua employs a coarse-to-fine prompt compression method that involves three main components: a budget controller, iterative token-level compression algorithm, and distribution alignment. The budget controller dynamically allocates compression ratios to different prompt components (instruction, demonstrations, question) based on their importance. The iterative token-level compression algorithm then refines this by considering conditional dependencies between tokens within segments. Finally, distribution alignment via instruction tuning is applied to align the small LM's output distribution with the target LLM's distribution, improving token selection accuracy. This approach enables high compression ratios while maintaining semantic integrity and task performance.

## Key Results
- Achieves up to 20x compression ratio across four datasets (GSM8K, BBH, ShareGPT, Arxiv-March23)
- Maintains minimal performance loss compared to full-length prompts
- Outperforms prior methods including Selective-Context and K-Nearest Neighbor
- Distribution alignment improves performance by 0.56 EM points on GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity-guided token selection effectively preserves essential information in compressed prompts.
- Mechanism: Uses a small language model to compute perplexity for each token or segment. Tokens with higher perplexity are retained as they are more informative for the target LLM's understanding.
- Core assumption: Tokens with higher perplexity contain more task-relevant information for the black-box LLM.
- Evidence anchors:
  - [abstract]: "Motivated by this, Li (2023) propose Selective-Context, which first employs a small language model to compute the self-information of each lexical unit... and then drops the less informative content for prompt compression."
  - [section 4.2]: "we use the smaller model Mùë† to obtain the perplexity distribution of all segments."
- Break condition: If the small LM's perplexity distribution poorly aligns with the target LLM's information needs, critical tokens might be dropped.

### Mechanism 2
- Claim: Coarse-to-fine compression maintains semantic integrity under high compression ratios.
- Mechanism: First allocates different compression ratios to prompt components based on their relative importance, then applies iterative token-level compression considering conditional dependencies.
- Core assumption: Different prompt components have varying sensitivity to compression, and demonstration-level filtering preserves core reasoning chains.
- Evidence anchors:
  - [abstract]: "LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm..."
  - [section 4.1]: "we first present a budget controller to dynamically allocate different compression ratios to various components in original prompts such as the instruction, demonstrations, and the question..."
- Break condition: If budget allocation is incorrect or iterative compression fails to capture token interdependencies, semantic information could be lost.

### Mechanism 3
- Claim: Distribution alignment between small LM and target LLM improves compression quality.
- Mechanism: Instruction tuning aligns the small LM's output distribution with the target LLM's distribution using data generated by the target LLM.
- Core assumption: The small LM's distribution must approximate the target LLM's distribution for effective token selection.
- Evidence anchors:
  - [section 4.3]: "we align the two distributions via instruction tuning... The optimization of Mùë† can be formulated as: min Œ∏ùë† E [1/N Œ£·µ¢ L(x·µ¢, y·µ¢,LLM; Œ∏Mùë†)]"
  - [section 5.4]: "Distribution Alignment allows small LMs to generate distributions that more closely resemble those of target LLMs, resulting in a further improvement of 0.56 on GSM8K."
- Break condition: If instruction tuning data is insufficient or small LM cannot adequately approximate target LLM's distribution, alignment will fail.

## Foundational Learning

- Concept: Perplexity as a measure of token informativeness
  - Why needed here: The method relies on perplexity scores to identify and retain informative tokens while compressing prompts.
  - Quick check question: Why would a token with high perplexity be more important to retain than one with low perplexity?
  - Answer: High perplexity indicates the token is less predictable by the model, suggesting it carries more unique information critical for understanding the prompt.

- Concept: Conditional probability in language models
  - Why needed here: The iterative compression algorithm estimates p(eùë† ùëó) considering previous compressed tokens, addressing the independence assumption limitation.
  - Quick check question: How does considering p(eùë† ùëó) with previous compressed tokens improve upon simple token-by-token perplexity filtering?
  - Answer: It accounts for the interdependence between tokens, preserving contextual relationships that single-token filtering might miss.

- Concept: Instruction tuning for distribution alignment
  - Why needed here: Aligns the small LM's output distribution with the target LLM's distribution to improve token selection accuracy.
  - Quick check question: What is the purpose of using LLM-generated data to fine-tune the small LM in this context?
  - Answer: To make the small LM's predictions more closely match how the target LLM would process similar prompts, improving the quality of compression.

## Architecture Onboarding

- Component map: Budget Controller -> Iterative Token-level Compression (ITPC) -> Distribution Alignment -> Target LLM
- Critical path:
  1. Compute initial perplexity scores for all tokens
  2. Apply budget controller to filter demonstrations and allocate remaining budget
  3. Apply ITPC to iteratively compress tokens while considering conditional dependencies
  4. Generate output from target LLM using compressed prompt
- Design tradeoffs:
  - Higher compression ratio vs. performance degradation
  - Small LM quality vs. computational overhead for distribution alignment
  - Segment size in ITPC vs. accuracy of conditional probability estimation
- Failure signatures:
  - Performance drops when compression ratio exceeds certain threshold
  - Inconsistent results across different target LLMs
  - Degradation in complex reasoning tasks compared to simple tasks
- First 3 experiments:
  1. Test budget controller alone with fixed compression ratio to verify demonstration-level filtering effectiveness
  2. Test ITPC with perfect budget allocation to measure improvement from conditional probability consideration
  3. Test full pipeline with different small LMs (Alpaca-7B vs GPT2-Alpaca) to measure impact of distribution alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of compression ratio that maintains acceptable performance across different task types?
- Basis in paper: [explicit] The paper mentions "we might observe a notable performance drop when trying to achieve excessively high compression ratios such as 25x-30x on GSM8K" and notes that "The upper limit of the compression ratio for different prompts varies, depending on factors such as prompt length, task type, and the number of sentences involved."
- Why unresolved: The paper provides some examples of compression limits but doesn't establish a systematic relationship between compression ratio limits and task characteristics.
- What evidence would resolve it: A comprehensive study testing different compression ratios across multiple task types with varying prompt lengths and complexities, identifying clear thresholds where performance degradation becomes unacceptable.

### Open Question 2
- Question: How does the distribution alignment technique scale when the gap between small LM and target LLM becomes very large?
- Basis in paper: [explicit] The paper states "Even with distribution alignment, it is still difficult to directly estimate the target LLM using the distribution from the small language model" and shows that "the results obtained by Alpaca finetuned GPT2-small are weaker than those obtained by Alpaca-7B with a performance drop of 2.06, 0.99, and 1.06 EM points at different compression ratios."
- Why unresolved: The paper demonstrates some limitations of distribution alignment but doesn't explore the full extent of what happens when the model gap is very large.
- What evidence would resolve it: Experiments comparing alignment effectiveness across a wide range of model size differences and capabilities, potentially including much smaller LMs or non-LLM models.

### Open Question 3
- Question: What is the relationship between the computational overhead of LLMLingua and the achieved compression ratio?
- Basis in paper: [explicit] The paper provides a formula for computational cost and mentions "c ‚âà 0.264 ¬∑ L¬∑cLLMs ‚âà 1/4 ¬∑ L¬∑cLLMs" when ùúè = 5, but doesn't explore how this relationship varies across different compression ratios.
- Why unresolved: The paper only analyzes one specific compression ratio and doesn't provide a complete picture of how overhead scales with compression.
- What evidence would resolve it: A systematic analysis of computational overhead at various compression ratios (1x, 2x, 5x, 10x, etc.) showing the trade-off between compression gains and computational costs.

## Limitations
- Limited evaluation scope with only four datasets, raising questions about generalizability to other domains and task types
- Lack of comprehensive analysis of edge cases such as prompts with specialized terminology or multi-modal inputs
- Absence of qualitative analysis to understand what types of information are lost during compression

## Confidence
- High Confidence: The coarse-to-fine compression framework is well-defined and the experimental methodology is sound with reproducible results
- Medium Confidence: The effectiveness of distribution alignment through instruction tuning is demonstrated but relies on limited ablation evidence with variable improvements across datasets
- Low Confidence: Claims about "outperforming prior methods" are based on comparisons with only two previous approaches on a subset of tasks without establishing advantages against newer methods

## Next Checks
1. **Cross-domain generalization test**: Apply LLMLingua to prompts from completely different domains (e.g., medical diagnosis, legal document analysis, code generation) not represented in the original four datasets to verify if the 20x compression ratio with minimal performance loss holds across diverse contexts.

2. **Robustness under distribution shift**: Evaluate compression quality when target LLMs are fine-tuned on domain-specific data or when prompts contain out-of-distribution examples to reveal whether distribution alignment is robust to shifts between training data and real-world usage.

3. **Human evaluation of semantic preservation**: Conduct human studies where evaluators compare compressed vs. full-length prompts for task comprehension and information completeness, particularly for high-stakes applications where automated metrics may not capture semantic fidelity accurately.