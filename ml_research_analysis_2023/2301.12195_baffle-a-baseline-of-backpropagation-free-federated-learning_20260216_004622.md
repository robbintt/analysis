---
ver: rpa2
title: 'BAFFLE: A Baseline of Backpropagation-Free Federated Learning'
arxiv_id: '2301.12195'
source_url: https://arxiv.org/abs/2301.12195
tags:
- learning
- baffle
- federated
- clients
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BAFFLE, a backpropagation-free federated learning
  framework. It replaces gradient computation with multiple forward passes using finite
  differences and Stein's identity to estimate gradients.
---

# BAFFLE: A Baseline of Backpropagation-Free Federated Learning

## Quick Facts
- **arXiv ID**: 2301.12195
- **Source URL**: https://arxiv.org/abs/2301.12195
- **Reference count**: 40
- **Primary result**: Backpropagation-free FL framework achieving 48-95% accuracy on benchmark datasets

## Executive Summary
BAFFLE introduces a novel backpropagation-free federated learning framework that replaces gradient computation with multiple forward passes using finite differences and Stein's identity. The method enables training on inference-only hardware and trusted execution environments while maintaining privacy through secure aggregation. BAFFLE achieves competitive performance on standard datasets like MNIST and CIFAR, with accuracy ranging from 48% to 95% depending on model and dataset complexity.

## Method Summary
BAFFLE replaces traditional backpropagation with gradient estimation via multiple forward passes. It applies Gaussian perturbations to model parameters and uses Stein's identity to estimate gradients from loss differences. The framework employs secure aggregation to protect client privacy and operates efficiently in memory-constrained environments by executing layer-by-layer inference. Training proceeds through server-client communication rounds where clients compute perturbed forward passes and upload aggregated loss differences.

## Key Results
- Achieves 95.04% accuracy on MNIST with LeNet
- Reaches 48.11% accuracy on CIFAR-100 with WideResNet-10-2
- Reduces memory usage to 5-10% compared to traditional FL
- Compatible with inference-only hardware and TEEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stein's identity allows unbiased gradient estimation using only forward passes.
- Mechanism: By applying Stein's identity to a smoothed loss function with Gaussian perturbations, the gradient can be estimated as an expectation over loss differences from perturbed forward passes.
- Core assumption: The loss function is continuously differentiable and the perturbation distribution is Gaussian with small variance.
- Evidence anchors:
  - [abstract] "we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients"
  - [section] "We define a smoothed loss function as: Lσ(W; D) := Eδ∼N (0,σ2I)L(W + δ; D)" and "Stein (1981) proves the Stein's identity as (we recap the proof in Appendix A)"
  - [corpus] No direct evidence, but related work on backpropagation-free methods suggests this is plausible
- Break condition: If the loss function is not differentiable or perturbations are too large, the gradient estimate becomes biased.

### Mechanism 2
- Claim: Secure aggregation protects client privacy while enabling gradient estimation.
- Mechanism: Clients add zero-sum noise to their loss differences before uploading, allowing the server to recover the sum of loss differences without learning individual client contributions.
- Core assumption: The zero-sum noise negotiation protocol is correctly implemented and all clients participate honestly.
- Evidence anchors:
  - [abstract] "BAFFLE is also compatible with recent advances in inference approaches for TEE (Tramer & Boneh, 2019; Truong et al., 2021), providing an efficient solution for combining TEE into FL and preventing white-box evasion"
  - [section] "We adopt the secure aggregation method (Bonawitz et al., 2017a) that was originally proposed for conventional FL and apply it to BAFFLE"
  - [corpus] No direct evidence, but secure aggregation is a well-established technique in FL
- Break condition: If the noise negotiation is compromised or clients collude, privacy guarantees fail.

### Mechanism 3
- Claim: Memory efficiency enables deployment on resource-constrained devices.
- Mechanism: By using only forward passes and avoiding backpropagation, BAFFLE eliminates the need to store intermediate activations, drastically reducing memory requirements.
- Core assumption: The forward computation can be executed layer-by-layer without requiring the full computation graph in memory.
- Evidence anchors:
  - [abstract] "BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning"
  - [section] "BAFFLE can execute in TEE due to its little memory cost" and "BAFFLE reduces the memory cost to 5% ~ 10% by executing layer-by-layer inference"
  - [corpus] No direct evidence, but inference-only hardware optimization is a known technique
- Break condition: If the layer-by-layer execution introduces significant overhead or the model architecture prevents efficient splitting, memory benefits may be reduced.

## Foundational Learning

- Concept: Zero-order optimization
  - Why needed here: BAFFLE replaces gradient computation with finite difference approximations, which is a form of zero-order optimization
  - Quick check question: What is the key difference between zero-order and first-order optimization methods?

- Concept: Secure multi-party computation
  - Why needed here: The secure aggregation protocol requires techniques from secure multi-party computation to ensure privacy
  - Quick check question: How does secure aggregation prevent the server from learning individual client contributions?

- Concept: Trusted execution environments
  - Why needed here: BAFFLE's memory efficiency makes it compatible with TEEs, which have strict memory constraints
  - Quick check question: What are the typical memory constraints of TEEs and why do they matter for federated learning?

## Architecture Onboarding

- Component map: Server (model parameters, random seed generation, gradient aggregation) -> Clients (local dataset, perturbation generation, forward computation) -> TEE module (optional, for secure execution)
- Critical path: Server sends model parameters and random seeds → Clients generate perturbations and compute loss differences → Clients upload noisy loss differences → Server aggregates and updates model
- Design tradeoffs: Memory vs. computation (layer-by-layer execution saves memory but may be slower), Privacy vs. accuracy (more noise for privacy reduces gradient accuracy), Communication rounds vs. computation per round (more perturbations improve accuracy but increase computation)
- Failure signatures: Poor accuracy (too few perturbations or too much noise), Communication failures (clients unable to upload due to bandwidth constraints), Privacy breaches (secure aggregation compromised)
- First 3 experiments:
  1. Run BAFFLE on MNIST with a small LeNet model to verify basic functionality
  2. Compare memory usage of BAFFLE vs. backpropagation on the same model
  3. Test secure aggregation by simulating malicious clients trying to recover individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BAFFLE's performance scale with increasingly complex deep learning models beyond MobileNet?
- Basis in paper: [inferred] The paper tests BAFFLE on LeNet, WideResNet-10-2, and MobileNet, but only evaluates MobileNet in transfer learning scenarios with frozen backbones. The paper mentions MobileNet has 1.3 × 10^7 parameters, suggesting scalability concerns exist but aren't thoroughly explored.
- Why unresolved: The experiments focus on relatively small models (LeNet: 2.7 × 10^4 parameters, WRN-10-2: 3.0 × 10^5 parameters) and only test MobileNet in constrained transfer learning settings. The convergence rate analysis (O(√n/K)) suggests performance degrades with model size, but empirical validation on larger models is missing.
- What evidence would resolve it: Systematic evaluation of BAFFLE across a spectrum of model sizes (e.g., ResNet variants, Vision Transformers) showing accuracy degradation patterns and computational cost scaling with parameter count.

### Open Question 2
- Question: Can alternative perturbation distributions beyond Gaussian improve BAFFLE's gradient estimation efficiency?
- Basis in paper: [explicit] The paper states "Since the convergence rate of Gaussian perturbations is O(√n/K), the sampling efficiency may be improved by choosing an alternative distribution for perturbations" and mentions this as a future direction.
- Why unresolved: The paper only experiments with Gaussian noise (N(0,σ²I)) as specified by Stein's identity, despite acknowledging that other distributions might offer better convergence properties. No empirical comparison with alternatives like uniform, Laplace, or structured random perturbations is provided.
- What evidence would resolve it: Comparative experiments measuring gradient estimation variance and training convergence speed across multiple perturbation distributions while keeping computational cost constant.

### Open Question 3
- Question: What is the theoretical relationship between BAFFLE's noise injection and differential privacy guarantees?
- Basis in paper: [explicit] The paper states "Intuitively, this smoothness is related to differential privacy in FL, but determining their relationship requires theoretical derivations" and mentions this as an open research question.
- Why unresolved: While the paper draws an intuitive connection between the Gaussian noise in Stein's identity and differential privacy, it doesn't provide formal privacy guarantees or analyze the privacy-utility tradeoff inherent in BAFFLE's design.
- What evidence would resolve it: Mathematical framework establishing bounds on information leakage through BAFFLE's loss differences, and analysis of how privacy parameters (e.g., ε, δ) relate to the perturbation scale σ and number of samples K.

## Limitations

- Performance gap with traditional FL remains significant (48% vs 95% accuracy on CIFAR-100)
- Secure aggregation protocol implementation details are not fully specified
- Layer-by-layer execution computational overhead is not thoroughly quantified

## Confidence

- **High confidence**: Memory efficiency claims (well-supported by layer-by-layer execution analysis)
- **Medium confidence**: Privacy benefits (secure aggregation is established, but implementation details matter)
- **Low confidence**: Performance claims (accuracy results vary significantly across datasets and models)

## Next Checks

1. Implement a targeted collusion attack on the secure aggregation protocol to measure actual privacy leakage under realistic threat models
2. Benchmark the computational overhead of layer-by-layer execution compared to batch backpropagation across different model architectures
3. Conduct ablation studies varying K and σ to quantify the accuracy-memory tradeoff curve and identify optimal configurations for different use cases