---
ver: rpa2
title: Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games
arxiv_id: '2310.01468'
source_url: https://arxiv.org/abs/2310.01468
tags:
- celebrity
- gpt-4
- used
- game
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Entity-Deduction Arena (EDA), a benchmark
  for evaluating large language models' (LLMs) capabilities in strategic reasoning
  and planning through a multi-turn game where models must deduce an unknown entity
  by asking a series of yes/no/maybe questions. The authors systematically evaluate
  various LLMs including GPT-4, Claude, and Vicuna on two datasets (Things and Celebrities),
  finding significant differences in performance.
---

# Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games

## Quick Facts
- arXiv ID: 2310.01468
- Source URL: https://arxiv.org/abs/2310.01468
- Reference count: 17
- Key outcome: GPT-4 outperforms other models and human players in the Entity-Deduction Arena benchmark, demonstrating superior planning and reasoning abilities

## Executive Summary
This paper introduces the Entity-Deduction Arena (EDA), a benchmark for evaluating large language models' capabilities in strategic reasoning and planning through a multi-turn game where models must deduce an unknown entity by asking yes/no/maybe questions. The authors systematically evaluate various LLMs including GPT-4, Claude, and Vicuna on two datasets (Things and Celebrities), finding significant differences in performance. GPT-4 outperforms other models and human players, demonstrating superior planning and reasoning abilities. The study also explores whether smaller open-source models can benefit from imitating larger closed-source models through behavior cloning, finding significant improvements. Additionally, reinforcement learning from game play further enhances model performance, particularly for larger models like Vicuna 13B.

## Method Summary
The paper evaluates LLMs on their ability to play a 20 Questions game where they must deduce an unknown entity through strategic yes/no/maybe questioning. The benchmark uses two datasets (Things and Celebrities) with 30 evaluation items each. Models are tested as "guessers" with GPT-3.5-turbo serving as the "judge" providing responses. Performance is measured through success rate, number of turns, and a combined score. The authors conduct behavior cloning experiments by fine-tuning Vicuna models on game demonstrations from GPT-3.5-turbo, and use reinforcement learning (PPO) to further improve Vicuna models through self-play.

## Key Results
- GPT-4 achieves the highest success rate and lowest average turns across both evaluation datasets
- Behavior cloning significantly improves weaker models like Vicuna-7B, increasing success rate by 12.1%
- Reinforcement learning from game-play provides additional performance gains beyond behavior cloning
- Vicuna-13B outperforms Vicuna-7B, demonstrating the benefits of larger model sizes for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The entity-deduction game serves as an effective evaluation framework for probing multi-turn conversational reasoning and planning capabilities of LLMs.
- Mechanism: By requiring models to deduce an unknown entity through a series of yes/no/maybe questions, the game creates a controlled environment that isolates and tests the model's ability to track dialogue state, strategically plan questions, and reason deductively.
- Core assumption: The ability to play this game well correlates with the ability to handle complex, multi-turn conversational tasks in real-world applications.
- Evidence anchors:
  - [abstract]: "This entity-deducing game can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models."
  - [section 3.1]: Describes the experimental setup and evaluation metrics used to assess model performance.
  - [corpus]: The corpus includes papers on strategic reasoning in LLMs and multi-turn planning, supporting the relevance of this evaluation approach.
- Break condition: If the game environment doesn't accurately reflect real-world conversational complexity or if the evaluation metrics don't capture the full range of reasoning and planning skills.

### Mechanism 2
- Claim: Behavior Cloning (BC) can effectively transfer planning and reasoning capabilities from stronger to weaker models.
- Mechanism: By finetuning weaker models on game demonstrations from stronger models, the weaker models can learn effective strategies for question selection and entity deduction.
- Core assumption: The planning and reasoning strategies used by stronger models can be effectively captured and transferred through imitation learning.
- Evidence anchors:
  - [section 5.1]: Reports significant performance improvements in weaker models after BC finetuning.
  - [corpus]: The corpus includes papers on imitation learning and knowledge transfer in LLMs, supporting the feasibility of this approach.
- Break condition: If the weaker models can't generalize the learned strategies to new entities or if the transfer doesn't lead to improved reasoning and planning capabilities.

### Mechanism 3
- Claim: Reinforcement Learning from Game-Play (RLGP) can further enhance the reasoning and planning capabilities of LLMs.
- Mechanism: By training models to play the entity-deduction game against a judge, RLGP can improve the model's ability to strategically select questions and deduce entities through self-play.
- Core assumption: The reward structure of the game (fewer turns, higher success rate) provides a suitable signal for RL to improve reasoning and planning.
- Evidence anchors:
  - [section 5.2]: Reports performance improvements in RLGP-trained models compared to vanilla models.
  - [corpus]: The corpus includes papers on RLHF and PPO, supporting the applicability of these techniques to this task.
- Break condition: If the RL training doesn't lead to improved performance or if the model overfits to the specific game environment.

## Foundational Learning

- Concept: Multi-turn dialogue state tracking
  - Why needed here: To understand which questions have been asked and what information has been gathered, crucial for strategic planning in the entity-deduction game.
  - Quick check question: How does the model keep track of which attributes of the entity have been confirmed or ruled out based on previous questions?

- Concept: Strategic question selection
  - Why needed here: To efficiently narrow down the search space and deduce the entity in as few turns as possible.
  - Quick check question: What strategies can the model use to select questions that maximally partition the remaining possibilities?

- Concept: Deductive reasoning
  - Why needed here: To make an educated final guess based on all the information gathered during the game.
  - Quick check question: How does the model use the yes/no/maybe responses to eliminate possibilities and arrive at the correct entity?

## Architecture Onboarding

- Component map: Judge model -> Guesser model -> Evaluation metrics -> Training methods (BC/RLGP)

- Critical path:
  1. Judge receives entity and generates response to guesser's question
  2. Guesser receives dialogue history and generates next question or final guess
  3. Game continues until entity is guessed or maximum turns reached
  4. Performance is evaluated based on success rate and number of turns

- Design tradeoffs:
  - Model size vs. performance: Larger models tend to perform better but are more computationally expensive
  - Finetuning vs. prompting: Finetuning can improve performance but requires labeled data, while prompting is zero-shot but may be less effective
  - BC vs. RLGP: BC can quickly transfer knowledge but may not improve beyond the teacher model, while RLGP can potentially lead to better strategies but requires more training

- Failure signatures:
  - Early enumeration: Model starts guessing specific entities too early without narrowing down the search space
  - Redundancy: Model asks questions similar to previous ones, not efficiently gathering new information
  - Inconsistency: Model's final guess is inconsistent with the answers received during the game

- First 3 experiments:
  1. Benchmark a set of LLMs (e.g., GPT-4, Claude-2, Vicuna) on the entity-deduction game to establish a baseline
  2. Implement BC finetuning on a weaker model using demonstrations from a stronger model and evaluate performance
  3. Implement RLGP training on a model and compare performance to BC finetuned and vanilla models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies (e.g., Chain-of-Thought, zero-shot, few-shot) impact the entity deduction performance of LLMs?
- Basis in paper: Inferred from discussion of future research directions
- Why unresolved: The paper evaluates models using standard prompting but doesn't systematically explore different prompting techniques
- What evidence would resolve it: Comparative experiments measuring performance across various prompting strategies on the EDA benchmark

### Open Question 2
- Question: Can the entity deduction game framework be effectively extended to multimodal contexts where entities have both visual and textual characteristics?
- Basis in paper: Inferred from discussion of visual reasoning tasks in related work
- Why unresolved: The paper focuses solely on text-based entity deduction without exploring multimodal extensions
- What evidence would resolve it: Development and evaluation of a multimodal version of the EDA benchmark with corresponding model performance metrics

### Open Question 3
- Question: What is the relationship between model size and the ability to effectively plan and reason in multi-turn entity deduction games?
- Basis in paper: Explicit mention in RQ6 and discussion of model size effects
- Why unresolved: While the paper shows Vicuna 13B outperforms Vicuna 7B, it doesn't systematically study the scaling relationship
- What evidence would resolve it: Performance scaling analysis across multiple model sizes (e.g., 1B, 3B, 7B, 13B, 33B) with detailed breakdowns of planning vs reasoning capabilities at each scale

## Limitations
- Data contamination concerns due to small evaluation dataset and potential memorization effects
- Generalizability issues regarding how well results translate to real-world conversational scenarios
- Behavioral consistency uncertainties about whether performance differences reflect fundamental reasoning capabilities or artifacts of prompting strategies

## Confidence

**High Confidence**: GPT-4 demonstrates superior performance in the EDA benchmark compared to other evaluated models and human baselines, as evidenced by statistically significant improvements in success rate and number of turns across both evaluation datasets.

**Medium Confidence**: Behavior cloning effectively transfers planning and reasoning capabilities from stronger to weaker models, with Vicuna-7B showing 12.1% improvement in success rate. However, the extent of knowledge transfer may depend on the quality and diversity of demonstration data.

**Medium Confidence**: Reinforcement learning from game-play provides additional performance gains beyond behavior cloning, particularly for larger models like Vicuna-13B. The improvement magnitude suggests RLGP captures strategic patterns not fully represented in the demonstration data.

## Next Checks
1. Conduct significance testing across multiple runs with different random seeds to establish confidence intervals for performance metrics, particularly for the smaller improvements observed in behavior cloning and RLGP experiments.

2. Test the best-performing models on a third, held-out dataset of entities to verify that performance improvements generalize beyond the two evaluation domains and aren't artifacts of dataset-specific patterns.

3. Systematically vary judge model confidence thresholds and response patterns to understand how judge behavior influences guesser performance, helping isolate whether observed differences reflect genuine reasoning capabilities or sensitivity to judge characteristics.