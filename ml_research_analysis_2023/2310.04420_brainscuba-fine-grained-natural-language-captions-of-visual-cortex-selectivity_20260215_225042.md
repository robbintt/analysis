---
ver: rpa2
title: 'BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity'
arxiv_id: '2310.04420'
source_url: https://arxiv.org/abs/2310.04420
tags:
- images
- visual
- brain
- image
- brainscuba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BrainSCUBA, a data-driven method for generating
  fine-grained natural language captions that describe the visual stimuli that maximally
  activate individual voxels in higher-order visual cortex. The method builds on the
  embedding space of a contrastive vision-language model and utilizes a pre-trained
  large language model to generate interpretable captions.
---

# BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity

## Quick Facts
- **arXiv ID**: 2310.04420
- **Source URL**: https://arxiv.org/abs/2310.04420
- **Reference count**: 40
- **Primary result**: BrainSCUBA generates fine-grained natural language captions describing visual stimuli that maximally activate individual voxels in higher-order visual cortex, validated through semantic coherence and predicted activation strength.

## Executive Summary
This paper introduces BrainSCUBA, a method for generating interpretable natural language captions that describe the visual stimuli activating individual voxels in higher-order visual cortex. The approach builds on CLIP's vision-language embedding space, using a linear probe to predict voxel responses and a projection technique to bridge the gap between optimal embeddings and natural image embeddings. BrainSCUBA is validated through voxel-level captioning and text-conditioned image synthesis, demonstrating semantically coherent outputs that yield high predicted brain activations. The method enables exploratory investigations of fine-grained semantic selectivity in visual cortex, particularly revealing person-related representations in body-selective areas.

## Method Summary
BrainSCUBA uses a pre-trained CLIP model to encode images into embeddings, then trains voxel-wise linear probes to predict fMRI responses. For each voxel, the optimal embedding is derived from the linear weights, projected onto the space of natural image embeddings using a softmax-weighted sum, and used to generate captions with a fine-tuned GPT-2. The captions are validated through semantic alignment with known functional regions and used to synthesize images via text-to-image diffusion models, with the generated images validated by their predicted ability to activate the target voxels.

## Key Results
- BrainSCUBA generates semantically coherent captions for individual voxels that align with known functional regions
- Text-conditioned image synthesis using the captions produces images that are semantically consistent with their respective brain regions
- The method discovers fine-grained semantic selectivity in body-selective areas, particularly person-related representations not previously characterized

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The linear probe trained on CLIP embeddings can capture voxel-level semantic selectivity in higher visual cortex.
- **Mechanism**: The encoder architecture maps CLIP embeddings (M dimensions) to voxel activations (N dimensions) using a linear transformation. This linear mapping learns to represent each voxel's selectivity as a weight vector in the CLIP embedding space.
- **Core assumption**: Voxel responses to visual stimuli can be approximated as linear functions of CLIP embedding features.
- **Evidence anchors**:
  - [abstract] "builds upon the rich embedding space learned by a contrastive vision-language model"
  - [section] "we employ CLIP as our encoder backbone" and "evaluate the encoder on the test set...and find that our encoder can achieve high R2"
  - [corpus] Weak - no direct corpus evidence of linear probes capturing voxel selectivity, though related work exists on brain alignment
- **Break condition**: If voxel responses are highly nonlinear or involve complex interactions not captured by linear projections, the model would fail to accurately predict activations.

### Mechanism 2
- **Claim**: The softmax-weighted projection closes the modality gap between optimal CLIP embeddings and natural image embeddings.
- **Mechanism**: The optimal embedding for each voxel (derived from linear weights) rarely corresponds to actual CLIP embeddings of natural images. The softmax projection finds a weighted combination of natural image embeddings that best approximates the optimal embedding.
- **Core assumption**: The space of natural image CLIP embeddings contains sufficient diversity to approximate any optimal voxel embedding through weighted combinations.
- **Evidence anchors**:
  - [section] "To close this modality gap, we utilize a softmax weighted sum to project the voxel weights onto the space of natural images"
  - [section] "We measure the average cosine similarity between pre-/post-projection weights, and find it increases as the number of images used is increased"
  - [corpus] Weak - the projection technique is similar to approaches in caption generation but direct evidence of effectiveness for brain data is limited
- **Break condition**: If the natural image embedding space lacks sufficient diversity or if optimal embeddings lie outside the convex hull of natural image embeddings, the projection would fail to accurately approximate voxel preferences.

### Mechanism 3
- **Claim**: Text-conditioned image synthesis with BrainSCUBA captions produces semantically coherent images that activate targeted brain regions.
- **Mechanism**: The generated captions describe visual stimuli that maximally activate specific voxels. When used as prompts for text-to-image models, these descriptions guide synthesis toward images that are both semantically aligned with the captions and likely to activate the corresponding brain regions.
- **Core assumption**: Text-to-image models can faithfully translate semantic descriptions into visual content, and this content will activate the predicted brain regions.
- **Evidence anchors**:
  - [abstract] "We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations"
  - [section] "we use the captions as input to text-to-image diffusion models to generate novel images, and confirm the images are semantically consistent within their respective regions"
  - [corpus] Moderate - related work shows text-to-image models can generate category-specific content, but direct evidence of brain activation correlation is limited
- **Break condition**: If text-to-image models fail to capture the semantic nuances of captions or if the relationship between caption semantics and brain activation is weak, the generated images would not activate the predicted regions.

## Foundational Learning

- **CLIP model architecture and contrastive learning**
  - Why needed here: BrainSCUBA relies on CLIP's embedding space as the foundation for mapping between images and brain activations. Understanding how CLIP learns aligned visual and textual representations is crucial for grasping the method's design.
  - Quick check question: How does CLIP's contrastive loss function create meaningful alignment between visual and textual embedding spaces?

- **fMRI data preprocessing and GLM analysis**
  - Why needed here: The method requires understanding how brain responses are measured and preprocessed, including GLMSingle's role in computing beta values and the normalization procedures used.
  - Quick check question: Why is it necessary to normalize voxel responses to have μ = 0, σ² = 1 on a session basis before training the encoder?

- **Diffusion models for image generation**
  - Why needed here: The text-to-image synthesis component uses a diffusion model conditioned on generated captions. Understanding the mechanics of diffusion models and how they use text conditioning is essential for the full pipeline.
  - Quick check question: How does a diffusion model use text embeddings during the image generation process to ensure semantic consistency with the prompt?

## Architecture Onboarding

- **Component map**: CLIP image encoder (frozen) → Linear probe (trainable) → Voxel activation predictions → Optimal embeddings → Softmax projection onto natural image embeddings → Caption generation (frozen CLIPCap) → Captions → Text-to-image diffusion model → Synthesized images → Brain activation prediction (separate encoder for validation)

- **Critical path**:
  1. Train linear probe on CLIP embeddings to predict voxel activations
  2. Derive optimal embeddings from learned weights
  3. Project optimal embeddings onto natural image embedding space
  4. Generate captions from projected embeddings
  5. Use captions for text-conditioned image synthesis
  6. Validate generated images activate target regions

- **Design tradeoffs**:
  - Using a frozen CLIP encoder simplifies training but limits adaptability to brain-specific features
  - Linear probes are computationally efficient but may miss nonlinear voxel response patterns
  - Text-based conditioning provides interpretability but may constrain image diversity compared to gradient-based approaches

- **Failure signatures**:
  - Low R² values during encoder training indicate poor alignment between CLIP features and voxel responses
  - Generated captions that don't match known functional regions suggest projection or caption generation issues
  - Synthesized images that fail to activate predicted regions indicate disconnect between caption semantics and actual brain responses

- **First 3 experiments**:
  1. Train the linear probe on a subset of voxels and validate R² performance to ensure the basic encoding framework works
  2. Test the softmax projection by comparing cosine similarity between original and projected weights with varying numbers of natural images
  3. Generate captions for a single known category-selective region (e.g., FFA) and manually verify they contain relevant semantic content (faces, people)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modality gap between optimal CLIP embeddings and natural image embeddings affect the quality and interpretability of generated captions across different brain regions?
- Basis in paper: [explicit] The paper discusses the modality gap and uses a softmax weighted sum to project voxel weights onto the space of natural images, but does not extensively explore how this affects caption quality across regions.
- Why unresolved: The paper mentions the method to close the gap but doesn't provide a detailed analysis of how this affects the semantic coherence of captions for different types of visual stimuli or brain regions.
- What evidence would resolve it: A systematic evaluation comparing captions generated with and without the projection step across different brain regions, along with human judgment of caption quality and semantic relevance to the region's known selectivity.

### Open Question 2
- Question: Can BrainSCUBA be extended to capture fine-grained selectivity for categories beyond "person," such as specific types of objects, scenes, or actions, and how would this impact its utility for neuroscientific discovery?
- Basis in paper: [inferred] The paper demonstrates BrainSCUBA's ability to capture person-related selectivity and mentions its potential for hypothesis-driven investigation, but does not explore its ability to capture selectivity for other fine-grained categories.
- Why unresolved: The paper focuses on person representations and does not test BrainSCUBA's capability to identify fine-grained selectivity for other semantic categories, limiting understanding of its full potential.
- What evidence would resolve it: Applying BrainSCUBA to analyze selectivity for a variety of semantic categories (e.g., animals, tools, actions) and comparing the results with known functional regions and previous studies.

### Open Question 3
- Question: How do the generated images from BrainSCUBA compare to those from BrainDiVE in terms of capturing the full complexity and variability of natural stimuli that activate specific brain regions?
- Basis in paper: [explicit] The paper compares BrainSCUBA-generated images with BrainDiVE in terms of semantic coherence and predicted activations, noting that BrainSCUBA images are more visually coherent.
- Why unresolved: While the paper shows that BrainSCUBA images are more coherent, it does not thoroughly investigate whether they capture the same level of complexity and variability as natural stimuli or BrainDiVE images.
- What evidence would resolve it: A detailed comparison of the diversity and representativeness of BrainSCUBA-generated images versus natural stimuli and BrainDiVE images, using metrics like image similarity, category coverage, and variability within and across categories.

## Limitations
- Reliance on linear approximations for voxel encoding may miss complex nonlinear interactions in brain responses
- Assumes CLIP's embedding space adequately captures semantic dimensions relevant to brain selectivity
- Requires extensive pre-trained models and large-scale datasets, limiting accessibility and reproducibility

## Confidence
- **High confidence**: The core mechanism of using CLIP embeddings to predict voxel responses and generate captions is well-supported by the quantitative results (R² values, semantic alignment with functional regions)
- **Medium confidence**: The effectiveness of the softmax projection and text-conditioned image synthesis, while promising, relies on fewer direct comparisons and could benefit from additional ablation studies
- **Medium confidence**: The exploratory discovery of fine-grained semantic selectivity in body-selective areas is compelling but requires further validation through independent experimental testing

## Next Checks
1. Perform ablation studies removing the softmax projection to quantify its contribution to caption quality and semantic alignment
2. Test the framework on independent fMRI datasets (different subjects or imaging protocols) to assess generalizability
3. Conduct behavioral validation by comparing generated captions against human-generated descriptions of the same stimuli