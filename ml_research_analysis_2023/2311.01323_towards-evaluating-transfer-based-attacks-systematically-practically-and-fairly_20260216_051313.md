---
ver: rpa2
title: Towards Evaluating Transfer-based Attacks Systematically, Practically, and
  Fairly
arxiv_id: '2311.01323'
source_url: https://arxiv.org/abs/2311.01323
tags:
- substitute
- adversarial
- methods
- attacks
- victim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper establishes TA-Bench, a comprehensive benchmark for
  evaluating transfer-based adversarial attacks, addressing the lack of a standardized
  framework for fair and practical comparisons. It implements 30+ methods across four
  categories: input augmentation/optimizer, gradient computation, substitute model
  training, and generative modeling.'
---

# Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly

## Quick Facts
- arXiv ID: 2311.01323
- Source URL: https://arxiv.org/abs/2311.01323
- Reference count: 40
- Primary result: TA-Bench provides comprehensive benchmark for 30+ transfer-based adversarial attack methods across 25 models, revealing vision transformers as superior substitute models

## Executive Summary
This paper introduces TA-Bench, a comprehensive benchmark for evaluating transfer-based adversarial attacks. The benchmark systematically implements and compares over 30 attack methods across four categories: input augmentation/optimizer, gradient computation, substitute model training, and generative modeling. Using ImageNet with 5,000 benign examples and 25 popular models, the authors provide the first standardized framework for fair and practical comparisons of transfer-based attacks. Key findings include the superiority of vision transformers as substitute models and the effectiveness of advanced optimization back-ends like UN-DP-DI2-TI-PI-FGSM. The benchmark also reveals critical insights about victim model preprocessing pipelines and their impact on attack evaluation.

## Method Summary
TA-Bench implements a systematic evaluation framework for transfer-based adversarial attacks. The method involves four main modules: (1) input augmentation and optimizer selection, (2) gradient computation methods, (3) substitute model training approaches, and (4) generative modeling techniques. Evaluations are conducted on 25 popular models using 5,000 ImageNet validation examples, with attacks generated under ℓ∞ and ℓ2 constraints (ϵ = 8/255 and ϵ = 5 respectively) using 100 iterations. The framework computes three key metrics: average attack accuracy (AAA), worst-case attack accuracy (WAA), and best-case attack accuracy (BAA). A unified model zoo and standardized preprocessing pipeline ensure fair comparisons across all methods.

## Key Results
- Vision transformers (ViT-B, DeiT-B, Swin-B) as substitute models achieve significantly higher attack success rates than convolutional models when attacking CNN victims
- UN-DP-DI2-TI-PI-FGSM, combining multiple input augmentations with momentum optimization, achieves the best average performance at 42.42% AAA
- Victim model preprocessing pipelines substantially impact attack evaluation, with average accuracy increasing from 86.16% to 87.79% when preprocessing is considered

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision transformers generally yield superior attack performance compared to traditional convolutional models when used as substitute models in transfer-based attacks.
- Mechanism: Vision transformers capture long-range dependencies and global context better than CNNs, leading to more transferable adversarial perturbations that exploit vulnerabilities present across different architectures.
- Core assumption: The architectural differences between transformers and CNNs create distinct vulnerability patterns that can be exploited to generate adversarial examples effective across both model types.
- Evidence anchors:
  - [abstract]: "New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided. Key findings include the superiority of vision transformers as substitute models..."
  - [section]: "It can be seen that transferring from vision transformers to convolutional networks seems easier. When utilizing ViT-B as the substitute model, the accuracy of convolutional networks shows a range in [28.32%, 37.24%]..."
  - [corpus]: Weak - no direct corpus evidence comparing transformer vs CNN substitute models.

### Mechanism 2
- Claim: Combining multiple input augmentation mechanisms (UN-DP-DI2-TI-PI-FGSM) creates more powerful attacks than using individual augmentations alone.
- Mechanism: Each augmentation addresses different aspects of transferability - UN adds noise robustness, DP reduces sensitivity to spatial patterns, DI2 provides transformation invariance, TI ensures translation invariance, and PI incorporates momentum for smoother optimization paths.
- Core assumption: The augmentations are complementary and their combination covers more failure modes of individual adversarial examples.
- Evidence anchors:
  - [section]: "In general, more input augmentation mechanisms leads to more powerful attacks... the optimal AAA is achieved by UN-DP-DI2-TI-PI-FGSM, which is 42.42%."
  - [section]: "The performance gap between MI-FGSM, NI-FGSM, and PI-FGSM is marginal... In most cases, PGD leads to slightly inferior performance than that of I-FGSM..."
  - [corpus]: Weak - no corpus evidence directly comparing multi-augmentation combinations.

### Mechanism 3
- Claim: Proper evaluation of transfer-based attacks requires accounting for victim model preprocessing pipelines.
- Mechanism: Victim models in practice include preprocessing steps (resize, crop, normalization) that are inaccessible to attackers, so evaluating without these steps overestimates vulnerability.
- Core assumption: The preprocessing pipeline significantly alters the input space and thus the effectiveness of adversarial perturbations.
- Evidence anchors:
  - [section]: "We observed degraded attack performance under such circumstances... the average accuracy of victim models increases to 87.79% (from 86.16%) with pre-processing..."
  - [section]: "Considering that the pre-processing pipeline of the victim model is inaccessible to the adversary, it is infeasible for the adversary to follow the same pipeline when generating adversarial examples."
  - [corpus]: Weak - no corpus evidence discussing preprocessing impact on transfer attack evaluation.

## Foundational Learning

- Concept: Adversarial examples and their transferability properties
  - Why needed here: The entire benchmark is built around evaluating how adversarial examples crafted on one model transfer to others, which is the fundamental premise of transfer-based attacks
  - Quick check question: Why do adversarial examples often transfer between different models trained on the same dataset?

- Concept: Gradient-based optimization for adversarial example generation
  - Why needed here: All 30+ methods in the benchmark use gradient-based optimization, so understanding how gradients are computed and applied is crucial
  - Quick check question: How does the choice of optimization method (I-FGSM vs PGD vs momentum-based) affect the convergence and transferability of adversarial examples?

- Concept: Vision transformer architectures vs convolutional neural networks
  - Why needed here: The benchmark explicitly compares the effectiveness of transformers vs CNNs as substitute models, requiring understanding their architectural differences
  - Quick check question: What are the key architectural differences between vision transformers and CNNs that might affect their vulnerability to adversarial attacks?

## Architecture Onboarding

- Component map: Input augmentation/optimizer selection -> Gradient computation -> Substitute model training -> Generative modeling -> Model zoo (25 models) -> Evaluation pipeline -> AAA/WAA/BAA metrics
- Critical path: For evaluating a new attack method: (1) Select substitute model from model zoo, (2) Apply preprocessing matching victim model requirements, (3) Generate adversarial examples using chosen method, (4) Evaluate against all victim models, (5) Compute AAA, WAA, BAA metrics.
- Design tradeoffs: The benchmark prioritizes comprehensiveness (30+ methods, 25 models) over computational efficiency, leading to long evaluation times but thorough comparisons. The modular design allows adding new methods without affecting existing implementations.
- Failure signatures: Common failure modes include: (1) Method crashes due to architectural incompatibility (e.g., ConBP requiring smooth activations), (2) Poor performance due to incorrect hyperparameter selection, (3) Results not reproducible due to random seed issues, (4) Memory overflow when using large models with complex augmentations.
- First 3 experiments:
  1. Run UN-DP-DI2-TI-PI-FGSM on ResNet-50 as substitute and evaluate on all victims to establish baseline performance
  2. Test SGM method on ViT-B substitute model to verify transformer advantage observation
  3. Compare NAA with I-FGSM baseline on the same substitute model to validate gradient computation improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of input augmentations and optimizers for transfer-based adversarial attacks across different model architectures?
- Basis in paper: [explicit] The paper systematically evaluates combinations of input augmentations and optimizers, finding that UN-DP-DI2-TI-PI-FGSM performs best on average.
- Why unresolved: While the paper identifies the best combination on average, the optimal combination may vary depending on the specific substitute and victim model architectures.
- What evidence would resolve it: Further experimentation with a wider range of model architectures and more extensive hyperparameter tuning could reveal architecture-specific optimal combinations.

### Open Question 2
- Question: How does the choice of substitute model architecture impact the transferability of adversarial examples to different victim model architectures?
- Basis in paper: [explicit] The paper demonstrates that vision transformers generally yield superior attack performance as substitute models compared to traditional convolutional models.
- Why unresolved: The paper focuses on a specific set of substitute and victim models, and the generalizability of these findings to other architectures remains unclear.
- What evidence would resolve it: Evaluating the transferability of adversarial examples generated on a broader range of substitute models to a diverse set of victim models would provide insights into the impact of substitute model architecture.

### Open Question 3
- Question: What is the relative importance of gradient computation methods versus substitute model training methods in enhancing adversarial transferability?
- Basis in paper: [explicit] The paper compares the effectiveness of gradient computation methods and substitute model training methods, finding that the MoreBayesian method significantly outperforms other methods in the substitute model training category.
- Why unresolved: The paper does not directly compare the performance of the best gradient computation method with the best substitute model training method.
- What evidence would resolve it: Conducting a head-to-head comparison between the top-performing gradient computation method and the top-performing substitute model training method across a range of models and attack scenarios would clarify their relative importance.

## Limitations

- Computational intensity limits exploration of hyperparameter spaces and may miss optimal configurations
- Focus on ImageNet may not generalize to other domains or task types
- Some methods show sensitivity to random initialization and training dynamics
- Preprocessing discrepancy between attackers and victims represents a fundamental limitation in realistic threat models

## Confidence

- High confidence: The superiority of vision transformers as substitute models (supported by multiple quantitative comparisons across different victim model types)
- Medium confidence: The effectiveness of multi-augmentation combinations (UN-DP-DI2-TI-PI-FGSM) due to potential overfitting to ImageNet characteristics
- Medium confidence: The impact of preprocessing pipelines on evaluation fairness, though the exact magnitude may vary with different preprocessing implementations

## Next Checks

1. Test the transferability of attacks across different datasets (e.g., CIFAR-10, medical imaging) to verify generalizability beyond ImageNet
2. Conduct ablation studies on the multi-augmentation combination to isolate which individual augmentations contribute most to performance gains
3. Evaluate the impact of different random seeds on attack success rates to quantify the stability of the observed performance differences