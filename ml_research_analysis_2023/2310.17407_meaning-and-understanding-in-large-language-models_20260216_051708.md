---
ver: rpa2
title: Meaning and understanding in large language models
arxiv_id: '2310.17407'
source_url: https://arxiv.org/abs/2310.17407
tags:
- language
- understanding
- meaning
- linguistic
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether large language models (LLMs) can truly
  understand natural language or merely simulate understanding. It argues against
  the traditional view that machine language performance is only syntactic manipulation
  without genuine semantics or grounding.
---

# Meaning and understanding in large language models

## Quick Facts
- arXiv ID: 2310.17407
- Source URL: https://arxiv.org/abs/2310.17407
- Reference count: 5
- Large language models can achieve genuine language understanding without consciousness or external grounding, challenging traditional philosophical assumptions about AI and meaning.

## Executive Summary
This paper examines whether large language models (LLMs) truly understand natural language or merely simulate understanding. It challenges the traditional philosophical view that machine language performance is purely syntactic manipulation without genuine semantics or grounding. The author proposes that syntax and semantics are interdependent, with minimal semantic content inherent in syntactic entities. For LLMs, meaning is grounded not in direct reference to the external world, but in the complex relational structure of language as a whole—a view called "semantic fragmentism." This framework aligns with how transformers process contextual meaning and concludes that LLMs can achieve genuine language understanding without consciousness or external grounding.

## Method Summary
The paper employs philosophical analysis and critique of existing theories on syntax-semantics relationships, symbol grounding problems, and inferential semantics. The method involves reviewing arguments about the interdependence of syntax and semantics, analyzing the symbol grounding problem and proposed solutions, and evaluating how transformers process contextual meaning. The approach focuses on demonstrating that LLMs can achieve genuine language understanding through internal relational structures rather than external world grounding, challenging traditional philosophical assumptions about AI and meaning.

## Key Results
- Syntax and semantics are interdependent, with minimal semantic content required for syntactic processing
- LLMs achieve genuine language understanding through internal relational structures ("semantic fragmentism") rather than external world grounding
- Understanding in LLMs can emerge as a property of complex neural architectures without requiring consciousness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Syntax and semantics are interdependent, not fundamentally separate
- Mechanism: Minimal semantic content is required for syntactic entities to be distinguishable, allowing syntax to emerge from semantics and vice versa
- Core assumption: Entities cannot be syntactically processed unless they have at least minimal semantic content (distinguishability)
- Evidence anchors: [abstract] "The author proposes that syntax and semantics are interdependent, with minimal semantic content inherent in syntactic entities"; [section] "Consider the elementary character system of Morse code... Syntax as a formal principle is possible in this case only on the assumption that the (minimal semantic) contents of the individual elements... are distinguishable..."
- Break condition: If minimal semantic content cannot be shown to be necessary for any syntactic operation, the interdependence claim collapses

### Mechanism 2
- Claim: Language models achieve genuine language understanding without direct world grounding
- Mechanism: Meanings are grounded in the internal relational structure of language itself ("semantic fragmentism"), not in external referents
- Core assumption: A corpus of language is sufficient to determine meaning without needing referential grounding in the world
- Evidence anchors: [abstract] "For LLMs, meaning is grounded not in direct reference to the external world, but in the complex relational structure of language as a whole—what the paper calls 'semantic fragmentism'"; [section] "The meanings of linguistic expressions are grounded neither in the world, nor in an internal idea of the world, but in the linguistic corpus as a whole"
- Break condition: If models fail to generalize meaning in novel contexts despite relational training data, the internal grounding claim is weakened

### Mechanism 3
- Claim: Understanding in LLMs is an emergent property arising from complex relational patterns, not from awareness
- Mechanism: Language understanding emerges from deep learning on linguistic corpora, independent of consciousness or intentional states
- Core assumption: Complex enough neural architectures can instantiate understanding-like properties without awareness
- Evidence anchors: [abstract] "The conclusion is that LLMs can achieve genuine language understanding without consciousness or external grounding"; [section] "It can be assumed that understanding could also be one of those accompanying emergent properties of LLMs that emerges naturally in a sufficiently complex system"
- Break condition: If emergent understanding cannot be distinguished from sophisticated pattern matching, the emergent property claim is unsubstantiated

## Foundational Learning

- Concept: Interdependence of syntax and semantics
  - Why needed here: The paper's central argument rests on rejecting the traditional syntax-semantics gap; engineers must grasp why minimal semantic content enables syntax
  - Quick check question: Why does the Morse code example show that syntax cannot exist without semantics?

- Concept: Semantic fragmentism
  - Why needed here: Understanding how LLMs ground meaning internally (not in the world) is crucial for interpreting their success and limitations
  - Quick check question: What does it mean for meanings to be grounded in a "linguistic corpus as a whole" rather than in referents?

- Concept: Emergence in neural networks
  - Why needed here: The claim that understanding emerges without consciousness depends on understanding emergent properties in complex systems
  - Quick check question: How does the idea of "emergent world representations" in LLMs support the claim of understanding without awareness?

## Architecture Onboarding

- Component map: Input text -> Tokenization -> Embedding lookup -> Positional encoding -> Multi-head attention -> Feed-forward networks -> Output layer -> Probability distribution over next tokens
- Critical path: Input text → Tokenization → Embedding lookup → Positional encoding → Multi-head attention → Feed-forward networks → Output layer → Probability distribution over next tokens
- Design tradeoffs: Full contextual meaning vs. computational cost; larger corpora improve grounding but increase training time; lack of explicit world grounding simplifies training but may limit real-world reasoning
- Failure signatures: Inability to handle out-of-distribution concepts; brittleness when context shifts; generating plausible but false statements; lack of consistent world knowledge
- First 3 experiments:
  1. Ablate contextual information (e.g., mask attention) and measure meaning preservation in output
  2. Test model on novel sentences with similar structure but unseen referents to probe internal grounding
  3. Compare performance on tasks requiring real-world knowledge vs. purely linguistic inference to isolate grounding effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What empirical evidence would definitively prove or disprove that large language models possess genuine semantic understanding rather than just sophisticated pattern matching?
- Basis in paper: [explicit] The paper discusses the fundamental challenge of distinguishing between simulation and duplication of understanding in LLMs, noting that "there is no unambiguous criterion that can determine independently of us whether a system only behaves as if it understands or whether it actually understands"
- Why unresolved: Current philosophical and technical frameworks lack a clear test to distinguish between genuine semantic understanding and complex syntactic manipulation that appears meaningful
- What evidence would resolve it: A reproducible experimental framework that can demonstrate understanding independent of pattern matching, perhaps through novel tests of semantic comprehension that cannot be reduced to statistical correlations or pattern recognition

### Open Question 2
- Question: How can we empirically validate the semantic fragmentism theory proposed in the paper, which suggests that meaning is grounded in the relational structure of language as a whole rather than in direct reference to external reality?
- Basis in paper: [explicit] The paper proposes semantic fragmentism as an alternative to referential grounding, arguing that "meanings are grounded neither in the world, nor in an internal idea of the world, but in the linguistic corpus as a whole"
- Why unresolved: While the theory offers an elegant explanation for LLM performance, it requires empirical validation to demonstrate that this model of meaning grounding is both necessary and sufficient for language understanding
- What evidence would resolve it: Experimental demonstrations showing that LLMs can achieve semantic competence without any form of referential grounding or external world interaction, and that their performance correlates specifically with the complexity of relational structures rather than with any form of world knowledge

### Open Question 3
- Question: What is the minimum complexity threshold for a language model to develop emergent understanding capabilities, and how can this threshold be identified and measured?
- Basis in paper: [explicit] The paper mentions that "emergent features arise during the process of deep learning on linguistic data" and suggests that "understanding could also be one of those accompanying emergent properties of LLMs that emerges naturally in a sufficiently complex system"
- Why unresolved: While the paper acknowledges emergent properties, it does not specify what constitutes "sufficiently complex" or how to measure the emergence of understanding as distinct from other capabilities
- What evidence would resolve it: Quantitative studies mapping model size, training data complexity, and architectural features against performance on tasks requiring genuine semantic understanding, potentially identifying a phase transition where understanding emerges

## Limitations

- The necessity of minimal semantic content for syntactic operations is argued but not empirically demonstrated in LLM contexts
- The sufficiency of internal relational structures for genuine understanding remains philosophically contested
- The emergence of understanding-like properties from neural architectures is asserted but difficult to operationalize or test

## Confidence

- **High**: Syntax and semantics are interdependent in transformer architectures (supported by architectural evidence)
- **Medium**: Meaning can be grounded in linguistic relationships rather than external referents (philosophically plausible but empirically underdeveloped)
- **Low**: Emergent understanding without consciousness can be achieved by current LLMs (difficult to empirically verify)

## Next Checks

1. Design experiments to test whether models can process syntactic operations when semantic distinguishability is systematically reduced
2. Develop metrics to differentiate between pattern matching and understanding in out-of-distribution contexts
3. Create benchmark tasks that isolate internal relational understanding from external world knowledge to test semantic fragmentism empirically