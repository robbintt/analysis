---
ver: rpa2
title: The Synergy of Speculative Decoding and Batching in Serving Large Language
  Models
arxiv_id: '2310.18813'
source_url: https://arxiv.org/abs/2310.18813
tags:
- speculation
- length
- decoding
- batch
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the low hardware utilization of Large Language
  Models (LLMs) during inference due to their sequential token-by-token generation
  process. To tackle this, the authors study the synergy between two techniques: batching,
  which merges multiple user requests to increase parallelism, and speculative decoding,
  which uses a smaller model to predict multiple tokens and then verifies them in
  parallel with the original LLM.'
---

# The Synergy of Speculative Decoding and Batching in Serving Large Language Models

## Quick Facts
- arXiv ID: 2310.18813
- Source URL: https://arxiv.org/abs/2310.18813
- Reference count: 11
- Key outcome: Adaptive speculative decoding strategy achieves up to 9% latency reduction by dynamically adjusting speculation length based on batch size

## Executive Summary
This paper addresses the challenge of low GPU hardware utilization in LLM inference caused by sequential token-by-token generation. The authors study how batching and speculative decoding can work together synergistically, finding that optimal speculation length varies inversely with batch size - larger batches need shorter speculation lengths. They propose an adaptive strategy that dynamically adjusts speculation length based on current batch size, achieving equal or better performance compared to fixed speculation schemes across both uniform and time-varying traffic patterns.

## Method Summary
The method involves profiling different combinations of batch sizes and speculation lengths to build a look-up table mapping optimal speculation lengths to batch sizes. During execution, the system dynamically selects the optimal speculation length for incoming requests. The authors use OPT-6.7B as the LLM and OPT-125M as the small speculative model, testing on NVIDIA RTX 3090, RTX 4090, and A100 GPUs with the Chatbot Instruction Prompts dataset. A quantitative model is developed to explain the observed relationship between speculation length, batch size, and latency.

## Key Results
- Optimal speculation length decreases as batch size increases, with batch size 1 performing best at length 4-5 and batch size 32 at length 1-2
- Adaptive speculation length selection achieves up to 9% latency reduction in time-varying request scenarios compared to fixed schemes
- Fixed speculation lengths that are optimal for one batch size can cause significant performance degradation for other batch sizes
- The adaptive strategy maintains high performance across both uniform and dynamic traffic patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal speculation length depends on batch size
- Mechanism: Larger batch sizes already fully utilize GPU resources, so longer speculation lengths create unnecessary verification overhead. Smaller batch sizes have underutilized GPUs, so longer speculation lengths help saturate the hardware.
- Core assumption: GPU utilization is the primary bottleneck in LLM inference.
- Evidence anchors: Abstract observation about batch size dependence; section finding that large batches fully utilize GPU resources; corpus papers focus on speculation control but not batch size interaction.

### Mechanism 2
- Claim: Speculative decoding uses a smaller model to predict tokens that are then verified by the original LLM in parallel
- Mechanism: The small speculative model generates a sequence of tokens, which are verified by the LLM. Incorrect predictions are corrected, and verification is parallelized to increase hardware utilization.
- Core assumption: The SSM can accurately predict tokens, and verification overhead is less than time saved by parallelization.
- Evidence anchors: Abstract description of parallel verification; section explanation of SSM and LLM roles; corpus papers discuss speculation but lack detailed mechanism explanations.

### Mechanism 3
- Claim: Adaptive speculation length selection improves performance across varying traffic patterns
- Mechanism: By profiling optimal configurations and dynamically adjusting based on current batch sizes, the system maintains high performance as request patterns change, avoiding degradation from fixed speculation lengths.
- Core assumption: Traffic patterns are variable enough that fixed speculation lengths will often be suboptimal.
- Evidence anchors: Abstract proposal of adaptive strategy; section description of dynamic adjustment; corpus papers discuss adaptation but not specifically for batch size variation.

## Foundational Learning

- Concept: GPU hardware utilization in deep learning
  - Why needed here: Understanding GPU utilization is crucial for grasping why optimal speculation length varies with batch size.
  - Quick check question: Why does increasing batch size generally improve GPU utilization in LLM inference?

- Concept: Autoregressive generation in LLMs
  - Why needed here: Speculative decoding accelerates this sequential process, so understanding how it works is key to the paper's contributions.
  - Quick check question: What is the main challenge of autoregressive generation that speculative decoding aims to address?

- Concept: Profiling and adaptive configuration in serving systems
  - Why needed here: The adaptive strategy relies on profiling optimal configurations and dynamically adjusting based on current conditions.
  - Quick check question: Why might profiling overhead be negligible in long-running LLM serving tasks?

## Architecture Onboarding

- Component map: Request queue -> Batching module -> Speculative decoding engine -> Adaptive configuration manager -> LLM serving backend
- Critical path: Request → Batching → Speculative decoding (with adaptive length selection) → LLM verification → Response
- Design tradeoffs:
  - Batching vs. latency: Larger batches improve throughput but increase individual request latency
  - Speculation length vs. accuracy: Longer speculation can waste computation if predictions are wrong
  - Profiling frequency vs. overhead: More frequent profiling adapts better but adds overhead
- Failure signatures:
  - Consistently high latency despite low load: May indicate speculation length too long for current batch size
  - GPU underutilized: May need to increase speculation length or batch size
  - Memory errors: Batch size may be too large for available memory
- First 3 experiments:
  1. Measure latency vs. speculation length for batch size 1 to establish baseline relationship
  2. Measure latency vs. speculation length for batch size 32 to confirm inverse relationship
  3. Implement and test adaptive speculation length selection with time-varying batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal speculation length vary with different types of LLM architectures beyond OPT and Llama models?
- Basis in paper: The paper tests OPT and Llama models but suggests optimal speculation length depends on model characteristics.
- Why unresolved: Only two model families are evaluated, leaving uncertainty about generalizability to other architectures like GPT-3, GPT-4, or domain-specific models.
- What evidence would resolve it: Systematic evaluation across diverse LLM architectures including autoregressive, encoder-decoder, and hybrid models.

### Open Question 2
- Question: What is the theoretical limit of speculation length before verification overhead outweighs benefits, and how does this vary with GPU memory bandwidth?
- Basis in paper: The paper notes very large speculation lengths can deteriorate performance but doesn't quantify the exact threshold.
- Why unresolved: Empirical observations are provided without theoretical framework for speculation length ceiling.
- What evidence would resolve it: Mathematical modeling of verification overhead versus speculation length, validated across different GPU memory bandwidths and model sizes.

### Open Question 3
- Question: How does the adaptive speculation strategy perform under heterogeneous GPU cluster environments with varying compute capabilities?
- Basis in paper: The paper evaluates on RTX 3090, RTX 4090, and A100 but doesn't address heterogeneous cluster scenarios.
- Why unresolved: Real-world deployments often involve heterogeneous GPU fleets, which could affect speculation length optimization.
- What evidence would resolve it: Performance evaluation across multi-GPU heterogeneous clusters with dynamic load balancing.

### Open Question 4
- Question: Can speculative decoding be effectively combined with other LLM optimization techniques like quantization or pruning without compromising speculation accuracy?
- Basis in paper: The paper focuses on batching and speculative decoding synergy but doesn't explore combinations with other optimization methods.
- Why unresolved: Modern LLM serving often employs multiple optimization techniques simultaneously, and their interactions remain unexplored.
- What evidence would resolve it: Comparative analysis of speculation decoding performance when combined with quantization, pruning, or distillation techniques.

## Limitations
- Limited to OPT models and NVIDIA GPUs, raising questions about generalizability to other architectures
- Optimal configurations must be profiled for each new deployment environment, potentially adding operational overhead
- Performance benefits depend on variable traffic patterns, with fixed speculation potentially sufficient for stable workloads

## Confidence
**High confidence**: The core mechanism linking batch size to optimal speculation length is well-supported by empirical evidence across multiple batch sizes and hardware configurations.

**Medium confidence**: The adaptive strategy's performance benefits under dynamic traffic patterns are demonstrated but may be sensitive to the specific traffic generation model used.

**Medium confidence**: The quantitative model explaining the relationship between speculation length, batch size, and latency provides theoretical grounding but depends on assumptions about GPU utilization that may vary across deployments.

## Next Checks
1. **Cross-architecture validation**: Test the adaptive speculation length strategy on a different LLM architecture (e.g., LLaMA, BLOOM) and hardware (e.g., AMD GPUs or cloud TPUs) to verify generalizability.

2. **Production deployment monitoring**: Implement the adaptive strategy in a real-world LLM serving system with actual user traffic to measure performance benefits over extended periods.

3. **Sensitivity analysis to traffic patterns**: Systematically vary traffic distribution parameters (mean, variance, burstiness) to determine the range of conditions where adaptive strategy provides meaningful improvements versus when fixed speculation suffices.