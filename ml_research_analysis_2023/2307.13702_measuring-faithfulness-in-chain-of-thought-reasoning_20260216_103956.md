---
ver: rpa2
title: Measuring Faithfulness in Chain-of-Thought Reasoning
arxiv_id: '2307.13702'
source_url: https://arxiv.org/abs/2307.13702
tags:
- reasoning
- answer
- tasks
- faithfulness
- thought
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a set of methods to measure faithfulness of
  Chain-of-Thought (CoT) reasoning in LLMs, where faithfulness means the reasoning
  accurately reflects the model's true reasoning process. The authors hypothesize
  that CoT may be unfaithful due to post-hoc reasoning, extra test-time computation,
  or encoded information in phrasing.
---

# Measuring Faithfulness in Chain-of-Thought Reasoning

## Quick Facts
- **arXiv ID**: 2307.13702
- **Source URL**: https://arxiv.org/abs/2307.13702
- **Reference count**: 26
- **Key outcome**: Models vary greatly in how much they rely on CoT across different tasks

## Executive Summary
This paper proposes methods to measure the faithfulness of Chain-of-Thought (CoT) reasoning in LLMs, where faithfulness means the reasoning accurately reflects the model's true reasoning process. The authors test hypotheses about CoT unfaithfulness through interventions like truncation, mistake injection, filler tokens, and paraphrasing. They find that CoT faithfulness varies significantly across tasks and model sizes, with smaller models often producing more faithful reasoning than larger ones on most tasks.

## Method Summary
The paper investigates CoT faithfulness by intervening on the CoT reasoning process and measuring how model answers change. They use multiple choice tasks and test various intervention methods: truncating CoT steps to detect post-hoc reasoning, adding mistakes to measure reasoning dependency, replacing CoT with filler tokens to test computation benefits, and paraphrasing CoT to check phrasing effects. The experiments are conducted across different model sizes (13B, 175B parameters) and multiple tasks including ARC Challenge, AQuA, HellaSwag, and others.

## Key Results
- Adding mistakes to CoT changes answers more on tasks where CoT is less faithful, indicating post-hoc reasoning
- Replacing CoT with filler tokens does not improve accuracy, suggesting extra computation alone doesn't explain CoT's benefit
- Paraphrasing CoT does not reduce accuracy, indicating phrasing doesn't encode crucial information
- Smaller models often produce more faithful reasoning than larger ones on most tasks
- On easier tasks, smaller models show more faithful reasoning than larger ones

## Why This Works (Mechanism)

### Mechanism 1: Post-hoc Reasoning Detection Through Truncation
- Claim: Truncating CoT steps reveals whether reasoning is post-hoc by measuring if answer changes with reduced reasoning steps
- Mechanism: Models rely on complete CoT for answers if reasoning is faithful; truncation reduces this reliance if reasoning is post-hoc
- Core assumption: If CoT is faithful, removing steps should change the answer because each step contributes causally to the conclusion
- Evidence anchors:
  - [abstract] "CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT"
  - [section] "If the model is no longer updating its answer based on further steps of the chain of thought, it stands to reason that the produced reasoning is post-hoc"
  - [corpus] Weak evidence - only 1 of 5 related papers directly addresses truncation methods for faithfulness measurement
- Break condition: Models use reasoning steps as memory triggers rather than logical steps, making truncation ineffective for detecting post-hoc reasoning

### Mechanism 2: Mistake Injection Reveals Reasoning Dependency
- Claim: Adding mistakes to CoT steps and observing answer changes measures reasoning faithfulness
- Mechanism: If model's answer changes when CoT contains mistakes, the model was actually using the CoT reasoning
- Core assumption: Faithful reasoning requires the model to follow each step logically; mistakes should derail this process
- Evidence anchors:
  - [abstract] "We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT"
  - [section] "If inserting a mistake into the CoT changes the model's final answer, then the model is likely not ignoring the CoT"
  - [corpus] Moderate evidence - 2 related papers mention mistake injection or similar perturbation techniques
- Break condition: Models can recover from mistakes through independent reasoning, making mistake injection unreliable for faithfulness detection

### Mechanism 3: Paraphrasing Ablates Phrasing-Dependent Information
- Claim: Paraphrasing CoT steps while maintaining meaning tests if phrasing encodes reasoning information
- Mechanism: If paraphrased CoT produces same answers as original, reasoning information is in content not phrasing
- Core assumption: Faithful reasoning conveys information through semantic content rather than linguistic patterns
- Evidence anchors:
  - [abstract] "We find similar performance when replacing CoT with paraphrased CoT, indicating that the particular phrasing of the CoT is not a driver of performance"
  - [section] "If phrasing-encoded information is responsible for the accuracy boost conveyed by CoT, we should expect to see degraded performance under paraphrased reasoning"
  - [corpus] Weak evidence - only 1 related paper mentions paraphrasing in the context of CoT faithfulness
- Break condition: Models use subtle phrasing patterns for reasoning that paraphrasing preserves, making this test ineffective

## Foundational Learning

- Concept: Causal inference through intervention
  - Why needed here: The paper relies on experimental interventions (truncation, mistakes, paraphrasing) to infer causal relationships between CoT and answers
  - Quick check question: If you intervene on X and Y changes, what can you conclude about the relationship between X and Y?

- Concept: Model conditioning and probability distributions
  - Why needed here: Understanding how models condition on CoT steps and generate answers based on these conditions is crucial for interpreting faithfulness measurements
  - Quick check question: How does a language model's probability distribution over answers change when given different CoT inputs?

- Concept: Scaling laws and model capability
  - Why needed here: The paper finds inverse scaling in faithfulness, requiring understanding of how model size affects reasoning capabilities
  - Quick check question: What relationship between model size and task performance would you expect for tasks requiring explicit reasoning?

## Architecture Onboarding

- Component map: Data collection pipeline → Intervention generation → Model inference → Answer comparison → Faithfulness metrics
- Critical path: Generate CoT → Apply intervention → Get model answer → Compare to baseline → Calculate faithfulness metric
- Design tradeoffs: Computational cost vs measurement granularity (running full early answering vs. single point measurements)
- Failure signatures: Uniform answer distribution across interventions suggests model ignores CoT; perfect preservation suggests post-hoc reasoning
- First 3 experiments:
  1. Run early answering truncation on a single task to verify post-hoc reasoning detection works
  2. Implement mistake injection on the same task to validate consistency with truncation results
  3. Test paraphrasing intervention to confirm phrasing independence of reasoning information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model size affect CoT faithfulness across different types of tasks beyond the ones studied in this paper?
- Basis in paper: Explicit
- Why unresolved: The paper only studies faithfulness across model sizes on a limited set of tasks (8 multiple choice tasks and synthetic addition tasks). It's unclear if the inverse scaling relationship between model size and faithfulness observed on these tasks generalizes to other task types or domains.
- What evidence would resolve it: Conducting a comprehensive study of CoT faithfulness across a wider variety of task types (e.g., different domains, formats, reasoning types) and model sizes would provide evidence for or against the generalizability of the observed inverse scaling relationship.

### Open Question 2
- Question: Can the faithfulness of CoT be improved through model training techniques beyond the RLHF fine-tuning used in this paper?
- Basis in paper: Inferred
- Why unresolved: The paper uses RLHF-finetuned models and observes varying levels of CoT faithfulness. However, it does not investigate whether alternative training methods or architectural modifications could lead to more faithful reasoning. Pretrained LLMs, for example, may behave differently in terms of CoT faithfulness.
- What evidence would resolve it: Experimenting with different training objectives, architectures, or fine-tuning strategies (e.g., training with explicit faithfulness incentives, using different model architectures) and evaluating their impact on CoT faithfulness would provide evidence for or against the potential for improving faithfulness through training.

### Open Question 3
- Question: How does the length of the CoT impact its faithfulness, and is there an optimal length for faithful reasoning?
- Basis in paper: Inferred
- Why unresolved: The paper truncates CoT at different points to measure post-hoc reasoning, but it does not systematically investigate how the overall length of the CoT affects faithfulness. It's unclear if there's a point at which adding more reasoning steps becomes detrimental to faithfulness.
- What evidence would resolve it: Conducting experiments that vary the length of the CoT (e.g., by generating reasoning with a fixed number of steps or until a certain condition is met) and measuring faithfulness at each length would provide evidence for or against the existence of an optimal CoT length for faithful reasoning.

## Limitations

- The causal interpretation of intervention results may not accurately reflect true reasoning versus other factors like attention mechanisms or learned heuristics
- Measurement methodology assumes answer changes directly reflect reasoning process, which may not hold if models use internal mechanisms beyond observable CoT
- Findings based on limited model scales (13B, 175B) may not generalize across full spectrum of model sizes or different architectural families

## Confidence

**High Confidence**: CoT interventions can successfully detect unfaithful reasoning behaviors; Paraphrasing does not degrade CoT performance; Model size effects on faithfulness are observable across multiple tasks

**Medium Confidence**: Post-hoc reasoning detection through truncation is reliable for identifying some unfaithful behaviors; Mistake injection effectively reveals reasoning dependency in most tested scenarios; Filler tokens do not improve accuracy

**Low Confidence**: The precise relationship between model size and faithfulness across all task difficulties; Whether observed behaviors generalize to non-fine-tuned or differently-trained models; The extent to which findings apply to generation-only versus instruction-tuned models

## Next Checks

1. **Causal Attribution Validation**: Conduct ablation studies where both CoT and internal attention patterns are manipulated simultaneously to determine whether answer changes stem from reasoning fidelity or other mechanisms

2. **Cross-Architecture Replication**: Test the intervention methods on different model families (e.g., decoder-only vs encoder-decoder, sparse vs dense attention) to verify that observed faithfulness patterns are architecture-independent

3. **Longitudinal Faithfulness Tracking**: Implement continuous monitoring of CoT faithfulness across multiple training checkpoints to determine whether unfaithful behaviors emerge during training or are present from initialization